<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Audio-Visual Speech Enhancement In Complex Scenarios With Separation And   Dereverberation Joint Modeling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5332907d9467d4304128133b4b0caee7')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="Audio-Visual-Speech-Enhancement-In-Complex-Scenarios-With-Separation-And-Dereverberation-Joint-Modeling"><a href="#Audio-Visual-Speech-Enhancement-In-Complex-Scenarios-With-Separation-And-Dereverberation-Joint-Modeling" class="headerlink" title="Audio-Visual Speech Enhancement In Complex Scenarios With Separation And   Dereverberation Joint Modeling"></a>Audio-Visual Speech Enhancement In Complex Scenarios With Separation And   Dereverberation Joint Modeling</h2><p><strong>Authors:Jiarong Du, Zhan Jin, Peijun Yang, Juan Liu, Zhuo Li, Xin Liu, Ming Li</strong></p>
<p>Audio-visual speech enhancement (AVSE) is a task that uses visual auxiliary information to extract a target speakerâ€™s speech from mixed audio. In real-world scenarios, there often exist complex acoustic environments, accompanied by various interfering sounds and reverberation. Most previous methods struggle to cope with such complex conditions, resulting in poor perceptual quality of the extracted speech. In this paper, we propose an effective AVSE system that performs well in complex acoustic environments. Specifically, we design a â€œseparation before dereverberationâ€ pipeline that can be extended to other AVSE networks. The 4th COGMHEAR Audio-Visual Speech Enhancement Challenge (AVSEC) aims to explore new approaches to speech processing in multimodal complex environments. We validated the performance of our system in AVSEC-4: we achieved excellent results in the three objective metrics on the competition leaderboard, and ultimately secured first place in the human subjective listening test. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰æ˜¯ä¸€é¡¹åˆ©ç”¨è§†è§‰è¾…åŠ©ä¿¡æ¯ä»æ··åˆéŸ³é¢‘ä¸­æå–ç›®æ ‡è¯´è¯äººè¯­éŸ³çš„ä»»åŠ¡ã€‚åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œé€šå¸¸å­˜åœ¨å¤æ‚çš„å£°å­¦ç¯å¢ƒï¼Œä¼´éšç€å„ç§å¹²æ‰°å£°éŸ³å’Œå›å£°ã€‚ä¹‹å‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½å¾ˆéš¾åº”å¯¹è¿™æ ·çš„å¤æ‚æ¡ä»¶ï¼Œå¯¼è‡´æå–çš„è¯­éŸ³æ„ŸçŸ¥è´¨é‡è¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½çš„æœ‰æ•ˆAVSEç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªâ€œå…ˆåˆ†ç¦»åå»æ··å“â€çš„ç®¡é“ï¼Œå¯æ‰©å±•åˆ°å…¶ä»–AVSEç½‘ç»œã€‚ç¬¬4å±ŠCOGMHEARè§†å¬è¯­éŸ³å¢å¼ºæŒ‘æˆ˜ï¼ˆAVSECï¼‰æ—¨åœ¨æ¢ç´¢åœ¨å¤šç§æ¨¡å¼å¤æ‚ç¯å¢ƒä¸‹çš„è¯­éŸ³å¤„ç†æ–°æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨AVSEC-4ä¸­éªŒè¯äº†ç³»ç»Ÿçš„æ€§èƒ½ï¼šåœ¨ç«èµ›æ’è¡Œæ¦œä¸Šçš„ä¸‰ä¸ªå®¢è§‚æŒ‡æ ‡ä¸­å–å¾—äº†ä¼˜å¼‚æˆç»©ï¼Œå¹¶åœ¨ä¸»è§‚äººå·¥å¬åŠ›æµ‹è¯•ä¸­è·å¾—ç¬¬ä¸€åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26825v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†è§†å¬è¯­éŸ³å¢å¼ºç³»ç»Ÿï¼Œé€šè¿‡ä½¿ç”¨è§†è§‰è¾…åŠ©ä¿¡æ¯ä»æ··åˆéŸ³é¢‘ä¸­æå–ç›®æ ‡è¯´è¯äººçš„è¯­éŸ³ã€‚åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸­ï¼Œè¯¥ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œè®¾è®¡äº†ä¸€ä¸ªâ€œå…ˆåˆ†ç¦»åå»æ··å“â€çš„ç®¡é“ï¼Œå¯æ‰©å±•åˆ°å…¶ä»–è§†å¬è¯­éŸ³å¢å¼ºç½‘ç»œã€‚åœ¨ç¬¬å››å±Šä¸­å›½è§†å¬è¯­éŸ³å¢å¼ºæŒ‘æˆ˜èµ›ï¼ˆAVSEC-4ï¼‰ä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¸‰ä¸ªå®¢è§‚æŒ‡æ ‡ä¸Šå–å¾—ä¼˜å¼‚ç»“æœï¼Œå¹¶åœ¨ä¸»è§‚å¬åŠ›æµ‹è¯•ä¸­è£è·ç¬¬ä¸€åã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è§†å¬è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰æ˜¯åˆ©ç”¨è§†è§‰è¾…åŠ©ä¿¡æ¯ä»æ··åˆéŸ³é¢‘ä¸­æå–ç›®æ ‡è¯´è¯äººè¯­éŸ³çš„ä»»åŠ¡ã€‚</li>
<li>å¤æ‚å£°å­¦ç¯å¢ƒå’Œå„ç§å¹²æ‰°å£°éŸ³å¯¹è¯­éŸ³æå–é€ æˆæŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½çš„AVSEç³»ç»Ÿï¼Œé‡‡ç”¨â€œå…ˆåˆ†ç¦»åå»æ··å“â€çš„è®¾è®¡ç®¡é“ã€‚</li>
<li>è¯¥ç³»ç»Ÿå¯æ‰©å±•åˆ°å…¶ä»–AVSEç½‘ç»œã€‚</li>
<li>åœ¨ç¬¬å››å±Šä¸­å›½è§†å¬è¯­éŸ³å¢å¼ºæŒ‘æˆ˜èµ›ï¼ˆAVSEC-4ï¼‰ä¸­éªŒè¯äº†ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨ä¸‰ä¸ªå®¢è§‚æŒ‡æ ‡ä¸Šå–å¾—ä¼˜å¼‚ç»“æœï¼Œè¡¨ç°é¢†å…ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8f95170d53bbf080570248c7e268f6e" align="middle">
<img src="https://picx.zhimg.com/v2-9f8ad61e5f32b3c51470c884fd398f6e" align="middle">
<img src="https://picx.zhimg.com/v2-8382c26522ab681dc5ae1f7244378df9" align="middle">
<img src="https://picx.zhimg.com/v2-4a360970ed6a4aaf081b0261be3e877b" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="See-the-Speaker-Crafting-High-Resolution-Talking-Faces-from-Speech-with-Prior-Guidance-and-Region-Refinement"><a href="#See-the-Speaker-Crafting-High-Resolution-Talking-Faces-from-Speech-with-Prior-Guidance-and-Region-Refinement" class="headerlink" title="See the Speaker: Crafting High-Resolution Talking Faces from Speech with   Prior Guidance and Region Refinement"></a>See the Speaker: Crafting High-Resolution Talking Faces from Speech with   Prior Guidance and Region Refinement</h2><p><strong>Authors:Jinting Wang, Jun Wang, Hei Victor Cheng, Li Liu</strong></p>
<p>Unlike existing methods that rely on source images as appearance references and use source speech to generate motion, this work proposes a novel approach that directly extracts information from the speech, addressing key challenges in speech-to-talking face. Specifically, we first employ a speech-to-face portrait generation stage, utilizing a speech-conditioned diffusion model combined with statistical facial prior and a sample-adaptive weighting module to achieve high-quality portrait generation. In the subsequent speech-driven talking face generation stage, we embed expressive dynamics such as lip movement, facial expressions, and eye movements into the latent space of the diffusion model and further optimize lip synchronization using a region-enhancement module. To generate high-resolution outputs, we integrate a pre-trained Transformer-based discrete codebook with an image rendering network, enhancing video frame details in an end-to-end manner. Experimental results demonstrate that our method outperforms existing approaches on the HDTF, VoxCeleb, and AVSpeech datasets. Notably, this is the first method capable of generating high-resolution, high-quality talking face videos exclusively from a single speech input. </p>
<blockquote>
<p>ä¸åŒäºä¾èµ–æºå›¾åƒä½œä¸ºå¤–è§‚å‚è€ƒå¹¶ä½¿ç”¨æºè¯­éŸ³ç”Ÿæˆè¿åŠ¨çš„ç°æœ‰æ–¹æ³•ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§ç›´æ¥ä»è¯­éŸ³ä¸­æå–ä¿¡æ¯çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†è¯­éŸ³åˆ°è¯´è¯äººè„¸çš„å…³é”®æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨è¯­éŸ³åˆ°äººè„¸è‚–åƒç”Ÿæˆé˜¶æ®µï¼Œåˆ©ç”¨å—è¯­éŸ³æ§åˆ¶çš„æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆç»Ÿè®¡é¢éƒ¨å…ˆéªŒå’Œæ ·æœ¬è‡ªé€‚åº”åŠ æƒæ¨¡å—ï¼Œå®ç°é«˜è´¨é‡è‚–åƒç”Ÿæˆã€‚åœ¨éšåçš„è¯­éŸ³é©±åŠ¨è¯´è¯äººè„¸ç”Ÿæˆé˜¶æ®µï¼Œæˆ‘ä»¬å°†è¡¨è¾¾æ€§åŠ¨æ€ï¼ˆå¦‚å˜´å”‡è¿åŠ¨ã€é¢éƒ¨è¡¨æƒ…å’Œçœ¼ç›è¿åŠ¨ï¼‰åµŒå…¥æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶è¿›ä¸€æ­¥ä½¿ç”¨åŒºåŸŸå¢å¼ºæ¨¡å—ä¼˜åŒ–å˜´å”‡åŒæ­¥ã€‚ä¸ºäº†ç”Ÿæˆé«˜åˆ†è¾¨ç‡è¾“å‡ºï¼Œæˆ‘ä»¬å°†é¢„è®­ç»ƒçš„åŸºäºTransformerçš„ç¦»æ•£ç æœ¬ä¸å›¾åƒæ¸²æŸ“ç½‘ç»œç›¸ç»“åˆï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å¢å¼ºè§†é¢‘å¸§ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨HDTFã€VoxCelebå’ŒAVSpeechæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯ç¬¬ä¸€ç§èƒ½å¤Ÿä»…ä»å•ä¸ªè¯­éŸ³è¾“å…¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é«˜è´¨é‡è¯´è¯äººè„¸è§†é¢‘çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26819v1">PDF</a> 16 pages,15 figures, accepted by TASLP</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼Œç›´æ¥ä»è¯­éŸ³ä¸­æå–ä¿¡æ¯æ¥è§£å†³è¯­éŸ³åˆ°è¯´è¯äººè„¸çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯è¯­éŸ³é©±åŠ¨çš„äººè„¸è‚–åƒç”Ÿæˆé˜¶æ®µï¼Œåˆ©ç”¨è¯­éŸ³æ§åˆ¶çš„æ‰©æ•£æ¨¡å‹ã€é¢éƒ¨ç»Ÿè®¡å…ˆéªŒå’Œæ ·æœ¬è‡ªé€‚åº”åŠ æƒæ¨¡å—å®ç°é«˜è´¨é‡è‚–åƒç”Ÿæˆï¼›æ¥ä¸‹æ¥æ˜¯è¯­éŸ³é©±åŠ¨çš„è¯´è¯äººè„¸ç”Ÿæˆé˜¶æ®µï¼Œå°†è¡¨æƒ…åŠ¨æ€åµŒå…¥æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶ä½¿ç”¨åŒºåŸŸå¢å¼ºæ¨¡å—ä¼˜åŒ–å”‡åŒæ­¥ã€‚é€šè¿‡é›†æˆé¢„è®­ç»ƒçš„åŸºäºTransformerçš„ç¦»æ•£ç æœ¬å’Œå›¾åƒæ¸²æŸ“ç½‘ç»œï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼æå‡è§†é¢‘å¸§çš„ç»†èŠ‚åˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨HDTFã€VoxCelebå’ŒAVSpeechæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”æ˜¯é¦–ä¸ªä»…ä»å•ä¸€è¯­éŸ³è¾“å…¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é«˜è´¨é‡è¯´è¯äººè„¸è§†é¢‘çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–¹æ³•ç›´æ¥ä»è¯­éŸ³ä¸­æå–ä¿¡æ¯ï¼Œä¸ä¾èµ–æºå›¾åƒä½œä¸ºå¤–è§‚å‚è€ƒå’Œæºè¯­éŸ³æ¥ç”ŸæˆåŠ¨ä½œã€‚</li>
<li>åŒ…æ‹¬è¯­éŸ³é©±åŠ¨çš„äººè„¸è‚–åƒç”Ÿæˆå’Œè¯´è¯äººè„¸ç”Ÿæˆä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>ä½¿ç”¨äº†è¯­éŸ³æ§åˆ¶çš„æ‰©æ•£æ¨¡å‹ã€é¢éƒ¨ç»Ÿè®¡å…ˆéªŒå’Œæ ·æœ¬è‡ªé€‚åº”åŠ æƒæ¨¡å—æ¥å®ç°é«˜è´¨é‡è‚–åƒç”Ÿæˆã€‚</li>
<li>å°†è¡¨æƒ…åŠ¨æ€å¦‚å˜´å”‡è¿åŠ¨ã€é¢éƒ¨è¡¨æƒ…å’Œçœ¼ç›è¿åŠ¨åµŒå…¥æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ã€‚</li>
<li>ä½¿ç”¨åŒºåŸŸå¢å¼ºæ¨¡å—ä¼˜åŒ–å”‡åŒæ­¥ã€‚</li>
<li>é€šè¿‡é›†æˆé¢„è®­ç»ƒçš„åŸºäºTransformerçš„ç¦»æ•£ç æœ¬å’Œå›¾åƒæ¸²æŸ“ç½‘ç»œï¼Œæé«˜äº†è§†é¢‘å¸§çš„ç»†èŠ‚åˆ†è¾¨ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa24aaab9b941586a58f03e38f6a9525" align="middle">
<img src="https://picx.zhimg.com/v2-aa9181bd10a9be2f9da70ea828680c21" align="middle">
<img src="https://picx.zhimg.com/v2-d2d8b228e62f52fbd6afc2be16ff1ac8" align="middle">
<img src="https://picx.zhimg.com/v2-0fc5501fa0cba588a199bd7cf803ea2e" align="middle">
<img src="https://picx.zhimg.com/v2-839c07f1e8172a1131c6a0d545624d78" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UniTok-Audio-A-Unified-Audio-Generation-Framework-via-Generative-Modeling-on-Discrete-Codec-Tokens"><a href="#UniTok-Audio-A-Unified-Audio-Generation-Framework-via-Generative-Modeling-on-Discrete-Codec-Tokens" class="headerlink" title="UniTok-Audio: A Unified Audio Generation Framework via Generative   Modeling on Discrete Codec Tokens"></a>UniTok-Audio: A Unified Audio Generation Framework via Generative   Modeling on Discrete Codec Tokens</h2><p><strong>Authors:Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Yinghao Liu, Zheng Xue, Gang Song, Boyang Zhou</strong></p>
<p>Generative modeling has recently achieved remarkable success across text, image, and audio domains, demonstrating powerful capabilities for unified representation learning. However, audio generation models still face challenges in terms of audio quality and generalization ability across tasks. This fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility. To address these issues, we propose \textbf{UniTok-Audio}, a scalable and extensible framework for unified audio generation tasks. Specifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner; 2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework; 3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction. Experimental results demonstrate that UniTok-Audio achieves competitive performance in comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks: speech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation. To foster future research, we will open-source our codebase. The demo page of our work can be found here: <a target="_blank" rel="noopener" href="https://alibaba.github.io/unified-audio">https://alibaba.github.io/unified-audio</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘é¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œè¯æ˜äº†å…¶ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ çš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒéŸ³é¢‘ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘è´¨é‡å’Œè·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™ç§ç¢ç‰‡åŒ–å¯¼è‡´äº†å¼€å‘å·¥ä½œçš„å†—ä½™ã€æ€§èƒ½çš„ä¸ä¸€è‡´å’Œæ‰©å±•çš„æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniTok-Audioï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»Ÿä¸€éŸ³é¢‘ç”Ÿæˆä»»åŠ¡çš„å¯æ‰©å±•å’Œå¯æ‰©å±•æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œ1ï¼‰UniTok-Audioä»¥è‡ªå›å½’çš„æ–¹å¼æå–è¿ç»­çš„ç‰¹å¾æ¡ä»¶æ¥ç”Ÿæˆç›®æ ‡éŸ³é¢‘çš„ç¦»æ•£ä»¤ç‰Œï¼›2ï¼‰ç‰¹æ®Šä»»åŠ¡æ ‡è¯†ç¬¦ä»¤ç‰Œç»Ÿä¸€äº†å•ä¸ªæ¡†æ¶ä¸­å¤šä¸ªä»»åŠ¡çš„ä¸åŒå­¦ä¹ æ¨¡å¼ï¼›3ï¼‰å¼€å‘äº†ä¸€ä¸ªåŒ…å«å£°å­¦å’Œè¯­ä¹‰åˆ†æ”¯çš„åŒæµéŸ³é¢‘ç¼–è§£ç å™¨ï¼Œç”¨äºé«˜ä¿çœŸæ³¢å½¢é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniTok-Audioåœ¨äº”ä¸ªæ—¶é—´å¯¹é½çš„ä»»åŠ¡ä¸Šå®ç°äº†ä¸æœ€æ–°ä»»åŠ¡ç‰¹å®šæˆ–å¤šä»»åŠ¡ç³»ç»Ÿç›¸å½“çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³ä¿®å¤ã€ç›®æ ‡è¯´è¯äººæå–ã€è¯­éŸ³åˆ†ç¦»ã€è¯­éŸ³è½¬æ¢å’Œè¯­è¨€æŸ¥è¯¢éŸ³é¢‘æºåˆ†ç¦»ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å¼€æºæˆ‘ä»¬çš„ä»£ç åº“ã€‚æˆ‘ä»¬å·¥ä½œçš„æ¼”ç¤ºé¡µé¢å¯åœ¨æ­¤æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://alibaba.github.io/unified-audio">https://alibaba.github.io/unified-audio</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26372v1">PDF</a> 21 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>UniTok-Audioæ¡†æ¶è§£å†³äº†éŸ³é¢‘ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘è´¨é‡å’Œè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå®ç°äº†ç»Ÿä¸€éŸ³é¢‘ç”Ÿæˆä»»åŠ¡çš„å¯æ‰©å±•å’Œå¯å»¶ä¼¸æ€§ã€‚å®ƒé€šè¿‡æå–è¿ç»­ç‰¹å¾æ¡ä»¶ç”Ÿæˆç›®æ ‡éŸ³é¢‘çš„ç¦»æ•£ä»¤ç‰Œï¼Œé‡‡ç”¨ç‰¹æ®Šä»»åŠ¡æ ‡è¯†ç¬¦ä»¤ç‰Œç»Ÿä¸€å¤šä¸ªä»»åŠ¡çš„å­¦ä¹ æ¨¡å¼ï¼Œå¹¶å¼€å‘åŒæµéŸ³é¢‘ç¼–è§£ç å™¨è¿›è¡Œé«˜ä¿çœŸæ³¢å½¢é‡å»ºã€‚åœ¨äº”ä¸ªæ—¶é—´å¯¹é½çš„ä»»åŠ¡ä¸Šï¼ŒUniTok-Audioä¸æœ€æ–°ä»»åŠ¡ç‰¹å®šæˆ–å¤šä»»åŠ¡ç³»ç»Ÿç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniTok-Audioæ˜¯ä¸€ä¸ªé’ˆå¯¹ç»Ÿä¸€éŸ³é¢‘ç”Ÿæˆä»»åŠ¡çš„å¯æ‰©å±•å’Œå¯å»¶ä¼¸çš„æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æå–è¿ç»­ç‰¹å¾æ¡ä»¶ä»¥ç”Ÿæˆç›®æ ‡éŸ³é¢‘çš„ç¦»æ•£ä»¤ç‰Œï¼Œå¹¶é‡‡ç”¨ç‰¹æ®Šä»»åŠ¡æ ‡è¯†ç¬¦ä»¤ç‰Œç»Ÿä¸€å¤šä¸ªä»»åŠ¡çš„å­¦ä¹ æ¨¡å¼ã€‚</li>
<li>UniTok-Audioå¼€å‘äº†ä¸€ä¸ªåŒæµéŸ³é¢‘ç¼–è§£ç å™¨ï¼ŒåŒ…æ‹¬å£°å­¦åˆ†æ”¯å’Œè¯­ä¹‰åˆ†æ”¯ï¼Œä»¥å®ç°é«˜ä¿çœŸæ³¢å½¢é‡å»ºã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨äº”ä¸ªæ—¶é—´å¯¹é½çš„ä»»åŠ¡ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³ä¿®å¤ã€ç›®æ ‡è¯´è¯äººæå–ã€è¯­éŸ³åˆ†ç¦»ã€è¯­éŸ³è½¬æ¢å’Œè¯­è¨€æŸ¥è¯¢éŸ³é¢‘æºåˆ†ç¦»ã€‚</li>
<li>UniTok-Audioæ¡†æ¶çš„åœ°å€å°†åœ¨demoé¡µé¢ä¸Šå…¬å¼€ï¼Œä»¥ä¾¿ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºè§£å†³éŸ³é¢‘ç”Ÿæˆæ¨¡å‹åœ¨éŸ³é¢‘è´¨é‡å’Œä»»åŠ¡æ³›åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29a824eae558ee9f3b281cefe348a00a" align="middle">
<img src="https://picx.zhimg.com/v2-76ef4df4936caa0595caa762ac8f6139" align="middle">
<img src="https://picx.zhimg.com/v2-a78c3f27b9e16a203d93d59d13370faa" align="middle">
<img src="https://picx.zhimg.com/v2-0b89caa61e2f5f0b48096bb2fcd65e39" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Modeling-strategies-for-speech-enhancement-in-the-latent-space-of-a-neural-audio-codec"><a href="#Modeling-strategies-for-speech-enhancement-in-the-latent-space-of-a-neural-audio-codec" class="headerlink" title="Modeling strategies for speech enhancement in the latent space of a   neural audio codec"></a>Modeling strategies for speech enhancement in the latent space of a   neural audio codec</h2><p><strong>Authors:Sofiene Kammoun, Xavier Alameda-Pineda, Simon Leglaive</strong></p>
<p>Neural audio codecs (NACs) provide compact latent speech representations in the form of sequences of continuous vectors or discrete tokens. In this work, we investigate how these two types of speech representations compare when used as training targets for supervised speech enhancement. We consider both autoregressive and non-autoregressive speech enhancement models based on the Conformer architecture, as well as a simple baseline where the NAC encoder is simply fine-tuned for speech enhancement. Our experiments reveal three key findings: predicting continuous latent representations consistently outperforms discrete token prediction; autoregressive models achieve higher quality but at the expense of intelligibility and efficiency, making non-autoregressive models more attractive in practice; and encoder fine-tuning yields the strongest enhancement metrics overall, though at the cost of degraded codec reconstruction. The code and audio samples are available online. </p>
<blockquote>
<p>ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ï¼ˆNACsï¼‰æä¾›ç´§å‡‘çš„æ½œåœ¨è¯­éŸ³è¡¨ç¤ºå½¢å¼ï¼Œä»¥è¿ç»­çš„å‘é‡åºåˆ—æˆ–ç¦»æ•£æ ‡è®°çš„å½¢å¼å‘ˆç°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è¿™ä¸¤ç§è¯­éŸ³è¡¨ç¤ºå½¢å¼åœ¨ç”¨äºç›‘ç£è¯­éŸ³å¢å¼ºä½œä¸ºè®­ç»ƒç›®æ ‡æ—¶çš„æ¯”è¾ƒã€‚æˆ‘ä»¬è€ƒè™‘äº†åŸºäºConformeræ¶æ„çš„è‡ªå›å½’å’Œéè‡ªå›å½’è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªç®€å•çš„åŸºçº¿ï¼Œå…¶ä¸­NACç¼–ç å™¨åªæ˜¯é€šè¿‡å¾®è°ƒç”¨äºè¯­éŸ³å¢å¼ºã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šé¢„æµ‹è¿ç»­æ½œåœ¨è¡¨ç¤ºå½¢å¼å§‹ç»ˆä¼˜äºé¢„æµ‹ç¦»æ•£æ ‡è®°ï¼›è‡ªå›å½’æ¨¡å‹è™½ç„¶å®ç°äº†æ›´é«˜è´¨é‡ï¼Œä½†ç‰ºç‰²äº†å¯æ‡‚æ€§å’Œæ•ˆç‡ï¼Œè¿™ä½¿å¾—éè‡ªå›å½’æ¨¡å‹åœ¨å®è·µä¸­æ›´å…·å¸å¼•åŠ›ï¼›ç¼–ç å™¨å¾®è°ƒæ€»ä½“å¢å¼ºæŒ‡æ ‡æœ€å¼ºï¼Œä½†ä»¥ç¼–è§£ç å™¨é‡å»ºèƒ½åŠ›ä¸‹é™ä¸ºä»£ä»·ã€‚ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26299v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ï¼ˆNACsï¼‰æä¾›äº†ç´§å‡‘çš„æ½œåœ¨è¯­éŸ³è¡¨ç¤ºå½¢å¼ï¼ŒåŒ…æ‹¬è¿ç»­å‘é‡åºåˆ—æˆ–ç¦»æ•£ä»¤ç‰Œã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†è¿™ä¸¤ç§è¯­éŸ³è¡¨ç¤ºå½¢å¼åœ¨ç”¨ä½œç›‘ç£è¯­éŸ³å¢å¼ºçš„è®­ç»ƒç›®æ ‡æ—¶çš„æ¯”è¾ƒã€‚æˆ‘ä»¬è€ƒè™‘äº†åŸºäºConformeræ¶æ„çš„è‡ªå›å½’å’Œéè‡ªå›å½’è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªç®€å•çš„åŸºçº¿ï¼Œå…¶ä¸­NACç¼–ç å™¨åªæ˜¯ç»è¿‡å¾®è°ƒç”¨äºè¯­éŸ³å¢å¼ºã€‚å®éªŒæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šé¢„æµ‹è¿ç»­æ½œåœ¨è¡¨ç¤ºå½¢å¼å§‹ç»ˆä¼˜äºé¢„æµ‹ç¦»æ•£ä»¤ç‰Œï¼›è‡ªå›å½’æ¨¡å‹è™½ç„¶è´¨é‡æ›´é«˜ï¼Œä½†ç‰ºç‰²äº†å¯ç†è§£æ€§å’Œæ•ˆç‡ï¼Œä½¿å¾—éè‡ªå›å½’æ¨¡å‹åœ¨å®è·µä¸­æ›´å…·å¸å¼•åŠ›ï¼›ç¼–ç å™¨å¾®è°ƒæ€»ä½“ä¸Šè·å¾—äº†æœ€å¼ºçš„å¢å¼ºæŒ‡æ ‡ï¼Œä½†ä»¥ç‰ºç‰²ç¼–è§£ç å™¨é‡å»ºèƒ½åŠ›ä¸ºä»£ä»·ã€‚ç›¸å…³ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨ç½‘ä¸Šè·å–ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨æä¾›äº†ç´§å‡‘çš„æ½œåœ¨è¯­éŸ³è¡¨ç¤ºå½¢å¼ï¼Œå¯ä½œä¸ºè®­ç»ƒç›®æ ‡ç”¨äºç›‘ç£è¯­éŸ³å¢å¼ºã€‚</li>
<li>å¯¹æ¯”äº†è¿ç»­æ½œåœ¨è¡¨ç¤ºå’Œç¦»æ•£ä»¤ç‰Œä¸¤ç§å½¢å¼çš„è¯­éŸ³è¡¨ç¤ºåœ¨è¯­éŸ³å¢å¼ºä¸­çš„è¡¨ç°ã€‚</li>
<li>é¢„æµ‹è¿ç»­æ½œåœ¨è¡¨ç¤ºå½¢å¼åœ¨è¯­éŸ³å¢å¼ºä¸­è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>è‡ªå›å½’æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­è´¨é‡æ›´é«˜ï¼Œä½†éè‡ªå›å½’æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­æ›´å…·å¸å¼•åŠ›ã€‚</li>
<li>ç®€å•å¾®è°ƒNACç¼–ç å™¨ç”¨äºè¯­éŸ³å¢å¼ºèƒ½è·å¾—è¾ƒå¼ºçš„å¢å¼ºæŒ‡æ ‡ã€‚</li>
<li>è¯¥ç ”ç©¶æ­ç¤ºäº†è‡ªå›å½’æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­çš„æƒè¡¡é—®é¢˜ï¼Œå³åœ¨è´¨é‡å’Œæ•ˆç‡ä¹‹é—´çš„å–èˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba4b06bbb4fb9ed7b678fcfb33ea8973" align="middle">
<img src="https://picx.zhimg.com/v2-52642c24cbf5715562f3d3cd96c70cac" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-Emotion-Recognition-in-Spoken-Language-Models-on-Emotionally-Incongruent-Speech"><a href="#Evaluating-Emotion-Recognition-in-Spoken-Language-Models-on-Emotionally-Incongruent-Speech" class="headerlink" title="Evaluating Emotion Recognition in Spoken Language Models on Emotionally   Incongruent Speech"></a>Evaluating Emotion Recognition in Spoken Language Models on Emotionally   Incongruent Speech</h2><p><strong>Authors:Pedro CorrÃªa, JoÃ£o Lima, Victor Moreno, Lucas Ueda, Paula Dornhofer Paro Costa</strong></p>
<p>Advancements in spoken language processing have driven the development of spoken language models (SLMs), designed to achieve universal audio understanding by jointly learning text and audio representations for a wide range of tasks. Although promising results have been achieved, there is growing discussion regarding these modelsâ€™ generalization capabilities and the extent to which they truly integrate audio and text modalities in their internal representations. In this work, we evaluate four SLMs on the task of speech emotion recognition using a dataset of emotionally incongruent speech samples, a condition under which the semantic content of the spoken utterance conveys one emotion while speech expressiveness conveys another. Our results indicate that SLMs rely predominantly on textual semantics rather than speech emotion to perform the task, indicating that text-related representations largely dominate over acoustic representations. We release both the code and the Emotionally Incongruent Synthetic Speech dataset (EMIS) to the community. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„è¿›æ­¥æ¨åŠ¨äº†å£è¯­æ¨¡å‹ï¼ˆSLMï¼‰çš„å‘å±•ï¼Œè¿™äº›æ¨¡å‹æ—¨åœ¨é€šè¿‡è”åˆå­¦ä¹ æ–‡æœ¬å’ŒéŸ³é¢‘è¡¨ç¤ºæ¥å®Œæˆä¸€ç³»åˆ—ä»»åŠ¡ï¼Œä»è€Œå®ç°é€šç”¨éŸ³é¢‘ç†è§£ã€‚å°½ç®¡å·²ç»å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å¯¹äºè¿™äº›æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä»¥åŠå®ƒä»¬åœ¨å®é™…æ“ä½œä¸­åœ¨å†…éƒ¨è¡¨ç¤ºä¸­çœŸæ­£èåˆéŸ³é¢‘å’Œæ–‡æœ¬æ¨¡å¼çš„ç¨‹åº¦ï¼Œè®¨è®ºçš„å£°éŸ³è¶Šæ¥è¶Šå¤§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æƒ…æ„Ÿä¸ä¸€è‡´çš„è¯­éŸ³æ ·æœ¬æ•°æ®é›†è¯„ä¼°äº†å››ç§å£è¯­æ¨¡å‹åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œè¿™æ˜¯ä¸€ç§æƒ…å†µä¸‹ï¼Œå£å¤´è¯è¯­çš„è¯­ä¹‰å†…å®¹ä¼ è¾¾äº†ä¸€ç§æƒ…æ„Ÿï¼Œè€Œè¯­éŸ³è¡¨è¾¾æ€§åˆ™ä¼ è¾¾äº†å¦ä¸€ç§æƒ…æ„Ÿã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå£è¯­æ¨¡å‹ä¸»è¦ä¾èµ–æ–‡æœ¬è¯­ä¹‰è€Œä¸æ˜¯è¯­éŸ³æƒ…æ„Ÿæ¥å®Œæˆä»»åŠ¡ï¼Œè¿™è¡¨æ˜æ–‡æœ¬ç›¸å…³çš„è¡¨ç¤ºåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¶…è¿‡äº†å£°å­¦è¡¨ç¤ºã€‚æˆ‘ä»¬å‘ç¤¾åŒºå‘å¸ƒäº†ä»£ç å’Œæƒ…ç»ªä¸ä¸€è‡´åˆæˆè¯­éŸ³æ•°æ®é›†ï¼ˆEMISï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25054v2">PDF</a> Submitted to IEEE ICASSP 2026. Copyright 2026 IEEE. Personal use of   this material is permitted. Permission from IEEE must be obtained for all   other uses</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰çš„å‘å±•åŠå…¶åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å°½ç®¡SLMsåœ¨è®¾è®¡ä¸Šæ—¨åœ¨é€šè¿‡è”åˆå­¦ä¹ æ–‡æœ¬å’ŒéŸ³é¢‘è¡¨ç¤ºæ¥å®ç°é€šç”¨éŸ³é¢‘ç†è§£ï¼Œä½†ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†æƒ…æ„Ÿä¸ä¸€è‡´çš„è¯­éŸ³æ ·æœ¬æ—¶ï¼Œä¸»è¦ä¾èµ–æ–‡æœ¬è¯­ä¹‰è€Œéè¯­éŸ³æƒ…æ„Ÿæ¥å®Œæˆä»»åŠ¡ï¼Œè¡¨æ˜æ–‡æœ¬ç›¸å…³è¡¨ç¤ºåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¸»å¯¼äº†å£°å­¦è¡¨ç¤ºã€‚æœ¬æ–‡åŒæ—¶å…¬å¼€äº†ç›¸å…³çš„ä»£ç å’Œæƒ…æ„Ÿä¸ä¸€è‡´çš„åˆæˆè¯­éŸ³æ•°æ®é›†ï¼ˆEMISï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰é€šè¿‡è”åˆå­¦ä¹ æ–‡æœ¬å’ŒéŸ³é¢‘è¡¨ç¤ºï¼Œæ—¨åœ¨å®ç°é€šç”¨éŸ³é¢‘ç†è§£ã€‚</li>
<li>åœ¨å¤„ç†æƒ…æ„Ÿä¸ä¸€è‡´çš„è¯­éŸ³æ ·æœ¬æ—¶ï¼ŒSLMsä¸»è¦ä¾èµ–æ–‡æœ¬è¯­ä¹‰è¿›è¡Œä»»åŠ¡å¤„ç†ã€‚</li>
<li>è¯­éŸ³æƒ…æ„Ÿåœ¨SLMsä¸­çš„è¡¨ç¤ºä½œç”¨æœ‰é™ï¼Œè¡¨æ˜å£°å­¦è¡¨ç¤ºåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«æ–‡æœ¬ç›¸å…³è¡¨ç¤ºæ‰€ä¸»å¯¼ã€‚</li>
<li>SLMsåœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°å—åˆ°å…³æ³¨ï¼Œå­˜åœ¨å…³äºå…¶æ³›åŒ–èƒ½åŠ›çš„è®¨è®ºã€‚</li>
<li>æœ¬æ–‡å…¬å¼€äº†æƒ…æ„Ÿä¸ä¸€è‡´çš„åˆæˆè¯­éŸ³æ•°æ®é›†ï¼ˆEMISï¼‰ä»¥åŠç›¸å…³çš„ä»£ç ã€‚</li>
<li>SLMsåœ¨å¤„ç†éŸ³é¢‘æ—¶ï¼Œå¦‚ä½•å¹³è¡¡æ–‡æœ¬å’Œè¯­éŸ³æƒ…æ„Ÿä¿¡æ¯æ˜¯ä¸€ä¸ªéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e3adb44bf8ed190c0ffe932bf3bbb6d" align="middle">
<img src="https://picx.zhimg.com/v2-80c2cd127c4a1cd33c3859228e497a5c" align="middle">
<img src="https://picx.zhimg.com/v2-05fd82ce21ac5988b22f3aa51e847edc" align="middle">
<img src="https://picx.zhimg.com/v2-bebfe724336ec10aeb2a85642bdd8ecd" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model"><a href="#POWSM-A-Phonetic-Open-Whisper-Style-Speech-Foundation-Model" class="headerlink" title="POWSM: A Phonetic Open Whisper-Style Speech Foundation Model"></a>POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</h2><p><strong>Authors:Chin-Jou Li, Kalvin Chang, Shikhar Bharadwaj, Eunjung Yeo, Kwanghee Choi, Jian Zhu, David Mortensen, Shinji Watanabe</strong></p>
<p>Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science. </p>
<blockquote>
<p>è¿‘æœŸè¯­éŸ³å¤„ç†é¢†åŸŸçš„è¿›å±•åœ¨éŸ³ç´ ä»»åŠ¡æ–¹é¢å–å¾—äº†é‡å¤§çªç ´ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€éŸ³ç´ è¯†åˆ«ï¼ˆPRï¼‰ã€å­—æ¯åˆ°éŸ³ç´ è½¬æ¢ï¼ˆG2Pï¼‰å’ŒéŸ³ç´ åˆ°å­—æ¯è½¬æ¢ï¼ˆP2Gï¼‰ã€‚å°½ç®¡è¿™äº›ä»»åŠ¡åœ¨æ¦‚å¿µä¸Šå…·æœ‰ç›¸ä¼¼æ€§ï¼Œä½†å®ƒä»¬åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¸€ç›´è¢«å­¤ç«‹ç ”ç©¶ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½ä¾èµ–äºç‰¹å®šçš„æ¶æ„å’Œæ•°æ®é›†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†POWSMï¼ˆè¯­éŸ³é£æ ¼è¯­éŸ³æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿè”åˆæ‰§è¡Œå¤šä¸ªä¸éŸ³ç´ ç›¸å…³çš„ä»»åŠ¡çš„ç»Ÿä¸€æ¡†æ¶ã€‚POWSMå®ç°äº†éŸ³é¢‘ã€æ–‡æœ¬ï¼ˆå­—æ¯ï¼‰å’ŒéŸ³ç´ ä¹‹é—´çš„æ— ç¼è½¬æ¢ï¼Œä¸ºé€šç”¨å’Œä½èµ„æºè¯­éŸ³å¤„ç†å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è”åˆæ”¯æŒG2Pã€P2Gå’ŒASRçš„åŒæ—¶ï¼Œæ€§èƒ½ä¼˜äºæˆ–åŒ¹é…äº†ç±»ä¼¼è§„æ¨¡çš„ä¸“é—¨PRæ¨¡å‹ï¼ˆWav2Vec2Phonemeå’ŒZIPAï¼‰ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ã€ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¼€æ”¾ç§‘å­¦çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24992v1">PDF</a> 14 pages, under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†POWSMï¼ˆè¯­éŸ³éŸ³ç´ å¼€æ”¾æ¡†æ¶ï¼‰ï¼Œå®ƒæ˜¯é¦–ä¸ªèƒ½å¤ŸåŒæ—¶æ‰§è¡Œå¤šé¡¹è¯­éŸ³ç›¸å…³çš„ç»Ÿä¸€æ¡†æ¶ã€‚POWSMå¯å®ç°éŸ³é¢‘ã€æ–‡æœ¬ï¼ˆå­—æ¯ï¼‰å’Œè¯­éŸ³ä¹‹é—´çš„æ— ç¼è½¬æ¢ï¼Œä¸ºé€šç”¨å’Œä½èµ„æºè¯­éŸ³å¤„ç†å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚è¯¥æ¨¡å‹åœ¨è”åˆæ”¯æŒéŸ³ç´ åˆ°å­—æ¯ã€å­—æ¯åˆ°éŸ³ç´ å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡æ—¶ï¼Œæ€§èƒ½ä¼˜äºæˆ–åŒ¹é…äº†ç±»ä¼¼è§„æ¨¡çš„ä¸“é—¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼ˆWav2Vec2Phonemeå’ŒZIPAï¼‰ã€‚æœ¬æ–‡å…¬å¼€äº†è®­ç»ƒæ•°æ®ã€ä»£ç å’Œæ¨¡å‹ï¼Œä»¥æ¨åŠ¨å¼€æ”¾ç§‘å­¦çš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>POWSMæ˜¯é¦–ä¸ªèƒ½å¤ŸåŒæ—¶æ‰§è¡Œå¤šé¡¹è¯­éŸ³ç›¸å…³çš„ç»Ÿä¸€æ¡†æ¶ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€å­—æ¯åˆ°éŸ³ç´ è½¬æ¢å’ŒéŸ³ç´ åˆ°å­—æ¯è½¬æ¢ç­‰ä»»åŠ¡ã€‚</li>
<li>POWSMå®ç°äº†éŸ³é¢‘ã€æ–‡æœ¬å’Œè¯­éŸ³ä¹‹é—´çš„æ— ç¼è½¬æ¢ï¼Œä¸ºè¯­éŸ³å¤„ç†å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹æ€§èƒ½ä¼˜è¶Šï¼Œä¸ç±»ä¼¼è§„æ¨¡çš„ä¸“é—¨è¯­éŸ³æ¨¡å‹ç›¸æ¯”ï¼Œå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>POWSMæ”¯æŒå¤šç§è¯­éŸ³ä»»åŠ¡ï¼Œæé«˜äº†è¯­éŸ³å¤„ç†çš„é€šç”¨æ€§ï¼Œå¹¶é™ä½äº†èµ„æºéœ€æ±‚ã€‚</li>
<li>è¯¥æ¨¡å‹çš„è®­ç»ƒæ•°æ®ã€ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€ï¼Œä¿ƒè¿›äº†å¼€æ”¾ç§‘å­¦çš„å‘å±•ã€‚</li>
<li>POWSMæ¡†æ¶çš„å¼•å…¥å°†ä¿ƒè¿›è¯­éŸ³ä»»åŠ¡çš„ç ”ç©¶å’Œå‘å±•ï¼Œæ¨åŠ¨è¯­éŸ³å¤„ç†æŠ€æœ¯çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd2ebaf1fb265c3fcfc8ed6eee6e5f73" align="middle">
<img src="https://picx.zhimg.com/v2-2eb7de89867a139496c6963ed0ec483c" align="middle">
<img src="https://picx.zhimg.com/v2-f26f09a7f16ca0ba62d576904cfead01" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation"><a href="#Ming-Flash-Omni-A-Sparse-Unified-Architecture-for-Multimodal-Perception-and-Generation" class="headerlink" title="Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal   Perception and Generation"></a>Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal   Perception and Generation</h2><p><strong>Authors:Inclusion AI,  :, Bowen Ma, Cheng Zou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianing Li, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jianping Jiang, Jun Peng, Kaixiang Ji, Kaimeng Ren, Libin Wang, Lixiang Ru, Longhua Tan, Lan Wang, Mochen Bai, Ning Gao, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Ruobing Zheng, Sirui Gao, Tianqi Li, Tinghao Liu, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaolong Wang, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yuting Xiao, Yunxiao Sun, Yipeng Chen, Yifan Mao, Yifei Wu, Yongjie Lyu, Ziping Ma, Zhiqiang Fang, Zhihao Qiu, Ziyuan Huang, Zizheng Yang, Zhengyu He</strong></p>
<p>We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†â€œMing-Flash-Omniâ€ï¼Œå®ƒæ˜¯Ming-Omniçš„å‡çº§ç‰ˆï¼Œå»ºç«‹åœ¨æ›´ç¨€ç–çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å˜ä½“çš„Ling-Flash-2.0ä¹‹ä¸Šï¼Œæ€»å…±æœ‰100äº¿å‚æ•°ï¼Œå…¶ä¸­æ¯ä»¤ç‰Œåªæœ‰6.1äº¿æ˜¯æ´»è·ƒçš„ã€‚è¿™ç§æ¶æ„å®ç°äº†é«˜æ•ˆæ‰©å±•ï¼ˆåœ¨å¤§å¹…æé«˜è®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æ‰©å¤§äº†æ¨¡å‹å®¹é‡ï¼‰ï¼Œå¹¶åœ¨è§†è§‰ã€è¯­éŸ³å’Œè¯­è¨€çš„è·¨æ¨¡æ€æ™ºèƒ½ä¸­èµ‹äºˆäº†æ›´å¼ºçš„ç»Ÿä¸€èƒ½åŠ›ï¼Œæ˜¯æœç€é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰è¿ˆå‡ºçš„å…³é”®ä¸€æ­¥ã€‚ä¸å‰ä»£äº§å“ç›¸æ¯”ï¼Œå‡çº§ç‰ˆåœ¨è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›æ­¥ã€‚æˆ‘ä»¬æ˜¾ç€æé«˜äº†è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œåœ¨ä¸Šä¸‹æ–‡ASRä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ–¹è¨€æ„ŸçŸ¥ASRä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒMing-Flash-Omniå¼•å…¥äº†é«˜ä¿çœŸæ–‡æœ¬æ¸²æŸ“ï¼Œå¹¶åœ¨åœºæ™¯ä¸€è‡´æ€§å’Œèº«ä»½ä¿ç•™çš„å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜æ˜¾è¿›æ­¥ã€‚æ­¤å¤–ï¼ŒMing-Flash-Omniå¼•å…¥äº†ç”Ÿæˆåˆ†å‰²åŠŸèƒ½ï¼Œè¿™ä¸€åŠŸèƒ½ä¸ä»…å®ç°äº†å¼ºå¤§çš„ç‹¬ç«‹åˆ†å‰²æ€§èƒ½ï¼Œè¿˜æé«˜äº†å›¾åƒç”Ÿæˆçš„ç©ºé—´æ§åˆ¶åŠ›å¹¶æ”¹å–„äº†ç¼–è¾‘ä¸€è‡´æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMing-Flash-Omniåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç”Ÿæˆåˆ†å‰²æ–¹é¢å–å¾—äº†æœ€æ–°ç»“æœï¼Œå¹¶åœ¨æ‰€æœ‰12ä¸ªä¸Šä¸‹æ–‡ASRåŸºå‡†æµ‹è¯•ä¸­åˆ›é€ äº†æ–°çºªå½•ï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨ä¸€ä¸ªç»Ÿä¸€æ¶æ„å†…å®Œæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24821v1">PDF</a> 18 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¨€ç–çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å˜ä½“Ling-Flash-2.0æ„å»ºçš„Ming-Flash-Omniæ˜¯ä¸€ä¸ªå‡çº§ç‰ˆæ¨¡å‹ï¼Œæ‹¥æœ‰é«˜æ•ˆçš„æ‰©å±•èƒ½åŠ›å¹¶èƒ½æé«˜å¤šæ¨¡æ€çš„æ™ºèƒ½ç»Ÿä¸€æ€§èƒ½ï¼Œå®ç°è§†è§‰ã€è¯­éŸ³å’Œè¯­è¨€çš„ç»Ÿä¸€ï¼Œæœç€é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰è¿ˆå‡ºé‡è¦ä¸€æ­¥ã€‚æ–°æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œæå‡äº†è¯­éŸ³è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸Šä¸‹æ–‡ASRå’Œæ–¹è¨€æ„ŸçŸ¥ASRæ–¹é¢è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚æ­¤å¤–ï¼ŒMing-Flash-Omniè¿˜å¼•å…¥äº†é«˜ä¿çœŸæ–‡æœ¬æ¸²æŸ“å’Œåœºæ™¯ä¸€è‡´æ€§åŠèº«ä»½ä¿ç•™çš„å›¾åƒç¼–è¾‘å¢ç›ŠåŠŸèƒ½ã€‚è¯¥æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç”Ÿæˆåˆ†å‰²æ–¹é¢è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„ç»“æœã€‚æ€»ä½“æ¥è¯´ï¼ŒMing-Flash-Omniæä¾›äº†ä¸€ç§ç»Ÿä¸€çš„æ¶æ„æ¥å®ç°é«˜æ•ˆçš„æ‰©å±•å’Œå¤šæ ·åŒ–çš„åŠŸèƒ½æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ming-Flash-Omniæ˜¯åŸºäºç¨€ç–çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å˜ä½“Ling-Flash-2.0æ„å»ºçš„å‡çº§æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰é«˜æ•ˆæ‰©å±•èƒ½åŠ›ï¼Œå¯å¤§å¹…æé«˜è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹å®¹é‡ã€‚</li>
<li>Ming-Flash-Omniåœ¨å¤šæ¨¡æ€æ™ºèƒ½æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ç»Ÿä¸€æ€§èƒ½ï¼Œæ¶µç›–è§†è§‰ã€è¯­éŸ³å’Œè¯­è¨€ã€‚</li>
<li>æ–°æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ASRå’Œæ–¹è¨€æ„ŸçŸ¥ASRæ–¹é¢è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>Ming-Flash-Omniå¼•å…¥äº†é«˜ä¿çœŸæ–‡æœ¬æ¸²æŸ“åŠŸèƒ½ï¼Œæé«˜äº†å›¾åƒç¼–è¾‘çš„åœºæ™¯ä¸€è‡´æ€§å’Œèº«ä»½ä¿ç•™èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç”Ÿæˆåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e9002c809666d4496fb854c16246af9" align="middle">
<img src="https://picx.zhimg.com/v2-389a28d5797b63230d3322ba6c321e6d" align="middle">
<img src="https://picx.zhimg.com/v2-e7381059101b39c0c9f693413e455e36" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="BEST-RQ-Based-Self-Supervised-Learning-for-Whisper-Domain-Adaptation"><a href="#BEST-RQ-Based-Self-Supervised-Learning-for-Whisper-Domain-Adaptation" class="headerlink" title="BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation"></a>BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation</h2><p><strong>Authors:RaphaÃ«l Bagat, Irina Illina, Emmanuel Vincent</strong></p>
<p>Automatic Speech Recognition (ASR) systems, despite large multilingual training, struggle in out-of-domain and low-resource scenarios where labeled data is scarce. We propose BEARD (BEST-RQ Encoder Adaptation with Re-training and Distillation), a novel framework designed to adapt Whisperâ€™s encoder using unlabeled data. Unlike traditional self-supervised learning methods, BEARD uniquely combines a BEST-RQ objective with knowledge distillation from a frozen teacher encoder, ensuring the encoderâ€™s complementarity with the pre-trained decoder. Our experiments focus on the ATCO2 corpus from the challenging Air Traffic Control (ATC) communications domain, characterized by non-native speech, noise, and specialized phraseology. Using about 5,000 hours of untranscribed speech for BEARD and 2 hours of transcribed speech for fine-tuning, the proposed approach significantly outperforms previous baseline and fine-tuned model, achieving a relative improvement of 12% compared to the fine-tuned model. To the best of our knowledge, this is the first work to use a self-supervised learning objective for domain adaptation of Whisper. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨å¤§å‹å¤šè¯­è¨€è®­ç»ƒçš„èƒŒæ™¯ä¸‹ï¼Œåœ¨é¢†åŸŸå¤–åŠèµ„æºåŒ®ä¹çš„åœºæ™¯ä¸­ï¼Œå½“æ ‡æ³¨æ•°æ®ç¨€ç¼ºæ—¶ï¼Œä¼šé‡åˆ°æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†BEARDï¼ˆç»“åˆæœ€ä½³è¯·æ±‚ç¼–ç å™¨çš„å†è®­ç»ƒå’Œè’¸é¦çš„è‡ªé€‚åº”æ¡†æ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ç”¨æœªæ ‡è®°æ•°æ®é€‚åº”Whisperçš„ç¼–ç å™¨ã€‚ä¸åŒäºä¼ ç»Ÿçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ŒBEARDç‹¬ç‰¹åœ°ç»“åˆäº†æœ€ä½³è¯·æ±‚ç›®æ ‡ä»¥åŠæ¥è‡ªå†»ç»“æ•™å¸ˆç¼–ç å™¨çš„çŸ¥è¯†è’¸é¦ï¼Œç¡®ä¿ç¼–ç å™¨ä¸é¢„è®­ç»ƒè§£ç å™¨çš„äº’è¡¥æ€§ã€‚æˆ‘ä»¬çš„å®éªŒä¸»è¦é›†ä¸­åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„èˆªç©ºäº¤é€šç®¡åˆ¶ï¼ˆATCï¼‰é€šä¿¡é¢†åŸŸçš„ATCO2è¯­æ–™åº“ä¸Šï¼Œè¯¥è¯­æ–™åº“çš„ç‰¹ç‚¹æ˜¯å£éŸ³æ··æ‚ã€å™ªå£°å¹²æ‰°ä»¥åŠä¸“ä¸šæœ¯è¯­ã€‚ä½¿ç”¨å¤§çº¦5000å°æ—¶çš„æœªè½¬å½•è¯­éŸ³è¿›è¡ŒBEARDè®­ç»ƒï¼Œä»¥åŠä½¿ç”¨2å°æ—¶çš„è½¬å½•è¯­éŸ³è¿›è¡Œå¾®è°ƒï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¹‹å‰çš„åŸºå‡†æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ï¼Œç›¸å¯¹äºå¾®è°ƒæ¨¡å‹å®ç°äº†é«˜è¾¾12%çš„ç›¸å¯¹æ”¹è¿›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†è‡ªç›‘ç£å­¦ä¹ ç›®æ ‡åº”ç”¨äºWhisperé¢†åŸŸè‡ªé€‚åº”çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24570v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>æ€»ç»“</strong><br>    é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨ç¦»åŸŸå’Œå°‘èµ„æºåœºæ™¯ä¸­ç¼ºä¹æ ‡æ³¨æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†BEARDï¼ˆBEST-RQç¼–ç å™¨é€‚åº”ä¸å†è®­ç»ƒå’Œè’¸é¦ï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®é€‚åº”whisperç¼–ç å™¨ã€‚ä¸ä¼ ç»Ÿçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒBEARDç»“åˆäº†BEST-RQç›®æ ‡å’Œæ¥è‡ªå†»ç»“æ•™å¸ˆç¼–ç å™¨çš„çŸ¥è¯†è’¸é¦ï¼Œç¡®ä¿ç¼–ç å™¨ä¸é¢„è®­ç»ƒè§£ç å™¨çš„äº’è¡¥æ€§ã€‚åœ¨èˆªç©ºäº¤é€šç®¡åˆ¶ï¼ˆATCï¼‰é€šä¿¡é¢†åŸŸçš„ATCO2è¯­æ–™åº“ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨çº¦5000å°æ—¶çš„æ— è½¬å½•è¯­éŸ³è¿›è¡ŒBEARDå’Œä»…ä½¿ç”¨2å°æ—¶è½¬å½•è¯­éŸ³è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ï¼Œç›¸å¯¹æé«˜äº†12%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†è‡ªç›‘ç£å­¦ä¹ ç›®æ ‡åº”ç”¨äºwhisperé¢†åŸŸçš„åŸŸé€‚åº”ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>BEARDæ¡†æ¶æ—¨åœ¨ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®é€‚åº”ASRç³»ç»Ÿçš„whisperç¼–ç å™¨ã€‚</li>
<li>ä¼ ç»Ÿè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒBEARDç»“åˆäº†BEST-RQç›®æ ‡å’ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ã€‚</li>
<li>BEARDç¡®ä¿ç¼–ç å™¨ä¸é¢„è®­ç»ƒè§£ç å™¨çš„äº’è¡¥æ€§ã€‚</li>
<li>åœ¨èˆªç©ºäº¤é€šç®¡åˆ¶ï¼ˆATCï¼‰é€šä¿¡é¢†åŸŸçš„ATCO2è¯­æ–™åº“ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>ä½¿ç”¨å¤§é‡æ— è½¬å½•è¯­éŸ³æ•°æ®ï¼ˆçº¦5000å°æ—¶ï¼‰è¿›è¡ŒBEARDè®­ç»ƒã€‚</li>
<li>ä½¿ç”¨ä»…2å°æ—¶è½¬å½•è¯­éŸ³æ•°æ®è¿›è¡Œå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35d904b7740f96687c10aa3229424bd6" align="middle">
<img src="https://picx.zhimg.com/v2-04f236e1b4e62767d4a8f14dd66ce728" align="middle">
<img src="https://picx.zhimg.com/v2-ea8b9f915ecf7a811f261036e6142ab9" align="middle">
<img src="https://picx.zhimg.com/v2-f15452b1d3e2c01dabe84f77724194d3" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LibriConvo-Simulating-Conversations-from-Read-Literature-for-ASR-and-Diarization"><a href="#LibriConvo-Simulating-Conversations-from-Read-Literature-for-ASR-and-Diarization" class="headerlink" title="LibriConvo: Simulating Conversations from Read Literature for ASR and   Diarization"></a>LibriConvo: Simulating Conversations from Read Literature for ASR and   Diarization</h2><p><strong>Authors:MÃ¡tÃ© Gedeon, PÃ©ter Mihajlik</strong></p>
<p>We introduce LibriConvo, a simulated multi-speaker conversational dataset based on speaker-aware conversation simulation (SASC), designed to support training and evaluation of speaker diarization and automatic speech recognition (ASR) systems. Unlike prior resources that mostly rely on semantically disconnected utterances and implausible temporal gaps, LibriConvo ensures semantic coherence and realistic conversational timing. Our pipeline leverages CallHome with external VAD for reliable boundaries, applies compression to reduce unnaturally long silences, and organizes LibriTTS utterances by book to maintain contextual consistency. Acoustic realism is enhanced via a novel room impulse response selection procedure that ranks speaker-microphone configurations by spatial plausibility, balancing realism and diversity. The dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers, split in a speaker-disjoint manner for robust evaluation. Baselines show that the sortformer model outperforms the pyannote pipeline in diarization, while a fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves 7.29% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides a valuable resource for advancing multi-speaker speech processing research with realistic conversational dynamics and controlled experimental conditions. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†LibriConvoï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè¯´è¯äººæ„ŸçŸ¥å¯¹è¯æ¨¡æ‹Ÿï¼ˆSASCï¼‰çš„æ¨¡æ‹Ÿå¤šè¯´è¯äººå¯¹è¯æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒè¯´è¯äººåˆ†åŒ–å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚ä¸åŒäºä»¥å‰ä¸»è¦ä¾èµ–äºè¯­ä¹‰ä¸è¿è´¯çš„è¯´è¯å’Œä¸å¯èƒ½çš„æ—¶é—´é—´éš”çš„èµ„æºï¼ŒLibriConvoç¡®ä¿äº†è¯­ä¹‰è¿è´¯å’Œç°å®çš„å¯¹è¯æ—¶é—´ã€‚æˆ‘ä»¬çš„ç®¡é“åˆ©ç”¨CallHomeå’Œå¤–éƒ¨VADè¿›è¡Œå¯é çš„è¾¹ç•Œåˆ’åˆ†ï¼Œé€šè¿‡å‹ç¼©å‡å°‘ä¸è‡ªç„¶çš„é•¿æ—¶é—´æ²‰é»˜ï¼Œå¹¶æŒ‰ä¹¦ç±ç»„ç»‡LibriTTSçš„è¯´è¯å†…å®¹ï¼Œä»¥ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚é€šè¿‡ä¸€ç§æ–°çš„æˆ¿é—´å†²å‡»å“åº”é€‰æ‹©ç¨‹åºå¢å¼ºå£°éŸ³ç°å®æ„Ÿï¼Œè¯¥ç¨‹åºæ ¹æ®ç©ºé—´åˆç†æ€§å¯¹è¯´è¯äºº-éº¦å…‹é£é…ç½®è¿›è¡Œæ’åï¼Œå¹³è¡¡ç°å®æ„Ÿå’Œå¤šæ ·æ€§ã€‚è¯¥æ•°æ®é›†åŒ…å«240.1å°æ—¶ã€è·¨è¶Š1496ä¸ªå¯¹è¯çš„éŸ³é¢‘å†…å®¹ï¼Œç”±830ä¸ªç‹¬ç‰¹è¯´è¯äººæ„æˆï¼Œä»¥è¯´è¯äººæ— å…³è”çš„æ–¹å¼è¿›è¡Œåˆ†å‰²ä»¥è¿›è¡Œç¨³å¥è¯„ä¼°ã€‚åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œsortformeræ¨¡å‹åœ¨åˆ†åŒ–æ–¹é¢ä¼˜äºpyannoteç®¡é“ï¼Œè€Œç»è¿‡ç²¾ç»†è®­ç»ƒçš„Fast Conformer-CTC XLargeæ¨¡å‹é‡‡ç”¨åºåˆ—åŒ–è¾“å‡ºè®­ç»ƒæ–¹å¼å®ç°ASRçš„7.29%è¯é”™è¯¯ç‡ï¼Œè¶…è¿‡äº†é›¶æ ·æœ¬çš„Whisper-large-v3æ¨¡å‹ã€‚LibriConvoæä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„èµ„æºï¼Œç”¨äºåœ¨å…·æœ‰ç°å®å¯¹è¯åŠ¨åŠ›å’Œå—æ§å®éªŒæ¡ä»¶ä¸‹æ¨è¿›å¤šè¯´è¯äººè¯­éŸ³å¤„ç†ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23320v1">PDF</a> Submitted to LREC 2026</p>
<p><strong>Summary</strong></p>
<p>LibriConvoæ˜¯ä¸€ä¸ªåŸºäºè¯´è¯äººæ„ŸçŸ¥å¯¹è¯æ¨¡æ‹Ÿï¼ˆSASCï¼‰çš„æ¨¡æ‹Ÿå¤šè¯´è¯äººå¯¹è¯æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°è¯´è¯äººåˆ†åŒ–å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿã€‚è¯¥æ•°æ®é›†æ³¨é‡è¯­ä¹‰è¿è´¯æ€§å’Œç°å®å¯¹è¯çš„æ—¶é—´å®‰æ’ï¼Œé‡‡ç”¨ä¸€ç³»åˆ—æŠ€æœ¯å¤„ç†æ•°æ®ä»¥ç»´æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§å¹¶å¢å¼ºå£°å­¦çœŸå®æ€§ã€‚LibriConvoåŒ…å«ç°å®å¯¹è¯çš„åŠ¨æ€æ€§å’Œå¯æ§å®éªŒæ¡ä»¶ï¼Œä¸ºæ¨è¿›å¤šè¯´è¯äººè¯­éŸ³å¤„ç†ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LibriConvoæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå¤šè¯´è¯äººå¯¹è¯æ•°æ®é›†ï¼ŒåŸºäºè¯´è¯äººæ„ŸçŸ¥å¯¹è¯æ¨¡æ‹Ÿï¼ˆSASCï¼‰ã€‚</li>
<li>å®ƒè¢«è®¾è®¡ç”¨äºè®­ç»ƒå’Œè¯„ä¼°è¯´è¯äººåˆ†åŒ–å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿã€‚</li>
<li>LibriConvoæ³¨é‡è¯­ä¹‰è¿è´¯æ€§å’Œç°å®å¯¹è¯çš„æ—¶é—´å®‰æ’ã€‚</li>
<li>æ•°æ®é›†å¤„ç†æŠ€æœ¯åŒ…æ‹¬åˆ©ç”¨CallHomeå’Œå¤–éƒ¨VADç¡®ä¿å¯é è¾¹ç•Œï¼Œå‹ç¼©ä»¥å‡å°‘ä¸è‡ªç„¶çš„é•¿æ—¶é—´æ²‰é»˜ï¼Œå¹¶æŒ‰ä¹¦ç±ç»„ç»‡LibriTTSçš„å‘è¨€ä»¥ç»´æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡æ–°é¢–çš„æˆ¿é—´å†²å‡»å“åº”é€‰æ‹©ç¨‹åºå¢å¼ºå£°å­¦çœŸå®æ€§ï¼Œè¯¥ç¨‹åºæŒ‰ç©ºé—´åˆç†æ€§å¯¹è¯´è¯äºº-éº¦å…‹é£é…ç½®è¿›è¡Œæ’åï¼Œå¹³è¡¡ç°å®æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>LibriConvoåŒ…å«240.1å°æ—¶ã€1496ä¸ªå¯¹è¯å’Œ830ä¸ªç‹¬ç‰¹è¯´è¯äººçš„æ•°æ®ï¼Œä»¥è¯´è¯äººåˆ†ç«‹çš„æ–¹å¼è¿›è¡Œåˆ†å‰²ï¼Œç”¨äºç¨³å¥è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-532ddd14697b89ef1ce4106d64e685f7" align="middle">
<img src="https://picx.zhimg.com/v2-fabdb0117acf295b3ba10298002eb38d" align="middle">
<img src="https://picx.zhimg.com/v2-e4b226027b14a0397d9cf432f9d114fd" align="middle">
<img src="https://picx.zhimg.com/v2-6f929e51af5d02c08d16eef3a0913902" align="middle">
<img src="https://picx.zhimg.com/v2-97ec62e6c46bd2d3da21abb5f3b73afe" align="middle">
<img src="https://picx.zhimg.com/v2-1fc9a1e0466064c645f9b8e9093b965f" align="middle">
<img src="https://picx.zhimg.com/v2-11960b44c6f6b46e3ebd21fbdfe10d1c" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="UltraVoice-Scaling-Fine-Grained-Style-Controlled-Speech-Conversations-for-Spoken-Dialogue-Models"><a href="#UltraVoice-Scaling-Fine-Grained-Style-Controlled-Speech-Conversations-for-Spoken-Dialogue-Models" class="headerlink" title="UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations   for Spoken Dialogue Models"></a>UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations   for Spoken Dialogue Models</h2><p><strong>Authors:Wenming Tu, Guanrou Yang, Ruiqi Yan, Wenxi Chen, Ziyang Ma, Yipeng Kang, Kai Yu, Xie Chen, Zilong Zheng</strong></p>
<p>Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce UltraVoice, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice. Moreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the datasetâ€™s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a target="_blank" rel="noopener" href="https://github.com/bigai-nlco/UltraVoice">https://github.com/bigai-nlco/UltraVoice</a>. </p>
<blockquote>
<p>å½“å‰ï¼Œå£è¯­å¯¹è¯æ¨¡å‹ç¼ºä¹ç²¾ç»†çš„è¯­éŸ³é£æ ¼æ§åˆ¶èƒ½åŠ›ï¼Œè¿™å¯¹äºç±»ä¼¼äºäººç±»çš„äº¤äº’è‡³å…³é‡è¦ï¼Œå¾€å¾€ä¼šè¢«å¿½è§†ï¼Œè€Œæ›´ä¾§é‡äºæ¨ç†å’Œé—®ç­”ç­‰çº¯åŠŸèƒ½æ€§çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UltraVoiceï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸ºå¤šç§ç²¾ç»†è¯­éŸ³é£æ ¼æ§åˆ¶è€Œè®¾è®¡çš„å¤§å‹è¯­éŸ³å¯¹è¯æ•°æ®é›†ã€‚UltraVoiceåŒ…å«è¶…è¿‡830å°æ—¶çš„è¯­éŸ³å¯¹è¯ï¼Œæ¶µç›–äº†å…­ç§å…³é”®çš„è¯­éŸ³é£æ ¼ç»´åº¦ï¼šæƒ…æ„Ÿã€è¯­é€Ÿã€éŸ³é‡ã€å£éŸ³ã€è¯­è¨€å’Œå¤åˆé£æ ¼ã€‚å¯¹SLAM-Omniå’ŒVocalNetç­‰é¢†å…ˆæ¨¡å‹è¿›è¡ŒUltraVoiceå¾®è°ƒï¼Œå¯æ˜¾è‘—æé«˜å…¶ç²¾ç»†è¯­éŸ³é£æ ¼çš„æ§åˆ¶èƒ½åŠ›ï¼Œè€Œä¸ä¼šé™ä½å…¶æ ¸å¿ƒå¯¹è¯èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹åœ¨å¤šç»´åº¦æ§åˆ¶ä»»åŠ¡ä¸Šå®ç°äº†å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰æé«˜29.12-42.33%ï¼ŒæŒ‡ä»¤éµå¾ªç‡ï¼ˆIFRï¼‰æé«˜14.61-40.09ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œåœ¨URO-BenchåŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹åœ¨åŸºæœ¬è®¾ç½®ä¸Šå¹³å‡æé«˜äº†+10.84%ï¼Œåœ¨ä¸“ä¸šè®¾ç½®ä¸Šå¹³å‡æé«˜äº†+7.87%ï¼Œåœ¨æ ¸å¿ƒç†è§£ã€æ¨ç†å’Œå¯¹è¯èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜å¯ç”¨äºè®­ç»ƒå¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œå‡¸æ˜¾å…¶é«˜è´¨é‡å’Œå¹¿æ³›çš„è¡¨è¾¾èƒ½åŠ›åˆæˆé€‚ç”¨æ€§ã€‚å®Œæ•´çš„æ•°æ®é›†å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å¯ç”¨äºï¼š<a target="_blank" rel="noopener" href="https://github.com/bigai-nlco/UltraVoice%E3%80%82">https://github.com/bigai-nlco/UltraVoiceã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22588v1">PDF</a> 23 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†UltraVoiceæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é’ˆå¯¹å¤šç§ç²¾ç»†ç²’åº¦çš„è¯­éŸ³é£æ ¼æ§åˆ¶è®¾è®¡ï¼Œè§£å†³äº†å½“å‰å£è¯­å¯¹è¯æ¨¡å‹ç¼ºä¹ç²¾ç»†é£æ ¼æ§åˆ¶èƒ½åŠ›çš„é—®é¢˜ã€‚UltraVoiceåŒ…å«è¶…è¿‡830å°æ—¶çš„å¯¹è¯è¯­éŸ³ï¼Œæ¶µç›–æƒ…æ„Ÿã€è¯­é€Ÿã€éŸ³é‡ã€å£éŸ³ã€è¯­è¨€å’Œå¤åˆé£æ ¼ç­‰å…­ä¸ªå…³é”®è¯­éŸ³é£æ ¼ç»´åº¦ã€‚åœ¨UltraVoiceä¸Šå¾®è°ƒSLAM-Omniå’ŒVocalNetç­‰é¢†å…ˆæ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†ç²¾ç»†é£æ ¼çš„æ§åˆ¶èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æŸå¤±æ ¸å¿ƒå¯¹è¯èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜ç”¨äºè®­ç»ƒå¯æ§æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶é«˜è´¨é‡å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UltraVoiceæ˜¯é¦–ä¸ªé’ˆå¯¹å¤šç§ç²¾ç»†ç²’åº¦çš„è¯­éŸ³é£æ ¼æ§åˆ¶çš„å¤§å‹è¯­éŸ³å¯¹è¯æ•°æ®é›†ã€‚</li>
<li>UltraVoiceåŒ…å«è¶…è¿‡830å°æ—¶çš„å¯¹è¯è¯­éŸ³ï¼Œæ¶µç›–å…­ä¸ªå…³é”®è¯­éŸ³é£æ ¼ç»´åº¦ã€‚</li>
<li>é€šè¿‡å¯¹é¢†å…ˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¦‚SLAM-Omniå’ŒVocalNetï¼Œæ˜¾è‘—æé«˜äº†ç²¾ç»†é£æ ¼çš„æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>ç»†å¾®è°ƒæ¨¡å‹åœ¨è®¾è®¡çš„å¤šç»´åº¦æ§åˆ¶ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¦‚åœ¨Mean Opinion Score (MOS)å’ŒInstruction Following Rate (IFR)ä¸Šåˆ†åˆ«æé«˜äº†29.12-42.33%å’Œ14.61-40.09ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
<li>åœ¨URO-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨æ ¸å¿ƒç†è§£ã€æ¨ç†å’Œå¯¹è¯èƒ½åŠ›æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>UltraVoiceæ•°æ®é›†å¯ç”¨äºè®­ç»ƒå¯æ§æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œå±•ç¤ºå…¶é«˜è´¨é‡å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fffa7c22c84344a1227c0fdca97a1b8" align="middle">
<img src="https://picx.zhimg.com/v2-0191c7a1641352c5cc10828bdb447f54" align="middle">
<img src="https://picx.zhimg.com/v2-0de9fe2684b3946f92907ac70fa74a02" align="middle">
<img src="https://picx.zhimg.com/v2-e85b8adb000c74d6f6024273b1f92fb2" align="middle">
<img src="https://picx.zhimg.com/v2-bc659d1c4c365bb5ee67f300d7bfa7b3" align="middle">
<img src="https://picx.zhimg.com/v2-fa6b0558845584e4d856287584663f62" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FlexIO-Flexible-Single-and-Multi-Channel-Speech-Separation-and-Enhancement"><a href="#FlexIO-Flexible-Single-and-Multi-Channel-Speech-Separation-and-Enhancement" class="headerlink" title="FlexIO: Flexible Single- and Multi-Channel Speech Separation and   Enhancement"></a>FlexIO: Flexible Single- and Multi-Channel Speech Separation and   Enhancement</h2><p><strong>Authors:Yoshiki Masuyama, Kohei Saijo, Francesco Paissan, Jiangyu Han, Marc Delcroix, Ryo Aihara, FranÃ§ois G. Germain, Gordon Wichern, Jonathan Le Roux</strong></p>
<p>Speech separation and enhancement (SSE) has advanced remarkably and achieved promising results in controlled settings, such as a fixed number of speakers and a fixed array configuration. Towards a universal SSE system, single-channel systems have been extended to deal with a variable number of speakers (i.e., outputs). Meanwhile, multi-channel systems accommodating various array configurations (i.e., inputs) have been developed. However, these attempts have been pursued separately. In this paper, we propose a flexible input and output SSE system, named FlexIO. It performs conditional separation using prompt vectors, one per speaker as a condition, allowing separation of an arbitrary number of speakers. Multi-channel mixtures are processed together with the prompt vectors via an array-agnostic channel communication mechanism. Our experiments demonstrate that FlexIO successfully covers diverse conditions with one to five microphones and one to three speakers. We also confirm the robustness of FlexIO on CHiME-4 real data. </p>
<blockquote>
<p>è¯­éŸ³åˆ†ç¦»ä¸å¢å¼ºï¼ˆSSEï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå¹¶åœ¨å›ºå®šæ•°é‡çš„å‘è¨€äººå’Œå›ºå®šé˜µåˆ—é…ç½®ç­‰å—æ§ç¯å¢ƒä¸­å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ä¸ºäº†æ„å»ºä¸€ä¸ªé€šç”¨çš„SSEç³»ç»Ÿï¼Œå•é€šé“ç³»ç»Ÿå·²ç»è¢«æ‰©å±•åˆ°å¤„ç†å¯å˜æ•°é‡çš„å‘è¨€äººï¼ˆå³è¾“å‡ºï¼‰ã€‚åŒæ—¶ï¼Œå®¹çº³å„ç§é˜µåˆ—é…ç½®ï¼ˆå³è¾“å…¥ï¼‰çš„å¤šé€šé“ç³»ç»Ÿä¹Ÿå·²ç»å¼€å‘å‡ºæ¥ã€‚ç„¶è€Œï¼Œè¿™äº›å°è¯•æ˜¯åˆ†å¼€è¿›è¡Œçš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»è¾“å…¥å’Œè¾“å‡ºçš„SSEç³»ç»Ÿï¼Œåä¸ºFlexIOã€‚å®ƒä½¿ç”¨æ¯å‘è¨€è€…ä¸€ä¸ªçš„æç¤ºå‘é‡è¿›è¡Œæ¡ä»¶åˆ†ç¦»ï¼Œå…è®¸ä»»æ„æ•°é‡çš„å‘è¨€äººè¿›è¡Œåˆ†ç¦»ã€‚å¤šé€šé“æ··åˆç‰©ä¸æç¤ºå‘é‡é€šè¿‡é˜µåˆ—æ— å…³çš„é€šé“é€šä¿¡æœºåˆ¶ä¸€èµ·å¤„ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒFlexIOæˆåŠŸè¦†ç›–äº†ä»ä¸€è‡³äº”ä¸ªéº¦å…‹é£å’Œä¸€è‡³ä¸‰ä¸ªå‘è¨€äººçš„å„ç§æ¡ä»¶ã€‚æˆ‘ä»¬ä¹Ÿåœ¨CHiME-4çœŸå®æ•°æ®ä¸Šè¯å®äº†FlexIOçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21485v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§çµæ´»è¾“å…¥è¾“å‡ºè¯­éŸ³åˆ†ç¦»ä¸å¢å¼ºç³»ç»Ÿï¼ˆFlexIOï¼‰ï¼Œé‡‡ç”¨æ¡ä»¶åˆ†ç¦»æ–¹æ³•ï¼Œåˆ©ç”¨ä¸ºæ¯ä¸ªè¯´è¯äººæä¾›çš„æç¤ºå‘é‡è¿›è¡Œä»»æ„æ•°é‡çš„è¯´è¯äººåˆ†ç¦»ã€‚è¯¥ç³»ç»Ÿå¯ä»¥å¤„ç†å¤šé€šé“æ··åˆä¿¡å·ï¼Œå¹¶é€šè¿‡ç‹¬ç«‹äºé˜µåˆ—çš„é€šé“é€šä¿¡æœºåˆ¶å¤„ç†æç¤ºå‘é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒFlexIOåœ¨æ¶µç›–ä¸åŒæ¡ä»¶ä¸‹è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ä»ä¸€è‡³äº”ä¸ªéº¦å…‹é£è¾“å…¥å’Œä¸€åˆ°ä¸‰ä¸ªè¯´è¯äººçš„åˆ†ç¦»ä»»åŠ¡ã€‚åœ¨CHiME-4çœŸå®æ•°æ®ä¸ŠéªŒè¯äº†FlexIOçš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlexIOç³»ç»Ÿæ˜¯ä¸€ä¸ªçµæ´»è¾“å…¥è¾“å‡ºè¯­éŸ³åˆ†ç¦»ä¸å¢å¼ºç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°ä»»æ„æ•°é‡è¯´è¯äººçš„è¯­éŸ³åˆ†ç¦»ã€‚</li>
<li>FlexIOé‡‡ç”¨æ¡ä»¶åˆ†ç¦»æ–¹æ³•ï¼Œåˆ©ç”¨ä¸ºæ¯ä¸ªè¯´è¯äººæä¾›çš„æç¤ºå‘é‡ä½œä¸ºæ¡ä»¶ã€‚</li>
<li>è¯¥ç³»ç»Ÿå¯ä»¥å¤„ç†å¤šé€šé“æ··åˆä¿¡å·ï¼Œå¹¶å€ŸåŠ©ç‹¬ç«‹äºé˜µåˆ—çš„é€šé“é€šä¿¡æœºåˆ¶å¤„ç†æç¤ºå‘é‡ã€‚</li>
<li>å®éªŒè¡¨æ˜FlexIOåœ¨å¤šç§æ¡ä»¶ä¸‹è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ä¸åŒéº¦å…‹é£æ•°é‡å’Œä¸åŒè¯´è¯äººæ•°é‡ã€‚</li>
<li>FlexIOåœ¨CHiME-4çœŸå®æ•°æ®ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§ã€‚</li>
<li>FlexIOç³»ç»Ÿæœ‰æœ›ä¸ºè¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºæŠ€æœ¯å¸¦æ¥æ–°çš„çªç ´ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚ç¯å¢ƒå’Œå¤šå˜æ¡ä»¶ä¸‹çš„è¯­éŸ³ä¿¡å·æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21485">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7c74b5d44958261d241a5cc769e3961" align="middle">
<img src="https://picx.zhimg.com/v2-4a5e826450527def852dec187df424e1" align="middle">
<img src="https://picx.zhimg.com/v2-5332907d9467d4304128133b4b0caee7" align="middle">
<img src="https://picx.zhimg.com/v2-f25c245d44ef97f2325c076167f3cd15" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Can-large-audio-language-models-understand-child-stuttering-speech-speech-summarization-and-source-separation"><a href="#Can-large-audio-language-models-understand-child-stuttering-speech-speech-summarization-and-source-separation" class="headerlink" title="Can large audio language models understand child stuttering speech?   speech summarization, and source separation"></a>Can large audio language models understand child stuttering speech?   speech summarization, and source separation</h2><p><strong>Authors:Chibuzor Okocha, Maya Bakri, Christan Grant</strong></p>
<p>Child speech differs from adult speech in acoustics, prosody, and language development, and disfluencies (repetitions, prolongations, blocks) further challenge Automatic Speech Recognition (ASR) and downstream Natural Language Processing (NLP). Recent large audio-language models (LALMs) demonstrate strong cross-modal audio understanding; however, their behavior in disfluent child speech remains underexplored. We evaluate several state-of-the-art LALMs in two settings: an interview (mixed speakers) and a reading task (single child). The tasks are (i) single-channel source separation to isolate the child and (ii) child-only summarization that preserves clinically relevant disfluencies and avoids adult-speech leakage.   Evaluation combines Large Language Model (LLM) as a judge, human expert ratings, and BERTScore (F1), and we report agreement between models and between models and humans to assess reliability. Our findings delineate the conditions under which LALMs produce faithful child-only summaries from mixed audio and where they fail, offering practical guidance for clinical and educational deployments. We provide prompts and evaluation scripts to support replication. </p>
<blockquote>
<p>å„¿ç«¥è¯­éŸ³åœ¨å£°å­¦ã€è¯­è°ƒå’Œè¯­è¨€å‘å±•æ–¹é¢ä¸æˆäººè¯­éŸ³å­˜åœ¨å·®å¼‚ï¼Œä¸æµç•…ï¼ˆé‡å¤ã€å»¶é•¿ã€é˜»å¡ï¼‰è¿›ä¸€æ­¥æŒ‘æˆ˜äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œä¸‹æ¸¸çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€‚æœ€è¿‘çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰å±•ç¤ºäº†å¼ºå¤§çš„è·¨æ¨¡æ€éŸ³é¢‘ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨ä¸æµç•…çš„è¯­éŸ³ä¸­çš„è¡¨ç°ä»ç„¶é²œæœ‰ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§ç¯å¢ƒä¸­è¯„ä¼°äº†å¤šç§æœ€å…ˆè¿›çš„LALMï¼šè®¿è°ˆï¼ˆæ··åˆè¯´è¯è€…ï¼‰å’Œé˜…è¯»ä»»åŠ¡ï¼ˆå•ä¸€å„¿ç«¥ï¼‰ã€‚ä»»åŠ¡åŒ…æ‹¬ï¼šï¼ˆiï¼‰å•é€šé“æºåˆ†ç¦»ï¼Œä»¥éš”ç¦»å„¿ç«¥è¯´è¯ï¼›ï¼ˆiiï¼‰ä»…é’ˆå¯¹å„¿ç«¥çš„æ‘˜è¦ï¼Œä¿ç•™ä¸´åºŠç›¸å…³çš„è¯­è¨€ä¸æµç•…æ€§å¹¶é¿å…æˆäººè¯­éŸ³æ³„æ¼ã€‚è¯„ä¼°ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤æ ‡å‡†ã€äººç±»ä¸“å®¶è¯„åˆ†å’ŒBERTScoreï¼ˆF1ï¼‰ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†æ¨¡å‹ä¹‹é—´çš„å…±è¯†ä»¥åŠæ¨¡å‹ä¸äººç±»ä¹‹é—´çš„å…±è¯†æ¥è¯„ä¼°å¯é æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæŒ‡å‡ºäº†åœ¨å“ªäº›æƒ…å†µä¸‹LALMèƒ½å¤Ÿä»æ··åˆéŸ³é¢‘ä¸­äº§ç”Ÿå¿ å®äºå„¿ç«¥çš„æ‘˜è¦ï¼Œä»¥åŠåœ¨å“ªäº›æƒ…å†µä¸‹å®ƒä»¬ä¼šå¤±è´¥ï¼Œä¸ºä¸´åºŠå’Œæ•™è‚²éƒ¨ç½²æä¾›äº†å®é™…æŒ‡å¯¼ã€‚æˆ‘ä»¬æä¾›äº†æç¤ºå’Œè¯„ä¼°è„šæœ¬æ¥æ”¯æŒå¤åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20850v1">PDF</a> 7 pages, 1 Figure, 8 tables, Under review ICASSP 2026</p>
<p><strong>æ€»ç»“</strong><br>    æœ€æ–°ç ”ç©¶è¯„ä¼°äº†é’ˆå¯¹å„¿ç«¥å’Œæˆäººæ··åˆè¯­éŸ³çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æƒ…å†µä¸‹çš„æ€§èƒ½ã€‚ç ”ç©¶é€šè¿‡è®¾ç½®ä¸¤ç§åœºæ™¯ï¼šè®¿è°ˆï¼ˆæ··åˆè¯´è¯è€…ï¼‰å’Œé˜…è¯»ä»»åŠ¡ï¼ˆå•ä¸€å„¿ç«¥ï¼‰ï¼Œå¯¹ä¸€ç³»åˆ—å‰æ²¿çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚è¯„ä¼°å†…å®¹åŒ…æ‹¬è¯­éŸ³åˆ†ç¦»å’Œå„¿ç«¥è¯­éŸ³æ‘˜è¦ç”Ÿæˆç­‰ä»»åŠ¡ã€‚ç ”ç©¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€äººç±»ä¸“å®¶è¯„åˆ†å’ŒBERTScoreï¼ˆF1ï¼‰ç­‰è¯„ä¼°æ‰‹æ®µï¼Œæ—¨åœ¨ç¡®ä¿æ¨¡å‹çš„å¯é æ€§ã€‚ç ”ç©¶å‘ç°è¿™äº›æ¨¡å‹åœ¨æŸäº›æ¡ä»¶ä¸‹èƒ½å¤Ÿç”Ÿæˆå¿ å®äºå„¿ç«¥è¯­éŸ³çš„æ‘˜è¦ï¼Œä½†åœ¨å…¶ä»–æƒ…å†µä¸‹åˆ™ä¼šå‡ºç°é—®é¢˜ã€‚è¿™ä¸ºä¸´åºŠå’Œæ•™è‚²éƒ¨ç½²æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å„¿ç«¥è¯­éŸ³ä¸æˆäººè¯­éŸ³åœ¨å£°å­¦ã€è¯­è°ƒå’Œè¯­è¨€å‘å±•æ–¹é¢å­˜åœ¨å·®å¼‚ï¼Œè¿™ä½¿å¾—è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨è·¨æ¨¡æ€éŸ³é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å„¿ç«¥è¯­éŸ³æ—¶çš„è¡¨ç°ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸¤ç§åœºæ™¯ï¼ˆè®¿è°ˆå’Œé˜…è¯»ä»»åŠ¡ï¼‰è¯„ä¼°äº†å¤šä¸ªå‰æ²¿çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹è¯„ä¼°ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€äººç±»ä¸“å®¶è¯„åˆ†å’ŒBERTScoreï¼ˆF1ï¼‰ç­‰å¤šç§æ–¹æ³•ï¼Œä»¥ç¡®ä¿è¯„ä¼°çš„å¯é æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨ç”Ÿæˆå¿ å®äºå„¿ç«¥è¯­éŸ³çš„æ‘˜è¦æ–¹é¢æœ‰ä¸€å®šæ•ˆæœï¼Œä½†ä¹Ÿå­˜åœ¨å¤±è´¥çš„æƒ…å†µï¼Œè¿™å–å†³äºç‰¹å®šæ¡ä»¶ã€‚</li>
<li>ç ”ç©¶ä¸ºä¸´åºŠå’Œæ•™è‚²éƒ¨ç½²æä¾›äº†å…³äºå¦‚ä½•ä½¿ç”¨è¿™äº›æ¨¡å‹çš„å®ç”¨æŒ‡å¯¼ã€‚</li>
<li>ç ”ç©¶æä¾›äº†æ”¯æŒå¤åˆ¶å®éªŒæ‰€éœ€çš„æç¤ºå’Œè¯„ä¼°è„šæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24642f52ef38bb731f175e6bce06ab9f" align="middle">
<img src="https://picx.zhimg.com/v2-e4dd7141bf9e0dc44902dd8b93195e33" align="middle">
<img src="https://picx.zhimg.com/v2-defc227591dff79bd9b96c1086e7e1f1" align="middle">
<img src="https://picx.zhimg.com/v2-6f30ca30ab8c442ca80bec1887836029" align="middle">
<img src="https://picx.zhimg.com/v2-36deac6a522025ce95f63cf9ed715207" align="middle">
<img src="https://picx.zhimg.com/v2-88980cd99367354094589782706bd4a9" align="middle">
<img src="https://picx.zhimg.com/v2-0b55cb71d3b8407144f998f14ec45b07" align="middle">
<img src="https://picx.zhimg.com/v2-1322b1bb669778da7c0e99d85cf7a3db" align="middle">
<img src="https://picx.zhimg.com/v2-bdb717fdd1991f146d0ed81e4fcd6afe" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="WEST-LLM-based-Speech-Toolkit-for-Speech-Understanding-Generation-and-Interaction"><a href="#WEST-LLM-based-Speech-Toolkit-for-Speech-Understanding-Generation-and-Interaction" class="headerlink" title="WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction"></a>WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and   Interaction</h2><p><strong>Authors:Binbin Zhang, Chengdong Liang, Shuai Wang, Xuelong Geng, Zhao Guo, Haoyu Li, Hao Yin, Xipeng Yang, Pengshen Zhang, Changwei Ma, Lei Xie</strong></p>
<p>In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/west/">https://github.com/wenet-e2e/west/</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†WESTï¼ˆWEè¯­éŸ³å·¥å…·åŒ…ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³å·¥å…·åŒ…ï¼Œç”¨äºè¯­éŸ³ç†è§£ã€ç”Ÿæˆå’Œäº¤äº’ã€‚WESTæœ‰ä¸‰ä¸ªå…³é”®ç‰¹ç‚¹ï¼š1ï¼‰å®Œå…¨åŸºäºLLMï¼šåˆ©ç”¨å¤§å‹æ¨¡å‹çš„æˆç†Ÿæ¶æ„ã€ç”Ÿæ€ç³»ç»Ÿï¼ˆä¾‹å¦‚Hugging Faceï¼‰å’Œæ–¹æ³•ï¼ˆä¾‹å¦‚åºåˆ—æ‰“åŒ…ï¼‰ï¼Œç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šã€‚2ï¼‰å…¨æ ˆæ”¯æŒï¼šæ”¯æŒè¯†åˆ«ã€åˆæˆã€ç†è§£ã€å¯¹è¯å’Œå¤šæ¨¡å¼åŠŸèƒ½ç­‰ä»»åŠ¡ï¼Œå¯æ‰©å±•ä»¥çº³å…¥å¼€æºæ¨¡å‹ã€‚3ï¼‰ç®€å•æ˜äº†ï¼šä¸€ä¸ªç®€å•æ˜äº†çš„è¯­éŸ³å·¥å…·åŒ…ï¼Œæ¯ä¸ªäººéƒ½èƒ½è½»æ¾ä½¿ç”¨ã€‚æ­¤å¤–ï¼ŒWESTæä¾›ä¸¤ç§ç±»å‹çš„æ¨¡å‹ã€é£Ÿè°±å’Œå®éªŒç»“æœã€‚ç¬¬ä¸€ç§å®Œå…¨åŸºäºå¼€æºæ¨¡å‹å’Œå¼€æºæ•°æ®ï¼Œå…è®¸ç”¨æˆ·å®Œå…¨å¤åˆ¶æœ¬æ–‡ä¸­çš„å®éªŒï¼Œå¹¶ä½œä¸ºéªŒè¯ç³»ç»Ÿæˆ–æœ€å°ç³»ç»ŸåŸºçº¿ã€‚ç¬¬äºŒç§æ˜¯åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œæä¾›å“è¶Šæ€§èƒ½ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚WESTåœ¨<a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/west/%E5%BC%BA%E5%BC%BA%E5%AE%9A%E4%BD%BF%E7%9A%84%E5%BC%BA%E5%BC%BA%E5%AE%9A%E4%BD%BF%E7%9A%84%E5%BC%BA%E5%BC%BA%E5%AE%9A%E4%BB%BBOpen">https://github.com/wenet-e2e/west/å…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19902v2">PDF</a> </p>
<p><strong>Summary</strong><br>WESTæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³å·¥å…·åŒ…ï¼Œå…·æœ‰ä¸‰å¤§ç‰¹ç‚¹ï¼šå…¨LLMåŸºç¡€ã€å…¨æ ˆæ”¯æŒå’Œç®€å•æ˜“ç”¨ã€‚å®ƒæ”¯æŒè¯­éŸ³è¯†åˆ«ã€åˆæˆã€ç†è§£å’Œå¯¹è¯ç­‰ä»»åŠ¡ï¼Œå¹¶æä¾›ä¸¤ç§ç±»å‹çš„æ¨¡å‹ï¼šåŸºäºå¼€æºæ¨¡å‹å’Œæ•°æ®çš„éªŒè¯ç³»ç»Ÿæˆ–æœ€å°ç³»ç»ŸåŸºçº¿ï¼Œä»¥åŠç»è¿‡å¤§è§„æ¨¡æ•°æ®è®­ç»ƒæä¾›å“è¶Šæ€§èƒ½çš„ç›´æ¥åº”ç”¨æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WESTæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³å·¥å…·åŒ…ã€‚</li>
<li>å®ƒæ”¯æŒè¯­éŸ³è¯†åˆ«ã€åˆæˆã€ç†è§£å’Œå¯¹è¯ç­‰ä»»åŠ¡ï¼Œå…·æœ‰å…¨æ ˆåŠŸèƒ½ã€‚</li>
<li>WESTå…·æœ‰ä¸‰å¤§ç‰¹ç‚¹ï¼šå…¨LLMåŸºç¡€ã€åˆ©ç”¨ç°æœ‰æˆç†Ÿæ¶æ„å’Œç”Ÿæ€ç³»ç»Ÿä»¥åŠç®€å•æ˜“ç”¨ã€‚</li>
<li>WESTæä¾›ä¸¤ç§ç±»å‹çš„æ¨¡å‹ï¼šåŸºäºå¼€æºæ¨¡å‹å’Œæ•°æ®çš„æ¨¡å‹ï¼Œä»¥åŠç»è¿‡å¤§è§„æ¨¡æ•°æ®è®­ç»ƒçš„é«˜æ€§èƒ½æ¨¡å‹ã€‚</li>
<li>WESTåŒ…å«å®éªŒç»“æœçš„åˆ†äº«ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥åˆ©ç”¨WESTçš„å·¥å…·è¿›è¡Œè¯­éŸ³äº¤äº’å’Œç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3d8b5751e736acfe2c12b1606366ed9" align="middle">
<img src="https://picx.zhimg.com/v2-a7aebdefc68a0afeb8f510e10a338f37" align="middle">
<img src="https://picx.zhimg.com/v2-9b028003bfa661bbe008b2eeec6f9ddc" align="middle">
<img src="https://picx.zhimg.com/v2-7014297b4d924d8489747d92b0aed1d0" align="middle">
<img src="https://picx.zhimg.com/v2-7857ceef61c98497d19470cfe0ceec6e" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="High-Energy-Concentration-for-Federated-Learning-in-Frequency-Domain"><a href="#High-Energy-Concentration-for-Federated-Learning-in-Frequency-Domain" class="headerlink" title="High-Energy Concentration for Federated Learning in Frequency Domain"></a>High-Energy Concentration for Federated Learning in Frequency Domain</h2><p><strong>Authors:Haozhi Shi, Weiying Xie, Hangyu Ye, Daixun Li, Jitao Ma, Yunsong Li, Leyuan Fang</strong></p>
<p>Federated Learning (FL) presents significant potential for collaborative optimization without data sharing. Since synthetic data is sent to the server, leveraging the popular concept of dataset distillation, this FL framework protects real data privacy while alleviating data heterogeneity. However, such methods are still challenged by the redundant information and noise in entire spatial-domain designs, which inevitably increases the communication burden. In this paper, we propose a novel Frequency-Domain aware FL method with high-energy concentration (FedFD) to address this problem. Our FedFD is inspired by the discovery that the discrete cosine transform predominantly distributes energy to specific regions, referred to as high-energy concentration. The principle behind FedFD is that low-energy like high-frequency components usually contain redundant information and noise, thus filtering them helps reduce communication costs and optimize performance. Our FedFD is mathematically formulated to preserve the low-frequency components using a binary mask, facilitating an optimal solution through frequency-domain distribution alignment. In particular, real data-driven synthetic classification is imposed into the loss to enhance the quality of the low-frequency components. On five image and speech datasets, FedFD achieves superior performance than state-of-the-art methods while reducing communication costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha &#x3D; 0.01$, FedFD achieves a minimum reduction of 37.78% in the communication cost, while attaining a 10.88% performance gain. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰åœ¨æ— éœ€æ•°æ®å…±äº«çš„æƒ…å†µä¸‹ï¼Œä¸ºå®ç°åä½œä¼˜åŒ–æä¾›äº†å·¨å¤§çš„æ½œåŠ›ã€‚ç”±äºåˆæˆæ•°æ®è¢«å‘é€åˆ°æœåŠ¡å™¨ï¼Œå¹¶å€ŸåŠ©æ•°æ®é›†è’¸é¦çš„æµè¡Œæ¦‚å¿µï¼Œè¿™ç§è”é‚¦å­¦ä¹ æ¡†æ¶ä¿æŠ¤äº†çœŸå®æ•°æ®çš„éšç§ï¼ŒåŒæ—¶ç¼“è§£äº†æ•°æ®å¼‚æ„æ€§ã€‚ç„¶è€Œï¼Œæ•´ä¸ªç©ºé—´åŸŸè®¾è®¡ä¸­çš„å†—ä½™ä¿¡æ¯å’Œå™ªå£°ä»ç„¶ç»™è¿™äº›æ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œè¿™ä¸å¯é¿å…åœ°å¢åŠ äº†é€šä¿¡è´Ÿæ‹…ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é«˜èƒ½é‡é›†ä¸­è”é‚¦å­¦ä¹ é¢‘ç‡åŸŸæ„ŸçŸ¥æ–¹æ³•ï¼ˆFedFDï¼‰ã€‚æˆ‘ä»¬çš„FedFDæ–¹æ³•çµæ„Ÿæ¥è‡ªäºç¦»æ•£ä½™å¼¦å˜æ¢èƒ½å°†èƒ½é‡ä¸»è¦åˆ†å¸ƒåˆ°ç‰¹å®šåŒºåŸŸçš„å‘ç°ï¼Œè¢«ç§°ä¸ºé«˜èƒ½é‡é›†ä¸­ã€‚FedFDçš„åŸç†æ˜¯ï¼Œä½é¢‘éƒ¨åˆ†å¦‚é«˜é¢‘æˆåˆ†é€šå¸¸åŒ…å«å†—ä½™ä¿¡æ¯å’Œå™ªå£°ï¼Œå› æ­¤è¿‡æ»¤å®ƒä»¬æœ‰åŠ©äºå‡å°‘é€šä¿¡æˆæœ¬å¹¶ä¼˜åŒ–æ€§èƒ½ã€‚æˆ‘ä»¬çš„FedFDé€šè¿‡æ•°å­¦æ¨¡å‹ä½¿ç”¨äºŒè¿›åˆ¶æ©ç ä¿ç•™ä½é¢‘æˆåˆ†ï¼Œå¹¶é€šè¿‡é¢‘ç‡åŸŸåˆ†å¸ƒå¯¹é½å®ç°æœ€ä¼˜è§£ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨æŸå¤±ä¸­åŠ å…¥äº†åŸºäºçœŸå®æ•°æ®çš„åˆæˆåˆ†ç±»ï¼Œä»¥æé«˜ä½é¢‘æˆåˆ†çš„è´¨é‡ã€‚åœ¨äº”ä¸ªå›¾åƒå’Œè¯­éŸ³æ•°æ®é›†ä¸­ï¼ŒFedFDç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯æ–¹æ³•å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†é€šä¿¡æˆæœ¬ã€‚ä¾‹å¦‚ï¼Œåœ¨CIFAR-10æ•°æ®é›†ä¸Šï¼Œä»¥ç‹„åˆ©å…‹é›·ç³»æ•°Î±&#x3D;0.01ä¸ºä¾‹ï¼ŒFedFDçš„é€šä¿¡æˆæœ¬é™ä½äº†è‡³å°‘37.78%ï¼ŒåŒæ—¶æ€§èƒ½æå‡äº†10.88%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12630v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰åœ¨æ— éœ€æ•°æ®å…±äº«çš„æƒ…å†µä¸‹å‘ˆç°åä½œä¼˜åŒ–çš„å·¨å¤§æ½œåŠ›ã€‚é€šè¿‡åˆ©ç”¨æ•°æ®é›†è’¸é¦çš„æµè¡Œæ¦‚å¿µï¼Œä½¿ç”¨åˆæˆæ•°æ®ä»£æ›¿çœŸå®æ•°æ®å‘é€åˆ°æœåŠ¡å™¨ï¼Œä¿æŠ¤çœŸå®æ•°æ®éšç§å¹¶ç¼“è§£æ•°æ®å¼‚è´¨æ€§ã€‚ç„¶è€Œï¼Œæ•´ä¸ªç©ºé—´åŸŸè®¾è®¡ä¸­çš„å†—ä½™ä¿¡æ¯å’Œå™ªå£°ä»ç„¶æŒ‘æˆ˜ç€è¿™ç§æ–¹æ³•ï¼Œè¿™ä¸å¯é¿å…åœ°å¢åŠ äº†é€šä¿¡è´Ÿæ‹…ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢‘ç‡åŸŸæ„ŸçŸ¥è”é‚¦å­¦ä¹ çš„æ–¹æ³•FedFDï¼Œå…·æœ‰é«˜èƒ½é›†ä¸­åº¦ã€‚FedFDçš„çµæ„Ÿæ¥è‡ªäºç¦»æ•£ä½™å¼¦å˜æ¢èƒ½å°†èƒ½é‡ä¸»è¦åˆ†å¸ƒåˆ°ç‰¹å®šåŒºåŸŸçš„å‘ç°ï¼Œç§°ä¸ºé«˜èƒ½é›†ä¸­åº¦ã€‚FedFDçš„åŸç†æ˜¯ä½èƒ½é‡ï¼ˆå¦‚é«˜é¢‘æˆåˆ†ï¼‰é€šå¸¸åŒ…å«å†—ä½™ä¿¡æ¯å’Œå™ªå£°ï¼Œå› æ­¤è¿‡æ»¤å®ƒä»¬æœ‰åŠ©äºé™ä½é€šä¿¡æˆæœ¬å¹¶ä¼˜åŒ–æ€§èƒ½ã€‚æˆ‘ä»¬çš„FedFDé€šè¿‡æ•°å­¦å…¬å¼åŒ–ä¿ç•™ä½é¢‘æˆåˆ†å¹¶ä½¿ç”¨äºŒè¿›åˆ¶è’™ç‰ˆï¼Œé€šè¿‡é¢‘ç‡åŸŸåˆ†å¸ƒå¯¹é½å®ç°æœ€ä¼˜è§£å†³æ–¹æ¡ˆã€‚ç‰¹åˆ«æ˜¯ï¼Œå°†çœŸå®æ•°æ®é©±åŠ¨çš„åˆæˆåˆ†ç±»å¼•å…¥æŸå¤±ä»¥æé«˜ä½é¢‘æˆåˆ†çš„è´¨é‡ã€‚åœ¨äº”ä¸ªå›¾åƒå’Œè¯­éŸ³æ•°æ®é›†ä¸Šï¼ŒFedFDè¾ƒæœ€æ–°æŠ€æœ¯å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†é€šä¿¡æˆæœ¬ã€‚ä¾‹å¦‚ï¼Œåœ¨CIFAR-10æ•°æ®é›†ä¸Šï¼Œå½“Dirichletç³»æ•°ä¸ºÎ±&#x3D;0.01æ—¶ï¼ŒFedFDå°†é€šä¿¡æˆæœ¬é™ä½äº†è‡³å°‘37.78%ï¼ŒåŒæ—¶æ€§èƒ½æé«˜äº†10.88%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ (FL)èƒ½å¤Ÿåœ¨ä¸å…±äº«æ•°æ®çš„æƒ…å†µä¸‹å®ç°åä½œä¼˜åŒ–ï¼Œä¿æŠ¤çœŸå®æ•°æ®éšç§å¹¶ç¼“è§£æ•°æ®å¼‚è´¨æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´æ•´ä¸ªç©ºé—´åŸŸè®¾è®¡ä¸­çš„å†—ä½™ä¿¡æ¯å’Œå™ªå£°é—®é¢˜ï¼Œå¢åŠ äº†é€šä¿¡è´Ÿæ‹…ã€‚</li>
<li>FedFDæ˜¯ä¸€ç§æ–°çš„é¢‘ç‡åŸŸæ„ŸçŸ¥è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä¿ç•™ä½é¢‘æˆåˆ†å¹¶è¿‡æ»¤å†—ä½™ä¿¡æ¯å’Œå™ªå£°çš„ä¼˜åŒ–è§£å†³æ–¹æ¡ˆã€‚</li>
<li>FedFDåˆ©ç”¨ç¦»æ•£ä½™å¼¦å˜æ¢çš„é«˜èƒ½é›†ä¸­åº¦ç‰¹æ€§ï¼Œå°†èƒ½é‡ä¸»è¦åˆ†å¸ƒåˆ°ç‰¹å®šåŒºåŸŸã€‚</li>
<li>FedFDé€šè¿‡æ•°å­¦å…¬å¼åŒ–ä¿ç•™ä½é¢‘æˆåˆ†å¹¶ä½¿ç”¨äºŒè¿›åˆ¶è’™ç‰ˆå®ç°é¢‘ç‡åŸŸåˆ†å¸ƒå¯¹é½ã€‚</li>
<li>FedFDåœ¨çœŸå®æ•°æ®é©±åŠ¨çš„åˆæˆåˆ†ç±»ä¸­å®ç°äº†é«˜è´¨é‡çš„ä½é¢‘æˆåˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d17d1aec409afb76cdbc1d8f39ee65f" align="middle">
<img src="https://picx.zhimg.com/v2-f3df9d8730e547449bddd19348c79f9b" align="middle">
<img src="https://picx.zhimg.com/v2-4df4c78e8093a2779594036bef80e7c4" align="middle">
<img src="https://picx.zhimg.com/v2-f6323752e962212e1d74c73e6b06f410" align="middle">
<img src="https://picx.zhimg.com/v2-d942ea68eee94af69992e1282bb58835" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation"><a href="#SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation" class="headerlink" title="SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation"></a>SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation</h2><p><strong>Authors:Chenyang Le, Bing Han, Jinshun Li, Songyong Chen, Yanmin Qian</strong></p>
<p>Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs. </p>
<blockquote>
<p>åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSimulSTï¼‰é€šè¿‡è”åˆä¼˜åŒ–è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘ï¼Œåœ¨ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶ä¸‹å®ç°å®æ—¶è·¨è¯­è¨€äº¤æµã€‚ç°æœ‰ç³»ç»Ÿåœ¨å¹³è¡¡ç¿»è¯‘è´¨é‡ã€å»¶è¿Ÿå’Œè¯­ä¹‰è¿è´¯æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­è¨€å¤šå¯¹å¤šçš„åœºæ™¯ä¸­ï¼Œä¸åŒçš„è¯»å–å’Œå†™å…¥ç­–ç•¥é˜»ç¢äº†ç»Ÿä¸€ç­–ç•¥çš„å­¦ä¹ ã€‚æœ¬æ–‡ä»‹ç»äº†SimulMEGAï¼ˆåŸºäºä¸“å®¶æ··åˆé—¨æ§çš„åŒæ­¥ç”Ÿæˆï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— ç›‘ç£çš„ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†å‰ç¼€è®­ç»ƒä¸ä¸“å®¶æ··åˆç²¾ç‚¼å™¨ç›¸ç»“åˆï¼Œä»¥éšå¼çš„æ–¹å¼å­¦ä¹ æœ‰æ•ˆçš„è¯»å–å’Œå†™å…¥å†³ç­–ï¼Œè€Œä¸ä¼šå¢åŠ æ¨ç†æ—¶é—´å¼€é”€ã€‚æˆ‘ä»¬çš„è®¾è®¡åªéœ€è¦å¯¹æ ‡å‡†å˜å‹å™¨æ¶æ„è¿›è¡Œæœ€å°çš„ä¿®æ”¹ï¼Œå°±å¯ä»¥æ¨å¹¿åˆ°è¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµå¼å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹å…­ç§è¯­è¨€å¯¹çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬çš„50äº¿å‚æ•°è¯­éŸ³åˆ°æ–‡æœ¬æ¨¡å‹ä¼˜äºæ— ç¼åŸºçº¿ï¼Œåœ¨å¹³å‡å»¶è¿Ÿ1.5ç§’çš„æƒ…å†µä¸‹ï¼ŒBLEUå€¼é™ä½ä¸åˆ°7%ï¼Œåœ¨3ç§’å»¶è¿Ÿçš„æƒ…å†µä¸‹ï¼Œé™ä½ä¸åˆ°3%ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å°†å…¶æ‰©å±•åˆ°å…·æœ‰å•å‘éª¨å¹²çš„æµå¼TTSæ¥å±•ç¤ºSimulMEGAçš„é€šç”¨æ€§ï¼Œä»è€Œè·å¾—å‡ºè‰²çš„å»¶è¿Ÿè´¨é‡æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01200v2">PDF</a> NeurIPS 2025 poster</p>
<p><strong>æ‘˜è¦</strong><br>SimulMEGAåˆ©ç”¨æ··åˆä¸“å®¶é—¨æ§æœºåˆ¶å®ç°äº†å®æ—¶å¤šè¯­è¨€åŒæ­¥ç¿»è¯‘ã€‚å®ƒé€šè¿‡å‰ç¼€è®­ç»ƒä¸æ··åˆä¸“å®¶ç²¾ç‚¼å™¨ç›¸ç»“åˆï¼Œä»¥éšå¼æ–¹å¼å­¦ä¹ æœ‰æ•ˆçš„è¯»å†™å†³ç­–ï¼Œä¼˜åŒ–äº†è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘è¿‡ç¨‹ï¼Œè§£å†³äº†å¤šè¯­è¨€ç¿»è¯‘ä¸­å­˜åœ¨ç¿»è¯‘è´¨é‡ã€å»¶è¿Ÿå’Œè¯­ä¹‰è¿è´¯æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†å˜å‹å™¨æ¶æ„ä¸Šåªéœ€è¿›è¡Œå°‘é‡ä¿®æ”¹ï¼Œå¹¶èƒ½é€‚ç”¨äºè¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµåª’ä½“ä»»åŠ¡ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒSimulMEGAåœ¨å…­ç§è¯­è¨€å¯¹ä¸Šä¼˜äºæ— ç¼åŸºçº¿æ¨¡å‹ï¼Œåœ¨å¹³å‡å»¶è¿Ÿä¸º1.5ç§’çš„æƒ…å†µä¸‹BLEUå€¼é™ä½ä¸åˆ°7%ï¼Œåœ¨å»¶è¿Ÿä¸º3ç§’çš„æƒ…å†µä¸‹BLEUå€¼é™ä½ä¸åˆ°3%ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å±•ç¤ºäº†åœ¨å…·æœ‰å•å‘éª¨å¹²çš„æµåª’ä½“TTSä¸­çš„é€šç”¨æ€§ï¼Œåœ¨å»¶è¿Ÿè´¨é‡æƒè¡¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SimulMEGAè§£å†³äº†SimulSTä¸­ç¿»è¯‘è´¨é‡ã€å»¶è¿Ÿå’Œè¯­ä¹‰è¿è´¯æ€§çš„å¹³è¡¡é—®é¢˜ã€‚</li>
<li>SimulMEGAé‡‡ç”¨æ··åˆä¸“å®¶é—¨æ§æœºåˆ¶è¿›è¡Œæ”¿ç­–å­¦ä¹ ï¼Œç»“åˆäº†å‰ç¼€è®­ç»ƒå’Œæ··åˆä¸“å®¶ç²¾ç‚¼å™¨ã€‚</li>
<li>è¯¥æ–¹æ³•ä»¥éšå¼æ–¹å¼å­¦ä¹ æœ‰æ•ˆçš„è¯»å†™å†³ç­–ï¼Œä¼˜åŒ–è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘è¿‡ç¨‹ã€‚</li>
<li>SimulMEGAé€‚ç”¨äºè¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµåª’ä½“ä»»åŠ¡ã€‚</li>
<li>åœ¨å…­ç§è¯­è¨€å¯¹çš„è¯„ä¼°ä¸­ï¼ŒSimulMEGAè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨å»¶è¿Ÿå’Œç¿»è¯‘è´¨é‡æ–¹é¢å–å¾—è‰¯å¥½ç»“æœã€‚</li>
<li>SimulMEGAå…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºæµåª’ä½“TTSä¸­ï¼Œå±•ç¤ºäº†è‰¯å¥½çš„å»¶è¿Ÿè´¨é‡æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28f48bfdbe354c503fc6249000511955" align="middle">
<img src="https://picx.zhimg.com/v2-c89addb260e018c87e9e640d736dda3e" align="middle">
<img src="https://picx.zhimg.com/v2-38e5b98d73926dee6d1adfcf8a4fd647" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="HM-Talker-Hybrid-Motion-Modeling-for-High-Fidelity-Talking-Head-Synthesis"><a href="#HM-Talker-Hybrid-Motion-Modeling-for-High-Fidelity-Talking-Head-Synthesis" class="headerlink" title="HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head   Synthesis"></a>HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head   Synthesis</h2><p><strong>Authors:Shiyu Liu, Kui Jiang, Xianming Liu, Hongxun Yao, Xiaocheng Feng</strong></p>
<p>Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlationsâ€“an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit&#x2F;explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit&#x2F;explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talkerâ€™s superiority over state-of-the-art methods in visual quality and lip-sync accuracy. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨è§†é¢‘ç”Ÿæˆæé«˜äº†äººæœºäº¤äº’ä¸­çš„ç”¨æˆ·å‚ä¸åº¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ç»å¸¸äº§ç”Ÿè¿åŠ¨æ¨¡ç³Šå’Œå˜´å”‡æŠ–åŠ¨çš„è§†é¢‘ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºéŸ³é¢‘é¢éƒ¨è¿åŠ¨å…³è”çš„éšå¼å»ºæ¨¡â€”â€”è¿™ç§æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„å‘éŸ³å…ˆéªŒçŸ¥è¯†ï¼ˆå³ä¸è¯­éŸ³ç›¸å…³çš„é¢éƒ¨è¿åŠ¨çš„è§£å‰–æŒ‡å¯¼ï¼‰ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†HM-Talkerï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´è¿è´¯çš„è¯´è¯äººå¤´éƒ¨çš„å…¨æ–°æ¡†æ¶ã€‚HM-Talkeråˆ©ç”¨äº†ä¸€ç§æ··åˆè¿åŠ¨è¡¨ç¤ºæ³•ï¼Œç»“åˆäº†éšå¼å’Œæ˜¾å¼è¿åŠ¨çº¿ç´¢ã€‚æ˜¾å¼çº¿ç´¢ä½¿ç”¨è¡Œä¸ºå•ä½ï¼ˆAUsï¼‰ï¼Œå³é¢éƒ¨è§£å‰–ä¸Šå®šä¹‰çš„è‚Œè‚‰è¿åŠ¨ï¼Œä»¥åŠéšå¼ç‰¹å¾æ¥æœ€å°åŒ–éŸ³ç´ -é¢éƒ¨åŠ¨ä½œä¸åŒ¹é…ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„è·¨æ¨¡æ€åˆ†ç¦»æ¨¡å—ï¼ˆCMDMï¼‰åœ¨é¢„æµ‹ä¸è§†è§‰çº¿ç´¢å¯¹é½çš„éŸ³é¢‘è¾“å…¥çš„ç›´æ¥AUsæ—¶ï¼Œæå–äº†äº’è¡¥çš„éšå¼&#x2F;æ˜¾å¼è¿åŠ¨ç‰¹å¾ã€‚ä¸ºäº†å‡è½»æ˜¾å¼ç‰¹å¾ä¸­çš„èº«ä»½ç›¸å…³åè§ï¼Œå¢å¼ºè·¨ä¸»ä½“æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ··åˆè¿åŠ¨å»ºæ¨¡æ¨¡å—ï¼ˆHMMMï¼‰ã€‚è¯¥æ¨¡å—åŠ¨æ€åˆå¹¶éšæœºé…å¯¹çš„éšå¼&#x2F;æ˜¾å¼ç‰¹å¾ï¼Œå¼ºåˆ¶æ‰§è¡Œèº«ä»½æ— å…³çš„å­¦ä¹ ã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œå®ç°äº†è·¨ä¸åŒèº«ä»½çš„ç¨³å¥å˜´å”‡åŒæ­¥ï¼Œæ¨åŠ¨äº†ä¸ªæ€§åŒ–è¯´è¯äººå¤´éƒ¨åˆæˆçš„è¿›æ­¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHM-Talkeråœ¨è§†è§‰è´¨é‡å’Œå˜´å”‡åŒæ­¥ç²¾åº¦æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10566v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶HM-Talkerï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´è¿è´¯çš„è°ˆè¯å¤´è§†é¢‘ã€‚å®ƒé€šè¿‡ç»“åˆéšå¼å’Œæ˜¾å¼è¿åŠ¨çº¿ç´¢æ¥è§£å†³ç°æœ‰æ–¹æ³•ä¸­å¸¸è§çš„è¿åŠ¨æ¨¡ç³Šå’Œå”‡éƒ¨æŠ–åŠ¨é—®é¢˜ã€‚HM-Talkerä½¿ç”¨ä¸€ç§æ··åˆè¿åŠ¨è¡¨ç¤ºæ–¹æ³•ï¼Œä½¿ç”¨åŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰ç­‰æ˜¾å¼çº¿ç´¢å’Œéšå¼ç‰¹å¾æ¥æœ€å°åŒ–éŸ³ç´ -è¡¨æƒ…åŠ¨ä½œä¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥è·¨æ¨¡æ€åˆ†è§£æ¨¡å—å’Œæ··åˆè¿åŠ¨å»ºæ¨¡æ¨¡å—ï¼Œæé«˜äº†è·¨ä¸»ä½“æ³›åŒ–èƒ½åŠ›ï¼Œå®ç°äº†ç¨³å¥çš„å”‡éƒ¨åŒæ­¥ã€‚å®éªŒè¡¨æ˜ï¼ŒHM-Talkeråœ¨è§†è§‰è´¨é‡å’Œå”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HM-Talkeræ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆé«˜ä¿çœŸè°ˆè¯å¤´è§†é¢‘çš„å…¨æ–°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆéšå¼å’Œæ˜¾å¼è¿åŠ¨çº¿ç´¢æ¥è§£å†³è¿åŠ¨æ¨¡ç³Šå’Œå”‡éƒ¨æŠ–åŠ¨é—®é¢˜ã€‚</li>
<li>HM-Talkerä½¿ç”¨åŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰ç­‰æ˜¾å¼çº¿ç´¢è¿›è¡Œé¢éƒ¨è‚Œè‚‰è¿åŠ¨çš„å»ºæ¨¡ã€‚</li>
<li>è·¨æ¨¡æ€åˆ†è§£æ¨¡å—ï¼ˆCMDMï¼‰ç”¨äºæå–éšå¼å’Œæ˜¾å¼è¿åŠ¨ç‰¹å¾çš„äº’è¡¥ä¿¡æ¯ï¼Œå¹¶é¢„æµ‹ä¸è§†è§‰çº¿ç´¢å¯¹é½çš„éŸ³é¢‘è¾“å…¥çš„AUsã€‚</li>
<li>æ··åˆè¿åŠ¨å»ºæ¨¡æ¨¡å—ï¼ˆHMMMï¼‰é€šè¿‡åŠ¨æ€åˆå¹¶éšå¼å’Œæ˜¾å¼ç‰¹å¾æ¥å‡è½»èº«ä»½ç›¸å…³åè§ï¼Œæé«˜è·¨ä¸»ä½“æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºHM-Talkeråœ¨è§†è§‰è´¨é‡å’Œå”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca59ea4c6cec848c51ba9e0202a6f93d" align="middle">
<img src="https://picx.zhimg.com/v2-40ba1f350c6a5ffe7bc08e3f4e2e2217" align="middle">
<img src="https://picx.zhimg.com/v2-63ac38f4ef71ce02b0f52513267ddb22" align="middle">
<img src="https://picx.zhimg.com/v2-89c0fba6949d44d082d9bd511e4d327c" align="middle">
<img src="https://picx.zhimg.com/v2-c7f0a8842fca7a54c0a776066ddaef55" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="PESTO-Real-Time-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective"><a href="#PESTO-Real-Time-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective" class="headerlink" title="PESTO: Real-Time Pitch Estimation with Self-supervised   Transposition-equivariant Objective"></a>PESTO: Real-Time Pitch Estimation with Self-supervised   Transposition-equivariant Objective</h2><p><strong>Authors:Alain Riou, Bernardo Torres, Ben Hayes, Stefan Lattner, GaÃ«tan Hadjeres, GaÃ«l Richard, Geoffroy Peeters</strong></p>
<p>In this paper, we introduce PESTO, a self-supervised learning approach for single-pitch estimation using a Siamese architecture. Our model processes individual frames of a Variable-$Q$ Transform (VQT) and predicts pitch distributions. The neural network is designed to be equivariant to translations, notably thanks to a Toeplitz fully-connected layer. In addition, we construct pitch-shifted pairs by translating and cropping the VQT frames and train our model with a novel class-based transposition-equivariant objective, eliminating the need for annotated data. Thanks to this architecture and training objective, our model achieves remarkable performances while being very lightweight ($130$k parameters). Evaluations on music and speech datasets (MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms self-supervised baselines but also competes with supervised methods, exhibiting superior cross-dataset generalization. Finally, we enhance PESTOâ€™s practical utility by developing a streamable VQT implementation using cached convolutions. Combined with our modelâ€™s low latency (less than 10 ms) and minimal parameter count, this makes PESTO particularly suitable for real-time applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†PESTOï¼Œè¿™æ˜¯ä¸€ç§åŸºäºSiameseæ¶æ„çš„å•éŸ³é«˜ä¼°è®¡è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¤„ç†å¯å˜Qå˜æ¢ï¼ˆVQTï¼‰çš„å•ä¸ªå¸§ï¼Œå¹¶é¢„æµ‹éŸ³é«˜åˆ†å¸ƒã€‚ç¥ç»ç½‘ç»œè¢«è®¾è®¡æˆå¯¹ç­‰ä½ç¿»è¯‘ï¼Œç‰¹åˆ«æ˜¯å¾—ç›Šäºæ‰˜æ™®åˆ©å…¹å…¨è¿æ¥å±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç¿»è¯‘å’Œè£å‰ªVQTå¸§æ¥æ„å»ºéŸ³é«˜å¯¹ï¼Œå¹¶ç”¨æ–°å‹ç±»åˆ«å¹³ç§»ç­‰å˜ç›®æ ‡æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹æ³¨é‡Šæ•°æ®çš„éœ€æ±‚ã€‚ç”±äºè¿™ç§æ¶æ„å’ŒåŸ¹è®­ç›®æ ‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼ŒåŒæ—¶éå¸¸è½»é‡çº§ï¼ˆ130kä¸ªå‚æ•°ï¼‰ã€‚åœ¨éŸ³ä¹å’Œè¯­éŸ³æ•°æ®é›†ï¼ˆMIR-1Kã€MDB-stem-synthå’ŒPTDBï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPESTOä¸ä»…è¶…è¶Šäº†è‡ªç›‘ç£åŸºçº¿ï¼Œè€Œä¸”ä¸ç›‘ç£æ–¹æ³•ç›¸ç«äº‰ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¼“å­˜å·ç§¯å¼€å‘äº†å¯æµå¼ä¼ è¾“çš„VQTå®ç°ï¼Œå¢å¼ºäº†PESTOçš„å®é™…æ•ˆç”¨ã€‚ç»“åˆæˆ‘ä»¬çš„æ¨¡å‹ä½å»¶è¿Ÿï¼ˆå°äº10æ¯«ç§’ï¼‰å’Œæå°‘çš„å‚æ•°æ•°é‡ï¼Œè¿™ä½¿å¾—PESTOç‰¹åˆ«é€‚åˆå®æ—¶åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01488v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PESTOï¼Œä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„å•éŸ³é«˜ä¼°è®¡æ–¹æ³•ï¼Œé‡‡ç”¨Siameseæ¶æ„ã€‚è¯¥æ¨¡å‹å¤„ç†å¯å˜Qå€¼å˜æ¢ï¼ˆVQTï¼‰çš„å•ä¸ªå¸§ï¼Œé¢„æµ‹éŸ³é«˜åˆ†å¸ƒã€‚ç¥ç»ç½‘ç»œè®¾è®¡ä¸ºå¯¹ç¿»è¯‘å…·æœ‰ç­‰å˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¾—ç›ŠäºToeplitzå…¨è¿æ¥å±‚ã€‚é€šè¿‡æ„å»ºéŸ³é«˜ç§»ä½å¯¹ï¼Œç¿»è¯‘å’Œè£å‰ªVQTå¸§ï¼Œä»¥åŠä½¿ç”¨æ–°å‹åŸºäºç±»åˆ«çš„è½¬ç½®ç­‰å˜ç›®æ ‡è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæ— éœ€æ ‡æ³¨æ•°æ®ã€‚è¯¥æ¨¡å‹å’Œè®­ç»ƒç›®æ ‡ä½¿æ¨¡å‹åœ¨ä¿æŒè½»é‡çº§ï¼ˆ130kå‚æ•°ï¼‰çš„åŒæ—¶å–å¾—äº†æ˜¾è‘—æ€§èƒ½ã€‚åœ¨MUSICå’Œè¯­éŸ³æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPESTOä¸ä»…ä¼˜äºè‡ªç›‘ç£åŸºçº¿ï¼Œè€Œä¸”ä¸ç›‘ç£æ–¹æ³•ç«äº‰ï¼Œå±•ç°å‡ºä¼˜è¶Šçš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œé€šè¿‡ç¼“å­˜å·ç§¯å¼€å‘äº†ä¸€ä¸ªå¯æµå¼ä¼ è¾“çš„VQTå®ç°ï¼Œå¢å¼ºäº†PESTOçš„å®ç”¨æ€§ã€‚å…¶ä½å»¶è¿Ÿå’Œå°‘é‡å‚æ•°ä½¿å…¶æˆä¸ºå®æ—¶åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PESTOæ˜¯ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„å•éŸ³é«˜ä¼°è®¡æ–¹æ³•ï¼Œé‡‡ç”¨Siameseæ¶æ„è¿›è¡ŒéŸ³é«˜é¢„æµ‹ã€‚</li>
<li>è¯¥æ¨¡å‹å¤„ç†VQTå¸§ï¼Œå¹¶é¢„æµ‹éŸ³é«˜åˆ†å¸ƒï¼Œåˆ©ç”¨Toeplitzå…¨è¿æ¥å±‚å®ç°ç¿»è¯‘ç­‰å˜æ€§ã€‚</li>
<li>é€šè¿‡æ„å»ºéŸ³é«˜ç§»ä½å¯¹ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>PESTOæ¨¡å‹å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å‚æ•°æ•°é‡ä¸Šä¿æŒè½»é‡çº§ï¼ˆä»…æœ‰130kå‚æ•°ï¼‰ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒPESTOåœ¨è‡ªç›‘ç£å­¦ä¹ å’Œç›‘ç£å­¦ä¹ æ–¹é¢éƒ½æœ‰å‡ºè‰²çš„è¡¨ç°ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>PESTOå…·æœ‰å®æ—¶åº”ç”¨çš„æ½œåŠ›ï¼Œä½å»¶è¿Ÿå’Œå¯æµå¼ä¼ è¾“çš„VQTå®ç°å¢å¼ºäº†å…¶å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-730defe69f429bdc72a357d7bb60d8cd" align="middle">
<img src="https://picx.zhimg.com/v2-122aead001be4282167efab35cf0e291" align="middle">
<img src="https://picx.zhimg.com/v2-7da7476c6d9ad825786e4b52c6996d5b" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Recent-Trends-in-Distant-Conversational-Speech-Recognition-A-Review-of-CHiME-7-and-8-DASR-Challenges"><a href="#Recent-Trends-in-Distant-Conversational-Speech-Recognition-A-Review-of-CHiME-7-and-8-DASR-Challenges" class="headerlink" title="Recent Trends in Distant Conversational Speech Recognition: A Review of   CHiME-7 and 8 DASR Challenges"></a>Recent Trends in Distant Conversational Speech Recognition: A Review of   CHiME-7 and 8 DASR Challenges</h2><p><strong>Authors:Samuele Cornell, Christoph Boeddeker, Taejin Park, He Huang, Desh Raj, Matthew Wiesner, Yoshiki Masuyama, Xuankai Chang, Zhong-Qiu Wang, Stefano Squartini, Paola Garcia, Shinji Watanabe</strong></p>
<p>The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on multi-channel, generalizable, joint automatic speech recognition (ASR) and diarization of conversational speech. With participation from 9 teams submitting 32 diverse systems, these challenges have contributed to state-of-the-art research in the field. This paper outlines the challengesâ€™ design, evaluation metrics, datasets, and baseline systems while analyzing key trends from participant submissions. From this analysis it emerges that: 1) Most participants use end-to-end (e2e) ASR systems, whereas hybrid systems were prevalent in previous CHiME challenges. This transition is mainly due to the availability of robust large-scale pre-trained models, which lowers the data burden for e2e-ASR. 2) Despite recent advances in neural speech separation and enhancement (SSE), all teams still heavily rely on guided source separation, suggesting that current neural SSE techniques are still unable to reliably deal with complex scenarios and different recording setups. 3) All best systems employ diarization refinement via target-speaker diarization techniques. Accurate speaker counting in the first diarization pass is thus crucial to avoid compounding errors and CHiME-8 DASR participants especially focused on this part. 4) Downstream evaluation via meeting summarization can correlate weakly with transcription quality due to the remarkable effectiveness of large-language models in handling errors. On the NOTSOFAR-1 scenario, even systems with over 50% time-constrained minimum permutation WER can perform roughly on par with the most effective ones (around 11%). 5) Despite recent progress, accurately transcribing spontaneous speech in challenging acoustic environments remains difficult, even when using computationally intensive system ensembles. </p>
<blockquote>
<p>CHiME-7å’ŒCHiME-8è¿œç¨‹è¯­éŸ³è¯†åˆ«ï¼ˆDASRï¼‰æŒ‘æˆ˜çš„é‡ç‚¹æ˜¯å¤šé€šé“ã€é€šç”¨åŒ–ã€è”åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¯¹è¯è¯­éŸ³çš„æ‘˜è¦åŒ–ã€‚å…±æœ‰9æ”¯é˜Ÿä¼æäº¤äº†32ç§ä¸åŒçš„ç³»ç»Ÿå‚ä¸æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„æœ€æ–°ç ”ç©¶ã€‚æœ¬æ–‡æ¦‚è¿°äº†æŒ‘æˆ˜çš„è®¾è®¡ã€è¯„ä¼°æŒ‡æ ‡ã€æ•°æ®é›†å’ŒåŸºå‡†ç³»ç»Ÿï¼ŒåŒæ—¶åˆ†æäº†å‚ä¸è€…æäº¤çš„å…³é”®è¶‹åŠ¿ã€‚ä»åˆ†æä¸­å¯ä»¥çœ‹å‡ºï¼š1ï¼‰å¤§å¤šæ•°å‚ä¸è€…ä½¿ç”¨ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰ASRç³»ç»Ÿï¼Œè€Œä¹‹å‰çš„CHiMEæŒ‘æˆ˜ä¸­æ··åˆç³»ç»Ÿæ›´ä¸ºæ™®éã€‚è¿™ç§è½¬å˜ä¸»è¦æ˜¯ç”±äºå¼ºå¤§çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å‡ºç°ï¼Œé™ä½äº†ç«¯åˆ°ç«¯ASRçš„æ•°æ®è´Ÿæ‹…ã€‚2ï¼‰å°½ç®¡ç¥ç»ç½‘ç»œè¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºï¼ˆSSEï¼‰æŠ€æœ¯å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œæ‰€æœ‰å›¢é˜Ÿä»ç„¶ä¸¥é‡ä¾èµ–äºå¼•å¯¼æºåˆ†ç¦»ï¼Œè¿™è¡¨æ˜å½“å‰çš„ç¥ç»ç½‘ç»œSSEæŠ€æœ¯ä»ç„¶æ— æ³•å¯é åœ°å¤„ç†å¤æ‚åœºæ™¯å’Œä¸åŒå½•éŸ³è®¾ç½®ã€‚3ï¼‰æ‰€æœ‰æœ€ä½³ç³»ç»Ÿéƒ½é€šè¿‡ç›®æ ‡è¯´è¯äººæ‘˜è¦åŒ–æŠ€æœ¯è¿›è¡Œäº†æ‘˜è¦åŒ–æ”¹è¿›ã€‚å› æ­¤ï¼Œç¬¬ä¸€æ¬¡æ‘˜è¦åŒ–è¿‡ç¨‹ä¸­çš„å‡†ç¡®è¯´è¯äººæ•°è‡³å…³é‡è¦ï¼Œå¯ä»¥é¿å…ç´¯ç§¯é”™è¯¯ï¼ŒCHiME-8 DASRå‚ä¸è€…å°¤å…¶å…³æ³¨è¿™ä¸€ç‚¹ã€‚4ï¼‰é€šè¿‡ä¼šè®®æ‘˜è¦è¿›è¡Œçš„ä¸‹æ¸¸è¯„ä¼°ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é”™è¯¯æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ï¼Œå¯èƒ½ä¸è½¬å½•è´¨é‡å­˜åœ¨å¾®å¼±çš„ç›¸å…³æ€§ã€‚åœ¨NOTSOFAR-1åœºæ™¯ä¸­ï¼Œå³ä½¿æ—¶é—´å—é™æ—¶çš„æœ€å°ç½®æ¢WERè¶…è¿‡50%çš„ç³»ç»Ÿï¼Œå…¶è¡¨ç°ä¹Ÿå¤§è‡´ä¸æœ€å‡ºè‰²çš„ç³»ç»Ÿï¼ˆçº¦11%ï¼‰ç›¸å½“ã€‚5ï¼‰å°½ç®¡å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œä½†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­å‡†ç¡®è½¬å½•å³å…´æ¼”è®²ä»ç„¶å¾ˆå›°éš¾ï¼Œå³ä½¿ä½¿ç”¨è®¡ç®—å¯†é›†å‹çš„ç³»ç»Ÿé›†åˆä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18161v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>CHiME-7å’ŒCHiME-8çš„è¿œè·ç¦»è¯­éŸ³è¯†åˆ«ï¼ˆDASRï¼‰æŒ‘æˆ˜èšç„¦äºå¤šé€šé“ã€é€šç”¨ã€è”åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¯¹è¯è¯­éŸ³çš„æ‘˜è¦åŒ–ã€‚è¿™äº›æŒ‘æˆ˜å¸å¼•äº†9ä¸ªå›¢é˜Ÿçš„å‚ä¸ï¼Œå…±æäº¤äº†32ä¸ªå¤šæ ·åŒ–çš„ç³»ç»Ÿï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶è´¡çŒ®äº†æœ€å‰æ²¿çš„æˆæœã€‚æœ¬æ–‡æ¦‚è¿°äº†æŒ‘æˆ˜çš„è®¾è®¡ã€è¯„ä¼°æŒ‡æ ‡ã€æ•°æ®é›†å’ŒåŸºçº¿ç³»ç»Ÿï¼ŒåŒæ—¶åˆ†æäº†å‚ä¸è€…æäº¤çš„å…³é”®è¶‹åŠ¿ã€‚åˆ†æè¡¨æ˜ï¼š1)å¤§å¤šæ•°å‚ä¸è€…ä½¿ç”¨ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰ASRç³»ç»Ÿï¼Œè€Œä¹‹å‰çš„CHiMEæŒ‘æˆ˜ä¸­æ··åˆç³»ç»Ÿæ›´ä¸ºæ™®éã€‚è¿™ä¸€è½¬å˜ä¸»è¦æ˜¯ç”±äºå¼ºå¤§çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å¯è·å¾—æ€§ï¼Œé™ä½äº†e2e-ASRçš„æ•°æ®è´Ÿæ‹…ã€‚2)å°½ç®¡ç¥ç»è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºï¼ˆSSEï¼‰æ–¹é¢å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œæ‰€æœ‰å›¢é˜Ÿä»ç„¶ä¸¥é‡ä¾èµ–äºå¼•å¯¼æºåˆ†ç¦»ï¼Œè¡¨æ˜å½“å‰çš„ç¥ç»SSEæŠ€æœ¯ä»ç„¶æ— æ³•å¯é åœ°å¤„ç†å¤æ‚åœºæ™¯å’Œä¸åŒå½•éŸ³è®¾ç½®ã€‚3)æ‰€æœ‰æœ€ä½³ç³»ç»Ÿéƒ½é€šè¿‡ç›®æ ‡è¯´è¯äººæ‘˜è¦æŠ€æœ¯è¿›è¡Œäº†æ‘˜è¦ä¿®æ­£ã€‚å› æ­¤ï¼Œç¬¬ä¸€æ¬¡æ‘˜è¦è¿‡ç¨‹ä¸­çš„å‡†ç¡®è¯´è¯äººè®¡æ•°è‡³å…³é‡è¦ï¼Œå¯ä»¥é¿å…å¤åˆé”™è¯¯ï¼ŒCHiME-8 DASRå‚ä¸è€…å°¤å…¶å…³æ³¨è¿™ä¸€éƒ¨åˆ†ã€‚4)é€šè¿‡ä¼šè®®æ‘˜è¦è¿›è¡Œçš„ä¸‹æ¸¸è¯„ä¼°ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é”™è¯¯æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ï¼Œå¯èƒ½ä¸è½¬å½•è´¨é‡ç›¸å…³æ€§è¾ƒå¼±ã€‚åœ¨NOTSOFAR-1åœºæ™¯ä¸­ï¼Œå³ä½¿ç³»ç»Ÿçš„æœ€å°æ’åˆ—WERè¶…è¿‡æ—¶é—´é™åˆ¶çš„50%ï¼Œå…¶æ€§èƒ½ä¹Ÿå¯èƒ½ä¸æœ€æœ‰æ•ˆçš„æ–¹æ³•å¤§è‡´ç›¸åŒï¼ˆçº¦11%ï¼‰ã€‚5)å°½ç®¡å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œä½†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­å‡†ç¡®è½¬å½•è‡ªå‘è¯­éŸ³ä»ç„¶å¾ˆå›°éš¾ï¼Œå³ä½¿ä½¿ç”¨è®¡ç®—å¯†é›†å‹çš„ç³»ç»Ÿé›†åˆä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CHiME-7å’ŒCHiME-8æŒ‘æˆ˜æ¨åŠ¨äº†å¤šé€šé“å’Œé€šç”¨è”åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»¥åŠå¯¹è¯è¯­éŸ³æ‘˜è¦åŒ–çš„ç ”ç©¶å‰æ²¿ã€‚</li>
<li>ç«¯åˆ°ç«¯ASRç³»ç»Ÿé€æ¸å–ä»£æ··åˆç³»ç»Ÿï¼Œå¾—ç›Šäºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„æ™®åŠã€‚</li>
<li>å°½ç®¡ç¥ç»è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºæŠ€æœ¯å–å¾—è¿›å±•ï¼Œä½†å›¢é˜Ÿä»ä¾èµ–å¼•å¯¼æºåˆ†ç¦»æ–¹æ³•ï¼Œæš—ç¤ºå½“å‰æŠ€æœ¯é¢å¯¹å¤æ‚åœºæ™¯å’Œå¤šå˜å½•éŸ³ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚</li>
<li>æœ€ä½³ç³»ç»Ÿå‡é‡è§†é€šè¿‡ç›®æ ‡è¯´è¯äººæ‘˜è¦æŠ€æœ¯è¿›è¡Œæ‘˜è¦ä¿®æ­£ï¼Œå‡†ç¡®è¯´è¯äººè®¡æ•°å¯¹é¿å…é”™è¯¯ç´¯ç§¯è‡³å…³é‡è¦ã€‚</li>
<li>ä¸‹æ¸¸è¯„ä¼°å¦‚ä¼šè®®æ‘˜è¦ä¸è½¬å½•è´¨é‡çš„ç›¸å…³æ€§å¯èƒ½è¾ƒå¼±ï¼Œç‰¹åˆ«æ˜¯å½“ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­å‡†ç¡®è½¬å½•è‡ªå‘è¯­éŸ³ä»ç„¶æ˜¯ä¸€ä¸ªéš¾é¢˜ï¼Œéœ€è¦è®¡ç®—å¯†é›†å‹çš„ç³»ç»Ÿé›†åˆæ¥å¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44450e560ab6b754f6d7d7cbf7d6cc27" align="middle">
<img src="https://picx.zhimg.com/v2-9380dd3efc4cbc5d9291c1f7107acc28" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Application-of-Whisper-in-Clinical-Practice-the-Post-Stroke-Speech-Assessment-during-a-Naming-Task"><a href="#Application-of-Whisper-in-Clinical-Practice-the-Post-Stroke-Speech-Assessment-during-a-Naming-Task" class="headerlink" title="Application of Whisper in Clinical Practice: the Post-Stroke Speech   Assessment during a Naming Task"></a>Application of Whisper in Clinical Practice: the Post-Stroke Speech   Assessment during a Naming Task</h2><p><strong>Authors:Milena Davudova, Ziyuan Cai, Valentina Giunchiglia, Dragos C. Gruia, Giulia Sanguedolce, Adam Hampshire, Fatemeh Geranmayeh</strong></p>
<p>Detailed assessment of language impairment following stroke remains a cognitively complex and clinician-intensive task, limiting timely and scalable diagnosis. Automatic Speech Recognition (ASR) foundation models offer a promising pathway to augment human evaluation through intelligent systems, but their effectiveness in the context of speech and language impairment remains uncertain. In this study, we evaluate whether Whisper, a state-of-the-art ASR foundation model, can be applied to transcribe and analyze speech from patients with stroke during a commonly used picture-naming task. We assess both verbatim transcription accuracy and the modelâ€™s ability to support downstream prediction of language function, which has major implications for outcomes after stroke. Our results show that the baseline Whisper model performs poorly on single-word speech utterances. Nevertheless, fine-tuning Whisper significantly improves transcription accuracy (reducing Word Error Rate by 87.72% in healthy speech and 71.22% in speech from patients). Further, learned representations from the model enable accurate prediction of speech quality (average F1 Macro of 0.74 for healthy, 0.75 for patients). However, evaluations on an unseen (TORGO) dataset reveal limited generalizability, highlighting the inability of Whisper to perform zero-shot transcription of single-word utterances on out-of-domain clinical speech and emphasizing the need to adapt models to specific clinical populations. While challenges remain in cross-domain generalization, these findings highlight the potential of foundation models, when appropriately fine-tuned, to advance automated speech and language assessment and rehabilitation for stroke-related impairments. </p>
<blockquote>
<p>å¯¹ä¸­é£åè¯­è¨€éšœç¢çš„è¯¦ç»†è¯„ä¼°ä»ç„¶æ˜¯ä¸€ä¸ªè®¤çŸ¥ä¸Šå¤æ‚ä¸”éœ€è¦å¤§é‡ä¸´åºŠåŒ»ç”Ÿçš„ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†åŠæ—¶å’Œå¯è§„æ¨¡åŒ–çš„è¯Šæ–­ã€‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åŸºç¡€æ¨¡å‹é€šè¿‡æ™ºèƒ½ç³»ç»Ÿå¢å¼ºäººç±»è¯„ä¼°æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„é€”å¾„ï¼Œä½†å®ƒä»¬åœ¨è¯­éŸ³å’Œè¯­è¨€éšœç¢çš„æƒ…å¢ƒä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶ä¸ç¡®å®šã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å‰æ²¿çš„ASRåŸºç¡€æ¨¡å‹Whisperï¼Œæ˜¯å¦å¯ä»¥åº”ç”¨äºè½¬å½•å’Œåˆ†æä¸­é£æ‚£è€…åœ¨å¸¸ç”¨çš„å›¾ç‰‡å‘½åä»»åŠ¡ä¸­çš„è¯­éŸ³ã€‚æˆ‘ä»¬è¯„ä¼°äº†é€å­—è½¬å½•çš„å‡†ç¡®æ€§å’Œæ¨¡å‹æ”¯æŒè¯­è¨€åŠŸèƒ½ä¸‹æ¸¸é¢„æµ‹çš„èƒ½åŠ›ï¼Œè¿™å¯¹ä¸­é£åçš„ç»“æœå…·æœ‰é‡å¤§æ„ä¹‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºçº¿Whisperæ¨¡å‹åœ¨å•ä¸ªå•è¯çš„è¯­éŸ³å‘éŸ³ä¸Šè¡¨ç°ä¸ä½³ã€‚ç„¶è€Œï¼Œå¯¹Whisperè¿›è¡Œå¾®è°ƒæ˜¾è‘—æé«˜äº†è½¬å½•å‡†ç¡®æ€§ï¼ˆå°†å¥åº·è¯­éŸ³çš„å•è¯é”™è¯¯ç‡é™ä½äº†87.72%ï¼Œæ‚£è€…è¯­éŸ³é™ä½äº†71.22%ï¼‰ã€‚æ­¤å¤–ï¼Œä»æ¨¡å‹ä¸­å­¦ä¹ çš„è¡¨ç¤ºèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹è¯­éŸ³è´¨é‡ï¼ˆå¥åº·è¯­éŸ³çš„å¹³å‡F1 Macroä¸º0.74ï¼Œæ‚£è€…è¯­éŸ³ä¸º0.75ï¼‰ã€‚ç„¶è€Œï¼Œåœ¨æœªè§è¿‡çš„ï¼ˆTORGOï¼‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºå‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™çªæ˜¾äº†Whisperæ— æ³•å¯¹è¶…å‡ºé¢†åŸŸçš„ä¸´åºŠè¯­éŸ³è¿›è¡Œé›¶æ ·æœ¬è½¬å½•å•ä¸ªå•è¯å‘éŸ³çš„èƒ½åŠ›ä¸è¶³ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦å¯¹ç‰¹å®šä¸´åºŠäººç¾¤é€‚åº”æ¨¡å‹çš„éœ€æ±‚ã€‚è™½ç„¶è·¨åŸŸæ³›åŒ–ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†è¿™äº›å‘ç°çªå‡ºäº†åŸºç¡€æ¨¡å‹åœ¨é€‚å½“å¾®è°ƒåçš„æ½œåŠ›ï¼Œå¯ä»¥ä¿ƒè¿›ä¸­é£ç›¸å…³éšœç¢çš„è‡ªåŠ¨åŒ–è¯­è¨€å’Œè¯­éŸ³è¯„ä¼°å’Œåº·å¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17326v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åŸºç¡€æ¨¡å‹åœ¨è„‘å’ä¸­æ‚£è€…è¯­è¨€éšœç¢è¯„ä¼°ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶åŸºç¡€æ¨¡å‹åœ¨å•è¯å‘éŸ³ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œä½†é€šè¿‡å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜è½¬å½•å‡†ç¡®æ€§ï¼Œå¹¶æ”¯æŒé¢„æµ‹æ‚£è€…çš„è¯­è¨€åŠŸèƒ½ã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾ç¤ºå…¶æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œå¼ºè°ƒéœ€è¦é’ˆå¯¹ç‰¹å®šä¸´åºŠäººç¾¤è¿›è¡Œæ¨¡å‹é€‚åº”ã€‚è¿™å‡¸æ˜¾äº†é€‚å½“å¾®è°ƒåŸºç¡€æ¨¡å‹åœ¨æ¨åŠ¨è„‘å’ä¸­ç›¸å…³éšœç¢çš„è‡ªåŠ¨åŒ–è¯­è¨€å’Œè¯­éŸ³è¯„ä¼°åŠåº·å¤ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRåŸºç¡€æ¨¡å‹åœ¨è„‘å’ä¸­è¯­è¨€éšœç¢è¯„ä¼°ä¸­å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>åŸºç¡€æ¨¡å‹åœ¨å•è¯å‘éŸ³ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œä½†å¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜è½¬å½•å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹æ”¯æŒé¢„æµ‹æ‚£è€…çš„è¯­è¨€åŠŸèƒ½ï¼Œå¯¹è„‘å’ä¸­åç»“æœæœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šè¡¨ç°æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéœ€è¦é’ˆå¯¹ç‰¹å®šä¸´åºŠäººç¾¤è¿›è¡Œé€‚åº”ã€‚</li>
<li>æŒ‘æˆ˜ä»ç„¶å­˜åœ¨è·¨åŸŸæ³›åŒ–é—®é¢˜ã€‚</li>
<li>é€‚å½“å¾®è°ƒåŸºç¡€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–è¯­è¨€å’Œè¯­éŸ³è¯„ä¼°åŠåº·å¤ä¸­æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3cd49ec10bcf1264d69eefeac787fc5f" align="middle">
<img src="https://picx.zhimg.com/v2-3c29273c8044122997a788247d595c06" align="middle">
<img src="https://picx.zhimg.com/v2-cd5e1dcbbb2df250b24ce8692903c6bd" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FastLongSpeech-Enhancing-Large-Speech-Language-Models-for-Efficient-Long-Speech-Processing"><a href="#FastLongSpeech-Enhancing-Large-Speech-Language-Models-for-Efficient-Long-Speech-Processing" class="headerlink" title="FastLongSpeech: Enhancing Large Speech-Language Models for Efficient   Long-Speech Processing"></a>FastLongSpeech: Enhancing Large Speech-Language Models for Efficient   Long-Speech Processing</h2><p><strong>Authors:Shoutao Guo, Shaolei Zhang, Qingkai Fang, Zhengrui Ma, Min Zhang, Yang Feng</strong></p>
<p>The rapid advancement of Large Language Models (LLMs) has spurred significant progress in Large Speech-Language Models (LSLMs), enhancing their capabilities in both speech understanding and generation. While existing LSLMs often concentrate on augmenting speech generation or tackling a diverse array of short-speech tasks, the efficient processing of long-form speech remains a critical yet underexplored challenge. This gap is primarily attributed to the scarcity of long-speech training datasets and the high computational costs associated with long sequences. To address these limitations, we introduce FastLongSpeech, a novel framework designed to extend LSLM capabilities for efficient long-speech processing without necessitating dedicated long-speech training data. FastLongSpeech incorporates an iterative fusion strategy that can compress excessively long-speech sequences into manageable lengths. To adapt LSLMs for long-speech inputs, it introduces a dynamic compression training approach, which exposes the model to short-speech sequences at varying compression ratios, thereby transferring the capabilities of LSLMs to long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop a long-speech understanding benchmark called LongSpeech-Eval. Experiments show that our method exhibits strong performance in both long-speech and short-speech tasks, while greatly improving inference efficiency. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆLSLMï¼‰ä¹Ÿå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œå…¶åœ¨è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†å¢å¼ºã€‚å°½ç®¡ç°æœ‰çš„LSLMé€šå¸¸ä¸“æ³¨äºå¢å¼ºè¯­éŸ³ç”Ÿæˆæˆ–åº”å¯¹å„ç§çŸ­æœŸè¯­éŸ³ä»»åŠ¡ï¼Œä½†é•¿è¯­éŸ³çš„é«˜æ•ˆå¤„ç†ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®ä¸”å°šæœªå……åˆ†æ¢ç´¢çš„æŒ‘æˆ˜ã€‚è¿™ä¸€å·®è·ä¸»è¦å½’å› äºé•¿è¯­éŸ³è®­ç»ƒæ•°æ®é›†çš„ç¨€ç¼ºä»¥åŠä¸é•¿åºåˆ—ç›¸å…³çš„é«˜è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†FastLongSpeechï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æ‰©å±•LSLMçš„åŠŸèƒ½ï¼Œä»¥é«˜æ•ˆåœ°è¿›è¡Œé•¿è¯­éŸ³å¤„ç†ï¼Œè€Œæ— éœ€ä¸“ç”¨çš„é•¿è¯­éŸ³è®­ç»ƒæ•°æ®ã€‚FastLongSpeeché‡‡ç”¨äº†ä¸€ç§è¿­ä»£èåˆç­–ç•¥ï¼Œå¯ä»¥å°†è¿‡é•¿çš„è¯­éŸ³åºåˆ—å‹ç¼©æˆå¯ç®¡ç†çš„é•¿åº¦ã€‚ä¸ºäº†é€‚åº”LSLMçš„é•¿è¯­éŸ³è¾“å…¥ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§åŠ¨æ€å‹ç¼©è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹æš´éœ²åœ¨ä¸åŒå‹ç¼©æ¯”ç‡ä¸‹çš„çŸ­è¯­éŸ³åºåˆ—ä¸­ï¼Œä»è€Œå°†LSLMçš„èƒ½åŠ›è½¬ç§»åˆ°é•¿è¯­éŸ³ä»»åŠ¡ä¸Šã€‚ä¸ºäº†è¯„ä¼°LSLMçš„é•¿è¯­éŸ³èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºLongSpeech-Evalçš„é•¿è¯­éŸ³ç†è§£åŸºå‡†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é•¿çŸ­è¯­éŸ³ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§æé«˜äº†æ¨ç†æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14815v2">PDF</a> NeurIPS 2025. The code is at   <a target="_blank" rel="noopener" href="https://github.com/ictnlp/FastLongSpeech">https://github.com/ictnlp/FastLongSpeech</a>. This model is at   <a target="_blank" rel="noopener" href="https://huggingface.co/ICTNLP/FastLongSpeech">https://huggingface.co/ICTNLP/FastLongSpeech</a>. The dataset is at   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval">https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹çš„æ˜¾è‘—è¿›æ­¥ï¼Œæé«˜äº†å…¶åœ¨è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å¤„ç†é•¿è¯­éŸ³æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ç›¸åº”çš„è®­ç»ƒæ•°æ®é›†ä»¥åŠå¤„ç†é•¿åºåˆ—çš„é«˜è®¡ç®—æˆæœ¬æ˜¯ä¸»è¦éš¾é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FastLongSpeechæ¡†æ¶ï¼Œæ—¨åœ¨æ‰©å±•å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå®ç°é«˜æ•ˆçš„é•¿è¯­éŸ³å¤„ç†ï¼Œè€Œæ— éœ€ä¸“é—¨çš„é•¿è¯­éŸ³è®­ç»ƒæ•°æ®ã€‚FastLongSpeeché‡‡ç”¨è¿­ä»£èåˆç­–ç•¥ï¼Œå°†è¿‡é•¿çš„è¯­éŸ³åºåˆ—å‹ç¼©æˆå¯ç®¡ç†çš„é•¿åº¦ã€‚ä¸ºé€‚åº”é•¿è¯­éŸ³è¾“å…¥ï¼Œå®ƒé‡‡ç”¨åŠ¨æ€å‹ç¼©è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æš´éœ²æ¨¡å‹äºä¸åŒå‹ç¼©æ¯”ä¾‹ä¸‹çš„çŸ­è¯­éŸ³åºåˆ—ï¼Œå°†å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è½¬ç§»åˆ°é•¿è¯­éŸ³ä»»åŠ¡ä¸Šã€‚ä¸ºè¯„ä¼°å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹åœ¨é•¿è¯­éŸ³ä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºLongSpeech-Evalçš„é•¿è¯­éŸ³ç†è§£åŸºå‡†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é•¿çŸ­è¯­éŸ³ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§æé«˜äº†æ¨ç†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†è¯­éŸ³è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨å¤„ç†é•¿è¯­éŸ³æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹è®­ç»ƒæ•°æ®é›†å’Œé«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>FastLongSpeechæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°é«˜æ•ˆé•¿è¯­éŸ³å¤„ç†ã€‚</li>
<li>FastLongSpeeché‡‡ç”¨è¿­ä»£èåˆç­–ç•¥å’ŒåŠ¨æ€å‹ç¼©è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>è¿­ä»£èåˆç­–ç•¥å¯å°†é•¿è¯­éŸ³åºåˆ—å‹ç¼©æˆå¯ç®¡ç†é•¿åº¦ã€‚</li>
<li>åŠ¨æ€å‹ç¼©è®­ç»ƒæ–¹æ³•é€šè¿‡æš´éœ²æ¨¡å‹äºä¸åŒå‹ç¼©æ¯”ä¾‹çš„çŸ­è¯­éŸ³åºåˆ—ï¼Œæå‡æ¨¡å‹å¤„ç†é•¿è¯­éŸ³çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-036419147c56beb567d57f3b725ed1dd" align="middle">
<img src="https://picx.zhimg.com/v2-dfd5194d6ab666769a0acfd61e25ee3b" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-efb8fac75d807eaf0f619a9101b68e1d" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d0f90bbd3658233a51e726c7ddddd87b" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Flip Learning Weakly Supervised Erase to Segment Nodules in Breast   Ultrasound
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
