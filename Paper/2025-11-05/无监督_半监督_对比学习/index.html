<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
    <meta name="description" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  C-LEAD Contrastive Learning for Enhanced Adversarial Defense">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-f3d84d5c6fc318e0b2c1b4da98317fbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293223&auth_key=1762293223-0-0-b4ae185d60664d53a11a692fa5fffa73&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    59 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="C-LEAD-Contrastive-Learning-for-Enhanced-Adversarial-Defense"><a href="#C-LEAD-Contrastive-Learning-for-Enhanced-Adversarial-Defense" class="headerlink" title="C-LEAD: Contrastive Learning for Enhanced Adversarial Defense"></a>C-LEAD: Contrastive Learning for Enhanced Adversarial Defense</h2><p><strong>Authors:Suklav Ghosh, Sonal Kumar, Arijit Sur</strong></p>
<p>Deep neural networks (DNNs) have achieved remarkable success in computer vision tasks such as image classification, segmentation, and object detection. However, they are vulnerable to adversarial attacks, which can cause incorrect predictions with small perturbations in input images. Addressing this issue is crucial for deploying robust deep-learning systems. This paper presents a novel approach that utilizes contrastive learning for adversarial defense, a previously unexplored area. Our method leverages the contrastive loss function to enhance the robustness of classification models by training them with both clean and adversarially perturbed images. By optimizing the modelâ€™s parameters alongside the perturbations, our approach enables the network to learn robust representations that are less susceptible to adversarial attacks. Experimental results show significant improvements in the modelâ€™s robustness against various types of adversarial perturbations. This suggests that contrastive loss helps extract more informative and resilient features, contributing to the field of adversarial robustness in deep learning. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å¨èƒï¼Œè¿™äº›æ”»å‡»å¯ä»¥é€šè¿‡è¾“å…¥å›¾åƒä¸­çš„å¾®å°æ‰°åŠ¨å¯¼è‡´é¢„æµ‹é”™è¯¯ã€‚è§£å†³è¿™ä¸€é—®é¢˜å¯¹äºéƒ¨ç½²ç¨³å¥çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œå¯¹æŠ—é˜²å¾¡çš„æ–°æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å‰æœªè¢«æ¢ç´¢çš„é¢†åŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°ï¼Œé€šè¿‡ç”¨å¹²å‡€å’Œå¯¹æŠ—æ€§æ‰°åŠ¨å›¾åƒè®­ç»ƒåˆ†ç±»æ¨¡å‹ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹å‚æ•°å’Œæ‰°åŠ¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ ä¸æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„ç¨³å®šè¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹å¯¹å„ç§ç±»å‹çš„å¯¹æŠ—æ€§æ‰°åŠ¨å…·æœ‰æ˜¾è‘—çš„ç¨³å¥æ€§æé«˜ã€‚è¿™è¡¨æ˜å¯¹æ¯”æŸå¤±æœ‰åŠ©äºæå–æ›´å…·ä¿¡æ¯é‡å’Œå¼¹æ€§çš„ç‰¹å¾ï¼Œä¸ºæ·±åº¦å­¦ä¹ é¢†åŸŸçš„å¯¹æŠ—ç¨³å¥æ€§åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27249v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¯¹æ¯”å­¦ä¹ å¢å¼ºæ¨¡å‹é²æ£’æ€§çš„æ–°æ–¹æ³•ï¼Œå¯¹æŠ—å‡»å¯¹æŠ—æ€§æ”»å‡»è¿™ä¸€å…³é”®é—®é¢˜ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹ä»¥åŒæ—¶å¤„ç†å¹²å‡€å’Œå¯¹æŠ—æ€§æ‰°åŠ¨å›¾åƒï¼Œå¹¶åˆ©ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œç½‘ç»œèƒ½å­¦ä¹ æ›´å…·é²æ£’æ€§çš„è¡¨ç¤ºï¼Œä»è€Œå‡å°‘å—æ”»å‡»çš„é£é™©ã€‚å®éªŒè¯æ˜ï¼Œå¯¹æ¯”æŸå¤±æœ‰åŠ©äºæé«˜æ¨¡å‹å¯¹å„ç§å¯¹æŠ—æ€§æ‰°åŠ¨çš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å­¦ä¹ ç”¨äºå¢å¼ºæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„é²æ£’æ€§ï¼Œå¯¹æŠ—å¯¹æŠ—æ€§æ”»å‡»é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åŸºäºå¯¹æ¯”æŸå¤±å‡½æ•°è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶å¤„ç†å¹²å‡€å’Œå¯¹æŠ—æ‰°åŠ¨å›¾åƒã€‚</li>
<li>æ¨¡å‹é€šè¿‡ä¼˜åŒ–å‚æ•°ä»¥åº”å¯¹æ‰°åŠ¨ï¼Œå­¦ä¹ æ›´é²æ£’çš„è¡¨ç¤ºã€‚</li>
<li>å¯¹æ¯”æŸå¤±æœ‰åŠ©äºæå–æ›´å…·ä¿¡æ¯é‡å’ŒæŠ—æ”»å‡»æ€§çš„ç‰¹å¾ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—æé«˜æ¨¡å‹å¯¹å„ç§å¯¹æŠ—æ€§æ‰°åŠ¨çš„é²æ£’æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ·±åº¦å­¦ä¹ ä¸­çš„å¯¹æŠ—æ€§ç¨³å¥æ€§é—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1263201491aa51235bd3b9d6d87d9424~resize:0:q75.jpg?source=1f5c5e47&expiration=1762292990&auth_key=1762292990-0-0-cda35104bc880a1fb64a17acdfd5eb9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f2ea01a280f9e3f024eb19974dbaae54~resize:0:q75.jpg?source=1f5c5e47&expiration=1762292998&auth_key=1762292998-0-0-814cf11b22d69e11927318af003cc042&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fed85f645a4ab3ecfb511088ca62223e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293004&auth_key=1762293004-0-0-232282ab2218694b4059b05af1667edf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9334ddef5053b41982566b6a6dd26a03~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293011&auth_key=1762293011-0-0-8a67d557a91819e7346909c9431ff0d3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Comparative-Study-of-UNet-based-Architectures-for-Liver-Tumor-Segmentation-in-Multi-Phase-Contrast-Enhanced-Computed-Tomography"><a href="#Comparative-Study-of-UNet-based-Architectures-for-Liver-Tumor-Segmentation-in-Multi-Phase-Contrast-Enhanced-Computed-Tomography" class="headerlink" title="Comparative Study of UNet-based Architectures for Liver Tumor   Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography"></a>Comparative Study of UNet-based Architectures for Liver Tumor   Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography</h2><p><strong>Authors:Doan-Van-Anh Ly, Thi-Thu-Hien Pham, Thanh-Hai Le</strong></p>
<p>Segmentation of liver structures in multi-phase contrast-enhanced computed tomography (CECT) plays a crucial role in computer-aided diagnosis and treatment planning for liver diseases, including tumor detection. In this study, we investigate the performance of UNet-based architectures for liver tumor segmentation, starting from the original UNet and extending to UNet3+ with various backbone networks. We evaluate ResNet, Transformer-based, and State-space (Mamba) backbones, all initialized with pretrained weights. Surprisingly, despite the advances in modern architecture, ResNet-based models consistently outperform Transformer- and Mamba-based alternatives across multiple evaluation metrics. To further improve segmentation quality, we introduce attention mechanisms into the backbone and observe that incorporating the Convolutional Block Attention Module (CBAM) yields the best performance. ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a Dice score of 0.755 and IoU of 0.662, but also achieved the most precise boundary delineation, evidenced by the lowest HD95 distance of 77.911. The modelâ€™s superiority was further cemented by its leading overall accuracy of 0.925 and specificity of 0.926, showcasing its robust capability in accurately identifying both lesion and healthy tissue. To further enhance interpretability, Grad-CAM visualizations were employed to highlight the regionâ€™s most influential predictions, providing insights into its decision-making process. These findings demonstrate that classical ResNet architecture, when combined with modern attention modules, remain highly competitive for medical image segmentation tasks, offering a promising direction for liver tumor detection in clinical practice. </p>
<blockquote>
<p>åœ¨å¤šæœŸç›¸å¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCECTï¼‰ä¸­ï¼Œè‚è„ç»“æ„çš„åˆ†å‰²å¯¹è‚è„ç–¾ç—…çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ï¼ŒåŒ…æ‹¬è‚¿ç˜¤æ£€æµ‹ï¼Œèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†åŸºäºUNetæ¶æ„çš„è‚è„è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ï¼Œä»åŸå§‹UNetæ‰©å±•åˆ°å¸¦æœ‰å„ç§éª¨å¹²ç½‘ç»œçš„UNet3+ã€‚æˆ‘ä»¬è¯„ä¼°äº†ResNetã€åŸºäºTransformerå’ŒState-spaceï¼ˆMambaï¼‰éª¨å¹²ç½‘ï¼Œæ‰€æœ‰ç½‘ç»œéƒ½ä½¿ç”¨é¢„è®­ç»ƒæƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå°½ç®¡ç°ä»£æ¶æ„å–å¾—äº†è¿›å±•ï¼Œä½†åŸºäºResNetçš„æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºäºTransformerå’ŒMambaçš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºè¿›ä¸€æ­¥æé«˜åˆ†å‰²è´¨é‡ï¼Œæˆ‘ä»¬åœ¨éª¨å¹²ä¸­å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶è§‚å¯Ÿåˆ°åŠ å…¥å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰ä¼šå–å¾—æœ€ä½³æ€§èƒ½ã€‚å¸¦æœ‰CBAMæ¨¡å—çš„ResNetUNet3+ä¸ä»…ä»¥Diceåˆ†æ•°0.755å’ŒIoUåˆ†æ•°0.662çš„æœ€ä½³é‡å åº¦æŒ‡æ ‡äº§å‡ºç»“æœï¼Œè€Œä¸”å®ç°äº†æœ€ç²¾ç¡®çš„è¾¹ç•Œæç»˜ï¼Œä»¥æœ€ä½çš„HD95è·ç¦»77.911ä¸ºè¯æ®ã€‚è¯¥æ¨¡å‹çš„ä¼˜è¶Šæ€§è¿˜ä½“ç°åœ¨å…¶æ•´ä½“å‡†ç¡®åº¦0.925å’Œç‰¹å¼‚æ€§0.926ä¸Šçš„é¢†å…ˆåœ°ä½ï¼Œå±•ç¤ºäº†å…¶å‡†ç¡®è¯†åˆ«ç—…å˜ç»„ç»‡å’Œå¥åº·ç»„ç»‡çš„ç¨³å¥èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå¯è§£é‡Šæ€§ï¼Œé‡‡ç”¨äº†Grad-CAMå¯è§†åŒ–æ¥çªå‡ºæ˜¾ç¤ºå¯¹é¢„æµ‹æœ€å…·æœ‰å½±å“åŠ›çš„åŒºåŸŸï¼Œä»è€Œæ·±å…¥äº†è§£å…¶å†³ç­–è¿‡ç¨‹ã€‚è¿™äº›ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»å…¸çš„ResNetæ¶æ„ä¸ç°ä»£æ³¨æ„åŠ›æ¨¡å—ç›¸ç»“åˆï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä»å…·æœ‰ç«äº‰åŠ›ï¼Œä¸ºä¸´åºŠå®è·µä¸­çš„è‚è„è‚¿ç˜¤æ£€æµ‹æä¾›äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25522v1">PDF</a> 27 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºUNetæ¶æ„çš„è‚è„è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ï¼Œä»åŸå§‹UNetåˆ°UNet3+çš„ä¸åŒç‰ˆæœ¬ï¼Œå¹¶é‡‡ç”¨äº†ResNetã€åŸºäºTransformerçš„Mambaç­‰ä¸åŒçš„éª¨å¹²ç½‘ç»œã€‚ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ç°ä»£æ¶æ„æœ‰æ‰€è¿›æ­¥ï¼Œä½†åŸºäºResNetçš„æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºåŸºäºTransformerå’ŒMambaçš„æ¨¡å‹ã€‚å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶åï¼Œä½¿ç”¨å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰çš„ResNetUNet3+ä¸ä»…è·å¾—äº†æœ€ä½³çš„Diceç³»æ•°å’ŒIoUå€¼ï¼Œè€Œä¸”å®ç°äº†æœ€ç²¾ç¡®çš„è¾¹ç•Œæç»˜ï¼Œè¯æ˜äº†å…¶åœ¨è‚è„è‚¿ç˜¤æ£€æµ‹ä¸­çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UNet-basedæ¶æ„åœ¨è‚è„è‚¿ç˜¤åˆ†å‰²ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>ç ”ç©¶äº†ä»UNetåˆ°UNet3+çš„ä¸åŒç‰ˆæœ¬ï¼Œå¹¶é‡‡ç”¨äº†å¤šç§éª¨å¹²ç½‘ç»œã€‚</li>
<li>åŸºäºResNetçš„æ¨¡å‹åœ¨è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ï¼Œè¡¨æ˜å…¶åœ¨è‚è„è‚¿ç˜¤æ£€æµ‹ä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶åï¼Œä½¿ç”¨CBAMçš„ResNetUNet3+è·å¾—æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†æœ€ä½³çš„Diceç³»æ•°å’ŒIoUå€¼ï¼Œä»¥åŠæœ€ç²¾ç¡®çš„è¾¹ç•Œæç»˜ã€‚</li>
<li>Grad-CAMå¯è§†åŒ–æŠ€æœ¯ç”¨äºå¢å¼ºæ¨¡å‹çš„è§£é‡Šæ€§ï¼Œæ­ç¤ºäº†é¢„æµ‹çš„æœ€å…³é”®åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-61181777eace2dd23fc7039458f6199e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293018&auth_key=1762293018-0-0-dbf9767f241c85bc53d5629e1c799841&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2add1e86abf51994ef6852dfabfeefdc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293025&auth_key=1762293025-0-0-2c30c49c3794576725a8628ac6910113&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="An-Enhanced-Dual-Transformer-Contrastive-Network-for-Multimodal-Sentiment-Analysis"><a href="#An-Enhanced-Dual-Transformer-Contrastive-Network-for-Multimodal-Sentiment-Analysis" class="headerlink" title="An Enhanced Dual Transformer Contrastive Network for Multimodal   Sentiment Analysis"></a>An Enhanced Dual Transformer Contrastive Network for Multimodal   Sentiment Analysis</h2><p><strong>Authors:Phuong Q. Dao, Mark Roantree, Vuong M. Ngo</strong></p>
<p>Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by jointly analyzing data from multiple modalities typically text and images offering a richer and more accurate interpretation than unimodal approaches. In this paper, we first propose BERT-ViT-EF, a novel model that combines powerful Transformer-based encoders BERT for textual input and ViT for visual input through an early fusion strategy. This approach facilitates deeper cross-modal interactions and more effective joint representation learning. To further enhance the modelâ€™s capability, we propose an extension called the Dual Transformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN incorporates an additional Transformer encoder layer after BERT to refine textual context (before fusion) and employs contrastive learning to align text and image representations, fostering robust multimodal feature learning. Empirical results on two widely used MSA benchmarks MVSA-Single and TumEmo demonstrate the effectiveness of our approach. DTCN achieves best accuracy (78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on MVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements highlight the benefits of early fusion and deeper contextual modeling in Transformer-based multimodal sentiment analysis. </p>
<blockquote>
<p>å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆMSAï¼‰æ—¨åœ¨é€šè¿‡è”åˆåˆ†ææ–‡æœ¬å’Œå›¾åƒç­‰å¤šç§æ¨¡æ€çš„æ•°æ®æ¥ç†è§£äººç±»æƒ…æ„Ÿï¼Œç›¸æ¯”äºå•æ¨¡æ€æ–¹æ³•ï¼Œå®ƒæä¾›äº†æ›´ä¸°å¯Œå’Œæ›´å‡†ç¡®çš„è§£é‡Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºBERT-ViT-EFæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå¼ºå¤§Transformerç¼–ç å™¨BERTï¼ˆç”¨äºæ–‡æœ¬è¾“å…¥ï¼‰å’ŒViTï¼ˆç”¨äºè§†è§‰è¾“å…¥ï¼‰çš„æ–°æ¨¡å‹ï¼Œé€šè¿‡æ—©æœŸèåˆç­–ç•¥å®ç°è·¨æ¨¡æ€æ·±åº¦äº¤äº’å’Œè”åˆè¡¨ç¤ºå­¦ä¹ ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºDual Transformer Contrastive Networkï¼ˆDTCNï¼‰çš„æ‰©å±•æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºBERT-ViT-EFæ„å»ºã€‚DTCNåœ¨BERTä¹‹åå¢åŠ äº†ä¸€å±‚é¢å¤–çš„Transformerç¼–ç å™¨å±‚æ¥ä¼˜åŒ–æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼ˆåœ¨èåˆä¹‹å‰ï¼‰ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ¥å¯¹é½æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºï¼Œä»è€Œä¿ƒè¿›é²æ£’çš„å¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ ã€‚åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„MSAåŸºå‡†æµ‹è¯•MVSA-Singleå’ŒTumEmoä¸Šçš„å®è¯ç»“æœè¡¨æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚DTCNåœ¨TumEmoä¸Šå–å¾—äº†æœ€ä½³å‡†ç¡®ç‡ï¼ˆ78.4%ï¼‰å’ŒF1åˆ†æ•°ï¼ˆ78.3%ï¼‰ï¼Œå¹¶åœ¨MVSA-Singleä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œè¾¾åˆ°76.6%çš„å‡†ç¡®ç‡å’Œ75.9%çš„F1åˆ†æ•°ã€‚è¿™äº›æ”¹è¿›çªæ˜¾äº†æ—©æœŸèåˆå’Œæ·±åº¦ä¸Šä¸‹æ–‡å»ºæ¨¡åœ¨åŸºäºTransformerçš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23617v1">PDF</a> The paper has been accepted for presentation at the MEDES 2025   conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºæ—©æœŸèåˆç­–ç•¥çš„BERT-ViT-EFæ¨¡å‹ï¼Œç”¨äºå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆMSAï¼‰ã€‚è¯¥æ¨¡å‹ç»“åˆäº†BERTå’ŒViTä¸¤ç§å¼ºå¤§çš„Transformerç¼–ç å™¨ï¼Œåˆ†åˆ«å¤„ç†æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ã€‚ä¸ºæé«˜æ¨¡å‹æ€§èƒ½ï¼Œè¿›ä¸€æ­¥æå‡ºäº†åŸºäºBERT-ViT-EFçš„æ‰©å±•æ¨¡å‹â€”â€”Dual Transformer Contrastive Network (DTCN)ã€‚DTCNåœ¨èåˆå‰å¢åŠ äº†ä¸€å±‚Transformerç¼–ç å™¨ä»¥ä¼˜åŒ–æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ å¯¹é½æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºï¼Œå®ç°äº†ç¨³å¥çš„å¤šæ¨¡æ€ç‰¹å¾å­¦ä¹ ã€‚åœ¨ä¸¤å¤§MSAåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æï¼ˆMSAï¼‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡è”åˆåˆ†ææ–‡æœ¬å’Œå›¾åƒæ•°æ®ï¼Œæä¾›æ›´ä¸°å¯Œå’Œå‡†ç¡®çš„æƒ…æ„Ÿè§£è¯»ã€‚</li>
<li>æå‡ºäº†BERT-ViT-EFæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨æ—©æœŸèåˆç­–ç•¥ï¼Œç»“åˆBERTå’ŒViTä¸¤ç§Transformerç¼–ç å™¨å¤„ç†æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ã€‚</li>
<li>BERT-ViT-EFæ¨¡å‹çš„æ‰©å±•ç‰ˆæœ¬DTCNè¢«ä»‹ç»ï¼Œå®ƒåœ¨èåˆå‰å¢åŠ äº†ä¸€å±‚Transformerç¼–ç å™¨ä¼˜åŒ–æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œå¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ å¯¹é½æ–‡æœ¬å’Œå›¾åƒè¡¨ç¤ºã€‚</li>
<li>DTCNåœ¨ä¸¤å¤§MSAåŸºå‡†æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¯æ˜äº†æ—©æœŸèåˆå’Œæ·±å±‚ä¸Šä¸‹æ–‡å»ºæ¨¡åœ¨TransformeråŸºå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æä¸­çš„ä¼˜åŠ¿ã€‚</li>
<li>DTCNåœ¨TumEmoæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡å’ŒF1åˆ†æ•°å‡è¾¾åˆ°æœ€ä½³ï¼ˆåˆ†åˆ«ä¸º78.4%å’Œ78.3%ï¼‰ã€‚</li>
<li>åœ¨MVSA-Singleæ•°æ®é›†ä¸Šï¼ŒDTCNè¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º76.6%å’Œ75.9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a1ad06489f0bb583f869d1be0b57f296~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293032&auth_key=1762293032-0-0-7493f96ed39bb9fda607e132a2d1420e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ffc4dcfb99eaef291b327767bef92a8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293039&auth_key=1762293039-0-0-629dc51790150d5342d2fbb6db4723fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7774acd2d4af8b20f3edf719ab110a73~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293045&auth_key=1762293045-0-0-8b6ba8b9c9fcad0de4d143b8ef774e86&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bi-Encoder-Contrastive-Learning-for-Fingerprint-and-Iris-Biometrics"><a href="#Bi-Encoder-Contrastive-Learning-for-Fingerprint-and-Iris-Biometrics" class="headerlink" title="Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics"></a>Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics</h2><p><strong>Authors:Matthew So, Judah Goldfeder, Mark Lis, Hod Lipson</strong></p>
<p>There has been a historic assumption that the biometrics of an individual are statistically uncorrelated. We test this assumption by training Bi-Encoder networks on three verification tasks, including fingerprint-to-fingerprint matching, iris-to-iris matching, and cross-modal fingerprint-to-iris matching using 274 subjects with $\sim$100k fingerprints and 7k iris images. We trained ResNet-50 and Vision Transformer backbones in Bi-Encoder architectures such that the contrastive loss between images sampled from the same individual is minimized. The iris ResNet architecture reaches 91 ROC AUC score for iris-to-iris matching, providing clear evidence that the left and right irises of an individual are correlated. Fingerprint models reproduce the positive intra-subject suggested by prior work in this space. This is the first work attempting to use Vision Transformers for this matching. Cross-modal matching rises only slightly above chance, which suggests that more data and a more sophisticated pipeline is needed to obtain compelling results. These findings continue challenge independence assumptions of biometrics and we plan to extend this work to other biometrics in the future. Code available: <a target="_blank" rel="noopener" href="https://github.com/MatthewSo/bio_fingerprints_iris">https://github.com/MatthewSo/bio_fingerprints_iris</a>. </p>
<blockquote>
<p>å­˜åœ¨ä¸€ç§å†å²æ‚ ä¹…çš„å‡è®¾ï¼Œå³ä¸ªä½“çš„ç”Ÿç‰©è¯†åˆ«ç‰¹å¾åœ¨ç»Ÿè®¡å­¦ä¸Šæ˜¯æ— å…³çš„ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒåŒç¼–ç å™¨ç½‘ç»œè¿›è¡Œä¸‰é¡¹éªŒè¯ä»»åŠ¡æ¥æµ‹è¯•è¿™ä¸€å‡è®¾ï¼ŒåŒ…æ‹¬æŒ‡çº¹ä¸æŒ‡çº¹åŒ¹é…ã€è™¹è†œä¸è™¹è†œåŒ¹é…ä»¥åŠè·¨æ¨¡æ€æŒ‡çº¹ä¸è™¹è†œåŒ¹é…ï¼Œä½¿ç”¨274åå—è¯•è€…ï¼Œçº¦10ä¸‡æšæŒ‡çº¹å’Œ7åƒå¼ è™¹è†œå›¾åƒã€‚æˆ‘ä»¬è®­ç»ƒäº†Bi-Encoderæ¶æ„ä¸­çš„ResNet-50å’ŒVision Transformeréª¨å¹²ç½‘ï¼Œä»¥ä½¿æ¥è‡ªåŒä¸€ä¸ªä½“çš„å›¾åƒæ ·æœ¬ä¹‹é—´çš„å¯¹æ¯”æŸå¤±æœ€å°åŒ–ã€‚è™¹è†œResNetæ¶æ„åœ¨è™¹è†œä¸è™¹è†œåŒ¹é…æ–¹é¢è¾¾åˆ°äº†91åˆ†çš„ROC AUCå¾—åˆ†ï¼Œè¿™æä¾›äº†æ˜ç¡®çš„è¯æ®è¡¨æ˜ä¸ªä½“çš„å·¦å³è™¹è†œæ˜¯ç›¸å…³çš„ã€‚æŒ‡çº¹æ¨¡å‹åœ¨æ­¤ç©ºé—´ä¸­å¤åˆ¶äº†å…ˆå‰å·¥ä½œçš„ç§¯æå†…éƒ¨ä¸»ä½“å»ºè®®ã€‚è¿™æ˜¯é¦–æ¬¡å°è¯•ä½¿ç”¨Vision Transformersè¿›è¡Œè¿™ç§åŒ¹é…çš„å·¥ä½œã€‚è·¨æ¨¡æ€åŒ¹é…ä»…ç•¥é«˜äºå¶ç„¶æ°´å¹³ï¼Œè¿™è¡¨æ˜éœ€è¦æ›´å¤šçš„æ•°æ®å’Œæ›´å¤æ‚çš„æµç¨‹æ‰èƒ½è·å¾—æœ‰è¯´æœåŠ›çš„ç»“æœã€‚è¿™äº›å‘ç°ç»§ç»­æŒ‘æˆ˜ç”Ÿç‰©è¯†åˆ«ç‰¹å¾çš„ç‹¬ç«‹æ€§å‡è®¾ï¼Œæˆ‘ä»¬è®¡åˆ’å°†æ¥å°†æ­¤å·¥ä½œæ‰©å±•è‡³å…¶ä»–ç”Ÿç‰©è¯†åˆ«ç‰¹å¾ã€‚ä»£ç å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/MatthewSo/bio_fingerprints_iris%E3%80%82">https://github.com/MatthewSo/bio_fingerprints_irisã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æµ‹è¯•äº†ä¸ªä½“ç”Ÿç‰©è¯†åˆ«ç‰¹å¾ç»Ÿè®¡æ— å…³çš„å‡è®¾ã€‚é€šè¿‡è®­ç»ƒåŒç¼–ç å™¨ç½‘ç»œè¿›è¡Œä¸‰é¡¹éªŒè¯ä»»åŠ¡ï¼ŒåŒ…æ‹¬æŒ‡çº¹å¯¹æ¯”æŒ‡çº¹ã€è™¹è†œå¯¹æ¯”è™¹è†œä»¥åŠè·¨æ¨¡æ€æŒ‡çº¹å¯¹æ¯”è™¹è†œåŒ¹é…ã€‚ä½¿ç”¨æ¥è‡ª274åä¸ªä½“çš„çº¦10ä¸‡æšæŒ‡çº¹å’Œ7åƒå¼ è™¹è†œå›¾åƒè¿›è¡Œè®­ç»ƒã€‚é‡‡ç”¨ResNet-50å’ŒVision Transformerä½œä¸ºåŒç¼–ç å™¨æ¶æ„çš„åŸºç¡€ï¼Œæœ€å°åŒ–åŒä¸€ä¸ªä½“é‡‡æ ·å›¾åƒçš„å¯¹æ¯”æŸå¤±ã€‚è™¹è†œResNetæ¶æ„åœ¨è™¹è†œå¯¹æ¯”è™¹è†œåŒ¹é…ä»»åŠ¡ä¸­è¾¾åˆ°91åˆ†çš„ROC AUCå¾—åˆ†ï¼Œæ˜ç¡®è¯æ˜äº†ä¸ªä½“çš„å·¦å³è™¹è†œä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚æŒ‡çº¹æ¨¡å‹é‡ç°äº†å…ˆå‰å·¥ä½œæå‡ºçš„ç§¯æä¸ªä½“å†…ç›¸å…³æ€§ã€‚è¿™æ˜¯é¦–æ¬¡å°è¯•ä½¿ç”¨Vision Transformerè¿›è¡Œæ­¤åŒ¹é…çš„å·¥ä½œã€‚è·¨æ¨¡æ€åŒ¹é…ç»“æœä»…ç•¥é«˜äºå¶ç„¶æ°´å¹³ï¼Œè¡¨æ˜éœ€è¦æ›´å¤šæ•°æ®å’Œæ›´å¤æ‚çš„æµç¨‹æ‰èƒ½è·å¾—æœ‰è¯´æœåŠ›çš„ç»“æœã€‚è¿™äº›å‘ç°ç»§ç»­æŒ‘æˆ˜ç”Ÿç‰©è¯†åˆ«ç‰¹å¾çš„ç‹¬ç«‹æ€§å‡è®¾ï¼Œå¹¶è®¡åˆ’å°†æ¥å°†æ­¤å·¥ä½œæ‰©å±•åˆ°å…¶ä»–ç”Ÿç‰©è¯†åˆ«é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æµ‹è¯•äº†ä¸ªä½“ç”Ÿç‰©è¯†åˆ«ç‰¹å¾ç»Ÿè®¡æ— å…³çš„å‡è®¾ï¼Œé€šè¿‡åŒç¼–ç å™¨ç½‘ç»œè¿›è¡Œä¸‰é¡¹éªŒè¯ä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨ResNet-50å’ŒVision Transformerä½œä¸ºåŒç¼–ç å™¨æ¶æ„åŸºç¡€ï¼Œæœ€å°åŒ–åŒä¸€äººä¸åŒç”Ÿç‰©è¯†åˆ«ç‰¹å¾é—´çš„å¯¹æ¯”æŸå¤±ã€‚</li>
<li>è™¹è†œå¯¹æ¯”è™¹è†œåŒ¹é…å®éªŒç»“æœæ˜¾ç¤ºå·¦å³è™¹è†œä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚</li>
<li>æŒ‡çº¹æ¨¡å‹é‡ç°äº†ä¸ªä½“å†…ç§¯æç›¸å…³æ€§ï¼Œä¸å…ˆå‰ç ”ç©¶ä¸€è‡´ã€‚</li>
<li>é¦–æ¬¡å°è¯•ä½¿ç”¨Vision Transformerè¿›è¡ŒåŒ¹é…ã€‚</li>
<li>è·¨æ¨¡æ€åŒ¹é…ç»“æœç•¥é«˜äºå¶ç„¶æ°´å¹³ï¼Œè¡¨æ˜éœ€æ›´å¤šæ•°æ®å’Œå¤æ‚æµç¨‹æ¥æå‡æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7ec5c0c8330555845ecdbefe6006e479~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293053&auth_key=1762293053-0-0-0444d02c1a3c5cda8449f36e1171c4ee&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-84259a82cc2c02b3eb0122d03d6c3354~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293060&auth_key=1762293060-0-0-a30b12a706cb1397325509950b49a0e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e0ff4a3bc2f0d15abc1307699d1bbd7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293066&auth_key=1762293066-0-0-5303c6e183476d07f2856278930f8d4b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-83b5ef29d6da10042d81ffda305fbae3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293073&auth_key=1762293073-0-0-485d66ce85700e254f75a7f4377e6194&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AutoSciDACT-Automated-Scientific-Discovery-through-Contrastive-Embedding-and-Hypothesis-Testing"><a href="#AutoSciDACT-Automated-Scientific-Discovery-through-Contrastive-Embedding-and-Hypothesis-Testing" class="headerlink" title="AutoSciDACT: Automated Scientific Discovery through Contrastive   Embedding and Hypothesis Testing"></a>AutoSciDACT: Automated Scientific Discovery through Contrastive   Embedding and Hypothesis Testing</h2><p><strong>Authors:Samuel Bright-Thonney, Christina Reissel, Gaia Grosso, Nathaniel Woodward, Katya Govorkova, Andrzej Novak, Sang Eon Park, Eric Moreno, Philip Harris</strong></p>
<p>Novelty detection in large scientific datasets faces two key challenges: the noisy and high-dimensional nature of experimental data, and the necessity of making statistically robust statements about any observed outliers. While there is a wealth of literature on anomaly detection via dimensionality reduction, most methods do not produce outputs compatible with quantifiable claims of scientific discovery. In this work we directly address these challenges, presenting the first step towards a unified pipeline for novelty detection adapted for the rigorous statistical demands of science. We introduce AutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive Testing), a general-purpose pipeline for detecting novelty in scientific data. AutoSciDACT begins by creating expressive low-dimensional data representations using a contrastive pre-training, leveraging the abundance of high-quality simulated data in many scientific domains alongside expertise that can guide principled data augmentation strategies. These compact embeddings then enable an extremely sensitive machine learning-based two-sample test using the New Physics Learning Machine (NPLM) framework, which identifies and statistically quantifies deviations in observed data relative to a reference distribution (null hypothesis). We perform experiments across a range of astronomical, physical, biological, image, and synthetic datasets, demonstrating strong sensitivity to small injections of anomalous data across all domains. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡ç§‘å­¦æ•°æ®é›†ä¸­çš„æ–°é¢–æ€§æ£€æµ‹é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå®éªŒæ•°æ®å­˜åœ¨å™ªå£°å’Œé«˜ç»´æ€§ï¼Œä»¥åŠéœ€è¦å¯¹ä»»ä½•è§‚å¯Ÿåˆ°çš„å¼‚å¸¸å€¼åšå‡ºç»Ÿè®¡ä¸Šç¨³å¥çš„é™ˆè¿°ã€‚è™½ç„¶å…³äºé€šè¿‡é™ç»´è¿›è¡Œå¼‚å¸¸æ£€æµ‹çš„æ–‡çŒ®éå¸¸ä¸°å¯Œï¼Œä½†å¤§å¤šæ•°æ–¹æ³•äº§ç”Ÿçš„è¾“å‡ºä¸èƒ½ä¸å¯é‡åŒ–çš„ç§‘å­¦å‘ç°å£°æ˜å…¼å®¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç›´æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœç€é€‚åº”ç§‘å­¦ä¸¥æ ¼ç»Ÿè®¡è¦æ±‚çš„æ–°é¢–æ€§æ£€æµ‹çš„ç»Ÿä¸€æµç¨‹è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ã€‚æˆ‘ä»¬å¼•å…¥äº†AutoSciDACTï¼ˆå€ŸåŠ©å¼‚å¸¸å¯¹æ¯”æµ‹è¯•çš„è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ£€æµ‹ç§‘å­¦æ•°æ®ä¸­æ–°é¢–æ€§çš„é€šç”¨æµç¨‹ã€‚AutoSciDACTé¦–å…ˆé€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒåˆ›å»ºè¡¨è¾¾æ€§çš„ä½ç»´æ•°æ®è¡¨ç¤ºï¼Œåˆ©ç”¨è®¸å¤šç§‘å­¦é¢†åŸŸä¸­æµ·é‡ä¼˜è´¨æ¨¡æ‹Ÿæ•°æ®ä»¥åŠå¯ä»¥æŒ‡å¯¼æœ‰åŸåˆ™çš„æ•°æ®å¢å¼ºç­–ç•¥çš„ä¸“å®¶çŸ¥è¯†ã€‚è¿™äº›ç´§å‡‘çš„åµŒå…¥ç„¶åä½¿èƒ½ä¸€ç§æå…¶æ•æ„Ÿçš„åŸºäºæœºå™¨å­¦ä¹ çš„ä¸¤æ ·æœ¬æµ‹è¯•ï¼Œä½¿ç”¨æ–°ç‰©ç†å­¦ä¹ æœºï¼ˆNPLMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å’Œç»Ÿè®¡åœ°é‡åŒ–è§‚å¯Ÿåˆ°çš„æ•°æ®ä¸å‚è€ƒåˆ†å¸ƒï¼ˆé›¶å‡è®¾ï¼‰ä¹‹é—´çš„åå·®ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—å¤©æ–‡ã€ç‰©ç†ã€ç”Ÿç‰©ã€å›¾åƒå’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯æ˜åœ¨æ‰€æœ‰é¢†åŸŸä¸­å¯¹å¾®å°å¼‚å¸¸æ•°æ®çš„æ³¨å…¥å…·æœ‰å¼ºå¤§çš„æ•æ„Ÿæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21935v1">PDF</a> Accepted at NeurIPS 2025; 32 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¤§å‹ç§‘å­¦æ•°æ®é›†ä¸­è¿›è¡Œæ–°é¢–æ€§æ£€æµ‹æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å®éªŒæ•°æ®çš„é«˜å™ªå£°å’Œé«˜ç»´åº¦æ€§è´¨ï¼Œä»¥åŠéœ€è¦å¯¹è§‚å¯Ÿåˆ°çš„å¼‚å¸¸å€¼åšå‡ºç»Ÿè®¡ä¸Šç¨³å¥çš„é™ˆè¿°ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAutoSciDACTï¼ˆåŸºäºå¼‚å¸¸å¯¹æ¯”æµ‹è¯•çš„è‡ªåŠ¨ç§‘å­¦å‘ç°ï¼‰çš„ç»Ÿä¸€ç®¡é“ï¼Œç”¨äºæ£€æµ‹ç§‘å­¦æ•°æ®ä¸­çš„æ–°é¢–æ€§ã€‚AutoSciDACTé€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒåˆ›å»ºè¡¨è¾¾æ€§ä½ç»´æ•°æ®è¡¨ç¤ºï¼Œåˆ©ç”¨è®¸å¤šç§‘å­¦é¢†åŸŸä¸­çš„é«˜è´¨é‡æ¨¡æ‹Ÿæ•°æ®ä¸°å¯Œçš„èµ„æºä»¥åŠæŒ‡å¯¼åŸåˆ™çš„æ•°æ®å¢å¼ºç­–ç•¥çš„ä¸“ä¸šçŸ¥è¯†ã€‚ç„¶åï¼Œè¿™äº›ç´§å‡‘åµŒå…¥ä½¿æœºå™¨å­¦ä¹ èƒ½å¤Ÿä½¿ç”¨æ–°ç‰©ç†å­¦ä¹ æœºï¼ˆNPLMï¼‰æ¡†æ¶è¿›è¡Œé«˜åº¦æ•æ„Ÿçš„ä¸¤æ ·æœ¬æµ‹è¯•ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å’Œç»Ÿè®¡è§‚å¯Ÿåˆ°çš„æ•°æ®ä¸å‚è€ƒåˆ†å¸ƒï¼ˆé›¶å‡è®¾ï¼‰ä¹‹é—´çš„åå·®ã€‚æœ¬æ–‡åœ¨å¤©æ–‡ã€ç‰©ç†ã€ç”Ÿç‰©ã€å›¾åƒå’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒï¼Œè¯æ˜åœ¨å„ç§é¢†åŸŸä¸­å¯¹å¾®å°å¼‚å¸¸æ•°æ®çš„æ³¨å…¥å…·æœ‰å¼ºå¤§çš„æ•æ„Ÿæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°é¢–æ€§æ£€æµ‹åœ¨ç§‘å­¦æ•°æ®é›†ä¸­é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®çš„å™ªå£°å’Œé«˜ç»´åº¦æ€§è´¨ä»¥åŠå¯¹è§‚å¯Ÿåˆ°çš„å¼‚å¸¸å€¼è¿›è¡Œç»Ÿè®¡ä¸Šç¨³å¥çš„é™ˆè¿°çš„éœ€æ±‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºAutoSciDACTçš„ç»Ÿä¸€ç®¡é“æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥ç®¡é“é€‚ç”¨äºç§‘å­¦æ•°æ®çš„æ–°é¢–æ€§æ£€æµ‹ã€‚</li>
<li>AutoSciDACTé€šè¿‡å¯¹æ¯”é¢„è®­ç»ƒåˆ›å»ºä½ç»´æ•°æ®è¡¨ç¤ºï¼Œç»“åˆä¸“ä¸šæŒ‡å¯¼åŸåˆ™çš„æ•°æ®å¢å¼ºç­–ç•¥æ¥å¢å¼ºæ–°é¢–æ€§æ£€æµ‹çš„èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨äº†ç§‘å­¦é¢†åŸŸçš„æ¨¡æ‹Ÿæ•°æ®ä»¥åŠé«˜çº¬åº¦æ¨¡æ‹Ÿç”Ÿæˆæ¨¡å‹è®­ç»ƒçš„æ•°æ®è¡¨è¾¾æ¨¡å¼æ¥æé«˜æ•°æ®è´¨é‡ã€‚</li>
<li>AutoSciDACTåˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸­çš„ä¸¤æ ·æœ¬æµ‹è¯•æ¥è¯†åˆ«å’Œç»Ÿè®¡è§‚å¯Ÿåˆ°çš„æ•°æ®ä¸å‚è€ƒåˆ†å¸ƒä¹‹é—´çš„åå·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-880a707ba4fbafeb5763480ae88ab86f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293080&auth_key=1762293080-0-0-e109670dee63ab4a63404c1c6290321d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1d058fc6304c1113f8d8d6fdc79e7a77~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293087&auth_key=1762293087-0-0-dd0bfe7dd7ff80a1f8295b2063cbb0a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Digital-Contrast-CT-Pulmonary-Angiography-Synthesis-from-Non-contrast-CT-for-Pulmonary-Vascular-Disease"><a href="#Digital-Contrast-CT-Pulmonary-Angiography-Synthesis-from-Non-contrast-CT-for-Pulmonary-Vascular-Disease" class="headerlink" title="Digital Contrast CT Pulmonary Angiography Synthesis from Non-contrast CT   for Pulmonary Vascular Disease"></a>Digital Contrast CT Pulmonary Angiography Synthesis from Non-contrast CT   for Pulmonary Vascular Disease</h2><p><strong>Authors:Ying Ming, Yue Lin, Longfei Zhao, Gengwan Li, Zuopeng Tan, Bing Li, Sheng Xie, Wei Song, Qiqi Xu</strong></p>
<p>Computed Tomography Pulmonary Angiography (CTPA) is the reference standard for diagnosing pulmonary vascular diseases such as Pulmonary Embolism (PE) and Chronic Thromboembolic Pulmonary Hypertension (CTEPH). However, its reliance on iodinated contrast agents poses risks including nephrotoxicity and allergic reactions, particularly in high-risk patients. This study proposes a method to generate Digital Contrast CTPA (DCCTPA) from Non-Contrast CT (NCCT) scans using a cascaded synthesizer based on Cycle-Consistent Generative Adversarial Networks (CycleGAN). Totally retrospective 410 paired CTPA and NCCT scans were obtained from three centers. The model was trained and validated internally on 249 paired images. Extra dataset that comprising 161 paired images was as test set for model generalization evaluation and downstream clinical tasks validation. Compared with state-of-the-art (SOTA) methods, the proposed method achieved the best comprehensive performance by evaluating quantitative metrics (For validation, MAE: 156.28, PSNR: 20.71 and SSIM: 0.98; For test, MAE: 165.12, PSNR: 20.27 and SSIM: 0.98) and qualitative visualization, demonstrating valid vessel enhancement, superior image fidelity and structural preservation. The approach was further applied to downstream tasks of pulmonary vessel segmentation and vascular quantification. On the test set, the average Dice, clDice, and clRecall of artery and vein pulmonary segmentation was 0.70, 0.71, 0.73 and 0.70, 0.72, 0.75 respectively, all markedly improved compared with NCCT inputs.@ Inter-class Correlation Coefficient (ICC) for vessel volume between DCCTPA and CTPA was significantly better than that between NCCT and CTPA (Average ICC : 0.81 vs 0.70), indicating effective vascular enhancement in DCCTPA, especially for small vessels. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æè‚ºåŠ¨è„‰é€ å½±ï¼ˆCTPAï¼‰æ˜¯è¯Šæ–­è‚ºè¡€ç®¡ç–¾ç—…å¦‚è‚ºæ “å¡ï¼ˆPEï¼‰å’Œæ…¢æ€§è¡€æ “æ “å¡æ€§è‚ºåŠ¨è„‰é«˜å‹ï¼ˆCTEPHï¼‰çš„å‚è€ƒæ ‡å‡†ã€‚ç„¶è€Œï¼Œå®ƒä¾èµ–äºç¢˜åŒ–é€ å½±å‰‚ï¼Œè¿™å¸¦æ¥äº†åŒ…æ‹¬è‚¾æ¯’æ€§å’Œè¿‡æ•ååº”åœ¨å†…çš„é£é™©ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å±æ‚£è€…ä¸­ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå‘¨æœŸä¸€è‡´æ€§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆCycleGANï¼‰çš„çº§è”åˆæˆå™¨ç”Ÿæˆéå¯¹æ¯”CTï¼ˆNCCTï¼‰æ‰«æçš„æ•°å­—å¯¹æ¯”CTPAï¼ˆDCCTPAï¼‰çš„æ–¹æ³•ã€‚ç ”ç©¶å…±ä»ä¸‰ä¸ªä¸­å¿ƒè·å–äº†410å¯¹å›é¡¾æ€§CTPAå’ŒNCCTæ‰«æã€‚è¯¥æ¨¡å‹åœ¨å†…éƒ¨ä½¿ç”¨249å¯¹å›¾åƒè¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚åŒ…å«161å¯¹å›¾åƒçš„é¢å¤–æ•°æ®é›†ç”¨äºæ¨¡å‹æ³›åŒ–è¯„ä¼°å’Œä¸‹æ¸¸ä¸´åºŠä»»åŠ¡éªŒè¯ã€‚ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡è¯„ä¼°ä¸Šå–å¾—äº†æœ€ä½³çš„ç»¼åˆæ€§èƒ½ï¼ˆå¯¹äºéªŒè¯é›†ï¼ŒMAEï¼š156.28ï¼ŒPSNRï¼š20.71ï¼ŒSSIMï¼š0.98ï¼›å¯¹äºæµ‹è¯•é›†ï¼ŒMAEï¼š165.12ï¼ŒPSNRï¼š20.27ï¼ŒSSIMï¼š0.98ï¼‰ï¼Œå¹¶åœ¨å®šæ€§å¯è§†åŒ–æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆçš„è¡€ç®¡å¢å¼ºã€è¾ƒé«˜çš„å›¾åƒä¿çœŸåº¦å’Œç»“æ„ä¿æŒã€‚è¯¥æ–¹æ³•è¿›ä¸€æ­¥åº”ç”¨äºè‚ºè¡€ç®¡åˆ†å‰²å’Œè¡€ç®¡é‡åŒ–çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨æµ‹è¯•é›†ä¸Šï¼ŒåŠ¨è„‰å’Œé™è„‰è‚ºåˆ†å‰²çš„å¹³å‡Diceã€clDiceå’ŒclRecallåˆ†åˆ«ä¸º0.70ã€0.71ã€0.73å’Œ0.70ã€0.72ã€0.75ï¼Œä¸NCCTè¾“å…¥ç›¸æ¯”å‡æœ‰æ˜¾è‘—æ”¹å–„ã€‚DCCTPAä¸CTPAä¹‹é—´çš„è¡€ç®¡ä½“ç§¯çš„ç±»é—´ç›¸å…³ç³»æ•°ï¼ˆICCï¼‰æ˜¾è‘—é«˜äºNCCTä¸CTPAä¹‹é—´çš„ç±»é—´ç›¸å…³ç³»æ•°ï¼ˆå¹³å‡ICCï¼š0.81 vs 0.70ï¼‰ï¼Œè¿™è¡¨æ˜DCCTPAä¸­çš„è¡€ç®¡å¢å¼ºæœ‰æ•ˆï¼Œå°¤å…¶æ˜¯å°è¡€ç®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21140v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºCycleGANçš„çº§è”åˆæˆå™¨ç”Ÿæˆæ•°å­—å¯¹æ¯”CTPAï¼ˆDCCTPAï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨éå¯¹æ¯”CTï¼ˆNCCTï¼‰æ‰«æç”ŸæˆCTPAå›¾åƒã€‚è¯¥ç ”ç©¶ä¸»è¦ç”¨äºè¯Šæ–­è‚ºåŠ¨è„‰ç–¾ç—…å¦‚è‚ºæ “å¡å’Œæ…¢æ€§è¡€æ “æ “å¡æ€§è‚ºåŠ¨è„‰é«˜å‹ã€‚æ­¤æ–¹æ³•é€šè¿‡åœ¨å†…éƒ¨å¯¹249å¯¹å›¾åƒè¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶åœ¨åŒ…å«161å¯¹å›¾åƒçš„é¢å¤–æ•°æ®é›†ä¸Šè¿›è¡Œæ¨¡å‹æ³›åŒ–è¯„ä¼°å’Œä¸‹æ¸¸ä¸´åºŠä»»åŠ¡éªŒè¯ï¼Œä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œåœ¨å®šé‡æŒ‡æ ‡ï¼ˆéªŒè¯é›†MAEï¼š156.28ï¼ŒPSNRï¼š20.71ï¼ŒSSIMï¼š0.98ï¼›æµ‹è¯•é›†MAEï¼š165.12ï¼ŒPSNRï¼š20.27å’ŒSSIMï¼š0.98ï¼‰å’Œå®šæ€§å¯è§†åŒ–æ–¹é¢å–å¾—äº†æœ€ä½³çš„ç»¼åˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜åº”ç”¨äºè‚ºè¡€ç®¡åˆ†å‰²å’Œè¡€ç®¡å®šé‡ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨æµ‹è¯•é›†ä¸Šï¼ŒåŠ¨è„‰å’Œé™è„‰è‚ºåˆ†å‰²çš„Diceã€clDiceå’ŒclRecallåˆ†åˆ«ä¸º0.70ã€0.71ã€0.73å’Œ0.70ã€0.72ã€0.75ï¼Œä¸NCCTè¾“å…¥ç›¸æ¯”æœ‰æ˜æ˜¾æ”¹å–„ã€‚DCCTPAä¸CTPAä¹‹é—´çš„è¡€ç®¡ä½“ç§¯çš„ICCæ˜¾è‘—é«˜äºNCCTä¸CTPAä¹‹é—´çš„ICCï¼ˆå¹³å‡ICCï¼š0.81 vs 0.70ï¼‰ï¼Œè¡¨æ˜DCCTPAåœ¨è¡€ç®¡å¢å¼ºæ–¹é¢å°¤å…¶å¯¹å°è¡€ç®¡æœ‰æ•ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºCycleGANçš„DCCTPAç”Ÿæˆæ–¹æ³•ï¼Œä½¿ç”¨NCCTæ‰«æç”ŸæˆCTPAå›¾åƒã€‚</li>
<li>æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§å¯è§†åŒ–æ–¹é¢å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œå®ç°äº†æœ‰æ•ˆçš„è¡€ç®¡å¢å¼ºã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„ç»¼åˆæ€§èƒ½æœ€ä½³ã€‚</li>
<li>æ–¹æ³•åœ¨ä¸‹æ¸¸ä»»åŠ¡å¦‚è‚ºè¡€ç®¡åˆ†å‰²å’Œè¡€ç®¡å®šé‡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>DCCTPAåœ¨æµ‹è¯•é›†ä¸Šçš„åŠ¨è„‰å’Œé™è„‰è‚ºåˆ†å‰²ç»“æœè¾ƒNCCTæœ‰æ˜æ˜¾æ”¹å–„ã€‚</li>
<li>DCCTPAä¸CTPAä¹‹é—´çš„è¡€ç®¡ä½“ç§¯çš„ICCæ˜¾è‘—é«˜äºNCCTä¸CTPAä¹‹é—´çš„ICCï¼Œè¡¨æ˜å…¶åœ¨å°è¡€ç®¡å¢å¼ºæ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-73a69bb1dc989522e23e5f2766914bd8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293094&auth_key=1762293094-0-0-11f92a7dbed2d9a5aad7ee101afcfda2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c76e1358ec19ff74cbdf33ecd3156d8f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293101&auth_key=1762293101-0-0-18d08c9cf4472a41ed5049dcf809ff80&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge"><a href="#Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge" class="headerlink" title="Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge"></a>Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge</h2><p><strong>Authors:Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</strong></p>
<p>Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home">https://sites.google.com/view/lddbm/home</a>. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•ä½¿æ‰©æ•£æ¨¡å‹æˆä¸ºå¤„ç†å¤æ‚æ•°æ®åˆ†å¸ƒé‡‡æ ·çš„æœ€å…ˆè¿›æŠ€æœ¯å·¥å…·ã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬çš„èƒ½åŠ›æ‹“å±•åˆ°è·¨ä¸åŒæ„Ÿå®˜æ¨¡æ€çš„ä¿¡æ¯ç¿»è¯‘ï¼ˆæ¨¡æ€ç¿»è¯‘ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé™åˆ¶æ€§å‡è®¾ï¼ŒåŒ…æ‹¬å…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œç‰¹å®šæ¨¡æ€æ¶æ„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„é€šç”¨æ€§å’Œç†è®ºåŸºç¡€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLatent Denoising Diffusion Bridge Modelï¼ŒLDDBMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹çš„æ½œåœ¨å˜é‡æ‰©å±•çš„é€šç”¨æ¨¡æ€ç¿»è¯‘æ¡†æ¶ã€‚é€šè¿‡åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— éœ€å¯¹é½ç»´åº¦çš„æƒ…å†µä¸‹å­¦ä¼šäº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯¹æ¯”å¯¹é½æŸå¤±æ¥å¼ºåˆ¶é…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é’ˆå¯¹æ½œåœ¨ç©ºé—´ä¸­çš„å™ªå£°é¢„æµ‹çš„åŸŸæ— å…³ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢„æµ‹æŸå¤±æ¥å¼•å¯¼è®­ç»ƒå®ç°å‡†ç¡®çš„è·¨åŸŸç¿»è¯‘ï¼Œå¹¶æ¢ç´¢äº†å¤šç§è®­ç»ƒç­–ç•¥æ¥æé«˜ç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒä»»æ„æ¨¡æ€å¯¹ï¼Œå¹¶åœ¨å¤šç§æ¨¡æ€ç¿»è¯‘ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼ŒåŒ…æ‹¬å¤šè§†å›¾åˆ°3Då½¢çŠ¶ç”Ÿæˆã€å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå¤šè§†å›¾åœºæ™¯åˆæˆã€‚ç»¼åˆå®éªŒå’Œæ¶ˆèå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ä¸€èˆ¬æ¨¡æ€ç¿»è¯‘ä¸­å»ºç«‹äº†æ–°çš„å¼ºåŠ²åŸºå‡†ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home%E3%80%82">https://sites.google.com/view/lddbm/homeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20819v2">PDF</a> Accepted as a poster at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨æ¨¡æ€è½¬æ¢ï¼ˆModality Translation, MTï¼‰é¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºä¸€ç§åŸºäºæ½œåœ¨å˜é‡æ‰©å±•çš„é™å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLatent Denoising Diffusion Bridge Model, LDBMï¼‰çš„é€šç”¨æ¡†æ¶ï¼Œå¯åœ¨æ— éœ€å¯¹é½ç»´åº¦çš„å…±äº«æ½œåœ¨ç©ºé—´å†…å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”å¯¹é½æŸå¤±å’Œé’ˆå¯¹æ½œåœ¨ç©ºé—´çš„å™ªå£°é¢„æµ‹è®¾è®¡çš„é¢†åŸŸæ— å…³ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå®ç°äº†å¯¹ä»»æ„æ¨¡æ€å¯¹çš„æ”¯æŒï¼Œå¹¶åœ¨å¤šç§MTä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å·²å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å°†å…¶æ‰©å±•åˆ°æ¨¡æ€è½¬æ¢ï¼ˆMTï¼‰ä»å…·æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–äºå…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œæ¨¡æ€ç‰¹å®šæ¶æ„ç­‰é™åˆ¶æ€§å‡è®¾ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œç†è®ºæ ¹åŸºã€‚</li>
<li>æå‡ºçš„Latent Denoising Diffusion Bridge Modelï¼ˆLDBMï¼‰æ˜¯ä¸€ä¸ªç”¨äºæ¨¡æ€è½¬æ¢çš„é€šç”¨æ¡†æ¶ï¼ŒåŸºäºæ½œåœ¨å˜é‡çš„æ‰©å±•ã€‚</li>
<li>LDBMåœ¨æ— éœ€å¯¹é½ç»´åº¦çš„å…±äº«æ½œåœ¨ç©ºé—´å†…å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ã€‚</li>
<li>å¼•å…¥å¯¹æ¯”å¯¹é½æŸå¤±æ¥å¼ºåˆ¶æ‰§è¡Œé…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡äº†é’ˆå¯¹æ½œåœ¨ç©ºé—´çš„å™ªå£°é¢„æµ‹çš„é¢†åŸŸæ— å…³ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-225a662404a8c32f6e7e723321d5bdb4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293109&auth_key=1762293109-0-0-2ba018f9d41c5e0b13f0051c0ad0137c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d66e2842144f846f899b138e2dd2d51~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293116&auth_key=1762293116-0-0-6035406c46e21ae76faf1f38407bc4ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-227bee8414c0a17e145209c970af48a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293123&auth_key=1762293123-0-0-da51526a032e1e0f58342d95ef0d94cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CARE-Contrastive-Alignment-for-ADL-Recognition-from-Event-Triggered-Sensor-Streams"><a href="#CARE-Contrastive-Alignment-for-ADL-Recognition-from-Event-Triggered-Sensor-Streams" class="headerlink" title="CARE: Contrastive Alignment for ADL Recognition from Event-Triggered   Sensor Streams"></a>CARE: Contrastive Alignment for ADL Recognition from Event-Triggered   Sensor Streams</h2><p><strong>Authors:Junhao Zhao, Zishuai Liu, Ruili Fang, Jin Lu, Linghan Zhang, Fei Dou</strong></p>
<p>The recognition of Activities of Daily Living (ADLs) from event-triggered ambient sensors is an essential task in Ambient Assisted Living, yet existing methods remain constrained by representation-level limitations. Sequence-based approaches preserve temporal order of sensor activations but are sensitive to noise and lack spatial awareness, while image-based approaches capture global patterns and implicit spatial correlations but compress fine-grained temporal dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation) fail to enforce alignment between sequence- and image-based representation views, underutilizing their complementary strengths. We propose Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an end-to-end framework that jointly optimizes representation learning via Sequence-Image Contrastive Alignment (SICA) and classification via cross-entropy, ensuring both cross-representation alignment and task-specific discriminability. CARE integrates (i) time-aware, noise-resilient sequence encoding with (ii) spatially-informed and frequency-sensitive image representations, and employs (iii) a joint contrastive-classification objective for end-to-end learning of aligned and discriminative embeddings. Evaluated on three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to sensor malfunctions and layout variability, highlighting its potential for reliable ADL recognition in smart homes. </p>
<blockquote>
<p>ä»äº‹ä»¶è§¦å‘çš„ç¯å¢ƒä¼ æ„Ÿå™¨è¯†åˆ«æ—¥å¸¸æ´»åŠ¨ï¼ˆADLsï¼‰æ˜¯æ™ºèƒ½è¾…åŠ©ç”Ÿæ´»çš„ä¸€ä¸ªé‡è¦ä»»åŠ¡ï¼Œä½†ç°æœ‰æ–¹æ³•ä»å—åˆ°è¡¨ç¤ºå±‚é¢ä¸Šçš„é™åˆ¶ã€‚åŸºäºåºåˆ—çš„æ–¹æ³•ä¿ç•™äº†ä¼ æ„Ÿå™¨æ¿€æ´»çš„æ—¶é—´é¡ºåºï¼Œä½†å¯¹å™ªå£°æ•æ„Ÿä¸”ç¼ºä¹ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œè€ŒåŸºäºå›¾åƒçš„æ–¹æ³•æ•æ‰äº†å…¨å±€æ¨¡å¼å’Œéšå¼ç©ºé—´ç›¸å…³æ€§ï¼Œä½†å‹ç¼©äº†ç²¾ç»†çš„æ—¶é—´åŠ¨æ€å¹¶æ‰­æ›²äº†ä¼ æ„Ÿå™¨å¸ƒå±€ã€‚ç®€å•çš„èåˆæ–¹æ³•ï¼ˆä¾‹å¦‚ç‰¹å¾æ‹¼æ¥ï¼‰æœªèƒ½å®ç°åºåˆ—å’Œå›¾åƒè¡¨ç¤ºè§†å›¾ä¹‹é—´çš„å¯¹é½ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºäº‹ä»¶è§¦å‘çš„ä¼ æ„Ÿå™¨æµè¿›è¡ŒADLè¯†åˆ«çš„å¯¹æ¯”å¯¹é½æ–¹æ³•ï¼ˆCAREï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé€šè¿‡åºåˆ—å›¾åƒå¯¹æ¯”å¯¹é½ï¼ˆSICAï¼‰å’Œäº¤å‰ç†µè¿›è¡Œåˆ†ç±»ï¼Œè”åˆä¼˜åŒ–è¡¨ç¤ºå­¦ä¹ ï¼Œç¡®ä¿è·¨è¡¨ç¤ºå¯¹é½å’Œä»»åŠ¡ç‰¹å®šçš„è¾¨åˆ«åŠ›ã€‚CAREé›†æˆäº†ï¼ˆiï¼‰æ—¶é—´æ„ŸçŸ¥ã€å™ªå£°è€ç”¨çš„åºåˆ—ç¼–ç ä»¥åŠï¼ˆiiï¼‰åŒ…å«ç©ºé—´ä¿¡æ¯å’Œé¢‘ç‡æ•æ„Ÿæ€§çš„å›¾åƒè¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨äº†ï¼ˆiiiï¼‰è”åˆå¯¹æ¯”åˆ†ç±»ç›®æ ‡æ¥è¿›è¡Œç«¯åˆ°ç«¯çš„å­¦ä¹ å’Œè¾¨åˆ«åµŒå…¥ã€‚åœ¨ä¸‰ä¸ªCASASæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒCAREè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆç±³å…°æ•°æ®é›†ä¸Š89.8%ï¼Œå¼€ç½—æ•°æ®é›†ä¸Š88.9%ï¼Œäº¬éƒ½7æ•°æ®é›†ä¸Š73.3%ï¼‰ï¼Œå¹¶è¯æ˜äº†å…¶å¯¹äºä¼ æ„Ÿå™¨æ•…éšœå’Œå¸ƒå±€å˜åŒ–çš„ç¨³å¥æ€§ï¼Œçªå‡ºäº†å…¶åœ¨æ™ºèƒ½å®¶åº­ä¸­å¯é è¯†åˆ«ADLçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16988v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ´»åŠ¨æ—¥å¸¸ç”Ÿæ´»ï¼ˆADLsï¼‰è¯†åˆ«æ˜¯æ™ºèƒ½å±…ä½ç¯å¢ƒä¸­çš„é‡è¦ä»»åŠ¡ä¹‹ä¸€ã€‚ç°æœ‰çš„è¯†åˆ«æ–¹æ³•ä»å­˜åœ¨è¡¨ç°å±‚é¢ä¸Šçš„å±€é™æ€§ã€‚åºåˆ—æ–¹æ³•è™½ä¿ç•™äº†ä¼ æ„Ÿå™¨æ¿€æ´»çš„ä¸´æ—¶é¡ºåºï¼Œä½†æ˜“å—å™ªå£°å¹²æ‰°ä¸”ç¼ºä¹ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€‚å›¾åƒæ–¹æ³•æ•æ‰å…¨å±€æ¨¡å¼å’Œéšå«çš„ç©ºé—´ç›¸å…³æ€§ï¼Œä½†ä¼šå‹ç¼©ç²¾ç»†çš„ä¸´æ—¶åŠ¨æ€å¹¶æ‰­æ›²ä¼ æ„Ÿå™¨å¸ƒå±€ã€‚ç®€å•çš„èåˆæ–¹æ³•ï¼ˆå¦‚ç‰¹å¾æ‹¼æ¥ï¼‰æœªèƒ½å¼ºåŒ–åºåˆ—å’Œå›¾åƒè¡¨ç¤ºä¹‹é—´çš„å¯¹é½ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨äºŒè€…çš„äº’è¡¥ä¼˜åŠ¿ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCAREï¼ˆå¯¹æ¯”å¯¹é½ç”¨äºäº‹ä»¶è§¦å‘ä¼ æ„Ÿå™¨æµçš„æ´»åŠ¨æ—¥å¸¸ç”Ÿæ´»è¯†åˆ«ï¼‰çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œé€šè¿‡åºåˆ—å›¾åƒå¯¹æ¯”å¯¹é½ï¼ˆSICAï¼‰å’Œäº¤å‰ç†µä¼˜åŒ–è¡¨ç¤ºå­¦ä¹ å’Œåˆ†ç±»ï¼Œç¡®ä¿è·¨è¡¨ç¤ºå¯¹é½å’Œä»»åŠ¡ç‰¹å®šé‰´åˆ«åŠ›ã€‚CAREé›†æˆäº†æ—¶é—´æ„ŸçŸ¥ã€å™ªå£°æŠ—æ‰°çš„åºåˆ—ç¼–ç å’Œå…·æœ‰ç©ºé—´æ„ŸçŸ¥åŠé¢‘ç‡æ•æ„Ÿæ€§çš„å›¾åƒè¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨è”åˆå¯¹æ¯”åˆ†ç±»ç›®æ ‡è¿›è¡Œç«¯åˆ°ç«¯å­¦ä¹ å’Œåˆ¤åˆ«åµŒå…¥ã€‚åœ¨ä¸‰ä¸ªCASASæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCAREè¾¾åˆ°äº†ç±³å…°89.8%ã€å¼€ç½—88.9%ã€äº¬éƒ½73.3%çš„æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹¶å±•ç¤ºäº†å¯¹ä¼ æ„Ÿå™¨æ•…éšœå’Œå¸ƒå±€å˜åŒ–çš„ç¨³å¥æ€§ï¼Œå‡¸æ˜¾å…¶åœ¨æ™ºèƒ½å®¶å±…ä¸­å®ç°å¯é ADLsè¯†åˆ«çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ´»åŠ¨æ—¥å¸¸ç”Ÿæ´»ï¼ˆADLsï¼‰è¯†åˆ«æ˜¯æ™ºèƒ½è¾…åŠ©å±…ä½ç¯å¢ƒçš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨è¡¨ç¤ºå±‚é¢çš„å±€é™æ€§ï¼Œå¦‚å™ªå£°æ•æ„Ÿã€ç¼ºä¹ç©ºé—´æ„ŸçŸ¥æˆ–å¤±çœŸä¼ æ„Ÿå™¨å¸ƒå±€ã€‚</li>
<li>ç®€å•èåˆåºåˆ—å’Œå›¾åƒæ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å…¶äº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºCAREçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œé€šè¿‡åºåˆ—å›¾åƒå¯¹æ¯”å¯¹é½ï¼ˆSICAï¼‰è”åˆä¼˜åŒ–è¡¨ç¤ºå­¦ä¹ å’Œåˆ†ç±»ã€‚</li>
<li>CAREé›†æˆäº†æ—¶é—´æ„ŸçŸ¥å’Œå™ªå£°æŠ—æ‰°çš„åºåˆ—ç¼–ç ï¼Œä»¥åŠå…·æœ‰ç©ºé—´æ„ŸçŸ¥å’Œé¢‘ç‡æ•æ„Ÿæ€§çš„å›¾åƒè¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨è”åˆå¯¹æ¯”åˆ†ç±»ç›®æ ‡è¿›è¡Œåˆ¤åˆ«åµŒå…¥çš„ç«¯åˆ°ç«¯å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cda0a4be1f149a362ba167d1fbbf85df~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293131&auth_key=1762293131-0-0-ac0464e3fc85f2474b5cab58aa9b6b14&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-873f490938d025b47c2f215663dd2d89~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293139&auth_key=1762293139-0-0-80d7284dffd48bf3733fc11182d8f321&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a915266bd7281e0d078ced30d21df9c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293145&auth_key=1762293145-0-0-aa6e3185fb494d21e1985778f954ef73&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fbba4378b0f80323ef632602a11a4c68~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293152&auth_key=1762293152-0-0-34d825a624e547bc9d4f8cbeff9834b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e29aa2273a8c5ba37a041e2201516a68~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293158&auth_key=1762293158-0-0-618da8e5b8fd4ec9716fdcd4f0a995a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-036948f5c5e8c09b4196125913f1b226~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293165&auth_key=1762293165-0-0-91af818c369b4280d051e3865ffc6956&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Style-Aware-Blending-and-Prototype-Based-Cross-Contrast-Consistency-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Style-Aware-Blending-and-Prototype-Based-Cross-Contrast-Consistency-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation"></a>Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Chaowei Chen, Xiang Zhang, Honglie Guo, Shunfang Wang</strong></p>
<p>Weak-strong consistency learning strategies are widely employed in semi-supervised medical image segmentation to train models by leveraging limited labeled data and enforcing weak-to-strong consistency. However, existing methods primarily focus on designing and combining various perturbation schemes, overlooking the inherent potential and limitations within the framework itself. In this paper, we first identify two critical deficiencies: (1) separated training data streams, which lead to confirmation bias dominated by the labeled stream; and (2) incomplete utilization of supervisory information, which limits exploration of strong-to-weak consistency. To tackle these challenges, we propose a style-aware blending and prototype-based cross-contrast consistency learning framework. Specifically, inspired by the empirical observation that the distribution mismatch between labeled and unlabeled data can be characterized by statistical moments, we design a style-guided distribution blending module to break the independent training data streams. Meanwhile, considering the potential noise in strong pseudo-labels, we introduce a prototype-based cross-contrast strategy to encourage the model to learn informative supervisory signals from both weak-to-strong and strong-to-weak predictions, while mitigating the adverse effects of noise. Experimental results demonstrate the effectiveness and superiority of our framework across multiple medical segmentation benchmarks under various semi-supervised settings. </p>
<blockquote>
<p>å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œé€šè¿‡åˆ©ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®å¹¶å¼ºåˆ¶æ‰§è¡Œå¼±åˆ°å¼ºçš„ä¸€è‡´æ€§æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡å’Œç»„åˆå„ç§æ‰°åŠ¨æ–¹æ¡ˆï¼Œè€Œå¿½ç•¥äº†æ¡†æ¶æœ¬èº«çš„å†…åœ¨æ½œåŠ›å’Œå±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºä¸¤ä¸ªå…³é”®ç¼ºé™·ï¼šï¼ˆ1ï¼‰è®­ç»ƒæ•°æ®æµåˆ†ç¦»ï¼Œå¯¼è‡´ä»¥æ ‡è®°æµä¸ºä¸»çš„ç¡®è®¤åè§ï¼›ï¼ˆ2ï¼‰ç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸å®Œå…¨ï¼Œé™åˆ¶äº†ä»å¼ºåˆ°å¼±çš„ä¸€è‡´æ€§çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé£æ ¼æ„ŸçŸ¥çš„æ··åˆå’ŒåŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼Œå—å®è¯è§‚å¯Ÿå¯å‘ï¼Œå³æ ‡è®°å’Œæ— æ ‡è®°æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…å¯ä»¥é€šè¿‡ç»Ÿè®¡çŸ©æ¥è¡¨å¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé£æ ¼å¼•å¯¼çš„åˆ†å¸ƒæ··åˆæ¨¡å—æ¥æ‰“ç ´ç‹¬ç«‹çš„è®­ç»ƒæ•°æ®æµã€‚åŒæ—¶ï¼Œè€ƒè™‘åˆ°å¼ºä¼ªæ ‡ç­¾ä¸­çš„æ½œåœ¨å™ªå£°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œä»¥é¼“åŠ±æ¨¡å‹ä»å¼±åˆ°å¼ºå’Œå¼ºåˆ°å¼±çš„é¢„æµ‹ä¸­å­¦ä¹ æœ‰ç”¨çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶å‡è½»å™ªå£°çš„ä¸åˆ©å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šç§åŒ»å­¦åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸‹ï¼Œåœ¨å„ç§åŠç›‘ç£è®¾ç½®ä¸­éƒ½å…·æœ‰æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20729v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶æ¥è§£å†³ç°æœ‰æ–¹æ³•ä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šè®­ç»ƒæ•°æ®æµåˆ†ç¦»å’Œç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸å®Œå…¨ã€‚é€šè¿‡é£æ ¼æ„ŸçŸ¥æ··åˆå’ŒåŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ‰“ç ´ç‹¬ç«‹è®­ç»ƒæ•°æ®æµï¼Œå¹¶ä»å¼±å¼ºå’Œå¼ºå¼±çš„é¢„æµ‹ä¸­å­¦ä¹ æœ‰ç”¨çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶å‡è½»å™ªå£°çš„ä¸åˆ©å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§åŒ»å­¦åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ä¸»è¦å…³æ³¨æ‰°åŠ¨æ–¹æ¡ˆçš„è®¾è®¡ï¼Œå¿½ç•¥äº†æ¡†æ¶æœ¬èº«çš„æ½œåœ¨èƒ½åŠ›å’Œå±€é™æ€§ã€‚</li>
<li>è®­ç»ƒæ•°æ®æµåˆ†ç¦»ä¼šå¯¼è‡´ç¡®è®¤åè§ï¼Œä¸»è¦ä¾èµ–äºæ ‡è®°æµã€‚</li>
<li>ç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸å®Œå…¨é™åˆ¶äº†å¼ºåˆ°å¼±çš„ä¸€è‡´æ€§çš„æ¢ç´¢ã€‚</li>
<li>æ–°çš„æ¡†æ¶ç»“åˆäº†é£æ ¼æ„ŸçŸ¥æ··åˆå’ŒåŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é£æ ¼æ„ŸçŸ¥æ··åˆæ¨¡å—é€šè¿‡æ‰“ç ´ç‹¬ç«‹è®­ç»ƒæ•°æ®æµæ¥ä¼˜åŒ–æ•°æ®åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥æœ‰åŠ©äºæ¨¡å‹ä»å¼±å¼ºå’Œå¼ºå¼±çš„é¢„æµ‹ä¸­å­¦ä¹ æœ‰ç”¨çš„ç›‘ç£ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-27d95456f3267c95a1f6a15ee6ffe323~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293172&auth_key=1762293172-0-0-aa89644d12f085c1f6b5c0ed933451a8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-806fca0e263f0dbc429c40c81c646715~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293180&auth_key=1762293180-0-0-8c12bba41e75aef61dfedeecfee87596&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-abb9bb92ba1fb693724a16a417297c79~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293187&auth_key=1762293187-0-0-2579b7bdc5daf4a283408156733af00f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-083d16bf92e9b63d057e3a5da2486d91~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293194&auth_key=1762293194-0-0-b11e1c862ea3db458d931629c09b6502&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe3e6c56c86c6c837741892c706c4d2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293202&auth_key=1762293202-0-0-fb7f3335c230a85d0d64cc17ea454f5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Contrastive-Conditional-Unconditional-Alignment-for-Long-tailed-Diffusion-Model"><a href="#Contrastive-Conditional-Unconditional-Alignment-for-Long-tailed-Diffusion-Model" class="headerlink" title="Contrastive Conditional-Unconditional Alignment for Long-tailed   Diffusion Model"></a>Contrastive Conditional-Unconditional Alignment for Long-tailed   Diffusion Model</h2><p><strong>Authors:Fang Chen, Alex Villa, Gongbo Liang, Xiaoyi Lu, Meng Tang</strong></p>
<p>Training data for class-conditional image synthesis often exhibit a long-tailed distribution with limited images for tail classes. Such an imbalance causes mode collapse and reduces the diversity of synthesized images for tail classes. For class-conditional diffusion models trained on imbalanced data, we aim to improve the diversity and fidelity of tail class images without compromising the quality of head class images. We achieve this by introducing two simple but highly effective loss functions. Firstly, we employ an Unsupervised Contrastive Loss (UCL) utilizing negative samples to increase the distance&#x2F;dissimilarity among synthetic images. Such regularization is coupled with a standard trick of batch resampling to further diversify tail-class images. Our second loss is an Alignment Loss (AL) that aligns class-conditional generation with unconditional generation at large timesteps. This second loss makes the denoising process insensitive to class conditions for the initial steps, which enriches tail classes through knowledge sharing from head classes. We successfully leverage contrastive learning and conditional-unconditional alignment for class-imbalanced diffusion models. Our framework is easy to implement as demonstrated on both U-Net based architecture and Diffusion Transformer. Our method outperforms vanilla denoising diffusion probabilistic models, score-based diffusion model, and alternative methods for class-imbalanced image generation across various datasets, in particular ImageNet-LT with 256x256 resolution. </p>
<blockquote>
<p>ç±»æ¡ä»¶å›¾åƒåˆæˆè®­ç»ƒæ•°æ®é€šå¸¸å‘ˆç°å‡ºé•¿å°¾åˆ†å¸ƒï¼Œå°¾ç±»å›¾åƒæœ‰é™ã€‚è¿™ç§ä¸å¹³è¡¡ä¼šå¯¼è‡´æ¨¡å¼å´©æºƒï¼Œå¹¶é™ä½å°¾ç±»åˆæˆå›¾åƒçš„å¤šæ ·æ€§ã€‚å¯¹äºåœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šè®­ç»ƒçš„ç±»æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨ä¸æŸå®³å¤´ç±»å›¾åƒè´¨é‡çš„æƒ…å†µä¸‹ï¼Œæé«˜å°¾ç±»å›¾åƒçš„å¤šæ ·æ€§å’Œä¿çœŸåº¦ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸¤ä¸ªç®€å•ä½†é«˜æ•ˆçš„æŸå¤±å‡½æ•°æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ©ç”¨è´Ÿæ ·æœ¬çš„æ— ç›‘ç£å¯¹æ¯”æŸå¤±ï¼ˆUCLï¼‰ï¼Œä»¥å¢åŠ åˆæˆå›¾åƒä¹‹é—´çš„è·ç¦»&#x2F;å·®å¼‚ã€‚è¿™ç§æ­£åˆ™åŒ–ä¸æ‰¹é‡é‡é‡‡æ ·çš„æ ‡å‡†æŠ€å·§ç›¸ç»“åˆï¼Œä»¥è¿›ä¸€æ­¥å¤šæ ·åŒ–å°¾ç±»å›¾åƒã€‚æˆ‘ä»¬çš„ç¬¬äºŒä¸ªæŸå¤±æ˜¯å¯¹é½æŸå¤±ï¼ˆALï¼‰ï¼Œå®ƒå°†ç±»æ¡ä»¶ç”Ÿæˆä¸æ— æ¡ä»¶ç”Ÿæˆåœ¨å¤§æ—¶é—´æ­¥é•¿ä¸Šå¯¹é½ã€‚è¿™ç§æŸå¤±ä½¿å¾—å»å™ªè¿‡ç¨‹å¯¹åˆå§‹æ­¥éª¤çš„ç±»åˆ«æ¡ä»¶ä¸æ•æ„Ÿï¼Œé€šè¿‡ä»å¤´ç±»å…±äº«çŸ¥è¯†æ¥ä¸°å¯Œå°¾ç±»ã€‚æˆ‘ä»¬æˆåŠŸåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å’Œæ¡ä»¶-æ— æ¡ä»¶å¯¹é½æŠ€æœ¯ï¼Œå®ç°äº†ç±»ä¸å¹³è¡¡æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ˜“äºå®ç°ï¼Œå·²åœ¨U-Netæ¶æ„å’ŒDiffusion Transformerä¸Šå¾—åˆ°éªŒè¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šäº†æ ‡å‡†çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹å’Œé’ˆå¯¹ç±»ä¸å¹³è¡¡å›¾åƒç”Ÿæˆçš„æ›¿ä»£æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†è¾¨ç‡ä¸º256x256çš„ImageNet-LTæ•°æ®é›†ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09052v2">PDF</a> 20 pages, 11 figures</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç±»åˆ«æ¡ä»¶å›¾åƒåˆæˆä¸­è®­ç»ƒæ•°æ®å‘ˆç°é•¿å°¾åˆ†å¸ƒçš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡å¼•å…¥ä¸¤ç§æŸå¤±å‡½æ•°ä¼˜åŒ–äº†ç±»ä¸å¹³è¡¡æ‰©æ•£æ¨¡å‹çš„å°¾ç±»å›¾åƒç”Ÿæˆæ•ˆæœã€‚ç¬¬ä¸€ç§æ˜¯æ— ç›‘ç£å¯¹æ¯”æŸå¤±ï¼ˆUCLï¼‰ï¼Œåˆ©ç”¨è´Ÿæ ·æœ¬å¢åŠ åˆæˆå›¾åƒé—´çš„è·ç¦»&#x2F;å·®å¼‚ï¼›ç¬¬äºŒç§æ˜¯å¯¹é½æŸå¤±ï¼ˆALï¼‰ï¼Œåœ¨å¤§æ­¥é•¿æ—¶å®ç°ç±»æ¡ä»¶ç”Ÿæˆä¸æ— æ¡ä»¶ç”Ÿæˆçš„å¯¹é½ã€‚è¯¥ç ”ç©¶æˆåŠŸè¿ç”¨å¯¹æ¯”å­¦ä¹ å’Œæ¡ä»¶-æ— æ¡ä»¶å¯¹é½ç­–ç•¥ï¼Œæé«˜äº†ç±»ä¸å¹³è¡¡æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œé€‚ç”¨äºU-Netæ¶æ„å’ŒDiffusion Transformerï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹ç±»åˆ«æ¡ä»¶å›¾åƒåˆæˆä¸­çš„ç±»ä¸å¹³è¡¡é—®é¢˜ï¼Œä¼˜åŒ–äº†å°¾ç±»å›¾åƒçš„ç”Ÿæˆã€‚</li>
<li>å¼•å…¥ä¸¤ç§æŸå¤±å‡½æ•°ï¼šæ— ç›‘ç£å¯¹æ¯”æŸå¤±ï¼ˆUCLï¼‰å’Œå¯¹é½æŸå¤±ï¼ˆALï¼‰ã€‚</li>
<li>UCLåˆ©ç”¨è´Ÿæ ·æœ¬å¢åŠ åˆæˆå›¾åƒé—´çš„å·®å¼‚ï¼Œé…åˆæ‰¹å¤„ç†é‡é‡‡æ ·æŠ€å·§è¿›ä¸€æ­¥å¤šæ ·åŒ–å°¾ç±»å›¾åƒã€‚</li>
<li>ALåœ¨å¤§æ­¥é•¿æ—¶å®ç°ç±»æ¡ä»¶ç”Ÿæˆä¸æ— æ¡ä»¶ç”Ÿæˆçš„å¯¹é½ï¼Œä½¿å»å™ªè¿‡ç¨‹å¯¹åˆæ­¥æ¡ä»¶ä¸æ•æ„Ÿï¼Œé€šè¿‡å¤´éƒ¨çŸ¥è¯†ä¸°å¯Œå°¾éƒ¨ç±»åˆ«ã€‚</li>
<li>ç ”ç©¶æˆåŠŸå°†å¯¹æ¯”å­¦ä¹ å’Œæ¡ä»¶-æ— æ¡ä»¶å¯¹é½ç­–ç•¥åº”ç”¨äºç±»ä¸å¹³è¡¡æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æ–¹æ³•åœ¨U-Netæ¶æ„å’ŒDiffusion Transformerä¸Šæ˜“äºå®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8f598de5768de5299dae0ae63bc3c271~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293209&auth_key=1762293209-0-0-81a90251e579a0cb6b141c38956b1b50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ff01e4ef7994745c36c4997a2bc21ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293216&auth_key=1762293216-0-0-600f23bae12371b050158a9460b26425&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f3d84d5c6fc318e0b2c1b4da98317fbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293223&auth_key=1762293223-0-0-b4ae185d60664d53a11a692fa5fffa73&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays"><a href="#AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays" class="headerlink" title="AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays"></a>AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays</h2><p><strong>Authors:Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨åŒ…æ‹¬åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨å†…çš„å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºCLIPæ¨¡å‹çš„å…¬å¹³æ€§å…³æ³¨ï¼ŒåŒ…æ‹¬äººå£ç»Ÿè®¡åè§ï¼Œå¹¶æœªå¾—åˆ°è¶³å¤Ÿçš„é‡è§†ã€‚è¿™ç§ç–å¿½å¯¼è‡´äº†å…³é”®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä¸ç§æ—å’Œæ€§åˆ«ç›¸å…³çš„é—®é¢˜ï¼Œè¿›è€Œå¯¼è‡´è¯Šæ–­ç»“æœçš„ä¸å…¬å¹³å’Œå¯¹ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“çš„å¯é æ€§é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AdFair-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨å¯¹æŠ—æ€§ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§çš„æ–°å‹æ¡†æ¶ï¼Œä»è€Œå‡è½»è™šå‡å…³è”ï¼Œæé«˜é¢„æµ‹å…¬å¹³æ€§ã€‚æˆ‘ä»¬åœ¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜AdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­ï¼Œæ˜¾è‘—æé«˜äº†å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœä¸ºCLIPåŸºåŒ»å­¦è¯Šæ–­æ¨¡å‹ä¸­çš„å…¬å¹³æ„è¯†å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯CXRåˆ†æï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23467v3">PDF</a> This preprint has been accepted by MICCAI 2025</p>
<p><strong>Summary</strong><br>     å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸­å­˜åœ¨å…¬å¹³æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä¸ç§æ—å’Œæ€§åˆ«ç›¸å…³çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºAdFair-CLIPæ¡†æ¶ï¼Œé‡‡ç”¨å¯¹æŠ—ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§ï¼Œå‡å°‘é”™è¯¯å…³è”ï¼Œæé«˜é¢„æµ‹å…¬å¹³æ€§ã€‚åœ¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æé«˜å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºCLIPåŸºç¡€çš„åŒ»ç–—è¯Šæ–­æ¨¡å‹çš„å…¬å¹³æ€§å­¦ä¹ æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸­å­˜åœ¨å…¬å¹³æ€§é—®é¢˜ã€‚</li>
<li>å…¬å¹³æ€§é—®é¢˜å¯èƒ½å¯¼è‡´è¯Šæ–­ç»“æœçš„ä¸å…¬å¹³å’Œå¯¹å°‘æ•°ç¾¤ä½“çš„è¯¯è¯Šã€‚</li>
<li>AdFair-CLIPæ¡†æ¶è¢«æå‡ºæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œé‡‡ç”¨å¯¹æŠ—ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§ã€‚</li>
<li>é€šè¿‡åœ¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒï¼ŒAdFair-CLIPæ˜¾è‘—æé«˜äº†å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>AdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>AdFair-CLIPä¸ºCLIPåŸºç¡€çš„åŒ»ç–—è¯Šæ–­æ¨¡å‹çš„å…¬å¹³æ€§å­¦ä¹ æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d34913087e59aa7a1ef4030d0128144b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293229&auth_key=1762293229-0-0-6168f49e0ae9611c147636f1c96eb7b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fb7463fac7f556fcb5c554575826a10~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293236&auth_key=1762293236-0-0-9dcf07325214623620624c6b96dab001&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6cf06751428b9c42c648bbaaf5f8bea1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293243&auth_key=1762293243-0-0-1406752343fb0c089a1e3244192f9b73&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MoralCLIP-Contrastive-Alignment-of-Vision-and-Language-Representations-with-Moral-Foundations-Theory"><a href="#MoralCLIP-Contrastive-Alignment-of-Vision-and-Language-Representations-with-Moral-Foundations-Theory" class="headerlink" title="MoralCLIP: Contrastive Alignment of Vision-and-Language Representations   with Moral Foundations Theory"></a>MoralCLIP: Contrastive Alignment of Vision-and-Language Representations   with Moral Foundations Theory</h2><p><strong>Authors:Ana Carolina Condez, Diogo Tavares, JoÃ£o MagalhÃ£es</strong></p>
<p>Recent advances in vision-language models have enabled rich semantic understanding across modalities. However, these encoding methods lack the ability to interpret or reason about the moral dimensions of content-a crucial aspect of human cognition. In this paper, we address this gap by introducing MoralCLIP, a novel embedding representation method that extends multimodal learning with explicit moral grounding based on Moral Foundations Theory (MFT). Our approach integrates visual and textual moral cues into a unified embedding space, enabling cross-modal moral alignment. MoralCLIP is grounded on the multi-label dataset Social-Moral Image Database to identify co-occurring moral foundations in visual content. For MoralCLIP training, we design a moral data augmentation strategy to scale our annotated dataset to 15,000 image-text pairs labeled with MFT-aligned dimensions. Our results demonstrate that explicit moral supervision improves both unimodal and multimodal understanding of moral content, establishing a foundation for morally-aware AI systems capable of recognizing and aligning with human moral values. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥å·²ç»å®ç°äº†è·¨æ¨¡æ€çš„ä¸°å¯Œè¯­ä¹‰ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›ç¼–ç æ–¹æ³•ç¼ºä¹è§£é‡Šæˆ–æ¨ç†å†…å®¹é“å¾·ç»´åº¦çš„èƒ½åŠ›â€”â€”è¿™æ˜¯äººç±»è®¤çŸ¥çš„å…³é”®æ–¹é¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥MoralCLIPæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé“å¾·åŸºç¡€ç†è®ºï¼ˆMFTï¼‰çš„æ–°å‹åµŒå…¥è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒæ‰©å±•äº†å¤šæ¨¡æ€å­¦ä¹ ï¼Œå®ç°äº†æ˜ç¡®çš„é“å¾·å®šä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è§†è§‰å’Œæ–‡æœ¬é“å¾·çº¿ç´¢æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ï¼Œå®ç°äº†è·¨æ¨¡æ€é“å¾·å¯¹é½ã€‚MoralCLIPå»ºç«‹åœ¨å¤šæ ‡ç­¾æ•°æ®é›†Social-Moral Image Databaseä¸Šï¼Œç”¨äºè¯†åˆ«è§†è§‰å†…å®¹ä¸­å…±åŒå­˜åœ¨çš„é“å¾·åŸºç¡€ã€‚ä¸ºäº†è®­ç»ƒMoralCLIPï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é“å¾·æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä»¥å°†æˆ‘ä»¬çš„æ ‡æ³¨æ•°æ®é›†æ‰©å±•åˆ°15000ä¸ªå›¾åƒæ–‡æœ¬å¯¹ï¼Œè¿™äº›å¯¹éƒ½ä½¿ç”¨ä¸MFTå¯¹é½çš„ç»´åº¦è¿›è¡Œæ ‡æ³¨ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ˜ç¡®çš„é“å¾·ç›‘ç£æé«˜äº†å¯¹é“å¾·å†…å®¹çš„å•æ¨¡æ€å’Œå¤šæ¨¡æ€ç†è§£ï¼Œä¸ºèƒ½å¤Ÿè¯†åˆ«å’Œç¬¦åˆäººç±»é“å¾·ä»·å€¼è§‚çš„å…·æœ‰é“å¾·æ„è¯†çš„AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05696v2">PDF</a> Updated version: corresponds to the ACM MM â€˜25 published paper and   includes full appendix material</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†MoralCLIPï¼Œä¸€ç§åŸºäºé“å¾·åŸºç¡€ç†è®ºï¼ˆMFTï¼‰çš„å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¼¥è¡¥å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§£è¯»é“å¾·ç»´åº¦å†…å®¹æ–¹é¢çš„ä¸è¶³ã€‚MoralCLIPå°†è§†è§‰å’Œæ–‡æœ¬é“å¾·çº¿ç´¢æ•´åˆåˆ°ç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ï¼Œå®ç°è·¨æ¨¡æ€é“å¾·å¯¹é½ã€‚è¯¥ç ”ç©¶ä½¿ç”¨ç¤¾ä¼šé“å¾·å›¾åƒæ•°æ®åº“å¤šæ ‡ç­¾æ•°æ®é›†æ¥è¯†åˆ«è§†è§‰å†…å®¹ä¸­å…±åŒå­˜åœ¨çš„é“å¾·åŸºç¡€ã€‚é€šè¿‡è®¾è®¡é“å¾·æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå°†æ ‡æ³¨æ•°æ®é›†æ‰©å±•åˆ°15000ä¸ªå›¾åƒæ–‡æœ¬å¯¹ï¼Œä»¥å®ç°ä¸MFTå¯¹é½çš„ç»´åº¦æ ‡æ³¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ˜ç¡®çš„é“å¾·ç›‘ç£èƒ½æé«˜å•æ¨¡æ€å’Œå¤šæ¨¡æ€å¯¹é“å¾·å†…å®¹çš„ç†è§£ï¼Œä¸ºå»ºç«‹èƒ½å¤Ÿè¯†åˆ«å¹¶ç¬¦åˆäººç±»é“å¾·ä»·å€¼è§‚çš„é“å¾·æ„ŸçŸ¥AIç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>ä»‹ç»äº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§£è¯»é“å¾·ç»´åº¦å†…å®¹æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>æå‡ºäº†MoralCLIPï¼Œä¸€ç§åŸºäºé“å¾·åŸºç¡€ç†è®ºï¼ˆMFTï¼‰çš„å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>MoralCLIPæ•´åˆäº†è§†è§‰å’Œæ–‡æœ¬é“å¾·çº¿ç´¢åˆ°ç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ï¼Œå®ç°è·¨æ¨¡æ€é“å¾·å¯¹é½ã€‚</li>
<li>ä½¿ç”¨ç¤¾ä¼šé“å¾·å›¾åƒæ•°æ®åº“å¤šæ ‡ç­¾æ•°æ®é›†æ¥è¯†åˆ«è§†è§‰å†…å®¹ä¸­çš„é“å¾·åŸºç¡€ã€‚</li>
<li>é€šè¿‡é“å¾·æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ‰©å±•äº†æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>æ˜ç¡®çš„é“å¾·ç›‘ç£æé«˜äº†å•æ¨¡æ€å’Œå¤šæ¨¡æ€å¯¹é“å¾·å†…å®¹çš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05696">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6378fc4f86cd215425dcd9ce08cfd533~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293251&auth_key=1762293251-0-0-d801606cdb394be2993d35d67cc524ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3be3ee87c2ccd7fa982ae5bf4bc6edbd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293259&auth_key=1762293259-0-0-a0ae09d3dcb69817fa3e66e4f220e23f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-853947994ddd910f4c5c867797532869~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293266&auth_key=1762293266-0-0-d5a8a7c1063aa5db13594ee26b1e3064&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-38a5d5d3e0e1459398d3773b0ab8d9d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293273&auth_key=1762293273-0-0-36408c41468bda1222108ddd359de991&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-53a3e2d65cb507f7b0dca4aabdb3ee6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293279&auth_key=1762293279-0-0-46db7f1ed2ef8f24bfd524dc27c1cb52&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Decoupling-Contrastive-Decoding-Robust-Hallucination-Mitigation-in-Multimodal-Large-Language-Models"><a href="#Decoupling-Contrastive-Decoding-Robust-Hallucination-Mitigation-in-Multimodal-Large-Language-Models" class="headerlink" title="Decoupling Contrastive Decoding: Robust Hallucination Mitigation in   Multimodal Large Language Models"></a>Decoupling Contrastive Decoding: Robust Hallucination Mitigation in   Multimodal Large Language Models</h2><p><strong>Authors:Wei Chen, Xin Yan, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, Long Chen</strong></p>
<p>Although multimodal large language models (MLLMs) exhibit remarkable reasoning capabilities on complex multimodal understanding tasks, they still suffer from the notorious hallucination issue: generating outputs misaligned with obvious visual or factual evidence. Currently, training-based solutions, like direct preference optimization (DPO), leverage paired preference data to suppress hallucinations. However, they risk sacrificing general reasoning capabilities due to the likelihood displacement. Meanwhile, training-free solutions, like contrastive decoding, achieve this goal by subtracting the estimated hallucination pattern from a distorted input. Yet, these handcrafted perturbations (e.g., add noise to images) may poorly capture authentic hallucination patterns. To avoid these weaknesses of existing methods, and realize robust hallucination mitigation (i.e., maintaining general reasoning performance), we propose a novel framework: Decoupling Contrastive Decoding (DCD). Specifically, DCD decouples the learning of positive and negative samples in preference datasets, and trains separate positive and negative image projections within the MLLM. The negative projection implicitly models real hallucination patterns, which enables vision-aware negative images in the contrastive decoding inference stage. Our DCD alleviates likelihood displacement by avoiding pairwise optimization and generalizes robustly without handcrafted degradation. Extensive ablations across hallucination benchmarks and general reasoning tasks demonstrate the effectiveness of DCD, i.e., it matches DPOâ€™s hallucination suppression while preserving general capabilities and outperforms the handcrafted contrastive decoding methods. </p>
<blockquote>
<p>è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚çš„å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶å—åˆ°è‘—åçš„å¹»è§‰é—®é¢˜çš„å›°æ‰°ï¼šç”Ÿæˆçš„è¾“å‡ºä¸æ˜æ˜¾çš„è§†è§‰æˆ–äº‹å®è¯æ®ä¸ä¸€è‡´ã€‚ç›®å‰ï¼ŒåŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œåˆ©ç”¨é…å¯¹åå¥½æ•°æ®æ¥æŠ‘åˆ¶å¹»è§‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯èƒ½ä¼šå› ä¸ºå¯èƒ½æ€§ä½ç§»è€Œç‰ºç‰²ä¸€èˆ¬çš„æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæ— è®­ç»ƒçš„æ–¹æ³•ï¼Œå¦‚å¯¹æ¯”è§£ç ï¼Œé€šè¿‡ä»æ‰­æ›²çš„è¾“å…¥ä¸­å‡å»ä¼°è®¡çš„å¹»è§‰æ¨¡å¼æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ‰‹å·¥åˆ¶ä½œçš„æ‰°åŠ¨ï¼ˆä¾‹å¦‚ç»™å›¾åƒæ·»åŠ å™ªå£°ï¼‰å¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ•æ‰çœŸå®çš„å¹»è§‰æ¨¡å¼ã€‚ä¸ºäº†é¿å…ç°æœ‰æ–¹æ³•çš„è¿™äº›å¼±ç‚¹ï¼Œå¹¶å®ç°ç¨³å¥çš„å¹»è§‰ç¼“è§£ï¼ˆå³ä¿æŒä¸€èˆ¬æ¨ç†æ€§èƒ½ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼šè§£è€¦å¯¹æ¯”è§£ç ï¼ˆDCDï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒDCDè§£è€¦äº†åå¥½æ•°æ®é›†ä¸­æ­£æ ·å’Œè´Ÿæ ·çš„å­¦ä¹ ï¼Œå¹¶åœ¨MLLMå†…è®­ç»ƒäº†å•ç‹¬çš„æ­£è´Ÿå›¾åƒæŠ•å½±ã€‚è´ŸæŠ•å½±éšå¼åœ°æ¨¡æ‹Ÿäº†çœŸå®çš„å¹»è§‰æ¨¡å¼ï¼Œè¿™ä½¿å¾—åœ¨å¯¹æ¯”è§£ç æ¨ç†é˜¶æ®µèƒ½å¤Ÿä½¿ç”¨è§†è§‰æ„ŸçŸ¥çš„è´Ÿå›¾åƒã€‚æˆ‘ä»¬çš„DCDé€šè¿‡é¿å…æˆå¯¹ä¼˜åŒ–æ¥ç¼“è§£å¯èƒ½æ€§ä½ç§»ï¼Œå¹¶åœ¨ä¸ä½¿ç”¨æ‰‹å·¥é€€åŒ–çš„æƒ…å†µä¸‹å®ç°ç¨³å¥æ³›åŒ–ã€‚åœ¨å¹»è§‰åŸºå‡†æµ‹è¯•å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šçš„å¹¿æ³›æ¶ˆèå®éªŒè¯æ˜äº†DCDçš„æœ‰æ•ˆæ€§ï¼Œå³å®ƒåœ¨ä¿æŒä¸€èˆ¬èƒ½åŠ›çš„åŒæ—¶ï¼Œèƒ½å¤Ÿè¾¾åˆ°ä¸DPOç›¸å½“çš„å¹»è§‰æŠ‘åˆ¶æ•ˆæœï¼Œå¹¶è¶…è¶Šäº†æ‰‹å·¥å¯¹æ¯”è§£ç æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08809v2">PDF</a> 17 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çš„å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨è‘—åçš„è™šæ„é—®é¢˜ï¼Œå³ç”Ÿæˆçš„è¾“å‡ºä¸æ˜æ˜¾çš„è§†è§‰æˆ–äº‹å®è¯æ®ä¸ä¸€è‡´ã€‚ç°æœ‰æ–¹æ³•å¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åˆ©ç”¨é…å¯¹åå¥½æ•°æ®æ¥æŠ‘åˆ¶è™šæ„ï¼Œä½†å¯èƒ½ç‰ºç‰²ä¸€èˆ¬æ¨ç†èƒ½åŠ›ã€‚è€Œå¯¹æ¯”è§£ç ç­‰æ— è®­ç»ƒæ–¹æ³•é€šè¿‡ä»å¤±çœŸè¾“å…¥ä¸­å‡å»ä¼°è®¡çš„è™šæ„æ¨¡å¼æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œä½†æ‰‹å·¥åˆ¶ä½œçš„æ‰°åŠ¨å¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ•æ‰çœŸå®çš„è™šæ„æ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„æ¡†æ¶ï¼šè§£è€¦å¯¹æ¯”è§£ç ï¼ˆDCDï¼‰ï¼Œæ—¨åœ¨å®ç°ç¨³å¥çš„è™šæ„ç¼“è§£ï¼ŒåŒæ—¶ä¿æŒä¸€èˆ¬æ¨ç†æ€§èƒ½ã€‚DCDé€šè¿‡è§£è€¦åå¥½æ•°æ®é›†ä¸­æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„å­¦ä¹ ï¼Œå¹¶åœ¨MLLMå†…è®­ç»ƒå•ç‹¬çš„æ­£è´Ÿå›¾åƒæŠ•å½±ã€‚è´ŸæŠ•å½±éšå«åœ°æ¨¡æ‹ŸçœŸå®çš„è™šæ„æ¨¡å¼ï¼Œä»è€Œåœ¨å¯¹æ¯”è§£ç æ¨æ–­é˜¶æ®µå®ç°è§†è§‰æ„ŸçŸ¥çš„è´Ÿå›¾åƒã€‚æˆ‘ä»¬çš„DCDé€šè¿‡é¿å…é…å¯¹ä¼˜åŒ–å‡è½»äº†å¯èƒ½æ€§ä½ç§»ï¼Œå¹¶å®ç°äº†ç¨³å¥çš„æ³›åŒ–ï¼Œæ— éœ€æ‰‹å·¥åˆ¶ä½œçš„é€€åŒ–ã€‚å¹¿æ³›çš„æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒDCDåœ¨æŠ‘åˆ¶è™šæ„çš„åŒæ—¶ä¿æŒäº†ä¸€èˆ¬èƒ½åŠ›ï¼Œå¹¶ä¼˜äºæ‰‹å·¥å¯¹æ¯”è§£ç æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨è™šæ„é—®é¢˜ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ç­‰ç°æœ‰æ–¹æ³•å¯èƒ½ç‰ºç‰²ä¸€èˆ¬æ¨ç†èƒ½åŠ›æ¥æŠ‘åˆ¶è™šæ„ã€‚</li>
<li>å¯¹æ¯”è§£ç ç­‰æ— è®­ç»ƒæ–¹æ³•ä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„æ‰°åŠ¨ï¼Œå¯èƒ½æ— æ³•æœ‰æ•ˆæ•æ‰çœŸå®è™šæ„æ¨¡å¼ã€‚</li>
<li>æå‡ºæ–°çš„æ¡†æ¶â€”â€”è§£è€¦å¯¹æ¯”è§£ç ï¼ˆDCDï¼‰ï¼Œæ—¨åœ¨å®ç°ç¨³å¥çš„è™šæ„ç¼“è§£å¹¶ç»´æŒä¸€èˆ¬æ¨ç†æ€§èƒ½ã€‚</li>
<li>DCDé€šè¿‡è§£è€¦æ­£è´Ÿé¢æ ·æœ¬å­¦ä¹ ï¼Œè®­ç»ƒå•ç‹¬çš„æ­£è´Ÿå›¾åƒæŠ•å½±æ¥æŠ‘åˆ¶è™šæ„ã€‚</li>
<li>DCDé€šè¿‡é¿å…é…å¯¹ä¼˜åŒ–å‡è½»å¯èƒ½æ€§ä½ç§»å¹¶å®ç°ç¨³å¥æ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f1cc2e280aab6c54f14d944b16ce8050~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293287&auth_key=1762293287-0-0-529d1a53252720dc738f32feeba704d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5ae5d047775c0970cbb8104eab653d26~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293294&auth_key=1762293294-0-0-935edcefc942cbdd368d03d2cc55e2b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d63c5b057dd626a3559a95dc1c4f376c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293301&auth_key=1762293301-0-0-d03a88d922b745024464bf0d74e93007&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Data-Matters-Most-Auditing-Social-Bias-in-Contrastive-Vision-Language-Models"><a href="#Data-Matters-Most-Auditing-Social-Bias-in-Contrastive-Vision-Language-Models" class="headerlink" title="Data Matters Most: Auditing Social Bias in Contrastive Vision Language   Models"></a>Data Matters Most: Auditing Social Bias in Contrastive Vision Language   Models</h2><p><strong>Authors:Zahraa Al Sahili, Ioannis Patras, Matthew Purver</strong></p>
<p>Vision-language models (VLMs) deliver strong zero-shot recognition but frequently inherit social biases from their training data. We systematically disentangle three design factors â€“ model size, training-data scale, and training-data source â€“ by comparing CLIP and OpenCLIP, two models that share an identical contrastive objective yet differ in encoder width and in the image-text corpora on which they are pre-trained (400M proprietary pairs vs. 400M&#x2F;2B LAION). Across balanced face-analysis benchmarks, enlarging the encoder reduces gender skew in CLIP but amplifies both gender and racial skew in OpenCLIP; increasing the LAION corpus from 400M to 2B further increases OpenCLIP bias. At matched model and data budgets, substituting proprietary data with LAION improves gender fairness while increasing racial skew, underscoring data source as the primary driver of bias patterns. We also evaluate three post-hoc, test-time debiasing strategies â€“ Bias Prompts, Prompt Array, and SANER. Debiasing reduces but does not eliminate harm, and its effectiveness is source- and size-dependent: Bias Prompts most effectively reduce gender skew in CLIP at smaller model sizes, whereas Prompt Array and SANER more reliably reduce racial skew in OpenCLIP; scaling LAION reconfigures which method is most fair. Taken together, these findings challenge the assumption that bigger models or datasets are automatically fairer and foreground training data source as the key determinant of both bias and mitigation efficacy. We release code and evaluation scripts to enable transparent, reproducible auditing of future VLMs. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½ç„¶å¯ä»¥å®ç°å¼ºå¤§çš„é›¶æ ·æœ¬è¯†åˆ«ï¼Œä½†ç»å¸¸ä»è®­ç»ƒæ•°æ®ä¸­ç»§æ‰¿ç¤¾ä¼šåè§ã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒCLIPå’ŒOpenCLIPè¿™ä¸¤ä¸ªæ¨¡å‹ï¼Œç³»ç»Ÿåœ°åˆ†æäº†æ¨¡å‹å¤§å°ã€è®­ç»ƒæ•°æ®è§„æ¨¡å’Œè®­ç»ƒæ•°æ®æ¥æºä¸‰ä¸ªè®¾è®¡å› ç´ ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹å…·æœ‰ç›¸åŒçš„å¯¹æ¯”ç›®æ ‡ï¼Œä½†åœ¨ç¼–ç å™¨å®½åº¦å’Œé¢„è®­ç»ƒçš„å›¾åƒæ–‡æœ¬è¯­æ–™åº“ï¼ˆåˆ†åˆ«ä¸º4äº¿ä¸ªä¸“æœ‰é…å¯¹å’Œé«˜è¾¾å››åäº¿è‡³ä¸¤äº¿ä¸ªLAIONæ•°æ®å¯¹ï¼‰ä¸Šæœ‰æ‰€ä¸åŒã€‚åœ¨å¹³è¡¡é¢éƒ¨åˆ†æåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰©å¤§ç¼–ç å™¨è§„æ¨¡å‡å°‘äº†CLIPä¸­çš„æ€§åˆ«åè§ï¼Œä½†åœ¨OpenCLIPä¸­å´æ”¾å¤§äº†æ€§åˆ«å’Œç§æ—åè§ï¼›å°†LAIONè¯­æ–™åº“ä»å››äº¿å¢åŠ åˆ°ä¸¤äº¿è¿›ä¸€æ­¥å¢åŠ äº†OpenCLIPåè§ã€‚åœ¨åŒ¹é…æ¨¡å‹å’Œæ•°æ®åº“é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œç”¨LAIONæ•°æ®æ›¿ä»£ä¸“æœ‰æ•°æ®æé«˜äº†æ€§åˆ«å…¬å¹³æ€§ï¼Œä½†å¢åŠ äº†ç§æ—åè§ï¼Œè¿™çªæ˜¾å‡ºè®­ç»ƒæ•°æ®æ¥æºæ˜¯åè§æ¨¡å¼çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†ä¸‰ç§äº‹åã€æµ‹è¯•æ—¶çš„å»åç­–ç•¥ï¼Œå³åç½®æç¤ºã€æç¤ºæ•°ç»„å’ŒSANERã€‚å»åå¯ä»¥å‡å°‘ä½†å¹¶ä¸èƒ½å®Œå…¨æ¶ˆé™¤ä¼¤å®³ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºæ¥æºå’Œè§„æ¨¡ï¼šåç½®æç¤ºåœ¨å°æ¨¡å‹è§„æ¨¡ä¸‹æ›´æœ‰æ•ˆåœ°å‡å°‘CLIPä¸­çš„æ€§åˆ«åè§ï¼Œè€Œæç¤ºæ•°ç»„å’ŒSANERæ›´å¯é åœ°å‡å°‘OpenCLIPä¸­çš„ç§æ—åè§ï¼›æ‰©å¤§LAIONè¯­æ–™åº“é‡æ–°å®šä¹‰äº†å“ªç§æ–¹æ³•æ›´ä¸ºå…¬å¹³ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™äº›å‘ç°æŒ‘æˆ˜äº†å‡è®¾æ›´å¤§çš„æ¨¡å‹æˆ–æ•°æ®é›†ä¼šè‡ªåŠ¨å˜å¾—æ›´å…¬å¹³çš„è§‚ç‚¹ï¼Œå¹¶å°†è®­ç»ƒæ•°æ®æ¥æºä½œä¸ºå…³é”®å› ç´ æ¥å½±å“åè§å’Œç¼“è§£æ•ˆæœã€‚æˆ‘ä»¬å‘å¸ƒä»£ç å’Œè¯„ä¼°è„šæœ¬ï¼Œä»¥å®ç°å¯¹æœªæ¥è§†è§‰è¯­è¨€æ¨¡å‹çš„é€æ˜ã€å¯é‡å¤å®¡è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13223v6">PDF</a> Accepted at TMLR</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›¶æ ·æœ¬è¯†åˆ«ä¸­çš„è¡¨ç°åŠå…¶æ‰€ç»§æ‰¿çš„ç¤¾ä¼šåè§é—®é¢˜ã€‚é€šè¿‡å¯¹æ¯”CLIPå’ŒOpenCLIPä¸¤ä¸ªæ¨¡å‹ï¼Œæ–‡ç« åˆ†æäº†æ¨¡å‹è§„æ¨¡ã€è®­ç»ƒæ•°æ®è§„æ¨¡å’Œè®­ç»ƒæ•°æ®æ¥æºä¸‰ä¸ªè®¾è®¡å› ç´ å¦‚ä½•å½±å“æ¨¡å‹çš„æ€§åˆ«å’Œç§æ—åè§ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜è¯„ä¼°äº†ä¸‰ç§äº‹åã€æµ‹è¯•æ—¶çš„å»åç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼Œè®­ç»ƒæ•°æ®æ¥æºæ˜¯åè§çš„ä¸»è¦é©±åŠ¨å› ç´ ï¼Œè€Œå»åç­–ç•¥çš„æ•ˆæœå–å†³äºæ•°æ®æºå’Œè§„æ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨é›¶æ ·æœ¬è¯†åˆ«ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¼šç»§æ‰¿è®­ç»ƒæ•°æ®ä¸­çš„ç¤¾ä¼šåè§ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡ã€è®­ç»ƒæ•°æ®è§„æ¨¡å’Œè®­ç»ƒæ•°æ®æ¥æºæ˜¯å½±å“VLMsåè§çš„å…³é”®å› ç´ ã€‚</li>
<li>ç›¸æ¯”CLIPæ¨¡å‹ï¼ŒOpenCLIPåœ¨æ€§åˆ«åè§ä¸Šè¡¨ç°æ›´å·®ï¼Œè€Œåœ¨ç§æ—åè§ä¸Šè¡¨ç°æ›´å¥½ã€‚</li>
<li>æ•°æ®æ¥æºæ˜¯å½±å“åè§æ¨¡å¼çš„ä¸»è¦å› ç´ ï¼Œæ›¿ä»£ä¸“æœ‰æ•°æ®ä½¿ç”¨LAIONä¼šå¢åŠ æ€§åˆ«å…¬å¹³æ€§ä½†å¢åŠ ç§æ—åè§ã€‚</li>
<li>ä¸‰ç§å»åç­–ç•¥å¯é™ä½ä½†æ— æ³•å®Œå…¨æ¶ˆé™¤åè§ï¼Œå…¶æ•ˆæœå–å†³äºæ•°æ®æºå’Œè§„æ¨¡ã€‚</li>
<li>æ›´å¤§çš„æ¨¡å‹æˆ–æ•°æ®é›†å¹¶ä¸ä¸€å®šæ›´å…¬å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e3dd6acf9a3cb0802ad88c4426ac2d41~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293308&auth_key=1762293308-0-0-d153ef330b376ee05352c8d96c75d1fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8e30e149dac0c2e5adc9c61cd61e35aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293315&auth_key=1762293315-0-0-7ff44610029c620b7b815cd59ea7c116&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d0f90bbd3658233a51e726c7ddddd87b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762293503&auth_key=1762293503-0-0-46ca319e8b2b14c4c435c39bdaa01b00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Flip Learning Weakly Supervised Erase to Segment Nodules in Breast   Ultrasound
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-8dcf21e4ab860d40fa1af25369ab4f00~resize:0:q75.jpg?source=1f5c5e47&expiration=1762292016&auth_key=1762292016-0-0-68064410eabf122525835aad0ac7f7fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via   Ocular Cropping
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
