<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Who Made This? Fake Detection and Source Attribution with Diffusion   Features">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2409.18932v4/page_4_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="Who-Made-This-Fake-Detection-and-Source-Attribution-with-Diffusion-Features"><a href="#Who-Made-This-Fake-Detection-and-Source-Attribution-with-Diffusion-Features" class="headerlink" title="Who Made This? Fake Detection and Source Attribution with Diffusion   Features"></a>Who Made This? Fake Detection and Source Attribution with Diffusion   Features</h2><p><strong>Authors:Simone Bonechi, Paolo Andreini, Barbara Toniella Corradini</strong></p>
<p>The rapid progress of generative diffusion models has enabled the creation of synthetic images that are increasingly difficult to distinguish from real ones, raising concerns about authenticity, copyright, and misinformation. Existing supervised detectors often struggle to generalize across unseen generators, requiring extensive labeled data and frequent retraining. We introduce FRIDA (Fake-image Recognition and source Identification via Diffusion-features Analysis), a lightweight framework that leverages internal activations from a pre-trained diffusion model for deepfake detection and source generator attribution. A k-nearest-neighbor classifier applied to diffusion features achieves state-of-the-art cross-generator performance without fine-tuning, while a compact neural model enables accurate source attribution. These results show that diffusion representations inherently encode generator-specific patterns, providing a simple and interpretable foundation for synthetic image forensics. </p>
<blockquote>
<p>ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—åˆæˆå›¾åƒçš„åˆ›å»ºè¶Šæ¥è¶Šéš¾ä»¥ä¸çœŸå®å›¾åƒåŒºåˆ†å¼€ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹çœŸå®æ€§ã€ç‰ˆæƒå’Œè¯¯å¯¼ä¿¡æ¯çš„æ‹…å¿§ã€‚ç°æœ‰çš„ç›‘ç£æ£€æµ‹å™¨å¾€å¾€éš¾ä»¥åœ¨æœªè§è¿‡çš„ç”Ÿæˆå™¨ä¹‹é—´è¿›è¡Œæ¨å¹¿ï¼Œéœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®å’Œé¢‘ç¹çš„é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬å¼•å…¥äº†FRIDAï¼ˆé€šè¿‡æ‰©æ•£ç‰¹å¾åˆ†æè¿›è¡Œè™šå‡å›¾åƒè¯†åˆ«å’Œæ¥æºè¯†åˆ«ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»æ¥è¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹å’Œæ•°æ®æºç”Ÿæˆå™¨å½’å±ã€‚åº”ç”¨äºæ‰©æ•£ç‰¹å¾çš„kæœ€è¿‘é‚»åˆ†ç±»å™¨åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹å®ç°äº†è·¨ç”Ÿæˆå™¨çš„æœ€ä½³æ€§èƒ½ï¼Œè€Œç´§å‡‘çš„ç¥ç»ç½‘ç»œæ¨¡å‹åˆ™å®ç°äº†ç²¾ç¡®çš„æ•°æ®æºå½’å±ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£è¡¨ç¤ºæ³•æœ¬è´¨ä¸Šç¼–ç äº†ç‰¹å®šç”Ÿæˆå™¨çš„æ¨¡å¼ï¼Œä¸ºåˆæˆå›¾åƒå–è¯æä¾›äº†ç®€å•ä¸”å¯è§£é‡Šçš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27602v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”Ÿæˆæ‰©æ•£æ¨¡å‹è¿…é€Ÿè¿›æ­¥ï¼Œèƒ½ç”Ÿæˆè¶Šæ¥è¶Šéš¾ä»¥åŒºåˆ†çœŸå®æ€§çš„åˆæˆå›¾åƒï¼Œå¼•å‘å¯¹çœŸå®æ€§ã€ç‰ˆæƒå’Œè¯¯å¯¼ä¿¡æ¯çš„æ‹…å¿§ã€‚ç°æœ‰ç›‘ç£æ£€æµ‹å™¨å¾€å¾€éš¾ä»¥åœ¨æœªè§è¿‡çš„ç”Ÿæˆå™¨ä¸Šå®ç°é€šç”¨åŒ–ï¼Œéœ€è¦å¤§é‡æ ‡è®°æ•°æ®å’Œé¢‘ç¹é‡æ–°è®­ç»ƒã€‚æœ¬æ–‡ä»‹ç»FRIDAï¼ˆåŸºäºæ‰©æ•£ç‰¹å¾åˆ†æçš„å‡å›¾åƒè¯†åˆ«å’Œæ¥æºè¯†åˆ«ï¼‰ï¼Œå®ƒæ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»è¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹å’Œæ¥æºç”Ÿæˆå™¨å½’å› ã€‚åº”ç”¨æ‰©æ•£ç‰¹å¾çš„kæœ€è¿‘é‚»åˆ†ç±»å™¨åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹å®ç°äº†è·¨ç”Ÿæˆå™¨çš„å“è¶Šæ€§èƒ½ï¼Œè€Œç´§å‡‘çš„ç¥ç»ç½‘ç»œæ¨¡å‹åˆ™å¯å®ç°å‡†ç¡®çš„æ¥æºå½’å› ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£è¡¨ç¤ºæ³•å¤©ç„¶ç¼–ç ç”Ÿæˆå™¨ç‰¹å®šæ¨¡å¼ï¼Œä¸ºåˆæˆå›¾åƒå–è¯æä¾›äº†ç®€å•å’Œå¯è§£é‡Šçš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´åˆæˆå›¾åƒéš¾ä»¥åŒºåˆ†çœŸå®æ€§å’ŒçœŸå®æ€§æ‹…å¿§ã€‚</li>
<li>ç°æœ‰ç›‘ç£æ£€æµ‹å™¨åœ¨è·¨ç”Ÿæˆå™¨ä¸Šé€šç”¨åŒ–æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>FRIDAæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»è¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹å’Œæ¥æºå½’å› ã€‚</li>
<li>kæœ€è¿‘é‚»åˆ†ç±»å™¨åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„è·¨ç”Ÿæˆå™¨æ€§èƒ½ã€‚</li>
<li>ç´§å‡‘çš„ç¥ç»ç½‘ç»œæ¨¡å‹å¯å®ç°å‡†ç¡®çš„æ¥æºå½’å› ã€‚</li>
<li>æ‰©æ•£è¡¨ç¤ºæ³•å¤©ç„¶ç¼–ç ç”Ÿæˆå™¨ç‰¹å®šæ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27602v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27602v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27602v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27602v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27602v1/page_5_1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Understanding-the-Implicit-User-Intention-via-Reasoning-with-Large-Language-Model-for-Image-Editing"><a href="#Understanding-the-Implicit-User-Intention-via-Reasoning-with-Large-Language-Model-for-Image-Editing" class="headerlink" title="Understanding the Implicit User Intention via Reasoning with Large   Language Model for Image Editing"></a>Understanding the Implicit User Intention via Reasoning with Large   Language Model for Image Editing</h2><p><strong>Authors:Yijia Wang, Yiqing Shen, Weiming Chen, Zhihai He</strong></p>
<p>Existing image editing methods can handle simple editing instructions very well. To deal with complex editing instructions, they often need to jointly fine-tune the large language models (LLMs) and diffusion models (DMs), which involves very high computational complexity and training cost. To address this issue, we propose a new method, called \textbf{C}omplex \textbf{I}mage \textbf{E}diting via \textbf{L}LM \textbf{R}easoning (CIELR), which converts a complex user instruction into a set of simple and explicit editing actions, eliminating the need for jointly fine-tuning the large language models and diffusion models. Specifically, we first construct a structured semantic representation of the input image using foundation models. Then, we introduce an iterative update mechanism that can progressively refine this representation, obtaining a fine-grained visual representation of the image scene. This allows us to perform complex and flexible image editing tasks. Extensive experiments on the SmartEdit Reasoning Scenario Set show that our method surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating its superior preservation of regions that should remain consistent. Due to the limited number of samples of public datasets of complex image editing with reasoning, we construct a benchmark named CIEBench, containing 86 image samples, together with a metric specifically for reasoning-based image editing. CIELR also outperforms previous methods on this benchmark. The code and dataset are available at \href{<a target="_blank" rel="noopener" href="https://github.com/Jia-shao/Reasoning-Editing%7D%7Bhttps://github.com/Jia-shao/Reasoning-Editing%7D">https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}</a>. </p>
<blockquote>
<p>ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•èƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†ç®€å•çš„ç¼–è¾‘æŒ‡ä»¤ã€‚å¯¹äºå¤æ‚çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦è”åˆå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼Œè¿™æ¶‰åŠéå¸¸é«˜çš„è®¡ç®—å¤æ‚æ€§å’Œè®­ç»ƒæˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºé€šè¿‡LLMæ¨ç†çš„å¤æ‚å›¾åƒç¼–è¾‘ï¼ˆCIELRï¼‰ï¼Œå®ƒå°†å¤æ‚çš„ç”¨æˆ·æŒ‡ä»¤è½¬æ¢ä¸ºä¸€ç³»åˆ—ç®€å•æ˜ç¡®çš„ç¼–è¾‘æ“ä½œï¼Œä»è€Œæ— éœ€è”åˆå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨åŸºç¡€æ¨¡å‹æ„å»ºè¾“å…¥å›¾åƒçš„ç»“æ„åŒ–è¯­ä¹‰è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¿­ä»£æ›´æ–°æœºåˆ¶ï¼Œå¯ä»¥é€æ­¥ä¼˜åŒ–æ­¤è¡¨ç¤ºï¼Œè·å¾—å›¾åƒåœºæ™¯çš„ç²¾ç»†è§†è§‰è¡¨ç¤ºã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ‰§è¡Œå¤æ‚è€Œçµæ´»çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚åœ¨SmartEditæ¨ç†åœºæ™¯é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œæé«˜äº†9.955 dBï¼Œè¿™è¡¨æ˜å®ƒåœ¨ä¿æŒåº”ä¿æŒä¸€è‡´çš„åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç”±äºå…¬å…±æ•°æ®é›†ä¸­å¤æ‚å›¾åƒç¼–è¾‘æ¨ç†çš„æ ·æœ¬æ•°é‡æœ‰é™ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºCIEBenchçš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«86ä¸ªå›¾åƒæ ·æœ¬ï¼Œä»¥åŠä¸€ä¸ªä¸“é—¨é’ˆå¯¹åŸºäºæ¨ç†çš„å›¾åƒç¼–è¾‘çš„åº¦é‡æ ‡å‡†ã€‚CIELRåœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¹Ÿè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/Jia-shao/Reasoning-Editing]%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Jia-shao/Reasoning-Editing]æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27335v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘æ–¹æ³•â€”â€”CIELRï¼Œé€šè¿‡è¯­è¨€æ¨¡å‹æ¨ç†è¿›è¡Œå¤æ‚å›¾åƒç¼–è¾‘ã€‚è¯¥æ–¹æ³•å°†å¤æ‚çš„ç”¨æˆ·æŒ‡ä»¤è½¬åŒ–ä¸ºç®€å•æ˜ç¡®çš„ç¼–è¾‘åŠ¨ä½œï¼Œæ— éœ€è”åˆå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œé™ä½äº†è®¡ç®—å¤æ‚åº¦å’Œè®­ç»ƒæˆæœ¬ã€‚CIELRä½¿ç”¨åŸºç¡€æ¨¡å‹æ„å»ºè¾“å…¥å›¾åƒçš„ç»“æ„åŒ–è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶å¼•å…¥è¿­ä»£æ›´æ–°æœºåˆ¶ï¼Œé€æ­¥ä¼˜åŒ–è¡¨ç¤ºï¼Œè·å¾—å›¾åƒçš„ç²¾ç»†è§†è§‰è¡¨ç¤ºï¼Œä»è€Œæ‰§è¡Œå¤æ‚çµæ´»çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚åœ¨SmartEdit Reasoning Scenario Setä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCIELRåœ¨PSNRä¸Šè¾ƒç°æœ‰æŠ€æœ¯æé«˜äº†9.955 dBï¼Œå¹¶ä¸”åœ¨CIEBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CIELRå°†å¤æ‚ç¼–è¾‘æŒ‡ä»¤è½¬åŒ–ä¸ºç®€å•æ˜ç¡®çš„ç¼–è¾‘åŠ¨ä½œã€‚</li>
<li>æ— éœ€è”åˆå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨åŸºç¡€æ¨¡å‹æ„å»ºè¾“å…¥å›¾åƒçš„ç»“æ„åŒ–è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥è¿­ä»£æ›´æ–°æœºåˆ¶é€æ­¥ä¼˜åŒ–è¡¨ç¤ºã€‚</li>
<li>CIELRåœ¨SmartEdit Reasoning Scenario Setä¸Šçš„PSNRè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æ„å»ºäº†é’ˆå¯¹å¤æ‚å›¾åƒç¼–è¾‘çš„æ¨ç†åŸºå‡†æµ‹è¯•CIEBenchã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27335v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27335v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27335v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27335v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27335v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27335v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="H2-Cache-A-Novel-Hierarchical-Dual-Stage-Cache-for-High-Performance-Acceleration-of-Generative-Diffusion-Models"><a href="#H2-Cache-A-Novel-Hierarchical-Dual-Stage-Cache-for-High-Performance-Acceleration-of-Generative-Diffusion-Models" class="headerlink" title="H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance   Acceleration of Generative Diffusion Models"></a>H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance   Acceleration of Generative Diffusion Models</h2><p><strong>Authors:Mingyu Sung, Il-Min Kim, Sangseok Yun, Jae-Mo Kang</strong></p>
<p>Diffusion models have emerged as state-of-the-art in image generation, but their practical deployment is hindered by the significant computational cost of their iterative denoising process. While existing caching techniques can accelerate inference, they often create a challenging trade-off between speed and fidelity, suffering from quality degradation and high computational overhead. To address these limitations, we introduce H2-Cache, a novel hierarchical caching mechanism designed for modern generative diffusion model architectures. Our method is founded on the key insight that the denoising process can be functionally separated into a structure-defining stage and a detail-refining stage. H2-cache leverages this by employing a dual-threshold system, using independent thresholds to selectively cache each stage. To ensure the efficiency of our dual-check approach, we introduce pooled feature summarization (PFS), a lightweight technique for robust and fast similarity estimation. Extensive experiments on the Flux architecture demonstrate that H2-cache achieves significant acceleration (up to 5.08x) while maintaining image quality nearly identical to the baseline, quantitatively and qualitatively outperforming existing caching methods. Our work presents a robust and practical solution that effectively resolves the speed-quality dilemma, significantly lowering the barrier for the real-world application of high-fidelity diffusion models. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache">https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å·²è¾¾åˆ°äº†ä¸šç•Œå‰æ²¿æ°´å¹³ï¼Œä½†å…¶å®è·µéƒ¨ç½²å´å—åˆ°äº†å…¶è¿­ä»£å»å™ªè¿‡ç¨‹å·¨å¤§è®¡ç®—æˆæœ¬çš„é˜»ç¢ã€‚å°½ç®¡ç°æœ‰çš„ç¼“å­˜æŠ€æœ¯å¯ä»¥åŠ é€Ÿæ¨ç†ï¼Œä½†å®ƒä»¬å¾€å¾€éœ€è¦åœ¨é€Ÿåº¦å’Œä¿çœŸåº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œå¹¶å­˜åœ¨è´¨é‡ä¸‹é™å’Œè®¡ç®—å¼€é”€é«˜çš„ç¼ºç‚¹ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†H2-Cacheï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºç°ä»£ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¶æ„è®¾è®¡çš„åˆ†å±‚ç¼“å­˜æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨è¿™æ ·ä¸€ä¸ªå…³é”®è§è§£ä¹‹ä¸Šï¼šå»å™ªè¿‡ç¨‹å¯ä»¥åŠŸèƒ½æ€§åœ°åˆ†ä¸ºç»“æ„å®šä¹‰é˜¶æ®µå’Œç»†èŠ‚ä¼˜åŒ–é˜¶æ®µã€‚H2-cacheé€šè¿‡é‡‡ç”¨åŒé˜ˆå€¼ç³»ç»Ÿæ¥åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œä½¿ç”¨ç‹¬ç«‹çš„é˜ˆå€¼æœ‰é€‰æ‹©åœ°ç¼“å­˜æ¯ä¸ªé˜¶æ®µã€‚ä¸ºäº†ä¿è¯æˆ‘ä»¬åŒæ£€æ–¹æ³•çš„æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ± ç‰¹å¾æ€»ç»“ï¼ˆPFSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¨³å¥å¿«é€Ÿç›¸ä¼¼åº¦ä¼°è®¡çš„è½»é‡çº§æŠ€æœ¯ã€‚åœ¨Fluxæ¶æ„ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒH2-Cacheå®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿï¼ˆæœ€é«˜è¾¾5.08å€ï¼‰ï¼ŒåŒæ—¶ä¿æŒçš„å›¾åƒè´¨é‡ä¸åŸºçº¿å‡ ä¹ç›¸åŒï¼Œåœ¨é‡å’Œè´¨ä¸Šå‡ä¼˜äºç°æœ‰çš„ç¼“å­˜æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸€ä¸ªç¨³å¥å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°è§£å†³äº†é€Ÿåº¦ä¸è´¨é‡ä¹‹é—´çš„å›°å¢ƒï¼Œå¤§å¤§é™ä½äº†é«˜ä¿çœŸæ‰©æ•£æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„éšœç¢ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cacheæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27171v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå›¾åƒç”Ÿæˆé¢†åŸŸçš„æœ€å‰æ²¿æŠ€æœ¯ï¼Œä½†å…¶è¿­ä»£å»å™ªè¿‡ç¨‹çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œé˜»ç¢äº†å…¶å®è·µåº”ç”¨ã€‚ç°æœ‰ç¼“å­˜æŠ€æœ¯è™½èƒ½åŠ é€Ÿæ¨ç†ï¼Œä½†å¾€å¾€åœ¨é€Ÿåº¦å’Œä¿çœŸåº¦ä¹‹é—´äº§ç”Ÿæƒè¡¡ï¼Œå­˜åœ¨è´¨é‡ä¸‹é™å’Œè®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æ¨å‡ºH2-Cacheï¼Œä¸€ç§ä¸ºç°ä»£ç”Ÿæˆå‹æ‰©æ•£æ¨¡å‹æ¶æ„è®¾è®¡çš„åˆ†å±‚ç¼“å­˜æœºåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè¿™æ ·ä¸€ä¸ªå…³é”®è§è§£ï¼šå»å™ªè¿‡ç¨‹å¯ä»¥åŠŸèƒ½æ€§åœ°åˆ†ä¸ºç»“æ„å®šä¹‰é˜¶æ®µå’Œç»†èŠ‚ä¼˜åŒ–é˜¶æ®µã€‚H2-cacheé€šè¿‡é‡‡ç”¨åŒé˜ˆå€¼ç³»ç»Ÿï¼Œåˆ©ç”¨ç‹¬ç«‹çš„é˜ˆå€¼é€‰æ‹©æ€§åœ°ç¼“å­˜æ¯ä¸ªé˜¶æ®µã€‚ä¸ºç¡®ä¿åŒæ£€æ–¹æ³•çš„æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ±‡æ€»ç‰¹å¾æ€»ç»“ï¼ˆPFSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¨³å¥å¿«é€Ÿç›¸ä¼¼æ€§ä¼°è®¡çš„è½»é‡çº§æŠ€æœ¯ã€‚åœ¨Fluxæ¶æ„ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒH2-cacheå®ç°äº†æ˜¾è‘—åŠ é€Ÿï¼ˆæœ€é«˜è¾¾5.08å€ï¼‰ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ä¸åŸºçº¿å‡ ä¹ç›¸åŒï¼Œåœ¨æ•°é‡å’Œè´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰ç¼“å­˜æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¦è§£å†³è¿­ä»£å»å™ªè¿‡ç¨‹çš„æ•ˆç‡é—®é¢˜ã€‚</li>
<li>ç°æœ‰ç¼“å­˜æŠ€æœ¯åœ¨åŠ é€Ÿæ¨ç†æ—¶å­˜åœ¨é€Ÿåº¦å’Œä¿çœŸåº¦ä¹‹é—´çš„æƒè¡¡ï¼Œå¯¼è‡´è´¨é‡ä¸‹é™å’Œè®¡ç®—å¼€é”€å¤§ã€‚</li>
<li>H2-Cacheæ˜¯ä¸€ç§é’ˆå¯¹ç°ä»£ç”Ÿæˆå‹æ‰©æ•£æ¨¡å‹çš„æ–°å‹åˆ†å±‚ç¼“å­˜æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯çš„å±€é™æ€§ã€‚</li>
<li>H2-CacheåŸºäºå»å™ªè¿‡ç¨‹å¯ä»¥åˆ†ä¸ºç»“æ„å®šä¹‰å’Œç»†èŠ‚ä¼˜åŒ–ä¸¤ä¸ªé˜¶æ®µçš„å…³é”®è§è§£ã€‚</li>
<li>H2-Cacheé‡‡ç”¨åŒé˜ˆå€¼ç³»ç»Ÿï¼Œé€šè¿‡ç‹¬ç«‹é˜ˆå€¼é€‰æ‹©æ€§åœ°ç¼“å­˜æ¯ä¸ªé˜¶æ®µï¼Œä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>æ±‡æ€»ç‰¹å¾æ€»ç»“ï¼ˆPFSï¼‰æ˜¯H2-Cacheçš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºç¨³å¥å¿«é€Ÿç›¸ä¼¼æ€§ä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27171v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27171v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27171v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DANCER-Dance-ANimation-via-Condition-Enhancement-and-Rendering-with-diffusion-model"><a href="#DANCER-Dance-ANimation-via-Condition-Enhancement-and-Rendering-with-diffusion-model" class="headerlink" title="DANCER: Dance ANimation via Condition Enhancement and Rendering with   diffusion model"></a>DANCER: Dance ANimation via Condition Enhancement and Rendering with   diffusion model</h2><p><strong>Authors:Yucheng Xing, Jinxing Yin, Xiaodong Liu</strong></p>
<p>Recently, diffusion models have shown their impressive ability in visual generation tasks. Besides static images, more and more research attentions have been drawn to the generation of realistic videos. The video generation not only has a higher requirement for the quality, but also brings a challenge in ensuring the video continuity. Among all the video generation tasks, human-involved contents, such as human dancing, are even more difficult to generate due to the high degrees of freedom associated with human motions. In this paper, we propose a novel framework, named as DANCER (Dance ANimation via Condition Enhancement and Rendering with Diffusion Model), for realistic single-person dance synthesis based on the most recent stable video diffusion model. As the video generation is generally guided by a reference image and a video sequence, we introduce two important modules into our framework to fully benefit from the two inputs. More specifically, we design an Appearance Enhancement Module (AEM) to focus more on the details of the reference image during the generation, and extend the motion guidance through a Pose Rendering Module (PRM) to capture pose conditions from extra domains. To further improve the generation capability of our model, we also collect a large amount of video data from Internet, and generate a novel datasetTikTok-3K to enhance the model training. The effectiveness of the proposed model has been evaluated through extensive experiments on real-world datasets, where the performance of our model is superior to that of the state-of-the-art methods. All the data and codes will be released upon acceptance. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚é™¤äº†é™æ€å›¾åƒå¤–ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…³æ³¨äºç°å®è§†é¢‘çš„ç”Ÿæˆã€‚è§†é¢‘ç”Ÿæˆä¸ä»…å¯¹è´¨é‡æœ‰æ›´é«˜çš„è¦æ±‚ï¼Œè€Œä¸”è¿˜åœ¨ç¡®ä¿è§†é¢‘è¿ç»­æ€§æ–¹é¢å¸¦æ¥äº†æŒ‘æˆ˜ã€‚åœ¨æ‰€æœ‰è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ¶‰åŠäººç±»å†…å®¹ï¼ˆä¾‹å¦‚èˆè¹ˆï¼‰çš„ç”Ÿæˆå°¤ä¸ºå›°éš¾ï¼Œå› ä¸ºä¸äººçš„åŠ¨ä½œç›¸å…³çš„è‡ªç”±åº¦å¾ˆé«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåä¸ºDANCERï¼ˆåŸºäºæ¡ä»¶å¢å¼ºå’Œæ‰©æ•£æ¨¡å‹çš„æ¸²æŸ“è¿›è¡Œèˆè¹ˆåŠ¨ç”»ï¼‰ï¼Œè¯¥æ¡†æ¶åŸºäºæœ€æ–°çš„ç¨³å®šè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåˆæˆé€¼çœŸçš„å•äººèˆè¹ˆã€‚è§†é¢‘ç”Ÿæˆé€šå¸¸ç”±å‚è€ƒå›¾åƒå’Œè§†é¢‘åºåˆ—å¼•å¯¼ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ¡†æ¶ä¸­å¼•å…¥äº†ä¸¤ä¸ªé‡è¦æ¨¡å—ï¼Œä»¥å……åˆ†åˆ©ç”¨è¿™ä¸¤ä¸ªè¾“å…¥çš„ä¼˜åŠ¿ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤–è§‚å¢å¼ºæ¨¡å—ï¼ˆAEMï¼‰ï¼Œä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ›´ä¸“æ³¨äºå‚è€ƒå›¾åƒçš„ç»†èŠ‚ï¼Œå¹¶é€šè¿‡å§¿æ€æ¸²æŸ“æ¨¡å—ï¼ˆPRMï¼‰æ‰©å±•è¿åŠ¨æŒ‡å¯¼ï¼Œä»¥ä»é¢å¤–é¢†åŸŸæ•è·å§¿æ€æ¡ä»¶ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æˆ‘ä»¬æ¨¡å‹çš„ç”Ÿäº§èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜ä»äº’è”ç½‘æ”¶é›†äº†å¤§é‡è§†é¢‘æ•°æ®ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†TikTok-3Kä»¥å¢å¼ºæ¨¡å‹è®­ç»ƒã€‚æ‰€æå‡ºæ¨¡å‹çš„æœ‰æ•ˆæ€§å·²é€šè¿‡åœ¨å®é™…æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¿›è¡Œäº†è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ã€‚æ‰€æœ‰æ•°æ®å’Œæ–¹æ³•ä¸€ç»æ¥å—å°†é€šè¿‡å¼€æ”¾æºä»£ç å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27169v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDANCERçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºåŸºäºæœ€æ–°çš„ç¨³å®šè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡ŒçœŸå®å•äººèˆè¹ˆåˆæˆã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªé‡è¦æ¨¡å—ï¼Œå¤–è§‚å¢å¼ºæ¨¡å—ï¼ˆAEMï¼‰å’Œå§¿æ€æ¸²æŸ“æ¨¡å—ï¼ˆPRMï¼‰ï¼Œä»¥å……åˆ†åˆ©ç”¨å‚è€ƒå›¾åƒå’Œè§†é¢‘åºåˆ—çš„è¾“å…¥ã€‚æ­¤å¤–ï¼Œè¿˜ä»äº’è”ç½‘æ”¶é›†äº†å¤§é‡è§†é¢‘æ•°æ®ï¼Œç”Ÿæˆäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†TikTok-3Kä»¥å¢å¼ºæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œè¶Šæ¥è¶Šå¤šåœ°ç”¨äºç”ŸæˆçœŸå®è§†é¢‘ã€‚</li>
<li>è§†é¢‘ç”Ÿæˆä¸ä»…è¦æ±‚é«˜è´¨ï¼Œè¿˜éœ€ç¡®ä¿è§†é¢‘è¿ç»­æ€§ï¼Œå…¶ä¸­æ¶‰åŠäººç±»æ´»åŠ¨çš„è§†é¢‘ç”Ÿæˆå°¤ä¸ºå›°éš¾ã€‚</li>
<li>æå‡ºçš„DANCERæ¡†æ¶åˆ©ç”¨æœ€æ–°çš„ç¨³å®šè§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡ŒçœŸå®å•äººèˆè¹ˆåˆæˆã€‚</li>
<li>æ¡†æ¶ä¸­å¼•å…¥äº†å¤–è§‚å¢å¼ºæ¨¡å—ï¼ˆAEMï¼‰å’Œå§¿æ€æ¸²æŸ“æ¨¡å—ï¼ˆPRMï¼‰ï¼Œåˆ†åˆ«å…³æ³¨å‚è€ƒå›¾åƒçš„ç»†èŠ‚å’Œå§¿æ€æ¡ä»¶çš„æ•æ‰ã€‚</li>
<li>ä¸ºäº†æé«˜æ¨¡å‹ç”Ÿæˆèƒ½åŠ›ï¼Œä»äº’è”ç½‘æ”¶é›†å¤§é‡è§†é¢‘æ•°æ®ï¼Œå¹¶åˆ›å»ºäº†æ–°çš„æ•°æ®é›†TikTok-3Kã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDANCERæ¡†æ¶åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27169v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27169v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27169v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27169v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27169v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="E-MMDiT-Revisiting-Multimodal-Diffusion-Transformer-Design-for-Fast-Image-Synthesis-under-Limited-Resources"><a href="#E-MMDiT-Revisiting-Multimodal-Diffusion-Transformer-Design-for-Fast-Image-Synthesis-under-Limited-Resources" class="headerlink" title="E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast   Image Synthesis under Limited Resources"></a>E-MMDiT: Revisiting Multimodal Diffusion Transformer Design for Fast   Image Synthesis under Limited Resources</h2><p><strong>Authors:Tong Shen, Jingai Yu, Dong Zhou, Dong Li, Emad Barsoum</strong></p>
<p>Diffusion models have shown strong capabilities in generating high-quality images from text prompts. However, these models often require large-scale training data and significant computational resources to train, or suffer from heavy structure with high latency. To this end, we propose Efficient Multimodal Diffusion Transformer (E-MMDiT), an efficient and lightweight multimodal diffusion model with only 304M parameters for fast image synthesis requiring low training resources. We provide an easily reproducible baseline with competitive results. Our model for 512px generation, trained with only 25M public data in 1.5 days on a single node of 8 AMD MI300X GPUs, achieves 0.66 on GenEval and easily reaches to 0.72 with some post-training techniques such as GRPO. Our design philosophy centers on token reduction as the computational cost scales significantly with the token count. We adopt a highly compressive visual tokenizer to produce a more compact representation and propose a novel multi-path compression module for further compression of tokens. To enhance our design, we introduce Position Reinforcement, which strengthens positional information to maintain spatial coherence, and Alternating Subregion Attention (ASA), which performs attention within subregions to further reduce computational cost. In addition, we propose AdaLN-affine, an efficient lightweight module for computing modulation parameters in transformer blocks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AMD-AGI/Nitro-E">https://github.com/AMD-AGI/Nitro-E</a> and we hope E-MMDiT serves as a strong and practical baseline for future research and contributes to democratization of generative AI models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²æ˜¾ç¤ºå‡ºä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„å¼ºå¤§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®å’Œå¤§é‡çš„è®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒï¼Œæˆ–è€…å­˜åœ¨ç»“æ„å¤æ‚ã€å»¶è¿Ÿè¾ƒé«˜çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆå¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆE-MMDiTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆä¸”è½»é‡çº§çš„å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹ï¼Œä»…åŒ…å«3.04äº¿ä¸ªå‚æ•°ï¼Œå¯ç”¨äºå¿«é€Ÿå›¾åƒåˆæˆï¼Œå¹¶ä¸”è®­ç»ƒèµ„æºéœ€æ±‚è¾ƒä½ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ˜“äºå¤åˆ¶çš„åŸºçº¿æ–¹æ¡ˆï¼Œå¹¶è·å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨512åƒç´ ç”Ÿæˆæ–¹é¢ï¼Œä»…ä½¿ç”¨2500ä¸‡ä»½å…¬å¼€æ•°æ®åœ¨8ä¸ªAMD MI300X GPUçš„å•èŠ‚ç‚¹ä¸Šè®­ç»ƒäº†1.5å¤©ï¼Œå°±è¾¾åˆ°äº†GenEvalçš„0.66åˆ†ï¼Œå¹¶ä¸”é‡‡ç”¨ä¸€äº›åè®­ç»ƒæŠ€æœ¯å¦‚GRPOåå¾ˆå®¹æ˜“è¾¾åˆ°0.72åˆ†ã€‚æˆ‘ä»¬çš„è®¾è®¡å“²å­¦ä»¥ä»¤ç‰Œå‡å°‘ä¸ºä¸­å¿ƒï¼Œå› ä¸ºè®¡ç®—æˆæœ¬éšç€ä»¤ç‰Œæ•°é‡çš„å¢åŠ è€Œæ˜¾è‘—å¢åŠ ã€‚æˆ‘ä»¬é‡‡ç”¨é«˜åº¦å‹ç¼©çš„è§†è§‰æ ‡è®°å™¨æ¥äº§ç”Ÿæ›´ç´§å‡‘çš„è¡¨ç¤ºï¼Œå¹¶æå‡ºäº†æ–°å‹çš„å¤šè·¯å¾„å‹ç¼©æ¨¡å—æ¥è¿›ä¸€æ­¥å‹ç¼©ä»¤ç‰Œã€‚ä¸ºäº†å¢å¼ºæˆ‘ä»¬çš„è®¾è®¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½ç½®å¼ºåŒ–ï¼Œä»¥åŠ å¼ºä½ç½®ä¿¡æ¯æ¥ä¿æŒç©ºé—´è¿è´¯æ€§ï¼Œä»¥åŠäº¤æ›¿å­åŒºåŸŸæ³¨æ„åŠ›ï¼ˆASAï¼‰ï¼Œå®ƒåœ¨å­åŒºåŸŸå†…æ‰§è¡Œæ³¨æ„åŠ›æ¥è¿›ä¸€æ­¥é™ä½è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†AdaLN-ä»¿å°„ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„è½»é‡çº§æ¨¡å—ï¼Œç”¨äºè®¡ç®—è½¬æ¢å™¨å—ä¸­çš„è°ƒåˆ¶å‚æ•°ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/AMD-AGI/Nitro-E%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9BE-MMDiT%E8%83%BD%E6%88%90%E4%B8%BA%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E7%9A%84%E6%9C%89%E5%8A%9B%E4%B8%94%E5%AE%9E%E7%94%A8%E7%9A%84%E5%9F%BA%E7%BA%BF%EF%BC%8C%E5%B9%B6%E4%B8%BA%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B0%91%E4%B8%BB%E5%8C%96%E5%81%9A%E5%87%BA%E8%B4%A1%E7%8C%AE%E3%80%82">https://github.com/AMD-AGI/Nitro-Eä¸Šæ‰¾åˆ°ï¼Œæˆ‘ä»¬å¸Œæœ›E-MMDiTèƒ½æˆä¸ºæœªæ¥ç ”ç©¶çš„æœ‰åŠ›ä¸”å®ç”¨çš„åŸºçº¿ï¼Œå¹¶ä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ°‘ä¸»åŒ–åšå‡ºè´¡çŒ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27135v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Efficient Multimodal Diffusion Transformerï¼ˆE-MMDiTï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆä¸”è½»é‡çº§çš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä½è®­ç»ƒèµ„æºä¸‹å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚è¯¥æ¨¡å‹å…·æœ‰ä»…304Mçš„å‚æ•°ï¼Œé€šè¿‡å‡å°‘è®¡ç®—é‡å’Œé‡‡ç”¨é«˜æ•ˆè®¾è®¡ç­–ç•¥ï¼Œå®ç°äº†ä½èµ„æºæ¶ˆè€—å’Œé«˜æ€§èƒ½ã€‚é€šè¿‡é‡‡ç”¨ç´§å‡‘çš„è§†è§‰æ ‡è®°å™¨å’Œå¤šè·¯å¾„å‹ç¼©æ¨¡å—ç­‰æŠ€æœ¯ï¼Œæ¨¡å‹å®ç°äº†é«˜æ•ˆçš„å›¾åƒåˆæˆã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜å¼ºåŒ–äº†ä½ç½®ä¿¡æ¯å’Œå­åŒºåŸŸæ³¨æ„åŠ›æœºåˆ¶ç­‰åˆ›æ–°æŠ€æœ¯ä»¥æå‡æ€§èƒ½ã€‚æ¨¡å‹çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šï¼Œå¸Œæœ›ä½œä¸ºæœªæ¥ç ”ç©¶å’Œå®è·µçš„åšå®åŸºç¡€ï¼Œå¹¶ä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ™®åŠåšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>E-MMDiTæ¨¡å‹æ˜¯ä¸€ç§é«˜æ•ˆä¸”è½»é‡çº§çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä»…éœ€è¦è¾ƒå°‘çš„è®¡ç®—èµ„æºå’Œè®­ç»ƒæ—¶é—´ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ç´§å‡‘çš„è§†è§‰æ ‡è®°å™¨å’Œå¤šè·¯å¾„å‹ç¼©æ¨¡å—ç­‰æŠ€æœ¯æ¥å‡å°‘è®¡ç®—é‡å’Œæé«˜æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨ä»…æœ‰25Må…¬å…±æ•°æ®çš„è®­ç»ƒä¸‹ï¼Œè¾¾åˆ°äº†GenEvalä¸Šçš„0.66åˆ†æ•°ï¼Œä½¿ç”¨æŸäº›åè®­ç»ƒæŠ€æœ¯å¦‚GRPOåï¼Œåˆ†æ•°æå‡è‡³0.72ã€‚</li>
<li>æ¨¡å‹å¼ºåŒ–äº†ä½ç½®ä¿¡æ¯ï¼Œç»´æŒç©ºé—´è¿è´¯æ€§ï¼›å¼•å…¥äº†äº¤æ›¿å­åŒºåŸŸæ³¨æ„åŠ›æœºåˆ¶ï¼ˆASAï¼‰ï¼Œé€šè¿‡å­åŒºåŸŸå†…çš„æ³¨æ„åŠ›æ¥é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>AdaLN-affineæ¨¡å—è¢«æå‡ºä¸ºä¸€ç§é«˜æ•ˆçš„è½»é‡çº§æ¨¡å—ï¼Œç”¨äºè®¡ç®—transformerå—ä¸­çš„è°ƒåˆ¶å‚æ•°ã€‚</li>
<li>E-MMDiTçš„ä»£ç å·²ç»å…¬å¼€å‘å¸ƒï¼Œå¹¶ä¸”è¯¥æ¨¡å‹æœ‰æœ›æˆä¸ºæœªæ¥ç ”ç©¶å’Œå®è·µçš„åšå®åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27135v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27135v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27135v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27135v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27135v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27135v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.27135v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation"><a href="#SViM3D-Stable-Video-Material-Diffusion-for-Single-Image-3D-Generation" class="headerlink" title="SViM3D: Stable Video Material Diffusion for Single Image 3D Generation"></a>SViM3D: Stable Video Material Diffusion for Single Image 3D Generation</h2><p><strong>Authors:Andreas Engelhardt, Mark Boss, Vikram Voleti, Chun-Han Yao, Hendrik P. A. Lensch, Varun Jampani</strong></p>
<p>We present Stable Video Materials 3D (SViM3D), a framework to predict multi-view consistent physically based rendering (PBR) materials, given a single image. Recently, video diffusion models have been successfully used to reconstruct 3D objects from a single image efficiently. However, reflectance is still represented by simple material models or needs to be estimated in additional steps to enable relighting and controlled appearance edits. We extend a latent video diffusion model to output spatially varying PBR parameters and surface normals jointly with each generated view based on explicit camera control. This unique setup allows for relighting and generating a 3D asset using our model as neural prior. We introduce various mechanisms to this pipeline that improve quality in this ill-posed setting. We show state-of-the-art relighting and novel view synthesis performance on multiple object-centric datasets. Our method generalizes to diverse inputs, enabling the generation of relightable 3D assets useful in AR&#x2F;VR, movies, games and other visual media. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ç¨³å®šè§†é¢‘ææ–™3Dï¼ˆSViM3Dï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯æ ¹æ®å•å¹…å›¾åƒé¢„æµ‹å¤šè§†è§’ä¸€è‡´æ€§çš„åŸºäºç‰©ç†çš„æ¸²æŸ“ï¼ˆPBRï¼‰ææ–™ã€‚æœ€è¿‘ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹å·²æˆåŠŸç”¨äºä»å•ä¸ªå›¾åƒé«˜æ•ˆé‡å»º3Då¯¹è±¡ã€‚ç„¶è€Œï¼Œåå°„ä»ç„¶ç”±ç®€å•çš„ææ–™æ¨¡å‹è¡¨ç¤ºï¼Œæˆ–è€…éœ€è¦åœ¨é™„åŠ æ­¥éª¤ä¸­ä¼°è®¡ä»¥å¯ç”¨é‡æ–°ç…§æ˜å’Œæ§åˆ¶å¤–è§‚ç¼–è¾‘ã€‚æˆ‘ä»¬æ‰©å±•äº†æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥è”åˆè¾“å‡ºç©ºé—´å˜åŒ–çš„PBRå‚æ•°å’Œè¡¨é¢æ³•çº¿ï¼Œä»¥åŠåŸºäºæ˜¾å¼ç›¸æœºæ§åˆ¶çš„æ¯ä¸ªç”Ÿæˆè§†å›¾ã€‚è¿™ç§ç‹¬ç‰¹çš„è®¾ç½®å…è®¸ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ä½œä¸ºç¥ç»å…ˆéªŒè¿›è¡Œé‡æ–°ç…§æ˜å’Œç”Ÿæˆ3Dèµ„äº§ã€‚æˆ‘ä»¬å‘æ­¤ç®¡é“å¼•å…¥äº†å„ç§æœºåˆ¶ï¼Œä»¥åœ¨è¿™ç§ä¸é€‚å®šçš„è®¾ç½®ä¸­æé«˜è´¨é‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„é‡æ–°ç…§æ˜å’Œæ–°è§†è§’åˆæˆæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºå„ç§è¾“å…¥ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯ç”¨äºAR&#x2F;VRã€ç”µå½±ã€æ¸¸æˆå’Œå…¶ä»–è§†è§‰åª’ä½“çš„é‡æ–°ç…§æ˜çš„3Dèµ„äº§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08271v2">PDF</a> Accepted by International Conference on Computer Vision (ICCV 2025).   Project page: <a target="_blank" rel="noopener" href="http://svim3d.aengelhardt.com/">http://svim3d.aengelhardt.com</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºStable Video Materials 3Dï¼ˆSViM3Dï¼‰çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒé¢„æµ‹å¤šè§†è§’ä¸€è‡´æ€§çš„ç‰©ç†åŸºç¡€æ¸²æŸ“ï¼ˆPBRï¼‰æè´¨ã€‚è¯¥æ¡†æ¶é€šè¿‡æ‰©å±•æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥è”åˆè¾“å‡ºç©ºé—´å˜åŒ–çš„PBRå‚æ•°å’Œè¡¨é¢æ³•çº¿ï¼ŒåŸºäºæ˜ç¡®çš„ç›¸æœºæ§åˆ¶ç”Ÿæˆæ¯ä¸ªè§†å›¾ã€‚è¿™ç§æ–¹æ³•å…è®¸é‡æ–°ç…§æ˜å’Œä½¿ç”¨ç¥ç»å…ˆéªŒç”Ÿæˆ3Dèµ„äº§ã€‚æœ¬æ–‡ä»‹ç»äº†æ”¹è¿›è¿™ä¸€ä¸é€‚å®šè®¾ç½®è´¨é‡çš„å¤šç§æœºåˆ¶ï¼Œå¹¶åœ¨å¤šä¸ªä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„é‡æ–°ç…§æ˜å’Œæ–°è§†è§’åˆæˆæ€§èƒ½ã€‚è¯¥æ–¹æ³•å¯æ¨å¹¿åˆ°å„ç§è¾“å…¥ï¼Œç”Ÿæˆçš„é‡æ–°ç…§æ˜3Dèµ„äº§å¯ç”¨äºAR&#x2F;VRã€ç”µå½±ã€æ¸¸æˆå’Œå…¶ä»–è§†è§‰åª’ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SViM3Dæ˜¯ä¸€ä¸ªèƒ½å¤Ÿä»å•å¼ å›¾åƒé¢„æµ‹å¤šè§†è§’ä¸€è‡´æ€§çš„PBRæè´¨çš„æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ‰©å±•æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥è”åˆè¾“å‡ºç©ºé—´å˜åŒ–çš„PBRå‚æ•°å’Œè¡¨é¢æ³•çº¿ã€‚</li>
<li>å…è®¸é‡æ–°ç…§æ˜å’Œä½¿ç”¨ç¥ç»å…ˆéªŒç”Ÿæˆ3Dèµ„äº§ã€‚</li>
<li>ä»‹ç»äº†å¤šç§æ”¹è¿›ä¸é€‚å®šè®¾ç½®è´¨é‡çš„æœºåˆ¶ã€‚</li>
<li>åœ¨å¤šä¸ªä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„é‡æ–°ç…§æ˜å’Œæ–°è§†è§’åˆæˆæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å¯æ¨å¹¿åˆ°å¤šç§ç±»å‹çš„è¾“å…¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.08271v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.08271v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.08271v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2510.08271v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition"><a href="#Does-FLUX-Already-Know-How-to-Perform-Physically-Plausible-Image-Composition" class="headerlink" title="Does FLUX Already Know How to Perform Physically Plausible Image   Composition?"></a>Does FLUX Already Know How to Perform Physically Plausible Image   Composition?</h2><p><strong>Authors:Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</strong></p>
<p>Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication. </p>
<blockquote>
<p>å›¾åƒç»„åˆæ—¨åœ¨æ— ç¼æ’å…¥ç”¨æˆ·æŒ‡å®šçš„å¯¹è±¡åˆ°æ–°çš„åœºæ™¯ä¸­ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚å…‰ç…§ï¼ˆä¾‹å¦‚å‡†ç¡®é˜´å½±ã€æ°´é¢åå°„ï¼‰å’Œå¤šæ ·åŒ–ã€é«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶é‡åˆ°å›°éš¾ã€‚ç°ä»£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚SD3.5ã€FLUXï¼‰å·²ç»ç¼–ç äº†åŸºæœ¬çš„ç‰©ç†å’Œåˆ†è¾¨ç‡å…ˆéªŒï¼Œä½†ç¼ºä¹ä¸€ä¸ªæ¡†æ¶æ¥é‡Šæ”¾å®ƒä»¬ï¼Œè€Œä¸æ±‚åŠ©äºæ½œåœ¨é€†è¿‡ç¨‹ï¼Œè¿™é€šå¸¸ä¼šå°†å¯¹è±¡å§¿åŠ¿é”å®šåœ¨ä¸Šä¸‹æ–‡ä¸é€‚å½“çš„æ–¹å‘ä¸Šï¼Œæˆ–è„†å¼±çš„æ³¨æ„åŠ›æ‰‹æœ¯ã€‚æˆ‘ä»¬æå‡ºSHINEï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ— ç¼ã€é«˜ä¿çœŸæ’å…¥æ¡†æ¶ï¼Œå…·æœ‰ä¸­æ€§åŒ–é”™è¯¯ã€‚SHINEå¼•å…¥äº†æµå½¢å¼•å¯¼é”šç‚¹æŸå¤±ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å®šåˆ¶é€‚é…å™¨ï¼ˆä¾‹å¦‚IP-Adapterï¼‰æ¥å¼•å¯¼æ½œåœ¨è¡¨ç¤ºä»¥å¿ å®äºä¸»é¢˜è¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒèƒŒæ™¯å®Œæ•´æ€§ã€‚æå‡ºé€€åŒ–æŠ‘åˆ¶æŒ‡å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆï¼Œä»¥è¿›ä¸€æ­¥æ¶ˆé™¤ä½è´¨é‡è¾“å‡ºå’Œå¯è§æ¥ç¼ã€‚ä¸ºäº†è§£å†³ç¼ºä¹ä¸¥æ ¼åŸºå‡†æµ‹è¯•çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ComplexCompoï¼Œå®ƒæ‹¥æœ‰å¤šç§åˆ†è¾¨ç‡å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ï¼Œå¦‚ä½å…‰ç…§ã€å¼ºçƒˆç…§æ˜ã€å¤æ‚é˜´å½±å’Œåå°„è¡¨é¢ç­‰ã€‚åœ¨ComplexCompoå’ŒDreamEditBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨æ ‡å‡†æŒ‡æ ‡ï¼ˆå¦‚DINOv2ï¼‰å’Œäººç±»å¯¹é½å¾—åˆ†ï¼ˆå¦‚DreamSimã€ImageRewardã€VisionRewardï¼‰ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä»£ç å’ŒåŸºå‡†æµ‹è¯•å°†åœ¨å‘è¡¨æ—¶å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21278v3">PDF</a> Preprint</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSHINEçš„æ— è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæ— ç¼ã€é«˜ä¿çœŸæ’å…¥ç”¨æˆ·æŒ‡å®šå¯¹è±¡ã€‚SHINEé€šè¿‡å¼•å…¥æµå½¢å¼•å¯¼çš„é”šç‚¹æŸå¤±å’Œé¢„è®­ç»ƒçš„å®šåˆ¶é€‚é…å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¸ç ´åèƒŒæ™¯å®Œæ•´æ€§çš„æƒ…å†µä¸‹ï¼Œå¿ å®å‘ˆç°ä¸»é¢˜è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†é™è´¨æŠ‘åˆ¶å¼•å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆç­‰æ–¹æ³•ï¼Œä»¥æ¶ˆé™¤ä½è´¨é‡è¾“å‡ºå’Œå¯è§æ¥ç¼ã€‚ä¸ºè§£å†³ç¼ºä¹ä¸¥æ ¼åŸºå‡†çš„é—®é¢˜ï¼Œå¼•å…¥äº†ComplexCompoåŸºå‡†æµ‹è¯•å¹³å°ï¼Œå¯¹å„ç§åˆ†è¾¨ç‡å’Œå¤æ‚æ¡ä»¶ï¼ˆå¦‚ä½å…‰ç…§ã€å¼ºçƒˆç…§æ˜ã€å¤æ‚é˜´å½±å’Œåå°„è¡¨é¢ï¼‰è¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æ ‡å‡†æŒ‡æ ‡å’Œäººç±»å¯¹é½å¾—åˆ†æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SHINEæ˜¯ä¸€ä¸ªæ— è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ— ç¼ã€é«˜ä¿çœŸåœ°æ’å…¥ç”¨æˆ·æŒ‡å®šå¯¹è±¡ã€‚</li>
<li>SHINEé€šè¿‡å¼•å…¥æµå½¢å¼•å¯¼çš„é”šç‚¹æŸå¤±ï¼Œåˆ©ç”¨é¢„è®­ç»ƒå®šåˆ¶é€‚é…å™¨æ¥æŒ‡å¯¼æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>SHINEä¿ç•™äº†èƒŒæ™¯å®Œæ•´æ€§ï¼ŒåŒæ—¶å¿ å®å‘ˆç°ä¸»é¢˜è¡¨ç¤ºã€‚</li>
<li>SHINEæå‡ºäº†é™è´¨æŠ‘åˆ¶å¼•å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆæ–¹æ³•ï¼Œä»¥æ”¹å–„è¾“å‡ºè´¨é‡ã€‚</li>
<li>ComplexCompoåŸºå‡†æµ‹è¯•å¹³å°è¢«å¼•å…¥ï¼Œç”¨ä»¥æµ‹è¯•å›¾åƒåˆæˆæ¨¡å‹åœ¨å„ç§åˆ†è¾¨ç‡å’Œå¤æ‚æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜SHINEåœ¨æ ‡å‡†æŒ‡æ ‡å’Œäººç±»å¯¹é½å¾—åˆ†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2509.21278v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2509.21278v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2509.21278v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2509.21278v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2509.21278v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Where-and-How-to-Perturb-On-the-Design-of-Perturbation-Guidance-in-Diffusion-and-Flow-Models"><a href="#Where-and-How-to-Perturb-On-the-Design-of-Perturbation-Guidance-in-Diffusion-and-Flow-Models" class="headerlink" title="Where and How to Perturb: On the Design of Perturbation Guidance in   Diffusion and Flow Models"></a>Where and How to Perturb: On the Design of Perturbation Guidance in   Diffusion and Flow Models</h2><p><strong>Authors:Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Minjae Kim, Jaewon Min, Wooseok Jang, Sangwu Lee, Sayak Paul, Susung Hong, Seungryong Kim</strong></p>
<p>Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose â€œHeadHunterâ€, a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected headâ€™s attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹ä¸­çš„æŒ‡å¯¼æ–¹æ³•é€šè¿‡æ‰°åŠ¨æ¨¡å‹æ¥æ„å»ºéšå¼å¼±æ¨¡å‹ï¼Œä»è€Œå¼•å¯¼åå‘é‡‡æ ·å¹¶é¿å…ç”Ÿæˆå—åˆ°å½±å“ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œæ³¨æ„åŠ›æ‰°åŠ¨åœ¨æ— æ¡ä»¶åœºæ™¯ï¼ˆå…¶ä¸­æ— åˆ†ç±»æŒ‡å¯¼ä¸é€‚ç”¨ï¼‰ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„å®è¯æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ³¨æ„åŠ›æ‰°åŠ¨æ–¹æ³•ç¼ºä¹åŸåˆ™æ€§çš„æ–¹æ³•æ¥ç¡®å®šæ‰°åŠ¨åº”åº”ç”¨äºä½•å¤„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰æ¶æ„ä¸­ï¼Œè´¨é‡ç›¸å…³çš„è®¡ç®—åˆ†å¸ƒåœ¨å„å±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ³¨æ„åŠ›æ‰°åŠ¨çš„ç²’åº¦ï¼Œä»å±‚çº§åˆ°å•ä¸ªæ³¨æ„åŠ›å¤´ï¼Œå¹¶å‘ç°ç‰¹å®šçš„å¤´æ§åˆ¶ç€ç»“æ„ã€é£æ ¼å’Œçº¹ç†è´¨é‡ç­‰ä¸åŒçš„è§†è§‰æ¦‚å¿µã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†â€œHeadHunterâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿæ¡†æ¶ï¼Œç”¨äºè¿­ä»£é€‰æ‹©ç¬¦åˆç”¨æˆ·ä¸­å¿ƒç›®æ ‡çš„æ³¨æ„åŠ›å¤´ï¼Œå®ç°å¯¹ç”Ÿæˆè´¨é‡å’Œè§†è§‰å±æ€§çš„ç²¾ç»†æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†SoftPAGï¼Œå®ƒé€šè¿‡çº¿æ€§æ’å€¼æ‰€é€‰å¤´éƒ¨çš„æ³¨æ„åŠ›å›¾æ¥ç”Ÿæˆèº«ä»½çŸ©é˜µï¼Œæä¾›äº†ä¸€ä¸ªè¿ç»­çš„æ—‹é’®æ¥è°ƒèŠ‚æ‰°åŠ¨å¼ºåº¦å¹¶æŠ‘åˆ¶ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ç¼“è§£äº†ç°æœ‰å±‚çº§æ‰°åŠ¨æ–¹æ³•çš„è¿‡åº¦å¹³æ»‘é—®é¢˜ï¼Œè€Œä¸”èƒ½å¤Ÿé€šè¿‡ç»„åˆå¤´éƒ¨é€‰æ‹©æ¥é’ˆå¯¹ç‰¹å®šçš„è§†è§‰é£æ ¼è¿›è¡Œæ“æ§ã€‚æˆ‘ä»¬åœ¨åŸºäºå¤§å‹DiTçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆåŒ…æ‹¬Stable Diffusion 3å’ŒFLUX.1ï¼‰ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨é€šç”¨è´¨é‡æå‡å’Œç‰¹å®šé£æ ¼æŒ‡å¯¼æ–¹é¢éƒ½è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œé¦–æ¬¡åœ¨æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œäº†å¤´éƒ¨å±‚é¢çš„æ³¨æ„åŠ›æ‰°åŠ¨åˆ†æï¼Œæ­ç¤ºäº†æ³¨æ„åŠ›å±‚å†…çš„å¯è§£é‡Šä¸“ä¸šåŒ–ï¼Œå¹¶å®ç°äº†æœ‰æ•ˆçš„æ‰°åŠ¨ç­–ç•¥çš„å®é™…è®¾è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10978v4">PDF</a> Accepted at NeurIPS 2025. Project page:   <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/HeadHunter/">https://cvlab-kaist.github.io/HeadHunter/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä¸­çš„æŒ‡å¯¼æ–¹æ³•ï¼Œé€šè¿‡æ‰°åŠ¨æ¨¡å‹è¿›è¡Œåå‘é‡‡æ ·ï¼Œæ„å»ºäº†éšå¼å¼±æ¨¡å‹ï¼Œå¹¶æŒ‡å¯¼ç”Ÿæˆè¿œç¦»å®ƒã€‚æ–‡ç« é‡ç‚¹ç ”ç©¶äº†æ³¨æ„åŠ›æ‰°åŠ¨çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨Diffusion Transformerï¼ˆDiTï¼‰æ¶æ„ä¸­ï¼Œå‘ç°äº†ç‰¹å®šæ³¨æ„åŠ›å¤´è´Ÿè´£ä¸åŒçš„è§†è§‰æ¦‚å¿µï¼Œå¦‚ç»“æ„ã€é£æ ¼å’Œçº¹ç†è´¨é‡ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†â€œHeadHunterâ€æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£é€‰æ‹©ä¸ç”¨æˆ·ä¸­å¿ƒç›®æ ‡å¯¹é½çš„æ³¨æ„åŠ›å¤´ï¼Œå®ç°å¯¹ç”Ÿæˆè´¨é‡å’Œè§†è§‰å±æ€§çš„ç²¾ç»†æ§åˆ¶ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†SoftPAGæ–¹æ³•ï¼Œé€šè¿‡çº¿æ€§æ’å€¼é€‰å®šçš„æ³¨æ„åŠ›å¤´æœå‘èº«ä»½çŸ©é˜µï¼Œæä¾›è¿ç»­çš„è°ƒèŠ‚æ‰°åŠ¨å¼ºåº¦å’ŒæŠ‘åˆ¶ä¼ªå½±çš„æ—‹é’®ã€‚è¯¥æ–¹æ³•ä¸ä»…è§£å†³äº†ç°æœ‰å±‚çº§æ‰°åŠ¨çš„è¿‡å¹³æ»‘é—®é¢˜ï¼Œè¿˜é€šè¿‡é€‰æ‹©æ€§å¤´éƒ¨æ“ä½œå®ç°äº†ç‰¹å®šè§†è§‰é£æ ¼çš„ç›®æ ‡æ“æ§ã€‚åœ¨åŸºäºå¤§å‹DiTçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸ŠéªŒè¯äº†æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸­çš„æŒ‡å¯¼æ–¹æ³•é€šè¿‡æ‰°åŠ¨æ¨¡å‹è¿›è¡Œåå‘é‡‡æ ·ï¼Œæ„å»ºéšå¼å¼±æ¨¡å‹ä»¥æŒ‡å¯¼ç”Ÿæˆã€‚</li>
<li>æ³¨æ„åŠ›æ‰°åŠ¨åœ¨æ— æ¡ä»¶åœºæ™¯ä¸­å…·æœ‰å¼ºå¤§çš„å®è¯æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨Diffusion Transformeræ¶æ„ä¸­ã€‚</li>
<li>ç‰¹å®šæ³¨æ„åŠ›å¤´è´Ÿè´£ä¸åŒçš„è§†è§‰æ¦‚å¿µï¼Œå¦‚ç»“æ„ã€é£æ ¼å’Œçº¹ç†ã€‚</li>
<li>æå‡ºâ€œHeadHunterâ€æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£é€‰æ‹©ä¸ç”¨æˆ·ä¸­å¿ƒç›®æ ‡å¯¹é½çš„æ³¨æ„åŠ›å¤´ï¼Œå®ç°ç»†ç²’åº¦æ§åˆ¶ã€‚</li>
<li>ä»‹ç»SoftPAGæ–¹æ³•ï¼Œé€šè¿‡çº¿æ€§æ’å€¼è°ƒèŠ‚é€‰å®šçš„æ³¨æ„åŠ›å¤´ï¼Œæä¾›è¿ç»­çš„è°ƒèŠ‚æ‰°åŠ¨å¼ºåº¦å’ŒæŠ‘åˆ¶ä¼ªå½±çš„æ—‹é’®ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…è§£å†³äº†ç°æœ‰å±‚çº§æ‰°åŠ¨çš„è¿‡å¹³æ»‘é—®é¢˜ï¼Œè¿˜å®ç°äº†ç‰¹å®šè§†è§‰é£æ ¼çš„ç›®æ ‡æ“æ§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.10978v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.10978v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.10978v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.10978v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="EDITOR-Effective-and-Interpretable-Prompt-Inversion-for-Text-to-Image-Diffusion-Models"><a href="#EDITOR-Effective-and-Interpretable-Prompt-Inversion-for-Text-to-Image-Diffusion-Models" class="headerlink" title="EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models"></a>EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models</h2><p><strong>Authors:Mingzhe Li, Gehao Zhang, Zhenting Wang, Guanhong Tao, Siqi Pan, Richard Cartwright, Juan Zhai, Shiqing Ma</strong></p>
<p>Text-to-image generation models~(e.g., Stable Diffusion) have achieved significant advancements, enabling the creation of high-quality and realistic images based on textual descriptions. Prompt inversion, the task of identifying the textual prompt used to generate a specific artifact, holds significant potential for applications including data attribution, model provenance, and watermarking validation. Recent studies introduced a delayed projection scheme to optimize for prompts representative of the vocabulary space, though challenges in semantic fluency and efficiency remain. Advanced image captioning models or visual large language models can generate highly interpretable prompts, but they often lack in image similarity. In this paper, we propose a prompt inversion technique called \sys for text-to-image diffusion models, which includes initializing embeddings using a pre-trained image captioning model, refining them through reverse-engineering in the latent space, and converting them to texts using an embedding-to-text model. Our experiments on the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our method outperforms existing methods in terms of image similarity, textual alignment, prompt interpretability and generalizability. We further illustrate the application of our generated prompts in tasks such as cross-concept image synthesis, concept manipulation, evolutionary multi-concept generation and unsupervised segmentation. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆä¾‹å¦‚Stable Diffusionï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°åˆ›å»ºé«˜è´¨é‡å’Œç°å®çš„å›¾åƒã€‚æç¤ºåè½¬ï¼ˆPrompt Inversionï¼‰çš„ä»»åŠ¡æ˜¯è¯†åˆ«ç”¨äºç”Ÿæˆç‰¹å®šå·¥ä»¶çš„æ–‡æœ¬æç¤ºï¼Œåœ¨æ•°æ®å½’å±ã€æ¨¡å‹æ¥æºå’Œæ°´å°éªŒè¯ç­‰æ–¹é¢å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚æœ€è¿‘çš„ç ”ç©¶å¼•å…¥äº†ä¸€ç§å»¶è¿ŸæŠ•å½±æ–¹æ¡ˆï¼Œä»¥ä¼˜åŒ–ä»£è¡¨è¯æ±‡ç©ºé—´çš„æç¤ºï¼Œå°½ç®¡åœ¨è¯­ä¹‰æµç•…æ€§å’Œæ•ˆç‡æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚å…ˆè¿›çš„å›¾åƒæè¿°æ¨¡å‹æˆ–è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜åº¦å¯è§£é‡Šçš„æç¤ºï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨å›¾åƒç›¸ä¼¼æ€§æ–¹é¢æœ‰æ‰€æ¬ ç¼ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æç¤ºåè½¬æŠ€æœ¯ï¼Œç§°ä¸ºâ€œsysâ€ã€‚å®ƒåŒ…æ‹¬ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒæè¿°æ¨¡å‹è¿›è¡ŒåµŒå…¥åˆå§‹åŒ–ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œåå‘å·¥ç¨‹è¿›è¡Œç»†åŒ–ï¼Œå¹¶ä½¿ç”¨åµŒå…¥åˆ°æ–‡æœ¬æ¨¡å‹å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬ã€‚æˆ‘ä»¬åœ¨MS COCOã€LAIONå’ŒFlickrç­‰å¸¸ç”¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒç›¸ä¼¼æ€§ã€æ–‡æœ¬å¯¹é½ã€æç¤ºå¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯´æ˜äº†æ‰€ç”Ÿæˆçš„æç¤ºåœ¨è·¨æ¦‚å¿µå›¾åƒåˆæˆã€æ¦‚å¿µæ“ä½œã€è¿›åŒ–å¤šæ¦‚å¿µç”Ÿæˆå’Œæ— ç›‘ç£åˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03067v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„å›¾åƒã€‚æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹çš„æç¤ºåè½¬æŠ€æœ¯ï¼Œé€šè¿‡é¢„è®­ç»ƒå›¾åƒæè¿°æ¨¡å‹è¿›è¡ŒåµŒå…¥åˆå§‹åŒ–ï¼Œåœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œåå‘å·¥ç¨‹ä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨åµŒå…¥åˆ°æ–‡æœ¬çš„æ¨¡å‹è¿›è¡Œè½¬æ¢ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒç›¸ä¼¼æ€§ã€æ–‡æœ¬å¯¹é½ã€æç¤ºå¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åº”ç”¨äºè·¨æ¦‚å¿µå›¾åƒåˆæˆã€æ¦‚å¿µæ“ä½œã€è¿›åŒ–å¤šæ¦‚å¿µç”Ÿæˆå’Œæ— ç›‘ç£åˆ†å‰²ç­‰ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹å¦‚Stable Diffusionèƒ½æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>æç¤ºåè½¬æŠ€æœ¯åœ¨æ•°æ®å½’å±ã€æ¨¡å‹æ¥æºå’Œæ°´å°éªŒè¯ç­‰æ–¹é¢æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸­è¯­ä¹‰æµç•…æ€§å’Œæ•ˆç‡çš„æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹çš„æç¤ºåè½¬æŠ€æœ¯\sysã€‚</li>
<li>è¯¥æŠ€æœ¯é€šè¿‡é¢„è®­ç»ƒå›¾åƒæè¿°æ¨¡å‹è¿›è¡ŒåµŒå…¥åˆå§‹åŒ–ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œåå‘å·¥ç¨‹ä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒç›¸ä¼¼æ€§ã€æ–‡æœ¬å¯¹é½ã€æç¤ºå¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.03067v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.03067v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.03067v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.03067v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.03067v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.03067v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2506.03067v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SPIRAL-Semantic-Aware-Progressive-LiDAR-Scene-Generation-and-Understanding"><a href="#SPIRAL-Semantic-Aware-Progressive-LiDAR-Scene-Generation-and-Understanding" class="headerlink" title="SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation and   Understanding"></a>SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation and   Understanding</h2><p><strong>Authors:Dekai Zhu, Yixuan Hu, Youquan Liu, Dongyue Lu, Lingdong Kong, Slobodan Ilic</strong></p>
<p>Leveraging recent diffusion models, LiDAR-based large-scale 3D scene generation has achieved great success. While recent voxel-based approaches can generate both geometric structures and semantic labels, existing range-view methods are limited to producing unlabeled LiDAR scenes. Relying on pretrained segmentation models to predict the semantic maps often results in suboptimal cross-modal consistency. To address this limitation while preserving the advantages of range-view representations, such as computational efficiency and simplified network design, we propose Spiral, a novel range-view LiDAR diffusion model that simultaneously generates depth, reflectance images, and semantic maps. Furthermore, we introduce novel semantic-aware metrics to evaluate the quality of the generated labeled range-view data. Experiments on the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves state-of-the-art performance with the smallest parameter size, outperforming two-step methods that combine the generative and segmentation models. Additionally, we validate that range images generated by Spiral can be effectively used for synthetic data augmentation in the downstream segmentation training, significantly reducing the labeling effort on LiDAR data. </p>
<blockquote>
<p>å€ŸåŠ©æœ€æ–°çš„æ‰©æ•£æ¨¡å‹ï¼ŒåŸºäºæ¿€å…‰é›·è¾¾çš„å¤§è§„æ¨¡3Dåœºæ™¯ç”Ÿæˆå·²ç»å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚è™½ç„¶æœ€è¿‘çš„åŸºäºä½“ç´ çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆå‡ ä½•ç»“æ„å’Œè¯­ä¹‰æ ‡ç­¾ï¼Œä½†ç°æœ‰çš„èŒƒå›´è§†å›¾æ–¹æ³•ä»…é™äºç”Ÿæˆæ— æ ‡ç­¾çš„æ¿€å…‰é›·è¾¾åœºæ™¯ã€‚ä¾èµ–é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹æ¥é¢„æµ‹è¯­ä¹‰åœ°å›¾å¾€å¾€ä¼šå¯¼è‡´è·¨æ¨¡æ€ä¸€è‡´æ€§ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™èŒƒå›´è§†å›¾è¡¨ç¤ºçš„ä¼˜ç‚¹ï¼Œå¦‚è®¡ç®—æ•ˆç‡å’Œç½‘ç»œè®¾è®¡ç®€åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†Spiralï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„èŒƒå›´è§†å›¾æ¿€å…‰é›·è¾¾æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆæ·±åº¦ã€åå°„å›¾åƒå’Œè¯­ä¹‰åœ°å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°å‹è¯­ä¹‰æ„ŸçŸ¥æŒ‡æ ‡æ¥è¯„ä¼°ç”Ÿæˆçš„æœ‰æ ‡ç­¾èŒƒå›´è§†å›¾æ•°æ®çš„è´¨é‡ã€‚åœ¨SemanticKITTIå’Œnuscenesæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSpiralåœ¨å‚æ•°å¤§å°æœ€å°çš„æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œè¶…è¿‡äº†å°†ç”Ÿæˆæ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ç»“åˆçš„ä¸¤æ­¥æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†Spiralç”Ÿæˆçš„èŒƒå›´å›¾åƒå¯ä»¥æœ‰æ•ˆåœ°ç”¨äºä¸‹æ¸¸åˆ†å‰²è®­ç»ƒä¸­çš„åˆæˆæ•°æ®å¢å¼ºï¼Œæ˜¾è‘—å‡å°‘æ¿€å…‰é›·è¾¾æ•°æ®çš„æ ‡æ³¨å·¥ä½œé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22643v2">PDF</a> NeurIPS 2025; 24 pages, 10 figures, 9 tables; Code at   <a target="_blank" rel="noopener" href="https://dekai21.github.io/SPIRAL/">https://dekai21.github.io/SPIRAL/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°çš„æ‰©æ•£æ¨¡å‹ï¼ŒLiDARåŸºçš„å¤§è§„æ¨¡3Dåœºæ™¯ç”Ÿæˆå·²å–å¾—å·¨å¤§æˆåŠŸã€‚å°½ç®¡è¿‘æœŸåŸºäºä½“ç´ çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆå‡ ä½•ç»“æ„å’Œè¯­ä¹‰æ ‡ç­¾ï¼Œä½†ç°æœ‰çš„èŒƒå›´è§†å›¾æ–¹æ³•ä»…é™äºç”Ÿæˆæœªæ ‡è®°çš„LiDARåœºæ™¯ã€‚ä¾èµ–é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹æ¥é¢„æµ‹è¯­ä¹‰åœ°å›¾å¾€å¾€ä¼šå¯¼è‡´è·¨æ¨¡æ€ä¸€è‡´æ€§è¾ƒå·®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼ŒåŒæ—¶ä¿ç•™èŒƒå›´è§†å›¾è¡¨ç¤ºçš„ä¼˜åŠ¿ï¼Œå¦‚è®¡ç®—æ•ˆç‡é«˜å’Œç®€åŒ–ç½‘ç»œè®¾è®¡ï¼Œæˆ‘ä»¬æå‡ºäº†Spiralï¼Œä¸€ç§æ–°å‹çš„èŒƒå›´è§†å›¾LiDARæ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆæ·±åº¦ã€åå°„å›¾åƒå’Œè¯­ä¹‰åœ°å›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒSpiralåœ¨å‚æ•°è§„æ¨¡æœ€å°çš„æƒ…å†µä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œè¶…è¶Šäº†ç»“åˆç”Ÿæˆæ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹çš„ä¸¤æ­¥æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†Spiralç”Ÿæˆçš„èŒƒå›´å›¾åƒå¯ä»¥æœ‰æ•ˆåœ°ç”¨äºä¸‹æ¸¸åˆ†å‰²è®­ç»ƒä¸­çš„åˆæˆæ•°æ®å¢å¼ºï¼Œå¤§å¤§å‡å°‘LiDARæ•°æ®çš„æ ‡æ³¨å·¥ä½œé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æœ€æ–°çš„æ‰©æ•£æ¨¡å‹ï¼ŒLiDARåŸºçš„å¤§è§„æ¨¡3Dåœºæ™¯ç”Ÿæˆå–å¾—å·¨å¤§æˆåŠŸã€‚</li>
<li>ç°æœ‰çš„èŒƒå›´è§†å›¾æ–¹æ³•ä¸»è¦ç”Ÿæˆæœªæ ‡è®°çš„LiDARåœºæ™¯ã€‚</li>
<li>Spiralæ˜¯ä¸€ç§æ–°å‹çš„èŒƒå›´è§†å›¾LiDARæ‰©æ•£æ¨¡å‹ï¼Œèƒ½åŒæ—¶ç”Ÿæˆæ·±åº¦ã€åå°„å›¾åƒå’Œè¯­ä¹‰åœ°å›¾ã€‚</li>
<li>Spiralåœ¨å‚æ•°è§„æ¨¡æœ€å°çš„æƒ…å†µä¸‹å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>Spiralç”Ÿæˆçš„èŒƒå›´å›¾åƒå¯ä»¥æœ‰æ•ˆç”¨äºåˆæˆæ•°æ®å¢å¼ºï¼Œå‡å°‘LiDARæ•°æ®çš„æ ‡æ³¨å·¥ä½œé‡ã€‚</li>
<li>Spiralé€šè¿‡ç»“åˆèŒƒå›´è§†å›¾è¡¨ç¤ºçš„ä¼˜åŠ¿ï¼Œå¦‚è®¡ç®—æ•ˆç‡é«˜å’Œç®€åŒ–ç½‘ç»œè®¾è®¡ï¼Œå®ç°äº†æ€§èƒ½çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.22643v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.22643v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.22643v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.22643v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Spatial-Knowledge-Graph-Guided-Multimodal-Synthesis"><a href="#Spatial-Knowledge-Graph-Guided-Multimodal-Synthesis" class="headerlink" title="Spatial Knowledge Graph-Guided Multimodal Synthesis"></a>Spatial Knowledge Graph-Guided Multimodal Synthesis</h2><p><strong>Authors:Yida Xue, Zhen Bi, Jinnan Yang, Jungang Lou, Kehai Chen, Min Zhang, Huajun Chen, Ningyu Zhang</strong></p>
<p>Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. Our approach addresses this critical gap by providing a systematic framework for generating spatially coherent data. In this work, we introduce SKG2DATA, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2DATA employs an automated pipeline for constructing Spatial Knowledge Graph (SKG) that effectively captures human-like spatial cognition, including directional and distance relationships. These structured representations then serve as precise guidance for our integrated synthesis pipeline, where a diffusion model generates spatially-consistent images while a MLLM produces corresponding textual descriptions. The automated construction of SKG enables scalable generation of diverse yet realistic spatial configurations, overcoming the limitations of manual data collection and annotation. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, enhance the spatial perception and reasoning abilities of MLLMs markedly, albeit with a slight cost to their general capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/Knowledge2Data">https://github.com/zjunlp/Knowledge2Data</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—å¢å¼ºäº†å…¶èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢çš„èƒ½åŠ›ä»å­˜åœ¨æ˜æ˜¾çš„å±€é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå¤šæ¨¡æ€æ•°æ®åˆæˆæä¾›äº†ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç¡®ä¿åˆæˆæ•°æ®ç¬¦åˆç©ºé—´å¸¸è¯†å¹¶éæ˜“äº‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æä¾›ç”Ÿæˆç©ºé—´è¿è´¯æ•°æ®çš„ç³»ç»Ÿæ¡†æ¶æ¥è§£å†³è¿™ä¸€å…³é”®å·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SKG2DATAï¼Œè¿™æ˜¯ä¸€ç§ç”±ç©ºé—´çŸ¥è¯†å›¾è°±å¼•å¯¼çš„æ–°å‹å¤šæ¨¡æ€åˆæˆæ–¹æ³•ï¼ŒåŸºäºçŸ¥è¯†åˆ°æ•°æ®ç”Ÿæˆçš„æ¦‚å¿µã€‚SKG2DATAé‡‡ç”¨è‡ªåŠ¨åŒ–ç®¡é“æ„å»ºç©ºé—´çŸ¥è¯†å›¾ï¼ˆSKGï¼‰ï¼Œæœ‰æ•ˆæ•æ‰äººç±»çš„ç©ºé—´è®¤çŸ¥ï¼ŒåŒ…æ‹¬æ–¹å‘å’Œè·ç¦»å…³ç³»ã€‚è¿™äº›ç»“æ„åŒ–è¡¨ç¤ºå½¢å¼ç„¶åä¸ºæˆ‘ä»¬é›†æˆçš„åˆæˆç®¡é“æä¾›ç²¾ç¡®çš„æŒ‡å¯¼ï¼Œå…¶ä¸­æ‰©æ•£æ¨¡å‹ç”Ÿæˆç©ºé—´ä¸€è‡´çš„å›¾åƒï¼Œè€ŒMLLMåˆ™äº§ç”Ÿç›¸åº”çš„æ–‡æœ¬æè¿°ã€‚SKGçš„è‡ªåŠ¨æ„å»ºèƒ½å¤Ÿå®ç°å„ç§çœŸå®ç©ºé—´é…ç½®çš„è§„æ¨¡åŒ–ç”Ÿæˆï¼Œå…‹æœäº†æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ³¨é‡Šçš„å±€é™æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä»å„ç§ç©ºé—´çŸ¥è¯†ä¸­åˆæˆçš„æ•°æ®ï¼ŒåŒ…æ‹¬æ–¹å‘å’Œè·ç¦»ï¼Œæ˜¾è‘—å¢å¼ºäº†MLLMçš„ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œå°½ç®¡å…¶æ•´ä½“èƒ½åŠ›ç•¥æœ‰ä¸‹é™ã€‚æˆ‘ä»¬å¸Œæœ›åŸºäºçŸ¥è¯†çš„æ•°æ®åˆæˆç†å¿µèƒ½å¤Ÿä¿ƒè¿›ç©ºé—´æ™ºèƒ½çš„å‘å±•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/Knowledge2Data%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjunlp/Knowledge2Dataæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22633v2">PDF</a> IEEE&#x2F;ACM Transactions on Audio, Speech and Language Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢çš„å±€é™ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€åˆæˆæ–¹æ³•SKG2DATAã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºç©ºé—´çŸ¥è¯†å›¾è°±ï¼ˆSKGï¼‰æ¥æŒ‡å¯¼æ•°æ®åˆæˆï¼Œå®ç°ç©ºé—´ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œåˆæˆæ•°æ®æé«˜äº†MLLMsçš„ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è™½æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚</li>
<li>å¤šæ¨¡æ€æ•°æ®åˆæˆæ˜¯è§£å†³è¿™ä¸€æŒ‘æˆ˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>SKG2DATAæ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€åˆæˆæ–¹æ³•ï¼Œé€šè¿‡ç©ºé—´çŸ¥è¯†å›¾è°±ï¼ˆSKGï¼‰æŒ‡å¯¼æ•°æ®åˆæˆã€‚</li>
<li>SKGæœ‰æ•ˆæ•æ‰äººç±»çš„ç©ºé—´è®¤çŸ¥ï¼ŒåŒ…æ‹¬æ–¹å‘å’Œè·ç¦»å…³ç³»ã€‚</li>
<li>åˆæˆæ•°æ®æé«˜äº†MLLMsçš„ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>SKGçš„è‡ªåŠ¨æ„å»ºå®ç°äº†ç©ºé—´é…ç½®çš„å¤šæ ·æ€§å’Œç°å®æ€§çš„å¯ä¼¸ç¼©ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.22633v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.22633v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.22633v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.22633v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.22633v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Joint-Reconstruction-of-Activity-and-Attenuation-in-PET-by-Diffusion-Posterior-Sampling-in-Wavelet-Coefficient-Space"><a href="#Joint-Reconstruction-of-Activity-and-Attenuation-in-PET-by-Diffusion-Posterior-Sampling-in-Wavelet-Coefficient-Space" class="headerlink" title="Joint Reconstruction of Activity and Attenuation in PET by Diffusion   Posterior Sampling in Wavelet Coefficient Space"></a>Joint Reconstruction of Activity and Attenuation in PET by Diffusion   Posterior Sampling in Wavelet Coefficient Space</h2><p><strong>Authors:ClÃ©mentine Phung-Ngoc, Alexandre Bousse, Antoine De Paepe, Thibaut Merlin, Baptiste Laurent, Hong-Phuong Dang, Olivier Saut, Dimitris Visvikis</strong></p>
<p>Attenuation correction (AC) is necessary for accurate activity quantification in positron emission tomography (PET). Conventional reconstruction methods typically rely on attenuation maps derived from a co-registered computed tomography (CT) or magnetic resonance imaging (MRI) scan. However, this additional scan may complicate the imaging workflow, introduce misalignment artifacts and increase radiation exposure. In this paper, we propose a joint reconstruction of activity and attenuation (JRAA) approach that eliminates the need for auxiliary anatomical imaging by relying solely on emission data. This framework combines wavelet diffusion model (WDM) and diffusion posterior sampling (DPS) to reconstruct fully three-dimensional (3-D) data. Experimental results show our method outperforms maximum likelihood activity and attenuation (MLAA) and MLAA with U-Net-based post processing, and yields high-quality noise-free reconstructions across various count settings when time-of-flight (TOF) information is available. It is also able to reconstruct non-TOF data, although the reconstruction quality significantly degrades in low-count (LC) conditions, limiting its practical effectiveness in such settings. Nonetheless, a non-TOF Biograph mMR data reconstruction with joint scatter estimation highlights the potential of the method for clinical applications. This approach represents a step towards stand-alone PET imaging by reducing the dependence on anatomical modalities while maintaining quantification accuracy, even in low-count scenarios when TOF information is available. Code will soon be available on GitHub at <a target="_blank" rel="noopener" href="https://github.com/clemphg/jraa-dps">https://github.com/clemphg/jraa-dps</a>. </p>
<blockquote>
<p>è¡°å‡æ ¡æ­£ï¼ˆACï¼‰åœ¨æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰çš„å‡†ç¡®æ´»åŠ¨é‡åŒ–ä¸­æ˜¯å¿…è¦çš„ã€‚ä¼ ç»Ÿé‡å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºä»å…±æ³¨å†Œçš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æä¸­å¾—å‡ºçš„è¡°å‡å›¾ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé¢å¤–çš„æ‰«æå¯èƒ½ä¼šä½¿æˆåƒå·¥ä½œæµç¨‹å¤æ‚åŒ–ï¼Œå¼•å…¥é”™ä½ä¼ªå½±å¹¶å¢åŠ è¾å°„æš´éœ²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆé‡å»ºæ´»åŠ¨å’Œè¡°å‡ï¼ˆJRAAï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä¾é å‘å°„æ•°æ®ï¼Œæ— éœ€è¾…åŠ©è§£å‰–æˆåƒã€‚è¯¥æ¡†æ¶ç»“åˆäº†å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰æ¥é‡å»ºå…¨ä¸‰ç»´ï¼ˆ3-Dï¼‰æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å¤§å¯èƒ½æ€§æ´»åŠ¨å’Œè¡°å‡ï¼ˆMLAAï¼‰ä»¥åŠä½¿ç”¨U-Netè¿›è¡Œåå¤„ç†çš„MLAAï¼Œå¹¶åœ¨å„ç§è®¡æ•°è®¾ç½®ä¸‹ï¼Œå½“é£è¡Œæ—¶é—´ï¼ˆTOFï¼‰ä¿¡æ¯å¯ç”¨æ—¶ï¼Œäº§ç”Ÿé«˜è´¨é‡çš„æ— å™ªå£°é‡å»ºã€‚è™½ç„¶å®ƒä¹Ÿèƒ½å¤Ÿé‡å»ºéTOFæ•°æ®ï¼Œä½†åœ¨ä½è®¡æ•°æ¡ä»¶ä¸‹é‡å»ºè´¨é‡ä¼šæ˜¾è‘—ä¸‹é™ï¼Œè¿™åœ¨å®è·µä¸­é™åˆ¶äº†å…¶åœ¨è¿™ç§ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä½¿ç”¨è”åˆæ•£å°„ä¼°è®¡çš„éTOF Biograph mMRæ•°æ®é‡å»ºçªå‡ºäº†è¯¥æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚è¿™ç§æ–¹æ³•æœç€ç‹¬ç«‹PETæˆåƒè¿ˆå‡ºäº†ä¸€æ­¥ï¼Œé€šè¿‡å‡å°‘å¯¹è§£å‰–æ¨¡æ€çš„ä¾èµ–åŒæ—¶ä¿æŒé‡åŒ–å‡†ç¡®æ€§ï¼Œå³ä½¿åœ¨ä½è®¡æ•°åœºæ™¯ä¸­å½“TOFä¿¡æ¯å¯ç”¨æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä»£ç å¾ˆå¿«å°†åœ¨GitHubä¸Šæä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/clemphg/jraa-dps">https://github.com/clemphg/jraa-dps</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18782v4">PDF</a> 11 pages, 8 figures, 3 tables</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‘å°„æ•°æ®çš„è”åˆé‡å»ºæ´»åŠ¨åŠè¡°å‡ï¼ˆJRAAï¼‰æ–¹æ³•ï¼Œæ— éœ€é¢å¤–çš„è§£å‰–æˆåƒã€‚è¯¥æ–¹æ³•ç»“åˆäº†å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ï¼Œé‡å»ºäº†å…¨ä¸‰ç»´æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ—¶é—´é£è¡Œï¼ˆTOFï¼‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºæœ€å¤§ä¼¼ç„¶æ´»åŠ¨åŠè¡°å‡ï¼ˆMLAAï¼‰æ–¹æ³•åŠå…¶U-Netåå¤„ç†ï¼Œå¹¶èƒ½è¿›è¡ŒéTOFæ•°æ®é‡å»ºã€‚å°½ç®¡åœ¨ä½è®¡æ•°æ¡ä»¶ä¸‹é‡å»ºè´¨é‡æœ‰æ‰€ä¸‹é™ï¼Œä½†åœ¨ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡°å‡æ ¡æ­£ï¼ˆACï¼‰åœ¨æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ä¸­çš„æ´»åŠ¨é‡åŒ–æ˜¯å¿…è¦çš„ã€‚</li>
<li>ä¼ ç»Ÿé‡å»ºæ–¹æ³•ä¾èµ–äºä»åˆå¹¶çš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«æå¾—åˆ°çš„è¡°å‡å›¾ã€‚</li>
<li>æå‡ºçš„è”åˆé‡å»ºæ´»åŠ¨åŠè¡°å‡ï¼ˆJRAAï¼‰æ–¹æ³•æ¶ˆé™¤äº†å¯¹è¾…åŠ©è§£å‰–æˆåƒçš„éœ€æ±‚ï¼Œä»…ä¾èµ–å‘å°„æ•°æ®ã€‚</li>
<li>JRAAæ–¹æ³•ç»“åˆäº†å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ï¼Œä»¥é‡å»ºå…¨ä¸‰ç»´æ•°æ®ã€‚</li>
<li>åœ¨æœ‰æ—¶é—´é£è¡Œï¼ˆTOFï¼‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒJRAAæ–¹æ³•æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶èƒ½è¿›è¡ŒéTOFæ•°æ®é‡å»ºã€‚</li>
<li>åœ¨ä½è®¡æ•°æ¡ä»¶ä¸‹ï¼ŒJRAAæ–¹æ³•çš„é‡å»ºè´¨é‡æœ‰æ‰€ä¸‹é™ï¼Œé™åˆ¶äº†å…¶åœ¨è¿™äº›åœºæ™¯ä¸­çš„å®é™…åº”ç”¨æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.18782v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.18782v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.18782v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.18782v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply"><a href="#Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply" class="headerlink" title="Diffusion Classifiers Understand Compositionality, but Conditions Apply"></a>Diffusion Classifiers Understand Compositionality, but Conditions Apply</h2><p><strong>Authors:Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach</strong></p>
<p>Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark \textsc{Self-Bench} comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality">https://github.com/eugene6923/Diffusion-Classifiers-Compositionality</a>. </p>
<blockquote>
<p>ç†è§£è§†è§‰åœºæ™¯å¯¹äººç±»æ™ºèƒ½è‡³å…³é‡è¦ã€‚è™½ç„¶åˆ¤åˆ«æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç»„åˆç†è§£æ–¹é¢å¾€å¾€é‡åˆ°å›°éš¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤æ‚åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™è¡¨æ˜å…¶å…·æœ‰å†…åœ¨çš„ç»„æˆèƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œé›¶æ ·æœ¬æ‰©æ•£åˆ†ç±»å™¨å·²è¢«æå‡ºç”¨äºå°†æ‰©æ•£æ¨¡å‹é‡æ–°ç”¨äºåˆ¤åˆ«ä»»åŠ¡ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œåœ¨åˆ¤åˆ«ç»„åˆåœºæ™¯æ–¹é¢æä¾›äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ç”±äºåŸºå‡†æµ‹è¯•æ•°é‡è¾ƒå°‘ä¸”å¯¹æ¨¡å‹æˆåŠŸçš„æ¡ä»¶åˆ†æç›¸å¯¹è‚¤æµ…ï¼Œè¿™äº›ç»“æœä»å±äºåˆæ­¥é˜¶æ®µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17955v3">PDF</a> NeurIPS 2025 Datasets and Benchmarks</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤æ‚åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å†…åœ¨çš„ç»„åˆèƒ½åŠ›ã€‚è¿‘æœŸæå‡ºçš„é›¶æ ·æœ¬æ‰©æ•£åˆ†ç±»å™¨å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåˆ¤åˆ«ä»»åŠ¡ï¼Œä½†åœ¨ç»„åˆåœºæ™¯ä¸‹çš„åˆ¤åˆ«èƒ½åŠ›ç ”ç©¶å°šæµ…ã€‚æœ¬ç ”ç©¶å…¨é¢æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å¤šç§ç»„åˆä»»åŠ¡ä¸Šçš„åˆ¤åˆ«èƒ½åŠ›ï¼Œæ¶‰åŠä¸‰ä¸ªæ‰©æ•£æ¨¡å‹ã€10ä¸ªæ•°æ®é›†å’Œ30å¤šä¸ªä»»åŠ¡ã€‚ç ”ç©¶è¿˜ä»‹ç»äº†æ–°è¯Šæ–­åŸºå‡†Self-Benchï¼Œä»¥åˆ†æç›®æ ‡æ•°æ®é›†é¢†åŸŸå¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¢è®¨äº†æ—¶é—´æ­¥é•¿æƒé‡çš„é‡è¦æ€§ï¼Œæ­ç¤ºäº†é¢†åŸŸå·®è·ä¸æ—¶é—´æ­¥é•¿æ•æ„Ÿæ€§çš„å…³ç³»ã€‚æ€»ä½“è€Œè¨€ï¼Œæ‰©æ•£åˆ†ç±»å™¨ç†è§£ç»„åˆæ€§ï¼Œä½†æ¡ä»¶åº”ç”¨æ˜¯å…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤æ‚åœºæ™¯æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå±•ç°å‡ºå†…åœ¨çš„ç»„åˆèƒ½åŠ›ã€‚</li>
<li>é›¶æ ·æœ¬æ‰©æ•£åˆ†ç±»å™¨å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåˆ¤åˆ«ä»»åŠ¡ã€‚</li>
<li>ç›®å‰å¯¹äºæ‰©æ•£æ¨¡å‹åœ¨ç»„åˆåœºæ™¯ä¸‹çš„åˆ¤åˆ«èƒ½åŠ›ç ”ç©¶å°šæµ…ã€‚</li>
<li>æœ¬ç ”ç©¶å…¨é¢è¯„ä¼°äº†ä¸‰ä¸ªæ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªç»„åˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ–°çš„è¯Šæ–­åŸºå‡†Self-Benchï¼Œä»¥åˆ†æç›®æ ‡æ•°æ®é›†é¢†åŸŸå¯¹æ‰©æ•£æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>ç ”ç©¶å‘ç°æ—¶é—´æ­¥é•¿æƒé‡çš„é‡è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†é¢†åŸŸå·®è·ä¸æ—¶é—´æ­¥é•¿æ•æ„Ÿæ€§ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨ç†è§£ç»„åˆæ€§ï¼Œä½†åº”ç”¨æ¡ä»¶æ˜¯å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.17955v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.17955v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.17955v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2505.17955v3/page_4_2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LocDiff-Identifying-Locations-on-Earth-by-Diffusing-in-the-Hilbert-Space"><a href="#LocDiff-Identifying-Locations-on-Earth-by-Diffusing-in-the-Hilbert-Space" class="headerlink" title="LocDiff: Identifying Locations on Earth by Diffusing in the Hilbert   Space"></a>LocDiff: Identifying Locations on Earth by Diffusing in the Hilbert   Space</h2><p><strong>Authors:Zhangyu Wang, Zeping Liu, Jielu Zhang, Zhongliang Zhou, Qian Cao, Nemin Wu, Lan Mu, Yang Song, Yiqun Xie, Ni Lao, Gengchen Mai</strong></p>
<p>Image geolocalization is a fundamental yet challenging task, aiming at inferring the geolocation on Earth where an image is taken. State-of-the-art methods employ either grid-based classification or gallery-based image-location retrieval, whose spatial generalizability significantly suffers if the spatial distribution of test images does not align with the choices of grids and galleries. Recently emerging generative approaches, while getting rid of grids and galleries, use raw geographical coordinates and suffer quality losses due to their lack of multi-scale information. To address these limitations, we propose a multi-scale latent diffusion model called LocDiff for image geolocalization. We developed a novel positional encoding-decoding framework called Spherical Harmonics Dirac Delta (SHDD) Representations, which encodes points on a spherical surface (e.g., geolocations on Earth) into a Hilbert space of Spherical Harmonics coefficients and decodes points (geolocations) by mode-seeking on spherical probability distributions. We also propose a novel SirenNet-based architecture (CS-UNet) to learn an image-based conditional backward process in the latent SHDD space by minimizing a latent KL-divergence loss. To the best of our knowledge, LocDiff is the first image geolocalization model that performs latent diffusion in a multi-scale location encoding space and generates geolocations under the guidance of images. Experimental results show that LocDiff can outperform all state-of-the-art grid-based, retrieval-based, and diffusion-based baselines across 5 challenging global-scale image geolocalization datasets, and demonstrates significantly stronger generalizability to unseen geolocations. </p>
<blockquote>
<p>å›¾åƒåœ°ç†å®šä½æ˜¯ä¸€é¡¹åŸºç¡€ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æ¨æ–­å›¾åƒæ‹æ‘„çš„åœ°çƒä¸Šçš„åœ°ç†ä½ç½®ã€‚æœ€æ–°æ–¹æ³•é‡‡ç”¨åŸºäºç½‘æ ¼çš„åˆ†ç±»æˆ–åŸºäºç”»å»Šçš„å›¾åƒä½ç½®æ£€ç´¢ï¼Œå¦‚æœæµ‹è¯•å›¾åƒçš„ç©ºé—´åˆ†å¸ƒä¸ç½‘æ ¼å’Œç”»å»Šçš„é€‰æ‹©ä¸ä¸€è‡´ï¼Œå…¶ç©ºé—´æ³›åŒ–èƒ½åŠ›ä¼šé­å—æ˜¾è‘—å½±å“ã€‚æœ€è¿‘å‡ºç°çš„ç”Ÿæˆæ–¹æ³•è™½ç„¶æ‘†è„±äº†ç½‘æ ¼å’Œç”»å»Šï¼Œä½†ä½¿ç”¨åŸå§‹åœ°ç†åæ ‡ï¼Œç”±äºç¼ºä¹å¤šå°ºåº¦ä¿¡æ¯è€Œé­å—è´¨é‡æŸå¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå›¾åƒåœ°ç†å®šä½çš„å¤šå°ºåº¦æ½œåœ¨æ‰©æ•£æ¨¡å‹LocDiffã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹çš„ä½ç½®ç¼–ç -è§£ç æ¡†æ¶ï¼Œç§°ä¸ºçƒé¢è°æ³¢ç‹„æ‹‰å…‹ä¸‰è§’ï¼ˆSHDDï¼‰è¡¨ç¤ºæ³•ï¼Œå®ƒå°†çƒé¢ä¸Šçš„ä¸€ç‚¹ï¼ˆä¾‹å¦‚åœ°çƒä¸Šçš„åœ°ç†ä½ç½®ï¼‰ç¼–ç ä¸ºçƒé¢è°æ³¢ç³»æ•°çš„å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼Œå¹¶é€šè¿‡çƒæ¦‚ç‡åˆ†å¸ƒçš„æ¨¡å¼æœç´¢æ¥è§£ç ç‚¹ï¼ˆåœ°ç†ä½ç½®ï¼‰ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºSirenNetçš„æ–°å‹æ¶æ„ï¼ˆCS-UNetï¼‰ï¼Œç”¨äºåœ¨æ½œåœ¨SHDDç©ºé—´ä¸­å­¦ä¹ åŸºäºå›¾åƒçš„æ¡ä»¶é€†å‘è¿‡ç¨‹ï¼Œé€šè¿‡æœ€å°åŒ–æ½œåœ¨KLæ•£åº¦æŸå¤±ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒLocDiffæ˜¯é¦–ä¸ªåœ¨å¤šå±‚æ¬¡ä½ç½®ç¼–ç ç©ºé—´ä¸­æ‰§è¡Œæ½œåœ¨æ‰©æ•£å¹¶æ ¹æ®å›¾åƒæŒ‡å¯¼ç”Ÿæˆåœ°ç†ä½ç½®çš„å›¾åƒåœ°ç†å®šä½æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLocDiffåœ¨5ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å…¨çƒå›¾åƒåœ°ç†å®šä½æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºæ‰€æœ‰æœ€å…ˆè¿›çš„åŸºäºç½‘æ ¼ã€åŸºäºæ£€ç´¢å’ŒåŸºäºæ‰©æ•£çš„åŸºçº¿ï¼Œå¹¶ä¸”å¯¹æœªè§è¿‡çš„åœ°ç†ä½ç½®è¡¨ç°å‡ºæ˜¾è‘—æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18142v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åƒåœ°ç†å®šä½æ˜¯ä¸€é¡¹åŸºæœ¬ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æ¨æ–­å›¾åƒæ‹æ‘„çš„åœ°çƒåœ°ç†ä½ç½®ã€‚æœ€æ–°æ–¹æ³•é‡‡ç”¨åŸºäºç½‘æ ¼çš„åˆ†ç±»æˆ–åŸºäºç”»å»Šçš„å›¾åƒä½ç½®æ£€ç´¢ï¼Œè‹¥æµ‹è¯•å›¾åƒçš„ç©ºé—´åˆ†å¸ƒä¸ç½‘æ ¼å’Œç”»å»Šçš„é€‰æ‹©ä¸ä¸€è‡´ï¼Œåˆ™å…¶ç©ºé—´æ³›åŒ–èƒ½åŠ›ä¼šå¤§å¤§å—æŸã€‚æ–°å…´çš„ç”Ÿæˆå¼æ–¹æ³•è™½æ‘†è„±äº†ç½‘æ ¼å’Œç”»å»Šçš„é™åˆ¶ï¼Œä½†ç”±äºç¼ºä¹å¤šå°ºåº¦ä¿¡æ¯è€Œå¯¼è‡´è´¨é‡æŸå¤±ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå›¾åƒåœ°ç†å®šä½çš„å¤šå°ºåº¦æ½œåœ¨æ‰©æ•£æ¨¡å‹LocDiffã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åä¸ºçƒå½¢è°æ³¢ç‹„æ‹‰å…‹å¢é‡ï¼ˆSHDDï¼‰è¡¨ç¤ºçš„æ–°å‹ä½ç½®ç¼–ç è§£ç æ¡†æ¶ï¼Œå°†çƒå½¢è¡¨é¢ä¸Šçš„ç‚¹ï¼ˆä¾‹å¦‚åœ°çƒä¸Šçš„åœ°ç†ä½ç½®ï¼‰ç¼–ç ä¸ºçƒå½¢è°æ³¢ç³»æ•°çš„Hilbertç©ºé—´ï¼Œå¹¶é€šè¿‡çƒå½¢æ¦‚ç‡åˆ†å¸ƒçš„æ¨¡å¼æœç´¢æ¥è§£ç ç‚¹ï¼ˆåœ°ç†ä½ç½®ï¼‰ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºSirenNetçš„æ¶æ„ï¼ˆCS-UNetï¼‰ï¼Œä»¥åœ¨æ½œåœ¨SHDDç©ºé—´ä¸­å­¦ä¹ åŸºäºå›¾åƒçš„æ¡ä»¶é€†å‘è¿‡ç¨‹ï¼Œé€šè¿‡æœ€å°åŒ–æ½œåœ¨KLæ•£åº¦æŸå¤±ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒLocDiffæ˜¯é¦–ä¸ªåœ¨å¤šç»´ä½ç½®ç¼–ç ç©ºé—´ä¸­è¿›è¡Œæ½œåœ¨æ‰©æ•£çš„å›¾åƒåœ°ç†å®šä½æ¨¡å‹ï¼Œå¯åœ¨å›¾åƒæŒ‡å¯¼ä¸‹ç”Ÿæˆåœ°ç†ä½ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLocDiffåœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å…¨çƒå›¾åƒåœ°ç†å®šä½æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºæ‰€æœ‰å…ˆè¿›çš„åŸºäºç½‘æ ¼ã€æ£€ç´¢å’Œæ‰©æ•£çš„åŸºçº¿ï¼Œå¹¶å¯¹æœªè§è¿‡çš„åœ°ç†ä½ç½®è¡¨ç°å‡ºæ˜¾è‘—æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåœ°ç†å®šä½æ˜¯æ¨æ–­å›¾åƒæ‹æ‘„åœ°ç†ä½ç½®çš„ä»»åŠ¡ï¼Œå…·æœ‰æŒ‘æˆ˜æ€§å’Œé‡è¦æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚ç½‘æ ¼åˆ†ç±»å’Œç”»å»Šæ£€ç´¢å­˜åœ¨ç©ºé—´æ³›åŒ–èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚</li>
<li>æ–°å…´ç”Ÿæˆå¼æ–¹æ³•è™½ç„¶æ‘†è„±äº†å¯¹ç½‘æ ¼å’Œç”»å»Šçš„ä¾èµ–ï¼Œä½†é¢ä¸´è´¨é‡æŸå¤±çš„é—®é¢˜ã€‚</li>
<li>LocDiffæ¨¡å‹é€šè¿‡ç»“åˆå¤šå°ºåº¦ä¿¡æ¯å’Œæ‰©æ•£æ¨¡å‹è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LocDiffé‡‡ç”¨æ–°å‹ä½ç½®ç¼–ç è§£ç æ¡†æ¶SHDDè¡¨ç¤ºï¼Œå°†åœ°ç†ä½ç½®ç¼–ç ä¸ºHilbertç©ºé—´ã€‚</li>
<li>LocDiffåˆ©ç”¨SirenNetæ¶æ„ï¼ˆCS-UNetï¼‰å­¦ä¹ å›¾åƒæ¡ä»¶ä¸‹çš„é€†å‘è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2503.18142v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2503.18142v2/page_1_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Split-Gibbs-Discrete-Diffusion-Posterior-Sampling"><a href="#Split-Gibbs-Discrete-Diffusion-Posterior-Sampling" class="headerlink" title="Split Gibbs Discrete Diffusion Posterior Sampling"></a>Split Gibbs Discrete Diffusion Posterior Sampling</h2><p><strong>Authors:Wenda Chu, Zihui Wu, Yifan Chen, Yang Song, Yisong Yue</strong></p>
<p>We study the problem of posterior sampling in discrete-state spaces using discrete diffusion models. While posterior sampling methods for continuous diffusion models have achieved remarkable progress, analogous methods for discrete diffusion models remain challenging. In this work, we introduce a principled plug-and-play discrete diffusion posterior sampling algorithm based on split Gibbs sampling, which we call SGDD. Our algorithm enables reward-guided generation and solving inverse problems in discrete-state spaces. We demonstrate the convergence of SGDD to the target posterior distribution and verify this through controlled experiments on synthetic benchmarks. Our method enjoys state-of-the-art posterior sampling performance on a range of benchmarks for discrete data, including DNA sequence design, discrete image inverse problems, and music infilling, achieving more than 30% improved performance compared to existing baselines. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/chuwd19/Split-Gibbs-Discrete-Diffusion-Posterior-Sampling">https://github.com/chuwd19/Split-Gibbs-Discrete-Diffusion-Posterior-Sampling</a>. </p>
<blockquote>
<p>æˆ‘ä»¬åˆ©ç”¨ç¦»æ•£æ‰©æ•£æ¨¡å‹ç ”ç©¶ç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­çš„åé‡‡æ ·é—®é¢˜ã€‚è™½ç„¶è¿ç»­æ‰©æ•£æ¨¡å‹çš„åé‡‡æ ·æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„åé‡‡æ ·æ–¹æ³•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåˆ†è£‚å‰å¸ƒæ–¯é‡‡æ ·çš„åŸåˆ™æ€§å³æ’å³ç”¨ç¦»æ•£æ‰©æ•£åé‡‡æ ·ç®—æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºSGDDã€‚æˆ‘ä»¬çš„ç®—æ³•èƒ½å¤Ÿå®ç°å¥–åŠ±å¼•å¯¼ç”Ÿæˆå’Œç¦»æ•£çŠ¶æ€ç©ºé—´ä¸­çš„é€†é—®é¢˜è§£å†³ã€‚æˆ‘ä»¬è¯æ˜äº†SGDDåœ¨ç›®æ ‡åéªŒåˆ†å¸ƒä¸Šçš„æ”¶æ•›æ€§ï¼Œå¹¶é€šè¿‡åˆæˆåŸºå‡†çš„å—æ§å®éªŒéªŒè¯äº†è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¦»æ•£æ•°æ®çš„ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸­äº«æœ‰æœ€å…ˆè¿›çš„åé‡‡æ ·æ€§èƒ½ï¼ŒåŒ…æ‹¬DNAåºåˆ—è®¾è®¡ã€ç¦»æ•£å›¾åƒé€†é—®é¢˜å’ŒéŸ³ä¹å¡«å……ï¼Œä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼Œæ€§èƒ½æé«˜äº†30%ä»¥ä¸Šã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/chuwd19/Split-Gibbs-Discrete-Diffusion-Posterior-Sampling%E3%80%82">https://github.com/chuwd19/Split-Gibbs-Discrete-Diffusion-Posterior-Samplingã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01161v3">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„åé‡‡æ ·é—®é¢˜ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºåˆ†å‰²Gibbsé‡‡æ ·çš„ç¦»æ•£æ‰©æ•£åé‡‡æ ·ç®—æ³•SGDDï¼Œå¯è§£å†³ç¦»æ•£çŠ¶æ€ç©ºé—´çš„å¥–åŠ±å¼•å¯¼ç”Ÿæˆå’Œé€†é—®é¢˜ã€‚æˆ‘ä»¬è¯æ˜äº†SGDDåœ¨ç›®æ ‡åéªŒåˆ†å¸ƒä¸Šçš„æ”¶æ•›æ€§ï¼Œå¹¶åœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†æ§åˆ¶å®éªŒéªŒè¯ã€‚è¯¥æ–¹æ³•åœ¨ç¦»æ•£æ•°æ®åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„åé‡‡æ ·æ€§èƒ½ï¼ŒåŒ…æ‹¬DNAåºåˆ—è®¾è®¡ã€ç¦»æ•£å›¾åƒé€†é—®é¢˜å’ŒéŸ³ä¹å¡«å……ï¼Œä¸ç°æœ‰åŸºçº¿ç›¸æ¯”æ€§èƒ½æé«˜äº†30%ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„åé‡‡æ ·é—®é¢˜ã€‚</li>
<li>æå‡ºåŸºäºåˆ†å‰²Gibbsé‡‡æ ·çš„ç¦»æ•£æ‰©æ•£åé‡‡æ ·ç®—æ³•SGDDã€‚</li>
<li>SGDDç®—æ³•å¯å®ç°å¥–åŠ±å¼•å¯¼ç”Ÿæˆå’Œç¦»æ•£çŠ¶æ€ç©ºé—´çš„é€†é—®é¢˜è§£å†³ã€‚</li>
<li>è¯æ˜äº†SGDDåœ¨ç›®æ ‡åéªŒåˆ†å¸ƒä¸Šçš„æ”¶æ•›æ€§ã€‚</li>
<li>åœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†æ§åˆ¶å®éªŒéªŒè¯ã€‚</li>
<li>SGDDåœ¨DNAåºåˆ—è®¾è®¡ã€ç¦»æ•£å›¾åƒé€†é—®é¢˜å’ŒéŸ³ä¹å¡«å……ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2503.01161v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2503.01161v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2503.01161v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Multi-scale-Latent-Point-Consistency-Models-for-3D-Shape-Generation"><a href="#Multi-scale-Latent-Point-Consistency-Models-for-3D-Shape-Generation" class="headerlink" title="Multi-scale Latent Point Consistency Models for 3D Shape Generation"></a>Multi-scale Latent Point Consistency Models for 3D Shape Generation</h2><p><strong>Authors:Biâ€™an Du, Wei Hu, Renjie Liao</strong></p>
<p>Consistency Models (CMs) have significantly accelerated the sampling process in diffusion models, yielding impressive results in synthesizing high-resolution images. To explore and extend these advancements to point-cloud-based 3D shape generation, we propose a novel Multi-scale Latent Point Consistency Model (MLPCM). Our MLPCM follows a latent diffusion framework and introduces hierarchical levels of latent representations, ranging from point-level to super-point levels, each corresponding to a different spatial resolution. We design a multi-scale latent integration module along with 3D spatial attention to effectively denoise the point-level latent representations conditioned on those from multiple super-point levels. Additionally, we propose a latent consistency model, learned through consistency distillation, that compresses the prior into a one-step generator. This significantly improves sampling efficiency while preserving the performance of the original teacher model. Extensive experiments on standard benchmarks ShapeNet and ShapeNet-Vol demonstrate that MLPCM achieves a 100x speedup in the generation process, while surpassing state-of-the-art diffusion models in terms of both shape quality and diversity. </p>
<blockquote>
<p>ä¸€è‡´æ€§æ¨¡å‹ï¼ˆCMsï¼‰æ˜¾è‘—åŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹ä¸­çš„é‡‡æ ·è¿‡ç¨‹ï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ä¸ºäº†æ¢ç´¢å¹¶å°†è¿™äº›è¿›å±•æ‰©å±•åˆ°åŸºäºç‚¹äº‘çš„3Då½¢çŠ¶ç”Ÿæˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦æ½œåœ¨ç‚¹ä¸€è‡´æ€§æ¨¡å‹ï¼ˆMLPCMï¼‰ã€‚æˆ‘ä»¬çš„MLPCMéµå¾ªæ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†ä»ç‚¹åˆ°è¶…ç‚¹çº§åˆ«çš„åˆ†å±‚æ½œåœ¨è¡¨ç¤ºï¼Œæ¯ä¸ªè¡¨ç¤ºéƒ½å¯¹åº”ä¸åŒçš„ç©ºé—´åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦æ½œåœ¨é›†æˆæ¨¡å—ï¼Œç»“åˆ3Dç©ºé—´æ³¨æ„åŠ›ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹åŸºäºå¤šä¸ªè¶…ç‚¹çº§åˆ«çš„ç‚¹çº§æ½œåœ¨è¡¨ç¤ºè¿›è¡Œå»å™ªã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ä¸€è‡´æ€§è’¸é¦å­¦ä¹ çš„ä¸€è‡´æ€§æ¨¡å‹ï¼Œå®ƒå°†å…ˆéªŒå‹ç¼©ä¸ºä¸€æ­¥ç”Ÿæˆå™¨ã€‚è¿™æ˜¾è‘—æé«˜äº†é‡‡æ ·æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨ShapeNetå’ŒShapeNet-Volæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMLPCMåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°äº†100å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨å½¢çŠ¶è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢éƒ½è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19413v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„ç‚¹äº‘ä¸€è‡´æ€§æ¨¡å‹ï¼ˆMLPCMï¼‰ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œå¼•å…¥å¤šå±‚æ¬¡æ½œåœ¨è¡¨ç¤ºï¼Œé€šè¿‡å¤šå°ºåº¦æ½œåœ¨æ•´åˆæ¨¡å—å’Œä¸‰ç»´ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹ç‚¹çº§æ½œåœ¨è¡¨ç¤ºè¿›è¡Œå»å™ªå¤„ç†ã€‚åŒæ—¶ï¼Œé€šè¿‡ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯å­¦ä¹ æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼Œå°†å…ˆéªŒçŸ¥è¯†å‹ç¼©ä¸ºä¸€æ­¥ç”Ÿæˆå™¨ï¼Œæé«˜äº†é‡‡æ ·æ•ˆç‡å¹¶ä¿æŒåŸæ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨ShapeNetå’ŒShapeNet-Volç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒMLPCMå®ç°äº†ç”Ÿæˆè¿‡ç¨‹çš„ç™¾å€åŠ é€Ÿï¼ŒåŒæ—¶åœ¨å½¢çŠ¶è´¨é‡å’Œå¤šæ ·æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰æ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CMsï¼ˆä¸€è‡´æ€§æ¨¡å‹ï¼‰æ˜¾è‘—åŠ é€Ÿäº†æ‰©æ•£æ¨¡å‹ä¸­çš„é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç‚¹äº‘ä¸€è‡´æ€§æ¨¡å‹ï¼ˆMLPCMï¼‰ï¼Œç”¨äºåŸºäºç‚¹äº‘çš„3Då½¢çŠ¶ç”Ÿæˆã€‚</li>
<li>MLPCMé‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œå¼•å…¥å¤šå±‚æ¬¡æ½œåœ¨è¡¨ç¤ºï¼Œæ¶‰åŠç‚¹çº§åˆ°è¶…ç‚¹çº§çš„å±‚æ¬¡ã€‚</li>
<li>é€šè¿‡å¤šå°ºåº¦æ½œåœ¨æ•´åˆæ¨¡å—å’Œä¸‰ç»´ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆå»å™ªç‚¹çº§æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>åˆ©ç”¨ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯å­¦ä¹ æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼Œå°†å…ˆéªŒçŸ¥è¯†å‹ç¼©ä¸ºä¸€æ­¥ç”Ÿæˆå™¨ï¼Œæé«˜é‡‡æ ·æ•ˆç‡ã€‚</li>
<li>MLPCMåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ç”Ÿæˆè¿‡ç¨‹çš„ç™¾å€åŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2412.19413v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2412.19413v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2412.19413v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FIRE-Robust-Detection-of-Diffusion-Generated-Images-via-Frequency-Guided-Reconstruction-Error"><a href="#FIRE-Robust-Detection-of-Diffusion-Generated-Images-via-Frequency-Guided-Reconstruction-Error" class="headerlink" title="FIRE: Robust Detection of Diffusion-Generated Images via   Frequency-Guided Reconstruction Error"></a>FIRE: Robust Detection of Diffusion-Generated Images via   Frequency-Guided Reconstruction Error</h2><p><strong>Authors:Beilin Chu, Xuan Xu, Xin Wang, Yufei Zhang, Weike You, Linna Zhou</strong></p>
<p>The rapid advancement of diffusion models has significantly improved high-quality image generation, making generated content increasingly challenging to distinguish from real images and raising concerns about potential misuse. In this paper, we observe that diffusion models struggle to accurately reconstruct mid-band frequency information in real images, suggesting the limitation could serve as a cue for detecting diffusion model generated images. Motivated by this observation, we propose a novel method called Frequency-guided Reconstruction Error (FIRE), which, to the best of our knowledge, is the first to investigate the influence of frequency decomposition on reconstruction error. FIRE assesses the variation in reconstruction error before and after the frequency decomposition, offering a robust method for identifying diffusion model generated images. Extensive experiments show that FIRE generalizes effectively to unseen diffusion models and maintains robustness against diverse perturbations. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•æå¤§åœ°æé«˜äº†é«˜è´¨é‡å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ï¼Œä½¿å¾—ç”Ÿæˆçš„å†…å®¹è¶Šæ¥è¶Šéš¾ä»¥ä¸çœŸå®å›¾åƒåŒºåˆ†å¼€ï¼Œå¹¶å¼•å‘äº†å…³äºæ½œåœ¨è¯¯ç”¨çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ‰©æ•£æ¨¡å‹åœ¨å‡†ç¡®é‡å»ºçœŸå®å›¾åƒçš„ä¸­é¢‘ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™è¡¨æ˜è¿™ä¸€å±€é™å¯ä½œä¸ºæ£€æµ‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„ä¸€ç§çº¿ç´¢ã€‚å—æ­¤è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºé¢‘ç‡å¼•å¯¼é‡å»ºè¯¯å·®ï¼ˆFIREï¼‰çš„æ–°æ–¹æ³•ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ç ”ç©¶é¢‘ç‡åˆ†è§£å¯¹é‡å»ºè¯¯å·®çš„å½±å“ã€‚FIREè¯„ä¼°é¢‘ç‡åˆ†è§£å‰åçš„é‡å»ºè¯¯å·®å˜åŒ–ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç¨³å¥çš„æ–¹æ³•æ¥è¯†åˆ«æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFIREèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¯¹å„ç§æ‰°åŠ¨ä¿æŒç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07140v3">PDF</a> 14 pages, 14 figures. Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•æå¤§æå‡äº†é«˜è´¨é‡å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œç”Ÿæˆçš„å›¾åƒä¸ç°å®å›¾åƒéš¾ä»¥åŒºåˆ†ï¼Œå¼•å‘å…³äºæ½œåœ¨è¯¯ç”¨çš„æ‹…å¿§ã€‚æœ¬æ–‡å‘ç°æ‰©æ•£æ¨¡å‹åœ¨é‡å»ºçœŸå®å›¾åƒçš„ä¸­é¢‘ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¯ä½œä¸ºæ£€æµ‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„ä¾æ®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„æ–¹æ³•â€”â€”é¢‘ç‡å¼•å¯¼é‡å»ºè¯¯å·®ï¼ˆFIREï¼‰ï¼Œé¦–æ¬¡æ¢ç©¶é¢‘ç‡åˆ†è§£å¯¹é‡å»ºè¯¯å·®çš„å½±å“ã€‚FIREè¯„ä¼°é¢‘ç‡åˆ†è§£å‰åçš„é‡å»ºè¯¯å·®å˜åŒ–ï¼Œä¸ºè¯†åˆ«æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæä¾›ç¨³å¥æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒFIREèƒ½æœ‰æ•ˆæ³›åŒ–è‡³æœªè§è¿‡çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¯¹å„ç§æ‰°åŠ¨ä¿æŒç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œä¸ç°å®å›¾åƒéš¾ä»¥åŒºåˆ†ï¼Œå¼•å‘å…³äºæ½œåœ¨è¯¯ç”¨çš„æ‹…å¿§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨é‡å»ºçœŸå®å›¾åƒçš„ä¸­é¢‘ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºé¢‘ç‡å¼•å¯¼é‡å»ºè¯¯å·®ï¼ˆFIREï¼‰æ–¹æ³•ï¼Œæ¢ç©¶é¢‘ç‡åˆ†è§£å¯¹é‡å»ºè¯¯å·®çš„å½±å“ã€‚</li>
<li>FIREé€šè¿‡è¯„ä¼°é¢‘ç‡åˆ†è§£å‰åçš„é‡å»ºè¯¯å·®å˜åŒ–ï¼Œä¸ºè¯†åˆ«æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæä¾›ç¨³å¥æ–¹æ³•ã€‚</li>
<li>FIREæ–¹æ³•èƒ½æœ‰æ•ˆæ³›åŒ–è‡³æœªè§è¿‡çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>FIREå¯¹å¤šç§å›¾åƒæ‰°åŠ¨å…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºè¯†åˆ«æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2412.07140v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2412.07140v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2412.07140v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2412.07140v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="High-Resolution-Seismic-Waveform-Generation-using-Denoising-Diffusion"><a href="#High-Resolution-Seismic-Waveform-Generation-using-Denoising-Diffusion" class="headerlink" title="High Resolution Seismic Waveform Generation using Denoising Diffusion"></a>High Resolution Seismic Waveform Generation using Denoising Diffusion</h2><p><strong>Authors:Kadek Hendrawan Palgunadi, Andreas Bergmeister, Andrea Bosisio, Laura Ermert, Maria Koroni, NathanaÃ«l Perraudin, Simon Dirmeier, Men-Andrin Meier</strong></p>
<p>Accurate prediction and synthesis of seismic waveforms are crucial for seismic-hazard assessment and earthquake-resistant infrastructure design. Existing prediction methods, such as ground-motion models and physics-based wave-field simulations, often fail to capture the full complexity of seismic wavefields, particularly at higher frequencies. This study introduces HighFEM, a novel, computationally efficient, and scalable (i.e., capable of generating many seismograms simultaneously) generative model for high-frequency seismic-waveform generation. Our approach leverages a spectrogram representation of the seismic-waveform data, which is reduced to a lower-dimensional manifold via an autoencoder. A state-of-the-art diffusion model is trained to generate this latent representation conditioned on key input parameters: earthquake magnitude, recording distance, site conditions, hypocenter depth, and azimuthal gap. The model generates waveforms with frequency content up to 50 Hz. Any scalar ground-motion statistic, such as peak ground-motion amplitudes and spectral accelerations, can be readily derived from the synthesized waveforms. We validate our model using commonly employed seismological metrics and performance metrics from image-generation studies. Our results demonstrate that the openly available model can generate realistic high-frequency seismic waveforms across a wide range of input parameters, even in data-sparse regions. For the scalar ground-motion statistics commonly used in seismic-hazard and earthquake-engineering studies, we show that our model accurately reproduces both the median trends of the real data and their variability. To evaluate and compare the growing number of these and similar Generative Waveform Models (GWMs), we argue that they should be openly available and included in community ground-motion-model evaluation efforts. </p>
<blockquote>
<p>åœ°éœ‡æ³¢å½¢çš„ç²¾ç¡®é¢„æµ‹å’Œåˆæˆå¯¹äºåœ°éœ‡å±é™©è¯„ä¼°å’ŒæŠ—éœ‡åŸºç¡€è®¾æ–½è®¾è®¡è‡³å…³é‡è¦ã€‚ç°æœ‰çš„é¢„æµ‹æ–¹æ³•ï¼Œå¦‚åœ°é¢è¿åŠ¨æ¨¡å‹å’ŒåŸºäºç‰©ç†çš„æ³¢åœºæ¨¡æ‹Ÿï¼Œå¾€å¾€æ— æ³•æ•æ‰åˆ°åœ°éœ‡æ³¢åœºçš„å…¨éƒ¨å¤æ‚æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒé«˜é¢‘ç‡ä¸‹ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†HighFEMï¼Œè¿™æ˜¯ä¸€ç§è®¡ç®—æ•ˆç‡é«˜ã€å¯æ‰©å±•æ€§å¼ºçš„ç”Ÿæˆæ¨¡å‹ï¼Œå¯ç”¨äºé«˜é¢‘åœ°éœ‡æ³¢å½¢çš„ç”Ÿæˆã€‚ï¼ˆå¯æ‰©å±•æ€§å¼ºæŒ‡çš„æ˜¯èƒ½å¤ŸåŒæ—¶ç”Ÿæˆå¤šä¸ªåœ°éœ‡æ³¢ã€‚ï¼‰æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åœ°éœ‡æ³¢å½¢æ•°æ®çš„é¢‘è°±å›¾è¡¨ç¤ºï¼Œé€šè¿‡è‡ªç¼–ç å™¨å°†å…¶é™ä½åˆ°ä½ç»´æµå½¢ã€‚é‡‡ç”¨æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä»¥æ ¹æ®å…³é”®è¾“å…¥å‚æ•°ç”Ÿæˆè¿™ç§æ½œåœ¨è¡¨ç¤ºï¼šåœ°éœ‡éœ‡çº§ã€è®°å½•è·ç¦»ã€åœºåœ°æ¡ä»¶ã€éœ‡æºæ·±åº¦å’Œæ–¹ä½è§’é—´éš™ã€‚è¯¥æ¨¡å‹ç”Ÿæˆçš„æ³¢å½¢é¢‘ç‡æˆåˆ†é«˜è¾¾50èµ«å…¹ã€‚å¯ä»¥ä»åˆæˆçš„æ³¢å½¢ä¸­è½»æ˜“å¾—å‡ºä»»ä½•æ ‡é‡åœ°é¢è¿åŠ¨ç»Ÿè®¡é‡ï¼Œä¾‹å¦‚å³°å€¼åœ°é¢è¿åŠ¨å¹…åº¦å’Œè°±åŠ é€Ÿåº¦ã€‚æˆ‘ä»¬ä½¿ç”¨å¸¸ç”¨çš„åœ°éœ‡å­¦æŒ‡æ ‡å’Œå›¾åƒç”Ÿæˆç ”ç©¶ä¸­çš„æ€§èƒ½æŒ‡æ ‡æ¥éªŒè¯æˆ‘ä»¬çš„æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å…¬å¼€æ¨¡å‹å¯ä»¥åœ¨å¹¿æ³›çš„è¾“å…¥å‚æ•°èŒƒå›´å†…ç”Ÿæˆé€¼çœŸçš„é«˜é¢‘åœ°éœ‡æ³¢å½¢ï¼Œå³ä½¿åœ¨æ•°æ®ç¨€ç–çš„åŒºåŸŸä¹Ÿæ˜¯å¦‚æ­¤ ç»“å¯¹äºåœ°éœ‡å±é™©æ€§å’Œå·¥ç¨‹ç ”ç©¶ä¸­å¸¸ç”¨çš„æ ‡é‡åœ°é¢è¿åŠ¨ç»Ÿè®¡é‡ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°å†ç°çœŸå®æ•°æ®çš„ä¸­ä½è¶‹åŠ¿åŠå…¶å˜åŒ–ã€‚ä¸ºäº†è¯„ä¼°å’Œæ¯”è¾ƒè¿™äº›ä»¥åŠç±»ä¼¼ç”Ÿæˆæ³¢å½¢æ¨¡å‹çš„æ—¥ç›Šå¢é•¿çš„æ•°é‡ï¼Œæˆ‘ä»¬ä¸»å¼ å®ƒä»¬åº”å…¬å¼€å¯ç”¨ï¼Œå¹¶çº³å…¥ç¤¾åŒºåœ°é¢è¿åŠ¨æ¨¡å‹è¯„ä¼°å·¥ä½œä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19343v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é«˜é¢‘åœ°éœ‡æ³¢å½¢ç”Ÿæˆæ–¹æ³•â€”â€”HighFEMã€‚è¯¥æ–¹æ³•é‡‡ç”¨è°±å›¾è¡¨ç¤ºåœ°éœ‡æ³¢å½¢æ•°æ®ï¼Œé€šè¿‡è‡ªç¼–ç å™¨é™ä½æ•°æ®ç»´åº¦ã€‚åˆ©ç”¨å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ½œåœ¨è¡¨ç°ï¼Œæ ¹æ®åœ°éœ‡éœ‡çº§ã€è®°å½•è·ç¦»ã€åœºåœ°æ¡ä»¶ã€éœ‡æºæ·±åº¦å’Œæ–¹ä½è§’ç¼ºå£ç­‰å…³é”®è¾“å…¥å‚æ•°è¿›è¡Œæ¡ä»¶åŒ–ã€‚æ‰€ç”Ÿæˆæ³¢å½¢çš„é¢‘ç‡å†…å®¹é«˜è¾¾50èµ«å…¹ã€‚å¯ä»åˆæˆæ³¢å½¢ä¸­è½»æ¾å¾—å‡ºå³°å€¼åœ°é¢è¿åŠ¨å¹…åº¦å’Œè°±åŠ é€Ÿåº¦ç­‰æ ‡é‡åœ°é¢è¿åŠ¨ç»Ÿè®¡é‡ã€‚é€šè¿‡åœ°éœ‡å­¦å¸¸ç”¨æŒ‡æ ‡å’Œå›¾åƒç”Ÿæˆç ”ç©¶ä¸­çš„æ€§èƒ½æŒ‡æ ‡éªŒè¯äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹å¯åœ¨å¹¿æ³›è¾“å…¥å‚æ•°èŒƒå›´å†…ç”ŸæˆçœŸå®é«˜é¢‘åœ°éœ‡æ³¢å½¢ï¼Œç”šè‡³åœ¨æ•°æ®ç¨€ç¼ºåœ°åŒºä¹Ÿèƒ½å¦‚æ­¤ã€‚å¯¹äºåœ°éœ‡å·¥ç¨‹å’Œåœ°éœ‡å±é™©æ€§ç ”ç©¶å¸¸ç”¨çš„æ ‡é‡åœ°é¢è¿åŠ¨ç»Ÿè®¡é‡ï¼Œè¯¥æ¨¡å‹èƒ½å‡†ç¡®å†ç°çœŸå®æ•°æ®çš„ä¸­ä½è¶‹åŠ¿åŠå…¶å˜å¼‚æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>HighFEMæ˜¯ä¸€ç§ç”¨äºé«˜é¢‘åœ°éœ‡æ³¢å½¢ç”Ÿæˆçš„æ–°å‹è®¡ç®—æ•ˆç‡é«˜ä¸”å¯æ‰©å±•çš„ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨è°±å›¾è¡¨ç¤ºåœ°éœ‡æ³¢å½¢æ•°æ®å¹¶é€šè¿‡è‡ªç¼–ç å™¨é™ä½å…¶ç»´åº¦ã€‚</li>
<li>HighFEMåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ½œåœ¨è¡¨ç°ï¼Œè¯¥è¡¨ç°å¯æ ¹æ®å¤šç§å…³é”®è¾“å…¥å‚æ•°è¿›è¡Œæ¡ä»¶åŒ–ï¼ŒåŒ…æ‹¬åœ°éœ‡éœ‡çº§ã€è®°å½•è·ç¦»ç­‰ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è¾¾50èµ«å…¹é¢‘ç‡çš„æ³¢å½¢ã€‚</li>
<li>å¯ä»¥ä»åˆæˆæ³¢å½¢ä¸­è½»æ¾æ¨å¯¼å‡ºæ ‡é‡åœ°é¢è¿åŠ¨ç»Ÿè®¡é‡ã€‚</li>
<li>é€šè¿‡åœ°éœ‡å­¦å¸¸ç”¨æŒ‡æ ‡éªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¨¡å‹åœ¨å¹¿æ³›çš„æ•°æ®è¾“å…¥å‚æ•°èŒƒå›´å†…è¡¨ç°è‰¯å¥½ï¼Œç”šè‡³é€‚ç”¨äºæ•°æ®ç¨€ç¼ºåœ°åŒºï¼Œå¹¶èƒ½å¤Ÿå‡†ç¡®åæ˜ çœŸå®æ•°æ®çš„è¶‹åŠ¿å’Œå˜å¼‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2410.19343v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2410.19343v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ReviveDiff-A-Universal-Diffusion-Model-for-Restoring-Images-in-Adverse-Weather-Conditions"><a href="#ReviveDiff-A-Universal-Diffusion-Model-for-Restoring-Images-in-Adverse-Weather-Conditions" class="headerlink" title="ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse   Weather Conditions"></a>ReviveDiff: A Universal Diffusion Model for Restoring Images in Adverse   Weather Conditions</h2><p><strong>Authors:Wenfeng Huang, Guoan Xu, Wenjing Jia, Stuart Perry, Guangwei Gao</strong></p>
<p>Images captured in challenging environmentsâ€“such as nighttime, smoke, rainy weather, and underwaterâ€“often suffer from significant degradation, resulting in a substantial loss of visual quality. The effective restoration of these degraded images is critical for the subsequent vision tasks. While many existing approaches have successfully incorporated specific priors for individual tasks, these tailored solutions limit their applicability to other degradations. In this work, we propose a universal network architecture, dubbed &#96;&#96;ReviveDiffâ€™â€™, which can address various degradations and bring images back to life by enhancing and restoring their quality. Our approach is inspired by the observation that, unlike degradation caused by movement or electronic issues, quality degradation under adverse conditions primarily stems from natural media (such as fog, water, and low luminance), which generally preserves the original structures of objects. To restore the quality of such images, we leveraged the latest advancements in diffusion models and developed ReviveDiff to restore image quality from both macro and micro levels across some key factors determining image quality, such as sharpness, distortion, noise level, dynamic range, and color accuracy. We rigorously evaluated ReviveDiff on seven benchmark datasets covering five types of degrading conditions: Rainy, Underwater, Low-light, Smoke, and Nighttime Hazy. Our experimental results demonstrate that ReviveDiff outperforms the state-of-the-art methods both quantitatively and visually. </p>
<blockquote>
<p>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­æ•æ‰çš„å›¾åƒï¼Œå¦‚å¤œé—´ã€çƒŸé›¾ã€é›¨å¤©å’Œæ°´ä¸‹ç¯å¢ƒï¼Œç»å¸¸ä¼šå‘ç”Ÿä¸¥é‡çš„é€€åŒ–ï¼Œå¯¼è‡´è§†è§‰è´¨é‡å¤§é‡æŸå¤±ã€‚è¿™äº›é€€åŒ–å›¾åƒçš„æœ‰æ•ˆæ¢å¤å¯¹äºåç»­çš„è§†è§‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚è™½ç„¶è®¸å¤šç°æœ‰æ–¹æ³•å·²ç»æˆåŠŸåœ°ç»“åˆäº†é’ˆå¯¹å„ä¸ªä»»åŠ¡çš„ç‰¹å®šå…ˆéªŒçŸ¥è¯†ï¼Œä½†è¿™äº›å®šåˆ¶è§£å†³æ–¹æ¡ˆé™åˆ¶äº†å®ƒä»¬åœ¨å¤„ç†å…¶ä»–é€€åŒ–é—®é¢˜æ—¶çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œReviveDiffâ€çš„é€šç”¨ç½‘ç»œæ¶æ„ï¼Œå®ƒå¯ä»¥è§£å†³å„ç§é€€åŒ–é—®é¢˜ï¼Œå¹¶é€šè¿‡å¢å¼ºå’Œæ¢å¤å›¾åƒè´¨é‡ä½¿å›¾åƒæ¢å¤ç”Ÿæœºã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°ä»¥ä¸‹è§‚å¯Ÿç»“æœçš„å¯å‘ï¼šä¸è¿åŠ¨æˆ–ç”µå­é—®é¢˜å¼•èµ·çš„é€€åŒ–ä¸åŒï¼Œæ¶åŠ£æ¡ä»¶ä¸‹çš„è´¨é‡é€€åŒ–ä¸»è¦æºäºè‡ªç„¶åª’ä»‹ï¼ˆå¦‚é›¾ã€æ°´å’Œä½äº®åº¦ï¼‰ï¼Œè¿™é€šå¸¸ä¿ç•™äº†ç‰©ä½“çš„åŸå§‹ç»“æ„ã€‚ä¸ºäº†æ¢å¤æ­¤ç±»å›¾åƒçš„è´¨é‡ï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œå¹¶å¼€å‘ReviveDiffæ¥ä»å®è§‚å’Œå¾®è§‚å±‚é¢æ¢å¤å›¾åƒè´¨é‡ï¼Œæ¶‰åŠå†³å®šå›¾åƒè´¨é‡çš„å…³é”®å› ç´ ï¼Œå¦‚æ¸…æ™°åº¦ã€å¤±çœŸã€å™ªå£°æ°´å¹³ã€åŠ¨æ€èŒƒå›´å’Œé¢œè‰²å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¯¹ReviveDiffåœ¨åŒ…å«äº”ç§é€€åŒ–æ¡ä»¶çš„ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼šé›¨å¤©ã€æ°´ä¸‹ã€ä½å…‰ã€çƒŸé›¾å’Œå¤œé—´é›¾éœ¾ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒReviveDiffåœ¨å®šé‡å’Œè§†è§‰ä¸Šéƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18932v4">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œReviveDiffâ€çš„é€šç”¨ç½‘ç»œæ¶æ„ï¼Œå¯åº”å¯¹å„ç§å›¾åƒé€€åŒ–é—®é¢˜ï¼Œå¹¶åœ¨æ¶åŠ£æ¡ä»¶ä¸‹æ¢å¤å›¾åƒè´¨é‡ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œä»å®å¾®è§‚å±‚é¢æ¢å¤å›¾åƒè´¨é‡ï¼Œæé«˜æ¸…æ™°åº¦ã€å¤±çœŸã€å™ªå£°æ°´å¹³ã€åŠ¨æ€èŒƒå›´å’Œè‰²å½©å‡†ç¡®æ€§ç­‰å…³é”®å› ç´ ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒReviveDiffåœ¨äº”ç§é€€åŒ–æ¡ä»¶ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåœ¨æŒ‘æˆ˜ç¯å¢ƒä¸‹ï¼ˆå¦‚å¤œé—´ã€çƒŸé›¾ã€é›¨å¤©ã€æ°´ä¸‹ï¼‰ç»å¸¸é­å—æ˜¾è‘—é€€åŒ–ï¼Œå¯¼è‡´è§†è§‰è´¨é‡æŸå¤±ã€‚</li>
<li>å›¾åƒæ¢å¤å¯¹äºåç»­è§†è§‰ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤šä¸ºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç‰¹å®šå…ˆéªŒï¼Œé™åˆ¶äº†å…¶åœ¨å…¶ä»–é€€åŒ–æƒ…å†µçš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„ç½‘ç»œæ¶æ„â€œReviveDiffâ€ï¼Œå¯ä»¥åº”å¯¹å„ç§é€€åŒ–å¹¶æ¢å¤å›¾åƒè´¨é‡ã€‚</li>
<li>ReviveDiffçš„çµæ„Ÿæ¥æºäºè§‚å¯Ÿï¼šæ¶åŠ£æ¡ä»¶ä¸‹çš„è´¨é‡é€€åŒ–ä¸»è¦æºäºè‡ªç„¶åª’ä½“ï¼ˆå¦‚é›¾ã€æ°´ã€ä½äº®åº¦ï¼‰ï¼Œè¿™é€šå¸¸ä¿ç•™äº†ç‰©ä½“çš„åŸå§‹ç»“æ„ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼ŒReviveDiffä»å®å¾®è§‚å±‚é¢æ¢å¤å›¾åƒè´¨é‡ï¼Œæ¶‰åŠæ¸…æ™°åº¦ã€å¤±çœŸã€å™ªå£°æ°´å¹³ã€åŠ¨æ€èŒƒå›´å’Œè‰²å½©å‡†ç¡®æ€§ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2409.18932v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2409.18932v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2409.18932v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2409.18932v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2409.18932v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Neural-Entropy"><a href="#Neural-Entropy" class="headerlink" title="Neural Entropy"></a>Neural Entropy</h2><p><strong>Authors:Akhil Premkumar</strong></p>
<p>We explore the connection between deep learning and information theory through the paradigm of diffusion models. A diffusion model converts noise into structured data by reinstating, imperfectly, information that is erased when data was diffused to noise. This information is stored in a neural network during training. We quantify this information by introducing a measure called neural entropy, which is related to the total entropy produced by diffusion. Neural entropy is a function of not just the data distribution, but also the diffusive process itself. Measurements of neural entropy on a few simple image diffusion models reveal that they are extremely efficient at compressing large ensembles of structured data. </p>
<blockquote>
<p>æˆ‘ä»¬é€šè¿‡æ‰©æ•£æ¨¡å‹è¿™ä¸€èŒƒå¼æ¥æ¢è®¨æ·±åº¦å­¦ä¹ å’Œä¿¡æ¯ç†è®ºä¹‹é—´çš„è”ç³»ã€‚æ‰©æ•£æ¨¡å‹é€šè¿‡å°†æ‰©æ•£åˆ°å™ªå£°ä¸­çš„ä¿¡æ¯é‡æ–°æ¢å¤ï¼ˆè™½ç„¶æ˜¯ä¸å®Œç¾çš„ï¼‰ï¼Œå°†å™ªå£°è½¬åŒ–ä¸ºç»“æ„åŒ–æ•°æ®ã€‚è¿™äº›ä¿¡æ¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜å‚¨åœ¨ç¥ç»ç½‘ç»œä¸­ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ç¥ç»ç†µè¿™ä¸€åº¦é‡æ–¹æ³•æ¥é‡åŒ–è¿™äº›ä¿¡æ¯ï¼Œå®ƒä¸æ‰©æ•£äº§ç”Ÿçš„æ€»ç†µæœ‰å…³ã€‚ç¥ç»ç†µä¸ä»…æ˜¯æ•°æ®åˆ†å¸ƒçš„å‡½æ•°ï¼Œè€Œä¸”æ˜¯æ‰©æ•£è¿‡ç¨‹æœ¬èº«çš„å‡½æ•°ã€‚å¯¹å‡ ä¸ªç®€å•çš„å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç¥ç»ç†µæµ‹é‡è¡¨æ˜ï¼Œå®ƒä»¬åœ¨å‹ç¼©å¤§é‡ç»“æ„åŒ–æ•°æ®æ—¶æä¸ºé«˜æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03817v3">PDF</a> 29 pages + references, 18 figures. Camera-ready version from NeurIPS   2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†æ·±åº¦å­¦ä¹ å’Œä¿¡æ¯ç†è®ºä¹‹é—´çš„è”ç³»ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹è¿™ä¸€èŒƒä¾‹è¿›è¡Œé˜è¿°ã€‚æ‰©æ•£æ¨¡å‹å¯ä»¥å°†å™ªå£°è½¬åŒ–ä¸ºç»“æ„åŒ–æ•°æ®ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹æ¥é‡æ–°è·å¾—åœ¨æ•°æ®æ‰©æ•£è¿‡ç¨‹ä¸­ä¸¢å¤±çš„ä¿¡æ¯ã€‚æœ¬æ–‡å¼•å…¥äº†ç¥ç»ç†µè¿™ä¸€åº¦é‡æ ‡å‡†æ¥è¡¡é‡ä¿¡æ¯ï¼Œå®ƒä¸æ‰©æ•£äº§ç”Ÿçš„æ€»ç†µæœ‰å…³ã€‚ç¥ç»ç†µä¸ä»…æ˜¯æ•°æ®åˆ†å¸ƒçš„å‡½æ•°ï¼Œè¿˜æ˜¯æ‰©æ•£è¿‡ç¨‹æœ¬èº«çš„å‡½æ•°ã€‚å¯¹å‡ ä¸ªç®€å•çš„å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç¥ç»ç†µæµ‹é‡è¡¨æ˜ï¼Œå®ƒä»¬åœ¨å‹ç¼©å¤§é‡ç»“æ„åŒ–æ•°æ®æ—¶éå¸¸é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å°†å™ªå£°è½¬åŒ–ä¸ºç»“æ„åŒ–æ•°æ®ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹é‡æ–°è·å¾—ä¸¢å¤±çš„ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥ç¥ç»ç†µæ¥è¡¡é‡æ‰©æ•£æ¨¡å‹ä¸­çš„ä¿¡æ¯ã€‚</li>
<li>ç¥ç»ç†µä¸æ•°æ®åˆ†å¸ƒå’Œæ‰©æ•£è¿‡ç¨‹éƒ½æœ‰å…³ã€‚</li>
<li>ç¥ç»ç†µåœ¨ç®€å•å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨è¡¨æ˜å…¶é«˜æ•ˆå‹ç¼©ç»“æ„åŒ–æ•°æ®çš„èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸå…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2409.03817v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-11-05\./crop_Diffusion Models/2409.03817v3/page_1_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-92e5e711b5c7ab70b00de81301a94f57~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304869&auth_key=1762304869-0-0-7a1abb02d01d77cd84e3da91b5b1a863&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Navigated hepatic tumor resection using intraoperative ultrasound   imaging
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-73ffebfdd9b9c86bdd553fc0d74142cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762311671&auth_key=1762311671-0-0-f3bf57155910a43b8b6341b70b683e57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  SAGS Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical   Endoscopic Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
