<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Overcoming Prompts Pool Confusion via Parameterized Prompt for   Incremental Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-66b578a681b895b6cbbe22c109096ff6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291343&auth_key=1762291343-0-0-2cb4278974fb1999121759ecb2987ce2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    62 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="Overcoming-Prompts-Pool-Confusion-via-Parameterized-Prompt-for-Incremental-Object-Detection"><a href="#Overcoming-Prompts-Pool-Confusion-via-Parameterized-Prompt-for-Incremental-Object-Detection" class="headerlink" title="Overcoming Prompts Pool Confusion via Parameterized Prompt for   Incremental Object Detection"></a>Overcoming Prompts Pool Confusion via Parameterized Prompt for   Incremental Object Detection</h2><p><strong>Authors:Zijia An, Boyu Diao, Ruiqi Liu, Libo Huang, Chuanguang Yang, Fei Wang, Zhulin An, Yongjun Xu</strong></p>
<p>Recent studies have demonstrated that incorporating trainable prompts into pretrained models enables effective incremental learning. However, the application of prompts in incremental object detection (IOD) remains underexplored. Existing prompts pool based approaches assume disjoint class sets across incremental tasks, which are unsuitable for IOD as they overlook the inherent co-occurrence phenomenon in detection images. In co-occurring scenarios, unlabeled objects from previous tasks may appear in current task images, leading to confusion in prompts pool. In this paper, we hold that prompt structures should exhibit adaptive consolidation properties across tasks, with constrained updates to prevent catastrophic forgetting. Motivated by this, we introduce Parameterized Prompts for Incremental Object Detection (P$^2$IOD). Leveraging neural networks global evolution properties, P$^2$IOD employs networks as the parameterized prompts to adaptively consolidate knowledge across tasks. To constrain prompts structure updates, P$^2$IOD further engages a parameterized prompts fusion strategy. Extensive experiments on PASCAL VOC2007 and MS COCO datasets demonstrate that P$^2$IODâ€™s effectiveness in IOD and achieves the state-of-the-art performance among existing baselines. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°†å¯è®­ç»ƒæç¤ºèå…¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥å®ç°æœ‰æ•ˆçš„å¢é‡å­¦ä¹ ã€‚ç„¶è€Œï¼Œæç¤ºåœ¨å¢é‡ç›®æ ‡æ£€æµ‹ï¼ˆIODï¼‰ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ã€‚ç°æœ‰çš„åŸºäºæç¤ºæ± çš„æ–¹æ³•å‡è®¾å¢é‡ä»»åŠ¡ä¹‹é—´çš„ç±»åˆ«é›†æ˜¯ä¸ç›¸äº¤çš„ï¼Œè¿™ä¸é€‚ç”¨äºIODï¼Œå› ä¸ºå®ƒä»¬å¿½è§†äº†æ£€æµ‹å›¾åƒä¸­å›ºæœ‰çš„å…±ç°ç°è±¡ã€‚åœ¨å…±ç°åœºæ™¯ä¸­ï¼Œæ¥è‡ªå…ˆå‰ä»»åŠ¡çš„æœªæ ‡è®°ç›®æ ‡å¯èƒ½ä¼šå‡ºç°åœ¨å½“å‰ä»»åŠ¡å›¾åƒä¸­ï¼Œä»è€Œå¯¼è‡´æç¤ºæ± ä¸­çš„æ··æ·†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºæç¤ºç»“æ„åº”åœ¨ä»»åŠ¡ä¹‹é—´è¡¨ç°å‡ºè‡ªé€‚åº”æ•´åˆç‰¹æ€§ï¼Œå¹¶é€šè¿‡çº¦æŸæ›´æ–°æ¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºå¢é‡ç›®æ ‡æ£€æµ‹çš„å‚æ•°åŒ–æç¤ºï¼ˆP$^2$IODï¼‰ã€‚åˆ©ç”¨ç¥ç»ç½‘ç»œçš„å…¨å±€æ¼”åŒ–ç‰¹æ€§ï¼ŒP$^2$IODåˆ©ç”¨ç½‘ç»œä½œä¸ºå‚æ•°åŒ–æç¤ºï¼Œä»¥è‡ªé€‚åº”åœ°æ•´åˆä»»åŠ¡é—´çš„çŸ¥è¯†ã€‚ä¸ºäº†çº¦æŸæç¤ºç»“æ„æ›´æ–°ï¼ŒP$^2$IODè¿›ä¸€æ­¥é‡‡ç”¨å‚æ•°åŒ–æç¤ºèåˆç­–ç•¥ã€‚åœ¨PASCAL VOC2007å’ŒMS COCOæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒP$^2$IODåœ¨IODä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27316v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå°†å¯è®­ç»ƒæç¤ºèå…¥é¢„è®­ç»ƒæ¨¡å‹å¯å®ç°æœ‰æ•ˆçš„å¢é‡å­¦ä¹ ã€‚ç„¶è€Œï¼Œåœ¨å¢é‡ç›®æ ‡æ£€æµ‹ï¼ˆIODï¼‰ä¸­æç¤ºçš„åº”ç”¨ä»è¢«å¿½è§†ã€‚å½“å‰åŸºäºæç¤ºæ± çš„æ–¹æ³•å‡è®¾è·¨å¢é‡ä»»åŠ¡çš„ç±»åˆ«é›†æ˜¯ä¸ç›¸äº¤çš„ï¼Œè¿™ä¸é€‚ç”¨äºIODï¼Œå› ä¸ºå¿½è§†äº†æ£€æµ‹å›¾åƒä¸­å›ºæœ‰çš„å…±ç°ç°è±¡ã€‚å…±ç°åœºæ™¯ä¸­ï¼Œæ¥è‡ªå…ˆå‰ä»»åŠ¡çš„æ— æ ‡ç­¾å¯¹è±¡å¯èƒ½ä¼šå‡ºç°åœ¨å½“å‰ä»»åŠ¡å›¾åƒä¸­ï¼Œå¯¼è‡´æç¤ºæ± æ··æ·†ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œæç¤ºç»“æ„åº”å±•ç°è·¨ä»»åŠ¡çš„è‡ªé€‚åº”æ•´åˆç‰¹æ€§ï¼Œå¹¶æœ‰é™åˆ¶æ€§æ›´æ–°æ¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºç”¨äºå¢é‡ç›®æ ‡æ£€æµ‹çš„å¸¦å‚æ•°æç¤ºï¼ˆPÂ²IODï¼‰ã€‚åˆ©ç”¨ç¥ç»ç½‘ç»œå…¨å±€è¿›åŒ–ç‰¹æ€§ï¼ŒPÂ²IODåˆ©ç”¨ç½‘ç»œä½œä¸ºå¸¦å‚æ•°æç¤ºæ¥è·¨ä»»åŠ¡è‡ªé€‚åº”æ•´åˆçŸ¥è¯†ã€‚ä¸ºäº†é™åˆ¶æç¤ºç»“æ„æ›´æ–°ï¼ŒPÂ²IODè¿›ä¸€æ­¥é‡‡ç”¨å¸¦å‚æ•°æç¤ºèåˆç­–ç•¥ã€‚åœ¨PASCAL VOC2007å’ŒMS COCOæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒPÂ²IODåœ¨IODä¸­çš„æœ‰æ•ˆæ€§ä»¥åŠç›¸è¾ƒäºç°æœ‰åŸºå‡†æµ‹è¯•æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸç ”ç©¶åˆ©ç”¨å¯è®­ç»ƒæç¤ºèå…¥é¢„è®­ç»ƒæ¨¡å‹å®ç°å¢é‡å­¦ä¹ ï¼Œä½†å…¶åœ¨å¢é‡ç›®æ ‡æ£€æµ‹ï¼ˆIODï¼‰ä¸­çš„åº”ç”¨ä»è¢«å¿½è§†ã€‚</li>
<li>å½“å‰åŸºäºæç¤ºæ± çš„æ–¹æ³•å¿½ç•¥äº†æ£€æµ‹å›¾åƒä¸­çš„å…±ç°ç°è±¡ï¼Œå¯¼è‡´åœ¨å…±ç°åœºæ™¯ä¸­å‡ºç°æ··æ·†ã€‚</li>
<li>æœ¬æ–‡ä¸»å¼ æç¤ºç»“æ„åº”å±•ç°è·¨ä»»åŠ¡çš„è‡ªé€‚åº”æ•´åˆç‰¹æ€§ï¼Œå¹¶æœ‰é™åˆ¶æ€§æ›´æ–°é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>æå‡ºç”¨äºå¢é‡ç›®æ ‡æ£€æµ‹çš„å¸¦å‚æ•°æç¤ºï¼ˆPÂ²IODï¼‰ã€‚</li>
<li>PÂ²IODåˆ©ç”¨ç¥ç»ç½‘ç»œå…¨å±€è¿›åŒ–ç‰¹æ€§æ¥è·¨ä»»åŠ¡è‡ªé€‚åº”æ•´åˆçŸ¥è¯†ã€‚</li>
<li>PÂ²IODé‡‡ç”¨å¸¦å‚æ•°æç¤ºèåˆç­–ç•¥æ¥é™åˆ¶æç¤ºç»“æ„æ›´æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c04175655a13582e71b5c40071bcd163~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291019&auth_key=1762291019-0-0-99915c453107509a4bd313afaf8376ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a0e12376d3e19a2e8aea541f476acf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291026&auth_key=1762291026-0-0-af175e9c9f02b3c7948392455940c64b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa30198ae6163fc9f24b0c1492e495bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291033&auth_key=1762291033-0-0-accc17720f90b551e6ad8d17ef91c639&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="M-3Detection-Multi-Frame-Multi-Level-Feature-Fusion-for-Multi-Modal-3D-Object-Detection-with-Camera-and-4D-Imaging-Radar"><a href="#M-3Detection-Multi-Frame-Multi-Level-Feature-Fusion-for-Multi-Modal-3D-Object-Detection-with-Camera-and-4D-Imaging-Radar" class="headerlink" title="M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D   Object Detection with Camera and 4D Imaging Radar"></a>M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D   Object Detection with Camera and 4D Imaging Radar</h2><p><strong>Authors:Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang</strong></p>
<p>Recent advances in 4D imaging radar have enabled robust perception in adverse weather, while camera sensors provide dense semantic information. Fusing the these complementary modalities has great potential for cost-effective 3D perception. However, most existing camera-radar fusion methods are limited to single-frame inputs, capturing only a partial view of the scene. The incomplete scene information, compounded by image degradation and 4D radar sparsity, hinders overall detection performance. In contrast, multi-frame fusion offers richer spatiotemporal information but faces two challenges: achieving robust and effective object feature fusion across frames and modalities, and mitigating the computational cost of redundant feature extraction. Consequently, we propose M^3Detection, a unified multi-frame 3D object detection framework that performs multi-level feature fusion on multi-modal data from camera and 4D imaging radar. Our framework leverages intermediate features from the baseline detector and employs the tracker to produce reference trajectories, improving computational efficiency and providing richer information for second-stage. In the second stage, we design a global-level inter-object feature aggregation module guided by radar information to align global features across candidate proposals and a local-level inter-grid feature aggregation module that expands local features along the reference trajectories to enhance fine-grained object representation. The aggregated features are then processed by a trajectory-level multi-frame spatiotemporal reasoning module to encode cross-frame interactions and enhance temporal representation. Extensive experiments on the VoD and TJ4DRadSet datasets demonstrate that M^3Detection achieves state-of-the-art 3D detection performance, validating its effectiveness in multi-frame detection with camera-4D imaging radar fusion. </p>
<blockquote>
<p>è¿‘æœŸ4Dæˆåƒé›·è¾¾æŠ€æœ¯çš„è¿›å±•ä½¿å¾—åœ¨æ¶åŠ£å¤©æ°”ä¸‹çš„ç¨³å¥æ„ŸçŸ¥æˆä¸ºå¯èƒ½ï¼Œè€Œç›¸æœºä¼ æ„Ÿå™¨æä¾›äº†å¯†é›†è¯­ä¹‰ä¿¡æ¯ã€‚èåˆè¿™äº›äº’è¡¥æ¨¡æ€åœ¨æˆæœ¬æ•ˆç›Šé«˜çš„3Dæ„ŸçŸ¥æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç›¸æœºé›·è¾¾èåˆæ–¹æ³•å¤§å¤šä»…é™äºå•å¸§è¾“å…¥ï¼Œåªèƒ½æ•æ‰åœºæ™¯çš„éƒ¨åˆ†è§†å›¾ã€‚ä¸å®Œæ•´çš„åœºæ™¯ä¿¡æ¯åŠ ä¸Šå›¾åƒé€€åŒ–å’Œ4Dé›·è¾¾ç¨€ç–æ€§ï¼Œé˜»ç¢äº†æ•´ä½“æ£€æµ‹æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šå¸§èåˆæä¾›äº†æ›´ä¸°å¯Œçš„æ—¶ç©ºä¿¡æ¯ï¼Œä½†é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šå®ç°è·¨å¸§å’Œè·¨æ¨¡æ€çš„ç¨³å¥æœ‰æ•ˆçš„ç›®æ ‡ç‰¹å¾èåˆï¼Œå¹¶å‡å°‘å†—ä½™ç‰¹å¾æå–çš„è®¡ç®—æˆæœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†M^3Detectionï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šå¸§3Dç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œå¯¹æ¥è‡ªç›¸æœºå’Œ4Dæˆåƒé›·è¾¾çš„å¤šæ¨¡æ€æ•°æ®è¿›è¡Œå¤šå±‚æ¬¡ç‰¹å¾èåˆã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨åŸºçº¿æ£€æµ‹å™¨çš„ä¸­é—´ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨è·Ÿè¸ªå™¨ç”Ÿæˆå‚è€ƒè½¨è¿¹ï¼Œæé«˜è®¡ç®—æ•ˆç‡å¹¶ä¸ºç¬¬äºŒé˜¶æ®µæä¾›æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå—é›·è¾¾ä¿¡æ¯å¼•å¯¼çš„å…¨å±€çº§ç›®æ ‡é—´ç‰¹å¾èšåˆæ¨¡å—ï¼Œä»¥å¯¹é½å€™é€‰ææ¡ˆçš„å…¨å±€ç‰¹å¾ï¼Œä»¥åŠä¸€ä¸ªå±€éƒ¨çº§ç½‘æ ¼é—´ç‰¹å¾èšåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—æ²¿ç€å‚è€ƒè½¨è¿¹æ‰©å±•å±€éƒ¨ç‰¹å¾ï¼Œä»¥å¢å¼ºç²¾ç»†ç›®æ ‡è¡¨ç¤ºã€‚ç„¶åï¼Œèšé›†çš„ç‰¹å¾é€šè¿‡è½¨è¿¹çº§å¤šå¸§æ—¶ç©ºæ¨ç†æ¨¡å—è¿›è¡Œå¤„ç†ï¼Œä»¥ç¼–ç è·¨å¸§äº¤äº’å¹¶å¢å¼ºæ—¶é—´è¡¨ç¤ºã€‚åœ¨VoDå’ŒTJ4DRadSetæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒM^3Detectionè¾¾åˆ°äº†æœ€å…ˆè¿›çš„3Dæ£€æµ‹æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šå¸§æ£€æµ‹ä¸ç›¸æœº4Dæˆåƒé›·è¾¾èåˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27166v1">PDF</a> 16 pages, 9 figures</p>
<p><strong>Summary</strong><br>     è¿‘æœŸ4Dæˆåƒé›·è¾¾æŠ€æœ¯å–å¾—è¿›å±•ï¼Œèƒ½åœ¨æ¶åŠ£å¤©æ°”ä¸­å®ç°ç¨³å¥æ„ŸçŸ¥ï¼Œè€Œç›¸æœºä¼ æ„Ÿå™¨æä¾›ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚èåˆè¿™ä¸¤ç§äº’è¡¥æŠ€æœ¯å…·æœ‰å®ç°æˆæœ¬æ•ˆç›Šé«˜çš„ä¸‰ç»´æ„ŸçŸ¥çš„æ½œåŠ›ã€‚ä½†ç°æœ‰çš„æ‘„åƒå¤´é›·è¾¾èåˆæ–¹æ³•ä»…é™äºå•å¸§è¾“å…¥ï¼Œä»…æ•è·åœºæ™¯çš„å±€éƒ¨è§†å›¾ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½å—é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šå¸§èåˆæä¾›æ›´ä¸°å¯Œçš„æ—¶ç©ºä¿¡æ¯ï¼Œä½†é¢ä¸´è·¨å¸§å’Œè·¨æ¨¡æ€çš„ç¨³å¥æœ‰æ•ˆç‰¹å¾èåˆä»¥åŠå†—ä½™ç‰¹å¾æå–çš„è®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºM^3Detectionç»Ÿä¸€å¤šå¸§ä¸‰ç»´ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œåœ¨æ¥è‡ªç›¸æœºå’Œ4Dæˆåƒé›·è¾¾çš„å¤šæ¨¡æ€æ•°æ®ä¸Šè¿›è¡Œå¤šå±‚æ¬¡ç‰¹å¾èåˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºçº¿æ£€æµ‹å™¨çš„ä¸­é—´ç‰¹å¾ï¼Œå¹¶é€šè¿‡è·Ÿè¸ªå™¨ç”Ÿæˆå‚è€ƒè½¨è¿¹ï¼Œæé«˜è®¡ç®—æ•ˆç‡å¹¶ä¸ºç¬¬äºŒé˜¶æ®µæä¾›æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…¨å±€çº§åˆ«çš„è·¨ç›®æ ‡ç‰¹å¾èšåˆæ¨¡å—å’Œå±€éƒ¨çº§åˆ«çš„è·¨ç½‘æ ¼ç‰¹å¾èšåˆæ¨¡å—ï¼Œä»¥æ²¿å‚è€ƒè½¨è¿¹æ‰©å±•å±€éƒ¨ç‰¹å¾ï¼Œå¢å¼ºç²¾ç»†ç›®æ ‡è¡¨ç¤ºã€‚èšåˆç‰¹å¾éšåé€šè¿‡è½¨è¿¹çº§å¤šå¸§æ—¶ç©ºæ¨ç†æ¨¡å—è¿›è¡Œå¤„ç†ï¼Œä»¥ç¼–ç è·¨å¸§äº¤äº’å¹¶å¢å¼ºæ—¶é—´è¡¨ç¤ºã€‚åœ¨VoDå’ŒTJ4DRadSetæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒM^3Detectionå®ç°äº†æœ€å…ˆè¿›çš„ä¸‰ç»´æ£€æµ‹æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šå¸§æ£€æµ‹ä¸æ‘„åƒå¤´4Dæˆåƒé›·è¾¾èåˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸ4Dæˆåƒé›·è¾¾æŠ€æœ¯èƒ½åœ¨æ¶åŠ£å¤©æ°”ä¸­å®ç°ç¨³å¥æ„ŸçŸ¥ï¼Œä¸ç›¸æœºä¼ æ„Ÿå™¨ä¿¡æ¯èåˆå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ‘„åƒå¤´é›·è¾¾èåˆæ–¹æ³•ä¸»è¦å±€é™äºå•å¸§è¾“å…¥ï¼Œå¯¼è‡´åœºæ™¯ä¿¡æ¯ä¸å®Œæ•´ã€‚</li>
<li>å¤šå¸§èåˆæä¾›æ›´ä¸°å¯Œçš„æ—¶ç©ºä¿¡æ¯ï¼Œä½†é¢ä¸´ç‰¹å¾èåˆå’Œè®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚</li>
<li>M^3Detectionæ¡†æ¶å®ç°å¤šå±‚æ¬¡ç‰¹å¾èåˆï¼Œç»“åˆç›¸æœºå’Œ4Dæˆåƒé›·è¾¾æ•°æ®ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨åŸºçº¿æ£€æµ‹å™¨çš„ä¸­é—´ç‰¹å¾å’Œè·Ÿè¸ªå™¨ç”Ÿæˆçš„å‚è€ƒè½¨è¿¹ã€‚</li>
<li>M^3Detectioné€šè¿‡è®¾è®¡å…¨å±€å’Œå±€éƒ¨ç‰¹å¾èšåˆæ¨¡å—ä»¥åŠè½¨è¿¹çº§å¤šå¸§æ¨ç†æ¨¡å—å¢å¼ºæ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9a5293c2236606def031e8bb49978fb2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291040&auth_key=1762291040-0-0-18529123f7c9a557b2be31acb460a48a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ea63c4ad17daba5b183b76732c37867~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291048&auth_key=1762291048-0-0-06e78516f2960e813816f468bbbbbb24&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-79eb32d9d2c80dac24bcdbb3b30d9686~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291055&auth_key=1762291055-0-0-beb93e61be76a13f4a3d3d6e295b54f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-12ab7009ccf2eec759d710d793929ed6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291062&auth_key=1762291062-0-0-f8dce1afe599c3905b2ae1b968f3806a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Test-Time-Adaptive-Object-Detection-with-Foundation-Model"><a href="#Test-Time-Adaptive-Object-Detection-with-Foundation-Model" class="headerlink" title="Test-Time Adaptive Object Detection with Foundation Model"></a>Test-Time Adaptive Object Detection with Foundation Model</h2><p><strong>Authors:Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang</strong></p>
<p>In recent years, test-time adaptive object detection has attracted increasing attention due to its unique advantages in online domain adaptation, which aligns more closely with real-world application scenarios. However, existing approaches heavily rely on source-derived statistical characteristics while making the strong assumption that the source and target domains share an identical category space. In this paper, we propose the first foundation model-powered test-time adaptive object detection method that eliminates the need for source data entirely and overcomes traditional closed-set limitations. Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for vision-language detector-driven test-time adaptation, which incorporates text and visual prompt tuning to adapt both language and vision representation spaces on the test data in a parameter-efficient manner. Correspondingly, we propose a Test-time Warm-start strategy tailored for the visual prompts to effectively preserve the representation capability of the vision branch. Furthermore, to guarantee high-quality pseudo-labels in every test batch, we maintain an Instance Dynamic Memory (IDM) module that stores high-quality pseudo-labels from previous test samples, and propose two novel strategies-Memory Enhancement and Memory Hallucination-to leverage IDMâ€™s high-quality instances for enhancing original predictions and hallucinating images without available pseudo-labels, respectively. Extensive experiments on cross-corruption and cross-dataset benchmarks demonstrate that our method consistently outperforms previous state-of-the-art methods, and can adapt to arbitrary cross-domain and cross-category target data. Code is available at <a target="_blank" rel="noopener" href="https://github.com/gaoyingjay/ttaod_foundation">https://github.com/gaoyingjay/ttaod_foundation</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ç”±äºå…¶åœ¨çº¿åŸŸè‡ªé€‚åº”æ–¹é¢çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œè¶Šæ¥è¶Šå—å…³æ³¨ï¼Œè¿™æ›´è´´è¿‘å®é™…åº”ç”¨åœºæ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºæºæ´¾ç”Ÿç»Ÿè®¡ç‰¹å¾ï¼Œå¹¶å‡è®¾æºåŸŸå’Œç›®æ ‡åŸŸå…±äº«ç›¸åŒçš„ç±»åˆ«ç©ºé—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªç”±åŸºç¡€æ¨¡å‹é©±åŠ¨çš„æ£€æµ‹æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å®Œå…¨æ¶ˆé™¤äº†å¯¹æºæ•°æ®çš„ä¾èµ–ï¼Œå¹¶çªç ´äº†ä¼ ç»Ÿçš„å°é—­é›†é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€æç¤ºåŸºå…ƒå‡å€¼æ•™å¸ˆæ¡†æ¶ï¼Œç”¨äºè§†è§‰è¯­è¨€æ£€æµ‹å™¨é©±åŠ¨çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼Œå®ƒé€šè¿‡æ–‡æœ¬å’Œè§†è§‰æç¤ºè°ƒæ•´æ¥é€‚åº”æµ‹è¯•æ•°æ®çš„è¯­è¨€å’Œè§†è§‰è¡¨ç¤ºç©ºé—´ï¼Œå¹¶ä¸”å‚æ•°æ•ˆç‡é«˜ã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä»¬é’ˆå¯¹è§†è§‰æç¤ºæå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶é¢„çƒ­ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°ä¿ç•™äº†è§†è§‰åˆ†æ”¯çš„è¡¨ç¤ºèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä¿è¯æ¯ä¸ªæµ‹è¯•æ‰¹æ¬¡çš„é«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼Œæˆ‘ä»¬ç»´æŠ¤äº†ä¸€ä¸ªå®ä¾‹åŠ¨æ€å†…å­˜æ¨¡å—ï¼ˆIDMï¼‰ï¼Œè¯¥æ¨¡å—å­˜å‚¨æ¥è‡ªå…ˆå‰æµ‹è¯•æ ·æœ¬çš„é«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ–°ç­–ç•¥â€”â€”å†…å­˜å¢å¼ºå’Œå†…å­˜å¹»è§‰â€”â€”ä»¥åˆ©ç”¨IDMçš„é«˜è´¨é‡å®ä¾‹æ¥å¢å¼ºåŸå§‹é¢„æµ‹å’Œåœ¨æ²¡æœ‰å¯ç”¨ä¼ªæ ‡ç­¾çš„æƒ…å†µä¸‹æ¨¡æ‹Ÿå›¾åƒã€‚è·¨è…è´¥å’Œè·¨æ•°æ®é›†åŸºå‡†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥é€‚åº”ä»»æ„è·¨åŸŸå’Œè·¨ç±»åˆ«çš„ç›®æ ‡æ•°æ®ã€‚ä»£ç å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://github.com/gaoyingjay/ttaod_foundation">https://github.com/gaoyingjay/ttaod_foundation</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25175v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘å¹´æ¥ï¼Œæµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹å› å…¶åœ¨åœ¨çº¿åŸŸè‡ªé€‚åº”ä¸­çš„ç‹¬ç‰¹ä¼˜åŠ¿è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é«˜åº¦ä¾èµ–äºæºæ´¾ç”Ÿç»Ÿè®¡ç‰¹å¾ï¼Œå¹¶å‡è®¾æºåŸŸå’Œç›®æ ‡åŸŸå…·æœ‰ç›¸åŒçš„ç±»åˆ«ç©ºé—´ï¼Œè¿™é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŸºç¡€æ¨¡å‹çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œæ— éœ€æºæ•°æ®ï¼Œçªç ´äº†ä¼ ç»Ÿå°é—­é›†çš„å±€é™ã€‚è¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡æ€æç¤ºåŸºç¡€çš„Mean-Teacheræ¡†æ¶ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰æç¤ºè°ƒæ•´ï¼Œä»¥å‚æ•°æœ‰æ•ˆçš„æ–¹å¼åœ¨æµ‹è¯•æ•°æ®ä¸Šé€‚åº”è¯­è¨€å’Œè§†è§‰è¡¨ç¤ºç©ºé—´ã€‚åŒæ—¶ï¼Œæå‡ºäº†æµ‹è¯•æ—¶é¢„çƒ­ç­–ç•¥ï¼Œé’ˆå¯¹è§†è§‰æç¤ºè¿›è¡Œæœ‰æ•ˆä¿ç•™è§†è§‰åˆ†æ”¯çš„è¡¨ç¤ºèƒ½åŠ›ã€‚ä¸ºä¿è¯æ¯æ‰¹æµ‹è¯•æ ·æœ¬çš„é«˜è´¨é‡ä¼ªæ ‡ç­¾ï¼Œç»´æŠ¤äº†ä¸€ä¸ªå®ä¾‹åŠ¨æ€å†…å­˜æ¨¡å—ï¼Œå¹¶æå‡ºä¸¤ç§æ–°ç­–ç•¥æ¥åˆ©ç”¨é«˜è´¨é‡å®ä¾‹å¢å¼ºåŸå§‹é¢„æµ‹å’Œè™šæ„æ— ä¼ªæ ‡ç­¾çš„å›¾åƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨è…è´¥å’Œè·¨æ•°æ®é›†åŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå¹¶å¯é€‚åº”ä»»æ„è·¨åŸŸå’Œè·¨ç±»åˆ«çš„ç›®æ ‡æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹åœ¨ç°å®åº”ç”¨ä¸­çš„ä»·å€¼åŠå…¶è¿‘å¹´æ¥å—åˆ°çš„å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºæºæ•°æ®å¹¶å­˜åœ¨å°é—­é›†é™åˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåŸºç¡€æ¨¡å‹çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œæ— éœ€æºæ•°æ®ã€‚</li>
<li>è®¾è®¡äº†å¤šæ¨¡æ€æç¤ºåŸºç¡€çš„Mean-Teacheræ¡†æ¶ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰æç¤ºè°ƒæ•´ã€‚</li>
<li>æå‡ºäº†æµ‹è¯•æ—¶é¢„çƒ­ç­–ç•¥ï¼Œæœ‰æ•ˆä¿ç•™è§†è§‰åˆ†æ”¯çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å®ä¾‹åŠ¨æ€å†…å­˜æ¨¡å—ä¿è¯é«˜è´¨é‡ä¼ªæ ‡ç­¾ã€‚</li>
<li>æå‡ºä¸¤ç§æ–°ç­–ç•¥æ¥åˆ©ç”¨é«˜è´¨é‡å®ä¾‹å¢å¼ºåŸå§‹é¢„æµ‹å’Œè™šæ„æ— ä¼ªæ ‡ç­¾çš„å›¾åƒã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8c0134a10ee8177e1801a55afc381e2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291069&auth_key=1762291069-0-0-42775e4afd60e9fdadb00cff19f308bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2981af6d9e84ee1c1736444a0df416ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291076&auth_key=1762291076-0-0-b8b9fea5236d580047d72248ab9cdace&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d88e54c852867f7cf1a1519592827bba~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291083&auth_key=1762291083-0-0-897264bf5bae6ed60c7fa94681665ab5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e22fb428f54b398c715f3d12db7abb5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291090&auth_key=1762291090-0-0-960044d92e6c194b151e8f7b6b2cefcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Classifier-Enhancement-Using-Extended-Context-and-Domain-Experts-for-Semantic-Segmentation"><a href="#Classifier-Enhancement-Using-Extended-Context-and-Domain-Experts-for-Semantic-Segmentation" class="headerlink" title="Classifier Enhancement Using Extended Context and Domain Experts for   Semantic Segmentation"></a>Classifier Enhancement Using Extended Context and Domain Experts for   Semantic Segmentation</h2><p><strong>Authors:Huadong Tang, Youpeng Zhao, Min Xu, Jun Wang, Qiang Wu</strong></p>
<p>Prevalent semantic segmentation methods generally adopt a vanilla classifier to categorize each pixel into specific classes.   Although such a classifier learns global information from the training data, this information is represented by a set of fixed parameters (weights and biases).   However, each image has a different class distribution, which prevents the classifier from addressing the unique characteristics of individual images.   At the dataset level, class imbalance leads to segmentation results being biased towards majority classes, limiting the modelâ€™s effectiveness in identifying and segmenting minority class regions.   In this paper, we propose an Extended Context-Aware Classifier (ECAC) that dynamically adjusts the classifier using global (dataset-level) and local (image-level) contextual information.   Specifically, we leverage a memory bank to learn dataset-level contextual information of each class, incorporating the class-specific contextual information from the current image to improve the classifier for precise pixel labeling.   Additionally, a teacher-student network paradigm is adopted, where the domain expert (teacher network) dynamically adjusts contextual information with ground truth and transfers knowledge to the student network.   Comprehensive experiments illustrate that the proposed ECAC can achieve state-of-the-art performance across several datasets, including ADE20K, COCO-Stuff10K, and Pascal-Context. </p>
<blockquote>
<p>å½“å‰æµè¡Œçš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•é€šå¸¸é‡‡ç”¨æ™®é€šåˆ†ç±»å™¨ï¼Œå°†æ¯ä¸ªåƒç´ åˆ†ç±»ä¸ºç‰¹å®šçš„ç±»åˆ«ã€‚è™½ç„¶è¿™æ ·çš„åˆ†ç±»å™¨ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å…¨å±€ä¿¡æ¯ï¼Œä½†è¿™äº›ä¿¡æ¯ç”±ä¸€ç»„å›ºå®šçš„å‚æ•°ï¼ˆæƒé‡å’Œåå·®ï¼‰è¡¨ç¤ºã€‚ç„¶è€Œï¼Œæ¯å¼ å›¾åƒçš„ç±»åˆ«åˆ†å¸ƒéƒ½æ˜¯ä¸åŒçš„ï¼Œè¿™å¯¼è‡´åˆ†ç±»å™¨æ— æ³•åº”å¯¹å•ä¸ªå›¾åƒçš„ç‹¬ç‰¹ç‰¹å¾ã€‚åœ¨æ•°æ®é›†å±‚é¢ï¼Œç±»åˆ«ä¸å¹³è¡¡å¯¼è‡´åˆ†å‰²ç»“æœåå‘äºå¤šæ•°ç±»åˆ«ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨è¯†åˆ«å’Œåˆ†å‰²å°‘æ•°ç±»åˆ«åŒºåŸŸæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ‰©å±•çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†ç±»å™¨ï¼ˆECACï¼‰ï¼Œè¯¥åˆ†ç±»å™¨èƒ½å¤Ÿåˆ©ç”¨å…¨å±€ï¼ˆæ•°æ®é›†çº§åˆ«ï¼‰å’Œå±€éƒ¨ï¼ˆå›¾åƒçº§åˆ«ï¼‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥åŠ¨æ€è°ƒæ•´åˆ†ç±»å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å­˜å‚¨åº“æ¥å­¦ä¹ æ¯ä¸ªç±»åˆ«çš„æ•°æ®é›†çº§åˆ«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ç»“åˆå½“å‰å›¾åƒä¸­çš„ç±»åˆ«ç‰¹å®šä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æé«˜åˆ†ç±»å™¨çš„ç²¾ç¡®åƒç´ æ ‡ç­¾èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¸ˆå¾’ç½‘ç»œèŒƒå¼ï¼Œé¢†åŸŸä¸“å®¶ï¼ˆæ•™å¸ˆç½‘ç»œï¼‰èƒ½å¤ŸåŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶ä¸çœŸå®æƒ…å†µè¿›è¡ŒåŒ¹é…ï¼Œå°†çŸ¥è¯†ä¼ æˆç»™å­¦ç”Ÿç½‘ç»œã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ECACåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡èƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ADE20Kã€COCO-Stuff10Kå’ŒPascal-Contextã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25174v1">PDF</a> Accepted at IEEE TRANSACTIONS ON MULTIMEDIA (TMM)</p>
<p><strong>Summary</strong><br>åœ¨æœ¬æ–‡ä¸­ï¼Œä¸ºäº†è§£å†³ç°æœ‰è¯­ä¹‰åˆ†å‰²æ–¹æ³•çš„å±€é™æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºæ‰©å±•ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†ç±»å™¨ï¼ˆECACï¼‰çš„æ–°æ–¹æ³•ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨å›ºå®šå‚æ•°çš„åˆ†ç±»å™¨å¯¹åƒç´ è¿›è¡Œåˆ†ç±»ï¼Œå¿½è§†äº†æ¯å¹…å›¾åƒçš„ç±»åˆ†å¸ƒå·®å¼‚ä»¥åŠæ•°æ®é›†çº§åˆ«çš„ç±»ä¸å¹³è¡¡é—®é¢˜ã€‚è€ŒECACåˆ©ç”¨å…¨å±€å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯åŠ¨æ€è°ƒæ•´åˆ†ç±»å™¨ï¼Œå€ŸåŠ©å†…å­˜åº“å­¦ä¹ æ¯ä¸ªç±»çš„æ•°æ®é›†çº§åˆ«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ç»“åˆå½“å‰å›¾åƒçš„ç±»ç‰¹å®šä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æé«˜åˆ†ç±»å™¨çš„ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†æ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œèŒƒå¼ï¼ŒåŠ¨æ€è°ƒæ•´çœŸå®ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶å‘å­¦ç”Ÿç½‘ç»œä¼ é€’çŸ¥è¯†ã€‚å®éªŒè¯æ˜ï¼ŒECACåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿè¯­ä¹‰åˆ†å‰²æ–¹æ³•é‡‡ç”¨å›ºå®šå‚æ•°åˆ†ç±»å™¨ï¼Œéš¾ä»¥é€‚åº”æ¯å¹…å›¾åƒçš„ä¸åŒç±»åˆ†å¸ƒã€‚</li>
<li>æ•°æ®é›†çº§åˆ«çš„ç±»ä¸å¹³è¡¡å¯¼è‡´æ¨¡å‹å¯¹å¤šæ•°ç±»çš„åˆ†å‰²ç»“æœåå‘ï¼Œå½±å“å¯¹å°‘æ•°ç±»çš„è¯†åˆ«å’Œåˆ†å‰²æ•ˆæœã€‚</li>
<li>ECACé€šè¿‡ç»“åˆå…¨å±€å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯åŠ¨æ€è°ƒæ•´åˆ†ç±»å™¨ï¼Œä»¥æé«˜åƒç´ æ ‡è®°çš„ç²¾åº¦ã€‚</li>
<li>ECACåˆ©ç”¨å†…å­˜åº“å­¦ä¹ æ¯ä¸ªç±»çš„æ•°æ®é›†çº§åˆ«ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œèŒƒå¼ç”¨äºåŠ¨æ€è°ƒæ•´çœŸå®ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶ä¼ é€’çŸ¥è¯†ç»™å­¦ç”Ÿç½‘ç»œã€‚</li>
<li>ECACåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7a11d2f13d08152b64c5954220ad46cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291097&auth_key=1762291097-0-0-b5a3aa67cfd3c93dc27b3f98db892fc0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ffed5d81b9e089568f62dabcaece06cc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291104&auth_key=1762291104-0-0-ae96a32ee4501a00e30fd960254a985c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a085d19308b383cc0806192cd4e8eef~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291112&auth_key=1762291112-0-0-fd789da71bdbb31d6645cc0eae7248bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3a89716657ba9fd8697c6a7402c2c6bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291118&auth_key=1762291118-0-0-5b428b1a572276d9656e4672e83057a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DINO-YOLO-Self-Supervised-Pre-training-for-Data-Efficient-Object-Detection-in-Civil-Engineering-Applications"><a href="#DINO-YOLO-Self-Supervised-Pre-training-for-Data-Efficient-Object-Detection-in-Civil-Engineering-Applications" class="headerlink" title="DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object   Detection in Civil Engineering Applications"></a>DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object   Detection in Civil Engineering Applications</h2><p><strong>Authors:Malaisree P, Youwai S, Kitkobsin T, Janrungautai S, Amorndechaphon D, Rojanavasu P</strong></p>
<p>Object detection in civil engineering applications is constrained by limited annotated data in specialized domains. We introduce DINO-YOLO, a hybrid architecture combining YOLOv12 with DINOv3 self-supervised vision transformers for data-efficient detection. DINOv3 features are strategically integrated at two locations: input preprocessing (P0) and mid-backbone enhancement (P3). Experimental validation demonstrates substantial improvements: Tunnel Segment Crack detection (648 images) achieves 12.4% improvement, Construction PPE (1K images) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while maintaining real-time inference (30-47 FPS). Systematic ablation across five YOLO scales and nine DINOv3 variants reveals that Medium-scale architectures achieve optimal performance with DualP0P3 integration (55.77% <a href="mailto:&#x6d;&#65;&#x50;&#64;&#x30;&#x2e;&#53;">&#x6d;&#65;&#x50;&#64;&#x30;&#x2e;&#53;</a>), while Small-scale requires Triple Integration (53.63%). The 2-4x inference overhead (21-33ms versus 8-16ms baseline) remains acceptable for field deployment on NVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil engineering datasets (&lt;10K images) while preserving computational efficiency, providing practical solutions for construction safety monitoring and infrastructure inspection in data-constrained environments. </p>
<blockquote>
<p>åœ¨åœŸæœ¨å·¥ç¨‹åº”ç”¨ä¸­ï¼Œç›®æ ‡æ£€æµ‹çš„å±€é™æ€§åœ¨äºç‰¹å®šé¢†åŸŸæ ‡æ³¨æ•°æ®çš„æœ‰é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†DINO-YOLOï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†YOLOv12å’ŒDINOv3è‡ªç›‘ç£è§†è§‰å˜æ¢å™¨çš„æ··åˆæ¶æ„ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ•°æ®æ£€æµ‹ã€‚DINOv3çš„ç‰¹æ€§è¢«æˆ˜ç•¥æ€§åœ°æ•´åˆåœ¨ä¸¤ä¸ªä½ç½®ï¼šè¾“å…¥é¢„å¤„ç†ï¼ˆP0ï¼‰å’Œä¸­é—´ä¸»å¹²å¢å¼ºï¼ˆP3ï¼‰ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºå‡ºäº†æ˜¾è‘—çš„æå‡ï¼šéš§é“æ®µè£‚ç¼æ£€æµ‹ï¼ˆ648å¼ å›¾åƒï¼‰æå‡äº†12.4%ï¼Œæ–½å·¥ä¸ªäººé˜²æŠ¤è£…å¤‡ï¼ˆ1000å¼ å›¾åƒï¼‰æå‡äº†13.7%ï¼ŒKITTIæ•°æ®é›†ï¼ˆ7000å¼ å›¾åƒï¼‰åˆ™æ˜¾ç¤ºäº†88.6%çš„æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¨ç†ï¼ˆæ¯ç§’å¤„ç†å¸§æ•°è¾¾åˆ°30-47å¸§ï¼‰ã€‚é€šè¿‡å¯¹äº”ä¸ªYOLOå°ºåº¦å’Œä¹ä¸ªDINOv3å˜ç§çš„ç³»ç»Ÿæ¶ˆèç ”ç©¶ï¼Œå‘ç°ä¸­ç­‰è§„æ¨¡çš„æ¶æ„åœ¨é‡‡ç”¨DualP0P3é›†æˆåè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼ˆ<a href="mailto:&#109;&#65;&#80;&#64;&#48;&#46;&#53;">&#109;&#65;&#80;&#64;&#48;&#46;&#53;</a>ä¸º55.77%ï¼‰ï¼Œè€Œå°è§„æ¨¡åˆ™éœ€è¦ä¸‰é‡é›†æˆï¼ˆmAPä¸º53.63%ï¼‰ã€‚è™½ç„¶æ¨ç†å¼€é”€ä¸ºåŸºå‡†æ—¶é—´çš„2-4å€ï¼ˆç›¸å¯¹äºåŸºçº¿å»¶è¿Ÿå¢åŠ çº¦æ¯å°æ—¶æ–°å¢çš„è¿è¡Œæ—¶é—´æ¶ˆè€—åœ¨æœ€é•¿è¿è¡Œæ—¶æ—¶é—´ä¸ºï¼‰ï¼Œä½†å¯ä»¥åœ¨NVIDIA RTX 5090ä¸Šéƒ¨ç½²ä½¿ç”¨åœºæ™¯ä»è¢«è®¤ä¸ºæ˜¯å¯æ¥å—çš„ã€‚DINO-YOLOä¸ºæ•°æ®çº¦æŸç¯å¢ƒä¸‹çš„åœŸæœ¨å·¥ç¨‹æ•°æ®é›†ï¼ˆå°äºä¸€ä¸‡å¼ å›¾åƒï¼‰æä¾›äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½è®¡ç®—æ•ˆç‡æå‡ï¼Œä¸ºå»ºç­‘æ–½å·¥å®‰å…¨ç›‘æµ‹å’ŒåŸºç¡€è®¾æ–½æ£€æµ‹æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25140v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åœŸæœ¨å·¥ç¨‹åº”ç”¨ä¸­ç›®æ ‡æ£€æµ‹å—é™äºä¸“ä¸šé¢†åŸŸå†…æ ‡æ³¨æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†DINO-YOLOæ··åˆæ¶æ„ã€‚è¯¥æ¶æ„ç»“åˆäº†YOLOv12å’ŒDINOv3è‡ªç›‘ç£è§†è§‰è½¬æ¢å™¨ï¼Œä»¥å®ç°æ•°æ®é«˜æ•ˆæ£€æµ‹ã€‚åœ¨éš§é“æ®µè½è£‚ç¼æ£€æµ‹ã€æ–½å·¥ä¸ªäººé˜²æŠ¤è®¾å¤‡æ£€æµ‹å’ŒKITTIæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒDINO-YOLOæœ‰æ˜¾è‘—æ”¹å–„ï¼ŒåŒæ—¶ä¿æŒå®æ—¶æ¨ç†é€Ÿåº¦ã€‚ç ”ç©¶è¿˜é€šè¿‡ç³»ç»Ÿæ€§æ¶ˆèå®éªŒæ­ç¤ºäº†æœ€ä½³æ€§èƒ½çš„ç»„åˆç­–ç•¥ã€‚DINO-YOLOåœ¨æ•°æ®å—é™ç¯å¢ƒä¸­ä¸ºåœŸæœ¨å·¥ç¨‹æ–½å·¥å®‰å…¨ç›‘æ§å’Œè®¾æ–½æ£€æµ‹æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥DINO-YOLOæ··åˆæ¶æ„ï¼Œç»“åˆäº†YOLOv12å’ŒDINOv3æŠ€æœ¯ï¼Œé€‚ç”¨äºåœŸæœ¨å·¥ç¨‹ä¸­çš„ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>DINOv3çš„ç‰¹æ€§è¢«æ•´åˆåœ¨è¾“å…¥é¢„å¤„ç†ï¼ˆP0ï¼‰å’Œä¸­æœŸä¸»å¹²å¢å¼ºï¼ˆP3ï¼‰ä¸¤ä¸ªä½ç½®ã€‚</li>
<li>åœ¨éš§é“æ®µè½è£‚ç¼æ£€æµ‹ã€æ–½å·¥ä¸ªäººé˜²æŠ¤è®¾å¤‡æ£€æµ‹å’ŒKITTIæ•°æ®é›†ä¸Šï¼ŒDINO-YOLOæœ‰æ˜¾è‘—æå‡æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ç³»ç»Ÿæ€§æ¶ˆèå®éªŒæ­ç¤ºäº†Medium-scaleæ¶æ„ä¸DualP0P3æ•´åˆç­–ç•¥è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</li>
<li>Small-scaleæ¶æ„éœ€è¦Triple Integrationç­–ç•¥ã€‚</li>
<li>DINO-YOLOæ¨ç†æ—¶é—´ç•¥æœ‰å¢åŠ ï¼Œä½†ä»åœ¨å¯æ¥å—çš„èŒƒå›´å†…ï¼Œé€‚åˆç°åœºéƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6461c5623fc14f5a00056fd7763a7ecd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291126&auth_key=1762291126-0-0-8053266cf198fa33d48fdd7f1777f70d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MIC-BEV-Multi-Infrastructure-Camera-Birdâ€™s-Eye-View-Transformer-with-Relation-Aware-Fusion-for-3D-Object-Detection"><a href="#MIC-BEV-Multi-Infrastructure-Camera-Birdâ€™s-Eye-View-Transformer-with-Relation-Aware-Fusion-for-3D-Object-Detection" class="headerlink" title="MIC-BEV: Multi-Infrastructure Camera Birdâ€™s-Eye-View Transformer with   Relation-Aware Fusion for 3D Object Detection"></a>MIC-BEV: Multi-Infrastructure Camera Birdâ€™s-Eye-View Transformer with   Relation-Aware Fusion for 3D Object Detection</h2><p><strong>Authors:Yun Zhang, Zhaoliang Zheng, Johnson Liu, Zhiyu Huang, Zewei Zhou, Zonglin Meng, Tianhui Cai, Jiaqi Ma</strong></p>
<p>Infrastructure-based perception plays a crucial role in intelligent transportation systems, offering global situational awareness and enabling cooperative autonomy. However, existing camera-based detection models often underperform in such scenarios due to challenges such as multi-view infrastructure setup, diverse camera configurations, degraded visual inputs, and various road layouts. We introduce MIC-BEV, a Transformer-based birdâ€™s-eye-view (BEV) perception framework for infrastructure-based multi-camera 3D object detection. MIC-BEV flexibly supports a variable number of cameras with heterogeneous intrinsic and extrinsic parameters and demonstrates strong robustness under sensor degradation. The proposed graph-enhanced fusion module in MIC-BEV integrates multi-view image features into the BEV space by exploiting geometric relationships between cameras and BEV cells alongside latent visual cues. To support training and evaluation, we introduce M2I, a synthetic dataset for infrastructure-based object detection, featuring diverse camera configurations, road layouts, and environmental conditions. Extensive experiments on both M2I and the real-world dataset RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D object detection. It also remains robust under challenging conditions, including extreme weather and sensor degradation. These results highlight the potential of MIC-BEV for real-world deployment. The dataset and source code are available at: <a target="_blank" rel="noopener" href="https://github.com/HandsomeYun/MIC-BEV">https://github.com/HandsomeYun/MIC-BEV</a>. </p>
<blockquote>
<p>åŸºäºåŸºç¡€è®¾æ–½çš„æ„ŸçŸ¥åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå®ƒæä¾›å…¨å±€æ€åŠ¿æ„ŸçŸ¥å¹¶ä¿ƒè¿›ååŒè‡ªä¸»ã€‚ç„¶è€Œï¼Œç”±äºå¤šè§†è§’åŸºç¡€è®¾æ–½è®¾ç½®ã€ä¸åŒæ‘„åƒå¤´é…ç½®ã€è§†è§‰è¾“å…¥é€€åŒ–ä»¥åŠå„ç§é“è·¯å¸ƒå±€ç­‰æŒ‘æˆ˜ï¼Œç°æœ‰çš„åŸºäºæ‘„åƒå¤´çš„æ£€æµ‹æ¨¡å‹åœ¨è¿™ç§æƒ…å†µä¸‹é€šå¸¸è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬å¼•å…¥äº†MIC-BEVï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºåŸºäºåŸºç¡€è®¾æ–½çš„å¤šæ‘„åƒå¤´3Då¯¹è±¡æ£€æµ‹ã€‚MIC-BEVçµæ´»æ”¯æŒå…·æœ‰ä¸åŒå›ºæœ‰å’Œå¤–åœ¨å‚æ•°çš„å¤šä¸ªæ‘„åƒå¤´ï¼Œå¹¶åœ¨ä¼ æ„Ÿå™¨é€€åŒ–çš„æƒ…å†µä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚MIC-BEVä¸­æå‡ºçš„å›¾å¢å¼ºèåˆæ¨¡å—é€šè¿‡åˆ©ç”¨æ‘„åƒå¤´å’ŒBEVå•å…ƒä¹‹é—´çš„å‡ ä½•å…³ç³»ä»¥åŠæ½œåœ¨è§†è§‰çº¿ç´¢ï¼Œå°†å¤šè§†è§’å›¾åƒç‰¹å¾é›†æˆåˆ°BEVç©ºé—´ä¸­ã€‚ä¸ºäº†æ”¯æŒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†M2Iï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŸºç¡€è®¾æ–½å¯¹è±¡æ£€æµ‹çš„åˆæˆæ•°æ®é›†ï¼Œå…·æœ‰å„ç§æ‘„åƒå¤´é…ç½®ã€é“è·¯å¸ƒå±€å’Œç¯å¢ƒæ¡ä»¶ã€‚åœ¨M2Iå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†RoScenesä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMIC-BEVåœ¨3Då¯¹è±¡æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å®ƒåœ¨æç«¯å¤©æ°”å’Œä¼ æ„Ÿå™¨é€€åŒ–ç­‰æŒ‘æˆ˜æ¡ä»¶ä¸‹ä»ç„¶ä¿æŒç¨³å¥ã€‚è¿™äº›ç»“æœçªå‡ºäº†MIC-BEVåœ¨ç°å®ä¸–ç•Œéƒ¨ç½²çš„æ½œåŠ›ã€‚æ•°æ®é›†å’Œæºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/HandsomeYun/MIC-BEV">https://github.com/HandsomeYun/MIC-BEV</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24688v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŸºç¡€è®¾æ–½çš„æ„ŸçŸ¥åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæä¾›å…¨å±€æ€åŠ¿æ„ŸçŸ¥å¹¶å®ç°ååŒè‡ªä¸»ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºäºæ‘„åƒå¤´çš„æ£€æµ‹æ¨¡å‹åœ¨è¿™ç§åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ï¼Œé¢ä¸´å¤šè§†è§’åŸºç¡€è®¾æ–½è®¾ç½®ã€ä¸åŒæ‘„åƒå¤´é…ç½®ã€è§†è§‰è¾“å…¥é€€åŒ–ä»¥åŠé“è·¯å¸ƒå±€å¤šæ ·ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºTransformerçš„ä¿¯è§†å›¾æ„ŸçŸ¥æ¡†æ¶MIC-BEVï¼Œæ”¯æŒå¤šç§åŸºç¡€è®¾æ–½æ‘„åƒå¤´çš„ä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚MIC-BEVçµæ´»æ”¯æŒä¸åŒæ•°é‡çš„æ‘„åƒå¤´å’Œå¤šæ ·åŒ–çš„å†…å¤–å‚æ•°ï¼Œåœ¨ä¼ æ„Ÿå™¨é€€åŒ–æƒ…å†µä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚å…¶åˆ›æ–°çš„å›¾å¢å¼ºèåˆæ¨¡å—åˆ©ç”¨æ‘„åƒå¤´ä¸ä¿¯è§†å›¾å•å…ƒä¹‹é—´çš„å‡ ä½•å…³ç³»ä»¥åŠæ½œåœ¨è§†è§‰çº¿ç´¢ï¼Œå°†å¤šè§†è§’å›¾åƒç‰¹å¾èåˆåˆ°ä¿¯è§†å›¾ç©ºé—´ä¸­ã€‚ä¸ºäº†æ”¯æŒå’Œè¯„ä¼°æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†M2Iè¿™ä¸€åˆæˆæ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒçš„æ‘„åƒå¤´é…ç½®ã€é“è·¯å¸ƒå±€å’Œç¯å¢ƒæ¡ä»¶ã€‚åœ¨M2Iå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†RoScenesä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMIC-BEVåœ¨ä¸‰ç»´ç‰©ä½“æ£€æµ‹æ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹¶åœ¨æç«¯å¤©æ°”å’Œä¼ æ„Ÿå™¨é€€åŒ–ç­‰æŒ‘æˆ˜æ¡ä»¶ä¸‹ä¿æŒç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºåŸºç¡€è®¾æ–½çš„æ„ŸçŸ¥åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­é‡è¦ï¼Œæä¾›å…¨å±€æ€åŠ¿æ„ŸçŸ¥ã€‚</li>
<li>ç°æœ‰æ‘„åƒå¤´æ£€æµ‹æ¨¡å‹åœ¨å¤šè§†è§’ã€å¤šæ ·åŒ–é“è·¯å¸ƒå±€ç­‰åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚</li>
<li>MIC-BEVæ˜¯ä¸€ä¸ªåŸºäºTransformerçš„ä¿¯è§†å›¾æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºæ”¯æŒå¤šæ‘„åƒå¤´ä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚</li>
<li>MIC-BEVè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œå¯çµæ´»æ”¯æŒä¸åŒæ‘„åƒå¤´å’Œå†…å¤–å‚æ•°ã€‚</li>
<li>å›¾å¢å¼ºèåˆæ¨¡å—åˆ©ç”¨å‡ ä½•å…³ç³»å’Œæ½œåœ¨è§†è§‰çº¿ç´¢è¿›è¡Œå¤šè§†è§’å›¾åƒç‰¹å¾èåˆã€‚</li>
<li>M2Iæ•°æ®é›†ç”¨äºæ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œæ¨¡æ‹Ÿå¤šæ ·åŒ–åœºæ™¯å’Œç¯å¢ƒæ¡ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-71790a9dc84a0fd3990f80e829e6d79c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291133&auth_key=1762291133-0-0-a1539833ba8b1d3b610962149526c27d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f69fe4b8b4415b92c28e5eee32d86976~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291141&auth_key=1762291141-0-0-8f7f3c32ea26661974519c16c12f55b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d349bbc73d42c7918092e4be762db029~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291148&auth_key=1762291148-0-0-4406f1425a03cb8cb039aa97ac0b7e03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-09b11e4dfb0fc8089b8d7988db304d42~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291155&auth_key=1762291155-0-0-f7ef3ec44757bed6bfaede49c9878001&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Delving-into-Cascaded-Instability-A-Lipschitz-Continuity-View-on-Image-Restoration-and-Object-Detection-Synergy"><a href="#Delving-into-Cascaded-Instability-A-Lipschitz-Continuity-View-on-Image-Restoration-and-Object-Detection-Synergy" class="headerlink" title="Delving into Cascaded Instability: A Lipschitz Continuity View on Image   Restoration and Object Detection Synergy"></a>Delving into Cascaded Instability: A Lipschitz Continuity View on Image   Restoration and Object Detection Synergy</h2><p><strong>Authors:Qing Zhao, Weijian Deng, Pengxu Wei, ZiYi Dong, Hannan Lu, Xiangyang Ji, Liang Lin</strong></p>
<p>To improve detection robustness in adverse conditions (e.g., haze and low light), image restoration is commonly applied as a pre-processing step to enhance image quality for the detector. However, the functional mismatch between restoration and detection networks can introduce instability and hinder effective integration â€“ an issue that remains underexplored. We revisit this limitation through the lens of Lipschitz continuity, analyzing the functional differences between restoration and detection networks in both the input space and the parameter space. Our analysis shows that restoration networks perform smooth, continuous transformations, while object detectors operate with discontinuous decision boundaries, making them highly sensitive to minor perturbations. This mismatch introduces instability in traditional cascade frameworks, where even imperceptible noise from restoration is amplified during detection, disrupting gradient flow and hindering optimization. To address this, we propose Lipschitz-regularized object detection (LROD), a simple yet effective framework that integrates image restoration directly into the detectorâ€™s feature learning, harmonizing the Lipschitz continuity of both tasks during training. We implement this framework as Lipschitz-regularized YOLO (LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive experiments on haze and low-light benchmarks demonstrate that LR-YOLO consistently improves detection stability, optimization smoothness, and overall accuracy. </p>
<blockquote>
<p>åœ¨æ¶åŠ£æ¡ä»¶ï¼ˆä¾‹å¦‚é›¾éœ¾å’Œä½å…‰ï¼‰ä¸‹ï¼Œä¸ºäº†æé«˜æ£€æµ‹ç¨³å¥æ€§ï¼Œé€šå¸¸å°†å›¾åƒæ¢å¤ä½œä¸ºé¢„å¤„ç†æ­¥éª¤åº”ç”¨äºæ£€æµ‹å™¨ï¼Œä»¥æé«˜å›¾åƒè´¨é‡ã€‚ç„¶è€Œï¼Œæ¢å¤ç½‘ç»œå’Œæ£€æµ‹ç½‘ç»œä¹‹é—´çš„åŠŸèƒ½ä¸åŒ¹é…å¯èƒ½ä¼šå¼•å…¥ä¸ç¨³å®šå› ç´ å¹¶é˜»ç¢æœ‰æ•ˆçš„é›†æˆï¼Œè¿™æ˜¯ä¸€ä¸ªå°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡åˆ©æ™®å¸ŒèŒ¨è¿ç»­æ€§æ¥åˆ†ææ¢å¤ç½‘ç»œå’Œæ£€æµ‹ç½‘ç»œåœ¨è¾“å…¥ç©ºé—´å’Œå‚æ•°ç©ºé—´ä¸Šçš„åŠŸèƒ½å·®å¼‚ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¢å¤ç½‘ç»œæ‰§è¡Œå¹³æ»‘ã€è¿ç»­çš„è½¬æ¢ï¼Œè€Œå¯¹è±¡æ£€æµ‹å™¨åˆ™å…·æœ‰ä¸è¿ç»­çš„å†³ç­–è¾¹ç•Œï¼Œä½¿å…¶å¯¹å¾®å°æ‰°åŠ¨æä¸ºæ•æ„Ÿã€‚è¿™ç§ä¸åŒ¹é…ä¼šç»™ä¼ ç»Ÿçš„çº§è”æ¡†æ¶å¸¦æ¥ä¸ç¨³å®šå› ç´ ï¼Œå…¶ä¸­æ¢å¤è¿‡ç¨‹ä¸­çš„å‡ ä¹ä¸å¯å¯Ÿè§‰çš„å™ªå£°åœ¨æ£€æµ‹è¿‡ç¨‹ä¸­ä¼šè¢«æ”¾å¤§ï¼Œç ´åæ¢¯åº¦æµå¹¶é˜»ç¢ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Lipschitzæ­£åˆ™åŒ–å¯¹è±¡æ£€æµ‹ï¼ˆLRODï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç›´æ¥å°†å›¾åƒæ¢å¤é›†æˆåˆ°æ£€æµ‹å™¨çš„ç‰¹å¾å­¦ä¹ ä¸­ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åè°ƒä¸¤ä¸ªä»»åŠ¡ä¹‹é—´çš„Lipschitzè¿ç»­æ€§ã€‚æˆ‘ä»¬å°†æ­¤æ¡†æ¶å®ç°ä¸ºLipschitzæ­£åˆ™åŒ–YOLOï¼ˆLR-YOLOï¼‰ï¼Œæ— ç¼æ‰©å±•åˆ°ç°æœ‰çš„YOLOæ£€æµ‹å™¨ã€‚åœ¨é›¾éœ¾å’Œä½å…‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLR-YOLOåœ¨æ£€æµ‹ç¨³å®šæ€§ã€ä¼˜åŒ–å¹³æ»‘åº¦å’Œæ€»ä½“å‡†ç¡®æ€§æ–¹é¢å‡æœ‰æ‰€æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24232v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨æ¶åŠ£ç¯å¢ƒä¸‹ï¼ˆå¦‚é›¾éœ¾å’Œä½å…‰ç…§ï¼‰æé«˜æ£€æµ‹ç¨³å¥æ€§çš„ç ”ç©¶ä¸­ï¼Œå›¾åƒä¿®å¤ä½œä¸ºé¢„å¤„ç†æ­¥éª¤è¢«å¹¿æ³›åº”ç”¨äºæé«˜å›¾åƒè´¨é‡ä»¥ä¾›æ£€æµ‹å™¨ä½¿ç”¨ã€‚ç„¶è€Œï¼Œä¿®å¤å’Œæ£€æµ‹ç½‘ç»œä¹‹é—´çš„åŠŸèƒ½ä¸åŒ¹é…ä¼šå¼•å‘ä¸ç¨³å®šå¹¶é˜»ç¢æœ‰æ•ˆé›†æˆï¼Œè¿™ä¸€é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡Lipschitzè¿ç»­æ€§æ¥åˆ†æä¿®å¤å’Œæ£€æµ‹ç½‘ç»œåœ¨è¾“å…¥ç©ºé—´å’Œå‚æ•°ç©ºé—´ä¸­çš„åŠŸèƒ½å·®å¼‚ã€‚åˆ†æè¡¨æ˜ï¼Œä¿®å¤ç½‘ç»œæ‰§è¡Œå¹³æ»‘ã€è¿ç»­çš„è½¬æ¢ï¼Œè€Œå¯¹è±¡æ£€æµ‹å™¨åˆ™å…·æœ‰ä¸è¿ç»­çš„å†³ç­–è¾¹ç•Œï¼Œä½¿å…¶å¯¹å¾®å°æ‰°åŠ¨é«˜åº¦æ•æ„Ÿã€‚è¿™ç§ä¸åŒ¹é…åœ¨ä¼ ç»Ÿçº§è”æ¡†æ¶ä¸­å¼•å…¥äº†ä¸ç¨³å®šæ€§ï¼Œå…¶ä¸­æ¥è‡ªä¿®å¤çš„å‡ ä¹æ— æ³•å¯Ÿè§‰çš„å™ªå£°åœ¨æ£€æµ‹è¿‡ç¨‹ä¸­ä¼šè¢«æ”¾å¤§ï¼Œç ´åæ¢¯åº¦æµå¹¶é˜»ç¢ä¼˜åŒ–ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Lipschitzæ­£åˆ™åŒ–ç›®æ ‡æ£€æµ‹ï¼ˆLRODï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç›´æ¥å°†å›¾åƒä¿®å¤é›†æˆåˆ°æ£€æµ‹å™¨çš„ç‰¹å¾å­¦ä¹ ä¸­ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åè°ƒä¸¤ä¸ªä»»åŠ¡çš„Lipschitzè¿ç»­æ€§ã€‚æˆ‘ä»¬åœ¨YOLOæ£€æµ‹å™¨ä¸Šå®ç°äº†æ­¤æ¡†æ¶çš„Lipschitzæ­£åˆ™åŒ–ç‰ˆæœ¬ï¼ˆLR-YOLOï¼‰ï¼Œå¯æ— ç¼é›†æˆåˆ°ç°æœ‰YOLOæ£€æµ‹å™¨ä¸­ã€‚åœ¨é›¾éœ¾å’Œä½å…‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLR-YOLOåœ¨æ£€æµ‹ç¨³å®šæ€§ã€ä¼˜åŒ–å¹³æ»‘åº¦å’Œæ€»ä½“å‡†ç¡®æ€§æ–¹é¢å‡æœ‰æ‰€æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒä¿®å¤ä½œä¸ºé¢„å¤„ç†æ­¥éª¤åœ¨æé«˜æ£€æµ‹ç¨³å¥æ€§æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶åŠ£ç¯å¢ƒä¸‹å¦‚é›¾éœ¾å’Œä½å…‰ç…§æ¡ä»¶ä¸‹ã€‚</li>
<li>ä¿®å¤å’Œæ£€æµ‹ç½‘ç»œä¹‹é—´çš„åŠŸèƒ½ä¸åŒ¹é…æ˜¯åˆ¶çº¦æœ‰æ•ˆé›†æˆçš„ä¸»è¦éšœç¢ä¹‹ä¸€ï¼Œè¿™ä¸€é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>é€šè¿‡Lipschitzè¿ç»­æ€§åˆ†æäº†ä¿®å¤å’Œæ£€æµ‹ç½‘ç»œçš„åŠŸèƒ½å·®å¼‚ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å¤„ç†å¾®å°æ‰°åŠ¨æ—¶çš„ä¸åŒè¡¨ç°ã€‚</li>
<li>è¿™ç§åŠŸèƒ½ä¸åŒ¹é…å¯èƒ½å¯¼è‡´ä¼ ç»Ÿçº§è”æ¡†æ¶ä¸­çš„ä¸ç¨³å®šï¼Œå…¶ä¸­å™ªå£°åœ¨æ£€æµ‹è¿‡ç¨‹ä¸­è¢«æ”¾å¤§å¹¶å½±å“æ¢¯åº¦æµå’Œä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†Lipschitzæ­£åˆ™åŒ–ç›®æ ‡æ£€æµ‹ï¼ˆLRODï¼‰æ¡†æ¶æ¥ç›´æ¥é›†æˆå›¾åƒä¿®å¤åˆ°æ£€æµ‹å™¨çš„ç‰¹å¾å­¦ä¹ ä¸­ï¼Œé€šè¿‡è®­ç»ƒè¿‡ç¨‹åè°ƒä¸¤ä¸ªä»»åŠ¡çš„Lipschitzè¿ç»­æ€§ã€‚</li>
<li>LR-YOLOæ˜¯LRODæ¡†æ¶åœ¨YOLOæ£€æµ‹å™¨ä¸Šçš„å®ç°ï¼Œå¯æ— ç¼é›†æˆåˆ°ç°æœ‰YOLOæ£€æµ‹å™¨ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a1f922e720d06fe46550963272b3c35d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291162&auth_key=1762291162-0-0-967769af03b393e6e7e130b62bd53feb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d52b810e0c1af0b5f46a1ebe55c992c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291169&auth_key=1762291169-0-0-738273b2094102c409ae9ea6614cc64f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-39d32905872cd3f2bd896d74868ed967~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291176&auth_key=1762291176-0-0-3db72acaaa2daac4819087b9c6a1a0ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95ea9a8ac1fa2a61e2c7d989a7625af4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291183&auth_key=1762291183-0-0-978f19e45a796d5d5c2f50879b4bb0c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Improving-Visual-Discriminability-of-CLIP-for-Training-Free-Open-Vocabulary-Semantic-Segmentation"><a href="#Improving-Visual-Discriminability-of-CLIP-for-Training-Free-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Improving Visual Discriminability of CLIP for Training-Free   Open-Vocabulary Semantic Segmentation"></a>Improving Visual Discriminability of CLIP for Training-Free   Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Jinxin Zhou, Jiachen Jiang, Zhihui Zhu</strong></p>
<p>Extending CLIP models to semantic segmentation remains challenging due to the misalignment between their image-level pre-training objectives and the pixel-level visual understanding required for dense prediction. While prior efforts have achieved encouraging results by reorganizing the final layer and features, they often inherit the global alignment bias of preceding layers, leading to suboptimal segmentation performance. In this work, we propose LHT-CLIP, a novel training-free framework that systematically exploits the visual discriminability of CLIP across layer, head, and token levels. Through comprehensive analysis, we reveal three key insights: (i) the final layers primarily strengthen image-text alignment with sacrifice of visual discriminability (e.g., last 3 layers in ViT-B&#x2F;16 and 8 layers in ViT-L&#x2F;14), partly due to the emergence of anomalous tokens; (ii) a subset of attention heads (e.g., 10 out of 144 in ViT-B&#x2F;16) display consistently strong visual discriminability across datasets; (iii) abnormal tokens display sparse and consistent activation pattern compared to normal tokens. Based on these findings, we propose three complementary techniques: semantic-spatial reweighting, selective head enhancement, and abnormal token replacement to effectively restore visual discriminability and improve segmentation performance without any additional training, auxiliary pre-trained networks, or extensive hyperparameter tuning. Extensive experiments on 8 common semantic segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art performance across diverse scenarios, highlighting its effectiveness and practicality for real-world deployment. </p>
<blockquote>
<p>å°†CLIPæ¨¡å‹æ‰©å±•åˆ°è¯­ä¹‰åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå›¾åƒçº§çš„é¢„è®­ç»ƒç›®æ ‡ä¸å¯†é›†é¢„æµ‹æ‰€éœ€çš„åƒç´ çº§è§†è§‰ç†è§£ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ã€‚å°½ç®¡å…ˆå‰çš„åŠªåŠ›é€šè¿‡é‡æ–°ç»„ç»‡æœ€åä¸€å±‚å’Œç‰¹å¾å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœï¼Œä½†å®ƒä»¬å¾€å¾€ç»§æ‰¿äº†å‰å±‚çš„å…¨å±€å¯¹é½åè§ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LHT-CLIPï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œç³»ç»Ÿåœ°åˆ©ç”¨CLIPåœ¨ä¸åŒå±‚æ¬¡ï¼ˆå±‚ã€å¤´å’Œä»¤ç‰Œï¼‰çš„è§†è§‰è¾¨åˆ«åŠ›ã€‚é€šè¿‡ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸‰ä¸ªå…³é”®è§è§£ï¼šï¼ˆiï¼‰æœ€åå‡ å±‚ä¸»è¦é€šè¿‡åŠ å¼ºå›¾åƒæ–‡æœ¬å¯¹é½æ¥ç‰ºç‰²è§†è§‰è¾¨åˆ«åŠ›ï¼ˆä¾‹å¦‚ViT-B&#x2F;16çš„æœ€å3å±‚å’ŒViT-L&#x2F;14çš„8å±‚ï¼‰ï¼Œéƒ¨åˆ†åŸå› æ˜¯ç”±äºå¼‚å¸¸ä»¤ç‰Œçš„å‡ºç°ï¼›ï¼ˆiiï¼‰ä¸€å°éƒ¨åˆ†æ³¨æ„åŠ›å¤´ï¼ˆä¾‹å¦‚ViT-B&#x2F;16ä¸­çš„10ä¸ªæ³¨æ„åŠ›å¤´ï¼‰åœ¨æ•°æ®é›†ä¸Šå§‹ç»ˆè¡¨ç°å‡ºå¼ºå¤§çš„è§†è§‰è¾¨åˆ«åŠ›ï¼›ï¼ˆiiiï¼‰å¼‚å¸¸ä»¤ç‰Œä¸æ­£å¸¸ä»¤ç‰Œç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºç¨€ç–ä¸”ä¸€è‡´çš„æ¿€æ´»æ¨¡å¼ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼šè¯­ä¹‰ç©ºé—´é‡æ–°åŠ æƒã€é€‰æ‹©æ€§å¤´éƒ¨å¢å¼ºå’Œå¼‚å¸¸ä»¤ç‰Œæ›¿æ¢ï¼Œä»¥æœ‰æ•ˆåœ°æ¢å¤è§†è§‰è¾¨åˆ«åŠ›ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€è¾…åŠ©é¢„è®­ç»ƒç½‘ç»œæˆ–å¹¿æ³›çš„è¶…å‚æ•°è°ƒæ•´ã€‚åœ¨8ä¸ªå¸¸è§çš„è¯­ä¹‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLHT-CLIPåœ¨å¤šç§åœºæ™¯ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23894v1">PDF</a> 23 pages, 10 figures, 14 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„LHT-CLIPæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç³»ç»Ÿåœ°åˆ©ç”¨CLIPåœ¨ä¸åŒå±‚çº§ï¼ˆå±‚ã€å¤´å’Œä»¤ç‰Œï¼‰çš„è§†è§‰è¾¨åˆ«èƒ½åŠ›ï¼Œæ¥è§£å†³CLIPæ¨¡å‹åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹æœ€ç»ˆå±‚åŠç‰¹å¾çš„é‡ç»„ï¼Œä»¥åŠå…¨é¢åˆ†æï¼Œæœ¬æ–‡æ­ç¤ºäº†ä¸‰ä¸ªå…³é”®è§è§£ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†ä¸‰ç§æŠ€æœ¯æ¥æ¢å¤è§†è§‰è¾¨åˆ«èƒ½åŠ›å¹¶æå‡åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨è¯­ä¹‰åˆ†å‰²ä¸Šä»é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå›¾åƒçº§åˆ«çš„é¢„è®­ç»ƒç›®æ ‡ä¸åƒç´ çº§åˆ«çš„è§†è§‰ç†è§£ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…ã€‚</li>
<li>é€šè¿‡å¯¹æœ€ç»ˆå±‚åŠç‰¹å¾çš„é‡ç»„ï¼Œä¹‹å‰çš„åŠªåŠ›å·²ç»å–å¾—äº†ä¸€äº›æˆæœï¼Œä½†ä»å­˜åœ¨å…¨å±€å¯¹é½åå·®çš„é—®é¢˜ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„LHT-CLIPæ¡†æ¶ç³»ç»Ÿåœ°åˆ©ç”¨CLIPåœ¨ä¸åŒå±‚çº§ï¼ˆå±‚ã€å¤´å’Œä»¤ç‰Œï¼‰çš„è§†è§‰è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼šæœ€ç»ˆå±‚ä¸»è¦å¼ºåŒ–å›¾åƒæ–‡æœ¬å¯¹é½ï¼Œä½†ç‰ºç‰²äº†è§†è§‰è¾¨åˆ«èƒ½åŠ›ï¼›éƒ¨åˆ†æ³¨æ„åŠ›å¤´åœ¨æ‰€æœ‰æ•°æ®é›†ä¸­å§‹ç»ˆè¡¨ç°å‡ºå¼ºå¤§çš„è§†è§‰è¾¨åˆ«èƒ½åŠ›ï¼›å¼‚å¸¸ä»¤ç‰Œå…·æœ‰ç¨€ç–ä¸”ä¸€è‡´çš„æ¿€æ´»æ¨¡å¼ã€‚</li>
<li>åŸºäºä¸Šè¿°å‘ç°ï¼Œæœ¬æ–‡æå‡ºä¸‰ç§äº’è¡¥æŠ€æœ¯ï¼šè¯­ä¹‰ç©ºé—´é‡æ–°åŠ æƒã€é€‰æ‹©æ€§å¤´éƒ¨å¢å¼ºå’Œå¼‚å¸¸ä»¤ç‰Œæ›¿æ¢ï¼Œä»¥æ¢å¤è§†è§‰è¾¨åˆ«èƒ½åŠ›å¹¶æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>åœ¨8ä¸ªå¸¸è§çš„è¯­ä¹‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šï¼ŒLHT-CLIPå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-60ec8ff7aca9dc68acde21ba1a162fcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291191&auth_key=1762291191-0-0-7be015e23141ecda3f92b53ab162f909&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe96ea22d8c747130f4c0d7110d6b972~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291198&auth_key=1762291198-0-0-63edde7611706a6a304e765a102c823f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6781613dedcc477e1ac98918c7709418~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291205&auth_key=1762291205-0-0-766a439df4f085edf6cdb8c7693a892a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c894c9fdbeb92d084d31e493f8ba22b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291211&auth_key=1762291211-0-0-7ce41c54ab08d5d4e5314c953dd1a518&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ce76d5e368d07766cd00c425f59ce374~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291218&auth_key=1762291218-0-0-4cee0301aad28fc041028bcd73f2fd41&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DQ3D-Depth-guided-Query-for-Transformer-Based-3D-Object-Detection-in-Traffic-Scenarios"><a href="#DQ3D-Depth-guided-Query-for-Transformer-Based-3D-Object-Detection-in-Traffic-Scenarios" class="headerlink" title="DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in   Traffic Scenarios"></a>DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in   Traffic Scenarios</h2><p><strong>Authors:Ziyu Wang, Wenhao Li, Ji Wu</strong></p>
<p>3D object detection from multi-view images in traffic scenarios has garnered significant attention in recent years. Many existing approaches rely on object queries that are generated from 3D reference points to localize objects. However, a limitation of these methods is that some reference points are often far from the target object, which can lead to false positive detections. In this paper, we propose a depth-guided query generator for 3D object detection (DQ3D) that leverages depth information and 2D detections to ensure that reference points are sampled from the surface or interior of the object. Furthermore, to address partially occluded objects in current frame, we introduce a hybrid attention mechanism that fuses historical detection results with depth-guided queries, thereby forming hybrid queries. Evaluation on the nuScenes dataset demonstrates that our method outperforms the baseline by 6.3% in terms of mean Average Precision (mAP) and 4.3% in the NuScenes Detection Score (NDS). </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œäº¤é€šåœºæ™¯ä¸­ä»å¤šè§†è§’å›¾åƒè¿›è¡Œ3Dç›®æ ‡æ£€æµ‹å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚è®¸å¤šç°æœ‰æ–¹æ³•ä¾èµ–äºä»3Då‚è€ƒç‚¹ç”Ÿæˆçš„ç›®æ ‡æŸ¥è¯¢æ¥å®šä½ç›®æ ‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„å±€é™æ€§åœ¨äºï¼ŒæŸäº›å‚è€ƒç‚¹å¾€å¾€è¿œç¦»ç›®æ ‡ç‰©ä½“ï¼Œä»è€Œå¯¼è‡´è¯¯æŠ¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äº3Dç›®æ ‡æ£€æµ‹çš„æ·±åº¦å¼•å¯¼æŸ¥è¯¢ç”Ÿæˆå™¨ï¼ˆDQ3Dï¼‰ï¼Œå®ƒåˆ©ç”¨æ·±åº¦ä¿¡æ¯å’Œ2Dæ£€æµ‹ç»“æœï¼Œç¡®ä¿å‚è€ƒç‚¹ä»ç‰©ä½“è¡¨é¢æˆ–å†…éƒ¨é‡‡æ ·ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³å½“å‰å¸§ä¸­éƒ¨åˆ†é®æŒ¡ç‰©ä½“çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ··åˆæ³¨æ„æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å°†å†å²æ£€æµ‹ç»“æœä¸æ·±åº¦å¼•å¯¼æŸ¥è¯¢ç›¸èåˆï¼Œå½¢æˆæ··åˆæŸ¥è¯¢ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ä¸Šæ¯”åŸºçº¿é«˜å‡º6.3%ï¼Œåœ¨NuScenesæ£€æµ‹åˆ†æ•°ï¼ˆNDSï¼‰ä¸Šé«˜å‡º4.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23144v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦å¼•å¯¼æŸ¥è¯¢ç”Ÿæˆå™¨ï¼ˆDQ3Dï¼‰ç”¨äºäº¤é€šåœºæ™¯ä¸­çš„ä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦ä¿¡æ¯å’ŒäºŒç»´æ£€æµ‹ç»“æœç¡®ä¿å‚è€ƒç‚¹ä»ç‰©ä½“è¡¨é¢æˆ–å†…éƒ¨é‡‡æ ·ï¼Œå¹¶å¼•å…¥æ··åˆæ³¨æ„åŠ›æœºåˆ¶èåˆå†å²æ£€æµ‹ç»“æœä¸æ·±åº¦å¼•å¯¼æŸ¥è¯¢ï¼Œä»¥å¤„ç†å½“å‰å¸§ä¸­çš„éƒ¨åˆ†é®æŒ¡ç‰©ä½“ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰å’ŒNuScenesæ£€æµ‹å¾—åˆ†ï¼ˆNDSï¼‰ä¸Šåˆ†åˆ«æé«˜äº†6.3%å’Œ4.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å¼•å¯¼æŸ¥è¯¢ç”Ÿæˆå™¨ï¼ˆDQ3Dï¼‰ç”¨äºä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚</li>
<li>DQ3Dåˆ©ç”¨æ·±åº¦ä¿¡æ¯å’ŒäºŒç»´æ£€æµ‹ç»“æœç¡®ä¿å‚è€ƒç‚¹æ›´å‡†ç¡®ï¼Œå‡å°‘è¯¯æ£€ã€‚</li>
<li>é’ˆå¯¹éƒ¨åˆ†é®æŒ¡ç‰©ä½“ï¼Œå¼•å…¥æ··åˆæ³¨æ„åŠ›æœºåˆ¶èåˆå†å²æ£€æµ‹ç»“æœä¸æ·±åº¦å¼•å¯¼æŸ¥è¯¢ï¼Œå½¢æˆæ··åˆæŸ¥è¯¢ã€‚</li>
<li>æ–¹æ³•åœ¨nuScenesæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ä¸Šæé«˜äº†6.3%ã€‚</li>
<li>åœ¨NuScenesæ£€æµ‹å¾—åˆ†ï¼ˆNDSï¼‰ä¸Šæé«˜äº†4.3%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0eacd0180ef6cb9f4cbf45556ad36e9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291226&auth_key=1762291226-0-0-a51f40f6785896d6ff620e9be18fb5b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b763609989af6cbd07d2752ca134e01~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291233&auth_key=1762291233-0-0-81839f50006344e8df6f55ac063a14b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8961c89b091d140fa2c232bb7afb662e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291240&auth_key=1762291240-0-0-2c27441226023e11d0750fd063f43c26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ca1d586cbbdc1fbac7044e52e912206~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291247&auth_key=1762291247-0-0-297049341ecd8b4a2100cc2d6c9e5972&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-763afb083c1757e624c43a541763b358~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291253&auth_key=1762291253-0-0-84b8eb53020adcb0eb977e3e79af15da&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a79e3473acd898561f73b6ea0d18066~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291260&auth_key=1762291260-0-0-77f4e9496ec32935caf64bd4abefdbe8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f4f8960992fb27af7b9ee60aa38d304b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291267&auth_key=1762291267-0-0-579519bdcac565ad5222025194ad883a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Diffusion-Driven-Two-Stage-Active-Learning-for-Low-Budget-Semantic-Segmentation"><a href="#Diffusion-Driven-Two-Stage-Active-Learning-for-Low-Budget-Semantic-Segmentation" class="headerlink" title="Diffusion-Driven Two-Stage Active Learning for Low-Budget Semantic   Segmentation"></a>Diffusion-Driven Two-Stage Active Learning for Low-Budget Semantic   Segmentation</h2><p><strong>Authors:Jeongin Kim, Wonho Bae, YouLee Han, Giyeong Oh, Youngjae Yu, Danica J. Sutherland, Junhyug Noh</strong></p>
<p>Semantic segmentation demands dense pixel-level annotations, which can be prohibitively expensive - especially under extremely constrained labeling budgets. In this paper, we address the problem of low-budget active learning for semantic segmentation by proposing a novel two-stage selection pipeline. Our approach leverages a pre-trained diffusion model to extract rich multi-scale features that capture both global structure and fine details. In the first stage, we perform a hierarchical, representation-based candidate selection by first choosing a small subset of representative pixels per image using MaxHerding, and then refining these into a diverse global pool. In the second stage, we compute an entropy-augmented disagreement score (eDALD) over noisy multi-scale diffusion features to capture both epistemic uncertainty and prediction confidence, selecting the most informative pixels for annotation. This decoupling of diversity and uncertainty lets us achieve high segmentation accuracy with only a tiny fraction of labeled pixels. Extensive experiments on four benchmarks (CamVid, ADE-Bed, Cityscapes, and Pascal-Context) demonstrate that our method significantly outperforms existing baselines under extreme pixel-budget regimes. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/jn-kim/two-stage-edald">https://github.com/jn-kim/two-stage-edald</a>. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²éœ€è¦å¯†é›†çš„åƒç´ çº§æ ‡æ³¨ï¼Œè¿™å¯èƒ½ä¼šéå¸¸æ˜‚è´µâ€”â€”ç‰¹åˆ«æ˜¯åœ¨æ ‡æ³¨é¢„ç®—æå…¶æœ‰é™çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡è§£å†³äº†ä½é¢„ç®—ä¸»åŠ¨å­¦ä¹ ä¸­è¯­ä¹‰åˆ†å‰²çš„é—®é¢˜ï¼Œé€šè¿‡æå‡ºä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µé€‰æ‹©æµç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥æå–ä¸°å¯Œçš„å¤šå°ºåº¦ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾æ—¢èƒ½æ•æ‰å…¨å±€ç»“æ„ï¼Œåˆèƒ½æ•æ‰ç»†èŠ‚ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åŸºäºè¡¨ç¤ºè¿›è¡Œåˆ†å±‚å€™é€‰é€‰æ‹©ï¼Œé¦–å…ˆä½¿ç”¨MaxHerdingé€‰æ‹©æ¯å¹…å›¾åƒä¸­å…·æœ‰ä»£è¡¨æ€§çš„å°éƒ¨åˆ†åƒç´ ï¼Œç„¶åå°†å…¶ç»†åŒ–ä¸ºå¤šæ ·åŒ–çš„å…¨å±€æ± ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬è®¡ç®—å¸¦æœ‰ç†µå¢å¼ºçš„åˆ†æ­§åˆ†æ•°ï¼ˆeDALDï¼‰ï¼Œé€šè¿‡å¯¹å™ªå£°å¤šå°ºåº¦æ‰©æ•£ç‰¹å¾è¿›è¡Œå»ºæ¨¡ï¼Œä»¥æ•æ‰è®¤çŸ¥ä¸ç¡®å®šæ€§å’Œé¢„æµ‹ç½®ä¿¡åº¦ï¼Œé€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„åƒç´ è¿›è¡Œæ ‡æ³¨ã€‚å¤šæ ·æ€§å’Œä¸ç¡®å®šæ€§çš„è¿™ç§è§£è€¦è®©æˆ‘ä»¬ä»…ä½¿ç”¨ä¸€å°éƒ¨åˆ†æ ‡è®°åƒç´ å°±èƒ½å®ç°é«˜åˆ†å‰²ç²¾åº¦ã€‚åœ¨CamVidã€ADE-Bedã€Cityscapeså’ŒPascal-Contextå››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨æç«¯åƒç´ é¢„ç®—æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/jn-kim/two-stage-edald%E3%80%82">https://github.com/jn-kim/two-stage-edaldã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22229v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†ä½é¢„ç®—ä¸‹è¯­ä¹‰åˆ†å‰²çš„ä¸»åŠ¨å­¦ä¹ é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µé€‰æ‹©ç®¡é“ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–ä¸°å¯Œçš„å¤šå°ºåº¦ç‰¹å¾ï¼Œç¬¬ä¸€é˜¶æ®µè¿›è¡ŒåŸºäºè¡¨ç¤ºçš„åˆ†å±‚å€™é€‰é€‰æ‹©ï¼Œç¬¬äºŒé˜¶æ®µè®¡ç®—åŒ…å«è®¤çŸ¥ä¸ç¡®å®šæ€§å’Œé¢„æµ‹ç½®ä¿¡åº¦çš„å¢å¼ºåˆ†æ­§åˆ†æ•°ï¼ˆeDALDï¼‰ï¼Œé€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„åƒç´ è¿›è¡Œæ ‡æ³¨ã€‚è¿™ç§æ–¹æ³•å®ç°äº†é«˜åˆ†å‰²ç²¾åº¦ï¼Œä¸”ä»…ä½¿ç”¨æå°‘é‡çš„æ ‡æ³¨åƒç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡è§£å†³äº†ä½é¢„ç®—ä¸‹è¯­ä¹‰åˆ†å‰²çš„ä¸»åŠ¨å­¦ä¹ é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µé€‰æ‹©ç®¡é“ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–å¤šå°ºåº¦ç‰¹å¾ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µè¿›è¡ŒåŸºäºè¡¨ç¤ºçš„åˆ†å±‚å€™é€‰é€‰æ‹©ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µè®¡ç®—åŒ…å«è®¤çŸ¥ä¸ç¡®å®šæ€§å’Œé¢„æµ‹ç½®ä¿¡åº¦çš„å¢å¼ºåˆ†æ­§åˆ†æ•°ï¼ˆeDALDï¼‰ã€‚</li>
<li>é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå®ç°äº†é«˜åˆ†å‰²ç²¾åº¦ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨æå°‘é‡çš„æ ‡æ³¨åƒç´ ã€‚</li>
<li>åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æç«¯åƒç´ é¢„ç®—ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-64f3f59e0d608042bd8c7ae775f0fefd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291275&auth_key=1762291275-0-0-d499676a1883492fd760dba6b3b5ce0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-874664debb2fd6fca1427efebcc3cdfc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291282&auth_key=1762291282-0-0-7092cba7cf3994cc553a7a5c5c124b82&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Object-Detection-Algorithms-for-Surface-Defect-Detection"><a href="#Comparative-Analysis-of-Object-Detection-Algorithms-for-Surface-Defect-Detection" class="headerlink" title="Comparative Analysis of Object Detection Algorithms for Surface Defect   Detection"></a>Comparative Analysis of Object Detection Algorithms for Surface Defect   Detection</h2><p><strong>Authors:Arpan Maity, Tamal Ghosh</strong></p>
<p>This article compares the performance of six prominent object detection algorithms, YOLOv11, RetinaNet, Fast R-CNN, YOLOv8, RT-DETR, and DETR, on the NEU-DET surface defect detection dataset, comprising images representing various metal surface defects, a crucial application in industrial quality control. Each modelâ€™s performance was assessed regarding detection accuracy, speed, and robustness across different defect types such as scratches, inclusions, and rolled-in scales. YOLOv11, a state-of-the-art real-time object detection algorithm, demonstrated superior performance compared to the other methods, achieving a remarkable 70% higher accuracy on average. This improvement can be attributed to YOLOv11s enhanced feature extraction capabilities and ability to process the entire image in a single forward pass, making it faster and more efficient in detecting minor surface defects. Additionally, YOLOv11â€™s architecture optimizations, such as improved anchor box generation and deeper convolutional layers, contributed to more precise localization of defects. In conclusion, YOLOv11â€™s outstanding performance in accuracy and speed solidifies its position as the most effective model for surface defect detection on the NEU dataset, surpassing competing algorithms by a substantial margin. </p>
<blockquote>
<p>æœ¬æ–‡åœ¨NEU-DETè¡¨é¢ç¼ºé™·æ£€æµ‹æ•°æ®é›†ä¸Šæ¯”è¾ƒäº†å…­ç§ä¸»æµç›®æ ‡æ£€æµ‹ç®—æ³•ï¼ˆYOLOv11ã€RetinaNetã€Fast R-CNNã€YOLOv8ã€RT-DETRå’ŒDETRï¼‰çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»£è¡¨å„ç§é‡‘å±è¡¨é¢ç¼ºé™·çš„å›¾åƒï¼Œæ˜¯å·¥ä¸šè´¨é‡æ§åˆ¶ä¸­çš„ä¸€é¡¹é‡è¦åº”ç”¨ã€‚è¯„ä¼°äº†æ¯ä¸ªæ¨¡å‹åœ¨æ£€æµ‹ç²¾åº¦ã€é€Ÿåº¦å’Œä¸åŒç¼ºé™·ç±»å‹ï¼ˆå¦‚åˆ’ç—•ã€å¤¹æ‚ç‰©å’Œè½§å…¥é³ç‰‡ç­‰ï¼‰çš„ç¨³å¥æ€§æ–¹é¢çš„æ€§èƒ½ã€‚YOLOv11æ˜¯æœ€å…ˆè¿›çš„å®æ—¶ç›®æ ‡æ£€æµ‹ç®—æ³•ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå…¶æ€§èƒ½æ›´ä¸ºä¼˜è¶Šï¼Œå¹³å‡å‡†ç¡®åº¦é«˜å‡º70%ã€‚è¿™ä¸€æ”¹è¿›å¯å½’å› äºYOLOv11å¢å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œä»¥åŠèƒ½å¤Ÿåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­å¤„ç†æ•´ä¸ªå›¾åƒçš„èƒ½åŠ›ï¼Œä½¿å…¶åœ¨æ£€æµ‹å¾®å°è¡¨é¢ç¼ºé™·æ—¶æ›´å¿«ã€æ›´é«˜æ•ˆã€‚æ­¤å¤–ï¼ŒYOLOv11çš„æ¶æ„ä¼˜åŒ–ï¼Œå¦‚æ”¹è¿›çš„é”šæ¡†ç”Ÿæˆå’Œæ›´æ·±çš„å·ç§¯å±‚ï¼Œæœ‰åŠ©äºæ›´ç²¾ç¡®åœ°å®šä½ç¼ºé™·ã€‚æ€»ä¹‹ï¼ŒYOLOv11åœ¨ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢çš„å‡ºè‰²è¡¨ç°ï¼Œä½¿å…¶åœ¨NEUæ•°æ®é›†ä¸Šæˆä¸ºè¡¨é¢ç¼ºé™·æ£€æµ‹çš„æœ€æœ‰æ•ˆæ¨¡å‹ï¼Œå¤§å¹…è¶…è¶Šäº†å…¶ä»–ç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21811v1">PDF</a> 14 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¯”è¾ƒäº†å…­ç§ä¸»æµç›®æ ‡æ£€æµ‹ç®—æ³•åœ¨NEU-DETè¡¨é¢ç¼ºé™·æ£€æµ‹æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬YOLOv11ã€RetinaNetã€Fast R-CNNã€YOLOv8ã€RT-DETRå’ŒDETRã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒYOLOv11åœ¨æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”å…¶å®ƒæ–¹æ³•é«˜å‡º70%ã€‚è¿™å¾—ç›ŠäºYOLOv11å¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ã€å¿«é€Ÿé«˜æ•ˆçš„å›¾åƒå¤„ç†èƒ½åŠ›ä»¥åŠä¼˜åŒ–çš„æ¶æ„ï¼Œå¦‚æ”¹è¿›é”šæ¡†ç”Ÿæˆå’Œæ›´æ·±çš„å·ç§¯å±‚ã€‚å› æ­¤ï¼ŒYOLOv11è¢«ç¡®è®¤ä¸ºNEUæ•°æ®é›†ä¸Šè¡¨é¢ç¼ºé™·æ£€æµ‹çš„æœ€æœ‰æ•ˆæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« å¯¹æ¯”äº†å…­ç§ä¸»æµç›®æ ‡æ£€æµ‹ç®—æ³•åœ¨è¡¨é¢ç¼ºé™·æ£€æµ‹æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>YOLOv11åœ¨æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>YOLOv11çš„å¹³å‡å‡†ç¡®ç‡æ¯”å…¶å®ƒæ–¹æ³•é«˜å‡º70%ã€‚</li>
<li>YOLOv11å¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›å’Œå¿«é€Ÿé«˜æ•ˆçš„å›¾åƒå¤„ç†èƒ½åŠ›æ˜¯æé«˜æ€§èƒ½çš„å…³é”®ã€‚</li>
<li>YOLOv11çš„æ¶æ„ä¼˜åŒ–ï¼Œå¦‚æ”¹è¿›é”šæ¡†ç”Ÿæˆå’Œæ›´æ·±çš„å·ç§¯å±‚ï¼Œæœ‰åŠ©äºæ›´ç²¾ç¡®åœ°å®šä½ç¼ºé™·ã€‚</li>
<li>YOLOv11è¢«ç¡®è®¤ä¸ºNEUæ•°æ®é›†ä¸Šè¡¨é¢ç¼ºé™·æ£€æµ‹çš„æœ€æœ‰æ•ˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5d38973685d6bfedb7d02aab6cd85b0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291289&auth_key=1762291289-0-0-4b541784d47c47206d927fcfe6c085fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a7dccdd540c70e5525d0a31cdf8feee2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291296&auth_key=1762291296-0-0-632418148ed3be22786feb58912ff0a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1696d35d4f8431d7470d3b70fa89ca8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291303&auth_key=1762291303-0-0-b6f6a17a565222ba4ccc504fe3c2c367&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b640180458140b3d70fd432898bf2b2c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291309&auth_key=1762291309-0-0-17544c10c13e03c82e7a0a46996527d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7f5a1361f2c9f8c6bac99ec9ba2bfcb2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291316&auth_key=1762291316-0-0-18a1653a9f058c5374a7a03bc0ee1d6a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="S3OD-Towards-Generalizable-Salient-Object-Detection-with-Synthetic-Data"><a href="#S3OD-Towards-Generalizable-Salient-Object-Detection-with-Synthetic-Data" class="headerlink" title="S3OD: Towards Generalizable Salient Object Detection with Synthetic Data"></a>S3OD: Towards Generalizable Salient Object Detection with Synthetic Data</h2><p><strong>Authors:Orest Kupyn, Hirokatsu Kataoka, Christian Rupprecht</strong></p>
<p>Salient object detection exemplifies data-bounded tasks where expensive pixel-precise annotations force separate model training for related subtasks like DIS and HR-SOD. We present a method that dramatically improves generalization through large-scale synthetic data generation and ambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000 high-resolution images created through our multi-modal diffusion pipeline that extracts labels from diffusion and DINO-v3 features. The iterative generation framework prioritizes challenging categories based on model performance. We propose a streamlined multi-mask decoder that naturally handles the inherent ambiguity in salient object detection by predicting multiple valid interpretations. Models trained solely on synthetic data achieve 20-50% error reduction in cross-dataset generalization, while fine-tuned versions reach state-of-the-art performance across DIS and HR-SOD benchmarks. </p>
<blockquote>
<p>æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹æ˜¯æ•°æ®è¾¹ç•Œä»»åŠ¡çš„å…¸å‹ç¤ºä¾‹ï¼Œæ˜‚è´µçš„åƒç´ ç²¾ç¡®æ ‡æ³¨è¿«ä½¿å¯¹å¦‚DISå’ŒHR-SODç­‰ç›¸å…³å­ä»»åŠ¡è¿›è¡Œå•ç‹¬æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡å¤§è§„æ¨¡åˆæˆæ•°æ®ç”Ÿæˆå’Œæ¨¡ç³Šæ„ŸçŸ¥æ¶æ„æ¥æ˜¾è‘—æé«˜æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†S3ODï¼Œè¿™æ˜¯ä¸€ä¸ªç”±æˆ‘ä»¬çš„å¤šæ¨¡æ€æ‰©æ•£ç®¡é“åˆ›å»ºçš„é«˜åˆ†è¾¨ç‡å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡139,000å¼ å›¾åƒï¼Œä»æ‰©æ•£å’ŒDINO-v3ç‰¹å¾ä¸­æå–æ ‡ç­¾ã€‚è¿­ä»£ç”Ÿæˆæ¡†æ¶æ ¹æ®æ¨¡å‹æ€§èƒ½ä¼˜å…ˆå¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„ç±»åˆ«ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€åŒ–çš„å¤šæ©è†œè§£ç å™¨ï¼Œå®ƒé€šè¿‡é¢„æµ‹å¤šä¸ªæœ‰æ•ˆè§£é‡Šæ¥è‡ªç„¶å¤„ç†æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ä¸­çš„å›ºæœ‰æ¨¡ç³Šæ€§ã€‚ä»…é€šè¿‡åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢å®ç°äº†20-50%çš„è¯¯å·®é™ä½ï¼Œè€Œå¾®è°ƒç‰ˆæœ¬åœ¨DISå’ŒHR-SODåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21605v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†æ˜¾è‘—ç›®æ ‡æ£€æµ‹ä¸­çš„æ•°æ®æœ‰ç•Œä»»åŠ¡ï¼Œé€šè¿‡å¤§è§„æ¨¡åˆæˆæ•°æ®ç”Ÿæˆå’Œæ¨¡ç³Šæ„ŸçŸ¥æ¶æ„æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æå‡ºäº†S3ODæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡13ä¸‡å¼ é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œé€šè¿‡å¤šæ¨¡æ€æ‰©æ•£ç®¡é“æå–æ ‡ç­¾ã€‚æ¨¡å‹åœ¨è·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢å®ç°äº†20-50%çš„é”™è¯¯ç‡é™ä½ï¼Œå¾®è°ƒåçš„ç‰ˆæœ¬åœ¨DISå’ŒHR-SODåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ˜¾è‘—ç›®æ ‡æ£€æµ‹æ˜¯æ•°æ®æœ‰ç•Œä»»åŠ¡çš„å…¸å‹ç¤ºä¾‹ï¼Œéœ€è¦æ˜‚è´µçš„åƒç´ ç²¾ç¡®æ³¨é‡Šæ¥è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šè¿‡å¤§è§„æ¨¡åˆæˆæ•°æ®ç”Ÿæˆæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ã€‚</li>
<li>ä»‹ç»äº†S3ODæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡13ä¸‡å¼ é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œé€šè¿‡å¤šæ¨¡æ€æ‰©æ•£ç®¡é“å’ŒDINO-v3ç‰¹æ€§æå–æ ‡ç­¾ã€‚</li>
<li>è¿­ä»£ç”Ÿæˆæ¡†æ¶ä¼˜å…ˆå¤„ç†åŸºäºæ¨¡å‹æ€§èƒ½çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„ç±»åˆ«ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€åŒ–çš„å¤šæ©è†œè§£ç å™¨ï¼Œèƒ½å¤Ÿè‡ªç„¶å¤„ç†æ˜¾è‘—ç›®æ ‡æ£€æµ‹ä¸­çš„å›ºæœ‰æ¨¡ç³Šæ€§ï¼Œé¢„æµ‹å¤šä¸ªæœ‰æ•ˆè§£é‡Šã€‚</li>
<li>ä»…åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨è·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢å®ç°äº†20-50%çš„é”™è¯¯ç‡é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-15ae92e90703de6d98797def991d9f9c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291323&auth_key=1762291323-0-0-71cd703a3584380f19963a00cbcea9a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56a6083945da5a293e4d5061bd61ba5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291330&auth_key=1762291330-0-0-425a12fbce389aa0ff158db26b1b227b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dee03d4715b5d844cd97d18e723159b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291337&auth_key=1762291337-0-0-fc4fb73c9cfd990b4c87d16df7b0a457&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66b578a681b895b6cbbe22c109096ff6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291343&auth_key=1762291343-0-0-2cb4278974fb1999121759ecb2987ce2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BioDet-Boosting-Industrial-Object-Detection-with-Image-Preprocessing-Strategies"><a href="#BioDet-Boosting-Industrial-Object-Detection-with-Image-Preprocessing-Strategies" class="headerlink" title="BioDet: Boosting Industrial Object Detection with Image Preprocessing   Strategies"></a>BioDet: Boosting Industrial Object Detection with Image Preprocessing   Strategies</h2><p><strong>Authors:Jiaqi Hu, Hongli Xu, Junwen Huang, Peter KT Yu, Slobodan Ilic, Benjamin Busam</strong></p>
<p>Accurate 6D pose estimation is essential for robotic manipulation in industrial environments. Existing pipelines typically rely on off-the-shelf object detectors followed by cropping and pose refinement, but their performance degrades under challenging conditions such as clutter, poor lighting, and complex backgrounds, making detection the critical bottleneck. In this work, we introduce a standardized and plug-in pipeline for 2D detection of unseen objects in industrial settings. Based on current SOTA baselines, our approach reduces domain shift and background artifacts through low-light image enhancement and background removal guided by open-vocabulary detection with foundation models. This design suppresses the false positives prevalent in raw SAM outputs, yielding more reliable detections for downstream pose estimation. Extensive experiments on real-world industrial bin-picking benchmarks from BOP demonstrate that our method significantly boosts detection accuracy while incurring negligible inference overhead, showing the effectiveness and practicality of the proposed method. </p>
<blockquote>
<p>ç²¾ç¡®çš„6Då§¿æ€ä¼°è®¡å¯¹äºå·¥ä¸šç¯å¢ƒä¸­çš„æœºå™¨äººæ“ä½œè‡³å…³é‡è¦ã€‚ç°æœ‰çš„æµç¨‹é€šå¸¸ä¾èµ–äºç°æˆçš„ç›®æ ‡æ£€æµ‹å™¨ï¼Œç„¶åæ˜¯è£å‰ªå’Œå§¿æ€ä¼˜åŒ–ï¼Œä½†åœ¨æ‚ä¹±ã€å…‰çº¿ä¸è¶³å’Œå¤æ‚èƒŒæ™¯ç­‰æŒ‘æˆ˜æ¡ä»¶ä¸‹ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šä¸‹é™ï¼Œä½¿å¾—æ£€æµ‹æˆä¸ºå…³é”®ç“¶é¢ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºå·¥ä¸šç¯å¢ƒä¸­æœªçŸ¥å¯¹è±¡çš„äºŒç»´æ£€æµ‹å¼•å…¥äº†ä¸€ç§æ ‡å‡†åŒ–å’Œå¯æ’å…¥çš„æµç¨‹ã€‚åŸºäºå½“å‰çš„æœ€ä½³åŸºçº¿æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä½å…‰å›¾åƒå¢å¼ºå’ŒèƒŒæ™¯å»é™¤æŠ€æœ¯å‡å°‘äº†é¢†åŸŸåç§»å’ŒèƒŒæ™¯ä¼ªå½±ï¼Œè¿™ç”±å¼€æ”¾è¯æ±‡æ£€æµ‹å¼•å¯¼å¹¶ä½¿ç”¨åŸºç¡€æ¨¡å‹å®ç°ã€‚è¿™ç§è®¾è®¡æŠ‘åˆ¶äº†åŸå§‹SAMè¾“å‡ºä¸­æ™®éå­˜åœ¨çš„è¯¯æŠ¥ï¼Œä¸ºä¸‹æ¸¸å§¿æ€ä¼°è®¡æä¾›äº†æ›´å¯é çš„æ£€æµ‹ç»“æœã€‚åœ¨æ¥è‡ªBOPçš„çœŸå®ä¸–ç•Œå·¥ä¸šè£…ç®±æŒ‘é€‰åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡ ä¹ä¸å¢åŠ æ¨ç†å¼€é”€çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦ï¼Œè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21000v1">PDF</a> 8 pages, accepted by ICCV 2025 R6D</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å·¥ä¸šç¯å¢ƒä¸­æœªè§ç‰©ä½“æ ‡å‡†åŒ–ä¸æ’ä»¶å¼çš„äºŒç»´æ£€æµ‹æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆè§£å†³äº†ç°æœ‰æµç¨‹åœ¨å¤æ‚ç¯å¢ƒä¸‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œå¦‚æ··ä¹±ã€ä½å…‰ç…§å’Œå¤æ‚èƒŒæ™¯ç­‰ã€‚é€šè¿‡ä½å…‰ç…§å›¾åƒå¢å¼ºå’ŒèƒŒæ™¯å»é™¤æŠ€æœ¯ï¼Œç»“åˆå¼€æ”¾è¯æ±‡æ£€æµ‹ä¸åŸºç¡€æ¨¡å‹å¼•å¯¼ï¼Œå‡å°‘äº†é¢†åŸŸåç§»å’ŒèƒŒæ™¯ä¼ªå½±ã€‚æ­¤è®¾è®¡æŠ‘åˆ¶äº†åŸå§‹SAMè¾“å‡ºä¸­çš„è¯¯æŠ¥ï¼Œä¸ºä¸‹æ¸¸å§¿æ€ä¼°è®¡æä¾›äº†æ›´å¯é çš„æ£€æµ‹ç»“æœã€‚åœ¨çœŸå®å·¥ä¸šç¯å¢ƒä¸­çš„æ‹£é€‰åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦ï¼ŒåŒæ—¶æ¨ç†å¼€é”€å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹å·¥ä¸šç¯å¢ƒä¸­æœªè§ç‰©ä½“çš„æ ‡å‡†åŒ–å’Œæ’ä»¶å¼äºŒç»´æ£€æµ‹æ–¹æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ¡ˆè§£å†³äº†ç°æœ‰æµç¨‹åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä½å…‰ç…§å›¾åƒå¢å¼ºå’ŒèƒŒæ™¯å»é™¤æŠ€æœ¯å‡å°‘é¢†åŸŸåç§»å’ŒèƒŒæ™¯ä¼ªå½±ã€‚</li>
<li>åˆ©ç”¨å¼€æ”¾è¯æ±‡æ£€æµ‹ä¸åŸºç¡€æ¨¡å‹å¼•å¯¼æŠ€æœ¯æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>å‡å°‘äº†åŸå§‹SAMè¾“å‡ºçš„è¯¯æŠ¥ï¼Œä¸ºä¸‹æ¸¸å§¿æ€ä¼°è®¡æä¾›äº†æ›´å¯é çš„æ£€æµ‹ç»“æœã€‚</li>
<li>åœ¨çœŸå®å·¥ä¸šç¯å¢ƒä¸­çš„å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-59ac1d228bcbd34770a4ac37d6c7c04b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291350&auth_key=1762291350-0-0-26ccd53890de176458b4f64a5f8e0f4f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7551bc98dabf4aae52e468373980c6e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291358&auth_key=1762291358-0-0-613406850b7bd51495f1a3f96c2f1e34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed37bcb18a67f7915ebd457eb746a67d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291364&auth_key=1762291364-0-0-b5549e1af262b230986b5d82ff9e0dc9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0cafe526c2e628174056b8712a6c746d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291371&auth_key=1762291371-0-0-9fbe6f7098f04d9aa7ed340dfa9a7358&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-416e9e236b136009e30b50a1784511a5~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291378&auth_key=1762291378-0-0-3466919e76b1466239ef04cc8d28260d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e4ec45510764f188b05d9eefb66c9cfe~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291385&auth_key=1762291385-0-0-15cd4d223b28432dfa48e4bffcc21f2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LEGNet-A-Lightweight-Edge-Gaussian-Network-for-Low-Quality-Remote-Sensing-Image-Object-Detection"><a href="#LEGNet-A-Lightweight-Edge-Gaussian-Network-for-Low-Quality-Remote-Sensing-Image-Object-Detection" class="headerlink" title="LEGNet: A Lightweight Edge-Gaussian Network for Low-Quality Remote   Sensing Image Object Detection"></a>LEGNet: A Lightweight Edge-Gaussian Network for Low-Quality Remote   Sensing Image Object Detection</h2><p><strong>Authors:Wei Lu, Si-Bao Chen, Hui-Dong Li, Qing-Ling Shu, Chris H. Q. Ding, Jin Tang, Bin Luo</strong></p>
<p>Remote sensing object detection (RSOD) often suffers from degradations such as low spatial resolution, sensor noise, motion blur, and adverse illumination. These factors diminish feature distinctiveness, leading to ambiguous object representations and inadequate foreground-background separation. Existing RSOD methods exhibit limitations in robust detection of low-quality objects. To address these pressing challenges, we introduce LEGNet, a lightweight backbone network featuring a novel Edge-Gaussian Aggregation (EGA) module specifically engineered to enhance feature representation derived from low-quality remote sensing images. EGA module integrates: (a) orientation-aware Scharr filters to sharpen crucial edge details often lost in low-contrast or blurred objects, and (b) Gaussian-prior-based feature refinement to suppress noise and regularize ambiguous feature responses, enhancing foreground saliency under challenging conditions. EGA module alleviates prevalent problems in reduced contrast, structural discontinuities, and ambiguous feature responses prevalent in degraded images, effectively improving model robustness while maintaining computational efficiency. Comprehensive evaluations across five benchmarks (DOTA-v1.0, v1.5, DIOR-R, FAIR1M-v1.0, and VisDrone2019) demonstrate that LEGNet achieves state-of-the-art performance, particularly in detecting low-quality objects.The code is available at <a target="_blank" rel="noopener" href="https://github.com/AeroVILab-AHU/LEGNet">https://github.com/AeroVILab-AHU/LEGNet</a>. </p>
<blockquote>
<p>é¥æ„Ÿç›®æ ‡æ£€æµ‹ï¼ˆRSODï¼‰ç»å¸¸å—åˆ°ç©ºé—´åˆ†è¾¨ç‡ä½ã€ä¼ æ„Ÿå™¨å™ªå£°ã€è¿åŠ¨æ¨¡ç³Šå’Œæ¶åŠ£ç…§æ˜ç­‰é—®é¢˜çš„å›°æ‰°ã€‚è¿™äº›å› ç´ é™ä½äº†ç‰¹å¾çš„åŒºåˆ†åº¦ï¼Œå¯¼è‡´ç›®æ ‡è¡¨ç¤ºä¸æ˜ç¡®ï¼Œå‰æ™¯ä¸èƒŒæ™¯åˆ†ç¦»ä¸è¶³ã€‚ç°æœ‰çš„RSODæ–¹æ³•åœ¨æ£€æµ‹ä½è´¨é‡ç›®æ ‡æ—¶è¡¨ç°å‡ºå±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›ç´§è¿«æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LEGNetï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ–°å‹è¾¹ç¼˜é«˜æ–¯èšåˆï¼ˆEGAï¼‰æ¨¡å—çš„è½»é‡çº§ä¸»å¹²ç½‘ç»œï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¢å¼ºä»ä½è´¨é‡é¥æ„Ÿå›¾åƒä¸­æ´¾ç”Ÿçš„ç‰¹å¾è¡¨ç¤ºã€‚EGAæ¨¡å—é›†æˆäº†ï¼šï¼ˆaï¼‰å®šå‘æ„ŸçŸ¥çš„Scharræ»¤æ³¢å™¨ï¼Œç”¨äºé”åŒ–ä½å¯¹æ¯”åº¦æˆ–æ¨¡ç³Šç›®æ ‡ä¸­ç»å¸¸ä¸¢å¤±çš„å…³é”®è¾¹ç¼˜ç»†èŠ‚ï¼›ï¼ˆbï¼‰åŸºäºé«˜æ–¯å…ˆéªŒçš„ç‰¹å¾ç»†åŒ–ï¼Œç”¨äºæŠ‘åˆ¶å™ªå£°å¹¶è§„èŒƒæ¨¡ç³Šçš„ç‰¹å¾å“åº”ï¼Œä»è€Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹å¢å¼ºå‰æ™¯æ˜¾è‘—æ€§ã€‚EGAæ¨¡å—å‡è½»äº†é€€åŒ–å›¾åƒä¸­å¸¸è§çš„å¯¹æ¯”åº¦é™ä½ã€ç»“æ„ä¸è¿ç»­å’Œç‰¹å¾å“åº”æ¨¡ç³Šçš„é—®é¢˜ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æœ‰æ•ˆæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆDOTA-v1.0ã€v1.5ã€DIOR-Rã€FAIR1M-v1.0å’ŒVisDrone2019ï¼‰ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒLEGNetå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹ä½è´¨é‡ç›®æ ‡æ–¹é¢ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Aerovilab-ahu/legnet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AeroVILab-AHU/LEGNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14012v3">PDF</a> 19 pages, 9 figures. Accepted by ICCV 2025 Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é¥æ„Ÿç‰©ä½“æ£€æµ‹ä¸­é‡åˆ°çš„ä½ç©ºé—´åˆ†è¾¨ç‡ã€ä¼ æ„Ÿå™¨å™ªå£°ã€è¿åŠ¨æ¨¡ç³Šå’Œä¸è‰¯ç…§æ˜ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLEGNetçš„è½»é‡çº§éª¨å¹²ç½‘ç»œã€‚è¯¥ç½‘ç»œåŒ…å«ä¸€ä¸ªæ–°å‹çš„è¾¹ç¼˜é«˜æ–¯èšåˆï¼ˆEGAï¼‰æ¨¡å—ï¼Œç”¨äºå¢å¼ºä»ä½è´¨é‡é¥æ„Ÿå›¾åƒä¸­æå–çš„ç‰¹å¾è¡¨ç¤ºã€‚EGAæ¨¡å—é€šè¿‡é›†æˆæ–¹å‘æ„ŸçŸ¥çš„Scharræ»¤æ³¢å™¨å’ŒåŸºäºé«˜æ–¯å…ˆéªŒçš„ç‰¹å¾ä¼˜åŒ–ï¼Œæé«˜äº†è¾¹ç¼˜ç»†èŠ‚çš„æ¸…æ™°åº¦ï¼Œå¹¶æŠ‘åˆ¶å™ªå£°å’Œæ¨¡ç³Šç‰¹å¾å“åº”ï¼Œä»è€Œæé«˜äº†å‰æ™¯åœ¨æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„æ˜¾è‘—æ€§ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒLEGNetåœ¨æ£€æµ‹ä½è´¨é‡ç‰©ä½“æ–¹é¢å–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿç‰©ä½“æ£€æµ‹ï¼ˆRSODï¼‰é¢ä¸´ä½ç©ºé—´åˆ†è¾¨ç‡ã€ä¼ æ„Ÿå™¨å™ªå£°ã€è¿åŠ¨æ¨¡ç³Šå’Œä¸è‰¯ç…§æ˜ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç‰¹å¾è¾¨è¯†åº¦çš„é™ä½å¯¼è‡´ç‰©ä½“è¡¨ç¤ºä¸æ˜ç¡®ï¼Œå‰æ™¯èƒŒæ™¯åˆ†ç¦»ä¸è¶³ã€‚</li>
<li>ç°æœ‰RSODæ–¹æ³•åœ¨æ£€æµ‹ä½è´¨é‡ç‰©ä½“æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LEGNetæ˜¯ä¸€ç§è½»é‡çº§éª¨å¹²ç½‘ç»œï¼ŒåŒ…å«æ–°å‹EGAæ¨¡å—ï¼Œæ—¨åœ¨å¢å¼ºä»ä½è´¨é‡é¥æ„Ÿå›¾åƒä¸­æ´¾ç”Ÿçš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>EGAæ¨¡å—é€šè¿‡é›†æˆæ–¹å‘æ„ŸçŸ¥çš„Scharræ»¤æ³¢å™¨ï¼Œæé«˜äº†è¾¹ç¼˜ç»†èŠ‚çš„æ¸…æ™°åº¦ã€‚</li>
<li>åŸºäºé«˜æ–¯å…ˆéªŒçš„ç‰¹å¾ä¼˜åŒ–æœ‰åŠ©äºæŠ‘åˆ¶å™ªå£°å’Œæ¨¡ç³Šç‰¹å¾å“åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ac7c2378e8ffd7affca6f8e626902bb0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291393&auth_key=1762291393-0-0-660c4b0a7f8d3981ac49382453219817&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5dbdcac895c261814c4e7caf7a127614~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291400&auth_key=1762291400-0-0-15683e29c3cb03ee7d97d39df3876f44&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b13374e6b6c4d5c830bf6072d8f78ecb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291407&auth_key=1762291407-0-0-b0cccf8fae26efba08072c15c9b462e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Circle-Representation-for-Medical-Instance-Object-Segmentation"><a href="#Circle-Representation-for-Medical-Instance-Object-Segmentation" class="headerlink" title="Circle Representation for Medical Instance Object Segmentation"></a>Circle Representation for Medical Instance Object Segmentation</h2><p><strong>Authors:Juming Xiong, Ethan H. Nguyen, Yilin Liu, Ruining Deng, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Haichun Yang, Agnes B. Fogo, Yuankai Huo</strong></p>
<p>Recently, circle representation has been introduced for medical imaging, designed specifically to enhance the detection of instance objects that are spherically shaped (e.g., cells, glomeruli, and nuclei). Given its outstanding effectiveness in instance detection, it is compelling to consider the application of circle representation for segmenting instance medical objects. In this study, we introduce CircleSnake, a simple end-to-end segmentation approach that utilizes circle contour deformation for segmenting ball-shaped medical objects at the instance level. The innovation of CircleSnake lies in these three areas: (1) It substitutes the complex bounding box-to-octagon contour transformation with a more consistent and rotation-invariant bounding circle-to-circle contour adaptation. This adaptation specifically targets ball-shaped medical objects. (2) The circle representation employed in CircleSnake significantly reduces the degrees of freedom to two, compared to eight in the octagon representation. This reduction enhances both the robustness of the segmentation performance and the rotational consistency of the method. (3) CircleSnake is the first end-to-end deep instance segmentation pipeline to incorporate circle representation, encompassing consistent circle detection, circle contour proposal, and circular convolution in a unified framework. This integration is achieved through the novel application of circular graph convolution within the context of circle detection and instance segmentation. In practical applications, such as the detection of glomeruli, nuclei, and eosinophils in pathological images, CircleSnake has demonstrated superior performance and greater rotation invariance when compared to benchmarks. The code has been made publicly available: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/CircleSnake">https://github.com/hrlblab/CircleSnake</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œåœ†å½¢è¡¨ç¤ºæ³•è¢«å¼•å…¥åˆ°åŒ»å­¦æˆåƒä¸­ï¼Œä¸“é—¨ç”¨äºå¢å¼ºçƒå½¢å®ä¾‹å¯¹è±¡ï¼ˆå¦‚ç»†èƒã€è‚¾å°çƒå’Œç»†èƒæ ¸ï¼‰çš„æ£€æµ‹ã€‚é‰´äºå…¶åœ¨å®ä¾‹æ£€æµ‹ä¸­çš„å‡ºè‰²æ•ˆæœï¼Œè€ƒè™‘å°†åœ†å½¢è¡¨ç¤ºæ³•åº”ç”¨äºåŒ»å­¦å®ä¾‹å¯¹è±¡çš„åˆ†å‰²æ˜¯éå¸¸æœ‰å¸å¼•åŠ›çš„ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†CircleSnakeï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„ç«¯åˆ°ç«¯åˆ†å‰²æ–¹æ³•ï¼Œåˆ©ç”¨åœ†å½¢è½®å»“å˜å½¢æ¥å¯¹çƒå½¢åŒ»å­¦å¯¹è±¡è¿›è¡Œå®ä¾‹çº§åˆ«çš„åˆ†å‰²ã€‚CircleSnakeçš„åˆ›æ–°ä¹‹å¤„åœ¨äºä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰å®ƒç”¨æ›´ä¸€è‡´ä¸”æ—‹è½¬ä¸å˜çš„åœ†å½¢è½®å»“æ›¿ä»£äº†å¤æ‚çš„è¾¹ç•Œæ¡†åˆ°å…«è¾¹å½¢è½®å»“çš„è½¬æ¢ã€‚è¿™ç§é€‚åº”æ€§ä¸“é—¨é’ˆå¯¹çƒå½¢åŒ»å­¦å¯¹è±¡ã€‚ï¼ˆ2ï¼‰CircleSnakeä¸­ä½¿ç”¨çš„åœ†å½¢è¡¨ç¤ºæ³•å°†è‡ªç”±åº¦é™ä½åˆ°ä¸¤ä¸ªï¼Œä¸å…«è¾¹å½¢è¡¨ç¤ºçš„å…«ä¸ªè‡ªç”±åº¦ç›¸æ¯”ã€‚è¿™ç§é™ä½å¢å¼ºäº†åˆ†å‰²æ€§èƒ½çš„é²æ£’æ€§å’Œæ–¹æ³•çš„æ—‹è½¬ä¸€è‡´æ€§ã€‚ï¼ˆ3ï¼‰CircleSnakeæ˜¯ç¬¬ä¸€ä¸ªé‡‡ç”¨åœ†å½¢è¡¨ç¤ºçš„ç«¯åˆ°ç«¯æ·±åº¦å®ä¾‹åˆ†å‰²ç®¡é“ï¼Œå®ƒåŒ…å«ä¸€è‡´çš„åœ†å½¢æ£€æµ‹ã€åœ†å½¢è½®å»“å»ºè®®å’Œåœ†å½¢å·ç§¯çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¿™ç§é›†æˆæ˜¯é€šè¿‡åœ¨åœ†å½¢æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²çš„ä¸Šä¸‹æ–‡ä¸­æ–°é¢–åœ°åº”ç”¨åœ†å½¢å›¾å·ç§¯æ¥å®ç°çš„ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚åœ¨ç—…ç†å›¾åƒä¸­æ£€æµ‹è‚¾å°çƒã€ç»†èƒæ ¸å’Œå—œé…¸æ€§ç²’ç»†èƒç­‰ï¼Œä¸åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒCircleSnakeè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ›´å¤§çš„æ—‹è½¬ä¸å˜æ€§ã€‚ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨ï¼š[<a target="_blank" rel="noopener" href="https://github.com/hrlblab/CircleSnake%E3%80%82]">https://github.com/hrlblab/CircleSnakeã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11507v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CircleSnakeæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹çƒå½¢åŒ»ç–—å¯¹è±¡å®ä¾‹çº§åˆ†å‰²çš„ç«¯åˆ°ç«¯åˆ†å‰²æ–¹æ³•ã€‚å®ƒé‡‡ç”¨åœ†å½¢è½®å»“å˜å½¢æŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥åœ†å½¢è¡¨ç¤ºï¼Œç®€åŒ–äº†å¤æ‚çš„è¾¹ç•Œæ¡†åˆ°å…«è¾¹å½¢è½®å»“è½¬æ¢ï¼Œå¹¶æ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½å’Œæ—‹è½¬ä¸€è‡´æ€§ã€‚CircleSnakeæ˜¯é¦–ä¸ªå°†åœ†å½¢è¡¨ç¤ºã€åœ†å½¢è½®å»“æè®®å’Œåœ†å½¢å·ç§¯æ•´åˆåœ¨ç»Ÿä¸€æ¡†æ¶ä¸­çš„æ·±åº¦å®ä¾‹åˆ†å‰²ç®¡é“ã€‚åœ¨è‚¾å°çƒã€ç»†èƒæ ¸å’Œç—…ç†å›¾åƒä¸­çš„å—œé…¸æ€§ç²’ç»†èƒæ£€æµ‹ç­‰å®é™…åº”ç”¨ä¸­ï¼Œä¸åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒCircleSnakeè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½å’Œæ›´é«˜çš„æ—‹è½¬ä¸å˜æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CircleSnakeæ˜¯ä¸€ç§é’ˆå¯¹çƒå½¢åŒ»ç–—å¯¹è±¡å®ä¾‹çº§åˆ†å‰²çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚</li>
<li>å®ƒé‡‡ç”¨åœ†å½¢è½®å»“å˜å½¢æŠ€æœ¯ï¼Œç®€åŒ–äº†è¾¹ç•Œæ¡†åˆ°å…«è¾¹å½¢è½®å»“è½¬æ¢ã€‚</li>
<li>åœ†å½¢è¡¨ç¤ºæ³•å‡å°‘äº†è‡ªç”±åº¦ï¼Œæé«˜äº†åˆ†å‰²æ€§èƒ½çš„é²æ£’æ€§å’Œæ—‹è½¬ä¸€è‡´æ€§ã€‚</li>
<li>CircleSnakeæ˜¯é¦–ä¸ªå°†åœ†å½¢è¡¨ç¤ºæ•´åˆåˆ°æ·±åº¦å®ä¾‹åˆ†å‰²ç®¡é“ä¸­çš„æ–¹æ³•ã€‚</li>
<li>åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒCircleSnakeç›¸å¯¹äºåŸºå‡†æµ‹è¯•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½å’Œæ—‹è½¬ä¸å˜æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a621882e286cf452e2ae205500afe8ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291415&auth_key=1762291415-0-0-0e51121b3cd466b4e8493e120f8b385f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b796d206d7321eb90adbf4c13850e069~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291423&auth_key=1762291423-0-0-95c07c4626058290a5d985a817a55fdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f2b32da03022f60a0bbff6497da9eda0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291431&auth_key=1762291431-0-0-c7b3e2423700d5d5d502d926b235ccd1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-401f664377888aecaf8320067548e9a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291438&auth_key=1762291438-0-0-b69f70a51044226f96fb089fbb1832cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-8dcf21e4ab860d40fa1af25369ab4f00~resize:0:q75.jpg?source=1f5c5e47&expiration=1762292016&auth_key=1762292016-0-0-68064410eabf122525835aad0ac7f7fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via   Ocular Cropping
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-755ac230a8575ea201695e23cbd4ac24~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289182&auth_key=1762289182-0-0-0e35274a626815f6ab7d24f4f01c7ee9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Vision Transformer for Robust Occluded Person Reidentification in   Complex Surveillance Scenes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
