<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Enabling Fast and Accurate Neutral Atom Readout through Image Denoising">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-225a662404a8c32f6e7e723321d5bdb4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286693&auth_key=1762286693-0-0-480c96c903aca0f3dfcfe11c18f308bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="Enabling-Fast-and-Accurate-Neutral-Atom-Readout-through-Image-Denoising"><a href="#Enabling-Fast-and-Accurate-Neutral-Atom-Readout-through-Image-Denoising" class="headerlink" title="Enabling Fast and Accurate Neutral Atom Readout through Image Denoising"></a>Enabling Fast and Accurate Neutral Atom Readout through Image Denoising</h2><p><strong>Authors:Chaithanya Naik Mude, Linipun Phuttitarn, Satvik Maurya, Kunal Sinha, Mark Saffman, Swamit Tannu</strong></p>
<p>Neutral atom quantum computers hold promise for scaling up to hundreds of thousands of qubits, but their progress is constrained by slow qubit readout. Measuring qubits currently takes milliseconds-much longer than the underlying quantum gate operations-making readout the primary bottleneck in deploying quantum error correction. Because each round of QEC depends on measurement, long readout times increase cycle duration and slow down program execution. Reducing the readout duration speeds up cycles and reduces decoherence errors that accumulate while qubits idle, but it also lowers the number of collected photons, making measurements noisier and more error-prone. This tradeoff leaves neutral atom systems stuck between slow but accurate readout and fast but unreliable readout.   We show that image denoising can resolve this tension. Our framework, GANDALF, uses explicit denoising using image translation to reconstruct clear signals from short, low-photon measurements, enabling reliable classification at up to 1.6x shorter readout times. Combined with lightweight classifiers and a pipelined readout design, our approach both reduces logical error rate by up to 35x and overall QEC cycle time up to 1.77x compared to state-of-the-art CNN-based readout for Cesium (Cs) Neutral Atom arrays. </p>
<blockquote>
<p>ä¸­æ€§åŸå­é‡å­è®¡ç®—æœºæœ‰æœ›å®ç°æ•°åä¸‡ä¸ªé‡å­æ¯”ç‰¹ï¼ˆqubitï¼‰çš„æ‰©å±•ï¼Œä½†å…¶è¿›å±•å—åˆ°ç¼“æ…¢é‡å­æ¯”ç‰¹è¯»å‡ºçš„é™åˆ¶ã€‚ç›®å‰æµ‹é‡é‡å­æ¯”ç‰¹éœ€è¦æ¯«ç§’æ—¶é—´ï¼Œè¿œè¿œè¶…è¿‡åº•å±‚é‡å­é—¨æ“ä½œçš„æŒç»­æ—¶é—´ï¼Œå› æ­¤è¯»å‡ºæ˜¯éƒ¨ç½²é‡å­é”™è¯¯æ ¡æ­£æ—¶çš„ä¸»ç“¶é¢ˆã€‚å› ä¸ºæ¯æ¬¡é‡å­é”™è¯¯æ ¡æ­£ï¼ˆQECï¼‰éƒ½ä¾èµ–äºæµ‹é‡ï¼Œæ‰€ä»¥è¯»å‡ºæ—¶é—´å»¶é•¿ä¼šå¢åŠ å‘¨æœŸæŒç»­æ—¶é—´å¹¶å‡æ…¢ç¨‹åºæ‰§è¡Œé€Ÿåº¦ã€‚å‡å°‘è¯»å‡ºæ—¶é—´å¯ä»¥åŠ å¿«å‘¨æœŸé€Ÿåº¦å¹¶å‡å°‘ç©ºé—²é‡å­ä½è„±ç›¸å¹²æ—¶ç§¯ç´¯çš„è¯¯å·®ï¼Œä½†å®ƒä¹Ÿä¼šå‡å°‘æ”¶é›†åˆ°çš„å…‰å­æ•°é‡ï¼Œä½¿å¾—æµ‹é‡ç»“æœæ›´å˜ˆæ‚ã€æ›´å®¹æ˜“å‡ºé”™ã€‚è¿™ç§æƒè¡¡è®©ä¸­æ€§åŸå­ç³»ç»Ÿå¤„äºæ…¢ä½†å‡†ç¡®çš„è¯»å‡ºå’Œå¿«ä½†ä¸å¯é çš„è¯»å‡ºä¹‹é—´ã€‚æˆ‘ä»¬è¯æ˜å›¾åƒå»å™ªå¯ä»¥ç¼“è§£è¿™ç§ç´§å¼ å…³ç³»ã€‚æˆ‘ä»¬çš„æ¡†æ¶GANDALFåˆ©ç”¨å›¾åƒè½¬æ¢æ˜¾å¼å»å™ªæ¥ä»çŸ­ã€ä½å…‰å­æµ‹é‡ä¸­é‡å»ºæ¸…æ™°ä¿¡å·ï¼Œä»è€Œåœ¨é«˜è¾¾1.6å€çš„è¾ƒçŸ­è¯»å‡ºæ—¶é—´å†…å®ç°å¯é åˆ†ç±»ã€‚ç»“åˆè½»é‡çº§åˆ†ç±»å™¨å’Œæµæ°´çº¿è¯»å‡ºè®¾è®¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†é€»è¾‘é”™è¯¯ç‡é™ä½äº†é«˜è¾¾35å€ï¼Œå¹¶ä¸”ä¸åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„é“¯ï¼ˆCsï¼‰ä¸­æ€§åŸå­é˜µåˆ—è¯»å‡ºç›¸æ¯”ï¼Œæ•´ä½“QECå‘¨æœŸæ—¶é—´ç¼©çŸ­äº†é«˜è¾¾1.77å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25982v1">PDF</a> 12 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>ä¸­æ€§åŸå­é‡å­è®¡ç®—æœºæœ‰æ½œåŠ›æ‰©å±•åˆ°æ•°åä¸‡é‡å­ä½ï¼Œä½†å…¶è¿›å±•å—åˆ°é‡å­ä½è¯»å‡ºé€Ÿåº¦è¾ƒæ…¢çš„é™åˆ¶ã€‚ç›®å‰é‡å­ä½çš„æµ‹é‡éœ€è¦æ¯«ç§’æ—¶é—´ï¼Œè¿œé•¿äºåº•å±‚é‡å­é—¨æ“ä½œçš„æ—¶é—´ï¼Œæˆä¸ºéƒ¨ç½²é‡å­é”™è¯¯æ ¡æ­£çš„ä¸»è¦ç“¶é¢ˆã€‚å‡å°‘è¯»å‡ºæ—¶é—´å¯ä»¥åŠ å¿«å¾ªç¯é€Ÿåº¦å¹¶å‡å°‘ç©ºé—²æ—¶é‡å­ä½çš„å¤±ç›¸å¹²é”™è¯¯ï¼Œä½†åŒæ—¶ä¹Ÿå‡å°‘äº†æ”¶é›†åˆ°çš„å…‰å­æ•°é‡ï¼Œä½¿æµ‹é‡ç»“æœæ›´åŠ å˜ˆæ‚ä¸”å®¹æ˜“å‡ºé”™ã€‚æœ¬æ–‡å±•ç¤ºäº†ä¸€ç§å›¾åƒå»å™ªæŠ€æœ¯å¯ä»¥è§£å†³è¿™ç§çŸ›ç›¾ã€‚ä½¿ç”¨å›¾åƒç¿»è¯‘è¿›è¡Œæ˜¾å¼å»å™ªçš„GANDALFæ¡†æ¶å¯ä»¥ä»çŸ­æš‚çš„ä½å…‰å­æµ‹é‡ä¸­é‡å»ºæ¸…æ™°çš„ä¿¡å·ï¼Œå®ç°åœ¨è¾ƒçŸ­çš„è¯»å‡ºæ—¶é—´å†…è¿›è¡Œå¯é çš„åˆ†ç±»ã€‚ç»“åˆè½»é‡çº§åˆ†ç±»å™¨å’Œç®¡é“è¯»å‡ºè®¾è®¡ï¼Œè¯¥æ–¹æ³•å°†é€»è¾‘é”™è¯¯ç‡é™ä½äº†é«˜è¾¾35å€ï¼Œå¹¶å°†åŸºäºé“¯ï¼ˆCsï¼‰ä¸­æ€§é˜µåˆ—çš„æœ€æ–°å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¯»å‡ºæŠ€æœ¯çš„æ€»ä½“QECå¾ªç¯æ—¶é—´ç¼©çŸ­äº†é«˜è¾¾1.77å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­æ€§åŸå­é‡å­è®¡ç®—æœºåœ¨æ‰©å±•é‡å­ä½æ–¹é¢æœ‰å¾ˆå¤§æ½œåŠ›ï¼Œä½†å—åˆ°æ…¢é‡å­ä½è¯»å‡ºçš„é™åˆ¶ã€‚</li>
<li>é‡å­ä½æµ‹é‡æ—¶é—´è¾ƒé•¿æ˜¯éƒ¨ç½²é‡å­é”™è¯¯æ ¡æ­£çš„ä¸»è¦ç“¶é¢ˆã€‚</li>
<li>è¯»å‡ºæ—¶é—´çš„ç¼©çŸ­å¯ä»¥å‡å°‘å¾ªç¯æ—¶é—´å’Œå‡å°‘ç©ºé—²æ—¶çš„å¤±ç›¸å¹²é”™è¯¯ã€‚</li>
<li>ç„¶è€Œï¼Œç¼©çŸ­è¯»å‡ºæ—¶é—´ä¼šå¯¼è‡´æ”¶é›†åˆ°çš„å…‰å­æ•°é‡å‡å°‘ï¼Œå¢åŠ æµ‹é‡ç»“æœçš„å™ªå£°å’Œé”™è¯¯ç‡ã€‚</li>
<li>GANDALFæ¡†æ¶ä½¿ç”¨å›¾åƒå»å™ªæŠ€æœ¯è§£å†³è¿™ä¸€çŸ›ç›¾ï¼Œèƒ½å¤Ÿä»çŸ­æš‚çš„ä½å…‰å­æµ‹é‡ä¸­é‡å»ºæ¸…æ™°çš„ä¿¡å·ã€‚</li>
<li>GANDALFæ¡†æ¶å¯ä»¥åœ¨è¾ƒçŸ­çš„è¯»å‡ºæ—¶é—´å†…å®ç°å¯é çš„åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ed4e9f4b8626aaea495d7d072034a793~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286482&auth_key=1762286482-0-0-fac28d58ba5885586a2bd519a80eb164&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad37b22da2e805f47ef6ce6b792b70de~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286489&auth_key=1762286489-0-0-764b3d5eef9b45975972c0538d46429c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f097610b3e53768ded0ad3d164f2d5bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286495&auth_key=1762286495-0-0-0086be233a61afbdc31c1c3bbe936a05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e788ccbf35f89fbfd06c56638c43cc98~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286501&auth_key=1762286501-0-0-12998d22cba249234437676c714dd0b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3478c575ae26c484cd92a864fe436342~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286508&auth_key=1762286508-0-0-95c12fce3d93153886457e383226e3ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-266437fb61027fd5f053f779c05931b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286515&auth_key=1762286515-0-0-8246557d4f02dcc1436f8ea5d3816ffe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Real-Time-Inference-of-Thin-Liquid-Film-Thickness-Profiles-from-Interference-Patterns-Using-Vision-Transformers"><a href="#Towards-Real-Time-Inference-of-Thin-Liquid-Film-Thickness-Profiles-from-Interference-Patterns-Using-Vision-Transformers" class="headerlink" title="Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from   Interference Patterns Using Vision Transformers"></a>Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from   Interference Patterns Using Vision Transformers</h2><p><strong>Authors:Gautam A. Viruthagiri, Arnuv Tandon, Gerald G. Fuller, Vinny Chandran Suja</strong></p>
<p>Thin film interferometry is a powerful technique for non-invasively measuring liquid film thickness with applications in ophthalmology, but its clinical translation is hindered by the challenges in reconstructing thickness profiles from interference patterns - an ill-posed inverse problem complicated by phase periodicity, imaging noise and ambient artifacts. Traditional reconstruction methods are either computationally intensive, sensitive to noise, or require manual expert analysis, which is impractical for real-time diagnostics. To address this challenge, here we present a vision transformer-based approach for real-time inference of thin liquid film thickness profiles directly from isolated interferograms. Trained on a hybrid dataset combining physiologically-relevant synthetic and experimental tear film data, our model leverages long-range spatial correlations to resolve phase ambiguities and reconstruct temporally coherent thickness profiles in a single forward pass from dynamic interferograms acquired in vivo and ex vivo. The network demonstrates state-of-the-art performance on noisy, rapidly-evolving films with motion artifacts, overcoming limitations of conventional phase-unwrapping and iterative fitting methods. Our data-driven approach enables automated, consistent thickness reconstruction at real-time speeds on consumer hardware, opening new possibilities for continuous monitoring of pre-lens ocular tear films and non-invasive diagnosis of conditions such as the dry eye disease. </p>
<blockquote>
<p>è–„è†œå¹²æ¶‰æµ‹é‡æœ¯æ˜¯ä¸€ç§å¼ºå¤§çš„æ— åˆ›æµ‹é‡æ¶²è†œåšåº¦çš„æŠ€æœ¯ï¼Œåœ¨çœ¼ç§‘æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œä½†å…¶ä¸´åºŠç¿»è¯‘å—åˆ°äº†ä»å¹²æ¶‰æ¨¡å¼é‡å»ºåšåº¦åˆ†å¸ƒçš„æŒ‘æˆ˜çš„é˜»ç¢â€”â€”è¿™æ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é€†é—®é¢˜ï¼Œç”±ç›¸ä½å‘¨æœŸæ€§ã€æˆåƒå™ªå£°å’Œç¯å¢ƒå¹²æ‰°ç‰©å¤æ‚åŒ–ã€‚ä¼ ç»Ÿçš„é‡å»ºæ–¹æ³•è¦ä¹ˆè®¡ç®—é‡å¤§ï¼Œå¯¹å™ªå£°æ•æ„Ÿï¼Œè¦ä¹ˆéœ€è¦æ‰‹åŠ¨ä¸“å®¶åˆ†æï¼Œè¿™å¯¹äºå®æ—¶è¯Šæ–­æ¥è¯´æ˜¯ä¸åˆ‡å®é™…çš„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰Transformerçš„æ–¹æ³•ï¼Œç”¨äºç›´æ¥ä»å­¤ç«‹çš„å¹²æ¶‰å›¾æ¨æ–­è–„æ¶²è†œçš„åšåº¦åˆ†å¸ƒï¼Œå®ç°å®æ—¶æ¨æ–­ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸€ä¸ªæ··åˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†ç»“åˆäº†ç”Ÿç†ä¸Šç›¸å…³çš„åˆæˆå’Œå®éªŒæ€§æ³ªè†œæ•°æ®ï¼Œåˆ©ç”¨è¿œç¨‹ç©ºé—´ç›¸å…³æ€§è§£å†³ç›¸ä½æ¨¡ç³Šé—®é¢˜ï¼Œå¹¶åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­ä»ä½“å†…å’Œç¦»ä½“è·å¾—çš„åŠ¨æ€å¹²æ¶‰å›¾é‡å»ºæ—¶é—´è¿è´¯çš„åšåº¦åˆ†å¸ƒã€‚è¯¥ç½‘ç»œåœ¨å¸¦æœ‰è¿åŠ¨ä¼ªå½±çš„å˜ˆæ‚ã€å¿«é€Ÿå˜åŒ–çš„è–„è†œä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…‹æœäº†ä¼ ç»Ÿç›¸ä½è§£å·å’Œè¿­ä»£æ‹Ÿåˆæ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é©±åŠ¨æ–¹æ³•èƒ½å¤Ÿåœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šä»¥å®æ—¶é€Ÿåº¦å®ç°è‡ªåŠ¨åŒ–ã€ä¸€è‡´çš„åšåº¦é‡å»ºï¼Œä¸ºè¿ç»­ç›‘æµ‹é¢„é€é•œçœ¼è¡¨æ³ªè†œå’Œéä¾µå…¥æ€§è¯Šæ–­å¹²çœ¼ç—‡ç­‰ç–¾ç—…æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25157v1">PDF</a> 6 pages, 2 figures, will be updated</p>
<p><strong>Summary</strong></p>
<p>è–„è†œå¹²æ¶‰æµ‹é‡æŠ€æœ¯æ˜¯ä¸€ç§éä¾µå…¥æ€§æµ‹é‡æ¶²ä½“è†œåšåº¦çš„æ–¹æ³•ï¼Œå¹¿æ³›åº”ç”¨äºçœ¼ç§‘ã€‚ç„¶è€Œï¼Œä»å¹²æ¶‰å›¾æ¡ˆé‡å»ºåšåº¦è½®å»“çš„æŒ‘æˆ˜é˜»ç¢äº†å…¶ä¸´åºŠè½¬åŒ–ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å˜å‹å™¨çš„å®æ—¶æ¨æ–­æ¶²ä½“è–„è†œåšåº¦è½®å»“çš„æ–¹æ³•ã€‚è¯¥æ¨¡å‹å¯ç›´æ¥ä»å­¤ç«‹çš„å¹²æ¶‰å›¾è¿›è¡Œæ¨æ–­ï¼Œåˆ©ç”¨é•¿æœŸç©ºé—´ç›¸å…³æ€§è§£å†³ç›¸ä½æ¨¡ç³Šé—®é¢˜ï¼Œå¹¶åœ¨åŠ¨æ€å¹²æ¶‰å›¾ä¸Šå®ç°å®æ—¶é€Ÿåº¦çš„ä¸€è‡´æ€§åšåº¦é‡å»ºã€‚æ­¤æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿç›¸ä½è§£å·å’Œè¿­ä»£æ‹Ÿåˆæ–¹æ³•çš„å±€é™æ€§ï¼Œä¸ºè¿ç»­ç›‘æµ‹é¢„é€é•œçœ¼å†…æ³ªè†œå’Œéä¾µå…¥æ€§è¯Šæ–­å¹²çœ¼ç—…ç­‰ç—…ç—‡æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è–„è†œå¹²æ¶‰æµ‹é‡æŠ€æœ¯å¯éä¾µå…¥æ€§åœ°æµ‹é‡æ¶²ä½“è†œåšåº¦ï¼Œåœ¨çœ¼ç§‘æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ä»å¹²æ¶‰å›¾æ¡ˆé‡å»ºåšåº¦è½®å»“æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œå—åˆ°ç›¸ä½å‘¨æœŸæ€§ã€æˆåƒå™ªå£°å’Œç¯å¢ƒå¹²æ‰°çš„å½±å“ã€‚</li>
<li>ä¼ ç»Ÿé‡å»ºæ–¹æ³•è®¡ç®—é‡å¤§ã€å™ªå£°æ•æ„Ÿæˆ–ä¾èµ–æ‰‹åŠ¨åˆ†æï¼Œä¸é€‚ç”¨äºå®æ—¶è¯Šæ–­ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å˜å‹å™¨çš„å®æ—¶æ¨æ–­æ–¹æ³•ï¼Œå¯ç›´æ¥ä»å¹²æ¶‰å›¾æ¨æ–­æ¶²ä½“è–„è†œåšåº¦è½®å»“ã€‚</li>
<li>æ¨¡å‹åˆ©ç”¨é•¿æœŸç©ºé—´ç›¸å…³æ€§è§£å†³ç›¸ä½æ¨¡ç³Šé—®é¢˜ï¼Œå®ç°å®æ—¶é€Ÿåº¦çš„ä¸€è‡´æ€§åšåº¦é‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿç›¸ä½è§£å·å’Œè¿­ä»£æ‹Ÿåˆæ–¹æ³•çš„å±€é™æ€§ï¼Œå…·æœ‰æ›´é«˜çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a9e40e0ff1b68f89c3f4ea85e682d9f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286522&auth_key=1762286522-0-0-3b3e7d65175b66e4a46e9ea5f883d33d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ffcdce91c21b7b78d23ed94df9ab599f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286530&auth_key=1762286530-0-0-f5a1e84bbb778548b67d31f77b9f9e18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-40912a792dee5f679db713d1e0efd2e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286537&auth_key=1762286537-0-0-1cb5efb528ca1f5603780dac89a0f646&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PISA-Bench-The-PISA-Index-as-a-Multilingual-and-Multimodal-Metric-for-the-Evaluation-of-Vision-Language-Models"><a href="#PISA-Bench-The-PISA-Index-as-a-Multilingual-and-Multimodal-Metric-for-the-Evaluation-of-Vision-Language-Models" class="headerlink" title="PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for   the Evaluation of Vision-Language Models"></a>PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for   the Evaluation of Vision-Language Models</h2><p><strong>Authors:Patrick Haller, Fabio Barth, Jonas Golde, Georg Rehm, Alan Akbik</strong></p>
<p>Vision-language models (VLMs) have demonstrated remarkable progress in multimodal reasoning. However, existing benchmarks remain limited in terms of high-quality, human-verified examples. Many current datasets rely on synthetically generated content by large language models (LLMs). Furthermore, most datasets are limited to English, as manual quality assurance of translated samples is time-consuming and costly. To fill this gap, we introduce PISA-Bench, a multilingual benchmark derived from English examples of the expert-created PISA tests, a unified framework for the assessment of student competencies in over eighty countries. Each example consists of human-extracted instructions, questions, answer options, and images, enriched with question type categories, and has been translated from English into five additional languages (Spanish, German, Chinese, French, and Italian), resulting in a fully parallel corpus covering six languages. We evaluate state-of-the-art vision-language models on PISA-Bench and find that especially small models (&lt;20B parameters) fail to achieve high test scores. We further find substantial performance degradation on non-English splits as well as high error-rates when models are tasked with spatial and geometric reasoning. By releasing the dataset and evaluation framework, we provide a resource for advancing research on multilingual multimodal reasoning. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•åœ¨é«˜è´¨é‡ã€ç»äººå·¥æ ¸å®çš„ä¾‹å­æ–¹é¢ä»æœ‰æ‰€é™åˆ¶ã€‚ç›®å‰è®¸å¤šæ•°æ®é›†ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆæˆç”Ÿæˆçš„å†…å®¹ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æ•°æ®é›†ä»…é™äºè‹±è¯­ï¼Œå› ä¸ºå¯¹ç¿»è¯‘æ ·æœ¬è¿›è¡Œäººå·¥è´¨é‡ä¿éšœæ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PISA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œæºäºä¸“å®¶åˆ›å»ºçš„PISAæµ‹è¯•ä¸­çš„è‹±è¯­æ ·æœ¬ã€‚PISAæµ‹è¯•æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å…«åå¤šä¸ªå›½å®¶çš„å­¦ç”Ÿèƒ½åŠ›ã€‚æ¯ä¸ªæ ·æœ¬éƒ½ç”±äººå·¥æå–çš„æŒ‡ä»¤ã€é—®é¢˜ã€ç­”æ¡ˆé€‰é¡¹å’Œå›¾åƒç»„æˆï¼Œè¿™äº›é—®é¢˜ç±»å‹ç±»åˆ«ä¸°å¯Œï¼Œå·²ä»è‹±è¯­ç¿»è¯‘æˆäº†å¦å¤–äº”ç§è¯­è¨€ï¼ˆè¥¿ç­ç‰™è¯­ã€å¾·è¯­ã€ä¸­æ–‡ã€æ³•è¯­å’Œæ„å¤§åˆ©è¯­ï¼‰ï¼Œå½¢æˆäº†ä¸€ä¸ªæ¶µç›–å…­ç§è¯­è¨€çš„å®Œå…¨å¹³è¡Œè¯­æ–™åº“ã€‚æˆ‘ä»¬åœ¨PISA-Benchä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å°¤å…¶æ˜¯å°å‹æ¨¡å‹ï¼ˆ&lt;20Bå‚æ•°ï¼‰å¾ˆéš¾å–å¾—è¾ƒé«˜çš„æµ‹è¯•æˆç»©ã€‚æˆ‘ä»¬è¿˜å‘ç°åœ¨éè‹±è¯­åˆ†å‰²ä¸Šçš„æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œä»¥åŠåœ¨ç©ºé—´æ¨¡å‹å’Œå‡ ä½•æ¨ç†ä»»åŠ¡ä¸­çš„é«˜é”™è¯¯ç‡ã€‚æˆ‘ä»¬é€šè¿‡å‘å¸ƒæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œä¸ºæ¨è¿›å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†ä¸€ä¸ªèµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24792v1">PDF</a> 8 pages, 11 tables and figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šå…ƒæ¨¡æ€æ¨ç†ä¸Šçš„çªå‡ºè¿›å±•ï¼ŒPISA-Benchçš„å‡ºç°å¼¥è¡¥äº†é«˜è´¨é‡å¤šè¯­è¨€æ•°æ®é›†çš„ç©ºç¼ºã€‚PISA-Benchæ¥æºäºè‹±è¯­åŸºå‡†æµ‹è¯•ä¾‹å­PISAçš„æµ‹è¯•å†…å®¹ï¼Œæ‰©å±•ä¸ºå¤šè¯­è¨€æ•°æ®é›†ã€‚å®ƒåŒ…æ‹¬äººå·¥æå–çš„æŒ‡ä»¤ã€é—®é¢˜ã€ç­”æ¡ˆé€‰é¡¹å’Œå›¾åƒç­‰ï¼Œè¦†ç›–å…­ä¸ªè¯­è¨€ï¼ˆè‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­ã€ä¸­æ–‡ç­‰ï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œå°å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨éè‹±è¯­æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´å‡ ä½•æ¨ç†æ–¹é¢å­˜åœ¨è¾ƒé«˜é”™è¯¯ç‡ã€‚PISA-Benchçš„å‘å¸ƒä¸ºæ¨è¿›å¤šè¯­è¨€å¤šå…ƒæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PISA-Benchæ˜¯åŸºäºä¸“å®¶åˆ›å»ºçš„PISAæµ‹è¯•è‹±è¯­ç¤ºä¾‹çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ•°æ®é›†åŒ…å«äººå·¥æå–çš„æŒ‡ä»¤ã€é—®é¢˜ã€ç­”æ¡ˆé€‰é¡¹å’Œå›¾åƒç­‰ä¸°å¯Œå†…å®¹ã€‚</li>
<li>æ•°æ®é›†ç¿»è¯‘è‡³å…­ç§è¯­è¨€ï¼Œå½¢æˆå®Œå…¨å¹³è¡Œçš„è¯­æ–™åº“ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨PISA-Benchä¸Šçš„è¡¨ç°å‚å·®ä¸é½ï¼Œå°å‹æ¨¡å‹æ€§èƒ½è¾ƒå·®ã€‚</li>
<li>éè‹±è¯­æ•°æ®é›†ä¸Šï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>æ¨¡å‹åœ¨ç©ºé—´å‡ ä½•æ¨ç†æ–¹é¢çš„é”™è¯¯ç‡è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e63170b9a741c09e5de0e2c62bb237d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286544&auth_key=1762286544-0-0-e8bcc5588515f2b00e4a406c34d14d19&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c17893b4a5da046157dfd2da811e9a86~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286552&auth_key=1762286552-0-0-8ad6ac1cf395a9a9eca76cce280cd40f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-24e69397f2b60d46dfaec1b86d7a79e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286558&auth_key=1762286558-0-0-1010058914ea6189dbe030dab4381f7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a91fa7f12361a809204cd2eaa081a8ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286564&auth_key=1762286564-0-0-e3957c082c61b0ba84996894c24fb6ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6e61c619902db031dcad741b9d2db12~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286571&auth_key=1762286571-0-0-509697509258f71f0d7cf2c7103aacf9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a13b322d5a06518b856ffa190480e608~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286577&auth_key=1762286577-0-0-a94541e21859ef97f594fd91bd9c7c54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2305c3b355441419437b6f7968658e8d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286584&auth_key=1762286584-0-0-af75411e46e6c62d617deca7076e341b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs"><a href="#Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs" class="headerlink" title="Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal   Reasoning in MLLMs"></a>Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal   Reasoning in MLLMs</h2><p><strong>Authors:Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending modelâ€™s textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: <a target="_blank" rel="noopener" href="https://latent-sketchpad.github.io/">https://latent-sketchpad.github.io/</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦è§†è§‰è§„åˆ’å’Œæƒ³è±¡åŠ›çš„å¤æ‚åœºæ™¯ä¸­å¾€å¾€è¡¨ç°æŒ£æ‰ã€‚æˆ‘ä»¬å—åˆ°äººç±»å¦‚ä½•ä½¿ç”¨è‰å›¾ä½œä¸ºè§†è§‰æ€ç»´çš„ä¸€ç§å½¢å¼æ¥å‘å±•å’Œäº¤æµæ€æƒ³çš„å¯å‘ï¼Œå¼•å…¥äº†â€œæ½œåœ¨è‰å›¾æ¿â€ï¼ˆLatent Sketchpadï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ºMLLMsé…å¤‡äº†å†…éƒ¨è§†è§‰è‰ç¨¿æ¿ã€‚MLLMsçš„å†…éƒ¨è§†è§‰è¡¨ç¤ºä¼ ç»Ÿä¸Šä»…é™äºæ„ŸçŸ¥ç†è§£ã€‚æˆ‘ä»¬é‡æ–°è®¾è®¡å®ƒä»¬ä»¥æ”¯æŒç”Ÿæˆæ€§è§†è§‰æ€ç»´ï¼ŒåŒæ—¶ä¸æŸå®³æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨å‰æ²¿çš„MLLMsä¹‹ä¸Šï¼Œå°†è§†è§‰ç”Ÿæˆç›´æ¥é›†æˆåˆ°å…¶æœ¬åœ°çš„è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ã€‚å®ƒå…è®¸æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†å’Œè§†è§‰æ½œåœ¨ç”Ÿæˆä¹‹é—´è¿›è¡Œäº¤æ›¿ã€‚è¿™äº›æ½œåœ¨å› ç´ å¼•å¯¼å†…éƒ¨æ€ç»´è¿‡ç¨‹ï¼Œå¹¶å¯è½¬åŒ–ä¸ºè‰å›¾å›¾åƒä»¥ä¾¿äºè§£é‡Šã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªç»„ä»¶ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰å¤´ï¼ˆContext-Aware Vision Headï¼‰è‡ªå›å½’åœ°ç”Ÿæˆè§†è§‰è¡¨ç¤ºï¼Œé¢„è®­ç»ƒçš„è‰å›¾è§£ç å™¨ï¼ˆSketch Decoderï¼‰å°†è¿™äº›ç”Ÿæˆå›¾åƒè½¬åŒ–ä¸ºäººç±»å¯ç†è§£çš„å›¾åƒã€‚æˆ‘ä»¬åœ¨æ–°çš„æ•°æ®é›†è¿·å®«è§„åˆ’ï¼ˆMazePlanningï¼‰ä¸Šè¯„ä¼°äº†æ¡†æ¶çš„æ€§èƒ½ã€‚å¯¹å„ç§MLLMsçš„å®éªŒè¡¨æ˜ï¼Œæ½œåœ¨è‰å›¾æ¿åœ¨æ¨ç†æ€§èƒ½ä¸Šå®ç°äº†ä¸åŸºå‡†æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½çš„è¡¨ç°ã€‚å®ƒè¿›ä¸€æ­¥æ¨å¹¿åˆ°äº†ä¸åŒçš„å‰æ²¿MLLMsï¼ŒåŒ…æ‹¬Gemma3å’ŒQwen2.5-VLã€‚é€šè¿‡å°†æ¨¡å‹çš„æ–‡æœ¬æ¨ç†æ‰©å±•åˆ°è§†è§‰æ€ç»´ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ºæ›´ä¸°å¯Œçš„äººæœºäº¤äº’å’Œæ›´å¹¿æ³›çš„åº”ç”¨æ‰“å¼€äº†æ–°çš„æœºä¼šã€‚æ›´å¤šç»†èŠ‚å’Œèµ„æºå¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://latent-sketchpad.github.io/%E3%80%82">https://latent-sketchpad.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24514v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦è§†è§‰è§„åˆ’å’Œæƒ³è±¡åŠ›çš„å¤æ‚åœºæ™¯ä¸­å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚é€šè¿‡å€Ÿé‰´äººç±»åˆ©ç”¨ç»˜ç”»ä½œä¸ºè§†è§‰æ€ç»´çš„æ–¹å¼æ¥è¡¨è¾¾å’Œæ²Ÿé€šæ€æƒ³ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†Latent Sketchpadæ¡†æ¶ï¼Œä¸ºMLLMæä¾›äº†ä¸€ä¸ªå†…éƒ¨è§†è§‰è‰ç¨¿ç®±ã€‚è¯¥æ¡†æ¶ä½¿MLLMèƒ½å¤Ÿåœ¨ä¸æŸå®³æ¨ç†èƒ½åŠ›çš„æƒ…å†µä¸‹æ”¯æŒç”Ÿæˆæ€§è§†è§‰æ€ç»´ã€‚å®ƒé›†æˆäº†å‰æ²¿çš„MLLMæŠ€æœ¯ï¼Œå°†è§†è§‰ç”Ÿæˆç›´æ¥èå…¥å…¶å¤©ç”Ÿçš„è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ã€‚è¯¥æ¡†æ¶å…è®¸æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†ä¸è§†è§‰æ½œåœ¨ç”Ÿæˆä¹‹é—´è¿›è¡Œäº¤æ›¿ï¼Œè¿™äº›æ½œåœ¨ç”Ÿæˆç‰©å¯ä»¥æŒ‡å¯¼å†…éƒ¨æ€ç»´è¿‡ç¨‹å¹¶å¯è½¬åŒ–ä¸ºè‰å›¾å›¾åƒä»¥ä¾¿äºè§£é‡Šã€‚Latent Sketchpadæ¡†æ¶é€šè¿‡ä¸¤ä¸ªç»„ä»¶å®ç°è¿™ä¸€ç›®æ ‡ï¼šè‡ªå›å½’ç”Ÿæˆè§†è§‰è¡¨ç¤ºçš„Context-Aware Vision Headå’Œå°†è§†è§‰è¡¨ç¤ºè½¬åŒ–ä¸ºäººç±»å¯ç†è§£çš„å›¾åƒçš„é¢„è®­ç»ƒSketch Decoderã€‚å®éªŒè¡¨æ˜ï¼ŒLatent Sketchpadæ¡†æ¶åœ¨å¤šä¸ªMLLMä¸Šçš„è¡¨ç°ä¸å…¶ä¸»å¹²æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼Œå¹¶å¯ä»¥å¹¿æ³›åº”ç”¨äºä¸åŒçš„å‰æ²¿MLLMæ¨¡å‹ï¼Œå¦‚Gemma3å’ŒQwen2.5-VLç­‰ã€‚é€šè¿‡æ‰©å±•æ¨¡å‹çš„æ–‡æœ¬æ¨ç†è‡³è§†è§‰æ€ç»´ï¼Œè¯¥æ¡†æ¶ä¸ºæ›´ä¸°å¯Œçš„äººæœºäº¤äº’å’Œæ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯æ‰“å¼€äº†æ–°çš„æœºä¼šã€‚æ›´å¤šç»†èŠ‚å’Œèµ„æºå¯è®¿é—®ç ”ç©¶å›¢é˜Ÿçš„é¡¹ç›®ç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMåœ¨è§†è§‰ç†è§£ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼ˆå¦‚éœ€è¦è§†è§‰è§„åˆ’å’Œæƒ³è±¡åŠ›çš„åœºæ™¯ï¼‰å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>äººç±»åˆ©ç”¨ç»˜ç”»ä½œä¸ºè§†è§‰æ€ç»´çš„æ–¹å¼å¯å‘ç ”ç©¶å›¢é˜Ÿå¼€å‘Latent Sketchpadæ¡†æ¶ã€‚</li>
<li>Latent Sketchpadä¸ºMLLMæä¾›äº†ä¸€ä¸ªå†…éƒ¨è§†è§‰è‰ç¨¿ç®±ï¼Œæ”¯æŒç”Ÿæˆæ€§è§†è§‰æ€ç»´è€Œä¸æŸå®³æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†å‰æ²¿çš„MLLMæŠ€æœ¯ï¼Œå°†è§†è§‰ç”Ÿæˆèå…¥å…¶è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ã€‚</li>
<li>Latent SketchpadåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šContext-Aware Vision Headå’Œé¢„è®­ç»ƒçš„Sketch Decoderã€‚</li>
<li>å®éªŒè¡¨æ˜Latent Sketchpadåœ¨å¤šä¸ªMLLMä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å¯ä»¥å¹¿æ³›åº”ç”¨äºä¸åŒçš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-58cb3b6cfc2b075a068e47d809d6c7bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286591&auth_key=1762286591-0-0-04dff5bb0c296f9a2ba0ee1cf21bc088&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-125df9d57d4798ee1929636060895bcb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286599&auth_key=1762286599-0-0-e1d5627d7c130deb23d56ea76ca48555&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-170fd4ccef3b6b3ac7230db060c55522~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286606&auth_key=1762286606-0-0-267fe204d855f2d76ea814fcbc7448d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Robust-and-Generalizable-Background-Subtraction-on-Images-of-Calorimeter-Jets-using-Unsupervised-Generative-Learning"><a href="#Robust-and-Generalizable-Background-Subtraction-on-Images-of-Calorimeter-Jets-using-Unsupervised-Generative-Learning" class="headerlink" title="Robust and Generalizable Background Subtraction on Images of Calorimeter   Jets using Unsupervised Generative Learning"></a>Robust and Generalizable Background Subtraction on Images of Calorimeter   Jets using Unsupervised Generative Learning</h2><p><strong>Authors:Yeonju Go, Dmitrii Torbunov, Yi Huang, Shuhang Li, Timothy Rinn, Haiwang Yu, Brett Viren, Meifeng Lin, Yihui Ren, Dennis Perepelitsa, Jin Huang</strong></p>
<p>Accurate separation of signal from background is one of the main challenges for precision measurements across high-energy and nuclear physics. Conventional supervised learning methods are insufficient here because the required paired signal and background examples are impossible to acquire in real experiments. Here, we introduce an unsupervised unpaired image-to-image translation neural network that learns to separate the signal and background from the input experimental data using cycle-consistency principles. We demonstrate the efficacy of this approach using images composed of simulated calorimeter data from the sPHENIX experiment, where physics signals (jets) are immersed in the extremely dense and fluctuating heavy-ion collision environment. Our method outperforms conventional subtraction algorithms in fidelity and overcomes the limitations of supervised methods. Furthermore, we evaluated the modelâ€™s robustness in an out-of-distribution test scenario designed to emulate modified jets as in real experimental data. The model, trained on a simpler dataset, maintained its high fidelity on a more realistic, highly modified jet signal. This work represents the first use of unsupervised unpaired generative models for full detector jet background subtraction and offers a path for novel applications in real experimental data, enabling high-precision analyses across a wide range of imaging-based experiments. </p>
<blockquote>
<p>ä»èƒŒæ™¯ä¸­å‡†ç¡®åˆ†ç¦»ä¿¡å·æ˜¯é«˜èƒ½å’Œæ ¸ç‰©ç†ç²¾ç¡®æµ‹é‡é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è¿™é‡Œæ˜¯ä¸å¤Ÿçš„ï¼Œå› ä¸ºåœ¨çœŸå®å®éªŒä¸­æ— æ³•è·å–æ‰€éœ€çš„é…å¯¹ä¿¡å·å’ŒèƒŒæ™¯ç¤ºä¾‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ— ç›‘ç£çš„ã€éé…å¯¹çš„å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œåˆ©ç”¨å¾ªç¯ä¸€è‡´æ€§åŸç†ï¼Œä»è¾“å…¥çš„å®éªŒæ•°æ®ä¸­å­¦ä¹ åˆ†ç¦»ä¿¡å·å’ŒèƒŒæ™¯ã€‚æˆ‘ä»¬ä½¿ç”¨ç”±sPHENIXå®éªŒæ¨¡æ‹Ÿçš„caloä»ªæ•°æ®ç»„æˆçš„å›¾åƒæ¥è¯æ˜è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰©ç†ä¿¡å·ï¼ˆjetï¼‰æ²‰æµ¸åœ¨æå…¶å¯†é›†ä¸”æ³¢åŠ¨çš„é‡ç¦»å­ç¢°æ’ç¯å¢ƒä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿çœŸåº¦ä¸Šä¼˜äºä¼ ç»Ÿçš„å‡æ³•ç®—æ³•ï¼Œå¹¶å…‹æœäº†ç›‘ç£æ–¹æ³•çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è®¾è®¡ç”¨äºæ¨¡æ‹ŸçœŸå®å®éªŒæ•°æ®ä¸­ä¿®æ”¹è¿‡çš„jetçš„ç¦»ç¾¤æµ‹è¯•åœºæ™¯ä¸­è¯„ä¼°äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨æ›´ç®€å•æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨å¯¹æ›´ä¸ºç°å®ã€é«˜åº¦ä¿®æ”¹è¿‡çš„jetä¿¡å·ä¸Šä»èƒ½ä¿æŒå…¶é«˜ä¿çœŸåº¦ã€‚è¿™é¡¹å·¥ä½œä»£è¡¨äº†æ— ç›‘ç£éé…å¯¹ç”Ÿæˆæ¨¡å‹åœ¨å®Œæ•´çš„æ¢æµ‹å™¨jetèƒŒæ™¯å‡æ³•ä¸­çš„é¦–æ¬¡åº”ç”¨ï¼Œå¹¶ä¸ºçœŸå®å®éªŒæ•°æ®ä¸­çš„æ–°å‹åº”ç”¨å¼€è¾Ÿäº†é“è·¯ï¼Œå¯åœ¨å¹¿æ³›çš„æˆåƒå®éªŒä¸­è¿›è¡Œé«˜ç²¾åº¦åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23717v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨é«˜èƒ½å’Œæ ¸ç‰©ç†å­¦ä¸­çš„ç²¾å¯†æµ‹é‡ä¸­å‡†ç¡®åœ°å°†ä¿¡å·ä¸èƒŒæ™¯åˆ†ç¦»ã€‚ç”±äºåœ¨å®é™…å®éªŒä¸­æ— æ³•è·å–æ‰€éœ€çš„é…å¯¹ä¿¡å·å’ŒèƒŒæ™¯æ ·æœ¬ï¼Œä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è¿™é‡Œå¹¶ä¸é€‚ç”¨ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ— ç›‘ç£çš„ã€æ— éœ€é…å¯¹çš„å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œåˆ©ç”¨å¾ªç¯ä¸€è‡´æ€§åŸç†ä»å®éªŒæ•°æ®ä¸­å­¦ä¹ ä¿¡å·å’ŒèƒŒæ™¯çš„åˆ†ç¦»ã€‚é€šè¿‡å¯¹æ¨¡æ‹Ÿçš„sPHENIXå®éªŒé‡èƒ½å™¨æ•°æ®çš„å›¾åƒè¿›è¡ŒéªŒè¯ï¼Œæœ¬æ–¹æ³•åœ¨ä¿çœŸåº¦ä¸Šä¼˜äºä¼ ç»Ÿå‡æ³•ç®—æ³•ï¼Œå¹¶å…‹æœäº†ç›‘ç£æ–¹æ³•çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œåœ¨æ¨¡æ‹ŸçœŸå®å®éªŒæ•°æ®çš„åˆ†å¸ƒå¤–æµ‹è¯•åœºæ™¯ä¸­è¯„ä¼°äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚å³ä½¿åœ¨ç»è¿‡é«˜åº¦ä¿®æ”¹çš„å–·æ°”ä¿¡å·ä¸Šï¼Œè¯¥æ¨¡å‹ä¾ç„¶ä¿æŒäº†è¾ƒé«˜çš„ä¿çœŸåº¦ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å°†æ— ç›‘ç£çš„éé…å¯¹ç”Ÿæˆæ¨¡å‹åº”ç”¨äºå…¨æ¢æµ‹å™¨å–·æ°”èƒŒæ™¯å‡æ³•ï¼Œå¹¶ä¸ºçœŸå®å®éªŒæ•°æ®ä¸­çš„æ–°é¢–åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œä½¿å¹¿æ³›çš„æˆåƒå®éªŒå®ç°é«˜ç²¾åº¦åˆ†ææˆä¸ºå¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯åœ¨é«˜èƒ½å’Œæ ¸ç‰©ç†å­¦ç²¾å¯†æµ‹é‡ä¸­å‡†ç¡®åŒºåˆ†ä¿¡å·å’ŒèƒŒæ™¯ã€‚</li>
<li>ä¼ ç»Ÿç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨æ­¤åœºæ™¯ä¸‹ä¸é€‚ç”¨ï¼Œå› ä¸ºæ— æ³•è·å–å®é™…å®éªŒä¸­çš„é…å¯¹ä¿¡å·å’ŒèƒŒæ™¯æ ·æœ¬ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ— ç›‘ç£çš„ã€æ— éœ€é…å¯¹çš„å›¾åƒåˆ°å›¾åƒè½¬æ¢ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œèƒ½åˆ©ç”¨å¾ªç¯ä¸€è‡´æ€§åŸç†ä»å®éªŒæ•°æ®ä¸­å­¦ä¹ ä¿¡å·å’ŒèƒŒæ™¯çš„åˆ†ç¦»ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿçš„sPHENIXå®éªŒæ•°æ®ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æ•ˆç”¨ï¼Œå¹¶æ˜¾ç¤ºäº†å®ƒåœ¨ä¿çœŸåº¦ä¸Šä¼˜äºä¼ ç»Ÿå‡æ³•ç®—æ³•ã€‚</li>
<li>æ¨¡å‹åœ¨æ¨¡æ‹ŸçœŸå®å®éªŒæ•°æ®çš„åˆ†å¸ƒå¤–æµ‹è¯•åœºæ™¯ä¸­è¡¨ç°å‡ºç¨³å¥æ€§ï¼Œå³ä½¿åœ¨é«˜åº¦ä¿®æ”¹çš„å–·æ°”ä¿¡å·ä¸Šä¹Ÿèƒ½ä¿æŒé«˜ä¿çœŸåº¦ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å°†æ— ç›‘ç£çš„éé…å¯¹ç”Ÿæˆæ¨¡å‹åº”ç”¨äºå…¨æ¢æµ‹å™¨å–·æ°”èƒŒæ™¯å‡æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ba5b1237987b0d0e7bc3c884732ed20c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286613&auth_key=1762286613-0-0-25b7c7741f0b78e6280e500241408b0b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d85977832bdea269afd7490e01122cd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286620&auth_key=1762286620-0-0-d6a131aecbc26669191cc7e17bc35e05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-903837594357302b92a95f7f04eed26e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286627&auth_key=1762286627-0-0-1e55ef7406e077c5bade8a2abf9fa711&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ccd490170ae3d096b1e063d5e895631~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286634&auth_key=1762286634-0-0-01555767a3f79575d710d4d6db06b968&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Gut-decisions-based-on-the-liver-A-radiomics-approach-to-boost-colorectal-cancer-screening"><a href="#Gut-decisions-based-on-the-liver-A-radiomics-approach-to-boost-colorectal-cancer-screening" class="headerlink" title="Gut decisions based on the liver: A radiomics approach to boost   colorectal cancer screening"></a>Gut decisions based on the liver: A radiomics approach to boost   colorectal cancer screening</h2><p><strong>Authors:Anna Hinterberger, Jonas Bohn, Dasha Trofimova, Nicolas Knabe, Julia Dettling, Tobias Norajitra, Fabian Isensee, Johannes Betge, Stefan O. SchÃ¶nberg, Dominik NÃ¶renberg, Sergio Grosu, Sonja Loges, Ralf Floca, Jakob Nikolas Kather, Klaus Maier-Hein, Freba Grawe</strong></p>
<p>Non-invasive colorectal cancer (CRC) screening represents a key opportunity to improve colonoscopy participation rates and reduce CRC mortality. This study explores the potential of the gut-liver axis for predicting colorectal neoplasia through liver-derived radiomic features extracted from routine CT images as a novel opportunistic screening approach. In this retrospective study, we analyzed data from 1,997 patients who underwent colonoscopy and abdominal CT. Patients either had no colorectal neoplasia (n&#x3D;1,189) or colorectal neoplasia (n_total&#x3D;808; adenomas n&#x3D;423, CRC n&#x3D;385). Radiomics features were extracted from 3D liver segmentations using the Radiomics Processing ToolKit (RPTK), which performed feature extraction, filtering, and classification. The dataset was split into training (n&#x3D;1,397) and test (n&#x3D;600) cohorts. Five machine learning models were trained with 5-fold cross-validation on the 20 most informative features, and the best model ensemble was selected based on the validation AUROC. The best radiomics-based XGBoost model achieved a test AUROC of 0.810, clearly outperforming the best clinical-only model (test AUROC: 0.457). Subclassification between colorectal cancer and adenoma showed lower accuracy (test AUROC: 0.674). Our findings establish proof-of-concept that liver-derived radiomics from routine abdominal CT can predict colorectal neoplasia. Beyond offering a pragmatic, widely accessible adjunct to CRC screening, this approach highlights the gut-liver axis as a novel biomarker source for opportunistic screening and sparks new mechanistic hypotheses for future translational research. </p>
<blockquote>
<p>éä¾µå…¥æ€§ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰ç­›æŸ¥æ˜¯æé«˜ç»“è‚ é•œæ£€æŸ¥å‚ä¸ç‡å¹¶é™ä½CRCæ­»äº¡ç‡çš„å…³é”®æœºä¼šã€‚æœ¬ç ”ç©¶é€šè¿‡ä»å¸¸è§„CTå›¾åƒä¸­æå–è‚è„è¡ç”Ÿçš„æ”¾å°„å­¦ç‰¹å¾æ¥æ¢ç´¢é€šè¿‡è‚ é“-è‚è„è½´é¢„æµ‹ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©çš„å¯èƒ½æ€§ï¼Œä½œä¸ºä¸€ç§æ–°å‹çš„æœºä¼šæ€§ç­›æŸ¥æ–¹æ³•ã€‚è¿™æ˜¯ä¸€é¡¹å›é¡¾æ€§ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ†æäº†1997åæ¥å—ç»“è‚ é•œæ£€æŸ¥å’Œè…¹éƒ¨CTæ£€æŸ¥çš„æ‚£è€…æ•°æ®ã€‚æ‚£è€…åˆ†ä¸ºæ— ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©ï¼ˆn&#x3D;1,189ï¼‰æˆ–ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©ï¼ˆæ€»è®¡n&#x3D;808ï¼›è…ºç˜¤n&#x3D;423ï¼ŒCRC n&#x3D;385ï¼‰ã€‚ä½¿ç”¨æ”¾å°„å­¦å¤„ç†å·¥å…·ç®±ï¼ˆRPTKï¼‰ä»3Dè‚è„åˆ†å‰²ä¸­æå–æ”¾å°„å­¦ç‰¹å¾ï¼Œè¯¥å·¥å…·ç®±æ‰§è¡Œç‰¹å¾æå–ï¼Œè¿‡æ»¤å’Œåˆ†ç±»ã€‚æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆn&#x3D;1,397ï¼‰å’Œæµ‹è¯•é›†ï¼ˆn&#x3D;600ï¼‰ã€‚ä½¿ç”¨æœ€å…·ä¿¡æ¯é‡çš„å‰20ä¸ªç‰¹å¾è¿›è¡Œäº”æŠ˜äº¤å‰éªŒè¯çš„äº”æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒï¼Œå¹¶æ ¹æ®éªŒè¯AUROCé€‰æ‹©æœ€ä½³æ¨¡å‹ç»„åˆã€‚åŸºäºæœ€ä½³æ”¾å°„å­¦çš„XGBoostæ¨¡å‹å®ç°äº†æµ‹è¯•AUROCä¸º0.810ï¼Œæ˜æ˜¾è¶…è¿‡äº†ä»…ä½¿ç”¨ä¸´åºŠæ¨¡å‹çš„æµ‹è¯•AUROCï¼ˆ0.457ï¼‰ã€‚åœ¨ç»“è‚ ç™Œå’Œè…ºç˜¤ä¹‹é—´è¿›è¡Œåˆ†ç±»çš„å‡†ç¡®åº¦è¾ƒä½ï¼ˆæµ‹è¯•AUROCï¼š0.674ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¯æ˜äº†ä»å¸¸è§„è…¹éƒ¨CTä¸­æå–çš„è‚è„è¡ç”Ÿæ”¾å°„å­¦é¢„æµ‹ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©çš„æ¦‚å¿µå¯è¡Œæ€§ã€‚é™¤äº†æä¾›ä¸€ä¸ªå®ç”¨ä¸”å¹¿æ³›å¯æ¥å…¥çš„CRCç­›æŸ¥è¾…åŠ©æ‰‹æ®µå¤–ï¼Œè¿™ç§æ–¹æ³•è¿˜çªå‡ºäº†è‚ é“-è‚è„è½´ä½œä¸ºæœºä¼šæ€§ç­›æŸ¥çš„æ–°ç”Ÿç‰©æ ‡å¿—ç‰©æ¥æºï¼Œå¹¶æ¿€å‘äº†å¯¹æœªæ¥ç¿»è¯‘ç ”ç©¶çš„æ–°çš„æœºåˆ¶å‡è®¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23687v1">PDF</a> Equal contribution between first, second, fifteenth, and sixteenth   authors</p>
<p><strong>æ‘˜è¦</strong><br>    åˆ©ç”¨å¸¸è§„è…¹éƒ¨CTå›¾åƒä¸­çš„è‚è„è¡ç”Ÿæ”¾å°„å­¦ç‰¹å¾ï¼Œé€šè¿‡éä¾µå…¥æ€§æ–¹æ³•é¢„æµ‹ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰ã€‚ç ”ç©¶é‡‡ç”¨å›é¡¾æ€§åˆ†ææ–¹æ³•ï¼Œå¯¹æ¥å—ç»“è‚ é•œæ£€æŸ¥å’Œè…¹éƒ¨CTæ‰«æçš„1997åæ‚£è€…æ•°æ®è¿›è¡Œåˆ†æã€‚æ ¹æ®æ˜¯å¦æœ‰ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©ï¼ˆæ— æ–°ç”Ÿç‰©æ‚£è€…1189åï¼Œç»“è‚ æ–°ç”Ÿç‰©æ‚£è€…æ€»æ•°808åï¼Œè…ºç˜¤æ‚£è€…423åï¼ŒCRCæ‚£è€…385åï¼‰è¿›è¡Œåˆ†ç±»ã€‚ä½¿ç”¨æ”¾å°„å­¦å¤„ç†å·¥å…·ç®±ï¼ˆRPTKï¼‰ä»ä¸‰ç»´è‚è„åˆ†å‰²ä¸­æå–æ”¾å°„å­¦ç‰¹å¾ï¼Œå¹¶è¿›è¡Œç‰¹å¾æå–ã€è¿‡æ»¤å’Œåˆ†ç±»ã€‚æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ1397äººï¼‰å’Œæµ‹è¯•é›†ï¼ˆ600äººï¼‰ã€‚åœ¨æœ€å…·ä¿¡æ¯æ€§çš„20ä¸ªç‰¹å¾ä¸Šè®­ç»ƒäº†äº”ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶é€šè¿‡éªŒè¯AUROCé€‰æ‹©äº†æœ€ä½³æ¨¡å‹ç»„åˆã€‚æœ€ä½³çš„åŸºäºæ”¾å°„å­¦ç‰¹å¾çš„XGBoostæ¨¡å‹çš„æµ‹è¯•AUROCä¸º0.810ï¼Œæ˜æ˜¾ä¼˜äºä»…åŸºäºä¸´åºŠæ¨¡å‹çš„æµ‹è¯•AUROCï¼ˆ0.457ï¼‰ã€‚ç»“è‚ ç›´è‚ ç™Œä¸è…ºç˜¤ä¹‹é—´çš„åˆ†ç±»ç²¾åº¦è¾ƒä½ï¼ˆæµ‹è¯•AUROCï¼š0.674ï¼‰ã€‚ç ”ç©¶è¯å®ï¼Œåˆ©ç”¨å¸¸è§„è…¹éƒ¨CTçš„è‚è„è¡ç”Ÿæ”¾å°„å­¦ç‰¹å¾é¢„æµ‹ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©çš„æ¦‚å¿µéªŒè¯ã€‚è¿™ä¸ä»…ä¸ºCRCç­›æŸ¥æä¾›äº†ä¸€ä¸ªå®ç”¨çš„è¾…åŠ©æ‰‹æ®µï¼Œè€Œä¸”çªæ˜¾äº†è‚ é“-è‚è„è½´ä½œä¸ºæœºä¼šæ€§ç­›æŸ¥çš„æ–°ç”Ÿç‰©æ ‡è®°æºï¼Œå¹¶ä¸ºæœªæ¥çš„ç¿»è¯‘ç ”ç©¶æä¾›äº†æ–°çš„æœºåˆ¶å‡è®¾ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éä¾µå…¥æ€§ç»“ç›´è‚ ç™Œç­›æŸ¥å¯é€šè¿‡æ”¹å–„ç»“è‚ é•œæ£€æŸ¥å‚ä¸ç‡å’Œé™ä½CRCæ­»äº¡ç‡æ¥å‘æŒ¥ä½œç”¨ã€‚</li>
<li>è‚è„è¡ç”Ÿæ”¾å°„å­¦ç‰¹å¾ä½œä¸ºé¢„æµ‹ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©çš„æ–°æ–¹æ³•ï¼Œå…¶é¢„æµ‹æ€§èƒ½é€šè¿‡æœºå™¨å­¦ä¹ æ–¹æ³•å¾—åˆ°éªŒè¯ã€‚</li>
<li>ä½¿ç”¨å¸¸è§„è…¹éƒ¨CTå›¾åƒçš„æ”¾å°„å­¦ç‰¹å¾æå–åœ¨é¢„æµ‹ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>XGBoostæ¨¡å‹åœ¨é¢„æµ‹ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼ˆæµ‹è¯•AUROCä¸º0.810ï¼‰ï¼Œç›¸è¾ƒäºä»…ä½¿ç”¨ä¸´åºŠæ¨¡å‹çš„æµ‹è¯•AUROCï¼ˆ0.457ï¼‰ï¼Œå…¶è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚</li>
<li>åœ¨åŒºåˆ†ç»“è‚ ç›´è‚ ç™Œå’Œè…ºç˜¤æ–¹é¢ï¼Œå°½ç®¡åˆ†ç±»å‡†ç¡®æ€§ç¨ä½ï¼ˆæµ‹è¯•AUROCï¼š0.674ï¼‰ï¼Œä½†è¿™ä¸€æ–¹æ³•ä»ç„¶æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶è¯æ˜äº†è‚ é“-è‚è„è½´åœ¨é¢„æµ‹ç»“è‚ ç›´è‚ æ–°ç”Ÿç‰©æ–¹é¢çš„ä½œç”¨ï¼Œä¸ºåç»­æœºåˆ¶ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-40901cb68c38ba8bb479541f55f54efd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286641&auth_key=1762286641-0-0-04c1843c8511de7d4a3ae259227a3eec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-Sun-as-an-X-ray-star-V-A-new-method-to-retrieve-coronal-filling-factors"><a href="#The-Sun-as-an-X-ray-star-V-A-new-method-to-retrieve-coronal-filling-factors" class="headerlink" title="The Sun as an X-ray star V.: A new method to retrieve coronal filling   factors"></a>The Sun as an X-ray star V.: A new method to retrieve coronal filling   factors</h2><p><strong>Authors:Wilhelmina Maryann Joseph, Beate Stelzer, Salvatore Orlando, Moritz Klawin</strong></p>
<p>Context. Stellar coronae are unresolved in X-rays, so inferences about their structure rely on spectral analysis. The â€œSun-as-an-X-ray-starâ€ (SaXS) approach uses the Sun as a spatially resolved template to interpret stellar spectra, but previous SaXS implementations were indirect and computationally heavy. Aims. We present a new SaXS implementation that converts solar emission measure distributions (EMDs) of distinct coronal region types into XSPEC spectral components and test whether broad-band X-ray spectra alone can recover their filling factors. Methods. We built XSPEC multi-temperature spectral models for four solar region types (background&#x2F;quiet corona, active regions, cores, and flares) by using EMDs derived from Yohkoh&#x2F;SXT data and translating each EMD bin into an isothermal apec component. These models were fit (using PyXspec) to two one-hour DAXSS spectra representative of quiescent (2022-06-29) and flaring (2022-04-25) states. Best-fit normalizations were converted into projected areas and filling factors and compared with near-coincident Hinode&#x2F;XRT full-disk images. Results. Using the Yohkoh&#x2F;SXT EMDs, the quiescent Sun spectrum is dominated by active region emission (filling factor <del>22%), with the background corona poorly constrained. The flaring Sun spectrum is best described by a combination of active regions, cores, and flares with filling factors of ~47.5%, ~4.1%, and ~0.062%, respectively. The dominant components match spatial features seen in Hinode&#x2F;XRT images. Limitations include the DAXSS low-energy cutoff (</del>0.7 keV) and the small, non-uniform Yohkoh EMD sample. Conclusions. Our SaXS implementation enables direct retrieval of coronal filling factors from broad-band X-ray spectra and provides a physically motivated alternative to ad hoc few-temperature fits, suitable for stellar X-ray analyses. </p>
<blockquote>
<p>èƒŒæ™¯ã€‚æ’æ˜Ÿå†•åœ¨Xå°„çº¿ä¸­æ— æ³•è§£æï¼Œå› æ­¤å¯¹å…¶ç»“æ„çš„æ¨æ–­ä¾èµ–äºå…‰è°±åˆ†æã€‚â€œæ—¥å†•ä¸ºXå°„çº¿æ’æ˜Ÿâ€ï¼ˆSaXSï¼‰æ–¹æ³•å°†å¤ªé˜³ä½œä¸ºç©ºé—´è§£ææ¨¡æ¿æ¥è§£é‡Šæ’æ˜Ÿå…‰è°±ï¼Œä½†ä¹‹å‰çš„SaXSå®ç°æ–¹æ³•æ˜¯é—´æ¥ä¸”è®¡ç®—ç¹é‡çš„ã€‚ç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„SaXSå®ç°æ–¹æ³•ï¼Œå°†å¤ªé˜³å‘å°„é‡åˆ†å¸ƒï¼ˆEMDsï¼‰çš„ä¸åŒå†•åŒºç±»å‹è½¬æ¢ä¸ºXSPECå…‰è°±æˆåˆ†ï¼Œå¹¶æµ‹è¯•äº†ä»…é€šè¿‡å®½å¸¦Xå°„çº¿å…‰è°±æ˜¯å¦èƒ½æ¢å¤å…¶å¡«å……å› å­ã€‚æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºYohkoh&#x2F;SXTæ•°æ®çš„EMDsä¸ºå››ç§å¤ªé˜³åŒºåŸŸç±»å‹ï¼ˆèƒŒæ™¯&#x2F;å®é™å†•åŒºã€æ´»åŠ¨åŒºã€æ ¸å¿ƒå’Œè€€æ–‘ï¼‰æ„å»ºäº†å¤šæ¸©åº¦XSPECå…‰è°±æ¨¡å‹ï¼Œå¹¶å°†æ¯ä¸ªEMDåŒºé—´è½¬æ¢ä¸ºç­‰æ¸©apecæˆåˆ†ã€‚è¿™äº›æ¨¡å‹ä½¿ç”¨PyXspecæ‹Ÿåˆäº†ä¸¤ä¸ªä»£è¡¨é™æ­¢çŠ¶æ€ï¼ˆ2022å¹´6æœˆ29æ—¥ï¼‰å’Œè€€æ–‘çŠ¶æ€ï¼ˆ2022å¹´4æœˆ25æ—¥ï¼‰çš„ä¸€å°æ—¶DAXSSå…‰è°±ã€‚æœ€ä½³æ‹Ÿåˆå½’ä¸€åŒ–å€¼è¢«è½¬æ¢ä¸ºæŠ•å½±é¢ç§¯å’Œå¡«å……å› å­ï¼Œå¹¶ä¸è¿‘ä¼¼çš„Hinode&#x2F;XRTå…¨æ—¥é¢å›¾åƒè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœã€‚ä½¿ç”¨Yohkoh&#x2F;SXTçš„EMDsï¼Œé™æ­¢å¤ªé˜³å…‰è°±ä¸»è¦ç”±æ´»åŠ¨åŒºå‘å°„ï¼ˆå¡«å……å› å­çº¦ä¸º22%ï¼‰ä¸»å¯¼ï¼ŒèƒŒæ™¯å†•åŒºçš„çº¦æŸè¾ƒå·®ã€‚è€€æ–‘å¤ªé˜³å…‰è°±æœ€å¥½ç”±æ´»åŠ¨åŒºã€æ ¸å¿ƒå’Œè€€æ–‘çš„ç»„åˆæ¥æè¿°ï¼Œå¡«å……å› å­åˆ†åˆ«ä¸º<del>47.5%ï¼Œ</del>4.1%ï¼Œå’Œ<del>0.062%ã€‚ä¸»è¦æˆåˆ†ä¸Hinode&#x2F;XRTå›¾åƒä¸­çš„ç©ºé—´ç‰¹å¾ç›¸åŒ¹é…ã€‚é™åˆ¶å› ç´ åŒ…æ‹¬DAXSSçš„ä½èƒ½æˆªæ­¢ï¼ˆ</del>0.7 keVï¼‰å’ŒYohkoh EMDæ ·æœ¬çš„å°è§„æ¨¡å’Œéå‡åŒ€æ€§ã€‚ç»“è®ºã€‚æˆ‘ä»¬çš„SaXSå®ç°æ–¹æ³•èƒ½å¤Ÿç›´æ¥ä»å®½å¸¦Xå°„çº¿å…‰è°±ä¸­æ£€ç´¢å‡ºå† å†•å¡«å……å› å­ï¼Œå¹¶æä¾›äº†ä¸€ç§æœ‰ç‰©ç†ä¾æ®çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚ç”¨äºæ’æ˜ŸXå°„çº¿åˆ†æä¸­çš„ä¸´æ—¶å‡ æ¸©åº¦æ‹Ÿåˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23161v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå¤ªé˜³Xå°„çº¿æ•°æ®çš„æ’æ˜Ÿå† å†•ç ”ç©¶çš„æ–¹æ³•ï¼ˆSaXSï¼‰ã€‚é€šè¿‡å°†ä¸åŒå† å†•åŒºåŸŸç±»å‹çš„å¤ªé˜³å‘å°„åº¦é‡åˆ†å¸ƒè½¬åŒ–ä¸ºXSPECå…‰è°±æˆåˆ†ï¼Œå¯¹å¤ªé˜³é™æ­¢å’Œæ´»åŠ¨çŠ¶æ€ä¸‹çš„Xå°„çº¿å…‰è°±è¿›è¡Œäº†æ¨¡æ‹Ÿåˆ†æã€‚æœ€ä½³æ‹Ÿåˆçš„æ­£å¸¸åŒ–æ•°æ®è¢«è½¬æ¢ä¸ºæŠ•å½±é¢ç§¯å’Œå¡«å……å› å­ï¼Œå¹¶ä¸Hinode &#x2F; XRTå…¨æ—¥é¢å›¾åƒè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œé™æ­¢å¤ªé˜³å…‰è°±ä¸»è¦ç”±æ´»åŠ¨åŒºå‘å°„ä¸»å¯¼ï¼Œè€ŒèƒŒæ™¯å† å†•éš¾ä»¥çº¦æŸï¼›æ´»åŠ¨å¤ªé˜³å…‰è°±åˆ™æ˜¯ç”±æ´»åŠ¨åŒºã€æ ¸å¿ƒå’Œè€€æ–‘å…±åŒä¸»å¯¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä»å®½å¸¦Xå°„çº¿å…‰è°±ç›´æ¥è·å–å† å†•å¡«å……å› å­çš„å¯èƒ½ï¼Œä¸ºæ’æ˜ŸXå°„çº¿åˆ†ææä¾›äº†ç‰©ç†é©±åŠ¨çš„æ›¿ä»£æ–¹æ³•ã€‚ç”±äºåŸºäºå¤ªé˜³ä½œä¸ºå‚è€ƒæ¨¡æ¿çš„åŸç†ï¼Œè¿™ç§æ–°æ–¹æ³•ä¸ºä¹‹å‰éš¾ä»¥è§£æçš„æ’æ˜Ÿå† å†•ç»“æ„æä¾›äº†æœ‰æ•ˆå·¥å…·ã€‚ </p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>æ–°çš„SaXSæ–¹æ³•é€šè¿‡å°†å¤ªé˜³ä½œä¸ºç©ºé—´è§£ææ¨¡æ¿ï¼Œè½¬åŒ–äº†ä¸åŒç±»å‹å† å†•åŒºåŸŸçš„å¤ªé˜³å‘å°„åº¦é‡åˆ†å¸ƒåˆ°XSPECå…‰è°±æˆåˆ†ã€‚</li>
<li>å¯¹é™æ­¢å’Œæ´»åŠ¨çŠ¶æ€ä¸‹çš„å¤ªé˜³Xå°„çº¿å…‰è°±è¿›è¡Œäº†æ¨¡æ‹Ÿåˆ†æï¼Œæå–äº†å† å†•å¡«å……å› å­ã€‚é™æ­¢å¤ªé˜³å…‰è°±ä¸»è¦ç”±æ´»åŠ¨åŒºä¸»å¯¼ï¼Œè€ŒèƒŒæ™¯å† å†•éš¾ä»¥çº¦æŸï¼›æ´»åŠ¨å¤ªé˜³å…‰è°±åˆ™æ¶‰åŠå¤šä¸ªåŒºåŸŸç±»å‹ã€‚</li>
<li>æ–°æ–¹æ³•ä½¿å¾—ç›´æ¥ä»å®½å¸¦Xå°„çº¿å…‰è°±è·å–å† å†•å¡«å……å› å­æˆä¸ºå¯èƒ½ï¼Œä¸ºæ’æ˜ŸXå°„çº¿åˆ†ææä¾›äº†ç‰©ç†é©±åŠ¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¿™ä¸€æ–¹æ³•å¯¹äºè§£ææ’æ˜Ÿå† å†•ç»“æ„å…·æœ‰é‡è¦çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4b1d90b5cd2bb1bbdd071744a678b539~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286648&auth_key=1762286648-0-0-30f8838e10a546348c6fc31b77d559fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee49a9f3aaaf45aee1a463391c0b46c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286656&auth_key=1762286656-0-0-0f8633840732a1287a3ceb4a6b4a22dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6ed15799dffa523af0c914e6848fdc0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286663&auth_key=1762286663-0-0-30b82826a7a14172d47e95d9d4305f7d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-538cefaa9e8c0e58b5aed484f09355d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286670&auth_key=1762286670-0-0-d9d0cce3ff7a936249b1f06ad23ec58a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Alias-Free-ViT-Fractional-Shift-Invariance-via-Linear-Attention"><a href="#Alias-Free-ViT-Fractional-Shift-Invariance-via-Linear-Attention" class="headerlink" title="Alias-Free ViT: Fractional Shift Invariance via Linear Attention"></a>Alias-Free ViT: Fractional Shift Invariance via Linear Attention</h2><p><strong>Authors:Hagay Michaeli, Daniel Soudry</strong></p>
<p>Transformers have emerged as a competitive alternative to convnets in vision tasks, yet they lack the architectural inductive bias of convnets, which may hinder their potential performance. Specifically, Vision Transformers (ViTs) are not translation-invariant and are more sensitive to minor image translations than standard convnets. Previous studies have shown, however, that convnets are also not perfectly shift-invariant, due to aliasing in downsampling and nonlinear layers. Consequently, anti-aliasing approaches have been proposed to certify convnetsâ€™ translation robustness. Building on this line of work, we propose an Alias-Free ViT, which combines two main components. First, it uses alias-free downsampling and nonlinearities. Second, it uses linear cross-covariance attention that is shift-equivariant to both integer and fractional translations, enabling a shift-invariant global representation. Our model maintains competitive performance in image classification and outperforms similar-sized models in terms of robustness to adversarial translations. </p>
<blockquote>
<p>Transformeråœ¨è§†è§‰ä»»åŠ¡ä¸­å·²æˆä¸ºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆconvnetsï¼‰çš„ç«äº‰æ›¿ä»£å“ï¼Œä½†å®ƒä»¬ç¼ºå°‘å·ç§¯ç¥ç»ç½‘ç»œçš„æ¶æ„å½’çº³åè§ï¼Œè¿™å¯èƒ½é˜»ç¢å…¶æ½œåœ¨æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè§†è§‰Transformerï¼ˆViTsï¼‰ä¸å…·æœ‰å¹³ç§»ä¸å˜æ€§ï¼Œå¯¹äºè½»å¾®çš„å›¾åƒå¹³ç§»æ¯”æ ‡å‡†å·ç§¯ç¥ç»ç½‘ç»œæ›´åŠ æ•æ„Ÿã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç”±äºä¸‹é‡‡æ ·å’Œéçº¿æ€§å±‚ä¸­çš„æ··å ï¼Œå·ç§¯ç¥ç»ç½‘ç»œä¹Ÿå¹¶éå®Œå…¨å¹³ç§»ä¸å˜ã€‚å› æ­¤ï¼Œäººä»¬æå‡ºäº†æŠ—æ··å æ–¹æ³•æ¥éªŒè¯å·ç§¯ç¥ç»ç½‘ç»œçš„å¹³ç§»é²æ£’æ€§ã€‚åŸºäºè¿™é¡¹å·¥ä½œï¼Œæˆ‘ä»¬æå‡ºäº†æ— æ··å ViTï¼Œå®ƒç»“åˆäº†ä¸¤ä¸ªä¸»è¦ç»„ä»¶ã€‚é¦–å…ˆï¼Œå®ƒä½¿ç”¨æ— æ··å çš„ä¸‹é‡‡æ ·å’Œéçº¿æ€§ã€‚å…¶æ¬¡ï¼Œå®ƒä½¿ç”¨å¯¹æ•´æ•°å’Œå¹³ç§»åˆ†æ•°éƒ½å…·æœ‰å¹³ç§»ç­‰å˜æ€§çš„çº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›ï¼Œä»è€Œå®ç°å¹³ç§»ä¸å˜çš„å…¨å±€è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­ä¿æŒäº†ç«äº‰åŠ›ï¼Œå¹¶ä¸”åœ¨å¯¹æŠ—å¹³ç§»æ–¹é¢è¡¨ç°ä¼˜äºç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22673v1">PDF</a> Accepted at NeurIPS 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/hmichaeli/alias_free_vit">https://github.com/hmichaeli/alias_free_vit</a></p>
<p><strong>Summary</strong></p>
<pre><code> è§†è§‰Transformerï¼ˆViTsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­å·²æˆä¸ºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆconvnetsï¼‰çš„æœ‰åŠ›ç«äº‰å¯¹æ‰‹ï¼Œä½†ç¼ºä¹å·ç§¯ç¥ç»ç½‘ç»œçš„æ¶æ„å½’çº³åè§å¯èƒ½ä¼šé˜»ç¢å…¶æ½œåœ¨æ€§èƒ½ã€‚å…·ä½“åœ°è¯´ï¼ŒViTså¯¹ç»†å¾®çš„å›¾åƒå¹³ç§»æ•æ„Ÿï¼Œå¹¶éå¹³ç§»ä¸å˜ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç”±äºä¸‹é‡‡æ ·å’Œéçº¿æ€§å±‚ä¸­çš„æ··å ï¼Œå·ç§¯ç¥ç»ç½‘ç»œä¹Ÿå¹¶éå®Œå…¨å¹³ç§»ä¸å˜ã€‚å› æ­¤ï¼Œæå‡ºäº†æŠ—æ··å æ–¹æ³•æ¥éªŒè¯å…¶å¹³ç§»é²æ£’æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†æ— æ··å ViTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†ä¸¤ç§ä¸»è¦æˆåˆ†ï¼šä¸€æ˜¯ä½¿ç”¨æ— æ··å çš„ä¸‹é‡‡æ ·å’Œéçº¿æ€§ï¼›äºŒæ˜¯ä½¿ç”¨å¯¹æ•´æ•°å’Œå¹³ç§»åˆ†æ•°å‡ä¿æŒå¹³ç§»ç­‰å˜çš„çº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œå®ç°å…¨å±€å¹³ç§»ä¸å˜è¡¨ç¤ºã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­ä¿æŒç«äº‰åŠ›ï¼Œå¹¶åœ¨å¯¹æŠ—å¹³ç§»æ–¹é¢å…·æœ‰ä¼˜äºåŒç±»è§„æ¨¡æ¨¡å‹çš„é²æ£’æ€§ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰Transformerï¼ˆViTsï¼‰å·²æˆä¸ºè§†è§‰ä»»åŠ¡ä¸­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆconvnetsï¼‰çš„æœ‰åŠ›ç«äº‰å¯¹æ‰‹ã€‚</li>
<li>ViTsç¼ºä¹å·ç§¯ç¥ç»ç½‘ç»œçš„æ¶æ„å½’çº³åè§ï¼Œå¯èƒ½ä¼šå½±å“å…¶æ€§èƒ½ã€‚</li>
<li>ViTså¹¶éå®Œå…¨å¹³ç§»ä¸å˜ï¼Œå¯¹ç»†å¾®çš„å›¾åƒå¹³ç§»æ•æ„Ÿã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œä¹Ÿå¹¶éå®Œå…¨å¹³ç§»ä¸å˜ï¼Œå­˜åœ¨æ··å æ•ˆåº”ã€‚</li>
<li>æŠ—æ··å æ–¹æ³•è¢«ç”¨æ¥å¢å¼ºå·ç§¯ç¥ç»ç½‘ç»œçš„å¹³ç§»é²æ£’æ€§ã€‚</li>
<li>æå‡ºäº†æ— æ··å ViTæ¨¡å‹ï¼Œç»“åˆäº†æ— æ··å ä¸‹é‡‡æ ·ã€éçº¿æ€§ä»¥åŠçº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨å¯¹æŠ—å¹³ç§»æ–¹é¢å…·æœ‰é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3ba4bb19dad6c8c8f7a5ff8c615ba89f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286678&auth_key=1762286678-0-0-569bfeb9a260e716cf9335b2b541e3f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3101ad4cd013f62cfc7f1cc3b171313a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286686&auth_key=1762286686-0-0-f451dadfd5409ff7b4cb9150f3af4063&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge"><a href="#Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge" class="headerlink" title="Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge"></a>Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge</h2><p><strong>Authors:Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</strong></p>
<p>Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home">https://sites.google.com/view/lddbm/home</a>. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆå»ºæ¨¡çš„è¿›å±•ä½¿å¾—æ‰©æ•£æ¨¡å‹æˆä¸ºä»å¤æ‚æ•°æ®åˆ†å¸ƒä¸­é‡‡æ ·çš„æœ€å…ˆè¿›çš„å·¥å…·ã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬çš„èƒ½åŠ›æ‹“å±•åˆ°è·¨ä¸åŒæ„Ÿå®˜æ¨¡æ€çš„ä¿¡æ¯ç¿»è¯‘ï¼ˆæ¨¡æ€ç¿»è¯‘ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé™åˆ¶æ€§å‡è®¾ï¼ŒåŒ…æ‹¬å…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œç‰¹å®šæ¨¡æ€æ¶æ„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„é€šç”¨æ€§å’Œç†è®ºæ ¹æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLatent Denoising Diffusion Bridge Modelï¼ŒLDDBMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹çš„æ½œåœ¨å˜é‡æ‰©å±•çš„é€šç”¨æ¨¡æ€ç¿»è¯‘æ¡†æ¶ã€‚é€šè¿‡åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— éœ€å¯¹é½ç»´åº¦çš„æƒ…å†µä¸‹å­¦ä¼šäº†åœ¨ä»»æ„æ¨¡æ€ä¹‹é—´æ­å»ºæ¡¥æ¢ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯¹æ¯”å¯¹é½æŸå¤±æ¥å¼ºåˆ¶é…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é’ˆå¯¹æ½œåœ¨ç©ºé—´ä¸­çš„å™ªå£°é¢„æµ‹çš„åŸŸæ— å…³ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†é¢„æµ‹æŸå¤±æ¥æŒ‡å¯¼è®­ç»ƒä»¥å®ç°å‡†ç¡®çš„è·¨åŸŸç¿»è¯‘ï¼Œå¹¶æ¢ç´¢äº†å¤šç§è®­ç»ƒç­–ç•¥æ¥æé«˜ç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒä»»æ„æ¨¡æ€å¯¹ï¼Œå¹¶åœ¨å¤šç§æ¨¡æ€ç¿»è¯‘ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼ŒåŒ…æ‹¬å¤šè§†å›¾åˆ°3Då½¢çŠ¶ç”Ÿæˆã€å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå¤šè§†å›¾åœºæ™¯åˆæˆã€‚å…¨é¢çš„å®éªŒå’Œæ¶ˆèå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¸€èˆ¬æ¨¡æ€ç¿»è¯‘å»ºç«‹äº†æ–°çš„å¼ºå¤§åŸºå‡†ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home%E3%80%82">https://sites.google.com/view/lddbm/homeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20819v2">PDF</a> Accepted as a poster at NeurIPS 2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹æœ€è¿‘åœ¨ç”Ÿæˆå»ºæ¨¡é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œå·²æˆä¸ºå¤æ‚æ•°æ®åˆ†å¸ƒé‡‡æ ·çš„é¡¶å°–å·¥å…·ã€‚å°½ç®¡å®ƒä»¬åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å°†å®ƒä»¬çš„èƒ½åŠ›æ‰©å±•åˆ°è·¨ä¸åŒæ„Ÿå®˜æ¨¡æ€çš„ç¿»è¯‘ï¼ˆæ¨¡æ€è½¬æ¢ï¼ŒMTï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLDDBMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹çš„æ½œåœ¨å˜é‡æ‰©å±•çš„é€šç”¨æ¨¡æ€è½¬æ¢æ¡†æ¶ã€‚é€šè¿‡åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä»»æ„æ¨¡æ€ä¹‹é—´å»ºç«‹æ¡¥æ¢ï¼Œæ— éœ€å¯¹é½ç»´åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡é¢†åŸŸæ˜¯æœ€æ–°é¡¶å°–å·¥å…·ï¼Œå°¤å…¶æ“…é•¿å¤æ‚æ•°æ®åˆ†å¸ƒé‡‡æ ·ã€‚</li>
<li>æ¨¡æ€è½¬æ¢ï¼ˆMTï¼‰æ˜¯å°†ä¿¡æ¯ä»ä¸€ç§æ„Ÿå®˜æ¨¡æ€è½¬æ¢ä¸ºå¦ä¸€ç§æ¨¡æ€ï¼Œç°æœ‰æ–¹æ³•åœ¨è¿™æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLDDBMï¼‰æ˜¯ä¸€ç§æ–°çš„é€šç”¨æ¨¡æ€è½¬æ¢æ¡†æ¶ï¼Œå¯åœ¨ä»»æ„æ¨¡æ€ä¹‹é—´å»ºç«‹æ¡¥æ¢ï¼Œæ— éœ€å¯¹é½ç»´åº¦ã€‚</li>
<li>LDDBMé€šè¿‡å¼•å…¥å¯¹æ¯”å¯¹é½æŸå¤±æ¥å¼ºåˆ¶é…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªé’ˆå¯¹æ½œåœ¨ç©ºé—´å™ªå£°é¢„æµ‹çš„åŸŸä¸è¯†åˆ«çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚</li>
<li>LDDBMæ”¯æŒä»»æ„æ¨¡æ€å¯¹ï¼Œå¹¶åœ¨å¤šç§MTä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼ŒåŒ…æ‹¬å¤šè§†å›¾åˆ°3Då½¢çŠ¶ç”Ÿæˆã€å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå¤šè§†å›¾åœºæ™¯åˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-225a662404a8c32f6e7e723321d5bdb4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286693&auth_key=1762286693-0-0-480c96c903aca0f3dfcfe11c18f308bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d66e2842144f846f899b138e2dd2d51~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286700&auth_key=1762286700-0-0-c652dbdb742a5e62c6e2de406ae972ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-227bee8414c0a17e145209c970af48a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286707&auth_key=1762286707-0-0-de65a6c56f329f15b611a4affd784546&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling"><a href="#Visual-Autoregressive-Models-Beat-Diffusion-Models-on-Inference-Time-Scaling" class="headerlink" title="Visual Autoregressive Models Beat Diffusion Models on Inference Time   Scaling"></a>Visual Autoregressive Models Beat Diffusion Models on Inference Time   Scaling</h2><p><strong>Authors:Erik Riise, Mehmet Onurcan Kaya, Dim P. Papadopoulos</strong></p>
<p>While inference-time scaling through search has revolutionized Large Language Models, translating these gains to image generation has proven difficult. Recent attempts to apply search strategies to continuous diffusion models show limited benefits, with simple random sampling often performing best. We demonstrate that the discrete, sequential nature of visual autoregressive models enables effective search for image generation. We show that beam search substantially improves text-to-image generation, enabling a 2B parameter autoregressive model to outperform a 12B parameter diffusion model across benchmarks. Systematic ablations show that this advantage comes from the discrete token space, which allows early pruning and computational reuse, and our verifier analysis highlights trade-offs between speed and reasoning capability. These findings suggest that model architecture, not just scale, is critical for inference-time optimization in visual generation. </p>
<blockquote>
<p>è™½ç„¶é€šè¿‡æœç´¢è¿›è¡Œæ¨ç†æ—¶é—´ç¼©æ”¾å·²ç»ä½¿å¤§å‹è¯­è¨€æ¨¡å‹å‘ç”Ÿäº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†å°†è¿™äº›ä¼˜åŠ¿è½¬åŒ–ä¸ºå›¾åƒç”Ÿæˆå´è¯æ˜æ˜¯å›°éš¾çš„ã€‚æœ€è¿‘å°†æœç´¢ç­–ç•¥åº”ç”¨äºè¿ç»­æ‰©æ•£æ¨¡å‹çš„å°è¯•æ˜¾ç¤ºå‡ºæœ‰é™çš„æ•ˆç›Šï¼Œè€Œç®€å•çš„éšæœºæŠ½æ ·é€šå¸¸è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè§†è§‰è‡ªå›å½’æ¨¡å‹çš„ç¦»æ•£åºåˆ—ç‰¹æ€§èƒ½å¤Ÿæœ‰æ•ˆåœ°ç”¨äºå›¾åƒç”Ÿæˆæœç´¢ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå…‰æŸæœç´¢å¯ä»¥å¤§å¤§æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œä½¿2Bå‚æ•°è‡ªå›å½’æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­ä¼˜äº12Bå‚æ•°æ‰©æ•£æ¨¡å‹ã€‚ç³»ç»Ÿåˆ†æè¡¨æ˜ï¼Œè¿™ä¸€ä¼˜åŠ¿æ¥è‡ªäºç¦»æ•£æ ‡è®°ç©ºé—´ï¼Œå®ƒå…è®¸æ—©æœŸä¿®å‰ªå’Œè®¡ç®—é‡ç”¨ï¼Œæˆ‘ä»¬çš„éªŒè¯å™¨åˆ†æå¼ºè°ƒäº†é€Ÿåº¦å’Œæ¨ç†èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ¨¡å‹æ¶æ„å¯¹äºè§†è§‰ç”Ÿæˆçš„æ¨ç†æ—¶é—´ä¼˜åŒ–è‡³å…³é‡è¦ï¼Œè€Œä¸ä»…ä»…æ˜¯è§„æ¨¡é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16751v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œæ¨ç†æ—¶é—´ç¼©æ”¾æœç´¢æŠ€æœ¯å–å¾—äº†é©å‘½æ€§è¿›å±•ï¼Œä½†åœ¨å›¾åƒç”Ÿæˆä¸­åº”ç”¨è¿™äº›æŠ€æœ¯å´é¢ä¸´æŒ‘æˆ˜ã€‚è¿‘æœŸå°è¯•å°†æœç´¢ç­–ç•¥åº”ç”¨äºè¿ç»­æ‰©æ•£æ¨¡å‹çš„æ•ˆæœæœ‰é™ï¼Œç®€å•éšæœºé‡‡æ ·é€šå¸¸è¡¨ç°æœ€ä½³ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†ç¦»æ•£åºåˆ—è§†è§‰è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­çš„æœ‰æ•ˆæœç´¢èƒ½åŠ›ã€‚é€šè¿‡åº”ç”¨å…‰æŸæœç´¢ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆè´¨é‡ï¼Œä½¿å¾—ä¸€ä¸ªæ‹¥æœ‰2äº¿å‚æ•°çš„è‡ªå›å½’æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ‹¥æœ‰12äº¿å‚æ•°çš„æ‰©æ•£æ¨¡å‹ã€‚ç³»ç»Ÿæ€§æ¶ˆèå®éªŒè¡¨æ˜ï¼Œè¿™ä¸€ä¼˜åŠ¿æ¥æºäºç¦»æ•£ç¬¦å·ç©ºé—´ï¼Œå…è®¸æ—©æœŸä¿®å‰ªå’Œè®¡ç®—å¤ç”¨ã€‚æˆ‘ä»¬çš„éªŒè¯åˆ†æå¼ºè°ƒäº†é€Ÿåº¦ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ¨¡å‹æ¶æ„è€Œéè§„æ¨¡å¯¹äºè§†è§‰ç”Ÿæˆçš„æ¨ç†æ—¶é—´ä¼˜åŒ–è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨ç†æ—¶é—´ç¼©æ”¾æœç´¢åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç¦»æ•£åºåˆ—è§†è§‰è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å±•ç°å‡ºæœ‰æ•ˆçš„æœç´¢èƒ½åŠ›ã€‚</li>
<li>å…‰æŸæœç´¢æ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>2äº¿å‚æ•°çš„è‡ªå›å½’æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­ä¼˜äº12äº¿å‚æ•°çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æ¶ˆèå®éªŒè¡¨æ˜ä¼˜åŠ¿æ¥æºäºç¦»æ•£ç¬¦å·ç©ºé—´ï¼Œå…è®¸æ—©æœŸä¿®å‰ªå’Œè®¡ç®—å¤ç”¨ã€‚</li>
<li>éªŒè¯åˆ†ææ­ç¤ºäº†é€Ÿåº¦å’Œæ¨ç†èƒ½åŠ›ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>æ¨¡å‹æ¶æ„å¯¹äºæ¨ç†æ—¶é—´ä¼˜åŒ–è‡³å…³é‡è¦ï¼Œè€Œä¸ä»…ä»…æ˜¯è§„æ¨¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a04d792d79dd6ee60c582acdeff44f0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286714&auth_key=1762286714-0-0-19b3ec4707370b006b0fd2cac20cb147&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a3e331aac125eceba2fd955c6d5bdbcf~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286722&auth_key=1762286722-0-0-79e1475a067d268d601f44a893e0f8d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c35ea5cc7781f0886a8bb383c56b075~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286728&auth_key=1762286728-0-0-91a7d8be2e5ea431e70a0f49543d2459&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-23221930c5f2dccf4db9b0ee83ceea3b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286734&auth_key=1762286734-0-0-a74cc36bc76fa5d57ab0e0c523fe55be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54a8ddf588dbc28437b9eb4d81feedf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286741&auth_key=1762286741-0-0-722dbf82fcb35a11afc395e03714b747&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6fdbb4e366dc6e0bce49717389c19c7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286748&auth_key=1762286748-0-0-e5c85e54d13bf5fd6146c6175b06931a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lightweight-Facial-Landmark-Detection-in-Thermal-Images-via-Multi-Level-Cross-Modal-Knowledge-Transfer"><a href="#Lightweight-Facial-Landmark-Detection-in-Thermal-Images-via-Multi-Level-Cross-Modal-Knowledge-Transfer" class="headerlink" title="Lightweight Facial Landmark Detection in Thermal Images via Multi-Level   Cross-Modal Knowledge Transfer"></a>Lightweight Facial Landmark Detection in Thermal Images via Multi-Level   Cross-Modal Knowledge Transfer</h2><p><strong>Authors:Qiyi Tong, Olivia Nocentini, Marta Lagomarsino, Kuanqi Cai, Marta Lorenzini, Arash Ajoudani</strong></p>
<p>Facial Landmark Detection (FLD) in thermal imagery is critical for applications in challenging lighting conditions, but it is hampered by the lack of rich visual cues. Conventional cross-modal solutions, like feature fusion or image translation from RGB data, are often computationally expensive or introduce structural artifacts, limiting their practical deployment. To address this, we propose Multi-Level Cross-Modal Knowledge Distillation (MLCM-KD), a novel framework that decouples high-fidelity RGB-to-thermal knowledge transfer from model compression to create both accurate and efficient thermal FLD models. A central challenge during knowledge transfer is the profound modality gap between RGB and thermal data, where traditional unidirectional distillation fails to enforce semantic consistency across disparate feature spaces. To overcome this, we introduce Dual-Injected Knowledge Distillation (DIKD), a bidirectional mechanism designed specifically for this task. DIKD establishes a connection between modalities: it not only guides the thermal student with rich RGB features but also validates the studentâ€™s learned representations by feeding them back into the frozen teacherâ€™s prediction head. This closed-loop supervision forces the student to learn modality-invariant features that are semantically aligned with the teacher, ensuring a robust and profound knowledge transfer. Experiments show that our approach sets a new state-of-the-art on public thermal FLD benchmarks, notably outperforming previous methods while drastically reducing computational overhead. </p>
<blockquote>
<p>é¢éƒ¨åœ°æ ‡æ£€æµ‹ï¼ˆFLDï¼‰åœ¨çƒ­æˆåƒä¸­çš„åº”ç”¨å¯¹äºåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç…§æ˜æ¡ä»¶ä¸‹çš„åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†å…¶å—åˆ°äº†ç¼ºä¹ä¸°å¯Œçš„è§†è§‰çº¿ç´¢çš„é˜»ç¢ã€‚ä¼ ç»Ÿçš„è·¨æ¨¡æ€è§£å†³æ–¹æ¡ˆï¼Œå¦‚ç‰¹å¾èåˆæˆ–RGBæ•°æ®çš„å›¾åƒè½¬æ¢ï¼Œé€šå¸¸è®¡ç®—æˆæœ¬é«˜æ˜‚æˆ–å¼•å…¥ç»“æ„ä¼ªå½±ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨éƒ¨ç½²ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šçº§è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼ˆMLCM-KDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†é«˜ä¿çœŸRGBåˆ°çƒ­æˆåƒçš„çŸ¥è¯†è½¬ç§»ä¸æ¨¡å‹å‹ç¼©è§£è€¦ï¼Œä»¥åˆ›å»ºæ—¢å‡†ç¡®åˆé«˜æ•ˆçš„çƒ­æˆåƒFLDæ¨¡å‹ã€‚çŸ¥è¯†è½¬ç§»æœŸé—´çš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜åœ¨äºRGBå’Œçƒ­æˆåƒæ•°æ®ä¹‹é—´çš„å›ºæœ‰æ¨¡æ€å·®è·ï¼Œä¼ ç»Ÿå•å‘è’¸é¦æ— æ³•åœ¨ä¸åŒç‰¹å¾ç©ºé—´ä¹‹é—´å®æ–½è¯­ä¹‰ä¸€è‡´æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒå‘æ³¨å…¥çŸ¥è¯†è’¸é¦ï¼ˆDIKDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹æ­¤ä»»åŠ¡è®¾è®¡çš„åŒå‘æœºåˆ¶ã€‚DIKDåœ¨æ¨¡æ€ä¹‹é—´å»ºç«‹äº†è”ç³»ï¼šå®ƒä¸ä»…ç”¨ä¸°å¯Œçš„RGBç‰¹å¾å¼•å¯¼çƒ­æˆåƒå­¦ç”Ÿï¼Œè€Œä¸”è¿˜é€šè¿‡å°†å­¦ç”Ÿçš„å·²å­¦è¡¨ç¤ºåé¦ˆåˆ°å†»ç»“çš„æ•™å¸ˆçš„é¢„æµ‹å¤´ä¸­æ¥éªŒè¯å­¦ç”Ÿçš„è¡¨ç¤ºã€‚è¿™ç§é—­ç¯ç›‘ç£ä¿ƒä½¿å­¦ç”Ÿå­¦ä¹ æ¨¡æ€ä¸å˜çš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾åœ¨è¯­ä¹‰ä¸Šä¸æ•™å¸ˆå¯¹é½ï¼Œç¡®ä¿ç¨³å¥ä¸”æ·±å…¥çš„çŸ¥è¯†è½¬ç§»ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¬å…±çƒ­æˆåƒFLDåŸºå‡†æµ‹è¯•ä¸­æ ‘ç«‹äº†æ–°çš„æ ‡æ†ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11128v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹çƒ­æˆåƒä¸­çš„äººè„¸å…³é”®ç‚¹æ£€æµ‹ï¼ˆFLDï¼‰ï¼Œåœ¨å¤æ‚å…‰ç…§æ¡ä»¶ä¸‹å…·æœ‰é‡è¦åº”ç”¨ä»·å€¼ï¼Œä½†ç¼ºä¹ä¸°å¯Œçš„è§†è§‰çº¿ç´¢æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹çš„å¤šå±‚æ¬¡è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼ˆMLCM-KDï¼‰æ¡†æ¶ï¼Œå®ç°äº†é«˜æ•ˆä¸”å‡†ç¡®åœ°åœ¨çƒ­æˆåƒä¸­å®æ–½äººè„¸å…³é”®ç‚¹æ£€æµ‹ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†åŒå‘çŸ¥è¯†è’¸é¦æœºåˆ¶â€”â€”åŒå‘æ³¨å…¥çŸ¥è¯†è’¸é¦ï¼ˆDIKDï¼‰ï¼Œç¼©å°äº†RGBä¸çƒ­æˆåƒæ•°æ®ä¹‹é—´çš„æ¨¡æ€å·®è·ï¼Œå®ç°äº†è·¨ç‰¹å¾ç©ºé—´çš„çŸ¥è¯†è½¬ç§»ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¬å¼€çš„çƒ­æˆåƒFLDåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨å…³é”®ç‚¹æ£€æµ‹ï¼ˆFLDï¼‰åœ¨çƒ­æˆåƒé¢†åŸŸå› å¤æ‚å…‰ç…§æ¡ä»¶è€Œå˜å¾—å…³é”®ï¼Œä½†ç”±äºç¼ºä¹ä¸°å¯Œçš„è§†è§‰çº¿ç´¢è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿçš„è·¨æ¨¡æ€è§£å†³æ–¹æ¡ˆï¼ˆå¦‚ç‰¹å¾èåˆæˆ–RGBæ•°æ®çš„å›¾åƒç¿»è¯‘ï¼‰è®¡ç®—æˆæœ¬é«˜æ˜‚å¹¶å¯èƒ½å¼•å…¥ç»“æ„ä¼ªå½±ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„å¤šå±‚æ¬¡è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼ˆMLCM-KDï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨åˆ›å»ºé«˜æ•ˆä¸”å‡†ç¡®çš„çƒ­æˆåƒFLDæ¨¡å‹ã€‚</li>
<li>åŒå‘çŸ¥è¯†è’¸é¦æœºåˆ¶â€”â€”åŒå‘æ³¨å…¥çŸ¥è¯†è’¸é¦ï¼ˆDIKDï¼‰è¢«å¼•å…¥ä»¥ç¼©å°RGBä¸çƒ­æˆåƒæ•°æ®ä¹‹é—´çš„æ¨¡æ€å·®è·ã€‚</li>
<li>DIKDæœºåˆ¶ä¸ä»…åˆ©ç”¨RGBç‰¹å¾å¼•å¯¼çƒ­æˆåƒå­¦ç”Ÿæ¨¡å‹ï¼Œè¿˜é€šè¿‡åé¦ˆå­¦ç”Ÿæ¨¡å‹çš„é¢„æµ‹ç»“æœæ¥éªŒè¯å…¶å­¦ä¹ åˆ°çš„è¡¨ç¤ºï¼Œå®ç°é—­ç¯ç›‘ç£ã€‚</li>
<li>é—­ç¯ç›‘ç£ä¿ƒä½¿å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ¨¡æ€ä¸å˜ä¸”è¯­ä¹‰å¯¹é½çš„ç‰¹å¾ï¼Œç¡®ä¿ç¨³å¥ä¸”æ·±å…¥çš„çŸ¥è¯†è½¬ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6ed0352f8a981ae54b981fdb1fbcfdd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286756&auth_key=1762286756-0-0-a25610164dc31542031dcc32e7b1dbf5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-429cf1c19d4e1cf5439632e6fafbfe99~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286763&auth_key=1762286763-0-0-759ebe1667ade6306b90cfdeaebb0ef1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReXGroundingCT-A-3D-Chest-CT-Dataset-for-Segmentation-of-Findings-from-Free-Text-Reports"><a href="#ReXGroundingCT-A-3D-Chest-CT-Dataset-for-Segmentation-of-Findings-from-Free-Text-Reports" class="headerlink" title="ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from   Free-Text Reports"></a>ReXGroundingCT: A 3D Chest CT Dataset for Segmentation of Findings from   Free-Text Reports</h2><p><strong>Authors:Mohammed Baharoon, Luyang Luo, Michael Moritz, Abhinav Kumar, Sung Eun Kim, Xiaoman Zhang, Miao Zhu, Mahmoud Hussain Alabbad, Maha Sbayel Alhazmi, Neel P. Mistry, Lucas Bijnens, Kent Ryan Kleinschmidt, Brady Chrisler, Sathvik Suryadevara, Sri Sai Dinesh Jaliparthi, Noah Michael Prudlo, Mark David Marino, Jeremy Palacio, Rithvik Akula, Di Zhou, Hong-Yu Zhou, Ibrahim Ethem Hamamci, Scott J. Adams, Hassan Rayhan AlOmaish, Pranav Rajpurkar</strong></p>
<p>We introduce ReXGroundingCT, the first publicly available dataset linking free-text findings to pixel-level 3D segmentations in chest CT scans. The dataset includes 3,142 non-contrast chest CT scans paired with standardized radiology reports from CT-RATE. Construction followed a structured three-stage pipeline. First, GPT-4 was used to extract and standardize findings, descriptors, and metadata from reports originally written in Turkish and machine-translated into English. Second, GPT-4o-mini categorized each finding into a hierarchical ontology of lung and pleural abnormalities. Third, 3D annotations were produced for all CT volumes: the training set was quality-assured by board-certified radiologists, and the validation and test sets were fully annotated by board-certified radiologists. Additionally, a complementary chain-of-thought dataset was created to provide step-by-step hierarchical anatomical reasoning for localizing findings within the CT volume, using GPT-4o and localization coordinates derived from organ segmentation models. ReXGroundingCT contains 16,301 annotated entities across 8,028 text-to-3D-segmentation pairs, covering diverse radiological patterns from 3,142 non-contrast CT scans. About 79% of findings are focal abnormalities and 21% are non-focal. The dataset includes a public validation set of 50 cases and a private test set of 100 cases, both annotated by board-certified radiologists. The dataset establishes a foundation for enabling free-text finding segmentation and grounded radiology report generation in CT imaging. Model performance on the private test set is hosted on a public leaderboard at <a target="_blank" rel="noopener" href="https://rexrank.ai/ReXGroundingCT">https://rexrank.ai/ReXGroundingCT</a>. The dataset is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT">https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†ReXGroundingCTæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå°†è‡ªç”±æ–‡æœ¬å‘ç°ä¸èƒ¸éƒ¨CTæ‰«æä¸­çš„åƒç´ çº§3Dåˆ†å‰²ç›¸é“¾æ¥çš„å…¬å¼€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªCT-RATEçš„æ ‡å‡†åŒ–æ”¾å°„å­¦æŠ¥å‘Šä¸3,142ä»½éå¯¹æ¯”æ€§èƒ¸éƒ¨CTæ‰«æé…å¯¹ã€‚æ„å»ºè¿‡ç¨‹éµå¾ªç»“æ„åŒ–ä¸‰é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆï¼Œä½¿ç”¨GPT-4æå–å¹¶æ ‡å‡†åŒ–æŠ¥å‘Šä¸­åŸæœ¬ç”¨åœŸè€³å…¶è¯­å†™æˆå¹¶é€šè¿‡æœºå™¨ç¿»è¯‘è‡³è‹±æ–‡çš„å‘ç°ã€æè¿°å’Œå…ƒæ•°æ®ï¼›å…¶æ¬¡ï¼ŒGPT-4o-miniå°†æ¯é¡¹å‘ç°å½’ç±»è‡³è‚ºå’Œèƒ¸è†œå¼‚å¸¸çš„å±‚æ¬¡åŒ–æœ¬ä½“ï¼›æœ€åï¼Œå¯¹æ‰€æœ‰CTä½“ç§¯è¿›è¡Œ3Dæ³¨é‡Šï¼šè®­ç»ƒé›†ç»è¿‡ä¸“ä¸šè®¤è¯æ”¾å°„ç§‘åŒ»å¸ˆçš„è´¨é‡ä¿è¯ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†åˆ™å®Œå…¨ç”±ä¸“ä¸šè®¤è¯æ”¾å°„ç§‘åŒ»å¸ˆè¿›è¡Œæ ‡æ³¨ã€‚æ­¤å¤–ï¼Œè¿˜åˆ›å»ºäº†è¡¥å……çš„â€œæ€è€ƒé“¾â€æ•°æ®é›†ï¼Œä»¥æä¾›é€æ­¥çš„å±‚æ¬¡åŒ–è§£å‰–æ¨ç†ï¼Œç”¨äºå®šä½CTä½“ç§¯å†…çš„å‘ç°ï¼Œä½¿ç”¨GPT-4oå’Œæ¥è‡ªå™¨å®˜åˆ†å‰²æ¨¡å‹çš„å®šä½åæ ‡ã€‚ReXGroundingCTåŒ…å«8,028ä¸ªæ–‡æœ¬åˆ°3Dåˆ†å‰²é…å¯¹ä¸­çš„16,301ä¸ªæ³¨é‡Šå®ä½“ï¼Œæ¶µç›–äº†æ¥è‡ª3,142ä»½éå¯¹æ¯”æ€§CTæ‰«æçš„å¤šæ ·åŒ–æ”¾å°„æ¨¡å¼ã€‚å¤§çº¦79%çš„å‘ç°æ˜¯å±€éƒ¨å¼‚å¸¸ï¼Œ21%æ˜¯éå±€éƒ¨å¼‚å¸¸ã€‚è¯¥æ•°æ®é›†åŒ…å«ç”±ä¸“ä¸šè®¤è¯æ”¾å°„ç§‘åŒ»å¸ˆæ ‡æ³¨çš„50ä¾‹å…¬å¼€éªŒè¯é›†å’Œ100ä¾‹ç§æœ‰æµ‹è¯•é›†ã€‚è¯¥æ•°æ®é›†ä¸ºCTæˆåƒä¸­çš„è‡ªç”±æ–‡æœ¬åˆ†å‰²å’ŒåŸºäºåœ°é¢çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå¥ å®šäº†åŸºç¡€ã€‚æ¨¡å‹åœ¨ç§æœ‰æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å·²åœ¨<a target="_blank" rel="noopener" href="https://rexrank.ai/ReXGroundingCT%E4%B8%8A%E7%9A%84%E5%85%AC%E5%BC%80%E6%8E%92%E8%A1%8C%E6%A6%9C%E4%B8%8A%E5%85%AC%E5%B8%83%E3%80%82%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%AF%E5%9C%A8https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCT%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://rexrank.ai/ReXGroundingCTä¸Šçš„å…¬å¼€æ’è¡Œæ¦œä¸Šå…¬å¸ƒã€‚æ•°æ®é›†å¯åœ¨https://huggingface.co/datasets/rajpurkarlab/ReXGroundingCTä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22030v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ReXGroundingCTæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é¦–æ¬¡å®ç°äº†è‡ªç”±æ–‡æœ¬å‘ç°ä¸èƒ¸éƒ¨CTæ‰«æçš„åƒç´ çº§3Dåˆ†å‰²ä¹‹é—´çš„é“¾æ¥ã€‚åŒ…å«æ ‡å‡†åŒ–æ”¾å°„å­¦æŠ¥å‘Šé…å¯¹çš„éå¯¹æ¯”èƒ¸éƒ¨CTæ‰«æï¼Œé€šè¿‡ç»“æ„åŒ–ä¸‰é˜¶æ®µç®¡é“æ„å»ºè€Œæˆã€‚åˆ©ç”¨GPT-4æå–å¹¶æ ‡å‡†åŒ–æŠ¥å‘Šä¸­çš„å‘ç°ã€æè¿°ç¬¦å’Œå…ƒæ•°æ®ï¼Œè¿›è¡Œåˆ†ç±»å¹¶åˆ†ä¸ºè‚ºéƒ¨å’Œèƒ¸è†œå¼‚å¸¸å±‚æ¬¡ç»“æ„ã€‚æ‰€æœ‰CTä½“ç§¯å‡äº§ç”Ÿ3Dæ³¨é‡Šï¼Œå¹¶ç”±ä¸“ä¸šè®¤è¯çš„æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œè´¨é‡ä¿è¯å’Œå®Œå…¨æ³¨é‡Šã€‚è¯¥æ•°æ®é›†ä¸ºCTå½±åƒä¸­çš„è‡ªç”±æ–‡æœ¬åˆ†å‰²å’Œæ¥åœ°æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå¥ å®šäº†åŸºç¡€ï¼Œæä¾›é€çº§çš„è§£å‰–æ¨ç†ï¼Œç”¨äºå®šä½CTä½“ç§¯ä¸­çš„å‘ç°ã€‚ReXGroundingCTåŒ…å«è¦†ç›–å¤šç§æ”¾å°„å­¦æ¨¡å¼çš„å¤šä¸ªæ³¨é‡Šå®ä½“å’Œæ–‡æœ¬åˆ°3Dåˆ†å‰²å¯¹ã€‚æ•°æ®é›†å·²åœ¨å…¬å…±æ’è¡Œæ¦œä¸Šæ‰˜ç®¡ç§æœ‰æµ‹è¯•é›†çš„æ¨¡å‹æ€§èƒ½ï¼Œå¹¶å¯é€šè¿‡é“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReXGroundingCTæ˜¯é¦–ä¸ªé“¾æ¥è‡ªç”±æ–‡æœ¬å‘ç°ä¸èƒ¸éƒ¨CTæ‰«æåƒç´ çº§3Dåˆ†å‰²çš„æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä¸CT-RATEæ ‡å‡†åŒ–æ”¾å°„å­¦æŠ¥å‘Šé…å¯¹çš„éå¯¹æ¯”èƒ¸éƒ¨CTæ‰«æã€‚</li>
<li>æ•°æ®é›†æ„å»ºé‡‡ç”¨ç»“æ„åŒ–ä¸‰é˜¶æ®µç®¡é“ï¼ŒåŒ…æ‹¬ä½¿ç”¨GPT-4æå–å’Œåˆ†ç±»æŠ¥å‘Šå‘ç°ã€‚</li>
<li>æ‰€æœ‰CTä½“ç§¯å‡è¿›è¡Œ3Dæ³¨é‡Šï¼Œå¹¶ç”±ä¸“ä¸šè®¤è¯çš„æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œè´¨é‡ä¿è¯å’Œå®Œå…¨æ³¨é‡Šã€‚</li>
<li>æ•°æ®é›†æä¾›é€çº§çš„è§£å‰–æ¨ç†ï¼Œæœ‰åŠ©äºå®šä½CTä½“ç§¯ä¸­çš„å¼‚å¸¸å‘ç°ã€‚</li>
<li>ReXGroundingCTåŒ…å«å¤šç§æ”¾å°„å­¦æ¨¡å¼çš„å¤šä¸ªæ³¨é‡Šå®ä½“å’Œæ–‡æœ¬åˆ°3Dåˆ†å‰²å¯¹ã€‚</li>
<li>æ¨¡å‹çš„æ€§èƒ½å·²åœ¨å…¬å…±æ’è¡Œæ¦œä¸Šæ‰˜ç®¡ï¼Œæ•°æ®é›†å¯é€šè¿‡æŒ‡å®šé“¾æ¥è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f3d212110d9f3623a2388f7ec8527792~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286771&auth_key=1762286771-0-0-7f1578304789732faa5b951b2e110b12&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf12fe93cbd82562b56f09377e076775~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286778&auth_key=1762286778-0-0-7a4d85fe80157e4556fac2291df0d0f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5cb555bbedd8ca3e844330d28e63921f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286785&auth_key=1762286785-0-0-445cbcd98ad1e65b1b1b3f5745b8780b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e115939f52816602cce1218f41095c92~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286792&auth_key=1762286792-0-0-19b53e7fa391d498df33d53a775b17a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-81014b5e98306b09d1ce343a1c81940a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286799&auth_key=1762286799-0-0-1f7ba6973d9434edf9329940ea1583f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DArFace-Deformation-Aware-Robustness-for-Low-Quality-Face-Recognition"><a href="#DArFace-Deformation-Aware-Robustness-for-Low-Quality-Face-Recognition" class="headerlink" title="DArFace: Deformation Aware Robustness for Low Quality Face Recognition"></a>DArFace: Deformation Aware Robustness for Low Quality Face Recognition</h2><p><strong>Authors:Sadaf Gulshad, Abdullah Aldahlawi</strong></p>
<p>Facial recognition systems have achieved remarkable success by leveraging deep neural networks, advanced loss functions, and large-scale datasets. However, their performance often deteriorates in real-world scenarios involving low-quality facial images. Such degradations, common in surveillance footage or standoff imaging include low resolution, motion blur, and various distortions, resulting in a substantial domain gap from the high-quality data typically used during training. While existing approaches attempt to address robustness by modifying network architectures or modeling global spatial transformations, they frequently overlook local, non-rigid deformations that are inherently present in real-world settings. In this work, we introduce \textbf{DArFace}, a \textbf{D}eformation-\textbf{A}ware \textbf{r}obust \textbf{Face} recognition framework that enhances robustness to such degradations without requiring paired high- and low-quality training samples. Our method adversarially integrates both global transformations (e.g., rotation, translation) and local elastic deformations during training to simulate realistic low-quality conditions. Moreover, we introduce a contrastive objective to enforce identity consistency across different deformed views. Extensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and IJB-C demonstrate that DArFace surpasses state-of-the-art methods, with significant gains attributed to the inclusion of local deformation modeling. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«ç³»ç»Ÿé€šè¿‡åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œã€å…ˆè¿›çš„æŸå¤±å‡½æ•°å’Œå¤§è§„æ¨¡æ•°æ®é›†å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ¶‰åŠä½è´¨é‡é¢éƒ¨å›¾åƒçš„ç°å®ä¸–ç•Œåœºæ™¯ä¸­æ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚è¿™ç§é€€åŒ–åœ¨ç›‘æ§å½•åƒæˆ–è¿œè·ç¦»æˆåƒä¸­å¾ˆå¸¸è§ï¼ŒåŒ…æ‹¬ä½åˆ†è¾¨ç‡ã€è¿åŠ¨æ¨¡ç³Šå’Œå„ç§å¤±çœŸï¼Œä¸è®­ç»ƒæœŸé—´é€šå¸¸ä½¿ç”¨çš„é«˜è´¨é‡æ•°æ®ä¹‹é—´å­˜åœ¨å·¨å¤§çš„é¢†åŸŸå·®è·ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡ä¿®æ”¹ç½‘ç»œæ¶æ„æˆ–å»ºæ¨¡å…¨å±€ç©ºé—´å˜æ¢æ¥æé«˜ç¨³å¥æ€§ï¼Œä½†å®ƒä»¬ç»å¸¸å¿½ç•¥ç°å®ç¯å¢ƒä¸­å›ºæœ‰çš„å±€éƒ¨éåˆšæ€§å˜å½¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>DArFace</strong>ï¼Œä¸€ä¸ª<strong>D</strong>eformation-<strong>A</strong>ware <strong>r</strong>obust <strong>Face</strong>è¯†åˆ«æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æé«˜äº†å¯¹è¿™ç§é€€åŒ–çš„ç¨³å¥æ€§ï¼Œè€Œæ— éœ€é…å¯¹çš„é«˜è´¨é‡ä½è´¨é‡è®­ç»ƒæ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æŠ—æ€§åœ°é›†æˆå…¨å±€å˜æ¢ï¼ˆä¾‹å¦‚æ—‹è½¬ã€å¹³ç§»ï¼‰å’Œå±€éƒ¨å¼¹æ€§å˜å½¢ï¼Œæ¥æ¨¡æ‹Ÿç°å®ç”Ÿæ´»ä¸­çš„ä½è´¨é‡æ¡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯¹æ¯”ç›®æ ‡ï¼Œä»¥å¼ºåˆ¶ä¸åŒå˜å½¢è§†å›¾ä¹‹é—´çš„èº«ä»½ä¸€è‡´æ€§ã€‚åœ¨ä½è´¨é‡åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬TinyFaceã€IJB-Bå’ŒIJB-Cï¼Œè¡¨æ˜DArFaceè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå…¶æ˜¾è‘—æ”¶ç›Šå½’å› äºå±€éƒ¨å˜å½¢å»ºæ¨¡çš„å¼•å…¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08423v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œå’Œå¯¹æ¯”ç›®æ ‡çš„é¢éƒ¨è¯†åˆ«æ¡†æ¶DArFaceï¼Œè¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿä½è´¨é‡æ¡ä»¶ä¸‹çš„å…¨å±€å˜æ¢å’Œå±€éƒ¨å¼¹æ€§å˜å½¢ï¼Œæé«˜äº†å¯¹ä½è´¨é‡é¢éƒ¨å›¾åƒçš„é²æ£’æ€§ã€‚åœ¨TinyFaceã€IJB-Bå’ŒIJB-Cç­‰ä½è´¨é‡åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDArFaceåœ¨åŒ…æ‹¬å±€éƒ¨å˜å½¢å»ºæ¨¡çš„æƒ…å†µä¸‹è¶…è¶Šäº†æœ€æ–°æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨è¯†åˆ«ç³»ç»Ÿåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œã€å…ˆè¿›çš„æŸå¤±å‡½æ•°å’Œå¤§è§„æ¨¡æ•°æ®é›†å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>åœ¨æ¶‰åŠä½è´¨é‡é¢éƒ¨å›¾åƒçš„ç°å®ä¸–ç•Œåœºæ™¯ä¸­ï¼Œé¢éƒ¨è¯†åˆ«ç³»ç»Ÿçš„æ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚</li>
<li>ä½è´¨é‡å›¾åƒå¸¸è§çš„é€€åŒ–åŒ…æ‹¬ä½åˆ†è¾¨ç‡ã€è¿åŠ¨æ¨¡ç³Šå’Œå„ç§æ‰­æ›²ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ä¿®æ”¹ç½‘ç»œæ¶æ„æˆ–å»ºæ¨¡å…¨å±€ç©ºé—´å˜æ¢æ¥æé«˜é²æ£’æ€§ï¼Œä½†å¿½ç•¥äº†ç°å®ç¯å¢ƒä¸­å›ºæœ‰çš„å±€éƒ¨éåˆšæ€§å˜å½¢ã€‚</li>
<li>DArFaceæ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿä½è´¨é‡æ¡ä»¶ä¸‹çš„å…¨å±€å˜æ¢å’Œå±€éƒ¨å¼¹æ€§å˜å½¢ï¼Œæé«˜äº†å¯¹ä½è´¨é‡é¢éƒ¨å›¾åƒçš„é²æ£’æ€§ã€‚</li>
<li>DArFaceä½¿ç”¨å¯¹æ¯”ç›®æ ‡æ¥å¼ºåˆ¶æ‰§è¡Œä¸åŒå˜å½¢è§†å›¾ä¹‹é—´çš„èº«ä»½ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-01b2c0776a5c722b7af4ce261d388481~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286806&auth_key=1762286806-0-0-6d06eddf4ad393b440a0b47a82a22306&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9adca9adb29ddc7743fb05ec7faa2788~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286814&auth_key=1762286814-0-0-5bb7415bc6a382ffe9550c488b445afc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9b921bc0f6eb30883ba83a72d8f2048~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286821&auth_key=1762286821-0-0-5a8747b5aad2fb47631754d448065c7f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MsEdF-A-Multi-stream-Encoder-decoder-Framework-for-Remote-Sensing-Image-Captioning"><a href="#MsEdF-A-Multi-stream-Encoder-decoder-Framework-for-Remote-Sensing-Image-Captioning" class="headerlink" title="MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image   Captioning"></a>MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image   Captioning</h2><p><strong>Authors:Swadhin Das, Raksha Sharma</strong></p>
<p>Remote sensing images contain complex spatial patterns and semantic structures, which makes the captioning model difficult to accurately describe. Encoder-decoder architectures have become the widely used approach for RSIC by translating visual content into descriptive text. However, many existing methods rely on a single-stream architecture, which weakens the model to accurately describe the image. Such single-stream architectures typically struggle to extract diverse spatial features or capture complex semantic relationships, limiting their effectiveness in scenes with high intraclass similarity or contextual ambiguity. In this work, we propose a novel Multi-stream Encoder-decoder Framework (MsEdF) which improves the performance of RSIC by optimizing both the spatial representation and language generation of encoder-decoder architecture. The encoder fuses information from two complementary image encoders, thereby promoting feature diversity through the integration of multiscale and structurally distinct cues. To improve the capture of context-aware descriptions, we refine the input sequenceâ€™s semantic modeling on the decoder side using a stacked GRU architecture with an element-wise aggregation scheme. Experiments on three benchmark RSIC datasets show that MsEdF outperforms several baseline models. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒåŒ…å«å¤æ‚çš„ç©ºé—´æ¨¡å¼å’Œè¯­ä¹‰ç»“æ„ï¼Œè¿™ä½¿å¾—æ ‡æ³¨æ¨¡å‹éš¾ä»¥å‡†ç¡®æè¿°ã€‚ç¼–ç å™¨-è§£ç å™¨æ¶æ„å·²æˆä¸ºé¥æ„Ÿå›¾åƒæ ‡æ³¨ï¼ˆRSICï¼‰çš„å¹¿æ³›åº”ç”¨æ–¹æ³•ï¼Œé€šè¿‡å°†è§†è§‰å†…å®¹ç¿»è¯‘ä¸ºæè¿°æ€§æ–‡æœ¬ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ–¹æ³•ä¾èµ–äºå•æµæ¶æ„ï¼Œè¿™å‰Šå¼±äº†æ¨¡å‹å‡†ç¡®æè¿°å›¾åƒçš„èƒ½åŠ›ã€‚è¿™ç§å•æµæ¶æ„é€šå¸¸éš¾ä»¥æå–å¤šæ ·çš„ç©ºé—´ç‰¹å¾æˆ–æ•æ‰å¤æ‚çš„è¯­ä¹‰å…³ç³»ï¼Œåœ¨å…·æœ‰é«˜ç±»å†…ç›¸ä¼¼æ€§æˆ–ä¸Šä¸‹æ–‡æ¨¡ç³Šçš„åœºæ™¯ä¸­ï¼Œå…¶æ•ˆæœæœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæµç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼ˆMsEdFï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„ç©ºé—´è¡¨ç¤ºå’Œè¯­è¨€ç”Ÿæˆï¼Œæé«˜äº†RSICçš„æ€§èƒ½ã€‚ç¼–ç å™¨èåˆäº†æ¥è‡ªä¸¤ä¸ªäº’è¡¥å›¾åƒç¼–ç å™¨çš„ä¿¡æ¯ï¼Œé€šè¿‡å¤šå°ºåº¦å’Œç»“æ„ä¸åŒç‰¹å¾çš„ç»“åˆï¼Œä¿ƒè¿›äº†ç‰¹å¾çš„å¤šæ ·æ€§ã€‚ä¸ºäº†æé«˜å¯¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥æè¿°æ•æ‰èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨è§£ç å™¨ç«¯ä½¿ç”¨å †å çš„GRUæ¶æ„å’Œå…ƒç´ çº§èšåˆæ–¹æ¡ˆï¼Œå¯¹è¾“å…¥åºåˆ—çš„è¯­ä¹‰å»ºæ¨¡è¿›è¡Œäº†æ”¹è¿›ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†RSICæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMsEdFä¼˜äºå‡ ç§åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09282v4">PDF</a> </p>
<p><strong>Summary</strong><br>è¿œç¨‹å›¾åƒçš„ç©ºé—´æ¨¡å¼å’Œè¯­ä¹‰ç»“æ„å¤æ‚ï¼Œå¯¼è‡´å›¾åƒæ ‡æ³¨æ¨¡å‹éš¾ä»¥å‡†ç¡®æè¿°ã€‚ç°æœ‰å¸¸ç”¨çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„é€šè¿‡è§†è§‰å†…å®¹ç¿»è¯‘æ¥æè¿°æ–‡æœ¬ã€‚ä½†å•ä¸€æµæ¶æ„æ–¹æ³•è¾ƒå¼±ï¼Œéš¾ä»¥å‡†ç¡®æè¿°å›¾åƒã€‚è¯¥æ–¹æ³•åœ¨æå–ç©ºé—´ç‰¹å¾å’Œæ•æ‰å¤æ‚è¯­ä¹‰å…³ç³»æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯¹é«˜å†…èšç±»æˆ–é«˜è¯­å¢ƒæ¨¡ç³Šåœºæ™¯çš„æœ‰æ•ˆæ€§å—é™ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°çš„å¤šæµç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼ˆMsEdFï¼‰ï¼Œä¼˜åŒ–ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„ç©ºé—´è¡¨ç¤ºå’Œè¯­è¨€ç”Ÿæˆï¼Œæé«˜è¿œç¨‹å›¾åƒæ ‡æ³¨çš„æ€§èƒ½ã€‚ç¼–ç å™¨èåˆä¸¤ä¸ªäº’è¡¥å›¾åƒç¼–ç å™¨çš„ä¿¡æ¯ï¼Œä¿ƒè¿›ç‰¹å¾å¤šæ ·æ€§çš„èåˆã€‚ä¸ºæé«˜è¯­å¢ƒæ„ŸçŸ¥æè¿°çš„æ•æ‰èƒ½åŠ›ï¼Œè§£ç å™¨ä¾§ä½¿ç”¨å †å GRUæ¶æ„å’Œé€å…ƒç´ èšåˆæ–¹æ¡ˆå¯¹è¾“å…¥åºåˆ—çš„è¯­ä¹‰å»ºæ¨¡è¿›è¡Œç»†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMsEdFåœ¨ä¸‰ä¸ªåŸºå‡†è¿œç¨‹å›¾åƒæ ‡æ³¨æ•°æ®é›†ä¸Šä¼˜äºå‡ ä¸ªåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹å›¾åƒæ ‡æ³¨é¢ä¸´å¤æ‚ç©ºé—´æ¨¡å¼å’Œè¯­ä¹‰ç»“æ„çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç¼–ç å™¨-è§£ç å™¨æ¶æ„åœ¨æè¿°å›¾åƒæ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„å¤šæµç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼ˆMsEdFï¼‰æ—¨åœ¨æé«˜è¿œç¨‹å›¾åƒæ ‡æ³¨çš„æ€§èƒ½ã€‚</li>
<li>MsEdFé€šè¿‡èåˆä¸¤ä¸ªäº’è¡¥å›¾åƒç¼–ç å™¨çš„ä¿¡æ¯ï¼Œä¿ƒè¿›ç‰¹å¾å¤šæ ·æ€§ã€‚</li>
<li>MsEdFé€šè¿‡ç»†åŒ–è§£ç å™¨ä¾§çš„è¯­ä¹‰å»ºæ¨¡ï¼Œæé«˜è¯­å¢ƒæ„ŸçŸ¥æè¿°çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMsEdFåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b96b1a4059fbd9ac838ed7dba48b9417~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286829&auth_key=1762286829-0-0-9a3106a2517aff9f0c85da34afb92ae0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-419b6499a216b04a6357c3f0193bf99c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286836&auth_key=1762286836-0-0-81044126c2b294489ad120e17c2daf06&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8495b4dce2c7e0b049dc03b1abccd137~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286842&auth_key=1762286842-0-0-be648bab6255c8aa09bbbd79ed2f3853&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7a71916af87e498d5147d455b3e89d09~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286848&auth_key=1762286848-0-0-815c288a261d03a4c8bb5416476db302&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b405df4379f40ee6d8847de718a7791~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286855&auth_key=1762286855-0-0-bd835e2de25eb965b112f85c71da526b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3e1002369b9cdadebc1bb2a391a0b4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286862&auth_key=1762286862-0-0-55ff044323f4720d9ee09fdd1b7d56a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Guided-MRI-Reconstruction-via-Schrodinger-Bridge"><a href="#Guided-MRI-Reconstruction-via-Schrodinger-Bridge" class="headerlink" title="Guided MRI Reconstruction via SchrÃ¶dinger Bridge"></a>Guided MRI Reconstruction via SchrÃ¶dinger Bridge</h2><p><strong>Authors:Yue Wang, Yuanbiao Yang, Zhuo-xu Cui, Tian Zhou, Bingsheng Huang, Hairong Zheng, Dong Liang, Yanjie Zhu</strong></p>
<p>Magnetic Resonance Imaging (MRI) is an inherently multi-contrast modality, where cross-contrast priors can be exploited to improve image reconstruction from undersampled data. Recently, diffusion models have shown remarkable performance in MRI reconstruction. However, they still struggle to effectively utilize such priors, mainly because existing methods rely on feature-level fusion in image or latent spaces, which lacks explicit structural correspondence and thus leads to suboptimal performance. To address this issue, we propose $\mathbf{I}^2$SB-Inversion, a multi-contrast guided reconstruction framework based on the Schr&quot;odinger Bridge (SB). The proposed method performs pixel-wise translation between paired contrasts, providing explicit structural constraints between the guidance and target images. Furthermore, an Inversion strategy is introduced to correct inter-modality misalignment, which often occurs in guided reconstruction, thereby mitigating artifacts and improving reconstruction accuracy. Experiments on paired T1- and T2-weighted datasets demonstrate that $\mathbf{I}^2$SB-Inversion achieves a high acceleration factor of up to 14.4 and consistently outperforms existing methods in both quantitative and qualitative evaluations. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ä¸€ç§å›ºæœ‰çš„å¤šå¯¹æ¯”åº¦æˆåƒæ¨¡å¼ï¼Œå¯ä»¥åˆ©ç”¨è·¨å¯¹æ¯”åº¦å…ˆéªŒä¿¡æ¯æ¥æ”¹å–„ä»æ¬ é‡‡æ ·æ•°æ®ä¸­é‡å»ºå›¾åƒã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨MRIé‡å»ºä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥æœ‰æ•ˆåˆ©ç”¨è¿™æ ·çš„å…ˆéªŒä¿¡æ¯ï¼Œä¸»è¦æ˜¯å› ä¸ºç°æœ‰æ–¹æ³•ä¾èµ–äºå›¾åƒæˆ–æ½œåœ¨ç©ºé—´ä¸­çš„ç‰¹å¾çº§èåˆï¼Œè¿™ç¼ºä¹æ˜ç¡®çš„ç»“æ„å¯¹åº”å…³ç³»ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè–›å®šè°”æ¡¥ï¼ˆSBï¼‰çš„å¤šå¯¹æ¯”åº¦å¼•å¯¼é‡å»ºæ¡†æ¶$\mathbf{I}^2$SB-Inversionã€‚è¯¥æ–¹æ³•æ‰§è¡Œé…å¯¹å¯¹æ¯”åº¦ä¹‹é—´çš„åƒç´ çº§è½¬æ¢ï¼Œä¸ºå¼•å¯¼å’Œç›®æ ‡å›¾åƒä¹‹é—´æä¾›æ˜ç¡®çš„ç»“æ„çº¦æŸã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åè½¬ç­–ç•¥ï¼Œä»¥è§£å†³åœ¨å¼•å¯¼é‡å»ºä¸­ç»å¸¸å‘ç”Ÿçš„è·¨æ¨¡æ€ä¸åŒ¹é…é—®é¢˜ï¼Œä»è€Œå‡è½»ä¼ªå½±ï¼Œæé«˜é‡å»ºç²¾åº¦ã€‚åœ¨é…å¯¹T1å’ŒT å®éªŒä¸­ï¼ŒåŠ æƒæ•°æ®é›†è¡¨æ˜$\mathbf{I}^2$SB-Inversionå®ç°äº†é«˜è¾¾14.4çš„é«˜åŠ é€Ÿå› å­ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14269v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MRIé‡å»ºä¸­ï¼Œå¤šå¯¹æ¯”åº¦å…ˆéªŒä¿¡æ¯èƒ½æœ‰æ•ˆæå‡æ¬ é‡‡æ ·æ•°æ®çš„å›¾åƒé‡å»ºæ•ˆæœã€‚é’ˆå¯¹ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨åˆ©ç”¨è¿™äº›å…ˆéªŒä¿¡æ¯æ—¶å­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºSchrÃ¶dinger Bridgeï¼ˆSBï¼‰çš„å¤šå¯¹æ¯”åº¦å¼•å¯¼é‡å»ºæ¡†æ¶$\mathbf{I}^2$SB-Inversionã€‚è¯¥æ–¹æ³•å®ç°äº†é…å¯¹å¯¹æ¯”åº¦ä¹‹é—´çš„åƒç´ çº§è½¬æ¢ï¼Œæä¾›äº†æ˜ç¡®çš„ç»“æ„çº¦æŸï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ ¡æ­£è·¨æ¨¡æ€é—´ä¸ä¸€è‡´æ€§çš„å€’åºç­–ç•¥ï¼Œæœ‰æ•ˆå‡è½»äº†å¼•å¯¼é‡å»ºä¸­çš„ä¼ªå½±é—®é¢˜ï¼Œæé«˜äº†é‡å»ºå‡†ç¡®æ€§ã€‚åœ¨T1å’ŒT2åŠ æƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œ$\mathbf{I}^2$SB-Inversionåœ¨åŠ é€Ÿå› å­é«˜è¾¾14.4çš„æƒ…å†µä¸‹ï¼Œæ— è®ºåœ¨å®šé‡è¿˜æ˜¯å®šæ€§è¯„ä¼°ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>$\mathbf{I}^2$SB-Inversionåˆ©ç”¨å¤šå¯¹æ¯”åº¦å…ˆéªŒä¿¡æ¯è¿›è¡ŒMRIé‡å»ºï¼Œèƒ½æœ‰æ•ˆæå‡æ¬ é‡‡æ ·æ•°æ®çš„å›¾åƒè´¨é‡ã€‚</li>
<li>é€šè¿‡åƒç´ çº§è½¬æ¢å®ç°äº†é…å¯¹å¯¹æ¯”åº¦é—´çš„æ˜ç¡®ç»“æ„çº¦æŸã€‚</li>
<li>å¼•å…¥å€’åºç­–ç•¥æ ¡æ­£è·¨æ¨¡æ€é—´çš„ä¸ä¸€è‡´æ€§ï¼Œæé«˜äº†é‡å»ºå‡†ç¡®æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œ$\mathbf{I}^2$SB-Inversionåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­è¡¨ç°æ›´ä¼˜å¼‚ã€‚</li>
<li>$\mathbf{I}^2$SB-Inversionæ–¹æ³•èƒ½å¤Ÿåœ¨é«˜åŠ é€Ÿå› å­ä¸‹å®ç°MRIé‡å»ºã€‚</li>
<li>ç‰¹å¾çº§èåˆå’Œå›¾åƒæˆ–æ½œåœ¨ç©ºé—´ä¸­çš„èåˆåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æ•ˆæœä¸ä½³ï¼Œç¼ºä¹æ˜ç¡®çš„ç»“æ„å¯¹åº”å…³ç³»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-39e6556184c8eebea28b815cf972d7f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286869&auth_key=1762286869-0-0-9a55af2e746325425d2881e36cb18fb7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-69fbc69a2e436abb81bc55311370d56c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286877&auth_key=1762286877-0-0-da0a1048e051ef008e2b10d7596a76a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8992ab8293acd582c75ad181b24be1e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286884&auth_key=1762286884-0-0-ab1060d9249457dcf7738c8e69132726&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1d8a4b9ebfc62708933e6f9e34ef1d3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286891&auth_key=1762286891-0-0-9b7630904a64b8cd3ecfe0ab406767ae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Bidirectional-Regression-for-Monocular-6DoF-Head-Pose-Estimation-and-Reference-System-Alignment"><a href="#Bidirectional-Regression-for-Monocular-6DoF-Head-Pose-Estimation-and-Reference-System-Alignment" class="headerlink" title="Bidirectional Regression for Monocular 6DoF Head Pose Estimation and   Reference System Alignment"></a>Bidirectional Regression for Monocular 6DoF Head Pose Estimation and   Reference System Alignment</h2><p><strong>Authors:Sungho Chun, Boeun Kim, Hyung Jin Chang, Ju Yong Chang</strong></p>
<p>Precise six-degree-of-freedom (6DoF) head pose estimation is crucial for safety-critical applications and human-computer interaction scenarios, yet existing monocular methods still struggle with robust pose estimation. We revisit this problem by introducing TRGv2, a lightweight extension of our previous Translation, Rotation, and Geometry (TRG) network, which explicitly models the bidirectional interaction between facial geometry and head pose. TRGv2 jointly infers facial landmarks and 6DoF pose through an iterative refinement loop with landmark-to-image projection, ensuring metric consistency among face size, rotation, and depth. To further improve generalization to out-of-distribution data, TRGv2 regresses correction parameters instead of directly predicting translation, combining them with a pinhole camera model for analytic depth estimation. In addition, we identify a previously overlooked source of bias in cross-dataset evaluations due to inconsistent head center definitions across different datasets. To address this, we propose a reference system alignment strategy that quantifies and corrects translation bias, enabling fair comparisons across datasets. Extensive experiments on ARKitFace, BIWI, and the challenging DD-Pose benchmarks demonstrate that TRGv2 outperforms state-of-the-art methods in both accuracy and efficiency. Code and newly annotated landmarks for DD-Pose will be publicly available. </p>
<blockquote>
<p>ç²¾ç¡®å…­è‡ªç”±åº¦ï¼ˆ6DoFï¼‰å¤´éƒ¨å§¿æ€ä¼°è®¡æ˜¯å®‰å…¨å…³é”®åº”ç”¨å’Œäººæœºäº¤äº’åœºæ™¯ä¸­çš„å…³é”®ï¼Œä½†ç°æœ‰çš„å•ç›®æ–¹æ³•ä»ç„¶éš¾ä»¥è¿›è¡Œç¨³å¥çš„å§¿æ€ä¼°è®¡ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥TRGv2æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„å…ˆå‰ç¿»è¯‘ã€æ—‹è½¬å’Œå‡ ä½•ï¼ˆTRGï¼‰ç½‘ç»œçš„è½»é‡çº§æ‰©å±•ï¼Œå®ƒæ˜¾å¼åœ°æ¨¡æ‹Ÿé¢éƒ¨å‡ ä½•å’Œå¤´éƒ¨å§¿æ€ä¹‹é—´çš„åŒå‘äº¤äº’ã€‚TRGv2é€šè¿‡å…·æœ‰åœ°æ ‡åˆ°å›¾åƒæŠ•å½±çš„è¿­ä»£ç»†åŒ–å¾ªç¯è”åˆæ¨æ–­é¢éƒ¨åœ°æ ‡å’Œ6DoFå§¿æ€ï¼Œç¡®ä¿é¢éƒ¨å¤§å°ã€æ—‹è½¬å’Œæ·±åº¦ä¹‹é—´çš„åº¦é‡ä¸€è‡´æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯¹ç¦»ç¾¤æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ï¼ŒTRGv2å›å½’æ ¡æ­£å‚æ•°è€Œä¸æ˜¯ç›´æ¥é¢„æµ‹ç¿»è¯‘ï¼Œå°†å®ƒä»¬ä¸é’ˆå­”ç›¸æœºæ¨¡å‹ç»“åˆè¿›è¡Œè§£ææ·±åº¦ä¼°è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†è·¨æ•°æ®é›†è¯„ä¼°ä¸­å› ä¸åŒæ•°æ®é›†ä¹‹é—´å¤´éƒ¨ä¸­å¿ƒå®šä¹‰ä¸ä¸€è‡´è€Œè¢«å¿½è§†ä¹‹å‰çš„åè§æ¥æºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‚è€ƒç³»ç»Ÿå¯¹é½ç­–ç•¥ï¼Œå¯¹ç¿»è¯‘åå·®è¿›è¡Œé‡åŒ–å’Œæ ¡æ­£ï¼Œå®ç°äº†è·¨æ•°æ®é›†å…¬å¹³æ¯”è¾ƒã€‚åœ¨ARKitFaceã€BIWIå’Œæœ‰æŒ‘æˆ˜æ€§çš„DD-PoseåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTRGv2åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚DD-Poseçš„ä»£ç å’Œæ–°æ³¨é‡Šçš„åœ°æ ‡å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.14136v2">PDF</a> This version extends the previously published preprint and has been   submitted to Pattern Recognition</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« ä»‹ç»äº†TRGv2ç½‘ç»œçš„é‡è¦æ€§åŠå…¶åœ¨ç²¾ç¡®å…­è‡ªç”±åº¦ï¼ˆ6DoFï¼‰å¤´éƒ¨å§¿æ€ä¼°è®¡ä¸­çš„åº”ç”¨ã€‚TRGv2æ˜¯å…ˆå‰ç¿»è¯‘ã€æ—‹è½¬å’Œå‡ ä½•ï¼ˆTRGï¼‰ç½‘ç»œçš„è½»é‡çº§æ‰©å±•ï¼Œå®ƒé€šè¿‡å¯¹é¢éƒ¨å‡ ä½•å’Œå¤´éƒ¨å§¿æ€ä¹‹é—´çš„åŒå‘äº¤äº’è¿›è¡Œæ˜¾å¼å»ºæ¨¡æ¥è§£å†³ç¨³å¥çš„å¤´éƒ¨å§¿æ€ä¼°è®¡é—®é¢˜ã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–å¾ªç¯å’Œåœ°æ ‡åˆ°å›¾åƒçš„æŠ•å½±ï¼ŒTRGv2è”åˆæ¨æ–­é¢éƒ¨åœ°æ ‡å’Œ6DoFå§¿æ€ï¼Œç¡®ä¿é¢éƒ¨å¤§å°ã€æ—‹è½¬å’Œæ·±åº¦ä¹‹é—´çš„åº¦é‡ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºå›å½’æ ¡æ­£å‚æ•°ä¸é’ˆå­”ç›¸æœºæ¨¡å‹ç›¸ç»“åˆè¿›è¡Œæ·±åº¦ä¼°è®¡ï¼Œä»¥æé«˜å¯¹ç¦»ç¾¤æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜è¯†åˆ«å¹¶è§£å†³äº†è·¨æ•°æ®é›†è¯„ä¼°ä¸­å› ä¸åŒæ•°æ®é›†é—´å¤´éƒ¨ä¸­å¿ƒå®šä¹‰ä¸ä¸€è‡´å¯¼è‡´çš„åè§é—®é¢˜ã€‚é€šè¿‡å¼•å…¥çš„å‚è€ƒç³»ç»Ÿå¯¹é½ç­–ç•¥é‡åŒ–å’Œçº æ­£äº†å¹³ç§»åå·®ï¼Œä½¿ä¸åŒæ•°æ®é›†ä¹‹é—´çš„æ¯”è¾ƒæ›´åŠ å…¬å¹³ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒTRGv2åœ¨å‡†ç¡®ç‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæœ€æ–°çš„æ–¹æ³•ã€‚æ–‡ç« æä¾›äº†å¯¹æ–°DD-PoseåŸºå‡†æ•°æ®é›†çš„ä»£ç å’Œæ–°æ³¨é‡Šã€‚è¿™äº›è¿›æ­¥æœ‰æœ›æé«˜å…³é”®é¢†åŸŸä¸­å®‰å…¨æ€§èƒ½ä»¥åŠäººç±»è®¡ç®—æœºäº¤äº’ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TRGv2ç½‘ç»œæ˜¯ç¿»è¯‘ã€æ—‹è½¬å’Œå‡ ä½•ï¼ˆTRGï¼‰ç½‘ç»œçš„è½»é‡çº§æ‰©å±•ï¼Œç”¨äºè§£å†³ç¨³å¥çš„å¤´éƒ¨å§¿æ€ä¼°è®¡é—®é¢˜ã€‚</li>
<li>TRGv2ç½‘ç»œå¯¹é¢éƒ¨å‡ ä½•ä¸å¤´éƒ¨å§¿æ€ä¹‹é—´çš„åŒå‘äº¤äº’è¿›è¡Œæ˜¾å¼å»ºæ¨¡ï¼Œå¹¶é€šè¿‡è¿­ä»£ä¼˜åŒ–å¾ªç¯è¿›è¡Œåœ°æ ‡å’Œå§¿æ€æ¨æ–­ã€‚</li>
<li>é€šè¿‡å›å½’æ ¡æ­£å‚æ•°ç»“åˆé’ˆå­”ç›¸æœºæ¨¡å‹æé«˜äº†æ·±åº¦ä¼°è®¡å’Œå¯¹ç¦»ç¾¤æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è·¨æ•°æ®é›†è¯„ä¼°æ—¶å‘ç°äº†ç”±äºå¤´éƒ¨ä¸­å¿ƒå®šä¹‰ä¸ä¸€è‡´äº§ç”Ÿçš„åè§é—®é¢˜ã€‚é€šè¿‡å¼•å…¥çš„å‚è€ƒç³»ç»Ÿå¯¹é½ç­–ç•¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.14136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d4ddbecddcf7f859da5ebdf01e82dbe1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286899&auth_key=1762286899-0-0-30cbd534802018a785d1d90a0d55afd3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7246bc82254980b32cfee4a55d10faf2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286906&auth_key=1762286906-0-0-aac7e4be3cb5a050e1267c7dd52dc597&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SRAGAN-Saliency-Regularized-and-Attended-Generative-Adversarial-Network-for-Chinese-Ink-wash-Painting-Style-Transfer"><a href="#SRAGAN-Saliency-Regularized-and-Attended-Generative-Adversarial-Network-for-Chinese-Ink-wash-Painting-Style-Transfer" class="headerlink" title="SRAGAN: Saliency Regularized and Attended Generative Adversarial Network   for Chinese Ink-wash Painting Style Transfer"></a>SRAGAN: Saliency Regularized and Attended Generative Adversarial Network   for Chinese Ink-wash Painting Style Transfer</h2><p><strong>Authors:Xiang Gao, Yuqi Zhang</strong></p>
<p>Recent style transfer problems are still largely dominated by Generative Adversarial Network (GAN) from the perspective of cross-domain image-to-image (I2I) translation, where the pivotal issue is to learn and transfer target-domain style patterns onto source-domain content images. This paper handles the problem of translating real pictures into traditional Chinese ink-wash paintings, i.e., Chinese ink-wash painting style transfer. Though a wide range of I2I models tackle this problem, a notable challenge is that the content details of the source image could be easily erased or corrupted due to the transfer of ink-wash style elements. To remedy this issue, we propose to incorporate saliency detection into the unpaired I2I framework to regularize image content, where the detected saliency map is utilized from two aspects: (\romannumeral1) we propose saliency IOU (SIOU) loss to explicitly regularize object content structure by enforcing saliency consistency before and after image stylization; (\romannumeral2) we propose saliency adaptive normalization (SANorm) which implicitly enhances object structure integrity of the generated paintings by dynamically injecting image saliency information into the generator to guide stylization process. Besides, we also propose saliency attended discriminator which harnesses image saliency information to focus generative adversarial attention onto the drawn objects, contributing to generating more vivid and delicate brush strokes and ink-wash textures. Extensive qualitative and quantitative experiments demonstrate superiority of our approach over related advanced image stylization methods in both GAN and diffusion model paradigms. </p>
<blockquote>
<p>ä»è·¨åŸŸå›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘çš„è§’åº¦æ¥çœ‹ï¼Œæœ€è¿‘çš„é£æ ¼è¿ç§»é—®é¢˜åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»ç„¶å—åˆ°ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„ä¸»å¯¼ã€‚å…¶ä¸­çš„å…³é”®é—®é¢˜æ˜¯å­¦ä¹ å’Œå°†ç›®æ ‡åŸŸçš„æ ·å¼æ¨¡å¼è½¬ç§»åˆ°æºåŸŸçš„å†…å®¹å›¾åƒä¸Šã€‚æœ¬æ–‡å¤„ç†å°†çœŸå®å›¾ç‰‡ç¿»è¯‘æˆä¼ ç»Ÿæ°´å¢¨ç”»çš„é—®é¢˜ï¼Œå³æ°´å¢¨ç”»é£æ ¼è½¬ç§»ã€‚å°½ç®¡æœ‰è®¸å¤šI2Iæ¨¡å‹å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œä½†ä¸€ä¸ªæ˜¾è‘—çš„æŒ‘æˆ˜æ˜¯æºå›¾åƒçš„å†…å®¹ç»†èŠ‚åœ¨è½¬ç§»æ°´å¢¨é£æ ¼å…ƒç´ æ—¶å¯èƒ½ä¼šè¢«è½»æ˜“æ“¦é™¤æˆ–ç ´åã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†æ˜¾è‘—æ€§æ£€æµ‹èå…¥åˆ°éé…å¯¹çš„I2Iæ¡†æ¶ä¸­æ¥è§„èŒƒå›¾åƒå†…å®¹ï¼Œä»ä¸¤ä¸ªæ–¹é¢åˆ©ç”¨æ£€æµ‹åˆ°çš„æ˜¾è‘—æ€§å›¾ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºæ˜¾è‘—æ€§IOUï¼ˆSIOUï¼‰æŸå¤±ï¼Œé€šè¿‡å¼ºåˆ¶æ˜¾è‘—æ€§ä¸€è‡´æ€§æ¥æ˜¾å¼åœ°è§„èŒƒå¯¹è±¡å†…å®¹ç»“æ„ï¼Œåœ¨å›¾åƒé£æ ¼åŒ–ä¹‹å‰å’Œä¹‹åï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºæ˜¾è‘—æ€§è‡ªé€‚åº”å½’ä¸€åŒ–ï¼ˆSANormï¼‰ï¼Œé€šè¿‡åŠ¨æ€å°†å›¾åƒæ˜¾è‘—æ€§ä¿¡æ¯æ³¨å…¥ç”Ÿæˆå™¨æ¥éšå¼åœ°å¢å¼ºç”Ÿæˆç”»ä½œçš„å¯¹è±¡ç»“æ„å®Œæ•´æ€§ï¼Œä»¥æŒ‡å¯¼é£æ ¼åŒ–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ˜¾è‘—æ€§å…³æ³¨é‰´åˆ«å™¨ï¼Œå®ƒåˆ©ç”¨å›¾åƒæ˜¾è‘—æ€§ä¿¡æ¯å°†ç”Ÿæˆå¯¹æŠ—æ³¨æ„åŠ›é›†ä¸­åœ¨ç»˜åˆ¶å¯¹è±¡ä¸Šï¼Œæœ‰åŠ©äºç”Ÿæˆæ›´ç”ŸåŠ¨ã€æ›´ç²¾ç»†çš„ç¬”è§¦å’Œæ°´å¢¨çº¹ç†ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨GANå’Œæ‰©æ•£æ¨¡å‹èŒƒå¼ä¸­å‡ä¼˜äºç›¸å…³çš„é«˜çº§å›¾åƒé£æ ¼åŒ–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15743v3">PDF</a> Pattern Recognition, Volume 162, June 2025, 111344</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„è·¨åŸŸå›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘ä¸­çš„é£æ ¼è½¬æ¢é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å°†çœŸå®å›¾ç‰‡è½¬æ¢ä¸ºä¼ ç»Ÿæ°´å¢¨ç”»æ—¶çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æºå›¾åƒå†…å®¹ç»†èŠ‚åœ¨è½¬æ¢æ°´å¢¨é£æ ¼å…ƒç´ æ—¶å®¹æ˜“ä¸¢å¤±æˆ–æŸåçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç»“åˆæ˜¾è‘—æ€§æ£€æµ‹çš„æ— é…å¯¹I2Iæ¡†æ¶æ¥è§„èŒƒå›¾åƒå†…å®¹ã€‚é€šè¿‡å¼•å…¥æ˜¾è‘—æ€§æ£€æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†æ˜¾è‘—æ€§IOUï¼ˆSIOUï¼‰æŸå¤±å’Œæ˜¾è‘—æ€§è‡ªé€‚åº”å½’ä¸€åŒ–ï¼ˆSANormï¼‰æ–¹æ³•ï¼Œåˆ†åˆ«ä»æ˜¾æ€§å’Œéšæ€§ä¸¤æ–¹é¢å¢å¼ºå¯¹è±¡å†…å®¹çš„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ˜¾è‘—æ€§å…³æ³¨é‰´åˆ«å™¨ï¼Œåˆ©ç”¨å›¾åƒæ˜¾è‘—æ€§ä¿¡æ¯å°†ç”Ÿæˆå¯¹æŠ—çš„æ³¨æ„åŠ›é›†ä¸­åœ¨ç»˜åˆ¶å¯¹è±¡ä¸Šï¼Œä»è€Œç”Ÿæˆæ›´ç”ŸåŠ¨ã€ç²¾ç»†çš„ç¬”è§¦å’Œæ°´å¢¨çº¹ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨è·¨åŸŸå›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘ä¸­çš„é£æ ¼è½¬æ¢é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯çœŸå®å›¾ç‰‡è½¬æ¢ä¸ºä¼ ç»Ÿæ°´å¢¨ç”»ã€‚</li>
<li>åœ¨è½¬æ¢è¿‡ç¨‹ä¸­ï¼Œæºå›¾åƒçš„å†…å®¹ç»†èŠ‚å®¹æ˜“ä¸¢å¤±æˆ–æŸåæ˜¯ä¸€ä¸ªæ˜¾è‘—æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥äº†æ˜¾è‘—æ€§æ£€æµ‹æ¥è§„èŒƒå›¾åƒå†…å®¹ï¼Œä»æ˜¾æ€§å’Œéšæ€§ä¸¤ä¸ªæ–¹é¢å¢å¼ºå¯¹è±¡å†…å®¹çš„å®Œæ•´æ€§ã€‚</li>
<li>æå‡ºäº†æ˜¾è‘—æ€§IOUï¼ˆSIOUï¼‰æŸå¤±å’Œæ˜¾è‘—æ€§è‡ªé€‚åº”å½’ä¸€åŒ–ï¼ˆSANormï¼‰æ–¹æ³•ï¼Œåˆ†åˆ«ç”¨äºæ˜¾å¼å’Œéšå¼åœ°ä¿æŠ¤å›¾åƒå†…å®¹ç»“æ„ã€‚</li>
<li>è¿˜æå‡ºäº†æ˜¾è‘—æ€§å…³æ³¨é‰´åˆ«å™¨ï¼Œåˆ©ç”¨å›¾åƒæ˜¾è‘—æ€§ä¿¡æ¯æ¥ç”Ÿæˆæ›´ç”ŸåŠ¨ã€ç²¾ç»†çš„ç¬”è§¦å’Œæ°´å¢¨çº¹ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨GANå’Œæ‰©æ•£æ¨¡å‹èŒƒå¼ä¸­å‡ä¼˜äºç›¸å…³çš„é«˜çº§å›¾åƒé£æ ¼åŒ–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5799f2c099b33b9b160635053ff8a3aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286913&auth_key=1762286913-0-0-c10321276faaa0b36f83f930b83c267a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfa3d8c03770e2a1e3d63df40c81942f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286920&auth_key=1762286920-0-0-f5619602153104ccf9949b546ce66008&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fdb1867304a1d698ec5a97aee4cd1b34~resize:0:q75.jpg?source=1f5c5e47&expiration=1762286926&auth_key=1762286926-0-0-c11bc693ffa4ca14366502a139fe7fa7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-79e6e73f8c70116953a199dc9cd38815~resize:0:q75.jpg?source=1f5c5e47&expiration=1762287694&auth_key=1762287694-0-0-b1d9885b9f598e763706ba2566b0f7b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  EgoExo-Con Exploring View-Invariant Video Temporal Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-0e51a2cddeb30fd7f0f218dc4fac60c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762284835&auth_key=1762284835-0-0-32957cf2b16cee57ef50088688b64eb3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Patient-Centered Summarization Framework for AI Clinical Summarization   A Mixed-Methods Design
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
