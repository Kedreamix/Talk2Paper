<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  SAGS Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical   Endoscopic Reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-73ffebfdd9b9c86bdd553fc0d74142cb')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="SAGS-Self-Adaptive-Alias-Free-Gaussian-Splatting-for-Dynamic-Surgical-Endoscopic-Reconstruction"><a href="#SAGS-Self-Adaptive-Alias-Free-Gaussian-Splatting-for-Dynamic-Surgical-Endoscopic-Reconstruction" class="headerlink" title="SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical   Endoscopic Reconstruction"></a>SAGS: Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical   Endoscopic Reconstruction</h2><p><strong>Authors:Wenfeng Huang, Xiangyun Liao, Yinling Qian, Hao Liu, Yongming Yang, Wenjing Jia, Qiong Wang</strong></p>
<p>Surgical reconstruction of dynamic tissues from endoscopic videos is a crucial technology in robot-assisted surgery. The development of Neural Radiance Fields (NeRFs) has greatly advanced deformable tissue reconstruction, achieving high-quality results from video and image sequences. However, reconstructing deformable endoscopic scenes remains challenging due to aliasing and artifacts caused by tissue movement, which can significantly degrade visualization quality. The introduction of 3D Gaussian Splatting (3DGS) has improved reconstruction efficiency by enabling a faster rendering pipeline. Nevertheless, existing 3DGS methods often prioritize rendering speed while neglecting these critical issues. To address these challenges, we propose SAGS, a self-adaptive alias-free Gaussian splatting framework. We introduce an attention-driven, dynamically weighted 4D deformation decoder, leveraging 3D smoothing filters and 2D Mip filters to mitigate artifacts in deformable tissue reconstruction and better capture the fine details of tissue movement. Experimental results on two public benchmarks, EndoNeRF and SCARED, demonstrate that our method achieves superior performance in all metrics of PSNR, SSIM, and LPIPS compared to the state of the art while also delivering better visualization quality. </p>
<blockquote>
<p>æ‰‹æœ¯å†…çª¥é•œè§†é¢‘ä¸­çš„åŠ¨æ€ç»„ç»‡é‡å»ºæ˜¯æœºå™¨äººè¾…åŠ©æ‰‹æœ¯ä¸­çš„ä¸€é¡¹å…³é”®æŠ€æœ¯ã€‚ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰çš„å‘å±•æå¤§åœ°æ¨åŠ¨äº†å¯å˜å½¢ç»„ç»‡çš„é‡å»ºï¼Œèƒ½å¤Ÿä»è§†é¢‘å’Œå›¾åƒåºåˆ—ä¸­è·å¾—é«˜è´¨é‡çš„ç»“æœã€‚ç„¶è€Œï¼Œç”±äºç»„ç»‡è¿åŠ¨å¯¼è‡´çš„æ··å å’Œä¼ªå½±ï¼Œé‡å»ºå¯å˜å½¢çš„å†…çª¥é•œåœºæ™¯ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™å¯èƒ½ä¼šä¸¥é‡é™ä½å¯è§†åŒ–è´¨é‡ã€‚ä¸‰ç»´é«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰çš„å¼•å…¥æé«˜äº†é‡å»ºæ•ˆç‡ï¼Œé€šè¿‡å®ç°æ›´å¿«çš„æ¸²æŸ“æµç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸‰ç»´é«˜æ–¯è´´å›¾æ–¹æ³•å¾€å¾€ä¼˜å…ˆå…³æ³¨æ¸²æŸ“é€Ÿåº¦è€Œå¿½ç•¥äº†è¿™äº›å…³é”®é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SAGSï¼Œä¸€ç§è‡ªé€‚åº”æ— æ··å é«˜æ–¯è´´å›¾æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›é©±åŠ¨ã€åŠ¨æ€åŠ æƒçš„å››ç»´å˜å½¢è§£ç å™¨ï¼Œåˆ©ç”¨ä¸‰ç»´å¹³æ»‘æ»¤æ³¢å™¨å’ŒäºŒç»´Mipæ»¤æ³¢å™¨æ¥å‡å°‘å¯å˜å½¢ç»„ç»‡é‡å»ºä¸­çš„ä¼ªå½±ï¼Œå¹¶æ›´å¥½åœ°æ•æ‰ç»„ç»‡è¿åŠ¨çš„ç»†èŠ‚ã€‚åœ¨EndoNeRFå’ŒSCAREDä¸¤ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œå±€éƒ¨æ„ŸçŸ¥å›¾åƒç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰ç­‰å„é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„å¯è§†åŒ–è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27318v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>NeRFæŠ€æœ¯ç»“åˆå†…çª¥é•œè§†é¢‘åœ¨æœºå™¨äººè¾…åŠ©æ‰‹æœ¯ä¸­ç”¨äºåŠ¨æ€ç»„ç»‡é‡å»ºå…·æœ‰é‡è¦æ„ä¹‰ã€‚ä¸ºåº”å¯¹å› ç»„ç»‡è¿åŠ¨å¯¼è‡´çš„æ··å å’Œä¼ªå½±é—®é¢˜ï¼Œæå‡ºä¸€ç§è‡ªé€‚åº”æ— æ··å é«˜æ–¯æ‘Šé“ºæ¡†æ¶SAGSã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§å…³æ³¨ç»†èŠ‚çš„åŠ¨æ€åŠ æƒ4Då˜å½¢è§£ç å™¨ï¼Œé€šè¿‡åˆ©ç”¨3Då¹³æ»‘æ»¤æ³¢å™¨å’Œ2D Mipæ»¤æ³¢å™¨æ¥å‡å°‘ä¼ªå½±å¹¶æ•æ‰ç»„ç»‡è¿åŠ¨çš„ç»†å¾®ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨PSNRã€SSIMå’ŒLPIPSç­‰æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡è¡¨ç°ä¼˜è¶Šï¼Œå¯è§†åŒ–è´¨é‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFæŠ€æœ¯å¯¹äºæœºå™¨äººè¾…åŠ©æ‰‹æœ¯ä¸­çš„åŠ¨æ€ç»„ç»‡é‡å»ºè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰é‡å»ºæ–¹æ³•åœ¨å¤„ç†å†…çª¥é•œè§†é¢‘æ—¶é¢ä¸´æ··å å’Œä¼ªå½±é—®é¢˜ã€‚</li>
<li>SAGSæ¡†æ¶é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ— æ··å é«˜æ–¯æ‘Šé“ºæŠ€æœ¯è§£å†³äº†è¿™äº›é—®é¢˜ã€‚</li>
<li>SAGSä½¿ç”¨åŠ¨æ€åŠ æƒçš„4Då˜å½¢è§£ç å™¨ï¼Œå…³æ³¨ç»„ç»‡è¿åŠ¨çš„ç»†å¾®ç»†èŠ‚ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆ3Då¹³æ»‘æ»¤æ³¢å™¨å’Œ2D Mipæ»¤æ³¢å™¨æ¥ä¼˜åŒ–é‡å»ºæ•ˆæœã€‚</li>
<li>åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSAGSåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e32fc98ca9358266ec21e43526abbf8" align="middle">
<img src="https://picx.zhimg.com/v2-33665b4c4d0d2048e168a6bdf6ad21bf" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Impact-of-AlN-buffer-thickness-on-electrical-and-thermal-characteristics-of-AlGaN-GaN-AlN-HEMTs"><a href="#Impact-of-AlN-buffer-thickness-on-electrical-and-thermal-characteristics-of-AlGaN-GaN-AlN-HEMTs" class="headerlink" title="Impact of AlN buffer thickness on electrical and thermal characteristics   of AlGaN&#x2F;GaN&#x2F;AlN HEMTs"></a>Impact of AlN buffer thickness on electrical and thermal characteristics   of AlGaN&#x2F;GaN&#x2F;AlN HEMTs</h2><p><strong>Authors:Minho Kim, Dat Q. Tran, Plamen P. Paskov, U. Choi, O. Nam, Vanya Darakchieva</strong></p>
<p>We investigate the influence of AlN buffer thickness on the structural, electrical, and thermal properties of AlGaN&#x2F;GaN high-electron mobility transistors (HEMTs) grown on semi-insulating SiC substrates by metal-organic chemical vapor deposition. X-ray diffraction and atomic force microscopy reveal that while thin AlN layers (120 nm) exhibit compressive strain and smooth step-flow surfaces, thicker single-layer buffers (550 nm) develop tensile strain and increased surface roughness. Multi-layer buffer structures up to 2 {\mu}m alleviate strain and maintain surface integrity. Low-temperature Hall measurements confirm that electron mobility decreases with increasing interface roughness, with the highest mobility observed in the structure with a thin AlN buffer. Transient thermoreflectance measurements show that thermal conductivity (ThC) of the AlN buffer increases with the thickness, reaching 188 W&#x2F;m.K at 300 K for the 2 {\mu}m buffer layer, which is approximately 60% of the bulk AlN ThC value. These results highlight the importance of optimizing AlN buffer design to balance strain relaxation, thermal management, and carrier transport for high-performance GaN-based HEMTs. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†AlNç¼“å†²å±‚åšåº¦å¯¹ç”Ÿé•¿åœ¨åŠç»ç¼˜SiCè¡¬åº•ä¸Šçš„AlGaN&#x2F;GaNé«˜ç”µå­è¿ç§»ç‡æ™¶ä½“ç®¡ï¼ˆHEMTï¼‰çš„ç»“æ„ã€ç”µå­¦å’Œçƒ­å­¦æ€§è´¨çš„å½±å“ã€‚é€šè¿‡Xå°„çº¿è¡å°„å’ŒåŸå­åŠ›æ˜¾å¾®é•œè§‚å¯Ÿå‘ç°ï¼Œè¾ƒè–„çš„AlNå±‚ï¼ˆ120çº³ç±³ï¼‰è¡¨ç°å‡ºå‹ç¼©åº”å˜å’Œå…‰æ»‘çš„è¡¨é¢å½¢æ€ï¼Œè€Œè¾ƒåšçš„å•å±‚ç¼“å†²å±‚ï¼ˆ550çº³ç±³ï¼‰åˆ™è¡¨ç°å‡ºæ‹‰ä¼¸åº”å˜å’Œæ›´é«˜çš„è¡¨é¢ç²—ç³™åº¦ã€‚å¤šå±‚ç¼“å†²ç»“æ„ï¼ˆé«˜è¾¾2å¾®ç±³ï¼‰å¯ä»¥ç¼“è§£åº”å˜å¹¶ä¿æŒè¡¨é¢å®Œæ•´æ€§ã€‚ä½æ¸©éœå°”æµ‹é‡ç»“æœè¡¨æ˜ï¼Œéšç€ç•Œé¢ç²—ç³™åº¦çš„å¢åŠ ï¼Œç”µå­è¿ç§»ç‡é™ä½ï¼Œåœ¨å…·æœ‰è–„AlNç¼“å†²å±‚çš„ç»“æ„ä¸­è§‚å¯Ÿåˆ°æœ€é«˜çš„è¿ç§»ç‡ã€‚ç¬æ€åå°„æµ‹é‡è¡¨æ˜ï¼ŒAlNç¼“å†²å±‚çš„å¯¼çƒ­ç³»æ•°éšåšåº¦å¢åŠ è€Œå¢åŠ ï¼Œåœ¨300Kæ—¶ï¼Œ2å¾®ç±³ç¼“å†²å±‚çš„å¯¼çƒ­ç³»æ•°è¾¾åˆ°188 W&#x2F;mÂ·Kï¼Œçº¦ä¸ºå—çŠ¶AlNå¯¼çƒ­ç³»æ•°çš„60%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä¼˜åŒ–AlNç¼“å†²å±‚è®¾è®¡çš„é‡è¦æ€§ï¼Œä»¥å®ç°åº”å˜æ¾å¼›ã€çƒ­ç®¡ç†å’Œè½½æµå­ä¼ è¾“ä¹‹é—´çš„å¹³è¡¡ï¼Œä»è€Œåˆ¶é€ å‡ºé«˜æ€§èƒ½çš„GaNåŸºHEMTã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26244v1">PDF</a> 5 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†AlNç¼“å†²å±‚åšåº¦å¯¹AlGaN&#x2F;GaNé«˜ç”µå­è¿ç§»ç‡æ™¶ä½“ç®¡ï¼ˆHEMTsï¼‰çš„ç»“æ„ã€ç”µå­¦å’Œçƒ­å­¦æ€§è´¨çš„å½±å“ã€‚é€šè¿‡é‡‘å±æœ‰æœºåŒ–åˆç‰©æ°”ç›¸æ²‰ç§¯åœ¨åŠå¯¼ä½“ç»ç¼˜SiCè¡¬åº•ä¸Šç”Ÿé•¿æ™¶ä½“ç®¡ã€‚ç ”ç©¶å‘ç°ï¼Œè–„AlNå±‚ï¼ˆ120nmï¼‰è¡¨ç°å‡ºå‹ç¼©åº”å˜å’Œå…‰æ»‘çš„è¡¨é¢å½¢æ€ï¼Œè€Œè¾ƒåšçš„å•å±‚ç¼“å†²å±‚ï¼ˆ550nmï¼‰åˆ™è¡¨ç°å‡ºæ‹‰ä¼¸åº”å˜å’Œå¢åŠ çš„è¡¨é¢ç²—ç³™åº¦ã€‚å¤šå±‚ç¼“å†²ç»“æ„ï¼ˆæœ€é«˜è¾¾2Î¼mï¼‰å¯ä»¥ç¼“è§£åº”å˜å¹¶ä¿æŒè¡¨é¢å®Œæ•´æ€§ã€‚ä½æ¸©éœå°”æµ‹é‡è¡¨æ˜ï¼Œç”µå­è¿ç§»ç‡éšç•Œé¢ç²—ç³™åº¦çš„å¢åŠ è€Œé™ä½ï¼Œåœ¨å…·æœ‰è–„AlNç¼“å†²å±‚çš„ç»“æ„ä¸­è§‚å¯Ÿåˆ°æœ€é«˜è¿ç§»ç‡ã€‚ç¬æ€çƒ­åå°„æµ‹é‡æ˜¾ç¤ºï¼ŒAlNç¼“å†²å±‚çš„çƒ­å¯¼ç‡éšåšåº¦å¢åŠ è€Œæé«˜ï¼Œ2Î¼mç¼“å†²å±‚çš„çƒ­å¯¼ç‡åœ¨300Kæ—¶è¾¾åˆ°188W&#x2F;mÂ·Kï¼Œçº¦ä¸ºAlNä½“ææ–™çƒ­å¯¼ç‡çš„60%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä¼˜åŒ–AlNç¼“å†²å±‚è®¾è®¡çš„é‡è¦æ€§ï¼Œä»¥å®ç°åº”å˜æ¾å¼›ã€çƒ­ç®¡ç†å’Œè½½æµå­ä¼ è¾“ä¹‹é—´çš„å¹³è¡¡ï¼Œä»¥æé«˜åŸºäºGaNçš„HEMTsçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AlNç¼“å†²å±‚åšåº¦å¯¹AlGaN&#x2F;GaN HEMTçš„ç»“æ„ã€ç”µå­¦å’Œçƒ­å­¦æ€§è´¨æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>è–„AlNå±‚è¡¨ç°å‡ºå‹ç¼©åº”å˜å’Œå…‰æ»‘è¡¨é¢ï¼Œè€Œåšå±‚åˆ™å‡ºç°æ‹‰ä¼¸åº”å˜å’Œè¡¨é¢ç²—ç³™ã€‚</li>
<li>å¤šå±‚ç¼“å†²ç»“æ„èƒ½å¤Ÿç¼“è§£åº”å˜å¹¶ä¿æŒè¡¨é¢å®Œæ•´æ€§ã€‚</li>
<li>ç”µå­è¿ç§»ç‡éšç•Œé¢ç²—ç³™åº¦å¢åŠ è€Œé™ä½ï¼Œåœ¨è–„AlNç¼“å†²å±‚ç»“æ„ä¸­è¾¾åˆ°æœ€é«˜ã€‚</li>
<li>AlNç¼“å†²å±‚çš„çƒ­å¯¼ç‡éšåšåº¦å¢åŠ è€Œæé«˜ã€‚</li>
<li>2Î¼mç¼“å†²å±‚çš„çƒ­å¯¼ç‡æ¥è¿‘AlNä½“ææ–™çš„60%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34aa76cb3bd753c194b9d1b6938e8252" align="middle">
<img src="https://picx.zhimg.com/v2-cb731a8a297b6085ed78377f73cbd0c0" align="middle">
<img src="https://picx.zhimg.com/v2-f22792ec1dd909f5c4946b154a449474" align="middle">
<img src="https://picx.zhimg.com/v2-5598423d9d8b2a34f4f9e1182e451322" align="middle">
<img src="https://picx.zhimg.com/v2-f79e8246738d55735095ad303a61b356" align="middle">
<img src="https://picx.zhimg.com/v2-3bb3561e228a407981cacc8c89ebb99d" align="middle">
<img src="https://picx.zhimg.com/v2-76c6d95f9778a27f55ebf4cd5b06b2ec" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BOLT-GAN-Bayes-Optimal-Loss-for-Stable-GAN-Training"><a href="#BOLT-GAN-Bayes-Optimal-Loss-for-Stable-GAN-Training" class="headerlink" title="BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training"></a>BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training</h2><p><strong>Authors:Mohammadreza Tavasoli Naeini, Ali Bereyhi, Morteza Noshad, Ben Liang, Alfred O. Hero III</strong></p>
<p>We introduce BOLT-GAN, a simple yet effective modification of the WGAN framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a different metric distance than the Earth Mover (Wasserstein) distance and achieves better training stability. Empirical evaluations on four standard image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60% lower Frechet Inception Distance (FID). Our results suggest that BOLT is a broadly applicable principle for enhancing GAN training. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†BOLT-GANï¼Œå®ƒæ˜¯WGANæ¡†æ¶çš„ç®€å•è€Œæœ‰æ•ˆçš„ä¿®æ”¹ï¼Œçµæ„Ÿæ¥æºäºè´å¶æ–¯æœ€ä¼˜å­¦ä¹ é˜ˆå€¼ï¼ˆBOLTï¼‰ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨Lipschitzè¿ç»­é‰´åˆ«å™¨ï¼ŒBOLT-GANéšå¼åœ°æœ€å°åŒ–äº†ä¸åœ°çƒç§»åŠ¨ï¼ˆWassersteinï¼‰è·ç¦»ä¸åŒçš„åº¦é‡è·ç¦»ï¼Œå¹¶å®ç°äº†æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§ã€‚åœ¨å››ä¸ªæ ‡å‡†çš„å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆCIFAR-10ã€CelebA-64ã€LSUN Bedroom-64å’ŒLSUN Church-64ï¼‰ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒBOLT-GANæŒç»­ä¼˜äºWGANï¼Œé™ä½äº†10-60%çš„Frechet Inception Distanceï¼ˆFIDï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒBOLTæ˜¯ä¸€ä¸ªå¹¿æ³›åº”ç”¨äºæé«˜GANè®­ç»ƒçš„åŸåˆ™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25609v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè´å¶æ–¯æœ€ä¼˜å­¦ä¹ é˜ˆå€¼ï¼ˆBOLTï¼‰å¯å‘çš„WGANæ¡†æ¶çš„æ”¹è¿›ç‰ˆæœ¬â€”â€”BOLT-GANã€‚é€šè¿‡å¼•å…¥Lipschitzè¿ç»­åˆ¤åˆ«å™¨ï¼ŒBOLT-GANéšå¼åœ°æœ€å°åŒ–äº†ä¸€ç§ä¸åŒäºåœ°çƒç§»åŠ¨ï¼ˆWassersteinï¼‰è·ç¦»çš„åº¦é‡è·ç¦»ï¼Œå¹¶å®ç°äº†æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§ã€‚åœ¨å››ä¸ªæ ‡å‡†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒBOLT-GANåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºWGANï¼Œé™ä½äº†10%~60%çš„Frechet Inception Distanceï¼ˆFIDï¼‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒBOLTæ˜¯å¹¿æ³›åº”ç”¨äºæé«˜GANè®­ç»ƒæ•ˆæœçš„åŸåˆ™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BOLT-GANæ˜¯åŸºäºè´å¶æ–¯æœ€ä¼˜å­¦ä¹ é˜ˆå€¼ï¼ˆBOLTï¼‰å¯å‘çš„WGANæ¡†æ¶çš„æ”¹è¿›ç‰ˆæœ¬ã€‚</li>
<li>é€šè¿‡å¼•å…¥Lipschitzè¿ç»­åˆ¤åˆ«å™¨ï¼ŒBOLT-GANéšå¼åœ°æœ€å°åŒ–äº†ä¸€ç§ä¸åŒäºåœ°çƒç§»åŠ¨è·ç¦»çš„åº¦é‡è·ç¦»ã€‚</li>
<li>BOLT-GANå®ç°äº†æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>åœ¨å››ä¸ªæ ‡å‡†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šï¼ŒBOLT-GANçš„æ€§èƒ½æŒç»­ä¼˜äºWGANã€‚</li>
<li>BOLT-GANé™ä½äº†10%~60%çš„Frechet Inception Distanceï¼ˆFIDï¼‰ã€‚</li>
<li>ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒBOLT-GANåœ¨å„ç§å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6593ed754a85e4f2771d40b3e67008e3" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OpenHype-Hyperbolic-Embeddings-for-Hierarchical-Open-Vocabulary-Radiance-Fields"><a href="#OpenHype-Hyperbolic-Embeddings-for-Hierarchical-Open-Vocabulary-Radiance-Fields" class="headerlink" title="OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary   Radiance Fields"></a>OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary   Radiance Fields</h2><p><strong>Authors:Lisa Weijler, Sebastian Koch, Fabio Poiesi, Timo Ropinski, Pedro Hermosilla</strong></p>
<p>Modeling the inherent hierarchical structure of 3D objects and 3D scenes is highly desirable, as it enables a more holistic understanding of environments for autonomous agents. Accomplishing this with implicit representations, such as Neural Radiance Fields, remains an unexplored challenge. Existing methods that explicitly model hierarchical structures often face significant limitations: they either require multiple rendering passes to capture embeddings at different levels of granularity, significantly increasing inference time, or rely on predefined, closed-set discrete hierarchies that generalize poorly to the diverse and nuanced structures encountered by agents in the real world. To address these challenges, we propose OpenHype, a novel approach that represents scene hierarchies using a continuous hyperbolic latent space. By leveraging the properties of hyperbolic geometry, OpenHype naturally encodes multi-scale relationships and enables smooth traversal of hierarchies through geodesic paths in latent space. Our method outperforms state-of-the-art approaches on standard benchmarks, demonstrating superior efficiency and adaptability in 3D scene understanding. </p>
<blockquote>
<p>å¯¹ä¸‰ç»´ç‰©ä½“å’Œä¸‰ç»´åœºæ™¯è¿›è¡Œå»ºæ¨¡éå¸¸ç†æƒ³ï¼Œå› ä¸ºå®ƒä¸ºè‡ªä¸»æ™ºèƒ½ä½“æä¾›äº†æ›´å…¨é¢çš„ç¯å¢ƒç†è§£ã€‚ç„¶è€Œï¼Œä½¿ç”¨éšå¼è¡¨ç¤ºï¼ˆå¦‚ç¥ç»è¾å°„åœºï¼‰æ¥å®ç°è¿™ä¸€ç‚¹ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«æ¢ç´¢çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ˜¾å¼å»ºæ¨¡å±‚æ¬¡ç»“æ„çš„æ–¹æ³•å¸¸å¸¸é¢ä¸´é‡å¤§å±€é™æ€§ï¼šå®ƒä»¬è¦ä¹ˆéœ€è¦å¤šæ¬¡æ¸²æŸ“ä»¥æ•è·ä¸åŒç²’åº¦çº§åˆ«çš„åµŒå…¥ï¼Œä»è€Œæ˜¾è‘—å¢åŠ æ¨ç†æ—¶é—´ï¼›è¦ä¹ˆä¾èµ–äºé¢„è®¾çš„ã€å°é—­çš„ç¦»æ•£å±‚æ¬¡ç»“æ„ï¼Œè¿™åœ¨æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œä¸­é‡åˆ°çš„å„ç§å¤šæ ·åŒ–å’Œç»†å¾®ç»“æ„ä¸­è¡¨ç°è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OpenHypeè¿™ä¸€æ–°æ–¹æ³•ï¼Œä½¿ç”¨è¿ç»­çš„åŒæ›²æ½œç©ºé—´æ¥è¡¨ç¤ºåœºæ™¯å±‚æ¬¡ç»“æ„ã€‚é€šè¿‡åˆ©ç”¨åŒæ›²å‡ ä½•çš„ç‰¹æ€§ï¼ŒOpenHypeè‡ªç„¶åœ°ç¼–ç äº†å¤šå°ºåº¦å…³ç³»ï¼Œå¹¶é€šè¿‡æ½œç©ºé—´ä¸­çš„æµ‹åœ°çº¿è·¯å¾„å®ç°äº†å±‚æ¬¡ç»“æ„çš„å¹³æ»‘éå†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸‰ç»´åœºæ™¯ç†è§£ä¸­çš„ä¼˜è¶Šæ•ˆç‡å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21441v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†OpenHypeæ–¹æ³•ï¼Œåˆ©ç”¨è¿ç»­çš„åŒæ›²æ½œåœ¨ç©ºé—´è¡¨ç¤ºåœºæ™¯å±‚æ¬¡ç»“æ„ï¼Œé€šè¿‡åŒæ›²å‡ ä½•çš„å±æ€§è‡ªç„¶åœ°ç¼–ç å¤šå°ºåº¦å…³ç³»ï¼Œå¹¶é€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„æµ‹åœ°çº¿è·¯å¾„å®ç°å¹³æ»‘éå†å±‚æ¬¡ç»“æ„ã€‚è¯¥æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨3Dåœºæ™¯ç†è§£ä¸­çš„é«˜æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenHypeæ–¹æ³•åˆ©ç”¨è¿ç»­çš„åŒæ›²æ½œåœ¨ç©ºé—´è¿›è¡Œåœºæ™¯å±‚æ¬¡å»ºæ¨¡ã€‚</li>
<li>å¤šå°ºåº¦å…³ç³»é€šè¿‡åŒæ›²å‡ ä½•çš„è‡ªç„¶å±æ€§è¿›è¡Œç¼–ç ã€‚</li>
<li>é€šè¿‡æ½œåœ¨ç©ºé—´ä¸­çš„æµ‹åœ°çº¿è·¯å¾„å®ç°å±‚æ¬¡ç»“æ„çš„å¹³æ»‘éå†ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOpenHypeåœ¨3Dåœºæ™¯ç†è§£æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>OpenHypeæ–¹æ³•æé«˜äº†æ•ˆç‡å¹¶å¢å¼ºäº†é€‚åº”æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†ç°å®ä¸–ç•Œä¸­çš„å¤šæ ·åŒ–å’Œç»†å¾®å±‚æ¬¡ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7d7cf8e6befc946501b6d37cc81cea5" align="middle">
<img src="https://picx.zhimg.com/v2-6640ab9d8805008341875faa9261f97f" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GauSSmart-Enhanced-3D-Reconstruction-through-2D-Foundation-Models-and-Geometric-Filtering"><a href="#GauSSmart-Enhanced-3D-Reconstruction-through-2D-Foundation-Models-and-Geometric-Filtering" class="headerlink" title="GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and   Geometric Filtering"></a>GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and   Geometric Filtering</h2><p><strong>Authors:Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang</strong></p>
<p>Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.   In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details.   We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone. </p>
<blockquote>
<p>åœºæ™¯é‡å»ºå·²æˆä¸ºè®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œé«˜æ–¯æ‹¼è´´ï¼ˆGaussian Splattingï¼‰ç­‰æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡é«˜æ–¯æ‹¼è´´åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨ç¨€ç–è¦†ç›–çš„åŒºåŸŸæ•æ‰ç²¾ç»†ç»†èŠ‚æˆ–ä¿æŒçœŸå®æ€§æ–¹é¢å¾€å¾€é‡åˆ°å›°éš¾ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¨€ç–çš„3Dè®­ç»ƒæ•°æ®å›ºæœ‰çš„å±€é™æ€§æ‰€è‡´ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•GauSSmartï¼Œå®ƒæœ‰æ•ˆåœ°æ¡¥æ¥äº†2DåŸºç¡€æ¨¡å‹å’Œ3Dé«˜æ–¯æ‹¼è´´é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†æˆç†Ÿçš„2Dè®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬å‡¸è¿‡æ»¤å’Œæ¥è‡ªåŸºç¡€æ¨¡å‹ï¼ˆå¦‚DINOï¼‰çš„è¯­ä¹‰ç‰¹å¾ç›‘ç£ï¼Œä»¥å¢å¼ºåŸºäºé«˜æ–¯çš„åœºæ™¯é‡å»ºã€‚é€šè¿‡åˆ©ç”¨2Dåˆ†å‰²å…ˆéªŒå’Œé«˜ç»´ç‰¹å¾åµŒå…¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å¯¼é«˜æ–¯æ‹¼è´´çš„è‡´å¯†åŒ–å’Œç²¾ç»†åŒ–ï¼Œæ”¹è¿›äº†æ¬ ä»£è¡¨åŒºåŸŸçš„è¦†ç›–å¹¶ä¿ç•™äº†å¤æ‚ç»“æ„ç»†èŠ‚ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒGauSSmartåœ¨å¤§å¤šæ•°è¯„ä¼°åœºæ™¯ä¸­å§‹ç»ˆä¼˜äºç°æœ‰é«˜æ–¯æ‹¼è´´ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†æ··åˆ2D-3Dæ–¹æ³•çš„å·¨å¤§æ½œåŠ›ï¼Œçªå‡ºäº†å¦‚ä½•å°†2DåŸºç¡€æ¨¡å‹ä¸3Dé‡å»ºç®¡é“ç›¸ç»“åˆï¼Œä»¥å…‹æœå•ä¸€æ–¹æ³•çš„å›ºæœ‰å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14270v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>NeRFé¢†åŸŸçš„ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºGauSSmartçš„æ··åˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†äºŒç»´åŸºç¡€æ¨¡å‹å’Œä¸‰ç»´é«˜æ–¯å–·ç»˜é‡å»ºæŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³åœºæ™¯é‡å»ºä¸­çš„ç²¾ç»†ç»†èŠ‚æ•æ‰å’Œç¨€ç–åŒºåŸŸçš„çœŸå®æ„Ÿä¿æŒé—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨äºŒç»´åˆ†å‰²å…ˆéªŒå’Œé«˜ç»´ç‰¹å¾åµŒå…¥ï¼ŒGauSSmartèƒ½å¤ŸæŒ‡å¯¼é«˜æ–¯å–·ç»˜çš„å¯†é›†åŒ–å’Œç²¾ç»†åŒ–ï¼Œæé«˜æ¬ ä»£è¡¨åŒºåŸŸçš„è¦†ç›–å¹¶ä¿ç•™ç²¾ç»†çš„ç»“æ„ç»†èŠ‚ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒGauSSmartåœ¨å¤šæ•°è¯„ä¼°åœºæ™¯ä¸­å‡ä¼˜äºç°æœ‰çš„é«˜æ–¯å–·ç»˜æŠ€æœ¯ï¼Œæ˜¾ç¤ºå‡ºæ··åˆäºŒç»´-ä¸‰ç»´æ–¹æ³•çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Neural Radiance Fields (NeRF) å’Œ Gaussian Splatting ç­‰æ–¹æ³•åœ¨åœºæ™¯é‡å»ºä¸­å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>Gaussian Splattingåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨æ•æ‰ç²¾ç»†ç»†èŠ‚å’Œä¿æŒç¨€ç–åŒºåŸŸçš„çœŸå®æ„Ÿæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>GauSSmartæ˜¯ä¸€ç§æ··åˆæ–¹æ³•ï¼Œç»“åˆäº†äºŒç»´åŸºç¡€æ¨¡å‹å’Œä¸‰ç»´é«˜æ–¯å–·ç»˜é‡å»ºæŠ€æœ¯ã€‚</li>
<li>GauSSmartåˆ©ç”¨äºŒç»´åˆ†å‰²å…ˆéªŒå’Œé«˜ç»´ç‰¹å¾åµŒå…¥ï¼ŒæŒ‡å¯¼é«˜æ–¯å–·ç»˜çš„å¯†é›†åŒ–å’Œç²¾ç»†åŒ–ã€‚</li>
<li>GauSSmartåœ¨æ¬ ä»£è¡¨åŒºåŸŸçš„è¦†ç›–å’Œç²¾ç»†ç»“æ„ç»†èŠ‚çš„ä¿ç•™æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒGauSSmartåœ¨å¤šæ•°è¯„ä¼°åœºæ™¯ä¸­ä¼˜äºç°æœ‰é«˜æ–¯å–·ç»˜æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-439c6dafcfefaecd87af483474985b50" align="middle">
<img src="https://picx.zhimg.com/v2-ce10a941529e531a97f079066de5d6d9" align="middle">
<img src="https://picx.zhimg.com/v2-5410135cd5e523629df20d961f072fa8" align="middle">
<img src="https://picx.zhimg.com/v2-cf51a1426d76308cf72aa9b8477ee678" align="middle">
<img src="https://picx.zhimg.com/v2-a091cc0ae86436254de50778d513e4f1" align="middle">
<img src="https://picx.zhimg.com/v2-cfaf455274dbb883378f7edb6c713edf" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NGGAN-Noise-Generation-GAN-Based-on-the-Practical-Measurement-Dataset-for-Narrowband-Powerline-Communications"><a href="#NGGAN-Noise-Generation-GAN-Based-on-the-Practical-Measurement-Dataset-for-Narrowband-Powerline-Communications" class="headerlink" title="NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset   for Narrowband Powerline Communications"></a>NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset   for Narrowband Powerline Communications</h2><p><strong>Authors:Ying-Ren Chien, Po-Heng Chou, You-Jie Peng, Chun-Yuan Huang, Hen-Wai Tsao, Yu Tsao</strong></p>
<p>To effectively process impulse noise for narrowband powerline communications (NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic asynchronous impulsive noise (APIN) is a critical task. However, existing mathematical noise generative models only capture part of the characteristics of noise. In this study, we propose a novel generative adversarial network (GAN) called noise generation GAN (NGGAN) that learns the complicated characteristics of practically measured noise samples for data synthesis. To closely match the statistics of complicated noise over the NB-PLC systems, we measured the NB-PLC noise via the analog coupling and bandpass filtering circuits of a commercial NB-PLC modem to build a realistic dataset. To train NGGAN, we adhere to the following principles: 1) we design the length of input signals that the NGGAN model can fit to facilitate cyclostationary noise generation; 2) the Wasserstein distance is used as a loss function to enhance the similarity between the generated noise and training data; and 3) to measure the similarity performances of GAN-based models based on the mathematical and practically measured datasets, we conduct both quantitative and qualitative analyses. The training datasets include: 1) a piecewise spectral cyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter; and 3) practical measurements from NB-PLC systems. Simulation results demonstrate that the generated noise samples from the proposed NGGAN are highly close to the real noise samples. The principal component analysis (PCA) scatter plots and Fr&#39;echet inception distance (FID) analysis have shown that NGGAN outperforms other GAN-based models by generating noise samples with superior fidelity and higher diversity. </p>
<blockquote>
<p>é’ˆå¯¹çª„å¸¦ç”µåŠ›çº¿é€šä¿¡ï¼ˆNB-PLCï¼‰æ”¶å‘å™¨ä¸­çš„è„‰å†²å™ªå£°å¤„ç†ï¼Œæ•è·éå‘¨æœŸæ€§å¼‚æ­¥è„‰å†²å™ªå£°ï¼ˆAPINï¼‰çš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯æ˜¯ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°å­¦å™ªå£°ç”Ÿæˆæ¨¡å‹åªèƒ½æ•æ‰å™ªå£°éƒ¨åˆ†ç‰¹å¾ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå™ªå£°ç”ŸæˆGANï¼ˆNGGANï¼‰çš„æ–°å‹ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œç”¨äºåˆæˆæ•°æ®ï¼Œå­¦ä¹ å®é™…æµ‹é‡å™ªå£°æ ·æœ¬çš„å¤æ‚ç‰¹å¾ã€‚ä¸ºäº†ç´§å¯†åŒ¹é…NB-PLCç³»ç»Ÿä¸Šçš„å¤æ‚å™ªå£°ç»Ÿè®¡ä¿¡æ¯ï¼Œæˆ‘ä»¬é€šè¿‡å•†ä¸šNB-PLCè°ƒåˆ¶è§£è°ƒå™¨çš„æ¨¡æ‹Ÿè€¦åˆå’Œå¸¦é€šæ»¤æ³¢ç”µè·¯æµ‹é‡NB-PLCå™ªå£°ï¼Œä»¥æ„å»ºç°å®æ•°æ®é›†ã€‚ä¸ºäº†è®­ç»ƒNGGANï¼Œæˆ‘ä»¬éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š1ï¼‰è®¾è®¡NGGANæ¨¡å‹èƒ½å¤Ÿé€‚åº”çš„è¾“å…¥ä¿¡å·é•¿åº¦ï¼Œä»¥ä¿ƒè¿›å¾ªç¯å¹³ç¨³å™ªå£°ç”Ÿæˆï¼›2ï¼‰ä½¿ç”¨Wassersteinè·ç¦»ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œæé«˜ç”Ÿæˆå™ªå£°ä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼›3ï¼‰ä¸ºäº†æµ‹é‡åŸºäºæ•°å­¦å’Œå®é™…æµ‹é‡æ•°æ®é›†çš„GANæ¨¡å‹çš„ç›¸ä¼¼æ€§æ€§èƒ½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®šé‡å’Œå®šæ€§åˆ†æã€‚è®­ç»ƒæ•°æ®é›†åŒ…æ‹¬ï¼š1ï¼‰åˆ†æ®µè°±å¾ªç¯å¹³ç¨³é«˜æ–¯æ¨¡å‹ï¼ˆPSCGMï¼‰ï¼›2ï¼‰é¢‘ç‡åç§»ï¼ˆFRESHï¼‰æ»¤æ³¢å™¨ï¼›3ï¼‰æ¥è‡ªNB-PLCç³»ç»Ÿçš„å®é™…æµ‹é‡ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„NGGANç”Ÿæˆçš„å™ªå£°æ ·æœ¬ä¸çœŸå®å™ªå£°æ ·æœ¬é«˜åº¦æ¥è¿‘ã€‚ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æ•£ç‚¹å›¾å’ŒFrÃ©chet inceptionè·ç¦»ï¼ˆFIDï¼‰åˆ†æè¡¨æ˜ï¼ŒNGGANé€šè¿‡ç”Ÿæˆå…·æœ‰æ›´é«˜ä¿çœŸåº¦å’Œå¤šæ ·æ€§çš„å™ªå£°æ ·æœ¬ï¼Œåœ¨å…¶ä»–åŸºäºGANçš„æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.01850v3">PDF</a> 16 pages, 15 figures, 11 tables, and published in IEEE Transactions   on Instrumentation and Measurement, 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹çª„å¸¦ç”µåŠ›çº¿é€šä¿¡ï¼ˆNB-PLCï¼‰çš„å™ªå£°ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆNGGANï¼‰ï¼Œç”¨äºæ•æ‰éå‘¨æœŸæ€§å¼‚æ­¥è„‰å†²å™ªå£°ï¼ˆAPINï¼‰çš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯ã€‚é€šè¿‡å•†ä¸šNB-PLCè°ƒåˆ¶è§£è°ƒå™¨çš„æ¨¡æ‹Ÿè€¦åˆå’Œå¸¦é€šæ»¤æ³¢ç”µè·¯é‡‡é›†å®é™…å™ªå£°æ•°æ®ï¼Œå»ºç«‹ç°å®æ•°æ®é›†ã€‚é‡‡ç”¨Wassersteinè·ç¦»ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¢å¼ºç”Ÿæˆå™ªå£°ä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒNGGANç”Ÿæˆçš„å™ªå£°æ ·æœ¬ä¸çœŸå®å™ªå£°æ ·æœ¬é«˜åº¦æ¥è¿‘ï¼Œä¸”é€šè¿‡ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æ•£ç‚¹å›¾å’ŒFrÃ©chet inceptionè·ç¦»ï¼ˆFIDï¼‰åˆ†æï¼Œè¡¨ç°å‡ºä¼˜äºå…¶ä»–GANæ¨¡å‹çš„ä¿çœŸåº¦å’Œå¤šæ ·æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä½¿ç”¨NGGANç½‘ç»œå¤„ç†NB-PLCä¸­çš„è„‰å†²å™ªå£°ï¼Œè¯¥ç½‘ç»œèƒ½æ•æ‰éå‘¨æœŸæ€§å¼‚æ­¥è„‰å†²å™ªå£°çš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡å•†ä¸šNB-PLCè°ƒåˆ¶è§£è°ƒå™¨é‡‡é›†å®é™…å™ªå£°æ•°æ®ï¼Œå»ºç«‹ç°å®æ•°æ®é›†ä»¥åŒ¹é…NB-PLCç³»ç»Ÿä¸­çš„å¤æ‚å™ªå£°ç»Ÿè®¡ã€‚</li>
<li>é‡‡ç”¨Wassersteinè·ç¦»ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œæé«˜ç”Ÿæˆå™ªå£°ä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>NGGANç”Ÿæˆçš„å™ªå£°æ ·æœ¬ä¸çœŸå®æ ·æœ¬é«˜åº¦æ¥è¿‘ã€‚</li>
<li>NGGANåœ¨ç”Ÿæˆå™ªå£°æ ·æœ¬çš„ä¿çœŸåº¦å’Œå¤šæ ·æ€§ä¸Šä¼˜äºå…¶ä»–GANæ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨PCAæ•£ç‚¹å›¾å’ŒFIDåˆ†ææ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.01850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00cdeef20148b54c351c5863a9a48f6d" align="middle">
<img src="https://picx.zhimg.com/v2-cb9a6533bf737a10d5d94d03ae2c5b1f" align="middle">
<img src="https://picx.zhimg.com/v2-b35973d4b6c4c3cc8676e493eded1838" align="middle">
<img src="https://picx.zhimg.com/v2-90d07c97bfc86de4f35c7b684e8f106a" align="middle">
<img src="https://picx.zhimg.com/v2-1a7b5f5170ba54c0550a95fbbdb4553d" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MS-GS-Multi-Appearance-Sparse-View-3D-Gaussian-Splatting-in-the-Wild"><a href="#MS-GS-Multi-Appearance-Sparse-View-3D-Gaussian-Splatting-in-the-Wild" class="headerlink" title="MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild"></a>MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild</h2><p><strong>Authors:Deming Li, Kaiwen Jiang, Yutao Tang, Ravi Ramamoorthi, Rama Chellappa, Cheng Peng</strong></p>
<p>In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis. Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting. In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS. To address the lack of support due to sparse initializations, our approach is built on the geometric priors elicited from monocular depth estimations. The key lies in extracting and utilizing local semantic regions with a Structure-from-Motion (SfM) points anchored algorithm for reliable alignment and geometry cues. Then, to introduce multi-view constraints, we propose a series of geometry-guided supervision steps at virtual views in pixel and feature levels to encourage 3D consistency and reduce overfitting. We also introduce a dataset and an in-the-wild experiment setting to set up more realistic benchmarks. We demonstrate that MS-GS achieves photorealistic renderings under various challenging sparse-view and multi-appearance conditions, and outperforms existing approaches significantly across different datasets. </p>
<blockquote>
<p>åœ¨é‡å¤–çš„ç…§ç‰‡é›†é€šå¸¸åŒ…å«æœ‰é™çš„å›¾åƒæ•°é‡ï¼Œå¹¶ä¸”å±•ç°å‡ºå¤šç§å¤–è§‚ï¼Œä¾‹å¦‚åœ¨ä¸€å¤©çš„ä¸åŒæ—¶é—´æˆ–å­£èŠ‚æ‹æ‘„çš„ç…§ç‰‡ï¼Œè¿™ç»™åœºæ™¯é‡å»ºå’Œæ–°å‹è§†å›¾åˆæˆå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘å¯¹ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æ‹¼æ¥ï¼ˆ3DGSï¼‰çš„æ”¹ç¼–åœ¨è¿™äº›é¢†åŸŸæœ‰æ‰€æ”¹å–„ï¼Œä½†å®ƒä»¬å¾€å¾€è¿‡äºå¹³æ»‘å¹¶å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MS-GSï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ç¨€ç–è§†å›¾åœºæ™¯ä¸­åˆ©ç”¨3DGSçš„å¤šå¤–è§‚èƒ½åŠ›è®¾è®¡çš„æ–°å‹æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç”±äºç¨€ç–åˆå§‹åŒ–è€Œå¯¼è‡´çš„æ”¯æŒä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨ä»å•çœ¼æ·±åº¦ä¼°è®¡ä¸­å¼•å‘çš„å‡ ä½•å…ˆéªŒä¹‹ä¸Šã€‚å…³é”®åœ¨äºé€šè¿‡ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰ç‚¹é”šå®šç®—æ³•æå–å’Œåˆ©ç”¨å±€éƒ¨è¯­ä¹‰åŒºåŸŸï¼Œä»¥å®ç°å¯é çš„å¯¹é½å’Œå‡ ä½•çº¿ç´¢ã€‚ç„¶åï¼Œä¸ºäº†å¼•å…¥å¤šè§†å›¾çº¦æŸï¼Œæˆ‘ä»¬åœ¨åƒç´ å’Œç‰¹å¾å±‚é¢æå‡ºäº†åœ¨è™šæ‹Ÿè§†å›¾ä¸­è¿›è¡Œçš„ä¸€ç³»åˆ—å‡ ä½•å¼•å¯¼çš„ç›‘ç£æ­¥éª¤ï¼Œä»¥é¼“åŠ±ä¸‰ç»´ä¸€è‡´æ€§å¹¶å‡å°‘è¿‡åº¦æ‹Ÿåˆã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ•°æ®é›†å’Œä¸€ä¸ªé‡å¤–å®éªŒè®¾ç½®ï¼Œä»¥å»ºç«‹æ›´ç°å®çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒMS-GSåœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¨€ç–è§†å›¾å’Œå¤šå¤–è§‚æ¡ä»¶ä¸‹å®ç°äº†é€¼çœŸçš„æ¸²æŸ“ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15548v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº3DGSçš„æ–°å‹æ¡†æ¶MS-GSï¼Œç”¨äºå¤„ç†é‡å¤–ç…§ç‰‡é›†ä¸­ç¨€ç–è§†è§’å’Œå¤šåœºæ™¯å¤–è§‚ä¸‹çš„é‡å»ºå’Œåˆæˆã€‚å®ƒé€šè¿‡åˆ©ç”¨å‡ ä½•å…ˆéªŒä¿¡æ¯æ¥è§£å†³å› ç¨€ç–åˆå§‹åŒ–è€Œå¸¦æ¥çš„ä¸è¶³ï¼Œé‡‡ç”¨åŸºäºå•ç›®æ·±åº¦ä¼°è®¡çš„ç»“æ„åŒ–å»ºæ¨¡ã€‚å¼•å…¥å¤šé‡å‡ ä½•æŒ‡å¯¼çš„çº¦æŸæ–¹æ³•ä»¥å®ç°è™šæ‹Ÿè§†å›¾çš„ç²¾å‡†å¯¹é½å’Œå‡ ä½•ä¸€è‡´æ€§ï¼Œå‡å°‘è¿‡æ‹Ÿåˆç°è±¡ã€‚å®éªŒè¯æ˜ï¼ŒMS-GSåœ¨å¤šç§ç¨€ç–è§†è§’å’Œå¤šåœºæ™¯å¤–è§‚æ¡ä»¶ä¸‹å®ç°äº†é€¼çœŸçš„æ¸²æŸ“æ•ˆæœï¼Œå¹¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é‡å¤–ç…§ç‰‡é›†ä¸­å­˜åœ¨å¤šç§æŒ‘æˆ˜ï¼Œå¦‚ä¸åŒæ—¶é—´ç‚¹å’Œå­£èŠ‚çš„å›¾åƒæ‹æ‘„é€ æˆçš„å¤æ‚åœºæ™¯é‡å»ºå’Œè§†è§’åˆæˆé—®é¢˜ã€‚</li>
<li>MS-GSæ¡†æ¶é‡‡ç”¨åŸºäºå‡ ä½•å…ˆéªŒä¿¡æ¯çš„å»ºæ¨¡æ–¹æ³•ï¼Œè§£å†³äº†ç¨€ç–åˆå§‹åŒ–é—®é¢˜ï¼Œé‡‡ç”¨ç»“æ„åŒ–å»ºæ¨¡ï¼Œæ—¨åœ¨åº”å¯¹å¤æ‚çš„å¤šå¤–è§‚åœºæ™¯ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨å•ç›®æ·±åº¦ä¼°è®¡ï¼Œå¹¶é‡‡ç”¨ç»“æ„åŒ–è¿åŠ¨ï¼ˆSfMï¼‰ç‚¹çš„å›ºå®šç®—æ³•è¿›è¡Œå¯é å¯¹é½å’Œå‡ ä½•çº¿ç´¢æå–ã€‚</li>
<li>ä¸ºå®ç°å¤šè§†è§’çº¦æŸï¼Œå¼•å…¥äº†ä¸€ç³»åˆ—å‡ ä½•æŒ‡å¯¼çš„ç›‘ç£æ­¥éª¤ï¼Œå®ç°è™šæ‹Ÿè§†è§’çš„ç²¾å‡†æ¸²æŸ“å¹¶å‡å°‘è¿‡æ‹Ÿåˆç°è±¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œå®éªŒè®¾ç½®ï¼Œä»¥å»ºç«‹æ›´ç°å®çš„åŸºå‡†æµ‹è¯•ç¯å¢ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7b75d110232b3e6bd71fab339d1132b" align="middle">
<img src="https://picx.zhimg.com/v2-f8b8cac7efed0745a278c42d79d2a930" align="middle">
<img src="https://picx.zhimg.com/v2-10398e16ea6f36eb706818c22c656efb" align="middle">
<img src="https://picx.zhimg.com/v2-8f072a24b12435f2f576e3f4b66e7318" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MuGS-Multi-Baseline-Generalizable-Gaussian-Splatting-Reconstruction"><a href="#MuGS-Multi-Baseline-Generalizable-Gaussian-Splatting-Reconstruction" class="headerlink" title="MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction"></a>MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction</h2><p><strong>Authors:Yaopeng Lou, Liao Shen, Tianqi Liu, Jiaqi Li, Zihao Huang, Huiqiang Sun, Zhiguo Cao</strong></p>
<p>We present Multi-Baseline Gaussian Splatting (MuGS), a generalized feed-forward approach for novel view synthesis that effectively handles diverse baseline settings, including sparse input views with both small and large baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and Monocular Depth Estimation (MDE) to enhance feature representations for generalizable reconstruction. Next, We propose a projection-and-sampling mechanism for deep depth fusion, which constructs a fine probability volume to guide the regression of the feature map. Furthermore, We introduce a reference-view loss to improve geometry and optimization efficiency. We leverage 3D Gaussian representations to accelerate training and inference time while enhancing rendering quality. MuGS achieves state-of-the-art performance across multiple baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360 datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/EuclidLou/MuGS">https://github.com/EuclidLou/MuGS</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å¤šåŸºçº¿é«˜æ–¯æ‹¼è´´ï¼ˆMuGSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ–°é¢–è§†å›¾åˆæˆçš„é€šç”¨å‰é¦ˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šç§åŸºçº¿è®¾ç½®ï¼ŒåŒ…æ‹¬å…·æœ‰å¤§å°å’ŒåŸºçº¿å·®è·è¾ƒå¤§ç¨€ç–è¾“å…¥è§†å›¾çš„åœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ•´åˆäº†å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰å’Œå•çœ¼æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰çš„ç‰¹å¾ï¼Œä»¥å¢å¼ºé€šç”¨é‡å»ºçš„ç‰¹å¾è¡¨ç¤ºã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ·±åº¦æ·±åº¦èåˆçš„æŠ•å½±å’Œé‡‡æ ·æœºåˆ¶ï¼Œæ„å»ºäº†ç²¾ç»†çš„æ¦‚ç‡ä½“ç§¯æ¥æŒ‡å¯¼ç‰¹å¾å›¾çš„å›å½’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‚è€ƒè§†å›¾æŸå¤±æ¥æé«˜å‡ ä½•å½¢çŠ¶å’Œä¼˜åŒ–æ•ˆç‡ã€‚æˆ‘ä»¬åˆ©ç”¨ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºæ¥åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†æ—¶é—´ï¼ŒåŒæ—¶æé«˜æ¸²æŸ“è´¨é‡ã€‚MuGSåœ¨å¤šåŸºçº¿è®¾ç½®å’Œå„ç§åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ä»ç®€å•å¯¹è±¡ï¼ˆDTUï¼‰åˆ°å¤æ‚çš„å®¤å†…å’Œå®¤å¤–åœºæ™¯ï¼ˆRealEstate10Kï¼‰ã€‚æˆ‘ä»¬åœ¨LLFFå’ŒMip-NeRF 360æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ‰å‰æ™¯çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EuclidLou/MuGS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/EuclidLou/MuGSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04297v2">PDF</a> This work is accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMulti-Baseline Gaussian Splattingï¼ˆMuGSï¼‰çš„æ–°å‹è§†å›¾åˆæˆå¹¿ä¹‰å‰é¦ˆæ–¹æ³•ï¼Œé€‚ç”¨äºå¤šç§åŸºçº¿è®¾ç½®ï¼ŒåŒ…æ‹¬è¾“å…¥è§†å›¾ç¨€ç–ã€åŸºçº¿è·ç¦»å¤§å°ä¸åŒçš„æƒ…å†µã€‚MuGSæ•´åˆäº†å¤šè§†è§’ç«‹ä½“ï¼ˆMVSï¼‰å’Œå•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰çš„ç‰¹å¾ï¼Œæå‡ç‰¹å¾è¡¨ç¤ºçš„å¯æ³›åŒ–é‡å»ºèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæå‡ºä¸€ç§æ·±åº¦èåˆæŠ•å½±é‡‡æ ·æœºåˆ¶ï¼Œæ„å»ºç²¾ç»†æ¦‚ç‡ä½“ç§¯ä»¥å¼•å¯¼ç‰¹å¾å›¾çš„å›å½’ã€‚å¼•å…¥å‚è€ƒè§†å›¾æŸå¤±ä»¥æé«˜å‡ ä½•å’Œä¼˜åŒ–æ•ˆç‡ï¼Œå¹¶åˆ©ç”¨3Dé«˜æ–¯è¡¨ç¤ºåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†æ—¶é—´ï¼Œæå‡æ¸²æŸ“è´¨é‡ã€‚MuGSåœ¨å¤šç§åŸºçº¿è®¾ç½®å’Œç®€å•å¯¹è±¡ï¼ˆDTUï¼‰åˆ°å¤æ‚å®¤å†…å®¤å¤–åœºæ™¯ï¼ˆRealEstate10Kï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨LLFFå’ŒMip-NeRF 360æ•°æ®é›†ä¸Šå±•ç°å‡ºé›¶æ ·æœ¬æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MuGSæ˜¯ä¸€ç§æ–°å‹è§†å›¾åˆæˆæ–¹æ³•ï¼Œé€‚ç”¨äºå¤šç§åŸºçº¿è®¾ç½®ã€‚</li>
<li>é›†æˆMVSå’ŒMDEç‰¹å¾ï¼Œæé«˜ç‰¹å¾è¡¨ç¤ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºæ·±åº¦èåˆæŠ•å½±é‡‡æ ·æœºåˆ¶ï¼Œæ„å»ºç²¾ç»†æ¦‚ç‡ä½“ç§¯å¼•å¯¼ç‰¹å¾å›¾å›å½’ã€‚</li>
<li>å¼•å…¥å‚è€ƒè§†å›¾æŸå¤±ï¼Œæ”¹å–„å‡ ä½•å’Œä¼˜åŒ–æ•ˆç‡ã€‚</li>
<li>åˆ©ç”¨3Dé«˜æ–¯è¡¨ç¤ºåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œæå‡æ¸²æŸ“è´¨é‡ã€‚</li>
<li>MuGSåœ¨å¤šç§åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ç®€å•å¯¹è±¡å’Œå¤æ‚å®¤å†…å®¤å¤–åœºæ™¯ã€‚</li>
<li>MuGSåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå±•ç°å‡ºé›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97b599254567453c49352f92c529d927" align="middle">
<img src="https://picx.zhimg.com/v2-b9735e16b71d856246759368983a4741" align="middle">
<img src="https://picx.zhimg.com/v2-8c5bc019eb9ef13d223a75982b3d0a9d" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LiGen-GAN-Augmented-Spectral-Fingerprinting-for-Indoor-Positioning"><a href="#LiGen-GAN-Augmented-Spectral-Fingerprinting-for-Indoor-Positioning" class="headerlink" title="LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning"></a>LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning</h2><p><strong>Authors:Jie Lin, Hsun-Yu Lee, Ho-Ming Li, Fang-Jing Wu</strong></p>
<p>Accurate and robust indoor localization is critical for smart building applications, yet existing Wi-Fi-based systems are often vulnerable to environmental conditions. This work presents a novel indoor localization system, called LiGen, that leverages the spectral intensity patterns of ambient light as fingerprints, offering a more stable and infrastructure-free alternative to radio signals. To address the limited spectral data, we design a data augmentation framework based on generative adversarial networks (GANs), featuring two variants: PointGAN, which generates fingerprints conditioned on coordinates, and FreeGAN, which uses a weak localization model to label unconditioned samples. Our positioning model, leveraging a Multi-Layer Perceptron (MLP) architecture to train on synthesized data, achieves submeter-level accuracy, outperforming Wi-Fi-based baselines by over 50%. LiGen also demonstrates strong robustness in cluttered environments. To the best of our knowledge, this is the first system to combine spectral fingerprints with GAN-based data augmentation for indoor localization. </p>
<blockquote>
<p>ç²¾ç¡®ä¸”ç¨³å®šçš„å®¤å†…å®šä½å¯¹äºæ™ºèƒ½å»ºç­‘åº”ç”¨è‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰çš„åŸºäºWi-Fiçš„ç³»ç»Ÿé€šå¸¸å®¹æ˜“å—åˆ°ç¯å¢ƒæ¡ä»¶çš„å¹²æ‰°ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§æ–°å‹çš„å®¤å†…å®šä½ç³»ç»Ÿï¼Œåä¸ºLiGenï¼Œå®ƒåˆ©ç”¨ç¯å¢ƒå…‰çš„å…‰è°±å¼ºåº¦æ¨¡å¼ä½œä¸ºæŒ‡çº¹ï¼Œä¸ºæ— çº¿ç”µä¿¡å·æä¾›äº†ä¸€ç§æ›´ç¨³å®šä¸”æ— åŸºç¡€è®¾æ–½çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³å…‰è°±æ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«ä¸¤ç§å˜ä½“ï¼šPointGANï¼Œå®ƒæ ¹æ®åæ ‡ç”ŸæˆæŒ‡çº¹ï¼›FreeGANï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªå¼±å®šä½æ¨¡å‹æ¥æ ‡è®°æ— æ¡ä»¶çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„å®šä½æ¨¡å‹é‡‡ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¶æ„åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†äºšç±³çº§ç²¾åº¦ï¼Œæ¯”åŸºäºWi-Fiçš„åŸºçº¿é«˜å‡º50%ä»¥ä¸Šã€‚LiGenåœ¨æ‚ä¹±çš„ç¯å¢ƒä¸­è¿˜è¡¨ç°å‡ºäº†å¼ºå¤§çš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å…‰è°±æŒ‡çº¹ä¸åŸºäºGANçš„æ•°æ®å¢å¼ºç›¸ç»“åˆç”¨äºå®¤å†…å®šä½çš„ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03024v2">PDF</a> 6 pages, 10 figures</p>
<p><strong>Summary</strong><br>å®¤å†…å®šä½å¯¹äºæ™ºèƒ½å»ºç­‘åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰Wi-Fiç³»ç»Ÿæ˜“å—ç¯å¢ƒå½±å“ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹å®¤å†…å®šä½ç³»ç»ŸLiGenï¼Œåˆ©ç”¨ç¯å¢ƒå…‰çš„è°±å¼ºåº¦æ¨¡å¼ä½œä¸ºæŒ‡çº¹ï¼Œä¸ºæ— çº¿ç”µä¿¡å·æä¾›æ›´ç¨³å®šã€æ— éœ€åŸºç¡€è®¾æ–½çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ºè§£å†³è°±æ•°æ®æœ‰é™é—®é¢˜ï¼Œè®¾è®¡åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ•°æ®å¢å¼ºæ¡†æ¶ï¼ŒåŒ…æ‹¬PointGANå’ŒFreeGANä¸¤ç§å˜ä½“ã€‚åˆ©ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¶æ„åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„å®šä½æ¨¡å‹ï¼Œå®ç°äºšç±³çº§ç²¾åº¦ï¼Œæ€§èƒ½è¾ƒWi-FiåŸºçº¿æå‡è¶…è¿‡50%ã€‚LiGenåœ¨æ‚ä¹±ç¯å¢ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œæ˜¯é¦–ä¸ªå°†å…‰è°±æŒ‡çº¹ä¸åŸºäºGANçš„æ•°æ®å¢å¼ºç›¸ç»“åˆçš„å®¤å†…å®šä½ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®¤å†…å®šä½å¯¹äºæ™ºèƒ½å»ºç­‘åº”ç”¨éå¸¸é‡è¦ï¼Œä½†ç°æœ‰Wi-Fiç³»ç»Ÿå­˜åœ¨ç¨³å®šæ€§é—®é¢˜ã€‚</li>
<li>LiGenç³»ç»Ÿåˆ©ç”¨ç¯å¢ƒå…‰çš„è°±å¼ºåº¦æ¨¡å¼ä½œä¸ºæŒ‡çº¹ï¼Œæä¾›ç¨³å®šçš„å®¤å†…å®šä½ã€‚</li>
<li>è®¾è®¡åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œè§£å†³è°±æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>å¼€å‘å‡ºPointGANå’ŒFreeGANä¸¤ç§æ•°æ®å¢å¼ºå˜ä½“ï¼Œç”¨äºç”ŸæˆæŒ‡çº¹å’Œæ ‡ç­¾ã€‚</li>
<li>åˆ©ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¶æ„åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå®ç°äºšç±³çº§å®šä½ç²¾åº¦ã€‚</li>
<li>LiGenæ€§èƒ½è¾ƒWi-FiåŸºçº¿æå‡è¶…è¿‡50%ï¼Œå¹¶åœ¨æ‚ä¹±ç¯å¢ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cca0edf24e221e3063f50b882df4dbb" align="middle">
<img src="https://picx.zhimg.com/v2-f8136347f67be2c39c8db8c6a7a28a00" align="middle">
<img src="https://picx.zhimg.com/v2-e332d701947a35b4ab54d08e56bccfc8" align="middle">
<img src="https://picx.zhimg.com/v2-71f08edd89889094d6555f6681fcd69d" align="middle">
<img src="https://picx.zhimg.com/v2-37f643524661633bc3a4517d250e87f7" align="middle">
<img src="https://picx.zhimg.com/v2-92022598ad5de156fb0a47e029dd88cd" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PlantSegNeRF-A-few-shot-cross-species-method-for-plant-3D-instance-point-cloud-reconstruction-via-joint-channel-NeRF-with-multi-view-image-instance-matching"><a href="#PlantSegNeRF-A-few-shot-cross-species-method-for-plant-3D-instance-point-cloud-reconstruction-via-joint-channel-NeRF-with-multi-view-image-instance-matching" class="headerlink" title="PlantSegNeRF: A few-shot, cross-species method for plant 3D instance   point cloud reconstruction via joint-channel NeRF with multi-view image   instance matching"></a>PlantSegNeRF: A few-shot, cross-species method for plant 3D instance   point cloud reconstruction via joint-channel NeRF with multi-view image   instance matching</h2><p><strong>Authors:Xin Yang, Ruiming Du, Hanyang Huang, Jiayang Xie, Pengyao Xie, Leisen Fang, Ziyue Guo, Nanjun Jiang, Yu Jiang, Haiyan Cen</strong></p>
<p>Organ segmentation of plant point clouds is a prerequisite for the high-resolution and accurate extraction of organ-level phenotypic traits. Although the fast development of deep learning has boosted much research on segmentation of plant point clouds, the existing techniques for organ segmentation still face limitations in resolution, segmentation accuracy, and generalizability across various plant species. In this study, we proposed a novel approach called plant segmentation neural radiance fields (PlantSegNeRF), aiming to directly generate high-precision instance point clouds from multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF performed 2D instance segmentation on the multi-view images to generate instance masks for each organ with a corresponding ID. The multi-view instance IDs corresponding to the same plant organ were then matched and refined using a specially designed instance matching module. The instance NeRF was developed to render an implicit scene, containing color, density, semantic and instance information. The implicit scene was ultimately converted into high-precision plant instance point clouds based on the volume density. The results proved that in semantic segmentation of point clouds, PlantSegNeRF outperformed the commonly used methods, demonstrating an average improvement of 16.1%, 18.3%, 17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the second-best results on structurally complex species. More importantly, PlantSegNeRF exhibited significant advantages in plant point cloud instance segmentation tasks. Across all plant species, it achieved average improvements of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively. This study extends the organ-level plant phenotyping and provides a high-throughput way to supply high-quality 3D data for the development of large-scale models in plant science. </p>
<blockquote>
<p>æ¤ç‰©ç‚¹äº‘å™¨å®˜åˆ†å‰²æ˜¯é«˜åˆ†è¾¨ç‡å’Œç²¾ç¡®æå–å™¨å®˜æ°´å¹³è¡¨å‹ç‰¹å¾çš„å‰æã€‚å°½ç®¡æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•ä¿ƒè¿›äº†æ¤ç‰©ç‚¹äº‘åˆ†å‰²çš„ç ”ç©¶ï¼Œä½†ç°æœ‰çš„å™¨å®˜åˆ†å‰²æŠ€æœ¯ä»é¢ä¸´åˆ†è¾¨ç‡ã€åˆ†å‰²ç²¾åº¦å’Œè·¨ç‰©ç§æ³›åŒ–èƒ½åŠ›çš„å±€é™æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºæ¤ç‰©åˆ†å‰²ç¥ç»è¾å°„åœºï¼ˆPlantSegNeRFï¼‰ï¼Œæ—¨åœ¨ç›´æ¥ä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ä¸ºå¹¿æ³›çš„æ¤ç‰©ç‰©ç§ç”Ÿæˆé«˜ç²¾åº¦å®ä¾‹ç‚¹äº‘ã€‚PlantSegNeRFåœ¨å¤šè§†è§’å›¾åƒä¸Šè¿›è¡Œ2Då®ä¾‹åˆ†å‰²ï¼Œä¸ºæ¯ä¸ªå™¨å®˜ç”Ÿæˆå…·æœ‰ç›¸åº”IDçš„å®ä¾‹æ©è†œã€‚ç„¶åï¼Œä½¿ç”¨ä¸“é—¨è®¾è®¡çš„å®ä¾‹åŒ¹é…æ¨¡å—åŒ¹é…å’Œç»†åŒ–å¯¹åº”äºåŒä¸€æ¤ç‰©å™¨å®˜çš„å¤šè§†è§’å®ä¾‹IDã€‚å¼€å‘äº†å®ä¾‹NeRFæ¥å‘ˆç°åŒ…å«é¢œè‰²ã€å¯†åº¦ã€è¯­ä¹‰å’Œå®ä¾‹ä¿¡æ¯çš„éšå¼åœºæ™¯ã€‚æœ€ç»ˆï¼ŒåŸºäºä½“ç§¯å¯†åº¦å°†éšå¼åœºæ™¯è½¬æ¢ä¸ºé«˜ç²¾åº¦æ¤ç‰©å®ä¾‹ç‚¹äº‘ã€‚ç»“æœè¯æ˜ï¼Œåœ¨ç‚¹äº‘è¯­ä¹‰åˆ†å‰²ä¸­ï¼ŒPlantSegNeRFä¼˜äºå¸¸ç”¨æ–¹æ³•ï¼Œåœ¨ç»“æ„å¤æ‚çš„ç‰©ç§ä¸Šï¼Œä¸ç¬¬äºŒå¥½çš„ç»“æœç›¸æ¯”ï¼Œç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒIoUå¹³å‡æé«˜äº†16.1%ã€18.3%ã€17.8%å’Œ24.2%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒPlantSegNeRFåœ¨æ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨æ‰€æœ‰æ¤ç‰©ç‰©ç§ä¸­ï¼ŒmPrecã€mRecã€mCovå’ŒmWCovå¹³å‡æé«˜äº†11.7%ã€38.2%ã€32.2%å’Œ25.3%ã€‚è¯¥ç ”ç©¶æ‰©å±•äº†æ¤ç‰©è¡¨å‹çš„å™¨å®˜æ°´å¹³ç ”ç©¶ï¼Œå¹¶ä¸ºæ¤ç‰©ç§‘å­¦ä¸­å¤§è§„æ¨¡æ¨¡å‹çš„å¼€å‘æä¾›äº†ä¸€ç§æä¾›é«˜è´¨é‡3Dæ•°æ®çš„é«˜é€šé‡æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00371v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ¤ç‰©ç‚¹äº‘å™¨å®˜åˆ†å‰²æ˜¯é«˜åˆ†è¾¨ç‡å’Œç²¾ç¡®æå–å™¨å®˜æ°´å¹³è¡¨å‹ç‰¹å¾çš„å‰æã€‚å°½ç®¡æ·±åº¦å­¦ä¹ å¿«é€Ÿå‘å±•æ¨åŠ¨äº†æ¤ç‰©ç‚¹äº‘åˆ†å‰²çš„ç ”ç©¶ï¼Œä½†ç°æœ‰æŠ€æœ¯ä»é¢ä¸´åˆ†è¾¨ç‡ã€åˆ†å‰²ç²¾åº¦å’Œè·¨ç‰©ç§æ³›åŒ–èƒ½åŠ›çš„å±€é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPlantSegNeRFçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ç›´æ¥ç”Ÿæˆé«˜ç²¾åº¦å®ä¾‹ç‚¹äº‘ï¼Œé€‚ç”¨äºå¹¿æ³›æ¤ç‰©ç‰©ç§ã€‚PlantSegNeRFå¯¹å¤šè§†è§’å›¾åƒè¿›è¡Œ2Då®ä¾‹åˆ†å‰²ï¼Œä¸ºæ¯ä¸ªå™¨å®˜ç”Ÿæˆå®ä¾‹æ©è†œå¹¶èµ‹äºˆç›¸åº”IDã€‚åˆ©ç”¨ä¸“é—¨è®¾è®¡çš„å®ä¾‹åŒ¹é…æ¨¡å—å¯¹åŒä¸€æ¤ç‰©å™¨å®˜çš„è·¨è§†è§’å®ä¾‹IDè¿›è¡ŒåŒ¹é…å’Œç»†åŒ–ã€‚å¼€å‘å®ä¾‹NeRFä»¥å‘ˆç°åŒ…å«é¢œè‰²ã€å¯†åº¦ã€è¯­ä¹‰å’Œå®ä¾‹ä¿¡æ¯çš„éšå¼åœºæ™¯ã€‚æœ€ç»ˆï¼Œæ ¹æ®ä½“ç§¯å¯†åº¦å°†éšå¼åœºæ™¯è½¬æ¢ä¸ºé«˜ç²¾åº¦æ¤ç‰©å®ä¾‹ç‚¹äº‘ã€‚ç»“æœè¯æ˜ï¼Œåœ¨ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ–¹é¢ï¼ŒPlantSegNeRFè¾ƒå¸¸ç”¨æ–¹æ³•è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œåœ¨ç»“æ„å¤æ‚ç‰©ç§çš„ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒIoUæ–¹é¢å¹³å‡æé«˜äº†16.1%ã€18.3%ã€17.8%å’Œ24.2%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨æ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒPlantSegNeRFå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œåœ¨æ‰€æœ‰ç‰©ç§ä¸­ï¼ŒmPrecã€mRecã€mCovå’ŒmWCovå¹³å‡æé«˜äº†11.7%ã€38.2%ã€32.2%å’Œ25.3%ã€‚æœ¬ç ”ç©¶æ‰©å±•äº†æ¤ç‰©è¡¨å‹å­¦ä¸­çš„å™¨å®˜æ°´å¹³ç ”ç©¶ï¼Œä¸ºæ¤ç‰©ç§‘å­¦ä¸­å¤§è§„æ¨¡æ¨¡å‹çš„å‘å±•æä¾›äº†ä¸€ç§äº§ç”Ÿé«˜è´¨é‡3Dæ•°æ®çš„é«˜é€šé‡æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¤ç‰©ç‚¹äº‘å™¨å®˜åˆ†å‰²å¯¹äºé«˜åˆ†è¾¨ç‡å’Œç²¾ç¡®æå–å™¨å®˜æ°´å¹³è¡¨å‹ç‰¹å¾è‡³å…³é‡è¦ã€‚</li>
<li>PlantSegNeRFæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ç”Ÿæˆé«˜ç²¾åº¦å®ä¾‹ç‚¹äº‘ï¼Œé€‚ç”¨äºå¤šç§æ¤ç‰©ç‰©ç§ã€‚</li>
<li>PlantSegNeRFé€šè¿‡2Då®ä¾‹åˆ†å‰²å’Œå¤šè§†è§’å®ä¾‹åŒ¹é…æ¨¡å—æé«˜äº†åˆ†å‰²ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®ä¾‹NeRFçš„å¼•å…¥ä¸ºå‘ˆç°åŒ…å«å¤šç§ä¿¡æ¯çš„éšå¼åœºæ™¯æä¾›äº†æ–°çš„æ‰‹æ®µã€‚</li>
<li>PlantSegNeRFåœ¨è¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²æ–¹é¢éƒ½è¡¨ç°å‡ºä¼˜äºå¸¸è§„æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶æ‰©å±•äº†æ¤ç‰©è¡¨å‹å­¦ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å™¨å®˜æ°´å¹³ä¸Šçš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-711af185f90bd7649adac63ee8cfcfdf" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LODGE-Level-of-Detail-Large-Scale-Gaussian-Splatting-with-Efficient-Rendering"><a href="#LODGE-Level-of-Detail-Large-Scale-Gaussian-Splatting-with-Efficient-Rendering" class="headerlink" title="LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient   Rendering"></a>LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient   Rendering</h2><p><strong>Authors:Jonas Kulhanek, Marie-Julie Rakotosaona, Fabian Manhardt, Christina Tsalicoglou, Michael Niemeyer, Torsten Sattler, Songyou Peng, Federico Tombari</strong></p>
<p>In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸º3Dé«˜æ–¯ç»˜å›¾æå‡ºäº†ä¸€ç§æ–°å‹çš„ç»†èŠ‚å±‚æ¬¡ï¼ˆLODï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å†…å­˜å—é™çš„è®¾å¤‡ä¸Šå®ç°å¤§è§„æ¨¡åœºæ™¯çš„å®æ—¶æ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§å±‚æ¬¡å‹LODè¡¨ç¤ºï¼Œæ ¹æ®æ‘„åƒæœºè·ç¦»è¿­ä»£é€‰æ‹©æœ€ä¼˜çš„é«˜æ–¯å­é›†ï¼Œä»è€Œå¤§å¤§é™ä½äº†æ¸²æŸ“æ—¶é—´å’ŒGPUå†…å­˜çš„ä½¿ç”¨ã€‚æˆ‘ä»¬é€šè¿‡åº”ç”¨æ·±åº¦æ„ŸçŸ¥çš„3Då¹³æ»‘æ»¤æ³¢å™¨æ„å»ºæ¯ä¸ªLODçº§åˆ«ï¼Œç„¶åé€šè¿‡åŸºäºé‡è¦æ€§çš„ä¿®å‰ªå’Œå¾®è°ƒæ¥ä¿æŒè§†è§‰ä¿çœŸåº¦ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜å¼€é”€ï¼Œæˆ‘ä»¬å°†åœºæ™¯åˆ†å‰²æˆç©ºé—´å—ï¼Œå¹¶åœ¨æ¸²æŸ“æ—¶åŠ¨æ€åŠ è½½ç›¸å…³çš„é«˜æ–¯å€¼ï¼Œé‡‡ç”¨é€æ˜åº¦æ··åˆæœºåˆ¶ä»¥é¿å…å—è¾¹ç•Œå¤„çš„è§†è§‰ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æˆ·å¤–ï¼ˆHierarchical 3DGSï¼‰å’Œå®¤å†…ï¼ˆZip-NeRFï¼‰æ•°æ®é›†ä¸Šå‡å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä»¥é™ä½å»¶è¿Ÿå’Œå†…å­˜éœ€æ±‚çš„åŒæ—¶æä¾›é«˜è´¨é‡çš„æ¸²æŸ“æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23158v2">PDF</a> NeurIPS 2025; Web: <a target="_blank" rel="noopener" href="https://lodge-gs.github.io/">https://lodge-gs.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹3Dé«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯çš„æ–°å‹ç»†èŠ‚å±‚æ¬¡ï¼ˆLODï¼‰æ–¹æ³•ï¼Œèƒ½åœ¨å†…å­˜å—é™çš„è®¾å¤‡ä¸Šå®ç°å¤§è§„æ¨¡åœºæ™¯çš„å®æ—¶æ¸²æŸ“ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å±‚æ¬¡åŒ–LODè¡¨ç¤ºï¼Œæ ¹æ®ç›¸æœºè·ç¦»è¿­ä»£é€‰æ‹©æœ€ä¼˜çš„é«˜æ–¯å­é›†ï¼Œä»è€Œå¤§å¹…å‡å°‘æ¸²æŸ“æ—¶é—´å’ŒGPUå†…å­˜ä½¿ç”¨ã€‚é€šè¿‡æ·±åº¦æ„ŸçŸ¥çš„3Då¹³æ»‘æ»¤æ³¢æ„å»ºæ¯ä¸ªLODå±‚æ¬¡ï¼Œç„¶åè¿›è¡ŒåŸºäºé‡è¦æ€§çš„ä¿®å‰ªå’Œå¾®è°ƒä»¥ç»´æŒè§†è§‰ä¿çœŸåº¦ã€‚ä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜å¼€é”€ï¼Œå°†åœºæ™¯åˆ†å‰²æˆç©ºé—´å—ï¼Œå¹¶åœ¨æ¸²æŸ“æ—¶ä»…åŠ¨æ€åŠ è½½ç›¸å…³çš„é«˜æ–¯å€¼ï¼Œé‡‡ç”¨é€æ˜åº¦æ··åˆæœºåˆ¶é¿å…å—è¾¹ç•Œå¤„çš„è§†è§‰ä¼ªå½±ã€‚è¯¥æ–¹æ³•åœ¨æˆ·å¤–ï¼ˆHierarchical 3DGSï¼‰å’Œå®¤å†…ï¼ˆZip-NeRFï¼‰æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¯å®ç°é«˜è´¨é‡æ¸²æŸ“ï¼Œé™ä½å»¶è¿Ÿå’Œå†…å­˜è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç»†èŠ‚å±‚æ¬¡ï¼ˆLODï¼‰æ–¹æ³•ï¼Œç”¨äºä¼˜åŒ–3Dé«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯çš„å®æ—¶æ¸²æŸ“æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å±‚æ¬¡åŒ–LODè¡¨ç¤ºï¼Œæ ¹æ®ç›¸æœºè·ç¦»é€‰æ‹©æœ€ä¼˜çš„é«˜æ–¯å­é›†ï¼Œé™ä½æ¸²æŸ“æ—¶é—´å’ŒGPUå†…å­˜ä½¿ç”¨ã€‚</li>
<li>åˆ©ç”¨æ·±åº¦æ„ŸçŸ¥çš„3Då¹³æ»‘æ»¤æ³¢æ„å»ºæ¯ä¸ªLODå±‚æ¬¡ï¼Œä¿æŒè§†è§‰ä¿çœŸåº¦ã€‚</li>
<li>é€šè¿‡é‡è¦æ€§ä¿®å‰ªå’Œå¾®è°ƒæŠ€æœ¯è¿›ä¸€æ­¥ä¼˜åŒ–æ¸²æŸ“æ•ˆæœã€‚</li>
<li>é‡‡ç”¨åœºæ™¯ç©ºé—´åˆ†å—å’ŒåŠ¨æ€åŠ è½½ç›¸å…³é«˜æ–¯å€¼çš„æ–¹æ³•ï¼Œé™ä½å†…å­˜å¼€é”€ã€‚</li>
<li>é‡‡ç”¨äº†é€æ˜åº¦æ··åˆæœºåˆ¶ï¼Œé¿å…å—è¾¹ç•Œå¤„çš„è§†è§‰ä¼ªå½±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-486b9dc1c75aa076019c2515e5fd01f5" align="middle">
<img src="https://picx.zhimg.com/v2-d1be0481afccd1378c703bd48f31726c" align="middle">
<img src="https://picx.zhimg.com/v2-45ce03e1287066d6975403723b5f0728" align="middle">
<img src="https://picx.zhimg.com/v2-568135090e32db17615de91c3aba66ab" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="NVS-SQA-Exploring-Self-Supervised-Quality-Representation-Learning-for-Neurally-Synthesized-Scenes-without-References"><a href="#NVS-SQA-Exploring-Self-Supervised-Quality-Representation-Learning-for-Neurally-Synthesized-Scenes-without-References" class="headerlink" title="NVS-SQA: Exploring Self-Supervised Quality Representation Learning for   Neurally Synthesized Scenes without References"></a>NVS-SQA: Exploring Self-Supervised Quality Representation Learning for   Neurally Synthesized Scenes without References</h2><p><strong>Authors:Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Weidong Cai, Tongliang Liu</strong></p>
<p>Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting, effectively creates photorealistic scenes from sparse viewpoints, typically evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However, these full-reference methods, which compare synthesized views to reference views, may not fully capture the perceptual quality of neurally synthesized scenes (NSS), particularly due to the limited availability of dense reference views. Furthermore, the challenges in acquiring human perceptual labels hinder the creation of extensive labeled datasets, risking model overfitting and reduced generalizability. To address these issues, we propose NVS-SQA, a NSS quality assessment method to learn no-reference quality representations through self-supervision without reliance on human labels. Traditional self-supervised learning predominantly relies on the â€œsame instance, similar representationâ€ assumption and extensive datasets. However, given that these conditions do not apply in NSS quality assessment, we employ heuristic cues and quality scores as learning objectives, along with a specialized contrastive pair preparation process to improve the effectiveness and efficiency of learning. The results show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e., on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second best) and even exceeds 16 full-reference methods across all evaluation metrics (i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best). </p>
<blockquote>
<p>ç¥ç»è§†å›¾åˆæˆï¼ˆNVSï¼‰ï¼Œå¦‚NeRFå’Œ3Dé«˜æ–¯å±•å¸ƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»ç¨€ç–è§†è§’ç”Ÿæˆé€¼çœŸçš„åœºæ™¯ï¼Œé€šå¸¸é€šè¿‡PSNRã€SSIMå’ŒLPIPSç­‰è´¨é‡è¯„ä¼°æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚ç„¶è€Œï¼Œè¿™äº›å…¨å‚è€ƒæ–¹æ³•å°†åˆæˆè§†å›¾ä¸å‚è€ƒè§†å›¾è¿›è¡Œæ¯”è¾ƒï¼Œå¯èƒ½æ— æ³•å®Œå…¨æ•æ‰ç¥ç»åˆæˆåœºæ™¯ï¼ˆNSSï¼‰çš„æ„ŸçŸ¥è´¨é‡ï¼Œå°¤å…¶æ˜¯å› ä¸ºå¯†é›†å‚è€ƒè§†å›¾çš„å¯è·å¾—æ€§æœ‰é™ã€‚æ­¤å¤–ï¼Œè·å–äººç±»æ„ŸçŸ¥æ ‡ç­¾çš„æŒ‘æˆ˜é˜»ç¢äº†å¤§è§„æ¨¡æ ‡è®°æ•°æ®é›†çš„åˆ›å»ºï¼Œä»è€Œå¢åŠ äº†æ¨¡å‹è¿‡åº¦æ‹Ÿåˆå’Œæ³›åŒ–èƒ½åŠ›é™ä½çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†NVS-SQAï¼Œè¿™æ˜¯ä¸€ç§NSSè´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ— å‚è€ƒè´¨é‡è¡¨ç¤ºï¼Œè€Œä¸ä¾èµ–äºäººå·¥æ ‡ç­¾ã€‚è™½ç„¶ä¼ ç»Ÿè‡ªç›‘ç£å­¦ä¹ ä¸»è¦ä¾èµ–äºâ€œåŒä¸€å®ä¾‹ï¼Œç›¸ä¼¼è¡¨ç¤ºâ€çš„å‡è®¾å’Œå¤§é‡æ•°æ®é›†ï¼Œä½†ç”±äºè¿™äº›æ¡ä»¶ä¸é€‚ç”¨äºNSSè´¨é‡è¯„ä¼°ï¼Œæˆ‘ä»¬é‡‡ç”¨å¯å‘å¼çº¿ç´¢å’Œè´¨é‡åˆ†æ•°ä½œä¸ºå­¦ä¹ ç›®æ ‡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„å¯¹æ¯”å¯¹å‡†å¤‡è¿‡ç¨‹ï¼Œä»¥æé«˜å­¦ä¹ çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ç»“æœè¡¨æ˜ï¼ŒNVS-SQAåœ¨17ç§æ— å‚è€ƒæ–¹æ³•ä¸­æœ‰å¾ˆå¤§çš„ä¼˜åŠ¿ï¼ˆä¾‹å¦‚ï¼ŒSRCCå¹³å‡æé«˜109.5%ï¼ŒPLCCæé«˜98.6%ï¼ŒKRCCæé«˜91.5%ï¼‰ï¼›åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šï¼Œç”šè‡³è¶…è¿‡äº†16ç§å…¨å‚è€ƒæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒSRCCæé«˜22.9%ï¼ŒPLCCæé«˜19.1%ï¼ŒKRCCæé«˜18.6%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06488v3">PDF</a> Accepted by TPAMI</p>
<p><strong>Summary</strong><br>     ç¥ç»ç½‘ç»œè§†å›¾åˆæˆï¼ˆNVSï¼‰å¦‚NeRFå’Œ3Dé«˜æ–¯å–·æ¶‚ç­‰æŠ€æœ¯ï¼Œèƒ½ä»ç¨€ç–è§†è§’ç”Ÿæˆé€¼çœŸçš„åœºæ™¯ã€‚ä½†ç”±äºç¼ºä¹å¯†é›†å‚è€ƒè§†è§’å’Œéš¾ä»¥è·å–äººç±»æ„ŸçŸ¥æ ‡ç­¾ï¼Œç°æœ‰çš„è´¨é‡è¯„ä¼°æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºNVS-SQAæ— å‚è€ƒè´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ— éœ€ä¾èµ–äººç±»æ ‡ç­¾å­¦ä¹ è´¨é‡è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¯å‘å¼çº¿ç´¢å’Œè´¨é‡åˆ†æ•°ä½œä¸ºå­¦ä¹ ç›®æ ‡ï¼Œå¹¶è®¾è®¡å¯¹æ¯”é…å¯¹å‡†å¤‡è¿‡ç¨‹ä»¥æé«˜å­¦ä¹ å’Œæ•ˆç‡ã€‚NVS-SQAåœ¨æ€§èƒ½ä¸Šå¤§å¹…è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œè§†å›¾åˆæˆï¼ˆNVSï¼‰æŠ€æœ¯å¦‚NeRFèƒ½ç”Ÿæˆé€¼çœŸçš„åœºæ™¯ï¼Œä½†è´¨é‡è¯„ä¼°å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç”±äºç¼ºä¹å¯†é›†å‚è€ƒè§†è§’å’Œéš¾ä»¥è·å–äººç±»æ„ŸçŸ¥æ ‡ç­¾ï¼Œç°æœ‰çš„å…¨å‚è€ƒè´¨é‡è¯„ä¼°æ–¹æ³•å¯èƒ½æ— æ³•å®Œå…¨æ•æ‰ç¥ç»åˆæˆåœºæ™¯ï¼ˆNSSï¼‰çš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>æå‡ºNVS-SQAæ–¹æ³•ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ— éœ€ä¾èµ–äººç±»æ ‡ç­¾å­¦ä¹ æ— å‚è€ƒè´¨é‡è¡¨ç¤ºã€‚</li>
<li>NVS-SQAé‡‡ç”¨å¯å‘å¼çº¿ç´¢å’Œè´¨é‡åˆ†æ•°ä½œä¸ºå­¦ä¹ ç›®æ ‡ï¼Œå¹¶è®¾è®¡å¯¹æ¯”é…å¯¹å‡†å¤‡è¿‡ç¨‹æé«˜å­¦ä¹ å’Œæ•ˆç‡ã€‚</li>
<li>NVS-SQAåœ¨æ€§èƒ½ä¸Šå¤§å¹…è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹³å‡åœ¨SRCCã€PLCCå’ŒKRCCä¸Šåˆ†åˆ«é«˜å‡ºç¬¬äºŒåæ–¹æ³•109.5%ã€98.6%å’Œ91.5%ã€‚</li>
<li>NVS-SQAç”šè‡³åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½è¶…è¿‡äº†éƒ¨åˆ†å…¨å‚è€ƒæ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8a36f80c5fe9d3fe9319555e583d6a9" align="middle">
<img src="https://picx.zhimg.com/v2-f5b1d86b88d8edaf72dcf1d16f3cc1d2" align="middle">
<img src="https://picx.zhimg.com/v2-eee994e5e363ab4f761b7b3aff4c8e6d" align="middle">
<img src="https://picx.zhimg.com/v2-489cabe05a87faee8d19db455dfe4c3b" align="middle">
<img src="https://picx.zhimg.com/v2-aef92708b64f552e887df56858a02a74" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="UV-Attack-Physical-World-Adversarial-Attacks-for-Person-Detection-via-Dynamic-NeRF-based-UV-Mapping"><a href="#UV-Attack-Physical-World-Adversarial-Attacks-for-Person-Detection-via-Dynamic-NeRF-based-UV-Mapping" class="headerlink" title="UV-Attack: Physical-World Adversarial Attacks for Person Detection via   Dynamic-NeRF-based UV Mapping"></a>UV-Attack: Physical-World Adversarial Attacks for Person Detection via   Dynamic-NeRF-based UV Mapping</h2><p><strong>Authors:Yanjie Li, Kaisheng Liang, Bin Xiao</strong></p>
<p>In recent research, adversarial attacks on person detectors using patches or static 3D model-based texture modifications have struggled with low success rates due to the flexible nature of human movement. Modeling the 3D deformations caused by various actions has been a major challenge. Fortunately, advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer new possibilities. In this paper, we introduce UV-Attack, a groundbreaking approach that achieves high success rates even with extensive and unseen human actions. We address the challenge above by leveraging dynamic-NeRF-based UV mapping. UV-Attack can generate human images across diverse actions and viewpoints, and even create novel actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying clothing textures is challenging because they are embedded in neural network parameters. To tackle this, UV-Attack generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes the attack more practical. We also propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views. Our experiments show that UV-Attack achieves a 92.7% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly outperforming the state-of-the-art AdvCamou attack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the latest YOLOv8 detector in black-box settings. This work highlights the potential of dynamic NeRF-based UV mapping for creating more effective adversarial attacks on person detectors, addressing key challenges in modeling human movement and texture modification. The code is available at <a target="_blank" rel="noopener" href="https://github.com/PolyLiYJ/UV-Attack">https://github.com/PolyLiYJ/UV-Attack</a>. </p>
<blockquote>
<p>åœ¨æœ€è¿‘çš„ç ”ç©¶ä¸­ï¼Œåˆ©ç”¨è¡¥ä¸æˆ–åŸºäºé™æ€3Dæ¨¡å‹çš„çº¹ç†ä¿®æ”¹å¯¹äººå‘˜æ£€æµ‹å™¨è¿›è¡Œå¯¹æŠ—æ€§æ”»å‡»çš„æˆåŠŸç‡è¾ƒä½ï¼Œè¿™æ˜¯ç”±äºäººç±»è¿åŠ¨çš„çµæ´»æ€§æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚å¯¹ç”±å„ç§åŠ¨ä½œå¼•èµ·çš„3Då˜å½¢è¿›è¡Œå»ºæ¨¡ä¸€ç›´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚å¹¸è¿çš„æ˜¯ï¼Œç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰åœ¨åŠ¨æ€äººä½“å»ºæ¨¡æ–¹é¢çš„è¿›å±•æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†UV-Attackï¼Œè¿™æ˜¯ä¸€ç§çªç ´æ€§çš„æ–¹æ³•ï¼Œå³ä½¿é¢å¯¹å¹¿æ³›ä¸”æœªè§çš„äººç±»åŠ¨ä½œï¼Œä¹Ÿèƒ½å®ç°è¾ƒé«˜çš„æˆåŠŸç‡ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨åŸºäºåŠ¨æ€NeRFçš„UVæ˜ å°„æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚UV-Attackå¯ä»¥ç”Ÿæˆå„ç§åŠ¨ä½œå’Œè§†è§’ä¸‹çš„äººç±»å›¾åƒï¼Œç”šè‡³å¯ä»¥é€šè¿‡é‡‡æ ·SMPLå‚æ•°ç©ºé—´æ¥åˆ›å»ºæ–°çš„åŠ¨ä½œã€‚è™½ç„¶åŠ¨æ€NeRFæ¨¡å‹èƒ½å¤Ÿå¯¹äººä½“è¿›è¡Œå»ºæ¨¡ï¼Œä½†ä¿®æ”¹è¡£ç‰©çº¹ç†å´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬è¢«åµŒå…¥åœ¨ç¥ç»ç½‘ç»œå‚æ•°ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒUV-Attackç”ŸæˆUVåœ°å›¾è€Œä¸æ˜¯RGBå›¾åƒï¼Œå¹¶ä¿®æ”¹çº¹ç†å †æ ˆã€‚è¿™ç§æ–¹æ³•å®ç°äº†å®æ—¶çº¹ç†ç¼–è¾‘ï¼Œä½¿æ”»å‡»æ›´åŠ å®ç”¨ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºå§¿æ€å˜æ¢æœŸæœ›çš„æŸå¤±ï¼ˆEoPTï¼‰ï¼Œä»¥æé«˜åœ¨æœªè§å§¿æ€å’Œè§†è§’ä¸‹çš„èº²é¿æˆåŠŸç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨åŠ¨æ€è§†é¢‘è®¾ç½®ä¸­ï¼Œé’ˆå¯¹å„ç§å§¿æ€çš„FastRCNNæ¨¡å‹ï¼ŒUV-Attackçš„æ”»å‡»æˆåŠŸç‡è¾¾åˆ°äº†92.7%ï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„AdvCamouæ”»å‡»ï¼Œå…¶ä»…æœ‰28.5%çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨é»‘ç®±è®¾ç½®ä¸­å®ç°äº†å¯¹æœ€æ–°YOLOv8æ£€æµ‹å™¨çš„49.5%ASRã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†åŠ¨æ€NeRFåŸºUVæ˜ å°„åœ¨åˆ›å»ºé’ˆå¯¹äººå‘˜æ£€æµ‹å™¨çš„æ›´æœ‰æ•ˆå¯¹æŠ—æ€§æ”»å‡»æ–¹é¢çš„æ½œåŠ›ï¼Œè§£å†³äº†å»ºæ¨¡äººç±»è¿åŠ¨å’Œçº¹ç†ä¿®æ”¹æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/PolyLiYJ/UV-Attack%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/PolyLiYJ/UV-Attackæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05783v2">PDF</a> 23 pages, 22 figures, accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºNeRFçš„åŠ¨æ€äººç±»å»ºæ¨¡æŠ€æœ¯ä¸ºæ”»å‡»äººç‰©æ£€æµ‹å™¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚æœ€æ–°ç ”ç©¶çš„UV-Attackæ–¹æ³•é€šè¿‡åˆ©ç”¨åŠ¨æ€NeRFçš„UVæ˜ å°„æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åŠ¨ä½œå’Œè§†è§’ä¸‹ç”Ÿæˆäººç±»å›¾åƒï¼Œå¹¶åˆ›å»ºæ–°å‹åŠ¨ä½œã€‚è¯¥æ–¹æ³•åœ¨å®æ—¶çº¹ç†ç¼–è¾‘å’Œæ”»å‡»å®ç”¨æ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼ŒæˆåŠŸè§£å†³äº†äººä½“åŠ¨ä½œå’Œçº¹ç†ä¿®æ”¹å»ºæ¨¡ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼ŒUV-Attackåœ¨åŠ¨æ€è§†é¢‘è®¾ç½®ä¸­å¯¹FastRCNNæ¨¡å‹çš„æ”»å‡»æˆåŠŸç‡è¾¾åˆ°92.7%ï¼Œæ˜¾è‘—ä¼˜äºä»…å…·æœ‰28.5%ASRçš„AdvCamouæ”»å‡»ã€‚æ­¤å¤–ï¼Œåœ¨é»‘è‰²ç›’å­è®¾ç½®ä¸‹ï¼Œå¯¹æœ€æ–°çš„YOLOv8æ£€æµ‹å™¨çš„ASRè¾¾åˆ°49.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UV-Attackåˆ©ç”¨NeRFæŠ€æœ¯å®ç°åŠ¨æ€äººç±»å»ºæ¨¡ï¼Œæé«˜äº†æ”»å‡»äººç‰©æ£€æµ‹å™¨çš„æˆåŠŸç‡ã€‚</li>
<li>UV-Attackè§£å†³äº†ä¼ ç»Ÿè¡¥ä¸æˆ–é™æ€3Dæ¨¡å‹çº¹ç†ä¿®æ”¹å¯¹åŠ¨æ€äººç±»åŠ¨ä½œçš„å»ºæ¨¡æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨åŠ¨æ€NeRFçš„UVæ˜ å°„æŠ€æœ¯ï¼ŒUV-Attackå¯ä»¥ç”Ÿæˆä¸åŒåŠ¨ä½œå’Œè§†è§’ä¸‹çš„å›¾åƒå¹¶åˆ›å»ºæ–°å‹åŠ¨ä½œã€‚</li>
<li>UV-Attackå®ç°äº†å®æ—¶çº¹ç†ç¼–è¾‘ï¼Œæé«˜äº†æ”»å‡»çš„å®ç”¨æ€§ã€‚</li>
<li>UV-Attacké€šè¿‡å¼•å…¥æœŸæœ›å§¿æ€å˜æ¢æŸå¤±ï¼ˆEoPTï¼‰æé«˜äº†æœªè§å§¿æ€å’Œè§†è§’çš„é€ƒé¿æˆåŠŸç‡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒUV-Attackåœ¨åŠ¨æ€è§†é¢‘è®¾ç½®ä¸­å¯¹FastRCNNæ¨¡å‹çš„æ”»å‡»æˆåŠŸç‡è¿œé«˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b0913f83722092c0742ae912f25d971" align="middle">
<img src="https://picx.zhimg.com/v2-59f2b00e13844a31974a0d28da08dde6" align="middle">
<img src="https://picx.zhimg.com/v2-1b7d7d400b662e61dd7a628c98db7d9c" align="middle">
<img src="https://picx.zhimg.com/v2-23f453acc7e600b0731afcea458bdc90" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GS-ProCams-Gaussian-Splatting-based-Projector-Camera-Systems"><a href="#GS-ProCams-Gaussian-Splatting-based-Projector-Camera-Systems" class="headerlink" title="GS-ProCams: Gaussian Splatting-based Projector-Camera Systems"></a>GS-ProCams: Gaussian Splatting-based Projector-Camera Systems</h2><p><strong>Authors:Qingyue Deng, Jijiang Li, Haibin Ling, Bingyao Huang</strong></p>
<p>We present GS-ProCams, the first Gaussian Splatting-based framework for projector-camera systems (ProCams). GS-ProCams is not only view-agnostic but also significantly enhances the efficiency of projection mapping (PM) that requires establishing geometric and radiometric mappings between the projector and the camera. Previous CNN-based ProCams are constrained to a specific viewpoint, limiting their applicability to novel perspectives. In contrast, NeRF-based ProCams support view-agnostic projection mapping, however, they require an additional co-located light source and demand significant computational and memory resources. To address this issue, we propose GS-ProCams that employs 2D Gaussian for scene representations, and enables efficient view-agnostic ProCams applications. In particular, we explicitly model the complex geometric and photometric mappings of ProCams using projector responses, the projection surfaceâ€™s geometry and materials represented by Gaussians, and the global illumination component. Then, we employ differentiable physically-based rendering to jointly estimate them from captured multi-view projections. Compared to state-of-the-art NeRF-based methods, our GS-ProCams eliminates the need for additional devices, achieving superior ProCams simulation quality. It also uses only 1&#x2F;10 of the GPU memory for training and is 900 times faster in inference speed. Please refer to our project page for the code and dataset: <a target="_blank" rel="noopener" href="https://realqingyue.github.io/GS-ProCams/">https://realqingyue.github.io/GS-ProCams/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†GS-ProCamsï¼Œè¿™æ˜¯åŸºäºé«˜æ–¯å±•å¸ƒï¼ˆGaussian Splattingï¼‰çš„æŠ•å½±ä»ªæ‘„åƒå¤´ç³»ç»Ÿï¼ˆProCamsï¼‰çš„é¦–ä¸ªæ¡†æ¶ã€‚GS-ProCamsä¸ä»…ä¸å—è§†è§’çš„é™åˆ¶ï¼Œè¿˜èƒ½æ˜¾è‘—æé«˜æŠ•å½±ä»ªæ˜ å°„ï¼ˆPMï¼‰çš„æ•ˆç‡ï¼Œåè€…éœ€è¦åœ¨æŠ•å½±ä»ªå’Œæ‘„åƒå¤´ä¹‹é—´å»ºç«‹å‡ ä½•å’Œè¾å°„åº¦é‡æ˜ å°„ã€‚ä»¥å‰çš„åŸºäºCNNçš„ProCamså—é™äºç‰¹å®šçš„è§†è§’ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ–°å‹è§†è§’ä¸‹çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºNeRFçš„ProCamsæ”¯æŒä¸å—è§†è§’é™åˆ¶çš„æŠ•å½±æ˜ å°„ï¼Œä½†å®ƒä»¬éœ€è¦é¢å¤–çš„å…±ç½®å…‰æºï¼Œå¹¶éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GS-ProCamsï¼Œå®ƒé‡‡ç”¨2Dé«˜æ–¯è¿›è¡Œåœºæ™¯è¡¨ç¤ºï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„è§†è§’æ— å…³ProCamsåº”ç”¨ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ•å½±ä»ªå“åº”æ¥æ˜¾å¼åœ°å»ºæ¨¡ProCamsçš„å¤æ‚å‡ ä½•å’Œå…‰åº¦æ˜ å°„ï¼Œä»¥åŠç”±é«˜æ–¯è¡¨ç¤ºçš„æŠ•å½±è¡¨é¢çš„å‡ ä½•å½¢çŠ¶å’Œææ–™ï¼Œå’Œå…¨å±€ç…§æ˜ç»„ä»¶ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºç‰©ç†çš„å¯å¾®æ¸²æŸ“æŠ€æœ¯ï¼Œä»æ•è·çš„å¤šè§†è§’æŠ•å½±ä¸­è”åˆä¼°è®¡å®ƒä»¬ã€‚ä¸æœ€å…ˆè¿›çš„åŸºäºNeRFçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„GS-ProCamsä¸éœ€è¦é¢å¤–çš„è®¾å¤‡ï¼Œå®ç°äº†æ›´é«˜çš„ProCamsæ¨¡æ‹Ÿè´¨é‡ã€‚å®ƒçš„è®­ç»ƒåªéœ€ä½¿ç”¨ååˆ†ä¹‹ä¸€ä¸åˆ°çš„GPUå†…å­˜ï¼Œæ¨ç†é€Ÿåº¦ä¹Ÿå¿«äº†900å€ã€‚æœ‰å…³ä»£ç å’Œæ•°æ®é›†ï¼Œè¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://realqingyue.github.io/GS-ProCams/%E3%80%82">https://realqingyue.github.io/GS-ProCams/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11762v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§åŸºäºé«˜æ–¯èåˆæŠ€æœ¯çš„æŠ•å½±ä»ªç›¸æœºç³»ç»Ÿï¼ˆGS-ProCamsï¼‰ã€‚ç›¸è¾ƒäºä»¥å¾€çš„æ–¹æ³•ï¼Œå®ƒä¸ä»…è§†è§’æ— å…³ï¼Œå¢å¼ºäº†æŠ•å½±æ˜ å°„çš„æ•ˆç‡ï¼Œæ— éœ€é¢å¤–çš„å…‰æºè®¾å¤‡ã€‚é‡‡ç”¨äºŒç»´é«˜æ–¯å‡½æ•°è¡¨è¾¾åœºæ™¯ï¼Œæ„å»ºå¤æ‚å‡ ä½•å’Œå…‰åº¦æ˜ å°„æ¨¡å‹ï¼Œé€šè¿‡ç‰©ç†æ¸²æŸ“æŠ€æœ¯å®ç°å¤šè§†è§’æŠ•å½±çš„è”åˆä¼°è®¡ã€‚ç›¸è¾ƒäºåŸºäºNeRFçš„æ–¹æ³•ï¼ŒGS-ProCamsæ¨¡æ‹Ÿè´¨é‡æ›´ä¼˜ï¼Œä½¿ç”¨GPUå†…å­˜ä»…ä¸ºååˆ†ä¹‹ä¸€ï¼Œæ¨ç†é€Ÿåº¦æå‡900å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GS-ProCamsæ˜¯ä¸€ä¸ªåŸºäºé«˜æ–¯èåˆæŠ€æœ¯çš„æŠ•å½±ä»ªç›¸æœºç³»ç»Ÿæ¡†æ¶ã€‚</li>
<li>å®ƒè§£å†³äº†ä»¥å¾€è§†è§’ç›¸å…³æ€§çš„æŠ•å½±ä»ªç›¸æœºç³»ç»Ÿçš„å±€é™ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨äºŒç»´é«˜æ–¯å‡½æ•°è¡¨è¾¾åœºæ™¯æé«˜äº†æŠ•å½±æ˜ å°„çš„æ•ˆç‡ã€‚</li>
<li>æ„å»ºå¤æ‚å‡ ä½•å’Œå…‰åº¦æ˜ å°„æ¨¡å‹ï¼ŒåŒ…æ‹¬æŠ•å½±ä»ªå“åº”ã€æŠ•å½±è¡¨é¢å‡ ä½•å’Œææ–™å±æ€§ä»¥åŠå…¨å±€ç…§æ˜æˆåˆ†ã€‚</li>
<li>åˆ©ç”¨ç‰©ç†æ¸²æŸ“æŠ€æœ¯ä»å¤šè§†è§’æŠ•å½±ä¸­è”åˆä¼°è®¡è¿™äº›å‚æ•°ã€‚</li>
<li>GS-ProCamsç›¸è¾ƒäºåŸºäºNeRFçš„æ–¹æ³•åœ¨æ¨¡æ‹Ÿè´¨é‡ä¸Šæ›´èƒœä¸€ç­¹ï¼Œä½¿ç”¨è¾ƒå°‘çš„GPUèµ„æºå¹¶ä¸”æ¨ç†é€Ÿåº¦æ›´å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d83a28069a505f99f0041d36c85c44f" align="middle">
<img src="https://picx.zhimg.com/v2-241b9a90c64e4fb101633a9fa65c3245" align="middle">
<img src="https://picx.zhimg.com/v2-52391460b5f55bfb00eb2f4b4e5691b9" align="middle">
<img src="https://picx.zhimg.com/v2-7f4512a3d17925f39eef95476e08cfe5" align="middle">
<img src="https://picx.zhimg.com/v2-1224b7f54706ba1e5d7226f26b005081" align="middle">
<img src="https://picx.zhimg.com/v2-f3a2d9e6c2933a86f150e3206f59adfa" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussians"><a href="#MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussians" class="headerlink" title="MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussians"></a>MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussians</h2><p><strong>Authors:Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu</strong></p>
<p>Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We use a unified mixed Gaussian representation to integrate the two modalities of 2D image and 3D mesh. Furthermore, the comprehensive experiments demonstrate the superiority of MixedGaussianAvatar. The code will be released. </p>
<blockquote>
<p>é‡å»ºé«˜ä¿çœŸ3Då¤´åƒåœ¨è™šæ‹Ÿç°å®ä¸­æœ‰ç€å¹¿æ³›åº”ç”¨ï¼Œå…·æœ‰é‡è¦çš„ä»·å€¼ã€‚å…ˆé©±çš„æ–¹æ³•ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é‡å»ºé€¼çœŸçš„å¤´åƒï¼Œä½†å—é™äºè®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ã€‚åŸºäº3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„æœ€æ–°æ–¹æ³•æ˜¾è‘—æé«˜äº†è®­ç»ƒå’Œæ¸²æŸ“çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œ3DGSçš„è¡¨é¢ä¸ä¸€è‡´å¯¼è‡´å‡ ä½•ç²¾åº¦ä¸ä½³ï¼›ä¹‹åçš„2DGSä½¿ç”¨2Dè¡¨é¢æ¥æé«˜å‡ ä½•ç²¾åº¦ï¼Œä½†ç‰ºç‰²äº†æ¸²æŸ“çš„ä¿çœŸåº¦ã€‚ä¸ºäº†ç»“åˆ2DGSå’Œ3DGSçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMixedGaussianAvatarçš„æ–°æ–¹æ³•ï¼Œç”¨äºçœŸå®ä¸”å‡ ä½•å‡†ç¡®çš„å¤´åƒé‡å»ºã€‚æˆ‘ä»¬çš„ä¸»è¦æƒ³æ³•æ˜¯ä½¿ç”¨2Dé«˜æ–¯é‡å»º3Då¤´åƒçš„è¡¨é¢ï¼Œä»¥ç¡®ä¿å‡ ä½•ç²¾åº¦ã€‚æˆ‘ä»¬å°†2Dé«˜æ–¯é™„åŠ åˆ°FLAMEæ¨¡å‹çš„ä¸‰è§’ç½‘æ ¼ä¸Šï¼Œå¹¶åœ¨2DGSæ¸²æŸ“è´¨é‡ä¸è¶³çš„åœ°æ–¹è¿æ¥åˆ°é¢å¤–çš„3Dé«˜æ–¯ï¼Œåˆ›å»ºæ··åˆçš„2D-3Dé«˜æ–¯è¡¨ç¤ºã€‚è¿™äº›2D-3Dé«˜æ–¯å¯ä»¥ä½¿ç”¨FLAMEå‚æ•°è¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ¸è¿›çš„è®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆè®­ç»ƒ2Dé«˜æ–¯ï¼Œç„¶åå¯¹æ··åˆçš„2D-3Dé«˜æ–¯è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬ä½¿ç”¨ç»Ÿä¸€çš„æ··åˆé«˜æ–¯è¡¨ç¤ºæ¥é›†æˆ2Då›¾åƒå’Œ3Dç½‘æ ¼çš„ä¸¤ç§æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç»¼åˆå®éªŒè¯æ˜äº†MixedGaussianAvatarçš„ä¼˜åŠ¿ã€‚ä»£ç å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04955v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯ï¼Œé‡å»ºé«˜ä¿çœŸ3Då¤´åƒåœ¨è™šæ‹Ÿç°å®ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰æ–¹æ³•å¦‚åŸºäº3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„æ–¹æ³•è™½ç„¶æé«˜äº†è®­ç»ƒå’Œæ¸²æŸ“æ•ˆç‡ï¼Œä½†å‡ ä½•ç²¾åº¦æœ‰é™ï¼›è€ŒäºŒç»´é«˜æ–¯æ‹¼è´´ï¼ˆ2DGSï¼‰è™½ç„¶æå‡äº†å‡ ä½•ç²¾åº¦ï¼Œä½†ç‰ºç‰²äº†æ¸²æŸ“è´¨é‡ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMixedGaussianAvatarçš„æ··åˆæ–¹æ³•ï¼Œç»“åˆäºŒç»´é«˜æ–¯ä¸ä¸‰ç»´é«˜æ–¯çš„ä¼˜åŠ¿ï¼Œç¡®ä¿å‡ ä½•ç²¾åº¦ä¸æ¸²æŸ“è´¨é‡ã€‚è¯¥æ–¹æ³•å°†äºŒç»´é«˜æ–¯ç”¨äºé‡å»ºä¸‰ç»´å¤´åƒè¡¨é¢ï¼Œå°†å…¶é™„ç€äºFLAMEæ¨¡å‹çš„ä¸‰è§’ç½‘æ ¼ä¸Šï¼Œå¹¶åœ¨å¿…è¦æ—¶å¢åŠ ä¸‰ç»´é«˜æ–¯ä»¥æé«˜æ¸²æŸ“è´¨é‡ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œå…ˆè®­ç»ƒäºŒç»´é«˜æ–¯ï¼Œå†å¾®è°ƒæ··åˆäºŒç»´-ä¸‰ç»´é«˜æ–¯ã€‚å®éªŒè¯æ˜MixedGaussianAvatarçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MixedGaussianAvatarç»“åˆäº†äºŒç»´é«˜æ–¯å’Œä¸‰ç»´é«˜æ–¯çš„ä¼˜åŠ¿ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸå’Œå‡ ä½•å‡†ç¡®çš„å¤´åƒé‡å»ºã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨äºŒç»´é«˜æ–¯é‡å»ºä¸‰ç»´å¤´åƒè¡¨é¢å¹¶é™„ç€äºFLAMEæ¨¡å‹çš„ä¸‰è§’ç½‘æ ¼ä¸Šã€‚</li>
<li>åœ¨éœ€è¦æ—¶ï¼Œé€šè¿‡å¢åŠ ä¸‰ç»´é«˜æ–¯æ¥æé«˜æ¸²æŸ“è´¨é‡ï¼Œå½¢æˆæ··åˆçš„äºŒç»´-ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨æ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œå…ˆè®­ç»ƒäºŒç»´é«˜æ–¯ï¼Œå†å¾®è°ƒæ··åˆè¡¨ç¤ºã€‚</li>
<li>æ–¹æ³•é€šè¿‡ç»Ÿä¸€çš„æ··åˆé«˜æ–¯è¡¨ç¤ºé›†æˆäº†äºŒç»´å›¾åƒå’Œä¸‰ç»´ç½‘æ ¼çš„ä¸¤ç§æ¨¡å¼ã€‚</li>
<li>å®éªŒè¯æ˜MixedGaussianAvataråœ¨å¤´åƒé‡å»ºæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5cb35835bad466b3d4ed9a8d15a7665e" align="middle">
<img src="https://picx.zhimg.com/v2-1a3f575e5c248324c5057e01ff7ce428" align="middle">
<img src="https://picx.zhimg.com/v2-b8441d0aad2bda3f885a758eb11354df" align="middle">
<img src="https://picx.zhimg.com/v2-458a143d2cb57029074f19c1e7a98771" align="middle">
<img src="https://picx.zhimg.com/v2-5721eb29f4a3354c159607e25342ed94" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Gaussian-Splashing-Direct-Volumetric-Rendering-Underwater"><a href="#Gaussian-Splashing-Direct-Volumetric-Rendering-Underwater" class="headerlink" title="Gaussian Splashing: Direct Volumetric Rendering Underwater"></a>Gaussian Splashing: Direct Volumetric Rendering Underwater</h2><p><strong>Authors:Nir Mualem, Roy Amoyal, Oren Freifeld, Derya Akkaynak</strong></p>
<p>In underwater images, most useful features are occluded by water. The extent of the occlusion depends on imaging geometry and can vary even across a sequence of burst images. As a result, 3D reconstruction methods robust on in-air scenes, like Neural Radiance Field methods (NeRFs) or 3D Gaussian Splatting (3DGS), fail on underwater scenes. While a recent underwater adaptation of NeRFs achieved state-of-the-art results, it is impractically slow: reconstruction takes hours and its rendering rate, in frames per second (FPS), is less than 1. Here, we present a new method that takes only a few minutes for reconstruction and renders novel underwater scenes at 140 FPS. Named Gaussian Splashing, our method unifies the strengths and speed of 3DGS with an image formation model for capturing scattering, introducing innovations in the rendering and depth estimation procedures and in the 3DGS loss function. Despite the complexities of underwater adaptation, our method produces images at unparalleled speeds with superior details. Moreover, it reveals distant scene details with far greater clarity than other methods, dramatically improving reconstructed and rendered images. We demonstrate results on existing datasets and a new dataset we have collected.   Additional visual results are available at: <a target="_blank" rel="noopener" href="https://bgu-cs-vil.github.io/gaussiansplashingUW.github.io/">https://bgu-cs-vil.github.io/gaussiansplashingUW.github.io/</a> . </p>
<blockquote>
<p>åœ¨æ°´ä¸‹å›¾åƒä¸­ï¼Œå¤§å¤šæ•°æœ‰ç”¨çš„ç‰¹å¾éƒ½è¢«æ°´é®æŒ¡ä½äº†ã€‚é®æŒ¡çš„ç¨‹åº¦å–å†³äºæˆåƒå‡ ä½•ï¼Œç”šè‡³åœ¨è¿ç»­çš„å¤šå¸§å›¾åƒä¸­ä¹Ÿä¼šæœ‰æ‰€ä¸åŒã€‚å› æ­¤ï¼Œåœ¨ç©ºæ°”åœºæ™¯ä¸­ç¨³å¥çš„3Dé‡å»ºæ–¹æ³•ï¼Œå¦‚ç¥ç»ç½‘ç»œè¾å°„åœºæ–¹æ³•ï¼ˆNeRFsï¼‰æˆ–ä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰ï¼Œåœ¨æ°´ä¸‹åœºæ™¯ä¸­éƒ½ä¼šå¤±æ•ˆã€‚è™½ç„¶æœ€è¿‘å¯¹NeRFsçš„æ°´ä¸‹é€‚åº”æ€§æ”¹è¿›å–å¾—äº†æœ€æ–°ç»“æœï¼Œä½†å…¶å¤„ç†é€Ÿåº¦è¿‡äºç¼“æ…¢ï¼šé‡å»ºéœ€è¦æ•°å°æ—¶ï¼Œå¹¶ä¸”æ¯ç§’æ¸²æŸ“å¸§æ•°ï¼ˆFPSï¼‰ä½äº1ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆé‡å»ºï¼Œå¹¶ä»¥æ¯ç§’140å¸§çš„é€Ÿåº¦æ¸²æŸ“æ–°çš„æ°´ä¸‹åœºæ™¯ã€‚æˆ‘ä»¬å°†å…¶å‘½åä¸ºé«˜æ–¯é£æº…æ³•ï¼ˆGaussian Splashingï¼‰ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ä¸‰ç»´é«˜æ–¯å¹³é“ºçš„ä¼˜ç‚¹å’Œé€Ÿåº¦ï¼ŒåŒæ—¶é‡‡ç”¨äº†å›¾åƒå½¢æˆæ¨¡å‹æ¥æ•æ‰æ•£å°„ç°è±¡ï¼Œå¹¶åœ¨æ¸²æŸ“å’Œæ·±åº¦ä¼°è®¡ç¨‹åºä»¥åŠä¸‰ç»´é«˜æ–¯å¹³é“ºçš„æŸå¤±å‡½æ•°ä¸­å¼•å…¥äº†åˆ›æ–°ã€‚å°½ç®¡æ°´ä¸‹é€‚åº”çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥æ— ä¸ä¼¦æ¯”çš„é€Ÿåº¦ç”Ÿæˆå…·æœ‰å“è¶Šç»†èŠ‚çš„å›¾åƒã€‚æ­¤å¤–ï¼Œå®ƒæ¯”å…¶ä»–æ–¹æ³•æ›´æ¸…æ™°åœ°æ­ç¤ºäº†è¿œå¤„çš„åœºæ™¯ç»†èŠ‚ï¼Œæå¤§åœ°æé«˜äº†é‡å»ºå’Œæ¸²æŸ“çš„å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬åœ¨ç°æœ‰çš„æ•°æ®é›†å’Œæˆ‘ä»¬æ–°æ”¶é›†çš„æ•°æ®é›†ä¸Šå±•ç¤ºäº†ç»“æœã€‚æ›´å¤šè§†è§‰ç»“æœå¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://bgu-cs-vil.github.io/gaussiansplashingUW.github.io%E3%80%82">https://bgu-cs-vil.github.io/gaussiansplashingUW.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19588v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æ°´ä¸‹å›¾åƒä¸­ï¼Œç”±äºæ°´çš„å½±å“ï¼Œå¤§å¤šæ•°æœ‰ç”¨ç‰¹å¾éƒ½è¢«é®æŒ¡äº†ã€‚æˆåƒå‡ ä½•å†³å®šé®æŒ¡ç¨‹åº¦ï¼Œå³ä½¿åœ¨è¿ç»­å›¾åƒåºåˆ—ä¸­ä¹Ÿä¼šæœ‰æ‰€ä¸åŒã€‚å› æ­¤ï¼Œåœ¨ç©ºæ°”åœºæ™¯æœ‰æ•ˆçš„ä¸‰ç»´é‡å»ºæ–¹æ³•ï¼Œå¦‚ç¥ç»ç½‘ç»œè¾å°„åœºæ–¹æ³•ï¼ˆNeRFsï¼‰æˆ–ä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹ï¼ˆ3DGSï¼‰ï¼Œåœ¨æ°´ä¸‹åœºæ™¯ä¸­åˆ™è¡¨ç°ä¸ä½³ã€‚è™½ç„¶æœ€è¿‘å¯¹NeRFsçš„æ°´ä¸‹é€‚åº”è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œä½†å®ƒéå¸¸è€—æ—¶ï¼šé‡å»ºéœ€è¦æ•°å°æ—¶ï¼Œå¹¶ä¸”å…¶æ¸²æŸ“é€Ÿåº¦æ¯ç§’å¸§æ•°ï¼ˆFPSï¼‰ä½äºä¸€å¸§ã€‚è¿™é‡Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå®ƒèƒ½åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆé‡å»ºå¹¶ä»¥æ¯ç§’140å¸§çš„é€Ÿåº¦æ¸²æŸ“æ°´ä¸‹æ–°åœºæ™¯ã€‚åä¸ºé«˜æ–¯é£æº…çš„æ–¹æ³•ç»“åˆäº†ä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹çš„å¼ºåº¦ä¸é€Ÿåº¦ï¼Œå¹¶é‡‡ç”¨äº†å›¾åƒå½¢æˆæ¨¡å‹æ¥æ•æ‰æ•£å°„ç°è±¡ï¼ŒåŒæ—¶æ”¹è¿›äº†æ¸²æŸ“å’Œæ·±åº¦ä¼°è®¡ç¨‹åºä»¥åŠä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹çš„æŸå¤±å‡½æ•°ã€‚å°½ç®¡æ°´ä¸‹é€‚åº”çš„å¤æ‚æ€§ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»¥æ— ä¸ä¼¦æ¯”çš„é€Ÿåº¦ç”Ÿæˆå…·æœ‰ä¼˜è¶Šç»†èŠ‚çš„å›¾åƒã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒèƒ½å¤Ÿæ­ç¤ºæ›´è¿œçš„åœºæ™¯ç»†èŠ‚ï¼Œæå¤§åœ°æé«˜äº†é‡å»ºå’Œæ¸²æŸ“çš„å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬åœ¨ç°æœ‰çš„æ•°æ®é›†å’Œæˆ‘ä»¬æ”¶é›†çš„æ–°æ•°æ®é›†ä¸Šå±•ç¤ºäº†ç»“æœã€‚æ›´å¤šè§†è§‰ç»“æœå¯é€šè¿‡é“¾æ¥æŸ¥çœ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å›¾åƒçš„ç‰¹å¾å¸¸å¸¸è¢«æ°´é®æŒ¡ï¼Œå½±å“ç¨‹åº¦å–å†³äºæˆåƒå‡ ä½•ã€‚</li>
<li>å½“å‰çš„ä¸‰ç»´é‡å»ºæ–¹æ³•åœ¨ç©ºæ°”åœºæ™¯ä¸­æ•ˆæœå¾ˆå¥½ï¼Œä½†åœ¨æ°´ä¸‹åœºæ™¯ä¸­æ•ˆæœæ¬ ä½³ã€‚</li>
<li>ä¸€ç§æ–°çš„æ°´ä¸‹å›¾åƒå¤„ç†æ–¹æ³•â€”â€”é«˜æ–¯é£æº…ç»“åˆäº†ä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹çš„ä¼˜åŠ¿å’Œé€Ÿåº¦ï¼Œå¹¶è€ƒè™‘äº†å›¾åƒå½¢æˆä¸­çš„æ•£å°„ç°è±¡ã€‚</li>
<li>é«˜æ–¯é£æº…æ–¹æ³•èƒ½å¤Ÿåœ¨å‡ åˆ†é’Ÿå†…å®Œæˆé‡å»ºå¹¶ä»¥æ¯ç§’é«˜è¾¾140å¸§çš„é€Ÿåº¦æ¸²æŸ“æ°´ä¸‹æ–°åœºæ™¯ã€‚</li>
<li>é«˜æ–¯é£æº…æ–¹æ³•åœ¨æ¸²æŸ“é€Ÿåº¦å’Œå›¾åƒè´¨é‡æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯èƒ½å¤Ÿæ­ç¤ºæ›´è¿œåœºæ™¯ç»†èŠ‚çš„èƒ½åŠ›ã€‚</li>
<li>é«˜æ–¯é£æº…æ–¹æ³•é€šè¿‡æ”¹è¿›æ¸²æŸ“å’Œæ·±åº¦ä¼°è®¡ç¨‹åºä»¥åŠä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹çš„æŸå¤±å‡½æ•°æ¥å®ç°å…¶é«˜æ•ˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55d149c46adf3853880f3476e4d01aa2" align="middle">
<img src="https://picx.zhimg.com/v2-17b70bb6687c0e341cc405ea0d2ebe47" align="middle">
<img src="https://picx.zhimg.com/v2-73ffebfdd9b9c86bdd553fc0d74142cb" align="middle">
<img src="https://picx.zhimg.com/v2-095e04379f76763ca61c42ae50eca409" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EF-3DGS-Event-Aided-Free-Trajectory-3D-Gaussian-Splatting"><a href="#EF-3DGS-Event-Aided-Free-Trajectory-3D-Gaussian-Splatting" class="headerlink" title="EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting"></a>EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting</h2><p><strong>Authors:Bohao Liao, Wei Zhai, Zengyu Wan, Zhixin Cheng, Wenfei Yang, Tianzhu Zhang, Yang Cao, Zheng-Jun Zha</strong></p>
<p>Scene reconstruction from casually captured videos has wide applications in real-world scenarios. With recent advancements in differentiable rendering techniques, several methods have attempted to simultaneously optimize scene representations (NeRF or 3DGS) and camera poses. Despite recent progress, existing methods relying on traditional camera input tend to fail in high-speed (or equivalently low-frame-rate) scenarios. Event cameras, inspired by biological vision, record pixel-wise intensity changes asynchronously with high temporal resolution, providing valuable scene and motion information in blind inter-frame intervals. In this paper, we introduce the event camera to aid scene construction from a casually captured video for the first time, and propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly integrates the advantages of event cameras into 3DGS through three key components. First, we leverage the Event Generation Model (EGM) to fuse events and frames, supervising the rendered views observed by the event stream. Second, we adopt the Contrast Maximization (CMax) framework in a piece-wise manner to extract motion information by maximizing the contrast of the Image of Warped Events (IWE), thereby calibrating the estimated poses. Besides, based on the Linear Event Generation Model (LEGM), the brightness information encoded in the IWE is also utilized to constrain the 3DGS in the gradient domain. Third, to mitigate the absence of color information of events, we introduce photometric bundle adjustment (PBA) to ensure view consistency across events and frames. We evaluate our method on the public Tanks and Temples benchmark and a newly collected real-world dataset, RealEv-DAVIS. Our project page is <a target="_blank" rel="noopener" href="https://lbh666.github.io/ef-3dgs/">https://lbh666.github.io/ef-3dgs/</a>. </p>
<blockquote>
<p>åœºæ™¯é‡å»ºåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨å¹¿æ³›ï¼Œæ¶‰åŠä»éšæ„æ‹æ‘„çš„è§†é¢‘ä¸­è¿›è¡Œé‡å»ºã€‚éšç€å¯å¾®åˆ†æ¸²æŸ“æŠ€æœ¯çš„æœ€æ–°å‘å±•ï¼Œä¸€äº›æ–¹æ³•å·²ç»å°è¯•åŒæ—¶ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºï¼ˆNeRFæˆ–3DGSï¼‰å’Œç›¸æœºå§¿æ€ã€‚å°½ç®¡å–å¾—äº†æœ€æ–°çš„è¿›å±•ï¼Œä½†ä¾èµ–ä¼ ç»Ÿç›¸æœºè¾“å…¥çš„æ–¹æ³•å¾€å¾€åœ¨é«˜é€Ÿï¼ˆæˆ–ç­‰æ•ˆåœ°ï¼Œä½å¸§ç‡ï¼‰åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚äº‹ä»¶ç›¸æœºå—åˆ°ç”Ÿç‰©è§†è§‰çš„å¯å‘ï¼Œèƒ½å¤Ÿå¼‚æ­¥è®°å½•åƒç´ çº§çš„å¼ºåº¦å˜åŒ–ï¼Œå…·æœ‰å¾ˆé«˜çš„æ—¶é—´åˆ†è¾¨ç‡ï¼Œå¹¶åœ¨ç›²å¸§é—´éš”æä¾›äº†æœ‰ä»·å€¼çš„åœºæ™¯å’Œè¿åŠ¨ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†äº‹ä»¶ç›¸æœºæ¥å¸®åŠ©ä»éšæ„æ‹æ‘„çš„è§†é¢‘ä¸­è¿›è¡Œåœºæ™¯æ„å»ºï¼Œå¹¶æå‡ºäº†åä¸ºEF-3DGSçš„äº‹ä»¶è¾…åŠ©è‡ªç”±è½¨è¿¹3DGSã€‚å®ƒé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶æ— ç¼é›†æˆäº‹ä»¶ç›¸æœºçš„ä¼˜åŠ¿åˆ°3DGSä¸­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆEGMï¼‰èåˆäº‹ä»¶å’Œå¸§ï¼Œç›‘ç£äº‹ä»¶æµè§‚å¯Ÿåˆ°çš„æ¸²æŸ“è§†å›¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†æ®µå¯¹æ¯”æœ€å¤§åŒ–ï¼ˆCMaxï¼‰æ¡†æ¶ï¼Œé€šè¿‡æœ€å¤§åŒ–å˜å½¢äº‹ä»¶çš„å›¾åƒå¯¹æ¯”åº¦æ¥æå–è¿åŠ¨ä¿¡æ¯ï¼Œä»è€Œæ ¡å‡†ä¼°è®¡çš„å§¿æ€ã€‚æ­¤å¤–ï¼ŒåŸºäºçº¿æ€§äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆLEGMï¼‰ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨IWEä¸­ç¼–ç çš„äº®åº¦ä¿¡æ¯åœ¨æ¢¯åº¦åŸŸçº¦æŸ3DGSã€‚ç¬¬ä¸‰ï¼Œä¸ºäº†ç¼“è§£äº‹ä»¶é¢œè‰²ä¿¡æ¯çš„ç¼ºå¤±ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…‰åº¦æ†ç»‘è°ƒæ•´ï¼ˆPBAï¼‰ä»¥ç¡®ä¿äº‹ä»¶å’Œå¸§ä¹‹é—´çš„è§†å›¾ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨å…¬å…±çš„Tanks and TemplesåŸºå‡†æµ‹è¯•å’Œæ–°æ”¶é›†çš„ç°å®ä¸–ç•Œæ•°æ®é›†RealEv-DAVISä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://lbh666.github.io/ef-3dgs/%E3%80%82">https://lbh666.github.io/ef-3dgs/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15392v4">PDF</a> Accepted to NeurIPS 2025,Project Page:   <a target="_blank" rel="noopener" href="https://lbh666.github.io/ef-3dgs/">https://lbh666.github.io/ef-3dgs/</a></p>
<p><strong>Summary</strong><br>    å¼•å…¥äº‹ä»¶ç›¸æœºè¾…åŠ©ä»éšæ‰‹æ‹æ‘„çš„è§†é¢‘ä¸­è¿›è¡Œåœºæ™¯æ„å»ºï¼Œæå‡ºEvent-Aided Free-Trajectory 3DGSï¼ˆEF-3DGSï¼‰æ–¹æ³•ï¼Œèåˆäº‹ä»¶ç›¸æœºçš„ä¼˜åŠ¿ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å®ç°åœºæ™¯é‡å»ºçš„ä¼˜åŒ–ã€‚åˆ©ç”¨äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆEGMï¼‰èåˆäº‹ä»¶å’Œå¸§ä¿¡æ¯ï¼Œé‡‡ç”¨å¯¹æ¯”æœ€å¤§åŒ–ï¼ˆCMaxï¼‰æ¡†æ¶æå–è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶åŸºäºçº¿æ€§äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆLEGMï¼‰åˆ©ç”¨äº®åº¦ä¿¡æ¯çº¦æŸ3DGSã€‚æ­¤å¤–ï¼Œå¼•å…¥å…‰åº¦æ†ç»‘è°ƒæ•´ï¼ˆPBAï¼‰ç¡®ä¿äº‹ä»¶å’Œå¸§ä¹‹é—´çš„è§†å›¾ä¸€è‡´æ€§ã€‚åœ¨å…¬å…±çš„Tankså’ŒTemplesåŸºå‡†æµ‹è¯•ä»¥åŠæ–°æ”¶é›†çš„çœŸå®ä¸–ç•Œæ•°æ®é›†RealEv-DAVISä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‹ä»¶ç›¸æœºè¢«å¼•å…¥ä»¥è¾…åŠ©ä»éšæ‰‹æ‹æ‘„çš„è§†é¢‘ä¸­è¿›è¡Œåœºæ™¯æ„å»ºã€‚</li>
<li>æå‡ºäº†Event-Aided Free-Trajectory 3DGSï¼ˆEF-3DGSï¼‰æ–¹æ³•ï¼Œæ— ç¼é›†æˆäº‹ä»¶ç›¸æœºçš„ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å®ç°åœºæ™¯é‡å»ºçš„ä¼˜åŒ–ï¼šäº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆEGMï¼‰ã€å¯¹æ¯”æœ€å¤§åŒ–ï¼ˆCMaxï¼‰å’Œçº¿æ€§äº‹ä»¶ç”Ÿæˆæ¨¡å‹ï¼ˆLEGMï¼‰ã€‚</li>
<li>åˆ©ç”¨äº‹ä»¶ç›¸æœºçš„äº®åº¦ä¿¡æ¯åœ¨æ¢¯åº¦åŸŸçº¦æŸ3Dåœºæ™¯ç»“æ„ã€‚</li>
<li>å¼•å…¥å…‰åº¦æ†ç»‘è°ƒæ•´ï¼ˆPBAï¼‰ä»¥ç¡®ä¿äº‹ä»¶å’Œå¸§ä¹‹é—´çš„è§†å›¾ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å…¬å…±åŸºå‡†æµ‹è¯•RealEv-DAVISä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå±•ç¤ºäº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf22dd10019677cf43f96b081b96b2c9" align="middle">
<img src="https://picx.zhimg.com/v2-a885fb8f942afa812f6f2a813b1a7af2" align="middle">
<img src="https://picx.zhimg.com/v2-5714db73b428d417ae6f282962434acd" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="NerfBaselines-Consistent-and-Reproducible-Evaluation-of-Novel-View-Synthesis-Methods"><a href="#NerfBaselines-Consistent-and-Reproducible-Evaluation-of-Novel-View-Synthesis-Methods" class="headerlink" title="NerfBaselines: Consistent and Reproducible Evaluation of Novel View   Synthesis Methods"></a>NerfBaselines: Consistent and Reproducible Evaluation of Novel View   Synthesis Methods</h2><p><strong>Authors:Jonas Kulhanek, Torsten Sattler</strong></p>
<p>Novel view synthesis is an important problem with many applications, including AR&#x2F;VR, gaming, and robotic simulations. With the recent rapid development of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) methods, it is becoming difficult to keep track of the current state of the art (SoTA) due to methods using different evaluation protocols, codebases being difficult to install and use, and methods not generalizing well to novel 3D scenes. In our experiments, we show that even tiny differences in the evaluation protocols of various methods can artificially boost the performance of these methods. This raises questions about the validity of quantitative comparisons performed in the literature. To address these questions, we propose NerfBaselines, an evaluation framework which provides consistent benchmarking tools, ensures reproducibility, and simplifies the installation and use of various methods. We validate our implementation experimentally by reproducing the numbers reported in the original papers. For improved accessibility, we release a web platform that compares commonly used methods on standard benchmarks. We strongly believe NerfBaselines is a valuable contribution to the community as it ensures that quantitative results are comparable and thus truly measure progress in the field of novel view synthesis. </p>
<blockquote>
<p>æ–°å‹è§†è§’åˆæˆæ˜¯ä¸€ä¸ªå…·æœ‰è®¸å¤šåº”ç”¨çš„é‡è¦é—®é¢˜ï¼ŒåŒ…æ‹¬AR&#x2F;VRã€æ¸¸æˆå’Œæœºå™¨äººæ¨¡æ‹Ÿã€‚éšç€ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰å’Œ3Dé«˜æ–¯æ¶‚æŠ¹ï¼ˆ3DGSï¼‰æ–¹æ³•çš„å¿«é€Ÿå‘å±•ï¼Œç”±äºä½¿ç”¨äº†ä¸åŒçš„è¯„ä¼°åè®®ã€ä»£ç åº“éš¾ä»¥å®‰è£…å’Œä½¿ç”¨ã€ä»¥åŠæ–¹æ³•ä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°æ–°å‹3Dåœºæ™¯ï¼Œå¾ˆéš¾è·Ÿè¸ªå½“å‰æœ€å‰æ²¿æŠ€æœ¯ï¼ˆSoTAï¼‰ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå³ä½¿è¯„ä»·åè®®ä¸­çš„å¾®å°å·®å¼‚ä¹Ÿä¼šäººä¸ºåœ°æå‡å„ç§æ–¹æ³•çš„æ€§èƒ½ã€‚è¿™å¼•å‘äº†å…³äºæ–‡çŒ®ä¸­è¿›è¡Œçš„å®šé‡æ¯”è¾ƒçœŸå®æ€§çš„é—®é¢˜ã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†NeRFåŸºçº¿ï¼ˆNerfBaselinesï¼‰ï¼Œä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæä¾›ä¸€è‡´çš„æ€§èƒ½è¯„ä¼°å·¥å…·ï¼Œç¡®ä¿å¯é‡å¤æ€§ï¼Œå¹¶ç®€åŒ–äº†å„ç§æ–¹æ³•çš„å®‰è£…å’Œä½¿ç”¨ã€‚æˆ‘ä»¬é€šè¿‡å¤åˆ¶åŸå§‹è®ºæ–‡ä¸­æŠ¥å‘Šçš„æ•°å€¼æ¥éªŒè¯æˆ‘ä»¬çš„å®ç°ã€‚ä¸ºäº†å¢å¼ºå¯è®¿é—®æ€§ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªç½‘ç»œå¹³å°ï¼Œè¯¥å¹³å°å¯ä»¥åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå¯¹å¸¸ç”¨æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬åšä¿¡NeRFåŸºçº¿å¯¹ç¤¾åŒºæ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„è´¡çŒ®ï¼Œå› ä¸ºå®ƒç¡®ä¿äº†å®šé‡ç»“æœçš„å¯æ¯”æ€§ï¼Œä»è€ŒçœŸæ­£è¡¡é‡äº†æ–°å‹è§†è§’åˆæˆé¢†åŸŸçš„è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17345v2">PDF</a> NeurIPS 2025 D&B; Web: <a target="_blank" rel="noopener" href="https://jkulhanek.com/nerfbaselines">https://jkulhanek.com/nerfbaselines</a></p>
<p><strong>Summary</strong><br>æ–°å‹è§†å›¾åˆæˆæ˜¯ä¸€ä¸ªæ¶µç›–å¢å¼ºç°å®ï¼ˆARï¼‰ã€è™šæ‹Ÿç°å®ï¼ˆVRï¼‰ã€æ¸¸æˆå’Œæœºå™¨äººæ¨¡æ‹Ÿç­‰é¢†åŸŸçš„é‡è¦è¯¾é¢˜ã€‚è¿‘æœŸéšç€ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æ‹¼æ¥æŠ€æœ¯ï¼ˆ3DGSï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè·Ÿè¸ªå½“å‰å‰æ²¿æŠ€æœ¯å˜å¾—å›°éš¾é‡é‡ã€‚å½“å‰çš„æ–¹æ³•é‡‡ç”¨ä¸åŒçš„è¯„ä¼°åè®®ï¼Œä»£ç åº“éš¾ä»¥å®‰è£…å’Œä½¿ç”¨ï¼Œéš¾ä»¥åº”ç”¨äºæ–°å‹ä¸‰ç»´åœºæ™¯ã€‚æœ¬ç ”ç©¶é€šè¿‡å®éªŒè¡¨æ˜ï¼Œè¯„ä¼°åè®®ä¸­çš„å¾®å°å·®å¼‚å¯èƒ½ä¼šäººä¸ºæå‡æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ï¼Œè¿™å¼•å‘äº†å…³äºæ–‡çŒ®ä¸­å®šé‡æ¯”è¾ƒæœ‰æ•ˆæ€§çš„è´¨ç–‘ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†NeRFåŸºçº¿è¯„ä¼°æ¡†æ¶ï¼Œæä¾›ä¸€è‡´çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§ï¼Œå¹¶ç®€åŒ–äº†æ–¹æ³•çš„å®‰è£…å’Œä½¿ç”¨æµç¨‹ã€‚é€šè¿‡å®éªŒéªŒè¯æˆ‘ä»¬é‡æ–°äº§ç”ŸæŠ¥å‘Šçš„åŸå§‹è®ºæ–‡ä¸­çš„æ•°å­—ç»“æœï¼Œè¯æ˜äº†å®æ–½çš„æœ‰æ•ˆæ€§ã€‚ä¸ºæå‡å®ç”¨æ€§ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªåœ¨çº¿å¹³å°ï¼Œå¯ä»¥åœ¨æ ‡å‡†åŸºå‡†ä¸Šæ¯”è¾ƒå¸¸ç”¨çš„æ–¹æ³•ã€‚æˆ‘ä»¬ç›¸ä¿¡NeRFåŸºçº¿å¯¹ç¤¾åŒºæ˜¯ä¸€å¤§è´¡çŒ®ï¼Œç¡®ä¿äº†å®šé‡ç»“æœçš„å¯æ¯”æ€§ï¼Œä¸ºè§†å›¾åˆæˆé¢†åŸŸçš„çœŸå®è¿›æ­¥è¡¡é‡æ‰“ä¸‹åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹è§†å›¾åˆæˆæ˜¯ä¸€ä¸ªæ¶µç›–å¤šä¸ªé¢†åŸŸçš„é‡è¦é—®é¢˜ï¼ŒåŒ…æ‹¬AR&#x2F;VRã€æ¸¸æˆå’Œæœºå™¨äººæ¨¡æ‹Ÿç­‰ã€‚</li>
<li>å½“å‰NeRFå’Œ3DGSç­‰æŠ€æœ¯çš„å¿«é€Ÿè¿›å±•ä½¿å¾—è·Ÿè¸ªå‰æ²¿æŠ€æœ¯å˜å¾—å›°éš¾ã€‚</li>
<li>æ–‡çŒ®ä¸­çš„å®šé‡æ¯”è¾ƒæœ‰æ•ˆæ€§å—åˆ°è´¨ç–‘ï¼Œå› ä¸ºè¯„ä¼°åè®®å·®å¼‚å¯èƒ½å¯¼è‡´æ€§èƒ½è¡¨ç°æå‡çš„äººä¸ºæ•ˆåº”ã€‚</li>
<li>æå‡ºNeRFåŸºçº¿è¯„ä¼°æ¡†æ¶ä»¥æä¾›ä¸€è‡´çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼Œç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶ç®€åŒ–äº†æ–¹æ³•çš„å®‰è£…å’Œä½¿ç”¨æµç¨‹ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å®æ–½çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å‘å¸ƒåœ¨çº¿å¹³å°ï¼Œç”¨äºåœ¨æ ‡å‡†åŸºå‡†ä¸Šæ¯”è¾ƒå¸¸ç”¨æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-904c4a86d529c082d18296126095055c" align="middle">
<img src="https://picx.zhimg.com/v2-088bd89d19968d0242e5dbd12f800273" align="middle">
<img src="https://picx.zhimg.com/v2-3efc690a72171274248bea032936ad94" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="OpenMaterial-A-Large-scale-Dataset-of-Complex-Materials-for-3D-Reconstruction"><a href="#OpenMaterial-A-Large-scale-Dataset-of-Complex-Materials-for-3D-Reconstruction" class="headerlink" title="OpenMaterial: A Large-scale Dataset of Complex Materials for 3D   Reconstruction"></a>OpenMaterial: A Large-scale Dataset of Complex Materials for 3D   Reconstruction</h2><p><strong>Authors:Zheng Dang, Jialu Huang, Fei Wang, Mathieu Salzmann</strong></p>
<p>Recent advances in deep learning, such as neural radiance fields and implicit neural representations, have significantly advanced 3D reconstruction. However, accurately reconstructing objects with complex optical properties, such as metals, glass, and plastics, remains challenging due to the breakdown of multi-view color consistency in the presence of specular reflections, refractions, and transparency. This limitation is further exacerbated by the lack of benchmark datasets that explicitly model material-dependent light transport. To address this, we introduce OpenMaterial, a large-scale semi-synthetic dataset for benchmarking material-aware 3D reconstruction. It comprises 1,001 objects spanning 295 distinct materials, including conductors, dielectrics, plastics, and their roughened variants, captured under 714 diverse lighting conditions. By integrating lab-measured Index of Refraction (IOR) spectra, OpenMaterial enables the generation of high-fidelity multi-view images that accurately simulate complex light-matter interactions. It provides multi-view images, 3D shape models, camera poses, depth maps, and object masks, establishing the first extensive benchmark for evaluating 3D reconstruction on challenging materials. We evaluate 11 state-of-the-art methods for 3D reconstruction and novel view synthesis, conducting ablation studies to assess the impact of material type, shape complexity, and illumination on reconstruction performance. Our results indicate that OpenMaterial provides a strong and fair basis for developing more robust, physically-informed 3D reconstruction techniques to better handle real-world optical complexities. </p>
<blockquote>
<p>æœ€è¿‘æ·±åº¦å­¦ä¹ é¢†åŸŸçš„ç¥ç»è¾å°„åœºå’Œéšå¼ç¥ç»è¡¨ç¤ºç­‰è¿›å±•æå¤§åœ°æ¨åŠ¨äº†3Dé‡å»ºçš„å‘å±•ã€‚ç„¶è€Œï¼Œç”±äºå…‰åå°„ã€æŠ˜å°„å’Œé€æ˜åº¦çš„å­˜åœ¨ï¼Œå¯¼è‡´å¤šè§†è§’é¢œè‰²ä¸€è‡´æ€§å¤±æ•ˆï¼Œä½¿å¾—å¯¹é‡‘å±ã€ç»ç’ƒå’Œå¡‘æ–™ç­‰å…·æœ‰å¤æ‚å…‰å­¦ç‰¹æ€§çš„ç‰©ä½“çš„ç²¾ç¡®é‡å»ºä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™ä¸€å±€é™æ€§è¿˜å› ç¼ºä¹æ˜¾å¼å»ºæ¨¡ææ–™ä¾èµ–çš„å…‰ä¼ è¾“çš„åŸºå‡†æ•°æ®é›†è€ŒåŠ å‰§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OpenMaterialï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŸºå‡†æµ‹è¯•çš„ææ–™æ„ŸçŸ¥3Dé‡å»ºçš„å¤§å‹åŠåˆæˆæ•°æ®é›†ã€‚å®ƒåŒ…å«1001ä¸ªå¯¹è±¡ï¼Œè·¨è¶Š295ç§ä¸åŒææ–™ï¼ŒåŒ…æ‹¬å¯¼ä½“ã€ä»‹ç”µä½“ã€å¡‘æ–™åŠå…¶ç²—ç³™å˜ä½“ï¼Œåœ¨714ç§ä¸åŒçš„å…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„ã€‚é€šè¿‡æ•´åˆå®éªŒå®¤æµ‹é‡çš„æŠ˜å°„ç‡ï¼ˆIORï¼‰å…‰è°±ï¼ŒOpenMaterialèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸå¤šè§†è§’å›¾åƒï¼Œå‡†ç¡®æ¨¡æ‹Ÿå¤æ‚çš„å…‰ä¸ç‰©è´¨ç›¸äº’ä½œç”¨ã€‚å®ƒæä¾›äº†å¤šè§†è§’å›¾åƒã€3Då½¢çŠ¶æ¨¡å‹ã€ç›¸æœºå§¿æ€ã€æ·±åº¦å›¾å’Œå¯¹è±¡æ©è†œï¼Œä¸ºè¯„ä¼°åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ææ–™ä¸Šè¿›è¡Œ3Dé‡å»ºå»ºç«‹äº†é¦–ä¸ªå¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¯„ä¼°äº†11ç§æœ€å…ˆè¿›çš„3Dé‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæ–¹æ³•ï¼Œè¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥è¯„ä¼°ææ–™ç±»å‹ã€å½¢çŠ¶å¤æ‚æ€§å’Œç…§æ˜å¯¹é‡å»ºæ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒOpenMaterialä¸ºå¼€å‘æ›´ç¨³å¥ã€ç‰©ç†é©±åŠ¨çš„3Dé‡å»ºæŠ€æœ¯æä¾›äº†å¼ºå¤§è€Œå…¬å¹³çš„åŸºç¡€ï¼Œä»¥æ›´å¥½åœ°åº”å¯¹ç°å®ä¸–ç•Œçš„å…‰å­¦å¤æ‚æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08894v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¦‚ç¥ç»è¾å°„åœºå’Œéšå¼ç¥ç»è¡¨ç¤ºæ³•ï¼Œåœ¨3Dé‡å»ºæ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹äºå…·æœ‰å¤æ‚å…‰å­¦ç‰¹æ€§çš„ç‰©ä½“çš„å‡†ç¡®é‡å»ºï¼Œå¦‚é‡‘å±ã€ç»ç’ƒå’Œå¡‘æ–™ï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†OpenMaterialï¼Œä¸€ä¸ªç”¨äºåŸºå‡†æµ‹è¯•çš„ææ–™æ„ŸçŸ¥3Dé‡å»ºçš„å¤§å‹åŠåˆæˆæ•°æ®é›†ã€‚å®ƒåŒ…å«1001ä¸ªå¯¹è±¡ï¼Œè·¨è¶Š295ç§ä¸åŒææ–™ï¼Œå¹¶åœ¨714ç§ä¸åŒçš„ç…§æ˜æ¡ä»¶ä¸‹æ‹æ‘„ã€‚é€šè¿‡æ•´åˆå®éªŒå®¤æµ‹é‡çš„æŠ˜å°„ç‡ï¼ˆIORï¼‰å…‰è°±ï¼ŒOpenMaterialèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸå¤šè§†è§’å›¾åƒï¼Œå‡†ç¡®æ¨¡æ‹Ÿå¤æ‚çš„å…‰ä¸ç‰©è´¨ç›¸äº’ä½œç”¨ã€‚å®ƒä¸º3Dé‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæä¾›äº†ç¬¬ä¸€ä¸ªå¹¿æ³›çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»è¾å°„åœºå’Œéšå¼ç¥ç»è¡¨ç¤ºæ³•åœ¨3Dé‡å»ºæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>é‡å»ºå…·æœ‰å¤æ‚å…‰å­¦ç‰¹æ€§çš„ç‰©ä½“ï¼ˆå¦‚é‡‘å±ã€ç»ç’ƒå’Œå¡‘æ–™ï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹æ˜ç¡®æ¨¡æ‹Ÿææ–™ä¾èµ–å…‰çº¿ä¼ è¾“çš„åŸºå‡†æ•°æ®é›†ã€‚</li>
<li>å¼•å…¥OpenMaterialæ•°æ®é›†ï¼ŒåŒ…å«1001ä¸ªå¯¹è±¡ï¼Œè·¨è¶Š295ç§ä¸åŒææ–™ï¼Œå¹¶åœ¨å¤šç§ç…§æ˜æ¡ä»¶ä¸‹æ‹æ‘„ã€‚</li>
<li>OpenMaterialæ•°æ®é›†é€šè¿‡æ•´åˆIORå…‰è°±ï¼Œæ¨¡æ‹Ÿå¤æ‚çš„å…‰ä¸ç‰©è´¨ç›¸äº’ä½œç”¨ã€‚</li>
<li>æä¾›å¤šè§†è§’å›¾åƒã€3Då½¢çŠ¶æ¨¡å‹ã€ç›¸æœºå§¿æ€ã€æ·±åº¦å›¾å’Œå¯¹è±¡æ©è†œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.08894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fbdd3f3bd9936030a3574394af0e4ba0" align="middle">
<img src="https://picx.zhimg.com/v2-f2e48afca9178deed823337e0f6c69b9" align="middle">
<img src="https://picx.zhimg.com/v2-8391efe40edee6bfdc931430c28fe4a3" align="middle">
<img src="https://picx.zhimg.com/v2-e45d81535ac373c64b936b0f6722836f" align="middle">
<img src="https://picx.zhimg.com/v2-7fc3e881dcec16d8e305977e72dc93ce" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-09f9281634df8b2feca498a1dcc8bd99" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Who Made This? Fake Detection and Source Attribution with Diffusion   Features
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-58c899a334cf40efffae65498ea1a5d8" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  SAGS Self-Adaptive Alias-Free Gaussian Splatting for Dynamic Surgical   Endoscopic Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
