<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Patient-Centered Summarization Framework for AI Clinical Summarization   A Mixed-Methods Design">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0e51a2cddeb30fd7f0f218dc4fac60c8')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="Patient-Centered-Summarization-Framework-for-AI-Clinical-Summarization-A-Mixed-Methods-Design"><a href="#Patient-Centered-Summarization-Framework-for-AI-Clinical-Summarization-A-Mixed-Methods-Design" class="headerlink" title="Patient-Centered Summarization Framework for AI Clinical Summarization:   A Mixed-Methods Design"></a>Patient-Centered Summarization Framework for AI Clinical Summarization:   A Mixed-Methods Design</h2><p><strong>Authors:Maria Lizarazo Jimenez, Ana Gabriela Claros, Kieran Green, David Toro-Tobon, Felipe Larios, Sheena Asthana, Camila Wenczenovicz, Kerly Guevara Maldonado, Luis Vilatuna-Andrango, Cristina Proano-Velez, Satya Sai Sri Bandi, Shubhangi Bagewadi, Megan E. Branda, Misk Al Zahidy, Saturnino Luz, Mirella Lapata, Juan P. Brito, Oscar J. Ponce-Ponte</strong></p>
<p>Large Language Models (LLMs) are increasingly demonstrating the potential to reach human-level performance in generating clinical summaries from patient-clinician conversations. However, these summaries often focus on patientsâ€™ biology rather than their preferences, values, wishes, and concerns. To achieve patient-centered care, we propose a new standard for Artificial Intelligence (AI) clinical summarization tasks: Patient-Centered Summaries (PCS). Our objective was to develop a framework to generate PCS that capture patient values and ensure clinical utility and to assess whether current open-source LLMs can achieve human-level performance in this task. We used a mixed-methods process. Two Patient and Public Involvement groups (10 patients and 8 clinicians) in the United Kingdom participated in semi-structured interviews exploring what personal and contextual information should be included in clinical summaries and how it should be structured for clinical use. Findings informed annotation guidelines used by eight clinicians to create gold-standard PCS from 88 atrial fibrillation consultations. Sixteen consultations were used to refine a prompt aligned with the guidelines. Five open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and Qwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot prompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients emphasized lifestyle routines, social support, recent stressors, and care values. Clinicians sought concise functional, psychosocial, and emotional context. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L 0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B (ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between experts and models, while correctness and patient-centeredness favored human PCS. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”ŸæˆåŸºäºåŒ»æ‚£å¯¹è¯çš„ä¸´åºŠæ‘˜è¦æ–¹é¢ï¼Œè¶Šæ¥è¶Šæ˜¾ç¤ºå‡ºè¾¾åˆ°äººç±»æ°´å¹³çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ‘˜è¦å¾€å¾€å…³æ³¨æ‚£è€…çš„ç”Ÿç‰©å­¦ç‰¹å¾ï¼Œè€Œéæ‚£è€…çš„åå¥½ã€ä»·å€¼è§‚ã€æ„¿æœ›å’Œå…³åˆ‡ã€‚ä¸ºäº†å®ç°ä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„åŒ»ç–—æŠ¤ç†ï¼Œæˆ‘ä»¬ä¸ºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸´åºŠæ‘˜è¦ä»»åŠ¡æå‡ºäº†ä¸€é¡¹æ–°æ ‡å‡†ï¼šä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„æ‘˜è¦ï¼ˆPCSï¼‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªæ¡†æ¶æ¥ç”Ÿæˆæ•æ‰æ‚£è€…ä»·å€¼è§‚ã€ç¡®ä¿ä¸´åºŠå®ç”¨æ€§çš„PCSï¼Œå¹¶è¯„ä¼°å½“å‰çš„å¼€æºLLMæ˜¯å¦èƒ½åœ¨è¿™ä¸€ä»»åŠ¡ä¸­è¾¾åˆ°äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨äº†æ··åˆæ–¹æ³•ã€‚è‹±å›½çš„ä¸¤ä¸ªæ‚£è€…å’Œå…¬ä¼—å‚ä¸å°ç»„ï¼ˆå„åŒ…æ‹¬10åæ‚£è€…å’Œ8åä¸´åºŠåŒ»ç”Ÿï¼‰å‚ä¸äº†åŠç»“æ„åŒ–è®¿è°ˆï¼Œæ¢è®¨äº†ä¸´åºŠæ‘˜è¦ä¸­åº”åŒ…å«å“ªäº›ä¸ªäººå’Œæƒ…å¢ƒä¿¡æ¯ï¼Œä»¥åŠå¦‚ä½•ç»“æ„åŒ–è¿™äº›ä¿¡æ¯ä»¥ä¾›ä¸´åºŠä½¿ç”¨ã€‚è®¿è°ˆç»“æœæŒ‡å¯¼äº†ç”±å…«åä¸´åºŠåŒ»ç”Ÿæ ¹æ®ç»“æœåˆ¶å®šçš„æ ‡æ³¨æŒ‡å—ï¼Œä»–ä»¬åˆ©ç”¨è¿™äº›æŒ‡å—åˆ›å»ºäº†åŸºäº88æ¬¡æˆ¿é¢¤ä¼šè¯Šçš„é»„é‡‘æ ‡å‡†PCSã€‚åå…­æ¬¡ä¼šè¯Šè¢«ç”¨æ¥å®Œå–„ä¸æŒ‡å—ç›¸ç¬¦çš„æç¤ºã€‚äº”ä¸ªå¼€æºLLMï¼ˆLlama-3.2-3Bã€Llama-3.1-8Bã€Mistral-8Bã€Gemma-3-4Bå’ŒQwen3-8Bï¼‰ä½¿ç”¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºæŠ€æœ¯ä¸ºä¸ƒåäºŒæ¬¡ä¼šè¯Šç”Ÿæˆæ‘˜è¦ï¼Œå¹¶ä½¿ç”¨ROUGE-Lã€BERTScoreå’Œå®šæ€§æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚æ‚£è€…å¼ºè°ƒç”Ÿæ´»æ–¹å¼ã€ç¤¾ä¼šæ”¯æŒã€è¿‘æœŸå‹åŠ›å› å­å’ŒæŠ¤ç†ä»·å€¼è§‚ã€‚ä¸´åºŠåŒ»ç”Ÿå¯»æ±‚ç®€æ´æ˜äº†çš„åŠŸèƒ½æ€§ã€ç¤¾ä¼šå¿ƒç†å’Œæƒ…æ„ŸèƒŒæ™¯ã€‚é›¶æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°æœ€å¥½çš„æ˜¯Mistral-8Bï¼ˆROUGE-Lå¾—åˆ†ä¸º0.189ï¼‰å’ŒLlama-3.1-8Bï¼ˆBERTScoreå¾—åˆ†ä¸º0.673ï¼‰ï¼›è€Œåœ¨æœ‰å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹è¡¨ç°æœ€å¥½çš„ä»æ˜¯Llama-3.1-8Bï¼ˆROUGE-Lå¾—åˆ†ä¸º0.206ï¼ŒBERTScoreå¾—åˆ†ä¸º0.683ï¼‰ã€‚åœ¨ä¸“å®¶å’Œæ¨¡å‹ä¹‹é—´ï¼Œå®Œæ•´æ€§æµç•…æ€§ç›¸ä¼¼ï¼Œä½†æ­£ç¡®æ€§å’Œä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„è§‚ç‚¹åˆ™æ›´å€¾å‘äºäººç±»ç”Ÿæˆçš„PCSã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27535v1">PDF</a> The first two listed authors contributed equally Pages: 21;   Figures:2; Tables:3</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„æ‘˜è¦æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å¾€å¾€åé‡ç”Ÿç‰©å­¦ä¿¡æ¯è€Œå¿½è§†æ‚£è€…åå¥½ã€ä»·å€¼è§‚å’Œå…³åˆ‡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å»ºç«‹ä¸€ä¸ªæ–°çš„AIä¸´åºŠæ‘˜è¦æ ‡å‡†â€”â€”æ‚£è€…ä¸­å¿ƒæ‘˜è¦ï¼ˆPCSï¼‰ï¼Œæ—¨åœ¨æ•æ‰æ‚£è€…ä»·å€¼è§‚å’Œç¡®ä¿ä¸´åºŠå®ç”¨æ€§ï¼Œå¹¶è¯„ä¼°ç°æœ‰å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šæ˜¯å¦èƒ½è¾¾åˆ°äººç±»æ°´å¹³è¡¨ç°ã€‚é€šè¿‡æ··åˆæ–¹æ³•ï¼ŒåŒ…æ‹¬æ‚£è€…å’Œå…¬ä¼—å‚ä¸ç»„çš„åŠç»“æ„åŒ–è®¿è°ˆï¼Œä»¥åŠåŒ»ç”Ÿåˆ›å»ºçš„é‡‘æ ‡å‡†æ‚£è€…ä¸­å¿ƒæ‘˜è¦ï¼Œç ”ç©¶å‘ç°æ‚£è€…å¼ºè°ƒç”Ÿæ´»æ–¹å¼ã€ç¤¾ä¼šæ”¯æŒã€è¿‘æœŸå‹åŠ›ä»¥åŠæŠ¤ç†ä»·å€¼è§‚ç­‰ä¿¡æ¯çš„é‡è¦æ€§ã€‚å¯¹äº”ç§å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œé›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬æç¤ºä¸‹ç”Ÿæˆçš„æ‘˜è¦åœ¨ROUGE-Lå’ŒBERTScoreè¯„ä»·ä¸­æœ‰è‰¯å¥½è¡¨ç°ã€‚æ€»ä½“è€Œè¨€ï¼Œæ¨¡å‹çš„å®Œæ•´æ€§å’Œæµç•…æ€§ä¸ä¸“å®¶ç›¸ä¼¼ï¼Œä½†åœ¨æ­£ç¡®æ€§å’Œæ‚£è€…ä¸­å¿ƒæ€§æ–¹é¢ä»ç•¥é€Šäºäººç±»ç”Ÿæˆçš„æ‚£è€…ä¸­å¿ƒæ‘˜è¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠæ‘˜è¦ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›å·¨å¤§ï¼Œä½†ä»éœ€å…³æ³¨æ‚£è€…ä¸ºä¸­å¿ƒçš„æŠ¤ç†éœ€æ±‚ã€‚</li>
<li>æ‚£è€…å’ŒåŒ»ç”Ÿå¯¹äºä¸´åºŠæ‘˜è¦ä¸­çš„ä¿¡æ¯æœ‰ä¸åŒçš„ä¾§é‡ç‚¹ï¼Œå¦‚æ‚£è€…æ›´å…³æ³¨ç”Ÿæ´»æ–¹å¼å’Œç¤¾ä¼šæ”¯æŒç­‰ï¼ŒåŒ»ç”Ÿåˆ™å¯»æ±‚ç®€æ´çš„åŠŸèƒ½æ€§ã€å¿ƒç†å’Œç¤¾ä¼šèƒŒæ™¯ä¿¡æ¯ã€‚</li>
<li>å»ºç«‹æ–°çš„AIä¸´åºŠæ‘˜è¦æ ‡å‡†â€”â€”æ‚£è€…ä¸­å¿ƒæ‘˜è¦ï¼ˆPCSï¼‰ï¼Œä»¥æ•æ‰æ‚£è€…ä»·å€¼è§‚å’Œç¡®ä¿ä¸´åºŠå®ç”¨æ€§ã€‚</li>
<li>é€šè¿‡æ··åˆæ–¹æ³•ç ”ç©¶ï¼Œåˆ¶å®šäº†ç›¸åº”çš„æ³¨è§£æŒ‡å—å¹¶ä¸ºæ¨¡å‹ç”Ÿæˆæ‘˜è¦æä¾›ä¾æ®ã€‚</li>
<li>äº”ç§å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬æç¤ºä¸‹çš„è¡¨ç°è‰¯å¥½ï¼Œä½†ä»æœ‰å¾…æé«˜åœ¨æ­£ç¡®æ€§å’Œæ‚£è€…ä¸­å¿ƒæ€§æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>åœ¨è¯„ä¼°ç”Ÿæˆçš„æ‘˜è¦æ—¶ï¼ŒROUGE-Lå’ŒBERTScoreæ˜¯é‡è¦çš„è¯„ä»·æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0dbf3068caf8878c9842a58177d9407c" align="middle">
<img src="https://picx.zhimg.com/v2-dcbbc6a2996d025c5eb74ffd99ffc821" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Modality-Alignment-across-Trees-on-Heterogeneous-Hyperbolic-Manifolds"><a href="#Modality-Alignment-across-Trees-on-Heterogeneous-Hyperbolic-Manifolds" class="headerlink" title="Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds"></a>Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds</h2><p><strong>Authors:Wu Wei, Xiaomeng Fan, Yuwei Wu, Zhi Gao, Pengxiang Li, Yunde Jia, Mehrtash Harandi</strong></p>
<p>Modality alignment is critical for vision-language models (VLMs) to effectively integrate information across modalities. However, existing methods extract hierarchical features from text while representing each image with a single feature, leading to asymmetric and suboptimal alignment. To address this, we propose Alignment across Trees, a method that constructs and aligns tree-like hierarchical features for both image and text modalities. Specifically, we introduce a semantic-aware visual feature extraction framework that applies a cross-attention mechanism to visual class tokens from intermediate Transformer layers, guided by textual cues to extract visual features with coarse-to-fine semantics. We then embed the feature trees of the two modalities into hyperbolic manifolds with distinct curvatures to effectively model their hierarchical structures. To align across the heterogeneous hyperbolic manifolds with different curvatures, we formulate a KL distance measure between distributions on heterogeneous manifolds, and learn an intermediate manifold for manifold alignment by minimizing the distance. We prove the existence and uniqueness of the optimal intermediate manifold. Experiments on taxonomic open-set classification tasks across multiple image datasets demonstrate that our method consistently outperforms strong baselines under few-shot and cross-domain settings. </p>
<blockquote>
<p>æ¨¡æ€å¯¹é½å¯¹äºè·¨æ¨¡æ€æœ‰æ•ˆæ•´åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ä¿¡æ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä»æ–‡æœ¬ä¸­æå–å±‚æ¬¡ç‰¹å¾ï¼ŒåŒæ—¶ä½¿ç”¨å•ä¸€ç‰¹å¾è¡¨ç¤ºæ¯å¹…å›¾åƒï¼Œå¯¼è‡´ä¸å¯¹ç§°å’Œæ¬¡ä¼˜å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ ‘é—´å¯¹é½â€æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ„å»ºå¹¶å¯¹é½å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€çš„æ ‘çŠ¶å±‚æ¬¡ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥çš„è§†è§‰ç‰¹å¾æå–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯¹ä¸­é—´Transformerå±‚çš„è§†è§‰ç±»åˆ«æ ‡è®°åº”ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æ–‡æœ¬çº¿ç´¢å¼•å¯¼ä»¥æå–å…·æœ‰ä»ç²—åˆ°ç»†è¯­ä¹‰çš„è§†è§‰ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä¸¤ç§æ¨¡æ€çš„ç‰¹å¾æ ‘åµŒå…¥å…·æœ‰ä¸åŒæ›²ç‡çš„åŒæ›²æµå½¢ä¸­ï¼Œä»¥æœ‰æ•ˆåœ°å»ºæ¨¡å…¶å±‚æ¬¡ç»“æ„ã€‚ä¸ºäº†å¯¹é½å…·æœ‰ä¸åŒæ›²ç‡çš„å¼‚æ„åŒæ›²æµå½¢ï¼Œæˆ‘ä»¬åœ¨å¼‚æ„æµå½¢ä¸Šçš„åˆ†å¸ƒä¹‹é—´åˆ¶å®šäº†ä¸€ç§KLè·ç¦»åº¦é‡ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–è·ç¦»æ¥å­¦ä¹ ç”¨äºæµå½¢å¯¹é½çš„ä¸­é—´æµå½¢ã€‚æˆ‘ä»¬è¯æ˜äº†æœ€ä¼˜ä¸­é—´æµå½¢çš„å­˜åœ¨æ€§å’Œå”¯ä¸€æ€§ã€‚åœ¨å¤šä¸ªå›¾åƒæ•°æ®é›†ä¸Šçš„å¼€æ”¾å¼åˆ†ç±»ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å°‘æ ·æœ¬å’Œè·¨åŸŸè®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27391v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œè·¨æ ‘å¯¹é½â€çš„æ–¹æ³•ï¼Œæ„å»ºå¹¶å®ç°å¯¹å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€çš„å±‚æ¬¡ç‰¹å¾è¿›è¡Œå¯¹é½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥è¯­ä¹‰æ„ŸçŸ¥çš„è§†è§‰ç‰¹å¾æå–æ¡†æ¶ï¼Œä½¿ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶å¯¹è§†è§‰ç±»æ ‡è®°è¿›è¡Œç²¾ç»†åˆ°ç²—ç³™çš„è¯­ä¹‰æå–ã€‚ä¹‹åï¼Œå°†ä¸¤ç§æ¨¡æ€çš„ç‰¹å¾æ ‘åµŒå…¥åˆ°å…·æœ‰ä¸åŒæ›²ç‡çš„åŒæ›²æµå½¢ä¸Šï¼Œå¹¶æå‡ºäº†ä¸€ç§KLè·ç¦»åº¦é‡æ–¹æ³•ä»¥å®ç°å¯¹ä¸åŒæ›²ç‡çš„å¼‚è´¨åŒæ›²æµå½¢çš„å¯¹é½ã€‚æœ¬æ–‡è¯æ˜å­˜åœ¨å”¯ä¸€æœ€ä¼˜çš„ä¸­é—´æµå½¢ã€‚åœ¨å¤šä¸ªå›¾åƒæ•°æ®é›†ä¸Šçš„åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å°‘æ ·æœ¬å’Œè·¨åŸŸè®¾ç½®ä¸‹å‡è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡æ€å¯¹é½å¯¹å¤šæ¨¡æ€æ¨¡å‹è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯æ•´åˆæ—¶ã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨ç‰¹å¾å¯¹é½ä¸å¯¹ç§°é—®é¢˜ï¼Œå› æ­¤æœ¬æ–‡æå‡ºè·¨æ ‘å¯¹é½æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æå‡ºè¯­ä¹‰æ„ŸçŸ¥çš„è§†è§‰ç‰¹å¾æå–æ¡†æ¶ï¼Œä½¿ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶å¯¹è§†è§‰ç±»æ ‡è®°è¿›è¡Œç²¾ç»†åˆ°ç²—ç³™çš„è¯­ä¹‰æå–ã€‚</li>
<li>ç‰¹å¾æ ‘è¢«åµŒå…¥åˆ°å…·æœ‰ä¸åŒæ›²ç‡çš„åŒæ›²æµå½¢ä¸Šï¼Œä»¥æœ‰æ•ˆå»ºæ¨¡å…¶å±‚æ¬¡ç»“æ„ã€‚</li>
<li>æå‡ºä¸€ç§KLè·ç¦»åº¦é‡æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸åŒæ›²ç‡çš„å¼‚è´¨åŒæ›²æµå½¢ä¹‹é—´è¿›è¡Œå¯¹é½ã€‚</li>
<li>å­˜åœ¨å”¯ä¸€æœ€ä¼˜çš„ä¸­é—´æµå½¢ç”¨äºå¯¹é½ï¼Œå¹¶è¿›è¡Œäº†è¯æ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-648777db0443de66d6dfa15bb57c6f75" align="middle">
<img src="https://picx.zhimg.com/v2-9262a7cb22a7cda37c31178896e1901f" align="middle">
<img src="https://picx.zhimg.com/v2-dc2eff52576c870422efcd2f677f6d16" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Discovering-EV-Charging-Site-Archetypes-Through-Few-Shot-Forecasting-The-First-U-S-Wide-Study"><a href="#Discovering-EV-Charging-Site-Archetypes-Through-Few-Shot-Forecasting-The-First-U-S-Wide-Study" class="headerlink" title="Discovering EV Charging Site Archetypes Through Few Shot Forecasting:   The First U.S.-Wide Study"></a>Discovering EV Charging Site Archetypes Through Few Shot Forecasting:   The First U.S.-Wide Study</h2><p><strong>Authors:Kshitij Nikhal, Luke Ackerknecht, Benjamin S. Riggan, Phil Stahlfeld</strong></p>
<p>The decarbonization of transportation relies on the widespread adoption of electric vehicles (EVs), which requires an accurate understanding of charging behavior to ensure cost-effective, grid-resilient infrastructure. Existing work is constrained by small-scale datasets, simple proximity-based modeling of temporal dependencies, and weak generalization to sites with limited operational history. To overcome these limitations, this work proposes a framework that integrates clustering with few-shot forecasting to uncover site archetypes using a novel large-scale dataset of charging demand. The results demonstrate that archetype-specific expert models outperform global baselines in forecasting demand at unseen sites. By establishing forecast performance as a basis for infrastructure segmentation, we generate actionable insights that enable operators to lower costs, optimize energy and pricing strategies, and support grid resilience critical to climate goals. </p>
<blockquote>
<p>äº¤é€šè¿è¾“çš„è„±ç¢³ä¾èµ–äºç”µåŠ¨æ±½è½¦ï¼ˆEVsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œè¿™éœ€è¦å‡†ç¡®äº†è§£å……ç”µè¡Œä¸ºï¼Œä»¥ç¡®ä¿å…·æœ‰æˆæœ¬æ•ˆç›Šå’Œç”µç½‘éŸ§æ€§çš„åŸºç¡€è®¾æ–½ã€‚ç°æœ‰å·¥ä½œå—é™äºå°è§„æ¨¡æ•°æ®é›†ã€åŸºäºç®€å•æ¥è¿‘åº¦çš„æ—¶åºä¾èµ–å»ºæ¨¡ï¼Œä»¥åŠå¯¹æ“ä½œå†å²æœ‰é™ç«™ç‚¹çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†èšç±»ä¸å°‘é‡é¢„æµ‹ç›¸ç»“åˆï¼Œåˆ©ç”¨å……ç”µéœ€æ±‚çš„æ–°å‹å¤§è§„æ¨¡æ•°æ®é›†æ¥æ­ç¤ºç«™ç‚¹åŸå‹ã€‚ç»“æœè¡¨æ˜ï¼Œç‰¹å®šäºåŸå‹çš„ä¸“å®¶æ¨¡å‹åœ¨é¢„æµ‹æœªè§ç«™ç‚¹ä¸Šçš„éœ€æ±‚æ—¶ä¼˜äºå…¨çƒåŸºå‡†ã€‚é€šè¿‡ä»¥é¢„æµ‹æ€§èƒ½ä½œä¸ºåŸºç¡€è®¾æ–½ç»†åˆ†çš„åŸºç¡€ï¼Œæˆ‘ä»¬äº§ç”Ÿäº†å¯æ“ä½œæ€§çš„è§è§£ï¼Œä½¿è¿è¥å•†èƒ½å¤Ÿé™ä½æˆæœ¬ã€ä¼˜åŒ–èƒ½æºå’Œå®šä»·ç­–ç•¥ï¼Œå¹¶æ”¯æŒå¯¹æ°”å€™ç›®æ ‡è‡³å…³é‡è¦çš„ç”µç½‘éŸ§æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26910v1">PDF</a> Tackling Climate Change with Machine Learning: Workshop at NeurIPS   2025</p>
<p><strong>Summary</strong></p>
<p>äº¤é€šè¿è¾“çš„è„±ç¢³ä¾èµ–äºç”µåŠ¨æ±½è½¦ï¼ˆEVsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œè¿™éœ€è¦å¯¹å……ç”µè¡Œä¸ºæœ‰å‡†ç¡®äº†è§£ä»¥ç¡®ä¿æˆæœ¬æ•ˆç›Šé«˜ã€ç”µç½‘éŸ§æ€§å¼ºçš„åŸºç¡€è®¾æ–½å»ºè®¾ã€‚ä¸ºå…‹æœç°æœ‰å·¥ä½œå—å°è§„æ¨¡æ•°æ®é›†ã€ç®€å•åŸºäºè·ç¦»çš„æ—¶ç©ºä¾èµ–æ€§å»ºæ¨¡ä»¥åŠç¼ºä¹å¯¹æ–°ç«™ç‚¹æ¨å¹¿çš„å±€é™æ€§ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ä¸ªç»“åˆèšç±»ä¸å°‘é‡é¢„æµ‹æ•°æ®çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ–°å‹å¤§è§„æ¨¡å……ç”µéœ€æ±‚æ•°æ®é›†æ­ç¤ºç«™ç‚¹åŸå‹ã€‚ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹åŸå‹å®šåˆ¶çš„æ¨¡å‹åœ¨æœªè§ç«™ç‚¹ä¸Šçš„éœ€æ±‚é¢„æµ‹è¡¨ç°ä¼˜äºå…¨çƒåŸºå‡†æ¨¡å‹ã€‚é€šè¿‡å»ºç«‹é¢„æµ‹æ€§èƒ½ä½œä¸ºåŸºç¡€è®¾æ–½åˆ†æ®µçš„åŸºç¡€ï¼Œæˆ‘ä»¬äº§ç”Ÿäº†å¯æ“ä½œçš„è§è§£ï¼Œå¸®åŠ©è¿è¥å•†é™ä½æˆæœ¬ã€ä¼˜åŒ–èƒ½æºå’Œå®šä»·ç­–ç•¥ï¼Œå¹¶ä¸ºå®ç°æ°”å€™ç›®æ ‡è‡³å…³é‡è¦çš„ç”µç½‘éŸ§æ€§æä¾›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº¤é€šè¿è¾“è„±ç¢³ä¾èµ–ç”µåŠ¨æ±½è½¦å¹¿æ³›é‡‡ç”¨ï¼Œéœ€æ·±å…¥äº†è§£å……ç”µè¡Œä¸ºä»¥æ”¯æŒåŸºç¡€è®¾æ–½å»ºè®¾ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å—é™äºå°è§„æ¨¡æ•°æ®é›†ã€ç®€å•å»ºæ¨¡åŠå¼±æ¨å¹¿èƒ½åŠ›ã€‚</li>
<li>æå‡ºç»“åˆèšç±»å’Œå°‘é‡é¢„æµ‹æ•°æ®çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§è§„æ¨¡å……ç”µéœ€æ±‚æ•°æ®é›†ã€‚</li>
<li>åŸå‹ç‰¹å®šæ¨¡å‹åœ¨æœªè§ç«™ç‚¹ä¸Šçš„éœ€æ±‚é¢„æµ‹è¡¨ç°ä¼˜äºå…¨çƒåŸºå‡†æ¨¡å‹ã€‚</li>
<li>å»ºç«‹é¢„æµ‹æ€§èƒ½ä½œä¸ºåŸºç¡€è®¾æ–½åˆ†æ®µçš„åŸºç¡€ï¼Œä¸ºè¿è¥å•†æä¾›é™ä½æˆæœ¬ã€ä¼˜åŒ–èƒ½æºå’Œå®šä»·ç­–ç•¥çš„å»ºè®®ã€‚</li>
<li>è¯¥ç ”ç©¶æœ‰åŠ©äºå®ç°ç”µç½‘éŸ§æ€§ï¼Œå¯¹å®ç°æ°”å€™ç›®æ ‡è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26910">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-873d551b4a0720cf2649c5b5ad209f59" align="middle">
<img src="https://picx.zhimg.com/v2-c29c5a196d4d08a5460891aa8d50b37e" align="middle">
<img src="https://picx.zhimg.com/v2-1f8c3cc2f0ddb5aba56ddea43a8b07cc" align="middle">
<img src="https://picx.zhimg.com/v2-0e51a2cddeb30fd7f0f218dc4fac60c8" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Questionnaire-meets-LLM-A-Benchmark-and-Empirical-Study-of-Structural-Skills-for-Understanding-Questions-and-Responses"><a href="#Questionnaire-meets-LLM-A-Benchmark-and-Empirical-Study-of-Structural-Skills-for-Understanding-Questions-and-Responses" class="headerlink" title="Questionnaire meets LLM: A Benchmark and Empirical Study of Structural   Skills for Understanding Questions and Responses"></a>Questionnaire meets LLM: A Benchmark and Empirical Study of Structural   Skills for Understanding Questions and Responses</h2><p><strong>Authors:Duc-Hai Nguyen, Vijayakumar Nanjappan, Barry Oâ€™Sullivan, Hoang D. Nguyen</strong></p>
<p>Millions of people take surveys every day, from market polls and academic studies to medical questionnaires and customer feedback forms. These datasets capture valuable insights, but their scale and structure present a unique challenge for large language models (LLMs), which otherwise excel at few-shot reasoning over open-ended text. Yet, their ability to process questionnaire data or lists of questions crossed with hundreds of respondent rows remains underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics, SPSS, REDCap) are typically designed for humans in the workflow, limiting such data integration with LLM and AI-empowered automation. This gap leaves scientists, surveyors, and everyday users without evidence-based guidance on how to best represent questionnaires for LLM consumption. We address this by introducing QASU (Questionnaire Analysis and Structural Understanding), a benchmark that probes six structural skills, including answer lookup, respondent count, and multi-hop inference, across six serialization formats and multiple prompt strategies. Experiments on contemporary LLMs show that choosing an effective format and prompt combination can improve accuracy by up to 8.8% points compared to suboptimal formats. For specific tasks, carefully adding a lightweight structural hint through self-augmented prompting can yield further improvements of 3-4% points on average. By systematically isolating format and prompting effects, our open source benchmark offers a simple yet versatile foundation for advancing both research and real-world practice in LLM-based questionnaire analysis. </p>
<blockquote>
<p>æ¯å¤©æœ‰æ•°ç™¾ä¸‡äººå‚ä¸å„ç§è°ƒæŸ¥ï¼Œä»å¸‚åœºæ°‘æ„è°ƒæŸ¥ã€å­¦æœ¯ç ”ç©¶åˆ°åŒ»ç–—é—®å·å’Œå®¢æˆ·åé¦ˆè¡¨ã€‚è¿™äº›æ•°æ®é›†æ•æ‰äº†å®è´µçš„è§è§£ï¼Œä½†å…¶è§„æ¨¡å’Œç»“æ„ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œè€Œè¿™äº›æ¨¡å‹åœ¨å…¶ä»–æƒ…å†µä¸‹å–„äºè¿›è¡Œå°‘æ ·æœ¬å¼€æ”¾å¼æ–‡æœ¬çš„æ¨ç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†é—®å·æ•°æ®æˆ–ä¸æ•°ç™¾åå—è®¿è€…è¡Œçš„æ•°æ®äº¤å‰æ—¶çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å½“å‰çš„æ£€ç´¢å’Œè°ƒæŸ¥åˆ†æå·¥å…·ï¼ˆä¾‹å¦‚Qualtricsã€SPSSã€REDCapï¼‰é€šå¸¸æ˜¯ä¸ºäººç±»å·¥ä½œæµç¨‹è®¾è®¡çš„ï¼Œè¿™é™åˆ¶äº†æ­¤ç±»æ•°æ®ä¸LLMå’ŒAIèµ‹èƒ½è‡ªåŠ¨åŒ–çš„é›†æˆã€‚è¿™ä¸€ç©ºç™½ä½¿å¾—ç§‘å­¦å®¶ã€è°ƒæŸ¥äººå‘˜ä»¥åŠæ—¥å¸¸ç”¨æˆ·ç¼ºä¹åŸºäºè¯æ®çš„æŒ‡å—ï¼Œæ¥æŒ‡å¯¼ä»–ä»¬å¦‚ä½•ä»¥æœ€é€‚åˆLLMçš„æ–¹å¼å‘ˆç°é—®å·ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥QASUï¼ˆé—®å·åˆ†æä¸ç»“æ„ç†è§£ï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå®ƒå¯ä»¥æ¢æŸ¥å…­ç§ç»“æ„æŠ€èƒ½ï¼ŒåŒ…æ‹¬ç­”æ¡ˆæŸ¥æ‰¾ã€å—è®¿è€…è®¡æ•°å’Œå¤šè·³æ¨ç†ç­‰ï¼Œæ¶µç›–å…­ç§åºåˆ—åŒ–æ ¼å¼å’Œå¤šç§æç¤ºç­–ç•¥ã€‚åœ¨å½“ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æ¬¡ä¼˜æ ¼å¼ç›¸æ¯”ï¼Œé€‰æ‹©æœ‰æ•ˆçš„æ ¼å¼å’Œæç¤ºç»„åˆå¯ä»¥å°†å‡†ç¡®æ€§æé«˜é«˜è¾¾8.8ä¸ªç™¾åˆ†ç‚¹ã€‚å¯¹äºç‰¹å®šä»»åŠ¡ï¼Œé€šè¿‡è‡ªæˆ‘å¢å¼ºæç¤ºä»”ç»†æ·»åŠ è½»é‡çº§ç»“æ„æç¤ºå¹³å‡å¯ä»¥è¿›ä¸€æ­¥æé«˜3-4ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡ç³»ç»Ÿåœ°éš”ç¦»æ ¼å¼å’Œæç¤ºæ•ˆæœï¼Œæˆ‘ä»¬å¼€æ”¾çš„åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªç®€å•è€Œé€šç”¨çš„åŸºç¡€ï¼Œå¯æ¨åŠ¨LLMåœ¨é—®å·è°ƒæŸ¥åˆ†ææ–¹é¢çš„ç ”ç©¶å’Œå®é™…åº”ç”¨çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26238v1">PDF</a> 14 pages, 3 figures, 8 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é—®å·åˆ†æç»“æ„ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆQASUï¼‰æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é—®å·æ•°æ®æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å…­ç§ç»“æ„æŠ€èƒ½ï¼Œæ¶‰åŠç­”æ¡ˆæŸ¥æ‰¾ã€å—è®¿è€…è®¡æ•°å’Œå¤šè·³æ¨ç†ç­‰ï¼Œè·¨è¶Šå…­ç§åºåˆ—åŒ–æ ¼å¼å’Œå¤šç§æç¤ºç­–ç•¥ã€‚å®éªŒæ˜¾ç¤ºï¼Œé€‰æ‹©æœ‰æ•ˆçš„æ ¼å¼å’Œæç¤ºç»„åˆå¯ä»¥æé«˜å‡†ç¡®ç‡è¾¾8.8%ã€‚é€šè¿‡ç³»ç»Ÿåœ°éš”ç¦»æ ¼å¼å’Œæç¤ºæ•ˆæœï¼ŒQASUä¸ºæ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é—®å·åˆ†æé¢†åŸŸçš„ç ”ç©¶å’Œå®è·µæä¾›äº†ç®€å•è€Œé€šç”¨çš„åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é—®å·æ•°æ®å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦ä¸“é—¨çš„åŸºå‡†æµ‹è¯•å¦‚QASUæ¥è§£å†³ã€‚</li>
<li>QASUåŒ…æ‹¬å¤šç§ç»“æ„æŠ€èƒ½ï¼Œæ¶µç›–ç­”æ¡ˆæŸ¥æ‰¾ã€å—è®¿è€…è®¡æ•°å’Œå¤šè·³æ¨ç†ç­‰ã€‚</li>
<li>ä¸åŒçš„åºåˆ—åŒ–æ ¼å¼å’Œæç¤ºç­–ç•¥å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>é€‰æ‹©æœ‰æ•ˆçš„æ ¼å¼å’Œæç¤ºç»„åˆå¯ä»¥æé«˜å‡†ç¡®ç‡è¾¾8.8%ã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘å¢å¼ºæç¤ºæ·»åŠ è½»é‡çº§ç»“æ„æç¤ºï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>QASUä¸ºæ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é—®å·åˆ†æé¢†åŸŸçš„ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26238">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-350bf6bc215a5b0f502014f69e579a05" align="middle">
<img src="https://picx.zhimg.com/v2-4d078367d5456f161746f5e7d32828a3" align="middle">
<img src="https://picx.zhimg.com/v2-3a066f0605d24d896b797cdc0b29af16" align="middle">
<img src="https://picx.zhimg.com/v2-b7c6410cb6e2d63e1c57f2fae5f3706e" align="middle">
<img src="https://picx.zhimg.com/v2-54ffde9848f2b5c99b7de94bc4837667" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Prototype-Driven-Adaptation-for-Few-Shot-Object-Detection"><a href="#Prototype-Driven-Adaptation-for-Few-Shot-Object-Detection" class="headerlink" title="Prototype-Driven Adaptation for Few-Shot Object Detection"></a>Prototype-Driven Adaptation for Few-Shot Object Detection</h2><p><strong>Authors:Yushen Huang, Zhiming Wang</strong></p>
<p>Few-shot object detection (FSOD) often suffers from base-class bias and unstable calibration when only a few novel samples are available. We propose Prototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN that provides a prototype-based â€œsecond opinionâ€ complementary to the linear classifier. PDA maintains support-only prototypes in a learnable identity-initialized projection space and optionally applies prototype-conditioned RoI alignment to reduce geometric mismatch. During fine-tuning, prototypes can be adapted via exponential moving average(EMA) updates on labeled foreground RoIs-without introducing class-specific parameters-and are frozen at inference to ensure strict protocol compliance. PDA employs a best-of-K matching scheme to capture intra-class multi-modality and temperature-scaled fusion to combine metric similarities with detector logits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently improves novel-class performance with minimal impact on base classes and negligible computational overhead. </p>
<blockquote>
<p>å°æ ·æ£€æµ‹ï¼ˆFSODï¼‰é€šå¸¸åœ¨åªæœ‰å°‘é‡æ–°æ ·æœ¬å¯ç”¨æ—¶é¢ä¸´åŸºç¡€ç±»åˆ«åå·®å’Œä¸ç¨³å®šæ ¡å‡†çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†åŸå‹é©±åŠ¨å¯¹é½ï¼ˆPDAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ã€å³æ’å³ç”¨çš„åº¦é‡å¤´ï¼Œé€‚ç”¨äºDeFRCNï¼Œåœ¨çº¿æ€§åˆ†ç±»å™¨çš„åŸºç¡€ä¸Šæä¾›åŸºäºåŸå‹çš„â€œç¬¬äºŒæ„è§â€ä½œä¸ºè¡¥å……ã€‚PDAåœ¨å¯å­¦ä¹ çš„èº«ä»½åˆå§‹åŒ–æŠ•å½±ç©ºé—´ä¸­ç»´æŠ¤ä»…æ”¯æŒåŸå‹ï¼Œå¹¶å¯é€‰æ‹©åº”ç”¨åŸå‹æ¡ä»¶åŒ–RoIå¯¹é½ä»¥å‡å°‘å‡ ä½•ä¸åŒ¹é…ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥é€šè¿‡å¯¹æ ‡è®°å‰æ™¯RoIsçš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ›´æ–°æ¥é€‚åº”åŸå‹ï¼Œè€Œæ— éœ€å¼•å…¥ç‰¹å®šç±»åˆ«çš„å‚æ•°ï¼Œå¹¶åœ¨æ¨ç†æ—¶å†»ç»“åŸå‹ä»¥ç¡®ä¿ä¸¥æ ¼éµå®ˆåè®®ã€‚PDAé‡‡ç”¨æœ€ä½³KåŒ¹é…æ–¹æ¡ˆæ¥æ•æ‰ç±»å†…å¤šæ¨¡æ€æ€§ï¼Œå¹¶é‡‡ç”¨æ¸©åº¦ç¼©æ”¾èåˆå°†åº¦é‡ç›¸ä¼¼æ€§ä¸æ£€æµ‹å™¨å¯¹æ•°å‡ ç‡ç›¸ç»“åˆã€‚åœ¨VOC FSODå’ŒGFSODåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPDAåœ¨åŸºç¡€ç±»åˆ«ä¸Šçš„å½±å“å¾®ä¹å…¶å¾®ï¼Œè®¡ç®—å¼€é”€å¯å¿½ç•¥ä¸è®¡çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆæé«˜äº†æ–°ç±»åˆ«çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25318v1">PDF</a> 7 pages,1 figure,2 tables,Preprint</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰ä¸­é‡åˆ°çš„åŸºç±»åå·®å’Œæ–°æ ·æœ¬æ•°é‡æœ‰é™å¯¼è‡´çš„ä¸ç¨³å®šæ ¡å‡†é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPrototype-Driven Alignmentï¼ˆPDAï¼‰çš„è½»é‡çº§æ’ä»¶åº¦é‡å¤´ã€‚PDAé€šè¿‡æä¾›åŸºäºåŸå‹çš„â€œç¬¬äºŒæ„è§â€æ¥è¡¥å……çº¿æ€§åˆ†ç±»å™¨ï¼Œé€šè¿‡ç»´æŠ¤æ”¯æŒåŸå‹æ¥å‡å°‘å‡ ä½•ä¸åŒ¹é…å¹¶ä¼˜åŒ–åŸå‹å¯¹é½æ–¹å¼ï¼Œè¿›è€Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒåŸå‹å¯ä»¥é€šè¿‡å¯¹æ ‡è®°å‰æ™¯RoIsçš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ›´æ–°è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼ŒåŒæ—¶ç¡®ä¿æ¨ç†æ—¶éµå¾ªä¸¥æ ¼çš„åè®®ã€‚PDAé‡‡ç”¨æœ€ä½³KåŒ¹é…æ–¹æ¡ˆå’Œæ¸©åº¦ç¼©æ”¾èåˆç­–ç•¥ï¼Œä»¥ç»“åˆåº¦é‡ç›¸ä¼¼æ€§å’Œæ£€æµ‹å™¨å¯¹æ•°æ¦‚ç‡å€¼ã€‚åœ¨VOC FSODå’ŒGFSODåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPDAèƒ½å¤ŸæŒç»­æé«˜æ–°ç±»åˆ«çš„æ€§èƒ½ï¼Œå¯¹åŸºç±»çš„å½±å“æå°ä¸”è®¡ç®—å¼€é”€å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PDAé’ˆå¯¹å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ä¸­çš„åŸºç±»åå·®å’Œæ–°æ ·æœ¬ä¸ç¨³å®šæ ¡å‡†é—®é¢˜æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>PDAæ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ’ä»¶åº¦é‡å¤´ï¼Œä¸ºDeFRCNæä¾›äº†åŸºäºåŸå‹çš„â€œç¬¬äºŒæ„è§â€è¡¥å……ã€‚</li>
<li>PDAé€šè¿‡ç»´æŠ¤å­¦ä¹ èº«ä»½åˆå§‹åŒ–çš„æŠ•å½±ç©ºé—´ä¸­çš„æ”¯æŒåŸå‹æ¥å‡å°‘å‡ ä½•ä¸åŒ¹é…ã€‚</li>
<li>åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼ŒåŸå‹å¯ä»¥é€šè¿‡EMAæ›´æ–°è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ï¼ŒåŒæ—¶ç¡®ä¿æ¨ç†æ—¶éµå¾ªä¸¥æ ¼çš„åè®®ã€‚</li>
<li>PDAé‡‡ç”¨æœ€ä½³KåŒ¹é…æ–¹æ¡ˆæ¥æ•æ‰ç±»å†…å¤šæ¨¡æ€æ€§ï¼Œå¹¶ç»“åˆæ¸©åº¦ç¼©æ”¾èåˆç­–ç•¥æ¥ç»“åˆåº¦é‡ç›¸ä¼¼æ€§å’Œæ£€æµ‹å™¨å¯¹æ•°æ¦‚ç‡å€¼ã€‚</li>
<li>PDAåœ¨VOC FSODå’ŒGFSODåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œèƒ½å¤ŸæŒç»­æé«˜æ–°ç±»åˆ«çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a1eba453b300c26e5bc62c80c921f14" align="middle">
<img src="https://picx.zhimg.com/v2-1891df55f6f9ebe8112c9189246a2696" align="middle">
<img src="https://picx.zhimg.com/v2-e77590b2ba75a9a34e180e8b5283ced1" align="middle">
<img src="https://picx.zhimg.com/v2-8b7a3afff62b441d5d4d8c29536a16fe" align="middle">
<img src="https://picx.zhimg.com/v2-6551c8ea041294968b541da01cb207c9" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Remote-Sensing-Image-Scene-Classification-with-CLIP-and-Prompt-Learning"><a href="#Few-Shot-Remote-Sensing-Image-Scene-Classification-with-CLIP-and-Prompt-Learning" class="headerlink" title="Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt   Learning"></a>Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt   Learning</h2><p><strong>Authors:Ivica Dimitrovski, Vlatko Spasev, Ivan Kitanovski</strong></p>
<p>Remote sensing applications increasingly rely on deep learning for scene classification. However, their performance is often constrained by the scarcity of labeled data and the high cost of annotation across diverse geographic and sensor domains. While recent vision-language models like CLIP have shown promise by learning transferable representations at scale by aligning visual and textual modalities, their direct application to remote sensing remains suboptimal due to significant domain gaps and the need for task-specific semantic adaptation. To address this critical challenge, we systematically explore prompt learning as a lightweight and efficient adaptation strategy for few-shot remote sensing image scene classification. We evaluate several representative methods, including Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating Constraints. These approaches reflect complementary design philosophies: from static context optimization to conditional prompts for enhanced generalization, multi-modal prompts for joint vision-language adaptation, and semantically regularized prompts for stable learning without forgetting. We benchmark these prompt-learning methods against two standard baselines: zero-shot CLIP with hand-crafted prompts and a linear probe trained on frozen CLIP features. Through extensive experiments on multiple benchmark remote sensing datasets, including cross-dataset generalization tests, we demonstrate that prompt learning consistently outperforms both baselines in few-shot scenarios. Notably, Prompting with Self-Regulating Constraints achieves the most robust cross-domain performance. Our findings underscore prompt learning as a scalable and efficient solution for bridging the domain gap in satellite and aerial imagery, providing a strong foundation for future research in this field. </p>
<blockquote>
<p>é¥æ„Ÿåº”ç”¨è¶Šæ¥è¶Šä¾èµ–æ·±åº¦å­¦ä¹ è¿›è¡Œåœºæ™¯åˆ†ç±»ã€‚ç„¶è€Œï¼Œç”±äºå…¶ç¼ºä¹æ ‡æ³¨æ•°æ®å’Œåœ¨ä¸åŒåœ°ç†å’Œä¼ æ„Ÿå™¨é¢†åŸŸçš„æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œå®ƒä»¬çš„æ€§èƒ½å¾€å¾€å—åˆ°é™åˆ¶ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰é€šè¿‡å¤§è§„æ¨¡å­¦ä¹ å¯è½¬ç§»è¡¨ç¤ºå¹¶é€šè¿‡è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„å¯¹é½æ˜¾ç¤ºå‡ºå‰æ™¯ï¼Œä½†å®ƒä»¬ç›´æ¥åº”ç”¨äºé¥æ„Ÿä»ç„¶ä¸æ˜¯æœ€ä¼˜çš„ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå­˜åœ¨æ˜æ˜¾çš„é¢†åŸŸå·®è·å’Œéœ€è¦ç‰¹å®šä»»åŠ¡çš„è¯­ä¹‰é€‚åº”ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†æç¤ºå­¦ä¹ ä½œä¸ºä¸€ç§è½»é‡çº§ã€é«˜æ•ˆçš„é€‚åº”ç­–ç•¥ï¼Œç”¨äºå°‘æ•°é¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§å…·æœ‰ä»£è¡¨æ€§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€æ¡ä»¶ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€å¤šæ¨¡æ€æç¤ºå­¦ä¹ å’Œå¸¦æœ‰è‡ªæˆ‘è°ƒæ§çº¦æŸçš„æç¤ºã€‚è¿™äº›æ–¹æ³•åæ˜ äº†äº’è¡¥çš„è®¾è®¡ç†å¿µï¼šä»é™æ€ä¸Šä¸‹æ–‡ä¼˜åŒ–åˆ°æ¡ä»¶æç¤ºä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œå¤šæ¨¡æ€æç¤ºç”¨äºè”åˆè§†è§‰è¯­è¨€é€‚åº”ï¼Œä»¥åŠè¯­ä¹‰æ­£åˆ™åŒ–æç¤ºä»¥å®ç°ç¨³å®šå­¦ä¹ è€Œä¸ä¼šé—å¿˜ã€‚æˆ‘ä»¬å°†è¿™äº›æç¤ºå­¦ä¹ æ–¹æ³•ä¸ä¸¤ä¸ªæ ‡å‡†åŸºå‡†çº¿è¿›è¡Œæ¯”è¾ƒï¼šä½¿ç”¨æ‰‹å·¥æç¤ºçš„é›¶æ ·æœ¬CLIPå’Œåœ¨çº¿æ€§æ¢æµ‹å™¨ä¸Šè®­ç»ƒçš„å†·å†»CLIPç‰¹å¾ã€‚é€šè¿‡å¯¹å¤šä¸ªåŸºå‡†é¥æ„Ÿæ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬è·¨æ•°æ®é›†æ³›åŒ–æµ‹è¯•ï¼Œæˆ‘ä»¬è¯æ˜åœ¨å°‘æ•°åœºæ™¯ä¸­ï¼Œæç¤ºå­¦ä¹ å§‹ç»ˆä¼˜äºä¸¤ä¸ªåŸºå‡†çº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œâ€œå¸¦æœ‰è‡ªæˆ‘è°ƒæ§çº¦æŸçš„æç¤ºâ€å®ç°äº†æœ€ç¨³å¥çš„è·¨åŸŸæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†æç¤ºå­¦ä¹ ä½œä¸ºç¼©å°å«æ˜Ÿå’Œèˆªç©ºå›¾åƒé¢†åŸŸå·®è·çš„å¯æ‰©å±•å’Œé«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24321v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ åœ¨é¥æ„Ÿåœºæ™¯åˆ†ç±»ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†ç”±äºç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œå…¶æ€§èƒ½å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡æ¢ç´¢äº†æç¤ºå­¦ä¹ ä½œä¸ºä¸€ç§è½»é‡çº§å’Œé«˜æ•ˆçš„é€‚åº”ç­–ç•¥ï¼Œç”¨äºè§£å†³é¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»ä¸­çš„å°æ ·é—®é¢˜ã€‚é€šè¿‡å¯¹æ¯”å‡ ç§ä»£è¡¨æ€§çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€æ¡ä»¶ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€å¤šæ¨¡æ€æç¤ºå­¦ä¹ å’Œå¸¦æœ‰è‡ªæˆ‘è°ƒèŠ‚çº¦æŸçš„æç¤ºç­‰ï¼Œå‘ç°æç¤ºå­¦ä¹ åœ¨è·¨æ•°æ®é›†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºè§£å†³å«æ˜Ÿå’Œèˆªç©ºå›¾åƒçš„åŸŸé—´éš™é—®é¢˜æä¾›äº†å¯æ‰©å±•å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±åº¦å­¦ä¹ åœ¨é¥æ„Ÿåœºæ™¯åˆ†ç±»ä¸­çš„åº”ç”¨å—åˆ°æ•°æ®æ ‡æ³¨ç¨€ç¼ºæ€§çš„é™åˆ¶ã€‚</li>
<li>æç¤ºå­¦ä¹ ä½œä¸ºä¸€ç§è½»é‡çº§å’Œé«˜æ•ˆçš„é€‚åº”ç­–ç•¥ï¼Œç”¨äºè§£å†³å°æ ·é—®é¢˜ã€‚</li>
<li>ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€æ¡ä»¶ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€å¤šæ¨¡æ€æç¤ºå­¦ä¹ å’Œå¸¦æœ‰è‡ªæˆ‘è°ƒèŠ‚çº¦æŸçš„æç¤ºç­‰æ–¹æ³•å…·æœ‰ä¸åŒçš„è®¾è®¡å“²å­¦ã€‚</li>
<li>æç¤ºå­¦ä¹ åœ¨è·¨æ•°æ®é›†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªæˆ‘è°ƒèŠ‚çº¦æŸæç¤ºä¸‹å®ç°æœ€ç¨³å¥çš„è·¨åŸŸæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f040b1abc5063cb60d024a6deab3d045" align="middle">
<img src="https://picx.zhimg.com/v2-f5b7a2b3910548c55f1c0b0c00fa75cb" align="middle">
<img src="https://picx.zhimg.com/v2-3d106d9dbe7329084d13b68523457cdd" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Transparent-Reasoning-What-Drives-Faithfulness-in-Large-Language-Models"><a href="#Towards-Transparent-Reasoning-What-Drives-Faithfulness-in-Large-Language-Models" class="headerlink" title="Towards Transparent Reasoning: What Drives Faithfulness in Large   Language Models?"></a>Towards Transparent Reasoning: What Drives Faithfulness in Large   Language Models?</h2><p><strong>Authors:Teague McMillan, Gabriele Dominici, Martin Gjoreski, Marc Langheinrich</strong></p>
<p>Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions), and manipulate the number and type of few-shot examples, prompting strategies, and training procedure. Our results show: (i) both the quantity and quality of few-shot examples significantly impact model faithfulness; (ii) faithfulness is sensitive to prompting design; (iii) the instruction-tuning phase improves measured faithfulness on MedQA. These findings offer insights into strategies for enhancing the interpretability and trustworthiness of LLMs in sensitive domains. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äº§ç”Ÿçš„è§£é‡Šé€šå¸¸ä¸èƒ½çœŸå®åœ°åæ˜ å…¶é¢„æµ‹èƒŒåçš„å› ç´ ã€‚åœ¨åŒ»ç–—ç¯å¢ƒä¸­ï¼Œè¿™ç§ä¸å¿ å°¤å…¶æˆé—®é¢˜ï¼šè§£é‡Šä¸­çœç•¥é‡è¦çš„ä¸´åºŠçº¿ç´¢æˆ–æ©ç›–é”™è¯¯çš„æ·å¾„ä¼šç ´ååŒ»ç”Ÿå¯¹æ¨¡å‹çš„ä¿¡ä»»ï¼Œå¯¼è‡´ä¸å®‰å…¨çš„å†³ç­–æ”¯æŒã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¨ç†å’Œè®­ç»ƒæ—¶çš„é€‰æ‹©å¦‚ä½•å½±å“è§£é‡Šçš„å¿ å®åº¦ï¼Œé‡ç‚¹å…³æ³¨éƒ¨ç½²é˜¶æ®µå®è·µè€…å¯ä»¥æ§åˆ¶çš„å› ç´ ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†BBQï¼ˆç¤¾äº¤åè§ï¼‰å’ŒMedQAï¼ˆåŒ»å­¦è®¸å¯é—®é¢˜ï¼‰ä¸Šè¯„ä¼°äº†ä¸‰ä¸ªLLMï¼ˆGPT-4.1 miniã€LLaMA 70Bã€LLaMA 8Bï¼‰ï¼Œå¹¶æ“ä½œäº†å°‘é‡ç¤ºä¾‹çš„æ•°é‡å’Œç±»å‹ã€æç¤ºç­–ç•¥ä»¥åŠè®­ç»ƒç¨‹åºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼šï¼ˆiï¼‰æ— è®ºæ˜¯æ•°é‡è¿˜æ˜¯è´¨é‡ä¸Šï¼Œå°‘é‡çš„ä¾‹å­éƒ½ä¼šå¯¹æ¨¡å‹çš„å¿ å®åº¦äº§ç”Ÿæ˜¾è‘—å½±å“ï¼›ï¼ˆiiï¼‰å¿ å®åº¦å¯¹æç¤ºè®¾è®¡æ•æ„Ÿï¼›ï¼ˆiiiï¼‰æŒ‡ä»¤è°ƒæ•´é˜¶æ®µæé«˜äº†åœ¨MedQAä¸Šçš„è¡¡é‡å¿ å®åº¦ã€‚è¿™äº›å‘ç°å¯¹äºæé«˜æ•æ„Ÿé¢†åŸŸLLMçš„å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦æä¾›äº†ç­–ç•¥ä¸Šçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24236v2">PDF</a> 39th Conference on Neural Information Processing Systems (NeurIPS   2025) Workshop: NeurIPS 2025 Workshop on Evaluating the Evolving LLM   Lifecycle: Benchmarks, Emergent Abilities, and Scaling</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äº§ç”Ÿçš„è§£é‡Šé€šå¸¸ä¸èƒ½å¿ å®åæ˜ å…¶é¢„æµ‹çš„å› ç´ ã€‚åœ¨åŒ»ç–—ç¯å¢ƒä¸­ï¼Œä¸å¿ å®çš„è§£é‡Šå°¤å…¶æˆé—®é¢˜ï¼šçœç•¥é‡è¦ä¸´åºŠçº¿ç´¢æˆ–æ©ç›–è™šå‡æ·å¾„çš„è§£é‡Šå¯èƒ½ä¼šç ´ååŒ»ç”Ÿä¿¡ä»»ï¼Œå¯¼è‡´å†³ç­–æ”¯æŒçš„æ½œåœ¨é£é™©ã€‚æœ¬ç ”ç©¶æ¢è®¨æ¨ç†å’Œè®­ç»ƒæ—¶é—´çš„é€‰æ‹©å¦‚ä½•å½±å“è§£é‡Šçš„å¿ å®æ€§ï¼Œé‡ç‚¹å…³æ³¨éƒ¨ç½²æ—¶ä»ä¸šè€…å¯ä»¥æ§åˆ¶çš„å› ç´ ã€‚å¯¹ä¸‰ç§LLMsï¼ˆGPT-4.1-miniã€LLaMA 70Bã€LLaMA 8Bï¼‰åœ¨ä¸¤ä¸ªæ•°æ®é›†ï¼ˆBBQç¤¾ä¼šåè§å’ŒMedQAåŒ»å­¦è®¸å¯é—®é¢˜ï¼‰ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ“ä½œå°‘é‡ç¤ºä¾‹çš„æ•°é‡å’Œç±»å‹ã€æç¤ºç­–ç•¥å’ŒåŸ¹è®­ç¨‹åºã€‚ç»“æœè¡¨æ˜ï¼šï¼ˆiï¼‰å°‘é‡ç¤ºä¾‹çš„æ•°é‡å’Œè´¨é‡æ˜¾è‘—å½±å“æ¨¡å‹çš„å¿ å®æ€§ï¼›ï¼ˆiiï¼‰å¿ å®æ€§å¯¹æç¤ºè®¾è®¡æ•æ„Ÿï¼›ï¼ˆiiiï¼‰æŒ‡ä»¤è°ƒæ•´é˜¶æ®µæé«˜äº†åœ¨MedQAä¸Šçš„æµ‹é‡å¿ å®æ€§ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæ·±å…¥äº†è§£å¦‚ä½•æ”¹è¿›æ•æ„Ÿé¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è§£é‡Šä¸æ€»æ˜¯å¿ å®äºé¢„æµ‹å› ç´ ã€‚</li>
<li>åœ¨åŒ»ç–—ç¯å¢ƒä¸­ï¼Œä¸å¿ å®çš„è§£é‡Šå¯èƒ½å¯¼è‡´åŒ»ç”Ÿä¿¡ä»»å—æŸå’Œå†³ç­–é£é™©ã€‚</li>
<li>æ¨ç†å’Œè®­ç»ƒæ—¶é—´çš„é€‰æ‹©å½±å“è§£é‡Šçš„å¿ å®æ€§ã€‚</li>
<li>å°‘é‡ç¤ºä¾‹çš„æ•°é‡å’Œè´¨é‡å¯¹æ¨¡å‹çš„å¿ å®æ€§æœ‰é‡è¦å½±å“ã€‚</li>
<li>æ¨¡å‹å¿ å®æ€§å¯¹æç¤ºè®¾è®¡æ•æ„Ÿã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¯ä»¥æé«˜æ¨¡å‹çš„å¿ å®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca379376d958fa488cfc7b12463ad1da" align="middle">
<img src="https://picx.zhimg.com/v2-99a05e74cfacac98b95473984bc5dcca" align="middle">
<img src="https://picx.zhimg.com/v2-7ab7b93fb26830065bb014c0460ee308" align="middle">
<img src="https://picx.zhimg.com/v2-a141f3c5c43b7dfec11dc97c5691e85c" align="middle">
<img src="https://picx.zhimg.com/v2-ff47b0358375d7b80fdf4d1e19e2e236" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MoEMeta-Mixture-of-Experts-Meta-Learning-for-Few-Shot-Relational-Learning"><a href="#MoEMeta-Mixture-of-Experts-Meta-Learning-for-Few-Shot-Relational-Learning" class="headerlink" title="MoEMeta: Mixture-of-Experts Meta Learning for Few-Shot Relational   Learning"></a>MoEMeta: Mixture-of-Experts Meta Learning for Few-Shot Relational   Learning</h2><p><strong>Authors:Han Wu, Jie Yin</strong></p>
<p>Few-shot knowledge graph relational learning seeks to perform reasoning over relations given only a limited number of training examples. While existing approaches largely adopt a meta-learning framework for enabling fast adaptation to new relations, they suffer from two key pitfalls. First, they learn relation meta-knowledge in isolation, failing to capture common relational patterns shared across tasks. Second, they struggle to effectively incorporate local, task-specific contexts crucial for rapid adaptation. To address these limitations, we propose MoEMeta, a novel meta-learning framework that disentangles globally shared knowledge from task-specific contexts to enable both effective generalization and rapid adaptation. MoEMeta introduces two key innovations: (i) a mixture-of-experts (MoE) model that learns globally shared relational prototypes to enhance generalization, and (ii) a task-tailored adaptation mechanism that captures local contexts for fast task-specific adaptation. By balancing global generalization with local adaptability, MoEMeta significantly advances few-shot relational learning. Extensive experiments and analyses on three KG benchmarks demonstrate that MoEMeta consistently outperforms existing baselines, achieving state-of-the-art performance. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬çŸ¥è¯†å›¾è°±å…³ç³»å­¦ä¹ æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬è¿›è¡Œå…³ç³»æ¨ç†ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨å…ƒå­¦ä¹ æ¡†æ¶ï¼Œä»¥å®ç°å¯¹æ–°å…³ç³»çš„å¿«é€Ÿé€‚åº”ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºé™·ã€‚é¦–å…ˆï¼Œå®ƒä»¬å­¤ç«‹åœ°å­¦ä¹ å…³ç³»å…ƒçŸ¥è¯†ï¼Œæ— æ³•æ•è·è·¨ä»»åŠ¡å…±äº«çš„å¸¸è§å…³ç³»æ¨¡å¼ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬éš¾ä»¥æœ‰æ•ˆåœ°èå…¥å¯¹å¿«é€Ÿé€‚åº”è‡³å…³é‡è¦çš„å±€éƒ¨ç‰¹å®šä»»åŠ¡ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MoEMetaï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å…ƒå­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè§£æ„å…¨å±€å…±äº«çŸ¥è¯†å¹¶èå…¥ç‰¹å®šä»»åŠ¡çš„ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œä»¥å®ç°æœ‰æ•ˆçš„æ³›åŒ–å’Œå¿«é€Ÿé€‚åº”ã€‚MoEMetaå¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œå­¦ä¹ å…¨å±€å…±äº«çš„å…³ç³»åŸå‹ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼›äºŒæ˜¯ä»»åŠ¡å®šåˆ¶é€‚åº”æœºåˆ¶ï¼Œæ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡ç¯å¢ƒä»¥å®ç°ç‰¹å®šä»»åŠ¡çš„å¿«é€Ÿé€‚åº”ã€‚é€šè¿‡å¹³è¡¡å…¨å±€æ³›åŒ–ä¸å±€éƒ¨é€‚åº”æ€§ï¼ŒMoEMetaå¤§å¹…æå‡äº†å°‘é‡æ ·æœ¬å…³ç³»å­¦ä¹ ã€‚åœ¨ä¸‰ä¸ªçŸ¥è¯†å›¾è°±åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒå’Œåˆ†æè¡¨æ˜ï¼ŒMoEMetaå§‹ç»ˆä¼˜äºç°æœ‰åŸºçº¿ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23013v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>å°‘é‡æ ·æœ¬çŸ¥è¯†å›¾è°±å…³ç³»å­¦ä¹ æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬è¿›è¡Œå…³ç³»æ¨ç†ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡ç”¨å…ƒå­¦ä¹ æ¡†æ¶ï¼Œä»¥å®ç°å¯¹æ–°å…³ç³»çš„å¿«é€Ÿé€‚åº”ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºé™·ã€‚é¦–å…ˆï¼Œå®ƒä»¬å­¤ç«‹åœ°å­¦ä¹ å…³ç³»å…ƒçŸ¥è¯†ï¼Œæ— æ³•æ•è·ä»»åŠ¡é—´å…±äº«çš„å¸¸è§å…³ç³»æ¨¡å¼ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬éš¾ä»¥æœ‰æ•ˆåœ°èå…¥å¯¹å¿«é€Ÿä»»åŠ¡é€‚åº”è‡³å…³é‡è¦çš„å±€éƒ¨ç‰¹å®šä»»åŠ¡ä¸Šä¸‹æ–‡ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MoEMetaè¿™ä¸€æ–°å‹å…ƒå­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ†ç¦»å…¨å±€å…±äº«çŸ¥è¯†ä¸ä»»åŠ¡ç‰¹å®šä¸Šä¸‹æ–‡ï¼Œå®ç°äº†æœ‰æ•ˆçš„æ³›åŒ–å’Œå¿«é€Ÿé€‚åº”ã€‚MoEMetaå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆiï¼‰æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œå­¦ä¹ å…¨å±€å…±äº«å…³ç³»åŸå‹ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼›ï¼ˆiiï¼‰ä»»åŠ¡å®šåˆ¶é€‚åº”æœºåˆ¶ï¼Œæ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡ä»¥å®ç°ç‰¹å®šä»»åŠ¡çš„å¿«é€Ÿé€‚åº”ã€‚é€šè¿‡å¹³è¡¡å…¨å±€æ³›åŒ–ä¸å±€éƒ¨é€‚åº”æ€§ï¼ŒMoEMetaå¤§å¹…æå‡äº†å°‘é‡æ ·æœ¬å…³ç³»å­¦ä¹ ã€‚åœ¨ä¸‰ä¸ªçŸ¥è¯†å›¾è°±åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒå’Œåˆ†æè¡¨æ˜ï¼ŒMoEMetaå§‹ç»ˆä¼˜äºç°æœ‰åŸºçº¿ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot knowledge graph relational learning æ—¨åœ¨åˆ©ç”¨æœ‰é™è®­ç»ƒæ ·æœ¬è¿›è¡Œå…³ç³»æ¨ç†ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é‡‡ç”¨å…ƒå­¦ä¹ æ¡†æ¶ï¼Œä½†å­˜åœ¨å­¤ç«‹å­¦ä¹ å…³ç³»å…ƒçŸ¥è¯†å’Œéš¾ä»¥èå…¥å±€éƒ¨ç‰¹å®šä»»åŠ¡ä¸Šä¸‹æ–‡çš„é—®é¢˜ã€‚</li>
<li>MoEMetaæ¡†æ¶é€šè¿‡åˆ†ç¦»å…¨å±€å…±äº«çŸ¥è¯†ä¸ä»»åŠ¡ç‰¹å®šä¸Šä¸‹æ–‡ï¼Œå®ç°æœ‰æ•ˆæ³›åŒ–å’Œå¿«é€Ÿé€‚åº”ã€‚</li>
<li>MoEMetaå¼•å…¥æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œå­¦ä¹ å…¨å±€å…±äº«å…³ç³»åŸå‹ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MoEMetaæå‡ºä»»åŠ¡å®šåˆ¶é€‚åº”æœºåˆ¶ï¼Œæ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡ä»¥å®ç°ç‰¹å®šä»»åŠ¡çš„å¿«é€Ÿé€‚åº”ã€‚</li>
<li>MoEMetaé€šè¿‡å¹³è¡¡å…¨å±€æ³›åŒ–ä¸å±€éƒ¨é€‚åº”æ€§ï¼Œæ˜¾è‘—æå‡äº†å°‘é‡æ ·æœ¬å…³ç³»å­¦ä¹ æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0d3571ccbcd4815921513131ef164af" align="middle">
<img src="https://picx.zhimg.com/v2-fa39d8be40c68b199928207a0641ec7a" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Conjugate-Relation-Modeling-for-Few-Shot-Knowledge-Graph-Completion"><a href="#Conjugate-Relation-Modeling-for-Few-Shot-Knowledge-Graph-Completion" class="headerlink" title="Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion"></a>Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion</h2><p><strong>Authors:Zilong Wang, Qingtian Zeng, Hua Duan, Cheng Cheng, Minghao Zou, Ziyang Wang</strong></p>
<p>Few-shot Knowledge Graph Completion (FKGC) infers missing triples from limited support samples, tackling long-tail distribution challenges. Existing methods, however, struggle to capture complex relational patterns and mitigate data sparsity. To address these challenges, we propose a novel FKGC framework for conjugate relation modeling (CR-FKGC). Specifically, it employs a neighborhood aggregation encoder to integrate higher-order neighbor information, a conjugate relation learner combining an implicit conditional diffusion relation module with a stable relation module to capture stable semantics and uncertainty offsets, and a manifold conjugate decoder for efficient evaluation and inference of missing triples in manifold space. Experiments on three benchmarks demonstrate that our method achieves superior performance over state-of-the-art methods. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆFKGCï¼‰ä»æœ‰é™çš„æ”¯æ’‘æ ·æœ¬ä¸­æ¨æ–­å‡ºç¼ºå¤±çš„ä¸‰å…ƒç»„ï¼Œè§£å†³äº†é•¿å°¾åˆ†å¸ƒæŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚çš„æ¨¡å¼å…³ç³»å¹¶ç¼“è§£æ•°æ®ç¨€ç–é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå…±è½­å…³ç³»å»ºæ¨¡çš„FKGCæ–°æ¡†æ¶ï¼ˆCR-FKGCï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé‡‡ç”¨é‚»åŸŸèšåˆç¼–ç å™¨æ¥æ•´åˆé«˜é˜¶é‚»å±…ä¿¡æ¯ï¼Œä¸€ä¸ªç»“åˆéšå¼æ¡ä»¶æ‰©æ•£å…³ç³»æ¨¡å—å’Œç¨³å®šå…³ç³»æ¨¡å—çš„å…±è½­å…³ç³»å­¦ä¹ è€…æ¥æ•æ‰ç¨³å®šè¯­ä¹‰å’Œä¸ç¡®å®šæ€§åç§»ï¼Œä»¥åŠä¸€ä¸ªæµå½¢å…±è½­è§£ç å™¨ï¼Œç”¨äºæµå½¢ç©ºé—´ä¸­ç¼ºå¤±ä¸‰å…ƒç»„çš„æœ‰æ•ˆè¯„ä¼°å’Œæ¨ç†ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¼˜äºæœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22656v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Few-shotçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆFKGCï¼‰çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å¯¹å¤æ‚å…³ç³»æ¨¡å¼å’Œç¨€ç–æ•°æ®æŒ‘æˆ˜æ—¶æ‰è¥Ÿè§è‚˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„FKGCæ¡†æ¶â€”â€”å…±è½­å…³ç³»å»ºæ¨¡ï¼ˆCR-FKGCï¼‰ã€‚è¯¥æ¡†æ¶é‡‡ç”¨é‚»åŸŸèšåˆç¼–ç å™¨æ•´åˆé«˜é˜¶é‚»åŸŸä¿¡æ¯ï¼Œé€šè¿‡éšæ€§æ¡ä»¶æ‰©æ•£å…³ç³»æ¨¡å—ä¸ç¨³å®šå…³ç³»æ¨¡å—çš„ç»“åˆï¼Œæ•æ‰ç¨³å®šè¯­ä¹‰å’Œä¸ç¡®å®šæ€§åç§»ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ªæµå½¢å…±è½­è§£ç å™¨ï¼Œç”¨äºåœ¨æµå½¢ç©ºé—´ä¸­å¯¹ç¼ºå¤±çš„ä¸‰å…ƒç»„è¿›è¡Œé«˜æ•ˆè¯„ä¼°å’Œæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shotçŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆFKGCï¼‰æ—¨åœ¨ä»æœ‰é™çš„æ ·æœ¬ä¸­æ¨æ–­å‡ºç¼ºå¤±çš„ä¸‰å…ƒç»„ï¼Œä»¥è§£å†³é•¿å°¾åˆ†å¸ƒçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æ•æ‰å¤æ‚å…³ç³»æ¨¡å¼å’Œç¼“è§£æ•°æ®ç¨€ç–æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºçš„CR-FKGCæ¡†æ¶é‡‡ç”¨é‚»åŸŸèšåˆç¼–ç å™¨æ•´åˆé«˜é˜¶é‚»åŸŸä¿¡æ¯ã€‚</li>
<li>CR-FKGCç»“åˆéšæ€§æ¡ä»¶æ‰©æ•£å…³ç³»æ¨¡å—ä¸ç¨³å®šå…³ç³»æ¨¡å—ï¼Œä»¥æ•æ‰ç¨³å®šè¯­ä¹‰å’Œä¸ç¡®å®šæ€§åç§»ã€‚</li>
<li>æµå½¢å…±è½­è§£ç å™¨ç”¨äºåœ¨æµå½¢ç©ºé—´ä¸­å¯¹ç¼ºå¤±çš„ä¸‰å…ƒç»„è¿›è¡Œé«˜æ•ˆè¯„ä¼°å’Œæ¨ç†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCR-FKGCåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acd45f66c98770a7aca2415dc524a9ee" align="middle">
<img src="https://picx.zhimg.com/v2-91326b172ea2b620a87f357359f55c45" align="middle">
<img src="https://picx.zhimg.com/v2-7dd7c81b60636cbd164f2284087e6dfe" align="middle">
<img src="https://picx.zhimg.com/v2-f8071f3611627b04192052e02e86053a" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Knowledge-Distillation-of-LLMs-With-Counterfactual-Explanations"><a href="#Few-Shot-Knowledge-Distillation-of-LLMs-With-Counterfactual-Explanations" class="headerlink" title="Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations"></a>Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations</h2><p><strong>Authors:Faisal Hamman, Pasan Dissanayake, Yanjun Fu, Sanghamitra Dutta</strong></p>
<p>Knowledge distillation is a promising approach to transfer capabilities from complex teacher models to smaller, resource-efficient student models that can be deployed easily, particularly in task-aware scenarios. However, existing methods of task-aware distillation typically require substantial quantities of data which may be unavailable or expensive to obtain in many practical scenarios. In this paper, we address this challenge by introducing a novel strategy called Counterfactual-explanation-infused Distillation CoD for few-shot task-aware knowledge distillation by systematically infusing counterfactual explanations. Counterfactual explanations (CFEs) refer to inputs that can flip the output prediction of the teacher model with minimum perturbation. Our strategy CoD leverages these CFEs to precisely map the teacherâ€™s decision boundary with significantly fewer samples. We provide theoretical guarantees for motivating the role of CFEs in distillation, from both statistical and geometric perspectives. We mathematically show that CFEs can improve parameter estimation by providing more informative examples near the teacherâ€™s decision boundary. We also derive geometric insights on how CFEs effectively act as knowledge probes, helping the students mimic the teacherâ€™s decision boundaries more effectively than standard data. We perform experiments across various datasets and LLMs to show that CoD outperforms standard distillation approaches in few-shot regimes (as low as 8-512 samples). Notably, CoD only uses half of the original samples used by the baselines, paired with their corresponding CFEs and still improves performance. </p>
<blockquote>
<p>çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å¤æ‚çš„æ•™å¸ˆæ¨¡å‹å‘æ›´å°ã€èµ„æºæ•ˆç‡æ›´é«˜ã€æ˜“äºéƒ¨ç½²çš„å­¦ç”Ÿæ¨¡å‹è½¬ç§»èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡æ„ŸçŸ¥åœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»»åŠ¡æ„ŸçŸ¥è’¸é¦æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®ï¼Œè¿™åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­å¯èƒ½æ— æ³•è·å¾—æˆ–æˆæœ¬é«˜æ˜‚ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç§°ä¸ºâ€œèåˆåäº‹å®è§£é‡Šçš„è’¸é¦â€ï¼ˆCoDï¼‰çš„æ–°ç­–ç•¥ï¼Œç”¨äºå°‘æ ·æœ¬ä»»åŠ¡æ„ŸçŸ¥çŸ¥è¯†è’¸é¦ï¼Œé€šè¿‡ç³»ç»Ÿèåˆåäº‹å®è§£é‡Šã€‚åäº‹å®è§£é‡Šï¼ˆCFEï¼‰æ˜¯æŒ‡èƒ½å¤Ÿç”¨æœ€å°‘çš„æ‰°åŠ¨æ”¹å˜æ•™å¸ˆæ¨¡å‹è¾“å‡ºé¢„æµ‹çš„æŠ•å…¥ã€‚æˆ‘ä»¬çš„CoDç­–ç•¥åˆ©ç”¨è¿™äº›CFEæ¥ç²¾ç¡®æ˜ å°„æ•™å¸ˆçš„å†³ç­–è¾¹ç•Œï¼Œæ‰€éœ€æ ·æœ¬æ•°é‡å¤§å¤§å‡å°‘ã€‚æˆ‘ä»¬ä»ç»Ÿè®¡å’Œå‡ ä½•ä¸¤ä¸ªè§’åº¦ä¸ºåäº‹å®è§£é‡Šåœ¨è’¸é¦ä¸­çš„ä½œç”¨æä¾›äº†ç†è®ºä¿è¯ã€‚æˆ‘ä»¬æ•°å­¦ä¸Šè¯æ˜ï¼Œåäº‹å®è§£é‡Šå¯ä»¥é€šè¿‡æä¾›æ›´å¤šæ¥è¿‘æ•™å¸ˆå†³ç­–è¾¹ç•Œçš„ç¤ºä¾‹æ¥æ”¹å–„å‚æ•°ä¼°è®¡ã€‚æˆ‘ä»¬è¿˜ä»å‡ ä½•è§’åº¦æ´å¯Ÿäº†åäº‹å®è§£é‡Šå¦‚ä½•æœ‰æ•ˆåœ°ä½œä¸ºçŸ¥è¯†æ¢é’ˆå‘æŒ¥ä½œç”¨ï¼Œå¸®åŠ©å­¦ç”Ÿæ›´æœ‰æ•ˆåœ°æ¨¡ä»¿æ•™å¸ˆçš„å†³ç­–è¾¹ç•Œï¼Œè€Œä¸æ˜¯æ ‡å‡†æ•°æ®ã€‚æˆ‘ä»¬åœ¨å„ç§æ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹ï¼ŒCoDä¼˜äºæ ‡å‡†è’¸é¦æ–¹æ³•ï¼ˆä½è‡³8-512ä¸ªæ ·æœ¬ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒCoDä»…ä½¿ç”¨åŸºçº¿çš„ä¸€åŠæ ·æœ¬ï¼Œé…å¯¹ç›¸åº”çš„CFEï¼Œä»ç„¶å¯ä»¥æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21631v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCoDï¼ˆèåˆåäº‹å®è§£é‡Šçš„è’¸é¦ç­–ç•¥ï¼‰ï¼Œç”¨äºè§£å†³ä»»åŠ¡æ„ŸçŸ¥è’¸é¦ä¸­çš„å°‘æ ·æœ¬æŒ‘æˆ˜ã€‚è¯¥ç­–ç•¥é€šè¿‡ç³»ç»Ÿåœ°èå…¥åäº‹å®è§£é‡Šï¼ˆCFEï¼‰ï¼Œèƒ½å¤Ÿåœ¨å°‘é‡æ ·æœ¬ä¸‹ç²¾ç¡®åœ°æ˜ å°„æ•™å¸ˆæ¨¡å‹çš„å†³ç­–è¾¹ç•Œã€‚åäº‹å®è§£é‡Šæ˜¯é‚£äº›èƒ½å¤Ÿæœ€å°åŒ–æ‰°åŠ¨å¹¶æ”¹å˜æ•™å¸ˆæ¨¡å‹é¢„æµ‹ç»“æœçš„è¾“å…¥ã€‚æœ¬æ–‡é€šè¿‡ç†è®ºå’Œæ•°å­¦è¯æ˜ï¼Œè¯æ˜äº†åäº‹å®è§£é‡Šåœ¨è’¸é¦ä¸­çš„é‡è¦ä½œç”¨ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†åœ¨å„ç§æ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šï¼ŒCoDåœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹ä¼˜äºæ ‡å‡†è’¸é¦æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§ä»å¤æ‚æ•™å¸ˆæ¨¡å‹å‘æ›´å°ã€èµ„æºæ•ˆç‡æ›´é«˜çš„å­¦ç”Ÿæ¨¡å‹è½¬ç§»èƒ½åŠ›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡æ„ŸçŸ¥åœºæ™¯ä¸­ã€‚</li>
<li>ç°æœ‰çš„ä»»åŠ¡æ„ŸçŸ¥è’¸é¦æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ•°æ®ï¼Œè¿™åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­å¯èƒ½æ— æ³•è·å¾—æˆ–æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>CoDç­–ç•¥é€šè¿‡èå…¥åäº‹å®è§£é‡Šï¼ˆCFEï¼‰æ¥è§£å†³å°‘æ ·æœ¬è’¸é¦çš„æŒ‘æˆ˜ï¼Œèƒ½å¤Ÿç²¾ç¡®åœ°æ˜ å°„æ•™å¸ˆæ¨¡å‹çš„å†³ç­–è¾¹ç•Œã€‚</li>
<li>åäº‹å®è§£é‡Šæ˜¯æŒ‡é‚£äº›èƒ½å¤Ÿæœ€å°åŒ–æ‰°åŠ¨å¹¶æ”¹å˜æ•™å¸ˆæ¨¡å‹é¢„æµ‹ç»“æœçš„è¾“å…¥ã€‚</li>
<li>ç†è®ºå’Œæ•°å­¦è¯æ˜æ”¯æŒäº†åäº‹å®è§£é‡Šåœ¨è’¸é¦ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§æ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼ŒCoDåœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹æ˜¾è‘—ä¼˜äºæ ‡å‡†è’¸é¦æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3cba2e15f5f8832690dcea176471b72" align="middle">
<img src="https://picx.zhimg.com/v2-17dee2bed7a14d088ac3d39ea762eb70" align="middle">
<img src="https://picx.zhimg.com/v2-efad1a8281fdd7d07c3302825cafa76a" align="middle">
<img src="https://picx.zhimg.com/v2-9ea715ccfa27a8ede3f91006f7a8c28f" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="An-Automatic-Detection-Method-for-Hematoma-Features-in-Placental-Abruption-Ultrasound-Images-Based-on-Few-Shot-Learning"><a href="#An-Automatic-Detection-Method-for-Hematoma-Features-in-Placental-Abruption-Ultrasound-Images-Based-on-Few-Shot-Learning" class="headerlink" title="An Automatic Detection Method for Hematoma Features in Placental   Abruption Ultrasound Images Based on Few-Shot Learning"></a>An Automatic Detection Method for Hematoma Features in Placental   Abruption Ultrasound Images Based on Few-Shot Learning</h2><p><strong>Authors:Xiaoqing Liu, Jitai Han, Hua Yan, Peng Li, Sida Tang, Ying Li, Kaiwen Zhang, Min Yu</strong></p>
<p>Placental abruption is a severe complication during pregnancy, and its early accurate diagnosis is crucial for ensuring maternal and fetal safety. Traditional ultrasound diagnostic methods heavily rely on physician experience, leading to issues such as subjective bias and diagnostic inconsistencies. This paper proposes an improved model, EH-YOLOv11n (Enhanced Hemorrhage-YOLOv11n), based on small-sample learning, aiming to achieve automatic detection of hematoma features in placental ultrasound images. The model enhances performance through multidimensional optimization: it integrates wavelet convolution and coordinate convolution to strengthen frequency and spatial feature extraction; incorporates a cascaded group attention mechanism to suppress ultrasound artifacts and occlusion interference, thereby improving bounding box localization accuracy. Experimental results demonstrate a detection accuracy of 78%, representing a 2.5% improvement over YOLOv11n and a 13.7% increase over YOLOv8. The model exhibits significant superiority in precision-recall curves, confidence scores, and occlusion scenarios. Combining high accuracy with real-time processing, this model provides a reliable solution for computer-aided diagnosis of placental abruption, holding significant clinical application value. </p>
<blockquote>
<p>èƒç›˜æ—©å‰¥æ˜¯å¦Šå¨ è¿‡ç¨‹ä¸­çš„ä¸€ç§ä¸¥é‡å¹¶å‘ç—‡ï¼Œæ—©æœŸå‡†ç¡®è¯Šæ–­å¯¹ç¡®ä¿æ¯å©´å®‰å…¨è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è¶…å£°è¯Šæ–­æ–¹æ³•ä¸¥é‡ä¾èµ–åŒ»ç”Ÿç»éªŒï¼Œå¯¼è‡´ä¸»è§‚åè§å’Œè¯Šæ–­ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚æœ¬æ–‡é’ˆå¯¹èƒç›˜è¶…å£°å›¾åƒä¸­çš„è¡€è‚¿ç‰¹å¾è‡ªåŠ¨æ£€æµ‹ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå°æ ·æœ¬å­¦ä¹ çš„æ”¹è¿›æ¨¡å‹EH-YOLOv11nï¼ˆå¢å¼ºå‡ºè¡€-YOLOv11nï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šç»´ä¼˜åŒ–æå‡æ€§èƒ½ï¼šå®ƒèåˆäº†å°æ³¢å·ç§¯å’Œåæ ‡å·ç§¯ï¼Œä»¥å¼ºåŒ–é¢‘ç‡å’Œç©ºé—´ç‰¹å¾æå–ï¼›å¼•å…¥çº§è”ç»„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŠ‘åˆ¶è¶…å£°ä¼ªå½±å’Œé®æŒ¡å¹²æ‰°ï¼Œä»è€Œæé«˜è¾¹ç•Œæ¡†å®šä½ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ£€æµ‹å‡†ç¡®ç‡ä¸º78%ï¼Œç›¸è¾ƒäºYOLOv11næé«˜2.5%ï¼Œç›¸è¾ƒäºYOLOv8æé«˜13.7%ã€‚è¯¥æ¨¡å‹åœ¨ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ã€ç½®ä¿¡åº¦å¾—åˆ†å’Œé®æŒ¡åœºæ™¯ç­‰æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚å…¼å…·é«˜å‡†ç¡®ç‡å’Œå®æ—¶å¤„ç†èƒ½åŠ›çš„è¯¥æ¨¡å‹ï¼Œä¸ºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­èƒç›˜æ—©å‰¥æä¾›äº†å¯é è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æ˜¾è‘—çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21495v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå°æ ·æœ¬å­¦ä¹ çš„æ”¹è¿›æ¨¡å‹EH-YOLOv11nï¼Œç”¨äºè‡ªåŠ¨æ£€æµ‹èƒç›˜è¶…å£°å›¾åƒä¸­çš„è¡€è‚¿ç‰¹å¾ï¼Œå®ç°èƒç›˜æ—©å‰¥çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šç»´åº¦ä¼˜åŒ–æé«˜æ€§èƒ½ï¼Œé›†æˆå°æ³¢å·ç§¯å’Œåæ ‡å·ç§¯ï¼ŒåŠ å¼ºé¢‘ç‡å’Œç©ºé—´ç‰¹å¾æå–ï¼›é‡‡ç”¨çº§è”ç»„æ³¨æ„æœºåˆ¶ï¼ŒæŠ‘åˆ¶è¶…å£°ä¼ªå½±å’Œé®æŒ¡å¹²æ‰°ï¼Œæé«˜è¾¹ç•Œæ¡†å®šä½ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ°äº†78%ï¼Œç›¸è¾ƒäºYOLOv11nå’ŒYOLOv8åˆ†åˆ«æé«˜äº†2.5%å’Œ13.7%ã€‚åœ¨ç²¾åº¦-å¬å›ç‡æ›²çº¿ã€ç½®ä¿¡åº¦å¾—åˆ†å’Œé®æŒ¡åœºæ™¯ä¸‹ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¼å…·é«˜å‡†ç¡®æ€§å’Œå®æ—¶å¤„ç†æ€§èƒ½ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒç›˜æ—©å‰¥çš„å‡†ç¡®æ—©æœŸè¯Šæ–­å¯¹ç¡®ä¿æ¯å©´å®‰å…¨è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿè¶…å£°è¯Šæ–­æ–¹æ³•å­˜åœ¨ä¸»è§‚åè§å’Œè¯Šæ–­ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>EH-YOLOv11næ¨¡å‹åŸºäºå°æ ·æœ¬å­¦ä¹ ï¼Œæ—¨åœ¨è‡ªåŠ¨æ£€æµ‹èƒç›˜è¶…å£°å›¾åƒä¸­çš„è¡€è‚¿ç‰¹å¾ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¤šç»´åº¦ä¼˜åŒ–æé«˜æ€§èƒ½ï¼ŒåŒ…æ‹¬é›†æˆå°æ³¢å·ç§¯å’Œåæ ‡å·ç§¯ï¼Œä»¥åŠé‡‡ç”¨çº§è”ç»„æ³¨æ„æœºåˆ¶ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒEH-YOLOv11næ¨¡å‹çš„æ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ°äº†78%ï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>EH-YOLOv11næ¨¡å‹åœ¨ç²¾åº¦-å¬å›ç‡æ›²çº¿ã€ç½®ä¿¡åº¦å¾—åˆ†å’Œé®æŒ¡åœºæ™¯ä¸‹è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75d9b30bced417568a418dca91077ecc" align="middle">
<img src="https://picx.zhimg.com/v2-bf9725757cd57f76076382b90c704d0e" align="middle">
<img src="https://picx.zhimg.com/v2-95be76fe08f3c13300048cc1a380a310" align="middle">
<img src="https://picx.zhimg.com/v2-4386265e4e18ca57a644a52c80121235" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Parameter-Free-Hypergraph-Neural-Network-for-Few-Shot-Node-Classification"><a href="#Parameter-Free-Hypergraph-Neural-Network-for-Few-Shot-Node-Classification" class="headerlink" title="Parameter-Free Hypergraph Neural Network for Few-Shot Node   Classification"></a>Parameter-Free Hypergraph Neural Network for Few-Shot Node   Classification</h2><p><strong>Authors:Chaewoon Bae, Doyun Choi, Jaehyun Lee, Jaemin Yoo</strong></p>
<p>Few-shot node classification on hypergraphs requires models that generalize from scarce labels while capturing high-order structures. Existing hypergraph neural networks (HNNs) effectively encode such structures but often suffer from overfitting and scalability issues due to complex, black-box architectures. In this work, we propose ZEN (Zero-Parameter Hypergraph Neural Network), a fully linear and parameter-free model that achieves both expressiveness and efficiency. Built upon a unified formulation of linearized HNNs, ZEN introduces a tractable closed-form solution for the weight matrix and a redundancy-aware propagation scheme to avoid iterative training and to eliminate redundant self information. On 11 real-world hypergraph benchmarks, ZEN consistently outperforms eight baseline models in classification accuracy while achieving up to 696x speedups over the fastest competitor. Moreover, the decision process of ZEN is fully interpretable, providing insights into the characteristic of a dataset. Our code and datasets are fully available at <a target="_blank" rel="noopener" href="https://github.com/chaewoonbae/ZEN">https://github.com/chaewoonbae/ZEN</a>. </p>
<blockquote>
<p>åœ¨è¶…å›¾ä¸Šè¿›è¡Œå°æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»éœ€è¦èƒ½å¤Ÿä»ç¨€ç¼ºæ ‡ç­¾ä¸­æ¦‚æ‹¬åŒæ—¶æ•è·é«˜é˜¶ç»“æ„çš„æ¨¡å‹ã€‚ç°æœ‰çš„è¶…å›¾ç¥ç»ç½‘ç»œï¼ˆHNNï¼‰å¯ä»¥æœ‰æ•ˆåœ°ç¼–ç æ­¤ç±»ç»“æ„ï¼Œä½†ç”±äºå¤æ‚çš„é»‘åŒ£å­æ¶æ„ï¼Œå¸¸å¸¸é¢ä¸´è¿‡æ‹Ÿåˆå’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ZENï¼ˆé›¶å‚æ•°è¶…å›¾ç¥ç»ç½‘ç»œï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨çº¿æ€§ä¸”æ— å‚æ•°çš„æ¨¡å‹ï¼Œå®ç°äº†è¡¨è¾¾æ€§å’Œæ•ˆç‡ã€‚ZENå»ºç«‹åœ¨çº¿æ€§åŒ–HNNsçš„ç»Ÿä¸€å…¬å¼ä¹‹ä¸Šï¼Œå¼•å…¥äº†ä¸€ç§å¯è¡Œçš„é—­å¼è§£æƒé‡çŸ©é˜µå’Œä¸€ç§é¿å…å†—ä½™ä¿¡æ¯çš„ä¼ æ’­æ–¹æ¡ˆï¼Œä»¥é¿å…è¿­ä»£è®­ç»ƒå’Œæ¶ˆé™¤å†—ä½™çš„è‡ªèº«ä¿¡æ¯ã€‚åœ¨11ä¸ªçœŸå®ä¸–ç•Œçš„è¶…å›¾åŸºå‡†æµ‹è¯•ä¸­ï¼ŒZENåœ¨åˆ†ç±»ç²¾åº¦ä¸Šå§‹ç»ˆä¼˜äº8ä¸ªåŸºå‡†æ¨¡å‹ï¼ŒåŒæ—¶åœ¨é€Ÿåº¦ä¸Šå®ç°äº†æœ€å¿«ç«äº‰å¯¹æ‰‹é«˜è¾¾696å€çš„æå‡ã€‚æ­¤å¤–ï¼ŒZENçš„å†³ç­–è¿‡ç¨‹æ˜¯å®Œå…¨å¯è§£é‡Šçš„ï¼Œä¸ºæ•°æ®é›†çš„ç‰¹å¾æä¾›äº†æ·±å…¥è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chaewoonbae/ZEN%E5%AE%8C%E5%85%A8%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/chaewoonbae/ZENå®Œå…¨è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç®€æ´æœ‰æ•ˆçš„è¶…å›¾ç¥ç»ç½‘ç»œæ¨¡å‹â€”â€”ZENã€‚è¯¥æ¨¡å‹å®ç°äº†çº¿æ€§åŒ–ä¸”æ— éœ€å‚æ•°ï¼Œé€šè¿‡å¼•å…¥æƒé‡çŸ©é˜µçš„å°é—­è§£å’Œå†—ä½™æ„ŸçŸ¥ä¼ æ’­æ–¹æ¡ˆï¼Œè§£å†³äº†ç°æœ‰è¶…å›¾ç¥ç»ç½‘ç»œï¼ˆHNNsï¼‰é¢ä¸´çš„è¿‡æ‹Ÿåˆå’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚ZENåœ¨åˆ†ç±»å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šå…«ä¸ªåŸºå‡†æ¨¡å‹ï¼Œä¸”å¤„ç†é€Ÿåº¦æœ€å¿«å¯è¾¾åˆ°ç°æœ‰æœ€ä½³æ¨¡å‹çš„696å€ã€‚å…¶å†³ç­–è¿‡ç¨‹å…·æœ‰å®Œå…¨çš„å¯è§£é‡Šæ€§ï¼Œæœ‰åŠ©äºæ·±å…¥äº†è§£æ•°æ®é›†çš„ç‰¹æ€§ã€‚æ•°æ®é›†ä¸ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZENæ˜¯ä¸€ä¸ªæ— éœ€å‚æ•°çš„è¶…å›¾ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå®ç°äº†çº¿æ€§åŒ–ï¼Œå…¼å…·è¡¨è¾¾åŠ›å’Œæ•ˆç‡ã€‚</li>
<li>ZENè§£å†³äº†ç°æœ‰è¶…å›¾ç¥ç»ç½‘ç»œé¢ä¸´çš„è¿‡æ‹Ÿåˆå’Œå¯æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>åœ¨11ä¸ªçœŸå®ä¸–ç•Œçš„è¶…å›¾åŸºå‡†æµ‹è¯•ä¸­ï¼ŒZENçš„åˆ†ç±»å‡†ç¡®æ€§è¶…è¶Šäº†å…«ä¸ªåŸºå‡†æ¨¡å‹ã€‚</li>
<li>ZENå¤„ç†é€Ÿåº¦æœ€å¿«å¯è¾¾åˆ°ç°æœ‰æœ€ä½³æ¨¡å‹çš„696å€ã€‚</li>
<li>ZENçš„å†³ç­–è¿‡ç¨‹å…·æœ‰å®Œå…¨çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ZENé€šè¿‡å¼•å…¥æƒé‡çŸ©é˜µçš„å°é—­è§£å’Œå†—ä½™æ„ŸçŸ¥ä¼ æ’­æ–¹æ¡ˆï¼Œä¼˜åŒ–äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85547c8a5b2e74b95e7668d073d0c69f" align="middle">
<img src="https://picx.zhimg.com/v2-7eae1c5de3d8cd1bb646038eef4efa8a" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="M-GLC-Motif-Driven-Global-Local-Context-Graphs-for-Few-shot-Molecular-Property-Prediction"><a href="#M-GLC-Motif-Driven-Global-Local-Context-Graphs-for-Few-shot-Molecular-Property-Prediction" class="headerlink" title="M-GLC: Motif-Driven Global-Local Context Graphs for Few-shot Molecular   Property Prediction"></a>M-GLC: Motif-Driven Global-Local Context Graphs for Few-shot Molecular   Property Prediction</h2><p><strong>Authors:Xiangyang Xu, Hongyang Gao</strong></p>
<p>Molecular property prediction (MPP) is a cornerstone of drug discovery and materials science, yet conventional deep learning approaches depend on large labeled datasets that are often unavailable. Few-shot Molecular property prediction (FSMPP) addresses this scarcity by incorporating relational inductive bias through a context graph that links molecule nodes to property nodes, but such molecule-property graphs offer limited structural guidance. We propose a comprehensive solution: Motif Driven Global-Local Context Graph for few-shot molecular property prediction, which enriches contextual information at both the global and local levels. At the global level, chemically meaningful motif nodes representing shared substructures, such as rings or functional groups, are introduced to form a global tri-partite heterogeneous graph, yielding motif-molecule-property connections that capture long-range compositional patterns and enable knowledge transfer among molecules with common motifs. At the local level, we build a subgraph for each node in the molecule-property pair and encode them separately to concentrate the modelâ€™s attention on the most informative neighboring molecules and motifs. Experiments on five standard FSMPP benchmarks demonstrate that our framework consistently outperforms state-of-the-art methods. These results underscore the effectiveness of integrating global motif knowledge with fine-grained local context to advance robust few-shot molecular property prediction. </p>
<blockquote>
<p>åˆ†å­å±æ€§é¢„æµ‹ï¼ˆMPPï¼‰æ˜¯è¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç„¶è€Œä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ä¾èµ–äºå¤§é‡å¯ç”¨çš„æ ‡è®°æ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®é€šå¸¸å¹¶ä¸å¯ç”¨ã€‚å°æ ·æœ¬åˆ†å­å±æ€§é¢„æµ‹ï¼ˆFSMPPï¼‰é€šè¿‡ç»“åˆå…³ç³»å½’çº³åè§æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥åè§é€šè¿‡ä¸Šä¸‹æ–‡å›¾é“¾æ¥åˆ†å­èŠ‚ç‚¹å’Œå±æ€§èŠ‚ç‚¹ï¼Œä½†è¿™ç§åˆ†å­-å±æ€§å›¾æä¾›çš„ç»“æ„æŒ‡å¯¼æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è§£å†³æ–¹æ¡ˆï¼šé’ˆå¯¹å°æ ·æœ¬åˆ†å­å±æ€§é¢„æµ‹çš„åŠ¨æœºé©±åŠ¨å…¨å±€-å±€éƒ¨ä¸Šä¸‹æ–‡å›¾ï¼Œå®ƒåŒæ—¶ä¸°å¯Œå…¨å±€å’Œå±€éƒ¨çº§åˆ«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨å…¨å±€å±‚é¢ï¼Œå¼•å…¥ä»£è¡¨å…±äº«å­ç»“æ„çš„åŒ–å­¦æ„ä¹‰åŸºå…ƒèŠ‚ç‚¹ï¼ˆå¦‚ç¯æˆ–å®˜èƒ½å›¢ï¼‰ï¼Œå½¢æˆå…¨å±€ä¸‰æ–¹å¼‚è´¨å›¾ï¼Œäº§ç”ŸåŸºå…ƒ-åˆ†å­-å±æ€§è¿æ¥ï¼Œæ•è·é•¿ç¨‹ç»„åˆæ¨¡å¼ï¼Œå¹¶åœ¨å…·æœ‰å…±åŒåŸºå…ƒçš„åˆ†å­ä¹‹é—´è¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚åœ¨å±€éƒ¨å±‚é¢ï¼Œæˆ‘ä»¬ä¸ºåˆ†å­-å±æ€§å¯¹ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æ„å»ºå­å›¾å¹¶è¿›è¡Œå•ç‹¬ç¼–ç ï¼Œä½¿æ¨¡å‹å…³æ³¨æœ€å…·æœ‰ä¿¡æ¯é‡çš„é‚»è¿‘åˆ†å­å’ŒåŸºå…ƒã€‚åœ¨äº”ä¸ªæ ‡å‡†FSMPPåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å°†å…¨å±€åŸºå…ƒçŸ¥è¯†ä¸ç²¾ç»†çš„å±€éƒ¨ä¸Šä¸‹æ–‡ç›¸ç»“åˆï¼Œä»¥æ¨åŠ¨ç¨³å¥çš„å°æ ·æœ¬åˆ†å­å±æ€§é¢„æµ‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21088v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åˆ†å­å±æ€§é¢„æµ‹çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„é—®é¢˜ã€‚é€šè¿‡æ„å»ºå…¨å±€å’Œå±€éƒ¨çš„ä¸Šä¸‹æ–‡å›¾ï¼Œå¼•å…¥åŒ–å­¦æ„ä¹‰ä¸Šçš„åŸºå…ƒèŠ‚ç‚¹ï¼Œå½¢æˆå…¨å±€ä¸‰å…ƒå¼‚æ„å›¾ï¼Œå®ç°é•¿ç¨‹ç»„åˆæ¨¡å¼çš„æ•æ‰å’Œåˆ†å­é—´çŸ¥è¯†çš„è½¬ç§»ã€‚åŒæ—¶ï¼Œä¸ºæ¯ä¸ªåˆ†å­å±æ€§å¯¹èŠ‚ç‚¹æ„å»ºå­å›¾ï¼Œåˆ†åˆ«ç¼–ç ï¼Œä½¿æ¨¡å‹å…³æ³¨æœ€å…·æœ‰ä¿¡æ¯é‡çš„é‚»è¿‘åˆ†å­å’ŒåŸºå…ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªæ ‡å‡†å°‘æ ·æœ¬åˆ†å­å±æ€§é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†å­å±æ€§é¢„æµ‹ï¼ˆMPPï¼‰æ˜¯è¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦çš„æ ¸å¿ƒï¼Œä½†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™åœ¨ç°å®ä¸­å¸¸å¸¸æ— æ³•è·å¾—ã€‚</li>
<li>å°‘æ ·æœ¬åˆ†å­å±æ€§é¢„æµ‹ï¼ˆFSMPPï¼‰é€šè¿‡å…³ç³»å½’çº³åè§è§£å†³äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡å›¾è¿æ¥åˆ†å­èŠ‚ç‚¹å’Œå±æ€§èŠ‚ç‚¹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ–¹æ³•â€”â€”åŸºäºåŸºå…ƒé©±åŠ¨çš„å…¨å±€-å±€éƒ¨ä¸Šä¸‹æ–‡å›¾ï¼Œä»¥ä¸°å¯Œå…¨å±€å’Œå±€éƒ¨å±‚é¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åœ¨å…¨å±€å±‚é¢ï¼Œå¼•å…¥åŒ–å­¦æ„ä¹‰ä¸Šçš„åŸºå…ƒèŠ‚ç‚¹ï¼Œå½¢æˆå…¨å±€ä¸‰å…ƒå¼‚æ„å›¾ï¼Œæ•æ‰é•¿ç¨‹ç»„åˆæ¨¡å¼ï¼Œå®ç°åˆ†å­é—´çŸ¥è¯†è½¬ç§»ã€‚</li>
<li>åœ¨å±€éƒ¨å±‚é¢ï¼Œä¸ºæ¯ä¸ªåˆ†å­å±æ€§å¯¹èŠ‚ç‚¹æ„å»ºå­å›¾å¹¶åˆ†åˆ«ç¼–ç ï¼Œä½¿æ¨¡å‹å…³æ³¨æœ€ç›¸å…³çš„é‚»è¿‘åˆ†å­å’ŒåŸºå…ƒã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†å°‘æ ·æœ¬åˆ†å­å±æ€§é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21fc198800cfbc03c76359fb44533ff6" align="middle">
<img src="https://picx.zhimg.com/v2-4cb9a16a05c1e2fe8edb2aec3be532cb" align="middle">
<img src="https://picx.zhimg.com/v2-ff1b56c69e922fc63c82e4986f9b906b" align="middle">
<img src="https://picx.zhimg.com/v2-f1beac7d4e242f99fb471cdef05210f3" align="middle">
<img src="https://picx.zhimg.com/v2-d2fd20f0ae55a62189fcfff7155389c3" align="middle">
<img src="https://picx.zhimg.com/v2-56cc0f7b0866e103cbb58ee37b32c137" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="On-the-Fly-OVD-Adaptation-with-FLAME-Few-shot-Localization-via-Active-Marginal-Samples-Exploration"><a href="#On-the-Fly-OVD-Adaptation-with-FLAME-Few-shot-Localization-via-Active-Marginal-Samples-Exploration" class="headerlink" title="On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active   Marginal-Samples Exploration"></a>On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active   Marginal-Samples Exploration</h2><p><strong>Authors:Yehonathan Refael, Amit Aides, Aviad Barzilai, George Leifman, Genady Beryozkin, Vered Silverman, Bolous Jaber, Tomer Shekel</strong></p>
<p>Open-vocabulary object detection (OVD) models offer remarkable flexibility by detecting objects from arbitrary text queries. However, their zero-shot performance in specialized domains like Remote Sensing (RS) is often compromised by the inherent ambiguity of natural language, limiting critical downstream applications. For instance, an OVD model may struggle to distinguish between fine-grained classes such as â€œfishing boatâ€ and â€œyachtâ€ since their embeddings are similar and often inseparable. This can hamper specific user goals, such as monitoring illegal fishing, by producing irrelevant detections. To address this, we propose a cascaded approach that couples the broad generalization of a large pre-trained OVD model with a lightweight few-shot classifier. Our method first employs the zero-shot model to generate high-recall object proposals. These proposals are then refined for high precision by a compact classifier trained in real-time on only a handful of user-annotated examples - drastically reducing the high costs of RS imagery annotation.The core of our framework is FLAME, a one-step active learning strategy that selects the most informative samples for training. FLAME identifies, on the fly, uncertain marginal candidates near the decision boundary using density estimation, followed by clustering to ensure sample diversity. This efficient sampling technique achieves high accuracy without costly full-model fine-tuning and enables instant adaptation, within less then a minute, which is significantly faster than state-of-the-art alternatives.Our method consistently surpasses state-of-the-art performance on RS benchmarks, establishing a practical and resource-efficient framework for adapting foundation models to specific user needs. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰æ¨¡å‹é€šè¿‡ä»ä»»æ„æ–‡æœ¬æŸ¥è¯¢ä¸­æ£€æµ‹å¯¹è±¡æä¾›äº†æ˜¾è‘—çš„çµæ´»æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é¥æ„Ÿï¼ˆRSï¼‰ç­‰ç‰¹å®šé¢†åŸŸçš„é›¶æ ·æœ¬æ€§èƒ½é€šå¸¸å—åˆ°è‡ªç„¶è¯­è¨€å›ºæœ‰æ¨¡ç³Šæ€§çš„é™åˆ¶ï¼Œä»è€Œå½±å“å…³é”®ä¸‹æ¸¸åº”ç”¨ã€‚ä¾‹å¦‚ï¼ŒOVDæ¨¡å‹å¯èƒ½éš¾ä»¥åŒºåˆ†ç»†ç²’åº¦ç±»åˆ«ï¼Œå¦‚â€œæ¸”èˆ¹â€å’Œâ€œæ¸¸è‰‡â€ï¼Œå› ä¸ºå®ƒä»¬çš„åµŒå…¥ç›¸ä¼¼ä¸”é€šå¸¸æ— æ³•åŒºåˆ†ã€‚è¿™å¯èƒ½ä¼šé˜»ç¢ç‰¹å®šçš„ç”¨æˆ·ç›®æ ‡ï¼Œä¾‹å¦‚ç›‘æµ‹éæ³•æ•é±¼ï¼Œå› ä¸ºä¼šäº§ç”Ÿä¸ç›¸å…³çš„æ£€æµ‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çº§è”æ–¹æ³•ï¼Œå°†å¤§å‹é¢„è®­ç»ƒOVDæ¨¡å‹çš„å¹¿æ³›æ³›åŒ–ä¸è½»é‡çº§å°æ ·æœ¬åˆ†ç±»å™¨ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨é›¶æ ·æœ¬æ¨¡å‹ç”Ÿæˆé«˜å¬å›ç‡çš„å¯¹è±¡ææ¡ˆã€‚ç„¶åï¼Œä½¿ç”¨ä»…ä½¿ç”¨å°‘é‡ç”¨æˆ·æ³¨é‡Šå®ä¾‹å®æ—¶è®­ç»ƒçš„ç´§å‡‘åˆ†ç±»å™¨å¯¹è¿™äº›ææ¡ˆè¿›è¡Œç²¾ç‚¼ï¼Œä»¥æé«˜ç²¾åº¦â€”â€”å¤§å¤§é™ä½äº†é¥æ„Ÿå›¾åƒæ³¨é‡Šçš„é«˜æˆæœ¬ã€‚æˆ‘ä»¬æ¡†æ¶çš„æ ¸å¿ƒæ˜¯FLAMEï¼Œè¿™æ˜¯ä¸€ç§ä¸€æ­¥å¼ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œå¯é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚FLAMEå³æ—¶è¯†åˆ«å†³ç­–è¾¹ç•Œé™„è¿‘çš„ä¸ç¡®å®šæ€§è¾¹ç¼˜å€™é€‰è€…ï¼Œå¹¶ä½¿ç”¨å¯†åº¦ä¼°è®¡è¿›è¡Œèšç±»ï¼Œä»¥ç¡®ä¿æ ·æœ¬å¤šæ ·æ€§ã€‚è¿™ç§é«˜æ•ˆçš„é‡‡æ ·æŠ€æœ¯æ— éœ€æ˜‚è´µçš„å…¨æ¨¡å‹å¾®è°ƒå³å¯å®ç°é«˜å‡†ç¡®æ€§ï¼Œå¹¶èƒ½åœ¨ä¸åˆ°ä¸€åˆ†é’Ÿçš„æ—¶é—´å†…å®ç°å³æ—¶é€‚åº”ï¼Œè¿™æ˜æ˜¾å¿«äºæœ€å…ˆè¿›çš„æ›¿ä»£å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨é¥æ„ŸåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å§‹ç»ˆè¶…è¿‡æœ€æ–°æŠ€æœ¯ï¼Œä¸ºå°†åŸºç¡€æ¨¡å‹é€‚åº”ç‰¹å®šç”¨æˆ·éœ€æ±‚å»ºç«‹äº†ä¸€ä¸ªå®ç”¨ä¸”èµ„æºé«˜æ•ˆçš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17670v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰æ¨¡å‹åœ¨é¥æ„Ÿï¼ˆRSï¼‰é¢†åŸŸé›¶æ ·æœ¬æ€§èƒ½ä¸ä½³çš„é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ç»“åˆå¤§è§„æ¨¡é¢„è®­ç»ƒOVDæ¨¡å‹çš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ä¸è½»é‡çº§çš„å°æ ·æœ¬åˆ†ç±»å™¨ï¼Œè¯¥æ–¹æ¡ˆé¦–å…ˆåˆ©ç”¨é›¶æ ·æœ¬æ¨¡å‹ç”Ÿæˆé«˜å¬å›ç‡çš„å¯¹è±¡ææ¡ˆï¼Œç„¶åé€šè¿‡å®æ—¶è®­ç»ƒå°‘é‡ç”¨æˆ·æ ‡æ³¨çš„æ ·æœ¬å¯¹ææ¡ˆè¿›è¡Œç²¾ç‚¼ï¼Œä»¥æé«˜ç²¾åº¦ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€æ­¥å¼ä¸»åŠ¨å­¦ä¹ ç­–ç•¥FLAMEï¼Œå®ƒèƒ½é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡å¯†åº¦ä¼°è®¡å’Œèšç±»æŠ€æœ¯å¿«é€Ÿé€‚åº”å†³ç­–è¾¹ç•Œçš„ä¸ç¡®å®šæ€§è¾¹é™…å€™é€‰ï¼Œå®ç°é«˜æ•ˆé‡‡æ ·ï¼Œä¸”æ— éœ€æ˜‚è´µçš„å…¨æ¨¡å‹å¾®è°ƒï¼Œèƒ½åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆé€‚åº”ã€‚æ­¤æ–¹æ¡ˆåœ¨é¥æ„ŸåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œä¸ºå°†åŸºç¡€æ¨¡å‹é€‚åº”ç‰¹å®šç”¨æˆ·éœ€æ±‚å»ºç«‹äº†ä¸€ä¸ªå®ç”¨ä¸”èµ„æºé«˜æ•ˆæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼ˆOVDï¼‰æ¨¡å‹èƒ½çµæ´»åœ°é€šè¿‡æ–‡æœ¬æŸ¥è¯¢æ£€æµ‹å¯¹è±¡ï¼Œä½†åœ¨é¥æ„Ÿï¼ˆRSï¼‰é¢†åŸŸçš„é›¶æ ·æœ¬æ€§èƒ½å—é™ã€‚</li>
<li>æå‡ºçš„æ–¹æ¡ˆç»“åˆäº†é¢„è®­ç»ƒOVDæ¨¡å‹çš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ä¸å°æ ·æœ¬åˆ†ç±»å™¨ï¼Œæ—¨åœ¨æé«˜åœ¨é¥æ„Ÿå›¾åƒä¸­çš„å¯¹è±¡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨é›¶æ ·æœ¬æ¨¡å‹ç”Ÿæˆé«˜å¬å›ç‡çš„å¯¹è±¡ææ¡ˆï¼Œç„¶åé€šè¿‡å®æ—¶è®­ç»ƒçš„å°‘é‡ç”¨æˆ·æ ‡æ³¨æ ·æœ¬æé«˜ç²¾åº¦ã€‚</li>
<li>æ¡†æ¶æ ¸å¿ƒä¸ºä¸€æ­¥å¼ä¸»åŠ¨å­¦ä¹ ç­–ç•¥FLAMEï¼Œèƒ½é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå®ç°é«˜æ•ˆé‡‡æ ·ä¸”æ— éœ€å…¨æ¨¡å‹å¾®è°ƒã€‚</li>
<li>FLAMEé€šè¿‡å¯†åº¦ä¼°è®¡å’Œèšç±»æŠ€æœ¯é€‚åº”å†³ç­–è¾¹ç•Œçš„ä¸ç¡®å®šæ€§è¾¹é™…å€™é€‰ã€‚</li>
<li>è¯¥æ–¹æ¡ˆåœ¨é¥æ„ŸåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d63e82e2114bb31da71684a4819b8362" align="middle">
<img src="https://picx.zhimg.com/v2-bf81bf38ec33de064391dab2da18ad4e" align="middle">
<img src="https://picx.zhimg.com/v2-085c8e89d54a6ef7216cea1f0b97bdbd" align="middle">
<img src="https://picx.zhimg.com/v2-4dad567964f24d52877ca03289e62078" align="middle">
<img src="https://picx.zhimg.com/v2-016b21d865fc4cf8a106a2a7d2399415" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Preference-driven-Knowledge-Distillation-for-Few-shot-Node-Classification"><a href="#Preference-driven-Knowledge-Distillation-for-Few-shot-Node-Classification" class="headerlink" title="Preference-driven Knowledge Distillation for Few-shot Node   Classification"></a>Preference-driven Knowledge Distillation for Few-shot Node   Classification</h2><p><strong>Authors:Xing Wei, Chunchun Chen, Rui Fan, Xiaofeng Cao, Sourav Medya, Wei Ye</strong></p>
<p>Graph neural networks (GNNs) can efficiently process text-attributed graphs (TAGs) due to their message-passing mechanisms, but their training heavily relies on the human-annotated labels. Moreover, the complex and diverse local topologies of nodes of real-world TAGs make it challenging for a single mechanism to handle. Large language models (LLMs) perform well in zero-&#x2F;few-shot learning on TAGs but suffer from a scalability challenge. Therefore, we propose a preference-driven knowledge distillation (PKD) framework to synergize the complementary strengths of LLMs and various GNNs for few-shot node classification. Specifically, we develop a GNN-preference-driven node selector that effectively promotes prediction distillation from LLMs to teacher GNNs. To further tackle nodesâ€™ intricate local topologies, we develop a node-preference-driven GNN selector that identifies the most suitable teacher GNN for each node, thereby facilitating tailored knowledge distillation from teacher GNNs to the student GNN. Extensive experiments validate the efficacy of our proposed framework in few-shot node classification on real-world TAGs. Our code is be available. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ç”±äºå…¶ä¿¡æ¯ä¼ é€’æœºåˆ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬çš„è®­ç»ƒä¸¥é‡ä¾èµ–äºäººå·¥æ ‡æ³¨çš„æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œç°å®ä¸–ç•Œä¸­çš„TAGèŠ‚ç‚¹çš„å¤æ‚å’Œå¤šæ ·çš„å±€éƒ¨æ‹“æ‰‘ç»“æ„ä½¿å¾—å•ä¸€æœºåˆ¶å¤„ç†èµ·æ¥å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨TAGçš„é›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†é¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åå¥½é©±åŠ¨çš„çŸ¥è¯†è’¸é¦ï¼ˆPKDï¼‰æ¡†æ¶ï¼Œä»¥ååŒLLMså’Œå„ç§GNNsçš„äº’è¡¥ä¼˜åŠ¿ï¼Œç”¨äºå°‘æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§GNNåå¥½é©±åŠ¨èŠ‚ç‚¹é€‰æ‹©å™¨ï¼Œæœ‰æ•ˆåœ°ä¿ƒè¿›äº†ä»LLMsåˆ°æ•™å¸ˆGNNsçš„é¢„æµ‹è’¸é¦ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³èŠ‚ç‚¹çš„å¤æ‚å±€éƒ¨æ‹“æ‰‘ç»“æ„é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†èŠ‚ç‚¹åå¥½é©±åŠ¨GNNé€‰æ‹©å™¨ï¼Œå®ƒä¸ºæ¯ä¸ªèŠ‚ç‚¹ç¡®å®šæœ€åˆé€‚çš„æ•™å¸ˆGNNï¼Œä»è€Œä¿ƒè¿›äº†ä»æ•™å¸ˆGNNåˆ°å­¦ç”ŸGNNçš„å®šåˆ¶çŸ¥è¯†è’¸é¦ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„TAGå°‘æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»ä¸­æå‡ºçš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10116v3">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨å¤„ç†æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰æ—¶çš„ä¼˜åŠ¿ï¼Œç»“åˆå…¶ä¿¡æ¯ä¼ é€’æœºåˆ¶ï¼Œä½†è®­ç»ƒè¿‡ç¨‹é«˜åº¦ä¾èµ–äººå·¥æ ‡æ³¨æ ‡ç­¾ã€‚é’ˆå¯¹çœŸå®ä¸–ç•ŒTAGsèŠ‚ç‚¹å¤æ‚å¤šå˜çš„å±€éƒ¨æ‹“æ‰‘ç»“æ„ï¼Œå•ä¸€æœºåˆ¶éš¾ä»¥åº”å¯¹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ å¯¹TAGsè¡¨ç°è‰¯å¥½ï¼Œä½†é¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§åå¥½é©±åŠ¨çš„çŸ¥è¯†è’¸é¦ï¼ˆPKDï¼‰æ¡†æ¶ï¼ŒèåˆLLMså’Œå„ç§GNNsçš„äº’è¡¥ä¼˜åŠ¿ï¼Œç”¨äºå°‘æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬GNNåå¥½é©±åŠ¨èŠ‚ç‚¹é€‰æ‹©å™¨å’ŒèŠ‚ç‚¹åå¥½é©±åŠ¨GNNé€‰æ‹©å™¨ï¼Œæœ‰æ•ˆä¿ƒè¿›ä»LLMsåˆ°æ•™å¸ˆGNNsçš„é¢„æµ‹è’¸é¦ï¼Œå¹¶è§£å†³äº†èŠ‚ç‚¹çš„å¤æ‚å±€éƒ¨æ‹“æ‰‘é—®é¢˜ã€‚åœ¨çœŸå®ä¸–ç•ŒTAGsä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨å°‘æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰èƒ½é«˜æ•ˆå¤„ç†æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ï¼Œå¾—ç›Šäºå…¶æ¶ˆæ¯ä¼ é€’æœºåˆ¶ã€‚</li>
<li>GNNsçš„è®­ç»ƒä¸¥é‡ä¾èµ–äººå·¥æ ‡æ³¨æ ‡ç­¾ã€‚</li>
<li>çœŸå®ä¸–ç•Œçš„TAGsèŠ‚ç‚¹å…·æœ‰å¤æ‚å’Œå¤šæ ·çš„å±€éƒ¨æ‹“æ‰‘ç»“æ„ï¼Œä½¿å¾—å•ä¸€æœºåˆ¶åº”å¯¹å›°éš¾ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ å¯¹TAGsè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œä½†å­˜åœ¨å¯æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åå¥½é©±åŠ¨çš„çŸ¥è¯†è’¸é¦ï¼ˆPKDï¼‰æ¡†æ¶ï¼Œèåˆäº†LLMså’ŒGNNsçš„ä¼˜åŠ¿ï¼Œç”¨äºå°‘æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»ã€‚</li>
<li>PKDæ¡†æ¶åŒ…æ‹¬GNNåå¥½é©±åŠ¨èŠ‚ç‚¹é€‰æ‹©å™¨å’ŒèŠ‚ç‚¹åå¥½é©±åŠ¨GNNé€‰æ‹©å™¨ï¼Œåˆ†åˆ«ä¿ƒè¿›é¢„æµ‹è’¸é¦å’Œé’ˆå¯¹èŠ‚ç‚¹å¤æ‚å±€éƒ¨æ‹“æ‰‘çš„å®šåˆ¶çŸ¥è¯†è’¸é¦ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•ŒTAGsä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨å°‘æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a969fddca64f381e2a7d2ad9c8a13b4f" align="middle">
<img src="https://picx.zhimg.com/v2-6a70496d6c324451735f13e5ae71c414" align="middle">
<img src="https://picx.zhimg.com/v2-b98ce31523af5b7a0dd04c708c7f7f9b" align="middle">
<img src="https://picx.zhimg.com/v2-ff27140dd0281f45e132d10701cc964f" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning"><a href="#SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning" class="headerlink" title="SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic   Representation Learning"></a>SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic   Representation Learning</h2><p><strong>Authors:Weijian Mai, Jiamin Wu, Yu Zhu, Zhouheng Yao, Dongzhan Zhou, Andrew F. Luo, Qihao Zheng, Wanli Ouyang, Chunfeng Song</strong></p>
<p>Deciphering how visual stimuli are transformed into cortical responses is a fundamental challenge in computational neuroscience. This visual-to-neural mapping is inherently a one-to-many relationship, as identical visual inputs reliably evoke variable hemodynamic responses across trials, contexts, and subjects. However, existing deterministic methods struggle to simultaneously model this biological variability while capturing the underlying functional consistency that encodes stimulus information. To address these limitations, we propose SynBrain, a generative framework that simulates the transformation from visual semantics to neural responses in a probabilistic and biologically interpretable manner. SynBrain introduces two key components: (i) BrainVAE models neural representations as continuous probability distributions via probabilistic learning while maintaining functional consistency through visual semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic transmission pathway, projecting visual semantics into the neural response manifold to facilitate high-fidelity fMRI synthesis. Experimental results demonstrate that SynBrain surpasses state-of-the-art methods in subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain adapts efficiently to new subjects with few-shot data and synthesizes high-quality fMRI signals that are effective in improving data-limited fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional consistency across trials and subjects, with synthesized signals capturing interpretable patterns shaped by biological neural variability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MichaelMaiii/SynBrain">https://github.com/MichaelMaiii/SynBrain</a>. </p>
<blockquote>
<p>è§£æè§†è§‰åˆºæ¿€å¦‚ä½•è½¬åŒ–ä¸ºçš®å±‚ååº”æ˜¯è®¡ç®—ç¥ç»ç§‘å­¦ä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ã€‚è¿™ç§è§†è§‰åˆ°ç¥ç»çš„æ˜ å°„æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ä¸€å¯¹å¤šçš„å…³ç³»ï¼Œå› ä¸ºç›¸åŒçš„è§†è§‰è¾“å…¥åœ¨è¯•éªŒã€ä¸Šä¸‹æ–‡å’Œå—è¯•è€…ä¹‹é—´å¯é åœ°å¼•å‘äº†å¯å˜çš„è¡€æµåŠ¨åŠ›å­¦ååº”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¡®å®šæ€§æ–¹æ³•å¾ˆéš¾åŒæ—¶æ¨¡æ‹Ÿè¿™ç§ç”Ÿç‰©å˜å¼‚ï¼ŒåŒæ—¶æ•æ‰ç¼–ç åˆºæ¿€ä¿¡æ¯çš„æ½œåœ¨åŠŸèƒ½ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SynBrainï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆæ€§æ¡†æ¶ï¼Œä»¥æ¦‚ç‡å’Œç”Ÿç‰©å­¦ä¸Šå¯è§£é‡Šçš„æ–¹å¼æ¨¡æ‹Ÿä»è§†è§‰è¯­ä¹‰åˆ°ç¥ç»ååº”çš„è½¬åŒ–ã€‚SynBrainæœ‰ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆiï¼‰BrainVAEé€šè¿‡æ¦‚ç‡å­¦ä¹ å°†ç¥ç»è¡¨å¾å»ºæ¨¡ä¸ºè¿ç»­æ¦‚ç‡åˆ†å¸ƒï¼ŒåŒæ—¶é€šè¿‡è§†è§‰è¯­ä¹‰çº¦æŸç»´æŒåŠŸèƒ½ä¸€è‡´æ€§ï¼›ï¼ˆiiï¼‰è¯­ä¹‰åˆ°ç¥ç»æ˜ å°„å™¨å……å½“è¯­ä¹‰ä¼ è¾“è·¯å¾„ï¼Œå°†è§†è§‰è¯­ä¹‰æŠ•å½±åˆ°ç¥ç»å“åº”æµå½¢ä¸­ï¼Œä»¥å®ç°é«˜ä¿çœŸåº¦çš„fMRIåˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynBrainåœ¨é’ˆå¯¹ç‰¹å®šä¸»é¢˜çš„è§†è§‰åˆ°fMRIç¼–ç æ€§èƒ½æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSynBrainèƒ½å¤Ÿé«˜æ•ˆé€‚åº”æ–°ä¸»é¢˜å¹¶ç”Ÿæˆé«˜è´¨é‡fMRIä¿¡å·ï¼Œæœ‰æ•ˆæé«˜äº†æ•°æ®æœ‰é™çš„fMRIåˆ°å›¾åƒè§£ç æ€§èƒ½ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒSynBrainæ­ç¤ºäº†è¯•éªŒå’Œå—è¯•è€…ä¹‹é—´çš„åŠŸèƒ½ä¸€è‡´æ€§ï¼Œåˆæˆçš„ä¿¡å·æ•æ‰åˆ°äº†ç”±ç”Ÿç‰©ç¥ç»å˜å¼‚æ‰€å¡‘é€ çš„å¯è§£é‡Šæ¨¡å¼ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº <a target="_blank" rel="noopener" href="https://github.com/MichaelMaiii/SynBrain%E3%80%82">https://github.com/MichaelMaiii/SynBrainã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10298v3">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è®¡ç®—ç¥ç»ç§‘å­¦ä¸­çš„è§†è§‰åˆºæ¿€å¦‚ä½•è½¬åŒ–ä¸ºçš®å±‚å“åº”çš„é—®é¢˜ã€‚ç”±äºç›¸åŒçš„è§†è§‰è¾“å…¥åœ¨ä¸åŒè¯•éªŒã€æƒ…å¢ƒå’Œå—è¯•è€…ä¸­å¼•å‘çš„è¡€æµåŠ¨åŠ›å­¦å“åº”æ˜¯å¯å˜çš„ï¼Œå› æ­¤è¿™ç§è§†è§‰åˆ°ç¥ç»çš„æ˜ å°„æ˜¯ä¸€ç§ä¸€å¯¹ä¸€å¤šçš„å…³ç³»ã€‚é’ˆå¯¹ç°æœ‰ç¡®å®šæ€§æ–¹æ³•æ— æ³•åŒæ—¶æ¨¡æ‹Ÿè¿™ç§ç”Ÿç‰©å˜å¼‚æ€§å’Œç¼–ç åˆºæ¿€ä¿¡æ¯çš„åº•å±‚åŠŸèƒ½ä¸€è‡´æ€§çš„å±€é™æ€§ï¼Œæå‡ºäº†SynBrainè¿™ä¸€ç”Ÿæˆæ¡†æ¶ã€‚å®ƒé€šè¿‡æ¦‚ç‡å­¦ä¹ å’Œè¯­ä¹‰çº¦æŸæ¨¡æ‹Ÿè§†è§‰è¯­ä¹‰åˆ°ç¥ç»å“åº”çš„è½¬æ¢ï¼Œå¹¶å¼•å…¥BrainVAEå’Œè¯­ä¹‰åˆ°ç¥ç»æ˜ å°„å™¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynBrainåœ¨ç‰¹å®šå—è¯•è€…çš„è§†è§‰åˆ°fMRIç¼–ç æ€§èƒ½ä¸Šè¶…è¶Šäº†æœ€æ–°æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°é€‚åº”æ–°çš„å—è¯•è€…è¿›è¡Œå°æ ·æœ¬æ•°æ®çš„fMRIä¿¡å·åˆæˆï¼Œæé«˜æ•°æ®å—é™çš„fMRIåˆ°å›¾åƒçš„è§£ç æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSynBrainæ­ç¤ºäº†è¯•éªŒå’Œå—è¯•è€…ä¹‹é—´çš„åŠŸèƒ½ä¸€è‡´æ€§ï¼Œåˆæˆçš„ä¿¡å·æ•æ‰åˆ°äº†ç”±ç”Ÿç‰©ç¥ç»å˜å¼‚å¡‘é€ çš„å¯è§£é‡Šæ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†è®¡ç®—ç¥ç»ç§‘å­¦ä¸­çš„è§†è§‰åˆºæ¿€è½¬åŒ–ä¸ºçš®å±‚å“åº”é—®é¢˜ï¼ŒæŒ‡å‡ºè¿™æ˜¯ä¸€ä¸ªä¸€å¯¹ä¸€å¤šçš„å…³ç³»ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥åŒæ—¶æ¨¡æ‹Ÿç”Ÿç‰©å˜å¼‚æ€§å’Œåº•å±‚åŠŸèƒ½ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†SynBrainç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡æ¦‚ç‡å­¦ä¹ å’Œè¯­ä¹‰çº¦æŸæ¨¡æ‹Ÿè§†è§‰è¯­ä¹‰åˆ°ç¥ç»å“åº”çš„è½¬æ¢ã€‚</li>
<li>SynBrainåŒ…æ‹¬BrainVAEå’Œè¯­ä¹‰åˆ°ç¥ç»æ˜ å°„å™¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>SynBrainåœ¨è§†è§‰åˆ°fMRIç¼–ç æ€§èƒ½ä¸Šè¶…è¶Šäº†æœ€æ–°æ–¹æ³•ã€‚</li>
<li>SynBrainèƒ½é«˜æ•ˆé€‚åº”æ–°å—è¯•è€…çš„å°æ ·æœ¬æ•°æ®ï¼Œæé«˜fMRIä¿¡å·åˆæˆçš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2461b05b6ab8136eead69af80eb96c9a" align="middle">
<img src="https://picx.zhimg.com/v2-a96cfc9b81da67d98cc82dde2b7a884f" align="middle">
<img src="https://picx.zhimg.com/v2-63a8681f97d14003b6c1dd8510fc22ba" align="middle">
<img src="https://picx.zhimg.com/v2-2187603fb56f91ab0208538ff16dda5a" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CompoST-A-Benchmark-for-Analyzing-the-Ability-of-LLMs-To-Compositionally-Interpret-Questions-in-a-QALD-Setting"><a href="#CompoST-A-Benchmark-for-Analyzing-the-Ability-of-LLMs-To-Compositionally-Interpret-Questions-in-a-QALD-Setting" class="headerlink" title="CompoST: A Benchmark for Analyzing the Ability of LLMs To   Compositionally Interpret Questions in a QALD Setting"></a>CompoST: A Benchmark for Analyzing the Ability of LLMs To   Compositionally Interpret Questions in a QALD Setting</h2><p><strong>Authors:David Maria Schmidt, Raoul Schubert, Philipp Cimiano</strong></p>
<p>Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they â€œunderstandâ€ the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries. </p>
<blockquote>
<p>è¯­è¨€ç†è§£æ˜¯ä¸€ä¸ªç»„åˆè¿‡ç¨‹ï¼Œå…¶ä¸­æ›´å¤æ‚çš„è¯­è¨€ç»“æ„çš„å«ä¹‰æ˜¯ä»å…¶ç»„æˆéƒ¨åˆ†ä¸­æ¨æ–­å‡ºæ¥çš„ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ‹¥æœ‰æ˜¾è‘—çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¹¶å·²æˆåŠŸåº”ç”¨äºå°†é—®é¢˜æ˜ å°„åˆ°SPARQLæŸ¥è¯¢ä»¥è¿›è¡Œç†è§£ã€‚ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜æ˜¯è¿™ç§ç†è§£è¿‡ç¨‹æœ‰å¤šç³»ç»Ÿã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä»¥è°ƒæŸ¥LLMsåœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¤Ÿç†è§£é—®é¢˜çš„ç»„åˆæ€§è´¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŸºäºDBpediaä¸­çš„å›¾å½¢æ¨¡å¼ç”Ÿæˆäº†ä¸‰ä¸ªä¸åŒéš¾åº¦çš„æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨Lemonè¯å…¸è¿›è¡Œå£è¯­åŒ–è¡¨è¾¾ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ä»¥éå¸¸å—æ§çš„æ–¹å¼åˆ›å»ºï¼Œæ—¨åœ¨æµ‹è¯•LLMsåœ¨ç†è§£ç»“æ„å¤æ‚é—®é¢˜çš„èƒ½åŠ›ï¼Œåœ¨ç»™å®šä»–ä»¬å·²æ¥è§¦åˆ°çš„åŸºæœ¬è¦ç´ çš„å‰æä¸‹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿè¯„ä¼°LLMsèƒ½å¤Ÿåœ¨å¤šå¤§ç¨‹åº¦ä¸Šç†è§£å¤æ‚é—®é¢˜ï¼Œå…¶ä¸­â€œç†è§£â€äº†åŸå­éƒ¨åˆ†ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸åŒå¤§å°çš„æ¨¡å‹è¿›è¡Œå®éªŒï¼Œé‡‡ç”¨å„ç§æç¤ºå’Œå°‘æ ·æœ¬ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥åŠå¾®è°ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å®è§‚F1çš„ç»©æ•ˆæ–¹é¢ï¼Œéšç€ä¸ä¼˜åŒ–æ ·æœ¬çš„åå·®è¶Šæ¥è¶Šå¤§ï¼Œä»0.45é™è‡³0.09ã€‚å³ä½¿åœ¨è¾“å…¥ä¸­æä¾›äº†æ‰€æœ‰å¿…è¦ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå¯¹äºå¤æ‚åº¦æœ€ä½çš„æ•°æ®é›†è€Œè¨€ï¼ŒF1å¾—åˆ†ä¹Ÿæ²¡æœ‰è¶…è¿‡0.57ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼ŒLLMsåœ¨ç³»ç»Ÿæ€§åœ°ã€ç»„åˆåœ°ç†è§£é—®é¢˜å¹¶å°†å…¶æ˜ å°„ä¸ºSPARQLæŸ¥è¯¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21257v2">PDF</a> Research Track, 24th International Semantic Web Conference (ISWC   2025), November 2-6, 2025, Nara, Japan</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£è¯»é—®é¢˜å¹¶å°†å…¶è½¬åŒ–ä¸ºSPARQLæŸ¥è¯¢æ—¶çš„èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªè¯„ä¼°LLMsåœ¨è§£è¯»é—®é¢˜æ—¶ï¼Œæ˜¯å¦èƒ½å¤Ÿç»„åˆè¿ç”¨çš„èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡ç”ŸæˆåŸºäºDBpediaå›¾è°±æ¨¡å¼çš„ä¸‰ä¸ªä¸åŒéš¾åº¦çš„æ•°æ®é›†æ¥è¿›è¡Œæµ‹è¯•ï¼Œå¹¶åˆ©ç”¨Lemonè¯å…¸è¿›è¡Œè¡¨è¾¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œéšç€ä¸è®­ç»ƒæ ·æœ¬çš„åå·®å¢å¤§ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é—®é¢˜è§£è¯»æ–¹é¢çš„æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚å› æ­¤ï¼ŒLLMsåœ¨ç³»ç»Ÿæ€§ç»„åˆè§£è¯»é—®é¢˜å’Œè½¬åŒ–ä¸ºSPARQLæŸ¥è¯¢æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡æ˜¾è‘—çš„è¯­è¨€è§£è¯»èƒ½åŠ›ï¼Œå¹¶èƒ½æˆåŠŸåº”ç”¨äºå°†é—®é¢˜è§£è¯»è½¬åŒ–ä¸ºSPARQLæŸ¥è¯¢ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªè¯„ä¼°LLMsè§£è¯»é—®é¢˜èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ç”ŸæˆåŸºäºDBpediaå›¾è°±æ¨¡å¼çš„ä¸‰ä¸ªä¸åŒéš¾åº¦çš„æ•°æ®é›†è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>å®éªŒé‡‡ç”¨ä¸åŒçš„æ¨¡å‹å¤§å°ã€æç¤ºå’Œå°‘æ ·æœ¬ä¼˜åŒ–æŠ€æœ¯ä»¥åŠå¾®è°ƒæ–¹æ³•è¿›è¡Œç ”ç©¶ã€‚</li>
<li>æ€§èƒ½åœ¨å®è§‚F1å¾—åˆ†æ–¹é¢ï¼Œä»ä¼˜åŒ–æ ·æœ¬çš„åç¦»åº¦å¢åŠ æ—¶ï¼Œä»0.45é™è‡³0.09ã€‚</li>
<li>å³ä½¿æ‰€æœ‰å¿…è¦ä¿¡æ¯éƒ½æä¾›ç»™æ¨¡å‹ï¼Œå¯¹äºæœ€ä½å¤æ‚åº¦æ•°æ®é›†ï¼ŒF1å¾—åˆ†ä¹Ÿä¸è¶…è¿‡0.57ã€‚</li>
<li>LLMsåœ¨ç³»ç»Ÿæ€§ç»„åˆè§£è¯»é—®é¢˜å’Œå°†å…¶è½¬åŒ–ä¸ºSPARQLæŸ¥è¯¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afaa5203dcddcc5187215dcf9d8304f0" align="middle">
<img src="https://picx.zhimg.com/v2-69c8b22ffeda3f35d1e619fd151bad20" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Context-Tuning-for-In-Context-Optimization"><a href="#Context-Tuning-for-In-Context-Optimization" class="headerlink" title="Context Tuning for In-Context Optimization"></a>Context Tuning for In-Context Optimization</h2><p><strong>Authors:Jack Lu, Ryan Teehan, Zhenbang Yang, Mengye Ren</strong></p>
<p>We introduce Context Tuning, a simple and effective method to significantly enhance few-shot adaptation of language models (LLMs) without fine-tuning model parameters. While prompt-based adaptation techniques have demonstrated the effectiveness of lightweight adaptation methods for LLMs, they typically initialize a trainable prompt or prefix with irrelevant tokens for the task at hand. In contrast, Context Tuning initializes the trainable prompt or prefix with task-specific demonstration examples, leveraging the modelâ€™s inherent In-Context Learning (ICL) ability to extract relevant information for improved few-shot learning performance. Extensive evaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard, and ARC demonstrate that Context Tuning outperforms traditional prompt-based adaptation methods and achieves competitive accuracy to Test-Time Training with significantly higher training efficiency. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†Context Tuningï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜LLMçš„å°‘é‡é€‚åº”ã€‚è™½ç„¶åŸºäºæç¤ºçš„é€‚åº”æŠ€æœ¯å·²ç»è¯æ˜äº†å…¶å¯¹LLMçš„è½»é‡çº§é€‚åº”æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä½†å®ƒä»¬é€šå¸¸ä½¿ç”¨ä¸å½“å‰ä»»åŠ¡ä¸ç›¸å…³çš„ä»¤ç‰Œæ¥åˆå§‹åŒ–å¯è®­ç»ƒçš„æç¤ºæˆ–å‰ç¼€ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒContext Tuningä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„æ¼”ç¤ºç¤ºä¾‹æ¥åˆå§‹åŒ–å¯è®­ç»ƒçš„æç¤ºæˆ–å‰ç¼€ï¼Œåˆ©ç”¨æ¨¡å‹å›ºæœ‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›æ¥æå–ç›¸å…³ä¿¡æ¯ï¼Œä»¥æé«˜å°‘é‡å­¦ä¹ çš„æ€§èƒ½ã€‚åœ¨CrossFitã€UnifiedQAã€MMLUã€BIG-Bench Hardå’ŒARCç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒContext Tuningä¼˜äºä¼ ç»Ÿçš„åŸºäºæç¤ºçš„é€‚åº”æ–¹æ³•ï¼Œå¹¶å®ç°äº†ä¸æµ‹è¯•æ—¶é—´è®­ç»ƒç›¸å½“çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04221v2">PDF</a> A short version of this paper was accepted at ICML 2025 Workshop on   Test-Time Adaptation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Context Tuningï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯åœ¨ä¸å¾®è°ƒè¯­è¨€æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜å…¶å°æ ·æœ¬é€‚åº”æ€§ã€‚ä¸åŸºäºæç¤ºçš„é€‚åº”æŠ€æœ¯ç›¸æ¯”ï¼ŒContext Tuningé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„æ¼”ç¤ºç¤ºä¾‹åˆå§‹åŒ–å¯è®­ç»ƒçš„æç¤ºæˆ–å‰ç¼€ï¼Œåˆ©ç”¨æ¨¡å‹çš„å›ºæœ‰ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ¥æå–ç›¸å…³ä¿¡æ¯ï¼Œä»è€Œæé«˜å°æ ·æœ¬å­¦ä¹ çš„æ€§èƒ½ã€‚åœ¨CrossFitã€UnifiedQAã€MMLUã€BIG-Bench Hardå’ŒARCç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒContext Tuningä¼˜äºä¼ ç»Ÿçš„åŸºäºæç¤ºçš„é€‚åº”æ–¹æ³•ï¼Œå¹¶å®ç°äº†ä¸æµ‹è¯•æ—¶é—´è®­ç»ƒç›¸å½“çš„ç²¾åº¦ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Context Tuningæ˜¯ä¸€ç§å¢å¼ºè¯­è¨€æ¨¡å‹å°æ ·æœ¬é€‚åº”æ€§çš„ç®€å•æœ‰æ•ˆæ–¹æ³•ï¼Œæ— éœ€å¾®è°ƒæ¨¡å‹å‚æ•°ã€‚</li>
<li>ä¸å…¶ä»–åŸºäºæç¤ºçš„é€‚åº”æŠ€æœ¯ä¸åŒï¼ŒContext Tuningé€šè¿‡ä»»åŠ¡ç‰¹å®šæ¼”ç¤ºç¤ºä¾‹åˆå§‹åŒ–å¯è®­ç»ƒçš„æç¤ºæˆ–å‰ç¼€ã€‚</li>
<li>Context Tuningåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å›ºæœ‰ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ¥æå–ç›¸å…³ä¿¡æ¯ã€‚</li>
<li>Context Tuningåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¦‚CrossFitã€UnifiedQAã€MMLUã€BIG-Bench Hardå’ŒARCã€‚</li>
<li>Context Tuningä¼˜äºä¼ ç»Ÿçš„åŸºäºæç¤ºçš„é€‚åº”æ–¹æ³•ã€‚</li>
<li>Context Tuningå®ç°äº†ä¸æµ‹è¯•æ—¶é—´è®­ç»ƒç›¸å½“çš„ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1148ce4e0aa72b11c4f5a99236df00d6" align="middle">
<img src="https://picx.zhimg.com/v2-740223ef27ebc8fddbe7dc4db6ee954d" align="middle">
<img src="https://picx.zhimg.com/v2-ec63fda2ad4ee25eeccf4d8a82e66ccf" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PlantSegNeRF-A-few-shot-cross-species-method-for-plant-3D-instance-point-cloud-reconstruction-via-joint-channel-NeRF-with-multi-view-image-instance-matching"><a href="#PlantSegNeRF-A-few-shot-cross-species-method-for-plant-3D-instance-point-cloud-reconstruction-via-joint-channel-NeRF-with-multi-view-image-instance-matching" class="headerlink" title="PlantSegNeRF: A few-shot, cross-species method for plant 3D instance   point cloud reconstruction via joint-channel NeRF with multi-view image   instance matching"></a>PlantSegNeRF: A few-shot, cross-species method for plant 3D instance   point cloud reconstruction via joint-channel NeRF with multi-view image   instance matching</h2><p><strong>Authors:Xin Yang, Ruiming Du, Hanyang Huang, Jiayang Xie, Pengyao Xie, Leisen Fang, Ziyue Guo, Nanjun Jiang, Yu Jiang, Haiyan Cen</strong></p>
<p>Organ segmentation of plant point clouds is a prerequisite for the high-resolution and accurate extraction of organ-level phenotypic traits. Although the fast development of deep learning has boosted much research on segmentation of plant point clouds, the existing techniques for organ segmentation still face limitations in resolution, segmentation accuracy, and generalizability across various plant species. In this study, we proposed a novel approach called plant segmentation neural radiance fields (PlantSegNeRF), aiming to directly generate high-precision instance point clouds from multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF performed 2D instance segmentation on the multi-view images to generate instance masks for each organ with a corresponding ID. The multi-view instance IDs corresponding to the same plant organ were then matched and refined using a specially designed instance matching module. The instance NeRF was developed to render an implicit scene, containing color, density, semantic and instance information. The implicit scene was ultimately converted into high-precision plant instance point clouds based on the volume density. The results proved that in semantic segmentation of point clouds, PlantSegNeRF outperformed the commonly used methods, demonstrating an average improvement of 16.1%, 18.3%, 17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the second-best results on structurally complex species. More importantly, PlantSegNeRF exhibited significant advantages in plant point cloud instance segmentation tasks. Across all plant species, it achieved average improvements of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively. This study extends the organ-level plant phenotyping and provides a high-throughput way to supply high-quality 3D data for the development of large-scale models in plant science. </p>
<blockquote>
<p>æ¤ç‰©ç‚¹äº‘å™¨å®˜åˆ†å‰²æ˜¯å®ç°é«˜åˆ†è¾¨ç‡å’Œç²¾ç¡®æå–å™¨å®˜æ°´å¹³è¡¨å‹ç‰¹å¾çš„å‰æã€‚å°½ç®¡æ·±åº¦å­¦ä¹ å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å¤§é‡å…³äºæ¤ç‰©ç‚¹äº‘åˆ†å‰²çš„ç ”ç©¶ï¼Œä½†ç°æœ‰çš„å™¨å®˜åˆ†å‰²æŠ€æœ¯åœ¨åˆ†è¾¨ç‡ã€åˆ†å‰²ç²¾åº¦å’Œè·¨å¤šç§æ¤ç‰©ç‰©ç§çš„é€šç”¨æ€§æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPlantSegNeRFçš„æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨ä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ç›´æ¥ç”Ÿæˆé«˜ç²¾åº¦çš„å®ä¾‹ç‚¹äº‘ï¼Œé€‚ç”¨äºå¤šç§æ¤ç‰©ç‰©ç§ã€‚PlantSegNeRFå¯¹å¤šè§†è§’å›¾åƒè¿›è¡ŒäºŒç»´å®ä¾‹åˆ†å‰²ï¼Œä¸ºæ¯ä¸ªå™¨å®˜ç”Ÿæˆå…·æœ‰ç›¸åº”IDçš„å®ä¾‹æ©è†œã€‚ç„¶åï¼Œä½¿ç”¨ä¸“é—¨è®¾è®¡çš„å®ä¾‹åŒ¹é…æ¨¡å—åŒ¹é…å’Œç»†åŒ–å¯¹åº”äºåŒä¸€æ¤ç‰©å™¨å®˜çš„å¤šè§†è§’å®ä¾‹IDã€‚å¼€å‘äº†å®ä¾‹NeRFæ¥å‘ˆç°åŒ…å«é¢œè‰²ã€å¯†åº¦ã€è¯­ä¹‰å’Œå®ä¾‹ä¿¡æ¯çš„éšå¼åœºæ™¯ã€‚æœ€ç»ˆï¼ŒåŸºäºä½“ç§¯å¯†åº¦å°†éšå¼åœºæ™¯è½¬æ¢ä¸ºé«˜ç²¾åº¦çš„æ¤ç‰©å®ä¾‹ç‚¹äº‘ã€‚ç»“æœè¯æ˜ï¼Œåœ¨ç‚¹äº‘è¯­ä¹‰åˆ†å‰²ä¸­ï¼ŒPlantSegNeRFä¼˜äºå¸¸ç”¨æ–¹æ³•ï¼Œåœ¨ç»“æ„å¤æ‚çš„ç‰©ç§ä¸Šï¼Œç›¸è¾ƒäºç¬¬äºŒå¥½çš„ç»“æœï¼Œå…¶åœ¨ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒIoUä¸Šåˆ†åˆ«å¹³å‡æé«˜äº†16.1%ã€18.3%ã€17.8%å’Œ24.2%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒPlantSegNeRFåœ¨æ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨æ‰€æœ‰æ¤ç‰©ç‰©ç§ä¸­ï¼Œå®ƒåœ¨mPrecã€mRecã€mCovå’ŒmWCovä¸Šåˆ†åˆ«å¹³å‡æé«˜äº†11.7%ã€38.2%ã€32.2%å’Œ25.3%ã€‚æœ¬ç ”ç©¶æ‰©å±•äº†å™¨å®˜æ°´å¹³çš„æ¤ç‰©è¡¨å‹åˆ†æï¼Œå¹¶ä¸ºæ¤ç‰©ç§‘å­¦ä¸­å¤§è§„æ¨¡æ¨¡å‹çš„å¼€å‘æä¾›äº†ä¸€ç§é«˜é€šé‡çš„æ–¹æ³•æ¥æä¾›é«˜è´¨é‡çš„ä¸‰ç»´æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00371v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPlantSegNeRFçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ç›´æ¥ç”Ÿæˆé«˜ç²¾åº¦æ¤ç‰©å™¨å®˜ç‚¹äº‘ã€‚è¯¥æ–¹æ³•é€šè¿‡2Då®ä¾‹åˆ†å‰²ç”Ÿæˆå®ä¾‹æ©æ¨¡ï¼Œå¹¶åˆ©ç”¨ä¸“é—¨è®¾è®¡çš„å®ä¾‹åŒ¹é…æ¨¡å—è¿›è¡ŒåŒ¹é…å’Œç»†åŒ–ã€‚ä¹‹åå¼€å‘å®ä¾‹NeRFæ¸²æŸ“éšå¼åœºæ™¯ï¼Œæœ€ååŸºäºä½“ç§¯å¯†åº¦è½¬æ¢ä¸ºé«˜ç²¾åº¦æ¤ç‰©å®ä¾‹ç‚¹äº‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­ä¹‰åˆ†å‰²å’Œæ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜è¶Šï¼Œä¸ºæ¤ç‰©ç§‘å­¦çš„å¤§å‹æ¨¡å‹å¼€å‘æä¾›é«˜è´¨é‡3Dæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PlantSegNeRFæ–¹æ³•èƒ½å¤Ÿç›´æ¥ç”Ÿæˆé«˜ç²¾åº¦æ¤ç‰©å™¨å®˜ç‚¹äº‘ã€‚</li>
<li>é€šè¿‡å¤šè§†è§’RGBå›¾åƒåºåˆ—è¿›è¡Œ2Då®ä¾‹åˆ†å‰²ï¼Œç”Ÿæˆå®ä¾‹æ©æ¨¡ã€‚</li>
<li>ä¸“é—¨è®¾è®¡çš„å®ä¾‹åŒ¹é…æ¨¡å—ç”¨äºåŒ¹é…å’Œç»†åŒ–å¤šè§†è§’å®ä¾‹ã€‚</li>
<li>å¼€å‘å®ä¾‹NeRFä»¥æ¸²æŸ“åŒ…å«é¢œè‰²ã€å¯†åº¦ã€è¯­ä¹‰å’Œå®ä¾‹ä¿¡æ¯çš„éšå¼åœºæ™¯ã€‚</li>
<li>PlantSegNeRFåœ¨è¯­ä¹‰åˆ†å‰²å’Œæ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ä¸å…¶ä»–å¸¸ç”¨æ–¹æ³•ç›¸æ¯”ï¼ŒPlantSegNeRFåœ¨ç»“æ„æ€§å¤æ‚ç‰©ç§çš„ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒIoUç­‰æ–¹é¢å¹³å‡æé«˜äº†16.1%ã€18.3%ã€17.8%å’Œ24.2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-711af185f90bd7649adac63ee8cfcfdf" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays"><a href="#AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays" class="headerlink" title="AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays"></a>AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays</h2><p><strong>Authors:Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨åŒ…æ‹¬åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨å†…çš„å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºCLIPæ¨¡å‹çš„å…¬å¹³æ€§å…³æ³¨ï¼ŒåŒ…æ‹¬äººå£ç»Ÿè®¡åè§ï¼Œå¹¶æœªå¾—åˆ°è¶³å¤Ÿçš„é‡è§†ã€‚è¿™ç§ç–å¿½å¯¼è‡´äº†å…³é”®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä¸ç§æ—å’Œæ€§åˆ«æœ‰å…³çš„é—®é¢˜ï¼Œä»è€Œå¯¼è‡´è¯Šæ–­ç»“æœå­˜åœ¨å·®è·ï¼Œå¹¶å¯¹ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“é™ä½äº†å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AdFair-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨å¯¹æŠ—æ€§ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§çš„æ–°å‹æ¡†æ¶ï¼Œä»è€Œå‡è½»å¶ç„¶ç›¸å…³æ€§ï¼Œæé«˜é¢„æµ‹å…¬å¹³æ€§ã€‚æˆ‘ä»¬åœ¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜AdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­ï¼Œæ˜¾è‘—æé«˜äº†å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœä¸ºCLIPåŸºåŒ»å­¦è¯Šæ–­æ¨¡å‹ä¸­çš„å…¬å¹³æ„è¯†å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯CXRåˆ†æï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23467v3">PDF</a> This preprint has been accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>CLIPæ¨¡å‹åœ¨åŒ…æ‹¬åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨å†…çš„å„ç§è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨å…¬å¹³æ€§é—®é¢˜ä¸Šï¼Œç‰¹åˆ«æ˜¯ç§æ—å’Œæ€§åˆ«æ–¹é¢çš„åè§å—åˆ°çš„å…³æ³¨æœ‰é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºAdFair-CLIPæ¡†æ¶ï¼Œé‡‡ç”¨å¯¹æŠ—ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§ï¼Œå‡å°‘å¶ç„¶å…³è”å¹¶æå‡é¢„æµ‹å…¬å¹³æ€§ã€‚åœ¨èƒ¸éƒ¨Xå…‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹æ˜¾è‘—æå‡äº†å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸ºCLIPåŒ»å­¦è¯Šæ–­æ¨¡å‹ä¸­çš„å…¬å¹³æ„è¯†å­¦ä¹ æ ‘ç«‹äº†æ–°åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨CXRåˆ†ææ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ç­‰è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å…¬å¹³æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ç§æ—å’Œæ€§åˆ«åè§ã€‚</li>
<li>AdFair-CLIPæ¡†æ¶è¢«æå‡ºç”¨äºè§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡å¯¹æŠ—ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§ã€‚</li>
<li>AdFair-CLIPèƒ½å‡å°‘å¶ç„¶å…³è”ï¼Œæé«˜é¢„æµ‹å…¬å¹³æ€§ã€‚</li>
<li>åœ¨èƒ¸éƒ¨Xå…‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹æå‡äº†å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>AdFair-CLIPæ¡†æ¶çš„å¼•å…¥ä¸ºCLIPåŒ»å­¦è¯Šæ–­æ¨¡å‹ä¸­çš„å…¬å¹³æ„è¯†å­¦ä¹ æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d34913087e59aa7a1ef4030d0128144b" align="middle">
<img src="https://picx.zhimg.com/v2-5fb7463fac7f556fcb5c554575826a10" align="middle">
<img src="https://picx.zhimg.com/v2-6cf06751428b9c42c648bbaaf5f8bea1" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-225a662404a8c32f6e7e723321d5bdb4" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Enabling Fast and Accurate Neutral Atom Readout through Image Denoising
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2f43dd3462e62bdce8e3ed2f3f7e8aa7" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  InnovatorBench Evaluating Agents' Ability to Conduct Innovative LLM   Research
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
