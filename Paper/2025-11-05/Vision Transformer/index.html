<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Vision Transformer for Robust Occluded Person Reidentification in   Complex Surveillance Scenes">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-755ac230a8575ea201695e23cbd4ac24~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289182&auth_key=1762289182-0-0-0e35274a626815f6ab7d24f4f01c7ee9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="Vision-Transformer-for-Robust-Occluded-Person-Reidentification-in-Complex-Surveillance-Scenes"><a href="#Vision-Transformer-for-Robust-Occluded-Person-Reidentification-in-Complex-Surveillance-Scenes" class="headerlink" title="Vision Transformer for Robust Occluded Person Reidentification in   Complex Surveillance Scenes"></a>Vision Transformer for Robust Occluded Person Reidentification in   Complex Surveillance Scenes</h2><p><strong>Authors:Bo Li, Duyuan Zheng, Xinyang Liu, Qingwen Li, Hong Li, Hongyan Cui, Ge Gao, Chen Liu</strong></p>
<p>Person re-identification (ReID) in surveillance is challenged by occlusion, viewpoint distortion, and poor image quality. Most existing methods rely on complex modules or perform well only on clear frontal images. We propose Sh-ViT (Shuffling Vision Transformer), a lightweight and robust model for occluded person ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a Shuffle module in the final Transformer layer to break spatial correlations and enhance robustness to occlusion and blur; Second, scenario-adapted augmentation (geometric transforms, erasing, blur, and color adjustment) to simulate surveillance conditions; Third, DeiT-based knowledge distillation to improve learning with limited labels.To support real-world evaluation, we construct the MyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base station inspections, with frequent equipment occlusion and camera variations. Experiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT, outperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on Market1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves robustness to occlusion and blur without external modules, offering a practical solution for surveillance-based personnel monitoring. </p>
<blockquote>
<p>ç›‘æ§åœºæ™¯ä¸‹çš„è¡Œäººå†è¯†åˆ«ï¼ˆReIDï¼‰é¢ä¸´é®æŒ¡ã€è§†è§’ç•¸å˜å’Œå›¾åƒè´¨é‡å·®ç­‰æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤æ‚çš„æ¨¡å—æˆ–è€…ä»…åœ¨æ¸…æ™°çš„æ­£é¢å›¾åƒä¸Šè¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬æå‡ºäº†Sh-ViTï¼ˆShuffling Vision Transformerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé®æŒ¡è¡ŒäººReIDçš„è½»é‡çº§ä¸”ç¨³å¥çš„æ¨¡å‹ã€‚Sh-ViTåŸºäºViT-Baseæ„å»ºï¼Œå¼•å…¥äº†ä¸‰ä¸ªç»„ä»¶ï¼šé¦–å…ˆï¼Œåœ¨æœ€åçš„Transformerå±‚ä¸­åŠ å…¥Shuffleæ¨¡å—ï¼Œæ‰“ç ´ç©ºé—´ç›¸å…³æ€§ï¼Œå¢å¼ºå¯¹é®æŒ¡å’Œæ¨¡ç³Šçš„é²æ£’æ€§ï¼›å…¶æ¬¡ï¼Œæƒ…æ™¯é€‚åº”æ€§å¢å¼ºï¼ˆå‡ ä½•å˜æ¢ã€æ“¦é™¤ã€æ¨¡ç³Šå’Œè‰²å½©è°ƒæ•´ï¼‰æ¥æ¨¡æ‹Ÿç›‘æ§æ¡ä»¶ï¼›ç¬¬ä¸‰ï¼ŒåŸºäºDeiTçš„çŸ¥è¯†è’¸é¦æ¥æ”¹å–„æœ‰é™æ ‡ç­¾çš„å­¦ä¹ ã€‚ä¸ºäº†æ”¯æŒçœŸå®ä¸–ç•Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†MyTTæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10,000åè¡Œäººå’Œæ¥è‡ªåŸºç«™æ£€æŸ¥çš„30,000+å›¾åƒï¼Œå…·æœ‰é¢‘ç¹çš„è®¾å¤‡é®æŒ¡å’Œç›¸æœºå˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒSh-ViTåœ¨MyTTä¸Šå®ç°äº†83.2%çš„Rank-1å’Œ80.1%çš„mAPï¼Œè¶…è¿‡äº†CNNå’ŒViTåŸºçº¿ï¼Œåœ¨Market1501ä¸Šå®ç°äº†94.6%çš„Rank-1å’Œ87.5%çš„mAPï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ€»çš„æ¥è¯´ï¼ŒSh-ViTæé«˜äº†å¯¹é®æŒ¡å’Œæ¨¡ç³Šçš„é²æ£’æ€§ï¼Œä¸”æ— éœ€å¤–éƒ¨æ¨¡å—ï¼Œä¸ºåŸºäºç›‘æ§çš„äººå‘˜ç›‘æ§æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27677v1">PDF</a> 12 pages,conference</p>
<p><strong>Summary</strong></p>
<p>Sh-ViTæ˜¯ä¸€ç§é’ˆå¯¹é®æŒ¡è¡Œäººå†è¯†åˆ«é—®é¢˜çš„è½»é‡çº§ä¸”ç¨³å¥çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡å¼•å…¥Shuffleæ¨¡å—ã€åœºæ™¯é€‚åº”æ€§å¢å¼ºå’ŒDeiTçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œæé«˜äº†å¯¹é®æŒ¡å’Œæ¨¡ç³Šçš„é²æ£’æ€§ã€‚åœ¨MyTTæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSh-ViTåœ¨è¡Œäººå†è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ï¼Œå¹¶å®ç°äº†åœ¨å®é™…ç›‘æ§åœºæ™¯ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sh-ViTæ¨¡å‹é’ˆå¯¹é®æŒ¡è¡Œäººå†è¯†åˆ«é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
<li>Sh-ViTå¼•å…¥äº†Shuffleæ¨¡å—ï¼Œæ‰“ç ´äº†ç©ºé—´ç›¸å…³æ€§ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹é®æŒ¡å’Œæ¨¡ç³Šçš„é²æ£’æ€§ã€‚</li>
<li>åœºæ™¯é€‚åº”æ€§å¢å¼ºæŠ€æœ¯é€šè¿‡æ¨¡æ‹Ÿç›‘æ§æ¡ä»¶ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DeiTçŸ¥è¯†è’¸é¦æŠ€æœ¯ç”¨äºæé«˜æ¨¡å‹åœ¨æœ‰é™æ ‡ç­¾ä¸‹çš„å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>MyTTæ•°æ®é›†çš„æ„å»ºï¼Œæ”¯æŒäº†æ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„è¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSh-ViTåœ¨MyTTæ•°æ®é›†ä¸Šå–å¾—äº†83.2%çš„Rank-1å‡†ç¡®ç‡å’Œ80.1%çš„mAPï¼Œè¶…è¶Šäº†CNNå’ŒViTåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e6fc605638a6268366c4a12186a2a31f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289022&auth_key=1762289022-0-0-2e034ba5221fb98ec9eb1ac22c02939d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3bbb232da09c7e047f0abe8d6e1dffac~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289029&auth_key=1762289029-0-0-23091c5bdc4b67c9d22b8510630b7f57&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3578dd49dee5c5eb94e8191fc729f7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289036&auth_key=1762289036-0-0-7d90e00ab2a535e4923e72315379bf53&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9aa1a20a1f4970831818e9f0248dea83~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289042&auth_key=1762289042-0-0-69bc706f65c5933b3061ff71a9a5169a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f41e863db9bcdf7efcac58ccc2e07579~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289049&auth_key=1762289049-0-0-8c35daa949e0eed1bd8b418732c6fa99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CoMViT-An-Efficient-Vision-Backbone-for-Supervised-Classification-in-Medical-Imaging"><a href="#CoMViT-An-Efficient-Vision-Backbone-for-Supervised-Classification-in-Medical-Imaging" class="headerlink" title="CoMViT: An Efficient Vision Backbone for Supervised Classification in   Medical Imaging"></a>CoMViT: An Efficient Vision Backbone for Supervised Classification in   Medical Imaging</h2><p><strong>Authors:Aon Safdar, Mohamed Saadeldin</strong></p>
<p>Vision Transformers (ViTs) have demonstrated strong potential in medical imaging; however, their high computational demands and tendency to overfit on small datasets limit their applicability in real-world clinical scenarios. In this paper, we present CoMViT, a compact and generalizable Vision Transformer architecture optimized for resource-constrained medical image analysis. CoMViT integrates a convolutional tokenizer, diagonal masking, dynamic temperature scaling, and pooling-based sequence aggregation to improve performance and generalization. Through systematic architectural optimization, CoMViT achieves robust performance across twelve MedMNIST datasets while maintaining a lightweight design with only ~4.5M parameters. It matches or outperforms deeper CNN and ViT variants, offering up to 5-20x parameter reduction without sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT consistently attends to clinically relevant regions despite its compact size. These results highlight the potential of principled ViT redesign for developing efficient and interpretable models in low-resource medical imaging settings. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTï¼‰åœ¨åŒ»å­¦æˆåƒé¢†åŸŸè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬è®¡ç®—é‡å¤§ä¸”åœ¨å°æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡åº¦æ‹Ÿåˆï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CoMViTï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹èµ„æºå—é™åŒ»å­¦å›¾åƒåˆ†æè€Œä¼˜åŒ–çš„ç´§å‡‘ä¸”é€šç”¨çš„è§†è§‰Transformeræ¶æ„ã€‚CoMViTé›†æˆäº†å·ç§¯æ ‡è®°å™¨ã€å¯¹è§’æ©ç ã€åŠ¨æ€æ¸©åº¦ç¼©æ”¾å’ŒåŸºäºæ± åŒ–çš„åºåˆ—èšåˆï¼Œä»¥æé«˜æ€§èƒ½å’Œé€šç”¨æ€§ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶æ„ä¼˜åŒ–ï¼ŒCoMViTåœ¨åäºŒä¸ªMedMNISTæ•°æ®é›†ä¸Šå®ç°äº†ç¨³å¥çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§è®¾è®¡ï¼Œä»…æœ‰~450ä¸‡å‚æ•°ã€‚å®ƒåŒ¹é…æˆ–è¶…è¶Šäº†æ›´æ·±çš„CNNå’ŒViTå˜ä½“ï¼Œåœ¨ä¸å½±å“ç²¾åº¦çš„æƒ…å†µä¸‹å®ç°äº†é«˜è¾¾5-20å€çš„å‚æ•°å‡å°‘ã€‚å®šæ€§Grad-CAMåˆ†æè¡¨æ˜ï¼Œå°½ç®¡CoMViTå°ºå¯¸ç´§å‡‘ï¼Œä½†å®ƒå§‹ç»ˆå…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸã€‚è¿™äº›ç»“æœçªå‡ºäº†åŸºäºåŸåˆ™é‡æ–°è®¾è®¡çš„ViTåœ¨ä½èµ„æºåŒ»å­¦æˆåƒç¯å¢ƒä¸­å¼€å‘é«˜æ•ˆå’Œå¯è§£é‡Šæ¨¡å‹çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27442v1">PDF</a> Preprint (submitted manuscript). Accepted at the MICCAI 2025 MIRASOL   Workshop; to appear in the Springer proceedings volume. This is the   pre-review version (not the Version of Record). DOI will be added after   publication. [Optional: 8 pages, 4 figures, 4 tables.]</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é€‚ç”¨äºèµ„æºå—é™åŒ»å­¦å›¾åƒåˆ†æçš„ç´§å‡‘é€šç”¨å‹è§†è§‰Transformeræ¶æ„â€”â€”CoMViTã€‚å®ƒé€šè¿‡æ•´åˆå·ç§¯åˆ†è¯å™¨ã€å¯¹è§’æ©ç ã€åŠ¨æ€æ¸©åº¦ç¼©æ”¾å’ŒåŸºäºæ± åŒ–çš„åºåˆ—èšåˆç­‰æŠ€æœ¯ï¼Œä¼˜åŒ–äº†æ€§èƒ½å¹¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚CoMViTåœ¨åäºŒä¸ªMedMNISTæ•°æ®é›†ä¸Šå®ç°äº†ç¨³å¥çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§è®¾è®¡ï¼Œä»…çº¦4.5Må‚æ•°ã€‚å®ƒåŒ¹é…æˆ–ä¼˜äºè¾ƒæ·±çš„CNNå’ŒViTå˜ä½“ï¼Œæä¾›äº†é«˜è¾¾5-20å€çš„å‚æ•°ç¼©å‡ï¼ŒåŒæ—¶ä¸æŸå¤±ç²¾åº¦ã€‚ç»“æœçªå‡ºäº†è®¾è®¡åŸåˆ™ViTçš„æ½œåŠ›ï¼Œå¯ä»¥åœ¨ä½èµ„æºåŒ»å­¦æˆåƒç¯å¢ƒä¸­å¼€å‘é«˜æ•ˆå¯è§£é‡Šæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoMViTæ˜¯ä¸€ç§é’ˆå¯¹èµ„æºå—é™åŒ»å­¦å›¾åƒåˆ†æä¼˜åŒ–çš„ç´§å‡‘é€šç”¨å‹Vision Transformeræ¶æ„ã€‚</li>
<li>CoMViTé€šè¿‡æ•´åˆå¤šç§æŠ€æœ¯ï¼ˆå·ç§¯åˆ†è¯å™¨ã€å¯¹è§’æ©ç ç­‰ï¼‰æé«˜äº†æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CoMViTåœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†ç¨³å¥æ€§èƒ½ï¼Œå‚æ•°æ•°é‡ç›¸å¯¹è¾ƒå°‘ï¼ˆä»…çº¦4.5Mï¼‰ã€‚</li>
<li>CoMViTä¸æ·±åº¦CNNå’ŒViTå˜ä½“ç›¸æ¯”ï¼Œåœ¨å‚æ•°æ•ˆç‡ä¸Šæœ‰æ‰€çªç ´ï¼Œå®ç°äº†é«˜å‚æ•°ç¼©å‡ç‡ï¼ˆ5-20å€ï¼‰ã€‚</li>
<li>CoMViTåœ¨ä¿è¯å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œé™ä½äº†è®¡ç®—éœ€æ±‚ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒã€‚</li>
<li>å®šæ€§Grad-CAMåˆ†ææ˜¾ç¤ºï¼ŒCoMViTèƒ½å¤Ÿå…³æ³¨ä¸´åºŠä¸Šç›¸å…³çš„åŒºåŸŸï¼Œå³ä½¿åœ¨å…¶ç´§å‡‘å¤§å°çš„æƒ…å†µä¸‹ä¹Ÿå¦‚æ­¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-14b105cab70abc4c7b41d4dc4fe205f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289055&auth_key=1762289055-0-0-35a75585d95d3e39558320244b7cd269&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da8d596ea94a9dea47e80ec836fd15e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289063&auth_key=1762289063-0-0-4ba8ed8288c287f1741494b4a84e195e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ef9be54cd2e370db89885b8e56b3671~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289069&auth_key=1762289069-0-0-8ed700361377a55eed84c1d1cd1b16c5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3fa47173a434053af3c83d341781e38f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289076&auth_key=1762289076-0-0-8f98e72190fb6aaf3f8c38db8f753b09&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46fafd6b76660509c00ab243a022bb60~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289082&auth_key=1762289082-0-0-2fdaefa7210cbbe0c29bdeadb28f1560&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3ec09ef0f276aee12a1f9752b87eeaf~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289090&auth_key=1762289090-0-0-cce84dac344c4526afa6bb3f021117ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Sparse-Model-Inversion-Efficient-Inversion-of-Vision-Transformers-for-Data-Free-Applications"><a href="#Sparse-Model-Inversion-Efficient-Inversion-of-Vision-Transformers-for-Data-Free-Applications" class="headerlink" title="Sparse Model Inversion: Efficient Inversion of Vision Transformers for   Data-Free Applications"></a>Sparse Model Inversion: Efficient Inversion of Vision Transformers for   Data-Free Applications</h2><p><strong>Authors:Zixuan Hu, Yongxian Wei, Li Shen, Zhenyi Wang, Lei Li, Chun Yuan, Dacheng Tao</strong></p>
<p>Model inversion, which aims to reconstruct the original training data from pre-trained discriminative models, is especially useful when the original training data is unavailable due to privacy, usage rights, or size constraints. However, existing dense inversion methods attempt to reconstruct the entire image area, making them extremely inefficient when inverting high-resolution images from large-scale Vision Transformers (ViTs). We further identify two underlying causes of this inefficiency: the redundant inversion of noisy backgrounds and the unintended inversion of spurious correlationsâ€“a phenomenon we term â€œhallucinationâ€ in model inversion. To address these limitations, we propose a novel sparse model inversion strategy, as a plug-and-play extension to speed up existing dense inversion methods with no need for modifying their original loss functions. Specifically, we selectively invert semantic foregrounds while stopping the inversion of noisy backgrounds and potential spurious correlations. Through both theoretical and empirical studies, we validate the efficacy of our approach in achieving significant inversion acceleration (up to 3.79 faster) while maintaining comparable or even enhanced downstream performance in data-free model quantization and data-free knowledge transfer. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Egg-Hu/SMI">https://github.com/Egg-Hu/SMI</a>. </p>
<blockquote>
<p>æ¨¡å‹åè½¬æ—¨åœ¨ä»é¢„è®­ç»ƒçš„åˆ¤åˆ«æ¨¡å‹ä¸­é‡æ„åŸå§‹è®­ç»ƒæ•°æ®ï¼Œåœ¨ç”±äºéšç§ã€ä½¿ç”¨æƒæˆ–è§„æ¨¡é™åˆ¶ç­‰åŸå› å¯¼è‡´åŸå§‹è®­ç»ƒæ•°æ®ä¸å¯ç”¨çš„æƒ…å†µä¸‹å°¤å…¶æœ‰ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¯†é›†åè½¬æ–¹æ³•è¯•å›¾é‡æ„æ•´ä¸ªå›¾åƒåŒºåŸŸï¼Œåœ¨ä»å¤§è§„æ¨¡è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åè½¬é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ï¼Œè¿™ä½¿å…¶æ•ˆç‡æä½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯†åˆ«å‡ºè¿™ç§ä½æ•ˆç‡çš„ä¸¤ä¸ªæ ¹æœ¬åŸå› ï¼šå˜ˆæ‚èƒŒæ™¯çš„å†—ä½™åè½¬å’Œæ„å¤–åè½¬çš„å¶ç„¶å…³è”â€”â€”æˆ‘ä»¬åœ¨æ¨¡å‹åè½¬ä¸­ç§°ä¹‹ä¸ºâ€œå¹»è§‰â€çš„ç°è±¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–æ¨¡å‹åè½¬ç­–ç•¥ï¼Œä½œä¸ºä¸€ç§å³æ’å³ç”¨æ‰©å±•ï¼Œå¯ä»¥åŠ é€Ÿç°æœ‰çš„å¯†é›†åè½¬æ–¹æ³•ï¼Œè€Œæ— éœ€ä¿®æ”¹å…¶åŸå§‹æŸå¤±å‡½æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æœ‰é€‰æ‹©åœ°åè½¬è¯­ä¹‰å‰æ™¯ï¼ŒåŒæ—¶åœæ­¢å˜ˆæ‚èƒŒæ™¯å’Œæ½œåœ¨å¶ç„¶å…³è”çš„åè½¬ã€‚é€šè¿‡ç†è®ºç ”ç©¶å’Œå®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨è¾¾åˆ°æ˜¾è‘—çš„å¿«é€Ÿåè½¬ï¼ˆæœ€é«˜è¾¾3.79å€ï¼‰çš„åŒæ—¶ï¼Œåœ¨æ— éœ€æ•°æ®çš„æ¨¡å‹é‡åŒ–å’Œæ— éœ€æ•°æ®çš„çŸ¥è¯†è½¬ç§»ä¸­ä¿æŒç›¸å½“ç”šè‡³æ›´å¥½çš„ä¸‹æ¸¸æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Egg-Hu/SMI%E3%80%82">https://github.com/Egg-Hu/SMIã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.27186v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹é¢„è®­ç»ƒåˆ¤åˆ«æ¨¡å‹ï¼Œæå‡ºä¸€ç§æ–°å‹ç¨€ç–æ¨¡å‹åæ¼”ç­–ç•¥ï¼Œæ—¨åœ¨ä»å¤§å‹è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ä¸­åæ¼”é«˜åˆ†è¾¨ç‡å›¾åƒã€‚è¯¥ç­–ç•¥æ—¨åœ¨è§£å†³ç°æœ‰å¯†é›†åæ¼”æ–¹æ³•åœ¨é¢å¯¹å¤§å‹æ¨¡å‹æ—¶çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åæ¼”è¯­ä¹‰å‰æ™¯æ—¶åœæ­¢åæ¼”å˜ˆæ‚èƒŒæ™¯å’Œæ½œåœ¨è™šå‡å…³è”ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„åæ¼”åŠ é€Ÿï¼ˆæœ€é«˜å¯è¾¾3.79å€ï¼‰ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜åœ¨æ— æ•°æ®æ¨¡å‹é‡åŒ–ã€æ— æ•°æ®çŸ¥è¯†è¿ç§»ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€äºGitHubã€‚</p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ul>
<li>æ¨¡å‹åæ¼”ï¼šæŠ€æœ¯ç”¨äºä»é¢„è®­ç»ƒåˆ¤åˆ«æ¨¡å‹ä¸­é‡å»ºåŸå§‹è®­ç»ƒæ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ³•è·å–åŸå§‹æ•°æ®æ—¶ç”±äºéšç§ä¿æŠ¤æˆ–æ•°æ®é‡é™åˆ¶å°¤å…¶æœ‰ç”¨ã€‚ä½†ç°æœ‰çš„å¯†é›†åæ¼”æ–¹æ³•å°è¯•é‡å»ºæ•´ä¸ªå›¾åƒåŒºåŸŸï¼Œå¯¹äºå¤§å‹è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰æ¥è¯´æ•ˆç‡æä½ã€‚</li>
<li>æ•ˆç‡é—®é¢˜åŸå› ï¼šå†—ä½™çš„åè½¬å˜ˆæ‚èƒŒæ™¯ä»¥åŠæ„å¤–çš„åè½¬è™šå‡å…³è”ï¼Œè¢«ç§°ä¸ºâ€œå¹»è§‰â€ã€‚è¿™ç§æ•ˆç‡é—®é¢˜ä½¿å¾—åæ¼”è¿‡ç¨‹ç¼“æ…¢ä¸”ç»“æœè´¨é‡ä¸ä½³ã€‚</li>
<li>ç¨€ç–æ¨¡å‹åæ¼”ç­–ç•¥ï¼šä½œä¸ºä¸€ç§å³æ’å³ç”¨æ‰©å±•ï¼Œèƒ½å¤ŸåŠ é€Ÿç°æœ‰å¯†é›†åæ¼”æ–¹æ³•ï¼Œæ— éœ€ä¿®æ”¹å…¶åŸå§‹æŸå¤±å‡½æ•°ã€‚è¯¥ç­–ç•¥é€‰æ‹©æ€§åœ°åæ¼”è¯­ä¹‰å‰æ™¯ï¼ŒåŒæ—¶åœæ­¢åæ¼”å˜ˆæ‚èƒŒæ™¯å’Œæ½œåœ¨è™šå‡å…³è”ã€‚è¿™ç§ç­–ç•¥æ˜¾è‘—æé«˜äº†åæ¼”é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†æˆ–æé«˜äº†åœ¨æ— æ•°æ®æ¨¡å‹é‡åŒ–ã€æ— æ•°æ®çŸ¥è¯†è¿ç§»ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¿™æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆæ¥è§£å†³ç°æœ‰æ¨¡å‹åæ¼”çš„å±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.27186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-89d7fce5d0c3f38dac083e93c995edd2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289097&auth_key=1762289097-0-0-913e908538e8ea75ecc847bc69e93511&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6edf97c10af0bb80a0d6f1bbec1f5628~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289104&auth_key=1762289104-0-0-76973530a0d9dcd7cd2923c7a755ef21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-660b429c38b39c32db518ed3439b90d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289111&auth_key=1762289111-0-0-475e5e38ed6b8bf91a8a85bb03115a9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d9c02971bbeca1f651fc7ed449c861a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289118&auth_key=1762289118-0-0-de00e62cf469375ff850b299a089d6c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-127b6eea8c380ce112b90e0f6dddd07c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289125&auth_key=1762289125-0-0-658d059a82e8fe1c712ffaa9ef65f56a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SteerVLM-Robust-Model-Control-through-Lightweight-Activation-Steering-for-Vision-Language-Models"><a href="#SteerVLM-Robust-Model-Control-through-Lightweight-Activation-Steering-for-Vision-Language-Models" class="headerlink" title="SteerVLM: Robust Model Control through Lightweight Activation Steering   for Vision Language Models"></a>SteerVLM: Robust Model Control through Lightweight Activation Steering   for Vision Language Models</h2><p><strong>Authors:Anushka Sivakumar, Andrew Zhang, Zaber Hakim, Chris Thomas</strong></p>
<p>This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLMâ€™s size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†SteerVLMï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¯¼å‘æ¨¡å—ï¼Œæ—¨åœ¨å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰äº§ç”Ÿæ›´ç¬¦åˆæ‰€éœ€æŒ‡ä»¤çš„è¾“å‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»ç¼–ç ç›®æ ‡è¡Œä¸ºå’Œåå‘è¡Œä¸ºçš„é…å¯¹æç¤ºçš„æ½œåœ¨åµŒå…¥ä¸­å­¦ä¹ ï¼ŒåŠ¨æ€è°ƒæ•´è¿æ¥è¯­è¨€æ¨¡æ€ä¸å›¾åƒä¸Šä¸‹æ–‡ä¹‹é—´çš„æ¿€æ´»ã€‚è¿™å…è®¸åœ¨æ¨ç†æ—¶é—´å¯¹å¤æ‚è¾“å‡ºè¯­ä¹‰è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼ŒåŒæ—¶åœ¨ä¸ä¿®æ”¹æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ä¿æŒéç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å¯¼å‘æ¨¡å—éœ€è¦å­¦ä¹ çš„å‚æ•°ä»…ä¸ºåŸå§‹VLMå¤§å°çš„0.14%ã€‚æˆ‘ä»¬çš„å¯¼å‘æ¨¡å—é€šè¿‡ç»´åº¦æ¿€æ´»è°ƒåˆ¶å’Œè·¨å±‚çš„è‡ªé€‚åº”å¯¼å‘è·å¾—æ¨¡å‹æ§åˆ¶ï¼Œè€Œæ— éœ€é¢„å…ˆæå–çš„é™æ€å‘é‡æˆ–æ‰‹åŠ¨è°ƒæ•´å¹²é¢„ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†VNIAï¼ˆè§†è§‰å™äº‹æ„å›¾å¯¹é½ï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯ä¸“é—¨ä¸ºä¿ƒè¿›VLMè½¬å‘æŠ€æœ¯çš„å‘å±•å’Œè¯„ä¼°è€Œåˆ›å»ºçš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨VLMçš„è½¬å‘å’Œå¹»è§‰ç¼“è§£åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰çš„å¹²é¢„æŠ€æœ¯ï¼Œå¹¶é€šè¿‡æ¿€æ´»å·¥ç¨‹æå‡ºäº†å¤šæ¨¡æ€æ¨¡å‹æ§åˆ¶çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26769v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºSteerVLMï¼Œä¸€ä¸ªè½»é‡çº§çš„æ§åˆ¶æ¨¡å—ï¼Œæ—¨åœ¨å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”Ÿæˆæ›´ç¬¦åˆæŒ‡ä»¤è¦æ±‚çš„ç»“æœã€‚SteerVLMé€šè¿‡å­¦ä¹ ç›®æ ‡è¡Œä¸ºä¸åå‘è¡Œä¸ºé…å¯¹æç¤ºçš„æ½œåœ¨åµŒå…¥ï¼ŒåŠ¨æ€è°ƒæ•´è¿æ¥è¯­è¨€æ¨¡æ€ä¸å›¾åƒä¸Šä¸‹æ–‡ä¹‹é—´çš„æ¿€æ´»ï¼Œä»è€Œå®ç°ç²¾ç»†ç²’åº¦çš„æ¨ç†æ§åˆ¶ï¼ŒåŒæ—¶ä¸å½±å“æ¨¡å‹å¯¹åç¦»ç›®æ ‡çš„ä»»åŠ¡çš„æ€§èƒ½ã€‚è¯¥æ§åˆ¶æ¨¡å—æ‰€éœ€çš„å­¦ä¹ å‚æ•°ä»…ä¸ºåŸå§‹VLMè§„æ¨¡çš„0.14%ã€‚é€šè¿‡é€å±‚æ¿€æ´»è°ƒåˆ¶å’Œè‡ªé€‚åº”æ§åˆ¶ï¼ŒSteerVLMæ— éœ€é¢„å…ˆæå–çš„é™æ€å‘é‡æˆ–æ‰‹åŠ¨è°ƒæ•´å¹²é¢„ç‚¹å³å¯å®ç°æ¨¡å‹æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†VNIAï¼ˆè§†è§‰å™äº‹æ„å›¾å¯¹é½ï¼‰æ•°æ®é›†ï¼Œä¸“ä¸ºä¿ƒè¿›VLMæ§åˆ¶æŠ€æœ¯çš„å‘å±•å’Œè¯„ä¼°è€Œè®¾è®¡ã€‚åœ¨æ§åˆ¶å’Œç¼“è§£å¹»è§‰çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„å¹²é¢„æŠ€æœ¯ï¼Œå¹¶é€šè¿‡æ¿€æ´»å·¥ç¨‹æå‡ºäº†å¯¹å¤šæ¨¡æ€æ¨¡å‹æ§åˆ¶çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SteerVLMæ˜¯ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œæ—¨åœ¨å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»¥ç¬¦åˆæŒ‡ä»¤çš„æ–¹å¼è¾“å‡ºã€‚</li>
<li>æ¨¡å—é€šè¿‡å­¦ä¹ é…å¯¹æç¤ºçš„æ½œåœ¨åµŒå…¥æ¥åŠ¨æ€è°ƒæ•´æ¿€æ´»ã€‚</li>
<li>å®ç°æ¨ç†æ—¶çš„ç²¾ç»†ç²’åº¦æ§åˆ¶ï¼Œä¸å½±å“æ¨¡å‹å¯¹åç¦»ç›®æ ‡çš„ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>æ§åˆ¶æ¨¡å—æ‰€éœ€çš„å­¦ä¹ å‚æ•°ä»…ä¸ºåŸå§‹VLMè§„æ¨¡çš„æå°éƒ¨åˆ†ï¼ˆ0.14%ï¼‰ã€‚</li>
<li>æ¨¡å—é€šè¿‡é€å±‚æ¿€æ´»è°ƒåˆ¶å’Œè‡ªé€‚åº”æ§åˆ¶å®ç°æ¨¡å‹æ§åˆ¶ï¼Œæ— éœ€é¢å¤–çš„é™æ€å‘é‡æˆ–æ‰‹åŠ¨è°ƒæ•´å¹²é¢„ç‚¹ã€‚</li>
<li>å¼•å…¥äº†VNIAæ•°æ®é›†ï¼Œç”¨äºå‘å±•å’Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„æ§åˆ¶æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ§åˆ¶å’Œç¼“è§£å¹»è§‰çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-688f4b03787dd4b4126a685a78bd65de~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289132&auth_key=1762289132-0-0-164c83842170ee00fc7d605de16b8e07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-00e7d2aa91fffda3d7e612fbdce23442~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289139&auth_key=1762289139-0-0-6df89e4e23bffac3439a4a4ce47ba746&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-978f7f6a2a1999be6e1a87bddd1f6bc7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289146&auth_key=1762289146-0-0-dc9c81fb611b8be2a283247ced8fc4f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1eeb8848152c9d7e464516a4a314887b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289153&auth_key=1762289153-0-0-fa23f38475efb27f75a9289f2065cd13&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56e0aaa3c77884cdf118173646b872c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289160&auth_key=1762289160-0-0-49f3ee57e79551618a9335a4bbf6b870&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Visual-Diversity-and-Region-aware-Prompt-Learning-for-Zero-shot-HOI-Detection"><a href="#Visual-Diversity-and-Region-aware-Prompt-Learning-for-Zero-shot-HOI-Detection" class="headerlink" title="Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI   Detection"></a>Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI   Detection</h2><p><strong>Authors:Chanhyeong Yang, Taehoon Song, Jihwan Park, Hyunwoo J. Kim</strong></p>
<p>Zero-shot Human-Object Interaction detection aims to localize humans and objects in an image and recognize their interaction, even when specific verb-object pairs are unseen during training. Recent works have shown promising results using prompt learning with pretrained vision-language models such as CLIP, which align natural language prompts with visual features in a shared embedding space. However, existing approaches still fail to handle the visual complexity of interaction, including (1) intra-class visual diversity, where instances of the same verb appear in diverse poses and contexts, and (2) inter-class visual entanglement, where distinct verbs yield visually similar patterns. To address these challenges, we propose VDRP, a framework for Visual Diversity and Region-aware Prompt learning. First, we introduce a visual diversity-aware prompt learning strategy that injects group-wise visual variance into the context embedding. We further apply Gaussian perturbation to encourage the prompts to capture diverse visual variations of a verb. Second, we retrieve region-specific concepts from the human, object, and union regions. These are used to augment the diversity-aware prompt embeddings, yielding region-aware prompts that enhance verb-level discrimination. Experiments on the HICO-DET benchmark demonstrate that our method achieves state-of-the-art performance under four zero-shot evaluation settings, effectively addressing both intra-class diversity and inter-class visual entanglement. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mlvlab/VDRP">https://github.com/mlvlab/VDRP</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬äººä½“ç›®æ ‡äº¤äº’æ£€æµ‹æ—¨åœ¨å®šä½å›¾åƒä¸­çš„äººä½“å’Œç‰©ä½“ï¼Œå¹¶è¯†åˆ«ä»–ä»¬çš„äº¤äº’ï¼Œå³ä½¿è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§è¿‡ç‰¹å®šçš„åŠ¨è¯-ç‰©ä½“å¯¹ã€‚è¿‘æœŸçš„ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è¿›è¡Œæç¤ºå­¦ä¹ å¾ˆæœ‰å‰æ™¯ï¼Œè¿™ç§æ¨¡å‹å¯ä»¥åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å¯¹è‡ªç„¶è¯­è¨€æç¤ºå’Œè§†è§‰ç‰¹å¾è¿›è¡Œå¯¹é½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»æ— æ³•å¤„ç†äº¤äº’çš„è§†è§‰å¤æ‚æ€§ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰ç±»å†…è§†è§‰å¤šæ ·æ€§ï¼Œå…¶ä¸­åŒä¸€åŠ¨è¯çš„å®ä¾‹ä»¥ä¸åŒçš„å§¿åŠ¿å’Œä¸Šä¸‹æ–‡å‡ºç°ï¼›ï¼ˆ2ï¼‰ç±»é—´è§†è§‰çº ç¼ ï¼Œå…¶ä¸­ä¸åŒçš„åŠ¨è¯ä¼šäº§ç”Ÿè§†è§‰ä¸Šç›¸ä¼¼çš„æ¨¡å¼ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†VDRPï¼Œä¸€ä¸ªè§†è§‰å¤šæ ·æ€§å’ŒåŒºåŸŸæ„ŸçŸ¥æç¤ºå­¦ä¹ æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§†è§‰å¤šæ ·æ€§æ„ŸçŸ¥æç¤ºå­¦ä¹ ç­–ç•¥ï¼Œå°†ç¾¤ä½“è§†è§‰æ–¹å·®æ³¨å…¥ä¸Šä¸‹æ–‡åµŒå…¥ã€‚æˆ‘ä»¬è¿˜åº”ç”¨é«˜æ–¯æ‰°åŠ¨æ¥é¼“åŠ±æç¤ºæ•æ‰åŠ¨è¯çš„å¤šç§è§†è§‰å˜åŒ–ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä»äººç±»ã€ç‰©ä½“å’Œè”åˆåŒºåŸŸæ£€ç´¢ç‰¹å®šåŒºåŸŸæ¦‚å¿µã€‚è¿™äº›ç”¨äºå¢å¼ºå¤šæ ·æ€§æ„ŸçŸ¥æç¤ºåµŒå…¥ï¼Œäº§ç”Ÿå¢å¼ºåŠ¨è¯çº§åˆ«è¾¨åˆ«çš„åŒºåŸŸæ„ŸçŸ¥æç¤ºã€‚åœ¨HICO-DETåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å››ç§é›¶æ ·æœ¬è¯„ä¼°è®¾ç½®ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ç±»å†…å¤šæ ·æ€§å’Œç±»é—´è§†è§‰çº ç¼ é—®é¢˜ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/mlvlab/VDRP%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mlvlab/VDRPè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25094v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong><br>åœ¨é›¶æ ·æœ¬äººç±»å¯¹è±¡äº¤äº’æ£€æµ‹ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è¿›è¡Œæç¤ºå­¦ä¹ å±•ç°å‡ºè‰¯å¥½æ•ˆæœã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´è§†è§‰å¤æ‚æ€§æŒ‘æˆ˜ï¼Œå¦‚è§†è§‰å¤šæ ·æ€§é—®é¢˜å’Œè§†è§‰çº ç¼ é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VDRPæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¯è§†åŒ–å¤šæ ·æ€§å’ŒåŒºåŸŸæ„ŸçŸ¥æç¤ºå­¦ä¹ æå‡è¡¨ç°ã€‚å®ç°äº†é€šè¿‡æ·»åŠ ç¾¤ç»„å’Œç‰¹æ®Šæ¨¡å‹æ‰°åŠ¨çš„å¤šæ ·åŒ–å’Œæœ‰ç›®æ ‡çš„åŒºåˆ†è¯å‘é‡æ¥ä¿ƒè¿›ç²¾å‡†æç¤ºçš„å½¢æˆå’Œå‘å¸ƒï¼Œå¹¶åˆ©ç”¨äººã€å¯¹è±¡å’Œè”åˆåŒºåŸŸçš„ç‰¹å®šæ¦‚å¿µæ¥å¢åŠ å¢å¼ºè§†è§‰ç‰¹å¾çš„æ„ŸçŸ¥å’Œçµæ´»æ€§æç¤ºç”Ÿæˆç»“æœï¼Œæ˜¾è‘—æå‡äº†åŠ¨è¯çº§åˆ«çš„åˆ¤åˆ«èƒ½åŠ›ã€‚ä»£ç å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é›¶æ ·æœ¬äººç±»å¯¹è±¡äº¤äº’æ£€æµ‹æ—¨åœ¨å®šä½å›¾åƒä¸­çš„äººä¸ç‰©ä½“å¹¶è¯†åˆ«å…¶äº¤äº’ã€‚</li>
<li>CLIPç­‰é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬æ£€æµ‹ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´è§†è§‰å¤šæ ·æ€§å’Œè§†è§‰çº ç¼ ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0d9da73dcc39458b229a1e0eb367d292~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289167&auth_key=1762289167-0-0-4f06d265db5500a27c01c2a82d0f9225&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f28f048cd240da8a78f7d38a123c0a90~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289175&auth_key=1762289175-0-0-d5a8eb995e6a2a2692a1f9790e31c2fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-755ac230a8575ea201695e23cbd4ac24~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289182&auth_key=1762289182-0-0-0e35274a626815f6ab7d24f4f01c7ee9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c8d75db77c0b030607ce37864f1bcd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289189&auth_key=1762289189-0-0-fdfe1415616e668873be94b833792738&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Remote-Sensing-Image-Scene-Classification-with-CLIP-and-Prompt-Learning"><a href="#Few-Shot-Remote-Sensing-Image-Scene-Classification-with-CLIP-and-Prompt-Learning" class="headerlink" title="Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt   Learning"></a>Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt   Learning</h2><p><strong>Authors:Ivica Dimitrovski, Vlatko Spasev, Ivan Kitanovski</strong></p>
<p>Remote sensing applications increasingly rely on deep learning for scene classification. However, their performance is often constrained by the scarcity of labeled data and the high cost of annotation across diverse geographic and sensor domains. While recent vision-language models like CLIP have shown promise by learning transferable representations at scale by aligning visual and textual modalities, their direct application to remote sensing remains suboptimal due to significant domain gaps and the need for task-specific semantic adaptation. To address this critical challenge, we systematically explore prompt learning as a lightweight and efficient adaptation strategy for few-shot remote sensing image scene classification. We evaluate several representative methods, including Context Optimization, Conditional Context Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating Constraints. These approaches reflect complementary design philosophies: from static context optimization to conditional prompts for enhanced generalization, multi-modal prompts for joint vision-language adaptation, and semantically regularized prompts for stable learning without forgetting. We benchmark these prompt-learning methods against two standard baselines: zero-shot CLIP with hand-crafted prompts and a linear probe trained on frozen CLIP features. Through extensive experiments on multiple benchmark remote sensing datasets, including cross-dataset generalization tests, we demonstrate that prompt learning consistently outperforms both baselines in few-shot scenarios. Notably, Prompting with Self-Regulating Constraints achieves the most robust cross-domain performance. Our findings underscore prompt learning as a scalable and efficient solution for bridging the domain gap in satellite and aerial imagery, providing a strong foundation for future research in this field. </p>
<blockquote>
<p>é¥æ„Ÿåº”ç”¨è¶Šæ¥è¶Šä¾èµ–æ·±åº¦å­¦ä¹ è¿›è¡Œåœºæ™¯åˆ†ç±»ã€‚ç„¶è€Œï¼Œå…¶æ€§èƒ½å¾€å¾€å—åˆ°æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œè·¨ä¸åŒåœ°ç†å’Œä¼ æ„Ÿå™¨é¢†åŸŸæ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é™åˆ¶ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰é€šè¿‡è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„å¯¹é½ï¼Œåœ¨å¤§è§„æ¨¡å­¦ä¹ å¯è½¬ç§»è¡¨ç¤ºæ–¹é¢æ˜¾ç¤ºå‡ºå‰æ™¯ï¼Œä½†å®ƒä»¬ç›´æ¥åº”ç”¨äºé¥æ„Ÿä»ç„¶ä¸å°½äººæ„ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºé¢†åŸŸå·®è·æ˜¾è‘—ä»¥åŠéœ€è¦ä»»åŠ¡ç‰¹å®šçš„è¯­ä¹‰é€‚åº”ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†æç¤ºå­¦ä¹ ä½œä¸ºä¸€ç§è½»é‡çº§ã€é«˜æ•ˆçš„é€‚åº”ç­–ç•¥ï¼Œç”¨äºè§£å†³é¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»çš„å°æ ·æœ¬é—®é¢˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§ä»£è¡¨æ€§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€æ¡ä»¶ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€å¤šæ¨¡æ€æç¤ºå­¦ä¹ å’Œå¸¦æœ‰è‡ªæˆ‘è°ƒèŠ‚çº¦æŸçš„æç¤ºã€‚è¿™äº›æ–¹æ³•åæ˜ äº†äº’è¡¥çš„è®¾è®¡å“²å­¦ï¼šä»é™æ€ä¸Šä¸‹æ–‡ä¼˜åŒ–åˆ°å¢å¼ºæ³›åŒ–çš„æ¡ä»¶æç¤ºï¼Œç”¨äºè”åˆè§†è§‰è¯­è¨€é€‚åº”çš„å¤šæ¨¡æ€æç¤ºï¼Œä»¥åŠç”¨äºç¨³å®šå­¦ä¹ çš„è¯­ä¹‰æ­£åˆ™åŒ–æç¤ºï¼Œè€Œä¸ä¼šå‡ºç°é—å¿˜ç°è±¡ã€‚æˆ‘ä»¬å°†è¿™äº›æç¤ºå­¦ä¹ æ–¹æ³•ä¸ä¸¤ç§æ ‡å‡†åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼šä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„æç¤ºè¿›è¡Œé›¶æ ·æœ¬CLIPï¼Œä»¥åŠä½¿ç”¨å†»ç»“çš„CLIPç‰¹å¾è¿›è¡Œçº¿æ€§æ¢é’ˆè®­ç»ƒã€‚é€šè¿‡å¤šä¸ªåŸºå‡†é¥æ„Ÿæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬è·¨æ•°æ®é›†æ³›åŒ–æµ‹è¯•ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼Œæç¤ºå­¦ä¹ å§‹ç»ˆä¼˜äºè¿™ä¸¤ç§åŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œâ€œå¸¦æœ‰è‡ªæˆ‘è°ƒèŠ‚çº¦æŸçš„æç¤ºâ€å®ç°äº†æœ€ç¨³å¥çš„è·¨åŸŸæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒï¼Œæç¤ºå­¦ä¹ æ˜¯å¼¥åˆå«æ˜Ÿå’Œèˆªç©ºå½±åƒé¢†åŸŸå·®è·çš„å¯æ‰©å±•å’Œé«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä¸ºè¿™ä¸€é¢†åŸŸæœªæ¥çš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24321v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹æ„ŸçŸ¥åº”ç”¨è¶Šæ¥è¶Šå¤šåœ°ä¾èµ–æ·±åº¦å­¦ä¹ è¿›è¡Œåœºæ™¯åˆ†ç±»ï¼Œä½†å—é™äºæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§å’Œè·¨ä¸åŒåœ°ç†å’Œä¼ æ„Ÿå™¨åŸŸæ ‡æ³¨çš„é«˜æˆæœ¬ã€‚æœ€è¿‘å‡ºç°çš„è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPæ˜¾ç¤ºå‡ºé€šè¿‡å¤§è§„æ¨¡å­¦ä¹ å¯è¿ç§»è¡¨ç¤ºæ¥å¯¹é½è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨é¥æ„Ÿé¢†åŸŸçš„ç›´æ¥åº”ç”¨ä»ä¸ç†æƒ³ï¼Œå­˜åœ¨æ˜¾è‘—çš„é¢†åŸŸå·®è·å’Œä»»åŠ¡ç‰¹å®šè¯­ä¹‰é€‚åº”çš„éœ€æ±‚ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢è®¨äº†æç¤ºå­¦ä¹ ä½œä¸ºè½»é‡çº§ã€é«˜æ•ˆçš„é€‚åº”ç­–ç•¥ï¼Œç”¨äºå°æ ·æœ¬çš„é¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§ä»£è¡¨æ€§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€æ¡ä»¶ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€å¤šæ¨¡æ€æç¤ºå­¦ä¹ å’Œå¸¦æœ‰è‡ªæˆ‘è°ƒèŠ‚çº¦æŸçš„æç¤ºã€‚è¿™äº›æ–¹æ³•åæ˜ äº†ä»é™æ€ä¸Šä¸‹æ–‡ä¼˜åŒ–åˆ°æ¡ä»¶æç¤ºå¢å¼ºæ³›åŒ–ã€å¤šæ¨¡æ€æç¤ºè¿›è¡Œè”åˆè§†è§‰è¯­è¨€é€‚åº”ä»¥åŠè¯­ä¹‰æ­£åˆ™åŒ–æç¤ºè¿›è¡Œç¨³å®šå­¦ä¹ çš„ä¸åŒè®¾è®¡å“²å­¦ã€‚é€šè¿‡å¯¹è¿™äº›æç¤ºå­¦ä¹ æ–¹æ³•ä¸ä¸¤ç§åŸºå‡†æ–¹æ³•ï¼ˆæ— æç¤ºçš„CLIPå’Œæ‰‹å·¥è‰ºå“æç¤ºçš„çº¿æ€§æ¢é’ˆï¼‰åœ¨å¤šä¸ªé¥æ„Ÿæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬è·¨æ•°æ®é›†æ³›åŒ–æµ‹è¯•ï¼Œæˆ‘ä»¬è¯æ˜äº†æç¤ºå­¦ä¹ åœ¨å°å‹æ ·æœ¬åœºæ™¯ä¸­çš„è¡¨ç°å§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯å¸¦æœ‰è‡ªæˆ‘è°ƒèŠ‚çº¦æŸçš„æç¤ºå®ç°äº†æœ€ç¨³å¥çš„è·¨åŸŸæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†æç¤ºå­¦ä¹ ä½œä¸ºç¼©å°å«æ˜Ÿå’Œèˆªç©ºå›¾åƒé¢†åŸŸå·®è·çš„å¯æ‰©å±•å’Œé«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥åœ¨è¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹æ„ŸçŸ¥åº”ç”¨é¢ä¸´æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œè·¨åŸŸæ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>CLIPç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é¥æ„Ÿé¢†åŸŸçš„åº”ç”¨å­˜åœ¨é¢†åŸŸå·®è·å’Œè¯­ä¹‰é€‚åº”éœ€æ±‚ã€‚</li>
<li>æç¤ºå­¦ä¹ æ˜¯è½»é‡çº§ã€é«˜æ•ˆçš„é€‚åº”ç­–ç•¥ï¼Œç”¨äºå°æ ·æœ¬çš„é¥æ„Ÿå›¾åƒåœºæ™¯åˆ†ç±»ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§æç¤ºå­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€æ¡ä»¶ä¸Šä¸‹æ–‡ä¼˜åŒ–ã€å¤šæ¨¡æ€æç¤ºå­¦ä¹ å’Œå¸¦æœ‰è‡ªæˆ‘è°ƒèŠ‚çº¦æŸçš„æç¤ºã€‚</li>
<li>æç¤ºå­¦ä¹ æ–¹æ³•åœ¨è·¨æ•°æ®é›†æ³›åŒ–æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯å¸¦æœ‰è‡ªæˆ‘è°ƒèŠ‚çº¦æŸçš„æç¤ºã€‚</li>
<li>æç¤ºå­¦ä¹ æœ‰åŠ©äºç¼©å°å«æ˜Ÿå’Œèˆªç©ºå›¾åƒé¢†åŸŸçš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f040b1abc5063cb60d024a6deab3d045~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289195&auth_key=1762289195-0-0-05376a242eef6363c6f1d7ac48539423&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5b7a2b3910548c55f1c0b0c00fa75cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289202&auth_key=1762289202-0-0-756c41aab2ac146b242c5f4c7f9e26d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d106d9dbe7329084d13b68523457cdd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289210&auth_key=1762289210-0-0-f248a9e69014a752a811c783fe162df5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Probing-the-Representational-Geometry-of-Color-Qualia-Dissociating-Pure-Perception-from-Task-Demands-in-Brains-and-AI-Models"><a href="#Probing-the-Representational-Geometry-of-Color-Qualia-Dissociating-Pure-Perception-from-Task-Demands-in-Brains-and-AI-Models" class="headerlink" title="Probing the Representational Geometry of Color Qualia: Dissociating Pure   Perception from Task Demands in Brains and AI Models"></a>Probing the Representational Geometry of Color Qualia: Dissociating Pure   Perception from Task Demands in Brains and AI Models</h2><p><strong>Authors:Jing Xu</strong></p>
<p>Probing the computational underpinnings of subjective experience, or qualia, remains a central challenge in cognitive neuroscience. This project tackles this question by performing a rigorous comparison of the representational geometry of color qualia between state-of-the-art AI models and the human brain. Using a unique fMRI dataset with a â€œno-reportâ€ paradigm, we use Representational Similarity Analysis (RSA) to compare diverse vision models against neural activity under two conditions: pure perception (â€œno-reportâ€) and task-modulated perception (â€œreportâ€). Our analysis yields three principal findings. First, nearly all models align better with neural representations of pure perception, suggesting that the cognitive processes involved in task execution are not captured by current feedforward architectures. Second, our analysis reveals a critical interaction between training paradigm and architecture, challenging the simple assumption that Contrastive Language-Image Pre-training(CLIP) training universally improves neural plausibility. In our direct comparison, this multi-modal training method enhanced brain-alignment for a vision transformer(ViT), yet had the opposite effect on a ConvNet. Our work contributes a new benchmark task for color qualia to the field, packaged in a Brain-Score compatible format. This benchmark reveals a fundamental divergence in the inductive biases of artificial and biological vision systems, offering clear guidance for developing more neurally plausible models. </p>
<blockquote>
<p>æ¢ç©¶ä¸»è§‚ç»éªŒæˆ–æ„Ÿå—çš„è®¡ç®—åŸºç¡€ä»ç„¶æ˜¯è®¤çŸ¥ç¥ç»ç§‘å­¦ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚æ­¤é¡¹ç›®é€šè¿‡ä¸¥æ ¼æ¯”è¾ƒæœ€å‰æ²¿äººå·¥æ™ºèƒ½æ¨¡å‹ä¸å¤§è„‘çš„é¢œè‰²æ„Ÿå—è´¨è¡¨å¾å‡ ä½•ç»“æ„æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬åˆ©ç”¨ç‹¬ç‰¹çš„â€œæ— æŠ¥å‘Šâ€èŒƒå¼åŠŸèƒ½ç£å…±æŒ¯æˆåƒæ•°æ®é›†ï¼Œä½¿ç”¨è¡¨å¾ç›¸ä¼¼æ€§åˆ†ææ–¹æ³•æ¯”è¾ƒå„ç§è§†è§‰æ¨¡å‹åœ¨ä¸¤ç§æ¡ä»¶ä¸‹çš„ç¥ç»æ´»åŠ¨ï¼šçº¯ç²¹æ„ŸçŸ¥ï¼ˆâ€œæ— æŠ¥å‘Šâ€ï¼‰å’Œä»»åŠ¡è°ƒèŠ‚æ„ŸçŸ¥ï¼ˆâ€œæŠ¥å‘Šâ€ï¼‰ã€‚æˆ‘ä»¬çš„åˆ†æå¾—å‡ºäº†ä¸‰ä¸ªä¸»è¦å‘ç°ã€‚é¦–å…ˆï¼Œå‡ ä¹æ‰€æœ‰æ¨¡å‹éƒ½ä¸çº¯ç²¹æ„ŸçŸ¥çš„ç¥ç»è¡¨å¾æ›´ä¸ºä¸€è‡´ï¼Œè¿™è¡¨æ˜å½“å‰çš„å‰é¦ˆæ¶æ„å¹¶æœªæ•æ‰åˆ°ä¸ä»»åŠ¡æ‰§è¡Œç›¸å…³çš„è®¤çŸ¥è¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†è®­ç»ƒèŒƒå¼å’Œæ¶æ„ä¹‹é—´çš„å…³é”®äº’åŠ¨ï¼Œè¿™æŒ‘æˆ˜äº†ç®€å•å‡è®¾â€”â€”å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ™®éæé«˜äº†ç¥ç»å¯ä¿¡åº¦ã€‚åœ¨æˆ‘ä»¬çš„ç›´æ¥æ¯”è¾ƒä¸­ï¼Œè¿™ç§å¤šæ¨¡å¼è®­ç»ƒæ–¹æ³•æé«˜äº†è§†è§‰å˜å‹å™¨çš„è„‘å¯¹é½åº¦ï¼Œä½†å¯¹å·ç§¯ç½‘ç»œäº§ç”Ÿäº†ç›¸åçš„æ•ˆæœã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè¯¥é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ–°çš„é¢œè‰²æ„Ÿå—è´¨åŸºå‡†æµ‹è¯•ä»»åŠ¡ï¼Œä»¥Brain-Scoreå…¼å®¹æ ¼å¼å‘ˆç°ã€‚è¯¥åŸºå‡†æµ‹è¯•æ­ç¤ºäº†äººå·¥ä¸ç”Ÿç‰©è§†è§‰ç³»ç»Ÿçš„å½’çº³åè§ä¹‹é—´çš„æ ¹æœ¬åˆ†æ­§ï¼Œä¸ºå¼€å‘æ›´å…·ç¥ç»å¯ä¿¡åº¦çš„æ¨¡å‹æä¾›äº†æ˜ç¡®æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è®¤çŸ¥ç¥ç»ç§‘å­¦ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜â€”â€”ä¸»è§‚ç»éªŒæˆ–æ„Ÿå—æ€§è´¨çš„è®¡ç®—åŸºç¡€ã€‚è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¯”æœ€å…ˆè¿›çš„AIæ¨¡å‹å’Œäººç±»å¤§è„‘çš„é¢œè‰²æ„Ÿå—æ€§è´¨è¡¨ç¤ºå‡ ä½•ç»“æ„ï¼Œå¯¹è¿™ä¸€é—®é¢˜è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚åˆ©ç”¨ç‹¬ç‰¹çš„fMRIæ•°æ®é›†å’Œâ€œæ— æŠ¥å‘Šâ€èŒƒå¼ï¼Œé€šè¿‡ä»£è¡¨æ€§ç›¸ä¼¼æ€§åˆ†æï¼ˆRSAï¼‰æ¯”è¾ƒäº†ä¸åŒè§†è§‰æ¨¡å‹åœ¨ä¸¤ç§æ¡ä»¶ä¸‹çš„è¡¨ç°ï¼šçº¯æ„ŸçŸ¥ï¼ˆâ€œæ— æŠ¥å‘Šâ€ï¼‰å’Œä»»åŠ¡è°ƒèŠ‚æ„ŸçŸ¥ï¼ˆâ€œæŠ¥å‘Šâ€ï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å¤šæ•°æ¨¡å‹ä¸çº¯æ„ŸçŸ¥çš„ç¥ç»ç½‘ç»œè¡¨ç¤ºæ›´ä¸ºä¸€è‡´ï¼Œè¡¨æ˜å½“å‰çš„å‰é¦ˆæ¶æ„å¹¶æœªæ¶‰åŠä»»åŠ¡æ‰§è¡Œçš„è®¤çŸ¥è¿‡ç¨‹ï¼›å¤šæ¨¡æ€è®­ç»ƒå¦‚å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰å’Œå¯¹å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetï¼‰çš„å¤§è„‘å¯¹é½æ•ˆæœæˆªç„¶ä¸åŒï¼›è¯¥ç ”ç©¶ä¸ºé¢œè‰²æ„Ÿå—æ€§è´¨é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ä»»åŠ¡ï¼Œä»¥Brain-Scoreå…¼å®¹æ ¼å¼å‘ˆç°ã€‚è¯¥åŸºå‡†ä»»åŠ¡æ­ç¤ºäº†äººå·¥å’Œç”Ÿç‰©è§†è§‰ç³»ç»Ÿçš„å½’çº³åç½®ä¹‹é—´çš„æ ¹æœ¬å·®å¼‚ï¼Œä¸ºå¼€å‘æ›´å…·ç¥ç»å¯ä¿¡åº¦çš„æ¨¡å‹æä¾›äº†æ˜ç¡®æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é€šè¿‡å¯¹æ¯”AIæ¨¡å‹å’Œäººç±»å¤§è„‘çš„é¢œè‰²æ„Ÿå—æ€§è´¨è¡¨ç¤ºå‡ ä½•ç»“æ„ï¼Œæ¢è®¨ä¸»è§‚ç»éªŒçš„è®¡ç®—åŸºç¡€ã€‚</li>
<li>åˆ©ç”¨fMRIæ•°æ®é›†å’Œä»£è¡¨æ€§ç›¸ä¼¼æ€§åˆ†ææ–¹æ³•ï¼Œæ¯”è¾ƒäº†ä¸åŒè§†è§‰æ¨¡å‹åœ¨çº¯æ„ŸçŸ¥å’Œä»»åŠ¡è°ƒèŠ‚æ„ŸçŸ¥æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚</li>
<li>å¤§å¤šæ•°æ¨¡å‹ä¸çº¯æ„ŸçŸ¥çš„ç¥ç»ç½‘ç»œè¡¨ç¤ºæ›´ä¸ºä¸€è‡´ï¼Œè¡¨æ˜å½“å‰å‰é¦ˆæ¶æ„æœªå……åˆ†æ¶‰åŠä»»åŠ¡æ‰§è¡Œçš„è®¤çŸ¥è¿‡ç¨‹ã€‚</li>
<li>å¤šæ¨¡æ€è®­ç»ƒå¦‚å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå¯¹ä¸åŒçš„ç½‘ç»œæ¶æ„ï¼ˆå¦‚è§†è§‰å˜å‹å™¨å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼‰çš„å¤§è„‘å¯¹é½æ•ˆæœä¸åŒã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºé¢œè‰²æ„Ÿå—æ€§è´¨é¢†åŸŸæä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ä»»åŠ¡ï¼Œä»¥Brain-Scoreå…¼å®¹æ ¼å¼å‘ˆç°ï¼Œæœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„ç¥ç»å¯ä¿¡åº¦ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†äººå·¥å’Œç”Ÿç‰©è§†è§‰ç³»ç»Ÿçš„å½’çº³åç½®ä¹‹é—´çš„æ ¹æœ¬å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-50f2d67d781b866152e5bd27f6ff636a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289217&auth_key=1762289217-0-0-206feb5e05262e494720fa30f6517f0d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d7e05e0112ffca206e1a44ff36448fa9~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289224&auth_key=1762289224-0-0-e380e2bfa3905237eca489e620b23a53&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c4a451ae36629d9aa79fd1f5117ae57~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289231&auth_key=1762289231-0-0-d0ddda5a0245de7f6e94bcb244a7dccd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e67b2ef9be1a13c769dbb7bff699e147~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289237&auth_key=1762289237-0-0-46e37d028300746cae8bc73aa880362f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ConMatFormer-A-Multi-attention-and-Transformer-Integrated-ConvNext-based-Deep-Learning-Model-for-Enhanced-Diabetic-Foot-Ulcer-Classification"><a href="#ConMatFormer-A-Multi-attention-and-Transformer-Integrated-ConvNext-based-Deep-Learning-Model-for-Enhanced-Diabetic-Foot-Ulcer-Classification" class="headerlink" title="ConMatFormer: A Multi-attention and Transformer Integrated ConvNext   based Deep Learning Model for Enhanced Diabetic Foot Ulcer Classification"></a>ConMatFormer: A Multi-attention and Transformer Integrated ConvNext   based Deep Learning Model for Enhanced Diabetic Foot Ulcer Classification</h2><p><strong>Authors:Raihan Ahamed Rifat, Fuyad Hasan Bhoyan, Md Humaion Kabir Mehedi, Md Kaviul Hossain, Md. Jakir Hossen, M. F. Mridha</strong></p>
<p>Diabetic foot ulcer (DFU) detection is a clinically significant yet challenging task due to the scarcity and variability of publicly available datasets. To solve these problems, we propose ConMatFormer, a new hybrid deep learning architecture that combines ConvNeXt blocks, multiple attention mechanisms convolutional block attention module (CBAM) and dual attention network (DANet), and transformer modules in a way that works together. This design facilitates the extraction of better local features and understanding of the global context, which allows us to model small skin patterns across different types of DFU very accurately. To address the class imbalance, we used data augmentation methods. A ConvNeXt block was used to obtain detailed local features in the initial stages. Subsequently, we compiled the model by adding a transformer module to enhance long-range dependency. This enabled us to pinpoint the DFU classes that were underrepresented or constituted minorities. Tests on the DS1 (DFUC2021) and DS2 (diabetic foot ulcer (DFU)) datasets showed that ConMatFormer outperformed state-of-the-art (SOTA) convolutional neural network (CNN) and Vision Transformer (ViT) models in terms of accuracy, reliability, and flexibility. The proposed method achieved an accuracy of 0.8961 and a precision of 0.9160 in a single experiment, which is a significant improvement over the current standards for classifying DFUs. In addition, by 4-fold cross-validation, the proposed model achieved an accuracy of 0.9755 with a standard deviation of only 0.0031. We further applied explainable artificial intelligence (XAI) methods, such as Grad-CAM, Grad-CAM++, and LIME, to consistently monitor the transparency and trustworthiness of the decision-making process.. Our findings set a new benchmark for DFU classification and provide a hybrid attention transformer framework for medical image analysis. </p>
<blockquote>
<p>ç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰æ£€æµ‹æ˜¯ä¸€é¡¹å…·æœ‰ä¸´åºŠæ„ä¹‰ä½†å……æ»¡æŒ‘æˆ˜çš„ä»»åŠ¡ï¼Œå› ä¸ºå¯ç”¨çš„å…¬å…±æ•°æ®é›†ç¨€ç¼ºä¸”å¤šå˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ConMatFormerï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ··åˆæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå®ƒç»“åˆäº†ConvNeXtå—ã€å¤šç§æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰å’ŒåŒæ³¨æ„åŠ›ç½‘ç»œï¼ˆDANetï¼‰ä»¥åŠååŒå·¥ä½œçš„å˜å‹å™¨æ¨¡å—ã€‚è¿™ç§è®¾è®¡æœ‰åŠ©äºæå–æ›´å¥½çš„å±€éƒ¨ç‰¹å¾å¹¶ç†è§£å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿéå¸¸å‡†ç¡®åœ°å»ºæ¨¡ä¸åŒç±»å‹DFUä¸Šçš„å°çš®è‚¤æ¨¡å¼ã€‚ä¸ºäº†è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ•°æ®å¢å¼ºæ–¹æ³•ã€‚åœ¨åˆå§‹é˜¶æ®µï¼Œä½¿ç”¨ConvNeXtå—æ¥è·å¾—è¯¦ç»†çš„å±€éƒ¨ç‰¹å¾ã€‚éšåï¼Œæˆ‘ä»¬é€šè¿‡æ·»åŠ å˜å‹å™¨æ¨¡å—æ¥å¢å¼ºé•¿æœŸä¾èµ–æ€§ï¼Œä»è€Œç¼–è¯‘æ¨¡å‹ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¡®å®šä»£è¡¨æ€§ä¸è¶³æˆ–å å°‘æ•°çš„DFUç±»åˆ«ã€‚åœ¨DS1ï¼ˆDFUC2021ï¼‰å’ŒDS2ï¼ˆç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰ï¼‰æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œåœ¨å‡†ç¡®æ€§ã€å¯é æ€§å’Œçµæ´»æ€§æ–¹é¢ï¼ŒConMatFormerè¶…è¶Šäº†æœ€å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰æ¨¡å‹ã€‚åœ¨ä¸€æ¬¡å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†0.8961çš„å‡†ç¡®ç‡å’Œ0.9160çš„ç²¾ç¡®åº¦ï¼Œè¿™æ˜¯å¯¹å½“å‰DFUåˆ†ç±»æ ‡å‡†çš„é‡å¤§æ”¹è¿›ã€‚æ­¤å¤–ï¼Œé€šè¿‡4æŠ˜äº¤å‰éªŒè¯ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°äº†0.9755çš„å‡†ç¡®ç‡ï¼Œæ ‡å‡†åå·®ä»…ä¸º0.0031ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åº”ç”¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•ï¼Œå¦‚Grad-CAMã€Grad-CAM++å’ŒLIMEï¼Œä»¥æŒç»­ç›‘æ§å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºDFUåˆ†ç±»è®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œå¹¶ä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æ··åˆæ³¨æ„åŠ›å˜å‹å™¨æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22743v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºConMatFormerçš„æ–°å‹æ··åˆæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç”¨äºç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰æ£€æµ‹ã€‚è¯¥æ¶æ„ç»“åˆäº†ConvNeXtå—ã€å¤šç§æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—CBAMå’ŒåŒæ³¨æ„åŠ›ç½‘ç»œDANetï¼‰å’Œå˜å‹å™¨æ¨¡å—ï¼Œä»¥æå–æ›´å¥½çš„å±€éƒ¨ç‰¹å¾å¹¶ç†è§£å…¨å±€ä¸Šä¸‹æ–‡ã€‚ä¸ºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œé‡‡ç”¨äº†æ•°æ®å¢å¼ºæ–¹æ³•ã€‚åœ¨DS1ï¼ˆDFUC2021ï¼‰å’ŒDS2ï¼ˆç³–å°¿ç—…è¶³æºƒç–¡DFUï¼‰æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒConMatFormeråœ¨å‡†ç¡®æ€§ã€å¯é æ€§å’Œçµæ´»æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹è¿˜é€šè¿‡4å€äº¤å‰éªŒè¯è¾¾åˆ°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶åº”ç”¨äº†å¯è§£é‡Šçš„çš„äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•æ¥ç›‘æ§å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ConMatFormeræ˜¯ä¸€ç§æ–°å‹çš„æ··åˆæ·±åº¦å­¦ä¹ æ¶æ„ï¼Œé€‚ç”¨äºç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰æ£€æµ‹ã€‚</li>
<li>è¯¥æ¶æ„ç»“åˆäº†å¤šç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬ConvNeXtå—ã€CBAMå’ŒDANetæ³¨æ„åŠ›æœºåˆ¶ä»¥åŠå˜å‹å™¨æ¨¡å—ã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>åœ¨DS1å’ŒDS2æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨ç°è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡4å€äº¤å‰éªŒè¯è¾¾åˆ°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</li>
<li>åº”ç”¨äº†XAIæ–¹æ³•æ¥ç¡®ä¿å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1193cdf5f8e16f751192cca59bb405c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289244&auth_key=1762289244-0-0-b65f264c3c2ef7e275f4a06b0497cc38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a8ccb7cb39621e76a04023dca8dfc99f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289251&auth_key=1762289251-0-0-5c5dc3fde0d1fb444fb7678658902b93&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6c07f3f13f8473876c4de853a2c8a6b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289258&auth_key=1762289258-0-0-625256013444d5cba500dac3ee2b9bcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fae3e214f115b8aff984416b62e916f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289264&auth_key=1762289264-0-0-2f933650dc33095c95d1869fab19dfaa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Cross-view-Localization-and-Synthesis-â€“-Datasets-Challenges-and-Opportunities"><a href="#Cross-view-Localization-and-Synthesis-â€“-Datasets-Challenges-and-Opportunities" class="headerlink" title="Cross-view Localization and Synthesis â€“ Datasets, Challenges and   Opportunities"></a>Cross-view Localization and Synthesis â€“ Datasets, Challenges and   Opportunities</h2><p><strong>Authors:Ningli Xu, Rongjun Qin</strong></p>
<p>Cross-view localization and synthesis are two fundamental tasks in cross-view visual understanding, which deals with cross-view datasets: overhead (satellite or aerial) and ground-level imagery. These tasks have gained increasing attention due to their broad applications in autonomous navigation, urban planning, and augmented reality. Cross-view localization aims to estimate the geographic position of ground-level images based on information provided by overhead imagery while cross-view synthesis seeks to generate ground-level images based on information from the overhead imagery. Both tasks remain challenging due to significant differences in viewing perspective, resolution, and occlusion, which are widely embedded in cross-view datasets. Recent years have witnessed rapid progress driven by the availability of large-scale datasets and novel approaches. Typically, cross-view localization is formulated as an image retrieval problem where ground-level features are matched with tiled overhead images feature, extracted by convolutional neural networks (CNNs) or vision transformers (ViTs) for cross-view feature embedding. Cross-view synthesis, on the other hand, seeks to generate ground-level views based on information from overhead imagery, generally using generative adversarial networks (GANs) or diffusion models. This paper presents a comprehensive survey of advances in cross-view localization and synthesis, reviewing widely used datasets, highlighting key challenges, and providing an organized overview of state-of-the-art techniques. Furthermore, it discusses current limitations, offers comparative analyses, and outlines promising directions for future research. We also include the project page via <a target="_blank" rel="noopener" href="https://github.com/GDAOSU/Awesome-Cross-View-Methods">https://github.com/GDAOSU/Awesome-Cross-View-Methods</a>. </p>
<blockquote>
<p>è·¨è§†å›¾å®šä½ä¸åˆæˆæ˜¯è·¨è§†å›¾è§†è§‰ç†è§£ä¸­çš„ä¸¤ä¸ªåŸºæœ¬ä»»åŠ¡ï¼Œè¯¥é¢†åŸŸå¤„ç†è·¨è§†å›¾æ•°æ®é›†ï¼šä¿¯è§†å›¾ï¼ˆå«æ˜Ÿæˆ–èˆªç©ºï¼‰å’Œåœ°é¢çº§åˆ«å›¾åƒã€‚ç”±äºè¿™äº›ä»»åŠ¡åœ¨è‡ªä¸»å¯¼èˆªã€åŸå¸‚è§„åˆ’å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚è·¨è§†å›¾å®šä½æ—¨åœ¨æ ¹æ®ä¿¯è§†å›¾å›¾åƒæä¾›çš„ä¿¡æ¯ä¼°è®¡åœ°é¢çº§åˆ«å›¾åƒçš„åœ°ç†ä½ç½®ï¼Œè€Œè·¨è§†å›¾åˆæˆåˆ™æ—¨åœ¨åŸºäºä¿¯è§†å›¾å›¾åƒçš„ä¿¡æ¯ç”Ÿæˆåœ°é¢çº§åˆ«å›¾åƒã€‚ç”±äºè·¨è§†å›¾æ•°æ®é›†ä¸­è§†è§’ã€åˆ†è¾¨ç‡å’Œé®æŒ¡ç­‰æ–¹é¢çš„æ˜¾è‘—å·®å¼‚ï¼Œè¿™ä¸¤ä¸ªä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿‘å¹´æ¥ï¼Œç”±äºå¤§è§„æ¨¡æ•°æ®é›†å’Œæ–°é¢–æ–¹æ³•çš„å‡ºç°ï¼Œè¿›å±•è¿…é€Ÿã€‚é€šå¸¸ï¼Œè·¨è§†å›¾å®šä½è¢«åˆ¶å®šä¸ºå›¾åƒæ£€ç´¢é—®é¢˜ï¼Œå…¶ä¸­åœ°é¢ç‰¹å¾ä¼šä¸ä¿¯è§†å›¾å›¾åƒç‰¹å¾è¿›è¡ŒåŒ¹é…ï¼Œè¿™äº›ç‰¹å¾ç”±å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œè·¨è§†å›¾ç‰¹å¾åµŒå…¥æ¥æå–ã€‚å¦ä¸€æ–¹é¢ï¼Œè·¨è§†å›¾åˆæˆåˆ™æ—¨åœ¨åŸºäºä¿¯è§†å›¾å›¾åƒçš„ä¿¡æ¯ç”Ÿæˆåœ°é¢è§†å›¾ï¼Œé€šå¸¸ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æˆ–æ‰©æ•£æ¨¡å‹ã€‚æœ¬æ–‡å¯¹è·¨è§†å›¾å®šä½å’Œåˆæˆçš„è¿›å±•è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œå›é¡¾äº†å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ï¼Œå¼ºè°ƒäº†å…³é”®æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†æœ€æ–°æŠ€æœ¯çš„æœ‰æ¡ç†æ¦‚è¿°ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è®¨è®ºäº†å½“å‰å±€é™æ€§ã€æä¾›äº†æ¯”è¾ƒåˆ†æï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚æˆ‘ä»¬è¿˜é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/GDAOSU/Awesome-Cross-View-Methods%E5%8C%85%E5%90%AB%E9%A1%B9%E7%9B%AE%E9%A1%B5%E9%9D%A2%E3%80%82">https://github.com/GDAOSU/Awesome-Cross-View-MethodsåŒ…å«é¡¹ç›®é¡µé¢ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22736v2">PDF</a> 15 Figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è·¨è§†è§’è§†è§‰ç†è§£çš„ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šè·¨è§†è§’å®šä½å’Œè·¨è§†è§’åˆæˆã€‚ä¸¤ä¸ªä»»åŠ¡éƒ½æ¶‰åŠå¤„ç†ç©ºä¸­ï¼ˆå«æ˜Ÿæˆ–èˆªç©ºï¼‰å’Œåœ°é¢æ°´å¹³çš„å›¾åƒæ•°æ®ã€‚è·¨è§†è§’å®šä½æ—¨åœ¨åŸºäºç©ºä¸­å›¾åƒä¿¡æ¯ä¼°è®¡åœ°é¢å›¾åƒçš„åœ°ç†ä½ç½®ï¼Œè€Œè·¨è§†è§’åˆæˆåˆ™æ—¨åœ¨åŸºäºç©ºä¸­å›¾åƒä¿¡æ¯ç”Ÿæˆåœ°é¢å›¾åƒã€‚ç”±äºæ˜¾è‘—çš„è§†è§’å·®å¼‚ã€åˆ†è¾¨ç‡å·®å¼‚å’Œé®æŒ¡é—®é¢˜ï¼Œè¿™ä¸¤ä¸ªä»»åŠ¡éƒ½é¢ä¸´æŒ‘æˆ˜ã€‚è¿‘å¹´æ¥ï¼Œç”±äºå¤§è§„æ¨¡æ•°æ®é›†çš„å‡ºç°å’Œæ–°æ–¹æ³•çš„æå‡ºï¼Œè¿™ä¸¤ä¸ªé¢†åŸŸéƒ½å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚é€šå¸¸ï¼Œè·¨è§†è§’å®šä½è¢«åˆ¶å®šä¸ºå›¾åƒæ£€ç´¢é—®é¢˜ï¼Œé€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æå–åœ°é¢å’Œç©ºä¸­å›¾åƒçš„ç‰¹å¾è¿›è¡Œè·¨è§†è§’ç‰¹å¾åµŒå…¥åŒ¹é…ã€‚è·¨è§†è§’åˆæˆåˆ™ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æˆ–æ‰©æ•£æ¨¡å‹åŸºäºç©ºä¸­å›¾åƒä¿¡æ¯ç”Ÿæˆåœ°é¢è§†å›¾ã€‚æœ¬æ–‡å¯¹è·¨è§†è§’å®šä½å’Œåˆæˆçš„è¿›å±•è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œä»‹ç»äº†å¸¸ç”¨æ•°æ®é›†ã€å…³é”®æŒ‘æˆ˜ã€æœ€æ–°æŠ€æœ¯ï¼Œå¹¶è®¨è®ºäº†å½“å‰å±€é™æ€§ã€å¯¹æ¯”åˆ†æä»¥åŠæœªæ¥ç ”ç©¶çš„æœ‰å‰é€”æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨è§†è§’è§†è§‰ç†è§£åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šè·¨è§†è§’å®šä½å’Œè·¨è§†è§’åˆæˆã€‚</li>
<li>è·¨è§†è§’å®šä½æ—¨åœ¨åŸºäºç©ºä¸­å›¾åƒä¿¡æ¯ä¼°è®¡åœ°é¢å›¾åƒçš„åœ°ç†ä½ç½®ã€‚</li>
<li>è·¨è§†è§’åˆæˆæ—¨åœ¨åŸºäºç©ºä¸­å›¾åƒä¿¡æ¯ç”Ÿæˆåœ°é¢å›¾åƒã€‚</li>
<li>ä¸¤ä¸ªä»»åŠ¡é¢ä¸´è§†è§’å·®å¼‚ã€åˆ†è¾¨ç‡å·®å¼‚å’Œé®æŒ¡é—®é¢˜çš„æŒ‘æˆ˜ã€‚</li>
<li>è·¨è§†è§’å®šä½é€šå¸¸è¢«åˆ¶å®šä¸ºå›¾åƒæ£€ç´¢é—®é¢˜ï¼Œä½¿ç”¨CNNæˆ–ViTè¿›è¡Œç‰¹å¾åŒ¹é…ã€‚</li>
<li>è·¨è§†è§’åˆæˆä½¿ç”¨GANsæˆ–æ‰©æ•£æ¨¡å‹ç”Ÿæˆåœ°é¢è§†å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e2e15f4bd462a76e632ba7f8cad3193d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289272&auth_key=1762289272-0-0-a237b382548ea912b9d620662850356f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9152fe1d65f0f70cc737b09082dae9c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289279&auth_key=1762289279-0-0-451088052c8f50b78c1e5d551cd5c45d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f70a148096c4f608ef16e0df6a81055~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289286&auth_key=1762289286-0-0-b8ed20a5f0b5649c40a55273e7c9601f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67f80b8c5567312a647bc324244527b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289292&auth_key=1762289292-0-0-b39c880fae1ce3bd75a70d5f352fb631&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fea87ab0ef324ca484facd451c7dcd8d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289299&auth_key=1762289299-0-0-f010afeef567221e8979744078a37daa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Alias-Free-ViT-Fractional-Shift-Invariance-via-Linear-Attention"><a href="#Alias-Free-ViT-Fractional-Shift-Invariance-via-Linear-Attention" class="headerlink" title="Alias-Free ViT: Fractional Shift Invariance via Linear Attention"></a>Alias-Free ViT: Fractional Shift Invariance via Linear Attention</h2><p><strong>Authors:Hagay Michaeli, Daniel Soudry</strong></p>
<p>Transformers have emerged as a competitive alternative to convnets in vision tasks, yet they lack the architectural inductive bias of convnets, which may hinder their potential performance. Specifically, Vision Transformers (ViTs) are not translation-invariant and are more sensitive to minor image translations than standard convnets. Previous studies have shown, however, that convnets are also not perfectly shift-invariant, due to aliasing in downsampling and nonlinear layers. Consequently, anti-aliasing approaches have been proposed to certify convnetsâ€™ translation robustness. Building on this line of work, we propose an Alias-Free ViT, which combines two main components. First, it uses alias-free downsampling and nonlinearities. Second, it uses linear cross-covariance attention that is shift-equivariant to both integer and fractional translations, enabling a shift-invariant global representation. Our model maintains competitive performance in image classification and outperforms similar-sized models in terms of robustness to adversarial translations. </p>
<blockquote>
<p>Transformeråœ¨è§†è§‰ä»»åŠ¡ä¸­å·²æˆä¸ºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆconvnetsï¼‰çš„æœ‰åŠ›ç«äº‰å¯¹æ‰‹ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å·ç§¯ç¥ç»ç½‘ç»œçš„æ¶æ„å½’çº³åè§ï¼Œå¯èƒ½ä¼šé™åˆ¶å…¶æ½œåœ¨æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œè§†è§‰Transformerï¼ˆViTsï¼‰ä¸å…·æœ‰å¹³ç§»ä¸å˜æ€§ï¼Œå¯¹äºè½»å¾®çš„å›¾åƒå¹³ç§»æ¯”æ ‡å‡†å·ç§¯ç¥ç»ç½‘ç»œæ›´æ•æ„Ÿã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç”±äºä¸‹é‡‡æ ·å’Œéçº¿æ€§å±‚ä¸­çš„æ··å ï¼Œå·ç§¯ç¥ç»ç½‘ç»œä¹Ÿå¹¶éå®Œå…¨å¹³ç§»ä¸å˜ã€‚å› æ­¤ï¼Œäººä»¬å·²ç»æå‡ºäº†æŠ—æ··å æ–¹æ³•æ¥éªŒè¯å·ç§¯ç¥ç»ç½‘ç»œçš„å¹³ç§»é²æ£’æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— æ··å ViTï¼Œå®ƒä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼Œå®ƒä½¿ç”¨æ— æ··å çš„ä¸‹é‡‡æ ·å’Œéçº¿æ€§ã€‚å…¶æ¬¡ï¼Œå®ƒé‡‡ç”¨å¯¹æ•´æ•°å’Œåˆ†æ•°å¹³ç§»å…·æœ‰å¹³ç§»ç­‰ä»·æ€§çš„çº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›ï¼Œä»¥å®ç°å¹³ç§»ä¸å˜çš„å…¨å±€è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ä¸­ä¿æŒäº†ç«äº‰åŠ›ï¼Œå¹¶ä¸”åœ¨é¢å¯¹å¯¹æŠ—æ€§å¹³ç§»æ—¶ï¼Œä¼˜äºåŒç±»å¤§å°çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22673v1">PDF</a> Accepted at NeurIPS 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/hmichaeli/alias_free_vit">https://github.com/hmichaeli/alias_free_vit</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Vision Transformersï¼ˆViTsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ä¸å±€é™ï¼Œç‰¹åˆ«æ˜¯å…¶ç›¸è¾ƒäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆconvnetsï¼‰åœ¨å›¾åƒç¿»è¯‘æ–¹é¢çš„æ•æ„Ÿæ€§ã€‚ä¸ºæå‡ViTsçš„ç¿»è¯‘é²æ£’æ€§ï¼Œæœ¬æ–‡æå‡ºäº†æ— åˆ«åViTæ¨¡å‹ï¼Œé€šè¿‡é‡‡ç”¨æ— åˆ«åä¸‹é‡‡æ ·å’Œéçº¿æ€§æ€§ï¼Œä»¥åŠå…·æœ‰æ•´æ•°å’Œåˆ†æ•°ç¿»è¯‘ç­‰å˜çš„çº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†å…¨å±€çš„å¹³ç§»ä¸å˜æ€§è¡¨ç¤ºï¼Œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ä¿æŒäº†ç«äº‰åŠ›ï¼Œå¹¶åœ¨å¯¹æŠ—æ€§ç¿»è¯‘æ–¹é¢ä¼˜äºåŒç±»è§„æ¨¡æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) å·²æˆä¸ºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆconvnetsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„ç«äº‰æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ViTs ç¼ºä¹ convnets çš„æ¶æ„å½’çº³åç½®ï¼Œå¯èƒ½å½±å“å…¶æ€§èƒ½ã€‚</li>
<li>ViTs å¯¹è½»å¾®å›¾åƒç¿»è¯‘æ•æ„Ÿï¼Œç¼ºä¹ç¿»è¯‘ä¸å˜æ€§ã€‚</li>
<li>convnets å¹¶éå®Œå…¨å¹³ç§»ä¸å˜ï¼Œä¸‹é‡‡æ ·å’Œéçº¿æ€§å±‚ä¼šäº§ç”Ÿæ··å ã€‚</li>
<li>ä¸ºæå‡ ViTs çš„ç¿»è¯‘é²æ£’æ€§ï¼Œæå‡ºäº†æ— åˆ«å ViT æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨æ— åˆ«åä¸‹é‡‡æ ·å’Œéçº¿æ€§æ€§ï¼Œä»¥åŠå…·æœ‰å¹³ç§»ç­‰å˜æ€§çš„çº¿æ€§äº¤å‰åæ–¹å·®æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3ba4bb19dad6c8c8f7a5ff8c615ba89f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289307&auth_key=1762289307-0-0-ae077b1e52379ceaf052c31572fa2c22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3101ad4cd013f62cfc7f1cc3b171313a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289314&auth_key=1762289314-0-0-7d57727fa9cd9b9dddf80e562acca33b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-Multitask-System-for-Generating-E-Commerce-Text-Listings-from-Images"><a href="#A-Multimodal-Multitask-System-for-Generating-E-Commerce-Text-Listings-from-Images" class="headerlink" title="A Multimodal, Multitask System for Generating E Commerce Text Listings   from Images"></a>A Multimodal, Multitask System for Generating E Commerce Text Listings   from Images</h2><p><strong>Authors:Nayan Kumar Singh</strong></p>
<p>Manually generating catchy descriptions and names is labor intensive and a slow process for retailers. Although generative AI provides an automation solution in form of Vision to Language Models (VLM), the current VLMs are prone to factual â€œhallucinationsâ€. Siloed, single task models are not only inefficient but also fail to capture interdependent relationships between features. To address these challenges, we propose an end to end, multi task system that generates factually grounded textual listings from a single image. The contributions of this study are two proposals for the model architecture. First, application of multi task learning approach for fine tuning a vision encoder where a single vision backbone is jointly trained on attribute prediction such as color, hemline and neck style and price regression. Second, introduction of a hierarchical generation process where the modelâ€™s own predicted attributes are embedded in a prompt and fed to the text decoder to improve factual consistency. The experiments demonstrate the superiority of this architecture. The multi tasking approach outperforms both the independent price regression, with a 3.6% better R2 Value and attribute classification, with a 6.6% improvement F1 score. Critically, the hierarchical generation process proves highly effective, slashing the factual hallucination rate from 12.7% to 7.1%, a 44.5% relative reduction, compared to a non hierarchical ablation. The hierarchical approach also reduces the latency of the autoregressive text generation process by a factor of 3.5 when compared to direct vision to language model of similar size. One minor caveat is that the model does perform 3.5% worse than direct vision-to-language model on ROUGE-L score. </p>
<blockquote>
<p>æ‰‹åŠ¨ç”Ÿæˆå¼•äººæ³¨ç›®çš„æè¿°å’Œåç§°å¯¹äºé›¶å”®å•†è€Œè¨€æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†å‹ä¸”ç¼“æ…¢çš„è¿‡ç¨‹ã€‚å°½ç®¡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä»¥è§†è§‰åˆ°è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å½¢å¼æä¾›äº†è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œä½†å½“å‰çš„VLMå®¹æ˜“é™·å…¥äº‹å®ä¸Šçš„â€œå¹»è§‰â€ã€‚å­¤ç«‹çš„å•ä»»åŠ¡æ¨¡å‹ä¸ä»…æ•ˆç‡ä½ä¸‹ï¼Œè€Œä¸”æ— æ³•æ•æ‰ç‰¹å¾ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¤šä»»åŠ¡ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯ä»¥ä»å•ä¸ªå›¾åƒç”ŸæˆåŸºäºäº‹å®çš„æ–‡å­—åˆ—è¡¨ã€‚æœ¬ç ”ç©¶æœ‰ä¸¤ä¸ªæ¨¡å‹æ¶æ„çš„ææ¡ˆï¼Œè¿™æ˜¯å…¶è´¡çŒ®ã€‚é¦–å…ˆï¼Œåº”ç”¨å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•å¯¹è§†è§‰ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­å•ä¸ªè§†è§‰éª¨å¹²ç½‘è”åˆè®­ç»ƒå±æ€§é¢„æµ‹ï¼ˆå¦‚é¢œè‰²ã€è£™æ‘†å’Œé¢ˆéƒ¨æ ·å¼ï¼‰å’Œä»·æ ¼å›å½’ã€‚å…¶æ¬¡ï¼Œå¼•å…¥åˆ†å±‚ç”Ÿæˆè¿‡ç¨‹ï¼Œæ¨¡å‹è‡ªèº«é¢„æµ‹çš„å±æ€§åµŒå…¥åˆ°æç¤ºä¸­å¹¶é¦ˆé€ç»™æ–‡æœ¬è§£ç å™¨ï¼Œä»¥æé«˜äº‹å®ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜äº†è¯¥æ¶æ„çš„ä¼˜è¶Šæ€§ã€‚å¤šä»»åŠ¡æ–¹æ³•ä¼˜äºç‹¬ç«‹çš„ä»·æ ¼å›å½’ï¼ŒRÂ²å€¼æé«˜äº†3.6%ï¼Œå±æ€§åˆ†ç±»çš„F1åˆ†æ•°æé«˜äº†6.6%ã€‚å…³é”®çš„æ˜¯ï¼Œåˆ†å±‚ç”Ÿæˆè¿‡ç¨‹è¢«è¯æ˜éå¸¸æœ‰æ•ˆï¼Œå°†äº‹å®å¹»è§‰ç‡ä»12.7%é™ä½åˆ°7.1%ï¼Œç›¸å¯¹å‡å°‘äº†44.5%ï¼Œä¸æ— åˆ†å±‚çš„æ¶ˆèç ”ç©¶ç›¸æ¯”ã€‚ä¸ç±»ä¼¼å¤§å°çš„ç›´æ¥è§†è§‰åˆ°è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œåˆ†å±‚æ–¹æ³•è¿˜å°†è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹çš„å»¶è¿Ÿé™ä½äº†3.5å€ã€‚æœ‰ä¸€ä¸ªå°ç¼ºé™·æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨ROUGE-Låˆ†æ•°ä¸Šæ¯”ç›´æ¥è§†è§‰åˆ°è¯­è¨€æ¨¡å‹å·®äº†3.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21835v1">PDF</a> 24 pages, 10 figures, 11 tables. Code can be found at:   <a target="_blank" rel="noopener" href="https://github.com/SinghNayanKumar/multimodal-product-lister/">https://github.com/SinghNayanKumar/multimodal-product-lister/</a></p>
<p><strong>æ‘˜è¦</strong><br>ä¸€ç§æ–°å‹çš„è§†è§‰åˆ°è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æå‡ºï¼Œæ—¨åœ¨è§£å†³æ‰‹åŠ¨ç”Ÿæˆå¸å¼•äººçš„æè¿°å’Œåç§°å¯¹äºé›¶å”®å•†æ¥è¯´åŠ³åŠ¨å¼ºåº¦å¤§ä¸”è¿‡ç¨‹ç¼“æ…¢çš„é—®é¢˜ã€‚å½“å‰VLMå®¹æ˜“çŠ¯äº‹å®â€œå¹»è§‰â€ï¼Œè€Œå•ç‹¬çš„å•ä¸€ä»»åŠ¡æ¨¡å‹ä¸ä»…æ•ˆç‡ä½ä¸‹ï¼Œè€Œä¸”æ— æ³•æ•æ‰ç‰¹å¾ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¤šä»»åŠ¡ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿä»å•å¼ å›¾åƒç”ŸæˆåŸºäºäº‹å®çš„æ–‡å­—åˆ—è¡¨ã€‚æœ¬ç ”ç©¶çš„è´¡çŒ®åœ¨äºæå‡ºäº†ä¸¤ç§æ¨¡å‹æ¶æ„çš„ææ¡ˆã€‚é¦–å…ˆï¼Œé‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•å¯¹è§†è§‰ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­å•ä¸ªè§†è§‰éª¨å¹²ç½‘è”åˆè®­ç»ƒå±æ€§é¢„æµ‹ï¼ˆå¦‚é¢œè‰²ã€è£™æ‘†å’Œé¢†å£é£æ ¼ï¼‰å’Œä»·æ ¼å›å½’ã€‚å…¶æ¬¡ï¼Œå¼•å…¥å±‚æ¬¡ç”Ÿæˆè¿‡ç¨‹ï¼Œå°†æ¨¡å‹è‡ªèº«é¢„æµ‹çš„å±æ€§åµŒå…¥æç¤ºä¸­å¹¶é¦ˆé€ç»™æ–‡æœ¬è§£ç å™¨ï¼Œä»¥æé«˜äº‹å®ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜äº†è¯¥æ¶æ„çš„ä¼˜è¶Šæ€§ã€‚å¤šä»»åŠ¡æ–¹æ³•ä¼˜äºç‹¬ç«‹çš„ä»·æ ¼å›å½’å’Œå±æ€§åˆ†ç±»ï¼Œå…¶Rå€¼æé«˜äº†3.6%ï¼ŒFå€¼æé«˜äº†6.6%ã€‚ç‰¹åˆ«æ˜¯ï¼Œå±‚æ¬¡ç”Ÿæˆè¿‡ç¨‹è¯æ˜éå¸¸æœ‰æ•ˆï¼Œå°†äº‹å®å¹»è§‰ç‡ä»12.7%é™ä½åˆ°7.1%ï¼Œç›¸å¯¹äºéå±‚æ¬¡ç»“æ„å‡å°‘äº†44.5%ï¼ŒåŒæ—¶å±‚æ¬¡æ–¹æ³•è¿˜å°†è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹çš„å»¶è¿Ÿå‡å°‘äº†3.5å€ã€‚ä½†æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼Œè¯¥æ¨¡å‹åœ¨ROUGE-Lå¾—åˆ†ä¸Šæ¯”ç›´æ¥è§†è§‰åˆ°è¯­è¨€æ¨¡å‹ä½3.5%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰æ‰‹åŠ¨ä¸ºé›¶å”®å•†ç”Ÿæˆå•†å“æè¿°å’Œåç§°æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†ä¸”ä½æ•ˆçš„è¿‡ç¨‹ã€‚</li>
<li>å½“å‰ä½¿ç”¨çš„ç”Ÿæˆæ€§AIå­˜åœ¨äº‹å®â€œå¹»è§‰â€çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šä»»åŠ¡ç³»ç»Ÿæ¶æ„ï¼Œèƒ½å¤Ÿä»å•ä¸€å›¾åƒç”ŸæˆåŸºäºäº‹å®çš„æ–‡å­—æè¿°ã€‚</li>
<li>é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ å’Œå±‚æ¬¡ç”Ÿæˆè¿‡ç¨‹æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¤šä»»åŠ¡æ–¹æ³•ä¼˜åŒ–äº†ä»·æ ¼å›å½’å’Œå±æ€§åˆ†ç±»çš„æ€§èƒ½ã€‚</li>
<li>å±‚æ¬¡ç”Ÿæˆæ–¹æ³•æ˜¾è‘—é™ä½äº†äº‹å®å¹»è§‰ç‡å¹¶åŠ å¿«äº†æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ROUGE-Lå¾—åˆ†æ–¹é¢ç›¸å¯¹ç›´æ¥è§†è§‰åˆ°è¯­è¨€æ¨¡å‹ç•¥æœ‰ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a8c9aa374b8d66e965d8e66ce1ff1442~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289321&auth_key=1762289321-0-0-ccc23b0e04f513b6648ca37b98dda34d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-531c45cc6e98afefb40a0f420a86eb65~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289328&auth_key=1762289328-0-0-c5c76150e5aeca738a618285f4dac8fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-004f247fde29132de98a1b6e162bc7a5~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289334&auth_key=1762289334-0-0-2a12db147ece116f0068e7a3d9bf7811&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-00250813d122795c93e880dcdd28c454~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289342&auth_key=1762289342-0-0-af7a29d2a80c12b9b88e82d0fdca961d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e764e22ebd64788e8119cd103477d358~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289348&auth_key=1762289348-0-0-57c8cdf7a92f4c93b8c3aeacbe66c87b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Bridging-the-gap-to-real-world-language-grounded-visual-concept-learning"><a href="#Bridging-the-gap-to-real-world-language-grounded-visual-concept-learning" class="headerlink" title="Bridging the gap to real-world language-grounded visual concept learning"></a>Bridging the gap to real-world language-grounded visual concept learning</h2><p><strong>Authors:Whie Jung, Semin Kim, Junee Kim, Seunghoon Hong</strong></p>
<p>Human intelligence effortlessly interprets visual scenes along a rich spectrum of semantic dimensions. However, existing approaches to language-grounded visual concept learning are limited to a few predefined primitive axes, such as color and shape, and are typically explored in synthetic datasets. In this work, we propose a scalable framework that adaptively identifies image-related concept axes and grounds visual concepts along these axes in real-world scenes. Leveraging a pretrained vision-language model and our universal prompting strategy, our framework identifies a diverse image-related axes without any prior knowledge. Our universal concept encoder adaptively binds visual features to the discovered axes without introducing additional model parameters for each concept. To ground visual concepts along the discovered axes, we optimize a compositional anchoring objective, which ensures that each axis can be independently manipulated without affecting others. We demonstrate the effectiveness of our framework on subsets of ImageNet, CelebA-HQ, and AFHQ, showcasing superior editing capabilities across diverse real-world concepts that are too varied to be manually predefined. Our method also exhibits strong compositional generalization, outperforming existing visual concept learning and text-based editing methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/whieya/Language-grounded-VCL">https://github.com/whieya/Language-grounded-VCL</a>. </p>
<blockquote>
<p>äººç±»æ™ºèƒ½å¯ä»¥æ¯«ä¸è´¹åŠ›åœ°è§£é‡Šä¸°å¯Œçš„è¯­ä¹‰ç»´åº¦ä¸­çš„è§†è§‰åœºæ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºè¯­è¨€çš„åœ°è§†è§‰æ¦‚å¿µå­¦ä¹ æ–¹æ³•ä»…é™äºä¸€äº›é¢„å®šä¹‰çš„åŸºæœ¬è½´ï¼Œå¦‚é¢œè‰²å’Œå½¢çŠ¶ï¼Œå¹¶ä¸”é€šå¸¸åœ¨åˆæˆæ•°æ®é›†ä¸­è¿›è¡Œæ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥è‡ªé€‚åº”åœ°è¯†åˆ«ä¸å›¾åƒç›¸å…³çš„æ¦‚å¿µè½´ï¼Œå¹¶æ²¿ç€è¿™äº›è½´åœ¨ç°å®åœºæ™¯ä¸­å¯¹è§†è§‰æ¦‚å¿µè¿›è¡Œå®šä½ã€‚æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œæˆ‘ä»¬é€šç”¨çš„æç¤ºç­–ç•¥ï¼Œåœ¨æ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œè¯†åˆ«å‡ºå¤šæ ·åŒ–çš„ä¸å›¾åƒç›¸å…³çš„è½´ã€‚æˆ‘ä»¬çš„é€šç”¨æ¦‚å¿µç¼–ç å™¨å¯ä»¥è‡ªé€‚åº”åœ°å°†è§†è§‰ç‰¹å¾ç»‘å®šåˆ°å‘ç°çš„è½´ä¸Šï¼Œè€Œæ— éœ€ä¸ºæ¯ä¸ªæ¦‚å¿µå¼•å…¥é¢å¤–çš„æ¨¡å‹å‚æ•°ã€‚ä¸ºäº†æ²¿ç€å‘ç°çš„è½´å®šä½è§†è§‰æ¦‚å¿µï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†ä¸€ä¸ªç»„åˆé”šå®šç›®æ ‡ï¼Œç¡®ä¿æ¯ä¸ªè½´éƒ½å¯ä»¥ç‹¬ç«‹æ“ä½œè€Œä¸å½±å“å…¶ä»–è½´ã€‚æˆ‘ä»¬åœ¨ImageNetã€CelebA-HQå’ŒAFHQçš„å­é›†ä¸Šå±•ç¤ºäº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†åœ¨å¤šç§ç°å®æ¦‚å¿µä¸­çš„å“è¶Šç¼–è¾‘èƒ½åŠ›ï¼Œè¿™äº›æ¦‚å¿µå¤ªè¿‡å¤šæ ·è€Œæ— æ³•é¢„å…ˆæ‰‹åŠ¨å®šä¹‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜è¡¨ç°å‡ºå¼ºå¤§çš„ç»„åˆæ³›åŒ–èƒ½åŠ›ï¼Œä¼˜äºç°æœ‰çš„è§†è§‰æ¦‚å¿µå­¦ä¹ æ–¹æ³•å’ŒåŸºäºæ–‡æœ¬çš„ç¼–è¾‘æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/whieya/Language-grounded-VCL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/whieya/Language-grounded-VCLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21412v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°è¯†åˆ«å›¾åƒç›¸å…³çš„æ¦‚å¿µè½´ï¼Œå¹¶åœ¨çœŸå®åœºæ™¯ä¸­å°†è§†è§‰æ¦‚å¿µä¸è¿™äº›è½´ç›¸ç»“åˆã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œé€šç”¨æç¤ºç­–ç•¥ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹å‘ç°å¤šæ ·åŒ–çš„å›¾åƒç›¸å…³è½´ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–ç»„åˆé”šå®šç›®æ ‡ï¼Œç¡®ä¿æ¯ä¸ªè½´èƒ½ç‹¬ç«‹æ“ä½œè€Œä¸ä¼šç›¸äº’å½±å“ã€‚è¯¥æ–¹æ³•åœ¨ImageNetã€CelebA-HQå’ŒAFHQçš„å­é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„ç¼–è¾‘èƒ½åŠ›ï¼Œå±•ç¤ºäº†åœ¨ä¸åŒçœŸå®æ¦‚å¿µä¸Šçš„ä¼˜è¶Šè¡¨ç°ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„ç»„åˆæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°è¯†åˆ«å›¾åƒç›¸å…³çš„æ¦‚å¿µè½´ï¼Œä¸å†å±€é™äºé¢„è®¾çš„åŸå§‹è½´ï¼ˆå¦‚é¢œè‰²å’Œå½¢çŠ¶ï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½åœ¨çœŸå®åœºæ™¯ä¸­å°†è§†è§‰æ¦‚å¿µä¸è¯†åˆ«çš„è½´ç›¸ç»“åˆã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œé€šç”¨æç¤ºç­–ç•¥ï¼Œå¯åœ¨æ— å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹å‘ç°å›¾åƒç›¸å…³è½´ã€‚</li>
<li>é€šç”¨æ¦‚å¿µç¼–ç å™¨èƒ½å¤Ÿè‡ªé€‚åº”åœ°å°†è§†è§‰ç‰¹å¾ç»‘å®šåˆ°å‘ç°çš„è½´ä¸Šï¼Œæ— éœ€ä¸ºæ¯ä¸ªæ¦‚å¿µå¼•å…¥é¢å¤–çš„æ¨¡å‹å‚æ•°ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ç»„åˆé”šå®šç›®æ ‡ï¼Œç¡®ä¿æ¯ä¸ªè½´èƒ½ç‹¬ç«‹æ“ä½œè€Œä¸å½±å“å…¶ä»–è½´ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„ç¼–è¾‘èƒ½åŠ›å’Œå¼ºå¤§çš„ç»„åˆæ³›åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21412">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bc7faa9ab30faa6a55d22e0a26c22169~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289355&auth_key=1762289355-0-0-d128ae43c9d7bbdd3320ba56b26ad185&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed98b398672baa67bef26abfd9488553~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289363&auth_key=1762289363-0-0-3f33ad78823a8ea871bf75cb83a06b89&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78c974fe8784c38b37d2137f2c4974c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289370&auth_key=1762289370-0-0-de3e9c8c699304d3e381509e5afd5371&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Knowledge-Driven-Vision-Language-Model-for-Plexus-Detection-in-Hirschsprungâ€™s-Disease"><a href="#Knowledge-Driven-Vision-Language-Model-for-Plexus-Detection-in-Hirschsprungâ€™s-Disease" class="headerlink" title="Knowledge-Driven Vision-Language Model for Plexus Detection in   Hirschsprungâ€™s Disease"></a>Knowledge-Driven Vision-Language Model for Plexus Detection in   Hirschsprungâ€™s Disease</h2><p><strong>Authors:Youssef Megahed, Atallah Madi, Dina El Demellawy, Adrian D. C. Chan</strong></p>
<p>Hirschsprungâ€™s disease is defined as the congenital absence of ganglion cells in some segment(s) of the colon. The muscle cannot make coordinated movements to propel stool in that section, most commonly leading to obstruction. The diagnosis and treatment for this disease require a clear identification of different region(s) of the myenteric plexus, where ganglion cells should be present, on the microscopic view of the tissue slide. While deep learning approaches, such as Convolutional Neural Networks, have performed very well in this task, they are often treated as black boxes, with minimal understanding gained from them, and may not conform to how a physician makes decisions. In this study, we propose a novel framework that integrates expert-derived textual concepts into a Contrastive Language-Image Pre-training-based vision-language model to guide plexus classification. Using prompts derived from expert sources (e.g., medical textbooks and papers) generated by large language models and reviewed by our team before being encoded with QuiltNet, our approach aligns clinically relevant semantic cues with visual features. Experimental results show that the proposed model demonstrated superior discriminative capability across different classification metrics as it outperformed CNN-based models, including VGG-19, ResNet-18, and ResNet-50; achieving an accuracy of 83.9%, a precision of 86.6%, and a specificity of 87.6%. These findings highlight the potential of multi-modal learning in histopathology and underscore the value of incorporating expert knowledge for more clinically relevant model outputs. </p>
<blockquote>
<p>å¸Œæ–½æ™®æ—æ ¼ç—…è¢«å®šä¹‰ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªç»“è‚ æ®µå…ˆå¤©æ€§ç¼ºä¹ç¥ç»ç»†èƒã€‚è¯¥éƒ¨ä½çš„è‚Œè‚‰æ— æ³•åè°ƒè¿åŠ¨æ¨åŠ¨å¤§ä¾¿ï¼Œæœ€å¸¸è§çš„æƒ…å†µæ˜¯å¯¼è‡´æ¢—é˜»ã€‚å¯¹äºè¿™ç§ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—ï¼Œéœ€è¦åœ¨æ˜¾å¾®é•œä¸‹æ¸…æ¥šåœ°è¯†åˆ«è‚ ç³»è†œä¸›çš„ä¸åŒåŒºåŸŸï¼Œè¿™äº›åŒºåŸŸåº”è¯¥å­˜åœ¨ç¥ç»ç»†èƒã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬é€šå¸¸è¢«è§†ä¸ºé»‘ç®±æ¨¡å‹ï¼Œäººä»¬ä»ä¸­è·å¾—çš„ç†è§£æœ‰é™ï¼Œå¯èƒ½ä¸ç¬¦åˆåŒ»ç”Ÿçš„å†³ç­–æ–¹å¼ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå®ƒå°†ä¸“å®¶è¡ç”Ÿçš„æ–‡æœ¬æ¦‚å¿µæ•´åˆåˆ°åŸºäºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥æŒ‡å¯¼ä¸›çŠ¶ç»„ç»‡çš„åˆ†ç±»ã€‚é€šè¿‡ä½¿ç”¨ä¸“å®¶æ¥æºï¼ˆå¦‚åŒ»å­¦æ•™ç§‘ä¹¦å’Œè®ºæ–‡ï¼‰ç”Ÿæˆçš„æç¤ºï¼Œè¿™äº›æç¤ºç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆï¼Œå¹¶ç”±æˆ‘ä»¬çš„å›¢é˜Ÿåœ¨ç¼–ç å‰è¿›è¡Œå®¡æŸ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†ä¸´åºŠç›¸å…³çš„è¯­ä¹‰çº¿ç´¢ä¸è§†è§‰ç‰¹å¾å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨å¤šç§åˆ†ç±»æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„è¾¨åˆ«èƒ½åŠ›ï¼Œä¼˜äºåŸºäºCNNçš„æ¨¡å‹ï¼ŒåŒ…æ‹¬VGG-19ã€ResNet-18å’ŒResNet-50ï¼›å…¶å‡†ç¡®åº¦è¾¾åˆ°83.9%ï¼Œç²¾ç¡®åº¦è¾¾åˆ°86.6%ï¼Œç‰¹å¼‚åº¦è¾¾åˆ°87.6%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å¤šæ¨¡æ€å­¦ä¹ åœ¨ç»„ç»‡ç—…ç†å­¦ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†èå…¥ä¸“å®¶çŸ¥è¯†å¯¹äºè·å¾—æ›´å…·ä¸´åºŠç›¸å…³æ€§çš„æ¨¡å‹è¾“å‡ºçš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21083v1">PDF</a> Accepted into the ICAAI 2025 - The 9th International Conference on   Advances in Artificial Intelligence</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆä¸“å®¶è¡ç”Ÿæ–‡æœ¬æ¦‚å¿µçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºåŸºäºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œä¸›çŠ¶åˆ†ç±»ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ¥è‡ªä¸“å®¶èµ„æºï¼ˆå¦‚åŒ»å­¦æ•™ç§‘ä¹¦å’Œè®ºæ–‡ï¼‰çš„æç¤ºï¼Œé€šè¿‡QuiltNetç¼–ç ï¼Œå°†ä¸´åºŠç›¸å…³çš„è¯­ä¹‰çº¿ç´¢ä¸è§†è§‰ç‰¹å¾å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§åˆ†ç±»æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„é‰´åˆ«èƒ½åŠ›ï¼Œä¼˜äºåŸºäºCNNçš„æ¨¡å‹ï¼ŒåŒ…æ‹¬VGG-19ã€ResNet-18å’ŒResNet-50ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†83.9%ï¼Œç²¾ç¡®åº¦è¾¾åˆ°äº†86.6%ï¼Œç‰¹å¼‚æ€§è¾¾åˆ°äº†87.6%ã€‚è¿™ä¸ºå¤šæ¨¡æ€å­¦ä¹ åœ¨ç—…ç†å­¦é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ½œåŠ›ï¼Œå¹¶å‡¸æ˜¾äº†èå…¥ä¸“å®¶çŸ¥è¯†å¯¹äºè·å¾—æ›´å…·ä¸´åºŠç›¸å…³æ€§çš„æ¨¡å‹è¾“å‡ºçš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶å®šä¹‰äº†å¸Œæ£®æ–½æ™®æœ—ç—…ä¸ºç»“è‚ éƒ¨åˆ†ç¥ç»å…ƒç¼ºå¤±å¯¼è‡´çš„ç–¾ç—…ï¼Œå½±å“è‚Œè‚‰åè°ƒè¿åŠ¨å¹¶å¯èƒ½å¯¼è‡´æ¢—é˜»ã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•å¦‚å·ç§¯ç¥ç»ç½‘ç»œåœ¨è¯¥ç—…è¯Šæ–­ä¸­å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹é€æ˜åº¦ï¼Œå¯èƒ½ä¸ç¬¦åˆåŒ»ç”Ÿçš„å†³ç­–æ–¹å¼ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆä¸“å®¶çŸ¥è¯†çš„å¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œä¸›çŠ¶åˆ†ç±»ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨æ¥è‡ªä¸“å®¶èµ„æºçš„æç¤ºï¼Œç»“åˆè§†è§‰ç‰¹å¾ï¼Œæé«˜æ¨¡å‹çš„é‰´åˆ«èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨åˆ†ç±»æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–CNNæ¨¡å‹ï¼ŒåŒ…æ‹¬VGG-19ã€ResNet-18å’ŒResNet-50ã€‚</li>
<li>æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°äº†83.9%ï¼Œç²¾ç¡®åº¦è¾¾åˆ°äº†86.6%ï¼Œç‰¹å¼‚æ€§è¾¾åˆ°äº†87.6%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fdedb51f1833f630105473d6f35cd9ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289377&auth_key=1762289377-0-0-a90f3f197b32b08af79a2ba7fd25aeaf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2422f07136042d4a89642bd82c633c55~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289384&auth_key=1762289384-0-0-0b5267ea5ae65fe8fb023a39fe8edc1d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d6dec391e354761058e54e8b72bcd26~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289390&auth_key=1762289390-0-0-0ac1d8074dc656591f135d4ac9f81b37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8506d4b389ddd5f2043e4df040131110~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289397&auth_key=1762289397-0-0-862a84e6b4f0fd6de37662c7a5fbccca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4f5e3df0e4c4e83b0e8eb2b7c220c7a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289406&auth_key=1762289406-0-0-7b1e0f5eadd7aa4f7b66abb615034d87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ImageNet-trained-CNNs-are-not-biased-towards-texture-Revisiting-feature-reliance-through-controlled-suppression"><a href="#ImageNet-trained-CNNs-are-not-biased-towards-texture-Revisiting-feature-reliance-through-controlled-suppression" class="headerlink" title="ImageNet-trained CNNs are not biased towards texture: Revisiting feature   reliance through controlled suppression"></a>ImageNet-trained CNNs are not biased towards texture: Revisiting feature   reliance through controlled suppression</h2><p><strong>Authors:Tom Burgert, Oliver Stoll, Paolo Rota, BegÃ¼m Demir</strong></p>
<p>The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at <a target="_blank" rel="noopener" href="https://github.com/tomburgert/feature-reliance">https://github.com/tomburgert/feature-reliance</a>. </p>
<blockquote>
<p>å‡è®¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æœ¬è´¨ä¸Šå…·æœ‰çº¹ç†åå‘æ€§ï¼Œè¿™ä¸€å‡è®¾åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½±å“äº†æ·±åº¦å­¦ä¹ ä¸­ç‰¹å¾ä½¿ç”¨çš„è®¨è®ºã€‚æˆ‘ä»¬é€šè¿‡é‡æ–°å®¡è§†Geirhosç­‰äººæå‡ºçš„çº¿ç´¢å†²çªå®éªŒä¸­çš„å±€é™æ€§æ¥é‡æ–°æ¢è®¨è¿™ä¸€å‡è®¾ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¢†åŸŸæ— å…³çš„æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æŠ‘åˆ¶å½¢çŠ¶ã€çº¹ç†å’Œé¢œè‰²çº¿ç´¢æ¥é‡åŒ–ç‰¹å¾ä¾èµ–ï¼Œé¿å…äº†å¼ºåˆ¶é€‰æ‹©å†²çªæ‰€å¸¦æ¥çš„æ··æ·†ã€‚é€šè¿‡åœ¨å—æ§æŠ‘åˆ¶æ¡ä»¶ä¸‹å¯¹äººç±»å’Œç¥ç»ç½‘ç»œè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°CNNå¹¶éæœ¬è´¨ä¸Šåå‘äºçº¹ç†ï¼Œè€Œæ˜¯ä¸»è¦ä¾èµ–äºå±€éƒ¨å½¢çŠ¶ç‰¹å¾ã€‚ç„¶è€Œï¼Œé€šè¿‡ç°ä»£è®­ç»ƒç­–ç•¥æˆ–æ¶æ„ï¼ˆConvNeXtã€ViTsï¼‰ï¼Œè¿™ç§ä¾èµ–å¯ä»¥å¤§å¹…åº¦å‡è½»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹è®¡ç®—æœºè§†è§‰ã€åŒ»å­¦æˆåƒå’Œé¥æ„ŸæŠ€æœ¯è¿›è¡Œäº†åˆ†æï¼Œç»“æœæ˜¾ç¤ºä¾èµ–æ¨¡å¼å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ï¼šè®¡ç®—æœºè§†è§‰æ¨¡å‹ä¼˜å…ˆè€ƒè™‘å½¢çŠ¶ï¼ŒåŒ»å­¦æˆåƒæ¨¡å‹å¼ºè°ƒé¢œè‰²ï¼Œé¥æ„Ÿæ¨¡å‹åˆ™è¡¨ç°å‡ºæ›´å¼ºçš„çº¹ç†ä¾èµ–ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tomburgert/feature-reliance%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tomburgert/feature-relianceè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20234v3">PDF</a> Accepted at NeurIPS 2025 (oral)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é‡æ–°å®¡è§†äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„ç‰¹å¾ä½¿ç”¨æ–¹å¼æ˜¯å¦å¤©ç”Ÿåå‘äºçº¹ç†çš„è§‚ç‚¹ã€‚ä½œè€…é€šè¿‡è§£å†³Geirhosç­‰äººåœ¨çº¿ç´¢å†²çªå®éªŒä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªé¢†åŸŸä¸å¯çŸ¥çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç³»ç»Ÿåœ°æŠ‘åˆ¶å½¢çŠ¶ã€çº¹ç†å’Œé¢œè‰²çº¿ç´¢æ¥é‡åŒ–ç‰¹å¾ä¾èµ–ï¼Œé¿å…äº†å¼ºåˆ¶é€‰æ‹©å†²çªæ‰€å¸¦æ¥çš„æ··æ·†ã€‚é€šè¿‡æ§åˆ¶æŠ‘åˆ¶æ¡ä»¶ä¸‹çš„å¯¹äººç±»å’Œç¥ç»ç½‘ç»œçš„è¯„ä¼°å‘ç°ï¼ŒCNNå¹¶éå¤©ç”Ÿåå‘äºçº¹ç†ï¼Œè€Œæ˜¯ä¸»è¦ä¾èµ–äºå±€éƒ¨å½¢çŠ¶ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¿™ç§ä¾èµ–å¯ä»¥é€šè¿‡ç°ä»£è®­ç»ƒç­–ç•¥æˆ–æ¶æ„ï¼ˆå¦‚ConvNeXtå’ŒViTsï¼‰æ¥å¤§å¹…å‡è½»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†è®¡ç®—æœºè§†è§‰ã€åŒ»å­¦æˆåƒå’Œé¥æ„Ÿç­‰ä¸åŒé¢†åŸŸä¸­çš„ä¾èµ–æ¨¡å¼å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½œè€…é‡æ–°è¯„ä¼°äº†CNNæ˜¯å¦å¤©ç”Ÿåå‘äºçº¹ç†çš„è§‚ç‚¹ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ¡†æ¶æ¥é‡åŒ–ç‰¹å¾ä¾èµ–ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç³»ç»Ÿåœ°æŠ‘åˆ¶å½¢çŠ¶ã€çº¹ç†å’Œé¢œè‰²çº¿ç´¢æ¥é¿å…å¼ºåˆ¶é€‰æ‹©å†²çªå¸¦æ¥çš„æ··æ·†ã€‚</li>
<li>é€šè¿‡å®éªŒå‘ç°ï¼ŒCNNä¸»è¦ä¾èµ–äºå±€éƒ¨å½¢çŠ¶ç‰¹å¾ï¼Œè€Œéå¤©ç”Ÿåå‘äºçº¹ç†ã€‚</li>
<li>ç°ä»£è®­ç»ƒç­–ç•¥æˆ–æ¶æ„ï¼ˆå¦‚ConvNeXtå’ŒViTsï¼‰å¯ä»¥å¤§å¹…å‡è½»å¯¹å±€éƒ¨å½¢çŠ¶ç‰¹å¾çš„ä¾èµ–ã€‚</li>
<li>ä¸åŒé¢†åŸŸä¸­çš„æ¨¡å‹ä¾èµ–æ¨¡å¼å­˜åœ¨å·®å¼‚ï¼Œè®¡ç®—æœºè§†è§‰æ¨¡å‹é‡è§†å½¢çŠ¶ï¼ŒåŒ»å­¦æˆåƒæ¨¡å‹å¼ºè°ƒé¢œè‰²ï¼Œè€Œé¥æ„Ÿæ¨¡å‹æ›´ä¾èµ–çº¹ç†ã€‚</li>
<li>ä½œè€…çš„ç ”ç©¶æä¾›äº†å…³äºç‰¹å¾å’Œä¾èµ–æ€§çš„æ–°è§è§£ï¼Œå¯¹æ·±åº¦å­¦ä¹ é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c8dcf76e0f17c135a930b32591b3fa55~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289414&auth_key=1762289414-0-0-e2a9f549e9c11b478e2d50f016778844&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6192fd49d32e11aaa74272ff6edccf34~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289421&auth_key=1762289421-0-0-735921bb5e65cc27ae2b1d4306d25f87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4e394c009733b6cd98245241dc38ac88~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289427&auth_key=1762289427-0-0-5b2b233e38889db7bad6f9bc5885e683&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e4d9d98a2016e9ce91c67861aeb5cd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289434&auth_key=1762289434-0-0-68c51b91610f54ff6d5d779d7d1e7acd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-58ae21870b673b091dbf7a121b76d528~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289440&auth_key=1762289440-0-0-744149fa66cb70070be57f14f4d630df&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="3DViT-GAT-A-Unified-Atlas-Based-3D-Vision-Transformer-and-Graph-Learning-Framework-for-Major-Depressive-Disorder-Detection-Using-Structural-MRI-Data"><a href="#3DViT-GAT-A-Unified-Atlas-Based-3D-Vision-Transformer-and-Graph-Learning-Framework-for-Major-Depressive-Disorder-Detection-Using-Structural-MRI-Data" class="headerlink" title="3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph   Learning Framework for Major Depressive Disorder Detection Using Structural   MRI Data"></a>3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph   Learning Framework for Major Depressive Disorder Detection Using Structural   MRI Data</h2><p><strong>Authors:Nojod M. Alotaibi, Areej M. Alhothali, Manar S. Ali</strong></p>
<p>Major depressive disorder (MDD) is a prevalent mental health condition that negatively impacts both individual well-being and global public health. Automated detection of MDD using structural magnetic resonance imaging (sMRI) and deep learning (DL) methods holds increasing promise for improving diagnostic accuracy and enabling early intervention. Most existing methods employ either voxel-level features or handcrafted regional representations built from predefined brain atlases, limiting their ability to capture complex brain patterns. This paper develops a unified pipeline that utilizes Vision Transformers (ViTs) for extracting 3D region embeddings from sMRI data and Graph Neural Network (GNN) for classification. We explore two strategies for defining regions: (1) an atlas-based approach using predefined structural and functional brain atlases, and (2) an cube-based method by which ViTs are trained directly to identify regions from uniformly extracted 3D patches. Further, cosine similarity graphs are generated to model interregional relationships, and guide GNN-based classification. Extensive experiments were conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of our model. With stratified 10-fold cross-validation, the best model obtained 78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and 78.98% F1-score. Further, atlas-based models consistently outperformed the cube-based approach, highlighting the importance of using domain-specific anatomical priors for MDD detection. </p>
<blockquote>
<p>æŠ‘éƒç—‡æ˜¯ä¸€ç§å¸¸è§çš„å¿ƒç†å¥åº·é—®é¢˜ï¼Œå¯¹ä¸ªäººç¦ç¥‰å’Œå…¨çƒå…¬å…±å«ç”Ÿéƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åˆ©ç”¨ç»“æ„ç£å…±æŒ¯æˆåƒï¼ˆsMRIï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•è¿›è¡ŒæŠ‘éƒç—‡çš„è‡ªåŠ¨åŒ–æ£€æµ‹ï¼Œåœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå®ç°æ—©æœŸå¹²é¢„æ–¹é¢æ˜¾ç¤ºå‡ºè¶Šæ¥è¶Šå¤§çš„æ½œåŠ›ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•è¦ä¹ˆä½¿ç”¨ä½“ç´ çº§ç‰¹å¾ï¼Œè¦ä¹ˆä½¿ç”¨åŸºäºé¢„å®šä¹‰è„‘å›¾è°±çš„æ‰‹å·¥åŒºåŸŸè¡¨ç¤ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰å¤æ‚è„‘æ¨¡å¼çš„èƒ½åŠ›ã€‚æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„æµç¨‹ï¼Œåˆ©ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ä»sMRIæ•°æ®ä¸­æå–ä¸‰ç»´åŒºåŸŸåµŒå…¥ï¼Œå¹¶åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§å®šä¹‰åŒºåŸŸçš„æ–¹æ³•ï¼šï¼ˆ1ï¼‰ä¸€ç§åŸºäºå›¾è°±çš„æ–¹æ³•ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„ç»“æ„å’ŒåŠŸèƒ½è„‘å›¾è°±ï¼›ï¼ˆ2ï¼‰ä¸€ç§åŸºäºç«‹æ–¹ä½“å—çš„æ–¹æ³•ï¼Œç›´æ¥è®­ç»ƒViTsä»å‡åŒ€æå–çš„ä¸‰ç»´å—ä¸­è¯†åˆ«åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¿˜ç”Ÿæˆäº†ä½™å¼¦ç›¸ä¼¼åº¦å›¾æ¥æ¨¡æ‹ŸåŒºåŸŸé—´çš„å…³ç³»ï¼Œå¹¶å¼•å¯¼åŸºäºGNNçš„åˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨REST-meta-MDDæ•°æ®é›†è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆ†å±‚10å€äº¤å‰éªŒè¯ï¼Œæœ€ä½³æ¨¡å‹è·å¾—äº†78.98%çš„å‡†ç¡®ç‡ã€76.54%çš„æ•æ„Ÿæ€§ã€81.58%çš„ç‰¹å¼‚æ€§å’Œç²¾ç¡®åº¦ä»¥åŠ78.98%çš„F1åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒåŸºäºå›¾è°±çš„æ¨¡å‹å§‹ç»ˆä¼˜äºåŸºäºç«‹æ–¹ä½“å—çš„æ–¹æ³•ï¼Œå¼ºè°ƒäº†åœ¨ä½¿ç”¨ç‰¹å®šé¢†åŸŸçš„è§£å‰–å­¦å…ˆéªŒè¿›è¡ŒæŠ‘éƒç—‡æ£€æµ‹æ–¹é¢çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12143v2">PDF</a> 14 pages, 1 figure, 7 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨Vision Transformerå’ŒGraph Neural Networkå¯¹æŠ‘éƒç—‡è¿›è¡Œè‡ªåŠ¨åŒ–æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–sMRIæ•°æ®çš„3DåŒºåŸŸåµŒå…¥å’Œæ„å»ºä½™å¼¦ç›¸ä¼¼æ€§å›¾è¿›è¡Œåˆ†ç±»ï¼Œæ¢ç´¢äº†åŸºäºå›¾è°±çš„æ–¹æ³•å’ŒåŸºäºç«‹æ–¹ä½“æ–¹æ³•ä¸¤ç§å®šä¹‰åŒºåŸŸç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºå›¾è°±çš„æ–¹æ³•è¡¨ç°æ›´ä½³ï¼Œå…¶å‡†ç¡®åº¦è¾¾åˆ°78.98%ï¼Œå¹¶å…·æœ‰æ•æ„Ÿæ€§ã€ç‰¹å¼‚æ€§ã€ç²¾ç¡®åº¦å’ŒF1åˆ†æ•°ç­‰å¤šé¡¹æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶ä¸ºæé«˜æŠ‘éƒç—‡è¯Šæ–­å‡†ç¡®æ€§å’Œå®ç°æ—©æœŸå¹²é¢„æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformerå’ŒGraph Neural Networkè¢«åº”ç”¨äºæŠ‘éƒç—‡çš„è‡ªåŠ¨åŒ–æ£€æµ‹ï¼Œæé«˜äº†è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡æå–sMRIæ•°æ®çš„3DåŒºåŸŸåµŒå…¥è¿›è¡Œåˆ†ç±»ï¼Œæ¢ç´¢äº†åŸºäºå›¾è°±çš„æ–¹æ³•å’ŒåŸºäºç«‹æ–¹ä½“æ–¹æ³•çš„åŒºåŸŸå®šä¹‰ç­–ç•¥ã€‚</li>
<li>å®éªŒåœ¨REST-meta-MDDæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œæ˜¾ç¤ºåŸºäºå›¾è°±çš„æ–¹æ³•è¡¨ç°ä¼˜äºåŸºäºç«‹æ–¹ä½“æ–¹æ³•ã€‚</li>
<li>åŸºäºå›¾è°±çš„æœ€ä½³æ¨¡å‹åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½ï¼ŒåŒ…æ‹¬å‡†ç¡®åº¦78.98%ï¼Œæ•æ„Ÿæ€§76.54%ï¼Œç‰¹å¼‚æ€§81.58%ï¼Œç²¾ç¡®åº¦81.58%ï¼ŒF1åˆ†æ•°78.98%ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨é¢†åŸŸç‰¹å®šçš„è§£å‰–å­¦å…ˆéªŒä¿¡æ¯å¯¹äºæŠ‘éƒç—‡æ£€æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æœ›æ”¹å–„æŠ‘éƒç—‡çš„è¯Šæ–­å‡†ç¡®æ€§å¹¶ä¿ƒè¿›æ—©æœŸå¹²é¢„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-67fd4b3747426df51b519535f10965d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289447&auth_key=1762289447-0-0-4615d7e7935f0262d083befd8a551e7f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ad4db1eb33984e939a0c754ef1116cd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289455&auth_key=1762289455-0-0-95488cede0972d07d864856beb06fadf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4978fca01f291372fb1a1d8f45cfd68c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289461&auth_key=1762289461-0-0-ef7af695165b86f2b9edd99eca3371c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CapRecover-A-Cross-Modality-Feature-Inversion-Attack-Framework-on-Vision-Language-Models"><a href="#CapRecover-A-Cross-Modality-Feature-Inversion-Attack-Framework-on-Vision-Language-Models" class="headerlink" title="CapRecover: A Cross-Modality Feature Inversion Attack Framework on   Vision Language Models"></a>CapRecover: A Cross-Modality Feature Inversion Attack Framework on   Vision Language Models</h2><p><strong>Authors:Kedong Xiu, Sai Qian Zhang</strong></p>
<p>As Vision-Language Models (VLMs) are increasingly deployed in split-DNN configurationsâ€“with visual encoders (e.g., ResNet, ViT) operating on user devices and sending intermediate features to the cloudâ€“there is a growing privacy risk from semantic information leakage. Existing approaches to reconstructing images from these intermediate features often result in blurry, semantically ambiguous images. To directly address semantic leakage, we propose CapRecover, a cross-modality inversion framework that recovers high-level semantic content, such as labels or captions, directly from intermediate features without image reconstruction.   We evaluate CapRecover on multiple datasets and victim models, demonstrating strong performance in semantic recovery. Specifically, CapRecover achieves up to 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from ResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis further reveals that deeper convolutional layers encode significantly more semantic information compared to shallow layers. To mitigate semantic leakage, we introduce a simple yet effective protection method: adding random noise to intermediate features at each layer and removing the noise in the next layer. Experimental results show that this approach prevents semantic leakage without additional training costs. Our code is available at <a target="_blank" rel="noopener" href="https://jus1mple.github.io/Image2CaptionAttack">https://jus1mple.github.io/Image2CaptionAttack</a>. </p>
<blockquote>
<p>éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åˆ†å¸ƒå¼æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰é…ç½®ä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šâ€”â€”è§†è§‰ç¼–ç å™¨ï¼ˆä¾‹å¦‚ResNetã€ViTï¼‰åœ¨ç”¨æˆ·è®¾å¤‡ä¸Šè¿è¡Œå¹¶å°†ä¸­é—´ç‰¹å¾å‘é€åˆ°äº‘ç«¯â€”â€”ç”±æ­¤äº§ç”Ÿçš„è¯­ä¹‰ä¿¡æ¯æ³„éœ²çš„éšç§é£é™©ä¹Ÿè¶Šæ¥è¶Šé«˜ã€‚ç°æœ‰æŠ€æœ¯é€šè¿‡ä¸­é—´ç‰¹å¾é‡å»ºå›¾åƒå¾€å¾€ä¼šå¯¼è‡´æ¨¡ç³Šã€è¯­ä¹‰æ¨¡ç³Šçš„å›¾åƒã€‚ä¸ºäº†ç›´æ¥è§£å†³è¯­ä¹‰æ³„éœ²é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CapCoverç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨æ¨¡æ€åè½¬æ¡†æ¶ï¼Œç›´æ¥ä»ä¸­é—´ç‰¹å¾æ¢å¤é«˜çº§è¯­ä¹‰å†…å®¹ï¼Œå¦‚æ ‡ç­¾æˆ–æ ‡é¢˜ï¼Œè€Œæ— éœ€å›¾åƒé‡å»ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†å’Œç›®æ ‡æ¨¡å‹ä¸Šè¯„ä¼°äº†CapCoverçš„æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨è¯­ä¹‰æ¢å¤æ–¹é¢çš„å¼ºå¤§æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒCapCoveråœ¨CIFAR-10ä¸Šè¾¾åˆ°äº†é«˜è¾¾92.71%çš„Top-1æ ‡ç­¾å‡†ç¡®ç‡ï¼Œå¹¶ä»ResNet50ç‰¹æ€§åœ¨COCO2017ä¸Šç”Ÿæˆäº†æµç•…æ€§è¾ƒé«˜çš„æ ‡é¢˜ï¼ŒROUGE-Lå¾—åˆ†é«˜è¾¾0.52ã€‚æˆ‘ä»¬çš„åˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œä¸æµ…å±‚ç›¸æ¯”ï¼Œæ·±å±‚å·ç§¯å±‚ç¼–ç çš„è¯­ä¹‰ä¿¡æ¯æ›´å¤šã€‚ä¸ºäº†ç¼“è§£è¯­ä¹‰æ³„éœ²é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¿æŠ¤æ–¹æ³•ï¼šå‘æ¯ä¸€å±‚çš„ä¸­é—´ç‰¹å¾æ·»åŠ éšæœºå™ªå£°å¹¶åœ¨ä¸‹ä¸€å±‚å»é™¤å™ªå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€é¢å¤–è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹ï¼Œèƒ½æœ‰æ•ˆåœ°é˜²æ­¢è¯­ä¹‰æ³„éœ²ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://jus1mple.github.io/Image2CaptionAttack%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://jus1mple.github.io/Image2CaptionAttackä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22828v3">PDF</a> 9 pages, accepted by the 2025 ACM Multimedia Conference. Code is   available at <a target="_blank" rel="noopener" href="https://jus1mple.github.io/Image2CaptionAttack">https://jus1mple.github.io/Image2CaptionAttack</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨Vision-Languageæ¨¡å‹ï¼ˆVLMsï¼‰é‡‡ç”¨åˆ†å‰²æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰é…ç½®æ—¶é¢ä¸´çš„è¯­ä¹‰ä¿¡æ¯æ³„éœ²éšç§é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡ä¸­é—´ç‰¹å¾é‡æ„å›¾åƒå¾€å¾€äº§ç”Ÿæ¨¡ç³Šã€è¯­ä¹‰æ¨¡ç³Šçš„å›¾åƒã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºCapCoveræ–¹æ³•ï¼Œç›´æ¥ä»ä¸­é—´ç‰¹å¾æ¢å¤é«˜çº§è¯­ä¹‰å†…å®¹ï¼Œå¦‚æ ‡ç­¾æˆ–æ ‡é¢˜ï¼Œæ— éœ€å›¾åƒé‡æ„ã€‚å®éªŒè¯æ˜ï¼ŒCapCoveråœ¨å¤šæ•°æ®é›†å’Œå—å®³è€…æ¨¡å‹ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¦‚CIFAR-10ä¸Šçš„Top-1æ ‡ç­¾å‡†ç¡®ç‡é«˜è¾¾92.71%ï¼ŒCOCO2017æ•°æ®é›†ä¸Šä»ResNet50ç‰¹å¾ç”Ÿæˆçš„æµç•…æ ‡é¢˜ROUGE-Lå¾—åˆ†è¾¾0.52ã€‚åˆ†ææ˜¾ç¤ºæ·±å±‚å·ç§¯å±‚æ¯”æµ…å±‚åŒ…å«æ›´å¤šè¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºè§£å†³è¯­ä¹‰æ³„éœ²é—®é¢˜ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¿æŠ¤æ–¹æ³•ï¼šå‘ä¸­é—´ç‰¹å¾æ·»åŠ éšæœºå™ªå£°å¹¶åœ¨ä¸‹ä¸€å±‚å»é™¤å™ªå£°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯åœ¨ä¸å¢åŠ è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹é˜²æ­¢è¯­ä¹‰æ³„éœ²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-Language Models (VLMs) in split-DNN configurations face privacy risks due to semantic information leakage.</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡ä¸­é—´ç‰¹å¾é‡æ„å›¾åƒç»“æœæ¨¡ç³Šä¸”è¯­ä¹‰æ¨¡ç³Šã€‚</li>
<li>CapCoveræ–¹æ³•èƒ½ç›´æ¥ä»ä¸­é—´ç‰¹å¾æ¢å¤é«˜çº§è¯­ä¹‰å†…å®¹ï¼Œå¦‚æ ‡ç­¾å’Œæ ‡é¢˜ï¼Œæ— éœ€å›¾åƒé‡æ„ã€‚</li>
<li>CapCoveråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¦‚CIFAR-10çš„Top-1æ ‡ç­¾å‡†ç¡®ç‡å’ŒCOCO2017çš„ROUGE-Lå¾—åˆ†ã€‚</li>
<li>æ·±å±‚å·ç§¯å±‚ç›¸è¾ƒäºæµ…å±‚åŒ…å«æ›´å¤šè¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>ä¸ºè§£å†³è¯­ä¹‰æ³„éœ²é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ·»åŠ éšæœºå™ªå£°è‡³ä¸­é—´ç‰¹å¾çš„ä¿æŠ¤æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-993ad0a884ad8a7f70644ca13cf7fddc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289470&auth_key=1762289470-0-0-0e481e3c8dfad8fa7403677257f32078&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5373b657f947b030f635324c08e90bad~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289477&auth_key=1762289477-0-0-dd19590ea22559c3f9e666c92bbd7c7e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98c1140d1d939b2da763c1030a8294dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289484&auth_key=1762289484-0-0-d093359c83c4bbced96453c09053173c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-45b851fcdfb8ba1724c69bdd9acaf7c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289491&auth_key=1762289491-0-0-a5f1ff86fce9c04582240f4628c9f6f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-80098be9e3650f8b1ff7765f2db1a21b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289498&auth_key=1762289498-0-0-e6c4bf4d68d424f191b790a445821734&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Boosting-Generative-Adversarial-Transferability-with-Self-supervised-Vision-Transformer-Features"><a href="#Boosting-Generative-Adversarial-Transferability-with-Self-supervised-Vision-Transformer-Features" class="headerlink" title="Boosting Generative Adversarial Transferability with Self-supervised   Vision Transformer Features"></a>Boosting Generative Adversarial Transferability with Self-supervised   Vision Transformer Features</h2><p><strong>Authors:Shangbo Wu, Yu-an Tan, Ruinan Ma, Wencong Ma, Dehua Zhu, Yuanzhang Li</strong></p>
<p>The ability of deep neural networks (DNNs) come from extracting and interpreting features from the data provided. By exploiting intermediate features in DNNs instead of relying on hard labels, we craft adversarial perturbation that generalize more effectively, boosting black-box transferability. These features ubiquitously come from supervised learning in previous work. Inspired by the exceptional synergy between self-supervised learning and the Transformer architecture, this paper explores whether exploiting self-supervised Vision Transformer (ViT) representations can improve adversarial transferability. We present dSVA â€“ a generative dual self-supervised ViT features attack, that exploits both global structural features from contrastive learning (CL) and local textural features from masked image modeling (MIM), the self-supervised learning paradigm duo for ViTs. We design a novel generative training framework that incorporates a generator to create black-box adversarial examples, and strategies to train the generator by exploiting joint features and the attention mechanism of self-supervised ViTs. Our findings show that CL and MIM enable ViTs to attend to distinct feature tendencies, which, when exploited in tandem, boast great adversarial generalizability. By disrupting dual deep features distilled by self-supervised ViTs, we are rewarded with remarkable black-box transferability to models of various architectures that outperform state-of-the-arts. Code available at <a target="_blank" rel="noopener" href="https://github.com/spencerwooo/dSVA">https://github.com/spencerwooo/dSVA</a>. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰çš„èƒ½åŠ›æ¥è‡ªäºæå–å’Œè§£é‡Šæ‰€æä¾›æ•°æ®çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä¸æ˜¯ä¾èµ–ç¡¬æ ‡ç­¾ï¼Œè€Œæ˜¯åˆ©ç”¨DNNä¸­çš„ä¸­é—´ç‰¹å¾ï¼Œæ¥åˆ¶é€ æ›´å…·é€šç”¨æ€§çš„å¯¹æŠ—æ‰°åŠ¨ï¼Œä»è€Œæå‡é»‘ç›’è¿ç§»æ€§ã€‚è¿™äº›ç‰¹å¾åœ¨ä»¥å‰çš„å·¥ä½œä¸­æ™®éæ¥è‡ªç›‘ç£å­¦ä¹ ã€‚å—è‡ªç›‘ç£å­¦ä¹ ä¸Transformeræ¶æ„ä¹‹é—´å“è¶ŠååŒçš„å¯å‘ï¼Œæœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨è‡ªç›‘ç£è§†è§‰Transformerï¼ˆViTï¼‰è¡¨ç¤ºæ˜¯å¦å¯ä»¥æé«˜å¯¹æŠ—è¿ç§»æ€§ã€‚æˆ‘ä»¬æå‡ºäº†dSVAâ€”â€”ä¸€ç§ç”Ÿæˆå¼åŒè‡ªç›‘ç£ViTç‰¹å¾æ”»å‡»æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰çš„å…¨å±€ç»“æ„ç‰¹å¾å’Œæ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰çš„å±€éƒ¨çº¹ç†ç‰¹å¾ï¼Œè¿™ä¸¤è€…æ˜¯è‡ªç›‘ç£å­¦ä¹ èŒƒå¼ä¸­ViTçš„åŒç’§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„ç”Ÿæˆå¼è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸€ä¸ªç”Ÿæˆå™¨æ¥åˆ›å»ºé»‘ç›’å¯¹æŠ—æ ·æœ¬ï¼Œä»¥åŠé€šè¿‡åˆ©ç”¨è‡ªç›‘ç£ViTçš„è”åˆç‰¹å¾å’Œæ³¨æ„åŠ›æœºåˆ¶æ¥è®­ç»ƒç”Ÿæˆå™¨çš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒCLå’ŒMIMä½¿ViTèƒ½å¤Ÿå…³æ³¨ä¸åŒçš„ç‰¹å¾å€¾å‘ï¼Œå½“å®ƒä»¬ç»“åˆä½¿ç”¨æ—¶ï¼Œå¯ä»¥å¤§å¤§æé«˜å¯¹æŠ—æ€§çš„é€šç”¨æ€§ã€‚é€šè¿‡ç ´åç”±è‡ªç›‘ç£ViTæç‚¼çš„åŒé‡æ·±åº¦ç‰¹å¾ï¼Œæˆ‘ä»¬åœ¨å„ç§æ¶æ„çš„æ¨¡å‹ä¸Šè·å¾—äº†æ˜¾è‘—çš„é»‘ç›’è¿ç§»æ€§ï¼Œè¶…è¿‡äº†ç°æœ‰æŠ€æœ¯çš„æ°´å¹³ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/spencerwooo/dSVA">é“¾æ¥</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21046v2">PDF</a> 14 pages, 9 figures, accepted at ICCV 2025</p>
<p><strong>Summary</strong><br>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰é€šè¿‡æå–å’Œè§£é‡Šæ•°æ®ç‰¹å¾å®ç°å…¶åŠŸèƒ½ã€‚æœ¬ç ”ç©¶é€šè¿‡åˆ©ç”¨DNNsçš„ä¸­é—´ç‰¹å¾è€Œéä¾èµ–ç¡¬æ ‡ç­¾ï¼Œè®¾è®¡å‡ºæ›´å…·é€šç”¨æ€§çš„å¯¹æŠ—æ‰°åŠ¨ï¼Œæé«˜äº†é»‘ç®±è¿ç§»èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å—è‡ªç›‘ç£å­¦ä¹ ä¸Transformeræ¶æ„é—´å“è¶ŠååŒçš„å¯å‘ï¼Œæ¢ç´¢äº†åˆ©ç”¨è‡ªç›‘ç£Vision Transformerï¼ˆViTï¼‰è¡¨ç¤ºæ˜¯å¦å¯æé«˜å¯¹æŠ—æ€§è¿ç§»èƒ½åŠ›ã€‚æå‡ºdSVAâ€”â€”ä¸€ç§ç”Ÿæˆå¼åŒè‡ªç›‘ç£ViTç‰¹å¾æ”»å‡»æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰çš„å…¨å±€ç»“æ„ç‰¹å¾å’Œæ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰çš„å±€éƒ¨çº¹ç†ç‰¹å¾ï¼Œè¿™æ˜¯ViTçš„è‡ªç›‘ç£å­¦ä¹ èŒƒå¼ç»„åˆã€‚è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆå¼è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå™¨åˆ›å»ºé»‘ç®±å¯¹æŠ—æ ·æœ¬ï¼Œå¹¶åˆ¶å®šäº†é€šè¿‡åˆ©ç”¨è‡ªç›‘ç£ViTçš„è”åˆç‰¹å¾å’Œæ³¨æ„åŠ›æœºåˆ¶æ¥è®­ç»ƒç”Ÿæˆå™¨çš„ç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼ŒCLå’ŒMIMä½¿ViTèƒ½å¤Ÿå…³æ³¨ä¸åŒçš„ç‰¹å¾å€¾å‘ï¼Œå½“ç»“åˆä½¿ç”¨æ—¶ï¼Œå¯å¤§å¤§æé«˜å¯¹æŠ—æ€§çš„ä¸€èˆ¬æ€§ã€‚é€šè¿‡ç ´åè‡ªç›‘ç£ViTæç‚¼çš„åŒé‡æ·±åº¦ç‰¹å¾ï¼Œå®ç°å¯¹å„ç§æ¶æ„æ¨¡å‹çš„å“è¶Šé»‘ç®±è¿ç§»èƒ½åŠ›ï¼Œå¹¶è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰çš„ä¸­é—´ç‰¹å¾ï¼Œé€šè¿‡è®¾è®¡å¯¹æŠ—æ€§æ‰°åŠ¨æé«˜é»‘ç®±è¿ç§»èƒ½åŠ›ã€‚</li>
<li>å—è‡ªç›‘ç£å­¦ä¹ ä¸TransformerååŒå·¥ä½œçš„å¯å‘ï¼Œç ”ç©¶ç„¦ç‚¹è½¬å‘è‡ªç›‘ç£Vision Transformerï¼ˆViTï¼‰ã€‚</li>
<li>æå‡ºdSVAæ–¹æ³•ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰çš„å…¨å±€ç»“æ„ç‰¹å¾å’Œæ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰çš„å±€éƒ¨çº¹ç†ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨äº†æ–°é¢–çš„ç”Ÿæˆå¼è®­ç»ƒæ¡†æ¶ï¼Œèå…¥ç”Ÿæˆå™¨æ¥åˆ›å»ºé»‘ç®±å¯¹æŠ—æ ·æœ¬ã€‚</li>
<li>ç»“åˆCLå’ŒMIMï¼ŒViTèƒ½å¤Ÿå…³æ³¨ä¸åŒçš„ç‰¹å¾å€¾å‘ï¼Œè¿™æœ‰åŠ©äºæé«˜å¯¹æŠ—æ€§çš„ä¸€èˆ¬æ€§ã€‚</li>
<li>é€šè¿‡æ”»å‡»è‡ªç›‘ç£ViTæç‚¼çš„åŒé‡æ·±åº¦ç‰¹å¾ï¼Œå®ç°å“è¶Šçš„é»‘ç®±è¿ç§»èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21046">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c100baa0160670b0ef536af8a6141d0a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289506&auth_key=1762289506-0-0-9615b335abb7749a24e12344a94c8e88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c03fdbc20684633bb3bcb535d52a8f5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289513&auth_key=1762289513-0-0-9a8e387e403583a8b52b99c59a4e4743&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ac21f0a2558dec6459cb5e4e01930f4b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289520&auth_key=1762289520-0-0-29a8320e53b713d1eb158f8483011a69&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c38abe5a68c2217f1de3de936e1c4f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289527&auth_key=1762289527-0-0-9e40c7f2ff3b117d7ea2630c38789aea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ceb9a243b072a674c76832c603a9223e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289535&auth_key=1762289535-0-0-42cb24808d14e05f4b9b3470f0e0935f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="PatchGuard-Adversarially-Robust-Anomaly-Detection-and-Localization-through-Vision-Transformers-and-Pseudo-Anomalies"><a href="#PatchGuard-Adversarially-Robust-Anomaly-Detection-and-Localization-through-Vision-Transformers-and-Pseudo-Anomalies" class="headerlink" title="PatchGuard: Adversarially Robust Anomaly Detection and Localization   through Vision Transformers and Pseudo Anomalies"></a>PatchGuard: Adversarially Robust Anomaly Detection and Localization   through Vision Transformers and Pseudo Anomalies</h2><p><strong>Authors:Mojtaba Nafez, Amirhossein Koochakian, Arad Maleki, Jafar Habibi, Mohammad Hossein Rohban</strong></p>
<p>Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields that demand high reliability, such as medical imaging and industrial monitoring. However, current AD and AL approaches are often susceptible to adversarial attacks due to limitations in training data, which typically include only normal, unlabeled samples. This study introduces PatchGuard, an adversarially robust AD and AL method that incorporates pseudo anomalies with localization masks within a Vision Transformer (ViT)-based architecture to address these vulnerabilities. We begin by examining the essential properties of pseudo anomalies, and follow it by providing theoretical insights into the attention mechanisms required to enhance the adversarial robustness of AD and AL systems. We then present our approach, which leverages Foreground-Aware Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware methods. Our method incorporates these crafted pseudo-anomaly samples into a ViT-based framework, with adversarial training guided by a novel loss function designed to improve model robustness, as supported by our theoretical analysis. Experimental results on well-established industrial and medical datasets demonstrate that PatchGuard significantly outperforms previous methods in adversarial settings, achieving performance gains of $53.2%$ in AD and $68.5%$ in AL, while also maintaining competitive accuracy in non-adversarial settings. The code repository is available at <a target="_blank" rel="noopener" href="https://github.com/rohban-lab/PatchGuard">https://github.com/rohban-lab/PatchGuard</a> . </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰å’Œå¼‚å¸¸å®šä½ï¼ˆALï¼‰åœ¨é«˜å¯é æ€§éœ€æ±‚çš„é¢†åŸŸï¼Œå¦‚åŒ»å­¦æˆåƒå’Œå·¥ä¸šç›‘æ§ä¸­ï¼Œå…·æœ‰è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ•°æ®çš„å±€é™æ€§ï¼Œé€šå¸¸ä»…åŒ…å«æ­£å¸¸æœªæ ‡è®°æ ·æœ¬ï¼Œå½“å‰çš„ADå’ŒALæ–¹æ³•å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å¹²æ‰°ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†PatchGuardï¼Œè¿™æ˜¯ä¸€ç§å¯¹æŠ—æ€§ç¨³å¥çš„ADå’ŒALæ–¹æ³•ï¼Œå®ƒç»“åˆäº†ä¼ªå¼‚å¸¸å’Œå®šä½æ©è†œåœ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¶æ„å†…è§£å†³è¿™äº›æ¼æ´ã€‚æˆ‘ä»¬é¦–å…ˆç ”ç©¶ä¼ªå¼‚å¸¸çš„åŸºæœ¬å±æ€§ï¼Œç„¶åé€šè¿‡ç†è®ºæ´å¯ŸåŠ›æ¢ç©¶å¢å¼ºADå’ŒALç³»ç»Ÿå¯¹æŠ—æ€§ç¨³å¥æ€§çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚ç„¶åæˆ‘ä»¬æå‡ºäº†åˆ©ç”¨å‰æ™¯æ„ŸçŸ¥ä¼ªå¼‚å¸¸çš„æ–¹æ³•æ¥è§£å†³ä»¥å‰å¼‚å¸¸æ„ŸçŸ¥æ–¹æ³•çš„ç¼ºé™·ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¿™äº›ç²¾å¿ƒåˆ¶ä½œçš„ä¼ªå¼‚å¸¸æ ·æœ¬èå…¥åŸºäºViTçš„æ¡†æ¶ä¸­ï¼Œç”±å¯¹æŠ—æ€§è®­ç»ƒå¼•å¯¼çš„æ–°å‹æŸå¤±å‡½æ•°æ”¹è¿›æ¨¡å‹ç¨³å¥æ€§ï¼Œå¹¶ç”±æˆ‘ä»¬çš„ç†è®ºåˆ†ææä¾›æ”¯æŒã€‚åœ¨æˆç†Ÿçš„å·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPatchGuardåœ¨å¯¹æŠ—è®¾ç½®ä¸‹æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œåœ¨ADå’ŒALä¸­çš„æ€§èƒ½åˆ†åˆ«æé«˜äº†53.2ï¼…å’Œ68.5ï¼…ï¼ŒåŒæ—¶åœ¨éå¯¹æŠ—è®¾ç½®ä¸­ä¹Ÿä¿æŒäº†ç«äº‰å‡†ç¡®æ€§ã€‚ä»£ç ä»“åº“å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/rohban-lab/PatchGuard%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/rohban-lab/PatchGuardè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09237v2">PDF</a> Accepted to the Conference on Computer Vision and Pattern Recognition   (CVPR) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PatchGuardï¼Œä¸€ç§é’ˆå¯¹Vision Transformerçš„å¯¹æŠ—æ€§ç¨³å¥å¼‚å¸¸æ£€æµ‹å’Œå®šä½æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡èå…¥ä¼ªå¼‚å¸¸å’Œå®šä½æ©è†œæ¥å…‹æœç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæé«˜å¯¹æŠ—æ€§ç¯å¢ƒä¸‹çš„æ¨¡å‹ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPatchGuardåœ¨å·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ—¢æé«˜äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œåˆå¢å¼ºäº†å¼‚å¸¸å®šä½çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PatchGuardæ˜¯ä¸€ç§åŸºäºVision Transformerçš„å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰å’Œå¼‚å¸¸å®šä½ï¼ˆALï¼‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨ä¼ªå¼‚å¸¸å’Œå®šä½æ©è†œï¼Œèå…¥ViTæ¶æ„ä»¥å¢å¼ºæ¨¡å‹åœ¨å¯¹æŠ—ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>PatchGuardé€šè¿‡èå…¥Foreground-Aware Pseudo-Anomalieså…‹æœäº†ä»¥å¾€å¼‚å¸¸æ„ŸçŸ¥æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†å¯¹æŠ—è®­ç»ƒï¼Œå¹¶é€šè¿‡æ–°å‹æŸå¤±å‡½æ•°æå‡æ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPatchGuardåœ¨ADå’ŒALæ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†53.2%å’Œ68.5%çš„æ€§èƒ½ã€‚</li>
<li>PatchGuardåœ¨éå¯¹æŠ—æ€§ç¯å¢ƒä¸‹ä¹Ÿä¿æŒäº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b87a829774276c53664e4ffbe47c7ca3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289542&auth_key=1762289542-0-0-c4ada79d49b65f1c203e3d2865137f3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bb240ce2885a425462ef83c434ef3e38~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289584&auth_key=1762289584-0-0-5a85df508c0dffc40bee1efe6c573052&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a6c82a47e2ab7cd1a3f2ef76ac14f08~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289590&auth_key=1762289590-0-0-c60893e8b292e24364adc2447a353b32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a02d3c96d6b95aa27098f5356df8a40~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289597&auth_key=1762289597-0-0-58233c92c2cd43e179bb1d9a20d1570e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8a2687c6955bfca0bf3b333ecbd5ee25~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289604&auth_key=1762289604-0-0-5cb913527dd09fa2c5e8d0c0916d29af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Learning-Knowledge-based-Prompts-for-Robust-3D-Mask-Presentation-Attack-Detection"><a href="#Learning-Knowledge-based-Prompts-for-Robust-3D-Mask-Presentation-Attack-Detection" class="headerlink" title="Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack   Detection"></a>Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack   Detection</h2><p><strong>Authors:Fangling Jiang, Qi Li, Bing Liu, Weining Wang, Caifeng Shan, Zhenan Sun, Ming-Hsuan Yang</strong></p>
<p>3D mask presentation attack detection is crucial for protecting face recognition systems against the rising threat of 3D mask attacks. While most existing methods utilize multimodal features or remote photoplethysmography (rPPG) signals to distinguish between real faces and 3D masks, they face significant challenges, such as the high costs associated with multimodal sensors and limited generalization ability. Detection-related text descriptions offer concise, universal information and are cost-effective to obtain. However, the potential of vision-language multimodal features for 3D mask presentation attack detection remains unexplored. In this paper, we propose a novel knowledge-based prompt learning framework to explore the strong generalization capability of vision-language models for 3D mask presentation attack detection. Specifically, our approach incorporates entities and triples from knowledge graphs into the prompt learning process, generating fine-grained, task-specific explicit prompts that effectively harness the knowledge embedded in pre-trained vision-language models. Furthermore, considering different input images may emphasize distinct knowledge graph elements, we introduce a visual-specific knowledge filter based on an attention mechanism to refine relevant elements according to the visual context. Additionally, we leverage causal graph theory insights into the prompt learning process to further enhance the generalization ability of our method. During training, a spurious correlation elimination paradigm is employed, which removes category-irrelevant local image patches using guidance from knowledge-based text features, fostering the learning of generalized causal prompts that align with category-relevant local patches. Experimental results demonstrate that the proposed method achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets. </p>
<blockquote>
<p>ä¸‰ç»´æ©è†œå±•ç¤ºæ”»å‡»æ£€æµ‹å¯¹äºä¿æŠ¤äººè„¸è¯†åˆ«ç³»ç»Ÿå…å—æ—¥ç›Šä¸¥é‡çš„ä¸‰ç»´æ©è†œæ”»å‡»å¨èƒè‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰æ–¹æ³•åˆ©ç”¨å¤šæ¨¡å¼ç‰¹å¾æˆ–è¿œç¨‹å…‰ç”µå®¹ç§¯è„‰ææ³¢æè®°æ³•ï¼ˆrPPGï¼‰ä¿¡å·æ¥åŒºåˆ†çœŸå®äººè„¸å’Œä¸‰ç»´æ©è†œï¼Œä½†å®ƒä»¬é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¦‚å¤šæ¨¡å¼ä¼ æ„Ÿå™¨çš„é«˜æˆæœ¬å’Œæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚æ£€æµ‹ç›¸å…³çš„æ–‡æœ¬æè¿°ç®€æ´æ˜äº†ï¼Œä¿¡æ¯é€šç”¨ä¸”æˆæœ¬ä½å»‰ã€‚ç„¶è€Œï¼Œå…³äºä¸‰ç»´æ©è†œå±•ç¤ºæ”»å‡»æ£€æµ‹ä¸­è§†è§‰è¯­è¨€å¤šæ¨¡å¼ç‰¹å¾çš„ç ”ç©¶æ½œåŠ›å°šæœªè¢«å‘æ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†æç¤ºå­¦ä¹ æ¡†æ¶æ¥æ¢ç´¢é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹å¯¹ä¸‰ç»´æ©è†œå±•ç¤ºæ”»å‡»æ£€æµ‹çš„å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å’Œä¸‰å…ƒç»„èå…¥æç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œç”Ÿæˆç²¾ç»†çš„ä»»åŠ¡ç‰¹å®šæ˜ç¡®æç¤ºï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨åµŒå…¥åœ¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°ä¸åŒçš„è¾“å…¥å›¾åƒå¯èƒ½ä¼šå¼ºè°ƒä¸åŒçš„çŸ¥è¯†å›¾è°±å…ƒç´ ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è§†è§‰ç‰¹å®šçŸ¥è¯†è¿‡æ»¤å™¨æ¥æ ¹æ®è§†è§‰ä¸Šä¸‹æ–‡ç²¾ç‚¼ç›¸å…³å…ƒç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨å› æœå›¾ç†è®ºçš„è§è§£æ¥ä¼˜åŒ–æç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ¶ˆé™¤å¶ç„¶ç›¸å…³æ€§çš„èŒƒå¼ï¼Œè¯¥èŒƒå¼åˆ©ç”¨åŸºäºçŸ¥è¯†çš„æ–‡æœ¬ç‰¹å¾æŒ‡å¯¼å»é™¤ä¸ç±»åˆ«æ— å…³çš„å±€éƒ¨å›¾åƒæ–‘å—ï¼Œä¿ƒè¿›å­¦ä¹ ç¬¦åˆç±»åˆ«ç›¸å…³å±€éƒ¨æ–‘å—çš„é€šç”¨å› æœæç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›ã€æœ€ä½³çš„åœºæ™¯å†…å¤–æ£€æµ‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03610v2">PDF</a> Accepted by TPAMI</p>
<p><strong>Summary</strong><br>åœ¨äººè„¸è¯†åˆ«ç³»ç»Ÿä¸­ï¼Œä¿æŠ¤ç³»ç»Ÿå…å—ä¸‰ç»´å£ç½©æ”»å‡»è‡³å…³é‡è¦ã€‚å½“å‰æ–¹æ³•ä¸»è¦åˆ©ç”¨å¤šæ¨¡æ€ç‰¹å¾æˆ–è¿œç¨‹å…‰ä½“ç§¯æè®°æœ¯ä¿¡å·æ¥åŒºåˆ†çœŸå®äººè„¸å’Œä¸‰ç»´å£ç½©ï¼Œä½†å­˜åœ¨æˆæœ¬é«˜å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºçŸ¥è¯†æç¤ºå­¦ä¹ æ¡†æ¶çš„æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡èå…¥çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å’Œä¸‰å…ƒç»„æ¥ç”Ÿæˆé’ˆå¯¹ä»»åŠ¡çš„æ˜ç¡®æç¤ºï¼Œå¹¶åˆ©ç”¨è§†è§‰ä¸Šä¸‹æ–‡ç›¸å…³çš„çŸ¥è¯†è¿‡æ»¤å™¨æ¥ä¼˜åŒ–ç›¸å…³å…ƒç´ ã€‚æ­¤å¤–ï¼Œç»“åˆå› æœå›¾ç†è®ºï¼Œæé«˜æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†è·¨åœºæ™¯çš„ä¼˜å¼‚æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³äºè¿™æ®µæ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹ä¸æ´å¯Ÿï¼š</p>
<ul>
<li>ä¸‰ç»´å£ç½©æ”»å‡»å¯¹äººè„¸è¯†åˆ«ç³»ç»Ÿçš„å®‰å…¨æ„æˆå¨èƒã€‚</li>
<li>å½“å‰åˆ©ç”¨å¤šæ¨¡æ€ç‰¹å¾å’Œè¿œç¨‹å…‰ä½“ç§¯æè®°æœ¯ä¿¡å·çš„æ£€æµ‹æ–¹æ³•é¢ä¸´é«˜æˆæœ¬å’Œæ³›åŒ–å›°éš¾ç­‰æŒ‘æˆ˜ã€‚</li>
<li>çŸ¥è¯†æç¤ºå­¦ä¹ æ¡†æ¶å¯ç”¨äºæ£€æµ‹ä¸‰ç»´å£ç½©æ”»å‡»ã€‚å®ƒèåˆäº†çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å’Œä¸‰å…ƒç»„ä»¥å¢å¼ºè¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>é’ˆå¯¹è§†è§‰ä¸Šä¸‹æ–‡çš„ä¿¡æ¯å·®å¼‚ï¼Œå¼•å…¥è§†è§‰ç‰¹å®šçŸ¥è¯†è¿‡æ»¤å™¨ä»¥ä¼˜åŒ–ç›¸å…³å…ƒç´ çš„é€‰æ‹©ã€‚</li>
<li>ç»“åˆå› æœå›¾ç†è®ºæ¥æé«˜æ–¹æ³•çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•é€šè¿‡æ¶ˆé™¤å†—ä½™ç›¸å…³æ€§å¹¶ä¾èµ–çŸ¥è¯†åŸºç¡€æ–‡æœ¬ç‰¹å¾ï¼Œæé«˜æ¨¡å‹å­¦ä¹ åˆ°çš„æç¤ºä¸ç±»åˆ«ç›¸å…³çš„å±€éƒ¨åŒºåŸŸå¯¹é½æ•ˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bc77d556a0351ea34baabf014d4c81eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289612&auth_key=1762289612-0-0-adfa82d699dd23b31452c64d9a31958a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa474e8e78dc204be6fbd196b15f2590~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289619&auth_key=1762289619-0-0-ea75a496c9ce122dd27d124efe38165a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c80be2c5bc5682661f1e17a9abb23828~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289625&auth_key=1762289625-0-0-db116fd1ddb08e96d2413f7d1a96933f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-64f2660ddc24a7acbefe511c2894ae05~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289666&auth_key=1762289666-0-0-0dfb04ac1f2be89f4d1d89d5f126265c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6de7ae16795f245ad38b5b90ba985940~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289673&auth_key=1762289673-0-0-49290e128f42caab8981926f578ec03c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Efficient-Remote-Sensing-Change-Detection-with-Change-State-Space-Models"><a href="#Efficient-Remote-Sensing-Change-Detection-with-Change-State-Space-Models" class="headerlink" title="Efficient Remote Sensing Change Detection with Change State Space Models"></a>Efficient Remote Sensing Change Detection with Change State Space Models</h2><p><strong>Authors:Elman Ghazaei, Erchan Aptoula</strong></p>
<p>Despite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Vision Mamba, an architecture based on State Space Models has emerged as an alternative addressing the aforementioned deficiencies and has been already applied to remote sensing change detection, though mostly as a feature extracting backbone. In this article the Change State Space Model is introduced, that has been specifically designed for change detection by focusing on the relevant changes between bi-temporal images, effectively filtering out irrelevant information. By concentrating solely on the changed features, the number of network parameters is reduced, enhancing significantly computational efficiency while maintaining high detection performance and robustness against input degradation. The proposed model has been evaluated via three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based counterparts at a fraction of their computational complexity. The implementation will be made available at <a target="_blank" rel="noopener" href="https://github.com/Elman295/CSSM">https://github.com/Elman295/CSSM</a> upon acceptance. </p>
<blockquote>
<p>å°½ç®¡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ç»å¸¸ç”¨äºå˜åŒ–æ£€æµ‹ï¼Œä½†å®ƒä»¬è¡¨ç°å‡ºäº†ä¸€äº›ä¼—æ‰€å‘¨çŸ¥çš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå‰è€…éš¾ä»¥å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œè€Œåè€…è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„â€œVision Mambaâ€æ¶æ„ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œè§£å†³äº†ä¸Šè¿°ç¼ºé™·ï¼Œå¹¶å·²åº”ç”¨äºé¥æ„Ÿå˜åŒ–æ£€æµ‹ï¼Œä½†å¤§å¤šä½œä¸ºç‰¹å¾æå–çš„éª¨å¹²ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸“ä¸ºå˜åŒ–æ£€æµ‹è€Œè®¾è®¡çš„â€œå˜åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹â€ï¼ˆChange State Space Modelï¼‰ã€‚è¯¥æ¨¡å‹å…³æ³¨åŒæ—¶æ€å›¾åƒä¹‹é—´çš„ç›¸å…³å˜åŒ–ï¼Œæœ‰æ•ˆåœ°è¿‡æ»¤æ‰æ— å…³ä¿¡æ¯ã€‚é€šè¿‡ä¸“æ³¨äºå˜åŒ–ç‰¹å¾ï¼Œå‡å°‘äº†ç½‘ç»œå‚æ•°çš„æ•°é‡ï¼Œåœ¨ä¿æŒé«˜æ£€æµ‹æ€§èƒ½å’Œå¯¹è¾“å…¥é™è§£çš„ç¨³å¥æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨è®¡ç®—å¤æ‚åº¦è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½ä¼˜äºConvNetsã€ViTså’ŒåŸºäºMambaçš„åŒç±»äº§å“ã€‚æ¨¡å‹å®ç°å°†åœ¨æ¥å—åå‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Elman295/CSSM%E3%80%82">https://github.com/Elman295/CSSMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11080v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„Vision Mambaæ¶æ„è§£å†³äº†ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰åœ¨å˜åŒ–æ£€æµ‹ä¸­çš„å±€é™æ€§ã€‚æ–‡ç« æå‡ºäº†ä¸“ä¸ºå˜åŒ–æ£€æµ‹è®¾è®¡çš„â€œå˜åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹â€ï¼ˆCSSMï¼‰ï¼Œé€šè¿‡ä¸“æ³¨äºåŒæ—¶æ€å›¾åƒä¹‹é—´çš„ç›¸å…³å˜åŒ–ï¼Œæœ‰æ•ˆè¿‡æ»¤æ‰æ— å…³ä¿¡æ¯ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡å¹¶ä¿æŒé«˜æ£€æµ‹æ€§èƒ½å’ŒæŠ—è¾“å…¥å¹²æ‰°çš„ç¨³å¥æ€§ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å‡å°‘è®¡ç®—å¤æ‚æ€§çš„åŒæ—¶ï¼Œä¼˜äºConvNetsã€ViTså’ŒåŸºäºMambaçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Mambaæ¶æ„è§£å†³äº†ConvNetså’ŒViTåœ¨å˜åŒ–æ£€æµ‹ä¸­çš„å±€é™æ€§ã€‚</li>
<li>å˜åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆCSSMï¼‰ä¸“æ³¨äºåŒæ—¶æ€å›¾åƒä¹‹é—´çš„ç›¸å…³å˜åŒ–ï¼Œè¿‡æ»¤æ— å…³ä¿¡æ¯ã€‚</li>
<li>CSSMæé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ£€æµ‹æ€§èƒ½å’Œç¨³å¥æ€§ã€‚</li>
<li>CSSMåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºConvNetsã€ViTså’ŒåŸºäºMambaçš„æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å‡å°‘äº†ç½‘ç»œå‚æ•°æ•°é‡ã€‚</li>
<li>æ¨¡å‹å®ç°çš„å¼€æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Elman295/CSSM%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Elman295/CSSMæä¾›ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9f278e7b83e194de68bc913ff4be5d13~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289680&auth_key=1762289680-0-0-81bbfa28c977c76bfe731b6de59e6db4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f9b40de7c3ecb4b836a21d2311b2f12~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289687&auth_key=1762289687-0-0-1a07e350c3c5b27874e88a264b67849c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3651a0399ff5d8189e47a2c311a2735a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289694&auth_key=1762289694-0-0-ab46aac2a6d2ead51cda3c51b47f61e9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b57d1ef8251200a3c9273bc329f2777f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289701&auth_key=1762289701-0-0-3638d9849b3c625e8c14ff7623bfff81&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20ae4981d2c53a0ee3730419b398e6b2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289707&auth_key=1762289707-0-0-97e67854c1a3bf427dc92e9b2a9f36ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d90ad8868425904bb7e2424010de4d3c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289714&auth_key=1762289714-0-0-2c2a4001b316c3505b00d1908b514ebb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a6cfb540c64e4af3af59222760b3917e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289720&auth_key=1762289720-0-0-2adf4300d1a0036a416ab73ba73538e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1bba8cdc856e6be06f06c2d9ae5cf912~resize:0:q75.jpg?source=1f5c5e47&expiration=1762289727&auth_key=1762289727-0-0-53a73629c2fdda617cf7e487750c46db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-66b578a681b895b6cbbe22c109096ff6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762291343&auth_key=1762291343-0-0-2cb4278974fb1999121759ecb2987ce2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Overcoming Prompts Pool Confusion via Parameterized Prompt for   Incremental Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-79e6e73f8c70116953a199dc9cd38815~resize:0:q75.jpg?source=1f5c5e47&expiration=1762287694&auth_key=1762287694-0-0-b1d9885b9f598e763706ba2566b0f7b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  EgoExo-Con Exploring View-Invariant Video Temporal Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
