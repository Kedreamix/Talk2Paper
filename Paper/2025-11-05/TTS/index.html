<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV   Networks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-91f52eeab94e5400651a411296db98a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306748&auth_key=1762306748-0-0-c7334f6ff1cb11b7e0091fb88d442d94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-05-æ›´æ–°"><a href="#2025-11-05-æ›´æ–°" class="headerlink" title="2025-11-05 æ›´æ–°"></a>2025-11-05 æ›´æ–°</h1><h2 id="Two-Timescale-Optimization-Framework-for-IAB-Enabled-Heterogeneous-UAV-Networks"><a href="#Two-Timescale-Optimization-Framework-for-IAB-Enabled-Heterogeneous-UAV-Networks" class="headerlink" title="Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV   Networks"></a>Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV   Networks</h2><p><strong>Authors:Jikang Deng, Hui Zhou, Mohamed-Slim Alouini</strong></p>
<p>In post-disaster scenarios, the rapid deployment of adequate communication infrastructure is essential to support disaster search, rescue, and recovery operations. To achieve this, uncrewed aerial vehicle (UAV) has emerged as a promising solution for emergency communication due to its low cost and deployment flexibility. However, conventional untethered UAV (U-UAV) is constrained by size, weight, and power (SWaP) limitations, making it incapable of maintaining the operation of a macro base station. To address this limitation, we propose a heterogeneous UAV-based framework that integrates tethered UAV (T-UAV) and U-UAVs, where U-UAVs are utilized to enhance the throughput of cell-edge ground user equipments (G-UEs) and guarantee seamless connectivity during G-UEsâ€™ mobility to safe zones. It is noted that the integrated access and backhaul (IAB) technique is adopted to support the wireless backhaul of U-UAVs. Accordingly, we formulate a two-timescale joint user scheduling and trajectory control optimization problem, aiming to maximize the downlink throughput under asymmetric traffic demands and G-UEsâ€™ mobility. To solve the formulated problem, we proposed a two-timescale multi-agent deep deterministic policy gradient (TTS-MADDPG) algorithm based on the centralized training and distributed execution paradigm. Numerical results show that the proposed algorithm outperforms other benchmarks, including the two-timescale multi-agent proximal policy optimization (TTS-MAPPO) algorithm and MADDPG scheduling method, with robust and higher throughput. Specifically, the proposed algorithm obtains up to 12.2% average throughput gain compared to the MADDPG scheduling method. </p>
<blockquote>
<p>åœ¨ç¾ååœºæ™¯ä¸­ï¼Œè¿…é€Ÿéƒ¨ç½²è¶³å¤Ÿçš„é€šä¿¡åŸºç¡€è®¾æ–½å¯¹äºæ”¯æŒç¾éš¾æœç´¢ã€æ•‘æ´å’Œæ¢å¤è¡ŒåŠ¨è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œç”±äºæ— äººæœºï¼ˆUAVï¼‰æˆæœ¬ä½ä¸”éƒ¨ç½²çµæ´»ï¼Œå®ƒå·²æˆä¸ºåº”æ€¥é€šä¿¡çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ— ç»³æ— äººæœºï¼ˆU-UAVï¼‰å—åˆ°å¤§å°ã€é‡é‡å’ŒåŠŸç‡ï¼ˆSWaPï¼‰é™åˆ¶çš„é™åˆ¶ï¼Œæ— æ³•ç»´æŒå®åŸºç«™çš„æ“ä½œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼‚æ„æ— äººæœºçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç³»ç•™æ— äººæœºï¼ˆT-UAVï¼‰å’ŒU-UAVï¼Œå…¶ä¸­U-UAVç”¨äºå¢å¼ºè¾¹ç¼˜åœ°é¢ç”¨æˆ·è®¾å¤‡ï¼ˆG-UEï¼‰çš„ååé‡ï¼Œå¹¶ä¿è¯åœ¨G-UEç§»åŠ¨åˆ°å®‰å…¨åŒºåŸŸæ—¶çš„æ— ç¼è¿æ¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé‡‡ç”¨é›†æˆæ¥å…¥å’Œå›ç¨‹ï¼ˆIABï¼‰æŠ€æœ¯æ¥æ”¯æŒU-UAVçš„æ— çº¿å›ç¨‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€ä¸ªä¸¤æ—¶é—´å°ºåº¦è”åˆç”¨æˆ·è°ƒåº¦å’Œè½¨è¿¹æ§åˆ¶ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨åœ¨å¯¹ç§°äº¤é€šéœ€æ±‚å’ŒG-UEç§»åŠ¨æ€§çš„æƒ…å†µä¸‹æœ€å¤§åŒ–ä¸‹è¡Œé“¾è·¯ååé‡ã€‚ä¸ºäº†è§£å†³æ‰€åˆ¶å®šçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé›†ä¸­è®­ç»ƒå’Œåˆ†å¸ƒå¼æ‰§è¡ŒèŒƒå¼çš„ä¸¤æ—¶é—´å°ºåº¦å¤šæ™ºèƒ½ä½“æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆTTS-MADDPGï¼‰ç®—æ³•ã€‚æ•°å€¼ç»“æœè¡¨æ˜ï¼Œæ‰€æç®—æ³•ä¼˜äºå…¶ä»–åŸºå‡†ç®—æ³•ï¼ŒåŒ…æ‹¬ä¸¤æ—¶é—´å°ºåº¦å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆTTS-MAPPOï¼‰ç®—æ³•å’ŒMADDPGè°ƒåº¦æ–¹æ³•ï¼Œå…·æœ‰æ›´å¼ºå¤§å’Œæ›´é«˜çš„ååé‡ã€‚å…·ä½“æ¥è¯´ï¼Œæ‰€æç®—æ³•ä¸MADDPGè°ƒåº¦æ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡ååé‡æé«˜äº†é«˜è¾¾12.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26578v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¾ååœºæ™¯ä¸­ï¼Œè¿…é€Ÿéƒ¨ç½²è¶³å¤Ÿçš„é€šä¿¡åŸºç¡€è®¾æ–½å¯¹äºæ”¯æŒç¾éš¾æœç´¢ã€æ•‘æ´å’Œæ¢å¤è¡ŒåŠ¨è‡³å…³é‡è¦ã€‚æ— äººæœºå› å…¶ä½æˆæœ¬å’Œéƒ¨ç½²çµæ´»æ€§è€Œæˆä¸ºåº”æ€¥é€šä¿¡çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ— äººæŸç¼šæ— äººæœºï¼ˆU-UAVï¼‰å—åˆ°å¤§å°ã€é‡é‡å’Œç”µåŠ›ï¼ˆSWaPï¼‰çš„é™åˆ¶ï¼Œæ— æ³•ç»´æŒå®åŸºç«™çš„æ“ä½œã€‚ä¸ºè§£å†³æ­¤é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åŸºäºå¼‚æ„æ— äººæœºçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æŸç¼šæ— äººæœºï¼ˆT-UAVï¼‰å’ŒU-UAVsï¼Œåˆ©ç”¨U-UAVså¢å¼ºè¾¹ç¼˜åœ°é¢ç”¨æˆ·è®¾å¤‡ï¼ˆG-UEsï¼‰çš„ååé‡ï¼Œå¹¶ä¿è¯åœ¨G-UEså‘å®‰å…¨åŒºåŸŸç§»åŠ¨æ—¶çš„æ— ç¼è¿æ¥ã€‚é‡‡ç”¨é›†æˆæ¥å…¥å’Œå›ä¼ ï¼ˆIABï¼‰æŠ€æœ¯æ¥æ”¯æŒU-UAVsçš„æ— çº¿å›ä¼ ã€‚æ®æ­¤ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸¤æ—¶é—´å°ºåº¦è”åˆç”¨æˆ·è°ƒåº¦å’Œè½¨è¿¹æ§åˆ¶ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä¸å¯¹ç§°äº¤é€šéœ€æ±‚ä¸‹G-UEsçš„ç§»åŠ¨æ€§ä¸‹çš„ä¸‹è¡Œé“¾è·¯ååé‡ã€‚ä¸ºäº†è§£å†³å»ºç«‹çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé›†ä¸­è®­ç»ƒå’Œåˆ†å¸ƒå¼æ‰§è¡ŒèŒƒå¼çš„ä¸¤æ—¶é—´å°ºåº¦å¤šæ™ºèƒ½ä½“æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆTTS-MADDPGï¼‰ç®—æ³•ã€‚æ•°å€¼ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•ä¼˜äºå…¶ä»–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸¤æ—¶é—´å°ºåº¦å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆTTS-MAPPOï¼‰ç®—æ³•å’Œé©¬åŒ¹æ”¿ç­–æ¢¯åº¦è°ƒåº¦æ–¹æ³•ï¼Œå…·æœ‰ç¨³å¥æ€§å’Œæ›´é«˜çš„ååé‡ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥ç®—æ³•ä¸é©¬åŒ¹æ”¿ç­–æ¢¯åº¦è°ƒåº¦æ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡ååé‡æé«˜äº†é«˜è¾¾12.2%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åœ¨ç¾ååœºæ™¯ä¸­ï¼Œè¿…é€Ÿéƒ¨ç½²é€šä¿¡åŸºç¡€è®¾æ–½å¯¹ç¾éš¾æœç´¢ã€æ•‘æ´å’Œæ¢å¤æ“ä½œè‡³å…³é‡è¦ã€‚</li>
<li>æ— äººæœºå› ä½æˆæœ¬å’Œéƒ¨ç½²çµæ´»æ€§æˆä¸ºåº”æ€¥é€šä¿¡è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä¼ ç»Ÿæ— äººæŸç¼šæ— äººæœºï¼ˆU-UAVï¼‰å—åˆ°SWaPé™åˆ¶ï¼Œæ— æ³•ç»´æŒå®åŸºç«™æ“ä½œã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå¼‚æ„æ— äººæœºçš„æ¡†æ¶ï¼Œç»“åˆæŸç¼šæ— äººæœºï¼ˆT-UAVï¼‰å’ŒU-UAVsï¼Œä¿è¯æ— ç¼è¿æ¥å¹¶å¢å¼ºè¾¹ç¼˜åœ°é¢ç”¨æˆ·è®¾å¤‡çš„ååé‡ã€‚</li>
<li>é‡‡ç”¨é›†æˆæ¥å…¥å’Œå›ä¼ ï¼ˆIABï¼‰æŠ€æœ¯æ”¯æŒU-UAVsçš„æ— çº¿å›ä¼ ã€‚</li>
<li>å»ºç«‹ä¸¤æ—¶é—´å°ºåº¦è”åˆç”¨æˆ·è°ƒåº¦å’Œè½¨è¿¹æ§åˆ¶ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä¸‹è¡Œé“¾è·¯ååé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c3e53886ab0c9eac72a832c4a1bd9bff~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306573&auth_key=1762306573-0-0-bf0307a25be81f0247cf31379ab472ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee3d78102f24ea8a57495cf52a955c12~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306580&auth_key=1762306580-0-0-e36d3c08168987116f4823e31e871078&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0fa9b93c81f9d0ca477a16c872b4c069~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306588&auth_key=1762306588-0-0-2cdee1fef6ea0b867a71f449c36ec209&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c4b33e11167f30f24a2a026446b1563~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306594&auth_key=1762306594-0-0-fc4a9ad0950fdbbaffabec6bb341c08b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SP-MCQA-Evaluating-Intelligibility-of-TTS-Beyond-the-Word-Level"><a href="#SP-MCQA-Evaluating-Intelligibility-of-TTS-Beyond-the-Word-Level" class="headerlink" title="SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level"></a>SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level</h2><p><strong>Authors:Hitomi Jin Ling Tee, Chaoren Wang, Zijie Zhang, Zhizheng Wu</strong></p>
<p>The evaluation of intelligibility for TTS has reached a bottleneck, as existing assessments heavily rely on word-by-word accuracy metrics such as WER, which fail to capture the complexity of real-world speech or reflect human comprehension needs. To address this, we propose Spoken-Passage Multiple-Choice Question Answering, a novel subjective approach evaluating the accuracy of key information in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour news-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal that low WER does not necessarily guarantee high key-information accuracy, exposing a gap between traditional metrics and practical intelligibility. SP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text normalization and phonetic accuracy. This work underscores the urgent need for high-level, more life-like evaluation criteria now that many systems already excel at WER yet may fall short on real-world intelligibility. </p>
<blockquote>
<p>å¯¹äºTTSçš„å¯æ‡‚åº¦è¯„ä¼°å·²ç»é‡åˆ°äº†ç“¶é¢ˆï¼Œå› ä¸ºç°æœ‰çš„è¯„ä¼°ä¸¥é‡ä¾èµ–äºè¯¸å¦‚WERçš„å­—å¯¹å­—å‡†ç¡®ç‡æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡æ— æ³•æ•æ‰ç°å®ä¸–ç•Œä¸­è¯­éŸ³çš„å¤æ‚æ€§æˆ–åæ˜ äººç±»çš„ç†è§£éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Spoken-Passage Multiple-Choice Question Answeringï¼ˆSP-MCQAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸»è§‚æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°åˆæˆè¯­éŸ³ä¸­çš„å…³é”®ä¿¡æ¯å‡†ç¡®æ€§ï¼Œå¹¶å‘å¸ƒäº†SP-MCQA-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºSP-MCQAè¯„ä¼°çš„8.76å°æ—¶æ–°é—»é£æ ¼çš„åŸºå‡†æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä½WERå¹¶ä¸ä¸€å®šä¿è¯é«˜å…³é”®ä¿¡æ¯å‡†ç¡®ç‡ï¼Œè¿™æš´éœ²äº†ä¼ ç»ŸæŒ‡æ ‡ä¸å®é™…å¯æ‡‚åº¦ä¹‹é—´çš„é¸¿æ²Ÿã€‚SP-MCQAè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„æŠ€æœ¯æ¨¡å‹ä»ç„¶ç¼ºä¹ç¨³å¥çš„æ–‡æœ¬å½’ä¸€åŒ–å’Œè¯­éŸ³å‡†ç¡®æ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†é«˜çº§åˆ«ã€æ›´è´´è¿‘ç°å®ç”Ÿæ´»çš„è¯„ä¼°æ ‡å‡†çš„è¿«åˆ‡éœ€æ±‚ï¼Œå› ä¸ºç°åœ¨è®¸å¤šç³»ç»Ÿåœ¨WERä¸Šå·²ç»è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å®é™…å¯æ‡‚åº¦ä¸Šå¯èƒ½ä»æœ‰ä¸è¶³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26190v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºTTSï¼ˆæ–‡æœ¬è½¬è¯­éŸ³ï¼‰çš„å¯æ‡‚åº¦è¯„ä¼°å­˜åœ¨ç“¶é¢ˆï¼Œç°æœ‰çš„è¯„ä¼°æ–¹å¼ä¸»è¦ä¾èµ–äºå­—è¯å‡†ç¡®ç‡ï¼ˆWERï¼‰ç­‰å•ä¸€æŒ‡æ ‡ï¼Œæ— æ³•å…¨é¢åæ˜ çœŸå®è¯­å¢ƒä¸‹çš„è¯­éŸ³å¤æ‚æ€§åŠäººç±»çš„ç†è§£éœ€æ±‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ä¸»è§‚è¯„ä¼°æ–¹æ³•â€”â€”Spoken-Passage Multiple-Choice Question Answeringï¼ˆSP-MCQAï¼‰ï¼Œå¹¶æ®æ­¤æ„å»ºäº†SP-MCQA-Evalæ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œä½WERå¹¶ä¸ç­‰åŒäºé«˜å…³é”®ä¿¡æ¯å‡†ç¡®ç‡ï¼Œå‡¸æ˜¾äº†ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡ä¸å®é™…å¯æ‡‚åº¦ä¹‹é—´çš„é¸¿æ²Ÿã€‚æ­¤å¤–ï¼ŒSP-MCQAæ˜¾ç¤ºå³ä¾¿æœ€å…ˆè¿›çš„æ¨¡å‹ä»é¢ä¸´æ–‡æœ¬å½’ä¸€åŒ–å’Œè¯­éŸ³å‡†ç¡®æ€§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼ºè°ƒï¼Œéšç€è®¸å¤šç³»ç»Ÿå·²åœ¨WERæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå´åœ¨å®é™…å¯æ‡‚åº¦ä¸Šè¡¨ç°æ¬ ä½³ï¼Œç°åœ¨äºŸéœ€æ›´è´´è¿‘å®é™…ç”Ÿæ´»çš„é«˜çº§è¯„ä¼°æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSçš„å¯æ‡‚åº¦è¯„ä¼°å­˜åœ¨ç“¶é¢ˆï¼Œå› ä¸ºç°æœ‰çš„è¯„ä¼°æ–¹æ³•è¿‡äºä¾èµ–å­—è¯å‡†ç¡®ç‡ï¼ˆWERï¼‰æŒ‡æ ‡ï¼Œéš¾ä»¥å…¨é¢åæ˜ çœŸå®è¯­å¢ƒä¸‹çš„è¯­éŸ³å¤æ‚æ€§ã€‚</li>
<li>SP-MCQAæ˜¯ä¸€ç§æ–°å‹çš„TTSè¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºé€‰æ‹©é¢˜æ¥æµ‹è¯•åˆæˆè¯­éŸ³ä¸­çš„å…³é”®ä¿¡æ¯å‡†ç¡®æ€§ã€‚</li>
<li>SP-MCQA-Evalæ•°æ®é›†è¢«å‘å¸ƒç”¨äºSP-MCQAçš„è¯„ä¼°ï¼ŒåŒ…å«8.76å°æ—¶æ–°é—»é£æ ¼çš„æ–‡æœ¬ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œä½WERå¹¶ä¸ä¿è¯é«˜å…³é”®ä¿¡æ¯å‡†ç¡®ç‡ï¼Œè¡¨æ˜ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡ä¸å®é™…å¯æ‡‚åº¦ä¹‹é—´å­˜åœ¨å·®è·ã€‚</li>
<li>SP-MCQAæ­ç¤ºäº†å³ä½¿æ˜¯æœ€å…ˆè¿›çš„TTSæ¨¡å‹åœ¨æ–‡æœ¬å½’ä¸€åŒ–å’Œè¯­éŸ³å‡†ç¡®æ€§æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰çš„TTSç³»ç»Ÿåœ¨çœŸå®ç¯å¢ƒä¸­çš„è¡¨ç°éœ€è¦æ›´é«˜çº§ã€æ›´è´´è¿‘å®é™…ç”Ÿæ´»çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3d91a9f24bd1011d1a4b27b15ac48645~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306602&auth_key=1762306602-0-0-e94906b1caca87b8f527e03600aa295b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7c53e62100ea37029ba655025ef238f8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306610&auth_key=1762306610-0-0-7685ab6085f3c3d37ba298d56cf28349&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f8480097b3ecbb0921f0f59f0659e4c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306616&auth_key=1762306616-0-0-64d82af6444b1f735367f85963077c62&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0a4d7e7b5e2e3f1ee7c6d1fa98c23116~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306623&auth_key=1762306623-0-0-c5661ec10551ebe55cad8088fbe64630&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa057bc212f9cf4f6b1c2286ac6592ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306630&auth_key=1762306630-0-0-9e03c70cc1271c81da244b63d0a8f953&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b4552f45249f0718ca2a3bcef5362c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306636&auth_key=1762306636-0-0-68dad73d530cb43cd7db340f77fff7f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-351b690274e4be8aad5696b0cea2bdec~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306643&auth_key=1762306643-0-0-fa6c44cca9644db8de58ab3c13d1c031&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reasoning-Path-Divergence-A-New-Metric-and-Curation-Strategy-to-Unlock-LLM-Diverse-Thinking"><a href="#Reasoning-Path-Divergence-A-New-Metric-and-Curation-Strategy-to-Unlock-LLM-Diverse-Thinking" class="headerlink" title="Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock   LLM Diverse Thinking"></a>Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock   LLM Diverse Thinking</h2><p><strong>Authors:Feng Ju, Zeyu Qin, Rui Min, Zhitao He, Lingpeng Kong, Yi R. Fung</strong></p>
<p>While Test-Time Scaling (TTS) has proven effective in improving the reasoning ability of large language models (LLMs), low diversity in model outputs often becomes a bottleneck; this is partly caused by the common â€œone problem, one solutionâ€ (1P1S) training practice, which provides a single canonical answer and can push models toward a narrow set of reasoning paths. To address this, we propose a â€œone problem, multiple solutionsâ€ (1PNS) training paradigm that exposes the model to a variety of valid reasoning trajectories and thus increases inference diversity. A core challenge for 1PNS is reliably measuring semantic differences between multi-step chains of thought, so we introduce Reasoning Path Divergence (RPD), a step-level metric that aligns and scores Long Chain-of-Thought solutions to capture differences in intermediate reasoning. Using RPD, we curate maximally diverse solution sets per problem and fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields more varied outputs and higher pass@k, with an average +2.80% gain in pass@16 over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that 1PNS further amplifies the effectiveness of TTS. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/fengjujf/Reasoning-Path-Divergence">https://github.com/fengjujf/Reasoning-Path-Divergence</a> . </p>
<blockquote>
<p>å°½ç®¡æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å·²è¯æ˜å…¶æœ‰æ•ˆæ€§ï¼Œä½†æ¨¡å‹è¾“å‡ºçš„ä½å¤šæ ·æ€§å¸¸å¸¸æˆä¸ºç“¶é¢ˆï¼›è¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯ç”±äºå¸¸è§çš„â€œä¸€ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªè§£å†³æ–¹æ¡ˆâ€ï¼ˆ1P1Sï¼‰çš„è®­ç»ƒå®è·µå¯¼è‡´çš„ï¼Œå®ƒæä¾›äº†å•ä¸€çš„è§„èŒƒç­”æ¡ˆï¼Œå¹¶å¯èƒ½ä½¿æ¨¡å‹èµ°å‘ç‹­çª„çš„æ¨ç†è·¯å¾„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä¸€ä¸ªé—®é¢˜ï¼Œå¤šç§è§£å†³æ–¹æ¡ˆâ€ï¼ˆ1PNSï¼‰çš„è®­ç»ƒèŒƒå¼ï¼Œè¯¥èŒƒå¼ä½¿æ¨¡å‹æ¥è§¦åˆ°å„ç§æœ‰æ•ˆçš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œå¢åŠ äº†æ¨ç†çš„å¤šæ ·æ€§ã€‚å¯¹äº1PNSçš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºå¯é åœ°è¡¡é‡å¤šæ­¥éª¤æ€ç»´é“¾ä¹‹é—´çš„è¯­ä¹‰å·®å¼‚ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†æ¨ç†è·¯å¾„å‘æ•£åº¦ï¼ˆRPDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ­¥éª¤çº§æŒ‡æ ‡ï¼Œç”¨äºå¯¹é½å’Œè¯„ä¼°é•¿æ€ç»´é“¾è§£å†³æ–¹æ¡ˆï¼Œä»¥æ•æ‰ä¸­é—´æ¨ç†ä¸­çš„å·®å¼‚ã€‚ä½¿ç”¨RPDï¼Œæˆ‘ä»¬é’ˆå¯¹æ¯ä¸ªé—®é¢˜åˆ›å»ºæœ€å¤§å¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆé›†ï¼Œå¹¶å¾®è°ƒQwen3-4B-Baseã€‚å®éªŒè¡¨æ˜ï¼ŒRPDé€‰æ‹©çš„è®­ç»ƒäº§ç”Ÿäº†æ›´å¤šæ ·åŒ–çš„è¾“å‡ºå’Œæ›´é«˜çš„pass@kï¼Œåœ¨å¼ºå¤§çš„1P1SåŸºå‡†æµ‹è¯•ä¸Šï¼Œpass@16å¹³å‡æé«˜äº†+2.80%ï¼Œåœ¨AIME24ä¸Šæé«˜äº†+4.99%ï¼Œè¿™è¡¨æ˜1PNSè¿›ä¸€æ­¥æ”¾å¤§äº†TTSçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fengjujf/Reasoning-Path-Divergence%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/fengjujf/Reasoning-Path-Divergenceä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.26122v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œä¸€ä¸ªé—®é¢˜ï¼Œå¤šç§è§£å†³æ–¹æ¡ˆâ€ï¼ˆ1PNSï¼‰çš„è®­ç»ƒèŒƒå¼ï¼Œä»¥è§£å†³æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ—¶é‡åˆ°çš„æ¨¡å‹è¾“å‡ºå¤šæ ·æ€§ä½çš„é—®é¢˜ã€‚è¯¥èŒƒå¼é€šè¿‡æš´éœ²æ¨¡å‹äºå¤šç§æœ‰æ•ˆçš„æ¨ç†è·¯å¾„æ¥å¢åŠ æ¨ç†å¤šæ ·æ€§ã€‚ä¸ºåº”å¯¹1PNSçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå³æµ‹é‡å¤šæ­¥æ¨ç†è·¯å¾„ä¹‹é—´çš„è¯­ä¹‰å·®å¼‚ï¼Œæ–‡ç« å¼•å…¥äº†Reasoning Path Divergenceï¼ˆRPDï¼‰è¿™ä¸€é€æ­¥åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºæ•æ‰ä¸­é—´æ¨ç†çš„å·®å¼‚ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨RPDé€‰æ‹©çš„è®­ç»ƒäº§å‡ºæ›´å¤šæ ·åŒ–çš„è¾“å‡ºï¼Œå¹¶åœ¨pass@kä¸Šå–å¾—æ›´é«˜çš„æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ—¶é¢ä¸´æ¨¡å‹è¾“å‡ºå¤šæ ·æ€§ä½çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰â€œä¸€ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªè§£å†³æ–¹æ¡ˆâ€ï¼ˆ1P1Sï¼‰çš„è®­ç»ƒå®è·µæ˜¯å¯¼è‡´è¿™ä¸€é—®é¢˜çš„éƒ¨åˆ†åŸå› ã€‚</li>
<li>æå‡ºâ€œä¸€ä¸ªé—®é¢˜ï¼Œå¤šç§è§£å†³æ–¹æ¡ˆâ€ï¼ˆ1PNSï¼‰çš„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡æš´éœ²æ¨¡å‹äºå¤šç§æœ‰æ•ˆçš„æ¨ç†è·¯å¾„æ¥å¢åŠ æ¨ç†å¤šæ ·æ€§ã€‚</li>
<li>å¼•å…¥Reasoning Path Divergenceï¼ˆRPDï¼‰è¿™ä¸€é€æ­¥åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºåœ¨å¤šç§è§£å†³æ–¹æ¡ˆä¸­æµ‹é‡è¯­ä¹‰å·®å¼‚ã€‚</li>
<li>ä½¿ç”¨RPDé€‰æ‹©çš„è®­ç»ƒäº§å‡ºæ›´å¤šæ ·åŒ–çš„è¾“å‡ºï¼Œå¹¶åœ¨pass@kæŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿçš„1P1Sè®­ç»ƒåŸºå‡†ï¼Œä½¿ç”¨RPDè®­ç»ƒçš„æ¨¡å‹åœ¨pass@16ä¸Šå¹³å‡æå‡äº†+2.80%çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.26122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bca80543f6f9d4202a55f0281f334de2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306650&auth_key=1762306650-0-0-d3880c5d9f35106db46b5638d47f75ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c874c3c53d5d66b067672a105db3568~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306657&auth_key=1762306657-0-0-17c7a7b3bb5d97a11211cecc6fa8890e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5281530bd79777c75ea5bca5adfeaf6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306664&auth_key=1762306664-0-0-350e096b8467b245fac668fc6f332fb9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Role-of-Verifiers-in-Test-Time-Scaling-for-Legal-Reasoning-Tasks"><a href="#Evaluating-the-Role-of-Verifiers-in-Test-Time-Scaling-for-Legal-Reasoning-Tasks" class="headerlink" title="Evaluating the Role of Verifiers in Test-Time Scaling for Legal   Reasoning Tasks"></a>Evaluating the Role of Verifiers in Test-Time Scaling for Legal   Reasoning Tasks</h2><p><strong>Authors:Davide Romano, Jonathan Schwarz, Daniele GiofrÃ©</strong></p>
<p>Test-time scaling (TTS) techniques can improve the performance of large language models (LLMs) at the expense of additional computation and latency. While TTS has proven effective in formal domains such as mathematics and programming, its value in argumentative domains such as law remains underexplored. We present an empirical study of verifier-based TTS methods for legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7 reward models, we evaluate both outcome-level (Best-of-$N$) and process-level (tree search) verification under realistic low-$N$ budgets. Our analysis systematically investigates how verifier utility is affected by key properties such as domain specialization, model size, and supervision type (process-supervised PRMs vs. outcome-only ORMs), even when applied across different roles. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æŠ€æœ¯å¯ä»¥åœ¨ç‰ºç‰²è®¡ç®—é‡å’Œå»¶è¿Ÿçš„æƒ…å†µä¸‹æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚è™½ç„¶TTSåœ¨æ­£å¼é¢†åŸŸï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å…¶å¯¹æ³•å¾‹ç­‰è®ºè¯é¢†åŸŸçš„ä»·å€¼å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬å¯¹åŸºäºéªŒè¯å™¨çš„TTSæ–¹æ³•åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ³•å¾‹å¤šé¡¹é€‰æ‹©é¢˜é—®ç­”ï¼ˆMCQAï¼‰è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚ä½¿ç”¨7ä¸ªå¥–åŠ±æ¨¡å‹å®¶æ—ï¼Œæˆ‘ä»¬åœ¨ç°å®çš„ä½Né¢„ç®—ä¸‹è¯„ä¼°äº†ç»“æœçº§ï¼ˆBest-of-$N$ï¼‰å’Œè¿‡ç¨‹çº§ï¼ˆæ ‘æœç´¢ï¼‰çš„éªŒè¯ã€‚æˆ‘ä»¬çš„åˆ†æç³»ç»Ÿåœ°ç ”ç©¶äº†éªŒè¯å™¨æ•ˆç”¨å¦‚ä½•å—åˆ°é¢†åŸŸä¸“ä¸šåŒ–ã€æ¨¡å‹å¤§å°å’Œç›‘ç£ç±»å‹ï¼ˆè¿‡ç¨‹ç›‘ç£çš„PRMsä¸ä»…ç»“æœçš„ORMsï¼‰ç­‰å…³é”®å±æ€§çš„å½±å“ï¼Œå³ä½¿è¿™äº›å±æ€§åœ¨ä¸åŒçš„è§’è‰²ä¸­åº”ç”¨ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25623v2">PDF</a> Accepted to EMNLP - NLLP Workshop</p>
<p><strong>Summary</strong></p>
<p>æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰æŠ€æœ¯å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œä½†ä¼šå¸¦æ¥é¢å¤–çš„è®¡ç®—å’Œå»¶è¿Ÿæˆæœ¬ã€‚å°½ç®¡TTSåœ¨æ­£å¼é¢†åŸŸï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å…¶åœ¨è®ºè¯é¢†åŸŸï¼ˆå¦‚æ³•å¾‹ï¼‰çš„ä»·å€¼ä»è¢«ä½ä¼°ã€‚æœ¬æ–‡å®è¯ç ”ç©¶äº†åŸºäºéªŒè¯å™¨çš„TTSæ–¹æ³•åœ¨æ³•å¾‹å¤šé¡¹é€‰æ‹©é¢˜é—®ç­”ï¼ˆMCQAï¼‰ä¸­çš„åº”ç”¨ï¼Œæ¶µç›–äº†äº”ä¸ªåŸºå‡†æµ‹è¯•ã€‚é€šè¿‡å®¶æ—ä¸­7ç§å¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ç°å®çš„ä½Né¢„ç®—ä¸‹è¯„ä¼°äº†ç»“æœçº§åˆ«ï¼ˆBest-of-Nï¼‰å’Œè¿‡ç¨‹çº§åˆ«ï¼ˆæ ‘æœç´¢ï¼‰çš„éªŒè¯ã€‚æˆ‘ä»¬çš„åˆ†æç³»ç»Ÿåœ°ç ”ç©¶äº†éªŒè¯å™¨æ•ˆç”¨å¦‚ä½•å—åˆ°é¢†åŸŸä¸“ä¸šåŒ–ã€æ¨¡å‹å¤§å°å’Œç›‘ç£ç±»å‹ï¼ˆè¿‡ç¨‹ç›‘ç£çš„PRMsä¸ä»…ç»“æœçš„ORMsï¼‰ç­‰å…³é”®å±æ€§çš„å½±å“ï¼Œå³ä½¿åœ¨ä¸åŒçš„è§’è‰²ä¸­åº”ç”¨ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSæŠ€æœ¯å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­£å¼é¢†åŸŸçš„æ€§èƒ½ï¼Œä½†åœ¨æ³•å¾‹ç­‰è®ºè¯é¢†åŸŸçš„ä»·å€¼å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶äº†åŸºäºéªŒè¯å™¨çš„TTSæ–¹æ³•åœ¨æ³•å¾‹å¤šé¡¹é€‰æ‹©é¢˜é—®ç­”ï¼ˆMCQAï¼‰çš„åº”ç”¨æ•ˆæœã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†7ç§å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°éªŒè¯å™¨åœ¨ç»“æœçº§åˆ«å’Œè¿‡ç¨‹çº§åˆ«çš„è¡¨ç°ã€‚</li>
<li>åœ¨ä½Né¢„ç®—çš„ç°å®æ¡ä»¶ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç ”ç©¶äº†éªŒè¯å™¨æ•ˆç”¨å—åˆ°çš„å…³é”®å±æ€§ï¼ŒåŒ…æ‹¬é¢†åŸŸä¸“ä¸šåŒ–ã€æ¨¡å‹å¤§å°å’Œä¸åŒçš„ç›‘ç£ç±»å‹ã€‚</li>
<li>ç ”ç©¶å‘ç°éªŒè¯å™¨æ•ˆç”¨å—åˆ°é¢†åŸŸä¸“ä¸šåŒ–ç­‰å› ç´ çš„å½±å“ï¼Œå³ä½¿æ˜¯åº”ç”¨äºä¸åŒçš„è§’è‰²ä¹Ÿä¾ç„¶å¦‚æ­¤ã€‚</li>
<li>æµ‹è¯•å‘ç°ä¸åŒå¥–åŠ±æ¨¡å‹å¯¹éªŒè¯å™¨æ€§èƒ½çš„å½±å“æ˜¾è‘—ï¼Œè¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ca2dc96d9b7b0fd21241d4b37c080f37~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306671&auth_key=1762306671-0-0-2136d06f46855ee19cb088f0de787534&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c21a403c00bde8f3872047795fae5397~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306679&auth_key=1762306679-0-0-cb17ccedc1c6d46a08a230fe712697b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eed5b341d8a1bbb74e57ca3e04915e0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306685&auth_key=1762306685-0-0-2fdcfe3e9ab9da71da0080aaa3664773&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6cba8cafcdb49cb47cfd6069cc0ac405~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306692&auth_key=1762306692-0-0-0b202f4bb2e1f8fc1752a8ed7021f31a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-53d42079c208cde4bdd659709beb965e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306700&auth_key=1762306700-0-0-93c245ef2e60e176ee0d47ecef8eaebc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-37ae5c3b31f08b655569168fed6ce311~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306706&auth_key=1762306706-0-0-ddfbd5955c3c5e1bbfafdd9d8de0a81c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-457f67f653cb44cb31ba861adab9be69~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306713&auth_key=1762306713-0-0-c5b834da5660e18bc8dce8cfd5665616&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bayesian-Speech-synthesizers-Can-Learn-from-Multiple-Teachers"><a href="#Bayesian-Speech-synthesizers-Can-Learn-from-Multiple-Teachers" class="headerlink" title="Bayesian Speech synthesizers Can Learn from Multiple Teachers"></a>Bayesian Speech synthesizers Can Learn from Multiple Teachers</h2><p><strong>Authors:Ziyang Zhang, Yifan Gao, Xuenan Xu,  Baoxiangli, Wen Wu, Chao Zhang</strong></p>
<p>Codec-based text-to-speech (TTS) models have recently gained traction for their efficiency and strong performance in voice cloning. However, codec-based TTS faces limitations due to the challenges of pretraining robust speech codecs and the quality degradation introduced by quantization errors. Emerging evidence suggests that continuous-valued generative models can alleviate these issues and serve as a promising alternative. Yet, effectively modelling diverse speech patterns and developing reliable sampling strategies for continuous-valued autoregressive (AR) TTS remains underexplored. In this work, we propose BELLE, Bayesian evidential learning with language modelling for TTS, a novel continuous-valued AR framework that directly predicts mel-spectrograms from textual input. BELLE treats each mel-spectrogram frame as a Gaussian distribution sampled from a learned hyper distribution, enabling principled uncertainty estimation, particularly in scenarios with parallel data (i.e., one text-audio prompt paired with multiple speech samples). To obtain such data, diverse speech samples are synthesized using multiple pre-trained TTS models given the same text-audio prompts, which are distilled into BELLE via Bayesian evidential learning. Experimental results indicate that BELLE demonstrates highly competitive performance compared with the current best open-source TTS models, even though BELLE is trained on a large amount of synthetic data and uses only approximately one-tenth of their training data. Audio samples generated by BELLE are available at <a target="_blank" rel="noopener" href="https://belletts.github.io/Belle/">https://belletts.github.io/Belle/</a>. The code, checkpoints, and synthetic data will be released after the paper is accepted. </p>
<blockquote>
<p>åŸºäºç¼–ç å™¨çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ç”±äºå…¶æ•ˆç‡å’Œåœ¨è¯­éŸ³å…‹éš†ä¸­çš„å‡ºè‰²è¡¨ç°è€Œæœ€è¿‘å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼ŒåŸºäºç¼–ç å™¨çš„TTSé¢ä¸´ç€ç”±äºé¢„è®­ç»ƒç¨³å¥è¯­éŸ³ç¼–ç å™¨çš„æŒ‘æˆ˜ä»¥åŠé‡åŒ–è¯¯å·®å¼•å…¥çš„è´¨é‡ä¸‹é™é—®é¢˜ã€‚æœ€æ–°è¯æ®è¡¨æ˜ï¼Œè¿ç»­å€¼ç”Ÿæˆæ¨¡å‹å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œå¹¶ä½œä¸ºæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¯¹äºåŸºäºè¿ç»­å€¼çš„è‡ªå›å½’ï¼ˆARï¼‰TTSï¼Œæœ‰æ•ˆå»ºæ¨¡å¤šæ ·çš„è¯­éŸ³æ¨¡å¼å’Œå‘å±•å¯é çš„é‡‡æ ·ç­–ç•¥ä»ç„¶ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BELLEï¼Œä¸€ä¸ªç”¨äºTTSçš„åŸºäºè´å¶æ–¯è¯æ®å­¦ä¹ çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è¿ç»­å€¼ARæ¡†æ¶ï¼Œå¯ç›´æ¥ä»æ–‡æœ¬è¾“å…¥é¢„æµ‹æ¢…å°”é¢‘è°±å›¾ã€‚BELLEå°†æ¯ä¸ªæ¢…å°”é¢‘è°±å›¾å¸§è§†ä¸ºä»å­¦ä¹ åˆ°çš„è¶…åˆ†å¸ƒé‡‡æ ·çš„é«˜æ–¯åˆ†å¸ƒï¼Œè¿™æœ‰åŠ©äºè¿›è¡Œæœ‰åŸåˆ™çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰å¹¶è¡Œæ•°æ®çš„åœºæ™¯ä¸­ï¼ˆå³ä¸€ä¸ªæ–‡æœ¬-éŸ³é¢‘æç¤ºä¸å¤šä¸ªè¯­éŸ³æ ·æœ¬é…å¯¹ï¼‰ã€‚ä¸ºäº†è·å¾—æ­¤ç±»æ•°æ®ï¼Œä½¿ç”¨å¤šä¸ªé¢„è®­ç»ƒçš„TTSæ¨¡å‹å¯¹ç›¸åŒçš„æ–‡æœ¬-éŸ³é¢‘æç¤ºè¿›è¡Œè¯­éŸ³æ ·æœ¬åˆæˆï¼Œç„¶åé€šè¿‡è´å¶æ–¯è¯æ®å­¦ä¹ å°†çŸ¥è¯†è’¸é¦åˆ°BELLEä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡BELLEæ˜¯åœ¨å¤§é‡åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨äº†å½“å‰æœ€ä½³å¼€æºTTSæ¨¡å‹ååˆ†ä¹‹ä¸€å·¦å³çš„æ•°æ®é‡ï¼Œä½†å®ƒä»ç„¶è¡¨ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚BELLEç”Ÿæˆçš„éŸ³é¢‘æ ·æœ¬å¯åœ¨[<a target="_blank" rel="noopener" href="https://belletts.github.io/Belle/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82%E8%AE%BA%E6%96%87%E8%A2%AB%E6%8E%A5%E5%8F%97%E5%90%8E%EF%BC%8C%E4%BB%A3%E7%A0%81%E3%80%81%E6%A3%80%E6%9F%A5%E7%82%B9%E5%92%8C%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E5%B0%86%E5%8F%91%E5%B8%83%E3%80%82">https://belletts.github.io/Belle/]ä¸Šæ‰¾åˆ°ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œä»£ç ã€æ£€æŸ¥ç‚¹å’Œåˆæˆæ•°æ®å°†å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24372v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºç¼–ç å™¨çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å› å…¶æ•ˆç‡åŠå…¶åœ¨è¯­éŸ³å…‹éš†ä¸­çš„å‡ºè‰²è¡¨ç°è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºé¢„è®­ç»ƒç¨³å¥è¯­éŸ³ç¼–ç å™¨çš„æŒ‘æˆ˜ä»¥åŠé‡åŒ–è¯¯å·®å¼•èµ·çš„è´¨é‡ä¸‹é™ï¼ŒåŸºäºç¼–ç å™¨çš„TTSé¢ä¸´é™åˆ¶ã€‚æ–°å…´è¯æ®è¡¨æ˜ï¼Œè¿ç»­å€¼ç”Ÿæˆæ¨¡å‹å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜å¹¶æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¯¹äºè¿ç»­å€¼è‡ªå›å½’ï¼ˆARï¼‰TTSï¼Œæœ‰æ•ˆå»ºæ¨¡å¤šæ ·çš„è¯­éŸ³æ¨¡å¼å’Œå¼€å‘å¯é çš„é‡‡æ ·ç­–ç•¥ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BELLEï¼Œä¸€ç§ç”¨äºTTSçš„åŸºäºè´å¶æ–¯è¯æ®å­¦ä¹ ä¸è¯­è¨€å»ºæ¨¡çš„è¿ç»­å€¼ARæ¡†æ¶ã€‚BELLEç›´æ¥ä»æ–‡æœ¬è¾“å…¥é¢„æµ‹æ¢…å°”é¢‘è°±å›¾ã€‚BELLEå°†æ¯ä¸ªæ¢…å°”é¢‘è°±å›¾å¸§è§†ä¸ºä»å­¦ä¹ åˆ°çš„è¶…åˆ†å¸ƒä¸­æå–çš„é«˜æ–¯åˆ†å¸ƒæ ·æœ¬ï¼Œè¿™åœ¨å¹¶è¡Œæ•°æ®åœºæ™¯ä¸­èƒ½å¤Ÿå®ç°åŸåˆ™æ€§çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆå³ä¸€ä¸ªæ–‡æœ¬-éŸ³é¢‘æç¤ºä¸å¤šä¸ªè¯­éŸ³æ ·æœ¬é…å¯¹ï¼‰ã€‚ä¸ºäº†è·å–æ­¤ç±»æ•°æ®ï¼Œä½¿ç”¨å¤šä¸ªé¢„è®­ç»ƒçš„TTSæ¨¡å‹å¯¹ç›¸åŒçš„æ–‡æœ¬-éŸ³é¢‘æç¤ºè¿›è¡Œåˆæˆå¤šæ ·çš„è¯­éŸ³æ ·æœ¬ï¼Œç„¶åé€šè¿‡è´å¶æ–¯è¯æ®å­¦ä¹ è’¸é¦åˆ°BELLEä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®ä»…ä¸ºå½“å‰æœ€ä½³å¼€æºTTSæ¨¡å‹çš„ååˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹ï¼ŒBELLEä¹Ÿè¡¨ç°å‡ºæå…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚BELLEç”Ÿæˆçš„éŸ³é¢‘æ ·æœ¬å¯åœ¨[<a target="_blank" rel="noopener" href="https://belletts.github.io/Belle/]%E8%AE%BF%E9%97%AE%E3%80%82%E8%AE%BA%E6%96%87%E8%A2%AB%E6%8E%A5%E5%8F%97%E5%90%8E%EF%BC%8C%E5%B0%86%E5%8F%91%E5%B8%83%E4%BB%A3%E7%A0%81%E3%80%81%E6%A3%80%E6%9F%A5%E7%82%B9%E5%92%8C%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E3%80%82">https://belletts.github.io/Belle/]è®¿é—®ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œå°†å‘å¸ƒä»£ç ã€æ£€æŸ¥ç‚¹å’Œåˆæˆæ•°æ®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¼–ç å™¨çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å› å…¶æ•ˆç‡å’Œè¯­éŸ³å…‹éš†æ€§èƒ½è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>åŸºäºç¼–ç å™¨çš„TTSé¢ä¸´é¢„è®­ç»ƒç¨³å¥è¯­éŸ³ç¼–ç å™¨çš„æŒ‘æˆ˜å’Œé‡åŒ–è¯¯å·®å¼•èµ·çš„è´¨é‡ä¸‹é™é—®é¢˜ã€‚</li>
<li>è¿ç»­å€¼ç”Ÿæˆæ¨¡å‹ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆæ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„è¿ç»­å€¼ARæ¡†æ¶BELLEï¼Œå…¶èƒ½ç›´æ¥é¢„æµ‹æ¢…å°”é¢‘è°±å›¾å¹¶å¤„ç†æ–‡æœ¬è¾“å…¥ã€‚</li>
<li>BELLEé€šè¿‡é‡‡ç”¨è´å¶æ–¯è¯æ®å­¦ä¹ ä»æ–‡æœ¬-éŸ³é¢‘æç¤ºä¸­åˆæˆå¤šæ ·çš„è¯­éŸ³æ ·æœ¬ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒBELLEçš„æ€§èƒ½ä¸å½“å‰æœ€ä½³å¼€æºTTSæ¨¡å‹ç›¸æ¯”æå…·ç«äº‰åŠ›ï¼Œå°½ç®¡å…¶ä½¿ç”¨äº†è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c797c87d532e723a0f0307d7ecebdb39~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306720&auth_key=1762306720-0-0-515532b3c9ee1681da6bc94daf22ed49&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e919df3607282f1c4bfed036b9e11d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306727&auth_key=1762306727-0-0-3f8d19d6a1ecf27ec55022bcf9db58b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SoulX-Podcast-Towards-Realistic-Long-form-Podcasts-with-Dialectal-and-Paralinguistic-Diversity"><a href="#SoulX-Podcast-Towards-Realistic-Long-form-Podcasts-with-Dialectal-and-Paralinguistic-Diversity" class="headerlink" title="SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and   Paralinguistic Diversity"></a>SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and   Paralinguistic Diversity</h2><p><strong>Authors:Hanke Xie, Haopeng Lin, Wenxiao Cao, Dake Guo, Wenjie Tian, Jun Wu, Hanlin Wen, Ruixuan Shang, Hongmei Liu, Zhiqi Jiang, Yuepeng Jiang, Wenxi Chen, Ruiqi Yan, Jiale Qian, Yichao Yan, Shunshun Yin, Ming Tao, Xie Chen, Lei Xie, Xinsheng Wang</strong></p>
<p>Recent advances in text-to-speech (TTS) synthesis have significantly improved speech expressiveness and naturalness. However, most existing systems are tailored for single-speaker synthesis and fall short in generating coherent multi-speaker conversational speech. This technical report presents SoulX-Podcast, a system designed for podcast-style multi-turn, multi-speaker dialogic speech generation, while also achieving state-of-the-art performance in conventional TTS tasks.   To meet the higher naturalness demands of multi-turn spoken dialogue, SoulX-Podcast integrates a range of paralinguistic controls and supports both Mandarin and English, as well as several Chinese dialects, including Sichuanese, Henanese, and Cantonese, enabling more personalized podcast-style speech generation. Experimental results demonstrate that SoulX-Podcast can continuously produce over 90 minutes of conversation with stable speaker timbre and smooth speaker transitions. Moreover, speakers exhibit contextually adaptive prosody, reflecting natural rhythm and intonation changes as dialogues progress. Across multiple evaluation metrics, SoulX-Podcast achieves state-of-the-art performance in both monologue TTS and multi-turn conversational speech synthesis. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæŠ€æœ¯çš„è¿›å±•æå¤§åœ°æé«˜äº†è¯­éŸ³çš„è¡¨æƒ…è¾¾å’Œè‡ªç„¶åº¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç³»ç»Ÿéƒ½æ˜¯é’ˆå¯¹å•æ¼”è®²è€…åˆæˆçš„ï¼Œè€Œåœ¨ç”Ÿæˆè¿è´¯çš„å¤šæ¼”è®²è€…å¯¹è¯è¯­éŸ³æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†SoulX-Podcastç³»ç»Ÿï¼Œå®ƒæ˜¯ä¸“ä¸ºæ’­å®¢é£æ ¼çš„å¤šè½®å¤šæ¼”è®²è€…å¯¹è¯è¯­éŸ³ç”Ÿæˆè€Œè®¾è®¡çš„ï¼ŒåŒæ—¶åœ¨å¸¸è§„TTSä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†æ»¡è¶³å¤šè½®å¯¹è¯è¯­éŸ³æ›´é«˜çš„è‡ªç„¶åº¦è¦æ±‚ï¼ŒSoulX-Podcasté›†æˆäº†å„ç§å‰¯è¯­è¨€æ§åˆ¶ï¼Œå¹¶æ”¯æŒæ™®é€šè¯å’Œè‹±è¯­ï¼Œä»¥åŠå¤šç§ä¸­æ–‡æ–¹è¨€ï¼ŒåŒ…æ‹¬å››å·è¯ã€æ¹–å—è¯å’Œç²¤è¯­ï¼Œä»è€Œå®ç°æ›´åŠ ä¸ªæ€§åŒ–çš„æ’­å®¢é£æ ¼è¯­éŸ³ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSoulX-Podcastèƒ½å¤Ÿè¿ç»­è¾“å‡ºè¶…è¿‡90åˆ†é’Ÿçš„å¯¹è¯ï¼Œè¯­éŸ³çš„æ¼”è®²è€…éŸ³è‰²ç¨³å®šï¼Œè¯­éŸ³è¿‡æ¸¡æµç•…ã€‚æ­¤å¤–ï¼Œæ¼”è®²è€…çš„éŸµå¾‹èƒ½å¤Ÿé€‚åº”è¯¥ä¸Šä¸‹æ–‡ï¼Œåæ˜ å¯¹è¯è¿›è¡Œä¸­çš„è‡ªç„¶èŠ‚å¥å’Œè¯­è°ƒå˜åŒ–ã€‚åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šï¼ŒSoulX-Podcaståœ¨ç‹¬ç™½TTSå’Œå¤šè½®å¯¹è¯è¯­éŸ³åˆæˆä¸­éƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23541v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤šäººå¯¹è¯å½¢å¼çš„Podcastçš„TTSåˆæˆç³»ç»Ÿâ€”â€”SoulX-Podcastã€‚å®ƒä¸ä»…å®ç°äº†ä¼ ç»ŸTTSä»»åŠ¡çš„æœ€æ–°æ€§èƒ½ï¼Œè¿˜å¯ä»¥è¿›è¡Œå¤šäººå¯¹è¯è¯­éŸ³ç”Ÿæˆï¼Œæ»¡è¶³ä¸åŒè¯­ç§å’Œæ–¹è¨€çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå¯ä»¥è¿ç»­äº§ç”Ÿè¶…è¿‡90åˆ†é’Ÿçš„å¯¹è¯ï¼Œå¹¶ç¨³å®šåœ°è¡¨ç°å‡ºè¯´è¯äººçš„éŸ³è‰²å’Œæµç•…çš„è¯´è¯äººè½¬æ¢ã€‚è¯¥ç³»ç»Ÿå…·æœ‰ä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„éŸµå¾‹ï¼Œå¯ä»¥åæ˜ å¯¹è¯è¿›ç¨‹ä¸­çš„è‡ªç„¶èŠ‚å¥å’Œè¯­è°ƒå˜åŒ–ã€‚æ€»ä½“ä¸Šï¼ŒSoulX-Podcaståœ¨ç‹¬ç™½TTSå’Œå¤šè½®å¯¹è¯è¯­éŸ³åˆæˆæ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SoulX-Podcastç³»ç»Ÿè®¾è®¡ç”¨äºPodcasté£æ ¼çš„å¤šè½®å¤šè¯´è¯è€…å¯¹è¯è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>é›†æˆå¤šç§è¯­è¨€æ§åˆ¶ï¼Œæ”¯æŒæ™®é€šè¯ã€è‹±è¯­åŠå¤šç§ä¸­æ–‡æ–¹è¨€ã€‚</li>
<li>èƒ½å¤Ÿè¿ç»­äº§ç”Ÿè¶…è¿‡90åˆ†é’Ÿçš„å¯¹è¯ï¼Œç¨³å®šè¡¨ç°è¯´è¯äººçš„éŸ³è‰²å’Œæµç•…è½¬æ¢ã€‚</li>
<li>è¯´è¯äººå±•ç°å‡ºä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„éŸµå¾‹ã€‚</li>
<li>åœ¨ç‹¬ç™½TTSå’Œå¤šè½®å¯¹è¯è¯­éŸ³åˆæˆæ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥ç³»ç»Ÿåœ¨è‡ªç„¶æ€§å’Œè¿è´¯æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9c5748041a462902998b0be9e6e231fa~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306734&auth_key=1762306734-0-0-4aac24db9cf66a33264eaa4c471528f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3973f3dc68444ad00ae8cf3700ad9e13~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306741&auth_key=1762306741-0-0-41d270f1561210af041832ca97bd719d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91f52eeab94e5400651a411296db98a2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306748&auth_key=1762306748-0-0-c7334f6ff1cb11b7e0091fb88d442d94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0262a76f990b0fbc3dad39d322521e7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306755&auth_key=1762306755-0-0-f005b51a22cbde3ce13ce8d0a7676a3e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UltraVoice-Scaling-Fine-Grained-Style-Controlled-Speech-Conversations-for-Spoken-Dialogue-Models"><a href="#UltraVoice-Scaling-Fine-Grained-Style-Controlled-Speech-Conversations-for-Spoken-Dialogue-Models" class="headerlink" title="UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations   for Spoken Dialogue Models"></a>UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations   for Spoken Dialogue Models</h2><p><strong>Authors:Wenming Tu, Guanrou Yang, Ruiqi Yan, Wenxi Chen, Ziyang Ma, Yipeng Kang, Kai Yu, Xie Chen, Zilong Zheng</strong></p>
<p>Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce UltraVoice, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice. Moreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the datasetâ€™s utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: <a target="_blank" rel="noopener" href="https://github.com/bigai-nlco/UltraVoice">https://github.com/bigai-nlco/UltraVoice</a>. </p>
<blockquote>
<p>å½“å‰ï¼Œè¯­éŸ³å¯¹è¯æ¨¡å‹ç¼ºä¹ç²¾ç»†ç²’åº¦çš„è¯­éŸ³é£æ ¼æ§åˆ¶èƒ½åŠ›ï¼Œè¿™å¯¹äºç±»ä¼¼äººç±»çš„äº¤äº’è‡³å…³é‡è¦ï¼Œé€šå¸¸è¢«å¿½è§†ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯è¯¸å¦‚æ¨ç†å’Œé—®ç­”ç­‰çº¯åŠŸèƒ½æ€§çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UltraVoiceï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘å¤šç§ç²¾ç»†ç²’åº¦è¯­éŸ³é£æ ¼æ§åˆ¶çš„å¤§å‹è¯­éŸ³å¯¹è¯æ•°æ®é›†ã€‚UltraVoiceåŒ…å«äº†è¶…è¿‡830å°æ—¶çš„è¯­éŸ³å¯¹è¯ï¼Œæä¾›äº†å…­ä¸ªå…³é”®è¯­éŸ³é£æ ¼ç»´åº¦çš„æŒ‡ä»¤ï¼šæƒ…æ„Ÿã€è¯­é€Ÿã€éŸ³é‡ã€å£éŸ³ã€è¯­è¨€å’Œç»„åˆé£æ ¼ã€‚ä½¿ç”¨UltraVoiceå¯¹SLAM-Omniå’ŒVocalNetç­‰é¢†å…ˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†å…¶ç²¾ç»†ç²’åº¦çš„è¯­éŸ³é£æ ¼æ§åˆ¶èƒ½åŠ›ï¼Œè€Œä¸ä¼šé™ä½å…¶æ ¸å¿ƒå¯¹è¯èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨UltraVoiceä¸­è®¾è®¡çš„å¤šç»´åº¦æ§åˆ¶ä»»åŠ¡ä¸Šï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ä¸Šæé«˜äº†29.12%~42.33%ï¼Œåœ¨æŒ‡ä»¤éµå¾ªç‡ï¼ˆIFRï¼‰ä¸Šæé«˜äº†14.61%~40.09ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œåœ¨URO-BenchåŸºå‡†æµ‹è¯•ä¸Šï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨æ ¸å¿ƒç†è§£ã€æ¨ç†å’Œå¯¹è¯èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒåŸºæœ¬è®¾ç½®ä¸Šå¹³å‡æé«˜äº†+10.84%ï¼Œä¸“ä¸šè®¾ç½®ä¸Šå¹³å‡æé«˜äº†+7.87%ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜é€‚ç”¨äºè®­ç»ƒå¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œå‡¸æ˜¾äº†å…¶é«˜è´¨é‡å’Œå¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œå¯ç”¨äºè¡¨è¾¾æ€§è¯­éŸ³åˆæˆã€‚å®Œæ•´çš„æ•°æ®é›†å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/bigai-nlco/UltraVoice">https://github.com/bigai-nlco/UltraVoice</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22588v1">PDF</a> 23 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>UltraVoiceæ˜¯ä¸€æ¬¾é’ˆå¯¹å¤šç§ç²¾ç»†è¯­éŸ³é£æ ¼æ§åˆ¶çš„å¤§å‹è¯­éŸ³å¯¹è¯æ•°æ®é›†ã€‚å®ƒåŒ…å«è¶…è¿‡830å°æ—¶çš„è¯­éŸ³å¯¹è¯ï¼Œæ¶µç›–äº†å…­ç§å…³é”®è¯­éŸ³é£æ ¼ç»´åº¦ï¼Œå¦‚æƒ…æ„Ÿã€è¯­é€Ÿã€éŸ³é‡ã€å£éŸ³ã€è¯­è¨€å’Œå¤åˆé£æ ¼ã€‚è¯¥æ•°æ®é›†å¯æ˜¾è‘—æå‡SLAM-Omniå’ŒVocalNetç­‰é¢†å…ˆæ¨¡å‹çš„ç²¾ç»†è¯­éŸ³é£æ ¼æ§åˆ¶èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æŸå®³å…¶æ ¸å¿ƒå¯¹è¯èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒUltraVoiceæ•°æ®é›†è¿˜å¯ç”¨äºè®­ç»ƒå¯æ§æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œå±•ç°å‡ºå…¶é«˜è´¨é‡å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UltraVoiceæ˜¯é¦–ä¸ªé’ˆå¯¹å¤šç§ç²¾ç»†è¯­éŸ³é£æ ¼æ§åˆ¶çš„å¤§å‹è¯­éŸ³å¯¹è¯æ•°æ®é›†ã€‚</li>
<li>åŒ…å«è¶…è¿‡830å°æ—¶çš„è¯­éŸ³å¯¹è¯ï¼Œè¦†ç›–å…­ç§å…³é”®è¯­éŸ³é£æ ¼ç»´åº¦ã€‚</li>
<li>é€šè¿‡UltraVoiceæ•°æ®é›†ï¼Œå¯ä»¥æ˜¾è‘—æå‡SLAM-Omniå’ŒVocalNetç­‰æ¨¡å‹çš„ç²¾ç»†è¯­éŸ³é£æ ¼æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šç»´åº¦æ§åˆ¶ä»»åŠ¡ä¸Šï¼Œfine-tunedæ¨¡å‹åœ¨Mean Opinion Score (MOS)å’ŒInstruction Following Rate (IFR)æ–¹é¢å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>UltraVoiceæ•°æ®é›†æœ‰åŠ©äºæé«˜æ ¸å¿ƒç†è§£ã€æ¨ç†å’Œå¯¹è¯èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨URO-BenchåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºè®­ç»ƒå¯æ§æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜è´¨é‡å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0fffa7c22c84344a1227c0fdca97a1b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306762&auth_key=1762306762-0-0-276cc7804e8e0eaeeecf65d8d57bf867&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0191c7a1641352c5cc10828bdb447f54~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306769&auth_key=1762306769-0-0-9502c57220a6f90a025155d2b64671ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0de9fe2684b3946f92907ac70fa74a02~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306776&auth_key=1762306776-0-0-51e9ddb824544976aa8ad852c7fc7c65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e85b8adb000c74d6f6024273b1f92fb2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306782&auth_key=1762306782-0-0-93e92a824ba2d41f46fce777bc021021&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bc659d1c4c365bb5ee67f300d7bfa7b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306789&auth_key=1762306789-0-0-1e576d7fa6159fd8c4657f5900af4181&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa6b0558845584e4d856287584663f62~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306795&auth_key=1762306795-0-0-bae4679796227445f05c09d325bba91d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="T2SMark-Balancing-Robustness-and-Diversity-in-Noise-as-Watermark-for-Diffusion-Models"><a href="#T2SMark-Balancing-Robustness-and-Diversity-in-Noise-as-Watermark-for-Diffusion-Models" class="headerlink" title="T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for   Diffusion Models"></a>T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for   Diffusion Models</h2><p><strong>Authors:Jindong Yang, Han Fang, Weiming Zhang, Nenghai Yu, Kejiang Chen</strong></p>
<p>Diffusion models have advanced rapidly in recent years, producing high-fidelity images while raising concerns about intellectual property protection and the misuse of generative AI. Image watermarking for diffusion models, particularly Noise-as-Watermark (NaW) methods, encode watermark as specific standard Gaussian noise vector for image generation, embedding the infomation seamlessly while maintaining image quality. For detection, the generation process is inverted to recover the initial noise vector containing the watermark before extraction. However, existing NaW methods struggle to balance watermark robustness with generation diversity. Some methods achieve strong robustness by heavily constraining initial noise sampling, which degrades user experience, while others preserve diversity but prove too fragile for real-world deployment. To address this issue, we propose T2SMark, a two-stage watermarking scheme based on Tail-Truncated Sampling (TTS). Unlike prior methods that simply map bits to positive or negative values, TTS enhances robustness by embedding bits exclusively in the reliable tail regions while randomly sampling the central zone to preserve the latent distribution. Our two-stage framework then ensures sampling diversity by integrating a randomly generated session key into both encryption pipelines. We evaluate T2SMark on diffusion models with both U-Net and DiT backbones. Extensive experiments show that it achieves an optimal balance between robustness and diversity. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/0xD009/T2SMark%7D%7Bhttps://github.com/0xD009/T2SMark%7D">https://github.com/0xD009/T2SMark}{https://github.com/0xD009/T2SMark}</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹å‘å±•è¿…é€Ÿï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸå›¾åƒï¼ŒåŒæ—¶å¼•å‘äº†å…³äºçŸ¥è¯†äº§æƒä¿æŠ¤å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ»¥ç”¨çš„æ‹…å¿§ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„å›¾åƒæ°´å°æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯å™ªå£°æ°´å°ï¼ˆNaWï¼‰æ–¹æ³•ï¼Œå°†æ°´å°ç¼–ç ä¸ºç‰¹å®šçš„æ ‡å‡†é«˜æ–¯å™ªå£°å‘é‡ï¼Œç”¨äºå›¾åƒç”Ÿæˆï¼Œå®ç°æ— ç¼åµŒå…¥ä¿¡æ¯çš„åŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚æ£€æµ‹æ—¶ï¼Œéœ€è¦é€†è½¬ç”Ÿæˆè¿‡ç¨‹ä»¥æ¢å¤åŒ…å«æ°´å°çš„åˆå§‹å™ªå£°å‘é‡å†è¿›è¡Œæå–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„NaWæ–¹æ³•å¾ˆéš¾åœ¨æ°´å°çš„ç¨³å¥æ€§å’Œç”Ÿæˆçš„å¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¸€äº›æ–¹æ³•é€šè¿‡ä¸¥æ ¼çº¦æŸåˆå§‹å™ªå£°é‡‡æ ·æ¥å®ç°å¼ºå¤§çš„ç¨³å¥æ€§ï¼Œè¿™é™ä½äº†ç”¨æˆ·ä½“éªŒï¼›è€Œå…¶ä»–æ–¹æ³•è™½ç„¶ä¿æŒäº†å¤šæ ·æ€§ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­å´æ˜¾å¾—è¿‡äºè„†å¼±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå°¾æˆªæ–­é‡‡æ ·ï¼ˆTail-Truncated Samplingï¼Œç®€ç§°TTSï¼‰çš„ä¸¤é˜¶æ®µæ°´å°æ–¹æ¡ˆT2SMarkã€‚ä¸åŒäºç°æœ‰æ–¹æ³•ç®€å•åœ°æ˜ å°„ä½åˆ°æ­£å€¼æˆ–è´Ÿå€¼ï¼ŒTTSé€šè¿‡ä»…åœ¨å¯é çš„å°¾åŒºåŸŸåµŒå…¥ä½å¹¶éšæœºé‡‡æ ·ä¸­å¿ƒåŒºåŸŸä»¥ä¿æŒæ½œåœ¨åˆ†å¸ƒï¼Œå¢å¼ºäº†ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µæ¡†æ¶éšåé€šè¿‡æ•´åˆéšæœºç”Ÿæˆçš„ä¼šè¯å¯†é’¥åˆ°åŠ å¯†ç®¡é“ä¸­ç¡®ä¿é‡‡æ ·å¤šæ ·æ€§ã€‚æˆ‘ä»¬åœ¨å…·æœ‰U-Netå’ŒDiTéª¨å¹²çš„æ‰©æ•£æ¨¡å‹ä¸Šè¯„ä¼°äº†T2SMarkã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨ç¨³å¥æ€§å’Œå¤šæ ·æ€§ä¹‹é—´è¾¾åˆ°äº†æœ€ä¼˜å¹³è¡¡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/0xD009/T2SMark%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/0xD009/T2SMarkä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22366v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹è¿‘å¹´æ¥å‘å±•è¿…é€Ÿï¼Œäº§ç”Ÿäº†é«˜è´¨é‡å›¾åƒï¼Œå¼•å‘äº†å…³äºçŸ¥è¯†äº§æƒä¿æŠ¤å’Œæ»¥ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ‹…å¿§ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„æ°´å°åµŒå…¥æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯å™ªå£°æ°´å°ï¼ˆNaWï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†æ°´å°ç¼–ç ä¸ºç‰¹å®šæ ‡å‡†é«˜æ–¯å™ªå£°å‘é‡å®ç°å›¾åƒç”Ÿæˆä¸­çš„æ— ç¼åµŒå…¥ã€‚æ£€æµ‹æ—¶ï¼Œé€šè¿‡åè½¬ç”Ÿæˆè¿‡ç¨‹æ¥æ¢å¤åŒ…å«æ°´å°çš„åˆå§‹å™ªå£°å‘é‡åè¿›è¡Œæå–ã€‚ç„¶è€Œï¼Œç°æœ‰NaWæ–¹æ³•åœ¨å¹³è¡¡æ°´å°çš„ç¨³å¥æ€§å’Œç”Ÿæˆå¤šæ ·æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ‰çš„æ–¹æ³•é€šè¿‡ä¸¥æ ¼é™åˆ¶åˆå§‹å™ªå£°é‡‡æ ·æ¥å®ç°å¼ºç¨³å¥æ€§ï¼Œä½†ç‰ºç‰²äº†ç”¨æˆ·ä½“éªŒï¼›æœ‰çš„æ–¹æ³•è™½èƒ½ä¿ç•™å¤šæ ·æ€§ï¼Œä½†åœ¨å®é™…éƒ¨ç½²ä¸­è¿‡äºè„†å¼±ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåŸºäºå°¾æˆªæ–­é‡‡æ ·ï¼ˆTTSï¼‰çš„ä¸¤é˜¶æ®µæ°´å°æ–¹æ¡ˆT2SMarkã€‚TTSä¸åŒäºä»¥å¾€ç®€å•åœ°å°†ä½æ˜ å°„åˆ°æ­£è´Ÿå€¼çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä»…åœ¨å¯é çš„å°¾åŒºåŸŸåµŒå…¥ä½æ¥æé«˜ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿ç•™ä¸­å¤®åŒºåŸŸçš„éšæœºé‡‡æ ·ä»¥ä¿æŒæ½œåœ¨åˆ†å¸ƒã€‚ä¸¤é˜¶æ®µæ¡†æ¶è¿˜é€šè¿‡é›†æˆéšæœºç”Ÿæˆçš„ä¼šè¯å¯†é’¥æ¥ä¿è¯é‡‡æ ·å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒT2SMarkåœ¨U-Netå’ŒDiTèƒŒä¹¦çš„æ‰©æ•£æ¨¡å‹ä¸Šå®ç°äº†ç¨³å¥æ€§å’Œå¤šæ ·æ€§çš„æœ€ä¼˜å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹äº§ç”Ÿé«˜è´¨é‡å›¾åƒï¼Œå¼•å‘çŸ¥è¯†äº§æƒä¿æŠ¤å’Œæ»¥ç”¨ç”Ÿæˆå¼AIçš„æ‹…å¿§ã€‚</li>
<li>ç°æœ‰å™ªå£°æ°´å°ï¼ˆNaWï¼‰æ–¹æ³•åœ¨å¹³è¡¡æ°´å°ç¨³å¥æ€§å’Œç”Ÿæˆå¤šæ ·æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„T2SMarkåŸºäºå°¾æˆªæ–­é‡‡æ ·ï¼ˆTTSï¼‰çš„ä¸¤é˜¶æ®µæ°´å°æ–¹æ¡ˆæ—¨åœ¨è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>TTSåœ¨å¯é çš„å°¾åŒºåŸŸåµŒå…¥ä½ä»¥æé«˜ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿ç•™ä¸­å¤®åŒºåŸŸçš„éšæœºé‡‡æ ·ã€‚</li>
<li>T2SMarkçš„ä¸¤é˜¶æ®µæ¡†æ¶é›†æˆéšæœºç”Ÿæˆçš„ä¼šè¯å¯†é’¥ï¼Œä¿è¯é‡‡æ ·å¤šæ ·æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒT2SMarkåœ¨å¤šç§æ‰©æ•£æ¨¡å‹ä¸Šå®ç°äº†ç¨³å¥æ€§å’Œå¤šæ ·æ€§çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22366">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8a0dd1f2f9cbc375fb55745ee1cc4ba4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306802&auth_key=1762306802-0-0-8eb729243bde9abccbe8bc4d493a3fc8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9abd2273cbd12f9241615f8b4806f3d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306810&auth_key=1762306810-0-0-3464ca0ac0529e81d8960073b62a2a8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f136625f89d54250cd7bcb3bf7f7308a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306817&auth_key=1762306817-0-0-e6e6c7264d42d06f7a6cb3c6d2982f5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SHAP-Meets-Tensor-Networks-Provably-Tractable-Explanations-with-Parallelism"><a href="#SHAP-Meets-Tensor-Networks-Provably-Tractable-Explanations-with-Parallelism" class="headerlink" title="SHAP Meets Tensor Networks: Provably Tractable Explanations with   Parallelism"></a>SHAP Meets Tensor Networks: Provably Tractable Explanations with   Parallelism</h2><p><strong>Authors:Reda Marzouk, Shahaf Bassan, Guy Katz</strong></p>
<p>Although Shapley additive explanations (SHAP) can be computed in polynomial time for simple models like decision trees, they unfortunately become NP-hard to compute for more expressive black-box models like neural networks - where generating explanations is often most critical. In this work, we analyze the problem of computing SHAP explanations for <em>Tensor Networks (TNs)</em>, a broader and more expressive class of models than those for which current exact SHAP algorithms are known to hold, and which is widely used for neural network abstraction and compression. First, we introduce a general framework for computing provably exact SHAP explanations for general TNs with arbitrary structures. Interestingly, we show that, when TNs are restricted to a <em>Tensor Train (TT)</em> structure, SHAP computation can be performed in <em>poly-logarithmic</em> time using <em>parallel</em> computation. Thanks to the expressiveness power of TTs, this complexity result can be generalized to many other popular ML models such as decision trees, tree ensembles, linear models, and linear RNNs, therefore tightening previously reported complexity results for these families of models. Finally, by leveraging reductions of binarized neural networks to Tensor Network representations, we demonstrate that SHAP computation can become <em>efficiently tractable</em> when the networkâ€™s <em>width</em> is fixed, while it remains computationally hard even with constant <em>depth</em>. This highlights an important insight: for this class of models, width - rather than depth - emerges as the primary computational bottleneck in SHAP computation. </p>
<blockquote>
<p>è™½ç„¶å¯¹äºå†³ç­–æ ‘ç­‰ç®€å•æ¨¡å‹ï¼ŒShapleyåŠ æ€§è§£é‡Šï¼ˆSHAPï¼‰å¯ä»¥åœ¨å¤šé¡¹å¼æ—¶é—´å†…è¿›è¡Œè®¡ç®—ï¼Œä½†å¯¹äºæ›´å¤æ‚çš„é»‘ç›’æ¨¡å‹ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰è¿›è¡ŒSHAPè®¡ç®—å°±ä¼šå˜å¾—NPéš¾è§£ï¼Œè€Œé€šå¸¸è¿™äº›æ¨¡å‹æ­£æ˜¯æœ€éœ€è¦è¿›è¡Œè§£é‡Šçš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸ºå¼ é‡ç½‘ç»œï¼ˆTNsï¼‰è®¡ç®—SHAPè§£é‡Šçš„é—®é¢˜ã€‚TNsæ˜¯ä¸€ä¸ªæ›´å¹¿æ³›ä¸”æ›´å…·è¡¨ç°åŠ›çš„æ¨¡å‹ç±»åˆ«ï¼Œè¶…å‡ºäº†å½“å‰å·²çŸ¥çš„ç²¾ç¡®SHAPç®—æ³•çš„é€‚ç”¨èŒƒå›´ï¼Œå¹¿æ³›åº”ç”¨äºç¥ç»ç½‘ç»œçš„æŠ½è±¡å’Œå‹ç¼©ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºè®¡ç®—ä¸€èˆ¬TNsçš„ç²¾ç¡®SHAPè§£é‡Šå¼•å…¥äº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰ä»»æ„ç»“æ„ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå½“æˆ‘ä»¬é™åˆ¶TNsä¸ºå¼ é‡åˆ—è½¦ï¼ˆTTï¼‰ç»“æ„æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å¹¶è¡Œè®¡ç®—ä»¥å¤šé¡¹å¼å¯¹æ•°æ—¶é—´åœ¨TTä¸Šè¿›è¡ŒSHAPè®¡ç®—ã€‚ç”±äºTTçš„è¡¨ç°åŠ›å¼ºå¤§ï¼Œè¿™ä¸ªå¤æ‚åº¦ç»“æœå¯ä»¥æ¨å¹¿åˆ°è®¸å¤šå…¶ä»–æµè¡Œçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¦‚å†³ç­–æ ‘ã€æ ‘é›†æˆã€çº¿æ€§æ¨¡å‹å’Œçº¿æ€§RNNï¼Œä»è€Œæé«˜äº†ä¹‹å‰å¯¹è¿™äº›æ¨¡å‹å®¶æ—æŠ¥å‘Šçš„å¤æ‚åº¦ç»“æœã€‚æœ€åï¼Œé€šè¿‡åˆ©ç”¨äºŒå€¼åŒ–ç¥ç»ç½‘ç»œåˆ°å¼ é‡ç½‘ç»œè¡¨ç¤ºçš„ç®€åŒ–ï¼Œæˆ‘ä»¬è¯æ˜å½“ç½‘ç»œçš„å®½åº¦å›ºå®šæ—¶ï¼ŒSHAPè®¡ç®—å¯ä»¥å˜å¾—é«˜æ•ˆå¯è¡Œï¼Œå³ä½¿æ·±åº¦æ’å®šï¼Œå®ƒä»ç„¶åœ¨è®¡ç®—ä¸Šæ˜¯å›°éš¾çš„ã€‚è¿™çªæ˜¾äº†ä¸€ä¸ªé‡è¦è§è§£ï¼šå¯¹äºè¿™ç±»æ¨¡å‹ï¼Œå®½åº¦è€Œä¸æ˜¯æ·±åº¦æ˜¯SHAPè®¡ç®—ä¸­çš„ä¸»è¦è®¡ç®—ç“¶é¢ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21599v1">PDF</a> To appear in NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è®¡ç®—Tensor Networksï¼ˆTNsï¼‰çš„SHAPè§£é‡Šçš„é—®é¢˜ã€‚å°½ç®¡å¯¹äºç®€å•çš„æ¨¡å‹å¦‚å†³ç­–æ ‘ï¼ŒSHAPå¯ä»¥åœ¨å¤šé¡¹å¼æ—¶é—´å†…è®¡ç®—ï¼Œä½†å¯¹äºæ›´å¤æ‚çš„é»‘ç®±æ¨¡å‹å¦‚ç¥ç»ç½‘ç»œï¼Œè®¡ç®—SHAPè§£é‡Šå˜å¾—NPéš¾ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶æ¥è®¡ç®—ä¸€èˆ¬TNsçš„ç¡®åˆ‡SHAPè§£é‡Šï¼Œå¹¶å‘ç°å½“TNsé™åˆ¶ä¸ºTensor Trainï¼ˆTTï¼‰ç»“æ„æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å¹¶è¡Œè®¡ç®—åœ¨å¤šé¡¹å¼å¯¹æ•°æ—¶é—´å†…è¿›è¡ŒSHAPè®¡ç®—ã€‚æ­¤å¤–ï¼Œè¯¥å¤æ‚æ€§ç»“æœå¯ä»¥æ¨å¹¿åˆ°è®¸å¤šå…¶ä»–æµè¡Œçš„MLæ¨¡å‹ï¼ŒåŒ…æ‹¬å†³ç­–æ ‘ã€æ ‘é›†åˆã€çº¿æ€§æ¨¡å‹å’Œçº¿æ€§RNNsç­‰ã€‚æœ€åï¼Œé€šè¿‡å°†äºŒå€¼åŒ–ç¥ç»ç½‘ç»œç®€åŒ–ä¸ºå¼ é‡ç½‘ç»œè¡¨ç¤ºï¼Œæˆ‘ä»¬å‘ç°å½“ç½‘ç»œçš„å®½åº¦å›ºå®šæ—¶ï¼ŒSHAPè®¡ç®—å˜å¾—æœ‰æ•ˆå¯è¡Œï¼Œè€Œå³ä½¿æ·±åº¦ä¸ºå¸¸æ•°æ—¶ï¼Œå®ƒä»ç„¶è®¡ç®—ä¸Šå¾ˆå›°éš¾ã€‚è¿™è¡¨æ˜å¯¹äºæ­¤ç±»æ¨¡å‹ï¼Œå®½åº¦è€Œä¸æ˜¯æ·±åº¦æ˜¯SHAPè®¡ç®—ä¸­çš„ä¸»è¦è®¡ç®—ç“¶é¢ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—SHAPè§£é‡Šå¯¹äºå¤æ‚çš„é»‘ç®±æ¨¡å‹å¦‚ç¥ç»ç½‘ç»œæ˜¯NPéš¾çš„ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶æ¥è®¡ç®—Tensor Networksï¼ˆTNsï¼‰çš„ç¡®åˆ‡SHAPè§£é‡Šã€‚</li>
<li>å½“TNsé™åˆ¶ä¸ºTensor Trainï¼ˆTTï¼‰ç»“æ„æ—¶ï¼ŒSHAPè®¡ç®—å¯ä»¥åœ¨å¤šé¡¹å¼å¯¹æ•°æ—¶é—´å†…è¿›è¡Œï¼Œå¹¶ä¸”è¿™ä¸€å¤æ‚æ€§ç»“æœå¯ä»¥æ¨å¹¿åˆ°è®¸å¤šå…¶ä»–æµè¡Œçš„MLæ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¼ é‡ç½‘ç»œè¡¨ç¤ºï¼ŒäºŒå€¼åŒ–ç¥ç»ç½‘ç»œçš„SHAPè®¡ç®—åœ¨å›ºå®šå®½åº¦æ—¶å˜å¾—æœ‰æ•ˆå¯è¡Œã€‚</li>
<li>SHAPè®¡ç®—çš„ç“¶é¢ˆåœ¨äºæ¨¡å‹çš„å®½åº¦è€Œéæ·±åº¦ã€‚</li>
<li>TTç»“æ„åœ¨å¹¶è¡Œè®¡ç®—ä¸­è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d2552bede46901288ca95cbdd135bf7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306824&auth_key=1762306824-0-0-de676c686cebffe2a1eb25140c165ac3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f0cb278c55758c90c4a04b59e02f0ed3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306832&auth_key=1762306832-0-0-919c897ad4c35a6e696edf351cf7a892&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Continuous-Token-Diffusion-for-Speaker-Referenced-TTS-in-Multimodal-LLMs"><a href="#Continuous-Token-Diffusion-for-Speaker-Referenced-TTS-in-Multimodal-LLMs" class="headerlink" title="Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs"></a>Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs</h2><p><strong>Authors:Xinlu He, Swayambhu Nath Ray, Harish Mallidi, Jia-Hong Huang, Ashwin Bellur, Chander Chandak, M. Maruf, Venkatesh Ravichandran</strong></p>
<p>Unified architectures in multimodal large language models (MLLM) have shown promise in handling diverse tasks within a single framework. In the text-to-speech (TTS) task, current MLLM-based approaches rely on discrete token representations, which disregard the inherently continuous nature of speech and can lead to loss of fine-grained acoustic information. In this work, we investigate the TTS within the MLLM paradigm using continuous speech representations. We design a dual-head architecture and implement two complementary training strategies for a robust model. (1) A diffusion head generating continuous speech representations is added on the MLLM, which is on frame-level and strictly autoregressive. (2) The original language model head is retained to preserve multitask capability and to control the start and end of speech synthesis. (3) Masked training is employed to address exposure bias in autoregressive decoding. (4) To stabilize optimization, we propose a two-stage scheme where the LM is frozen in the second stage, ensuring the diffusion head learns from a fixed input distribution. Evaluations on LibriSpeech(PC) test-clean show that our approach achieves state-of-the-art autoregressive performance, with a WER of 1.95%, speaker similarity of 0.54, and UTMOS of 4.00. The two-stage training yields a 46% relative WER reduction over the one-stage training baseline. These results highlight the effectiveness of combining autoregressive modeling with continuous-token diffusion, supported by a two-stage training procedure. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­ï¼Œç»Ÿä¸€æ¶æ„åœ¨å•ä¸ªæ¡†æ¶å†…å¤„ç†å„ç§ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ä»»åŠ¡ä¸­ï¼Œå½“å‰çš„åŸºäºMLLMçš„æ–¹æ³•ä¾èµ–äºç¦»æ•£ä»¤ç‰Œè¡¨ç¤ºï¼Œè¿™å¿½ç•¥äº†è¯­éŸ³çš„å›ºæœ‰è¿ç»­æ€§ï¼Œå¹¶å¯èƒ½å¯¼è‡´ç²¾ç»†çš„å£°å­¦ä¿¡æ¯ä¸¢å¤±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿ç»­è¯­éŸ³è¡¨ç¤ºæ¥ç ”ç©¶MLLMèŒƒå¼å†…çš„TTSã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒå¤´æ¶æ„ï¼Œå¹¶å®æ–½äº†ä¸¤ç§äº’è¡¥çš„è®­ç»ƒç­–ç•¥ï¼Œä»¥æ„å»ºç¨³å¥çš„æ¨¡å‹ã€‚ï¼ˆ1ï¼‰åœ¨MLLMä¸Šå¢åŠ äº†ä¸€ä¸ªç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºçš„åˆ†å½¢å¤´ï¼Œè¯¥å¤´æ˜¯æ¡†æ¶çº§çš„ï¼Œå¹¶ä¸”æ˜¯å®Œå…¨è‡ªå›å½’çš„ã€‚ï¼ˆ2ï¼‰ä¿ç•™åŸå§‹è¯­è¨€æ¨¡å‹å¤´ï¼Œä»¥ä¿æŒå¤šä»»åŠ¡èƒ½åŠ›å¹¶æ§åˆ¶è¯­éŸ³åˆæˆçš„å¼€å§‹å’Œç»“æŸã€‚ï¼ˆ3 to address exposure bias in autoregressive decoding, masked training is employed.ï¼ˆ3ï¼‰ä¸ºè§£å†³è‡ªå›å½’è§£ç ä¸­çš„æ›å…‰åè§ï¼Œé‡‡ç”¨äº†æ©æ¨¡è®­ç»ƒã€‚ï¼ˆ4ï¼‰ä¸ºäº†ä¼˜åŒ–ç¨³å®šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ–¹æ¡ˆï¼Œåœ¨ç¬¬äºŒé˜¶æ®µå†»ç»“LMï¼Œç¡®ä¿æ‰©æ•£å¤´ä»å›ºå®šçš„è¾“å…¥åˆ†å¸ƒä¸­å­¦ä¹ ã€‚åœ¨LibriSpeechï¼ˆPCï¼‰æµ‹è¯•æ¸…æ´ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è‡ªå›å½’æ€§èƒ½ï¼Œè¯é”™è¯¯ç‡ä¸º1.95%ï¼Œè¯´è¯äººç›¸ä¼¼åº¦ä¸º0.54ï¼ŒUTMOSä¸º4.00ã€‚ä¸å•é˜¶æ®µè®­ç»ƒåŸºçº¿ç›¸æ¯”ï¼Œä¸¤é˜¶æ®µè®­ç»ƒä½¿WERç›¸å¯¹é™ä½äº†46%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ç»“åˆè‡ªå›å½’å»ºæ¨¡ä¸è¿ç»­ä»¤ç‰Œæ‰©æ•£çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¾—åˆ°ä¸¤é˜¶æ®µè®­ç»ƒç¨‹åºçš„æ”¯æ’‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12995v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ç»Ÿä¸€æ¶æ„åœ¨å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚åœ¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ä»»åŠ¡ä¸­ï¼Œå½“å‰çš„MLLMæ–¹æ³•ä¾èµ–äºç¦»æ•£ä»¤ç‰Œè¡¨ç¤ºå½¢å¼ï¼Œå¿½ç•¥äº†è¯­éŸ³çš„å›ºæœ‰è¿ç»­æ€§ï¼Œå¯èƒ½å¯¼è‡´ç²¾ç»†å£°å­¦ä¿¡æ¯çš„ä¸¢å¤±ã€‚æœ¬ç ”ç©¶åœ¨MLLMæ¡†æ¶ä¸‹æ¢è®¨TTSçš„è¿ç»­è¯­éŸ³è¡¨ç¤ºå½¢å¼ã€‚è®¾è®¡äº†ä¸€ç§åŒå¤´æ¶æ„ï¼Œå¹¶å®æ–½ä¸¤ç§äº’è¡¥çš„è®­ç»ƒç­–ç•¥ä»¥å®ç°ç¨³å¥çš„æ¨¡å‹ï¼šæ·»åŠ ä¸€ä¸ªæ‰©æ•£å¤´ç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºå½¢å¼ï¼Œè¯¥å½¢å¼åŸºäºå¸§çº§ä¸”ä¸¥æ ¼è‡ªå›å½’ï¼›ä¿ç•™åŸå§‹è¯­è¨€æ¨¡å‹å¤´ä»¥ä¿ç•™å¤šä»»åŠ¡åŠŸèƒ½å¹¶æ§åˆ¶è¯­éŸ³åˆæˆçš„å¼€å§‹å’Œç»“æŸï¼›é‡‡ç”¨æ©è”½è®­ç»ƒè§£å†³è‡ªå›å½’è§£ç ä¸­çš„æš´éœ²åå·®é—®é¢˜ï¼›æå‡ºä¸€ç§ä¸¤é˜¶æ®µæ–¹æ¡ˆä»¥ç¨³å®šä¼˜åŒ–ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µå†»ç»“è¯­è¨€æ¨¡å‹ï¼Œç¡®ä¿æ‰©æ•£å¤´ä»å›ºå®šçš„è¾“å…¥åˆ†å¸ƒä¸­å­¦ä¹ ã€‚åœ¨LibriSpeechï¼ˆPCï¼‰æµ‹è¯•æ¸…æ´ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è‡ªå›å½’æ€§èƒ½ï¼Œè¯é”™è¯¯ç‡ä¸º1.95%ï¼Œè¯´è¯äººç›¸ä¼¼åº¦ä¸º0.54ï¼ŒUTMOSä¸º4.00ã€‚ä¸¤é˜¶æ®µè®­ç»ƒä¸å•é˜¶æ®µè®­ç»ƒåŸºçº¿ç›¸æ¯”ï¼Œç›¸å¯¹è¯é”™è¯¯ç‡é™ä½äº†46%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ç»“åˆè‡ªå›å½’å»ºæ¨¡ä¸è¿ç»­ä»¤ç‰Œæ‰©æ•£çš„æœ‰æ•ˆæ€§ï¼Œè¾…ä»¥ä¸¤é˜¶æ®µè®­ç»ƒç¨‹åºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¶æ„åœ¨å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>å½“å‰TTSä»»åŠ¡ä¸­çš„MLLMæ–¹æ³•ä¸»è¦ä¾èµ–ç¦»æ•£ä»¤ç‰Œè¡¨ç¤ºï¼Œè¿™å¯èƒ½å¯¼è‡´ç²¾ç»†å£°å­¦ä¿¡æ¯çš„æŸå¤±ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨è¿ç»­è¯­éŸ³è¡¨ç¤ºå½¢å¼æ”¹è¿›äº†TTSåœ¨MLLMæ¡†æ¶ä¸‹çš„è¡¨ç°ã€‚</li>
<li>è®¾è®¡äº†åŒå¤´æ¶æ„ï¼Œç»“åˆæ‰©æ•£å¤´å’ŒåŸå§‹è¯­è¨€æ¨¡å‹å¤´ï¼Œä»¥å®ç°ç¨³å¥çš„TTSæ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨äº†æ©è”½è®­ç»ƒæ¥è§£å†³è‡ªå›å½’è§£ç ä¸­çš„æš´éœ²åå·®é—®é¢˜ã€‚</li>
<li>æå‡ºä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å¹¶é™ä½è¯é”™è¯¯ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b6643f2702c6c71d4b27718a8e41e5cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306839&auth_key=1762306839-0-0-fc22c6b59fe9d4f334e199285a81f0d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c70f1847607fef6a922e9950524f98d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306847&auth_key=1762306847-0-0-92d2398181247097d433e4c6fc204c2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-698bf94b3c69268a3697bd2d462f6e26~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306853&auth_key=1762306853-0-0-cb0629864ef1dfa7dc045f1342626a91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models"><a href="#Video-LMM-Post-Training-A-Deep-Dive-into-Video-Reasoning-with-Large-Multimodal-Models" class="headerlink" title="Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large   Multimodal Models"></a>Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large   Multimodal Models</h2><p><strong>Authors:Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Junhua Huang, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, Chenliang Xu</strong></p>
<p>Video understanding represents the most challenging frontier in computer vision, requiring models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: <a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training">https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</a> </p>
<blockquote>
<p>è§†é¢‘ç†è§£æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ€å…·æŒ‘æˆ˜æ€§çš„å‰æ²¿è¯¾é¢˜ï¼Œå®ƒè¦æ±‚æ¨¡å‹èƒ½å¤Ÿæ¨ç†å¤æ‚çš„æ—¶ç©ºå…³ç³»ã€é•¿æœŸä¾èµ–å…³ç³»å’Œå¤šæ¨¡æ€è¯æ®ã€‚æœ€è¿‘å‡ºç°çš„è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆVideo-LMMsï¼‰ï¼Œé›†æˆäº†è§†è§‰ç¼–ç å™¨å’ŒåŸºäºå¼ºå¤§è§£ç å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ä»åŸºæœ¬æ„ŸçŸ¥ç³»ç»Ÿè½¬å˜ä¸ºå…ˆè¿›æ¨ç†å¼•æ“çš„å…³é”®é˜¶æ®µâ€”â€”å³åè®­ç»ƒé˜¶æ®µï¼Œåœ¨æ–‡çŒ®ä¸­ä»ç„¶åˆ†æ•£ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢è°ƒæŸ¥äº†è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„åè®­ç»ƒæ–¹æ³•è®ºï¼ŒåŒ…æ‹¬ä¸‰ä¸ªåŸºæœ¬æ”¯æŸ±ï¼šé€šè¿‡æ€è€ƒé“¾è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€åŸºäºå¯éªŒè¯ç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥åŠé€šè¿‡å¢å¼ºæ¨ç†è®¡ç®—è¿›è¡Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„åˆ†ç±»æ³•ï¼Œé˜æ˜äº†è¿™äº›æ–¹æ³•çš„ä½œç”¨ã€ç›¸äº’å…³è”ä»¥åŠé’ˆå¯¹è§†é¢‘çš„ç‰¹å®šé€‚åº”ï¼Œåº”å¯¹ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´å®šä½ã€æ—¶ç©ºå®šä½ã€é•¿è§†é¢‘æ•ˆç‡å’Œå¤šæ¨¡æ€è¯æ®é›†æˆã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ–¹æ³•çš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬ç»¼åˆäº†å…³é”®çš„è®¾è®¡åŸåˆ™ã€è§è§£å’Œè¯„ä¼°åè®®ï¼ŒåŒæ—¶ç¡®å®šäº†å¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜æ•´ç†äº†é‡è¦çš„åŸºå‡†æµ‹è¯•ã€æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œä»¥ä¾¿å¯¹åè®­ç»ƒçš„æœ‰æ•ˆæ€§è¿›è¡Œä¸¥æ ¼è¯„ä¼°ã€‚æœ¬è°ƒæŸ¥æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œä»¥æ¨è¿›è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›ã€‚æ›´å¤šèµ„æºå’Œæ›´æ–°ä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/yunlong10/Awesome-Video-LMM-Post-Training">https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.05034v5">PDF</a> Version v1.1</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘ç†è§£æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ€å…·æŒ‘æˆ˜æ€§çš„å‰æ²¿è¯¾é¢˜ï¼Œéœ€è¦æ¨¡å‹æ¨ç†å¤æ‚çš„æ—¶ç©ºå…³ç³»ã€é•¿æœŸä¾èµ–æ€§å’Œå¤šæ¨¡æ€è¯æ®ã€‚è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆVideo-LMMsï¼‰çš„å‡ºç°ï¼Œé€šè¿‡æ•´åˆè§†è§‰ç¼–ç å™¨å’ŒåŸºäºè¯­è¨€æ¨¡å‹çš„å¼ºå¤§è§£ç å™¨ï¼Œå±•ç°äº†è§†é¢‘ç†è§£ä»»åŠ¡çš„å“è¶Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ä»åŸºæœ¬æ„ŸçŸ¥ç³»ç»Ÿè½¬å˜ä¸ºå…ˆè¿›æ¨ç†å¼•æ“çš„åç»­è®­ç»ƒé˜¶æ®µï¼Œåœ¨æ–‡çŒ®ä¸­ä»ç„¶åˆ†æ•£ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢è°ƒæŸ¥äº†Video-LMMsçš„åç»­è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªåŸºæœ¬æ”¯æŸ±ï¼šé€šè¿‡æ€ç»´é“¾è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€åŸºäºå¯éªŒè¯ç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥åŠé€šè¿‡å¢å¼ºæ¨ç†è®¡ç®—è¿›è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–åˆ†ç±»æ³•ï¼Œé˜æ˜äº†è¿™äº›æ–¹æ³•åœ¨è§†é¢‘ç†è§£ä¸­çš„è§’è‰²ã€ç›¸äº’å…³è”å’Œç‰¹å®šé€‚åº”æ€§ï¼Œå¹¶åº”å¯¹ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´å®šä½ã€æ—¶ç©ºå®šä½ã€é•¿è§†é¢‘æ•ˆç‡å’Œå¤šæ¨¡æ€è¯æ®é›†æˆã€‚é€šè¿‡å¯¹ä»£è¡¨æ€§æ–¹æ³•çš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬ç»¼åˆäº†å…³é”®è®¾è®¡åŸåˆ™ã€è§è§£å’Œè¯„ä¼°åè®®ï¼ŒåŒæ—¶ç¡®å®šäº†å¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„å…³é”®å¼€æ”¾æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ•´ç†äº†é‡è¦çš„åŸºå‡†æµ‹è¯•ã€æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œä»¥ä¿ƒè¿›å¯¹åç»­è®­ç»ƒæœ‰æ•ˆæ€§çš„ä¸¥æ ¼è¯„ä¼°ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œä»¥æ¨è¿›Video-LMMçš„èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘ç†è§£é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚çš„æ—¶ç©ºå…³ç³»ã€é•¿æœŸä¾èµ–æ€§å’Œå¤šæ¨¡æ€è¯æ®çš„ç†è§£ã€‚</li>
<li>Video-Large Multimodal Modelsï¼ˆVideo-LMMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œé€šè¿‡æ•´åˆè§†è§‰ç¼–ç å™¨ä¸è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åç»­è®­ç»ƒæ˜¯ä½¿æ¨¡å‹ä»åŸºæœ¬æ„ŸçŸ¥ç³»ç»Ÿè½¬å˜ä¸ºå…ˆè¿›æ¨ç†å¼•æ“çš„å…³é”®é˜¶æ®µã€‚</li>
<li>è®ºæ–‡æä¾›äº†Video-LMMsçš„åç»­è®­ç»ƒæ–¹æ³•çš„é¦–æ¬¡å…¨é¢è°ƒæŸ¥ï¼ŒåŒ…æ‹¬æœ‰ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ä¸‰ä¸ªåŸºæœ¬æ”¯æŸ±ã€‚</li>
<li>è®ºæ–‡ä»‹ç»äº†è¿™äº›æ–¹æ³•åœ¨åº”å¯¹ç‹¬ç‰¹æŒ‘æˆ˜å¦‚æ—¶é—´å®šä½ã€æ—¶ç©ºå®šä½ã€é•¿è§†é¢‘æ•ˆç‡å’Œå¤šæ¨¡æ€è¯æ®é›†æˆæ–¹é¢çš„ä½œç”¨ã€‚</li>
<li>è®ºæ–‡è¿˜ç³»ç»Ÿåˆ†æäº†å…³é”®è®¾è®¡åŸåˆ™ã€å…³é”®è§è§£å’Œè¯„ä¼°åè®®ï¼Œå¹¶è¯†åˆ«å‡ºå¥–åŠ±è®¾è®¡ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ€§èƒ½ä¼˜åŒ–ç­‰æ–¹é¢çš„å¼€æ”¾æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æä¾›äº†é‡è¦çš„åŸºå‡†æµ‹è¯•ã€æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œä»¥ä¿ƒè¿›å¯¹åç»­è®­ç»ƒæœ‰æ•ˆæ€§çš„è¯„ä¼°ï¼Œå¹¶ä¸ºç ”ç©¶è€…å’Œå®è·µè€…æä¾›äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.05034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-09b6cd38de2ef56868d351186b9672b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306860&auth_key=1762306860-0-0-716e47c418ad17ff19d2de8108dc338d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-85a61878c24e7a8ad77236b533d85f14~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306868&auth_key=1762306868-0-0-98c4cff02398dbce4c767983f3e3e5f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c4cac533433a4e96b67feaede1a9e83c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306875&auth_key=1762306875-0-0-0eca3b1c3af2d9b6abfe7f4a13be55d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation"><a href="#SimulMEGA-MoE-Routers-are-Advanced-Policy-Makers-for-Simultaneous-Speech-Translation" class="headerlink" title="SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation"></a>SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous   Speech Translation</h2><p><strong>Authors:Chenyang Le, Bing Han, Jinshun Li, Songyong Chen, Yanmin Qian</strong></p>
<p>Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs. </p>
<blockquote>
<p>åŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSimulSTï¼‰é€šè¿‡è”åˆä¼˜åŒ–è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘ï¼Œåœ¨ä¸¥æ ¼çš„å»¶è¿Ÿé™åˆ¶ä¸‹å®ç°å®æ—¶è·¨è¯­è¨€æ²Ÿé€šã€‚ç°æœ‰ç³»ç»Ÿåœ¨å¹³è¡¡ç¿»è¯‘è´¨é‡ã€å»¶è¿Ÿå’Œè¯­ä¹‰è¿è´¯æ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­è¨€å¤šå¯¹å¤šçš„åœºæ™¯ä¸­ï¼Œä¸åŒçš„è¯»å–å’Œå†™å…¥ç­–ç•¥é˜»ç¢äº†ç»Ÿä¸€ç­–ç•¥å­¦ä¹ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SimulMEGAï¼ˆåŸºäºä¸“å®¶æ··åˆé—¨æ§çš„åŒæ­¥ç”Ÿæˆï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— ç›‘ç£çš„ç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†å‰ç¼€è®­ç»ƒä¸ä¸“å®¶æ··åˆç²¾ç‚¼å™¨ç›¸ç»“åˆï¼Œä»¥éšå¼çš„æ–¹å¼å­¦ä¹ æœ‰æ•ˆçš„è¯»å–å’Œå†™å…¥å†³ç­–ï¼Œè€Œä¸ä¼šå¢åŠ æ¨ç†æ—¶é—´å¼€é”€ã€‚æˆ‘ä»¬çš„è®¾è®¡åªéœ€è¦å¯¹æ ‡å‡†å˜å‹å™¨æ¶æ„è¿›è¡Œæœ€å°ä¿®æ”¹ï¼Œå°±å¯ä»¥å¹¿æ³›åº”ç”¨äºè¯­éŸ³åˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„æµå¼å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹å…­ç§è¯­è¨€å¯¹çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬çš„50äº¿å‚æ•°è¯­éŸ³åˆ°æ–‡æœ¬æ¨¡å‹ä¼˜äºæ— ç¼åŸºçº¿ï¼Œåœ¨å¹³å‡å»¶è¿Ÿ1.5ç§’çš„æƒ…å†µä¸‹ï¼ŒBLEUå¾—åˆ†é™ä½ä¸åˆ°7%ï¼Œåœ¨3ç§’å»¶è¿Ÿçš„æƒ…å†µä¸‹ï¼Œé™ä½ä¸åˆ°3%ã€‚æˆ‘ä»¬è¿˜å°†SimulMEGAæ‰©å±•åˆ°å…·æœ‰å•å‘ä¸»å¹²æµçš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶é€šç”¨æ€§ï¼Œåœ¨å»¶è¿Ÿè´¨é‡äº¤æ˜“æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01200v2">PDF</a> NeurIPS 2025 poster</p>
<p><strong>æ‘˜è¦</strong></p>
<p>SimulMEGAå®ç°äº†ä¸€ç§å¤šä»»åŠ¡ååŒçš„è¯­éŸ³å³æ—¶ç¿»è¯‘æŠ€æœ¯ï¼Œé€šè¿‡æ··åˆä¸“å®¶é—¨æ§ç­–ç•¥ä¼˜åŒ–è¯»å†™å­—ç­–ç•¥ï¼Œæé«˜ç¿»è¯‘è´¨é‡ã€å®æ—¶æ€§å’Œè¯­ä¹‰è¿è´¯æ€§ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºå¤šè¯­ç§åœºæ™¯ï¼Œå…·æœ‰éšå¼å­¦ä¹ ä¼˜åŠ¿ï¼Œå¯å¿«é€Ÿéƒ¨ç½²åœ¨æ ‡å‡†è½¬æ¢å™¨æ¶æ„ä¸Šï¼Œç”¨äºè¯­éŸ³è½¬æ–‡æœ¬å’Œæ–‡æœ¬è½¬è¯­éŸ³æµåª’ä½“ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å…­ç§è¯­è¨€å¯¹ä¸Šçš„è¡¨ç°ä¼˜äºæ— ç¼åŸºçº¿æŠ€æœ¯ï¼Œå¹³å‡å»¶è¿Ÿæ—¶é—´çŸ­ï¼Œç¿»è¯‘è´¨é‡é«˜ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜æˆåŠŸåº”ç”¨äºå•å‘éª¨æ¶TTSï¼Œå…·æœ‰ä¼˜å¼‚çš„å»¶è¿Ÿè´¨é‡æƒè¡¡æ•ˆæœã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>SimulMEGAæ˜¯ä¸€ç§å¤šä»»åŠ¡ååŒçš„è¯­éŸ³å³æ—¶ç¿»è¯‘æŠ€æœ¯æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨æ··åˆä¸“å®¶é—¨æ§ç­–ç•¥ï¼Œèƒ½æœ‰æ•ˆå¤„ç†å¤šç§ç¿»è¯‘ä¸­çš„è¯»å†™ç­–ç•¥é—®é¢˜ã€‚</li>
<li>SimulMEGAæé«˜äº†ç¿»è¯‘è´¨é‡ã€å®æ—¶æ€§å’Œè¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>è¯¥æŠ€æœ¯é€‚ç”¨äºå¤šè¯­ç§åœºæ™¯ä¸‹çš„ç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>SimulMEGAå…·æœ‰éšå¼å­¦ä¹ ä¼˜åŠ¿ï¼Œå¯å¿«é€Ÿéƒ¨ç½²åœ¨æ ‡å‡†è½¬æ¢å™¨æ¶æ„ä¸Šã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºSimulMEGAåœ¨è¯­éŸ³è½¬æ–‡æœ¬å’Œæ–‡æœ¬è½¬è¯­éŸ³æµåª’ä½“ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ç›¸è¾ƒäºæ— ç¼åŸºçº¿æŠ€æœ¯æœ‰æ›´ä½³çš„ç¿»è¯‘è´¨é‡å’Œå»¶è¿Ÿè¡¨ç°ã€‚åœ¨å…­ç§è¯­è¨€å¯¹ä¸Šçš„å®éªŒè¡¨ç°å¾—åˆ°äº†éªŒè¯ã€‚æ­¤å¤–åœ¨å»¶è¿Ÿå’ŒéŸ³è´¨æ–¹é¢è·å¾—äº†è¾ƒå¥½çš„æƒè¡¡æ•ˆæœã€‚å°¤å…¶åœ¨å»¶è¿Ÿè¶…è¿‡ä¸€å®šçš„ä¸´ç•Œå€¼åç¿»è¯‘è´¨é‡ä¿æŒä¼˜ç§€ã€‚å¹¶ä¸”èƒ½æˆåŠŸåº”ç”¨äºå•å‘éª¨æ¶TTSã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-28f48bfdbe354c503fc6249000511955~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306883&auth_key=1762306883-0-0-9fd46c49c6f454f67a5cfc52654d50dc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c89addb260e018c87e9e640d736dda3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306890&auth_key=1762306890-0-0-0c5f1582fdd92267391c992bacce639d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-38e5b98d73926dee6d1adfcf8a4fd647~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306897&auth_key=1762306897-0-0-6088a1a9d7d296af95c5968448a0a27d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EmoSteer-TTS-Fine-Grained-and-Training-Free-Emotion-Controllable-Text-to-Speech-via-Activation-Steering"><a href="#EmoSteer-TTS-Fine-Grained-and-Training-Free-Emotion-Controllable-Text-to-Speech-via-Activation-Steering" class="headerlink" title="EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable   Text-to-Speech via Activation Steering"></a>EmoSteer-TTS: Fine-Grained and Training-Free Emotion-Controllable   Text-to-Speech via Activation Steering</h2><p><strong>Authors:Tianxin Xie, Shan Yang, Chenxing Li, Dong Yu, Li Liu</strong></p>
<p>Text-to-speech (TTS) has shown great progress in recent years. However, most existing TTS systems offer only coarse and rigid emotion control, typically via discrete emotion labels or a carefully crafted and detailed emotional text prompt, making fine-grained emotion manipulation either inaccessible or unstable. These models also require extensive, high-quality datasets for training. To address these limitations, we propose EmoSteer-TTS, a novel training-free approach, to achieve fine-grained speech emotion control (conversion, interpolation, erasure) by activation steering. We first empirically observe that modifying a subset of the internal activations within a flow matching-based TTS model can effectively alter the emotional tone of synthesized speech. Building on this insight, we then develop a training-free and efficient algorithm, including activation extraction, emotional token searching, and inference-time steering, which can be seamlessly integrated into a wide range of pretrained models (e.g., F5-TTS, CosyVoice2, and E2-TTS). In addition, to derive effective steering vectors, we construct a curated emotional speech dataset with diverse speakers. Extensive experiments demonstrate that EmoSteer-TTS enables fine-grained, interpretable, and continuous control over speech emotion, outperforming the state-of-the-art (SOTA). To the best of our knowledge, this is the first method that achieves training-free and continuous fine-grained emotion control in TTS. Demo samples are available at <a target="_blank" rel="noopener" href="https://emosteer-tts-demo.pages.dev/">https://emosteer-tts-demo.pages.dev/</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åœ¨æœ€è¿‘å‡ å¹´å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„TTSç³»ç»Ÿåªæä¾›ç²—ç³™ä¸”åƒµåŒ–çš„æƒ…æ„Ÿæ§åˆ¶ï¼Œé€šå¸¸æ˜¯é€šè¿‡ç¦»æ•£çš„æƒ…ç»ªæ ‡ç­¾æˆ–ç²¾å¿ƒè®¾è®¡å’Œè¯¦ç»†è®¾è®¡çš„æƒ…ç»ªæ–‡æœ¬æç¤ºæ¥å®ç°çš„ï¼Œè¿™ä½¿å¾—ç²¾ç»†ç²’åº¦çš„æƒ…æ„Ÿæ“çºµè¦ä¹ˆæ— æ³•å®ç°ï¼Œè¦ä¹ˆä¸ç¨³å®šã€‚è¿™äº›æ¨¡å‹è¿˜éœ€è¦å¤§é‡é«˜è´¨é‡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EmoSteer-TTSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡æ¿€æ´»æ§åˆ¶å®ç°ç²¾ç»†ç²’åº¦çš„è¯­éŸ³æƒ…æ„Ÿæ§åˆ¶ï¼ˆè½¬æ¢ã€æ’å€¼ã€æ“¦é™¤ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆä»å®è¯ä¸Šè§‚å¯Ÿåˆ°ï¼ŒåŸºäºæµåŒ¹é…çš„TTSæ¨¡å‹å†…éƒ¨æ¿€æ´»çš„ä¸€ä¸ªå­é›†ä¿®æ”¹å¯ä»¥æœ‰æ•ˆåœ°æ”¹å˜åˆæˆè¯­éŸ³çš„æƒ…æ„ŸåŸºè°ƒã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œç„¶åæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ— è®­ç»ƒå’Œé«˜æ•ˆçš„ç®—æ³•ï¼ŒåŒ…æ‹¬æ¿€æ´»æå–ã€æƒ…æ„Ÿç¬¦å·æœç´¢å’Œæ¨ç†æ—¶é—´æ§åˆ¶ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°å„ç§é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¾‹å¦‚F5-TTSã€CosyVoice2å’ŒE2-TTSï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¾—å‡ºæœ‰æ•ˆçš„æ§åˆ¶å‘é‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…·æœ‰å¤šç§è¯´è¯äººçš„ç²¾é€‰æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEmoSteer-TTSèƒ½å¤Ÿå¯¹è¯­éŸ³æƒ…æ„Ÿè¿›è¡Œç²¾ç»†ã€å¯è§£é‡Šå’Œè¿ç»­çš„æ§åˆ¶ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ°´å¹³ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å®ç°æ— è®­ç»ƒå’Œè¿ç»­ç²¾ç»†æƒ…æ„Ÿæ§åˆ¶çš„TTSæ–¹æ³•ã€‚æ¼”ç¤ºæ ·å“å¯åœ¨<a target="_blank" rel="noopener" href="https://emosteer-tts-demo.pages.dev/%E6%89%BE%E5%88%B0%E3%80%82">https://emosteer-tts-demo.pages.dev/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03543v3">PDF</a> 25 pages, 9 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„æƒ…æ„Ÿæ§åˆ¶é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹æ–¹æ³•â€”â€”EmoSteer-TTSã€‚è¯¥æ–¹æ³•é€šè¿‡è§‚å¯Ÿå¹¶ä¿®æ”¹åŸºäºæµåŒ¹é…çš„TTSæ¨¡å‹å†…éƒ¨æ¿€æ´»çš„ä¸€éƒ¨åˆ†ï¼Œå®ç°å¯¹åˆæˆè¯­éŸ³æƒ…æ„Ÿçš„æœ‰æ•ˆæ§åˆ¶ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬æ¿€æ´»æå–ã€æƒ…æ„Ÿæ ‡è®°æœç´¢å’Œæ¨ç†æ—¶é—´æ§åˆ¶ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°å„ç§é¢„è®­ç»ƒæ¨¡å‹ä¸­ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†åŒ…å«ä¸åŒè¯´è¯äººçš„é«˜è´¨é‡æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ï¼Œç”¨äºè·å–æœ‰æ•ˆçš„æ§åˆ¶å‘é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒEmoSteer-TTSå¯å®ç°ç²¾ç»†ã€å¯è§£é‡Šå’Œè¿ç»­çš„æƒ…æ„Ÿæ§åˆ¶ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSæŠ€æœ¯åœ¨æƒ…æ„Ÿæ§åˆ¶æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¤§å¤šåªèƒ½è¿›è¡Œç²—ç•¥ä¸”å›ºå®šçš„æƒ…æ„Ÿè°ƒèŠ‚ã€‚</li>
<li>EmoSteer-TTSæ˜¯ä¸€ç§æ–°å‹ã€æ— éœ€è®­ç»ƒçš„TTSæ–¹æ³•ï¼Œå®ç°äº†å¯¹è¯­éŸ³æƒ…æ„Ÿçš„ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>EmoSteer-TTSé€šè¿‡ä¿®æ”¹TTSæ¨¡å‹çš„å†…éƒ¨æ¿€æ´»éƒ¨åˆ†æ¥æ”¹å˜åˆæˆè¯­éŸ³çš„æƒ…æ„Ÿè°ƒå­ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…æ‹¬æ¿€æ´»æå–ã€æƒ…æ„Ÿæ ‡è®°æœç´¢å’Œæ¨ç†æ—¶é—´æ§åˆ¶ç­‰æ­¥éª¤ã€‚</li>
<li>EmoSteer-TTSèƒ½æ— ç¼é›†æˆåˆ°å„ç§é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œå¹¶æ„å»ºäº†é«˜è´¨é‡çš„æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ã€‚</li>
<li>EmoSteer-TTSå®ç°äº†ç²¾ç»†ã€å¯è§£é‡Šå’Œè¿ç»­çš„æƒ…æ„Ÿæ§åˆ¶ï¼Œä¼˜äºå½“å‰çš„æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-73bdad71594a4f1c5965aea4268f5463~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306904&auth_key=1762306904-0-0-d2d3b40744d84d35705e7f7e4a4b80d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-07d8b22a40d7e542c71fcb161a4892ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306911&auth_key=1762306911-0-0-222ad9065322c2b75ddee8e639fe3ebe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8a53baeb651081819e77749b96b46e88~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306918&auth_key=1762306918-0-0-70b87b8a29cea6dbc916359909e86689&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a56e72b237fb0640d4a16a1b4a982699~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306925&auth_key=1762306925-0-0-da4f5794f1f18cba04e53c4712b62717&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="OpenS2S-Advancing-Fully-Open-Source-End-to-End-Empathetic-Large-Speech-Language-Model"><a href="#OpenS2S-Advancing-Fully-Open-Source-End-to-End-Empathetic-Large-Speech-Language-Model" class="headerlink" title="OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech   Language Model"></a>OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech   Language Model</h2><p><strong>Authors:Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang</strong></p>
<p>Empathetic interaction is a cornerstone of human-machine communication, due to the need for understanding speech enriched with paralinguistic cues and generating emotional and expressive responses. However, the most powerful empathetic LSLMs are increasingly closed off, leaving the crucial details about the architecture, data and development opaque to researchers. Given the critical need for transparent research into the LSLMs and empathetic behavior, we present OpenS2S, a fully open-source, transparent and end-to-end LSLM designed to enable empathetic speech interactions. Based on our empathetic speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved decoding architecture to achieve low-latency speech generation. To facilitate end-to-end training, OpenS2S incorporates an automated data construction pipeline that synthesizes diverse, high-quality empathetic speech dialogues at low cost. By leveraging large language models to generate empathetic content and controllable text-to-speech systems to introduce speaker and emotional variation, we construct a scalable training corpus with rich paralinguistic diversity and minimal human supervision. We release the fully open-source OpenS2S model, including the dataset, model weights, pre-training and fine-tuning codes, to empower the broader research community and accelerate innovation in empathetic speech systems. The project webpage can be accessed at <a target="_blank" rel="noopener" href="https://casia-lm.github.io/OpenS2S">https://casia-lm.github.io/OpenS2S</a> </p>
<blockquote>
<p>å…±æƒ…äº¤äº’æ˜¯äººç±»ä¸æœºå™¨é€šä¿¡çš„åŸºçŸ³ï¼Œè¿™æ˜¯å› ä¸ºéœ€è¦ç†è§£èå…¥å‰¯è¯­è¨€çº¿ç´¢çš„è¯­éŸ³å¹¶äº§ç”Ÿæƒ…æ„Ÿå’Œè¡¨è¾¾æ€§å›åº”ã€‚ç„¶è€Œï¼Œæœ€å¼ºå¤§çš„å…±æƒ…LSLMï¼ˆè¯­è¨€æ¨¡å‹ï¼‰å´è¶Šæ¥è¶Šå°é—­ï¼Œä½¿ç ”ç©¶è€…å¯¹æ¶æ„ã€æ•°æ®å’Œå‘å±•ç­‰å…³é”®ç»†èŠ‚æ„Ÿåˆ°å›°æƒ‘ã€‚é‰´äºå¯¹LSLMå’Œå…±æƒ…è¡Œä¸ºé€æ˜ç ”ç©¶çš„è¿«åˆ‡éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OpenS2Sï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºã€é€æ˜å’Œç«¯åˆ°ç«¯çš„LSLMï¼Œæ—¨åœ¨å®ç°å…±æƒ…è¯­éŸ³äº¤äº’ã€‚OpenS2SåŸºäºæˆ‘ä»¬çš„å…±æƒ…è¯­éŸ³åˆ°æ–‡æœ¬æ¨¡å‹BLSP-Emoï¼Œå¹¶è¿›ä¸€æ­¥é‡‡ç”¨æµå¼äº¤ç»‡è§£ç æ¶æ„æ¥å®ç°ä½å»¶è¿Ÿè¯­éŸ³ç”Ÿæˆã€‚ä¸ºäº†ä¿ƒè¿›ç«¯åˆ°ç«¯è®­ç»ƒï¼ŒOpenS2Sèå…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ•°æ®æ„å»ºç®¡é“ï¼Œä»¥ä½æˆæœ¬åˆæˆå¤šæ ·ã€é«˜è´¨é‡çš„å…±æƒ…è¯­éŸ³å¯¹è¯ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå…±æƒ…å†…å®¹ï¼Œå¹¶åˆ©ç”¨å¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿæ¥å¼•å…¥å‘è¨€è€…å’Œæƒ…æ„Ÿå˜åŒ–ï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªå…·æœ‰ä¸°å¯Œçš„å‰¯è¯­è¨€å¤šæ ·æ€§å’Œæœ€å°äººå·¥ç›‘ç£çš„å¯æ‰©å±•è®­ç»ƒè¯­æ–™åº“ã€‚æˆ‘ä»¬å‘å¸ƒäº†å®Œå…¨å¼€æºçš„OpenS2Sæ¨¡å‹å’Œä¸€ç³»åˆ—é…å¥—èµ„æºï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€æ¨¡å‹æƒé‡ã€é¢„è®­ç»ƒå’Œå¾®è°ƒä»£ç ç­‰ï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„ç ”ç©¶ç¾¤ä½“å¹¶åŠ é€Ÿå…±æƒ…è¯­éŸ³ç³»ç»Ÿçš„åˆ›æ–°ã€‚é¡¹ç›®ç½‘é¡µå¯è®¿é—®<a target="_blank" rel="noopener" href="https://casia-lm.github.io/OpenS2S%E3%80%82">https://casia-lm.github.io/OpenS2Sã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05177v3">PDF</a> Technical Report, Update on OpenS2S_v1.5</p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººç±»æƒ…æ„Ÿäº¤æµçš„éœ€æ±‚ï¼Œå…¬å¼€å’Œé€æ˜çš„LSLMæ¨¡å‹å¯¹å®ç°äººæ€§åŒ–äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºOpenS2Sæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¼€æ”¾æºä»£ç çš„æ–¹å¼æ¨åŠ¨å…±æƒ…è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„å‘å±•ã€‚è¯¥æ¨¡å‹ä»¥BLSP-Emoæ¨¡å‹ä¸ºåŸºç¡€ï¼Œå…·å¤‡æµå¼è§£ç æ¶æ„ï¼Œèƒ½å¤Ÿå®ç°ä½å»¶è¿Ÿè¯­éŸ³ç”Ÿæˆã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨äº†è‡ªåŠ¨åŒ–æ•°æ®æ„å»ºæµç¨‹ï¼Œä»¥ä½æˆæœ¬åˆæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å…±æƒ…å¯¹è¯æ•°æ®ã€‚é€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç”Ÿæˆå…±æƒ…å†…å®¹ï¼Œå¹¶ç»“åˆå¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿå¼•å…¥è¯´è¯äººå’Œæƒ…æ„Ÿå˜åŒ–ï¼Œæ„å»ºäº†ä¸€ä¸ªå…·æœ‰ä¸°å¯Œæƒ…æ„Ÿè¡¨è¾¾çš„å¯æ‰©å±•è®­ç»ƒè¯­æ–™åº“ã€‚OpenS2Sæ¨¡å‹çš„å¼€æºå‘å¸ƒå°†ä¿ƒè¿›æ›´å¹¿æ³›çš„ç ”ç©¶ç¤¾åŒºå‚ä¸å’Œå…±æƒ…è¯­éŸ³ç³»ç»Ÿçš„åˆ›æ–°åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…±æƒ…äº¤äº’æ˜¯äººä¸æœºå™¨é€šä¿¡çš„æ ¸å¿ƒï¼Œéœ€è¦ç†è§£åŒ…å«å‰¯è¯­è¨€çº¿ç´¢çš„è¯­éŸ³å¹¶ç”Ÿæˆæƒ…æ„Ÿä¸è¡¨è¾¾å›åº”ã€‚</li>
<li>OpenS2Sæ˜¯ä¸€ä¸ªå¼€æºã€é€æ˜ã€ç«¯åˆ°ç«¯çš„LSLMæ¨¡å‹ï¼Œæ—¨åœ¨å®ç°å…±æƒ…è¯­éŸ³è¯†åˆ«äº¤äº’ã€‚</li>
<li>OpenS2Sæ¨¡å‹åŸºäºBLSP-Emoæ¨¡å‹ï¼Œé‡‡ç”¨æµå¼è§£ç æ¶æ„å®ç°ä½å»¶è¿Ÿè¯­éŸ³ç”Ÿæˆã€‚</li>
<li>è‡ªåŠ¨åŒ–æ•°æ®æ„å»ºæµç¨‹ç”¨äºåˆæˆå¤šæ ·ã€é«˜è´¨é‡çš„å…±æƒ…å¯¹è¯æ•°æ®ï¼Œé™ä½æˆæœ¬ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç”Ÿæˆå…±æƒ…å†…å®¹ï¼Œå¹¶ç»“åˆå¯æ§æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿå¼•å…¥è¯´è¯äººå’Œæƒ…æ„Ÿå˜åŒ–ã€‚</li>
<li>OpenS2Sæ¨¡å‹æ„å»ºäº†ä¸€ä¸ªå…·æœ‰ä¸°å¯Œæƒ…æ„Ÿè¡¨è¾¾çš„å¯æ‰©å±•è®­ç»ƒè¯­æ–™åº“ï¼ŒåŒ…å«å¤šæ ·çš„å‰¯è¯­è¨€å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cae1c4dd439f0e81f2bb189c2eb1d086~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306932&auth_key=1762306932-0-0-cc6e10bdd5c799a735ecc783046e7bf9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7ced0aeeb0c8672c83d1717d091abae1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306939&auth_key=1762306939-0-0-9a29d75162c4395db483092bf92c34f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33742846ad1c974f90e064678638b0a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306946&auth_key=1762306946-0-0-fbc885f7486e13f3f3ed82e55a7c6813&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f1c648885774248d245aa9a82a49dd7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306953&auth_key=1762306953-0-0-d58eb403544057dac48659e157b4df4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="OmniResponse-Online-Multimodal-Conversational-Response-Generation-in-Dyadic-Interactions"><a href="#OmniResponse-Online-Multimodal-Conversational-Response-Generation-in-Dyadic-Interactions" class="headerlink" title="OmniResponse: Online Multimodal Conversational Response Generation in   Dyadic Interactions"></a>OmniResponse: Online Multimodal Conversational Response Generation in   Dyadic Interactions</h2><p><strong>Authors:Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem</strong></p>
<p>In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task designed to produce synchronized verbal and non-verbal listener feedback online, based on the speakerâ€™s multimodal inputs. OMCRG captures natural dyadic interactions and introduces new challenges in aligning generated audio with listenersâ€™ facial responses. To tackle these challenges, we incorporate text as an intermediate modality to connect audio and facial responses. We propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates accurate multimodal listener responses. OmniResponse leverages a pretrained LLM enhanced with two core components: Chrono-Text Markup, which precisely timestamps generated text tokens, and TempoVoice, a controllable online text-to-speech (TTS) module that outputs speech synchronized with facial responses. To advance OMCRG research, we offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and annotated facial behaviors. Comprehensive evaluations on ResponseNet demonstrate that OmniResponse outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality. Our dataset, code, and models are publicly available. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨çº¿å¤šæ¨¡æ€å¯¹è¯å“åº”ç”Ÿæˆï¼ˆOMCRGï¼‰è¿™ä¸€æ–°ä»»åŠ¡ï¼Œå®ƒæ—¨åœ¨åŸºäºè¯´è¯è€…çš„å¤šæ¨¡æ€è¾“å…¥ï¼Œåœ¨çº¿äº§ç”ŸåŒæ­¥çš„è¨€è¯­å’Œéè¨€è¯­å¬ä¼—åé¦ˆã€‚OMCRGæ•æ‰äº†è‡ªç„¶çš„äºŒå…ƒäº’åŠ¨ï¼Œå¹¶åœ¨å°†ç”Ÿæˆçš„éŸ³é¢‘ä¸å¬ä¼—çš„é¢éƒ¨å“åº”å¯¹é½æ–¹é¢å¼•å…¥äº†æ–°çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨æ–‡æœ¬ä½œä¸ºè¿æ¥éŸ³é¢‘å’Œé¢éƒ¨å“åº”çš„ä¸­é—´æ¨¡æ€ã€‚æˆ‘ä»¬æå‡ºäº†OmniResponseï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå‡†ç¡®çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ã€‚OmniResponseåˆ©ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­æœ‰ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šChrono-Text Markupï¼Œå®ƒç²¾ç¡®åœ°æ ‡è®°ç”Ÿæˆæ–‡æœ¬çš„æ—¶é—´æˆ³ï¼›ä»¥åŠTempoVoiceï¼Œä¸€ä¸ªå¯æ§çš„åœ¨çº¿æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ï¼Œè¾“å‡ºä¸é¢éƒ¨å“åº”åŒæ­¥çš„è¯­éŸ³ã€‚ä¸ºäº†æ¨åŠ¨OMCRGç ”ç©¶çš„å‘å±•ï¼Œæˆ‘ä»¬æä¾›äº†ResponseNetæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«696ä¸ªè¯¦ç»†çš„äºŒå…ƒäº’åŠ¨ç‰¹å¾ï¼Œå…·æœ‰åŒæ­¥åˆ†å±è§†é¢‘ã€å¤šé€šé“éŸ³é¢‘ã€æ–‡å­—è®°å½•ä»¥åŠæ³¨é‡Šçš„é¢éƒ¨è¡Œä¸ºã€‚åœ¨ResponseNetä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniResponseåœ¨è¯­ä¹‰è¯­éŸ³å†…å®¹ã€è§†å¬åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å‡å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21724v2">PDF</a> 25 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨çº¿å¤šæ¨¡æ€å¯¹è¯å“åº”ç”Ÿæˆï¼ˆOMCRGï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨åŸºäºè¯´è¯è€…çš„å¤šæ¨¡æ€è¾“å…¥ï¼Œåœ¨çº¿ç”ŸæˆåŒæ­¥çš„è¨€è¯­å’Œéè¨€è¯­å¬ä¼—åé¦ˆã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†OmniResponseè¿™ä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œè¯¥æ¨¡å‹å¯è‡ªåŠ¨ç”Ÿæˆå‡†ç¡®çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ã€‚OmniResponseåˆ©ç”¨äº†ä¸¤é¡¹æ ¸å¿ƒæŠ€æœ¯ï¼šChrono-Text Markupå’ŒTempoVoiceã€‚å‰è€…ä¸ºç”Ÿæˆçš„æ–‡æœ¬æ ‡è®°æä¾›ç²¾ç¡®æ—¶é—´æˆ³ï¼Œåè€…åˆ™æ˜¯å¯æ§çš„åœ¨çº¿æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ï¼Œå¯è¾“å‡ºä¸é¢éƒ¨å“åº”åŒæ­¥çš„è¯­éŸ³ã€‚ä¸ºæ¨è¿›OMCRGç ”ç©¶ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†ResponseNetæ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯æ˜äº†OmniResponseåœ¨è¯­ä¹‰è¯­éŸ³å†…å®¹ã€è§†å¬åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OMCRGä»»åŠ¡æ—¨åœ¨åœ¨çº¿ç”ŸæˆåŸºäºè¯´è¯è€…å¤šæ¨¡æ€è¾“å…¥çš„åŒæ­¥è¨€è¯­å’Œéè¨€è¯­å¬ä¼—åé¦ˆã€‚</li>
<li>OmniResponseæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆå‡†ç¡®çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ã€‚</li>
<li>OmniResponseåŒ…å«ä¸¤å¤§æ ¸å¿ƒæŠ€æœ¯ï¼šChrono-Text Markupå’ŒTempoVoiceï¼Œåˆ†åˆ«æä¾›ç²¾ç¡®æ—¶é—´æˆ³å’Œåœ¨çº¿æ–‡æœ¬åˆ°è¯­éŸ³çš„è½¬æ¢ã€‚</li>
<li>ResponseNetæ•°æ®é›†åŒ…å«äº†696ä¸ªè¯¦ç»†çš„åŒäººäº’åŠ¨æ•°æ®ï¼Œç‰¹å¾åŒ…æ‹¬åŒæ­¥åˆ†å±è§†é¢‘ã€å¤šé€šé“éŸ³é¢‘ã€æ–‡å­—è®°å½•ä»¥åŠé¢éƒ¨è¡Œä¸ºæ³¨é‡Šã€‚</li>
<li>OmniResponseåœ¨è¯­ä¹‰è¯­éŸ³å†…å®¹ã€è§†å¬åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>OmniResponseã€æ•°æ®é›†ä»¥åŠä»£ç å‡å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0df4a9ac575b127e34446fcd7a50c026~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306960&auth_key=1762306960-0-0-92bc9920ecc019c87465246190610b1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d896a6eac8a0f723e85c074d728ae289~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306968&auth_key=1762306968-0-0-9662b7dd1fec63377d69d77604a1d5a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-32a0bd84439c93acc23ae8db2fea79a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306974&auth_key=1762306974-0-0-64d408e68abf0fc5267ea834f805b65a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fdd417182d8fd617532a5348dc4b862~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306981&auth_key=1762306981-0-0-d3578b11b7e38d46db9da16edc1652ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rethinking-Optimal-Verification-Granularity-for-Compute-Efficient-Test-Time-Scaling"><a href="#Rethinking-Optimal-Verification-Granularity-for-Compute-Efficient-Test-Time-Scaling" class="headerlink" title="Rethinking Optimal Verification Granularity for Compute-Efficient   Test-Time Scaling"></a>Rethinking Optimal Verification Granularity for Compute-Efficient   Test-Time Scaling</h2><p><strong>Authors:Hao Mark Chen, Guanxi Lu, Yasuyuki Okoshi, Zhiwen Mo, Masato Motomura, Hongxiang Fan</strong></p>
<p>Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1% over Beam Search and 3.6% over Best-of-N, while reducing FLOPs by over 52%. We will open-source the code to support future research. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰å·²è¯æ˜å¯ä»¥æœ‰æ•ˆæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚éªŒè¯åœ¨TTSä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œç”±äºéªŒè¯çš„è´¨é‡å’Œè®¡ç®—æˆæœ¬ï¼Œå®ƒåŒæ—¶å½±å“ï¼ˆ1ï¼‰æ¨ç†æ€§èƒ½å’Œï¼ˆ2ï¼‰è®¡ç®—æ•ˆç‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æŒ‘æˆ˜äº†éªŒè¯çš„ä¼ ç»ŸèŒƒå¼ï¼Œå¹¶é¦–æ¬¡å°è¯•ç³»ç»Ÿåœ°ç ”ç©¶éªŒè¯ç²’åº¦çš„å½±å“ï¼Œå³éªŒè¯å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¢«è°ƒç”¨çš„é¢‘ç‡ï¼Œè€Œä¸ä»…ä»…æ˜¯éªŒè¯æœ€ç»ˆè¾“å‡ºæˆ–å•ä¸ªç”Ÿæˆæ­¥éª¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯å˜ç²’åº¦æœç´¢ï¼ˆVG-Searchï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¯è°ƒç²’åº¦å‚æ•°gæ¨å¹¿æŸæœç´¢å’ŒNé€‰ä½³é‡‡æ ·çš„ç»Ÿä¸€ç®—æ³•ã€‚ä½¿ç”¨VG-Searchåœ¨å¤šç§è®¡ç®—é¢„ç®—ã€ç”Ÿæˆå™¨-éªŒè¯å™¨é…ç½®å’Œä»»åŠ¡å±æ€§ä¸‹è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒåŠ¨æ€é€‰æ‹©gå¯ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œç¼©æ”¾æ€§èƒ½ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”VG-Searchç­–ç•¥ï¼Œä¸æŸæœç´¢ç›¸æ¯”æé«˜äº†é«˜è¾¾3.1%çš„å‡†ç¡®ç‡ï¼Œä¸Né€‰ä½³é‡‡æ ·ç›¸æ¯”æé«˜äº†3.6%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å‡å°‘äº†è¶…è¿‡52%çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€‚æˆ‘ä»¬å°†å…¬å¼€æºä»£ç ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11730v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰å·²è¯æ˜å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚éªŒè¯åœ¨TTSä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼ŒåŒæ—¶å½±å“æ¨ç†æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œè¿™å–å†³äºéªŒè¯çš„è´¨é‡å’Œè®¡ç®—æˆæœ¬ã€‚æœ¬ç ”ç©¶æŒ‘æˆ˜äº†éªŒè¯çš„ä¼ ç»ŸèŒƒå¼ï¼Œé¦–æ¬¡å°è¯•ç³»ç»Ÿåœ°ç ”ç©¶éªŒè¯ç²’åº¦çš„å½±å“ï¼Œå³éªŒè¯å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¢«è°ƒç”¨çš„é¢‘ç‡ï¼Œè€Œä¸ä»…ä»…æ˜¯éªŒè¯æœ€ç»ˆè¾“å‡ºæˆ–å•ä¸ªç”Ÿæˆæ­¥éª¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯å˜ç²’åº¦æœç´¢ï¼ˆVG-Searchï¼‰ï¼Œé€šè¿‡å¯è°ƒç²’åº¦å‚æ•°gï¼Œç»Ÿä¸€äº†æŸæœç´¢å’Œæœ€ä½³Né‡‡æ ·ã€‚åœ¨VG-Searchä¸‹çš„å„ç§è®¡ç®—é¢„ç®—ã€ç”Ÿæˆå™¨-éªŒè¯å™¨é…ç½®å’Œä»»åŠ¡å±æ€§è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒåŠ¨æ€é€‰æ‹©gå¯ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œç¼©æ”¾æ€§èƒ½ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”VG-Searchç­–ç•¥ï¼Œä¸æŸæœç´¢ç›¸æ¯”æé«˜äº†é«˜è¾¾3.1%çš„å‡†ç¡®ç‡ï¼Œä¸æœ€ä½³Né‡‡æ ·ç›¸æ¯”æé«˜äº†3.6%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å‡å°‘äº†è¶…è¿‡52%çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€‚æˆ‘ä»¬å°†å…¬å¼€æºä»£ç ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éªŒè¯åœ¨TTSä¸­åŒæ—¶å½±å“æ¨ç†æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„éªŒè¯æ–¹æ³•â€”â€”å¯å˜ç²’åº¦æœç´¢ï¼ˆVG-Searchï¼‰ï¼Œé€šè¿‡è°ƒæ•´ç²’åº¦å‚æ•°gæ¥ä¼˜åŒ–æœç´¢è¿‡ç¨‹ã€‚</li>
<li>VG-Searchåœ¨å¤šç§è®¡ç®—é¢„ç®—ã€ç”Ÿæˆå™¨-éªŒè¯å™¨é…ç½®å’Œä»»åŠ¡å±æ€§ä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒåŠ¨æ€è°ƒæ•´ç²’åº¦å‚æ•°gå¯ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œç¼©æ”¾æ€§èƒ½ã€‚</li>
<li>è‡ªé€‚åº”VG-Searchç­–ç•¥åœ¨å‡†ç¡®ç‡ä¸Šæœ‰äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶å‡å°‘äº†æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€‚</li>
<li>æœ¬ç ”ç©¶æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„éªŒè¯èŒƒå¼ï¼Œä¸ºæœªæ¥çš„è¯­è¨€æ¨¡å‹æ¨ç†æä¾›äº†æ–°æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c0005551245022ab0c2d503408d6e180~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306988&auth_key=1762306988-0-0-bff667df00435e1bcc80ad7d880810f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6389e0936ad5c8b1852e1514fde4630d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762306996&auth_key=1762306996-0-0-dd9fbaa42ab3399aae1edd85021e3946&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-74c4f442a346921b88c6605d15de7f7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307002&auth_key=1762307002-0-0-6180a8f12925b768f449c66ab3abbd68&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-32eda31e57248cb49422b611293c4efe~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307009&auth_key=1762307009-0-0-4d110a4195f0443f9fd8d35a4c7c416e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. Code has been released at <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦é›†ä¸­äºæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¾ƒå°‘å…³æ³¨è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œç”±äºåœ¨æ ¹æœ¬çš„æ¨¡æ€å·®å¼‚ï¼Œå®ç°åœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­éƒ½é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œé€æ­¥è®­ç»ƒLLMä»¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆä½¿æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè€Œä¸”èƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œè¯­éŸ³å¯¹è¯ï¼Œæ— éœ€å•ç‹¬çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å—ï¼Œæå¤§åœ°åŠ é€Ÿäº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°åŸºå‡†æµ‹è¯•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œèƒ½å¤Ÿå®ç°è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA%E3%80%82">https://github.com/VITA-MLLM/VITAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v4">PDF</a> NeurIPS 2025 Spotlight, Code 2.4K Stars:   <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a></p>
<p><strong>æ‘˜è¦</strong><br>    æœ€æ–°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸»è¦å…³æ³¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„èåˆï¼Œè¾ƒå°‘å…³æ³¨è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œç”±äºåŸºæœ¬æ¨¡æ€å·®å¼‚ï¼Œåœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­å®ç°é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œé€æ­¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆå®ç°æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜èƒ½åœ¨ä¸ä½¿ç”¨å•ç‹¬çš„ASRå’ŒTTSæ¨¡å—çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼Œä»è€Œæå¤§åœ°åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°å‰æ²¿æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ¨¡å‹åœ¨è§†è§‰å’Œè¯­éŸ³æ–¹é¢éƒ½å…·æœ‰å¼ºå¤§çš„åŠŸèƒ½ï¼Œå¯å®ç°è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA%E3%80%82">https://github.com/VITA-MLLM/VITAã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­å¯¹è¯­éŸ³çš„é‡è¦æ€§è¢«ä½ä¼°ã€‚</li>
<li>å®ç°è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡çš„é«˜æ€§èƒ½æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºåŸºç¡€æ¨¡æ€çš„å·®å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶ç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ã€‚</li>
<li>æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜èƒ½å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯åŠŸèƒ½ï¼Œæ— éœ€é¢å¤–çš„ASRå’ŒTTSæ¨¡å—ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚</li>
<li>ä¸å‰æ²¿æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸Šå…·æœ‰å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a862c6482be13d4067b24007a2c6afae~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307016&auth_key=1762307016-0-0-4123d2b2298f0f77f74a3cea40fef64d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dcced7eae39d85540050428837191e16~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307024&auth_key=1762307024-0-0-6118f5220ccfce2f6d994475bd6661a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-af64631cf671fc440e48a2e406545035~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307030&auth_key=1762307030-0-0-5e66822ee393e61aed96a28f706ec2b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed82f2ce1ffcecc27be32cc14be801b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307037&auth_key=1762307037-0-0-be5b25a337ded1ccd910a4c60c65629e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="UVIT-AstroSat-observation-of-TW-Hya"><a href="#UVIT-AstroSat-observation-of-TW-Hya" class="headerlink" title="UVIT&#x2F;AstroSat observation of TW Hya"></a>UVIT&#x2F;AstroSat observation of TW Hya</h2><p><strong>Authors:Prasanta K. Nayak, Mayank Narang, P. Manoj, D. K. Ojha, Blesson Mathew, T. Baug, S. Chandra, S. Vig, G. Maheswar, U. S. Kamath</strong></p>
<p>The paper demonstrates the spectroscopic and photometric capabilities of the Ultra-Violet Imaging Telescope (UVIT) to study T-Tauri stars (TTSs). We present the first UVIT&#x2F;Far-UV spectrum of a TTS, TW Hya. Based on C IV line luminosity, we estimated accretion luminosity (0.1 $L_\odot$) and mass accretion rate (2.2 $\times$ $10^{-8} M_\odot &#x2F;yr$) of TW Hya, and compared these values with the accretion luminosity (0.03 $L_\odot$) and mass accretion rate (0.6 $\times$ $10^{-8} M_\odot &#x2F;yr$) derived from spectral energy distribution (SED). From the SED, we derive best-fitted parameters for TW Hya: $T_{eff}$ &#x3D; 3900$\pm$50 K, radius &#x3D; 1.2$\pm$0.03 $R_\odot$, $\mathrm{log}, g &#x3D; 4.0$ and equivalent black-body temperatures corresponding to accretion luminosity as 14100$\pm$25 K. The parameters of TW Hya derived from UVIT observations were found to be matched well with the literature. Comparison with IUE spectra also suggests that UVIT can be used to study the spectroscopic variability of young stars. This study proposes leveraging the FUV spectroscopic capabilities of UVIT to contribute to the advancement of upcoming UV spectroscopic missions, including the Indian Spectroscopic Imaging Space Telescope (INSIST). </p>
<blockquote>
<p>æœ¬æ–‡å±•ç¤ºäº†ç´«å¤–æˆåƒæœ›è¿œé•œï¼ˆUVITï¼‰ç ”ç©¶T Tauriæ’æ˜Ÿï¼ˆTTSï¼‰çš„å…‰è°±å’Œå…‰åº¦æµ‹é‡èƒ½åŠ›ã€‚æˆ‘ä»¬å±•ç¤ºäº†TTSçš„UVIT&#x2F;è¿œç´«å¤–å…‰è°±çš„é¦–ä¸ªå®ä¾‹ï¼Œå³TW Hydraã€‚åŸºäºC IVçº¿å…‰åº¦ï¼Œæˆ‘ä»¬ä¼°è®¡äº†TW Hydraçš„å¸ç§¯å…‰åº¦ï¼ˆ0.1 $L_\odot$ï¼‰å’Œè´¨é‡å¸ç§¯ç‡ï¼ˆ2.2 x $10^{-8} M_\odot &#x2F;yr$ï¼‰ï¼Œå¹¶å°†è¿™äº›å€¼ä¸é€šè¿‡è°±èƒ½é‡åˆ†å¸ƒï¼ˆSEDï¼‰å¾—å‡ºçš„å¸ç§¯å…‰åº¦ï¼ˆ0.03 $L_\odot$ï¼‰å’Œè´¨é‡å¸ç§¯ç‡ï¼ˆ0.6 x $10^{-8} M_\odot &#x2F;yr$ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚é€šè¿‡SEDï¼Œæˆ‘ä»¬å¾—å‡ºTW Hydraçš„æœ€ä½³æ‹Ÿåˆå‚æ•°ï¼šæœ‰æ•ˆæ¸©åº¦$T_{eff}$ &#x3D; 3900Â±50 Kï¼ŒåŠå¾„&#x3D; 1.2Â±0.03 $R_\odot$ï¼Œå¯¹æ•°é‡åŠ› $\mathrm{log}, g &#x3D; 4.0$ ï¼Œä»¥åŠå¯¹åº”äºå¸ç§¯å…‰åº¦çš„ç­‰æ•ˆé»‘ä½“æ¸©åº¦ä¸º14100Â±25 Kã€‚ä»UVITè§‚æµ‹ç»“æœå¾—å‡ºçš„TW Hydraçš„å‚æ•°ä¸æ–‡çŒ®ä¸­çš„è®°å½•ç›¸åŒ¹é…ã€‚ä¸IUEå…‰è°±çš„æ¯”è¾ƒä¹Ÿè¡¨æ˜ï¼ŒUVITå¯ç”¨äºç ”ç©¶å¹´è½»æ’æ˜Ÿçš„å…‰è°±å˜åŒ–ã€‚è¯¥ç ”ç©¶å»ºè®®å……åˆ†åˆ©ç”¨UVITçš„è¿œç´«å¤–å…‰è°±åŠŸèƒ½ï¼Œä¸ºå³å°†åˆ°æ¥çš„ç´«å¤–å…‰è°±ä»»åŠ¡ï¼ˆåŒ…æ‹¬å°åº¦å…‰è°±æˆåƒç©ºé—´æœ›è¿œé•œï¼ˆINSISTï¼‰ï¼‰åšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.19478v2">PDF</a> The paper has not been published yet and a major revision is required</p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡å±•ç¤ºäº†ç´«å¤–çº¿æˆåƒæœ›è¿œé•œï¼ˆUVITï¼‰ç ”ç©¶T Tauriæ’æ˜Ÿï¼ˆTTSsï¼‰çš„å…‰è°±å’Œå…‰åº¦èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹TW Hydraè¿›è¡Œäº†é¦–æ¬¡UVIT&#x2F;è¿œç´«å¤–å…‰è°±è§‚æµ‹ï¼Œæ ¹æ®å…¶CIVçº¿å…‰åº¦ä¼°è®¡äº†å¸ç§¯å…‰åº¦ï¼ˆ0.1 LâŠ™ï¼‰å’Œè´¨é‡å¸ç§¯ç‡ï¼ˆ2.2Ã—10^-8 MâŠ™&#x2F;yrï¼‰ï¼Œå¹¶ä¸é€šè¿‡å…‰è°±èƒ½é‡åˆ†å¸ƒï¼ˆSEDï¼‰å¾—å‡ºçš„å¸ç§¯å…‰åº¦ï¼ˆ0.03 LâŠ™ï¼‰å’Œè´¨é‡å¸ç§¯ç‡ï¼ˆ0.6Ã—10^-8 MâŠ™&#x2F;yrï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚UVITè§‚æµ‹çš„TW Hydraå‚æ•°ä¸æ–‡çŒ®è®°è½½å»åˆè‰¯å¥½ã€‚å¯¹æ¯”IUESå…‰è°±è¡¨æ˜ï¼ŒUVITå¯ç”¨äºç ”ç©¶å¹´è½»æ’æ˜Ÿçš„å…‰è°±å˜åŒ–ã€‚è¯¥ç ”ç©¶å»ºè®®åˆ©ç”¨UVITçš„è¿œç´«å¤–å…‰è°±åŠŸèƒ½ï¼Œä¸ºæœªæ¥çš„ç´«å¤–å…‰è°±ä»»åŠ¡ï¼ŒåŒ…æ‹¬å°åº¦å…‰è°±æˆåƒç©ºé—´æœ›è¿œé•œï¼ˆINSISTï¼‰åšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UVITå…·æœ‰ç ”ç©¶TTSsçš„å…‰è°±å’Œå…‰åº¦èƒ½åŠ›ã€‚</li>
<li>TW Hydraçš„é¦–æ¬¡UVIT&#x2F;Far-UVå…‰è°±æ˜¾ç¤ºå…¶ç‹¬ç‰¹çš„ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡CIVçº¿äº®åº¦ä¼°è®¡äº†TW Hydraçš„å¸ç§¯äº®åº¦å’Œè´¨é‡å¸ç§¯ç‡ã€‚</li>
<li>UVITè§‚æµ‹å¾—å‡ºçš„TW Hydraå‚æ•°ä¸ç°æœ‰æ–‡çŒ®æ•°æ®ç›¸ç¬¦ã€‚</li>
<li>UVITè§‚æµ‹ç»“æœä¸ä¼ ç»ŸIUEå…‰è°±å¯¹æ¯”ï¼ŒéªŒè¯äº†å…¶åœ¨ç ”ç©¶å¹´è½»æ’æ˜Ÿå…‰è°±å˜åŒ–æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>UVITçš„FUVå…‰è°±åŠŸèƒ½å¯¹äºæ¨è¿›æœªæ¥çš„ç´«å¤–å…‰è°±ä»»åŠ¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.19478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c3f3bfce16e539a09c5cf10c75c6cc4a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307045&auth_key=1762307045-0-0-80f85c7f1ace328e79de850a1786a1f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-911a635d669b8cef07768c52e8f9b425~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307052&auth_key=1762307052-0-0-e58a901d35d9c02fbf55515948262011&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4306c4fb8e3e05e2ea60da1bd67c4192~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307059&auth_key=1762307059-0-0-fc33df71ac9fb7c733050a786e1a384c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb333d0770af3956e00b5c16fa637f62~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307067&auth_key=1762307067-0-0-88818ae898850268734cf7661a1ae0ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a7350cd756c59e6ebdd3f179ad4b3df3~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307074&auth_key=1762307074-0-0-bf4315db2f6129847401851e3994ee11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-05/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-1988fbce9480b6158d44653acfadfc9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762307888&auth_key=1762307888-0-0-7ef442949a63ffdeb62d5e872ae8c432&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  LISTEN to Your Preferences An LLM Framework for Multi-Objective   Selection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-92e5e711b5c7ab70b00de81301a94f57~resize:0:q75.jpg?source=1f5c5e47&expiration=1762304869&auth_key=1762304869-0-0-7a1abb02d01d77cd84e3da91b5b1a863&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-05  Navigated hepatic tumor resection using intraoperative ultrasound   imaging
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
