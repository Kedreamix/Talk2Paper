<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-23  Automated Measurement of Eczema Severity with Self-Supervised Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-198ba6e2b646fbf10214dabbcb289805.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-23-更新"><a href="#2025-04-23-更新" class="headerlink" title="2025-04-23 更新"></a>2025-04-23 更新</h1><h2 id="Automated-Measurement-of-Eczema-Severity-with-Self-Supervised-Learning"><a href="#Automated-Measurement-of-Eczema-Severity-with-Self-Supervised-Learning" class="headerlink" title="Automated Measurement of Eczema Severity with Self-Supervised Learning"></a>Automated Measurement of Eczema Severity with Self-Supervised Learning</h2><p><strong>Authors:Neelesh Kumar, Oya Aran</strong></p>
<p>Automated diagnosis of eczema using images acquired from digital camera can enable individuals to self-monitor their recovery. The process entails first segmenting out the eczema region from the image and then measuring the severity of eczema in the segmented region. The state-of-the-art methods for automated eczema diagnosis rely on deep neural networks such as convolutional neural network (CNN) and have shown impressive performance in accurately measuring the severity of eczema. However, these methods require massive volume of annotated data to train which can be hard to obtain. In this paper, we propose a self-supervised learning framework for automated eczema diagnosis under limited training data regime. Our framework consists of two stages: i) Segmentation, where we use an in-context learning based algorithm called SegGPT for few-shot segmentation of eczema region from the image; ii) Feature extraction and classification, where we extract DINO features from the segmented regions and feed it to a multi-layered perceptron (MLP) for 4-class classification of eczema severity. When evaluated on a dataset of annotated “in-the-wild” eczema images, we show that our method outperforms (Weighted F1: 0.67 $\pm$ 0.01) the state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted F1: 0.44 $\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\pm$ 0.22). Our results show that self-supervised learning can be a viable solution for automated skin diagnosis where labeled data is scarce. </p>
<blockquote>
<p>使用数字相机拍摄的图片进行湿疹自动化诊断，可以让患者自行监测康复情况。该流程包括首先从图片中分割出湿疹区域，然后测量分割区域中湿疹的严重程度。目前先进的湿疹自动化诊断方法依赖于深度神经网络，如卷积神经网络（CNN），在准确测量湿疹严重程度方面表现出令人印象深刻的性能。然而，这些方法需要大量标注数据进行训练，而获取这些数据可能很困难。在本文中，我们提出了一种在有限训练数据下用于自动化湿疹诊断的自监督学习框架。我们的框架分为两个阶段：i）分割，我们使用基于上下文学习的算法SegGPT，对图片中的湿疹区域进行少量样本分割；ii）特征提取和分类，我们从分割区域中提取DINO特征，并将其输入多层感知器（MLP）中进行湿疹严重程度的4类分类。在“野生”湿疹图片标注数据集上进行评估，我们的方法（加权F1：0.67±0.01）优于最新的深度学习方法，如微调后的Resnet-18（加权F1：0.44±0.16）和视觉转换器（加权F1：0.40±0.22）。我们的结果表明，在标记数据稀缺的情况下，自监督学习可作为皮肤疾病自动化诊断的一种可行解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15193v1">PDF</a> </p>
<p><strong>Summary</strong><br>     利用图像自动识别技术，实现湿疹的自我监控与诊断。该研究采用自监督学习方法，在少量训练数据下，通过分段算法和特征提取分类器对湿疹进行自动诊断，并表现出优异性能。该方法可解决数据标注困难的问题。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用数字相机采集的图像进行湿疹自动诊断，使个人能够自我监控康复情况。</li>
<li>通过分段算法识别湿疹区域，并测量其严重程度。</li>
<li>当前先进的自动湿疹诊断方法主要依赖深度神经网络，如卷积神经网络（CNN）。</li>
<li>深度学习方法需要大量标注数据进行训练，但获取这些数据可能很困难。</li>
<li>提出了一种自监督学习框架，用于在有限训练数据下进行自动化湿疹诊断。</li>
<li>框架分为两个阶段：分段和特征提取分类，使用SegGPT算法进行少数样本湿疹区域分段，通过提取DINO特征和多层感知器（MLP）进行湿疹严重程度的四分类。</li>
<li>在实际湿疹图像数据集上的评估表明，该方法优于其他先进深度学习方法的性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15193">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-733349722c93e83633b3b9a9df376a0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-087dc3d45f2100f5b76ca3588a250bd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-202794dc3116530c82410b8881f9641a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46c40c9671262f70089989f243881624.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6866e13d809c4f3ea079f498e6a295df.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Code2API-A-Tool-for-Generating-Reusable-APIs-from-Stack-Overflow-Code-Snippets"><a href="#Code2API-A-Tool-for-Generating-Reusable-APIs-from-Stack-Overflow-Code-Snippets" class="headerlink" title="Code2API: A Tool for Generating Reusable APIs from Stack Overflow Code   Snippets"></a>Code2API: A Tool for Generating Reusable APIs from Stack Overflow Code   Snippets</h2><p><strong>Authors:Yubo Mai, Zhipeng Gao, Xing Hu, Lingfeng Bao, Jingyuan Chen, Jianling Sun</strong></p>
<p>Nowadays, developers often turn to Stack Overflow for solutions to daily problems, however, these code snippets are partial code that cannot be tested and verified properly. One way to test these code snippets is to transform them into APIs (Application Program Interface) that developers can be directly invoked and executed. However, it is often costly and error-prone for developers to manually perform this transformation (referred to as AIPzation task) due to different actions to be taken (e.g., summarizing proper method names, inferring input parameters list and return statements). To help developers quickly reuse code snippets in Stack Overflow, in this paper, we propose Code2API, a Google Chrome extension that uses Large Language Models (LLMs) to automatically perform APIzation of code snippets on Stack Overflow. \toolname guides LLMs through well-designed prompts to generate reusable APIs, using Chain-of-Thought reasoning and few-shot in-context learning to help LLMs understand and solve the APIzation task in a developer-like manner. The evaluation results show that Code2API significantly outperforms the rule-based approach by a large margin. </p>
<blockquote>
<p>如今，开发者经常转向Stack Overflow寻找日常问题的解决方案，然而，这些代码片段是部分代码，无法进行适当的测试和验证。测试这些代码片段的一种方法是将其转换为应用程序编程接口（API），开发者可以直接调用和执行。然而，由于需要采取的不同行动（例如，总结适当的方法名称、推断输入参数列表和返回语句），开发者手动执行这种转换（称为API化任务）通常成本高昂且容易出错。为了帮助开发者快速重用Stack Overflow中的代码片段，本文提出了Code2API，这是一个Google Chrome扩展程序，它使用大型语言模型（LLM）自动执行Stack Overflow上的代码片段的API化。通过精心设计提示来指导LLM生成可重用的API，利用Chain-of-Thought推理和少量上下文学习来帮助LLM以开发者类似的方式理解和解决API化任务。评估结果表明，Code2API在规则基础上的方法上具有显著优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14331v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Code2API是一款基于Google Chrome的扩展工具，利用大型语言模型（LLMs）自动执行Stack Overflow上代码片段的API化任务。该工具通过精心设计提示来引导LLMs生成可重用的API，并利用Chain-of-Thought推理和少量上下文学习来帮助LLMs以开发者方式理解和解决API化任务。评估结果显示，Code2API大幅优于基于规则的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开发者常使用Stack Overflow查找代码片段，但这些代码片段通常是部分代码，无法充分测试和验证。</li>
<li>将代码片段转化为API（应用程序编程接口）是解决此问题的一种方法，但手动执行此任务（称为API化任务）既昂贵又容易出错。</li>
<li>Code2API是一个Google Chrome扩展，可自动执行Stack Overflow上的代码片段的API化任务。</li>
<li>Code2API利用大型语言模型（LLMs）通过精心设计提示来生成可重用的API。</li>
<li>Chain-of-Thought推理和少量上下文学习技术被用于帮助LLMs像开发者一样理解和解决API化任务。</li>
<li>Code2API显著优于基于规则的方法。</li>
<li>此工具能够简化开发者的工作流程，提高代码的可重用性和测试性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14331">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-80f6a3105074cb70d655cd1c2a51efd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81daa56f97b39c2f5047f7f941321575.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0356a0ca69fb6dd216eecf5f2b2fa94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a535eeb6a346ec4e656da853c8d60e53.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HFBRI-MAE-Handcrafted-Feature-Based-Rotation-Invariant-Masked-Autoencoder-for-3D-Point-Cloud-Analysis"><a href="#HFBRI-MAE-Handcrafted-Feature-Based-Rotation-Invariant-Masked-Autoencoder-for-3D-Point-Cloud-Analysis" class="headerlink" title="HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked   Autoencoder for 3D Point Cloud Analysis"></a>HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked   Autoencoder for 3D Point Cloud Analysis</h2><p><strong>Authors:Xuanhua Yin, Dingxin Zhang, Jianhui Yu, Weidong Cai</strong></p>
<p>Self-supervised learning (SSL) has demonstrated remarkable success in 3D point cloud analysis, particularly through masked autoencoders (MAEs). However, existing MAE-based methods lack rotation invariance, leading to significant performance degradation when processing arbitrarily rotated point clouds in real-world scenarios. To address this limitation, we introduce Handcrafted Feature-Based Rotation-Invariant Masked Autoencoder (HFBRI-MAE), a novel framework that refines the MAE design with rotation-invariant handcrafted features to ensure stable feature learning across different orientations. By leveraging both rotation-invariant local and global features for token embedding and position embedding, HFBRI-MAE effectively eliminates rotational dependencies while preserving rich geometric structures. Additionally, we redefine the reconstruction target to a canonically aligned version of the input, mitigating rotational ambiguities. Extensive experiments on ModelNet40, ScanObjectNN, and ShapeNetPart demonstrate that HFBRI-MAE consistently outperforms existing methods in object classification, segmentation, and few-shot learning, highlighting its robustness and strong generalization ability in real-world 3D applications. </p>
<blockquote>
<p>自监督学习（SSL）在3D点云分析领域已经取得了显著的成果，尤其是通过掩码自动编码器（MAE）实现。然而，现有的基于MAE的方法缺乏旋转不变性，导致在处理现实世界场景中任意旋转的点云时性能显著下降。为了解决这一局限性，我们引入了基于手工特征的旋转不变掩码自动编码器（HFBRI-MAE），这是一种新型框架，它通过引入旋转不变的手工特征来优化MAE设计，以确保在不同方向上的稳定特征学习。HFBRI-MAE利用旋转不变的局部和全局特征进行令牌嵌入和位置嵌入，有效地消除了旋转依赖性，同时保留了丰富的几何结构。此外，我们将重建目标重新定义为输入的标准对齐版本，以消除旋转模糊。在ModelNet40、ScanObjectNN和ShapeNetPart上的大量实验表明，HFBRI-MAE在目标分类、分割和少样本学习上始终优于现有方法，突显了其在现实世界的3D应用中的稳健性和强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14132v1">PDF</a> 12 pages, 9 figures, accepted by IJCNN 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于手工特征的旋转不变掩码自编码器（HFBRI-MAE）框架，用于解决现有的自监督学习（SSL）在点云分析领域存在的旋转不变性问题。通过融合旋转不变的局部和全局特征，实现对不同方向下的稳定特征学习。同时，重新定义重建目标为输入的标准对齐版本，减少旋转模糊性。实验证明，HFBRI-MAE在ModelNet40、ScanObjectNN和ShapeNetPart数据集上的物体分类、分割和少样本学习效果卓越，表现出其强大的稳健性和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HFBRI-MAE框架解决了现有自监督学习在点云分析中的旋转不变性问题。</li>
<li>通过结合旋转不变的局部和全局特征进行令牌嵌入和位置嵌入，HFBRI-MAE有效消除旋转依赖性并保留丰富的几何结构。</li>
<li>重新定义的重建目标是输入的标准对齐版本，减少旋转模糊性。</li>
<li>HFBRI-MAE在多个数据集上的实验表现优异，包括ModelNet40、ScanObjectNN和ShapeNetPart。</li>
<li>HFBRI-MAE在物体分类、分割和少样本学习任务中均表现出强大的性能。</li>
<li>HFBRI-MAE具有卓越的稳健性和泛化能力，适用于实际的三维应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14132">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d0a6bac82349f159a7443d08af3006f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-030927b8a31ab7baa34467360b943011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-900eef9a60698b26d0e114d0df3e2530.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fabe09855aaf542c02bc4c5c944e98c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f538cc624aa49bb5171f9ebc366ce36b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Baseline-for-Self-state-Identification-and-Classification-in-Mental-Health-Data-CLPsych-2025-Task"><a href="#A-Baseline-for-Self-state-Identification-and-Classification-in-Mental-Health-Data-CLPsych-2025-Task" class="headerlink" title="A Baseline for Self-state Identification and Classification in Mental   Health Data: CLPsych 2025 Task"></a>A Baseline for Self-state Identification and Classification in Mental   Health Data: CLPsych 2025 Task</h2><p><strong>Authors:Laerdon Kim</strong></p>
<p>We present a baseline for the CLPsych 2025 A.1 task: classifying self-states in mental health data taken from Reddit. We use few-shot learning with a 4-bit quantized Gemma 2 9B model and a data preprocessing step which first identifies relevant sentences indicating self-state evidence, and then performs a binary classification to determine whether the sentence is evidence of an adaptive or maladaptive self-state. This system outperforms our other method which relies on an LLM to highlight spans of variable length independently. We attribute the performance of our model to the benefits of this sentence chunking step for two reasons: partitioning posts into sentences 1) broadly matches the granularity at which self-states were human-annotated and 2) simplifies the task for our language model to a binary classification problem. Our system places third out of fourteen systems submitted for Task A.1, achieving a test-time recall of 0.579. </p>
<blockquote>
<p>我们对CLPsych 2025 A.1任务提出了一个基线方法：对来自Reddit的心理健康数据进行自我状态分类。我们使用少量样本学习，结合4位量化Gemma 2 9B模型和数据处理步骤，首先识别表示自我状态证据的相关句子，然后对句子进行二元分类，以确定句子是适应性还是适应不良的自我状态证据。我们的系统表现优于另一种依赖于大型语言模型独立突出显示可变长度段落的方法。我们将模型的性能归功于句子分块步骤的两个好处：将帖子分成句子1）与人类注释自我状态的大致粒度相匹配；2）将任务简化为二元分类问题，这简化了我们的语言模型的任务。我们的系统在A.1任务中提交的14个系统中排名第三，测试时的召回率为0.579。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14066v1">PDF</a> Accepted to CLPsych Workshop, NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对CLPsych 2025 A.1任务的分类基线：使用少量学习样本，采用4位量化的Gemma 2 9B模型对来自Reddit的心理健康数据进行自我状态分类。通过数据预处理步骤，首先识别表示自我状态证据的相关句子，然后进行二元分类，确定句子是适应性的还是适应不良的。该系统优于依赖LLM进行独立变量长度跨距突出的另一种方法。作者认为模型性能的提升得益于句子分块步骤的两个优点：将帖子分成句子，这大致匹配人类标注自我状态的粒度并简化了语言模型的任务为二元分类问题。该系统在A.1任务中位列第三，测试时的召回率为0.579。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究人员提出了针对CLPsych 2025 A.1任务的分类基线方法，旨在从Reddit数据中分类自我状态。</li>
<li>采用少量学习样本和4位量化的Gemma 2 9B模型进行任务处理。</li>
<li>数据预处理步骤包括识别表示自我状态证据的相关句子，并进行二元分类。</li>
<li>系统性能得益于句子分块步骤，因为它使模型能够更好地处理语言模型任务为二元分类问题并符合人类标注的自我状态粒度。</li>
<li>该系统在测试时的召回率为0.579，表现良好。</li>
<li>与依赖LLM独立标注跨距长度的另一种方法相比，该系统的性能有所提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14066">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5150853aa6f487912528c72ec9ed6746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7aceafd88cce7acb8d09425cc5819e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e94e241885d5607e5c923a8612ad7e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efa35bf25ab3512de0da69bd73808761.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Flow-to-Learn-Flow-Matching-on-Neural-Network-Parameters"><a href="#Flow-to-Learn-Flow-Matching-on-Neural-Network-Parameters" class="headerlink" title="Flow to Learn: Flow Matching on Neural Network Parameters"></a>Flow to Learn: Flow Matching on Neural Network Parameters</h2><p><strong>Authors:Daniel Saragih, Deyu Cao, Tejas Balaji, Ashwin Santhosh</strong></p>
<p>Foundational language models show a remarkable ability to learn new concepts during inference via context data. However, similar work for images lag behind. To address this challenge, we introduce FLoWN, a flow matching model that learns to generate neural network parameters for different tasks. Our approach models the flow on latent space, while conditioning the process on context data. Experiments verify that FLoWN attains various desiderata for a meta-learning model. In addition, it matches or exceeds baselines on in-distribution tasks, provides better initializations for classifier training, and is performant on out-of-distribution few-shot tasks while having a fine-tuning mechanism to improve performance. </p>
<blockquote>
<p>基础语言模型表现出在推理过程中通过上下文数据学习新概念的显著能力。然而，类似的图像相关工作仍落后。为了应对这一挑战，我们引入了FLoWN，这是一个流匹配模型，旨在学习为不同任务生成神经网络参数。我们的方法模拟了潜在空间上的流，同时根据上下文数据进行处理。实验证明，FLoWN满足了元学习模型的各种需求。此外，它在内部任务上达到或超过了基线水平，为分类器训练提供了更好的初始化方案，并在外部少量任务的场景下表现出良好的性能，同时具有微调机制以提高性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19371v2">PDF</a> Accepted at the ICLR Workshop on Neural Network Weights as a New Data   Modality 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为FLoWN的流匹配模型，该模型能够在不同任务中生成神经网络参数。它通过模拟潜在空间的流来建模，并以上下文数据为条件进行过程控制。实验证明，FLoWN达到了元学习模型的各种期望要求，并在内部任务上匹配或超过了基线水平，为分类器训练提供了更好的初始化，并在外部任务的少样本情况下表现出良好的性能，同时具有微调机制以提高性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FLoWN模型通过在潜在空间上建模流来解决图像领域的概念学习问题。</li>
<li>该模型能够在不同任务中生成神经网络参数。</li>
<li>FLoWN能够以上下文数据为条件进行过程控制。</li>
<li>实验表明，FLoWN达到了元学习模型的期望要求。</li>
<li>FLoWN在内部任务上表现出良好的性能，匹配或超过了基线水平。</li>
<li>FLoWN能够为分类器训练提供更好的初始化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19371">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f00fb6372a6302faa53b83b2287df0a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-198ba6e2b646fbf10214dabbcb289805.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f50c7d8b9b229a2aa0db3ce5345909a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e1ea762a176456a19d88e912e1beaa6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TLAC-Two-stage-LMM-Augmented-CLIP-for-Zero-Shot-Classification"><a href="#TLAC-Two-stage-LMM-Augmented-CLIP-for-Zero-Shot-Classification" class="headerlink" title="TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification"></a>TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification</h2><p><strong>Authors:Ans Munir, Faisal Z. Qureshi, Muhammad Haris Khan, Mohsen Ali</strong></p>
<p>Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot performance on image classification. However, state-of-the-art methods often rely on fine-tuning techniques like prompt learning and adapter-based tuning to optimize CLIP’s performance. The necessity for fine-tuning significantly limits CLIP’s adaptability to novel datasets and domains. This requirement mandates substantial time and computational resources for each new dataset. To overcome this limitation, we introduce simple yet effective training-free approaches, Single-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC), that leverages powerful Large Multimodal Models (LMMs), such as Gemini, for image classification. The proposed methods leverages the capabilities of pre-trained LMMs, allowing for seamless adaptation to diverse datasets and domains without the need for additional training. Our approaches involve prompting the LMM to identify objects within an image. Subsequently, the CLIP text encoder determines the image class by identifying the dataset class with the highest semantic similarity to the LLM predicted object. Our models achieved superior accuracy on 9 of 11 base-to-novel datasets, including ImageNet, SUN397, and Caltech101, while maintaining a strictly training-free paradigm. Our TLAC model achieved an overall accuracy of 83.44%, surpassing the previous state-of-the-art few-shot methods by a margin of 6.75%. Compared to other training-free approaches, our TLAC method achieved 83.6% average accuracy across 13 datasets, a 9.7% improvement over the previous methods. Our Code is available at <a target="_blank" rel="noopener" href="https://github.com/ans92/TLAC">https://github.com/ans92/TLAC</a> </p>
<blockquote>
<p>对比语言图像预训练（CLIP）在图像分类方面展现出了令人印象深刻的零样本性能。然而，最先进的方法通常依赖于微调技术，如基于提示的学习和基于适配器的调整，以优化CLIP的性能。这种对微调的必要性显著限制了CLIP对新数据集和领域的适应能力。对于每个新数据集，这种要求都需要大量的时间和计算资源。为了克服这一局限性，我们引入了简单而有效的无训练方法，即单阶段LMM增强CLIP（SLAC）和双阶段LMM增强CLIP（TLAC），这些方法利用强大的大型多媒体模型（LMMs），如Gemini，进行图像分类。所提出的方法利用预训练的LMM的能力，无需额外的训练即可无缝适应各种数据集和领域。我们的方法包括提示LMM来识别图像中的对象。然后，CLIP文本编码器通过识别与LLM预测对象语义相似性最高的数据集类别来确定图像类别。我们的模型在包括ImageNet、SUN397和Caltech101在内的11个基础到新颖数据集中的9个上实现了更高的准确性，同时保持了严格的无需训练的模式。我们的TLAC模型的总准确率为83.44%，较之前的先进少样本方法提高了6.75%。与其他无训练的方法相比，我们的TLAC方法在13个数据集上取得了平均83.6%的准确率，比之前的方法提高了9.7%。我们的代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/ans92/TLAC%E3%80%82">https://github.com/ans92/TLAC。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12206v2">PDF</a> Added code link in the abstract</p>
<p><strong>Summary</strong></p>
<p>本文介绍了通过利用强大的大型多模态模型（LMMs），如Gemini，来克服CLIP模型在零样本学习中的局限性。文章提出了两种训练免费的方法：单阶段LMM增强CLIP（SLAC）和双阶段LMM增强CLIP（TLAC）。这些方法通过在预训练的大型多模态模型的帮助下进行提示操作，允许无缝适应不同的数据集和领域而无需额外的训练。在多个基准数据集上的实验结果表明，TLAC模型在训练和零样本学习方面均取得了显著的优势。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIP模型在图像分类的零样本学习中表现出强大的性能，但仍需要微调以适应新的数据集和领域。</li>
<li>新的方法SLAC和TLAC利用预训练的大型多模态模型（LMMs）进行图像分类，无需额外的训练。</li>
<li>SLAC和TLAC通过提示大型多模态模型识别图像中的对象，并通过CLIP文本编码器确定图像类别。</li>
<li>TLAC模型在多个数据集上实现了显著的性能提升，相较于其他方法有更优秀的准确率和适应力。</li>
<li>TLAC模型的整体准确率达到83.44%，在少数数据集上的准确率超过了之前的最先进方法。</li>
<li>与其他无训练方法相比，TLAC方法在多个数据集上的平均准确率提高了9.7%。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12206">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dcc88bdf0ce877764ff06468835925a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48eb7474a975f787486e1a49586da6cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ceda9e733daad28b675c3d19d216ac65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7380b37afbfe03f99eb74962a0e7dd0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aa40195e0ff90161c97c3b2af5aa0a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4987d9b7b2f485445a45db2baf1971b2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Tree-of-Attributes-Prompt-Learning-for-Vision-Language-Models"><a href="#Tree-of-Attributes-Prompt-Learning-for-Vision-Language-Models" class="headerlink" title="Tree of Attributes Prompt Learning for Vision-Language Models"></a>Tree of Attributes Prompt Learning for Vision-Language Models</h2><p><strong>Authors:Tong Ding, Wanhua Li, Zhongqi Miao, Hanspeter Pfister</strong></p>
<p>Prompt learning has proven effective in adapting vision language models for downstream tasks. However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully leverage the rich context indicated in the category name. To address this issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs LLMs to generate a tree of attributes with a “concept - attribute - description” structure for each category, and then learn the hierarchy with vision and text prompt tokens. Unlike existing methods that merely augment category names with a set of unstructured descriptions, our approach essentially distills structured knowledge graphs associated with class names from LLMs. Furthermore, our approach introduces text and vision prompts designed to explicitly learn the corresponding visual attributes, effectively serving as domain experts. Additionally, the general and diverse descriptions generated based on the class names may be wrong or absent in the specific given images. To address this misalignment, we further introduce a vision-conditional pooling module to extract instance-specific text features. Extensive experimental results demonstrate that our approach outperforms state-of-the-art methods on the zero-shot base-to-novel generalization, cross-dataset transfer, as well as few-shot classification across 11 diverse datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HHenryD/TAP">https://github.com/HHenryD/TAP</a>. </p>
<blockquote>
<p>提示学习在适应视觉语言模型以进行下游任务方面已被证明是有效的。然而，现有方法通常仅将可学习的提示令牌附加到类别名称上以获得文本特征，这未能充分利用类别名称中指示的丰富上下文。为了解决此问题，我们提出了属性树提示学习（TAP）方法。该方法首先指导大型语言模型为每个类别生成具有“概念-属性-描述”结构的属性树，然后学习与视觉和文本提示令牌对应的层次结构。与仅将一系列非结构化的描述附加到类别名称上的现有方法不同，我们的方法本质上是从大型语言模型中提炼出与类名相关的结构化知识图谱。此外，我们的方法引入了用于明确学习相应视觉属性的文本和视觉提示，有效地充当领域专家。基于类名生成的通用且多样的描述在具体给定的图像中可能出错或缺失。为了解决这种不匹配问题，我们进一步引入了一个视觉条件池模块来提取特定实例的文本特征。广泛的实验结果表明，我们的方法在零样本基础到新颖的泛化、跨数据集迁移以及在11个不同数据集的少量样本分类上均优于最新方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/HHenryD/TAP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HHenryD/TAP上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11201v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Prompt学习的方法能有效适应视觉语言模型进行下游任务。但现有方法主要通过添加类别名称的可学习提示符号来获得文本特征，这未能充分利用类别名称中的丰富上下文信息。为解决这一问题，我们提出了属性树提示学习（TAP）方法。TAP首先指导大型语言模型为每个类别生成一个属性树，形成“概念-属性-描述”的结构，然后通过视觉和文本提示符号学习层次结构。与仅通过添加类别名称的一系列非结构化描述不同，TAP本质上是从大型语言模型中提炼出与类别名称相关的结构化知识图谱。此外，TAP还引入了文本和视觉提示，旨在明确学习相应的视觉属性，有效充当领域专家。为了解决特定图像中基于类别名称生成的通用和多样化描述可能存在的错误或缺失问题，我们进一步引入了视觉条件池模块，以提取特定实例的文本特征。实验结果表明，TAP在零样本基础到新颖类别的推广、跨数据集迁移以及11个不同数据集的少样本分类任务上均优于现有最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TAP方法通过生成属性树来丰富类别名称的上下文信息，提高Prompt学习的效果。</li>
<li>TAP从大型语言模型中提炼与类别名称相关的结构化知识图谱。</li>
<li>TAP引入文本和视觉提示来学习视觉属性，增强模型的表现力。</li>
<li>TAP解决了特定图像描述可能存在的错误或缺失问题，通过视觉条件池模块提取特定实例的文本特征。</li>
<li>TAP在多个数据集上的实验表现优于现有方法，尤其在零样本和少样本学习任务上。</li>
<li>TAP方法具有良好的通用性，可适应不同的下游任务和数据集。</li>
<li>TAP方法的代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11201">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3898defe56ce39f9a99c26949d694faf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-722c847def8fad2a116798d8be87ec11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-874bb9fdbf0b09085549e51626372a42.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Aligning-Language-Models-with-Demonstrated-Feedback"><a href="#Aligning-Language-Models-with-Demonstrated-Feedback" class="headerlink" title="Aligning Language Models with Demonstrated Feedback"></a>Aligning Language Models with Demonstrated Feedback</h2><p><strong>Authors:Omar Shaikh, Michelle S. Lam, Joey Hejna, Yijia Shao, Hyundong Cho, Michael S. Bernstein, Diyi Yang</strong></p>
<p>Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (&lt; 10) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user’s demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users’ demonstrations as preferred over output from the LLM and its intermediate checkpoints. Concretely, DITTO operates by having an LLM generate examples that are presumed to be inferior to expert demonstrations. The method iteratively constructs pairwise preference relationships between these LLM-generated samples and expert demonstrations, potentially including comparisons between different training checkpoints. These constructed preference pairs are then used to train the model using a preference optimization algorithm (e.g. DPO). We evaluate DITTO’s ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants (N &#x3D; 16). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an avg. of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs. </p>
<blockquote>
<p>语言模型旨在模拟许多人的集体声音，从而产生与特定人无关的输。通过监督微调或强化学习反馈（RLHF）可以避免语言大模型产生通用输出，但这需要针对新任务的特定数据集且数据量巨大。我们认为，通过利用极少量的演示（&lt;10个）作为反馈，可以将语言大模型（LLM）与特定场景进行匹配。我们的方法——Demonstration ITerated Task Optimization（DITTO）直接使语言模型输出与用户演示行为保持一致。该方法基于在线模仿学习的思想，通过将用户的演示视为首选输出，与语言模型及其中间检查点的输出进行比较，从而低成本地生成在线比较数据。具体来说，DITTO的操作是让语言模型生成被认为是逊于专家演示的例子。该方法通过构建这些语言模型生成样本与专家演示之间的配对偏好关系来迭代操作，可能包括不同训练检查点之间的比较。这些构建的偏好配对随后被用于使用偏好优化算法（例如DPO）训练模型。我们评估了DITTO在不同领域（如新闻文章、电子邮件和博客文章）学习精细风格和任务对齐的能力。此外，我们进行了一项用户研究，从参与者中征集了一系列演示（N&#x3D;16）。在我们的基准测试和用户研究中，我们发现DITTO的胜率比少样本提示、监督微调和其他自我对抗方法平均高出19个百分点。通过直接使用演示作为反馈，DITTO提供了一种有效定制语言大模型的新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00888v2">PDF</a> ICLR 2025; 28 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>该文介绍了通过利用用户示范来进行在线比较数据的低成本生成的新方法，即迭代任务优化（DITTO）。此方法基于用户示范进行迭代训练和优化，从而使语言模型的输出更符合用户需求。评价表明，在新闻文章、电子邮件和博客文章等不同领域，DITTO的性能优于其他方法。通过用户示范作为直接反馈，DITTO为有效定制大型语言模型提供了新的途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型通过模仿集体声音产生通用输出，但可通过监督微调或强化学习人类反馈（RLHF）等方法进行引导以产生特定输出。</li>
<li>使用少量示范（&lt; 10个）作为反馈来对齐语言模型到特定设置是可能的。</li>
<li>DITTO方法利用在线模仿学习的思想，通过用户的示范来低成本地生成在线比较数据。</li>
<li>DITTO通过将语言模型生成的例子视为不如专家示范优越，来迭代构建偏好关系。</li>
<li>DITTO使用偏好优化算法（如DPO）来训练模型。</li>
<li>在新闻、电子邮件和博客等不同领域，DITTO在精细风格和任务对齐方面的学习能力得到了评价。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.00888">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5dc40740180f34ed3bb685df86b41b92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3996dbe6f08a280518c5c10fe9641d5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c04f63f24ba48aefef58d5ce026377f4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GLoRE-Evaluating-Logical-Reasoning-of-Large-Language-Models"><a href="#GLoRE-Evaluating-Logical-Reasoning-of-Large-Language-Models" class="headerlink" title="GLoRE: Evaluating Logical Reasoning of Large Language Models"></a>GLoRE: Evaluating Logical Reasoning of Large Language Models</h2><p><strong>Authors:Hanmeng liu, Zhiyang Teng, Ruoxi Ning, Yiran Ding, Xiulai Li, Xiaozhang Liu, Yue Zhang</strong></p>
<p>Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI’s o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities. </p>
<blockquote>
<p>大规模语言模型（LLM）已经展现出显著的一般语言理解能力。然而，目前缺乏对这些LLM的逻辑推理能力进行评估的尝试，这是自然语言理解的重要组成部分。为了鼓励在这一领域的进一步研究，我们引入了GLoRE，一个通用逻辑推理评估平台，它不仅整合了多样化的数据集，还将它们标准化为一个统一的格式，适用于在零样本和少样本场景下评估大型语言模型。我们的实验结果表明，与人类和经过监督微调模型的性能相比，大型推理模型（如OpenAI的o1 mini、DeepSeek R1和QwQ-32B）的逻辑推理能力已经取得了显著的提升，其中QwQ-32B达到了迄今为止的最高基准性能。GLoRE被设计为一个持续集成的项目，不断融入新的数据集和模型，便于在商业和Huggingface社区中对模型性能进行稳健和比较性的评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09107v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型展现出强大的自然语言理解力，但在逻辑理解方面的评估仍显不足。为此，我们推出通用逻辑推理评估平台GLoRE，旨在评估大型语言模型在不同场景下的零样本和少样本能力。实验结果显示，相较于人类和经过微调训练的模型，大型推理模型的逻辑推理能力已有显著进步，其中QwQ-32B取得了迄今为止的最高性能。GLoRE将持续集成新数据集和模型，为商业和Huggingface社区提供稳健的模型性能比较评估平台。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型展现出强大的自然语言理解能力，但逻辑理解方面的评估仍然不足。</li>
<li>GLoRE平台旨在评估大型语言模型的逻辑推理能力，支持零样本和少样本场景的评估。</li>
<li>实验结果显示，大型推理模型的逻辑推理能力已经取得显著进步。</li>
<li>QwQ-32B在逻辑推理方面取得了迄今为止的最高性能。</li>
<li>GLoRE平台是一个持续发展的项目，能够集成新数据集和模型。</li>
<li>GLoRE平台为商业和Huggingface社区提供了模型性能比较评估的稳健平台。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.09107">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6507ce15a08e1e6d5afbe25fb6d8fba2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0af45720ebde4848157394035469e164.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaf32ba08c3ebcce4a788e65a0d09bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5159674ed85fbfbce48f13662d4a1d82.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Holistic-Evaluation-of-Piano-Sound-Quality"><a href="#A-Holistic-Evaluation-of-Piano-Sound-Quality" class="headerlink" title="A Holistic Evaluation of Piano Sound Quality"></a>A Holistic Evaluation of Piano Sound Quality</h2><p><strong>Authors:Monan Zhou, Shangda Wu, Shaohua Ji, Zijin Li, Wei Li</strong></p>
<p>This paper aims to develop a holistic evaluation method for piano sound quality to assist in purchasing decisions. Unlike previous studies that focused on the effect of piano performance techniques on sound quality, this study evaluates the inherent sound quality of different pianos. To derive quality evaluation systems, the study uses subjective questionnaires based on a piano sound quality dataset. The method selects the optimal piano classification models by comparing the fine-tuning results of different pre-training models of Convolutional Neural Networks (CNN). To improve the interpretability of the models, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. The results reveal that musically trained individuals are better able to distinguish between the sound quality differences of different pianos. The best fine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3% as the piano classifier. However, the dataset is limited, and the audio is sliced to increase its quantity, resulting in a lack of diversity and balance, so we use focal loss to reduce the impact of data imbalance. To optimize the method, the dataset will be expanded, or few-shot learning techniques will be employed in future research. </p>
<blockquote>
<p>本文旨在开发一种全面的钢琴音质评估方法，以辅助购买决策。不同于以往专注于钢琴演奏技巧对音质影响的研究，本研究旨在评估不同钢琴的固有音质。为了得出质量评估系统，本研究基于钢琴音质数据集，使用主观问卷的方法。该方法通过比较不同预训练模型的微调结果，选择最佳的钢琴分类模型。为了提高模型的解释性，研究应用了等效矩形带宽（ERB）分析。结果表明，受过音乐训练的人更能区分不同钢琴的音质差异。最佳的微调CNN预训练主干作为钢琴分类器，实现了高达98.3%的准确率。然而，数据集有限，音频被切片以增加其数量，导致缺乏多样性和平衡性，因此我们使用焦点损失来减少数据不平衡的影响。为了优化该方法，将扩大数据集或在未来的研究中采用小样本学习技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04722v3">PDF</a> 15 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>本文旨在开发一种全面的钢琴音质评估方法，以辅助购买决策。研究不同于以往关注钢琴演奏技巧对音质影响的研究，而是评估不同钢琴的固有音质。研究使用基于钢琴音质数据集的主观问卷来推导质量评估系统，通过比较不同预训练模型的微调结果来选择最佳的钢琴分类模型。为提高模型的可解释性，研究应用了等效矩形带宽（ERB）分析。结果显示，受过音乐训练的人更能区分不同钢琴的音质差异。最佳微调CNN预训练主干作为钢琴分类器，准确率高达98.3%。但数据集有限，音频被切片以增加数量，导致缺乏多样性和平衡性，因此使用焦点损失来减少数据不平衡的影响。未来研究将扩大数据集或采用少样本学习技术以优化方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究目的是开发一种全面的钢琴音质评估方法，帮助购买决策。</li>
<li>与其他研究不同，该研究侧重于评估不同钢琴的固有音质。</li>
<li>研究通过主观问卷和基于数据集的质量评估系统来推导评估方法。</li>
<li>选择最佳钢琴分类模型是通过比较不同预训练模型的微调结果。</li>
<li>应用等效矩形带宽（ERB）分析以提高模型的可解释性。</li>
<li>受过音乐训练的人更能区分不同钢琴的音质差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.04722">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-718ec1bd93778693c99cdd3b5d80fa80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c6a4fc0cfaab7ed0a93ce3a81af4872.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed02193f2135c9d56dd83e195be3db6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a611290a6031460fa7fb44644130714a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3cef8151233a3370d61c421aa281eb56.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-23  ECViT Efficient Convolutional Vision Transformer with Local-Attention   and Multi-scale Stages
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3b7326e352312fc14c4f2e02b43f5add.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-23  FlowReasoner Reinforcing Query-Level Meta-Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
