<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  Automated Measurement of Eczema Severity with Self-Supervised Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-198ba6e2b646fbf10214dabbcb289805.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    38 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-23-æ›´æ–°"><a href="#2025-04-23-æ›´æ–°" class="headerlink" title="2025-04-23 æ›´æ–°"></a>2025-04-23 æ›´æ–°</h1><h2 id="Automated-Measurement-of-Eczema-Severity-with-Self-Supervised-Learning"><a href="#Automated-Measurement-of-Eczema-Severity-with-Self-Supervised-Learning" class="headerlink" title="Automated Measurement of Eczema Severity with Self-Supervised Learning"></a>Automated Measurement of Eczema Severity with Self-Supervised Learning</h2><p><strong>Authors:Neelesh Kumar, Oya Aran</strong></p>
<p>Automated diagnosis of eczema using images acquired from digital camera can enable individuals to self-monitor their recovery. The process entails first segmenting out the eczema region from the image and then measuring the severity of eczema in the segmented region. The state-of-the-art methods for automated eczema diagnosis rely on deep neural networks such as convolutional neural network (CNN) and have shown impressive performance in accurately measuring the severity of eczema. However, these methods require massive volume of annotated data to train which can be hard to obtain. In this paper, we propose a self-supervised learning framework for automated eczema diagnosis under limited training data regime. Our framework consists of two stages: i) Segmentation, where we use an in-context learning based algorithm called SegGPT for few-shot segmentation of eczema region from the image; ii) Feature extraction and classification, where we extract DINO features from the segmented regions and feed it to a multi-layered perceptron (MLP) for 4-class classification of eczema severity. When evaluated on a dataset of annotated â€œin-the-wildâ€ eczema images, we show that our method outperforms (Weighted F1: 0.67 $\pm$ 0.01) the state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted F1: 0.44 $\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\pm$ 0.22). Our results show that self-supervised learning can be a viable solution for automated skin diagnosis where labeled data is scarce. </p>
<blockquote>
<p>ä½¿ç”¨æ•°å­—ç›¸æœºæ‹æ‘„çš„å›¾ç‰‡è¿›è¡Œæ¹¿ç–¹è‡ªåŠ¨åŒ–è¯Šæ–­ï¼Œå¯ä»¥è®©æ‚£è€…è‡ªè¡Œç›‘æµ‹åº·å¤æƒ…å†µã€‚è¯¥æµç¨‹åŒ…æ‹¬é¦–å…ˆä»å›¾ç‰‡ä¸­åˆ†å‰²å‡ºæ¹¿ç–¹åŒºåŸŸï¼Œç„¶åæµ‹é‡åˆ†å‰²åŒºåŸŸä¸­æ¹¿ç–¹çš„ä¸¥é‡ç¨‹åº¦ã€‚ç›®å‰å…ˆè¿›çš„æ¹¿ç–¹è‡ªåŠ¨åŒ–è¯Šæ–­æ–¹æ³•ä¾èµ–äºæ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œåœ¨å‡†ç¡®æµ‹é‡æ¹¿ç–¹ä¸¥é‡ç¨‹åº¦æ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œè·å–è¿™äº›æ•°æ®å¯èƒ½å¾ˆå›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹ç”¨äºè‡ªåŠ¨åŒ–æ¹¿ç–¹è¯Šæ–­çš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šiï¼‰åˆ†å‰²ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„ç®—æ³•SegGPTï¼Œå¯¹å›¾ç‰‡ä¸­çš„æ¹¿ç–¹åŒºåŸŸè¿›è¡Œå°‘é‡æ ·æœ¬åˆ†å‰²ï¼›iiï¼‰ç‰¹å¾æå–å’Œåˆ†ç±»ï¼Œæˆ‘ä»¬ä»åˆ†å‰²åŒºåŸŸä¸­æå–DINOç‰¹å¾ï¼Œå¹¶å°†å…¶è¾“å…¥å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¸­è¿›è¡Œæ¹¿ç–¹ä¸¥é‡ç¨‹åº¦çš„4ç±»åˆ†ç±»ã€‚åœ¨â€œé‡ç”Ÿâ€æ¹¿ç–¹å›¾ç‰‡æ ‡æ³¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ï¼ˆåŠ æƒF1ï¼š0.67Â±0.01ï¼‰ä¼˜äºæœ€æ–°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¦‚å¾®è°ƒåçš„Resnet-18ï¼ˆåŠ æƒF1ï¼š0.44Â±0.16ï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆåŠ æƒF1ï¼š0.40Â±0.22ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨æ ‡è®°æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œè‡ªç›‘ç£å­¦ä¹ å¯ä½œä¸ºçš®è‚¤ç–¾ç—…è‡ªåŠ¨åŒ–è¯Šæ–­çš„ä¸€ç§å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15193v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åˆ©ç”¨å›¾åƒè‡ªåŠ¨è¯†åˆ«æŠ€æœ¯ï¼Œå®ç°æ¹¿ç–¹çš„è‡ªæˆ‘ç›‘æ§ä¸è¯Šæ–­ã€‚è¯¥ç ”ç©¶é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œåœ¨å°‘é‡è®­ç»ƒæ•°æ®ä¸‹ï¼Œé€šè¿‡åˆ†æ®µç®—æ³•å’Œç‰¹å¾æå–åˆ†ç±»å™¨å¯¹æ¹¿ç–¹è¿›è¡Œè‡ªåŠ¨è¯Šæ–­ï¼Œå¹¶è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚è¯¥æ–¹æ³•å¯è§£å†³æ•°æ®æ ‡æ³¨å›°éš¾çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ©ç”¨æ•°å­—ç›¸æœºé‡‡é›†çš„å›¾åƒè¿›è¡Œæ¹¿ç–¹è‡ªåŠ¨è¯Šæ–­ï¼Œä½¿ä¸ªäººèƒ½å¤Ÿè‡ªæˆ‘ç›‘æ§åº·å¤æƒ…å†µã€‚</li>
<li>é€šè¿‡åˆ†æ®µç®—æ³•è¯†åˆ«æ¹¿ç–¹åŒºåŸŸï¼Œå¹¶æµ‹é‡å…¶ä¸¥é‡ç¨‹åº¦ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„è‡ªåŠ¨æ¹¿ç–¹è¯Šæ–­æ–¹æ³•ä¸»è¦ä¾èµ–æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†è·å–è¿™äº›æ•°æ®å¯èƒ½å¾ˆå›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹è¿›è¡Œè‡ªåŠ¨åŒ–æ¹¿ç–¹è¯Šæ–­ã€‚</li>
<li>æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåˆ†æ®µå’Œç‰¹å¾æå–åˆ†ç±»ï¼Œä½¿ç”¨SegGPTç®—æ³•è¿›è¡Œå°‘æ•°æ ·æœ¬æ¹¿ç–¹åŒºåŸŸåˆ†æ®µï¼Œé€šè¿‡æå–DINOç‰¹å¾å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰è¿›è¡Œæ¹¿ç–¹ä¸¥é‡ç¨‹åº¦çš„å››åˆ†ç±»ã€‚</li>
<li>åœ¨å®é™…æ¹¿ç–¹å›¾åƒæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å…ˆè¿›æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-733349722c93e83633b3b9a9df376a0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-087dc3d45f2100f5b76ca3588a250bd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-202794dc3116530c82410b8881f9641a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46c40c9671262f70089989f243881624.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6866e13d809c4f3ea079f498e6a295df.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Code2API-A-Tool-for-Generating-Reusable-APIs-from-Stack-Overflow-Code-Snippets"><a href="#Code2API-A-Tool-for-Generating-Reusable-APIs-from-Stack-Overflow-Code-Snippets" class="headerlink" title="Code2API: A Tool for Generating Reusable APIs from Stack Overflow Code   Snippets"></a>Code2API: A Tool for Generating Reusable APIs from Stack Overflow Code   Snippets</h2><p><strong>Authors:Yubo Mai, Zhipeng Gao, Xing Hu, Lingfeng Bao, Jingyuan Chen, Jianling Sun</strong></p>
<p>Nowadays, developers often turn to Stack Overflow for solutions to daily problems, however, these code snippets are partial code that cannot be tested and verified properly. One way to test these code snippets is to transform them into APIs (Application Program Interface) that developers can be directly invoked and executed. However, it is often costly and error-prone for developers to manually perform this transformation (referred to as AIPzation task) due to different actions to be taken (e.g., summarizing proper method names, inferring input parameters list and return statements). To help developers quickly reuse code snippets in Stack Overflow, in this paper, we propose Code2API, a Google Chrome extension that uses Large Language Models (LLMs) to automatically perform APIzation of code snippets on Stack Overflow. \toolname guides LLMs through well-designed prompts to generate reusable APIs, using Chain-of-Thought reasoning and few-shot in-context learning to help LLMs understand and solve the APIzation task in a developer-like manner. The evaluation results show that Code2API significantly outperforms the rule-based approach by a large margin. </p>
<blockquote>
<p>å¦‚ä»Šï¼Œå¼€å‘è€…ç»å¸¸è½¬å‘Stack Overflowå¯»æ‰¾æ—¥å¸¸é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œç„¶è€Œï¼Œè¿™äº›ä»£ç ç‰‡æ®µæ˜¯éƒ¨åˆ†ä»£ç ï¼Œæ— æ³•è¿›è¡Œé€‚å½“çš„æµ‹è¯•å’ŒéªŒè¯ã€‚æµ‹è¯•è¿™äº›ä»£ç ç‰‡æ®µçš„ä¸€ç§æ–¹æ³•æ˜¯å°†å…¶è½¬æ¢ä¸ºåº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰ï¼Œå¼€å‘è€…å¯ä»¥ç›´æ¥è°ƒç”¨å’Œæ‰§è¡Œã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦é‡‡å–çš„ä¸åŒè¡ŒåŠ¨ï¼ˆä¾‹å¦‚ï¼Œæ€»ç»“é€‚å½“çš„æ–¹æ³•åç§°ã€æ¨æ–­è¾“å…¥å‚æ•°åˆ—è¡¨å’Œè¿”å›è¯­å¥ï¼‰ï¼Œå¼€å‘è€…æ‰‹åŠ¨æ‰§è¡Œè¿™ç§è½¬æ¢ï¼ˆç§°ä¸ºAPIåŒ–ä»»åŠ¡ï¼‰é€šå¸¸æˆæœ¬é«˜æ˜‚ä¸”å®¹æ˜“å‡ºé”™ã€‚ä¸ºäº†å¸®åŠ©å¼€å‘è€…å¿«é€Ÿé‡ç”¨Stack Overflowä¸­çš„ä»£ç ç‰‡æ®µï¼Œæœ¬æ–‡æå‡ºäº†Code2APIï¼Œè¿™æ˜¯ä¸€ä¸ªGoogle Chromeæ‰©å±•ç¨‹åºï¼Œå®ƒä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨æ‰§è¡ŒStack Overflowä¸Šçš„ä»£ç ç‰‡æ®µçš„APIåŒ–ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡æç¤ºæ¥æŒ‡å¯¼LLMç”Ÿæˆå¯é‡ç”¨çš„APIï¼Œåˆ©ç”¨Chain-of-Thoughtæ¨ç†å’Œå°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ æ¥å¸®åŠ©LLMä»¥å¼€å‘è€…ç±»ä¼¼çš„æ–¹å¼ç†è§£å’Œè§£å†³APIåŒ–ä»»åŠ¡ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCode2APIåœ¨è§„åˆ™åŸºç¡€ä¸Šçš„æ–¹æ³•ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14331v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Code2APIæ˜¯ä¸€æ¬¾åŸºäºGoogle Chromeçš„æ‰©å±•å·¥å…·ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨æ‰§è¡ŒStack Overflowä¸Šä»£ç ç‰‡æ®µçš„APIåŒ–ä»»åŠ¡ã€‚è¯¥å·¥å…·é€šè¿‡ç²¾å¿ƒè®¾è®¡æç¤ºæ¥å¼•å¯¼LLMsç”Ÿæˆå¯é‡ç”¨çš„APIï¼Œå¹¶åˆ©ç”¨Chain-of-Thoughtæ¨ç†å’Œå°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ æ¥å¸®åŠ©LLMsä»¥å¼€å‘è€…æ–¹å¼ç†è§£å’Œè§£å†³APIåŒ–ä»»åŠ¡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒCode2APIå¤§å¹…ä¼˜äºåŸºäºè§„åˆ™çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€å‘è€…å¸¸ä½¿ç”¨Stack OverflowæŸ¥æ‰¾ä»£ç ç‰‡æ®µï¼Œä½†è¿™äº›ä»£ç ç‰‡æ®µé€šå¸¸æ˜¯éƒ¨åˆ†ä»£ç ï¼Œæ— æ³•å……åˆ†æµ‹è¯•å’ŒéªŒè¯ã€‚</li>
<li>å°†ä»£ç ç‰‡æ®µè½¬åŒ–ä¸ºAPIï¼ˆåº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼‰æ˜¯è§£å†³æ­¤é—®é¢˜çš„ä¸€ç§æ–¹æ³•ï¼Œä½†æ‰‹åŠ¨æ‰§è¡Œæ­¤ä»»åŠ¡ï¼ˆç§°ä¸ºAPIåŒ–ä»»åŠ¡ï¼‰æ—¢æ˜‚è´µåˆå®¹æ˜“å‡ºé”™ã€‚</li>
<li>Code2APIæ˜¯ä¸€ä¸ªGoogle Chromeæ‰©å±•ï¼Œå¯è‡ªåŠ¨æ‰§è¡ŒStack Overflowä¸Šçš„ä»£ç ç‰‡æ®µçš„APIåŒ–ä»»åŠ¡ã€‚</li>
<li>Code2APIåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ç²¾å¿ƒè®¾è®¡æç¤ºæ¥ç”Ÿæˆå¯é‡ç”¨çš„APIã€‚</li>
<li>Chain-of-Thoughtæ¨ç†å’Œå°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ æŠ€æœ¯è¢«ç”¨äºå¸®åŠ©LLMsåƒå¼€å‘è€…ä¸€æ ·ç†è§£å’Œè§£å†³APIåŒ–ä»»åŠ¡ã€‚</li>
<li>Code2APIæ˜¾è‘—ä¼˜äºåŸºäºè§„åˆ™çš„æ–¹æ³•ã€‚</li>
<li>æ­¤å·¥å…·èƒ½å¤Ÿç®€åŒ–å¼€å‘è€…çš„å·¥ä½œæµç¨‹ï¼Œæé«˜ä»£ç çš„å¯é‡ç”¨æ€§å’Œæµ‹è¯•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80f6a3105074cb70d655cd1c2a51efd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81daa56f97b39c2f5047f7f941321575.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0356a0ca69fb6dd216eecf5f2b2fa94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a535eeb6a346ec4e656da853c8d60e53.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HFBRI-MAE-Handcrafted-Feature-Based-Rotation-Invariant-Masked-Autoencoder-for-3D-Point-Cloud-Analysis"><a href="#HFBRI-MAE-Handcrafted-Feature-Based-Rotation-Invariant-Masked-Autoencoder-for-3D-Point-Cloud-Analysis" class="headerlink" title="HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked   Autoencoder for 3D Point Cloud Analysis"></a>HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked   Autoencoder for 3D Point Cloud Analysis</h2><p><strong>Authors:Xuanhua Yin, Dingxin Zhang, Jianhui Yu, Weidong Cai</strong></p>
<p>Self-supervised learning (SSL) has demonstrated remarkable success in 3D point cloud analysis, particularly through masked autoencoders (MAEs). However, existing MAE-based methods lack rotation invariance, leading to significant performance degradation when processing arbitrarily rotated point clouds in real-world scenarios. To address this limitation, we introduce Handcrafted Feature-Based Rotation-Invariant Masked Autoencoder (HFBRI-MAE), a novel framework that refines the MAE design with rotation-invariant handcrafted features to ensure stable feature learning across different orientations. By leveraging both rotation-invariant local and global features for token embedding and position embedding, HFBRI-MAE effectively eliminates rotational dependencies while preserving rich geometric structures. Additionally, we redefine the reconstruction target to a canonically aligned version of the input, mitigating rotational ambiguities. Extensive experiments on ModelNet40, ScanObjectNN, and ShapeNetPart demonstrate that HFBRI-MAE consistently outperforms existing methods in object classification, segmentation, and few-shot learning, highlighting its robustness and strong generalization ability in real-world 3D applications. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨3Dç‚¹äº‘åˆ†æé¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œå°¤å…¶æ˜¯é€šè¿‡æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰å®ç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºMAEçš„æ–¹æ³•ç¼ºä¹æ—‹è½¬ä¸å˜æ€§ï¼Œå¯¼è‡´åœ¨å¤„ç†ç°å®ä¸–ç•Œåœºæ™¯ä¸­ä»»æ„æ—‹è½¬çš„ç‚¹äº‘æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ‰‹å·¥ç‰¹å¾çš„æ—‹è½¬ä¸å˜æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆHFBRI-MAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¼•å…¥æ—‹è½¬ä¸å˜çš„æ‰‹å·¥ç‰¹å¾æ¥ä¼˜åŒ–MAEè®¾è®¡ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ–¹å‘ä¸Šçš„ç¨³å®šç‰¹å¾å­¦ä¹ ã€‚HFBRI-MAEåˆ©ç”¨æ—‹è½¬ä¸å˜çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾è¿›è¡Œä»¤ç‰ŒåµŒå…¥å’Œä½ç½®åµŒå…¥ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†æ—‹è½¬ä¾èµ–æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸°å¯Œçš„å‡ ä½•ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†é‡å»ºç›®æ ‡é‡æ–°å®šä¹‰ä¸ºè¾“å…¥çš„æ ‡å‡†å¯¹é½ç‰ˆæœ¬ï¼Œä»¥æ¶ˆé™¤æ—‹è½¬æ¨¡ç³Šã€‚åœ¨ModelNet40ã€ScanObjectNNå’ŒShapeNetPartä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHFBRI-MAEåœ¨ç›®æ ‡åˆ†ç±»ã€åˆ†å‰²å’Œå°‘æ ·æœ¬å­¦ä¹ ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„3Dåº”ç”¨ä¸­çš„ç¨³å¥æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14132v1">PDF</a> 12 pages, 9 figures, accepted by IJCNN 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰‹å·¥ç‰¹å¾çš„æ—‹è½¬ä¸å˜æ©ç è‡ªç¼–ç å™¨ï¼ˆHFBRI-MAEï¼‰æ¡†æ¶ï¼Œç”¨äºè§£å†³ç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨ç‚¹äº‘åˆ†æé¢†åŸŸå­˜åœ¨çš„æ—‹è½¬ä¸å˜æ€§é—®é¢˜ã€‚é€šè¿‡èåˆæ—‹è½¬ä¸å˜çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œå®ç°å¯¹ä¸åŒæ–¹å‘ä¸‹çš„ç¨³å®šç‰¹å¾å­¦ä¹ ã€‚åŒæ—¶ï¼Œé‡æ–°å®šä¹‰é‡å»ºç›®æ ‡ä¸ºè¾“å…¥çš„æ ‡å‡†å¯¹é½ç‰ˆæœ¬ï¼Œå‡å°‘æ—‹è½¬æ¨¡ç³Šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒHFBRI-MAEåœ¨ModelNet40ã€ScanObjectNNå’ŒShapeNetPartæ•°æ®é›†ä¸Šçš„ç‰©ä½“åˆ†ç±»ã€åˆ†å‰²å’Œå°‘æ ·æœ¬å­¦ä¹ æ•ˆæœå“è¶Šï¼Œè¡¨ç°å‡ºå…¶å¼ºå¤§çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HFBRI-MAEæ¡†æ¶è§£å†³äº†ç°æœ‰è‡ªç›‘ç£å­¦ä¹ åœ¨ç‚¹äº‘åˆ†æä¸­çš„æ—‹è½¬ä¸å˜æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»“åˆæ—‹è½¬ä¸å˜çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾è¿›è¡Œä»¤ç‰ŒåµŒå…¥å’Œä½ç½®åµŒå…¥ï¼ŒHFBRI-MAEæœ‰æ•ˆæ¶ˆé™¤æ—‹è½¬ä¾èµ–æ€§å¹¶ä¿ç•™ä¸°å¯Œçš„å‡ ä½•ç»“æ„ã€‚</li>
<li>é‡æ–°å®šä¹‰çš„é‡å»ºç›®æ ‡æ˜¯è¾“å…¥çš„æ ‡å‡†å¯¹é½ç‰ˆæœ¬ï¼Œå‡å°‘æ—‹è½¬æ¨¡ç³Šæ€§ã€‚</li>
<li>HFBRI-MAEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ModelNet40ã€ScanObjectNNå’ŒShapeNetPartã€‚</li>
<li>HFBRI-MAEåœ¨ç‰©ä½“åˆ†ç±»ã€åˆ†å‰²å’Œå°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>HFBRI-MAEå…·æœ‰å“è¶Šçš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå®é™…çš„ä¸‰ç»´åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d0a6bac82349f159a7443d08af3006f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-030927b8a31ab7baa34467360b943011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-900eef9a60698b26d0e114d0df3e2530.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fabe09855aaf542c02bc4c5c944e98c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f538cc624aa49bb5171f9ebc366ce36b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Baseline-for-Self-state-Identification-and-Classification-in-Mental-Health-Data-CLPsych-2025-Task"><a href="#A-Baseline-for-Self-state-Identification-and-Classification-in-Mental-Health-Data-CLPsych-2025-Task" class="headerlink" title="A Baseline for Self-state Identification and Classification in Mental   Health Data: CLPsych 2025 Task"></a>A Baseline for Self-state Identification and Classification in Mental   Health Data: CLPsych 2025 Task</h2><p><strong>Authors:Laerdon Kim</strong></p>
<p>We present a baseline for the CLPsych 2025 A.1 task: classifying self-states in mental health data taken from Reddit. We use few-shot learning with a 4-bit quantized Gemma 2 9B model and a data preprocessing step which first identifies relevant sentences indicating self-state evidence, and then performs a binary classification to determine whether the sentence is evidence of an adaptive or maladaptive self-state. This system outperforms our other method which relies on an LLM to highlight spans of variable length independently. We attribute the performance of our model to the benefits of this sentence chunking step for two reasons: partitioning posts into sentences 1) broadly matches the granularity at which self-states were human-annotated and 2) simplifies the task for our language model to a binary classification problem. Our system places third out of fourteen systems submitted for Task A.1, achieving a test-time recall of 0.579. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹CLPsych 2025 A.1ä»»åŠ¡æå‡ºäº†ä¸€ä¸ªåŸºçº¿æ–¹æ³•ï¼šå¯¹æ¥è‡ªRedditçš„å¿ƒç†å¥åº·æ•°æ®è¿›è¡Œè‡ªæˆ‘çŠ¶æ€åˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨å°‘é‡æ ·æœ¬å­¦ä¹ ï¼Œç»“åˆ4ä½é‡åŒ–Gemma 2 9Bæ¨¡å‹å’Œæ•°æ®å¤„ç†æ­¥éª¤ï¼Œé¦–å…ˆè¯†åˆ«è¡¨ç¤ºè‡ªæˆ‘çŠ¶æ€è¯æ®çš„ç›¸å…³å¥å­ï¼Œç„¶åå¯¹å¥å­è¿›è¡ŒäºŒå…ƒåˆ†ç±»ï¼Œä»¥ç¡®å®šå¥å­æ˜¯é€‚åº”æ€§è¿˜æ˜¯é€‚åº”ä¸è‰¯çš„è‡ªæˆ‘çŠ¶æ€è¯æ®ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè¡¨ç°ä¼˜äºå¦ä¸€ç§ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ç‹¬ç«‹çªå‡ºæ˜¾ç¤ºå¯å˜é•¿åº¦æ®µè½çš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†æ¨¡å‹çš„æ€§èƒ½å½’åŠŸäºå¥å­åˆ†å—æ­¥éª¤çš„ä¸¤ä¸ªå¥½å¤„ï¼šå°†å¸–å­åˆ†æˆå¥å­1ï¼‰ä¸äººç±»æ³¨é‡Šè‡ªæˆ‘çŠ¶æ€çš„å¤§è‡´ç²’åº¦ç›¸åŒ¹é…ï¼›2ï¼‰å°†ä»»åŠ¡ç®€åŒ–ä¸ºäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œè¿™ç®€åŒ–äº†æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨A.1ä»»åŠ¡ä¸­æäº¤çš„14ä¸ªç³»ç»Ÿä¸­æ’åç¬¬ä¸‰ï¼Œæµ‹è¯•æ—¶çš„å¬å›ç‡ä¸º0.579ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14066v1">PDF</a> Accepted to CLPsych Workshop, NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹CLPsych 2025 A.1ä»»åŠ¡çš„åˆ†ç±»åŸºçº¿ï¼šä½¿ç”¨å°‘é‡å­¦ä¹ æ ·æœ¬ï¼Œé‡‡ç”¨4ä½é‡åŒ–çš„Gemma 2 9Bæ¨¡å‹å¯¹æ¥è‡ªRedditçš„å¿ƒç†å¥åº·æ•°æ®è¿›è¡Œè‡ªæˆ‘çŠ¶æ€åˆ†ç±»ã€‚é€šè¿‡æ•°æ®é¢„å¤„ç†æ­¥éª¤ï¼Œé¦–å…ˆè¯†åˆ«è¡¨ç¤ºè‡ªæˆ‘çŠ¶æ€è¯æ®çš„ç›¸å…³å¥å­ï¼Œç„¶åè¿›è¡ŒäºŒå…ƒåˆ†ç±»ï¼Œç¡®å®šå¥å­æ˜¯é€‚åº”æ€§çš„è¿˜æ˜¯é€‚åº”ä¸è‰¯çš„ã€‚è¯¥ç³»ç»Ÿä¼˜äºä¾èµ–LLMè¿›è¡Œç‹¬ç«‹å˜é‡é•¿åº¦è·¨è·çªå‡ºçš„å¦ä¸€ç§æ–¹æ³•ã€‚ä½œè€…è®¤ä¸ºæ¨¡å‹æ€§èƒ½çš„æå‡å¾—ç›Šäºå¥å­åˆ†å—æ­¥éª¤çš„ä¸¤ä¸ªä¼˜ç‚¹ï¼šå°†å¸–å­åˆ†æˆå¥å­ï¼Œè¿™å¤§è‡´åŒ¹é…äººç±»æ ‡æ³¨è‡ªæˆ‘çŠ¶æ€çš„ç²’åº¦å¹¶ç®€åŒ–äº†è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡ä¸ºäºŒå…ƒåˆ†ç±»é—®é¢˜ã€‚è¯¥ç³»ç»Ÿåœ¨A.1ä»»åŠ¡ä¸­ä½åˆ—ç¬¬ä¸‰ï¼Œæµ‹è¯•æ—¶çš„å¬å›ç‡ä¸º0.579ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜æå‡ºäº†é’ˆå¯¹CLPsych 2025 A.1ä»»åŠ¡çš„åˆ†ç±»åŸºçº¿æ–¹æ³•ï¼Œæ—¨åœ¨ä»Redditæ•°æ®ä¸­åˆ†ç±»è‡ªæˆ‘çŠ¶æ€ã€‚</li>
<li>é‡‡ç”¨å°‘é‡å­¦ä¹ æ ·æœ¬å’Œ4ä½é‡åŒ–çš„Gemma 2 9Bæ¨¡å‹è¿›è¡Œä»»åŠ¡å¤„ç†ã€‚</li>
<li>æ•°æ®é¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬è¯†åˆ«è¡¨ç¤ºè‡ªæˆ‘çŠ¶æ€è¯æ®çš„ç›¸å…³å¥å­ï¼Œå¹¶è¿›è¡ŒäºŒå…ƒåˆ†ç±»ã€‚</li>
<li>ç³»ç»Ÿæ€§èƒ½å¾—ç›Šäºå¥å­åˆ†å—æ­¥éª¤ï¼Œå› ä¸ºå®ƒä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è¯­è¨€æ¨¡å‹ä»»åŠ¡ä¸ºäºŒå…ƒåˆ†ç±»é—®é¢˜å¹¶ç¬¦åˆäººç±»æ ‡æ³¨çš„è‡ªæˆ‘çŠ¶æ€ç²’åº¦ã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨æµ‹è¯•æ—¶çš„å¬å›ç‡ä¸º0.579ï¼Œè¡¨ç°è‰¯å¥½ã€‚</li>
<li>ä¸ä¾èµ–LLMç‹¬ç«‹æ ‡æ³¨è·¨è·é•¿åº¦çš„å¦ä¸€ç§æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5150853aa6f487912528c72ec9ed6746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7aceafd88cce7acb8d09425cc5819e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e94e241885d5607e5c923a8612ad7e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efa35bf25ab3512de0da69bd73808761.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Flow-to-Learn-Flow-Matching-on-Neural-Network-Parameters"><a href="#Flow-to-Learn-Flow-Matching-on-Neural-Network-Parameters" class="headerlink" title="Flow to Learn: Flow Matching on Neural Network Parameters"></a>Flow to Learn: Flow Matching on Neural Network Parameters</h2><p><strong>Authors:Daniel Saragih, Deyu Cao, Tejas Balaji, Ashwin Santhosh</strong></p>
<p>Foundational language models show a remarkable ability to learn new concepts during inference via context data. However, similar work for images lag behind. To address this challenge, we introduce FLoWN, a flow matching model that learns to generate neural network parameters for different tasks. Our approach models the flow on latent space, while conditioning the process on context data. Experiments verify that FLoWN attains various desiderata for a meta-learning model. In addition, it matches or exceeds baselines on in-distribution tasks, provides better initializations for classifier training, and is performant on out-of-distribution few-shot tasks while having a fine-tuning mechanism to improve performance. </p>
<blockquote>
<p>åŸºç¡€è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡ä¸Šä¸‹æ–‡æ•°æ®å­¦ä¹ æ–°æ¦‚å¿µçš„æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç±»ä¼¼çš„å›¾åƒç›¸å…³å·¥ä½œä»è½åã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FLoWNï¼Œè¿™æ˜¯ä¸€ä¸ªæµåŒ¹é…æ¨¡å‹ï¼Œæ—¨åœ¨å­¦ä¹ ä¸ºä¸åŒä»»åŠ¡ç”Ÿæˆç¥ç»ç½‘ç»œå‚æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¨¡æ‹Ÿäº†æ½œåœ¨ç©ºé—´ä¸Šçš„æµï¼ŒåŒæ—¶æ ¹æ®ä¸Šä¸‹æ–‡æ•°æ®è¿›è¡Œå¤„ç†ã€‚å®éªŒè¯æ˜ï¼ŒFLoWNæ»¡è¶³äº†å…ƒå­¦ä¹ æ¨¡å‹çš„å„ç§éœ€æ±‚ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å†…éƒ¨ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡äº†åŸºçº¿æ°´å¹³ï¼Œä¸ºåˆ†ç±»å™¨è®­ç»ƒæä¾›äº†æ›´å¥½çš„åˆå§‹åŒ–æ–¹æ¡ˆï¼Œå¹¶åœ¨å¤–éƒ¨å°‘é‡ä»»åŠ¡çš„åœºæ™¯ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰å¾®è°ƒæœºåˆ¶ä»¥æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19371v2">PDF</a> Accepted at the ICLR Workshop on Neural Network Weights as a New Data   Modality 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFLoWNçš„æµåŒ¹é…æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­ç”Ÿæˆç¥ç»ç½‘ç»œå‚æ•°ã€‚å®ƒé€šè¿‡æ¨¡æ‹Ÿæ½œåœ¨ç©ºé—´çš„æµæ¥å»ºæ¨¡ï¼Œå¹¶ä»¥ä¸Šä¸‹æ–‡æ•°æ®ä¸ºæ¡ä»¶è¿›è¡Œè¿‡ç¨‹æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒFLoWNè¾¾åˆ°äº†å…ƒå­¦ä¹ æ¨¡å‹çš„å„ç§æœŸæœ›è¦æ±‚ï¼Œå¹¶åœ¨å†…éƒ¨ä»»åŠ¡ä¸ŠåŒ¹é…æˆ–è¶…è¿‡äº†åŸºçº¿æ°´å¹³ï¼Œä¸ºåˆ†ç±»å™¨è®­ç»ƒæä¾›äº†æ›´å¥½çš„åˆå§‹åŒ–ï¼Œå¹¶åœ¨å¤–éƒ¨ä»»åŠ¡çš„å°‘æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰å¾®è°ƒæœºåˆ¶ä»¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FLoWNæ¨¡å‹é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸Šå»ºæ¨¡æµæ¥è§£å†³å›¾åƒé¢†åŸŸçš„æ¦‚å¿µå­¦ä¹ é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ä¸­ç”Ÿæˆç¥ç»ç½‘ç»œå‚æ•°ã€‚</li>
<li>FLoWNèƒ½å¤Ÿä»¥ä¸Šä¸‹æ–‡æ•°æ®ä¸ºæ¡ä»¶è¿›è¡Œè¿‡ç¨‹æ§åˆ¶ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFLoWNè¾¾åˆ°äº†å…ƒå­¦ä¹ æ¨¡å‹çš„æœŸæœ›è¦æ±‚ã€‚</li>
<li>FLoWNåœ¨å†…éƒ¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼ŒåŒ¹é…æˆ–è¶…è¿‡äº†åŸºçº¿æ°´å¹³ã€‚</li>
<li>FLoWNèƒ½å¤Ÿä¸ºåˆ†ç±»å™¨è®­ç»ƒæä¾›æ›´å¥½çš„åˆå§‹åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f00fb6372a6302faa53b83b2287df0a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-198ba6e2b646fbf10214dabbcb289805.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f50c7d8b9b229a2aa0db3ce5345909a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e1ea762a176456a19d88e912e1beaa6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TLAC-Two-stage-LMM-Augmented-CLIP-for-Zero-Shot-Classification"><a href="#TLAC-Two-stage-LMM-Augmented-CLIP-for-Zero-Shot-Classification" class="headerlink" title="TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification"></a>TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification</h2><p><strong>Authors:Ans Munir, Faisal Z. Qureshi, Muhammad Haris Khan, Mohsen Ali</strong></p>
<p>Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot performance on image classification. However, state-of-the-art methods often rely on fine-tuning techniques like prompt learning and adapter-based tuning to optimize CLIPâ€™s performance. The necessity for fine-tuning significantly limits CLIPâ€™s adaptability to novel datasets and domains. This requirement mandates substantial time and computational resources for each new dataset. To overcome this limitation, we introduce simple yet effective training-free approaches, Single-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC), that leverages powerful Large Multimodal Models (LMMs), such as Gemini, for image classification. The proposed methods leverages the capabilities of pre-trained LMMs, allowing for seamless adaptation to diverse datasets and domains without the need for additional training. Our approaches involve prompting the LMM to identify objects within an image. Subsequently, the CLIP text encoder determines the image class by identifying the dataset class with the highest semantic similarity to the LLM predicted object. Our models achieved superior accuracy on 9 of 11 base-to-novel datasets, including ImageNet, SUN397, and Caltech101, while maintaining a strictly training-free paradigm. Our TLAC model achieved an overall accuracy of 83.44%, surpassing the previous state-of-the-art few-shot methods by a margin of 6.75%. Compared to other training-free approaches, our TLAC method achieved 83.6% average accuracy across 13 datasets, a 9.7% improvement over the previous methods. Our Code is available at <a target="_blank" rel="noopener" href="https://github.com/ans92/TLAC">https://github.com/ans92/TLAC</a> </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å›¾åƒåˆ†ç±»æ–¹é¢å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¾®è°ƒæŠ€æœ¯ï¼Œå¦‚åŸºäºæç¤ºçš„å­¦ä¹ å’ŒåŸºäºé€‚é…å™¨çš„è°ƒæ•´ï¼Œä»¥ä¼˜åŒ–CLIPçš„æ€§èƒ½ã€‚è¿™ç§å¯¹å¾®è°ƒçš„å¿…è¦æ€§æ˜¾è‘—é™åˆ¶äº†CLIPå¯¹æ–°æ•°æ®é›†å’Œé¢†åŸŸçš„é€‚åº”èƒ½åŠ›ã€‚å¯¹äºæ¯ä¸ªæ–°æ•°æ®é›†ï¼Œè¿™ç§è¦æ±‚éƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç®€å•è€Œæœ‰æ•ˆçš„æ— è®­ç»ƒæ–¹æ³•ï¼Œå³å•é˜¶æ®µLMMå¢å¼ºCLIPï¼ˆSLACï¼‰å’ŒåŒé˜¶æ®µLMMå¢å¼ºCLIPï¼ˆTLACï¼‰ï¼Œè¿™äº›æ–¹æ³•åˆ©ç”¨å¼ºå¤§çš„å¤§å‹å¤šåª’ä½“æ¨¡å‹ï¼ˆLMMsï¼‰ï¼Œå¦‚Geminiï¼Œè¿›è¡Œå›¾åƒåˆ†ç±»ã€‚æ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„LMMçš„èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒå³å¯æ— ç¼é€‚åº”å„ç§æ•°æ®é›†å’Œé¢†åŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬æç¤ºLMMæ¥è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡ã€‚ç„¶åï¼ŒCLIPæ–‡æœ¬ç¼–ç å™¨é€šè¿‡è¯†åˆ«ä¸LLMé¢„æµ‹å¯¹è±¡è¯­ä¹‰ç›¸ä¼¼æ€§æœ€é«˜çš„æ•°æ®é›†ç±»åˆ«æ¥ç¡®å®šå›¾åƒç±»åˆ«ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åŒ…æ‹¬ImageNetã€SUN397å’ŒCaltech101åœ¨å†…çš„11ä¸ªåŸºç¡€åˆ°æ–°é¢–æ•°æ®é›†ä¸­çš„9ä¸ªä¸Šå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ä¸¥æ ¼çš„æ— éœ€è®­ç»ƒçš„æ¨¡å¼ã€‚æˆ‘ä»¬çš„TLACæ¨¡å‹çš„æ€»å‡†ç¡®ç‡ä¸º83.44%ï¼Œè¾ƒä¹‹å‰çš„å…ˆè¿›å°‘æ ·æœ¬æ–¹æ³•æé«˜äº†6.75%ã€‚ä¸å…¶ä»–æ— è®­ç»ƒçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„TLACæ–¹æ³•åœ¨13ä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†å¹³å‡83.6%çš„å‡†ç¡®ç‡ï¼Œæ¯”ä¹‹å‰çš„æ–¹æ³•æé«˜äº†9.7%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ans92/TLAC%E3%80%82">https://github.com/ans92/TLACã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12206v2">PDF</a> Added code link in the abstract</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ï¼Œå¦‚Geminiï¼Œæ¥å…‹æœCLIPæ¨¡å‹åœ¨é›¶æ ·æœ¬å­¦ä¹ ä¸­çš„å±€é™æ€§ã€‚æ–‡ç« æå‡ºäº†ä¸¤ç§è®­ç»ƒå…è´¹çš„æ–¹æ³•ï¼šå•é˜¶æ®µLMMå¢å¼ºCLIPï¼ˆSLACï¼‰å’ŒåŒé˜¶æ®µLMMå¢å¼ºCLIPï¼ˆTLACï¼‰ã€‚è¿™äº›æ–¹æ³•é€šè¿‡åœ¨é¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å¸®åŠ©ä¸‹è¿›è¡Œæç¤ºæ“ä½œï¼Œå…è®¸æ— ç¼é€‚åº”ä¸åŒçš„æ•°æ®é›†å’Œé¢†åŸŸè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTLACæ¨¡å‹åœ¨è®­ç»ƒå’Œé›¶æ ·æœ¬å­¦ä¹ æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPæ¨¡å‹åœ¨å›¾åƒåˆ†ç±»çš„é›¶æ ·æœ¬å­¦ä¹ ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ä»éœ€è¦å¾®è°ƒä»¥é€‚åº”æ–°çš„æ•°æ®é›†å’Œé¢†åŸŸã€‚</li>
<li>æ–°çš„æ–¹æ³•SLACå’ŒTLACåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰è¿›è¡Œå›¾åƒåˆ†ç±»ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚</li>
<li>SLACå’ŒTLACé€šè¿‡æç¤ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡ï¼Œå¹¶é€šè¿‡CLIPæ–‡æœ¬ç¼–ç å™¨ç¡®å®šå›¾åƒç±»åˆ«ã€‚</li>
<li>TLACæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æœ‰æ›´ä¼˜ç§€çš„å‡†ç¡®ç‡å’Œé€‚åº”åŠ›ã€‚</li>
<li>TLACæ¨¡å‹çš„æ•´ä½“å‡†ç¡®ç‡è¾¾åˆ°83.44%ï¼Œåœ¨å°‘æ•°æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡äº†ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>ä¸å…¶ä»–æ— è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒTLACæ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†9.7%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dcc88bdf0ce877764ff06468835925a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48eb7474a975f787486e1a49586da6cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ceda9e733daad28b675c3d19d216ac65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7380b37afbfe03f99eb74962a0e7dd0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aa40195e0ff90161c97c3b2af5aa0a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4987d9b7b2f485445a45db2baf1971b2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Tree-of-Attributes-Prompt-Learning-for-Vision-Language-Models"><a href="#Tree-of-Attributes-Prompt-Learning-for-Vision-Language-Models" class="headerlink" title="Tree of Attributes Prompt Learning for Vision-Language Models"></a>Tree of Attributes Prompt Learning for Vision-Language Models</h2><p><strong>Authors:Tong Ding, Wanhua Li, Zhongqi Miao, Hanspeter Pfister</strong></p>
<p>Prompt learning has proven effective in adapting vision language models for downstream tasks. However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully leverage the rich context indicated in the category name. To address this issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs LLMs to generate a tree of attributes with a â€œconcept - attribute - descriptionâ€ structure for each category, and then learn the hierarchy with vision and text prompt tokens. Unlike existing methods that merely augment category names with a set of unstructured descriptions, our approach essentially distills structured knowledge graphs associated with class names from LLMs. Furthermore, our approach introduces text and vision prompts designed to explicitly learn the corresponding visual attributes, effectively serving as domain experts. Additionally, the general and diverse descriptions generated based on the class names may be wrong or absent in the specific given images. To address this misalignment, we further introduce a vision-conditional pooling module to extract instance-specific text features. Extensive experimental results demonstrate that our approach outperforms state-of-the-art methods on the zero-shot base-to-novel generalization, cross-dataset transfer, as well as few-shot classification across 11 diverse datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HHenryD/TAP">https://github.com/HHenryD/TAP</a>. </p>
<blockquote>
<p>æç¤ºå­¦ä¹ åœ¨é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ä»¥è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡æ–¹é¢å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä»…å°†å¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œé™„åŠ åˆ°ç±»åˆ«åç§°ä¸Šä»¥è·å¾—æ–‡æœ¬ç‰¹å¾ï¼Œè¿™æœªèƒ½å……åˆ†åˆ©ç”¨ç±»åˆ«åç§°ä¸­æŒ‡ç¤ºçš„ä¸°å¯Œä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å±æ€§æ ‘æç¤ºå­¦ä¹ ï¼ˆTAPï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆå…·æœ‰â€œæ¦‚å¿µ-å±æ€§-æè¿°â€ç»“æ„çš„å±æ€§æ ‘ï¼Œç„¶åå­¦ä¹ ä¸è§†è§‰å’Œæ–‡æœ¬æç¤ºä»¤ç‰Œå¯¹åº”çš„å±‚æ¬¡ç»“æ„ã€‚ä¸ä»…å°†ä¸€ç³»åˆ—éç»“æ„åŒ–çš„æè¿°é™„åŠ åˆ°ç±»åˆ«åç§°ä¸Šçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æç‚¼å‡ºä¸ç±»åç›¸å…³çš„ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç”¨äºæ˜ç¡®å­¦ä¹ ç›¸åº”è§†è§‰å±æ€§çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæœ‰æ•ˆåœ°å……å½“é¢†åŸŸä¸“å®¶ã€‚åŸºäºç±»åç”Ÿæˆçš„é€šç”¨ä¸”å¤šæ ·çš„æè¿°åœ¨å…·ä½“ç»™å®šçš„å›¾åƒä¸­å¯èƒ½å‡ºé”™æˆ–ç¼ºå¤±ã€‚ä¸ºäº†è§£å†³è¿™ç§ä¸åŒ¹é…é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªè§†è§‰æ¡ä»¶æ± æ¨¡å—æ¥æå–ç‰¹å®šå®ä¾‹çš„æ–‡æœ¬ç‰¹å¾ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»ä»¥åŠåœ¨11ä¸ªä¸åŒæ•°æ®é›†çš„å°‘é‡æ ·æœ¬åˆ†ç±»ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HHenryD/TAP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HHenryD/TAPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11201v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºPromptå­¦ä¹ çš„æ–¹æ³•èƒ½æœ‰æ•ˆé€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡ã€‚ä½†ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ·»åŠ ç±»åˆ«åç§°çš„å¯å­¦ä¹ æç¤ºç¬¦å·æ¥è·å¾—æ–‡æœ¬ç‰¹å¾ï¼Œè¿™æœªèƒ½å……åˆ†åˆ©ç”¨ç±»åˆ«åç§°ä¸­çš„ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å±æ€§æ ‘æç¤ºå­¦ä¹ ï¼ˆTAPï¼‰æ–¹æ³•ã€‚TAPé¦–å…ˆæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆä¸€ä¸ªå±æ€§æ ‘ï¼Œå½¢æˆâ€œæ¦‚å¿µ-å±æ€§-æè¿°â€çš„ç»“æ„ï¼Œç„¶åé€šè¿‡è§†è§‰å’Œæ–‡æœ¬æç¤ºç¬¦å·å­¦ä¹ å±‚æ¬¡ç»“æ„ã€‚ä¸ä»…é€šè¿‡æ·»åŠ ç±»åˆ«åç§°çš„ä¸€ç³»åˆ—éç»“æ„åŒ–æè¿°ä¸åŒï¼ŒTAPæœ¬è´¨ä¸Šæ˜¯ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æç‚¼å‡ºä¸ç±»åˆ«åç§°ç›¸å…³çš„ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ã€‚æ­¤å¤–ï¼ŒTAPè¿˜å¼•å…¥äº†æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæ—¨åœ¨æ˜ç¡®å­¦ä¹ ç›¸åº”çš„è§†è§‰å±æ€§ï¼Œæœ‰æ•ˆå……å½“é¢†åŸŸä¸“å®¶ã€‚ä¸ºäº†è§£å†³ç‰¹å®šå›¾åƒä¸­åŸºäºç±»åˆ«åç§°ç”Ÿæˆçš„é€šç”¨å’Œå¤šæ ·åŒ–æè¿°å¯èƒ½å­˜åœ¨çš„é”™è¯¯æˆ–ç¼ºå¤±é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è§†è§‰æ¡ä»¶æ± æ¨¡å—ï¼Œä»¥æå–ç‰¹å®šå®ä¾‹çš„æ–‡æœ¬ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAPåœ¨é›¶æ ·æœ¬åŸºç¡€åˆ°æ–°é¢–ç±»åˆ«çš„æ¨å¹¿ã€è·¨æ•°æ®é›†è¿ç§»ä»¥åŠ11ä¸ªä¸åŒæ•°æ®é›†çš„å°‘æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TAPæ–¹æ³•é€šè¿‡ç”Ÿæˆå±æ€§æ ‘æ¥ä¸°å¯Œç±»åˆ«åç§°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜Promptå­¦ä¹ çš„æ•ˆæœã€‚</li>
<li>TAPä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æç‚¼ä¸ç±»åˆ«åç§°ç›¸å…³çš„ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ã€‚</li>
<li>TAPå¼•å…¥æ–‡æœ¬å’Œè§†è§‰æç¤ºæ¥å­¦ä¹ è§†è§‰å±æ€§ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨ç°åŠ›ã€‚</li>
<li>TAPè§£å†³äº†ç‰¹å®šå›¾åƒæè¿°å¯èƒ½å­˜åœ¨çš„é”™è¯¯æˆ–ç¼ºå¤±é—®é¢˜ï¼Œé€šè¿‡è§†è§‰æ¡ä»¶æ± æ¨¡å—æå–ç‰¹å®šå®ä¾‹çš„æ–‡æœ¬ç‰¹å¾ã€‚</li>
<li>TAPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šã€‚</li>
<li>TAPæ–¹æ³•å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¯é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡å’Œæ•°æ®é›†ã€‚</li>
<li>TAPæ–¹æ³•çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11201">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3898defe56ce39f9a99c26949d694faf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-722c847def8fad2a116798d8be87ec11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-874bb9fdbf0b09085549e51626372a42.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Aligning-Language-Models-with-Demonstrated-Feedback"><a href="#Aligning-Language-Models-with-Demonstrated-Feedback" class="headerlink" title="Aligning Language Models with Demonstrated Feedback"></a>Aligning Language Models with Demonstrated Feedback</h2><p><strong>Authors:Omar Shaikh, Michelle S. Lam, Joey Hejna, Yijia Shao, Hyundong Cho, Michael S. Bernstein, Diyi Yang</strong></p>
<p>Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (&lt; 10) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a userâ€™s demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating usersâ€™ demonstrations as preferred over output from the LLM and its intermediate checkpoints. Concretely, DITTO operates by having an LLM generate examples that are presumed to be inferior to expert demonstrations. The method iteratively constructs pairwise preference relationships between these LLM-generated samples and expert demonstrations, potentially including comparisons between different training checkpoints. These constructed preference pairs are then used to train the model using a preference optimization algorithm (e.g. DPO). We evaluate DITTOâ€™s ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants (N &#x3D; 16). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an avg. of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹æ—¨åœ¨æ¨¡æ‹Ÿè®¸å¤šäººçš„é›†ä½“å£°éŸ³ï¼Œä»è€Œäº§ç”Ÿä¸ç‰¹å®šäººæ— å…³çš„è¾“ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ åé¦ˆï¼ˆRLHFï¼‰å¯ä»¥é¿å…è¯­è¨€å¤§æ¨¡å‹äº§ç”Ÿé€šç”¨è¾“å‡ºï¼Œä½†è¿™éœ€è¦é’ˆå¯¹æ–°ä»»åŠ¡çš„ç‰¹å®šæ•°æ®é›†ä¸”æ•°æ®é‡å·¨å¤§ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé€šè¿‡åˆ©ç”¨æå°‘é‡çš„æ¼”ç¤ºï¼ˆ&lt;10ä¸ªï¼‰ä½œä¸ºåé¦ˆï¼Œå¯ä»¥å°†è¯­è¨€å¤§æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç‰¹å®šåœºæ™¯è¿›è¡ŒåŒ¹é…ã€‚æˆ‘ä»¬çš„æ–¹æ³•â€”â€”Demonstration ITerated Task Optimizationï¼ˆDITTOï¼‰ç›´æ¥ä½¿è¯­è¨€æ¨¡å‹è¾“å‡ºä¸ç”¨æˆ·æ¼”ç¤ºè¡Œä¸ºä¿æŒä¸€è‡´ã€‚è¯¥æ–¹æ³•åŸºäºåœ¨çº¿æ¨¡ä»¿å­¦ä¹ çš„æ€æƒ³ï¼Œé€šè¿‡å°†ç”¨æˆ·çš„æ¼”ç¤ºè§†ä¸ºé¦–é€‰è¾“å‡ºï¼Œä¸è¯­è¨€æ¨¡å‹åŠå…¶ä¸­é—´æ£€æŸ¥ç‚¹çš„è¾“å‡ºè¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œä½æˆæœ¬åœ°ç”Ÿæˆåœ¨çº¿æ¯”è¾ƒæ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼ŒDITTOçš„æ“ä½œæ˜¯è®©è¯­è¨€æ¨¡å‹ç”Ÿæˆè¢«è®¤ä¸ºæ˜¯é€Šäºä¸“å®¶æ¼”ç¤ºçš„ä¾‹å­ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºè¿™äº›è¯­è¨€æ¨¡å‹ç”Ÿæˆæ ·æœ¬ä¸ä¸“å®¶æ¼”ç¤ºä¹‹é—´çš„é…å¯¹åå¥½å…³ç³»æ¥è¿­ä»£æ“ä½œï¼Œå¯èƒ½åŒ…æ‹¬ä¸åŒè®­ç»ƒæ£€æŸ¥ç‚¹ä¹‹é—´çš„æ¯”è¾ƒã€‚è¿™äº›æ„å»ºçš„åå¥½é…å¯¹éšåè¢«ç”¨äºä½¿ç”¨åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆä¾‹å¦‚DPOï¼‰è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†DITTOåœ¨ä¸åŒé¢†åŸŸï¼ˆå¦‚æ–°é—»æ–‡ç« ã€ç”µå­é‚®ä»¶å’Œåšå®¢æ–‡ç« ï¼‰å­¦ä¹ ç²¾ç»†é£æ ¼å’Œä»»åŠ¡å¯¹é½çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹ç”¨æˆ·ç ”ç©¶ï¼Œä»å‚ä¸è€…ä¸­å¾é›†äº†ä¸€ç³»åˆ—æ¼”ç¤ºï¼ˆN&#x3D;16ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œç”¨æˆ·ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‘ç°DITTOçš„èƒœç‡æ¯”å°‘æ ·æœ¬æç¤ºã€ç›‘ç£å¾®è°ƒå’Œå…¶ä»–è‡ªæˆ‘å¯¹æŠ—æ–¹æ³•å¹³å‡é«˜å‡º19ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡ç›´æ¥ä½¿ç”¨æ¼”ç¤ºä½œä¸ºåé¦ˆï¼ŒDITTOæä¾›äº†ä¸€ç§æœ‰æ•ˆå®šåˆ¶è¯­è¨€å¤§æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00888v2">PDF</a> ICLR 2025; 28 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é€šè¿‡åˆ©ç”¨ç”¨æˆ·ç¤ºèŒƒæ¥è¿›è¡Œåœ¨çº¿æ¯”è¾ƒæ•°æ®çš„ä½æˆæœ¬ç”Ÿæˆçš„æ–°æ–¹æ³•ï¼Œå³è¿­ä»£ä»»åŠ¡ä¼˜åŒ–ï¼ˆDITTOï¼‰ã€‚æ­¤æ–¹æ³•åŸºäºç”¨æˆ·ç¤ºèŒƒè¿›è¡Œè¿­ä»£è®­ç»ƒå’Œä¼˜åŒ–ï¼Œä»è€Œä½¿è¯­è¨€æ¨¡å‹çš„è¾“å‡ºæ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚ã€‚è¯„ä»·è¡¨æ˜ï¼Œåœ¨æ–°é—»æ–‡ç« ã€ç”µå­é‚®ä»¶å’Œåšå®¢æ–‡ç« ç­‰ä¸åŒé¢†åŸŸï¼ŒDITTOçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚é€šè¿‡ç”¨æˆ·ç¤ºèŒƒä½œä¸ºç›´æ¥åé¦ˆï¼ŒDITTOä¸ºæœ‰æ•ˆå®šåˆ¶å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹é€šè¿‡æ¨¡ä»¿é›†ä½“å£°éŸ³äº§ç”Ÿé€šç”¨è¾“å‡ºï¼Œä½†å¯é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰æ–¹æ³•è¿›è¡Œå¼•å¯¼ä»¥äº§ç”Ÿç‰¹å®šè¾“å‡ºã€‚</li>
<li>ä½¿ç”¨å°‘é‡ç¤ºèŒƒï¼ˆ&lt; 10ä¸ªï¼‰ä½œä¸ºåé¦ˆæ¥å¯¹é½è¯­è¨€æ¨¡å‹åˆ°ç‰¹å®šè®¾ç½®æ˜¯å¯èƒ½çš„ã€‚</li>
<li>DITTOæ–¹æ³•åˆ©ç”¨åœ¨çº¿æ¨¡ä»¿å­¦ä¹ çš„æ€æƒ³ï¼Œé€šè¿‡ç”¨æˆ·çš„ç¤ºèŒƒæ¥ä½æˆæœ¬åœ°ç”Ÿæˆåœ¨çº¿æ¯”è¾ƒæ•°æ®ã€‚</li>
<li>DITTOé€šè¿‡å°†è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¾‹å­è§†ä¸ºä¸å¦‚ä¸“å®¶ç¤ºèŒƒä¼˜è¶Šï¼Œæ¥è¿­ä»£æ„å»ºåå¥½å…³ç³»ã€‚</li>
<li>DITTOä½¿ç”¨åå¥½ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚DPOï¼‰æ¥è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>åœ¨æ–°é—»ã€ç”µå­é‚®ä»¶å’Œåšå®¢ç­‰ä¸åŒé¢†åŸŸï¼ŒDITTOåœ¨ç²¾ç»†é£æ ¼å’Œä»»åŠ¡å¯¹é½æ–¹é¢çš„å­¦ä¹ èƒ½åŠ›å¾—åˆ°äº†è¯„ä»·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.00888">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5dc40740180f34ed3bb685df86b41b92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3996dbe6f08a280518c5c10fe9641d5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c04f63f24ba48aefef58d5ce026377f4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GLoRE-Evaluating-Logical-Reasoning-of-Large-Language-Models"><a href="#GLoRE-Evaluating-Logical-Reasoning-of-Large-Language-Models" class="headerlink" title="GLoRE: Evaluating Logical Reasoning of Large Language Models"></a>GLoRE: Evaluating Logical Reasoning of Large Language Models</h2><p><strong>Authors:Hanmeng liu, Zhiyang Teng, Ruoxi Ning, Yiran Ding, Xiulai Li, Xiaozhang Liu, Yue Zhang</strong></p>
<p>Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAIâ€™s o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºæ˜¾è‘—çš„ä¸€èˆ¬è¯­è¨€ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹å¯¹è¿™äº›LLMçš„é€»è¾‘æ¨ç†èƒ½åŠ›è¿›è¡Œè¯„ä¼°çš„å°è¯•ï¼Œè¿™æ˜¯è‡ªç„¶è¯­è¨€ç†è§£çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ä¸ºäº†é¼“åŠ±åœ¨è¿™ä¸€é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†GLoREï¼Œä¸€ä¸ªé€šç”¨é€»è¾‘æ¨ç†è¯„ä¼°å¹³å°ï¼Œå®ƒä¸ä»…æ•´åˆäº†å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œè¿˜å°†å®ƒä»¬æ ‡å‡†åŒ–ä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ ¼å¼ï¼Œé€‚ç”¨äºåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸äººç±»å’Œç»è¿‡ç›‘ç£å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ç›¸æ¯”ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚OpenAIçš„o1 miniã€DeepSeek R1å’ŒQwQ-32Bï¼‰çš„é€»è¾‘æ¨ç†èƒ½åŠ›å·²ç»å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå…¶ä¸­QwQ-32Bè¾¾åˆ°äº†è¿„ä»Šä¸ºæ­¢çš„æœ€é«˜åŸºå‡†æ€§èƒ½ã€‚GLoREè¢«è®¾è®¡ä¸ºä¸€ä¸ªæŒç»­é›†æˆçš„é¡¹ç›®ï¼Œä¸æ–­èå…¥æ–°çš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¾¿äºåœ¨å•†ä¸šå’ŒHuggingfaceç¤¾åŒºä¸­å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œç¨³å¥å’Œæ¯”è¾ƒæ€§çš„è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09107v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£åŠ›ï¼Œä½†åœ¨é€»è¾‘ç†è§£æ–¹é¢çš„è¯„ä¼°ä»æ˜¾ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºé€šç”¨é€»è¾‘æ¨ç†è¯„ä¼°å¹³å°GLoREï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºäººç±»å’Œç»è¿‡å¾®è°ƒè®­ç»ƒçš„æ¨¡å‹ï¼Œå¤§å‹æ¨ç†æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›å·²æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œå…¶ä¸­QwQ-32Bå–å¾—äº†è¿„ä»Šä¸ºæ­¢çš„æœ€é«˜æ€§èƒ½ã€‚GLoREå°†æŒç»­é›†æˆæ–°æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¸ºå•†ä¸šå’ŒHuggingfaceç¤¾åŒºæä¾›ç¨³å¥çš„æ¨¡å‹æ€§èƒ½æ¯”è¾ƒè¯„ä¼°å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œä½†é€»è¾‘ç†è§£æ–¹é¢çš„è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚</li>
<li>GLoREå¹³å°æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæ”¯æŒé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯çš„è¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§å‹æ¨ç†æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›å·²ç»å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>QwQ-32Båœ¨é€»è¾‘æ¨ç†æ–¹é¢å–å¾—äº†è¿„ä»Šä¸ºæ­¢çš„æœ€é«˜æ€§èƒ½ã€‚</li>
<li>GLoREå¹³å°æ˜¯ä¸€ä¸ªæŒç»­å‘å±•çš„é¡¹ç›®ï¼Œèƒ½å¤Ÿé›†æˆæ–°æ•°æ®é›†å’Œæ¨¡å‹ã€‚</li>
<li>GLoREå¹³å°ä¸ºå•†ä¸šå’ŒHuggingfaceç¤¾åŒºæä¾›äº†æ¨¡å‹æ€§èƒ½æ¯”è¾ƒè¯„ä¼°çš„ç¨³å¥å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.09107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6507ce15a08e1e6d5afbe25fb6d8fba2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0af45720ebde4848157394035469e164.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaf32ba08c3ebcce4a788e65a0d09bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5159674ed85fbfbce48f13662d4a1d82.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Holistic-Evaluation-of-Piano-Sound-Quality"><a href="#A-Holistic-Evaluation-of-Piano-Sound-Quality" class="headerlink" title="A Holistic Evaluation of Piano Sound Quality"></a>A Holistic Evaluation of Piano Sound Quality</h2><p><strong>Authors:Monan Zhou, Shangda Wu, Shaohua Ji, Zijin Li, Wei Li</strong></p>
<p>This paper aims to develop a holistic evaluation method for piano sound quality to assist in purchasing decisions. Unlike previous studies that focused on the effect of piano performance techniques on sound quality, this study evaluates the inherent sound quality of different pianos. To derive quality evaluation systems, the study uses subjective questionnaires based on a piano sound quality dataset. The method selects the optimal piano classification models by comparing the fine-tuning results of different pre-training models of Convolutional Neural Networks (CNN). To improve the interpretability of the models, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. The results reveal that musically trained individuals are better able to distinguish between the sound quality differences of different pianos. The best fine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3% as the piano classifier. However, the dataset is limited, and the audio is sliced to increase its quantity, resulting in a lack of diversity and balance, so we use focal loss to reduce the impact of data imbalance. To optimize the method, the dataset will be expanded, or few-shot learning techniques will be employed in future research. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ç§å…¨é¢çš„é’¢ç´éŸ³è´¨è¯„ä¼°æ–¹æ³•ï¼Œä»¥è¾…åŠ©è´­ä¹°å†³ç­–ã€‚ä¸åŒäºä»¥å¾€ä¸“æ³¨äºé’¢ç´æ¼”å¥æŠ€å·§å¯¹éŸ³è´¨å½±å“çš„ç ”ç©¶ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°ä¸åŒé’¢ç´çš„å›ºæœ‰éŸ³è´¨ã€‚ä¸ºäº†å¾—å‡ºè´¨é‡è¯„ä¼°ç³»ç»Ÿï¼Œæœ¬ç ”ç©¶åŸºäºé’¢ç´éŸ³è´¨æ•°æ®é›†ï¼Œä½¿ç”¨ä¸»è§‚é—®å·çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¯”è¾ƒä¸åŒé¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒç»“æœï¼Œé€‰æ‹©æœ€ä½³çš„é’¢ç´åˆ†ç±»æ¨¡å‹ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œç ”ç©¶åº”ç”¨äº†ç­‰æ•ˆçŸ©å½¢å¸¦å®½ï¼ˆERBï¼‰åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œå—è¿‡éŸ³ä¹è®­ç»ƒçš„äººæ›´èƒ½åŒºåˆ†ä¸åŒé’¢ç´çš„éŸ³è´¨å·®å¼‚ã€‚æœ€ä½³çš„å¾®è°ƒCNNé¢„è®­ç»ƒä¸»å¹²ä½œä¸ºé’¢ç´åˆ†ç±»å™¨ï¼Œå®ç°äº†é«˜è¾¾98.3%çš„å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œæ•°æ®é›†æœ‰é™ï¼ŒéŸ³é¢‘è¢«åˆ‡ç‰‡ä»¥å¢åŠ å…¶æ•°é‡ï¼Œå¯¼è‡´ç¼ºä¹å¤šæ ·æ€§å’Œå¹³è¡¡æ€§ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ç„¦ç‚¹æŸå¤±æ¥å‡å°‘æ•°æ®ä¸å¹³è¡¡çš„å½±å“ã€‚ä¸ºäº†ä¼˜åŒ–è¯¥æ–¹æ³•ï¼Œå°†æ‰©å¤§æ•°æ®é›†æˆ–åœ¨æœªæ¥çš„ç ”ç©¶ä¸­é‡‡ç”¨å°æ ·æœ¬å­¦ä¹ æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04722v3">PDF</a> 15 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ç§å…¨é¢çš„é’¢ç´éŸ³è´¨è¯„ä¼°æ–¹æ³•ï¼Œä»¥è¾…åŠ©è´­ä¹°å†³ç­–ã€‚ç ”ç©¶ä¸åŒäºä»¥å¾€å…³æ³¨é’¢ç´æ¼”å¥æŠ€å·§å¯¹éŸ³è´¨å½±å“çš„ç ”ç©¶ï¼Œè€Œæ˜¯è¯„ä¼°ä¸åŒé’¢ç´çš„å›ºæœ‰éŸ³è´¨ã€‚ç ”ç©¶ä½¿ç”¨åŸºäºé’¢ç´éŸ³è´¨æ•°æ®é›†çš„ä¸»è§‚é—®å·æ¥æ¨å¯¼è´¨é‡è¯„ä¼°ç³»ç»Ÿï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒé¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒç»“æœæ¥é€‰æ‹©æœ€ä½³çš„é’¢ç´åˆ†ç±»æ¨¡å‹ã€‚ä¸ºæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œç ”ç©¶åº”ç”¨äº†ç­‰æ•ˆçŸ©å½¢å¸¦å®½ï¼ˆERBï¼‰åˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼Œå—è¿‡éŸ³ä¹è®­ç»ƒçš„äººæ›´èƒ½åŒºåˆ†ä¸åŒé’¢ç´çš„éŸ³è´¨å·®å¼‚ã€‚æœ€ä½³å¾®è°ƒCNNé¢„è®­ç»ƒä¸»å¹²ä½œä¸ºé’¢ç´åˆ†ç±»å™¨ï¼Œå‡†ç¡®ç‡é«˜è¾¾98.3%ã€‚ä½†æ•°æ®é›†æœ‰é™ï¼ŒéŸ³é¢‘è¢«åˆ‡ç‰‡ä»¥å¢åŠ æ•°é‡ï¼Œå¯¼è‡´ç¼ºä¹å¤šæ ·æ€§å’Œå¹³è¡¡æ€§ï¼Œå› æ­¤ä½¿ç”¨ç„¦ç‚¹æŸå¤±æ¥å‡å°‘æ•°æ®ä¸å¹³è¡¡çš„å½±å“ã€‚æœªæ¥ç ”ç©¶å°†æ‰©å¤§æ•°æ®é›†æˆ–é‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ æŠ€æœ¯ä»¥ä¼˜åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç›®çš„æ˜¯å¼€å‘ä¸€ç§å…¨é¢çš„é’¢ç´éŸ³è´¨è¯„ä¼°æ–¹æ³•ï¼Œå¸®åŠ©è´­ä¹°å†³ç­–ã€‚</li>
<li>ä¸å…¶ä»–ç ”ç©¶ä¸åŒï¼Œè¯¥ç ”ç©¶ä¾§é‡äºè¯„ä¼°ä¸åŒé’¢ç´çš„å›ºæœ‰éŸ³è´¨ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸»è§‚é—®å·å’ŒåŸºäºæ•°æ®é›†çš„è´¨é‡è¯„ä¼°ç³»ç»Ÿæ¥æ¨å¯¼è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>é€‰æ‹©æœ€ä½³é’¢ç´åˆ†ç±»æ¨¡å‹æ˜¯é€šè¿‡æ¯”è¾ƒä¸åŒé¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒç»“æœã€‚</li>
<li>åº”ç”¨ç­‰æ•ˆçŸ©å½¢å¸¦å®½ï¼ˆERBï¼‰åˆ†æä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>å—è¿‡éŸ³ä¹è®­ç»ƒçš„äººæ›´èƒ½åŒºåˆ†ä¸åŒé’¢ç´çš„éŸ³è´¨å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.04722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-718ec1bd93778693c99cdd3b5d80fa80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c6a4fc0cfaab7ed0a93ce3a81af4872.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed02193f2135c9d56dd83e195be3db6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a611290a6031460fa7fb44644130714a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3cef8151233a3370d61c421aa281eb56.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  ECViT Efficient Convolutional Vision Transformer with Local-Attention   and Multi-scale Stages
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3b7326e352312fc14c4f2e02b43f5add.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  FlowReasoner Reinforcing Query-Level Meta-Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18799.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
