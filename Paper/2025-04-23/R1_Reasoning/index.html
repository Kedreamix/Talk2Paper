<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  Stop Summation Min-Form Credit Assignment Is All Process Reward Model   Needs for Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2b2a88e5bcbadd645ade413c80e756a4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-23-æ›´æ–°"><a href="#2025-04-23-æ›´æ–°" class="headerlink" title="2025-04-23 æ›´æ–°"></a>2025-04-23 æ›´æ–°</h1><h2 id="Stop-Summation-Min-Form-Credit-Assignment-Is-All-Process-Reward-Model-Needs-for-Reasoning"><a href="#Stop-Summation-Min-Form-Credit-Assignment-Is-All-Process-Reward-Model-Needs-for-Reasoning" class="headerlink" title="Stop Summation: Min-Form Credit Assignment Is All Process Reward Model   Needs for Reasoning"></a>Stop Summation: Min-Form Credit Assignment Is All Process Reward Model   Needs for Reasoning</h2><p><strong>Authors:Jie Cheng, Ruixi Qiao, Lijun Li, Chao Guo, Junle Wang, Gang Xiong, Yisheng Lv, Fei-Yue Wang</strong></p>
<p>Process reward models (PRMs) have proven effective for test-time scaling of Large Language Models (LLMs) on challenging reasoning tasks. However, reward hacking issues with PRMs limit their successful application in reinforcement fine-tuning. In this paper, we identify the main cause of PRM-induced reward hacking: the canonical summation-form credit assignment in reinforcement learning (RL), which defines the value as cumulative gamma-decayed future rewards, easily induces LLMs to hack steps with high rewards. To address this, we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation of PURE is a min-form credit assignment that formulates the value function as the minimum of future rewards. This method significantly alleviates reward hacking by limiting the value function range and distributing advantages more reasonably. Through extensive experiments on 3 base models, we show that PRM-based approaches enabling min-form credit assignment achieve comparable reasoning performance to verifiable reward-based methods within only 30% steps. In contrast, the canonical sum-form credit assignment collapses training even at the beginning! Additionally, when we supplement PRM-based fine-tuning with just 10% verifiable rewards, we further alleviate reward hacking and produce the best fine-tuned model based on Qwen2.5-Math-7B in our experiments, achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5 benchmarks. Moreover, we summarize the observed reward hacking cases and analyze the causes of training collapse. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/CJReinforce/PURE">https://github.com/CJReinforce/PURE</a>. </p>
<blockquote>
<p>æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡ä¸Šï¼Œå¯¹äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æµ‹è¯•æ—¶é—´æ‰©å±•å·²ç»è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼ŒPRMä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜é™åˆ¶äº†å®ƒä»¬åœ¨å¼ºåŒ–å¾®è°ƒä¸­çš„æˆåŠŸåº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†å¯¼è‡´PRMå¥–åŠ±é»‘å®¢é—®é¢˜çš„ä¸»è¦åŸå› ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„è§„èŒƒæ±‚å’Œå½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œå®ƒå°†ä»·å€¼å®šä¹‰ä¸ºç´¯ç§¯çš„æœªæ¥å¥–åŠ±çš„gammaè¡°å‡å½¢å¼ï¼Œå®¹æ˜“è¯±å¯¼LLMå»ç ´è§£é«˜å¥–åŠ±çš„æ­¥éª¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PUREï¼šè¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ã€‚PUREçš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºä¸€ç§æœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œå°†ä»·å€¼å‡½æ•°åˆ¶å®šä¸ºæœªæ¥çš„æœ€å°å¥–åŠ±ã€‚è¿™ç§æ–¹æ³•é€šè¿‡é™åˆ¶ä»·å€¼å‡½æ•°èŒƒå›´å’Œæ›´åˆç†åœ°åˆ†é…ä¼˜åŠ¿æ¥æ˜¾è‘—ç¼“è§£å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚é€šè¿‡å¯¹ä¸‰ä¸ªåŸºç¡€æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†åŸºäºPRMçš„æ–¹æ³•èƒ½å¤Ÿå®ç°æœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œå¹¶ä¸”åœ¨ä»…30%çš„æ­¥éª¤å†…å®ç°äº†ä¸å¯éªŒè¯å¥–åŠ±æ–¹æ³•ç›¸å½“çš„æ¨ç†æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„æ±‚å’Œå½¢å¼çš„ä¿¡ç”¨åˆ†é…ç”šè‡³åœ¨è®­ç»ƒå¼€å§‹æ—¶å°±ä¼šå´©æºƒï¼æ­¤å¤–ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨ä»…10%çš„å¯éªŒè¯å¥–åŠ±æ¥è¡¥å……åŸºäºPRMçš„å¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç¼“è§£äº†å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„å®éªŒä¸­åŸºäºQwen2.5-Math-7Bç”Ÿæˆäº†æœ€ä½³å¾®è°ƒæ¨¡å‹ï¼Œåœ¨AMC23ä¸Šçš„å‡†ç¡®ç‡ä¸º82.5%ï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡å‡†ç¡®ç‡ä¸º53.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ€»ç»“äº†è§‚å¯Ÿåˆ°çš„å¥–åŠ±é»‘å®¢æ¡ˆä¾‹å¹¶åˆ†æäº†è®­ç»ƒå´©æºƒçš„åŸå› ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CJReinforce/PURE%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CJReinforce/PUREä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15275v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‘æˆ˜æ€§æ¨ç†ä»»åŠ¡æµ‹è¯•æ—¶ç¼©æ”¾ä¸­æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼ŒPRMä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜é™åˆ¶äº†å…¶åœ¨å¼ºåŒ–å¾®è°ƒä¸­çš„æˆåŠŸåº”ç”¨ã€‚æœ¬æ–‡ç¡®å®šäº†PRMè¯±å¯¼å¥–åŠ±é»‘å®¢çš„ä¸»è¦åŸå› ï¼šå¼ºåŒ–å­¦ä¹ ä¸­çš„è§„èŒƒæ±‚å’Œå½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œå®ƒå°†ä»·å€¼å®šä¹‰ä¸ºæœªæ¥å¥–åŠ±çš„ç´¯ç§¯gammaè¡°å‡ï¼Œå®¹æ˜“è¯±å¯¼LLMé»‘å®¢æ”»å‡»é«˜å¥–åŠ±çš„æ­¥éª¤ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PUREï¼šæµç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ã€‚PUREçš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºæœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œå®ƒå°†ä»·å€¼å‡½æ•°åˆ¶å®šä¸ºæœªæ¥å¥–åŠ±çš„æœ€å°å€¼ã€‚é€šè¿‡å¹¿æ³›å®éªŒè¯æ˜ï¼Œåœ¨åŸºäºPRMçš„æ–¹æ³•ä¸­å®ç°æœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…å¯ä»¥åœ¨ä»…30%çš„æ­¥éª¤å†…è¾¾åˆ°å¯éªŒè¯å¥–åŠ±æ–¹æ³•ç›¸å½“çš„æ¨ç†æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§„èŒƒçš„æ±‚å’Œå½¢å¼çš„ä¿¡ç”¨åˆ†é…ç”šè‡³åœ¨è®­ç»ƒå¼€å§‹æ—¶å°±ä¼šå´©æºƒã€‚æ­¤å¤–ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨ä»…10%çš„å¯éªŒè¯å¥–åŠ±è¡¥å……åŸºäºPRMçš„å¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å‡è½»äº†å¥–åŠ±é»‘å®¢æ”»å‡»é—®é¢˜ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„å®éªŒä¸­åŸºäºQwen2.5-Math-7Bäº§ç”Ÿäº†æœ€ä½³çš„å¾®è°ƒæ¨¡å‹ï¼Œåœ¨AMC23ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°82.5%ï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä¸º53.3%ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>PRMåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œä½†åœ¨å¼ºåŒ–å¾®è°ƒä¸­é¢ä¸´å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚</li>
<li>PRMè¯±å¯¼å¥–åŠ±é»‘å®¢çš„ä¸»è¦åŸå› æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„è§„èŒƒæ±‚å’Œå½¢å¼çš„ä¿¡ç”¨åˆ†é…ã€‚</li>
<li>æå‡ºPUREæ–¹æ³•ï¼Œé€šè¿‡æœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…æ¥æ˜¾è‘—å‡è½»å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚</li>
<li>åœ¨åŸºäºPRMçš„æ–¹æ³•ä¸­å®ç°æœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…åœ¨ä»…30%çš„æ­¥éª¤å†…è¾¾åˆ°ä¸å¯éªŒè¯å¥–åŠ±æ–¹æ³•ç›¸å½“çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>åªç”¨10%çš„å¯éªŒè¯å¥–åŠ±è¡¥å……åŸºäºPRMçš„å¾®è°ƒå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å®éªŒä¸­ä½¿ç”¨Qwen2.5-Math-7Bæ¨¡å‹åœ¨AMC23ä¸Šè¾¾åˆ°82.5%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä¸º53.3%ã€‚</li>
<li>æ€»ç»“äº†è§‚å¯Ÿåˆ°çš„å¥–åŠ±é»‘å®¢æ¡ˆä¾‹ï¼Œå¹¶åˆ†æäº†è®­ç»ƒå´©æºƒçš„åŸå› ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2f136d345fb5af45f095e0fd2b0afad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1eb3d2c25a98f8c0492e973cc72b2ad8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0266eddd8ad0fc488116fa4e89873aff.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FlowReasoner-Reinforcing-Query-Level-Meta-Agents"><a href="#FlowReasoner-Reinforcing-Query-Level-Meta-Agents" class="headerlink" title="FlowReasoner: Reinforcing Query-Level Meta-Agents"></a>FlowReasoner: Reinforcing Query-Level Meta-Agents</h2><p><strong>Authors:Hongcheng Gao, Yue Liu, Yufei He, Longxu Dou, Chao Du, Zhijie Deng, Bryan Hooi, Min Lin, Tianyu Pang</strong></p>
<p>This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/FlowReasoner">https://github.com/sail-sg/FlowReasoner</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFlowReasonerçš„æŸ¥è¯¢çº§å…ƒä»£ç†ï¼Œç”¨äºè‡ªåŠ¨åŒ–è®¾è®¡æŸ¥è¯¢çº§å¤šä»£ç†ç³»ç»Ÿï¼Œå³é’ˆå¯¹æ¯ä¸ªç”¨æˆ·æŸ¥è¯¢è®¾è®¡ä¸€ä¸ªç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¤–éƒ¨æ‰§è¡Œåé¦ˆæ¥æ¿€åŠ±åŸºäºæ¨ç†çš„å…ƒä»£ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æç‚¼DeepSeek R1ï¼Œé¦–å…ˆèµ‹äºˆFlowReasonerå…³äºç”Ÿæˆå¤šä»£ç†ç³»ç»Ÿçš„åŸºæœ¬æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨å¤–éƒ¨æ‰§è¡Œåé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥å¢å¼ºäº†å®ƒçš„èƒ½åŠ›ã€‚è®¾è®¡äº†ä¸€ä¸ªå¤šç”¨é€”å¥–åŠ±æ¥ä»æ€§èƒ½ã€å¤æ‚æ€§å’Œæ•ˆç‡æ–¹é¢å¼•å¯¼RLè®­ç»ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒFlowReasonerèƒ½å¤Ÿé€šè¿‡å®¡æ…æ¨ç†ä¸ºæ¯ä¸ªç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆä¸ªæ€§åŒ–çš„å¤šä»£ç†ç³»ç»Ÿã€‚åœ¨å·¥ç¨‹å’Œç«èµ›ä»£ç åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†FlowReasonerçš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†o1-miniï¼Œå‡†ç¡®ç‡æé«˜äº†10.52%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sail-sg/FlowReasoner%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/sail-sg/FlowReasonerè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15257v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡æå‡ºä¸€ç§åä¸ºFlowReasonerçš„æŸ¥è¯¢çº§å…ƒä»£ç†ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–è®¾è®¡æŸ¥è¯¢çº§å¤šä»£ç†ç³»ç»Ÿï¼Œå³é’ˆå¯¹æ¯ä¸ªç”¨æˆ·æŸ¥è¯¢æ„å»ºä¸€ä¸ªç³»ç»Ÿã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¤–éƒ¨æ‰§è¡Œåé¦ˆæ¥æ¿€åŠ±åŸºäºæ¨ç†çš„å…ƒä»£ç†ã€‚é€šè¿‡æç‚¼DeepSeek R1ï¼Œä¸ºFlowReasonerèµ‹äºˆå…³äºå¤šä»£ç†ç³»ç»Ÿç”Ÿæˆçš„åŸºæœ¬æ¨ç†èƒ½åŠ›ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤–éƒ¨æ‰§è¡Œåé¦ˆè¿›ä¸€æ­¥å¢å¼ºå…¶åŠŸèƒ½ã€‚è®¾è®¡äº†ä¸€ä¸ªå¤šç”¨é€”å¥–åŠ±æ¥ä»æ€§èƒ½ã€å¤æ‚æ€§å’Œæ•ˆç‡æ–¹é¢æŒ‡å¯¼RLè®­ç»ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒFlowReasonerèƒ½å¤Ÿé€šè¿‡æ·±æ€ç†Ÿè™‘çš„æ¨ç†ä¸ºæ¯ä¸ªç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆä¸ªæ€§åŒ–çš„å¤šä»£ç†ç³»ç»Ÿã€‚å®éªŒè¡¨æ˜ï¼ŒFlowReasoneråœ¨å·¥ç¨‹å’Œç«èµ›ä»£ç åŸºå‡†æµ‹è¯•ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡†ç¡®ç‡æ¯”o1-minié«˜å‡º10.52%ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sail-sg/FlowReasoner%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sail-sg/FlowReasonerä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlowReasoneræ˜¯ä¸€ä¸ªæŸ¥è¯¢çº§å…ƒä»£ç†ï¼Œç”¨äºè‡ªåŠ¨åŒ–è®¾è®¡æŸ¥è¯¢çº§å¤šä»£ç†ç³»ç»Ÿã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡å¤–éƒ¨æ‰§è¡Œåé¦ˆæ¥æ¿€åŠ±åŸºäºæ¨ç†çš„å…ƒä»£ç†ã€‚</li>
<li>é€šè¿‡æç‚¼DeepSeek R1ï¼Œä¸ºFlowReasonerèµ‹äºˆåŸºæœ¬æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤–éƒ¨æ‰§è¡Œåé¦ˆç”¨äºå¢å¼ºFlowReasonerçš„åŠŸèƒ½ã€‚</li>
<li>è®¾è®¡äº†å¤šç”¨é€”å¥–åŠ±ä»¥æŒ‡å¯¼RLè®­ç»ƒï¼Œæ¶µç›–æ€§èƒ½ã€å¤æ‚æ€§å’Œæ•ˆç‡æ–¹é¢ã€‚</li>
<li>FlowReasonerå¯ä¸ºæ¯ä¸ªç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆä¸ªæ€§åŒ–çš„å¤šä»£ç†ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c43bc2b712d2f2f76a65c8e53b4c9283.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d39282ca023d88917336c94ee4dcdc44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c19b1efb806f2614556aaa93d497978.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93cb223284b6f0eae915449e14d0b82e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33cb3ed3256e3763d4943bca45a84278.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MR-Guard-Multilingual-Reasoning-Guardrail-using-Curriculum-Learning"><a href="#MR-Guard-Multilingual-Reasoning-Guardrail-using-Curriculum-Learning" class="headerlink" title="MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning"></a>MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning</h2><p><strong>Authors:Yahan Yang, Soham Dan, Shuo Li, Dan Roth, Insup Lee</strong></p>
<p>Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®¹æ˜“å—åˆ°å¦‚è¶Šç‹±ç­‰å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œå¯èƒ½ä¼šå¼•å‘æœ‰å®³æˆ–ä¸å®‰å…¨çš„è¡Œä¸ºã€‚åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ï¼Œç”±äºå¤šè¯­è¨€å®‰å…¨å¯¹é½æ•°æ®é€šå¸¸æœ‰é™ï¼Œè¿™ä¸€æ¼æ´ä¼šæ›´åŠ ä¸¥é‡ã€‚å› æ­¤ï¼Œå¼€å‘ä¸€ç§èƒ½å¤Ÿåœ¨å¤šç§è¯­è¨€ä¸­æ£€æµ‹å’Œè¿‡æ»¤ä¸å®‰å…¨å†…å®¹çš„æŠ¤æ ï¼Œå¯¹äºåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­éƒ¨ç½²LLMè‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ¨ç†çš„å¤šè¯­è¨€æŠ¤æ æ„å»ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åˆæˆå¤šè¯­è¨€æ•°æ®ç”Ÿæˆï¼Œèå…¥æ–‡åŒ–å’Œè¯­è¨€ç»†å¾®å˜åŒ–çš„å˜ä½“ï¼Œï¼ˆ2ï¼‰æœ‰ç›‘ç£å¾®è°ƒï¼Œä»¥åŠï¼ˆ3ï¼‰è¯¾ç¨‹å¼•å¯¼å¼ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¡†æ¶ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šè¯­è¨€æŠ¤æ åœ¨åŸŸå†…å’ŒåŸŸå¤–è¯­è¨€ä¸Šå‡æŒç»­ä¼˜äºæœ€è¿‘çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„æŠ¤æ çš„å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿç”Ÿæˆå¤šè¯­è¨€è§£é‡Šï¼Œè¿™å¯¹äºç†è§£å¤šè¯­è¨€å†…å®¹å®¡æ ¸ä¸­çš„è¯­è¨€ç‰¹å®šé£é™©å’Œæ¨¡ç³Šæ€§ç‰¹åˆ«æœ‰ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15241v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­ç§ç¯å¢ƒä¸‹æ˜“å—æ”»å‡»ï¼Œå­˜åœ¨å®‰å…¨éšæ‚£ã€‚å› æ­¤ï¼Œå¼€å‘ä¸€ç§å…·å¤‡æ£€æµ‹ä¸è¿‡æ»¤å¤šè¯­ç§ä¸å®‰å…¨å†…å®¹èƒ½åŠ›çš„é˜²æŠ¤æ è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ¨ç†çš„å¤šè¯­ç§é˜²æŠ¤æ æ„å»ºæ–¹æ³•ï¼ŒåŒ…æ‹¬åˆæˆå¤šè¯­ç§æ•°æ®ç”Ÿæˆã€ç›‘ç£å¾®è°ƒä»¥åŠè¯¾ç¨‹å¼•å¯¼çš„ç›¸å¯¹ç¾¤ä½“ä¼˜åŒ–æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é˜²æŠ¤æ åœ¨è·¨é¢†åŸŸè¯­è¨€å†…è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šè¯­ç§è§£é‡Šï¼Œæœ‰åŠ©äºç†è§£å¤šè¯­ç§å†…å®¹ç®¡ç†ä¸­çš„è¯­è¨€ç‰¹å®šé£é™©å’Œæ­§ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­ç§ç¯å¢ƒä¸‹å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œå®¹æ˜“å—åˆ°æ”»å‡»ã€‚</li>
<li>å¼€å‘å…·å¤‡æ£€æµ‹ä¸è¿‡æ»¤å¤šè¯­ç§ä¸å®‰å…¨å†…å®¹èƒ½åŠ›çš„é˜²æŠ¤æ æ˜¯å¿…è¦çš„ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ¨ç†çš„å¤šè¯­ç§é˜²æŠ¤æ æ„å»ºæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…æ‹¬åˆæˆå¤šè¯­ç§æ•°æ®ç”Ÿæˆã€ç›‘ç£å¾®è°ƒç­‰æŠ€æœ¯æ‰‹æ®µã€‚</li>
<li>è¯¾ç¨‹å¼•å¯¼çš„ç›¸å¯¹ç¾¤ä½“ä¼˜åŒ–æ¡†æ¶æœ‰åŠ©äºæé«˜é˜²æŠ¤æ çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥é˜²æŠ¤æ åœ¨è·¨é¢†åŸŸè¯­è¨€å†…è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-56ef2758021ba491a7c809fae596eb73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b8a0dc7d6c6a338f0c4ecdb11f16a2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-276b9e5d9c5321fd3232e97bb04f7155.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdf18cf6727a5ef0291e281ed754117b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="POLYRAG-Integrating-Polyviews-into-Retrieval-Augmented-Generation-for-Medical-Applications"><a href="#POLYRAG-Integrating-Polyviews-into-Retrieval-Augmented-Generation-for-Medical-Applications" class="headerlink" title="POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for   Medical Applications"></a>POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for   Medical Applications</h2><p><strong>Authors:Chunjing Gan, Dan Yang, Binbin Hu, Ziqi Liu, Yue Shen, Zhiqiang Zhang, Jian Wang, Jun Zhou</strong></p>
<p>Large language models (LLMs) have become a disruptive force in the industry, introducing unprecedented capabilities in natural language processing, logical reasoning and so on. However, the challenges of knowledge updates and hallucination issues have limited the application of LLMs in medical scenarios, where retrieval-augmented generation (RAG) can offer significant assistance. Nevertheless, existing retrieve-then-read approaches generally digest the retrieved documents, without considering the timeliness, authoritativeness and commonality of retrieval. We argue that these approaches can be suboptimal, especially in real-world applications where information from different sources might conflict with each other and even information from the same source in different time scale might be different, and totally relying on this would deteriorate the performance of RAG approaches. We propose PolyRAG that carefully incorporate judges from different perspectives and finally integrate the polyviews for retrieval augmented generation in medical applications. Due to the scarcity of real-world benchmarks for evaluation, to bridge the gap we propose PolyEVAL, a benchmark consists of queries and documents collected from real-world medical scenarios (including medical policy, hospital &amp; doctor inquiry and healthcare) with multiple tagging (e.g., timeliness, authoritativeness) on them. Extensive experiments and analysis on PolyEVAL have demonstrated the superiority of PolyRAG. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æˆä¸ºä¸šç•Œçš„ä¸€è‚¡é¢ è¦†æ€§åŠ›é‡ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€é€»è¾‘æ¨ç†ç­‰æ–¹é¢å¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒçŸ¥è¯†æ›´æ–°å’Œå¹»è§‰é—®é¢˜æ‰€å¸¦æ¥çš„æŒ‘æˆ˜é™åˆ¶äº†LLMåœ¨åŒ»ç–—åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œæ­¤æ—¶æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¯ä»¥æä¾›é‡å¤§å¸®åŠ©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…ˆæ£€ç´¢åé˜…è¯»æ–¹æ³•é€šå¸¸åªå…³æ³¨æ£€ç´¢æ–‡æ¡£çš„å†…å®¹ï¼Œè€Œæ²¡æœ‰è€ƒè™‘æ£€ç´¢æ–‡æ¡£çš„æ—¶æ•ˆæ€§ã€æƒå¨æ€§å’Œæ™®éæ€§ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›æ–¹æ³•å¯èƒ½æ˜¯æ¬¡ä¼˜çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨ä¸­ï¼Œæ¥è‡ªä¸åŒæ¥æºçš„ä¿¡æ¯å¯èƒ½ä¼šç›¸äº’å†²çªï¼Œç”šè‡³åŒä¸€æ¥æºåœ¨ä¸åŒæ—¶é—´å°ºåº¦çš„ä¿¡æ¯ä¹Ÿå¯èƒ½æœ‰æ‰€ä¸åŒï¼Œå®Œå…¨ä¾èµ–è¿™äº›æ–¹æ³•ä¼šæ¶åŒ–RAGçš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†PolyRAGï¼Œå®ƒä»”ç»†ç»“åˆäº†ä»ä¸åŒè§’åº¦è¿›è¡Œçš„åˆ¤æ–­ï¼Œå¹¶æœ€ç»ˆå°†è¿™äº›å¤šè§†è§’åˆ¤æ–­èå…¥åŒ»å­¦åº”ç”¨ä¸­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚ç”±äºç°å®ä¸–ç•Œç¼ºä¹åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æ€§èƒ½ï¼Œä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†PolyEVALåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…å«ä»ç°å®ä¸–ç•ŒåŒ»ç–—åœºæ™¯ï¼ˆåŒ…æ‹¬åŒ»ç–—æ”¿ç­–ã€åŒ»é™¢å’ŒåŒ»ç”Ÿå’¨è¯¢ä»¥åŠåŒ»ç–—ä¿å¥ï¼‰æ”¶é›†çš„æŸ¥è¯¢å’Œæ–‡æ¡£ï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œäº†å¤šé‡æ ‡ç­¾æ ‡æ³¨ï¼ˆä¾‹å¦‚æ—¶æ•ˆæ€§ã€æƒå¨æ€§ï¼‰ã€‚åœ¨PolyEVALä¸Šçš„å¹¿æ³›å®éªŒå’Œåˆ†æè¯æ˜äº†PolyRAGçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14917v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€é€»è¾‘æ¨ç†ç­‰æ–¹é¢è¡¨ç°å‡ºå‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦åœºæ™¯ä¸­åº”ç”¨æ—¶é¢ä¸´çŸ¥è¯†æ›´æ–°å’Œå¹»è§‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†PolyRAGæ–¹æ³•ï¼Œå®ƒä»ä¸åŒè§’åº¦ç»¼åˆè€ƒè™‘æ£€ç´¢ç»“æœï¼Œå¹¶ç”¨äºåŒ»å­¦åº”ç”¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚ä¸ºè¯„ä¼°å…¶æ€§èƒ½ï¼Œæå‡ºäº†PolyEVALåŸºå‡†æµ‹è¯•ï¼Œå®éªŒè¯æ˜PolyRAGçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»å­¦åœºæ™¯ä¸­å­˜åœ¨çŸ¥è¯†æ›´æ–°å’Œå¹»è§‰é—®é¢˜ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨åŒ»å­¦åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç°æœ‰æ£€ç´¢åé˜…è¯»æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œä¸è€ƒè™‘æ£€ç´¢ç»“æœçš„æ—¶æ•ˆæ€§ã€æƒå¨æ€§å’Œæ™®éæ€§ã€‚</li>
<li>PolyRAGæ–¹æ³•ä»ä¸åŒè§’åº¦ç»¼åˆè€ƒè™‘æ£€ç´¢ç»“æœï¼Œç”¨äºæ”¹è¿›åŒ»å­¦åº”ç”¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚</li>
<li>ç¼ºä¹ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMåœ¨åŒ»å­¦åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†PolyEVALåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¥è‡ªçœŸå®ä¸–ç•ŒåŒ»å­¦åœºæ™¯çš„æŸ¥è¯¢å’Œæ–‡æ¡£ï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œå¤šé‡æ ‡è®°ï¼ˆå¦‚æ—¶æ•ˆæ€§å’Œæƒå¨æ€§ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b21cb9b8082634899461f3f423aec84f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ddec27bdbd8d6e309e4d79469f2b3c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-666e96549299339c8f80075762cc138f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a709da0c7d1b0acd7bc3c40230e8e5b8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CRAVE-A-Conflicting-Reasoning-Approach-for-Explainable-Claim-Verification-Using-LLMs"><a href="#CRAVE-A-Conflicting-Reasoning-Approach-for-Explainable-Claim-Verification-Using-LLMs" class="headerlink" title="CRAVE: A Conflicting Reasoning Approach for Explainable Claim   Verification Using LLMs"></a>CRAVE: A Conflicting Reasoning Approach for Explainable Claim   Verification Using LLMs</h2><p><strong>Authors:Yingming Zheng, Xiaoliang Liu, Peng Wu, Li Pan</strong></p>
<p>The rapid spread of misinformation, driven by digital media and AI-generated content, has made automatic claim verification essential. Traditional methods, which depend on expert-annotated evidence, are labor-intensive and not scalable. Although recent automated systems have improved, they still struggle with complex claims that require nuanced reasoning. To address this, we propose CRAVE, a Conflicting Reasoning Approach for explainable claim VErification, that verify the complex claims based on the conflicting rationales reasoned by large language models (LLMs). Specifically, CRAVE introduces a three-module framework. Ambiguity Elimination enchanced Evidence Retrieval module performs ambiguity elimination and entity-based search to gather relevant evidence related to claim verification from external sources like Wikipedia. Conflicting Perspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to reason rationales with conflicting stances about claim verification from retrieved evidence across four dimensions, i.e., direct evidence, semantic relationships, linguistic patterns, and logical reasoning and make a preliminary judgment. Finally, Small Language Model (SLM) based Judge module is fine-tuned to make use of preliminary judgment from LLMs to assess the confidence of the conflicting rationales and make a final authenticity judgment. This methodology allows CRAVE to capture subtle inconsistencies in complex claims, improving both the accuracy and transparency of claim verification. Extensive experiments on two public claim verification datasets demonstrate that our CRAVE model achieves much better performance than state-of-the-art methods and exhibits a superior capacity for finding relevant evidence and explaining the model predictions. The code is provided at <a target="_blank" rel="noopener" href="https://github.com/8zym/CRAVE">https://github.com/8zym/CRAVE</a>. </p>
<blockquote>
<p>ç”±æ•°å­—åª’ä½“å’ŒAIç”Ÿæˆå†…å®¹é©±åŠ¨çš„è¯¯å¯¼ä¿¡æ¯çš„å¿«é€Ÿä¼ æ’­ï¼Œä½¿å¾—è‡ªåŠ¨ç´¢èµ”éªŒè¯å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºä¸“å®¶æ ‡æ³¨çš„è¯æ®ï¼ŒåŠ³åŠ¨å¼ºåº¦å¤§ä¸”æ— æ³•å¤§è§„æ¨¡æ‰©å±•ã€‚å°½ç®¡æœ€è¿‘çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿå·²ç»æ”¹è¿›äº†ï¼Œä½†å®ƒä»¬ä»ç„¶å¯¹éœ€è¦å¾®å¦™æ¨ç†çš„å¤æ‚ç´¢èµ”æ„Ÿåˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CRAVEï¼Œå³åŸºäºå¯è§£é‡Šä¸»å¼ éªŒè¯çš„å†²çªæ¨ç†æ–¹æ³•ã€‚CRAVEé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†²çªæ¨ç†æ¥éªŒè¯å¤æ‚çš„ä¸»å¼ ã€‚å…·ä½“æ¥è¯´ï¼ŒCRAVEå¼•å…¥äº†ä¸€ä¸ªä¸‰æ¨¡å—æ¡†æ¶ã€‚æ­§ä¹‰æ¶ˆé™¤å¢å¼ºè¯æ®æ£€ç´¢æ¨¡å—æ‰§è¡Œæ­§ä¹‰æ¶ˆé™¤å’ŒåŸºäºå®ä½“çš„æœç´¢ï¼Œä»å¤–éƒ¨æºï¼ˆå¦‚Wikipediaï¼‰æ”¶é›†ä¸ä¸»å¼ éªŒè¯ç›¸å…³çš„ç›¸å…³è¯æ®ã€‚å…·æœ‰LLMçš„å†²çªè§†è§’æ¨ç†å’Œåˆæ­¥åˆ¤æ–­æ¨¡å—é‡‡ç”¨LLMå¯¹æ£€ç´¢åˆ°çš„è¯æ®è¿›è¡Œå››ä¸ªç»´åº¦çš„å†²çªç«‹åœºæ¨ç†ï¼Œå³ç›´æ¥è¯æ®ã€è¯­ä¹‰å…³ç³»ã€è¯­è¨€æ¨¡å¼å’Œé€»è¾‘æ¨ç†ï¼Œå¹¶è¿›è¡Œåˆæ­¥åˆ¤æ–­ã€‚æœ€åï¼ŒåŸºäºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ³•å®˜æ¨¡å—ç»è¿‡å¾®è°ƒï¼Œä»¥åˆ©ç”¨LLMçš„åˆæ­¥åˆ¤æ–­æ¥è¯„ä¼°å†²çªç†ç”±çš„ç½®ä¿¡åº¦å¹¶åšå‡ºæœ€ç»ˆçš„çœŸå®æ€§åˆ¤æ–­ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—CRAVEèƒ½å¤Ÿæ•æ‰åˆ°å¤æ‚å£°æ˜ä¸­çš„å¾®å¦™çŸ›ç›¾ï¼Œæé«˜äº†ç´¢èµ”éªŒè¯çš„å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚åœ¨ä¸¤ä¸ªå…¬å…±ç´¢èµ”éªŒè¯æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„CRAVEæ¨¡å‹æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¯»æ‰¾ç›¸å…³è¯æ®å’Œè§£é‡Šæ¨¡å‹é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/8zym/CRAVE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/8zym/CRAVEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14905v1">PDF</a> </p>
<p><strong>Summary</strong><br>CRAVEæ¨¡å‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³æ•°å­—åª’ä½“å’ŒAIç”Ÿæˆå†…å®¹å¸¦æ¥çš„é”™è¯¯ä¿¡æ¯å¿«é€Ÿä¼ æ’­é—®é¢˜ã€‚CRAVEé€šè¿‡æ¶ˆé™¤æ­§ä¹‰ã€è¯æ®æ£€ç´¢å’Œå†²çªè§†è§’æ¨ç†ç­‰æ¨¡å—ï¼Œæé«˜äº†å¤æ‚å£°æ˜çš„éªŒè¯å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚å®éªŒè¯æ˜ï¼ŒCRAVEæ¨¡å‹æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½æ›´å¥½åœ°æ‰¾åˆ°ç›¸å…³è¯æ®å’Œè§£é‡Šæ¨¡å‹é¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—åª’ä½“å’ŒAIç”Ÿæˆå†…å®¹åŠ é€Ÿäº†é”™è¯¯ä¿¡æ¯çš„ä¼ æ’­ï¼Œè‡ªåŠ¨å£°æ˜éªŒè¯å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–ä¸“å®¶æ ‡æ³¨çš„è¯æ®ï¼Œæ—¢è€—è´¹äººåŠ›åˆéš¾ä»¥è§„æ¨¡åŒ–ã€‚</li>
<li>CRAVEæ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªä¸‰æ¨¡å—æ¡†æ¶ï¼Œç”¨äºéªŒè¯å¤æ‚å£°æ˜ã€‚</li>
<li>æ­§ä¹‰æ¶ˆé™¤å’Œè¯æ®æ£€ç´¢æ¨¡å—ä»å¤–éƒ¨æ¥æºå¦‚Wikipediaæ£€ç´¢ä¸å£°æ˜éªŒè¯ç›¸å…³çš„è¯æ®ã€‚</li>
<li>LLMsç”¨äºæ¨ç†æ¥è‡ªæ£€ç´¢åˆ°çš„è¯æ®çš„å…·æœ‰å†²çªç«‹åœºçš„ç†ç”±ï¼Œå¹¶åšå‡ºåˆæ­¥åˆ¤æ–­ã€‚</li>
<li>åŸºäºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ³•å®˜æ¨¡å—åˆ©ç”¨LLMsçš„åˆæ­¥åˆ¤æ–­æ¥è¯„ä¼°å†²çªç†ç”±çš„ç½®ä¿¡åº¦å¹¶åšå‡ºæœ€ç»ˆçš„çœŸå®åˆ¤æ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-930d6cf9aff0d753145eb22709fdaf97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c89c5e5a7df02f77c77c8664e60f492b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a9cb596500d03f4604a78a2aad57d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba4f60800f06e43c0830e15c013f611e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Retrieval-Augmented-Generation-Evaluation-in-the-Era-of-Large-Language-Models-A-Comprehensive-Survey"><a href="#Retrieval-Augmented-Generation-Evaluation-in-the-Era-of-Large-Language-Models-A-Comprehensive-Survey" class="headerlink" title="Retrieval Augmented Generation Evaluation in the Era of Large Language   Models: A Comprehensive Survey"></a>Retrieval Augmented Generation Evaluation in the Era of Large Language   Models: A Comprehensive Survey</h2><p><strong>Authors:Aoran Gan, Hao Yu, Kai Zhang, Qi Liu, Wenyu Yan, Zhenya Huang, Shiwei Tong, Guoping Hu</strong></p>
<p>Recent advancements in Retrieval-Augmented Generation (RAG) have revolutionized natural language processing by integrating Large Language Models (LLMs) with external information retrieval, enabling accurate, up-to-date, and verifiable text generation across diverse applications. However, evaluating RAG systems presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically reviewing traditional and emerging evaluation approaches, for system performance, factual accuracy, safety, and computational efficiency in the LLM era. We also compile and categorize the RAG-specific datasets and evaluation frameworks, conducting a meta-analysis of evaluation practices in high-impact RAG research. To the best of our knowledge, this work represents the most comprehensive survey for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG development. </p>
<blockquote>
<p>æœ€è¿‘Retrieval-Augmented Generationï¼ˆRAGï¼‰çš„è¿›å±•é€šè¿‡æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨ä¿¡æ¯æ£€ç´¢ï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå®ç°äº†è·¨å¤šç§åº”ç”¨çš„å‡†ç¡®ã€æœ€æ–°å’Œå¯éªŒè¯çš„æ–‡æœ¬ç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºRAGç³»ç»Ÿçš„æ··åˆæ¶æ„ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆç»„ä»¶ï¼Œä»¥åŠå®ƒä»¬å¯¹LLMæ—¶ä»£åŠ¨æ€çŸ¥è¯†æºçš„ä¾èµ–ï¼Œå¯¹å…¶è¯„ä¼°é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å›åº”è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡å…¨é¢å›é¡¾äº†RAGè¯„ä¼°æ–¹æ³•å’Œæ¡†æ¶ï¼Œç³»ç»Ÿå›é¡¾äº†ä¼ ç»Ÿå’Œæ–°å…´çš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬LLMæ—¶ä»£ç³»ç»Ÿæ€§èƒ½ã€äº‹å®å‡†ç¡®æ€§ã€å®‰å…¨æ€§å’Œè®¡ç®—æ•ˆç‡çš„è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å¯¹RAGç‰¹å®šæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶è¿›è¡Œäº†æ•´ç†å’Œåˆ†ç±»ï¼Œå¯¹å½±å“åŠ›é«˜çš„RAGç ”ç©¶ä¸­çš„è¯„ä¼°å®è·µè¿›è¡Œäº†å…ƒåˆ†æã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œä»£è¡¨äº†RAGè¯„ä¼°çš„æœ€å…¨é¢è°ƒæŸ¥ï¼Œèåˆäº†ä¼ ç»Ÿå’ŒLLMé©±åŠ¨çš„æ–¹æ³•ï¼Œæ˜¯æ¨åŠ¨RAGå‘å±•çš„å…³é”®èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14891v1">PDF</a> 18 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤–éƒ¨ä¿¡æ¯æ£€ç´¢æŠ€æœ¯çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„æœ€æ–°è¿›å±•ã€‚æ–‡ç« è¯¦ç»†æ¢è®¨äº†RAGçš„è¯„ä»·æ–¹æ³•ï¼ŒåŒ…æ‹¬ç³»ç»Ÿæ€§èƒ½ã€äº‹å®å‡†ç¡®æ€§ã€å®‰å…¨æ€§å’Œè®¡ç®—æ•ˆç‡ç­‰æ–¹é¢ï¼Œå¹¶å¯¹é«˜å½±å“åŠ›çš„RAGç ”ç©¶è¿›è¡Œäº†å…ƒåˆ†æã€‚æœ¬æ–‡ä»£è¡¨äº†è¿„ä»Šä¸ºæ­¢å¯¹RAGè¯„ä»·æœ€å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶èµ·äº†ä¼ ç»Ÿæ–¹æ³•å’ŒLLMé©±åŠ¨æ–¹æ³•çš„æ¡¥æ¢ï¼Œä¸ºæ¨è¿›RAGçš„å‘å±•æä¾›äº†é‡è¦èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤–éƒ¨ä¿¡æ¯æ£€ç´¢æŠ€æœ¯ï¼Œå®ç°äº†å‡†ç¡®ã€å®æ—¶å’Œå¯éªŒè¯çš„æ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>RAGè¯„ä»·é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå› å…¶æ··åˆæ¶æ„å’Œä¾èµ–åŠ¨æ€çŸ¥è¯†æºã€‚</li>
<li>æ–‡ç« å…¨é¢ç»¼è¿°äº†RAGçš„è¯„ä»·æ–¹æ³•ï¼ŒåŒ…æ‹¬ç³»ç»Ÿæ€§èƒ½ã€äº‹å®å‡†ç¡®æ€§ã€å®‰å…¨æ€§å’Œè®¡ç®—æ•ˆç‡ç­‰æ–¹é¢ã€‚</li>
<li>ä»‹ç»äº†RAGç‰¹å®šæ•°æ®é›†å’Œè¯„ä»·æ¡†æ¶çš„æ•´ç†å’Œåˆ†ç±»ã€‚</li>
<li>é«˜å½±å“åŠ›çš„RAGç ”ç©¶è¯„ä»·å®è·µè¿›è¡Œäº†å…ƒåˆ†æã€‚</li>
<li>æœ¬æ–‡æ˜¯ç›®å‰å¯¹RAGè¯„ä»·æœ€å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä¼ ç»Ÿå’ŒLLMé©±åŠ¨çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-394ba970a757108242dda2dc58915549.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a25796fe789e79c79ab8c5bef0c7650.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcd33e265f615a503a3282f268d583f1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OTC-Optimal-Tool-Calls-via-Reinforcement-Learning"><a href="#OTC-Optimal-Tool-Calls-via-Reinforcement-Learning" class="headerlink" title="OTC: Optimal Tool Calls via Reinforcement Learning"></a>OTC: Optimal Tool Calls via Reinforcement Learning</h2><p><strong>Authors:Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, Heng Ji</strong></p>
<p>Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools, such as search engines and code interpreters, to solve tasks beyond the capabilities of language-only reasoning. While reinforcement learning (RL) has shown promise in improving TIR by optimizing final answer correctness, existing approaches often overlook the efficiency and cost associated with tool usage. This can lead to suboptimal behavior, including excessive tool calls that increase computational and financial overhead, or insufficient tool use that compromises answer quality. In this work, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers correctness and tool efficiency, promoting high tool productivity. We instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 73.1% and improves tool productivity by up to 229.4%, while maintaining comparable answer accuracy. To the best of our knowledge, this is the first RL-based framework that explicitly optimizes tool-use efficiency in TIR. </p>
<blockquote>
<p>å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰é€šè¿‡è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œå¦‚æœç´¢å¼•æ“å’Œä»£ç è§£é‡Šå™¨ï¼Œå¢å¼ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³è¶…å‡ºä»…è¯­è¨€æ¨ç†èƒ½åŠ›ä»»åŠ¡çš„èƒ½åŠ›ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡ä¼˜åŒ–æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§åœ¨æ”¹è¿›TIRæ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†ä¸å·¥å…·ä½¿ç”¨ç›¸å…³çš„æ•ˆç‡å’Œæˆæœ¬ã€‚è¿™å¯èƒ½å¯¼è‡´æ¬¡ä¼˜è¡Œä¸ºï¼ŒåŒ…æ‹¬è¿‡å¤šçš„å·¥å…·è°ƒç”¨ä¼šå¢åŠ è®¡ç®—å’Œè´¢åŠ¡å¼€é”€ï¼Œæˆ–å·¥å…·ä½¿ç”¨ä¸è¶³ä¼šæŸå®³ç­”æ¡ˆè´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æœ€ä¼˜å·¥å…·è°ƒç”¨æ§åˆ¶ç­–ç•¥ä¼˜åŒ–ï¼ˆOTC-POï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„åŸºäºRLçš„æ¡†æ¶ï¼Œé¼“åŠ±æ¨¡å‹ä»¥æœ€å°‘çš„å·¥å…·è°ƒç”¨äº§ç”Ÿå‡†ç¡®çš„ç­”æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªè”åˆè€ƒè™‘æ­£ç¡®æ€§å’Œå·¥å…·æ•ˆç‡çš„å·¥å…·é›†æˆå¥–åŠ±ï¼Œä»¥ä¿ƒè¿›é«˜å·¥å…·ç”Ÿäº§ç‡ã€‚æˆ‘ä»¬åœ¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç»„ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸­å®ä¾‹åŒ–äº†è¿™ä¸€æ¡†æ¶ï¼Œå½¢æˆäº†OTC-PPOå’ŒOTC-GRPOã€‚åœ¨Qwen-2.5å’ŒQwen-Mathä¸Šçš„å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å·¥å…·è°ƒç”¨å‡å°‘äº†é«˜è¾¾73.1%ï¼Œå·¥å…·ç”Ÿäº§ç‡æé«˜äº†é«˜è¾¾229.4%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„ç­”æ¡ˆå‡†ç¡®æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ˜¾å¼ä¼˜åŒ–TIRä¸­å·¥å…·ä½¿ç”¨æ•ˆç‡çš„åŸºäºRLçš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14870v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰ä¸­çš„ä¼˜åŒ–å·¥å…·è°ƒç”¨ç­–ç•¥ã€‚é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæå‡ºä¸€ç§åä¸ºOTC-POçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯é¼“åŠ±æ¨¡å‹åœ¨å°½å¯èƒ½å°‘çš„å·¥å…·è°ƒç”¨æ¬¡æ•°ä¸‹äº§ç”Ÿå‡†ç¡®çš„ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­å‡å°‘äº†å·¥å…·è°ƒç”¨æ¬¡æ•°å¹¶æé«˜äº†å·¥å…·æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰èƒ½å¤Ÿå€ŸåŠ©å¤–éƒ¨å·¥å…·ï¼Œå¦‚æœç´¢å¼•æ“å’Œä»£ç è§£é‡Šå™¨ï¼Œè§£å†³è¯­è¨€æ¨¡å‹æ— æ³•å•ç‹¬å®Œæˆçš„ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†å·¥å…·ä½¿ç”¨çš„æ•ˆç‡å’Œæˆæœ¬ï¼Œå¯¼è‡´æ¬¡ä¼˜è¡Œä¸ºï¼Œå¦‚è¿‡å¤šçš„å·¥å…·è°ƒç”¨æˆ–å·¥å…·ä½¿ç”¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åä¸ºOTC-POçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é¼“åŠ±æ¨¡å‹åœ¨å°½é‡å°‘åœ°ä½¿ç”¨å·¥å…·çš„æƒ…å†µä¸‹äº§ç”Ÿå‡†ç¡®çš„ç­”æ¡ˆæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è€ƒè™‘æ­£ç¡®æ€§å’Œå·¥å…·æ•ˆç‡æ¥è”åˆå¥–åŠ±å·¥å…·é›†æˆï¼Œä»è€Œæé«˜å·¥å…·ç”Ÿäº§ç‡ã€‚</li>
<li>å°†è¯¥æ¡†æ¶åº”ç”¨äºProximal Policy Optimizationï¼ˆPPOï¼‰å’ŒGroup Relative Preference Optimizationï¼ˆGRPOï¼‰ï¼Œå½¢æˆOTC-PPOå’ŒOTC-GRPOæ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å‡å°‘äº†å·¥å…·è°ƒç”¨æ¬¡æ•°å¹¶æé«˜äº†å·¥å…·æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0b15e8320c7521450229c731b6d1cf9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6ac92d7939dc4600ab7de778cfedb28.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AlignRAG-An-Adaptable-Framework-for-Resolving-Misalignments-in-Retrieval-Aware-Reasoning-of-RAG"><a href="#AlignRAG-An-Adaptable-Framework-for-Resolving-Misalignments-in-Retrieval-Aware-Reasoning-of-RAG" class="headerlink" title="AlignRAG: An Adaptable Framework for Resolving Misalignments in   Retrieval-Aware Reasoning of RAG"></a>AlignRAG: An Adaptable Framework for Resolving Misalignments in   Retrieval-Aware Reasoning of RAG</h2><p><strong>Authors:Jiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie Qiu, Wei Wei, Jinzhe Li, Wanli Ouyang, Siqi Sun</strong></p>
<p>Retrieval-augmented generation (RAG) has emerged as a foundational paradigm for knowledge-grounded text generation. However, existing RAG pipelines often fail to ensure that the reasoning trajectories align with the evidential constraints imposed by retrieved content. In this paper, we reframe RAG as a problem of retrieval-aware reasoning and identify a core challenge: reasoning misalignment-the mismatch between a modelâ€™s reasoning trajectory and the retrieved evidence. To address this challenge, we propose AlignRAG, a novel test-time framework that mitigates reasoning misalignment through iterative Critique-Driven Alignment (CDA) steps. In contrast to prior approaches that rely on static training or post-hoc selection, AlignRAG actively refines reasoning trajectories during inference by enforcing fine-grained alignment with evidence. Our framework introduces a new paradigm for retrieval-aware reasoning by: (1) constructing context-rich training corpora; (2) generating contrastive critiques from preference-aware reasoning trajectories; (3) training a dedicated \textit{Critic Language Model (CLM)} to identify reasoning misalignments; and (4) applying CDA steps to optimize reasoning trajectories iteratively. Empirical results demonstrate that AlignRAG consistently outperforms all baselines and could integrate as a plug-and-play module into existing RAG pipelines without further changes. By reconceptualizing RAG as a structured reasoning trajectory and establishing the test-time framework for correcting reasoning misalignments in RAG, AlignRAG provides practical advancements for retrieval-aware generation. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºçŸ¥è¯†åŸºç¡€æ–‡æœ¬ç”Ÿæˆçš„åŸºç¡€èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RAGæµç¨‹å¾€å¾€æ— æ³•ä¿è¯æ¨ç†è½¨è¿¹ä¸æ£€ç´¢å†…å®¹æ‰€æ–½åŠ çš„è¯æ®çº¦æŸä¸€è‡´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†RAGé‡æ–°å®šä¹‰ä¸ºæ£€ç´¢æ„ŸçŸ¥æ¨ç†çš„é—®é¢˜ï¼Œå¹¶è¯†åˆ«å‡ºæ ¸å¿ƒæŒ‘æˆ˜ï¼šæ¨ç†é”™ä½â€”â€”æ¨¡å‹æ¨ç†è½¨è¿¹ä¸æ£€ç´¢åˆ°çš„è¯æ®ä¹‹é—´çš„ä¸åŒ¹é…ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AlignRAGï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æµ‹è¯•æ—¶é—´æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£æ‰¹åˆ¤é©±åŠ¨å¯¹é½ï¼ˆCDAï¼‰æ­¥éª¤æ¥ç¼“è§£æ¨ç†é”™ä½é—®é¢˜ã€‚ä¸ä¾èµ–é™æ€è®­ç»ƒæˆ–äº‹åé€‰æ‹©çš„å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒAlignRAGåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç§¯æä¼˜åŒ–æ¨ç†è½¨è¿¹ï¼Œé€šè¿‡å¼ºåˆ¶ä¸è¯æ®è¿›è¡Œç²¾ç»†å¯¹é½ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸ºæ£€ç´¢æ„ŸçŸ¥æ¨ç†å¼•å…¥äº†ä¸€ç§æ–°èŒƒå¼ï¼šï¼ˆ1ï¼‰æ„å»ºä¸°å¯Œçš„ä¸Šä¸‹æ–‡è®­ç»ƒè¯­æ–™åº“ï¼›ï¼ˆ2ï¼‰ä»åå¥½æ„ŸçŸ¥æ¨ç†è½¨è¿¹ç”Ÿæˆå¯¹æ¯”æ€§æ‰¹åˆ¤ï¼›ï¼ˆ3ï¼‰è®­ç»ƒä¸“é—¨çš„è¯„è®ºå®¶è¯­è¨€æ¨¡å‹ï¼ˆCLMï¼‰æ¥è¯†åˆ«æ¨ç†é”™ä½ï¼›ï¼ˆ4ï¼‰åº”ç”¨CDAæ­¥éª¤è¿­ä»£ä¼˜åŒ–æ¨ç†è½¨è¿¹ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒAlignRAGå§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œå¹¶ä¸”å¯ä»¥ä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—é›†æˆåˆ°ç°æœ‰çš„RAGæµç¨‹ä¸­ï¼Œæ— éœ€è¿›ä¸€æ­¥æ›´æ”¹ã€‚é€šè¿‡é‡æ–°æ„å»ºRAGä½œä¸ºç»“æ„åŒ–æ¨ç†è½¨è¿¹ï¼Œå¹¶å»ºç«‹æµ‹è¯•æ—¶é—´æ¡†æ¶æ¥çº æ­£RAGä¸­çš„æ¨ç†é”™ä½ï¼ŒAlignRAGä¸ºæ£€ç´¢æ„ŸçŸ¥ç”Ÿæˆæä¾›äº†å®é™…è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14858v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªæ–‡æœ¬ä¸­ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºæ£€ç´¢çš„æ–‡æœ¬ç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹çš„é—®é¢˜ï¼Œå³æ¨ç†è½¨è¿¹ä¸æ£€ç´¢å†…å®¹çš„è¯æ®çº¦æŸä¸åŒ¹é…çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAlignRAGçš„æ–°æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¿­ä»£æ‰¹åˆ¤é©±åŠ¨å¯¹é½ï¼ˆCDAï¼‰æ­¥éª¤æ¥å‡è½»æ¨ç†å¯¹é½é—®é¢˜ã€‚è¯¥æ¡†æ¶å…·æœ‰å¤šç§ç‹¬ç‰¹çš„ç‰¹ç‚¹å’ŒåŠŸèƒ½ï¼Œå¦‚æ„å»ºä¸°å¯Œçš„ä¸Šä¸‹æ–‡è®­ç»ƒè¯­æ–™åº“ã€ç”ŸæˆåŸºäºåå¥½æ„ŸçŸ¥æ¨ç†è½¨è¿¹çš„å¯¹æ¯”æ‰¹è¯„ã€è®­ç»ƒä¸“é—¨ç”¨äºè¯†åˆ«æ¨ç†è¯¯å¯¹é½çš„æ‰¹è¯„è¯­è¨€æ¨¡å‹ç­‰ã€‚å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒAlignRAGçš„æ€§èƒ½å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œå¹¶å¯ä»¥ä½œä¸ºç°æœ‰RAGç®¡é“çš„å³æ’å³ç”¨æ¨¡å—è¿›è¡Œé›†æˆï¼Œæ— éœ€è¿›ä¸€æ­¥æ›´æ”¹ã€‚è¿™ä¸ºæ£€ç´¢æ„ŸçŸ¥ç”Ÿæˆæä¾›äº†å®é™…åº”ç”¨ä¸­çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGæ¨¡å‹çš„æ¨ç†è½¨è¿¹ä¸æ£€ç´¢å†…å®¹çš„è¯æ®çº¦æŸå¸¸å­˜åœ¨ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>æå‡ºäº†AlignRAGæ¡†æ¶æ¥è§£å†³RAGä¸­çš„æ¨ç†å¯¹é½é—®é¢˜ï¼Œé€šè¿‡è¿­ä»£CDAæ­¥éª¤è¿›è¡Œç²¾ç»†å¯¹é½ã€‚</li>
<li>AlignRAGæ¡†æ¶å…·æœ‰æ„å»ºä¸°å¯Œçš„ä¸Šä¸‹æ–‡è®­ç»ƒè¯­æ–™åº“ã€ç”Ÿæˆå¯¹æ¯”æ‰¹è¯„ã€è®­ç»ƒæ‰¹è¯„è¯­è¨€æ¨¡å‹ç­‰åŠŸèƒ½ã€‚</li>
<li>AlignRAGæ¡†æ¶æ€§èƒ½ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰RAGç®¡é“ä¸­ã€‚</li>
<li>AlignRAGæ¡†æ¶ä¸ºæ£€ç´¢æ„ŸçŸ¥ç”Ÿæˆæä¾›äº†å®é™…åº”ç”¨ä¸­çš„è¿›æ­¥ã€‚</li>
<li>è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯é‡æ„RAGä½œä¸ºç»“æ„åŒ–æ¨ç†è½¨è¿¹çš„æ¦‚å¿µï¼Œå¹¶å»ºç«‹äº†æµ‹è¯•æ—¶çº æ­£RAGä¸­æ¨ç†è¯¯å¯¹é½çš„æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5505d669cd99c40c49622eb1bb8be92d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac671cab32d54e7e46815513f147fa86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a72216ffc4a1503941754f446bf18d7c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reliable-Multi-Modal-Object-Re-Identification-via-Modality-Aware-Graph-Reasoning"><a href="#Reliable-Multi-Modal-Object-Re-Identification-via-Modality-Aware-Graph-Reasoning" class="headerlink" title="Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph   Reasoning"></a>Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph   Reasoning</h2><p><strong>Authors:Xixi Wan, Aihua Zheng, Zi Wang, Bo Jiang, Jin Tang, Jixin Ma</strong></p>
<p>Multi-modal data provides abundant and diverse object information, crucial for effective modal interactions in Re-Identification (ReID) tasks. However, existing approaches often overlook the quality variations in local features and fail to fully leverage the complementary information across modalities, particularly in the case of low-quality features. In this paper, we propose to address this issue by leveraging a novel graph reasoning model, termed the Modality-aware Graph Reasoning Network (MGRNet). Specifically, we first construct modality-aware graphs to enhance the extraction of fine-grained local details by effectively capturing and modeling the relationships between patches. Subsequently, the selective graph nodes swap operation is employed to alleviate the adverse effects of low-quality local features by considering both local and global information, enhancing the representation of discriminative information. Finally, the swapped modality-aware graphs are fed into the local-aware graph reasoning module, which propagates multi-modal information to yield a reliable feature representation. Another advantage of the proposed graph reasoning approach is its ability to reconstruct missing modal information by exploiting inherent structural relationships, thereby minimizing disparities between different modalities. Experimental results on four benchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the proposed method achieves state-of-the-art performance in multi-modal object ReID. The code for our method will be available upon acceptance. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ•°æ®æä¾›äº†ä¸°å¯Œå¤šæ ·çš„ç›®æ ‡ä¿¡æ¯ï¼Œå¯¹äºé‡æ–°è¯†åˆ«ï¼ˆReIDï¼‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ¨¡æ€äº¤äº’è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†å±€éƒ¨ç‰¹å¾çš„è´¨é‡å˜åŒ–ï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†åˆ©ç”¨è·¨æ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å¾è´¨é‡è¾ƒä½çš„æƒ…å†µä¸‹ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåˆ©ç”¨ä¸€ç§æ–°å‹çš„å›¾æ¨ç†æ¨¡å‹ï¼Œç§°ä¸ºæ¨¡æ€æ„ŸçŸ¥å›¾æ¨ç†ç½‘ç»œï¼ˆMGRNetï¼‰ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºæ¨¡æ€æ„ŸçŸ¥å›¾ï¼Œé€šè¿‡æœ‰æ•ˆåœ°æ•è·å’Œå»ºæ¨¡æ–‘å—ä¹‹é—´çš„å…³ç³»ï¼Œå¢å¼ºç²¾ç»†å±€éƒ¨ç»†èŠ‚çš„æå–ã€‚éšåï¼Œé‡‡ç”¨é€‰æ‹©æ€§å›¾èŠ‚ç‚¹äº¤æ¢æ“ä½œï¼Œé€šè¿‡è€ƒè™‘å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯æ¥ç¼“è§£ä½è´¨é‡å±€éƒ¨ç‰¹å¾çš„ä¸åˆ©å½±å“ï¼Œå¢å¼ºåˆ¤åˆ«ä¿¡æ¯çš„è¡¨ç¤ºã€‚æœ€åï¼Œå°†äº¤æ¢åçš„æ¨¡æ€æ„ŸçŸ¥å›¾è¾“å…¥åˆ°å±€éƒ¨æ„ŸçŸ¥å›¾æ¨ç†æ¨¡å—ä¸­ï¼Œè¯¥æ¨¡å—ä¼ æ’­å¤šæ¨¡æ€ä¿¡æ¯ä»¥äº§ç”Ÿå¯é çš„ç‰¹å¾è¡¨ç¤ºã€‚æ‰€æå‡ºå›¾æ¨ç†æ–¹æ³•çš„å¦ä¸€ä¸ªä¼˜ç‚¹æ˜¯ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡åˆ©ç”¨å†…åœ¨çš„ç»“æ„å…³ç³»æ¥é‡å»ºç¼ºå¤±çš„æ¨¡æ€ä¿¡æ¯ï¼Œä»è€Œæœ€å°åŒ–ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆRGBNT201ã€Market1501-MMã€RGBNT100ã€MSVR310ï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šæ¨¡æ€ç›®æ ‡ReIDä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•çš„ç›¸å…³ä»£ç å°†åœ¨æ¥å—åæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14847v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¯¥æ–‡æå‡ºä¸€ç§å¤šæ¨¡æ€æ„ŸçŸ¥å›¾æ¨ç†ç½‘ç»œï¼ˆMGRNetï¼‰ï¼Œç”¨äºè§£å†³å¤šæ¨¡æ€æ•°æ®åœ¨å†è¯†åˆ«ä»»åŠ¡ä¸­çš„è´¨é‡é—®é¢˜ã€‚é€šè¿‡æ„å»ºæ¨¡æ€æ„ŸçŸ¥å›¾æ¥å¢å¼ºç»†ç²’åº¦å±€éƒ¨ç‰¹å¾çš„æå–ï¼Œå¹¶é‡‡ç”¨é€‰æ‹©æ€§å›¾èŠ‚ç‚¹äº¤æ¢æ“ä½œæ¥ç¼“è§£ä½è´¨é‡å±€éƒ¨ç‰¹å¾çš„ä¸åˆ©å½±å“ã€‚åŒæ—¶ï¼Œåˆ©ç”¨å±€éƒ¨æ„ŸçŸ¥å›¾æ¨ç†æ¨¡å—ä¼ æ’­å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå®ç°å¯é çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½é‡å»ºç¼ºå¤±çš„æ¨¡æ€ä¿¡æ¯ï¼Œæœ€å°åŒ–ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€å¯¹è±¡å†è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€æ•°æ®åœ¨å†è¯†åˆ«ä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•å¿½ç•¥äº†å±€éƒ¨ç‰¹å¾çš„è´¨é‡å·®å¼‚ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è·¨æ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥æ¨¡æ€æ„ŸçŸ¥å›¾æ¥å¢å¼ºç»†ç²’åº¦å±€éƒ¨ç‰¹å¾çš„æå–ï¼Œæœ‰æ•ˆæ•æ‰å’Œå»ºæ¨¡æ–‘å—ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>é‡‡ç”¨é€‰æ‹©æ€§å›¾èŠ‚ç‚¹äº¤æ¢æ“ä½œï¼Œç»“åˆå±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œç¼“è§£ä½è´¨é‡å±€éƒ¨ç‰¹å¾çš„ä¸åˆ©å½±å“ï¼Œå¢å¼ºåˆ¤åˆ«ä¿¡æ¯çš„è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡å±€éƒ¨æ„ŸçŸ¥å›¾æ¨ç†æ¨¡å—ä¼ æ’­å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå®ç°å¯é çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿé‡å»ºç¼ºå¤±çš„æ¨¡æ€ä¿¡æ¯ï¼Œæœ€å°åŒ–ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€å¯¹è±¡å†è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdc2c7bcf93f3ff91ab1d77185fcc655.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51642c81b87b526b4eb3a2c0064592fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e4ebd7a99e2fdcc73a48339c788660a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77b6854bb66f84b8fb3b0c1869acf749.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Relation-R1-Cognitive-Chain-of-Thought-Guided-Reinforcement-Learning-for-Unified-Relational-Comprehension"><a href="#Relation-R1-Cognitive-Chain-of-Thought-Guided-Reinforcement-Learning-for-Unified-Relational-Comprehension" class="headerlink" title="Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning   for Unified Relational Comprehension"></a>Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning   for Unified Relational Comprehension</h2><p><strong>Authors:Lin Li, Wei Chen, Jiahui Li, Long Chen</strong></p>
<p>Recent advances in multi-modal large language models (MLLMs) have significantly improved object-level grounding and region captioning, but remain limited in visual relation understanding (\eg, scene graph generation), particularly in modeling \textit{N}-ary relationships that identify multiple semantic roles among an action event. Such a lack of \textit{semantic dependencies} modeling among multi-entities leads to unreliable outputs, intensifying MLLMsâ€™ hallucinations and over-reliance on language priors. To this end, we propose Relation-R1, the first unified relational comprehension framework that explicitly integrates cognitive chain-of-thought (CoT)-guided Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) within a reinforcement learning (RL) paradigm. Specifically, we first establish foundational reasoning capabilities via SFT, enforcing structured outputs with thinking processes. Then, GRPO is utilized to refine these outputs via multi-reward optimization, prioritizing visual-semantic grounding over language-induced biases, thereby improving generalization capability. Extensive experiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1 achieves state-of-the-art performance in both binary and \textit{N}-ary relation understanding. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥åœ¨å¯¹è±¡çº§åˆ«çš„å®šä½ï¼ˆobject-level groundingï¼‰å’ŒåŒºåŸŸæè¿°ï¼ˆregion captioningï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹å–„ï¼Œä½†åœ¨è§†è§‰å…³ç³»ç†è§£ï¼ˆä¾‹å¦‚åœºæ™¯å›¾ç”Ÿæˆï¼‰æ–¹é¢ä»å­˜åœ¨å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹åŠ¨ä½œäº‹ä»¶ä¸­å¤šä¸ªè¯­ä¹‰è§’è‰²è¿›è¡Œå»ºæ¨¡çš„Nå…ƒå…³ç³»ç†è§£ä¸Šã€‚è¿™ç§å¤šå®ä½“é—´è¯­ä¹‰ä¾èµ–å»ºæ¨¡çš„ç¼ºå¤±å¯¼è‡´äº†è¾“å‡ºä¸å¯é ï¼ŒåŠ å‰§äº†MLLMsçš„å¹»è§‰å’Œå¯¹è¯­è¨€å…ˆéªŒçŸ¥è¯†çš„è¿‡åº¦ä¾èµ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Relation-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€çš„å…³ç³»ç†è§£æ¡†æ¶ï¼Œå®ƒæ˜ç¡®åœ°åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èŒƒå¼ä¸­é›†æˆäº†è®¤çŸ¥æ€ç»´é“¾å¼•å¯¼çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡SFTå»ºç«‹åŸºæœ¬çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡æ€è€ƒè¿‡ç¨‹æ¥å¼ºåˆ¶ç»“æ„åŒ–è¾“å‡ºã€‚ç„¶åï¼Œåˆ©ç”¨GRPOé€šè¿‡å¤šå¥–åŠ±ä¼˜åŒ–æ¥å®Œå–„è¿™äº›è¾“å‡ºï¼Œä¼˜å…ˆé‡è§†è§†è§‰è¯­ä¹‰å®šä½ï¼Œä»¥å‡å°‘è¯­è¨€è¯±å¯¼çš„åè§ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„PSGå’ŒSWiGæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRelation-R1åœ¨äºŒå…ƒå’ŒNå…ƒå…³ç³»ç†è§£æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14642v1">PDF</a> Ongoing project</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¯¹è±¡çº§å®šä½ä¸åŒºåŸŸæè¿°æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨è§†è§‰å…³ç³»ç†è§£ï¼ˆå¦‚åœºæ™¯å›¾ç”Ÿæˆï¼‰æ–¹é¢ä»å­˜åœ¨å±€é™ï¼Œå°¤å…¶éš¾ä»¥å»ºæ¨¡åŠ¨ä½œäº‹ä»¶ä¸­å¤šä¸ªå®ä½“é—´çš„è¯­ä¹‰è§’è‰²ï¼ˆN-aryå…³ç³»ï¼‰ã€‚ç”±äºç¼ºä¹å¤šå®ä½“é—´çš„è¯­ä¹‰ä¾èµ–æ€§å»ºæ¨¡ï¼Œå¯¼è‡´è¾“å‡ºä¸å¯é ï¼ŒåŠ å‰§MLLMsçš„å¹»è§‰å’Œè¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºRelation-R1ï¼Œé¦–ä¸ªç»Ÿä¸€çš„å…³ç³»ç†è§£æ¡†æ¶ï¼Œæ˜¾å¼æ•´åˆè®¤çŸ¥æ€ç»´é“¾å¼•å¯¼çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èŒƒå¼ä¸­ã€‚é€šè¿‡SFTå»ºç«‹åŸºç¡€æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç»“æ„åŒ–è¾“å‡ºå¼ºåŒ–æ€ç»´è¿‡ç¨‹ï¼›ç„¶åä½¿ç”¨GRPOé€šè¿‡å¤šå¥–åŠ±ä¼˜åŒ–æ¥æ”¹è¿›è¾“å‡ºï¼Œä¼˜å…ˆè§†è§‰è¯­ä¹‰å®šä½ï¼Œå‡å°‘è¯­è¨€è¯±å¯¼åè§ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„PSGå’ŒSWiGæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRelation-R1åœ¨äºŒå…ƒå’ŒNå…ƒå…³ç³»ç†è§£æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¯¹è±¡çº§å®šä½å’ŒåŒºåŸŸæè¿°ä¸Šæœ‰æ‰€è¿›æ­¥ï¼Œä½†åœ¨è§†è§‰å…³ç³»ç†è§£ä¸Šä»æœ‰å±€é™ã€‚</li>
<li>ç°æœ‰æ¨¡å‹éš¾ä»¥å¤„ç†åŠ¨ä½œäº‹ä»¶ä¸­å¤šä¸ªå®ä½“é—´çš„è¯­ä¹‰è§’è‰²ï¼ˆN-aryå…³ç³»ï¼‰ã€‚</li>
<li>ç¼ºä¹å¤šå®ä½“é—´çš„è¯­ä¹‰ä¾èµ–æ€§å»ºæ¨¡å¯¼è‡´è¾“å‡ºä¸å¯é å’Œæ¨¡å‹å¹»è§‰ã€‚</li>
<li>Relation-R1æ¡†æ¶é€šè¿‡æ•´åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>SFTç”¨äºå»ºç«‹åŸºç¡€æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç»“æ„åŒ–è¾“å‡ºå¼ºåŒ–æ€ç»´è¿‡ç¨‹ã€‚</li>
<li>GRPOé€šè¿‡å¤šå¥–åŠ±ä¼˜åŒ–æ”¹è¿›è¾“å‡ºï¼Œä¼˜å…ˆè§†è§‰è¯­ä¹‰å®šä½ï¼Œå‡å°‘è¯­è¨€åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-de3ba12d9d29f5ae2321f624bd873b9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c33892ee5ea41b78f30c6a94055f847.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19ca4d23fb41005f190b51af8dfc32c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d455fb5593bc9ee2037d61a59e47f20a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AlphaZero-Edu-Making-AlphaZero-Accessible-to-Everyone"><a href="#AlphaZero-Edu-Making-AlphaZero-Accessible-to-Everyone" class="headerlink" title="AlphaZero-Edu: Making AlphaZero Accessible to Everyone"></a>AlphaZero-Edu: Making AlphaZero Accessible to Everyone</h2><p><strong>Authors:Binjie Guo, Hanyu Zheng, Guowei Su, Ru Zhang, Haohan Jiang, Xurong Lin, Hongyan Wei, Aisheng Mo, Jie Li, Zhiyuan Qian, Zhuhao Zhang, Xiaoyuan Cheng</strong></p>
<p>Recent years have witnessed significant progress in reinforcement learning, especially with Zero-like paradigms, which have greatly boosted the generalization and reasoning abilities of large-scale language models. Nevertheless, existing frameworks are often plagued by high implementation complexity and poor reproducibility. To tackle these challenges, we present AlphaZero-Edu, a lightweight, education-focused implementation built upon the mathematical framework of AlphaZero. It boasts a modular architecture that disentangles key components, enabling transparent visualization of the algorithmic processes. Additionally, it is optimized for resource-efficient training on a single NVIDIA RTX 3090 GPU and features highly parallelized self-play data generation, achieving a 3.2-fold speedup with 8 processes. In Gomoku matches, the framework has demonstrated exceptional performance, achieving a consistently high win rate against human opponents. AlphaZero-Edu has been open-sourced at <a target="_blank" rel="noopener" href="https://github.com/StarLight1212/AlphaZero_Edu">https://github.com/StarLight1212/AlphaZero_Edu</a>, providing an accessible and practical benchmark for both academic research and industrial applications. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯ä»¥Zeroä¸ºä»£è¡¨çš„æ¨¡å¼ï¼Œæå¤§åœ°æå‡äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¡†æ¶å¾€å¾€å­˜åœ¨å®ç°å¤æ‚å’Œå¯é‡å¤æ€§å·®çš„å›°æ‰°ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AlphaZero-Eduï¼Œè¿™æ˜¯ä¸€æ¬¾ä»¥AlphaZeroæ•°å­¦æ¡†æ¶ä¸ºåŸºç¡€ï¼Œä»¥æ•™è‚²ä¸ºé‡ç‚¹çš„è½»é‡åŒ–å®ç°ã€‚å®ƒæ‹¥æœ‰æ¨¡å—åŒ–æ¶æ„ï¼Œèƒ½å¤Ÿåˆ†è§£å…³é”®ç»„ä»¶ï¼Œå®ç°ç®—æ³•è¿‡ç¨‹çš„é€æ˜å¯è§†åŒ–ã€‚æ­¤å¤–ï¼Œå®ƒé’ˆå¯¹åœ¨å•ä¸ªNVIDIA RTX 3090 GPUä¸Šçš„èµ„æºé«˜æ•ˆè®­ç»ƒè¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¹¶å®ç°äº†é«˜åº¦å¹¶è¡ŒåŒ–çš„è‡ªæˆ‘æ¸¸æˆæ•°æ®ç”Ÿæˆï¼Œä½¿ç”¨8ä¸ªè¿›ç¨‹å®ç°äº†3.2å€çš„åŠ é€Ÿã€‚åœ¨å›´æ£‹æ¯”èµ›ä¸­ï¼Œè¯¥æ¡†æ¶è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨ä¸äººç±»å¯¹æ‰‹çš„æ¯”èµ›ä¸­æŒç»­ä¿æŒè¾ƒé«˜çš„èƒœç‡ã€‚AlphaZero-Eduå·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/StarLight1212/AlphaZero_Edu%E4%B8%8A%E5%BC%80%E6%BA%90%EF%BC%8C%E4%B8%BA%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6%E5%92%8C%E5%B7%A5%E4%B8%9A%E5%BA%94%E7%94%A8%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%80%E4%B8%AA%E5%AE%9E%E7%94%A8%E4%B8%94%E5%8F%AF%E6%8E%A5%E8%A7%A6%E7%9A%84%E5%9F%BA%E5%87%86%E3%80%82">https://github.com/StarLight1212/AlphaZero_Eduä¸Šå¼€æºï¼Œä¸ºå­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨æä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”å¯æ¥è§¦çš„åŸºå‡†ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14636v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œå¼ºåŒ–å­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯Zeroç±»èŒƒå¼ï¼Œæå¤§åœ°æå‡äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¡†æ¶å¸¸é¢ä¸´å®ç°å¤æ‚åº¦é«˜å’Œå¯é‡å¤æ€§å·®çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ¨å‡ºAlphaZero-Eduæ¡†æ¶ï¼Œå®ƒä»¥AlphaZeroçš„æ•°å­¦æ¡†æ¶ä¸ºåŸºç¡€ï¼Œæ³¨é‡æ•™è‚²å®ç”¨æ€§ï¼Œå…·æœ‰æ¨¡å—åŒ–æ¶æ„ï¼Œèƒ½å¤Ÿæ¸…æ™°å±•ç¤ºç®—æ³•è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å•ä¸ªNVIDIA RTX 3090 GPUä¸Šè¿›è¡Œèµ„æºä¼˜åŒ–è®­ç»ƒï¼Œé€šè¿‡é«˜åº¦å¹¶è¡Œçš„è‡ªç©æ•°æ®ç”Ÿæˆå®ç°3.2å€åŠ é€Ÿã€‚åœ¨å›´æ£‹æ¯”èµ›ä¸­ï¼Œè¯¥æ¡†æ¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒæŒç»­ä¿æŒå¯¹äººç±»å¯¹æ‰‹çš„è¾ƒé«˜èƒœç‡ã€‚AlphaZero-Eduå·²å¼€æºï¼Œä¸ºå­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨æä¾›äº†å®ç”¨åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AlphaZero-Eduæ˜¯åŸºäºAlphaZeroæ•°å­¦æ¡†æ¶çš„æ•™è‚²å®ç”¨æ€§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰æ¨¡å—åŒ–æ¶æ„ï¼Œèƒ½æ¸…æ™°å±•ç¤ºç®—æ³•è¿‡ç¨‹ã€‚</li>
<li>AlphaZero-Eduå®ç°äº†èµ„æºä¼˜åŒ–è®­ç»ƒï¼Œå¯åœ¨å•ä¸ªNVIDIA RTX 3090 GPUä¸Šè¿è¡Œã€‚</li>
<li>æ¡†æ¶å…·æœ‰é«˜åº¦çš„è‡ªç©æ•°æ®ç”Ÿæˆå¹¶è¡Œæ€§ï¼Œå®ç°äº†3.2å€åŠ é€Ÿã€‚</li>
<li>åœ¨å›´æ£‹æ¯”èµ›ä¸­ï¼ŒAlphaZero-Eduè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¯¹äººç±»å¯¹æ‰‹ä¿æŒè¾ƒé«˜èƒœç‡ã€‚</li>
<li>AlphaZero-Eduå·²å¼€æºï¼Œä¸ºå­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨æä¾›äº†å®ç”¨åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b2a88e5bcbadd645ade413c80e756a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e65cf06451f60e84046c3e08a33a3529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd92b3f341eb40889e5fbd714c223a62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2857e27da5411a15f9676968b9f2566.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReasoningV-Efficient-Verilog-Code-Generation-with-Adaptive-Hybrid-Reasoning-Model"><a href="#ReasoningV-Efficient-Verilog-Code-Generation-with-Adaptive-Hybrid-Reasoning-Model" class="headerlink" title="ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid   Reasoning Model"></a>ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid   Reasoning Model</h2><p><strong>Authors:Haiyan Qin, Zhiwei Xie, Jingjing Li, Liangchen Li, Xiaotong Feng, Junzhan Liu, Wang Kang</strong></p>
<p>Large Language Models (LLMs) have advanced Verilog code generation significantly, yet face challenges in data quality, reasoning capabilities, and computational efficiency. This paper presents ReasoningV, a novel model employing a hybrid reasoning strategy that integrates trained intrinsic capabilities with dynamic inference adaptation for Verilog code generation. Our framework introduces three complementary innovations: (1) ReasoningV-5K, a high-quality dataset of 5,000 functionally verified instances with reasoning paths created through multi-dimensional filtering of PyraNet samples; (2) a two-stage training approach combining parameter-efficient fine-tuning for foundational knowledge with full-parameter optimization for enhanced reasoning; and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning depth based on problem complexity, reducing token consumption by up to 75% while preserving performance. Experimental results demonstrate ReasoningVâ€™s effectiveness with a pass@1 accuracy of 57.8% on VerilogEval-human, achieving performance competitive with leading commercial models like Gemini-2.0-flash (59.5%) and exceeding the previous best open-source model by 10.4 percentage points. ReasoningV offers a more reliable and accessible pathway for advancing AI-driven hardware design automation, with our model, data, and code available at <a target="_blank" rel="noopener" href="https://github.com/BUAA-CLab/ReasoningV">https://github.com/BUAA-CLab/ReasoningV</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨Verilogä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä»é¢ä¸´ç€æ•°æ®è´¨é‡ã€æ¨ç†èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReasoningVçš„æ–°å‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨æ··åˆæ¨ç†ç­–ç•¥ï¼Œå°†è®­ç»ƒçš„å†…åœ¨èƒ½åŠ›ä¸åŠ¨æ€æ¨ç†é€‚åº”ç›¸ç»“åˆï¼Œç”¨äºVerilogä»£ç ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸‰é¡¹äº’è¡¥åˆ›æ–°ï¼š1ï¼‰ReasoningV-5Kï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«5000ä¸ªç»è¿‡åŠŸèƒ½éªŒè¯çš„å®ä¾‹ï¼Œé€šè¿‡PyraNetæ ·æœ¬çš„å¤šç»´è¿‡æ»¤åˆ›å»ºæ¨ç†è·¯å¾„ï¼›2ï¼‰ç»“åˆå‚æ•°é«˜æ•ˆå¾®è°ƒåŸºç¡€çŸ¥è¯†ä¸å…¨å‚æ•°ä¼˜åŒ–å¢å¼ºæ¨ç†èƒ½åŠ›çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼›3ï¼‰è‡ªé€‚åº”æ¨ç†æœºåˆ¶ï¼Œæ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæœ€å¤šå‡å°‘75%çš„ä»¤ç‰Œæ¶ˆè€—ã€‚å®éªŒç»“æœè¯æ˜äº†ReasoningVçš„æœ‰æ•ˆæ€§ï¼Œåœ¨VerilogEval-humanä¸Šçš„pass@1å‡†ç¡®ç‡ä¸º57.8%ï¼Œæ€§èƒ½ä¸é¢†å…ˆçš„å•†ä¸šæ¨¡å‹å¦‚Gemini-2.0-flashï¼ˆ59.5%ï¼‰ç›¸ç«äº‰ï¼Œå¹¶è¶…å‡ºä¹‹å‰æœ€å¥½çš„å¼€æºæ¨¡å‹10.4ä¸ªç™¾åˆ†ç‚¹ã€‚ReasoningVä¸ºæ¨è¿›äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–æä¾›äº†æ›´å¯é ã€æ›´å¯è¡Œçš„é€”å¾„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BUAA-CLab/ReasoningV%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/BUAA-CLab/ReasoningVä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14560v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ReasoningVï¼Œä¸€ç§ç”¨äºVerilogä»£ç ç”Ÿæˆçš„æ–°å‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ··åˆæ¨ç†ç­–ç•¥ï¼Œé›†æˆäº†è®­ç»ƒçš„å†…ç”Ÿèƒ½åŠ›ä¸åŠ¨æ€æ¨ç†é€‚åº”ã€‚è®ºæ–‡æå‡ºäº†ä¸‰é¡¹åˆ›æ–°ï¼šä¸€æ˜¯ReasoningV-5Kæ•°æ®é›†ï¼ŒåŒ…å«5000ä¸ªåŠŸèƒ½éªŒè¯å®ä¾‹ï¼›äºŒæ˜¯ä¸¤é˜¶æ®µè®­ç»ƒæ³•ï¼Œç»“åˆå‚æ•°é«˜æ•ˆå¾®è°ƒä¸å…¨å‚æ•°ä¼˜åŒ–ï¼›ä¸‰æ˜¯è‡ªé€‚åº”æ¨ç†æœºåˆ¶ï¼Œå¯åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReasoningVåœ¨VerilogEval-humanä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†57.8%ï¼Œä¸å•†ä¸šæ¨¡å‹ç«äº‰ï¼Œå¹¶è¶…è¶Šäº†ä¹‹å‰çš„å¼€æºæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReasoningVæ˜¯ä¸€ä¸ªç”¨äºVerilogä»£ç ç”Ÿæˆçš„æ–°å‹æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆæ¨ç†ç­–ç•¥ã€‚</li>
<li>è¯¥æ¨¡å‹æå‡ºäº†ReasoningV-5Kæ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡çš„åŠŸèƒ½éªŒè¯å®ä¾‹ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒæ³•ç»“åˆäº†å‚æ•°é«˜æ•ˆå¾®è°ƒä¸å…¨å‚æ•°ä¼˜åŒ–ã€‚</li>
<li>è‡ªé€‚åº”æ¨ç†æœºåˆ¶å¯æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œå‡å°‘è®¡ç®—æ¶ˆè€—ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºReasoningVåœ¨VerilogEval-humanä¸Šçš„å‡†ç¡®ç‡è¾ƒé«˜ï¼Œä¸å•†ä¸šæ¨¡å‹ç«äº‰ã€‚</li>
<li>è¯¥æ¨¡å‹è¶…è¶Šäº†ä¹‹å‰çš„å¼€æºæ¨¡å‹ï¼Œå±•ç°äº†è¾ƒé«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3a5726e2e318662459d205b6f3ea341b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8c33f45c5435285781c93d8a44cd1fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-769e0e68c59e0b1db3d64af76bdaf248.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a18727e4f93e30ea118c3b57c1109b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a835824b656096b094fe81a1343d0b59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e0af25d961e1a04037ced2f0daa1ebd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4829f4de68cd7c40f2490601f079a102.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb7592651064cf83151be26ee30dda9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Are-Vision-LLMs-Road-Ready-A-Comprehensive-Benchmark-for-Safety-Critical-Driving-Video-Understanding"><a href="#Are-Vision-LLMs-Road-Ready-A-Comprehensive-Benchmark-for-Safety-Critical-Driving-Video-Understanding" class="headerlink" title="Are Vision LLMs Road-Ready? A Comprehensive Benchmark for   Safety-Critical Driving Video Understanding"></a>Are Vision LLMs Road-Ready? A Comprehensive Benchmark for   Safety-Critical Driving Video Understanding</h2><p><strong>Authors:Tong Zeng, Longfeng Wu, Liang Shi, Dawei Zhou, Feng Guo</strong></p>
<p>Vision Large Language Models (VLLMs) have demonstrated impressive capabilities in general visual tasks such as image captioning and visual question answering. However, their effectiveness in specialized, safety-critical domains like autonomous driving remains largely unexplored. Autonomous driving systems require sophisticated scene understanding in complex environments, yet existing multimodal benchmarks primarily focus on normal driving conditions, failing to adequately assess VLLMsâ€™ performance in safety-critical scenarios. To address this, we introduce DVBench, a pioneering benchmark designed to evaluate the performance of VLLMs in understanding safety-critical driving videos. Built around a hierarchical ability taxonomy that aligns with widely adopted frameworks for describing driving scenarios used in assessing highly automated driving systems, DVBench features 10,000 multiple-choice questions with human-annotated ground-truth answers, enabling a comprehensive evaluation of VLLMsâ€™ capabilities in perception and reasoning. Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal significant performance gaps, with no model achieving over 40% accuracy, highlighting critical limitations in understanding complex driving scenarios. To probe adaptability, we fine-tuned selected models using domain-specific data from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage points, with relative improvements of up to 43.59%. This improvement underscores the necessity of targeted adaptation to bridge the gap between general-purpose VLLMs and mission-critical driving applications. DVBench establishes an essential evaluation framework and research roadmap for developing VLLMs that meet the safety and robustness requirements for real-world autonomous systems. We released the benchmark toolbox and the fine-tuned model at: <a target="_blank" rel="noopener" href="https://github.com/tong-zeng/DVBench.git">https://github.com/tong-zeng/DVBench.git</a>. </p>
<blockquote>
<p>è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰åœ¨ä¸€èˆ¬çš„è§†è§‰ä»»åŠ¡ï¼ˆå¦‚å›¾åƒæ ‡æ³¨å’Œè§†è§‰é—®ç­”ï¼‰ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è‡ªåŠ¨é©¾é©¶ç­‰ç‰¹æ®Šä¸”å®‰å…¨å…³é”®çš„é¢†åŸŸä¸­çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°å¹¿æ³›æ¢ç´¢ã€‚è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿéœ€è¦åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œé«˜çº§åœºæ™¯ç†è§£ï¼Œç„¶è€Œç°æœ‰çš„å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æ­£å¸¸é©¾é©¶æ¡ä»¶ï¼Œæ— æ³•å……åˆ†è¯„ä¼°VLLMsåœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°VLLMsåœ¨ç†è§£å®‰å…¨å…³é”®é©¾é©¶è§†é¢‘æ–¹é¢çš„æ€§èƒ½çš„å¼€åˆ›æ€§åŸºå‡†æµ‹è¯•ã€‚DVBenchå›´ç»•ä¸è¯„ä¼°é«˜åº¦è‡ªåŠ¨åŒ–é©¾é©¶ç³»ç»Ÿæ—¶å¹¿æ³›é‡‡ç”¨çš„é©¾é©¶åœºæ™¯æè¿°æ¡†æ¶ç›¸ä¸€è‡´çš„åˆ†å±‚èƒ½åŠ›åˆ†ç±»è¡¨æ„å»ºï¼ŒåŒ…å«10,000é“å¸¦æœ‰äººç±»æ³¨é‡Šæ ‡å‡†ç­”æ¡ˆçš„é€‰æ‹©é¢˜ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°VLLMsåœ¨æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å¯¹14ä¸ªæœ€å…ˆè¿›çš„VLLMè¿›è¡Œçš„å®éªŒï¼Œå‚æ•°èŒƒå›´ä»0.5Båˆ°72Bï¼Œæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æ€§èƒ½å·®è·ï¼Œæ²¡æœ‰ä»»ä½•æ¨¡å‹çš„å‡†ç¡®ç‡è¶…è¿‡40%ï¼Œè¿™çªæ˜¾äº†åœ¨ç†è§£å¤æ‚é©¾é©¶åœºæ™¯æ–¹é¢çš„å…³é”®å±€é™æ€§ã€‚ä¸ºäº†æ£€éªŒé€‚åº”æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨DVBenchçš„ç‰¹å®šé¢†åŸŸæ•°æ®å¯¹é€‰å®šæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå‡†ç¡®ç‡æé«˜äº†5.24è‡³10.94ä¸ªç™¾åˆ†ç‚¹ï¼Œç›¸å¯¹æ”¹è¿›å¹…åº¦é«˜è¾¾43.59%ã€‚è¿™ç§æ”¹è¿›çªæ˜¾äº†æœ‰é’ˆå¯¹æ€§çš„é€‚åº”å¯¹äºå¼¥åˆé€šç”¨VLLMä¸å…³é”®ä»»åŠ¡é©¾é©¶åº”ç”¨ç¨‹åºä¹‹é—´çš„å·®è·çš„å¿…è¦æ€§ã€‚DVBenchä¸ºå¼€å‘æ»¡è¶³ç°å®ä¸–ç•Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå®‰å…¨å’Œç¨³å¥æ€§è¦æ±‚çš„VLLMå»ºç«‹äº†é‡è¦çš„è¯„ä¼°æ¡†æ¶å’Œç ”ç©¶è·¯çº¿å›¾ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/tong-zeng/DVBench.git%E5%8F%91%E5%B8%83%E4%BA%86%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7%E7%AE%B1%E5%92%8C%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E3%80%82">https://github.com/tong-zeng/DVBench.gitå‘å¸ƒäº†åŸºå‡†æµ‹è¯•å·¥å…·ç®±å’Œå¾®è°ƒæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14526v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VLLMåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨æŒ‘æˆ˜ã€‚é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿéœ€è¦ç†è§£å¤æ‚ç¯å¢ƒä¸‹çš„åœºæ™¯ï¼Œç°æœ‰çš„ä¸€èˆ¬è¯­è¨€æ¨¡å‹è¯„ä¼°æ ‡å‡†æ— æ³•å……åˆ†è¯„ä¼°å…¶åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†DVBenchï¼Œä¸€ä¸ªä¸“é—¨è¯„ä¼°VLLMåœ¨ç†è§£å®‰å…¨å…³é”®é©¾é©¶è§†é¢‘æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æµ‹è¯•åŒ…æ‹¬å¤§é‡å¤šé¡¹é€‰æ‹©é¢˜ï¼Œå¹¶æä¾›äººç±»æ³¨é‡Šçš„åŸºå‡†ç­”æ¡ˆï¼Œä»¥å…¨é¢è¯„ä¼°VLLMåœ¨æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸­å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œä¸”æ— æ¨¡å‹è¾¾åˆ°è¶…è¿‡40%çš„å‡†ç¡®ç‡ï¼Œçªæ˜¾äº†åœ¨ç†è§£å¤æ‚é©¾é©¶åœºæ™¯æ–¹é¢çš„å…³é”®å±€é™æ€§ã€‚é€šè¿‡é’ˆå¯¹DVBenchçš„ç‰¹å®šæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æœ‰æ‰€æé«˜ï¼Œè¯æ˜äº†æœ‰é’ˆå¯¹æ€§çš„é€‚åº”è®­ç»ƒå¯¹äºç¼©å°é€šç”¨VLLMå’Œå…³é”®ä»»åŠ¡é©¾é©¶åº”ç”¨ä¹‹é—´çš„å·®è·çš„å¿…è¦æ€§ã€‚DVBenchä¸ºå¼€å‘æ»¡è¶³ç°å®ä¸–ç•Œè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå®‰å…¨å’Œç¨³å¥æ€§è¦æ±‚çš„VLLMæä¾›äº†é‡è¦çš„è¯„ä¼°æ¡†æ¶å’Œç ”ç©¶è·¯çº¿å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLLMåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦ç†è§£å¤æ‚ç¯å¢ƒä¸‹çš„åœºæ™¯ã€‚</li>
<li>ç°æœ‰çš„ä¸€èˆ¬è¯­è¨€æ¨¡å‹è¯„ä¼°æ ‡å‡†æ— æ³•å……åˆ†è¯„ä¼°VLLMåœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>DVBenchæ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°VLLMåœ¨ç†è§£å®‰å…¨å…³é”®é©¾é©¶è§†é¢‘æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¤§é‡å¤šé¡¹é€‰æ‹©é¢˜å¹¶æä¾›äººç±»æ³¨é‡Šçš„åŸºå‡†ç­”æ¡ˆã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨DVBenchä¸­å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œä¸”è¡¨ç°ä¸ä½³ã€‚</li>
<li>é€šè¿‡é’ˆå¯¹DVBenchçš„ç‰¹å®šæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡æœ‰æ‰€æé«˜ã€‚</li>
<li>æœ‰é’ˆå¯¹æ€§çš„é€‚åº”è®­ç»ƒå¯¹äºç¼©å°é€šç”¨VLLMå’Œå…³é”®ä»»åŠ¡é©¾é©¶åº”ç”¨ä¹‹é—´çš„å·®è·æ˜¯å¿…è¦çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-184d3af8d585a2f20ebd33b1b08f26d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f3c92cba8e39d7227b1f84c5475d87e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbdf58663c4f4555dc5b2dad506b5aff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15b9361061cbf2c59e053012548c966c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9e48d34d47f460d6696269039600ea7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CoLoTa-A-Dataset-for-Entity-based-Commonsense-Reasoning-over-Long-Tail-Knowledge"><a href="#CoLoTa-A-Dataset-for-Entity-based-Commonsense-Reasoning-over-Long-Tail-Knowledge" class="headerlink" title="CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail   Knowledge"></a>CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail   Knowledge</h2><p><strong>Authors:Armin Toroghi, Willis Guo, Scott Sanner</strong></p>
<p>The rise of Large Language Models (LLMs) has redefined the AI landscape, particularly due to their ability to encode factual and commonsense knowledge, and their outstanding performance in tasks requiring reasoning. Despite these advances, hallucinations and reasoning errors remain a significant barrier to their deployment in high-stakes settings. In this work, we observe that even the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning errors and hallucinations on tasks requiring commonsense reasoning over obscure, long-tail entities. To investigate this limitation, we present a new dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that consists of 3,300 queries from question answering and claim verification tasks and covers a diverse range of commonsense reasoning skills. We remark that CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset since the support of knowledge required to answer its queries is present in the Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that merely focus on factoid questions, our CoLoTa queries also require commonsense reasoning. Our experiments with strong LLM-based KGQA methodologies indicate their severe inability to answer queries involving commonsense reasoning. Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM commonsense reasoning capabilities and their robustness to hallucinations on long-tail entities and (ii) the commonsense reasoning capabilities of KGQA methods. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å´›èµ·å·²ç»é‡æ–°å®šä¹‰äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå°¤å…¶æ˜¯å®ƒä»¬ç¼–ç äº‹å®å’Œå¸¸è¯†çŸ¥è¯†çš„èƒ½åŠ›ï¼Œä»¥åŠå®ƒä»¬åœ¨éœ€è¦æ¨ç†çš„ä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¹»è§‰å’Œæ¨ç†é”™è¯¯ä»æ˜¯å®ƒä»¬åœ¨é«˜é£é™©ç¯å¢ƒä¸­éƒ¨ç½²çš„é‡å¤§éšœç¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å³ä½¿æ˜¯æœ€çªå‡ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚OpenAI-o1ï¼Œåœ¨å¤„ç†æ¶‰åŠæ¨¡ç³Šã€é•¿å°¾å®ä½“çš„å¸¸è¯†æ¨ç†ä»»åŠ¡æ—¶ï¼Œä¹Ÿä¼šå‡ºç°è¾ƒé«˜çš„æ¨ç†é”™è¯¯å’Œå¹»è§‰ç‡ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œåä¸ºé•¿å°¾å®ä½“å¸¸è¯†æ¨ç†æ•°æ®é›†ï¼ˆCoLoTaï¼‰ï¼Œå®ƒåŒ…å«æ¥è‡ªé—®ç­”å’Œæ–­è¨€éªŒè¯ä»»åŠ¡çš„3300ä¸ªæŸ¥è¯¢ï¼Œæ¶µç›–äº†å¹¿æ³›çš„å¸¸è¯†æ¨ç†æŠ€èƒ½ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œç”±äºWikiDataçŸ¥è¯†å›¾è°±ä¸­åŒ…å«å›ç­”å…¶æŸ¥è¯¢æ‰€éœ€çš„çŸ¥è¯†æ”¯æŒï¼ŒCoLoTaä¹Ÿå¯ä»¥ä½œä¸ºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ•°æ®é›†ä½¿ç”¨ã€‚ç„¶è€Œï¼Œä¸ä»…å…³æ³¨äº‹å®é—®é¢˜çš„ç°æœ‰KGQAåŸºå‡†æµ‹è¯•ä¸åŒï¼Œæˆ‘ä»¬çš„CoLoTaæŸ¥è¯¢è¿˜éœ€è¦å¸¸è¯†æ¨ç†ã€‚æˆ‘ä»¬ä¸å¼ºå¤§çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„KGQAæ–¹æ³•çš„å®éªŒè¡¨æ˜ï¼Œå®ƒä»¬ä¸¥é‡æ— æ³•å›ç­”æ¶‰åŠå¸¸è¯†æ¨ç†çš„æŸ¥è¯¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºå°†CoLoTaä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿å°¾å®ä½“ä¸Šçš„å¸¸è¯†æ¨ç†èƒ½åŠ›å’Œå¯¹å¹»è§‰çš„ç¨³å¥æ€§çš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼ŒåŒæ—¶ä¹Ÿæ˜¯è¯„ä¼°KGQAæ–¹æ³•çš„å¸¸è¯†æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14462v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨AIé¢†åŸŸå…·æœ‰é‡è¦åœ°ä½ï¼Œå…¶èƒ½å¤Ÿç¼–ç äº‹å®å’Œå¸¸è¯†çŸ¥è¯†ï¼Œå¹¶åœ¨éœ€è¦æ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œå¹»è§‰å’Œæ¨ç†é”™è¯¯ä»æ˜¯å…¶åœ¨é«˜é£é™©ç¯å¢ƒä¸­éƒ¨ç½²çš„é‡å¤§éšœç¢ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚OpenAI-o1ï¼Œåœ¨å¤„ç†æ¨¡ç³Šçš„é•¿å°¾å®ä½“ç›¸å…³çš„å¸¸è¯†æ¨ç†ä»»åŠ¡æ—¶ï¼Œå­˜åœ¨é«˜æ¯”ä¾‹çš„æ¨ç†é”™è¯¯å’Œå¹»è§‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†CoLoTaï¼ŒåŒ…å«3300ä¸ªæ¥è‡ªé—®ç­”å’Œå£°æ˜éªŒè¯ä»»åŠ¡çš„æŸ¥è¯¢ï¼Œæ¶µç›–äº†å¹¿æ³›çš„å¸¸è¯†æ¨ç†æŠ€èƒ½ã€‚CoLoTaè¿˜å¯ä»¥ä½œä¸ºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ•°æ®é›†ï¼Œè™½ç„¶å®ƒæ”¯æŒçš„çŸ¥è¯†å­˜åœ¨äºWikidataçŸ¥è¯†å›¾è°±ä¸­ï¼Œä½†æˆ‘ä»¬çš„æŸ¥è¯¢è¿˜éœ€è¦å¸¸è¯†æ¨ç†ï¼Œè¿™ä¸ç°æœ‰çš„KGQAåŸºå‡†æµ‹è¯•ä»…ä¾§é‡äºäº‹å®é—®é¢˜ä¸åŒã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„å¼ºå¤§LLMåŸºKGQAæ–¹æ³•ä¸¥é‡æ— æ³•å›ç­”æ¶‰åŠå¸¸è¯†æ¨ç†çš„æŸ¥è¯¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æè®®å°†CoLoTaä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿å°¾å®ä½“çš„å¸¸è¯†æ¨ç†èƒ½åŠ›å’Œå¯¹å¹»è§‰çš„ç¨³å¥æ€§ä»¥åŠè¯„ä¼°KGQAæ–¹æ³•çš„å¸¸è¯†æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨AIé¢†åŸŸå…·æœ‰é‡è¦å½±å“ï¼Œå°¤å…¶åœ¨ç¼–ç äº‹å®å’Œå¸¸è¯†çŸ¥è¯†ä»¥åŠæ¨ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å¹»è§‰å’Œæ¨ç†é”™è¯¯æ˜¯é™åˆ¶LLMåœ¨é«˜é£é™©ç¯å¢ƒä¸­åº”ç”¨çš„ä¸»è¦éšœç¢ã€‚</li>
<li>æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ¶‰åŠé•¿å°¾å®ä½“çš„å¸¸è¯†æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨é«˜æ¯”ä¾‹çš„æ¨ç†é”™è¯¯å’Œå¹»è§‰ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†CoLoTaï¼Œç”¨äºè¯„ä¼°LLMåœ¨é•¿å°¾å®ä½“çš„å¸¸è¯†æ¨ç†èƒ½åŠ›å’Œå¯¹å¹»è§‰çš„ç¨³å¥æ€§ã€‚</li>
<li>CoLoTaæ•°æ®é›†åŒ…å«å¹¿æ³›çš„æŸ¥è¯¢ï¼Œæ¶µç›–å¤šç§å¸¸è¯†æ¨ç†æŠ€èƒ½ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ•°æ®é›†ä½¿ç”¨ã€‚</li>
<li>ä¸ç°æœ‰KGQAåŸºå‡†æµ‹è¯•ä¸åŒï¼ŒCoLoTaçš„æŸ¥è¯¢éœ€è¦æ—¢æ”¯æŒçš„çŸ¥è¯†ä¹Ÿéœ€è¦å¸¸è¯†æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de4711d91d2af6df006f9f94170f2c76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3aeaf5b4629b0e3337664760387419a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4dfc9295cf0a99230b37ea343708695f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4168e680900d895a1046e9eaf7543ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887df7306ea33c93506c36fd498fe591.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Hydra-An-Agentic-Reasoning-Approach-for-Enhancing-Adversarial-Robustness-and-Mitigating-Hallucinations-in-Vision-Language-Models"><a href="#Hydra-An-Agentic-Reasoning-Approach-for-Enhancing-Adversarial-Robustness-and-Mitigating-Hallucinations-in-Vision-Language-Models" class="headerlink" title="Hydra: An Agentic Reasoning Approach for Enhancing Adversarial   Robustness and Mitigating Hallucinations in Vision-Language Models"></a>Hydra: An Agentic Reasoning Approach for Enhancing Adversarial   Robustness and Mitigating Hallucinations in Vision-Language Models</h2><p><strong>Authors: Chung-En,  Yu,  Hsuan-Chih,  Chen, Brian Jalaian, Nathaniel D. Bastian</strong></p>
<p>To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing methods primarily focus on either adversarial defense or hallucination post-hoc correction, leaving a gap in unified robustness strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that enhances plug-in VLMs through iterative reasoning, structured critiques, and cross-model verification, improving both resilience to adversarial perturbations and intrinsic model errors. Hydra employs an Action-Critique Loop, where it retrieves and critiques visual information, leveraging Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to both adversarial manipulations and intrinsic model errors, making it robust to malicious perturbations and hallucination-related inaccuracies. We evaluate Hydra on four VLMs, three hallucination benchmarks, two adversarial attack strategies, and two adversarial defense methods, assessing performance on both clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs and state-of-the-art (SOTA) dehallucination methods, even without explicit adversarial defenses, demonstrating enhanced robustness and factual consistency. By bridging adversarial resistance and hallucination mitigation, Hydra provides a scalable, training-free solution for improving the reliability of VLMs in real-world applications. </p>
<blockquote>
<p>ä¸ºäº†å¼€å‘å¯ä¿¡èµ–çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œè§£å†³å¯¹æŠ—æ€§é²æ£’æ€§å’Œå¹»è§‰å‡è½»é—®é¢˜è‡³å…³é‡è¦ï¼Œä¸¤è€…éƒ½ä¼šå½±å“é«˜é£é™©åº”ç”¨ï¼ˆå¦‚å›½é˜²å’ŒåŒ»ç–—ä¿å¥ï¼‰çš„äº‹å®å‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¯¹æŠ—æ€§é˜²å¾¡æˆ–å¹»è§‰äº‹åæ ¡æ­£ï¼Œåœ¨ç»Ÿä¸€é²æ£’æ€§ç­–ç•¥æ–¹é¢å­˜åœ¨ç©ºç™½ã€‚æˆ‘ä»¬ä»‹ç»äº†æµ·å¾·æ‹‰ï¼ˆHydraï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”çš„æ™ºèƒ½æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¿­ä»£æ¨ç†ã€ç»“æ„åŒ–æ‰¹è¯„å’Œè·¨æ¨¡å‹éªŒè¯å¢å¼ºå³æ’å³ç”¨å‹VLMsï¼Œæé«˜å¯¹å¯¹æŠ—æ€§æ‰°åŠ¨å’Œå†…åœ¨æ¨¡å‹é”™è¯¯çš„é€‚åº”èƒ½åŠ›ã€‚æµ·å¾·æ‹‰é‡‡ç”¨è¡ŒåŠ¨-è¯„è®ºå¾ªç¯ï¼Œæ£€ç´¢å’Œè¯„è®ºè§†è§‰ä¿¡æ¯ï¼Œåˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æŠ€æœ¯åŠ¨æ€ä¼˜åŒ–è¾“å‡ºã€‚ä¸é™æ€äº‹åæ ¡æ­£æ–¹æ³•ä¸åŒï¼Œæµ·å¾·æ‹‰èƒ½å¤Ÿé€‚åº”å¯¹æŠ—æ€§æ“ä½œå’Œå†…åœ¨æ¨¡å‹é”™è¯¯ï¼Œä½¿å…¶å¯¹æ¶æ„æ‰°åŠ¨å’Œå¹»è§‰ç›¸å…³çš„ä¸å‡†ç¡®å…·æœ‰é²æ£’æ€§ã€‚æˆ‘ä»¬åœ¨å››ç§VLMsã€ä¸‰ä¸ªå¹»è§‰åŸºå‡†æµ‹è¯•ã€ä¸¤ç§å¯¹æŠ—æ€§æ”»å‡»ç­–ç•¥å’Œä¸¤ç§å¯¹æŠ—æ€§é˜²å¾¡æ–¹æ³•ä¸Šè¯„ä¼°äº†æµ·å¾·æ‹‰çš„æ€§èƒ½ï¼Œè¯„ä¼°å…¶åœ¨å¹²å‡€å’Œå¯¹æŠ—æ€§è¾“å…¥ä¸Šçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œæµ·å¾·æ‹‰è¶…è¶Šäº†å³æ’å³ç”¨å‹VLMså’Œæœ€æ–°å…ˆè¿›å»å¹»è§‰æ–¹æ³•ï¼Œç”šè‡³åœ¨æ²¡æœ‰æ˜ç¡®çš„å¯¹æŠ—æ€§é˜²å¾¡æªæ–½çš„æƒ…å†µä¸‹ï¼Œä¹Ÿè¡¨ç°å‡ºå¢å¼ºçš„é²æ£’æ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚é€šè¿‡å¯¹æŠ—æŠµæŠ—å’Œå¹»è§‰å‡è½»ä¹‹é—´çš„æ¡¥æ¢ä½œç”¨ï¼Œæµ·å¾·æ‹‰æä¾›äº†ä¸€ç§å¯æ‰©å±•ã€æ— éœ€è®­ç»ƒå³å¯æé«˜VLMåœ¨ç°å®ä¸–ç•Œä¸­åº”ç”¨å¯é æ€§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14395v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºHydraçš„é€‚åº”æ€§ä»£ç†æ¡†æ¶ï¼Œç”¨äºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„ç¨³å¥æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£æ¨ç†ã€ç»“æ„åŒ–è¯„ä»·å’Œè·¨æ¨¡å‹éªŒè¯ç­‰æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹å¯¹å¯¹æŠ—æ€§å¹²æ‰°å’Œå†…åœ¨æ¨¡å‹é”™è¯¯çš„æŠµå¾¡èƒ½åŠ›ã€‚Hydraé‡‡ç”¨è¡ŒåŠ¨-è¯„ä»·å¾ªç¯ï¼Œåˆ©ç”¨æ€ç»´é“¾å’Œä¸Šä¸‹æ–‡å­¦ä¹ æŠ€æœ¯åŠ¨æ€ä¼˜åŒ–è¾“å‡ºã€‚åœ¨å¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHydraè¡¨ç°å‡ºè¶…è¶Šæ’ä»¶å¼è§†è§‰è¯­è¨€æ¨¡å‹å’Œæœ€æ–°å»å¹»è§‰æ–¹æ³•çš„æ€§èƒ½ï¼Œæ— éœ€æ˜ç¡®çš„å¯¹æŠ—æ€§é˜²å¾¡å³å¯å¢å¼ºç¨³å¥æ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Hydraæ˜¯ä¸€ä¸ªé€‚åº”æ€§ä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£æ¨ç†ã€ç»“æ„åŒ–è¯„ä»·å’Œè·¨æ¨¡å‹éªŒè¯ç­‰æŠ€æœ¯æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Hydraé‡‡ç”¨è¡ŒåŠ¨-è¯„ä»·å¾ªç¯ï¼Œèƒ½å¤ŸåŠ¨æ€ä¼˜åŒ–è¾“å‡ºï¼Œé€‚åº”å¯¹æŠ—æ€§æ“ä½œå’Œå†…åœ¨æ¨¡å‹é”™è¯¯ã€‚</li>
<li>ä¸é™æ€çš„åå¤„ç†æ–¹æ³•ä¸åŒï¼ŒHydraæ¡†æ¶èƒ½å¤Ÿåº”å¯¹æ¶æ„å¹²æ‰°å’Œå¹»è§‰ç›¸å…³çš„ä¸å‡†ç¡®æ€§ã€‚</li>
<li>Hydraåœ¨å¤šä¸ªVLMsã€å¹»è§‰åŸºå‡†æµ‹è¯•ã€å¯¹æŠ—æ€§æ”»å‡»ç­–ç•¥å’Œé˜²å¾¡æ–¹æ³•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå®ƒè¶…è¶Šäº†æ’ä»¶å¼VLMså’Œæœ€æ–°çš„å»å¹»è§‰æ–¹æ³•ã€‚</li>
<li>Hydraæé«˜äº†æ¨¡å‹åœ¨å¹²å‡€å’Œå¯¹æŠ—æ€§è¾“å…¥ä¸Šçš„æ€§èƒ½ï¼Œå¢å¼ºäº†ç¨³å¥æ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e50cd3455d2637e42cb4507dca65d09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa558f0407a85c0fd29c66cce9d210ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12f8a538f1dcfee5370231de7774985f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Improving-RL-Exploration-for-LLM-Reasoning-through-Retrospective-Replay"><a href="#Improving-RL-Exploration-for-LLM-Reasoning-through-Retrospective-Replay" class="headerlink" title="Improving RL Exploration for LLM Reasoning through Retrospective Replay"></a>Improving RL Exploration for LLM Reasoning through Retrospective Replay</h2><p><strong>Authors:Shihan Dou, Muling Wu, Jingwen Xu, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Reinforcement learning (RL) has increasingly become a pivotal technique in the post-training of large language models (LLMs). The effective exploration of the output space is essential for the success of RL. We observe that for complex problems, during the early stages of training, the model exhibits strong exploratory capabilities and can identify promising solution ideas. However, its limited capability at this stage prevents it from successfully solving these problems. The early suppression of these potentially valuable solution ideas by the policy gradient hinders the modelâ€™s ability to revisit and re-explore these ideas later. Consequently, although the LLMâ€™s capabilities improve in the later stages of training, it still struggles to effectively address these complex problems. To address this exploration issue, we propose a novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL), which introduces a dynamic replay mechanism throughout the training process. RRL enables the model to revisit promising states identified in the early stages, thereby improving its efficiency and effectiveness in exploration. To evaluate the effectiveness of RRL, we conduct extensive experiments on complex reasoning tasks, including mathematical reasoning and code generation, and general dialogue tasks. The results indicate that RRL maintains high exploration efficiency throughout the training period, significantly enhancing the effectiveness of RL in optimizing LLMs for complicated reasoning tasks. Moreover, it also improves the performance of RLHF, making the model both safer and more helpful. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åæœŸè®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸æˆä¸ºäº†ä¸€é¡¹å…³é”®çš„æŠ€æœ¯ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæœ‰æ•ˆåœ°æ¢ç´¢è¾“å‡ºç©ºé—´å¯¹äºå…¶æˆåŠŸè‡³å…³é‡è¦ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¯¹äºå¤æ‚çš„é—®é¢˜ï¼Œåœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼Œæ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„æ¢ç´¢èƒ½åŠ›ï¼Œå¹¶èƒ½å¤Ÿè¯†åˆ«å‡ºæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¿™ä¸€é˜¶æ®µçš„æœ‰é™èƒ½åŠ›ä½¿å…¶æ— æ³•æˆåŠŸè§£å†³è¿™äº›é—®é¢˜ã€‚æ—©æœŸç­–ç•¥æ¢¯åº¦å¯¹è¿™äº›å¯èƒ½å…·æœ‰ä»·å€¼çš„è§£å†³æ–¹æ¡ˆçš„å‹åˆ¶é˜»ç¢äº†æ¨¡å‹æ—¥åé‡æ–°è®¿é—®å’Œå†æ¬¡æ¢ç´¢è¿™äº›è§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œå°½ç®¡LLMåœ¨è®­ç»ƒçš„åæœŸé˜¶æ®µçš„èƒ½åŠ›æœ‰æ‰€æå‡ï¼Œä½†å®ƒä»ç„¶éš¾ä»¥æœ‰æ•ˆåœ°è§£å†³è¿™äº›å¤æ‚çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ¢ç´¢é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŸºäºå›é¡¾çš„é‡æ”¾å¼ºåŒ–å­¦ä¹ ï¼ˆRRLï¼‰çš„æ–°ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†åŠ¨æ€é‡æ”¾æœºåˆ¶ã€‚RRLä½¿æ¨¡å‹èƒ½å¤Ÿé‡æ–°è®¿é—®æ—©æœŸé˜¶æ®µå‘ç°çš„å…·æœ‰å‰æ™¯çš„çŠ¶æ€ï¼Œä»è€Œæé«˜å…¶æ¢ç´¢çš„æ•ˆç‡ã€‚ä¸ºäº†è¯„ä¼°RRLçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼ˆåŒ…æ‹¬æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆï¼‰å’Œé€šç”¨å¯¹è¯ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒRRLåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­éƒ½èƒ½ä¿æŒè¾ƒé«˜çš„æ¢ç´¢æ•ˆç‡ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†å¼ºåŒ–å­¦ä¹ åœ¨ä¼˜åŒ–å¤æ‚æ¨ç†ä»»åŠ¡çš„LLMæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æé«˜äº†RLHFçš„æ€§èƒ½ï¼Œä½¿æ¨¡å‹æ›´åŠ å®‰å…¨å’Œæœ‰ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14363v1">PDF</a> 13 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨åè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºåœ¨è®­ç»ƒæ—©æœŸé˜¶æ®µæ¨¡å‹å…·æœ‰å¼ºå¤§çš„æ¢ç´¢èƒ½åŠ›ä½†ç¼ºä¹è§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºâ€œåŸºäºå›é¡¾çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRRLï¼‰â€çš„æ–°ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥åŠ¨æ€å›æ”¾æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé‡æ–°è®¿é—®æ—©æœŸé˜¶æ®µå‘ç°çš„å…·æœ‰æ½œåŠ›çš„çŠ¶æ€ï¼Œä»è€Œæé«˜æ¢ç´¢æ•ˆç‡å’Œæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRRLåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­æ˜¯å…³é”®æŠ€æœ¯ã€‚</li>
<li>è®­ç»ƒæ—©æœŸé˜¶æ®µï¼Œæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ¢ç´¢èƒ½åŠ›ä½†ç¼ºä¹è§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹æ—©æœŸæŠ‘åˆ¶çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆå¯èƒ½å¯¼è‡´å…¶åœ¨åç»­è®­ç»ƒä¸­æ— æ³•é‡æ–°æ¢ç´¢ã€‚</li>
<li>ä¸ºè§£å†³æ¢ç´¢é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå›é¡¾çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRRLï¼‰ç®—æ³•ã€‚</li>
<li>RRLç®—æ³•é€šè¿‡åŠ¨æ€å›æ”¾æœºåˆ¶ä½¿æ¨¡å‹èƒ½å¤Ÿé‡æ–°è®¿é—®æ—©æœŸå‘ç°çš„æœ‰æ½œåŠ›çŠ¶æ€ã€‚</li>
<li>å®éªŒè¡¨æ˜RRLåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜æ¨¡å‹çš„æ¢ç´¢æ•ˆç‡å’Œæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25d9931054a71748562f45beb92533bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efc19d08c0b296ba920bfb9ae9fe77fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af8c8365cafeac3297989ec9a6b1b3a7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SRPO-A-Cross-Domain-Implementation-of-Large-Scale-Reinforcement-Learning-on-LLM"><a href="#SRPO-A-Cross-Domain-Implementation-of-Large-Scale-Reinforcement-Learning-on-LLM" class="headerlink" title="SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement   Learning on LLM"></a>SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement   Learning on LLM</h2><p><strong>Authors:Xiaojiang Zhang, Jinghui Wang, Zifei Cheng, Wenhao Zhuang, Zheng Lin, Minglei Zhang, Shaojie Wang, Yinghan Cui, Chao Wang, Junyi Peng, Shimiao Jiang, Shiqi Kuang, Shouyu Yin, Chaohang Wen, Haotian Zhang, Bin Chen, Bing Yu</strong></p>
<p>Recent advances of reasoning models, exemplified by OpenAIâ€™s o1 and DeepSeekâ€™s R1, highlight the significant potential of Reinforcement Learning (RL) to enhance the reasoning capabilities of Large Language Models (LLMs). However, replicating these advancements across diverse domains remains challenging due to limited methodological transparency. In this work, we present two-Staged history-Resampling Policy Optimization (SRPO), which successfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24 and LiveCodeBench benchmarks. SRPO achieves this using the same base model as DeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised Fine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we introduce two key methodological innovations: (1) a two-stage cross-domain training paradigm designed to balance the development of mathematical reasoning and coding proficiency, and (2) History Resampling (HR), a technique to address ineffective samples. Our comprehensive experiments validate the effectiveness of our approach, dedicating to offer valuable insights into scaling LLM reasoning capabilities across diverse tasks. </p>
<blockquote>
<p>è¿‘æœŸæ¨ç†æ¨¡å‹çš„è¿›å±•ï¼Œä»¥OpenAIçš„o1å’ŒDeepSeekçš„R1ä¸ºä¾‹ï¼Œå‡¸æ˜¾äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ–¹æ³•é€æ˜åº¦çš„é™åˆ¶ï¼Œå°†è¿™äº›è¿›å±•å¤åˆ¶åˆ°ä¸åŒé¢†åŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é˜¶æ®µå†å²é‡é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆSRPOï¼‰ï¼Œå®ƒæˆåŠŸè¶…è¶Šäº†DeepSeek-R1-Zero-32Båœ¨AIME24å’ŒLiveCodeBenchåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚SRPOä½¿ç”¨ä¸DeepSeekç›¸åŒçš„åŸºå‡†æ¨¡å‹ï¼ˆå³Qwen2.5-32Bï¼‰ï¼Œä»…ä¾èµ–å¼ºåŒ–å­¦ä¹ ï¼Œæ— éœ€é¢„å…ˆè¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ã€‚åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§é‡è¦çš„æ–¹æ³•è®ºåˆ›æ–°ï¼šï¼ˆ1ï¼‰ä¸€ç§ä¸¤é˜¶æ®µè·¨åŸŸè®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨å¹³è¡¡æ•°å­¦æ¨ç†å’Œç¼–ç èƒ½åŠ›çš„å‘å±•ï¼›ï¼ˆ2ï¼‰å†å²é‡é‡‡æ ·ï¼ˆHRï¼‰ï¼Œä¸€ç§è§£å†³æ— æ•ˆæ ·æœ¬çš„æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè‡´åŠ›äºåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­æä¾›å…³äºæ‰©å±•LLMæ¨ç†èƒ½åŠ›çš„å®è´µè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14286v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ€è¿‘å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¦‚OpenAIçš„o1å’ŒDeepSeekçš„R1æ‰€ç¤ºã€‚ç„¶è€Œï¼Œåœ¨ä¸åŒé¢†åŸŸå¤åˆ¶è¿™äº›è¿›æ­¥å› æ–¹æ³•é€æ˜åº¦æœ‰é™è€Œé¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸¤é˜¶æ®µå†å²é‡é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆSRPOï¼‰ï¼Œåœ¨AIME24å’ŒLiveCodeBenchåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†DeepSeek-R1-Zero-32Bçš„æ€§èƒ½ã€‚SRPOä½¿ç”¨ä¸DeepSeekç›¸åŒçš„åŸºå‡†æ¨¡å‹ï¼ˆå³Qwen2.5-32Bï¼‰ï¼Œä»…ä¾èµ–RLï¼Œæ— éœ€å…ˆéªŒçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚åŸºäºé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„æ–¹æ³•åˆ›æ–°ï¼šä¸€æ˜¯ä¸¤é˜¶æ®µè·¨åŸŸè®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨å¹³è¡¡æ•°å­¦æ¨ç†å’Œç¼–ç¨‹èƒ½åŠ›çš„å‘å±•ï¼›äºŒæ˜¯å†å²é‡é‡‡æ ·ï¼ˆHRï¼‰æŠ€æœ¯ï¼Œä»¥è§£å†³æ— æ•ˆæ ·æœ¬é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>å¤åˆ¶è¿™äº›è¿›æ­¥åœ¨ä¸åŒé¢†åŸŸå› æ–¹æ³•é€æ˜åº¦æœ‰é™è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>SRPOæ–¹æ³•æˆåŠŸè¶…è¶Šäº†DeepSeek-R1-Zero-32Bçš„æ€§èƒ½ï¼Œä½¿ç”¨ç›¸åŒçš„åŸºå‡†æ¨¡å‹å¹¶ä»…ä¾èµ–å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>SRPOå¼•å…¥äº†ä¸¤é˜¶æ®µè·¨åŸŸè®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨å¹³è¡¡æ•°å­¦æ¨ç†å’Œç¼–ç¨‹èƒ½åŠ›çš„å‘å±•ã€‚</li>
<li>å†å²é‡é‡‡æ ·ï¼ˆHRï¼‰æŠ€æœ¯æ˜¯è§£å†³æ— æ•ˆæ ·æœ¬é—®é¢˜çš„ä¸€ç§æ–°æ–¹æ³•ã€‚</li>
<li>SRPOå»ºç«‹åœ¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä¹‹ä¸Šï¼Œè¿›è¡Œäº†ä¸¤ä¸ªå…³é”®çš„æ–¹æ³•åˆ›æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7ee31fdea377b7bccb8a21731d873ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ce5eb644c1233b18c94d292f3cc2c21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f3d9021b20875218cc50f1622ba19ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a61f7a144911a5348f3d8e65732f76d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-444b70ab701718c2d9507d763990c72b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61e1e11e5c3c19739e41a4f0b2179cd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70476916ccd00b76a5d30976c4ce9f2e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="InfiGUI-R1-Advancing-Multimodal-GUI-Agents-from-Reactive-Actors-to-Deliberative-Reasoners"><a href="#InfiGUI-R1-Advancing-Multimodal-GUI-Agents-from-Reactive-Actors-to-Deliberative-Reasoners" class="headerlink" title="InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to   Deliberative Reasoners"></a>InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to   Deliberative Reasoners</h2><p><strong>Authors:Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, Fei Wu</strong></p>
<p>Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at <a target="_blank" rel="noopener" href="https://github.com/Reallm-Labs/InfiGUI-R1">https://github.com/Reallm-Labs/InfiGUI-R1</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»ä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æä¾›äº†åŠ¨åŠ›ï¼Œåœ¨è‡ªåŠ¨åŒ–è®¡ç®—è®¾å¤‡ä¸Šçš„ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œå·²ç»å¼€å§‹æ¢ç´¢GUIä»»åŠ¡çš„æ¨ç†ï¼Œå¹¶è·å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œè®¸å¤šå½“å‰çš„æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„æ¨ç†æ¨¡æ¿ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨ç†åœ¨å¤æ‚çš„GUIç¯å¢ƒä¸­ä¸å¤Ÿç¨³å¥å’Œé€‚åº”ã€‚åŒæ—¶ï¼Œä¸€äº›ç°æœ‰çš„ä»£ç†ä»ç„¶ä»¥ååº”å¼çš„æ–¹å¼è¿è¡Œï¼Œä¸»è¦ä¾èµ–äºç¼ºä¹è¶³å¤Ÿæ·±åº¦çš„éšå¼æ¨ç†ï¼Œå¯¹äºéœ€è¦è§„åˆ’å’Œé”™è¯¯æ¢å¤çš„GUIä»»åŠ¡å¯èƒ½ä¸å¤Ÿå……åˆ†ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¦å‘å±•è¿™äº›ä»£ç†ï¼Œéœ€è¦ä»ååº”æ€§è¡Œä¸ºè½¬å‘åŸºäºç†æ€§æ€è€ƒçš„è¡Œä¸ºã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€è½¬å˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†InfiGUI-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºMLLMçš„GUIä»£ç†ï¼Œå®ƒæ˜¯é€šè¿‡æˆ‘ä»¬çš„Actor2Reasoneræ¡†æ¶å¼€å‘çš„ã€‚è¿™æ˜¯ä¸€ä¸ªä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„ã€ä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨é€æ­¥å°†ä»£ç†ä»ååº”å¼æ¼”å‘˜è¿›åŒ–ä¸ºæ·±æ€ç†Ÿè™‘çš„æ¨ç†è€…ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯æ¨ç†æ³¨å…¥ï¼Œä¾§é‡äºå»ºç«‹åŸºæœ¬çš„æ¨ç†è€…ã€‚æˆ‘ä»¬é‡‡ç”¨ç©ºé—´æ¨ç†è’¸é¦æ³•ï¼Œé€šè¿‡å…·æœ‰æ˜ç¡®æ¨ç†æ­¥éª¤çš„è½¨è¿¹ï¼Œå°†æ•™å¸ˆæ¨¡å‹çš„å¤šæ¨¡æ€ç©ºé—´æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°MLLMsä¸Šï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç”ŸæˆåŠ¨ä½œä¹‹å‰æ•´åˆGUIçš„è§†è§‰ç©ºé—´ä¿¡æ¯ä¸é€»è¾‘æ¨ç†ã€‚ç¬¬äºŒé˜¶æ®µæ˜¯ç²¾ç»†æ¨ç†å¢å¼ºé˜¶æ®µï¼Œå®ƒå°†åŸºæœ¬æ¨ç†è€…å‘å±•æˆæ·±æ€ç†Ÿè™‘çš„æ¨ç†è€…ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ã€‚è¿™ä¸€é˜¶æ®µå¼•å…¥äº†ä¸¤ç§æ–¹æ³•ï¼šå­ç›®æ ‡æŒ‡å¯¼ï¼Œå¥–åŠ±æ¨¡å‹ç”Ÿæˆå‡†ç¡®çš„ä¸­é—´å­ç›®æ ‡ï¼›é”™è¯¯æ¢å¤åœºæ™¯æ„å»ºï¼Œä»å·²è¯†åˆ«çš„æ˜“å‡ºé”™æ­¥éª¤ä¸­åˆ›å»ºå¤±è´¥å’Œæ¢å¤çš„è®­ç»ƒåœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInfiGUI-R1åœ¨GUIå®šä½å’Œè½¨è¿¹ä»»åŠ¡ä¸­å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç›¸å…³èµ„æºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Reallm-Labs/InfiGUI-R1%E3%80%82">https://github.com/Reallm-Labs/InfiGUI-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14239v1">PDF</a> 10 pages, 3 figures, work in progress</p>
<p><strong>Summary</strong></p>
<p>åœ¨æ–‡æœ¬ä¸­ï¼Œä¸»è¦è®¨è®ºäº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„å‘å±•ä¸ä¼˜åŒ–é—®é¢˜ã€‚ä¼ ç»Ÿçš„GUIä»£ç†ä¸»è¦ä¾é ååº”æ€§è¡Œä¸ºï¼Œç¼ºä¹æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚æ–‡ç« æå‡ºéœ€è¦å‘åŸºäºæ·±æ€ç†Ÿè™‘çš„æ¨ç†è¡Œä¸ºè½¬å˜ï¼Œå¹¶ä»‹ç»äº†InfiGUI-R1è¿™ä¸€åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„GUIä»£ç†ã€‚InfiGUI-R1é€šè¿‡Actor2Reasoneræ¡†æ¶å¼€å‘ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ³•é€æ­¥è¿›åŒ–ä»£ç†ä»ååº”æ€§è¡Œä¸ºåˆ°æ·±æ€ç†Ÿè™‘çš„æ¨ç†è¡Œä¸ºã€‚ç¬¬ä¸€é˜¶æ®µçš„æ¨ç†æ³¨å…¥ç€é‡å»ºç«‹åŸºæœ¬æ¨ç†èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µçš„å®¡è®®å¢å¼ºåˆ™åœ¨æ­¤åŸºç¡€ä¸Šé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ç²¾ç‚¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºInfiGUI-R1åœ¨GUIæ¥åœ°å’Œè½¨è¿¹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç›¸å…³èµ„æºå¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/Reallm-Labs/InfiGUI-R1">https://github.com/Reallm-Labs/InfiGUI-R1</a> äº†è§£æ›´å¤šä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æä¾›äº†æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨åŒ–è®¡ç®—è®¾å¤‡ä»»åŠ¡æ–¹é¢ã€‚</li>
<li>å½“å‰GUIä»£ç†åœ¨è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„æ¨ç†æ¨¡æ¿æˆ–è€…ä»…ä»…åŸºäºååº”æ€§è¡Œä¸ºçš„éšå¼æ¨ç†ã€‚</li>
<li>æ–‡ç« çš„ç›®çš„æ˜¯æ¨è¿›è¿™äº›ä»£ç†ä»ååº”æ€§è¡ŒåŠ¨å‘åŸºäºæ·±æ€ç†Ÿè™‘çš„æ¨ç†è¡ŒåŠ¨è½¬å˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºInfiGUI-R1çš„åŸºäºMLLMçš„GUIä»£ç†ï¼Œè¯¥ä»£ç†é€šè¿‡Actor2Reasoneræ¡†æ¶å¼€å‘ï¼Œè¯¥æ¡†æ¶å¼ºè°ƒä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„ä¸¤é˜¶æ®µè®­ç»ƒæ³•ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé‡ç‚¹æ˜¯å»ºç«‹åŸºæœ¬çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç©ºé—´æ¨ç†è’¸é¦æŠ€æœ¯ä»æ•™å¸ˆæ¨¡å‹è½¬ç§»è·¨æ¨¡æ€ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ç²¾ç‚¼åŸºæœ¬æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼•å…¥å­ç›®æ ‡æŒ‡å¯¼å’Œé”™è¯¯æ¢å¤åœºæ™¯æ„å»ºä¸¤ç§æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72b6a13a0282ee757f19bf36225e1833.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ade82bc643c80ee6312d434ee68ce19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41cbcf063cd63d533f4ab1de09eccd7a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Direct-Advantage-Regression-Aligning-LLMs-with-Online-AI-Reward"><a href="#Direct-Advantage-Regression-Aligning-LLMs-with-Online-AI-Reward" class="headerlink" title="Direct Advantage Regression: Aligning LLMs with Online AI Reward"></a>Direct Advantage Regression: Aligning LLMs with Online AI Reward</h2><p><strong>Authors:Li He, He Zhao, Stephen Wan, Dadong Wang, Lina Yao, Tongliang Liu</strong></p>
<p>Online AI Feedback (OAIF) presents a promising alternative to Reinforcement Learning from Human Feedback (RLHF) by utilizing online AI preference in aligning language models (LLMs). However, the straightforward replacement of humans with AI deprives LLMs from learning more fine-grained AI supervision beyond binary signals. In this paper, we propose Direct Advantage Regression (DAR), a simple alignment algorithm using online AI reward to optimize policy improvement through weighted supervised fine-tuning. As an RL-free approach, DAR maintains theoretical consistency with online RLHF pipelines while significantly reducing implementation complexity and improving learning efficiency. Our empirical results underscore that AI reward is a better form of AI supervision consistently achieving higher human-AI agreement as opposed to AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show that DAR outperforms both OAIF and online RLHF baselines. </p>
<blockquote>
<p>åœ¨çº¿äººå·¥æ™ºèƒ½åé¦ˆï¼ˆOAIFï¼‰é€šè¿‡åˆ©ç”¨åœ¨çº¿äººå·¥æ™ºèƒ½åå¥½å¯¹é½è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç®€å•åœ°å°†äººç±»æ›¿æ¢ä¸ºäººå·¥æ™ºèƒ½å‰¥å¤ºäº†LLMä»è¶…è¶ŠäºŒå…ƒä¿¡å·çš„ç²¾ç»†ç²’åº¦äººå·¥æ™ºèƒ½ç›‘ç£ä¸­å­¦ä¹ æœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç›´æ¥ä¼˜åŠ¿å›å½’ï¼ˆDARï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨åœ¨çº¿äººå·¥æ™ºèƒ½å¥–åŠ±è¿›è¡Œä¼˜åŒ–ç­–ç•¥æ”¹è¿›çš„ç®€å•å¯¹é½ç®—æ³•ï¼Œé€šè¿‡åŠ æƒç›‘ç£å¾®è°ƒã€‚ä½œä¸ºä¸€ç§æ— å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼ŒDARåœ¨ç†è®ºä¸Šä¸åœ¨çº¿RLHFç®¡é“ä¿æŒä¸€è‡´ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†å®ç°å¤æ‚æ€§å¹¶æé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºäººå·¥æ™ºèƒ½åå¥½ï¼Œäººå·¥æ™ºèƒ½å¥–åŠ±æ˜¯ä¸€ç§æ›´å¥½çš„ç›‘ç£å½¢å¼ï¼Œåœ¨ä¿æŒæ›´é«˜çš„äººæœºä¸€è‡´æ€§çš„åŒæ—¶æŒç»­å–å¾—æ›´å¥½çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œä½¿ç”¨GPT-4 Turboå’ŒMT-benchçš„è¯„ä¼°æ˜¾ç¤ºDARçš„è¡¨ç°ä¼˜äºOAIFå’Œåœ¨çº¿RLHFåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14177v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨çº¿AIåé¦ˆï¼ˆOAIFï¼‰ä¸ºå¼ºåŒ–å­¦ä¹ çš„äººç±»åé¦ˆï¼ˆRLHFï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡åˆ©ç”¨åœ¨çº¿AIåå¥½å¯¹é½è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨AIä»£æ›¿äººç±»ä¼šå¯¼è‡´LLMå¤±å»å­¦ä¹ è¶…è¶ŠäºŒå…ƒä¿¡å·çš„ç²¾ç»†ç²’åº¦AIç›‘ç£çš„æœºä¼šã€‚æœ¬æ–‡æå‡ºäº†ç›´æ¥ä¼˜åŠ¿å›å½’ï¼ˆDARï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„å¯¹é½ç®—æ³•ï¼Œä½¿ç”¨åœ¨çº¿AIå¥–åŠ±ä¼˜åŒ–ç­–ç•¥æ”¹è¿›ï¼Œé€šè¿‡åŠ æƒç›‘ç£å¾®è°ƒæ¥å®ç°ã€‚ä½œä¸ºä¸€ç§æ— å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼ŒDARåœ¨ç†è®ºä¿æŒä¸åœ¨çº¿RLHFç®¡é“çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†å®ç°å¤æ‚æ€§å¹¶æé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºAIåå¥½ï¼ŒAIå¥–åŠ±æ˜¯ä¸€ç§æ›´å¥½çš„AIç›‘ç£å½¢å¼ï¼Œå¹¶æŒç»­å®ç°æ›´é«˜çš„äººæœºå…±è¯†ã€‚æ­¤å¤–ï¼Œä½¿ç”¨GPT-4 Turboå’ŒMT-benchçš„è¯„ä¼°æ˜¾ç¤ºDARä¼˜äºOAIFå’Œåœ¨çº¿RLHFåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿AIåé¦ˆï¼ˆOAIFï¼‰æä¾›äº†ä¸€ç§å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåˆ©ç”¨åœ¨çº¿AIåå¥½å¯¹é½è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç›´æ¥ä½¿ç”¨AIä»£æ›¿äººç±»å¯èƒ½å¯¼è‡´LLMå¤±å»å­¦ä¹ è¶…è¶ŠäºŒå…ƒä¿¡å·çš„ç²¾ç»†ç²’åº¦AIç›‘ç£çš„æœºä¼šã€‚</li>
<li>ç›´æ¥ä¼˜åŠ¿å›å½’ï¼ˆDARï¼‰æ˜¯ä¸€ç§ä½¿ç”¨åœ¨çº¿AIå¥–åŠ±ä¼˜åŒ–ç­–ç•¥æ”¹è¿›çš„ç®€å•å¯¹é½ç®—æ³•ã€‚</li>
<li>DARä¿æŒä¸åœ¨çº¿RLHFç®¡é“çš„ç†è®ºä¸€è‡´æ€§ï¼ŒåŒæ—¶é™ä½äº†å®ç°å¤æ‚æ€§å¹¶æé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>AIå¥–åŠ±æ˜¯ä¸€ç§æ›´å¥½çš„AIç›‘ç£å½¢å¼ï¼Œç›¸æ¯”AIåå¥½ï¼ŒæŒç»­å®ç°æ›´é«˜çš„äººæœºå…±è¯†ã€‚</li>
<li>DARåœ¨è¯„ä¼°ä¸­è¡¨ç°å‡ºä¼˜äºOAIFå’Œåœ¨çº¿RLHFåŸºçº¿çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c63b3accdf4166625c624eb5a7aceedf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-482640bdd28f3f2e09beb1f82eac1381.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-017aae9cd3c1bb09306ee7a71685baa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8e609f49d52dd21b8400a759871bd43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db423f00df7fdd760114a24b871f1b7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec244f3c5bc3b38d6edd7029376252f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d6e7128717f96368cefded27a8a3c7b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="HF4Rec-Human-Like-Feedback-Driven-Optimization-Framework-for-Explainable-Recommendation"><a href="#HF4Rec-Human-Like-Feedback-Driven-Optimization-Framework-for-Explainable-Recommendation" class="headerlink" title="HF4Rec: Human-Like Feedback-Driven Optimization Framework for   Explainable Recommendation"></a>HF4Rec: Human-Like Feedback-Driven Optimization Framework for   Explainable Recommendation</h2><p><strong>Authors:Jiakai Tang, Jingsen Zhang, Zihang Tian, Xueyang Feng, Lei Wang, Xu Chen</strong></p>
<p>Recent advancements in explainable recommendation have greatly bolstered user experience by elucidating the decision-making rationale. However, the existing methods actually fail to provide effective feedback signals for potentially better or worse generated explanations due to their reliance on traditional supervised learning paradigms in sparse interaction data. To address these issues, we propose a novel human-like feedback-driven optimization framework. This framework employs a dynamic interactive optimization mechanism for achieving human-centered explainable requirements without incurring high labor costs. Specifically, we propose to utilize large language models (LLMs) as human simulators to predict human-like feedback for guiding the learning process. To enable the LLMs to deeply understand the task essence and meet userâ€™s diverse personalized requirements, we introduce a human-induced customized reward scoring method, which helps stimulate the language understanding and logical reasoning capabilities of LLMs. Furthermore, considering the potential conflicts between different perspectives of explanation quality, we introduce a principled Pareto optimization that transforms the multi-perspective quality enhancement task into a multi-objective optimization problem for improving explanation performance. At last, to achieve efficient model training, we design an off-policy optimization pipeline. By incorporating a replay buffer and addressing the data distribution biases, we can effectively improve data utilization and enhance model generality. Extensive experiments on four datasets demonstrate the superiority of our approach. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¯è§£é‡Šæ¨èæŠ€æœ¯çš„è¿›å±•é€šè¿‡é˜æ˜å†³ç­–ç†ç”±æå¤§åœ°æå‡äº†ç”¨æˆ·ä½“éªŒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”±äºä¾èµ–äºç¨€ç–äº¤äº’æ•°æ®ä¸­çš„ä¼ ç»Ÿç›‘ç£å­¦ä¹ èŒƒå¼ï¼Œæ— æ³•ä¸ºå¯èƒ½æ›´å¥½æˆ–æ›´å·®çš„ç”Ÿæˆè§£é‡Šæä¾›æœ‰æ•ˆçš„åé¦ˆä¿¡å·ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„äººæœºåé¦ˆé©±åŠ¨ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŠ¨æ€äº¤äº’ä¼˜åŒ–æœºåˆ¶ï¼Œä»¥å®ç°ä»¥äººç±»ä¸ºä¸­å¿ƒçš„å¯è§£é‡Šè¦æ±‚ï¼Œè€Œä¸ä¼šäº§ç”Ÿé«˜æ˜‚çš„åŠ³åŠ¨åŠ›æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æè®®åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºäººç±»æ¨¡æ‹Ÿå™¨ï¼Œé¢„æµ‹äººç±»åé¦ˆä»¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚ä¸ºäº†è®©å¤§å‹è¯­è¨€æ¨¡å‹æ·±å…¥äº†è§£ä»»åŠ¡æœ¬è´¨å¹¶æ»¡è¶³ç”¨æˆ·çš„å¤šæ ·åŒ–ä¸ªæ€§åŒ–éœ€æ±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§äººç±»å¼•å¯¼çš„è‡ªå®šåˆ¶å¥–åŠ±è¯„åˆ†æ–¹æ³•ï¼Œè¿™æœ‰åŠ©äºæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­è¨€ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°è§£é‡Šè´¨é‡ä¸åŒè§†è§’é—´æ½œåœ¨çš„å†²çªï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœ‰åŸåˆ™çš„Paretoä¼˜åŒ–ï¼Œå°†å¤šè§†è§’è´¨é‡æå‡ä»»åŠ¡è½¬åŒ–ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œä»¥æå‡è§£é‡Šæ€§èƒ½ã€‚æœ€åï¼Œä¸ºäº†å®ç°é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç¦»ç­–ç•¥ä¼˜åŒ–æµç¨‹ã€‚é€šè¿‡ç»“åˆå›æ”¾ç¼“å†²å¹¶è§£å†³æ•°æ®åˆ†å¸ƒåå·®é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆæé«˜æ•°æ®åˆ©ç”¨ç‡å¹¶å¢å¼ºæ¨¡å‹çš„é€šç”¨æ€§ã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14147v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„äººæœºäº¤äº’ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨èç³»ç»Ÿè§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹äººç±»åé¦ˆæ¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥ä¸ªæ€§åŒ–å¥–åŠ±è¯„åˆ†æ–¹æ³•åˆºæ¿€è¯­è¨€ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé‡‡ç”¨åŸºäºParetoä¼˜åŒ–çš„å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼Œæé«˜è§£é‡Šæ€§èƒ½ã€‚æœ€ç»ˆè®¾è®¡äº†ä¸€ç§ç¦»ç­–ç•¥ä¼˜åŒ–ç®¡é“ï¼Œæé«˜æ¨¡å‹æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ¨èç³»ç»Ÿè§£é‡Šæ€§æ–¹æ³•å­˜åœ¨åé¦ˆä¿¡å·ä¸æœ‰æ•ˆçš„é—®é¢˜ï¼Œå°¤å…¶åœ¨ç¨€ç–äº¤äº’æ•°æ®ä¸­ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„äººæœºäº¤äº’ä¼˜åŒ–æ¡†æ¶ï¼Œä»¥äººç±»ä¸ºä¸­å¿ƒï¼Œå®ç°æ›´é«˜æ•ˆçš„è§£é‡Šæ€§ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹äººç±»åé¦ˆï¼Œä»¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ï¼Œæ»¡è¶³ç”¨æˆ·ä¸ªæ€§åŒ–éœ€æ±‚ã€‚</li>
<li>å¼•å…¥ä¸ªæ€§åŒ–å¥–åŠ±è¯„åˆ†æ–¹æ³•ï¼Œå¢å¼ºè¯­è¨€ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨åŸºäºParetoä¼˜åŒ–çš„å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼Œè§£å†³ä¸åŒè§£é‡Šè´¨é‡è§‚ç‚¹é—´çš„æ½œåœ¨å†²çªã€‚</li>
<li>è®¾è®¡ç¦»ç­–ç•¥ä¼˜åŒ–ç®¡é“ï¼Œæé«˜æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€æ•°æ®åˆ©ç”¨ç‡å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4525a6e4d5b06f12eb97de773d34e9ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fae3c8b936e1ce49cb25971dfb560d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c34e2507070644818af372462aa37ec8.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fdf18cf6727a5ef0291e281ed754117b.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  Seeing from Another Perspective Evaluating Multi-View Understanding in   MLLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6e1c6944f56bf512853daf0c0173d1bf.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
