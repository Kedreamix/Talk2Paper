<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  DSPO Direct Semantic Preference Optimization for Real-World Image   Super-Resolution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-76770f72e5c3422b29c3bf5aa446794c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-23-æ›´æ–°"><a href="#2025-04-23-æ›´æ–°" class="headerlink" title="2025-04-23 æ›´æ–°"></a>2025-04-23 æ›´æ–°</h1><h2 id="DSPO-Direct-Semantic-Preference-Optimization-for-Real-World-Image-Super-Resolution"><a href="#DSPO-Direct-Semantic-Preference-Optimization-for-Real-World-Image-Super-Resolution" class="headerlink" title="DSPO: Direct Semantic Preference Optimization for Real-World Image   Super-Resolution"></a>DSPO: Direct Semantic Preference Optimization for Real-World Image   Super-Resolution</h2><p><strong>Authors:Miaomiao Cai, Simiao Li, Wei Li, Xudong Huang, Hanting Chen, Jie Hu, Yunhe Wang</strong></p>
<p>Recent advances in diffusion models have improved Real-World Image Super-Resolution (Real-ISR), but existing methods lack human feedback integration, risking misalignment with human preference and may leading to artifacts, hallucinations and harmful content generation. To this end, we are the first to introduce human preference alignment into Real-ISR, a technique that has been successfully applied in Large Language Models and Text-to-Image tasks to effectively enhance the alignment of generated outputs with human preferences. Specifically, we introduce Direct Preference Optimization (DPO) into Real-ISR to achieve alignment, where DPO serves as a general alignment technique that directly learns from the human preference dataset. Nevertheless, unlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR are difficult to reconcile with the image-level preferences of DPO, which can lead to the DPO being overly sensitive to local anomalies, leading to reduced generation quality. To resolve this dichotomy, we propose Direct Semantic Preference Optimization (DSPO) to align instance-level human preferences by incorporating semantic guidance, which is through two strategies: (a) semantic instance alignment strategy, implementing instance-level alignment to ensure fine-grained perceptual consistency, and (b) user description feedback strategy, mitigating hallucinations through semantic textual feedback on instance-level images. As a plug-and-play solution, DSPO proves highly effective in both one-step and multi-step SR frameworks. </p>
<blockquote>
<p>åœ¨æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸­ï¼ŒçœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰æŠ€æœ¯å¾—åˆ°äº†æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹äººç±»åé¦ˆé›†æˆï¼Œå­˜åœ¨ä¸äººç±»åå¥½ä¸ä¸€è‡´çš„é£é™©ï¼Œå¯èƒ½å¯¼è‡´å‡ºç°ä¼ªå½±ã€å¹»è§‰å’Œæœ‰å®³å†…å®¹ç”Ÿæˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†äººç±»åå¥½å¯¹é½æŠ€æœ¯å¼•å…¥Real-ISRã€‚è¯¥æŠ€æœ¯å·²æˆåŠŸåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ï¼Œæ—¨åœ¨æœ‰æ•ˆæé«˜ç”Ÿæˆè¾“å‡ºä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºReal-ISRå¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä»¥å®ç°å¯¹é½ï¼Œå…¶ä¸­DPOä½œä¸ºä¸€ç§é€šç”¨å¯¹é½æŠ€æœ¯ï¼Œç›´æ¥ä»äººç±»åå¥½æ•°æ®é›†ä¸­å­¦ä¹ ã€‚ç„¶è€Œï¼Œä¸é«˜çº§ä»»åŠ¡ä¸åŒï¼ŒReal-ISRçš„åƒç´ çº§é‡å»ºç›®æ ‡ä¸DPOçš„å›¾åƒçº§åå¥½éš¾ä»¥åè°ƒï¼Œå¯èƒ½å¯¼è‡´DPOå¯¹å±€éƒ¨å¼‚å¸¸è¿‡äºæ•æ„Ÿï¼Œä»è€Œé™ä½ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€çŸ›ç›¾ï¼Œæˆ‘ä»¬æå‡ºäº†ç›´æ¥è¯­ä¹‰åå¥½ä¼˜åŒ–ï¼ˆDSPOï¼‰ï¼Œé€šè¿‡èå…¥è¯­ä¹‰æŒ‡å¯¼æ¥å¯¹é½å®ä¾‹çº§äººç±»åå¥½ã€‚è¿™åŒ…æ‹¬ä¸¤ä¸ªç­–ç•¥ï¼šï¼ˆaï¼‰è¯­ä¹‰å®ä¾‹å¯¹é½ç­–ç•¥ï¼Œå®ç°å®ä¾‹çº§å¯¹é½ä»¥ç¡®ä¿ç²¾ç»†çš„æ„ŸçŸ¥ä¸€è‡´æ€§ï¼›ï¼ˆbï¼‰ç”¨æˆ·æè¿°åé¦ˆç­–ç•¥ï¼Œé€šè¿‡å®ä¾‹çº§å›¾åƒçš„è¯­ä¹‰æ–‡æœ¬åé¦ˆæ¥ç¼“è§£å¹»è§‰é—®é¢˜ã€‚ä½œä¸ºä¸€ç§å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆï¼ŒDSPOåœ¨å•æ­¥å’Œå¤šæ­¥è¶…åˆ†è¾¨ç‡æ¡†æ¶ä¸­éƒ½è¯æ˜äº†å…¶é«˜åº¦æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15176v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥äººç±»åå¥½å¯¹é½æŠ€æœ¯æ¥è§£å†³çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰ä¸­çš„ä¸€äº›é—®é¢˜ï¼Œå¦‚å¯¹é½äººç±»åå¥½ä¸è¶³å’Œå¯èƒ½äº§ç”Ÿä¼ªåƒå’Œæœ‰å®³å†…å®¹ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œå¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œå¹¶ç»“åˆè¯­ä¹‰æŒ‡å¯¼è¿›è¡Œæ”¹è¿›æå‡ºäº†ç›´æ¥è¯­ä¹‰åå¥½ä¼˜åŒ–ï¼ˆDSPOï¼‰ã€‚é€šè¿‡è¯­ä¹‰å®ä¾‹å¯¹é½ç­–ç•¥å’Œå®ä¾‹çº§åˆ«çš„ç”¨æˆ·æè¿°åé¦ˆç­–ç•¥å®ç°ç»†ç²’åº¦çš„æ„ŸçŸ¥ä¸€è‡´æ€§å¹¶å‡å°‘ä¼ªåƒã€‚DSPOä½œä¸ºä¸€ç§å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆï¼Œåœ¨ä¸€æ­¥å’Œå¤šæ­¥SRæ¡†æ¶ä¸­å‡è¡¨ç°å‡ºé«˜åº¦æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰©æ•£æ¨¡å‹åœ¨Real-ISRä¸­çš„æœ€æ–°è¿›å±•é¢ä¸´ä¸äººç±»åå¥½å¯¹é½ä¸è¶³çš„é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´ä¼ªåƒå’Œæœ‰å®³å†…å®¹ç”Ÿæˆã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œé¦–æ¬¡å°†äººç±»åå¥½å¯¹é½æŠ€æœ¯å¼•å…¥Real-ISRã€‚</li>
<li>é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æŠ€æœ¯å®ç°äººç±»åå¥½å¯¹é½ï¼Œä½†é¢ä¸´ä¸Real-ISRåƒç´ çº§é‡å»ºç›®æ ‡çš„ä¸åè°ƒé—®é¢˜ã€‚</li>
<li>æå‡ºç›´æ¥è¯­ä¹‰åå¥½ä¼˜åŒ–ï¼ˆDSPOï¼‰æ¥è§£å†³DPOçš„å±€é™æ€§ï¼Œç»“åˆè¯­ä¹‰æŒ‡å¯¼è¿›è¡Œå®ä¾‹çº§åˆ«çš„äººç±»åå¥½å¯¹é½ã€‚</li>
<li>DSPOé€šè¿‡ä¸¤ä¸ªç­–ç•¥å®ç°ï¼šè¯­ä¹‰å®ä¾‹å¯¹é½å’Œç”¨æˆ·æè¿°åé¦ˆï¼Œç¡®ä¿ç»†ç²’åº¦çš„æ„ŸçŸ¥ä¸€è‡´æ€§å¹¶å‡å°‘ä¼ªåƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5f8c7d046bf5732c6b24f67c04bcca98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b0157178f017a2ad3b8d8c69f41821e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3b55ed96bfa69e3a68a99e512026c58.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VistaDepth-Frequency-Modulation-With-Bias-Reweighting-For-Enhanced-Long-Range-Depth-Estimation"><a href="#VistaDepth-Frequency-Modulation-With-Bias-Reweighting-For-Enhanced-Long-Range-Depth-Estimation" class="headerlink" title="VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced   Long-Range Depth Estimation"></a>VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced   Long-Range Depth Estimation</h2><p><strong>Authors:Mingxia Zhan, Li Zhang, XiaoMeng Chu, Beibei Wang</strong></p>
<p>Monocular depth estimation (MDE) aims to predict per-pixel depth values from a single RGB image. Recent advancements have positioned diffusion models as effective MDE tools by framing the challenge as a conditional image generation task. Despite their progress, these methods often struggle with accurately reconstructing distant depths, due largely to the imbalanced distribution of depth values and an over-reliance on spatial-domain features. To overcome these limitations, we introduce VistaDepth, a novel framework that integrates adaptive frequency-domain feature enhancements with an adaptive weight-balancing mechanism into the diffusion process. Central to our approach is the Latent Frequency Modulation (LFM) module, which dynamically refines spectral responses in the latent feature space, thereby improving the preservation of structural details and reducing noisy artifacts. Furthermore, we implement an adaptive weighting strategy that modulates the diffusion loss in real-time, enhancing the modelâ€™s sensitivity towards distant depth reconstruction. These innovations collectively result in superior depth perception performance across both distance and detail. Experimental evaluations confirm that VistaDepth achieves state-of-the-art performance among diffusion-based MDE techniques, particularly excelling in the accurate reconstruction of distant regions. </p>
<blockquote>
<p>å•çœ¼æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰æ—¨åœ¨ä»å•ä¸€RGBå›¾åƒé¢„æµ‹æ¯ä¸ªåƒç´ çš„æ·±åº¦å€¼ã€‚æœ€è¿‘çš„è¿›å±•å°†æ‰©æ•£æ¨¡å‹å®šä½ä¸ºæœ‰æ•ˆçš„MDEå·¥å…·ï¼Œå°†æŒ‘æˆ˜è§†ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œè¿™äº›æ–¹æ³•åœ¨å‡†ç¡®é‡å»ºè¿œè·ç¦»æ·±åº¦æ–¹é¢ä»é¢ä¸´å›°éš¾ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ·±åº¦å€¼åˆ†å¸ƒä¸å¹³è¡¡ä»¥åŠè¿‡äºä¾èµ–ç©ºé—´åŸŸç‰¹å¾ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†VistaDepthï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå°†è‡ªé€‚åº”é¢‘åŸŸç‰¹å¾å¢å¼ºå’Œè‡ªé€‚åº”æƒé‡å¹³è¡¡æœºåˆ¶é›†æˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯æ½œåœ¨é¢‘ç‡è°ƒåˆ¶ï¼ˆLFMï¼‰æ¨¡å—ï¼Œå®ƒåŠ¨æ€åœ°ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ç©ºé—´ä¸­çš„å…‰è°±å“åº”ï¼Œä»è€Œæé«˜ç»“æ„ç»†èŠ‚çš„ä¿ç•™ï¼Œå‡å°‘å™ªå£°ä¼ªå½±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ç§è‡ªé€‚åº”åŠ æƒç­–ç•¥ï¼Œå®æ—¶è°ƒæ•´æ‰©æ•£æŸå¤±ï¼Œæé«˜æ¨¡å‹å¯¹è¿œè·ç¦»æ·±åº¦é‡å»ºçš„æ•æ„Ÿæ€§ã€‚è¿™äº›åˆ›æ–°å…±åŒå¯¼è‡´äº†åœ¨è·ç¦»å’Œç»†èŠ‚æ–¹é¢çš„æ·±åº¦æ„ŸçŸ¥æ€§èƒ½ä¼˜è¶Šã€‚å®éªŒè¯„ä¼°è¯å®ï¼ŒVistaDepthåœ¨åŸºäºæ‰©æ•£çš„MDEæŠ€æœ¯ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®é‡å»ºè¿œè·ç¦»åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15095v1">PDF</a> 8 pages, 6 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œé€šè¿‡å°†æŒ‘æˆ˜è½¬åŒ–ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸åœ¨é‡å»ºè¿œè·ç¦»æ·±åº¦æ—¶å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†VistaDepthæ¡†æ¶ï¼Œé›†æˆäº†è‡ªé€‚åº”é¢‘åŸŸç‰¹å¾å¢å¼ºå’Œè‡ªé€‚åº”æƒé‡å¹³è¡¡æœºåˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ½œé¢‘ç‡è°ƒåˆ¶ï¼ˆLFMï¼‰æ¨¡å—åŠ¨æ€ä¼˜åŒ–é¢‘è°±å“åº”ï¼Œæé«˜äº†ç»“æ„ç»†èŠ‚çš„ä¿ç•™å¹¶å‡å°‘äº†å™ªå£°ä¼ªå½±ã€‚æ­¤å¤–ï¼Œå®æ–½è‡ªé€‚åº”æƒé‡ç­–ç•¥ï¼Œå®æ—¶è°ƒæ•´æ‰©æ•£æŸå¤±ï¼Œæé«˜å¯¹è¿œè·ç¦»æ·±åº¦é‡å»ºçš„æ•æ„Ÿåº¦ã€‚è¿™äº›åˆ›æ–°ä½¿VistaDepthåœ¨è·ç¦»å’Œç»†èŠ‚æ–¹é¢çš„æ·±åº¦æ„ŸçŸ¥æ€§èƒ½å‡è¾¾åˆ°ä¸€æµæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å•ç›®æ·±åº¦ä¼°è®¡ä¸­å±•ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´é‡å»ºè¿œè·ç¦»æ·±åº¦çš„æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæ·±åº¦å€¼åˆ†å¸ƒä¸å¹³è¡¡åŠè¿‡åº¦ä¾èµ–ç©ºé—´åŸŸç‰¹å¾ã€‚</li>
<li>VistaDepthæ¡†æ¶é€šè¿‡é›†æˆè‡ªé€‚åº”é¢‘åŸŸç‰¹å¾å¢å¼ºå’Œè‡ªé€‚åº”æƒé‡å¹³è¡¡æœºåˆ¶æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>Latent Frequency Modulationï¼ˆLFMï¼‰æ¨¡å—åŠ¨æ€ä¼˜åŒ–é¢‘è°±å“åº”ï¼Œæé«˜ç»“æ„ç»†èŠ‚ä¿ç•™å¹¶å‡å°‘å™ªå£°ä¼ªå½±ã€‚</li>
<li>å®æ–½è‡ªé€‚åº”æƒé‡ç­–ç•¥ï¼Œå®æ—¶è°ƒæ•´æ‰©æ•£æŸå¤±ï¼Œå¢å¼ºæ¨¡å‹å¯¹è¿œè·ç¦»æ·±åº¦é‡å»ºçš„æ•æ„Ÿåº¦ã€‚</li>
<li>VistaDepthåœ¨æ·±åº¦æ„ŸçŸ¥æ€§èƒ½ä¸Šè¾¾åˆ°ä¸€æµæ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®é‡å»ºè¿œè·ç¦»åŒºåŸŸæ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-244ab64531acd72da5bbffc3f1d24544.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72cda878b092ffe90a7ac0e1850b497a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e61bd7439e03175edb293be32c720b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aada7c6e3a3d4d7c7ab217b83811009b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d38f9f6803a7a066c6cf2b5a5a508534.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Gaussian-Shading-Rethinking-the-Realistic-Deployment-Challenge-of-Performance-Lossless-Image-Watermark-for-Diffusion-Models"><a href="#Gaussian-Shading-Rethinking-the-Realistic-Deployment-Challenge-of-Performance-Lossless-Image-Watermark-for-Diffusion-Models" class="headerlink" title="Gaussian Shading++: Rethinking the Realistic Deployment Challenge of   Performance-Lossless Image Watermark for Diffusion Models"></a>Gaussian Shading++: Rethinking the Realistic Deployment Challenge of   Performance-Lossless Image Watermark for Diffusion Models</h2><p><strong>Authors:Zijin Yang, Xin Zhang, Kejiang Chen, Kai Zeng, Qiyi Yao, Han Fang, Weiming Zhang, Nenghai Yu</strong></p>
<p>Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. Existing methods primarily focus on ensuring that watermark embedding does not degrade the model performance. However, they often overlook critical challenges in real-world deployment scenarios, such as the complexity of watermark key management, user-defined generation parameters, and the difficulty of verification by arbitrary third parties. To address this issue, we propose Gaussian Shading++, a diffusion model watermarking method tailored for real-world deployment. We propose a double-channel design that leverages pseudorandom error-correcting codes to encode the random seed required for watermark pseudorandomization, achieving performance-lossless watermarking under a fixed watermark key and overcoming key management challenges. Additionally, we model the distortions introduced during generation and inversion as an additive white Gaussian noise channel and employ a novel soft decision decoding strategy during extraction, ensuring strong robustness even when generation parameters vary. To enable third-party verification, we incorporate public key signatures, which provide a certain level of resistance against forgery attacks even when model inversion capabilities are fully disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only maintains performance losslessness but also outperforms existing methods in terms of robustness, making it a more practical solution for real-world deployment. </p>
<blockquote>
<p>å…³äºæ‰©æ•£æ¨¡å‹çš„å®ç”¨åŒ–ï¼Œå­˜åœ¨å…³äºç‰ˆæƒä¿æŠ¤å’Œä¸å½“å†…å®¹ç”Ÿæˆçš„ä¼¦ç†é—®é¢˜æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨ç”Ÿæˆçš„å›¾åƒä¸Šæ·»åŠ æ°´å°ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç¡®ä¿æ°´å°åµŒå…¥ä¸ä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»–ä»¬å¾€å¾€å¿½è§†äº†çœŸå®éƒ¨ç½²åœºæ™¯ä¸­å…³é”®æŒ‘æˆ˜ï¼Œå¦‚æ°´å°å¯†é’¥ç®¡ç†çš„å¤æ‚æ€§ã€ç”¨æˆ·å®šä¹‰çš„ç”Ÿæˆå‚æ•°ä»¥åŠç¬¬ä¸‰æ–¹éªŒè¯çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ–¯é˜´å½±++ï¼ˆGaussian Shading++ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹çœŸå®ä¸–ç•Œéƒ¨ç½²çš„æ‰©æ•£æ¨¡å‹æ°´å°æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒé€šé“è®¾è®¡ï¼Œåˆ©ç”¨ä¼ªéšæœºçº é”™ç¼–ç å¯¹æ°´å°ä¼ªéšæœºåŒ–æ‰€éœ€çš„éšæœºç§å­è¿›è¡Œç¼–ç ï¼Œåœ¨å›ºå®šæ°´å°å¯†é’¥ä¸‹å®ç°æ€§èƒ½æ— æŸçš„æ°´å°ï¼Œå¹¶è§£å†³å¯†é’¥ç®¡ç†æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆå’Œåè½¬è¿‡ç¨‹ä¸­å¼•å…¥çš„å¤±çœŸå»ºæ¨¡ä¸ºåŠ æ€§ç™½è‰²é«˜æ–¯å™ªå£°é€šé“ï¼Œå¹¶åœ¨æå–è¿‡ç¨‹ä¸­é‡‡ç”¨æ–°å‹è½¯å†³ç­–è§£ç ç­–ç•¥ï¼Œç¡®ä¿å³ä½¿åœ¨ç”Ÿæˆå‚æ•°å˜åŒ–æ—¶ä¹Ÿå…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§ã€‚ä¸ºäº†å®ç°ç¬¬ä¸‰æ–¹éªŒè¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¬é’¥ç­¾åï¼Œå³ä½¿åœ¨æ¨¡å‹åè½¬èƒ½åŠ›å®Œå…¨å…¬å¼€çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½æä¾›ä¸€å®šçš„æŠ—ä¼ªé€ æ”»å‡»èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œé«˜æ–¯é˜´å½±++ä¸ä»…ä¿æŒäº†æ€§èƒ½æ— æŸï¼Œè€Œä¸”åœ¨ç¨³å¥æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä½¿å…¶æˆä¸ºçœŸå®ä¸–ç•Œéƒ¨ç½²çš„æ›´å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15026v1">PDF</a> 18 pages, 8 figures</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ‰©æ•£æ¨¡å‹å®é™…åº”ç”¨ä¸­çš„ç‰ˆæƒä¿æŠ¤ä¸ä¸å½“å†…å®¹ç”Ÿæˆç­‰ä¼¦ç†é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºGaussian Shading++çš„æ¨¡å‹æ°´å°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒé€šé“è®¾è®¡ï¼Œåˆ©ç”¨ä¼ªéšæœºçº é”™ç ç¼–ç æ°´å°éšæœºç§å­ï¼Œå®ç°å›ºå®šæ°´å°å¯†é’¥ä¸‹çš„æ€§èƒ½æ— æŸæ°´å°ï¼Œå¹¶å…‹æœå¯†é’¥ç®¡ç†æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œå¯¹ç”Ÿæˆå’Œåè½¬è¿‡ç¨‹ä¸­å¼•å…¥çš„å¤±çœŸè¿›è¡Œå»ºæ¨¡ï¼Œé‡‡ç”¨æ–°é¢–çš„è½¯å†³ç­–è§£ç ç­–ç•¥ï¼Œç¡®ä¿åœ¨ç”Ÿæˆå‚æ•°å˜åŒ–æ—¶ä»å…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§ã€‚ä¸ºäº†æ”¯æŒç¬¬ä¸‰æ–¹éªŒè¯ï¼Œé›†æˆäº†å…¬é’¥ç­¾åï¼Œå³ä½¿æ¨¡å‹åè½¬èƒ½åŠ›å®Œå…¨å…¬å¼€ï¼Œä¹Ÿèƒ½æä¾›ä¸€å®šç¨‹åº¦çš„é˜²ç¯¡æ”¹æ”»å‡»èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒGaussian Shading++ä¸ä»…ä¿æŒæ€§èƒ½æ— æŸï¼Œè€Œä¸”åœ¨ç¨³å¥æ€§æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæˆä¸ºæ›´é€‚ç”¨äºå®é™…éƒ¨ç½²çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å®é™…åº”ç”¨é¢ä¸´ç‰ˆæƒä¿æŠ¤ä¸å†…å®¹é€‚å½“æ€§æŒ‘æˆ˜ã€‚</li>
<li>æ°´å°åµŒå…¥éœ€ç¡®ä¿ä¸å½±å“æ¨¡å‹æ€§èƒ½ä¸”é€‚åº”ç°å®éƒ¨ç½²çš„å¤æ‚æ€§ã€‚</li>
<li>Gaussian Shading++é‡‡ç”¨åŒé€šé“è®¾è®¡åŠä¼ªéšæœºçº é”™ç å®ç°æ€§èƒ½æ— æŸæ°´å°ã€‚</li>
<li>è¯¥æ–¹æ³•å…‹æœå¯†é’¥ç®¡ç†éš¾é¢˜ï¼Œå¹¶ç¡®ä¿åœ¨ç”Ÿæˆå‚æ•°å˜åŒ–æ—¶ä»å…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡å»ºæ¨¡ç”Ÿæˆå’Œåè½¬è¿‡ç¨‹ä¸­çš„å¤±çœŸï¼Œé‡‡ç”¨è½¯å†³ç­–è§£ç ç­–ç•¥å¢å¼ºç¨³å¥æ€§ã€‚</li>
<li>é›†æˆå…¬é’¥ç­¾åä»¥æ”¯æŒç¬¬ä¸‰æ–¹éªŒè¯ï¼Œå¢å¼ºé˜²ç¯¡æ”¹èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f2350548ac88ea6a46a0442fe55eecb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-513d2a2c003a66c9d4a0143670cf89ba.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PIV-FlowDiffuser-Transfer-learning-based-denoising-diffusion-models-for-PIV"><a href="#PIV-FlowDiffuser-Transfer-learning-based-denoising-diffusion-models-for-PIV" class="headerlink" title="PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for   PIV"></a>PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for   PIV</h2><p><strong>Authors:Qianyu Zhu, Junjie Wang, Jeremiah Hu, Jia Ai, Yong Lee</strong></p>
<p>Deep learning algorithms have significantly reduced the computational time and improved the spatial resolution of particle image velocimetry<del>(PIV). However, the models trained on synthetic datasets might have a degraded performance on practical particle images due to domain gaps. As a result, special residual patterns are often observed for the vector fields of deep learning-based estimators. To reduce the special noise step-by-step, we employ a denoising diffusion model</del>(FlowDiffuser) for PIV analysis. And the data-hungry iterative denoising diffusion model is trained via a transfer learning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1) pre-training a FlowDiffuser model with multiple optical flow datasets of the computer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the pre-trained model on synthetic PIV datasets. Note that the PIV images are upsampled by a factor of two to resolve the small-scale turbulent flow structures. The visualized results indicate that our PIV-FlowDiffuser effectively suppresses the noise patterns. Therefore, the denoising diffusion model reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV baseline on the classic Caiâ€™s dataset. Besides, PIV-FlowDiffuser exhibits enhanced generalization performance on unseen particle images due to transfer learning. Overall, this study highlights the transfer-learning-based denoising diffusion models for PIV. And a detailed implementation is recommended for interested readers in the repository <a target="_blank" rel="noopener" href="https://github.com/Zhu-Qianyu/PIV-FlowDiffuser">https://github.com/Zhu-Qianyu/PIV-FlowDiffuser</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ç®—æ³•æ˜¾è‘—å‡å°‘äº†ç²’å­å›¾åƒæµ‹é€Ÿæ³•ï¼ˆPIVï¼‰çš„è®¡ç®—æ—¶é—´å¹¶æé«˜äº†å…¶ç©ºé—´åˆ†è¾¨ç‡ã€‚ç„¶è€Œï¼Œç”±äºåœ¨ç‰¹å®šé¢†åŸŸå­˜åœ¨çš„å·®è·ï¼Œé‚£äº›åœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å®é™…ç²’å­å›¾åƒä¸Šçš„æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚å› æ­¤ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„ä¼°è®¡å™¨çš„çŸ¢é‡åœºé€šå¸¸ä¼šå‡ºç°ç‰¹æ®Šçš„æ®‹ä½™æ¨¡å¼ã€‚ä¸ºäº†é€æ­¥å‡å°‘ç‰¹æ®Šå™ªå£°ï¼Œæˆ‘ä»¬ä¸ºPIVåˆ†æé‡‡ç”¨äº†ä¸€ç§å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆFlowDiffuserï¼‰ã€‚æ•°æ®æ¸´æ±‚å‹çš„è¿­ä»£å»å™ªæ‰©æ•£æ¨¡å‹é€šè¿‡è¿ç§»å­¦ä¹ ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œä»è€Œå½¢æˆäº†æˆ‘ä»¬çš„PIV-FlowDiffuseræ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œï¼ˆ1ï¼‰ä½¿ç”¨è®¡ç®—æœºè§†è§‰ç¤¾åŒºçš„å¤šä¸ªå…‰å­¦æµåŠ¨æ•°æ®é›†ï¼ˆå¦‚Sintelã€KITTIç­‰ï¼‰å¯¹FlowDiffuseræ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼›ï¼ˆ2ï¼‰åœ¨åˆæˆPIVæ•°æ®é›†ä¸Šå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¯·æ³¨æ„ï¼ŒPIVå›¾åƒè¢«æ”¾å¤§ä¸¤å€ä»¥è§£å†³å°å°ºåº¦æ¹æµç»“æ„çš„é—®é¢˜ã€‚å¯è§†åŒ–ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„PIV-FlowDiffuseræœ‰æ•ˆåœ°æŠ‘åˆ¶äº†å™ªå£°æ¨¡å¼ã€‚å› æ­¤ï¼Œåœ¨ç»å…¸çš„Caiæ•°æ®é›†ä¸Šï¼Œä¸RAFT256-PIVåŸºçº¿ç›¸æ¯”ï¼Œå»å™ªæ‰©æ•£æ¨¡å‹å°†ç»ˆç‚¹è¯¯å·®å¹³å‡å€¼ï¼ˆAEEï¼‰é™ä½äº†59.4%ã€‚æ­¤å¤–ï¼Œç”±äºè¿ç§»å­¦ä¹ ï¼ŒPIV-FlowDiffuseråœ¨æœªè§è¿‡çš„ç²’å­å›¾åƒä¸Šè¡¨ç°å‡ºå¢å¼ºçš„æ³›åŒ–æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶çªå‡ºäº†åŸºäºè¿ç§»å­¦ä¹ çš„å»å™ªæ‰©æ•£æ¨¡å‹åœ¨PIVä¸­çš„åº”ç”¨ã€‚æ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zhu-Qianyu/PIV-FlowDiffuser%E4%BB%93%E5%BA%93%E4%B8%AD%E6%89%BE%E5%88%B0%E8%AF%A6%E7%BB%86%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95%E3%80%82">https://github.com/Zhu-Qianyu/PIV-FlowDiffuserä»“åº“ä¸­æ‰¾åˆ°è¯¦ç»†çš„å®ç°æ–¹æ³•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14952v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ ç®—æ³•æ˜¾è‘—å‡å°‘äº†ç²’å­å›¾åƒæµ‹é€Ÿï¼ˆPIVï¼‰çš„è®¡ç®—æ—¶é—´ï¼Œæé«˜äº†ç©ºé—´åˆ†è¾¨ç‡ã€‚ä½†æ¨¡å‹åœ¨å®ç”¨ç²’å­å›¾åƒä¸Šå› é¢†åŸŸå·®è·å¯èƒ½å­˜åœ¨æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚ä¸ºé€æ­¥å‡å°‘ç‰¹æ®Šå™ªå£°ï¼Œé‡‡ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆFlowDiffuserï¼‰è¿›è¡ŒPIVåˆ†æã€‚é€šè¿‡è¿ç§»å­¦ä¹ ç­–ç•¥è®­ç»ƒæ•°æ®æ¸´æ±‚çš„è¿­ä»£å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œå½¢æˆPIV-FlowDiffuseræ–¹æ³•ã€‚é¢„è®­ç»ƒFlowDiffuseræ¨¡å‹äºè®¡ç®—æœºè§†è§‰ç¤¾åŒºçš„å¤šå…‰æµæ•°æ®é›†ï¼Œå¦‚Sintelã€KITTIç­‰ï¼Œå†å¯¹åˆæˆPIVæ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚PIVå›¾åƒæ”¾å¤§ä¸¤å€ä»¥è§£æå°å°ºåº¦æ¹æµç»“æ„ã€‚å¯è§†åŒ–ç»“æœè¡¨æ˜ï¼ŒPIV-FlowDiffuseræœ‰æ•ˆæŠ‘åˆ¶äº†å™ªå£°æ¨¡å¼ã€‚ç›¸è¾ƒäºRAFT256-PIVåŸºå‡†ï¼ŒPIV-FlowDiffuserå°†ç»ˆç‚¹å¹³å‡è¯¯å·®ï¼ˆAEEï¼‰é™ä½äº†59.4%ã€‚æ­¤å¤–ï¼Œç”±äºè¿ç§»å­¦ä¹ ï¼ŒPIV-FlowDiffuseråœ¨æœªè§è¿‡çš„ç²’å­å›¾åƒä¸Šå±•ç°å‡ºæ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ç®—æ³•åœ¨ç²’å­å›¾åƒæµ‹é€Ÿï¼ˆPIVï¼‰ä¸­æ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡å’Œç©ºé—´åˆ†è¾¨ç‡ã€‚</li>
<li>æ¨¡å‹åœ¨å®ç”¨ç²’å­å›¾åƒä¸Šçš„æ€§èƒ½å¯èƒ½å› é¢†åŸŸå·®è·è€Œä¸‹é™ã€‚</li>
<li>å¼•å…¥å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆFlowDiffuserï¼‰ä»¥å‡å°ç‰¹æ®Šå™ªå£°ã€‚</li>
<li>é‡‡ç”¨è¿ç§»å­¦ä¹ ç­–ç•¥è®­ç»ƒæ•°æ®æ¸´æ±‚çš„FlowDiffuseræ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é¢„è®­ç»ƒåŸºäºè®¡ç®—æœºè§†è§‰ç¤¾åŒºçš„å¤šå…‰æµæ•°æ®é›†ã€‚</li>
<li>PIVå›¾åƒæ”¾å¤§è§£æå°å°ºåº¦æ¹æµç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42d81124c658609d4192dd63d7c76a00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d4118d65a2bd2961475ab8ab965d7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-636492b9526472715891cf9904af92e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2b25184a8b37003d12e7746a04cd3f4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generative-Multimodal-Pretraining-with-Discrete-Diffusion-Timestep-Tokens"><a href="#Generative-Multimodal-Pretraining-with-Discrete-Diffusion-Timestep-Tokens" class="headerlink" title="Generative Multimodal Pretraining with Discrete Diffusion Timestep   Tokens"></a>Generative Multimodal Pretraining with Discrete Diffusion Timestep   Tokens</h2><p><strong>Authors:Kaihang Pan, Wang Lin, Zhongqi Yue, Tenglong Ao, Liyu Jia, Wei Zhao, Juncheng Li, Siliang Tang, Hanwang Zhang</strong></p>
<p>Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify visual comprehension and generation by combining LLM and diffusion models, the state-of-the-art in each task, respectively. Existing approaches rely on spatial visual tokens, where image patches are encoded and arranged according to a spatial order (e.g., raster scan). However, we show that spatial tokens lack the recursive structure inherent to languages, hence form an impossible language for LLM to master. In this paper, we build a proper visual language by leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our proposed tokens recursively compensate for the progressive attribute loss in noisy images as timesteps increase, enabling the diffusion model to reconstruct the original image at any timestep. This approach allows us to effectively integrate the strengths of LLMs in autoregressive reasoning and diffusion models in precise image generation, achieving seamless multimodal comprehension and generation within a unified framework. Extensive experiments show that we achieve superior performance for multimodal comprehension and generation simultaneously compared with other MLLMs. Project Page: <a target="_blank" rel="noopener" href="https://ddt-llama.github.io/">https://DDT-LLaMA.github.io/</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åŠªåŠ›æ—¨åœ¨é€šè¿‡ç»“åˆLLMå’Œæ‰©æ•£æ¨¡å‹ï¼ˆè¿™ä¸¤é¡¹ä»»åŠ¡ä¸­çš„æœ€æ–°æŠ€æœ¯ï¼‰æ¥ç»Ÿä¸€è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚ç°æœ‰çš„æ–¹æ³•ä¾èµ–äºç©ºé—´è§†è§‰ç¬¦å·ï¼Œå…¶ä¸­å›¾åƒè¡¥ä¸è¢«ç¼–ç å¹¶æŒ‰ç©ºé—´é¡ºåºæ’åˆ—ï¼ˆä¾‹å¦‚ï¼Œå…‰æ …æ‰«æï¼‰ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¡¨æ˜ç©ºé—´ç¬¦å·ç¼ºä¹è¯­è¨€å›ºæœ‰çš„é€’å½’ç»“æ„ï¼Œå› æ­¤å¯¹LLMæ¥è¯´å½¢æˆäº†ä¸€ç§æ— æ³•æŒæ¡çš„è¯­è¨€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ‰©æ•£æ—¶é—´æ­¥é•¿æ¥å­¦ä¹ ç¦»æ•£ã€é€’å½’çš„è§†è§‰ç¬¦å·ï¼Œæ„å»ºäº†ä¸€ç§é€‚å½“çš„è§†è§‰è¯­è¨€ã€‚æˆ‘ä»¬æå‡ºçš„ç¬¦å·éšç€æ—¶é—´æ­¥é•¿çš„å¢åŠ ï¼Œé€’å½’åœ°å¼¥è¡¥äº†å™ªå£°å›¾åƒä¸­é€æ¸ä¸§å¤±çš„å±æ€§ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»ä½•æ—¶é—´æ­¥é•¿é‡å»ºåŸå§‹å›¾åƒã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆLLMåœ¨è‡ªå›å½’æ¨ç†å’Œæ‰©æ•£æ¨¡å‹åœ¨ç²¾ç¡®å›¾åƒç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…å®ç°æ— ç¼çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢ä¸å…¶ä»–MLLMç›¸æ¯”å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ddt-llama.github.io/%E3%80%82">https://DDT-LLaMA.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14666v1">PDF</a> Accepted by CVPR 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€æ–°å°è¯•æ—¨åœ¨é€šè¿‡ç»“åˆLLMå’Œæ‰©æ•£æ¨¡å‹ï¼ˆæ¯ä¸ªä»»åŠ¡çš„æœ€æ–°æŠ€æœ¯ï¼‰ï¼Œç»Ÿä¸€è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºç©ºé—´è§†è§‰ç¬¦å·ï¼Œå›¾åƒå—æŒ‰ç©ºé—´é¡ºåºç¼–ç å’Œæ’åˆ—ï¼ˆä¾‹å¦‚ï¼Œæ …æ ¼æ‰«æï¼‰ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ç©ºé—´ç¬¦å·ç¼ºä¹è¯­è¨€å›ºæœ‰çš„é€’å½’ç»“æ„ï¼Œå› æ­¤å½¢æˆäº†ä¸€ç§LLMæ— æ³•æŒæ¡çš„ä¸å¯èƒ½è¯­è¨€ã€‚æœ¬æ–‡åˆ©ç”¨æ‰©æ•£æ—¶é—´æ­¥é•¿æ„å»ºäº†ä¸€ç§é€‚å½“çš„è§†è§‰è¯­è¨€ï¼Œå­¦ä¹ ç¦»æ•£ã€é€’å½’çš„è§†è§‰ç¬¦å·ã€‚æ‰€æå‡ºçš„ç¬¦å·éšç€æ—¶é—´æ­¥é•¿çš„å¢åŠ ï¼Œé€’å½’åœ°å¼¥è¡¥äº†å™ªå£°å›¾åƒä¸­é€æ¸ä¸§å¤±çš„å±æ€§ï¼Œä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»ä½•æ—¶é—´æ­¥é•¿é‡å»ºåŸå§‹å›¾åƒã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°ç»“åˆäº†LLMåœ¨è‡ªåŠ¨æ¨ç†ä¸­çš„ä¼˜åŠ¿å’Œæ‰©æ•£æ¨¡å‹åœ¨ç²¾ç¡®å›¾åƒç”Ÿæˆä¸­çš„ä¼˜åŠ¿ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…å®ç°äº†æ— ç¼çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–MLLMç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢åŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»“åˆäº†LLMå’Œæ‰©æ•£æ¨¡å‹ï¼Œä»¥ç»Ÿä¸€è§†è§‰ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨ç©ºé—´è§†è§‰ç¬¦å·ï¼Œä½†å­˜åœ¨é€’å½’ç»“æ„ç¼ºå¤±çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ—¶é—´æ­¥é•¿çš„è§†è§‰è¯­è¨€ï¼Œé€šè¿‡ç¦»æ•£ã€é€’å½’çš„è§†è§‰ç¬¦å·è§£å†³ç°æœ‰é—®é¢˜ã€‚</li>
<li>ç¬¦å·è®¾è®¡èƒ½éšæ—¶é—´æ­¥é•¿é€’å½’è¡¥å¿å›¾åƒå±æ€§æŸå¤±ï¼Œå®ç°åœ¨ä»»ä½•æ—¶é—´æ­¥é•¿é‡å»ºåŸå§‹å›¾åƒã€‚</li>
<li>æ•´åˆäº†LLMçš„è‡ªåŠ¨æ¨ç†ä¼˜åŠ¿å’Œæ‰©æ•£æ¨¡å‹çš„ç²¾ç¡®å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>åœ¨ç»Ÿä¸€æ¡†æ¶å†…å®ç°äº†æ— ç¼çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢æ€§èƒ½å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a6d03c293404c6fbca63ca2c27df88a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e258b2cd2c5c23306837158a244bffbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fa9dd88a971fa6bbf7e8333b078b7e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-714e6ecb3d9563339be1a57d2b2c1adb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6076b0995841eaab9e6ed03452a227c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SUDO-Enhancing-Text-to-Image-Diffusion-Models-with-Self-Supervised-Direct-Preference-Optimization"><a href="#SUDO-Enhancing-Text-to-Image-Diffusion-Models-with-Self-Supervised-Direct-Preference-Optimization" class="headerlink" title="SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised   Direct Preference Optimization"></a>SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised   Direct Preference Optimization</h2><p><strong>Authors:Liang Peng, Boxi Wu, Haoran Cheng, Yibo Zhao, Xiaofei He</strong></p>
<p>Previous text-to-image diffusion models typically employ supervised fine-tuning (SFT) to enhance pre-trained base models. However, this approach primarily minimizes the loss of mean squared error (MSE) at the pixel level, neglecting the need for global optimization at the image level, which is crucial for achieving high perceptual quality and structural coherence. In this paper, we introduce Self-sUpervised Direct preference Optimization (SUDO), a novel paradigm that optimizes both fine-grained details at the pixel level and global image quality. By integrating direct preference optimization into the model, SUDO generates preference image pairs in a self-supervised manner, enabling the model to prioritize global-level learning while complementing the pixel-level MSE loss. As an effective alternative to supervised fine-tuning, SUDO can be seamlessly applied to any text-to-image diffusion model. Importantly, it eliminates the need for costly data collection and annotation efforts typically associated with traditional direct preference optimization methods. Through extensive experiments on widely-used models, including Stable Diffusion 1.5 and XL, we demonstrate that SUDO significantly enhances both global and local image quality. The codes are provided at \href{<a target="_blank" rel="noopener" href="https://github.com/SPengLiang/SUDO%7D%7Bthis">https://github.com/SPengLiang/SUDO}{this</a> link}. </p>
<blockquote>
<p>ä¹‹å‰çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šå¸¸ä½¿ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥æå‡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸»è¦æœ€å°åŒ–åƒç´ çº§åˆ«çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±ï¼Œå¿½è§†äº†å›¾åƒçº§åˆ«å…¨å±€ä¼˜åŒ–çš„éœ€æ±‚ï¼Œè¿™å¯¹äºå®ç°é«˜æ„ŸçŸ¥è´¨é‡å’Œç»“æ„è¿è´¯æ€§è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæˆ‘ç›‘ç£ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆSUDOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„èŒƒå¼ï¼Œèƒ½å¤Ÿä¼˜åŒ–åƒç´ çº§åˆ«çš„ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€å›¾åƒè´¨é‡ã€‚é€šè¿‡å°†ç›´æ¥åå¥½ä¼˜åŒ–æ•´åˆåˆ°æ¨¡å‹ä¸­ï¼ŒSUDOèƒ½å¤Ÿä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼ç”Ÿæˆåå¥½å›¾åƒå¯¹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¼˜å…ˆå­¦ä¹ å…¨å±€çº§åˆ«çš„çŸ¥è¯†ï¼ŒåŒæ—¶è¡¥å……åƒç´ çº§åˆ«çš„MSEæŸå¤±ã€‚ä½œä¸ºæœ‰ç›‘ç£å¾®è°ƒçš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼ŒSUDOå¯ä»¥æ— ç¼åœ°åº”ç”¨äºä»»ä½•æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚é‡è¦çš„æ˜¯ï¼Œå®ƒæ¶ˆé™¤äº†ä¸ä¼ ç»Ÿç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•ç›¸å…³çš„æ˜‚è´µçš„æ•°æ®æ”¶é›†å’Œæ³¨é‡Šå·¥ä½œã€‚é€šè¿‡å¹¿æ³›ä½¿ç”¨çš„æ¨¡å‹çš„å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬Stable Diffusion 1.5å’ŒXLï¼Œæˆ‘ä»¬è¯æ˜äº†SUDOæ˜¾è‘—æé«˜äº†å…¨å±€å’Œå±€éƒ¨å›¾åƒè´¨é‡ã€‚ç›¸å…³ä»£ç æä¾›åœ¨è¿™ä¸ªé“¾æ¥ï¼š[<a target="_blank" rel="noopener" href="https://github.com/SPengLiang/SUDO]%E3%80%82">https://github.com/SPengLiang/SUDO]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14534v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¼˜åŒ–æ–¹æ³•â€”â€”Self-sUpervised Direct preference Optimizationï¼ˆSUDOï¼‰ã€‚SUDOä¸ä»…èƒ½ä¼˜åŒ–åƒç´ çº§åˆ«çš„ç»†èŠ‚ï¼Œè¿˜èƒ½æå‡å›¾åƒå…¨å±€è´¨é‡ã€‚å®ƒé€šè¿‡è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼ç”Ÿæˆåå¥½å›¾åƒå¯¹ï¼Œä½¿æ¨¡å‹åœ¨å…¨å±€å±‚é¢å­¦ä¹ ï¼ŒåŒæ—¶è¡¥å……åƒç´ çº§çš„å‡æ–¹è¯¯å·®æŸå¤±ã€‚SUDOå¯æ— ç¼åº”ç”¨äºä»»ä½•æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶æ˜¾è‘—æé«˜äº†å›¾åƒçš„å…¨å±€å’Œå±€éƒ¨è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥ä¼˜åŒ–é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œä½†è¿™ç§æ–¹æ³•ä¸»è¦å…³æ³¨åƒç´ çº§çš„å‡æ–¹è¯¯å·®æŸå¤±ï¼Œå¿½è§†äº†å…¨å±€ä¼˜åŒ–çš„é‡è¦æ€§ã€‚</li>
<li>SUDOæ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–èŒƒå¼ï¼Œèƒ½åŒæ—¶ä¼˜åŒ–åƒç´ çº§çš„ç»†èŠ‚å’Œå›¾åƒçš„å…¨å±€è´¨é‡ã€‚</li>
<li>SUDOé€šè¿‡è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼ç”Ÿæˆåå¥½å›¾åƒå¯¹ï¼Œä½¿æ¨¡å‹åœ¨å…¨å±€å±‚é¢å­¦ä¹ ï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>SUDOå¯æ— ç¼é›†æˆåˆ°ä»»ä½•æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½œä¸ºç›‘ç£å¾®è°ƒçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>SUDOæ˜¾è‘—æé«˜äº†å›¾åƒçš„å…¨å±€å’Œå±€éƒ¨è´¨é‡ï¼Œé€šè¿‡å¹¿æ³›ä½¿ç”¨çš„æ¨¡å‹å®éªŒå¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>SUDOé™ä½äº†å¯¹ä¼ ç»Ÿç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•æ‰€éœ€çš„é«˜æ˜‚æ•°æ®é‡‡é›†å’Œæ ‡æ³¨æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9be0bb5d8c0e3ec557470e9f9fa55090.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f722ab514001dd66a7a6b045cb0fdc3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b19ae2f6e6d9a76223875986e3eac52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8f4ed7a5ef66b7da11b4fa9afc79924.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DreamID-High-Fidelity-and-Fast-diffusion-based-Face-Swapping-via-Triplet-ID-Group-Learning"><a href="#DreamID-High-Fidelity-and-Fast-diffusion-based-Face-Swapping-via-Triplet-ID-Group-Learning" class="headerlink" title="DreamID: High-Fidelity and Fast diffusion-based Face Swapping via   Triplet ID Group Learning"></a>DreamID: High-Fidelity and Fast diffusion-based Face Swapping via   Triplet ID Group Learning</h2><p><strong>Authors:Fulong Ye, Miao Hua, Pengze Zhang, Xinghui Li, Qichao Sun, Songtao Zhao, Qian He, Xinglong Wu</strong></p>
<p>In this paper, we introduce DreamID, a diffusion-based face swapping model that achieves high levels of ID similarity, attribute preservation, image fidelity, and fast inference speed. Unlike the typical face swapping training process, which often relies on implicit supervision and struggles to achieve satisfactory results. DreamID establishes explicit supervision for face swapping by constructing Triplet ID Group data, significantly enhancing identity similarity and attribute preservation. The iterative nature of diffusion models poses challenges for utilizing efficient image-space loss functions, as performing time-consuming multi-step sampling to obtain the generated image during training is impractical. To address this issue, we leverage the accelerated diffusion model SD Turbo, reducing the inference steps to a single iteration, enabling efficient pixel-level end-to-end training with explicit Triplet ID Group supervision. Additionally, we propose an improved diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter. This robust architecture fully unlocks the power of the Triplet ID Group explicit supervision. Finally, to further extend our method, we explicitly modify the Triplet ID Group data during training to fine-tune and preserve specific attributes, such as glasses and face shape. Extensive experiments demonstrate that DreamID outperforms state-of-the-art methods in terms of identity similarity, pose and expression preservation, and image fidelity. Overall, DreamID achieves high-quality face swapping results at 512*512 resolution in just 0.6 seconds and performs exceptionally well in challenging scenarios such as complex lighting, large angles, and occlusions. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DreamIDï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºæ‰©æ•£æŠ€æœ¯çš„æ¢è„¸æ¨¡å‹ï¼Œå®ç°äº†é«˜æ°´å¹³çš„èº«ä»½ç›¸ä¼¼æ€§ã€å±æ€§ä¿ç•™ã€å›¾åƒä¿çœŸåº¦å’Œå¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚ä¸é€šå¸¸ä¾èµ–éšå¼ç›‘ç£ä¸”éš¾ä»¥è¾¾åˆ°æ»¡æ„ç»“æœçš„æ¢è„¸è®­ç»ƒè¿‡ç¨‹ä¸åŒï¼ŒDreamIDé€šè¿‡æ„å»ºä¸‰å…ƒç»„IDç»„æ•°æ®å®ç°äº†æ¢è„¸çš„æ˜¾å¼ç›‘ç£ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†èº«ä»½ç›¸ä¼¼æ€§å’Œå±æ€§ä¿ç•™ã€‚æ‰©æ•£æ¨¡å‹çš„è¿­ä»£æ€§è´¨ç»™åˆ©ç”¨é«˜æ•ˆçš„å›¾åƒç©ºé—´æŸå¤±å‡½æ•°å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨è®­ç»ƒæœŸé—´é€šè¿‡è€—æ—¶å¤šæ­¥é‡‡æ ·æ¥è·å¾—ç”Ÿæˆå›¾åƒæ˜¯ä¸åˆ‡å®é™…çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†åŠ é€Ÿæ‰©æ•£æ¨¡å‹SD Turboï¼Œå°†æ¨ç†æ­¥éª¤å‡å°‘åˆ°å•æ¬¡è¿­ä»£ï¼Œèƒ½å¤Ÿåœ¨æ˜¾å¼ä¸‰å…ƒç»„IDç»„ç›‘ç£ä¸‹è¿›è¡Œé«˜æ•ˆçš„åƒç´ çº§ç«¯åˆ°ç«¯è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„åŸºäºæ‰©æ•£çš„æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬SwapNetã€FaceNetå’ŒIDé€‚é…å™¨ã€‚è¿™ä¸€ç¨³å¥çš„æ¶æ„å……åˆ†é‡Šæ”¾äº†ä¸‰å…ƒç»„IDç»„æ˜¾å¼ç›‘ç£çš„å¨åŠ›ã€‚æœ€åï¼Œä¸ºäº†è¿›ä¸€æ­¥å®Œå–„æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾å¼ä¿®æ”¹äº†ä¸‰å…ƒç»„IDç»„æ•°æ®ï¼Œä»¥å¾®è°ƒå¹¶ä¿ç•™ç‰¹å®šå±æ€§ï¼Œå¦‚çœ¼é•œå’Œè„¸å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDreamIDåœ¨èº«ä»½ç›¸ä¼¼æ€§ã€å§¿åŠ¿å’Œè¡¨æƒ…ä¿ç•™ä»¥åŠå›¾åƒä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚æ€»ä½“è€Œè¨€ï¼ŒDreamIDåœ¨512*512åˆ†è¾¨ç‡ä¸‹å®ç°äº†é«˜è´¨é‡æ¢è„¸ï¼Œä»…éœ€0.6ç§’ï¼Œä¸”åœ¨å¤æ‚å…‰ç…§ã€å¤§è§’åº¦å’Œé®æŒ¡ç­‰æŒ‘æˆ˜åœºæ™¯ä¸‹è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14509v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨æ›¿æ¢æ¨¡å‹DreamIDï¼Œå®ƒå®ç°äº†é«˜èº«ä»½ç›¸ä¼¼æ€§ã€å±æ€§ä¿ç•™ã€å›¾åƒä¿çœŸåº¦å’Œå¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚DreamIDé€šè¿‡æ„å»ºTriplet ID Groupæ•°æ®å®ç°æ˜¾å¼ç›‘ç£ï¼Œæ˜¾è‘—æé«˜èº«ä»½ç›¸ä¼¼æ€§å’Œå±æ€§ä¿ç•™ã€‚ä¸ºè§£å†³æ‰©æ•£æ¨¡å‹çš„è¿­ä»£æ€§è´¨å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œé‡‡ç”¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹SD Turboï¼Œå°†æ¨ç†æ­¥éª¤å‡å°‘åˆ°å•æ¬¡è¿­ä»£ï¼Œå®ç°é«˜æ•ˆçš„åƒç´ çº§ç«¯åˆ°ç«¯è®­ç»ƒã€‚æ­¤å¤–ï¼Œæå‡ºæ”¹è¿›çš„åŸºäºæ‰©æ•£çš„æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬SwapNetã€FaceNetå’ŒIDé€‚é…å™¨ï¼Œå……åˆ†åˆ©ç”¨Triplet ID Groupçš„æ˜¾å¼ç›‘ç£ã€‚æœ€åï¼Œé€šè¿‡è®­ç»ƒæ—¶æ˜¾å¼ä¿®æ”¹Triplet ID Groupæ•°æ®ï¼Œå®ç°å¯¹ç‰¹å®šå±æ€§ï¼ˆå¦‚çœ¼é•œå’Œè„¸å‹ï¼‰çš„ç²¾ç»†è°ƒæ•´å’Œä¿ç•™ã€‚DreamIDåœ¨èº«ä»½ç›¸ä¼¼æ€§ã€å§¿åŠ¿å’Œè¡¨æƒ…ä¿ç•™ã€å›¾åƒä¿çœŸåº¦ç­‰æ–¹é¢è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ•ˆæœï¼Œåœ¨å¤æ‚åœºæ™¯ï¼ˆå¦‚å¤æ‚å…‰ç…§ã€å¤§è§’åº¦é®æŒ¡ï¼‰ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DreamIDæ˜¯åŸºäºæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨æ›¿æ¢æ¨¡å‹ï¼Œå®ç°äº†é«˜èº«ä»½ç›¸ä¼¼æ€§ã€å±æ€§ä¿ç•™ã€å›¾åƒä¿çœŸåº¦å’Œå¿«é€Ÿæ¨ç†é€Ÿåº¦ã€‚</li>
<li>DreamIDé€šè¿‡æ„å»ºTriplet ID Groupæ•°æ®å®ç°æ˜¾å¼ç›‘ç£ï¼Œæé«˜èº«ä»½ç›¸ä¼¼æ€§å’Œå±æ€§ä¿ç•™ã€‚</li>
<li>é‡‡ç”¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹SD Turboï¼Œå‡å°‘æ¨ç†æ­¥éª¤ï¼Œå®ç°é«˜æ•ˆåƒç´ çº§ç«¯åˆ°ç«¯è®­ç»ƒã€‚</li>
<li>æ”¹è¿›çš„åŸºäºæ‰©æ•£çš„æ¨¡å‹æ¶æ„åŒ…æ‹¬SwapNetã€FaceNetå’ŒIDé€‚é…å™¨ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å¼ç›‘ç£ã€‚</li>
<li>DreamIDé€šè¿‡ä¿®æ”¹Triplet ID Groupæ•°æ®åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ç‰¹å®šå±æ€§çš„ç²¾ç»†è°ƒæ•´å’Œä¿ç•™ã€‚</li>
<li>DreamIDåœ¨å¤šä¸ªæ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¦‚èº«ä»½ç›¸ä¼¼æ€§ã€å§¿åŠ¿å’Œè¡¨æƒ…ä¿ç•™ã€å›¾åƒä¿çœŸåº¦ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6059c8096db9c285a59b97acbfe71111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9402ddded452656899cd85d3c6729e4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3df1305be8367a6cc18f035376700d60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6ff863f76a239227b04663dd2b57c71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83efa0a045974afbd481f4861decccf.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SphereDiff-Tuning-free-Omnidirectional-Panoramic-Image-and-Video-Generation-via-Spherical-Latent-Representation"><a href="#SphereDiff-Tuning-free-Omnidirectional-Panoramic-Image-and-Video-Generation-via-Spherical-Latent-Representation" class="headerlink" title="SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video   Generation via Spherical Latent Representation"></a>SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video   Generation via Spherical Latent Representation</h2><p><strong>Authors:Minho Park, Taewoong Kang, Jooyeol Yun, Sungwon Hwang, Jaegul Choo</strong></p>
<p>The increasing demand for AR&#x2F;VR applications has highlighted the need for high-quality 360-degree panoramic content. However, generating high-quality 360-degree panoramic images and videos remains a challenging task due to the severe distortions introduced by equirectangular projection (ERP). Existing approaches either fine-tune pretrained diffusion models on limited ERP datasets or attempt tuning-free methods that still rely on ERP latent representations, leading to discontinuities near the poles. In this paper, we introduce SphereDiff, a novel approach for seamless 360-degree panoramic image and video generation using state-of-the-art diffusion models without additional tuning. We define a spherical latent representation that ensures uniform distribution across all perspectives, mitigating the distortions inherent in ERP. We extend MultiDiffusion to spherical latent space and propose a spherical latent sampling method to enable direct use of pretrained diffusion models. Moreover, we introduce distortion-aware weighted averaging to further improve the generation quality in the projection process. Our method outperforms existing approaches in generating 360-degree panoramic content while maintaining high fidelity, making it a robust solution for immersive AR&#x2F;VR applications. The code is available here. <a target="_blank" rel="noopener" href="https://github.com/pmh9960/SphereDiff">https://github.com/pmh9960/SphereDiff</a> </p>
<blockquote>
<p>éšç€AR&#x2F;VRåº”ç”¨çš„æ—¥ç›Šå¢é•¿ï¼Œå¯¹é«˜è´¨é‡360åº¦å…¨æ™¯å†…å®¹çš„éœ€æ±‚ä¹Ÿæ—¥ç›Šå‡¸æ˜¾ã€‚ç„¶è€Œï¼Œç”±äºç­‰è·æŠ•å½±ï¼ˆERPï¼‰å¼•å…¥çš„ä¸¥é‡å¤±çœŸï¼Œç”Ÿæˆé«˜è´¨é‡360åº¦å…¨æ™¯å›¾åƒå’Œè§†é¢‘ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥é€‚åº”æœ‰é™çš„ERPæ•°æ®é›†ï¼Œè¦ä¹ˆå°è¯•æ— éœ€è°ƒæ•´çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä»ç„¶ä¾èµ–äºERPæ½œåœ¨è¡¨ç¤ºï¼Œå¯¼è‡´æåœ°çš„é™„è¿‘å‡ºç°ä¸è¿ç»­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SphereDiffï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æœ€æ–°æ‰©æ•£æ¨¡å‹æ— ç¼ç”Ÿæˆ360åº¦å…¨æ™¯å›¾åƒå’Œè§†é¢‘çš„æ–°æ–¹æ³•ï¼Œæ— éœ€é¢å¤–è°ƒæ•´ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªçƒå½¢æ½œåœ¨è¡¨ç¤ºï¼Œç¡®ä¿æ‰€æœ‰è§’åº¦çš„å‡åŒ€åˆ†å¸ƒï¼Œä»¥ç¼“è§£ERPå›ºæœ‰çš„å¤±çœŸã€‚æˆ‘ä»¬å°†MultiDiffusionæ‰©å±•åˆ°çƒå½¢æ½œåœ¨ç©ºé—´ï¼Œå¹¶æå‡ºä¸€ç§çƒå½¢æ½œåœ¨é‡‡æ ·æ–¹æ³•ï¼Œä»¥ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¤±çœŸæ„ŸçŸ¥åŠ æƒå¹³å‡ï¼Œä»¥è¿›ä¸€æ­¥æ”¹è¿›æŠ•å½±è¿‡ç¨‹ä¸­çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆ360åº¦å…¨æ™¯å†…å®¹æ—¶ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†é«˜ä¿çœŸåº¦ï¼Œæˆä¸ºæ²‰æµ¸å¼AR&#x2F;VRåº”ç”¨çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/pmh9960/SphereDiff%E3%80%82">https://github.com/pmh9960/SphereDiffã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14396v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¢å¼ºç°å®ï¼ˆARï¼‰&#x2F;è™šæ‹Ÿç°å®ï¼ˆVRï¼‰åº”ç”¨éœ€æ±‚ä¸æ–­å¢åŠ çš„èƒŒæ™¯ä¸‹ï¼Œé«˜è´¨é‡çš„å…¨æ™¯å†…å®¹éœ€æ±‚å‡¸æ˜¾ã€‚ç„¶è€Œï¼Œç”Ÿæˆé«˜è´¨é‡çš„å…¨æ™¯å›¾åƒå’Œè§†é¢‘å› ç­‰è·æŠ•å½±ï¼ˆERPï¼‰å¼•èµ·çš„ä¸¥é‡æ‰­æ›²è€Œé¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¦ä¹ˆå°è¯•æ— è°ƒå‚æ–¹æ³•ï¼Œä½†éƒ½ä¾èµ–äºERPæ½œåœ¨è¡¨ç¤ºï¼Œå¯¼è‡´æç‚¹é™„è¿‘å‡ºç°ä¸è¿ç»­ã€‚æœ¬æ–‡æå‡ºSphereDiffï¼Œåˆ©ç”¨æœ€æ–°çš„æ‰©æ•£æ¨¡å‹æ— ç¼ç”Ÿæˆå…¨æ™¯å›¾åƒå’Œè§†é¢‘çš„æ–°æ–¹æ³•ï¼Œæ— éœ€é¢å¤–è°ƒæ•´ã€‚é€šè¿‡å®šä¹‰çƒå½¢æ½œåœ¨è¡¨ç¤ºï¼Œç¡®ä¿æ‰€æœ‰è§†è§’çš„å‡åŒ€åˆ†å¸ƒï¼Œå‡è½»ERPå›ºæœ‰çš„æ‰­æ›²ã€‚æ‰©å±•åˆ°çƒå½¢æ½œåœ¨ç©ºé—´å¹¶æå‡ºçƒå½¢æ½œåœ¨é‡‡æ ·æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¼•å…¥å¤±çœŸæ„ŸçŸ¥åŠ æƒå¹³å‡ï¼Œè¿›ä¸€æ­¥æ”¹è¿›æŠ•å½±è¿‡ç¨‹ä¸­çš„ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå…¨æ™¯å†…å®¹æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸï¼Œæˆä¸ºæ²‰æµ¸å¼AR&#x2F;VRåº”ç”¨çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AR&#x2F;VRåº”ç”¨éœ€æ±‚å¢åŠ ï¼Œéœ€è¦é«˜è´¨é‡çš„å…¨æ™¯å†…å®¹ã€‚</li>
<li>ç”Ÿæˆé«˜è´¨é‡å…¨æ™¯å›¾åƒå’Œè§†é¢‘é¢ä¸´ç­‰è·æŠ•å½±ï¼ˆERPï¼‰å¼•èµ·çš„ä¸¥é‡æ‰­æ›²çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºERPæ½œåœ¨è¡¨ç¤ºï¼Œå¯¼è‡´æç‚¹é™„è¿‘çš„ä¸è¿ç»­æ€§ã€‚</li>
<li>SphereDiffåˆ©ç”¨æœ€æ–°çš„æ‰©æ•£æ¨¡å‹æå‡ºä¸€ç§æ— ç¼ç”Ÿæˆå…¨æ™¯å›¾åƒå’Œè§†é¢‘çš„æ–¹æ³•ï¼Œæ— éœ€é¢å¤–è°ƒæ•´ã€‚</li>
<li>å®šä¹‰çƒå½¢æ½œåœ¨è¡¨ç¤ºï¼Œç¡®ä¿æ‰€æœ‰è§†è§’çš„å‡åŒ€åˆ†å¸ƒï¼Œå‡è½»ERPçš„æ‰­æ›²ã€‚</li>
<li>æ‰©å±•åˆ°çƒå½¢æ½œåœ¨ç©ºé—´ï¼Œæå‡ºçƒå½¢æ½œåœ¨é‡‡æ ·æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å¼•å…¥å¤±çœŸæ„ŸçŸ¥åŠ æƒå¹³å‡ï¼Œæ”¹è¿›æŠ•å½±è¿‡ç¨‹ä¸­çš„ç”Ÿæˆè´¨é‡ï¼Œä½¿æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-608348db7d3c45dd4a6ad9e4d7bd62c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-663c0c728e505ca7f9dbcac973e24d35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d56166a5596724dc910452216e69d17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a648d197dd2a250d449e1fc1cf12b48a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baa25e319a77489e9ef9398675c4f765.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5815ecae6cd5d4b0320de090d3e1fbeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ee99187932e0301bc520f605e182dbb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="From-Missing-Pieces-to-Masterpieces-Image-Completion-with-Context-Adaptive-Diffusion"><a href="#From-Missing-Pieces-to-Masterpieces-Image-Completion-with-Context-Adaptive-Diffusion" class="headerlink" title="From Missing Pieces to Masterpieces: Image Completion with   Context-Adaptive Diffusion"></a>From Missing Pieces to Masterpieces: Image Completion with   Context-Adaptive Diffusion</h2><p><strong>Authors:Pourya Shamsolmoali, Masoumeh Zareapoor, Huiyu Zhou, Michael Felsberg, Dacheng Tao, Xuelong Li</strong></p>
<p>Image completion is a challenging task, particularly when ensuring that generated content seamlessly integrates with existing parts of an image. While recent diffusion models have shown promise, they often struggle with maintaining coherence between known and unknown (missing) regions. This issue arises from the lack of explicit spatial and semantic alignment during the diffusion process, resulting in content that does not smoothly integrate with the original image. Additionally, diffusion models typically rely on global learned distributions rather than localized features, leading to inconsistencies between the generated and existing image parts. In this work, we propose ConFill, a novel framework that introduces a Context-Adaptive Discrepancy (CAD) model to ensure that intermediate distributions of known and unknown regions are closely aligned throughout the diffusion process. By incorporating CAD, our model progressively reduces discrepancies between generated and original images at each diffusion step, leading to contextually aligned completion. Moreover, ConFill uses a new Dynamic Sampling mechanism that adaptively increases the sampling rate in regions with high reconstruction complexity. This approach enables precise adjustments, enhancing detail and integration in restored areas. Extensive experiments demonstrate that ConFill outperforms current methods, setting a new benchmark in image completion. </p>
<blockquote>
<p>å›¾åƒè¡¥å…¨æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡®ä¿ç”Ÿæˆçš„å†…å®¹æ— ç¼é›†æˆåˆ°å›¾åƒç°æœ‰éƒ¨åˆ†æ—¶ã€‚è™½ç„¶æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸éš¾ä»¥åœ¨å·²çŸ¥å’ŒæœªçŸ¥ï¼ˆç¼ºå¤±ï¼‰åŒºåŸŸä¹‹é—´ä¿æŒè¿è´¯æ€§ã€‚è¿™ä¸€é—®é¢˜æºäºæ‰©æ•£è¿‡ç¨‹ä¸­ç¼ºä¹æ˜ç¡®çš„ç©ºé—´å’Œè¯­ä¹‰å¯¹é½ï¼Œå¯¼è‡´å†…å®¹æ— æ³•å¹³ç¨³åœ°èå…¥åŸå§‹å›¾åƒã€‚æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹é€šå¸¸ä¾èµ–äºå…¨å±€å­¦ä¹ åˆ†å¸ƒè€Œéå±€éƒ¨ç‰¹å¾ï¼Œå¯¼è‡´ç”Ÿæˆå›¾åƒéƒ¨åˆ†ä¸ç°æœ‰å›¾åƒéƒ¨åˆ†ä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´ã€‚</p>
</blockquote>
<p>åœ¨æœ¬å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ConFillï¼Œä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå¼•å…¥ä¸Šä¸‹æ–‡è‡ªé€‚åº”å·®å¼‚ï¼ˆCADï¼‰æ¨¡å‹ï¼Œä»¥ç¡®ä¿å·²çŸ¥å’ŒæœªçŸ¥åŒºåŸŸçš„ä¸­é—´åˆ†å¸ƒåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ç´§å¯†å¯¹é½ã€‚é€šè¿‡ç»“åˆCADï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¯ä¸€æ­¥æ‰©æ•£ä¸­é€æ¸å‡å°‘äº†ç”Ÿæˆå›¾åƒå’ŒåŸå§‹å›¾åƒä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œå®ç°ä¸Šä¸‹æ–‡å¯¹é½çš„è¡¥å…¨ã€‚æ­¤å¤–ï¼ŒConFillä½¿ç”¨ä¸€ç§æ–°çš„åŠ¨æ€é‡‡æ ·æœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°åœ¨é«˜é‡å»ºå¤æ‚åº¦çš„åŒºåŸŸå¢åŠ é‡‡æ ·ç‡ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå®ç°ç²¾ç¡®è°ƒæ•´ï¼Œå¢å¼ºæ¢å¤åŒºåŸŸçš„ç»†èŠ‚å’Œé›†æˆåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒConFillä¼˜äºå½“å‰æ–¹æ³•ï¼Œåœ¨å›¾åƒè¡¥å…¨æ–¹é¢æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14294v1">PDF</a> Accepted in TPAMI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºConFillçš„æ–°æ¡†æ¶ï¼Œç”¨äºå›¾åƒè¡¥å…¨ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä¸Šä¸‹æ–‡è‡ªé€‚åº”å·®å¼‚æ¨¡å‹ï¼Œç¡®ä¿å·²çŸ¥å’ŒæœªçŸ¥åŒºåŸŸçš„ä¸­é—´åˆ†å¸ƒåœ¨æ•´ä¸ªæ‰©æ•£è¿‡ç¨‹ä¸­ç´§å¯†å¯¹é½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜é‡‡ç”¨åŠ¨æ€é‡‡æ ·æœºåˆ¶ï¼Œåœ¨é‡å»ºå¤æ‚åº¦é«˜çš„åŒºåŸŸè‡ªé€‚åº”å¢åŠ é‡‡æ ·ç‡ï¼Œä»¥æé«˜æ¢å¤åŒºåŸŸçš„ç»†èŠ‚å’Œé›†æˆåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒConFillåœ¨å›¾åƒè¡¥å…¨ä»»åŠ¡ä¸Šä¼˜äºå½“å‰æ–¹æ³•ï¼Œæ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ConFillæ¡†æ¶è¢«æå‡ºç”¨äºè§£å†³å›¾åƒè¡¥å…¨ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç¡®ä¿ç”Ÿæˆå†…å®¹ä¸ç°æœ‰å›¾åƒéƒ¨åˆ†çš„æ— ç¼é›†æˆã€‚</li>
<li>ä¸Šä¸‹æ–‡è‡ªé€‚åº”å·®å¼‚æ¨¡å‹ï¼ˆCADï¼‰è¢«å¼•å…¥ï¼Œä»¥ç¡®ä¿å·²çŸ¥å’ŒæœªçŸ¥åŒºåŸŸçš„ä¸­é—´åˆ†å¸ƒåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„ç´§å¯†å¯¹é½ã€‚</li>
<li>ConFillé€šè¿‡é‡‡ç”¨åŠ¨æ€é‡‡æ ·æœºåˆ¶ï¼Œåœ¨é‡å»ºå¤æ‚åº¦é«˜çš„åŒºåŸŸè‡ªé€‚åº”å¢åŠ é‡‡æ ·ç‡ï¼Œæé«˜æ¢å¤åŒºåŸŸçš„ç»†èŠ‚å’Œé›†æˆåº¦ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å‡å°‘ç”Ÿæˆå›¾åƒå’ŒåŸå§‹å›¾åƒä¹‹é—´çš„å·®å¼‚ï¼Œå®ç°äº†ä¸Šä¸‹æ–‡å¯¹é½çš„è¡¥å…¨ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒConFillåœ¨å›¾åƒè¡¥å…¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ConFillçš„å¼•å…¥ä¸ºå›¾åƒè¡¥å…¨ä»»åŠ¡æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d60eb27e86998fc391c63e9b239bfeb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dc469fa8056e473173698d0d35690f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2549e82fa6e9fbe0ccba134c4f292343.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Text-Audio-Visual-conditioned-Diffusion-Model-for-Video-Saliency-Prediction"><a href="#Text-Audio-Visual-conditioned-Diffusion-Model-for-Video-Saliency-Prediction" class="headerlink" title="Text-Audio-Visual-conditioned Diffusion Model for Video Saliency   Prediction"></a>Text-Audio-Visual-conditioned Diffusion Model for Video Saliency   Prediction</h2><p><strong>Authors:Li Yu, Xuanzhe Sun, Wei Zhou, Moncef Gabbouj</strong></p>
<p>Video saliency prediction is crucial for downstream applications, such as video compression and human-computer interaction. With the flourishing of multimodal learning, researchers started to explore multimodal video saliency prediction, including audio-visual and text-visual approaches. Auditory cues guide the gaze of viewers to sound sources, while textual cues provide semantic guidance for understanding video content. Integrating these complementary cues can improve the accuracy of saliency prediction. Therefore, we attempt to simultaneously analyze visual, auditory, and textual modalities in this paper, and propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video saliency prediction. TAVDiff treats video saliency prediction as an image generation task conditioned on textual, audio, and visual inputs, and predicts saliency maps through stepwise denoising. To effectively utilize text, a large multimodal model is used to generate textual descriptions for video frames and introduce a saliency-oriented image-text response (SITR) mechanism to generate image-text response maps. It is used as conditional information to guide the model to localize the visual regions that are semantically related to the textual description. Regarding the auditory modality, it is used as another conditional information for directing the model to focus on salient regions indicated by sounds. At the same time, since the diffusion transformer (DiT) directly concatenates the conditional information with the timestep, which may affect the estimation of the noise level. To achieve effective conditional guidance, we propose Saliency-DiT, which decouples the conditional information from the timestep. Experimental results show that TAVDiff outperforms existing methods, improving 1.03%, 2.35%, 2.71% and 0.33% on SIM, CC, NSS and AUC-J metrics, respectively. </p>
<blockquote>
<p>è§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹å¯¹äºä¸‹æ¸¸åº”ç”¨è‡³å…³é‡è¦ï¼Œä¾‹å¦‚è§†é¢‘å‹ç¼©å’Œäººæœºäº¤äº’ã€‚éšç€å¤šæ¨¡æ€å­¦ä¹ çš„è“¬å‹ƒå‘å±•ï¼Œç ”ç©¶è€…å¼€å§‹æ¢ç´¢å¤šæ¨¡æ€è§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹ï¼ŒåŒ…æ‹¬è§†å¬å’Œæ–‡æœ¬è§†è§‰æ–¹æ³•ã€‚å¬è§‰çº¿ç´¢å¼•å¯¼è§‚ä¼—çš„ç›®å…‰æŒ‡å‘å£°æºï¼Œè€Œæ–‡æœ¬çº¿ç´¢ä¸ºç†è§£è§†é¢‘å†…å®¹æä¾›è¯­ä¹‰æŒ‡å¯¼ã€‚æ•´åˆè¿™äº›äº’è¡¥çº¿ç´¢å¯ä»¥æé«˜æ˜¾è‘—æ€§é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡å°è¯•åŒæ—¶åˆ†æè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œå¹¶æå‡ºTAVDiffï¼Œä¸€ç§ç”¨äºè§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹çš„æ–‡-éŸ³-è§†è§‰æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚TAVDiffå°†è§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹è§†ä¸ºä¸€é¡¹å—æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰è¾“å…¥å½±å“çš„å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¹¶é€šè¿‡é€æ­¥å»å™ªæ¥é¢„æµ‹æ˜¾è‘—æ€§åœ°å›¾ã€‚ä¸ºäº†æœ‰æ•ˆåˆ©ç”¨æ–‡æœ¬ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ¥ç”Ÿæˆè§†é¢‘å¸§çš„æ–‡æœ¬æè¿°ï¼Œå¹¶å¼•å…¥é¢å‘æ˜¾è‘—æ€§çš„å›¾åƒæ–‡æœ¬å“åº”ï¼ˆSITRï¼‰æœºåˆ¶ï¼Œä»¥ç”Ÿæˆå›¾åƒæ–‡æœ¬å“åº”å›¾ã€‚å®ƒä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼ŒæŒ‡å¯¼æ¨¡å‹å®šä½ä¸æ–‡æœ¬æè¿°è¯­ä¹‰ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚å¯¹äºå¬è§‰æ¨¡æ€ï¼Œå®ƒä½œä¸ºå¦ä¸€ç§æ¡ä»¶ä¿¡æ¯ï¼ŒæŒ‡å¯¼æ¨¡å‹å…³æ³¨å£°éŸ³æŒ‡ç¤ºçš„æ˜¾è‘—åŒºåŸŸã€‚åŒæ—¶ï¼Œç”±äºæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ç›´æ¥å°†æ¡ä»¶ä¿¡æ¯ä¸æ—¶é—´æ­¥é•¿è¿æ¥èµ·æ¥ï¼Œå¯èƒ½ä¼šå½±å“å™ªå£°æ°´å¹³çš„ä¼°è®¡ã€‚ä¸ºäº†å®ç°æœ‰æ•ˆçš„æ¡ä»¶æŒ‡å¯¼ï¼Œæˆ‘ä»¬æå‡ºäº†Saliency-DiTï¼Œå®ƒå°†æ¡ä»¶ä¿¡æ¯ä¸æ—¶é—´æ­¥é•¿è§£è€¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAVDiffåœ¨SIMã€CCã€NSSå’ŒAUC-JæŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†1.03%ã€2.35%ã€2.71%å’Œ0.33%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14267v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹çš„å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬éŸ³é¢‘è§†è§‰å’Œæ–‡æœ¬è§†è§‰æ–¹æ³•ã€‚é€šè¿‡æ•´åˆè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTAVDiffçš„æ–‡æœ¬éŸ³é¢‘è§†è§‰æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹ã€‚TAVDiffå°†è§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹è§†ä¸ºåŸºäºæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰è¾“å…¥çš„å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¹¶é€šè¿‡é€æ­¥å»å™ªé¢„æµ‹æ˜¾è‘—æ€§åœ°å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAVDiffåœ¨SIMã€CCã€NSSå’ŒAUC-JæŒ‡æ ‡ä¸Šçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†1.03ï¼…ã€2.35ï¼…ã€2.71ï¼…å’Œ0.33ï¼…ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹æ˜¯è§†é¢‘å‹ç¼©å’Œäººæœºäº¤äº’ç­‰ä¸‹æ¸¸åº”ç”¨çš„å…³é”®ã€‚</li>
<li>å¤šæ¨¡æ€å­¦ä¹ ï¼ˆåŒ…æ‹¬éŸ³é¢‘è§†è§‰å’Œæ–‡æœ¬è§†è§‰æ–¹æ³•ï¼‰åœ¨è§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å¬è§‰å’Œæ–‡æœ¬çº¿ç´¢èƒ½æé«˜æ˜¾è‘—æ€§é¢„æµ‹çš„ç²¾åº¦ã€‚</li>
<li>TAVDiffæ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹ã€‚</li>
<li>TAVDiffå°†è§†é¢‘æ˜¾è‘—æ€§é¢„æµ‹è§†ä¸ºåŸºäºå¤šæ¨¡æ€è¾“å…¥çš„å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>Saliency-DiTè¢«æå‡ºä»¥è§£å†³æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰åœ¨æ¡ä»¶ä¿¡æ¯å¤„ç†ä¸Šçš„é—®é¢˜ï¼Œå®ç°æ›´æœ‰æ•ˆçš„æ¡ä»¶å¼•å¯¼ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTAVDiffåœ¨å¤šä¸ªè¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-deb73cacc763e412cad391b1b5e90576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29b39f0531d7b21ab978a4ca3e4d48c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89672fbc8e0f94ca1ef2dc78c9df1aac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bd7e5be2c749ec86c6c3aa0080c6619.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4458a0c540335f67374cdbdc9d0b391a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Learning-Joint-ID-Textual-Representation-for-ID-Preserving-Image-Synthesis"><a href="#Learning-Joint-ID-Textual-Representation-for-ID-Preserving-Image-Synthesis" class="headerlink" title="Learning Joint ID-Textual Representation for ID-Preserving Image   Synthesis"></a>Learning Joint ID-Textual Representation for ID-Preserving Image   Synthesis</h2><p><strong>Authors:Zichuan Liu, Liming Jiang, Qing Yan, Yumin Jia, Hao Kang, Xin Lu</strong></p>
<p>We propose a novel framework for ID-preserving generation using a multi-modal encoding strategy rather than injecting identity features via adapters into pre-trained models. Our method treats identity and text as a unified conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal encoder that learns a joint embedding space for both identity and textual semantics. Given a reference face and a text prompt, FaceCLIP produces a unified representation that encodes both identity and text, which conditions a base diffusion model to generate images that are identity-consistent and text-aligned. We also present a multi-modal alignment algorithm to train FaceCLIP, using a loss that aligns its joint representation with face, text, and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL). Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait generation with better identity preservation and textual relevance. Extensive experiments demonstrate its quantitative and qualitative superiority. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„IDä¿ç•™ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨å¤šæ¨¡æ€ç¼–ç ç­–ç•¥ï¼Œè€Œä¸æ˜¯é€šè¿‡é€‚é…å™¨å‘é¢„è®­ç»ƒæ¨¡å‹æ³¨å…¥èº«ä»½ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†èº«ä»½å’Œæ–‡æœ¬è§†ä¸ºç»Ÿä¸€çš„æ¡ä»¶è¾“å…¥ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†FaceCLIPï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå®ƒå­¦ä¹ èº«ä»½å’Œæ–‡æœ¬è¯­ä¹‰çš„è”åˆåµŒå…¥ç©ºé—´ã€‚ç»™å®šå‚è€ƒé¢éƒ¨å’Œæ–‡æœ¬æç¤ºï¼ŒFaceCLIPç”Ÿæˆä¸€ä¸ªç»Ÿä¸€è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºå¯¹èº«ä»½å’Œæ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œä½¿åŸºç¡€æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆèº«ä»½ä¸€è‡´ä¸”æ–‡æœ¬å¯¹é½çš„å›¾åƒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä½¿ç”¨æŸå¤±å‡½æ•°å¯¹é½å…¶è”åˆè¡¨ç¤ºä¸é¢éƒ¨ã€æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç©ºé—´çš„FaceCLIPå¤šæ¨¡æ€å¯¹é½ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬å°†FaceCLIPä¸Stable Diffusion XLï¼ˆSDXLï¼‰ç›¸ç»“åˆï¼Œæ„å»ºäº†FaceCLIP-SDXLè¿™ä¸€IDä¿ç•™å›¾åƒåˆæˆç®¡é“ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFaceCLIP-SDXLèƒ½å¤Ÿå®ç°å…·æœ‰æ›´å¥½èº«ä»½ä¿ç•™å’Œæ–‡æœ¬ç›¸å…³æ€§çš„é€¼çœŸè‚–åƒç”Ÿæˆã€‚å¤§é‡å®éªŒè¯æ˜äº†å…¶åœ¨æ•°é‡å’Œè´¨é‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14202v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„èº«ä»½ä¿ç•™ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨å¤šæ¨¡æ€ç¼–ç ç­–ç•¥ï¼Œè€Œéé€šè¿‡é€‚é…å™¨æ³¨å…¥èº«ä»½ç‰¹å¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸­ã€‚è¯¥æ–¹æ³•å°†èº«ä»½å’Œæ–‡å­—è§†ä¸ºç»Ÿä¸€çš„æ¡ä»¶è¾“å…¥ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†FaceCLIPå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå­¦ä¹ èº«ä»½å’Œæ–‡æœ¬è¯­ä¹‰çš„è”åˆåµŒå…¥ç©ºé—´ã€‚ç»™å®šå‚è€ƒäººè„¸å’Œæ–‡å­—æç¤ºï¼ŒFaceCLIPäº§ç”Ÿç»Ÿä¸€è¡¨å¾ï¼Œç¼–ç èº«ä»½å’Œæ–‡æœ¬ï¼Œä»¥æ¡ä»¶åŸºç¡€æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€è‡´èº«ä»½å’Œæ–‡æœ¬å¯¹é½çš„å›¾åƒã€‚è¿˜æå‡ºäº†å¤šæ¨¡æ€å¯¹é½ç®—æ³•æ¥è®­ç»ƒFaceCLIPï¼Œä½¿ç”¨æŸå¤±å‡½æ•°å°†å…¶è”åˆè¡¨å¾ä¸äººè„¸ã€æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç©ºé—´å¯¹é½ã€‚é€šè¿‡æ•´åˆFaceCLIPä¸Stable Diffusion XLï¼ˆSDXLï¼‰ï¼Œæ„å»ºäº†FaceCLIP-SDXLèº«ä»½ä¿ç•™å›¾åƒåˆæˆç®¡é“ã€‚ç›¸æ¯”ä»¥å¾€æ–¹æ³•ï¼ŒFaceCLIP-SDXLèƒ½å®ç°æ›´é€¼çœŸçš„è‚–åƒç”Ÿæˆï¼Œå…·æœ‰æ›´å¥½çš„èº«ä»½ä¿ç•™å’Œæ–‡æœ¬ç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„IDä¿ç•™ç”Ÿæˆæ¡†æ¶ï¼Œä½¿ç”¨å¤šæ¨¡æ€ç¼–ç ç­–ç•¥ã€‚</li>
<li>æå‡ºäº†FaceCLIPå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œç”¨äºå­¦ä¹ èº«ä»½å’Œæ–‡æœ¬è¯­ä¹‰çš„è”åˆåµŒå…¥ç©ºé—´ã€‚</li>
<li>FaceCLIPèƒ½ç”Ÿæˆç»Ÿä¸€è¡¨å¾ï¼Œç¼–ç èº«ä»½å’Œæ–‡æœ¬ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›æ¡ä»¶ã€‚</li>
<li>å¼•å…¥äº†å¤šæ¨¡æ€å¯¹é½ç®—æ³•æ¥è®­ç»ƒFaceCLIPã€‚</li>
<li>æ„å»ºäº†FaceCLIP-SDXLå›¾åƒåˆæˆç®¡é“ï¼Œæ•´åˆäº†FaceCLIPä¸Stable Diffusion XLã€‚</li>
<li>FaceCLIP-SDXLèƒ½å®ç°æ›´é€¼çœŸçš„è‚–åƒç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨èº«ä»½ä¿ç•™å’Œæ–‡æœ¬ç›¸å…³æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fca49ad4f4d1b5786611bd82156138c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ae8c804e45b0934e4f34ffa7940fc52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-979e945736c5dfde69bf25baaa0b9df4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Rethinking-Target-Label-Conditioning-in-Adversarial-Attacks-A-2D-Tensor-Guided-Generative-Approach"><a href="#Rethinking-Target-Label-Conditioning-in-Adversarial-Attacks-A-2D-Tensor-Guided-Generative-Approach" class="headerlink" title="Rethinking Target Label Conditioning in Adversarial Attacks: A 2D   Tensor-Guided Generative Approach"></a>Rethinking Target Label Conditioning in Adversarial Attacks: A 2D   Tensor-Guided Generative Approach</h2><p><strong>Authors:Hangyu Liu, Bo Peng, Pengxiang Ding, Donglin Wang</strong></p>
<p>Compared to single-target adversarial attacks, multi-target attacks have garnered significant attention due to their ability to generate adversarial images for multiple target classes simultaneously. Existing generative approaches for multi-target attacks mainly analyze the effect of the use of target labels on noise generation from a theoretical perspective, lacking practical validation and comprehensive summarization. To address this gap, we first identify and validate that the semantic feature quality and quantity are critical factors affecting the transferability of targeted attacks: 1) Feature quality refers to the structural and detailed completeness of the implanted target features, as deficiencies may result in the loss of key discriminative information; 2) Feature quantity refers to the spatial sufficiency of the implanted target features, as inadequacy limits the victim modelâ€™s attention to this feature. Based on these findings, we propose the 2D Tensor-Guided Adversarial Fusion (2D-TGAF) framework, which leverages the powerful generative capabilities of diffusion models to encode target labels into two-dimensional semantic tensors for guiding adversarial noise generation. Additionally, we design a novel masking strategy tailored for the training process, ensuring that parts of the generated noise retain complete semantic information about the target class. Extensive experiments on the standard ImageNet dataset demonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in attack success rates, both on normally trained models and across various defense mechanisms. </p>
<blockquote>
<p>ç›¸è¾ƒäºå•ç›®æ ‡å¯¹æŠ—æ”»å‡»ï¼Œå¤šç›®æ ‡æ”»å‡»å› å…¶èƒ½å¤ŸåŒæ—¶ä¸ºå¤šä¸ªç›®æ ‡ç±»åˆ«ç”Ÿæˆå¯¹æŠ—æ ·æœ¬è€Œå¤‡å—å…³æ³¨ã€‚ç°æœ‰çš„å¤šç›®æ ‡æ”»å‡»çš„ç”Ÿæˆæ–¹æ³•ä¸»è¦ä»ç†è®ºè§’åº¦åˆ†æäº†ç›®æ ‡æ ‡ç­¾çš„ä½¿ç”¨å¯¹å™ªå£°ç”Ÿæˆçš„å½±å“ï¼Œç¼ºä¹å®é™…éªŒè¯å’Œå…¨é¢çš„æ€»ç»“ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šå¹¶éªŒè¯äº†è¯­ä¹‰ç‰¹å¾çš„è´¨é‡å’Œæ•°é‡æ˜¯å½±å“ç›®æ ‡æ”»å‡»è¿ç§»æ€§çš„å…³é”®å› ç´ ï¼š1ï¼‰ç‰¹å¾è´¨é‡æŒ‡çš„æ˜¯æ¤å…¥çš„ç›®æ ‡ç‰¹å¾çš„ç»“æ„å’Œç»†èŠ‚çš„å®Œæ•´æ€§ï¼Œå› ä¸ºç¼ºé™·å¯èƒ½ä¼šå¯¼è‡´å…³é”®åˆ¤åˆ«ä¿¡æ¯çš„ä¸¢å¤±ï¼›2ï¼‰ç‰¹å¾æ•°é‡æŒ‡çš„æ˜¯æ¤å…¥çš„ç›®æ ‡ç‰¹å¾çš„ç©ºé—´å……è¶³æ€§ï¼Œå› ä¸ºä¸è¶³ä¼šé™åˆ¶ç›®æ ‡æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„å…³æ³¨ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†äºŒç»´å¼ é‡å¼•å¯¼å¯¹æŠ—èåˆï¼ˆ2D-TGAFï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œå°†ç›®æ ‡æ ‡ç­¾ç¼–ç ä¸ºäºŒç»´è¯­ä¹‰å¼ é‡ï¼Œä»¥æŒ‡å¯¼å¯¹æŠ—å™ªå£°ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é’ˆå¯¹è®­ç»ƒè¿‡ç¨‹è®¾è®¡äº†ä¸€ç§æ–°å‹æ©ç ç­–ç•¥ï¼Œç¡®ä¿ç”Ÿæˆçš„å™ªå£°éƒ¨åˆ†ä¿ç•™æœ‰å…³ç›®æ ‡ç±»åˆ«çš„å®Œæ•´è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨æ ‡å‡†ImageNetæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨æ­£å¸¸è®­ç»ƒçš„æ¨¡å‹ä¸Šè¿˜æ˜¯åœ¨å„ç§é˜²å¾¡æœºåˆ¶ä¹‹é—´ï¼Œ2D-TGAFçš„æ”»å‡»åŠ›å§‹ç»ˆè¶…è¿‡ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14137v1">PDF</a> 12 pages, 4 figures</p>
<p><strong>Summary</strong><br>     å¤šç›®æ ‡æ”»å‡»å› å…¶èƒ½å¤ŸåŒæ—¶ä¸ºå¤šä¸ªç›®æ ‡ç±»åˆ«ç”Ÿæˆå¯¹æŠ—æ€§å›¾åƒè€Œå—åˆ°å…³æ³¨ã€‚ç°æœ‰ç”Ÿæˆå¼æ–¹æ³•ä¸»è¦ä»ç†è®ºè§’åº¦åˆ†æäº†ç›®æ ‡æ ‡ç­¾çš„ä½¿ç”¨å¯¹å™ªå£°ç”Ÿæˆçš„å½±å“ï¼Œç¼ºä¹å®é™…éªŒè¯å’Œå…¨é¢æ€»ç»“ã€‚ç ”ç©¶å‘ç°è¯­ä¹‰ç‰¹å¾çš„è´¨é‡å’Œæ•°é‡æ˜¯å½±å“ç›®æ ‡æ”»å‡»å¯è¿ç§»æ€§çš„å…³é”®å› ç´ ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„äºŒç»´å¼ é‡å¼•å¯¼å¯¹æŠ—èåˆæ¡†æ¶ï¼ˆ2D-TGAFï¼‰ï¼Œå°†ç›®æ ‡æ ‡ç­¾ç¼–ç ä¸ºäºŒç»´è¯­ä¹‰å¼ é‡ï¼Œå¼•å¯¼å¯¹æŠ—å™ªå£°ç”Ÿæˆã€‚åœ¨ImageNetæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œ2D-TGAFåœ¨æ”»å‡»æˆåŠŸç‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ— è®ºæ˜¯å¯¹æ­£å¸¸è®­ç»ƒçš„æ¨¡å‹è¿˜æ˜¯å„ç§é˜²å¾¡æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç›®æ ‡æ”»å‡»èƒ½åŒæ—¶ä¸ºå¤šä¸ªç›®æ ‡ç±»åˆ«ç”Ÿæˆå¯¹æŠ—æ€§å›¾åƒï¼Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä»ç†è®ºè§’åº¦ç ”ç©¶ç›®æ ‡æ ‡ç­¾å¯¹å™ªå£°ç”Ÿæˆçš„å½±å“ï¼Œç¼ºä¹å®é™…éªŒè¯å’Œå…¨é¢æ€»ç»“ã€‚</li>
<li>è¯­ä¹‰ç‰¹å¾çš„è´¨é‡å’Œæ•°é‡æ˜¯å½±å“ç›®æ ‡æ”»å‡»å¯è¿ç§»æ€§çš„å…³é”®å› ç´ ã€‚</li>
<li>ç‰¹å¾è´¨é‡æŒ‡æ¤å…¥ç›®æ ‡ç‰¹å¾çš„ç»“æ„å’Œç»†èŠ‚å®Œæ•´æ€§ï¼Œä¸è¶³ä¼šå¯¼è‡´å…³é”®åˆ¤åˆ«ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>ç‰¹å¾æ•°é‡æŒ‡æ¤å…¥ç›®æ ‡ç‰¹å¾çš„ç©ºé—´å……è¶³æ€§ï¼Œä¸è¶³ä¼šé™åˆ¶å—å®³è€…æ¨¡å‹çš„æ³¨æ„åŠ›ã€‚</li>
<li>æå‡ºçš„2D-TGAFæ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œå°†ç›®æ ‡æ ‡ç­¾ç¼–ç ä¸ºäºŒç»´è¯­ä¹‰å¼ é‡ï¼ŒæŒ‡å¯¼å¯¹æŠ—å™ªå£°ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d62600a8c2c761d396a7d80c244b854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ee577d0f955f8a0316f139bca5325d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c07939da60ee1289e339eab7c11725b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c87808b06e43a06e4bb5a3f46838462.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models"><a href="#ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models" class="headerlink" title="ID-Booth: Identity-consistent Face Generation with Diffusion Models"></a>ID-Booth: Identity-consistent Face Generation with Diffusion Models</h2><p><strong>Authors:Darian TomaÅ¡eviÄ‡, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir Å truc, Peter Peer</strong></p>
<p>Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at <a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth">https://github.com/dariant/ID-Booth</a>. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆå»ºæ¨¡æŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—èƒ½å¤Ÿç”Ÿæˆé€‚ç”¨äºå„ç§é¢†åŸŸçš„é«˜è´¨é‡åˆæˆæ•°æ®ï¼ŒåŒ…æ‹¬äººè„¸è¯†åˆ«ã€‚åœ¨è¿™é‡Œï¼Œæœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹é€šå¸¸ä¾èµ–äºå¯¹åŠŸèƒ½å¼ºå¤§çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶è®¾å®šå’Œå¾®è°ƒï¼Œä»¥ä¿ƒè¿›æ‰€éœ€èº«ä»½çš„é€¼çœŸå›¾åƒåˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾€å¾€ä¸è€ƒè™‘ä¸»ä½“çš„èº«ä»½ï¼Œå¯¼è‡´ç”Ÿæˆå›¾åƒä¸é¢„æœŸèº«ä»½ä¹‹é—´çš„ä¸€è‡´æ€§è¾ƒå·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé‡‡ç”¨åŸºäºèº«ä»½çš„è®­ç»ƒç›®æ ‡çš„æ–¹æ³•å¾€å¾€ä¼šåœ¨èº«ä»½çš„å„ä¸ªæ–¹é¢è¿‡åº¦æ‹Ÿåˆï¼Œä»è€Œé™ä½äº†å¯ä»¥ç”Ÿæˆçš„å›¾åƒå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºID-Boothã€‚ID-BoothåŒ…æ‹¬ä¸€ä¸ªè´Ÿè´£æ•°æ®ç”Ÿæˆçš„é™å™ªç½‘ç»œã€ä¸€ä¸ªç”¨äºå°†å›¾åƒæ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´åŠå…¶åå‘æ˜ å°„çš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ä»¥åŠä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å…è®¸åŸºäºæç¤ºæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº†ä¸€ç§æ–°å‹çš„ä¸‰é‡èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åˆæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°èº«ä»½ä¸€è‡´çš„å›¾åƒç”Ÿæˆã€‚åˆ©ç”¨æœ€å…ˆè¿›çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œä¸åŒçš„æç¤ºè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨èº«ä»½å†…ä¸€è‡´æ€§å’Œèº«ä»½é—´å¯åˆ†ç¦»æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒæ—¶å®ç°äº†è¾ƒé«˜çš„å›¾åƒå¤šæ ·æ€§ã€‚è¿›è€Œï¼Œç”Ÿæˆçš„æ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºå°è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä»¥ä¿æŠ¤éšç§çš„æ–¹å¼è®­ç»ƒæ€§èƒ½æ›´å¥½çš„è¯†åˆ«æ¨¡å‹ã€‚ID-Boothæ¡†æ¶çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/dariant/ID-Boothå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07392v3">PDF</a> IEEE International Conference on Automatic Face and Gesture   Recognition (FG) 2025, 14 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹ç”Ÿæˆæ¨¡å‹ID-Boothï¼Œè¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨äººè„¸è¯†åˆ«é¢†åŸŸé¢ä¸´çš„èº«ä»½ä¸€è‡´æ€§é—®é¢˜ã€‚ID-Boothé€šè¿‡ä½¿ç”¨å»å™ªç½‘ç»œè¿›è¡Œæ•°æ®ç”Ÿæˆï¼Œé€šè¿‡å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å®ç°å›¾åƒåˆ°ä½ç»´æ½œåœ¨ç©ºé—´çš„æ˜ å°„å’Œåæ˜ å°„ï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬ç¼–ç å™¨å®ç°åŸºäºæç¤ºçš„æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚å…¶é‡‡ç”¨æ–°é¢–çš„ä¸‰é‡èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åˆæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°èº«ä»½ä¸€è‡´çš„å›¾åƒç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç«äº‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨èº«ä»½å†…ä¸€è‡´æ€§å’Œèº«ä»½é—´å¯åˆ†ç¦»æ€§æ–¹é¢è¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶å®ç°æ›´é«˜çš„å›¾åƒå¤šæ ·æ€§ã€‚å› æ­¤ï¼Œç”Ÿæˆçš„æ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºå°è§„æ¨¡æ•°æ®é›†å¹¶è®­ç»ƒæ€§èƒ½æ›´å¥½çš„è¯†åˆ«æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ€æ–°ç”Ÿæˆæ¨¡å‹æŠ€æœ¯å·²èƒ½å¤Ÿå®ç°é«˜è´¨é‡åˆæˆæ•°æ®çš„ç”Ÿæˆï¼Œé€‚ç”¨äºåŒ…æ‹¬äººè„¸è¯†åˆ«åœ¨å†…çš„å¤šä¸ªé¢†åŸŸã€‚</li>
<li>ç°æœ‰å…ˆè¿›æŠ€æœ¯ç”Ÿæˆæ¨¡å‹åœ¨èº«ä»½ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨é—®é¢˜ï¼Œè®­ç»ƒæ—¶æœªè€ƒè™‘ä¸»ä½“èº«ä»½å¯¼è‡´ç”Ÿæˆå›¾åƒä¸é¢„æœŸèº«ä»½ä¸ä¸€è‡´ã€‚</li>
<li>åŸºäºèº«ä»½çš„è®­ç»ƒç›®æ ‡æ–¹æ³•è™½ç„¶èƒ½æé«˜èº«ä»½ä¸€è‡´æ€§ï¼Œä½†å¯èƒ½å¯¼è‡´å›¾åƒå¤šæ ·æ€§é™ä½ã€‚</li>
<li>ID-Boothæ¡†æ¶è§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡å»å™ªç½‘ç»œã€å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨å®ç°æ›´çµæ´»çš„å›¾åƒç”Ÿæˆæ§åˆ¶ã€‚</li>
<li>ID-Boothé‡‡ç”¨æ–°é¢–çš„ä¸‰é‡èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œå®ç°èº«ä»½ä¸€è‡´çš„å›¾åƒç”ŸæˆåŒæ—¶ä¿æŒè¾ƒé«˜çš„å›¾åƒå¤šæ ·æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒID-Boothæ¡†æ¶èƒ½æé«˜å›¾åƒç”Ÿæˆçš„åŒä¸€èº«ä»½å†…ä¸€è‡´æ€§ä»¥åŠä¸åŒèº«ä»½é—´çš„å¯åˆ†ç¦»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-43d96b8615b71195d4b765c3935f6fb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a1325fe4dfc46da49d23433810504b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-814022dbf79c1a1cc8eb9da0ac5986c4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Continuous-Locomotive-Crowd-Behavior-Generation"><a href="#Continuous-Locomotive-Crowd-Behavior-Generation" class="headerlink" title="Continuous Locomotive Crowd Behavior Generation"></a>Continuous Locomotive Crowd Behavior Generation</h2><p><strong>Authors:Inhwan Bae, Junoh Lee, Hae-Gon Jeon</strong></p>
<p>Modeling and reproducing crowd behaviors are important in various domains including psychology, robotics, transport engineering and virtual environments. Conventional methods have focused on synthesizing momentary scenes, which have difficulty in replicating the continuous nature of real-world crowds. In this paper, we introduce a novel method for automatically generating continuous, realistic crowd trajectories with heterogeneous behaviors and interactions among individuals. We first design a crowd emitter model. To do this, we obtain spatial layouts from single input images, including a segmentation map, appearance map, population density map and population probability, prior to crowd generation. The emitter then continually places individuals on the timeline by assigning independent behavior characteristics such as agentsâ€™ type, pace, and start&#x2F;end positions using diffusion models. Next, our crowd simulator produces their long-term locomotions. To simulate diverse actions, it can augment their behaviors based on a Markov chain. As a result, our overall framework populates the scenes with heterogeneous crowd behaviors by alternating between the proposed emitter and simulator. Note that all the components in the proposed framework are user-controllable. Lastly, we propose a benchmark protocol to evaluate the realism and quality of the generated crowds in terms of the scene-level population dynamics and the individual-level trajectory accuracy. We demonstrate that our approach effectively models diverse crowd behavior patterns and generalizes well across different geographical environments. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/InhwanBae/CrowdES">https://github.com/InhwanBae/CrowdES</a> . </p>
<blockquote>
<p>å¯¹äººç¾¤è¡Œä¸ºçš„å»ºæ¨¡å’Œå†ç°æ˜¯å¿ƒç†å­¦ã€æœºå™¨äººæŠ€æœ¯ã€äº¤é€šå·¥ç¨‹å’Œè™šæ‹Ÿç¯å¢ƒç­‰å¤šä¸ªé¢†åŸŸçš„é‡è¦è¯¾é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å³æ—¶åœºæ™¯çš„åˆæˆï¼Œå¾ˆéš¾å¤åˆ¶ç°å®ä¸–ç•Œä¸­äººç¾¤çš„è¿ç»­æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆè¿ç»­ã€ç°å®çš„äººç¾¤è½¨è¿¹çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¸åŒçš„ä¸ªä½“è¡Œä¸ºå’Œä¸ªä½“é—´çš„äº¤äº’ã€‚æˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªäººç¾¤å‘å°„å™¨æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»å•ä¸ªè¾“å…¥å›¾åƒä¸­è·å–ç©ºé—´å¸ƒå±€ï¼ŒåŒ…æ‹¬åˆ†å‰²å›¾ã€å¤–è§‚å›¾ã€äººå£å¯†åº¦å›¾å’Œäººå£æ¦‚ç‡ï¼Œç„¶åè¿›è¡Œäººç¾¤ç”Ÿæˆã€‚å‘å°„å™¨é€šè¿‡åˆ†é…ç‹¬ç«‹çš„è¡Œä¸ºç‰¹å¾ï¼ˆå¦‚ä»£ç†çš„ç±»å‹ã€é€Ÿåº¦ã€èµ·å§‹&#x2F;ç»“æŸä½ç½®ï¼‰åœ¨æ—¶é—´ä¸ŠæŒç»­æ”¾ç½®ä¸ªä½“ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„äººç¾¤æ¨¡æ‹Ÿå™¨äº§ç”Ÿä»–ä»¬çš„é•¿æœŸè¿åŠ¨ã€‚ä¸ºäº†æ¨¡æ‹Ÿå„ç§åŠ¨ä½œï¼Œå®ƒå¯ä»¥æ ¹æ®é©¬å°”å¯å¤«é“¾å¢å¼ºå…¶è¡Œä¸ºã€‚å› æ­¤ï¼Œé€šè¿‡äº¤æ›¿ä½¿ç”¨æ‰€æå‡ºçš„å‘å°„å™¨å’Œæ¨¡æ‹Ÿå™¨ï¼Œæˆ‘ä»¬çš„æ•´ä½“æ¡†æ¶èƒ½å¤Ÿåœ¨åœºæ™¯ä¸­å¡«å……å…·æœ‰ä¸åŒè¡Œä¸ºçš„äººç¾¤ã€‚è¯·æ³¨æ„ï¼Œè¯¥æ¡†æ¶ä¸­çš„æ‰€æœ‰ç»„ä»¶éƒ½æ˜¯ç”¨æˆ·å¯æ§çš„ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºå‡†åè®®ï¼Œä»¥è¯„ä¼°ç”Ÿæˆäººç¾¤çš„ç°å®æ€§å’Œè´¨é‡ï¼ŒåŒ…æ‹¬åœºæ™¯çº§çš„äººå£åŠ¨æ€å’Œä¸ªä½“çº§çš„è½¨è¿¹ç²¾åº¦ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿä¸åŒçš„äººç¾¤è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶åœ¨ä¸åŒçš„åœ°ç†ç¯å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/InhwanBae/CrowdES">https://github.com/InhwanBae/CrowdES</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04756v2">PDF</a> Accepted at CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://ihbae.com/publication/crowdes/">https://ihbae.com/publication/crowdes/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆè¿ç»­ã€çœŸå®äººç¾¤è½¨è¿¹çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ¨¡æ‹Ÿå‡ºäººç¾¤ä¸­çš„ä¸åŒè¡Œä¸ºå’Œä¸ªä½“é—´çš„äº¤äº’ã€‚é€šè¿‡è®¾è®¡äººç¾¤å‘å°„å™¨æ¨¡å‹ï¼Œä»å•ä¸€è¾“å…¥å›¾åƒä¸­è·å–ç©ºé—´å¸ƒå±€ï¼ŒåŒ…æ‹¬åˆ†å‰²å›¾ã€å¤–è§‚å›¾ã€äººå£å¯†åº¦å›¾å’Œäººå£æ¦‚ç‡ç­‰ï¼Œæ¥ç”Ÿæˆäººç¾¤ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸ºä¸ªä½“åˆ†é…ç‹¬ç«‹çš„è¡Œä¸ºç‰¹å¾ï¼Œå¦‚ç±»å‹ã€é€Ÿåº¦ä»¥åŠèµ·å§‹&#x2F;ç»ˆæ­¢ä½ç½®ï¼Œç„¶åç”±äººç¾¤æ¨¡æ‹Ÿå™¨äº§ç”Ÿé•¿æœŸè¿åŠ¨ã€‚æ¨¡æ‹Ÿå™¨å¯ä»¥é€šè¿‡é©¬å°”å¯å¤«é“¾å¢å¼ºå„ç§è¡Œä¸ºã€‚æ•´ä¸ªæ¡†æ¶é€šè¿‡äº¤æ›¿ä½¿ç”¨å‘å°„å™¨å’Œæ¨¡æ‹Ÿå™¨æ¥æ¨¡æ‹Ÿåœºæ™¯ä¸­çš„äººç¾¤è¡Œä¸ºã€‚æ‰€æœ‰ç»„ä»¶éƒ½æ˜¯ç”¨æˆ·å¯æ§çš„ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªåŸºå‡†åè®®ï¼Œä»¥è¯„ä¼°ç”Ÿæˆäººç¾¤çš„çœŸå®æ„Ÿå’Œè´¨é‡ï¼ŒåŒ…æ‹¬åœºæ™¯çº§åˆ«çš„äººå£åŠ¨æ€å’Œä¸ªä½“çº§åˆ«çš„è½¨è¿¹ç²¾åº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿå‡ºå¤šæ ·çš„äººç¾¤è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶åœ¨ä¸åŒçš„åœ°ç†ç¯å¢ƒä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆè¿ç»­ã€çœŸå®äººç¾¤è½¨è¿¹çš„æ–°æ–¹æ³•ï¼Œå¼¥è¡¥äº†ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å¤åˆ¶äººç¾¤è¿ç»­æ€§çš„ç¼ºç‚¹ã€‚</li>
<li>é€šè¿‡è®¾è®¡äººç¾¤å‘å°„å™¨æ¨¡å‹ï¼Œä»å•ä¸€è¾“å…¥å›¾åƒä¸­è·å–ç©ºé—´å¸ƒå±€ï¼Œä¸ºç”Ÿæˆäººç¾¤æä¾›åŸºç¡€ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸ºä¸ªä½“åˆ†é…ç‹¬ç«‹çš„è¡Œä¸ºç‰¹å¾ï¼Œå®ç°è¡Œä¸ºçš„å¤šæ ·åŒ–ã€‚</li>
<li>é‡‡ç”¨äº†äººç¾¤æ¨¡æ‹Ÿå™¨æ¥äº§ç”Ÿé•¿æœŸè¿åŠ¨ï¼Œå¹¶é€šè¿‡é©¬å°”å¯å¤«é“¾å¢å¼ºè¡Œä¸ºçš„å¤šæ ·æ€§ã€‚</li>
<li>æ¡†æ¶ä¸­çš„ç»„ä»¶éƒ½æ˜¯ç”¨æˆ·å¯æ§çš„ï¼Œæä¾›äº†é«˜åº¦çš„çµæ´»æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºå‡†åè®®æ¥è¯„ä¼°ç”Ÿæˆäººç¾¤çš„çœŸå®æ„Ÿå’Œè´¨é‡ï¼ŒåŒ…æ‹¬åœºæ™¯çº§åˆ«å’Œä¸ªä½“çº§åˆ«çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨¡æ‹Ÿå‡ºå¤šæ ·çš„äººç¾¤è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ab42de6bf485ae768782c0e4a71180d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eee7678fdb6e2eadbe594b82cbbb3633.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91b1739072f7ff162c7f933fe1dcb6f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f1223007f2bd2d9dd518c4f183a03518.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Scene4U-Hierarchical-Layered-3D-Scene-Reconstruction-from-Single-Panoramic-Image-for-Your-Immerse-Exploration"><a href="#Scene4U-Hierarchical-Layered-3D-Scene-Reconstruction-from-Single-Panoramic-Image-for-Your-Immerse-Exploration" class="headerlink" title="Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single   Panoramic Image for Your Immerse Exploration"></a>Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single   Panoramic Image for Your Immerse Exploration</h2><p><strong>Authors:Zilong Huang, Jun He, Junyan Ye, Lihan Jiang, Weijia Li, Yiping Chen, Ting Han</strong></p>
<p>The reconstruction of immersive and realistic 3D scenes holds significant practical importance in various fields of computer vision and computer graphics. Typically, immersive and realistic scenes should be free from obstructions by dynamic objects, maintain global texture consistency, and allow for unrestricted exploration. The current mainstream methods for image-driven scene construction involves iteratively refining the initial image using a moving virtual camera to generate the scene. However, previous methods struggle with visual discontinuities due to global texture inconsistencies under varying camera poses, and they frequently exhibit scene voids caused by foreground-background occlusions. To this end, we propose a novel layered 3D scene reconstruction framework from panoramic image, named Scene4U. Specifically, Scene4U integrates an open-vocabulary segmentation model with a large language model to decompose a real panorama into multiple layers. Then, we employs a layered repair module based on diffusion model to restore occluded regions using visual cues and depth information, generating a hierarchical representation of the scene. The multi-layer panorama is then initialized as a 3D Gaussian Splatting representation, followed by layered optimization, which ultimately produces an immersive 3D scene with semantic and structural consistency that supports free exploration. Scene4U outperforms state-of-the-art method, improving by 24.24% in LPIPS and 24.40% in BRISQUE, while also achieving the fastest training speed. Additionally, to demonstrate the robustness of Scene4U and allow users to experience immersive scenes from various landmarks, we build WorldVista3D dataset for 3D scene reconstruction, which contains panoramic images of globally renowned sites. The implementation code and dataset will be released at <a target="_blank" rel="noopener" href="https://github.com/LongHZ140516/Scene4U">https://github.com/LongHZ140516/Scene4U</a> . </p>
<blockquote>
<p>åŸºäºæ²‰æµ¸å¼ä¸çœŸå®çš„ä¸‰ç»´åœºæ™¯é‡å»ºåœ¨è®¡ç®—æœºè§†è§‰ä¸è®¡ç®—æœºå›¾å½¢å­¦é¢†åŸŸçš„å„ä¸ªå­é¢†åŸŸä¸­æœ‰ç€é‡å¤§çš„å®é™…åº”ç”¨ä»·å€¼ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œæ²‰æµ¸å¼ä¸çœŸå®åœºæ™¯åº”æ— åŠ¨æ€ç‰©ä½“çš„é®æŒ¡å¹²æ‰°ï¼Œä¿æŒå…¨å±€çº¹ç†çš„ä¸€è‡´æ€§ï¼Œå¹¶å…è®¸æ— é™åˆ¶çš„æ¢ç´¢ã€‚å½“å‰ä¸»æµçš„å›¾åƒé©±åŠ¨åœºæ™¯æ„å»ºæ–¹æ³•é€šè¿‡ç§»åŠ¨è™šæ‹Ÿç›¸æœºè¿­ä»£ä¼˜åŒ–åˆå§‹å›¾åƒä»¥ç”Ÿæˆåœºæ™¯ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„æ–¹æ³•åœ¨ç”±äºä¸åŒç›¸æœºå§¿æ€ä¸‹çš„å…¨å±€çº¹ç†ä¸ä¸€è‡´å¯¼è‡´çš„è§†è§‰æ–­å±‚æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œå¹¶ä¸”ä»–ä»¬ç»å¸¸ç”±äºå‰æ™¯èƒŒæ™¯é®æŒ¡å¯¼è‡´åœºæ™¯ç©ºæ´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå…¨æ™¯å›¾åƒçš„æ–°å‹åˆ†å±‚ä¸‰ç»´åœºæ™¯é‡å»ºæ¡†æ¶ï¼Œåä¸ºScene4Uã€‚å…·ä½“æ¥è¯´ï¼ŒScene4Uç»“åˆäº†å¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†çœŸå®å…¨æ™¯å›¾åƒåˆ†è§£ä¸ºå¤šä¸ªå±‚æ¬¡ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å±‚ä¿®å¤æ¨¡å—ï¼Œåˆ©ç”¨è§†è§‰çº¿ç´¢å’Œæ·±åº¦ä¿¡æ¯æ¢å¤è¢«é®æŒ¡åŒºåŸŸï¼Œç”Ÿæˆåœºæ™¯çš„å±‚æ¬¡è¡¨ç¤ºã€‚å¤šå±‚å…¨æ™¯å›¾åƒè¢«åˆå§‹åŒ–ä¸ºä¸‰ç»´é«˜æ–¯æº…å°„è¡¨ç¤ºï¼Œéšåè¿›è¡Œåˆ†å±‚ä¼˜åŒ–ï¼Œæœ€ç»ˆç”Ÿæˆå…·æœ‰è¯­ä¹‰å’Œç»“æ„ä¸€è‡´æ€§çš„æ²‰æµ¸å¼ä¸‰ç»´åœºæ™¯ï¼Œæ”¯æŒè‡ªç”±æ¢ç´¢ã€‚Scene4Uåœ¨LPIPSä¸Šæé«˜äº†24.24%ï¼Œåœ¨BRISQUEä¸Šæé«˜äº†24.40%ï¼ŒåŒæ—¶è¾¾åˆ°äº†æœ€å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯æ˜Scene4Uçš„ç¨³å¥æ€§å¹¶è®©ç”¨æˆ·èƒ½å¤Ÿä½“éªŒæ¥è‡ªä¸åŒåœ°æ ‡çš„æ²‰æµ¸å¼åœºæ™¯ï¼Œæˆ‘ä»¬å»ºç«‹äº†ç”¨äºä¸‰ç»´åœºæ™¯é‡å»ºçš„WorldVista3Dæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å…¨çƒçŸ¥åæ™¯ç‚¹çš„å…¨æ™¯å›¾åƒã€‚å®ç°ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/LongHZ140516/Scene4U%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/LongHZ140516/Scene4Uä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00387v2">PDF</a> CVPR 2025, 11 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å…¨æ™¯å›¾åƒé©±åŠ¨çš„ä¸‰ç»´åœºæ™¯é‡å»ºçš„é‡è¦æ€§å’Œç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚æå‡ºäº†ä¸€ç§æ–°å‹çš„åˆ†å±‚ä¸‰ç»´åœºæ™¯é‡å»ºæ¡†æ¶Scene4Uï¼Œåˆ©ç”¨å…¨æ™¯å›¾åƒè¿›è¡Œåœºæ™¯é‡å»ºï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å…¨çƒçº¹ç†ä¸ä¸€è‡´å’Œå‰æ™¯èƒŒæ™¯é®æŒ¡å¯¼è‡´çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†è§£å…¨æ™¯å›¾åƒä¸ºå¤šå±‚ï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¿®å¤é®æŒ¡åŒºåŸŸï¼Œç”Ÿæˆå…·æœ‰è¯­ä¹‰å’Œç»“æ„ä¸€è‡´æ€§çš„æ²‰æµ¸å¼ä¸‰ç»´åœºæ™¯ã€‚Scene4Uæ€§èƒ½ä¼˜è¶Šï¼Œåœ¨LPIPSå’ŒBRISQUEæŒ‡æ ‡ä¸Šåˆ†åˆ«æé«˜äº†24.24%å’Œ24.40%ï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æœ€å¿«ã€‚è¿˜æ„å»ºäº†WorldVista3Dæ•°æ®é›†ç”¨äºä¸‰ç»´åœºæ™¯é‡å»ºï¼ŒåŒ…å«å…¨çƒçŸ¥ååœ°ç‚¹çš„å…¨æ™¯å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡å»ºæ²‰æµ¸å¼ä¸”é€¼çœŸçš„ä¸‰ç»´åœºæ™¯åœ¨è®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æœºå›¾å½¢å­¦é¢†åŸŸå…·æœ‰å®é™…æ„ä¹‰ã€‚</li>
<li>å½“å‰ä¸»æµæ–¹æ³•é€šè¿‡è¿­ä»£ä¼˜åŒ–åˆå§‹å›¾åƒç”Ÿæˆåœºæ™¯ï¼Œä½†é¢ä¸´å…¨çƒçº¹ç†ä¸ä¸€è‡´å’Œå‰æ™¯èƒŒæ™¯é®æŒ¡çš„é—®é¢˜ã€‚</li>
<li>Scene4Uæ¡†æ¶åˆ©ç”¨å…¨æ™¯å›¾åƒè¿›è¡Œåˆ†å±‚åœºæ™¯é‡å»ºï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>Scene4Ué‡‡ç”¨å¼€æ”¾å¼è¯æ±‡åˆ†å‰²æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆï¼Œå¹¶é€šè¿‡æ‰©æ•£æ¨¡å‹ä¿®å¤é®æŒ¡åŒºåŸŸã€‚</li>
<li>Scene4Uç”Ÿæˆå…·æœ‰è¯­ä¹‰å’Œç»“æ„ä¸€è‡´æ€§çš„æ²‰æµ¸å¼ä¸‰ç»´åœºæ™¯ï¼Œå¹¶æ”¯æŒè‡ªç”±æ¢ç´¢ã€‚</li>
<li>Scene4Uæ€§èƒ½ä¼˜è¶Šï¼Œåœ¨LPIPSå’ŒBRISQUEæŒ‡æ ‡ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d05a9c8420aec71ed6761b3ca36ee789.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c000e424f6f3499c0851eeff16e9c85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8da3d3a6f4bf349f279be234eecfb8bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c27882cb9476477fe54a896f92c60d8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Edge-preserving-noise-for-diffusion-models"><a href="#Edge-preserving-noise-for-diffusion-models" class="headerlink" title="Edge-preserving noise for diffusion models"></a>Edge-preserving noise for diffusion models</h2><p><strong>Authors:Jente Vandersanden, Sascha Holl, Xingchang Huang, Gurprit Singh</strong></p>
<p>Classical generative diffusion models learn an isotropic Gaussian denoising process, treating all spatial regions uniformly, thus neglecting potentially valuable structural information in the data. Inspired by the long-established work on anisotropic diffusion in image processing, we present a novel edge-preserving diffusion model that generalizes over existing isotropic models by considering a hybrid noise scheme. In particular, we introduce an edge-aware noise scheduler that varies between edge-preserving and isotropic Gaussian noise. We show that our modelâ€™s generative process converges faster to results that more closely match the target distribution. We demonstrate its capability to better learn the low-to-mid frequencies within the dataset, which plays a crucial role in representing shapes and structural information. Our edge-preserving diffusion process consistently outperforms state-of-the-art baselines in unconditional image generation. It is also particularly more robust for generative tasks guided by a shape-based prior, such as stroke-to-image generation. We present qualitative and quantitative results (FID and CLIP score) showing consistent improvements of up to 30% for both tasks. </p>
<blockquote>
<p>ç»å…¸ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹å­¦ä¹ ç­‰å‘é«˜æ–¯å»å™ªè¿‡ç¨‹ï¼Œå¯¹æ‰€æœ‰ç©ºé—´åŒºåŸŸè¿›è¡Œç»Ÿä¸€å¤„ç†ï¼Œä»è€Œå¿½ç•¥äº†æ•°æ®ä¸­å¯èƒ½å­˜åœ¨çš„æœ‰ä»·å€¼çš„ç»“æ„ä¿¡æ¯ã€‚å—å›¾åƒå¤„ç†ä¸­å·²ä¹…ç»å»ºç«‹çš„å¼‚å‘æ‰©æ•£å·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¾¹ç¼˜ä¿æŒæ‰©æ•£æ¨¡å‹ï¼Œå®ƒé€šè¿‡è€ƒè™‘æ··åˆå™ªå£°æ–¹æ¡ˆæ¥æ¦‚æ‹¬ç°æœ‰çš„ç­‰å‘æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¾¹ç¼˜æ„ŸçŸ¥å™ªå£°è°ƒåº¦å™¨ï¼Œåœ¨è¾¹ç¼˜ä¿æŒå’Œç­‰å‘é«˜æ–¯å™ªå£°ä¹‹é—´è¿›è¡Œå˜åŒ–ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹æ›´å¿«æ”¶æ•›åˆ°æ›´è´´è¿‘ç›®æ ‡åˆ†å¸ƒçš„ç»“æœã€‚æˆ‘ä»¬è¯æ˜äº†å®ƒåœ¨å­¦ä¹ æ•°æ®é›†å†…çš„ä½ä¸­é¢‘éƒ¨åˆ†æ–¹é¢æ›´èƒœä¸€ç­¹ï¼Œè¿™åœ¨è¡¨ç¤ºå½¢çŠ¶å’Œç»“æ„ä¿¡æ¯æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„è¾¹ç¼˜ä¿æŒæ‰©æ•£è¿‡ç¨‹åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ–¹é¢å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›åŸºçº¿ã€‚å¯¹äºåŸºäºå½¢çŠ¶çš„å…ˆéªŒæŒ‡å¯¼çš„ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚ç¬”åˆ’åˆ°å›¾åƒç”Ÿæˆï¼Œå®ƒä¹Ÿå…·æœ‰æ›´å¼ºçš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬æä¾›äº†å®šæ€§å’Œå®šé‡ç»“æœï¼ˆFIDå’ŒCLIPå¾—åˆ†ï¼‰ï¼Œæ˜¾ç¤ºä¸¤é¡¹ä»»åŠ¡çš„æ”¹è¿›å‡æŒç»­é«˜è¾¾30%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01540v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¾¹ç¼˜ä¿æŒçš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è€ƒè™‘æ··åˆå™ªå£°æ–¹æ¡ˆï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ•°æ®é›†ä¸­æ›´æœ‰æ•ˆåœ°æ•æ‰ç»“æ„ä¿¡æ¯ã€‚å¼•å…¥çš„è¾¹ç¼˜æ„ŸçŸ¥å™ªå£°è°ƒåº¦å™¨èƒ½å¤Ÿåœ¨è¾¹ç¼˜ä¿æŒå’ŒåŒæ„é«˜æ–¯å™ªå£°ä¹‹é—´è¿›è¡Œå˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”èƒ½æ›´å¥½åœ°å­¦ä¹ æ•°æ®é›†ä¸­çš„ä½ä¸­é¢‘ä¿¡æ¯ï¼Œè¿™å¯¹äºè¡¨ç¤ºå½¢çŠ¶å’Œç»“æ„ä¿¡æ¯è‡³å…³é‡è¦ã€‚è¯¥æ¨¡å‹åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºå½¢çŠ¶çš„å…ˆéªŒå¼•å¯¼çš„ä»»åŠ¡ä¸­æ›´ä¸ºç¨³å¥ï¼Œå¦‚ç¬”åˆ’åˆ°å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸Šçš„æ”¹è¿›å‡è¾¾åˆ°äº†ä¸€è‡´çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„æ‰©æ•£æ¨¡å‹è€ƒè™‘äº†æ··åˆå™ªå£°æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•°æ®ä¸­çš„ç»“æ„ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥äº†è¾¹ç¼˜æ„ŸçŸ¥å™ªå£°è°ƒåº¦å™¨ï¼Œèƒ½å¤Ÿåœ¨è¾¹ç¼˜ä¿æŒå’ŒåŒæ„é«˜æ–¯å™ªå£°ä¹‹é—´å˜åŒ–ã€‚</li>
<li>æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”æ›´æ¥è¿‘ç›®æ ‡åˆ†å¸ƒã€‚</li>
<li>æ¨¡å‹èƒ½æ›´å¥½åœ°å­¦ä¹ æ•°æ®é›†ä¸­çš„ä½ä¸­é¢‘ä¿¡æ¯ï¼Œè¿™å¯¹è¡¨ç¤ºå½¢çŠ¶å’Œç»“æ„ä¿¡æ¯è‡³å…³é‡è¦ã€‚</li>
<li>æ¨¡å‹åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>æ¨¡å‹åœ¨åŸºäºå½¢çŠ¶çš„å…ˆéªŒå¼•å¯¼çš„ä»»åŠ¡ä¸­æ›´ä¸ºç¨³å¥ï¼Œå¦‚ç¬”åˆ’åˆ°å›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-26ce7ee74b02fd237e72ad8aa6c15885.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19dc522ae40d7b9e866ce1677b42adfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-046da5a2357d8e0aa06e12a9a85fd8fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef63bfb6701615e3ca60f631e7dd42b8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Controlling-Space-and-Time-with-Diffusion-Models"><a href="#Controlling-Space-and-Time-with-Diffusion-Models" class="headerlink" title="Controlling Space and Time with Diffusion Models"></a>Controlling Space and Time with Diffusion Models</h2><p><strong>Authors:Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, David J. Fleet</strong></p>
<p>We present 4DiM, a cascaded diffusion model for 4D novel view synthesis (NVS), supporting generation with arbitrary camera trajectories and timestamps, in natural scenes, conditioned on one or more images. With a novel architecture and sampling procedure, we enable training on a mixture of 3D (with camera pose), 4D (pose+time) and video (time but no pose) data, which greatly improves generalization to unseen images and camera pose trajectories over prior works that focus on limited domains (e.g., object centric). 4DiM is the first-ever NVS method with intuitive metric-scale camera pose control enabled by our novel calibration pipeline for structure-from-motion-posed data. Experiments demonstrate that 4DiM outperforms prior 3D NVS models both in terms of image fidelity and pose alignment, while also enabling the generation of scene dynamics. 4DiM provides a general framework for a variety of tasks including single-image-to-3D, two-image-to-video (interpolation and extrapolation), and pose-conditioned video-to-video translation, which we illustrate qualitatively on a variety of scenes. For an overview see <a target="_blank" rel="noopener" href="https://4d-diffusion.github.io/">https://4d-diffusion.github.io</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†4DiMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº4Dæ–°é¢–è§†å›¾åˆæˆï¼ˆNVSï¼‰çš„çº§è”æ‰©æ•£æ¨¡å‹ï¼Œæ”¯æŒåœ¨è‡ªç„¶ç¯å¢ƒåœºæ™¯ä¸­ï¼Œä»¥ä¸€ä¸ªæˆ–å¤šä¸ªå›¾åƒä¸ºæ¡ä»¶ï¼Œç”Ÿæˆå…·æœ‰ä»»æ„ç›¸æœºè½¨è¿¹å’Œæ—¶é—´æˆ³çš„å†…å®¹ã€‚é€šè¿‡é‡‡ç”¨æ–°å‹æ¶æ„å’Œé‡‡æ ·æµç¨‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ··åˆä½¿ç”¨3Dï¼ˆå¸¦æœ‰ç›¸æœºå§¿æ€ï¼‰ã€4Dï¼ˆå§¿æ€+æ—¶é—´ï¼‰å’Œè§†é¢‘ï¼ˆåªæœ‰æ—¶é—´æ²¡æœ‰å§¿æ€ï¼‰æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™æå¤§åœ°æé«˜äº†åœ¨æœªè§è¿‡çš„å›¾åƒå’Œç›¸æœºå§¿æ€è½¨è¿¹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œç›¸è¾ƒäºé‚£äº›ä¸“æ³¨äºæœ‰é™é¢†åŸŸï¼ˆä¾‹å¦‚ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„é¢†åŸŸï¼‰çš„å…ˆå‰å·¥ä½œã€‚4DiMæ˜¯é¦–ä¸ªèƒ½å¤Ÿé€šè¿‡æˆ‘ä»¬çš„æ–°å‹è¿åŠ¨æ¢å¤ç»“æ„æ ¡å‡†ç®¡é“å®ç°ç›´è§‚åº¦é‡å°ºåº¦ç›¸æœºå§¿æ€æ§åˆ¶çš„æ–°å‹è§†å›¾åˆæˆæ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å›¾åƒä¿çœŸåº¦å’Œå§¿æ€å¯¹é½æ–¹é¢ï¼Œ4DiMä¼˜äºå…ˆå‰çš„3D NVSæ¨¡å‹ï¼ŒåŒæ—¶èƒ½å¤Ÿå®ç°åœºæ™¯åŠ¨æ€ç”Ÿæˆã€‚4DiMä¸ºå„ç§ä»»åŠ¡æä¾›äº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼ŒåŒ…æ‹¬å•å›¾åƒåˆ°3Dã€ä¸¤å›¾åƒåˆ°è§†é¢‘ï¼ˆæ’å¸§å’Œå¤–æ’å¸§ï¼‰ï¼Œä»¥åŠå§¿æ€æ§åˆ¶è§†é¢‘åˆ°è§†é¢‘çš„è½¬æ¢ï¼Œæˆ‘ä»¬åœ¨å„ç§åœºæ™¯ä¸­å¯¹è¿™äº›è¿›è¡Œäº†å®šæ€§è¯´æ˜ã€‚æ¬²äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://4d-diffusion.github.io/">https://4d-diffusion.github.io</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07860v2">PDF</a> ICLR 2025, First three authors contributed equally</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åä¸º4DiMçš„çº§è”æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”¨äºæ”¯æŒä»¥ä»»æ„ç›¸æœºè½¨è¿¹å’Œæ—¶é—´æˆ³ç”Ÿæˆè‡ªç„¶åœºæ™¯çš„å››ç»´æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰ã€‚é€šè¿‡é‡‡ç”¨æ–°é¢–çš„æ¶æ„å’Œé‡‡æ ·ç¨‹åºï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ··åˆçš„3Dï¼ˆå¸¦æœ‰ç›¸æœºå§¿æ€ï¼‰ã€å››ç»´ï¼ˆå§¿æ€+æ—¶é—´ï¼‰å’Œè§†é¢‘ï¼ˆæ—¶é—´ä½†æ²¡æœ‰å§¿æ€ï¼‰æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæå¤§åœ°æé«˜äº†åœ¨æœªè§è¿‡çš„å›¾åƒå’Œç›¸æœºå§¿æ€è½¨è¿¹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„æ ¡å‡†ç®¡é“ä½¿æ¨¡å‹èƒ½å¤Ÿç›´è§‚æ§åˆ¶åº¦é‡å°ºåº¦ä¸‹çš„ç›¸æœºå§¿æ€ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„è¡¨ç°æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–å·¥ä½œç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜å›¾åƒä¿çœŸåº¦å’Œå§¿æ€å¯¹é½çš„åŒæ—¶ï¼Œè¿˜å¯ä»¥å®ç°åœºæ™¯åŠ¨æ€ç”Ÿæˆã€‚è¯¥æ¨¡å‹è¿˜ä¸ºå¤šç§ä»»åŠ¡æä¾›äº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼ŒåŒ…æ‹¬å•å›¾åƒåˆ°ä¸‰ç»´è½¬æ¢ã€åŒå›¾åƒåˆ°è§†é¢‘è½¬æ¢ç­‰ã€‚æ¬²äº†è§£æ›´å¤šè¯¦æƒ…ï¼Œè¯·è®¿é—®ç½‘ç«™é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†åä¸º4DiMçš„æ‰©æ•£æ¨¡å‹ç”¨äºå››ç»´æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰ã€‚</li>
<li>æ¨¡å‹æ”¯æŒä»»æ„ç›¸æœºè½¨è¿¹å’Œæ—¶é—´æˆ³çš„ç”Ÿæˆï¼Œåœ¨è‡ªç„¶åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39742d995786041feddb4917a38801b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fbec3e63935df018596bd5dda0b3603.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76770f72e5c3422b29c3bf5aa446794c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f875b006c3558ce51972a7f5b15cc130.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DreamDistribution-Learning-Prompt-Distribution-for-Diverse-In-distribution-Generation"><a href="#DreamDistribution-Learning-Prompt-Distribution-for-Diverse-In-distribution-Generation" class="headerlink" title="DreamDistribution: Learning Prompt Distribution for Diverse   In-distribution Generation"></a>DreamDistribution: Learning Prompt Distribution for Diverse   In-distribution Generation</h2><p><strong>Authors:Brian Nlong Zhao, Yuhang Xiao, Jiashu Xu, Xinyang Jiang, Yifan Yang, Dongsheng Li, Laurent Itti, Vibhav Vineet, Yunhao Ge</strong></p>
<p>The popularization of Text-to-Image (T2I) diffusion models enables the generation of high-quality images from text descriptions. However, generating diverse customized images with reference visual attributes remains challenging. This work focuses on personalizing T2I diffusion models at a more abstract concept or category level, adapting commonalities from a set of reference images while creating new instances with sufficient variations. We introduce a solution that allows a pretrained T2I diffusion model to learn a set of soft prompts, enabling the generation of novel images by sampling prompts from the learned distribution. These prompts offer text-guided editing capabilities and additional flexibility in controlling variation and mixing between multiple distributions. We also show the adaptability of the learned prompt distribution to other tasks, such as text-to-3D. Finally we demonstrate effectiveness of our approach through quantitative analysis including automatic evaluation and human assessment. Project website: <a target="_blank" rel="noopener" href="https://briannlongzhao.github.io/DreamDistribution">https://briannlongzhao.github.io/DreamDistribution</a> </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ™®åŠä½¿å¾—ä»æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡å›¾åƒæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå‚è€ƒè§†è§‰å±æ€§ç”Ÿæˆå¤šæ ·åŒ–å®šåˆ¶å›¾åƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™é¡¹å·¥ä½œä¸“æ³¨äºåœ¨æ›´æŠ½è±¡çš„æ¦‚å¿µæˆ–ç±»åˆ«å±‚é¢ä¸Šä¸ªæ€§åŒ–T2Iæ‰©æ•£æ¨¡å‹ï¼Œä»ä¸€ç»„å‚è€ƒå›¾åƒä¸­å¸å–å…±æ€§ï¼ŒåŒæ—¶åˆ›å»ºå…·æœ‰è¶³å¤Ÿå˜åŒ–çš„æ–°å®ä¾‹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œå…è®¸é¢„è®­ç»ƒçš„T2Iæ‰©æ•£æ¨¡å‹å­¦ä¹ ä¸€ç»„è½¯æç¤ºï¼ˆsoft promptsï¼‰ï¼Œé€šè¿‡ä»å·²å­¦ä¹ çš„åˆ†å¸ƒä¸­æŠ½æ ·æç¤ºæ¥ç”Ÿæˆæ–°å›¾åƒã€‚è¿™äº›æç¤ºæä¾›äº†æ–‡æœ¬æŒ‡å¯¼çš„ç¼–è¾‘åŠŸèƒ½ï¼Œå¹¶åœ¨æ§åˆ¶å¤šä¸ªåˆ†å¸ƒä¹‹é—´çš„å˜åŒ–å’Œæ··åˆæ–¹é¢æä¾›äº†é¢å¤–çš„çµæ´»æ€§ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å­¦ä¹ åˆ°çš„æç¤ºåˆ†å¸ƒå¯¹å…¶ä»–ä»»åŠ¡çš„é€‚åº”æ€§ï¼Œå¦‚æ–‡æœ¬åˆ°3Dã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åŒ…æ‹¬è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°åœ¨å†…çš„å®šé‡åˆ†æè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://briannlongzhao.github.io/DreamDistribution">https://briannlongzhao.github.io/DreamDistribution</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.14216v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ™®åŠä½¿å¾—ä»æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡å›¾åƒæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œæ ¹æ®å‚è€ƒè§†è§‰å±æ€§ç”Ÿæˆå¤šæ ·åŒ–å®šåˆ¶å›¾åƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡ä¸“æ³¨äºåœ¨æ›´æŠ½è±¡çš„æ¦‚å¿µæˆ–ç±»åˆ«çº§åˆ«ä¸ªæ€§åŒ–T2Iæ‰©æ•£æ¨¡å‹ï¼Œä»ä¸€ç»„å‚è€ƒå›¾åƒä¸­å¸å–å…±æ€§ï¼ŒåŒæ—¶åˆ›å»ºå…·æœ‰è¶³å¤Ÿå˜åŒ–çš„æ–°å®ä¾‹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œå…è®¸é¢„è®­ç»ƒçš„T2Iæ‰©æ•£æ¨¡å‹å­¦ä¹ ä¸€å¥—è½¯æç¤ºï¼Œé€šè¿‡ä»å­¦ä¹ çš„åˆ†å¸ƒä¸­é‡‡æ ·æç¤ºæ¥ç”Ÿæˆæ–°å›¾åƒã€‚è¿™äº›æç¤ºæä¾›æ–‡æœ¬æŒ‡å¯¼çš„ç¼–è¾‘åŠŸèƒ½ï¼Œå¹¶åœ¨æ§åˆ¶å¤šä¸ªåˆ†å¸ƒä¹‹é—´çš„å˜åŒ–å’Œæ··åˆæ–¹é¢æä¾›é¢å¤–çš„çµæ´»æ€§ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æ‰€å­¦æç¤ºåˆ†å¸ƒå¯¹å…¶ä»–ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ°3Dï¼‰çš„é€‚åº”æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡è‡ªåŠ¨è¯„ä¼°å’Œäººç±»è¯„ä¼°ç­‰å®šé‡åˆ†æè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹èƒ½å¤ŸåŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>ç”Ÿæˆå¤šæ ·åŒ–å®šåˆ¶å›¾åƒä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´é«˜çº§çš„ä¸ªæ€§åŒ–æŠ€æœ¯ã€‚</li>
<li>æœ¬æ–‡å…³æ³¨åœ¨æŠ½è±¡å±‚é¢ä¸ªæ€§åŒ–T2Iæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å­¦ä¹ è½¯æç¤ºæ¥é€‚åº”é¢„è®­ç»ƒçš„T2Iæ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆå…·æœ‰å˜åŒ–å’Œæ–°é¢–æ€§çš„å›¾åƒã€‚</li>
<li>æç¤ºæä¾›æ–‡æœ¬æŒ‡å¯¼çš„ç¼–è¾‘åŠŸèƒ½ï¼Œå¢åŠ æ§åˆ¶å›¾åƒå˜åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>æ‰€å­¦æç¤ºåˆ†å¸ƒå¯é€‚åº”å…¶ä»–ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°3Dã€‚</li>
<li>é€šè¿‡å®šé‡åˆ†æå’Œå®éªŒéªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.14216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64a306af38118b273dec7d6985baf5e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd75e3f3d174ba678d00edaea255c47d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abc9c1cebcbf71b0c8c79bf2c5a9b4c0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5c015040c65197a1ef8f66bcf0d13365.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  OPO Making Decision-Focused Data Acquisition Decisions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-bc3e010f839dfba9beb5bb49f4e34377.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  StyleMe3D Stylization with Disentangled Priors by Multiple Encoders on   3D Gaussians
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
