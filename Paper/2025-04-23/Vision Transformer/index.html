<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  Impact of Latent Space Dimension on IoT Botnet Detection Performance   VAE-Encoder Versus ViT-Encoder">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-722c847def8fad2a116798d8be87ec11.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    29 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-23-æ›´æ–°"><a href="#2025-04-23-æ›´æ–°" class="headerlink" title="2025-04-23 æ›´æ–°"></a>2025-04-23 æ›´æ–°</h1><h2 id="Impact-of-Latent-Space-Dimension-on-IoT-Botnet-Detection-Performance-VAE-Encoder-Versus-ViT-Encoder"><a href="#Impact-of-Latent-Space-Dimension-on-IoT-Botnet-Detection-Performance-VAE-Encoder-Versus-ViT-Encoder" class="headerlink" title="Impact of Latent Space Dimension on IoT Botnet Detection Performance:   VAE-Encoder Versus ViT-Encoder"></a>Impact of Latent Space Dimension on IoT Botnet Detection Performance:   VAE-Encoder Versus ViT-Encoder</h2><p><strong>Authors:Hassan Wasswa, Aziida Nanyonga, Timothy Lynar</strong></p>
<p>The rapid evolution of Internet of Things (IoT) technology has led to a significant increase in the number of IoT devices, applications, and services. This surge in IoT devices, along with their widespread presence, has made them a prime target for various cyber-attacks, particularly through IoT botnets. As a result, security has become a major concern within the IoT ecosystem. This study focuses on investigating how the latent dimension impacts the performance of different deep learning classifiers when trained on latent vector representations of the train dataset. The primary objective is to compare the outcomes of these models when encoder components from two cutting-edge architectures: the Vision Transformer (ViT) and the Variational Auto-Encoder (VAE) are utilized to project the high dimensional train dataset to the learned low dimensional latent space. The encoder components are employed to project high-dimensional structured .csv IoT botnet traffic datasets to various latent sizes. Evaluated on N-BaIoT and CICIoT2022 datasets, findings reveal that VAE-encoder based dimension reduction outperforms ViT-encoder based dimension reduction for both datasets in terms of four performance metrics including accuracy, precision, recall, and F1-score for all models which can be attributed to absence of spatial patterns in the datasets the ViT model attempts to learn and extract from image instances. </p>
<blockquote>
<p>éšç€ç‰©è”ç½‘ï¼ˆIoTï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç‰©è”ç½‘è®¾å¤‡ã€åº”ç”¨å’ŒæœåŠ¡æ•°é‡æ˜¾è‘—å¢åŠ ã€‚ç‰©è”ç½‘è®¾å¤‡çš„æ¿€å¢åŠå…¶å¹¿æ³›å­˜åœ¨ä½¿å…¶æˆä¸ºå„ç§ç½‘ç»œæ”»å‡»çš„ä¸»è¦ç›®æ ‡ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç‰©è”ç½‘åƒµå°¸ç½‘ç»œè¿›è¡Œçš„æ”»å‡»ã€‚å› æ­¤ï¼Œå®‰å…¨å·²æˆä¸ºç‰©è”ç½‘ç”Ÿæ€ç³»ç»Ÿä¸­çš„ä¸»è¦å…³æ³¨ç‚¹ã€‚æœ¬ç ”ç©¶é‡ç‚¹è°ƒæŸ¥æ½œåœ¨ç»´åº¦å¦‚ä½•å½±å“åœ¨ä¸åŒæ·±åº¦å­¦ä¹ åˆ†ç±»å™¨ä¸Šè®­ç»ƒæ—¶çš„æ€§èƒ½ï¼Œè¿™äº›åˆ†ç±»å™¨æ˜¯åœ¨è®­ç»ƒæ•°æ®é›†çš„æ½œåœ¨å‘é‡è¡¨ç¤ºä¸Šè®­ç»ƒçš„ã€‚ä¸»è¦ç›®æ ‡æ˜¯æ¯”è¾ƒä½¿ç”¨ä¸¤ç§å‰æ²¿æ¶æ„çš„ç¼–ç å™¨ç»„ä»¶æ—¶çš„æ¨¡å‹ç»“æœï¼šè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰ã€‚å®ƒä»¬è¢«ç”¨äºå°†é«˜ç»´åº¦çš„è®­ç»ƒæ•°æ®é›†æŠ•å½±åˆ°å­¦ä¹ åˆ°çš„ä½ç»´åº¦æ½œåœ¨ç©ºé—´ã€‚ç¼–ç å™¨ç»„ä»¶è¢«ç”¨äºå°†é«˜ç»´åº¦çš„ç»“æ„åŒ–.csvç‰©è”ç½‘åƒµå°¸ç½‘ç»œæµé‡æ•°æ®é›†æŠ•å½±åˆ°å„ç§æ½œåœ¨å¤§å°ã€‚åœ¨N-BaIoTå’ŒCICIoT2022æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç ”ç©¶å‘ç°åœ¨å››é¡¹æ€§èƒ½æŒ‡æ ‡ï¼ˆå‡†ç¡®æ€§ã€ç²¾ç¡®æ€§ã€å¬å›ç‡å’ŒF1åˆ†æ•°ï¼‰æ–¹é¢ï¼ŒåŸºäºVAEç¼–ç å™¨çš„é™ç»´åœ¨ä¸¤ç»„æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºäºViTç¼–ç å™¨çš„é™ç»´æ–¹æ³•ã€‚è¿™å¯ä»¥å½’å› äºæ•°æ®é›†ç¼ºä¹ç©ºé—´æ¨¡å¼ï¼Œè€ŒViTæ¨¡å‹è¯•å›¾ä»å›¾åƒå®ä¾‹ä¸­å­¦ä¹ å¹¶æå–è¿™äº›æ¨¡å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14879v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç‰©è”ç½‘æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´ç‰©è”ç½‘è®¾å¤‡ã€åº”ç”¨å’ŒæœåŠ¡æ•°é‡æ¿€å¢ï¼Œä½¿å¾—ç‰©è”ç½‘ç”Ÿæ€ç³»ç»Ÿé¢ä¸´ä¸¥é‡çš„å®‰å…¨å¨èƒï¼Œå°¤å…¶æ˜¯é€šè¿‡ç‰©è”ç½‘åƒµå°¸ç½‘ç»œè¿›è¡Œçš„å„ç§ç½‘ç»œæ”»å‡»ã€‚æœ¬ç ”ç©¶èšç„¦äºæ¢è®¨æ½œåœ¨ç»´åº¦å¯¹è®­ç»ƒåœ¨æ½œåœ¨å‘é‡è¡¨ç¤ºä¸Šçš„ä¸åŒæ·±åº¦å­¦ä¹ åˆ†ç±»å™¨æ€§èƒ½çš„å½±å“ã€‚ä¸»è¦ç›®çš„æ˜¯æ¯”è¾ƒä½¿ç”¨Vision Transformerï¼ˆViTï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„ç¼–ç å™¨ç»„ä»¶å°†é«˜ç»´ç»“æ„åŒ–.csvç‰©è”ç½‘åƒµå°¸ç½‘ç»œæµé‡æ•°æ®é›†æŠ•å½±åˆ°ä¸åŒçš„æ½œåœ¨ç»´åº¦ç©ºé—´æ—¶çš„æ¨¡å‹è¡¨ç°ã€‚åœ¨N-BaIoTå’ŒCICIoT2022æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒåŸºäºVAEç¼–ç å™¨çš„é™ç»´åœ¨å››ä¸ªæ€§èƒ½æŒ‡æ ‡ï¼ˆå‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ï¼‰ä¸Šå‡ä¼˜äºåŸºäºViTç¼–ç å™¨çš„é™ç»´ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ•°æ®é›†ä¸­ä¸å­˜åœ¨ç©ºé—´æ¨¡å¼ï¼Œè€ŒViTæ¨¡å‹è¯•å›¾ä»å›¾åƒå®ä¾‹ä¸­å­¦ä¹ å¹¶æå–è¿™äº›æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©è”ç½‘æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ä¼´éšç€ç‰©è”ç½‘è®¾å¤‡æ•°é‡çš„æ¿€å¢ï¼Œä½¿å…¶æˆä¸ºç½‘ç»œæ”»å‡»çš„ä¸»è¦ç›®æ ‡ã€‚</li>
<li>å®‰å…¨å·²æˆä¸ºç‰©è”ç½‘ç”Ÿæ€ç³»ç»Ÿçš„ä¸»è¦å…³æ³¨ç‚¹ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†æ½œåœ¨ç»´åº¦å¯¹æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨æ€§èƒ½çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç‰©è”ç½‘åƒµå°¸ç½‘ç»œæµé‡æ•°æ®æ—¶ã€‚</li>
<li>ä½¿ç”¨äº†Vision Transformerï¼ˆViTï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„ç¼–ç å™¨ç»„ä»¶è¿›è¡Œå®éªŒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºVAEç¼–ç å™¨çš„é™ç»´åœ¨å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šä¼˜äºåŸºäºViTç¼–ç å™¨çš„é™ç»´ã€‚</li>
<li>è¿™ç§å·®å¼‚å¯èƒ½æºäºæ•°æ®é›†ä¸­ç¼ºä¹ç©ºé—´æ¨¡å¼ï¼Œè€ŒViTæ¨¡å‹è¯•å›¾ä»è¿™äº›æ¨¡å¼ä¸­å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b7661e1667fbc1d8463267295e3685e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd06ad647eb2de3408af274670c9231a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-808c2907db03bd1306548c49933950c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e20565da00174fbaf5cbe3daeece6ea8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2915b27c64c9456e2de2e96eb89f7eba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3322d4148c9e285102d3d3914471f441.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ECViT-Efficient-Convolutional-Vision-Transformer-with-Local-Attention-and-Multi-scale-Stages"><a href="#ECViT-Efficient-Convolutional-Vision-Transformer-with-Local-Attention-and-Multi-scale-Stages" class="headerlink" title="ECViT: Efficient Convolutional Vision Transformer with Local-Attention   and Multi-scale Stages"></a>ECViT: Efficient Convolutional Vision Transformer with Local-Attention   and Multi-scale Stages</h2><p><strong>Authors:Zhoujie Qian</strong></p>
<p>Vision Transformers (ViTs) have revolutionized computer vision by leveraging self-attention to model long-range dependencies. However, ViTs face challenges such as high computational costs due to the quadratic scaling of self-attention and the requirement of a large amount of training data. To address these limitations, we propose the Efficient Convolutional Vision Transformer (ECViT), a hybrid architecture that effectively combines the strengths of CNNs and Transformers. ECViT introduces inductive biases such as locality and translation invariance, inherent to Convolutional Neural Networks (CNNs) into the Transformer framework by extracting patches from low-level features and enhancing the encoder with convolutional operations. Additionally, it incorporates local-attention and a pyramid structure to enable efficient multi-scale feature extraction and representation. Experimental results demonstrate that ECViT achieves an optimal balance between performance and efficiency, outperforming state-of-the-art models on various image classification tasks while maintaining low computational and storage requirements. ECViT offers an ideal solution for applications that prioritize high efficiency without compromising performance. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTï¼‰é€šè¿‡åˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯¹é•¿è·ç¦»ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå½»åº•æ”¹å˜äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚ç„¶è€Œï¼ŒViTé¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡æ‰©å±•å¯¼è‡´çš„é«˜è®¡ç®—æˆæœ¬å’Œå¯¹å¤§é‡è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆçš„å·ç§¯è§†è§‰Transformerï¼ˆECViTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆç»“åˆCNNå’ŒTransformerä¼˜åŠ¿çš„æ··åˆæ¶æ„ã€‚ECViTé€šè¿‡åœ¨Transformeræ¡†æ¶ä¸­å¼•å…¥å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ‰€å›ºæœ‰çš„å±€éƒ¨æ€§å’Œå¹³ç§»ä¸å˜æ€§ç­‰å½’çº³åè§ï¼Œé€šè¿‡ä»ä½çº§ç‰¹å¾ä¸­æå–è¡¥ä¸å¹¶å¢å¼ºç¼–ç å™¨ä¸­çš„å·ç§¯æ“ä½œæ¥å®ç°ã€‚æ­¤å¤–ï¼Œå®ƒç»“åˆäº†å±€éƒ¨æ³¨æ„åŠ›å’Œé‡‘å­—å¡”ç»“æ„ï¼Œä»¥å®ç°é«˜æ•ˆçš„å¤šå°ºåº¦ç‰¹å¾æå–å’Œè¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒECViTåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œåœ¨å„ç§å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†æœ€æ–°æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ã€‚å¯¹äºæ³¨é‡é«˜æ•ˆç‡è€Œä¸ç‰ºç‰²æ€§èƒ½çš„åº”ç”¨ï¼ŒECViTæä¾›äº†ç†æƒ³çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14825v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é«˜æ•ˆå·ç§¯è§†è§‰è½¬æ¢å™¨ï¼ˆECViTï¼‰ç»“åˆäº†CNNå’ŒTransformerçš„ä¼˜ç‚¹ï¼Œè§£å†³äº†è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰é¢ä¸´çš„é«˜è®¡ç®—æˆæœ¬å’Œéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å±€éƒ¨æ€§å’Œå¹³ç§»ä¸å˜æ€§ç­‰å½’çº³åè§ï¼Œä»¥åŠå±€éƒ¨æ³¨æ„åŠ›å’Œé‡‘å­—å¡”ç»“æ„ï¼ŒECViTå®ç°äº†æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„ä¼˜åŒ–å¹³è¡¡ï¼Œåœ¨å¤šç§å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºæœ€æ–°æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒä½è®¡ç®—å’Œå­˜å‚¨è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰é€šè¿‡è‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶å¯¹è®¡ç®—æœºè§†è§‰é¢†åŸŸäº§ç”Ÿäº†é©å‘½æ€§çš„å½±å“ã€‚</li>
<li>ViTé¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚</li>
<li>ECViTæ˜¯ä¸€ä¸ªæ··åˆæ¶æ„ï¼Œæ—¨åœ¨è§£å†³ViTçš„æŒ‘æˆ˜ï¼Œç»“åˆäº†CNNå’ŒTransformerçš„ä¼˜ç‚¹ã€‚</li>
<li>ECViTé€šè¿‡å°†CNNçš„å½’çº³åè§ï¼ˆå¦‚å±€éƒ¨æ€§å’Œå¹³ç§»ä¸å˜æ€§ï¼‰å¼•å…¥Transformeræ¡†æ¶æ¥å¢å¼ºæ€§èƒ½ã€‚</li>
<li>ECViTé€šè¿‡å¼•å…¥å±€éƒ¨æ³¨æ„åŠ›å’Œé‡‘å­—å¡”ç»“æ„ï¼Œå®ç°äº†å¤šå°ºåº¦ç‰¹å¾çš„æœ‰æ•ˆæå–å’Œè¡¨ç¤ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒECViTåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ï¼Œåœ¨å¤šç§å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a700e5a5585a977448f7a540fca8f88a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b805c1f949665125b705767fb9903849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92f41f7150ee991e396ffee15139836.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eedd34d2dc9cda7ad614964c819a842e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Zooming-In-on-Fakes-A-Novel-Dataset-for-Localized-AI-Generated-Image-Detection-with-Forgery-Amplification-Approach"><a href="#Zooming-In-on-Fakes-A-Novel-Dataset-for-Localized-AI-Generated-Image-Detection-with-Forgery-Amplification-Approach" class="headerlink" title="Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image   Detection with Forgery Amplification Approach"></a>Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image   Detection with Forgery Amplification Approach</h2><p><strong>Authors:Lvpan Cai, Haowei Wang, Jiayi Ji, YanShu ZhouMen, Yiwei Ma, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji</strong></p>
<p>The rise of AI-generated image editing tools has made localized forgeries increasingly realistic, posing challenges for visual content integrity. Although recent efforts have explored localized AIGC detection, existing datasets predominantly focus on object-level forgeries while overlooking broader scene edits in regions such as sky or ground. To address these limitations, we introduce \textbf{BR-Gen}, a large-scale dataset of 150,000 locally forged images with diverse scene-aware annotations, which are based on semantic calibration to ensure high-quality samples. BR-Gen is constructed through a fully automated Perception-Creation-Evaluation pipeline to ensure semantic coherence and visual realism. In addition, we further propose \textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that enhances the detection of localized forgeries by amplifying forgery-related features across the entire image. NFA-ViT mines heterogeneous regions in images, \emph{i.e.}, potential edited areas, by noise fingerprints. Subsequently, attention mechanism is introduced to compel the interaction between normal and abnormal features, thereby propagating the generalization traces throughout the entire image, allowing subtle forgeries to influence a broader context and improving overall detection robustness. Extensive experiments demonstrate that BR-Gen constructs entirely new scenarios that are not covered by existing methods. Take a step further, NFA-ViT outperforms existing methods on BR-Gen and generalizes well across current benchmarks. All data and codes are available at <a target="_blank" rel="noopener" href="https://github.com/clpbc/BR-Gen">https://github.com/clpbc/BR-Gen</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒç¼–è¾‘å·¥å…·çš„å…´èµ·ä½¿å¾—å±€éƒ¨ä¼ªé€ è¶Šæ¥è¶Šé€¼çœŸï¼Œç»™è§†è§‰å†…å®¹å®Œæ•´æ€§å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›å·²ç»æ¢ç´¢äº†å±€éƒ¨AIGCæ£€æµ‹ï¼Œä½†ç°æœ‰æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨å¯¹è±¡çº§åˆ«çš„ä¼ªé€ ï¼Œè€Œå¿½ç•¥äº†å¤©ç©ºæˆ–åœ°é¢ç­‰åŒºåŸŸçš„æ›´å¹¿æ³›çš„åœºæ™¯ç¼–è¾‘ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>BR-Gen</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«15ä¸‡å¼ å±€éƒ¨ä¼ªé€ å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…·æœ‰åŸºäºè¯­ä¹‰æ ¡å‡†çš„å¤šæ ·åŒ–åœºæ™¯æ„ŸçŸ¥æ³¨é‡Šï¼Œä»¥ç¡®ä¿é«˜è´¨é‡æ ·æœ¬ã€‚BR-Gené€šè¿‡å…¨è‡ªåŠ¨çš„æ„ŸçŸ¥-åˆ›å»º-è¯„ä¼°æµç¨‹æ„å»ºï¼Œä»¥ç¡®ä¿è¯­ä¹‰è¿è´¯å’Œè§†è§‰çœŸå®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†<strong>NFA-ViT</strong>ï¼Œä¸€ç§å™ªå£°å¼•å¯¼çš„ä¼ªé€ æ”¾å¤§è§†è§‰è½¬æ¢å™¨ï¼Œé€šè¿‡æ”¾å¤§æ•´ä¸ªå›¾åƒä¸­çš„ä¼ªé€ ç›¸å…³ç‰¹å¾ï¼Œå¢å¼ºå±€éƒ¨ä¼ªé€ çš„æ£€æµ‹ã€‚NFA-ViTé€šè¿‡å™ªå£°æŒ‡çº¹æŒ–æ˜å›¾åƒä¸­çš„å¼‚è´¨åŒºåŸŸï¼Œå³æ½œåœ¨ç¼–è¾‘åŒºåŸŸã€‚éšåï¼Œå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æ¥ä¿ƒä½¿æ­£å¸¸å’Œå¼‚å¸¸ç‰¹å¾ä¹‹é—´çš„äº¤äº’ï¼Œä»è€Œåœ¨æ•´å¹…å›¾åƒä¸­ä¼ æ’­æ³›åŒ–ç—•è¿¹ï¼Œä½¿ç»†å¾®çš„ä¼ªé€ èƒ½å¤Ÿå½±å“æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ï¼Œæé«˜æ•´ä½“æ£€æµ‹ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBR-Genæ„å»ºçš„åœºæ™¯å®Œå…¨è¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„è¦†ç›–èŒƒå›´ã€‚æ›´è¿›ä¸€æ­¥çš„æ˜¯ï¼ŒNFA-ViTåœ¨BR-Genä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å½“å‰åŸºå‡†æµ‹è¯•ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/clpbc/BR-Gen%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/clpbc/BR-Genæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11922v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†AIç”Ÿæˆå›¾åƒç¼–è¾‘å·¥å…·çš„å…´èµ·å¸¦æ¥çš„å±€éƒ¨ç¯¡æ”¹æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†ä¸»è¦å…³æ³¨å¯¹è±¡çº§ç¯¡æ”¹ï¼Œè€Œå¿½è§†å¤©ç©ºæˆ–åœ°é¢ç­‰æ›´å¹¿åœºæ™¯ç¼–è¾‘çš„é—®é¢˜ï¼Œæå‡ºäº†BR-Genå¤§å‹æœ¬åœ°åŒ–ä¼ªé€ å›¾åƒæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é€šè¿‡å…¨è‡ªåŠ¨çš„æ„ŸçŸ¥-åˆ›å»º-è¯„ä¼°æµç¨‹æ„å»ºï¼Œç¡®ä¿è¯­ä¹‰è¿è´¯å’Œè§†è§‰çœŸå®æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†NFA-ViTå™ªå£°å¼•å¯¼ä¼ªé€ æ”¾å¤§è§†è§‰è½¬æ¢å™¨ï¼Œé€šè¿‡æ”¾å¤§ä¼ªé€ ç›¸å…³ç‰¹å¾ï¼Œå¢å¼ºå±€éƒ¨ç¯¡æ”¹çš„æ£€æµ‹èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒBR-Genæ•°æ®é›†å¼€åˆ›äº†å…¨æ–°åœºæ™¯ï¼ŒNFA-ViTåœ¨BR-Genä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å½“å‰åŸºå‡†æµ‹è¯•ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆçš„å›¾åƒç¼–è¾‘å·¥å…·ä½¿å¾—å±€éƒ¨ä¼ªé€ è¶Šæ¥è¶Šé€¼çœŸï¼Œå¯¹è§†è§‰å†…å®¹å®Œæ•´æ€§æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ä¸»è¦å…³æ³¨å¯¹è±¡çº§ä¼ªé€ ï¼Œè€Œå¿½è§†æ›´å¹¿æ³›çš„åœºæ™¯ç¼–è¾‘ï¼Œå¦‚å¤©ç©ºæˆ–åœ°é¢ã€‚</li>
<li>å¼•å…¥BR-Genå¤§å‹æœ¬åœ°åŒ–ä¼ªé€ å›¾åƒæ•°æ®é›†ï¼Œé€šè¿‡å…¨è‡ªåŠ¨æµç¨‹æ„å»ºï¼Œç¡®ä¿è¯­ä¹‰è¿è´¯å’Œè§†è§‰çœŸå®æ€§ã€‚</li>
<li>æå‡ºNFA-ViTå™ªå£°å¼•å¯¼ä¼ªé€ æ”¾å¤§è§†è§‰è½¬æ¢å™¨ï¼Œå¢å¼ºå±€éƒ¨ç¯¡æ”¹æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>NFA-ViTé€šè¿‡æ”¾å¤§ä¼ªé€ ç›¸å…³ç‰¹å¾ï¼ŒæŒ–æ˜å›¾åƒä¸­çš„å¼‚è´¨åŒºåŸŸï¼Œå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºæ­£å¸¸ä¸å¼‚å¸¸ç‰¹å¾é—´çš„äº¤äº’ã€‚</li>
<li>NFA-ViTèƒ½åœ¨ç»†å¾®ç¯¡æ”¹ä¸­å½±å“æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ï¼Œæé«˜æ£€æµ‹ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e50e348e13ee20dbd74d571a59bd05f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4f3b990b18034e7e5069a16e847a975.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1149e5f29122b0b328ed32102afeef58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ff20f0212f1bcd8d63c0eccd9476360.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MedUnifier-Unifying-Vision-and-Language-Pre-training-on-Medical-Data-with-Vision-Generation-Task-using-Discrete-Visual-Representations"><a href="#MedUnifier-Unifying-Vision-and-Language-Pre-training-on-Medical-Data-with-Vision-Generation-Task-using-Discrete-Visual-Representations" class="headerlink" title="MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data   with Vision Generation Task using Discrete Visual Representations"></a>MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data   with Vision Generation Task using Discrete Visual Representations</h2><p><strong>Authors:Ziyang Zhang, Yang Yu, Yucheng Chen, Xulei Yang, Si Yong Yeo</strong></p>
<p>Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content. This gap hinders the modelâ€™s ability to synthesize coherent and novel visual representations from textual prompts, thereby reducing the effectiveness of multi-modal learning. In this work, we propose MedUnifier, a unified VLP framework tailored for medical data. MedUnifier seamlessly integrates text-grounded image generation capabilities with multi-modal learning strategies, including image-text contrastive alignment, image-text matching and image-grounded text generation. Unlike traditional methods that reply on continuous visual representations, our approach employs visual vector quantization, which not only facilitates a more cohesive learning strategy for cross-modal understanding but also enhances multi-modal generation quality by effectively leveraging discrete representations. Our frameworkâ€™s effectiveness is evidenced by the experiments on established benchmarks, including uni-modal tasks (supervised fine-tuning), cross-modal tasks (image-text retrieval and zero-shot image classification), and multi-modal tasks (medical report generation, image synthesis), where it achieves state-of-the-art performance across various tasks. MedUnifier also offers a highly adaptable tool for a wide range of language and vision tasks in healthcare, marking advancement toward the development of a generalizable AI model for medical applications. </p>
<blockquote>
<p>å°½ç®¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•ä¸»è¦ä¾§é‡äºç‰¹å¾æå–å’Œè·¨æ¨¡æ€ç†è§£ï¼Œå¯¹ç”Ÿæˆæˆ–è½¬æ¢è§†è§‰å†…å®¹çš„å…³æ³¨æœ‰é™ã€‚è¿™ä¸€å·®è·é˜»ç¢äº†æ¨¡å‹ä»æ–‡æœ¬æç¤ºä¸­åˆæˆè¿è´¯ä¸”æ–°é¢–çš„è§†è§‰è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œä»è€Œé™ä½äº†å¤šæ¨¡æ€å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹åŒ»ç–—æ•°æ®çš„ç»Ÿä¸€VLPæ¡†æ¶MedUnifierã€‚MedUnifieræ— ç¼é›†æˆäº†åŸºäºæ–‡æœ¬çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ä¸å¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬å¯¹æ¯”å¯¹é½ã€å›¾åƒæ–‡æœ¬åŒ¹é…å’ŒåŸºäºå›¾åƒçš„æ–‡æœ¬ç”Ÿæˆã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ä¾èµ–è¿ç»­è§†è§‰è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è§†è§‰å‘é‡é‡åŒ–ï¼Œè¿™ä¸ä»…æœ‰åŠ©äºæ›´è¿è´¯çš„è·¨æ¨¡æ€ç†è§£å­¦ä¹ ç­–ç•¥ï¼Œè€Œä¸”é€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç¦»æ•£è¡¨ç¤ºæé«˜äº†å¤šæ¨¡æ€ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ—¢å®šåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å•æ¨¡æ€ä»»åŠ¡ï¼ˆç›‘ç£å¾®è°ƒï¼‰ã€è·¨æ¨¡æ€ä»»åŠ¡ï¼ˆå›¾åƒæ–‡æœ¬æ£€ç´¢å’Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼ˆåŒ»å­¦æŠ¥å‘Šç”Ÿæˆã€å›¾åƒåˆæˆï¼‰ã€‚å®ƒåœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚MedUnifierè¿˜ä¸ºåŒ»ç–—é¢†åŸŸçš„å¹¿æ³›è¯­è¨€å’Œè§†è§‰ä»»åŠ¡æä¾›äº†é«˜åº¦é€‚åº”çš„å·¥å…·ï¼Œæ ‡å¿—ç€åœ¨å¼€å‘ç”¨äºåŒ»å­¦åº”ç”¨çš„å¯æ¨å¹¿äººå·¥æ™ºèƒ½æ¨¡å‹æ–¹é¢å–å¾—äº†è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01019v3">PDF</a> To be pubilshed in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒ»ç–—æ•°æ®çš„ç»Ÿä¸€è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶MedUnifierã€‚è¯¥æ¡†æ¶èåˆäº†æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆèƒ½åŠ›ä¸å¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬å¯¹æ¯”å¯¹é½ã€å›¾åƒæ–‡æœ¬åŒ¹é…å’Œå›¾åƒé©±åŠ¨æ–‡æœ¬ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„è¿ç»­è§†è§‰è¡¨ç¤ºæ–¹æ³•ä¸åŒï¼ŒMedUnifieré‡‡ç”¨è§†è§‰å‘é‡é‡åŒ–æŠ€æœ¯ï¼Œæ—¢æå‡äº†è·¨æ¨¡æ€ç†è§£çš„æ•´åˆå­¦ä¹ ç­–ç•¥ï¼Œåˆé€šè¿‡æœ‰æ•ˆè¿ç”¨ç¦»æ•£è¡¨ç¤ºæé«˜äº†å¤šæ¨¡æ€ç”Ÿæˆè´¨é‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒMedUnifierå®ç°äº†å„é¡¹ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å•æ¨¡æ€ä»»åŠ¡ã€è·¨æ¨¡æ€ä»»åŠ¡å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¹¶ä¸ºåŒ»ç–—å¥åº·é¢†åŸŸæä¾›äº†å¹¿æ³›é€‚åº”çš„è¯­è¨€å’Œè§†è§‰ä»»åŠ¡å·¥å…·ï¼Œæ ‡å¿—ç€åŒ»ç–—åº”ç”¨é€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹çš„å‘å±•è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedUnifieræ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»ç–—æ•°æ®çš„ç»Ÿä¸€è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>èåˆäº†æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆèƒ½åŠ›ä¸å¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ã€‚</li>
<li>æ¡†æ¶åŒ…å«å›¾åƒæ–‡æœ¬å¯¹æ¯”å¯¹é½ã€å›¾åƒæ–‡æœ¬åŒ¹é…å’Œå›¾åƒé©±åŠ¨æ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨è§†è§‰å‘é‡é‡åŒ–æŠ€æœ¯ï¼Œæå‡è·¨æ¨¡æ€ç†è§£çš„æ•´åˆå­¦ä¹ ç­–ç•¥åŠå¤šæ¨¡æ€ç”Ÿæˆè´¨é‡ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å•æ¨¡æ€ã€è·¨æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>æä¾›äº†é€‚åº”äºåŒ»ç–—å¥åº·é¢†åŸŸå¹¿æ³›è¯­è¨€å’Œè§†è§‰ä»»åŠ¡å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-997c6247229567188fb01e92ea809d20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0aebc8fa0d049e771d53e80967768da2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35cf3560f8fcbbb66ee188f4fcade67b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Tree-of-Attributes-Prompt-Learning-for-Vision-Language-Models"><a href="#Tree-of-Attributes-Prompt-Learning-for-Vision-Language-Models" class="headerlink" title="Tree of Attributes Prompt Learning for Vision-Language Models"></a>Tree of Attributes Prompt Learning for Vision-Language Models</h2><p><strong>Authors:Tong Ding, Wanhua Li, Zhongqi Miao, Hanspeter Pfister</strong></p>
<p>Prompt learning has proven effective in adapting vision language models for downstream tasks. However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully leverage the rich context indicated in the category name. To address this issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs LLMs to generate a tree of attributes with a â€œconcept - attribute - descriptionâ€ structure for each category, and then learn the hierarchy with vision and text prompt tokens. Unlike existing methods that merely augment category names with a set of unstructured descriptions, our approach essentially distills structured knowledge graphs associated with class names from LLMs. Furthermore, our approach introduces text and vision prompts designed to explicitly learn the corresponding visual attributes, effectively serving as domain experts. Additionally, the general and diverse descriptions generated based on the class names may be wrong or absent in the specific given images. To address this misalignment, we further introduce a vision-conditional pooling module to extract instance-specific text features. Extensive experimental results demonstrate that our approach outperforms state-of-the-art methods on the zero-shot base-to-novel generalization, cross-dataset transfer, as well as few-shot classification across 11 diverse datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HHenryD/TAP">https://github.com/HHenryD/TAP</a>. </p>
<blockquote>
<p>æç¤ºå­¦ä¹ å·²è¯æ˜åœ¨é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ä»¥è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡æ–¹é¢éå¸¸æœ‰æ•ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä»…å°†å¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œé™„åŠ åˆ°ç±»åˆ«åç§°ä¸Šä»¥è·å¾—æ–‡æœ¬ç‰¹å¾ï¼Œè¿™æœªèƒ½å……åˆ†åˆ©ç”¨ç±»åˆ«åç§°ä¸­æŒ‡ç¤ºçš„ä¸°å¯Œä¸Šä¸‹æ–‡ã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å±æ€§æ ‘æç¤ºå­¦ä¹ ï¼ˆTAPï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆå…·æœ‰â€œæ¦‚å¿µ-å±æ€§-æè¿°â€ç»“æ„çš„å±æ€§æ ‘ï¼Œç„¶åå­¦ä¹ ä¸è§†è§‰å’Œæ–‡æœ¬æç¤ºä»¤ç‰Œå¯¹åº”çš„å±‚æ¬¡ç»“æ„ã€‚ä¸ä»…å°†ç±»åˆ«åç§°ä¸ä¸€ç»„éç»“æ„æè¿°ç›¸ç»“åˆçš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è’¸é¦å‡ºä¸ç±»åç›¸å…³çš„ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç”¨äºæ˜ç¡®å­¦ä¹ ç›¸åº”è§†è§‰å±æ€§çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæœ‰æ•ˆåœ°å……å½“é¢†åŸŸä¸“å®¶ã€‚åŸºäºç±»åç”Ÿæˆçš„é€šç”¨ä¸”å¤šæ ·çš„æè¿°åœ¨ç‰¹å®šç»™å®šçš„å›¾åƒä¸­å¯èƒ½å‡ºé”™æˆ–ç¼ºå¤±ã€‚ä¸ºäº†è§£å†³è¿™ç§ä¸åŒ¹é…é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªè§†è§‰æ¡ä»¶æ± æ¨¡å—æ¥æå–å®ä¾‹ç‰¹å®šçš„æ–‡æœ¬ç‰¹å¾ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»ä»¥åŠåœ¨11ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å°‘æ ·æœ¬åˆ†ç±»æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/HHenryD/TAP%E3%80%82">https://github.com/HHenryD/TAPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11201v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTree of Attributes Promptï¼ˆTAPï¼‰çš„å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸‹æ¸¸ä»»åŠ¡é€‚åº”æ€§ã€‚TAPé€šè¿‡æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆå±æ€§æ ‘ï¼Œå½¢æˆâ€œæ¦‚å¿µ-å±æ€§-æè¿°â€çš„ç»“æ„ï¼Œå¹¶å­¦ä¹ ä¸è§†è§‰å’Œæ–‡æœ¬æç¤ºæ ‡è®°çš„å±‚æ¬¡ç»“æ„ã€‚ç›¸è¾ƒäºä»…å°†ç±»åˆ«åç§°ä¸ä¸€ç³»åˆ—éç»“æ„æè¿°ç›¸ç»“åˆçš„æ–¹æ³•ï¼ŒTAPèƒ½æç‚¼ä¸ç±»åˆ«åç§°ç›¸å…³çš„ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼Œå¹¶å¼•å…¥æ–‡æœ¬å’Œè§†è§‰æç¤ºä»¥æ˜ç¡®å­¦ä¹ ç›¸åº”çš„è§†è§‰å±æ€§ã€‚åŒæ—¶ï¼Œä¸ºè§£å†³ç‰¹å®šå›¾åƒä¸­å¯èƒ½å­˜åœ¨çš„æè¿°é”™è¯¯æˆ–ç¼ºå¤±é—®é¢˜ï¼Œå¼•å…¥è§†è§‰æ¡ä»¶æ± åŒ–æ¨¡å—ä»¥æå–å®ä¾‹ç‰¹å®šçš„æ–‡æœ¬ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTAPåœ¨é›¶æ ·æœ¬åŸºç¡€åˆ°æ–°é¢–ç±»åˆ«çš„æ¨å¹¿ã€è·¨æ•°æ®é›†è¿ç§»ä»¥åŠåœ¨11ä¸ªä¸åŒæ•°æ®é›†çš„å°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TAPé€šè¿‡ç”Ÿæˆå±æ€§æ ‘æ¥ä¸°å¯Œç±»åˆ«ä¿¡æ¯çš„è¯­å¢ƒè¡¨è¾¾ã€‚</li>
<li>TAPåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸ç±»åˆ«åç§°ç›¸å…³çš„ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ã€‚</li>
<li>TAPå¼•å…¥æ–‡æœ¬å’Œè§†è§‰æç¤ºæ¥å­¦ä¹ è§†è§‰å±æ€§ï¼Œç±»ä¼¼äºé¢†åŸŸä¸“å®¶ã€‚</li>
<li>é’ˆå¯¹ç‰¹å®šå›¾åƒå¯èƒ½å‡ºç°çš„æè¿°é”™è¯¯æˆ–ç¼ºå¤±ï¼Œå¼•å…¥è§†è§‰æ¡ä»¶æ± åŒ–æ¨¡å—ã€‚</li>
<li>TAPåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬å­¦ä¹ å’Œå°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šã€‚</li>
<li>TAPæ–¹æ³•å…¬å¼€å¯ç”¨ï¼Œç›¸å…³ä»£ç å¯è®¿é—®ç‰¹å®šé“¾æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11201">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3898defe56ce39f9a99c26949d694faf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-722c847def8fad2a116798d8be87ec11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-874bb9fdbf0b09085549e51626372a42.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Inspecting-Explainability-of-Transformer-Models-with-Additional-Statistical-Information"><a href="#Inspecting-Explainability-of-Transformer-Models-with-Additional-Statistical-Information" class="headerlink" title="Inspecting Explainability of Transformer Models with Additional   Statistical Information"></a>Inspecting Explainability of Transformer Models with Additional   Statistical Information</h2><p><strong>Authors:Hoang C. Nguyen, Haeil Lee, Junmo Kim</strong></p>
<p>Transformer becomes more popular in the vision domain in recent years so there is a need for finding an effective way to interpret the Transformer model by visualizing it. In recent work, Chefer et al. can visualize the Transformer on vision and multi-modal tasks effectively by combining attention layers to show the importance of each image patch. However, when applying to other variants of Transformer such as the Swin Transformer, this method can not focus on the predicted object. Our method, by considering the statistics of tokens in layer normalization layers, shows a great ability to interpret the explainability of Swin Transformer and ViT. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒTransformeråœ¨è§†è§‰é¢†åŸŸè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå› æ­¤æœ‰å¿…è¦æ‰¾åˆ°ä¸€ç§é€šè¿‡å¯è§†åŒ–æ¥è§£é‡ŠTransformeræ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ã€‚Cheferç­‰äººæœ€è¿‘çš„å·¥ä½œèƒ½å¤Ÿé€šè¿‡ç»“åˆæ³¨æ„åŠ›å±‚æ¥å±•ç¤ºæ¯ä¸ªå›¾åƒå—çš„é‡è¦æ€§ï¼Œä»è€Œåœ¨è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šæœ‰æ•ˆåœ°å¯è§†åŒ–Transformerã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºSwin Transformerç­‰å…¶ä»–Transformerå˜ä½“æ—¶ï¼Œè¿™ç§æ–¹æ³•æ— æ³•å…³æ³¨é¢„æµ‹ç›®æ ‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•è€ƒè™‘äº†å±‚å½’ä¸€åŒ–å±‚ä¸­tokençš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå±•ç¤ºäº†æé«˜çš„è§£é‡Šæ€§èƒ½åŠ›ï¼Œèƒ½å¤Ÿè§£é‡ŠSwin Transformerå’ŒViTçš„å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11378v2">PDF</a> Accepted at Responsible Computer Vision workshop at ECCV 2022</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸTransformeråœ¨è§†è§‰é¢†åŸŸè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå› æ­¤äºŸéœ€æ‰¾åˆ°ä¸€ç§æœ‰æ•ˆçš„å¯è§†åŒ–è§£é‡Šæ–¹æ³•ã€‚Cheferç­‰äººé€šè¿‡å°†æ³¨æ„åŠ›å±‚ç›¸ç»“åˆï¼Œå±•ç¤ºäº†ä¸€ç§é€‚ç”¨äºè§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡çš„Transformerå¯è§†åŒ–æ–¹æ³•ã€‚ä½†åº”ç”¨åˆ°å…¶ä»–å˜ç§å¦‚Swin Transformeræ—¶ï¼Œè¯¥æ–¹æ³•æ— æ³•å…³æ³¨é¢„æµ‹ç›®æ ‡ã€‚è€Œæˆ‘ä»¬çš„æ–¹æ³•åˆ™é€šè¿‡è€ƒè™‘å±‚å½’ä¸€åŒ–å±‚ä¸­çš„tokenç»Ÿè®¡ï¼Œå±•ç°å‡ºå¼ºå¤§çš„Swin Transformerå’ŒViTçš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Transformeræ¨¡å‹åœ¨è§†è§‰é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>å¯è§†åŒ–è§£é‡Šæ–¹æ³•å¯¹äºç†è§£Transformeræ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>Cheferç­‰äººçš„æ–¹æ³•é€‚ç”¨äºè§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡çš„Transformerå¯è§†åŒ–ã€‚</li>
<li>å½“åº”ç”¨äºSwin Transformerç­‰å˜ä½“æ—¶ï¼Œç°æœ‰æ–¹æ³•å¯èƒ½æ— æ³•å…³æ³¨é¢„æµ‹ç›®æ ‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡è€ƒè™‘å±‚å½’ä¸€åŒ–å±‚ä¸­çš„tokenç»Ÿè®¡æ¥è§£é‡ŠSwin Transformerå’ŒViTã€‚</li>
<li>è¯¥æ–°æ–¹æ³•å…·æœ‰å¼ºå¤§çš„å¯è§£é‡Šæ€§èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.11378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e7c73e43e1601e20ca97927424643c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbf3bcecfd3a91742d74ef4ac1de3823.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-124b962ec5e1110810d53519e65c7b55.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AnomalyCLIP-Object-agnostic-Prompt-Learning-for-Zero-shot-Anomaly-Detection"><a href="#AnomalyCLIP-Object-agnostic-Prompt-Learning-for-Zero-shot-Anomaly-Detection" class="headerlink" title="AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly   Detection"></a>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly   Detection</h2><p><strong>Authors:Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen</strong></p>
<p>Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects&#x2F;tumors on different products&#x2F;organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality&#x2F;normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at <a target="_blank" rel="noopener" href="https://github.com/zqhang/AnomalyCLIP">https://github.com/zqhang/AnomalyCLIP</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰è¦æ±‚ä½¿ç”¨è¾…åŠ©æ•°æ®è®­ç»ƒçš„æ£€æµ‹æ¨¡å‹èƒ½å¤Ÿåœ¨ç›®æ ‡æ•°æ®é›†ä¸­æ— éœ€ä»»ä½•è®­ç»ƒæ ·æœ¬å³å¯æ£€æµ‹å¼‚å¸¸å€¼ã€‚å½“ç”±äºå„ç§åŸå› æ— æ³•è·å–è®­ç»ƒæ•°æ®æ—¶ï¼Œè¿™æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ•°æ®éšç§ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹éœ€è¦æ³›åŒ–åˆ°ä¸åŒé¢†åŸŸçš„å¼‚å¸¸å€¼ï¼Œå› æ­¤é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå‰æ™¯å¯¹è±¡ã€å¼‚å¸¸åŒºåŸŸå’ŒèƒŒæ™¯ç‰¹å¾ï¼ˆå¦‚åœ¨ä¸åŒäº§å“&#x2F;å™¨å®˜ä¸Šçš„ç¼ºé™·&#x2F;è‚¿ç˜¤ï¼‰çš„å¤–è§‚å¯èƒ½ä¼šæœ‰å¾ˆå¤§å·®å¼‚ã€‚æœ€è¿‘çš„å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œåœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ZSADæ–¹é¢çš„è¡¨ç°è¾ƒå¼±ï¼Œå› ä¸ºVLMsæ›´ä¾§é‡äºå¯¹å‰æ™¯å¯¹è±¡çš„ç±»åˆ«è¯­ä¹‰è¿›è¡Œå»ºæ¨¡ï¼Œè€Œä¸æ˜¯å›¾åƒä¸­çš„å¼‚å¸¸&#x2F;æ­£å¸¸æƒ…å†µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå³AnomalyCLIPï¼Œè¯¥æ–¹æ³•æ—¨åœ¨ä½¿CLIPé€‚åº”è·¨ä¸åŒé¢†åŸŸçš„å‡†ç¡®ZSADã€‚AnomalyCLIPçš„å…³é”®åœ¨äºå­¦ä¹ å¯¹è±¡æ— å…³çš„æ–‡æœ¬æç¤ºï¼Œè¿™äº›æç¤ºå¯ä»¥æ•è·å›¾åƒä¸­çš„é€šç”¨æ­£å¸¸å’Œå¼‚å¸¸æƒ…å†µï¼Œè€Œæ— è®ºå…¶å‰æ™¯å¯¹è±¡å¦‚ä½•ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå…³æ³¨å¼‚å¸¸å›¾åƒåŒºåŸŸï¼Œè€Œä¸æ˜¯å¯¹è±¡è¯­ä¹‰ï¼Œä»è€Œå®ç°å¯¹å„ç§å¯¹è±¡ç±»å‹çš„é€šç”¨æ­£å¸¸å’Œå¼‚å¸¸è¯†åˆ«ã€‚åœ¨17ä¸ªçœŸå®ä¸–ç•Œçš„å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§è§„æ¨¡å®éªŒè¡¨æ˜ï¼ŒAnomalyCLIPåœ¨æ¥è‡ªå„ç§ç¼ºé™·æ£€æµ‹å’ŒåŒ»å­¦å½±åƒé¢†åŸŸçš„å…·æœ‰å„ç§ç±»åˆ«è¯­ä¹‰çš„æ•°æ®é›†ä¸Šå®ç°äº†å‡ºè‰²çš„é›¶æ ·æœ¬æ£€æµ‹å¼‚å¸¸å€¼æ€§èƒ½ã€‚ç›¸å…³ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zqhang/AnomalyCLIP">https://github.com/zqhang/AnomalyCLIP</a>ä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18961v9">PDF</a> Accepted by ICLR 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºCLIPæ¨¡å‹æ”¹è¿›çš„AnomalyCLIPæ–¹æ³•ï¼Œç”¨äºé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¯¹è±¡æ— å…³çš„æ–‡æœ¬æç¤ºæ¥å­¦ä¹ é€šç”¨çš„æ­£å¸¸æ€§å’Œå¼‚å¸¸æ€§ç‰¹å¾ï¼Œä»è€Œåœ¨å¤šç§é¢†åŸŸå®ç°å‡†ç¡®çš„å¼‚å¸¸æ£€æµ‹ã€‚è¿™ç§æ–¹æ³•é‡ç‚¹å…³æ³¨å¼‚å¸¸å›¾åƒåŒºåŸŸï¼Œè€Œä¸æ˜¯å¯¹è±¡è¯­ä¹‰ï¼Œä»è€Œåœ¨å„ç§æ•°æ®é›†ä¸Šå®ç°ä¼˜å¼‚çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²æ€§èƒ½ã€‚å…¶ä¸»è¦ä¼˜ç‚¹æ˜¯å¯ä»¥åº”ç”¨äºå…·æœ‰é«˜åº¦å¤šæ ·æ€§çš„ä¸åŒå¯¹è±¡ç±»å‹å’Œæ•°æ®é›†ï¼Œå±•ç¤ºå‡ºåœ¨ç¼ºé™·æ£€æµ‹å’ŒåŒ»å­¦å½±åƒé¢†åŸŸå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å…³é”®æŠ€æœ¯çš„æ ¸å¿ƒæ˜¯å»ºç«‹åŸºäºå¼‚å¸¸æ€§ä¸æ­£å¸¸æ€§çš„å›¾åƒåˆ†ç±»ä¸æ£€æµ‹æ¡†æ¶ï¼Œç”¨äºå„ç±»ç‰©å“ä¸Šçš„å¼‚å¸¸ç—…å˜åˆ†æã€‚ç›®å‰ç›¸å…³ä»£ç å·²ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnomalyCLIPæ˜¯ä¸€ç§åŸºäºCLIPæ¨¡å‹çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ”¹è¿›æ–¹æ³•ã€‚</li>
<li>å®ƒé€šè¿‡å¼•å…¥å¯¹è±¡æ— å…³çš„æ–‡æœ¬æç¤ºæ¥å­¦ä¹ é€šç”¨çš„æ­£å¸¸æ€§å’Œå¼‚å¸¸æ€§ç‰¹å¾ã€‚</li>
<li>AnomalyCLIPé‡ç‚¹å…³æ³¨å›¾åƒä¸­çš„å¼‚å¸¸åŒºåŸŸï¼Œè€Œä¸æ˜¯å¯¹è±¡è¯­ä¹‰ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜å¼‚çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ä»…å¯ä»¥åœ¨ä¸åŒç±»å‹å¯¹è±¡ä¸Šè¿›è¡Œï¼Œè€Œä¸”å¯¹é«˜åº¦çš„æ•°æ®é›†å¤šæ ·æ€§è¡¨ç°å‡ºäº†ä¼˜å¼‚é€‚ç”¨æ€§ã€‚æœ¬ç®—æ³•å±•ç°äº†æé«˜çš„çµæ•åº¦ï¼Œå¯¹å¾®å°ç—…å˜çš„è¯†åˆ«èƒ½åŠ›ä¹Ÿéå¸¸å‡ºè‰²ã€‚æœ¬ç®—æ³•æœ‰æœ›åœ¨åŒ»å­¦å½±åƒå¤„ç†é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç›®å‰ç›¸å…³ä»£ç å·²ä¸Šä¼ è‡³GitHubå¹³å°ä¾›å…¬ä¼—æŸ¥é˜…ä¸ä½¿ç”¨ã€‚å…¶åœ¨æ— è®­ç»ƒæ ·æœ¬å¯ç”¨æ—¶ï¼ˆå¦‚æ¶‰åŠæ•°æ®éšç§ç­‰åœºæ™¯ï¼‰å…·å¤‡æé«˜çš„å®ç”¨ä»·å€¼ã€‚æœ¬ç®—æ³•åœ¨å¤„ç†è§†é¢‘æµæ•°æ®æ–¹é¢ä¹Ÿè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å®æ—¶ç›‘æ§ä¸å®æ—¶å¼‚å¸¸é¢„è­¦çš„åº”ç”¨åœºæ™¯ä¸­ã€‚ç›®å‰å­¦ç•Œå’Œäº§ä¸šç•Œéƒ½å¯¹è¿™ç±»æ–¹æ³•è¡¨ç°å‡ºäº†æµ“åšçš„å…´è¶£ä¸ç ”ç©¶å‰æ™¯ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯¹ç¡¬ä»¶è®¾å¤‡çš„éœ€æ±‚å’Œç®—æ³•å¤æ‚åº¦ç­‰å…³é”®è¦ç´ è¿›è¡Œäº†ä¼˜åŒ–å¤„ç†ï¼Œä½¿å…¶åœ¨çœŸå®åº”ç”¨åœºæ™¯ä¸­å…·æœ‰æ›´é«˜çš„å®ç”¨æ€§å’Œæ¨å¹¿ä»·å€¼ã€‚æ­¤ç§è·¨é¢†åŸŸçš„æ”¹è¿›æ¨¡å¼ä¸æ–¹æ³•ææœ‰å¯èƒ½åœ¨æœªæ¥çš„å¤šæºèåˆä¿¡æ¯å¤„ç†æŠ€æœ¯ä¸­å‘æŒ¥ç€ä¸å¯æˆ–ç¼ºçš„é‡è¦ä½œç”¨ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå¤„ç†è·¨é¢†åŸŸçš„å¼‚å¸¸æƒ…å†µæ£€æµ‹ä»»åŠ¡ã€‚åœ¨å®é™…åº”ç”¨ä¸­å–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œå¦‚åœ¨ç”Ÿäº§è´¨é‡æ§åˆ¶ã€åŒ»å­¦å½±åƒè¯Šæ–­ç­‰é¢†åŸŸåº”ç”¨å‰æ™¯å¹¿é˜”ã€‚åŒæ—¶ï¼Œè¯¥è®ºæ–‡ä¹ŸæŒ‡å‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬å¦‚ä½•è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ç­‰é—®é¢˜å€¼å¾—è¿›ä¸€æ­¥æ¢è®¨å’Œç ”ç©¶ã€‚ä¾‹å¦‚å¯¹å¤šæ¨¡æ€æ•°æ®ã€å¤šåœºæ™¯èåˆç­‰é¢†åŸŸè¿›è¡Œæ·±å…¥ç ”ç©¶ä»¥æé«˜æ¨¡å‹çš„ç»¼åˆæ€§èƒ½ä¸é€‚åº”èƒ½åŠ›ç­‰ã€‚è¿™äº›ç ”ç©¶æ–¹å‘å°†æœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ä¸åº”ç”¨è½åœ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.18961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5772267d6d4b4da68d7969ac6ce265cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f73f9979f0c975511f28194aeb6a1b86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73298b80228ac83b9943ca1279459634.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-397502887d6913c096fe6abeecfc7329.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  Exploring Modality Guidance to Enhance VFM-based Feature Fusion for UDA   in 3D Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8a41d61b696fe88ae52cf88b6b3c8ffe.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  An LMM for Efficient Video Understanding via Reinforced Compression of   Video Cubes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
