<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  OPO Making Decision-Focused Data Acquisition Decisions">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5c015040c65197a1ef8f66bcf0d13365.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-23-æ›´æ–°"><a href="#2025-04-23-æ›´æ–°" class="headerlink" title="2025-04-23 æ›´æ–°"></a>2025-04-23 æ›´æ–°</h1><h2 id="OPO-Making-Decision-Focused-Data-Acquisition-Decisions"><a href="#OPO-Making-Decision-Focused-Data-Acquisition-Decisions" class="headerlink" title="OPO: Making Decision-Focused Data Acquisition Decisions"></a>OPO: Making Decision-Focused Data Acquisition Decisions</h2><p><strong>Authors:Egon PerÅ¡ak, Miguel F. Anjos</strong></p>
<p>We propose a model for making data acquisition decisions for variables in contextual stochastic optimisation problems. Data acquisition decisions are typically treated as separate and fixed. We explore problem settings in which the acquisition of contextual variables is costly and consequently constrained. The data acquisition problem is often solved heuristically for proxy objectives such as coverage. The more intuitive objective is the downstream decision quality as a result of data acquisition decisions. The whole pipeline can be characterised as an optimise-then-predict-then-optimise (OPO) problem. Analogously, much recent research has focused on how to integrate prediction and optimisation (PO) in the form of decision-focused learning. We propose leveraging differentiable optimisation to extend the integration to data acquisition. We solve the data acquisition problem with well-defined constraints by learning a surrogate linear objective function. We demonstrate an application of this model on a shortest path problem for which we first have to set a drone reconnaissance strategy to capture image segments serving as inputs to a model that predicts travel costs. We ablate the problem with a number of training modalities and demonstrate that the differentiable optimisation approach outperforms random search strategies. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºä¸€ç§ç”¨äºè§£å†³ä¸Šä¸‹æ–‡éšæœºä¼˜åŒ–é—®é¢˜ä¸­çš„å˜é‡æ•°æ®é‡‡é›†å†³ç­–æ¨¡å‹ã€‚é€šå¸¸ï¼Œæ•°æ®é‡‡é›†å†³ç­–è¢«è§†ä¸ºç‹¬ç«‹ä¸”å›ºå®šçš„ã€‚æˆ‘ä»¬æ¢ç´¢äº†åœ¨è·å–ä¸Šä¸‹æ–‡å˜é‡æˆæœ¬é«˜æ˜‚ä»è€Œå—åˆ°é™åˆ¶çš„æƒ…å¢ƒè®¾ç½®ã€‚æ•°æ®è·å–é—®é¢˜é€šå¸¸é€šè¿‡å¯å‘å¼æ–¹æ³•è§£å†³ï¼Œä»¥è¦†ç›–ç­‰ä»£ç†ç›®æ ‡ä¸ºä¸»ã€‚æ›´ç›´è§‚çš„ç›®æ ‡æ˜¯æ•°æ®è·å–å†³ç­–æ‰€å¯¼è‡´çš„ä¸‹æ¸¸å†³ç­–è´¨é‡ã€‚æ•´ä¸ªæµç¨‹å¯ä»¥ç‰¹å¾åŒ–ä¸ºä¸€ä¸ªå…ˆä¼˜åŒ–å†é¢„æµ‹å†ä¼˜åŒ–ï¼ˆOPOï¼‰é—®é¢˜ã€‚ç±»ä¼¼åœ°ï¼Œæœ€è¿‘çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¦‚ä½•å°†é¢„æµ‹å’Œä¼˜åŒ–ï¼ˆPOï¼‰ä»¥å†³ç­–èšç„¦å­¦ä¹ çš„å½¢å¼ç»“åˆèµ·æ¥ã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨å¯å¾®ä¼˜åŒ–æ¥å°†æ•°æ®é‡‡é›†æ‰©å±•åˆ°é¢„æµ‹å’Œä¼˜åŒ–é›†æˆä¸­ã€‚æˆ‘ä»¬é€šè¿‡å­¦ä¹ ä¸€ä¸ªæ›¿ä»£çš„çº¿æ€§ç›®æ ‡å‡½æ•°æ¥è§£å†³å…·æœ‰æ˜ç¡®çº¦æŸçš„æ•°æ®é‡‡é›†é—®é¢˜ã€‚æˆ‘ä»¬åœ¨æœ€çŸ­è·¯å¾„é—®é¢˜ä¸Šå±•ç¤ºäº†è¯¥æ¨¡å‹çš„ä¸€ä¸ªåº”ç”¨ï¼Œä¸ºæ­¤æˆ‘ä»¬å¿…é¡»é¦–å…ˆè®¾ç½®æ— äººæœºä¾¦å¯Ÿç­–ç•¥ä»¥æ•è·å›¾åƒç‰‡æ®µï¼Œå°†å…¶ä½œä¸ºé¢„æµ‹æ—…è¡Œæˆæœ¬çš„æ¨¡å‹çš„è¾“å…¥ã€‚æˆ‘ä»¬é€šè¿‡å¤šç§è®­ç»ƒæ¨¡å¼æ¥å‰¥ç¦»è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¯æ˜å¯å¾®ä¼˜åŒ–æ–¹æ³•ä¼˜äºéšæœºæœç´¢ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15062v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç”¨äºè§£å†³ä¸Šä¸‹æ–‡éšæœºä¼˜åŒ–é—®é¢˜ä¸­çš„å˜é‡æ•°æ®è·å–å†³ç­–æ¨¡å‹ã€‚é’ˆå¯¹æ•°æ®è·å–æˆæœ¬é«˜æ˜‚ä¸”å—é™çš„é—®é¢˜è®¾ç½®ï¼Œæ–‡ç« æ¢ç´¢äº†ä¸€ç§åˆ©ç”¨å¯å¾®ä¼˜åŒ–æ¥æ‰©å±•æ•°æ®è·å–ä¸­é¢„æµ‹ä¸ä¼˜åŒ–é›†æˆçš„æ–¹æ³•ã€‚é€šè¿‡è§£å†³å…·æœ‰æ˜ç¡®çº¦æŸçš„æ•°æ®è·å–é—®é¢˜ï¼Œå­¦ä¹ ä»£ç†çº¿æ€§ç›®æ ‡å‡½æ•°ã€‚åœ¨æ— äººæœºä¾¦å¯Ÿç­–ç•¥çš„æœ€çŸ­è·¯å¾„é—®é¢˜ä¸Šè¿›è¡Œäº†åº”ç”¨æ¼”ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸Šä¸‹æ–‡éšæœºä¼˜åŒ–é—®é¢˜ä¸­å˜é‡æ•°æ®è·å–å†³ç­–çš„æ–°å‹æ¨¡å‹ã€‚</li>
<li>æ•°æ®è·å–å†³ç­–é€šå¸¸è¢«è§†ä¸ºç‹¬ç«‹ä¸”å›ºå®šçš„ï¼Œä½†æ–‡ç« å¼ºè°ƒå…¶ä¼˜åŒ–é‡è¦æ€§ã€‚</li>
<li>å½“ä¸‹æ¸¸å†³ç­–è´¨é‡ä½œä¸ºæ•°æ®è·å–å†³ç­–æ›´ç›´è§‚çš„ç›®æ ‡æ—¶ï¼Œæ•°æ®è·å–é—®é¢˜å¸¸è¢«å¯å‘å¼åœ°è§£å†³ä¸ºä»£ç†ç›®æ ‡å¦‚è¦†ç›–é—®é¢˜ã€‚</li>
<li>æ–‡ç« æå‡ºåˆ©ç”¨å¯å¾®ä¼˜åŒ–æ¥æ‰©å±•é¢„æµ‹ä¸ä¼˜åŒ–ï¼ˆPOï¼‰åœ¨æ•°æ®è·å–ä¸­çš„é›†æˆã€‚</li>
<li>é€šè¿‡è§£å†³å…·æœ‰æ˜ç¡®çº¦æŸçš„æ•°æ®è·å–é—®é¢˜ï¼Œå­¦ä¹ ä»£ç†çº¿æ€§ç›®æ ‡å‡½æ•°æ˜¯æœ¬æ–‡çš„å…³é”®æ–¹æ³•ã€‚</li>
<li>æ–‡ç« åœ¨ä¸€ä¸ªæ— äººæœºä¾¦å¯Ÿç­–ç•¥çš„æœ€çŸ­è·¯å¾„é—®é¢˜ä¸Šå±•ç¤ºäº†è¯¥æ¨¡å‹çš„åº”ç”¨å®ä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d2583bd3bc89ac5a6d623f44de34355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4884430ecb15ba0c0f01f3cc6a56e0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-377dbffc04fcdf3a5e2b9654349fdb00.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Initiation-Route-of-Coronal-Mass-Ejections-II-The-Role-of-Filament-Mass"><a href="#Initiation-Route-of-Coronal-Mass-Ejections-II-The-Role-of-Filament-Mass" class="headerlink" title="Initiation Route of Coronal Mass Ejections: II. The Role of Filament   Mass"></a>Initiation Route of Coronal Mass Ejections: II. The Role of Filament   Mass</h2><p><strong>Authors:Chen Xing, Xin Cheng, Guillaume Aulanier, Mingde Ding</strong></p>
<p>The thorough understanding on the initiation of coronal mass ejections (CMEs), which is manifested as a slow rise of pre-eruptive structures before the impulsive ejection in kinematics, is the key for forecasting the solar eruptions. In our previous work, we showed that the slow rise of a hot flux rope with coronal mass density is caused by the moderate magnetic reconnection occurring in the hyperbolic flux tube (HFT) combined with the torus instability. However, it remains unclear how the initiation process varies when a filament is present in the pre-eruptive flux rope. In this work, we reveal the complete initiation route of a CME containing filament mass with a state-of-the-art full-magnetohydrodynamics simulation. The comprehensive analyses show that the filament mass has an important impact on the CME initiation through triggering and driving the slow rise of flux rope with its drainage, besides the contributions of HFT reconnection and torus instability. Finally, in combination with our previous work, we propose that the enhanced drainage of filament mass and various features related to the HFT reconnection, such as, the split of pre-eruptive structure and the pre-flare loops and X-ray emissions, can serve as the precursors of CME initiation in observations. </p>
<blockquote>
<p>å¯¹æ—¥å†•ç‰©è´¨æŠ›å°„ï¼ˆCMEsï¼‰èµ·å§‹è¿‡ç¨‹çš„ç†è§£ï¼Œè¡¨ç°ä¸ºåŠ¨åŠ›å­¦ä¸­çš„çˆ†å‘å‰ç»“æ„ç¼“æ…¢ä¸Šå‡ï¼Œæ˜¯é¢„æµ‹å¤ªé˜³çˆ†å‘çš„å…³é”®ã€‚åœ¨æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†çƒ­ç£é€šç»³ä¸æ—¥å†•ç‰©è´¨å¯†åº¦çš„ç¼“æ…¢ä¸Šå‡æ˜¯ç”±åŒæ›²ç£é€šç®¡ï¼ˆHFTï¼‰ä¸­å‘ç”Ÿçš„é€‚åº¦ç£é‡è”ä¸æ‰˜å¡é©¬å…‹ä¸ç¨³å®šå…±åŒä½œç”¨å¼•èµ·çš„ã€‚ç„¶è€Œï¼Œå½“å‰ä»å­˜åœ¨ç–‘é—®ï¼šå½“é¢„çˆ†å‘ç£é€šç»³ä¸­å­˜åœ¨æ—¥ç¥æ—¶ï¼Œèµ·å§‹è¿‡ç¨‹å¦‚ä½•å˜åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æœ€å…ˆè¿›çš„å…¨ç£æµä½“åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ­ç¤ºäº†åŒ…å«æ—¥ç¥è´¨é‡çš„CMEçš„å®Œæ•´èµ·å§‹é€”å¾„ã€‚ç»¼åˆåˆ†æè¡¨æ˜ï¼Œé™¤äº†HFTé‡è”å’Œæ‰˜å¡é©¬å…‹ä¸ç¨³å®šæ€§çš„è´¡çŒ®å¤–ï¼Œæ—¥ç¥è´¨é‡é€šè¿‡è§¦å‘å’Œé©±åŠ¨ç£é€šç»³çš„ç¼“æ…¢ä¸Šå‡å¯¹å…¶æ’æ°´è¿‡ç¨‹æœ‰é‡è¦å½±å“ã€‚æœ€åï¼Œç»“åˆæˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œï¼Œæˆ‘ä»¬æå‡ºå¢å¼ºçš„æ—¥ç¥è´¨é‡æ’æ°´ä»¥åŠä¸HFTé‡è”ç›¸å…³çš„å„ç§ç‰¹å¾ï¼Œå¦‚é¢„çˆ†å‘ç»“æ„çš„åˆ†è£‚ã€é¢„è€€æ–‘ç¯å’ŒXå°„çº¿å‘å°„ï¼Œå¯ä»¥ä½œä¸ºè§‚æµ‹ä¸­CMEèµ·å§‹çš„é¢„å…†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14876v1">PDF</a> 19 pages, 6 figures; accepted for publication in ApJ</p>
<p><strong>Summary</strong></p>
<p>åœ¨å…¨é¢ç†è§£æ—¥å†•ç‰©è´¨å–·å°„ï¼ˆCMEsï¼‰çš„å¯åŠ¨æœºåˆ¶æ–¹é¢ï¼Œå…³é”®åœ¨äºç†è§£åŠ¨åŠ›å­¦ä¸­çš„ç¼“æ…¢ä¸Šå‡é˜¶æ®µé¢„å–·ç»“æ„å‰çš„è¡¨ç°ã€‚è¿‡å»çš„ç ”ç©¶è¡¨æ˜ï¼Œçƒ­ç£é€šç»³çš„ç¼“æ…¢ä¸Šå‡æ˜¯ç”±äºåŒæ›²é€šé‡ç®¡ä¸­çš„ä¸­åº¦ç£é‡è”å’Œç¯é¢ä¸ç¨³å®šæ€§çš„å…±åŒä½œç”¨æ‰€è‡´ã€‚ç„¶è€Œï¼Œå½“å‰å·¥ä½œä¸­æ­ç¤ºäº†åœ¨é¢„å–·ç£é€šç»³ä¸­å­˜åœ¨ç»†ä¸ç‰©è´¨æ—¶ï¼Œå¯åŠ¨è¿‡ç¨‹çš„å®Œæ•´æ€§å˜åŒ–ã€‚å…¨é¢åˆ†æè¡¨æ˜ï¼Œé™¤äº†åŒæ›²é€šé‡ç®¡é‡è”å’Œç¯é¢ä¸ç¨³å®šæ€§çš„è´¡çŒ®å¤–ï¼Œç»†ä¸ç‰©è´¨çš„æ’æ°´å¯¹CMEå¯åŠ¨æœ‰é‡è¦å½±å“ï¼Œå¹¶èƒ½é©±åŠ¨ç£é€šç»³çš„ç¼“æ…¢ä¸Šå‡ã€‚ç»“åˆå…ˆå‰çš„å·¥ä½œï¼Œæˆ‘ä»¬æå‡ºå¢å¼ºçš„ç»†ä¸ç‰©è´¨æ’æ°´ä»¥åŠä¸åŒæ›²é€šé‡ç®¡é‡è”ç›¸å…³çš„å„ç§ç‰¹å¾ï¼ˆå¦‚é¢„å–·ç»“æ„çš„åˆ†è£‚å’Œé¢„è€€æ–‘ç¯ä»¥åŠXå°„çº¿å‘å°„ï¼‰å¯ä»¥ä½œä¸ºè§‚æµ‹ä¸­CMEå¯åŠ¨çš„å‰å…†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£æ—¥å†•ç‰©è´¨å–·å°„ï¼ˆCMEsï¼‰å¯åŠ¨æœºåˆ¶çš„å…³é”®åœ¨äºé¢„å–·ç»“æ„å‰çš„ç¼“æ…¢ä¸Šå‡é˜¶æ®µã€‚</li>
<li>è¿‡å»çš„ç ”ç©¶å·²ç»ç¡®å®šäº†çƒ­ç£é€šç»³ç¼“æ…¢ä¸Šå‡çš„åŸå› æ˜¯ç”±åŒæ›²é€šé‡ç®¡ä¸­çš„ç£é‡è”å’Œç¯é¢ä¸ç¨³å®šæ€§å…±åŒä½œç”¨æ‰€è‡´ã€‚</li>
<li>å½“å‰å·¥ä½œæ­ç¤ºäº†å½“é¢„å–·ç£é€šç»³ä¸­å­˜åœ¨ç»†ä¸ç‰©è´¨æ—¶ï¼ŒCMEå¯åŠ¨è¿‡ç¨‹çš„å®Œæ•´æ€§å˜åŒ–ã€‚</li>
<li>ç»†ä¸ç‰©è´¨çš„æ’æ°´å¯¹CMEå¯åŠ¨æœ‰é‡è¦å½±å“ï¼Œèƒ½é©±åŠ¨ç£é€šç»³çš„ç¼“æ…¢ä¸Šå‡ã€‚</li>
<li>å…¨é¢åˆ†æè¡¨æ˜ï¼Œé™¤äº†åŒæ›²é€šé‡ç®¡é‡è”å’Œç¯é¢ä¸ç¨³å®šæ€§çš„è´¡çŒ®å¤–ï¼Œç»†ä¸ç‰©è´¨åœ¨CMEå¯åŠ¨è¿‡ç¨‹ä¸­èµ·åˆ°è§¦å‘ä½œç”¨ã€‚</li>
<li>å¢å¼ºç»†ä¸ç‰©è´¨æ’æ°´æ˜¯è§‚æµ‹ä¸­CMEå¯åŠ¨çš„ä¸€ä¸ªé‡è¦å‰å…†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2672bad7eedf506fd9700c3144394a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2358e0aaf5b7e171b7c27ead78fc749.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e73c6099aa49b3f91480685edec7126e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36046856dc83e5a0e46a2b24e15516b8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Unveiling-Fine-Structure-and-Energy-driven-Transition-of-Photoelectron-Kikuchi-Diffraction"><a href="#Unveiling-Fine-Structure-and-Energy-driven-Transition-of-Photoelectron-Kikuchi-Diffraction" class="headerlink" title="Unveiling Fine Structure and Energy-driven Transition of Photoelectron   Kikuchi Diffraction"></a>Unveiling Fine Structure and Energy-driven Transition of Photoelectron   Kikuchi Diffraction</h2><p><strong>Authors:Trung-Phuc Vo, Olena Tkach, Aki Pulkkinen, Didier Sebilleau, Aimo Winkelmann, Olena Fedchenko, Yaryna Lytvynenko, Dmitry Vasilyev, Hans-Joachim Elmers, Gerd Schonhense, Jan Minar</strong></p>
<p>The intricate fine structure of Kikuchi diffraction plays a vital role in probing phase transformations and strain distributions in functional materials, particularly in electron microscopy. Beyond these applications, it also proves essential in photoemission spectroscopy (PES) at high photon energies, aiding in the disentanglement of complex angle-resolved PES data and enabling emitter-site-specific studies. However, the detection and analysis of these rich faint structures in photoelectron diffraction (PED), especially in the hard X-ray regime, remain highly challenging, with only a limited number of simulations successfully reproducing these patterns. The strong energy dependence of Kikuchi patterns further complicates their interpretation, necessitating advanced theoretical approaches. To enhance structural analysis, we present a comprehensive theoretical study of fine diffraction patterns and their evolution with energy by simulating core-level emissions from Ge(100) and Si(100). Using multiple-scattering theory and the fully relativistic one-step photoemission model, we simulate faint pattern networks for various core levels across different kinetic energies (106 eV - 4174 eV), avoiding cluster size convergence issues inherent in cluster-based methods. Broadening in patterns is discussed via the inelastic scattering treatment. For the first time, circular dichroism has been observed and successfully reproduced in the angular distribution of Si (100) 1s, revealing detailed features and asymmetries up to 31%. Notably, we successfully replicate experimental bulk and more â€œsurface-sensitivityâ€ diffraction features, further validating the robustness of our simulations. The results show remarkable agreement with the experimental data obtained using circularly polarized radiations, demonstrating the potential of this methodology for advancing high-energy PES investigations. </p>
<blockquote>
<p>èµ¤ç©—å›¾åƒçš„ç²¾ç»†ç»“æ„åœ¨æ¢æµ‹åŠŸèƒ½ææ–™çš„ç›¸å˜å’Œåº”å˜åˆ†å¸ƒï¼Œç‰¹åˆ«æ˜¯åœ¨ç”µå­æ˜¾å¾®é•œå­¦ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚é™¤äº†è¿™äº›åº”ç”¨ä¹‹å¤–ï¼Œå®ƒåœ¨é«˜èƒ½å…‰å­å…‰è°±çš„å…‰å‘å°„å…‰è°±ï¼ˆPESï¼‰ä¸­ä¹Ÿè¯æ˜æ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œæœ‰åŠ©äºè§£å†³å¤æ‚çš„è§’åº¦è§£æPESæ•°æ®ï¼Œå¹¶èƒ½å¤Ÿå®ç°å‘å°„å™¨ç‰¹å®šéƒ¨ä½çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œåœ¨å…‰ç”µå­è¡å°„ï¼ˆPEDï¼‰ä¸­æ£€æµ‹å’Œè§£æè¿™äº›ä¸°å¯Œçš„å¾®å¼±ç»“æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡¬Xå°„çº¿é¢†åŸŸï¼Œä»ç„¶æ˜¯éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œåªæœ‰æœ‰é™çš„æ¨¡æ‹ŸæˆåŠŸåœ°å¤åˆ¶äº†è¿™äº›æ¨¡å¼ã€‚èµ¤ç©—å›¾æ¡ˆçš„å¼ºèƒ½é‡ä¾èµ–æ€§è¿›ä¸€æ­¥å¤æ‚äº†å®ƒä»¬çš„è§£é‡Šï¼Œéœ€è¦é«˜çº§ç†è®ºæ–¹æ³•ã€‚ä¸ºäº†å¢å¼ºç»“æ„åˆ†æï¼Œæˆ‘ä»¬å¯¹ç²¾ç»†è¡å°„å›¾æ¡ˆåŠå…¶éšèƒ½é‡çš„æ¼”å˜è¿›è¡Œäº†å…¨é¢çš„ç†è®ºç ”ç©¶ï¼Œé€šè¿‡æ¨¡æ‹ŸGeï¼ˆ100ï¼‰å’ŒSiï¼ˆ100ï¼‰çš„æ ¸å¿ƒæ°´å¹³å‘å°„ã€‚æˆ‘ä»¬åˆ©ç”¨å¤šé‡æ•£å°„ç†è®ºå’Œå®Œå…¨ç›¸å¯¹è®ºçš„ä¸€æ­¥å…‰å‘å°„æ¨¡å‹ï¼Œæ¨¡æ‹Ÿäº†ä¸åŒæ ¸å¿ƒèƒ½çº§åœ¨ä¸åŒåŠ¨èƒ½ï¼ˆ106ç”µå­ä¼ç‰¹è‡³4174ç”µå­ä¼ç‰¹ï¼‰ä¸‹çš„å¾®å¼±å›¾æ¡ˆç½‘ç»œï¼Œé¿å…äº†é›†ç¾¤å¤§å°æ”¶æ•›é—®é¢˜ã€‚å›¾æ¡ˆçš„å±•å®½é€šè¿‡éå¼¹æ€§æ•£å°„å¤„ç†è¿›è¡Œäº†è®¨è®ºã€‚é¦–æ¬¡è§‚å¯Ÿåˆ°åœ†äºŒè‰²æ€§å¹¶åœ¨ç¡…ï¼ˆ100ï¼‰çš„è§’åˆ†å¸ƒä¸­æˆåŠŸå¤åˆ¶å‡ºæ¥ï¼Œæ­ç¤ºäº†é«˜è¾¾31%çš„è¯¦ç»†ç‰¹å¾å’Œä¸å¯¹ç§°æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å¤åˆ¶äº†å®éªŒä¸­çš„æ•´ä½“å’Œæ›´â€œè¡¨é¢æ•æ„Ÿâ€çš„è¡å°„ç‰¹å¾ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡æ‹Ÿçš„ç¨³å®šæ€§ã€‚ä¸åˆ©ç”¨åœ†åæŒ¯è¾å°„è·å¾—çš„å®éªŒæ•°æ®ç›¸æ¯”ï¼Œç»“æœè¡¨ç°å‡ºæ˜¾è‘—çš„ä¸€è‡´æ€§ï¼Œè¯æ˜äº†è¿™ç§æ–¹æ³•åœ¨é«˜èƒ½PESç ”ç©¶ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14758v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Kikuchiè¡å°„ç²¾ç»†ç»“æ„åœ¨åŠŸèƒ½ææ–™ç›¸å˜å’Œåº”å˜åˆ†å¸ƒç ”ç©¶ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”µå­æ˜¾å¾®é•œä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åœ¨é«˜èƒ½å…‰å­å…‰è°±å­¦ä¸­çš„å…‰å‘å°„å…‰è°±å­¦ï¼ˆPESï¼‰ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œæœ‰åŠ©äºè§£å¼€å¤æ‚çš„è§’åº¦è§£æPESæ•°æ®ï¼Œå¹¶æ”¯æŒè¿›è¡Œå‘å°„å™¨ä½ç‚¹çš„ä¸“é¡¹ç ”ç©¶ã€‚ç„¶è€Œï¼Œåœ¨å…‰ç”µå­è¡å°„ï¼ˆPEDï¼‰ä¸­æ£€æµ‹å’Œè§£æè¿™äº›ä¸°å¯Œçš„å¾®å¼±ç»“æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡¬Xå°„çº¿é¢†åŸŸï¼Œä»ç„¶å­˜åœ¨å·¨å¤§çš„æŒ‘æˆ˜ã€‚ä»…æœ‰æœ‰é™çš„æ¨¡æ‹ŸæˆåŠŸå¤åˆ¶äº†è¿™äº›æ¨¡å¼ã€‚Kikuchiå›¾æ¡ˆçš„å¼ºçƒˆèƒ½é‡ä¾èµ–æ€§è¿›ä¸€æ­¥å¢åŠ äº†å…¶è§£é‡Šçš„å¤æ‚æ€§ï¼Œéœ€è¦é«˜çº§ç†è®ºæ–¹æ³•ã€‚ä¸ºäº†å¢å¼ºç»“æ„åˆ†æï¼Œæˆ‘ä»¬å¯¹ç²¾ç»†è¡å°„å›¾æ¡ˆåŠå…¶éšèƒ½é‡çš„æ¼”å˜è¿›è¡Œäº†å…¨é¢çš„ç†è®ºç ”ç©¶ï¼Œé€šè¿‡å¯¹Geï¼ˆ100ï¼‰å’ŒSiï¼ˆ100ï¼‰çš„æ ¸å¿ƒæ°´å¹³å‘å°„è¿›è¡Œæ¨¡æ‹Ÿã€‚åˆ©ç”¨å¤šé‡æ•£å°„ç†è®ºå’Œå®Œå…¨ç›¸å¯¹è®ºçš„ä¸€é˜¶å…‰å‘å°„æ¨¡å‹ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäº†ä¸åŒåŠ¨èƒ½èŒƒå›´ï¼ˆ106 eV - 4174 eVï¼‰å†…å„ç§æ ¸å¿ƒå±‚æ¬¡çš„å¾®å¼±å›¾æ¡ˆç½‘ç»œï¼Œé¿å…äº†é›†ç¾¤å¤§å°æ”¶æ•›é—®é¢˜ã€‚é€šè¿‡éå¼¹æ€§æ•£å°„å¤„ç†è®¨è®ºäº†å›¾æ¡ˆçš„å±•å®½ã€‚é¦–æ¬¡è§‚å¯Ÿåˆ°ç¡…ï¼ˆ100ï¼‰1sçš„åœ†äºŒè‰²æ€§ï¼Œå¹¶åœ¨å…¶è§’åº¦åˆ†å¸ƒä¸­æˆåŠŸå¤åˆ¶ï¼Œæ­ç¤ºäº†é«˜è¾¾31%çš„ç‰¹å¾å’Œä¸å¯¹ç§°æ€§ã€‚æˆ‘ä»¬æˆåŠŸåœ°å¤åˆ¶äº†å®éªŒä¸­çš„å¤§ä½“å’Œæ›´â€œè¡¨é¢æ•æ„Ÿæ€§â€çš„è¡å°„ç‰¹å¾ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡æ‹Ÿçš„ç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶ç»“æœä¸å®éªŒæ•°æ®è¡¨ç°å‡ºæ˜¾è‘—çš„ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨åœ†åæŒ¯è¾å°„çš„æƒ…å†µä¸‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨é«˜èƒ½PESç ”ç©¶ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Kikuchiè¡å°„çš„ç²¾ç»†ç»“æ„åœ¨æ¢ç´¢åŠŸèƒ½ææ–™çš„ç›¸å˜å’Œåº”å˜åˆ†å¸ƒä¸­èµ·å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”µå­æ˜¾å¾®é•œå­¦ä¸­ã€‚</li>
<li>åœ¨é«˜èƒ½å…‰å­å…‰è°±å­¦ä¸­ï¼ŒKikuchiè¡å°„ä¹ŸåŠ©äºè§£å¼€å¤æ‚çš„å…‰å‘å°„å…‰è°±ï¼ˆPESï¼‰æ•°æ®å¹¶æ”¯æŒç‰¹å®šç ”ç©¶ã€‚</li>
<li>åœ¨ç¡¬Xå°„çº¿é¢†åŸŸæ£€æµ‹å’Œè§£æå…‰ç”µå­è¡å°„ï¼ˆPEDï¼‰ä¸­çš„å¾®å¼±ç»“æ„å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>åªæœ‰æœ‰é™çš„æ¨¡æ‹ŸæˆåŠŸå¤åˆ¶äº†è¿™äº›å›¾æ¡ˆç½‘ç»œçš„ä¸åŒåŠ¨èƒ½èŒƒå›´å†…çš„å„ç§æ ¸å¿ƒå±‚æ¬¡ã€‚</li>
<li>åˆ©ç”¨å¤šé‡æ•£å°„ç†è®ºå’Œå®Œå…¨ç›¸å¯¹è®ºçš„ä¸€é˜¶å…‰å‘å°„æ¨¡å‹æˆåŠŸæ¨¡æ‹Ÿäº†ç²¾ç»†è¡å°„å›¾æ¡ˆåŠå…¶éšèƒ½é‡çš„æ¼”å˜ã€‚</li>
<li>ç¬¬ä¸€æ¬¡è§‚å¯Ÿåˆ°ç¡…ï¼ˆSiï¼‰ä¸­çš„åœ†äºŒè‰²æ€§ç°è±¡å¹¶æˆåŠŸæ¨¡æ‹Ÿï¼Œæ­ç¤ºäº†å…¶è§’åº¦åˆ†å¸ƒçš„ç‰¹å¾å’Œä¸å¯¹ç§°æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-65f5fb572476725d14f2481ab423cb37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689dc74953eadba6a40c7f2f8ecb3ba8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SuperCL-Superpixel-Guided-Contrastive-Learning-for-Medical-Image-Segmentation-Pre-training"><a href="#SuperCL-Superpixel-Guided-Contrastive-Learning-for-Medical-Image-Segmentation-Pre-training" class="headerlink" title="SuperCL: Superpixel Guided Contrastive Learning for Medical Image   Segmentation Pre-training"></a>SuperCL: Superpixel Guided Contrastive Learning for Medical Image   Segmentation Pre-training</h2><p><strong>Authors:Shuang Zeng, Lei Zhu, Xinliang Zhang, Hangzhou He, Yanye Lu</strong></p>
<p>Medical image segmentation is a critical yet challenging task, primarily due to the difficulty of obtaining extensive datasets of high-quality, expert-annotated images. Contrastive learning presents a potential but still problematic solution to this issue. Because most existing methods focus on extracting instance-level or pixel-to-pixel representation, which ignores the characteristics between intra-image similar pixel groups. Moreover, when considering contrastive pairs generation, most SOTA methods mainly rely on manually setting thresholds, which requires a large number of gradient experiments and lacks efficiency and generalization. To address these issues, we propose a novel contrastive learning approach named SuperCL for medical image segmentation pre-training. Specifically, our SuperCL exploits the structural prior and pixel correlation of images by introducing two novel contrastive pairs generation strategies: Intra-image Local Contrastive Pairs (ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation. Considering superpixel cluster aligns well with the concept of contrastive pairs generation, we utilize the superpixel map to generate pseudo masks for both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also propose two modules named Average SuperPixel Feature Map Generation (ASP) and Connected Components Label Generation (CCL) to better exploit the prior structural information for IGCP. Finally, experiments on 8 medical image datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL achieves a superior performance with more precise predictions from visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best results on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released after acceptance. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦æ˜¯ç”±äºè·å¾—å¤§é‡é«˜è´¨é‡ã€ä¸“å®¶æ³¨é‡Šçš„å›¾åƒæ•°æ®é›†ååˆ†å›°éš¾ã€‚å¯¹æ¯”å­¦ä¹ ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ½œåœ¨ä½†ä»æœ‰é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å› ä¸ºå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸“æ³¨äºæå–å®ä¾‹çº§æˆ–åƒç´ å¯¹åƒç´ çš„è¡¨ç¤ºï¼Œè¿™å¿½ç•¥äº†å›¾åƒå†…ç›¸ä¼¼åƒç´ ç»„ä¹‹é—´çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œåœ¨å¯¹æ¯”å¯¹ç”Ÿæˆæ—¶ï¼Œå¤§å¤šæ•°æœ€å…ˆè¿›çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºäººå·¥è®¾ç½®é˜ˆå€¼ï¼Œè¿™éœ€è¦å¤§é‡çš„æ¢¯åº¦å®éªŒï¼Œç¼ºä¹æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSuperCLçš„æ–°å‹å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢„è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„SuperCLé€šè¿‡å¼•å…¥ä¸¤ç§æ–°å‹çš„å¯¹æ¯”å¯¹ç”Ÿæˆç­–ç•¥ï¼Œåˆ©ç”¨å›¾åƒçš„ç»“æ„å…ˆéªŒå’Œåƒç´ ç›¸å…³æ€§ï¼šå›¾åƒå†…å±€éƒ¨å¯¹æ¯”å¯¹ï¼ˆILCPï¼‰ç”Ÿæˆå’Œå›¾åƒé—´å…¨å±€å¯¹æ¯”å¯¹ï¼ˆIGCPï¼‰ç”Ÿæˆã€‚è€ƒè™‘åˆ°è¶…åƒç´ èšç±»ä¸å¯¹æ¯”å¯¹ç”Ÿæˆçš„æ¦‚å¿µç›¸å»åˆï¼Œæˆ‘ä»¬åˆ©ç”¨è¶…åƒç´ å›¾ç”ŸæˆILCPå’ŒIGCPçš„ä¼ªæ©ç ï¼Œä»¥æŒ‡å¯¼æœ‰ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åä¸ºå¹³å‡è¶…åƒç´ ç‰¹å¾å›¾ç”Ÿæˆï¼ˆASPï¼‰å’Œè¿é€šç»„ä»¶æ ‡ç­¾ç”Ÿæˆï¼ˆCCLï¼‰çš„ä¸¤ä¸ªæ¨¡å—ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨IGCPçš„å…ˆéªŒç»“æ„ä¿¡æ¯ã€‚æœ€åï¼Œåœ¨8ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SuperCLä¼˜äºç°æœ‰çš„12ç§æ–¹æ³•ã€‚å³ï¼Œæˆ‘ä»¬çš„SuperCLåœ¨å¯è§†åŒ–å›¾ä¸Šå®ç°äº†æ›´ç²¾ç¡®çš„é¢„æµ‹ï¼Œå¹¶ä¸”åœ¨MMWHSã€CHAOSã€Spleenæ•°æ®é›†ä¸Šåˆ†åˆ«æé«˜äº†3.15%ã€5.44%ã€7.89%çš„DSCåˆ†æ•°è¶…è¶Šä¹‹å‰æœ€å¥½çš„ç»“æœã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14737v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯ä¸€é¡¹é‡è¦è€Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦å› ä¸ºè·å–é«˜è´¨é‡ã€ä¸“å®¶æ ‡æ³¨çš„åºå¤§æ•°æ®é›†å¾ˆå›°éš¾ã€‚å¯¹æ¯”å­¦ä¹ ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ½œåœ¨æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¿½è§†äº†å›¾åƒå†…ç›¸ä¼¼åƒç´ ç¾¤ä¹‹é—´çš„ç‰¹æ€§ï¼Œä¸”åœ¨ç”Ÿæˆå¯¹æ¯”å¯¹æ—¶å¤šä¾èµ–äººå·¥è®¾å®šé˜ˆå€¼ï¼Œæ•ˆç‡ä½ä¸‹ä¸”ç¼ºä¹é€šç”¨æ€§ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºSuperCLçš„æ–°å‹å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²é¢„è®­ç»ƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒçš„ç»“æ„å…ˆéªŒå’Œåƒç´ ç›¸å…³æ€§ï¼Œå¼•å…¥ä¸¤ç§æ–°å‹å¯¹æ¯”å¯¹ç”Ÿæˆç­–ç•¥ï¼šILCPï¼ˆå›¾åƒå†…å±€éƒ¨å¯¹æ¯”å¯¹ç”Ÿæˆï¼‰å’ŒIGCPï¼ˆå›¾åƒé—´å…¨å±€å¯¹æ¯”å¯¹ç”Ÿæˆï¼‰ã€‚åŒæ—¶ï¼Œç»“åˆè¶…åƒç´ åœ°å›¾ç”Ÿæˆä¼ªæ©è†œæ¥æŒ‡å¯¼ç›‘ç£å¯¹æ¯”å­¦ä¹ ã€‚åœ¨8ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSuperCLç›¸è¾ƒäºå…¶ä»–12ç§æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œé¢„æµ‹æ›´ç²¾ç¡®ï¼Œå¹¶åœ¨MMWHSã€CHAOSå’Œè„¾è„æ•°æ®é›†ä¸Šåˆ†åˆ«æé«˜äº†3.15%ã€5.44%ã€7.89%çš„DSCå¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´è·å–é«˜è´¨é‡ã€ä¸“å®¶æ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›†å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æ˜¯è§£å†³è¿™ä¸€é—®é¢˜çš„æ½œåœ¨æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹å¯¹æ¯”å­¦ä¹ æ–¹æ³•SuperCLï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²é¢„è®­ç»ƒã€‚</li>
<li>SuperCLåˆ©ç”¨å›¾åƒçš„ç»“æ„å…ˆéªŒå’Œåƒç´ ç›¸å…³æ€§ï¼Œå¼•å…¥ILCPå’ŒIGCPä¸¤ç§æ–°å‹å¯¹æ¯”å¯¹ç”Ÿæˆç­–ç•¥ã€‚</li>
<li>ç»“åˆè¶…åƒç´ åœ°å›¾ç”Ÿæˆä¼ªæ©è†œæ¥æŒ‡å¯¼ç›‘ç£å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSuperCLè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-295bf680781806bc895992071195028c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20785c42e2b92bba440b0152ebbe93aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3ab8d67a3d9e41b84195f1c0256e085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b751324e40eb5bcd8619fc112eaf7b33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4fe68b20b8cf84dc22935fc79dc32ec.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Med-2D-SegNet-A-Light-Weight-Deep-Neural-Network-for-Medical-2D-Image-Segmentation"><a href="#Med-2D-SegNet-A-Light-Weight-Deep-Neural-Network-for-Medical-2D-Image-Segmentation" class="headerlink" title="Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image   Segmentation"></a>Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image   Segmentation</h2><p><strong>Authors:Md. Sanaullah Chowdhury, Salauddin Tapu, Noyon Kumar Sarkar, Ferdous Bin Ali, Lameya Sabrin</strong></p>
<p>Accurate and efficient medical image segmentation is crucial for advancing clinical diagnostics and surgical planning, yet remains a complex challenge due to the variability in anatomical structures and the demand for low-complexity models. In this paper, we introduced Med-2D SegNet, a novel and highly efficient segmentation architecture that delivers outstanding accuracy while maintaining a minimal computational footprint. Med-2D SegNet achieves state-of-the-art performance across multiple benchmark datasets, including KVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient (DSC) of 89.77% across 20 diverse datasets. Central to its success is the compact Med Block, a specialized encoder design that incorporates dimension expansion and parameter reduction, enabling precise feature extraction while keeping model parameters to a low count of just 2.07 million. Med-2D SegNet excels in cross-dataset generalization, particularly in polyp segmentation, where it was trained on KVASIR-SEG and showed strong performance on unseen datasets, demonstrating its robustness in zero-shot learning scenarios, even though we acknowledge that further improvements are possible. With top-tier performance in both binary and multi-class segmentation, Med-2D SegNet redefines the balance between accuracy and efficiency, setting a new benchmark for medical image analysis. This work paves the way for developing accessible, high-performance diagnostic tools suitable for clinical environments and resource-constrained settings, making it a step forward in the democratization of advanced medical technology. </p>
<blockquote>
<p>å‡†ç¡®é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºæ¨åŠ¨ä¸´åºŠè¯Šæ–­å’Œæ‰‹æœ¯è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†ç”±äºè§£å‰–ç»“æ„çš„å¯å˜æ€§å’Œå¯¹ä½å¤æ‚åº¦æ¨¡å‹çš„éœ€æ±‚ï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Med-2D SegNetï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é«˜æ•ˆçš„åˆ†å‰²æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå‡ºè‰²å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œä¿æŒè¾ƒä½çš„è®¡ç®—å¼€é”€ã€‚Med-2D SegNetåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬KVASIR-SEGã€PH2ã€EndoViså’ŒGLASï¼Œåœ¨20ä¸ªä¸åŒæ•°æ®é›†çš„å¹³å‡Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸º89.77%ã€‚å…¶æˆåŠŸçš„æ ¸å¿ƒæ˜¯ç´§å‡‘çš„Med Blockï¼Œè¿™æ˜¯ä¸€ç§ä¸“ç”¨çš„ç¼–ç å™¨è®¾è®¡ï¼Œç»“åˆäº†ç»´åº¦æ‰©å±•å’Œå‚æ•°å‡å°‘ï¼Œèƒ½å¤Ÿåœ¨ç²¾ç¡®æå–ç‰¹å¾çš„åŒæ—¶ï¼Œå°†æ¨¡å‹å‚æ•°ä¿æŒåœ¨ä¸€ä¸ªè¾ƒä½çš„æ•°é‡ï¼Œä»…æœ‰207ä¸‡ã€‚Med-2D SegNetåœ¨è·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¯è‚‰åˆ†å‰²æ–¹é¢ï¼Œå®ƒåœ¨KVASIR-SEGæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨é›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­çš„ç¨³å¥æ€§ï¼Œå°½ç®¡æˆ‘ä»¬æ‰¿è®¤è¿˜å¯ä»¥è¿›è¡Œè¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚æ— è®ºåœ¨äºŒå…ƒåˆ†å‰²å’Œå¤šç±»åˆ†å‰²æ–¹é¢ï¼ŒMed-2D SegNetéƒ½é‡æ–°å®šä¹‰äº†å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘é€‚åˆä¸´åºŠç¯å¢ƒå’Œèµ„æºå—é™ç¯å¢ƒçš„é«˜æ€§èƒ½è¯Šæ–­å·¥å…·é“ºå¹³äº†é“è·¯ï¼Œæ˜¯å…ˆè¿›åŒ»ç–—æŠ€æœ¯æ°‘ä¸»åŒ–çš„åˆä¸€è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14715v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„Med-2D SegNetï¼Œè¯¥æ¶æ„åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹³å‡Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸º89.77%ã€‚å…¶æ ¸å¿ƒåœ¨äºç´§å‡‘çš„Med Blockï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®çš„ç‰¹å¾æå–å¹¶ä¿æŒæ¨¡å‹å‚æ•°çš„ä½è®¡æ•°ã€‚Med-2D SegNetæ“…é•¿è·¨æ•°æ®é›†æ³›åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¯è‚‰åˆ†å‰²ä¸­è¡¨ç°çªå‡ºã€‚è¯¥æ¶æ„é‡æ–°å®šä¹‰äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æè®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œä¸ºä¸´åºŠç¯å¢ƒå’Œèµ„æºå—é™çš„ç¯å¢ƒå¼€å‘é«˜æ€§èƒ½è¯Šæ–­å·¥å…·é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-2D SegNetæ˜¯ä¸€ç§é«˜æ•ˆåŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„ï¼Œå…·å¤‡å‡ºè‰²çš„æ€§èƒ½ã€‚</li>
<li>å®ƒåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å¹³å‡Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸º89.77%çš„é«˜å‡†ç¡®ç‡ã€‚</li>
<li>Med Blockæ˜¯æ¶æ„çš„æ ¸å¿ƒï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®çš„ç‰¹å¾æå–å¹¶ä¿æŒæ¨¡å‹å‚æ•°çš„ä½è®¡æ•°ã€‚</li>
<li>Med-2D SegNetæ“…é•¿è·¨æ•°æ®é›†æ³›åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¯è‚‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ¶æ„å…·å¤‡ä¼˜å¼‚çš„äºŒè¿›åˆ¶å’Œå¤šç±»åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>Med-2D SegNetä¸ºåŒ»å­¦å›¾åƒåˆ†æè®¾å®šäº†æ–°çš„æ€§èƒ½åŸºå‡†ï¼Œå¹¶æœç€å¼€å‘é«˜æ€§èƒ½è¯Šæ–­å·¥å…·çš„ç›®æ ‡è¿ˆè¿›äº†ä¸€æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b0dd03f7444c8048e51d607840b4375.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05a05b43a70b15afb49e97d5efbfc6ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25d6dc6444cf8bffb1bead96467bbfd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a6ce867de4f9c2405f71693adf66d70.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OmniV-Med-Scaling-Medical-Vision-Language-Model-for-Universal-Visual-Understanding"><a href="#OmniV-Med-Scaling-Medical-Vision-Language-Model-for-Universal-Visual-Understanding" class="headerlink" title="OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual   Understanding"></a>OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual   Understanding</h2><p><strong>Authors:Songtao Jiang, Yuan Wang, Sibo Song, Yan Zhang, Zijie Meng, Bohan Lei, Jian Wu, Jimeng Sun, Zuozhu Liu</strong></p>
<p>The practical deployment of medical vision-language models (Med-VLMs) necessitates seamless integration of textual data with diverse visual modalities, including 2D&#x2F;3D images and videos, yet existing models typically employ separate encoders for different modalities. To address this limitation, we present OmniV-Med, a unified framework for multimodal medical understanding. Our technical contributions are threefold: First, we construct OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K instructional samples spanning 14 medical image modalities and 11 clinical tasks. Second, we devise a rotary position-adaptive encoder that processes multi-resolution 2D&#x2F;3D images and videos within a unified architecture, diverging from conventional modality-specific encoders. Third, we introduce a medical-aware token pruning mechanism that exploits spatial-temporal redundancy in volumetric data (e.g., consecutive CT slices) and medical videos, effectively reducing 60% of visual tokens without performance degradation. Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art performance on 7 benchmarks spanning 2D&#x2F;3D medical imaging and video understanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains comparable performance while requiring only 8 RTX3090 GPUs for training and supporting efficient long-video inference. Data, code and model will be released. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-VLMsï¼‰çš„å®é™…éƒ¨ç½²éœ€è¦æ— ç¼é›†æˆæ–‡æœ¬æ•°æ®ä¸åŒ…æ‹¬äºŒç»´&#x2F;ä¸‰ç»´å›¾åƒå’Œè§†é¢‘åœ¨å†…çš„å¤šç§è§†è§‰æ¨¡å¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸é’ˆå¯¹ä¸åŒæ¨¡å¼ä½¿ç”¨å•ç‹¬çš„ç¼–ç å™¨ã€‚é’ˆå¯¹è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†OmniV-Medï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡å¼åŒ»ç–—ç†è§£çš„ç»Ÿä¸€æ¡†æ¶ã€‚æˆ‘ä»¬çš„æŠ€æœ¯è´¡çŒ®æœ‰ä¸‰ç‚¹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†OmniV-Med-Instructï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«252KæŒ‡ä»¤æ ·æœ¬çš„ç»¼åˆå¤šæ¨¡å¼åŒ»ç–—æ•°æ®é›†ï¼Œæ¶µç›–14ç§åŒ»ç–—å›¾åƒæ¨¡å¼å’Œ11ç§ä¸´åºŠä»»åŠ¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ—‹è½¬ä½ç½®è‡ªé€‚åº”ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¶æ„ä¸­å¤„ç†å¤šåˆ†è¾¨ç‡çš„äºŒç»´&#x2F;ä¸‰ç»´å›¾åƒå’Œè§†é¢‘ï¼Œä¸ä¼ ç»Ÿçš„æ¨¡å¼ç‰¹å®šç¼–ç å™¨ä¸åŒã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒ»å­¦æ„ŸçŸ¥ä»¤ç‰Œä¿®å‰ªæœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨ä½“ç§¯æ•°æ®ï¼ˆä¾‹å¦‚è¿ç»­çš„CTåˆ‡ç‰‡ï¼‰å’ŒåŒ»ç–—è§†é¢‘ä¸­çš„æ—¶ç©ºå†—ä½™ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†60%çš„è§†è§‰ä»¤ç‰Œï¼Œè€Œä¸ä¼šå¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒOmniV-Med-7Båœ¨æ¶µç›–äºŒç»´&#x2F;ä¸‰ç»´åŒ»å­¦æˆåƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡çš„7ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è½»é‡çº§å˜ä½“ï¼ˆOmniV-Med-1.5Bï¼‰åœ¨æ€§èƒ½ä¸Šä¸ä¹‹ç›¸å½“ï¼ŒåŒæ—¶ä»…éœ€è¦8ä¸ªRTX3090 GPUè¿›è¡Œè®­ç»ƒï¼Œå¹¶æ”¯æŒé«˜æ•ˆçš„é•¿è§†é¢‘æ¨ç†ã€‚æ•°æ®ã€ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14692v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºOmniV-Medçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€åŒ»å­¦ç†è§£ã€‚è¯¥æ¡†æ¶å®ç°äº†æ–‡æœ¬æ•°æ®ä¸å¤šç§è§†è§‰æ¨¡æ€ï¼ˆåŒ…æ‹¬äºŒç»´&#x2F;ä¸‰ç»´å›¾åƒå’Œè§†é¢‘ï¼‰çš„æ— ç¼é›†æˆã€‚å…¶ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ„å»ºå¤šæ¨¡æ€åŒ»å­¦æ•°æ®é›†OmniV-Med-Instructï¼Œè®¾è®¡æ—‹è½¬ä½ç½®é€‚åº”æ€§ç¼–ç å™¨ä»¥åŠå¼•å…¥åŒ»å­¦æ„ŸçŸ¥ä»¤ç‰Œä¿®å‰ªæœºåˆ¶ã€‚è¯¥æ¡†æ¶åœ¨äºŒç»´&#x2F;ä¸‰ç»´åŒ»å­¦æˆåƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”æä¾›äº†è½»é‡çº§å˜ä½“ä»¥æ”¯æŒé«˜æ•ˆçš„é•¿è§†é¢‘æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniV-Medæ¡†æ¶å®ç°äº†å¤šæ¨¡æ€åŒ»å­¦æ•°æ®çš„ç»Ÿä¸€å¤„ç†ï¼ŒåŒ…æ‹¬äºŒç»´&#x2F;ä¸‰ç»´å›¾åƒå’Œè§†é¢‘ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€åŒ»å­¦æ•°æ®é›†OmniV-Med-Instructï¼ŒåŒ…å«252Kæ•™å­¦æ ·æœ¬ã€‚</li>
<li>æå‡ºäº†æ—‹è½¬ä½ç½®é€‚åº”æ€§ç¼–ç å™¨ï¼Œèƒ½åœ¨ç»Ÿä¸€æ¶æ„ä¸­å¤„ç†å¤šåˆ†è¾¨ç‡çš„äºŒç»´&#x2F;ä¸‰ç»´å›¾åƒå’Œè§†é¢‘ã€‚</li>
<li>å¼•å…¥äº†åŒ»å­¦æ„ŸçŸ¥ä»¤ç‰Œä¿®å‰ªæœºåˆ¶ï¼Œåˆ©ç”¨ä½“ç§¯æ•°æ®å’ŒåŒ»å­¦è§†é¢‘çš„æ—¶ç©ºå†—ä½™æ€§ï¼Œæœ‰æ•ˆå‡å°‘è§†è§‰ä»¤ç‰Œæ•°é‡ã€‚</li>
<li>OmniV-Medæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œæ¶µç›–äº†äºŒç»´&#x2F;ä¸‰ç»´åŒ»å­¦æˆåƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ã€‚</li>
<li>æä¾›äº†è½»é‡çº§å˜ä½“OmniV-Med-1.5Bï¼Œå¯åœ¨8ä¸ªRTX3090 GPUä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶æ”¯æŒé«˜æ•ˆçš„é•¿è§†é¢‘æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7c27e78d426b7d89294e521e52ea123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-952fc5118b7f2d56c3e39b785fdd04d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e4c8cc97338cd8bf36119c9d91940d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3330bb4e9352f3fa055a03c36a962275.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="K2MUSE-A-human-lower-limb-multimodal-dataset-under-diverse-conditions-for-facilitating-rehabilitation-robotics"><a href="#K2MUSE-A-human-lower-limb-multimodal-dataset-under-diverse-conditions-for-facilitating-rehabilitation-robotics" class="headerlink" title="K2MUSE: A human lower limb multimodal dataset under diverse conditions   for facilitating rehabilitation robotics"></a>K2MUSE: A human lower limb multimodal dataset under diverse conditions   for facilitating rehabilitation robotics</h2><p><strong>Authors:Jiwei Li, Bi Zhang, Xiaowei Tan, Wanxin Chen, Zhaoyuan Liu, Juanjuan Zhang, Weiguang Huo, Jian Huang, Lianqing Liu, Xingang Zhao</strong></p>
<p>The natural interaction and control performance of lower limb rehabilitation robots are closely linked to biomechanical information from various human locomotion activities. Multidimensional human motion data significantly deepen the understanding of the complex mechanisms governing neuromuscular alterations, thereby facilitating the development and application of rehabilitation robots in multifaceted real-world environments. However, currently available lower limb datasets are inadequate for supplying the essential multimodal data and large-scale gait samples necessary for effective data-driven approaches, and they neglect the significant effects of acquisition interference in real applications.To fill this gap, we present the K2MUSE dataset, which includes a comprehensive collection of multimodal data, comprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface electromyography (sEMG) measurements. The proposed dataset includes lower limb multimodal data from 30 able-bodied participants walking under different inclines (0$^\circ$, $\pm$5$^\circ$, and $\pm$10$^\circ$), various speeds (0.5 m&#x2F;s, 1.0 m&#x2F;s, and 1.5 m&#x2F;s), and different nonideal acquisition conditions (muscle fatigue, electrode shifts, and inter-day differences). The kinematic and ground reaction force data were collected via a Vicon motion capture system and an instrumented treadmill with embedded force plates, whereas the sEMG and AUS data were synchronously recorded for thirteen muscles on the bilateral lower limbs. This dataset offers a new resource for designing control frameworks for rehabilitation robots and conducting biomechanical analyses of lower limb locomotion. The dataset is available at <a target="_blank" rel="noopener" href="https://k2muse.github.io/">https://k2muse.github.io/</a>. </p>
<blockquote>
<p>ä¸‹æ–‡è¯¦ç»†é˜è¿°äº†ä¸‹è‚¢åº·å¤æœºå™¨äººçš„è‡ªç„¶äº¤äº’ä¸æ§åˆ¶æ€§èƒ½ä¸å„ç§äººç±»è¿åŠ¨æ´»åŠ¨çš„ç”Ÿç‰©åŠ›å­¦ä¿¡æ¯ä¹‹é—´çš„ç´§å¯†è”ç³»ã€‚å¤šç»´åº¦çš„è¿åŠ¨æ•°æ®æå¤§åœ°åŠ æ·±äº†æˆ‘ä»¬å¯¹ç¥ç»è‚Œè‚‰æ”¹å˜æ§åˆ¶çš„å¤æ‚æœºåˆ¶çš„ç†è§£ï¼Œä»è€Œä¿ƒè¿›äº†åº·å¤æœºå™¨äººåœ¨å¤šæ–¹é¢çš„çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„å¼€å‘å’Œåº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰å¯ç”¨çš„ä¸‹è‚¢æ•°æ®é›†ä¸è¶³ä»¥æä¾›å¿…è¦çš„å¤šæ¨¡å¼æ•°æ®å’Œå¤§è§„æ¨¡çš„æ­¥æ€æ ·æœ¬ï¼Œæ— æ³•æ»¡è¶³æœ‰æ•ˆçš„æ•°æ®é©±åŠ¨æ–¹æ³•çš„éœ€æ±‚ï¼Œè€Œä¸”å®ƒä»¬å¿½è§†äº†çœŸå®åº”ç”¨ä¸­é‡‡é›†å¹²æ‰°çš„æ˜¾è‘—å½±å“ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†K2MUSEæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å…¨é¢çš„å¤šæ¨¡å¼æ•°æ®é›†åˆï¼ŒåŒ…æ‹¬è¿åŠ¨å­¦ã€åŠ¨åŠ›å­¦ã€æŒ¯å¹…æ¨¡å¼è¶…å£°ï¼ˆAUSï¼‰å’Œè¡¨é¢è‚Œç”µå›¾ï¼ˆsEMGï¼‰æµ‹é‡å€¼ã€‚æ‰€æå‡ºçš„æ•°æ®é›†åŒ…æ‹¬æ¥è‡ª30åèº«ä½“å¥å…¨è€…åœ¨ä¸åŒçš„å€¾æ–œåº¦ï¼ˆ0Â°, Â±5Â°, å’ŒÂ±10Â°ï¼‰ã€å„ç§é€Ÿåº¦ï¼ˆ0.5 m&#x2F;s, 1.0 m&#x2F;s, å’Œ 1.5 m&#x2F;sï¼‰å’Œä¸åŒéç†æƒ³é‡‡é›†æ¡ä»¶ï¼ˆè‚Œè‚‰ç–²åŠ³ã€ç”µæç§»ä½å’Œæ—¥é—´å·®å¼‚ï¼‰ä¸‹çš„ä¸‹è‚¢å¤šæ¨¡å¼æ•°æ®ã€‚è¿åŠ¨å­¦å’Œåœ°é¢ååº”åŠ›æ•°æ®æ˜¯é€šè¿‡ViconåŠ¨ä½œæ•æ‰ç³»ç»Ÿå’Œé…å¤‡åŠ›æ¿çš„ä»ªå™¨åŒ–è·‘æ­¥æœºæ”¶é›†çš„ï¼Œè€ŒsEMGå’ŒAUSæ•°æ®åˆ™åŒæ­¥è®°å½•äº†åŒè…¿çš„åä¸‰å—è‚Œè‚‰ã€‚è¯¥æ•°æ®é›†ä¸ºè®¾è®¡åº·å¤æœºå™¨äººçš„æ§åˆ¶æ¡†æ¶å’Œè¿›è¡Œä¸‹è‚¢è¿åŠ¨ç”Ÿç‰©åŠ›å­¦åˆ†ææä¾›äº†æ–°çš„èµ„æºã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://k2muse.github.io/%E8%8E%B7%E5%8F%96%E3%80%82">https://k2muse.github.io/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14602v1">PDF</a> 23 pages, 13 figures,4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†K2MUSEæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šç§æ¨¡å¼çš„æ•°æ®ï¼ŒåŒ…æ‹¬è¿åŠ¨å­¦ã€åŠ¨åŠ›å­¦ã€æŒ¯å¹…æ¨¡å¼è¶…å£°æ³¢ï¼ˆAUSï¼‰å’Œè¡¨é¢è‚Œç”µå›¾ï¼ˆsEMGï¼‰æµ‹é‡å€¼ã€‚æ•°æ®é›†åŒ…å«30åèº«ä½“å¥å…¨è€…åœ¨ä¸åŒå¡åº¦ã€é€Ÿåº¦å’Œéç†æƒ³é‡‡é›†æ¡ä»¶ä¸‹çš„ä¸‹è‚¢å¤šæ¨¡å¼æ•°æ®ã€‚è¯¥æ•°æ®é›†ä¸ºåº·å¤æœºå™¨äººçš„æ§åˆ¶æ¡†æ¶è®¾è®¡å’Œä¸‹è‚¢è¿åŠ¨ç”Ÿç‰©åŠ›å­¦åˆ†ææä¾›äº†æ–°çš„èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>K2MUSEæ•°æ®é›†åŒ…å«å¤šç§æ¨¡å¼çš„æ•°æ®ï¼Œå¦‚è¿åŠ¨å­¦ã€åŠ¨åŠ›å­¦ã€AUSå’ŒsEMGæµ‹é‡å€¼ï¼Œç”¨äºæ·±å…¥åˆ†æä¸‹è‚¢åº·å¤æœºåˆ¶ã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ¥è‡ª30åèº«ä½“å¥å…¨è€…çš„ä¸‹è‚¢å¤šæ¨¡å¼æ•°æ®ï¼Œæ¶µç›–äº†ä¸åŒçš„è¡Œèµ°æ¡ä»¶ï¼Œå¦‚å¡åº¦ã€é€Ÿåº¦å’Œè‚Œè‚‰çŠ¶æ€ã€‚</li>
<li>æ•°æ®é›†è€ƒè™‘äº†éç†æƒ³é‡‡é›†æ¡ä»¶ï¼Œå¦‚è‚Œè‚‰ç–²åŠ³ã€ç”µæç§»ä½å’Œæ—¥é—´å·®å¼‚ï¼Œå¢åŠ äº†æ•°æ®çš„å®ç”¨æ€§ã€‚</li>
<li>K2MUSEæ•°æ®é›†æœ‰åŠ©äºä¸ºåº·å¤æœºå™¨äººè®¾è®¡æ§åˆ¶æ¡†æ¶ï¼Œå¹¶è¿›è¡Œä¸‹è‚¢è¿åŠ¨çš„ç”Ÿç‰©åŠ›å­¦åˆ†æã€‚</li>
<li>æ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://k2muse.github.io/%E8%AE%BF%E9%97%AE%E3%80%82">https://k2muse.github.io/è®¿é—®ã€‚</a></li>
<li>è¯¥æ•°æ®é›†çš„é‡è¦æ€§åœ¨äºå¡«è¡¥äº†ç°æœ‰ä¸‹è‚¢æ•°æ®é›†åœ¨æä¾›å¿…è¦å¤šæ¨¡å¼æ•°æ®å’Œå¤§è§„æ¨¡æ­¥æ€æ ·æœ¬æ–¹é¢çš„ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f587719b080115df8addadefe842f06f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5756e0f45314aa1fdf0af21973720dcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d9398f01e20307247fe711e77713a4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LGD-Leveraging-Generative-Descriptions-for-Zero-Shot-Referring-Image-Segmentation"><a href="#LGD-Leveraging-Generative-Descriptions-for-Zero-Shot-Referring-Image-Segmentation" class="headerlink" title="LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image   Segmentation"></a>LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image   Segmentation</h2><p><strong>Authors:Jiachen Li, Qing Xie, Xiaohan Yu, Hongyun Wang, Jinyu Xu, Yongjian Liu, Yongsheng Gao</strong></p>
<p>Zero-shot referring image segmentation aims to locate and segment the target region based on a referring expression, with the primary challenge of aligning and matching semantics across visual and textual modalities without training. Previous works address this challenge by utilizing Vision-Language Models and mask proposal networks for region-text matching. However, this paradigm may lead to incorrect target localization due to the inherent ambiguity and diversity of free-form referring expressions. To alleviate this issue, we present LGD (Leveraging Generative Descriptions), a framework that utilizes the advanced language generation capabilities of Multi-Modal Large Language Models to enhance region-text matching performance in Vision-Language Models. Specifically, we first design two kinds of prompts, the attribute prompt and the surrounding prompt, to guide the Multi-Modal Large Language Models in generating descriptions related to the crucial attributes of the referent object and the details of surrounding objects, referred to as attribute description and surrounding description, respectively. Secondly, three visual-text matching scores are introduced to evaluate the similarity between instance-level visual features and textual features, which determines the mask most associated with the referring expression. The proposed method achieves new state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU compared to previous methods. </p>
<blockquote>
<p>é›¶æ ·æœ¬æŒ‡ä»£å›¾åƒåˆ†å‰²æ—¨åœ¨åŸºäºæŒ‡ä»£è¡¨è¾¾å¼å®šä½å¹¶åˆ†å‰²ç›®æ ‡åŒºåŸŸï¼Œå…¶ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨æ²¡æœ‰è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰å¯¹é½å’ŒåŒ¹é…ã€‚ä¹‹å‰çš„å·¥ä½œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ©è†œææ¡ˆç½‘ç»œè¿›è¡ŒåŒºåŸŸæ–‡æœ¬åŒ¹é…æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼å¯èƒ½å¯¼è‡´ç”±äºè‡ªç”±å½¢å¼çš„æŒ‡ä»£è¡¨è¾¾å¼å›ºæœ‰çš„æ¨¡ç³Šæ€§å’Œå¤šæ ·æ€§è€Œå¯¼è‡´ç›®æ ‡å®šä½é”™è¯¯ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LGDï¼ˆåˆ©ç”¨ç”Ÿæˆæè¿°ï¼‰ï¼Œä¸€ä¸ªåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›è¯­è¨€ç”Ÿæˆèƒ½åŠ›æ¥æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ä¸­åŒºåŸŸæ–‡æœ¬åŒ¹é…æ€§èƒ½çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡ä¸¤ç§æç¤ºï¼Œå±æ€§æç¤ºå’Œå‘¨å›´æç¤ºï¼Œæ¥å¼•å¯¼å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸æŒ‡ä»£å¯¹è±¡çš„å…³é”®å±æ€§å’Œå‘¨å›´å¯¹è±¡çš„ç»†èŠ‚ç›¸å…³çš„æè¿°ï¼Œåˆ†åˆ«ç§°ä¸ºå±æ€§æè¿°å’Œå‘¨å›´æè¿°ã€‚å…¶æ¬¡ï¼Œå¼•å…¥äº†ä¸‰ç§è§†è§‰æ–‡æœ¬åŒ¹é…åˆ†æ•°ï¼Œä»¥è¯„ä¼°å®ä¾‹çº§è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œç¡®å®šä¸æŒ‡ä»£è¡¨è¾¾å¼æœ€ç›¸å…³çš„æ©è†œã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨IoUå’ŒmIoUæ–¹é¢åˆ†åˆ«æé«˜äº†æœ€é«˜9.97%å’Œ11.29%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14467v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆæè¿°èƒ½åŠ›ï¼Œå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†å‰²ä¸­çš„åŒºåŸŸæ–‡æœ¬åŒ¹é…æ€§èƒ½çš„æ–¹æ³•ã€‚é€šè¿‡è®¾è®¡å±æ€§æç¤ºå’Œç¯ç»•æç¤ºï¼Œç”Ÿæˆä¸æŒ‡ä»£å¯¹è±¡çš„å…³é”®å±æ€§å’Œå‘¨å›´å¯¹è±¡ç›¸å…³çš„æè¿°ï¼Œå¼•å…¥ä¸‰ç§è§†è§‰æ–‡æœ¬åŒ¹é…è¯„åˆ†ï¼Œæé«˜æŒ‡ä»£è¡¨è¾¾å¼ä¸å®ä¾‹çº§è§†è§‰ç‰¹å¾çš„ç›¸ä¼¼æ€§è¯„ä¼°å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é›¶æ ·æœ¬å›¾åƒåˆ†å‰²æ—¨åœ¨æ ¹æ®å¼•ç”¨è¡¨è¾¾å¼å®šä½å¹¶åˆ†å‰²ç›®æ ‡åŒºåŸŸï¼Œä¸»è¦æŒ‘æˆ˜æ˜¯åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´å®ç°è¯­ä¹‰å¯¹é½å’ŒåŒ¹é…ï¼Œè€Œæ— éœ€è®­ç»ƒã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ©è†œææ¡ˆç½‘ç»œè¿›è¡ŒåŒºåŸŸæ–‡æœ¬åŒ¹é…æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¯èƒ½å› è‡ªç”±å½¢å¼çš„å¼•ç”¨è¡¨è¾¾å¼å›ºæœ‰çš„æ¨¡ç³Šæ€§å’Œå¤šæ ·æ€§è€Œå¯¼è‡´ç›®æ ‡å®šä½é”™è¯¯ã€‚</li>
<li>LGDï¼ˆåˆ©ç”¨ç”Ÿæˆæè¿°ï¼‰æ¡†æ¶é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›è¯­è¨€ç”Ÿæˆèƒ½åŠ›æ¥å¢å¼ºåŒºåŸŸæ–‡æœ¬åŒ¹é…æ€§èƒ½ã€‚</li>
<li>LGDè®¾è®¡äº†å±æ€§æç¤ºå’Œç¯ç»•æç¤ºï¼Œä»¥ç”Ÿæˆä¸æŒ‡ä»£å¯¹è±¡çš„å…³é”®å±æ€§å’Œå‘¨å›´å¯¹è±¡ç›¸å…³çš„æè¿°ï¼Œç§°ä¸ºå±æ€§æè¿°å’Œç¯ç»•æè¿°ã€‚</li>
<li>å¼•å…¥ä¸‰ç§è§†è§‰æ–‡æœ¬åŒ¹é…è¯„åˆ†ï¼Œä»¥è¯„ä¼°å®ä¾‹çº§è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œç¡®å®šä¸å¼•ç”¨è¡¨è¾¾å¼æœ€ç›¸å…³çš„æ©è†œã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d14a3b2310b921ffab5bb132f039c99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c015040c65197a1ef8f66bcf0d13365.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="WT-BCP-Wavelet-Transform-based-Bidirectional-Copy-Paste-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#WT-BCP-Wavelet-Transform-based-Bidirectional-Copy-Paste-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="WT-BCP: Wavelet Transform based Bidirectional Copy-Paste for   Semi-Supervised Medical Image Segmentation"></a>WT-BCP: Wavelet Transform based Bidirectional Copy-Paste for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Mingya Zhang, Liang Wang, Limei Gu, Tingsheng Ling, Xianping Tao</strong></p>
<p>Semi-supervised medical image segmentation (SSMIS) shows promise in reducing reliance on scarce labeled medical data. However, SSMIS field confronts challenges such as distribution mismatches between labeled and unlabeled data, artificial perturbations causing training biases, and inadequate use of raw image information, especially low-frequency (LF) and high-frequency (HF) components.To address these challenges, we propose a Wavelet Transform based Bidirectional Copy-Paste SSMIS framework, named WT-BCP, which improves upon the Mean Teacher approach. Our method enhances unlabeled data understanding by copying random crops between labeled and unlabeled images and employs WT to extract LF and HF details.We propose a multi-input and multi-output model named XNet-Plus, to receive the fused information after WT. Moreover, consistency training among multiple outputs helps to mitigate learning biases introduced by artificial perturbations. During consistency training, the mixed images resulting from WT are fed into both models, with the student modelâ€™s output being supervised by pseudo-labels and ground-truth. Extensive experiments conducted on 2D and 3D datasets confirm the effectiveness of our model.Code: <a target="_blank" rel="noopener" href="https://github.com/simzhangbest/WT-BCP">https://github.com/simzhangbest/WT-BCP</a>. </p>
<blockquote>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰åœ¨å‡å°‘å¯¹ç¨€ç¼ºæ ‡è®°åŒ»å­¦æ•°æ®çš„ä¾èµ–æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒSSMISé¢†åŸŸé¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚æ ‡è®°å’Œæ— æ ‡è®°æ•°æ®åˆ†å¸ƒä¸åŒ¹é…ã€äººä¸ºæ‰°åŠ¨å¯¼è‡´è®­ç»ƒåè§ä»¥åŠæœªå……åˆ†åˆ©ç”¨åŸå§‹å›¾åƒä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯ä½é¢‘ï¼ˆLFï¼‰å’Œé«˜é¢‘ï¼ˆHFï¼‰æˆåˆ†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå°æ³¢å˜æ¢çš„åŒå‘å¤åˆ¶ç²˜è´´SSMISæ¡†æ¶ï¼Œåä¸ºWT-BCPï¼Œè¯¥æ¡†æ¶æ”¹è¿›äº†Mean Teacheræ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾å›¾åƒä¹‹é—´å¤åˆ¶éšæœºè£å‰ªéƒ¨åˆ†æ¥æé«˜å¯¹æ— æ ‡ç­¾æ•°æ®çš„ç†è§£ï¼Œå¹¶åˆ©ç”¨WTæå–LFå’ŒHFç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºXNet-Plusçš„å¤šè¾“å…¥å¤šè¾“å‡ºæ¨¡å‹ï¼Œä»¥æ¥æ”¶WTèåˆåçš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå¤šä¸ªè¾“å‡ºä¹‹é—´çš„ä¸€è‡´æ€§è®­ç»ƒæœ‰åŠ©äºå‡è½»äººä¸ºæ‰°åŠ¨å¼•å…¥çš„å­¦ä¹ åè§ã€‚åœ¨ä¸€è‡´æ€§è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡WTç”Ÿæˆçš„æ··åˆå›¾åƒè¢«è¾“å…¥åˆ°ä¸¤ä¸ªæ¨¡å‹ä¸­ï¼Œå­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºå—åˆ°ä¼ªæ ‡ç­¾å’ŒçœŸå®æ ‡ç­¾çš„ç›‘ç£ã€‚åœ¨äºŒç»´å’Œä¸‰ç»´æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯å®äº†æˆ‘ä»¬çš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/simzhangbest/WT-BCP%E3%80%82">https://github.com/simzhangbest/WT-BCPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14445v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŠç›‘ç£åˆ†å‰²ï¼ˆSSMISï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®åˆ†å¸ƒä¸åŒ¹é…ã€è®­ç»ƒåè§ä»¥åŠä½é¢‘ï¼ˆLFï¼‰å’Œé«˜é¢‘ï¼ˆHFï¼‰æˆåˆ†çš„ä½¿ç”¨ä¸è¶³ç­‰ã€‚ä¸ºæ­¤ï¼Œæå‡ºåŸºäºå°æ³¢å˜æ¢çš„åŒå‘å¤åˆ¶ç²˜è´´SSMISæ¡†æ¶WT-BCPï¼Œæ”¹è¿›Mean Teacheræ–¹æ³•ï¼Œæé«˜æœªæ ‡è®°æ•°æ®ç†è§£ï¼Œé€šè¿‡WTæå–LFå’ŒHFç»†èŠ‚ã€‚åŒæ—¶å¼•å…¥å¤šè¾“å…¥å¤šè¾“å‡ºæ¨¡å‹XNet-Pluså’Œä¸€è‡´æ€§è®­ç»ƒç­–ç•¥æ¥ç¼“è§£å­¦ä¹ åè§ã€‚ä»£ç å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSMISæŠ€æœ¯åœ¨å‡å°‘ä¾èµ–ç¨€ç¼ºæ ‡è®°åŒ»å­¦æ•°æ®æ–¹é¢å±•ç°æ½œåŠ›ã€‚</li>
<li>ç›®å‰SSMISé¢ä¸´æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®åˆ†å¸ƒä¸åŒ¹é…ã€è®­ç»ƒåè§ä»¥åŠä½é¢‘é«˜é¢‘æˆåˆ†ä½¿ç”¨ä¸è¶³ç­‰ã€‚</li>
<li>WT-BCPæ¡†æ¶é€šè¿‡å°æ³¢å˜æ¢æ”¹å–„æœªæ ‡è®°æ•°æ®ç†è§£ã€‚</li>
<li>WT-BCPé‡‡ç”¨åŸºäºMean Teacheræ–¹æ³•çš„æ”¹è¿›ã€‚</li>
<li>XNet-Plusæ¨¡å‹ç”¨äºæ¥æ”¶å°æ³¢å˜æ¢åçš„èåˆä¿¡æ¯ã€‚</li>
<li>ä¸€è‡´æ€§è®­ç»ƒç­–ç•¥æœ‰åŠ©äºç¼“è§£ç”±äººå·¥æ‰°åŠ¨å¼•å…¥çš„å­¦ä¹ åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2b550b1df53afa69ce1149340a880a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b02cb40e3d0cd9636fd4372e2b01f48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdaf9cf5093281115e995ac2da0eea0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4f5ee64e0f33b8ab059353961903185.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24e946f573ae8a7134066e6161af648c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29e718a63dba67bb65caedcf10112a7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-118849f28961f0690b43206b80c9151f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Exploring-Modality-Guidance-to-Enhance-VFM-based-Feature-Fusion-for-UDA-in-3D-Semantic-Segmentation"><a href="#Exploring-Modality-Guidance-to-Enhance-VFM-based-Feature-Fusion-for-UDA-in-3D-Semantic-Segmentation" class="headerlink" title="Exploring Modality Guidance to Enhance VFM-based Feature Fusion for UDA   in 3D Semantic Segmentation"></a>Exploring Modality Guidance to Enhance VFM-based Feature Fusion for UDA   in 3D Semantic Segmentation</h2><p><strong>Authors:Johannes Spoecklberger, Wei Lin, Pedro Hermosilla, Sivan Doveh, Horst Possegger, M. Jehanzeb Mirza</strong></p>
<p>Vision Foundation Models (VFMs) have become a de facto choice for many downstream vision tasks, like image classification, image segmentation, and object localization. However, they can also provide significant utility for downstream 3D tasks that can leverage the cross-modal information (e.g., from paired image data). In our work, we further explore the utility of VFMs for adapting from a labeled source to unlabeled target data for the task of LiDAR-based 3D semantic segmentation. Our method consumes paired 2D-3D (image and point cloud) data and relies on the robust (cross-domain) features from a VFM to train a 3D backbone on a mix of labeled source and unlabeled target data. At the heart of our method lies a fusion network that is guided by both the image and point cloud streams, with their relative contributions adjusted based on the target domain. We extensively compare our proposed methodology with different state-of-the-art methods in several settings and achieve strong performance gains. For example, achieving an average improvement of 6.5 mIoU (over all tasks), when compared with the previous state-of-the-art. </p>
<blockquote>
<p>è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰å·²æˆä¸ºè®¸å¤šä¸‹æ¸¸è§†è§‰ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€å›¾åƒåˆ†å‰²å’Œå¯¹è±¡å®šä½ï¼‰çš„å®é™…é€‰æ‹©ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¹Ÿå¯ä»¥ä¸ºä¸‹æ¸¸çš„3Dä»»åŠ¡æä¾›é‡è¦æ•ˆç”¨ï¼Œè¿™äº›ä»»åŠ¡å¯ä»¥åˆ©ç”¨è·¨æ¨¡æ€ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œæ¥è‡ªé…å¯¹å›¾åƒæ•°æ®ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†VFMåœ¨æ¿€å…‰é›·è¾¾åŸºäºçš„3Dè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­çš„ç”¨é€”ï¼Œè¯¥ä»»åŠ¡ä»æ ‡è®°æºæ•°æ®é€‚åº”åˆ°æ— æ ‡è®°ç›®æ ‡æ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨é…å¯¹2D-3Dï¼ˆå›¾åƒå’Œç‚¹äº‘ï¼‰æ•°æ®ï¼Œå¹¶ä¾èµ–äºVFMçš„ç¨³å¥ï¼ˆè·¨åŸŸï¼‰ç‰¹å¾ï¼Œåœ¨æ ‡è®°æºæ•°æ®å’Œæ— æ ‡è®°ç›®æ ‡æ•°æ®çš„æ··åˆä¸Šè®­ç»ƒä¸€ä¸ª3Dä¸»å¹²ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªèåˆç½‘ç»œï¼Œè¯¥ç½‘ç»œç”±å›¾åƒå’Œç‚¹äº‘æµå…±åŒå¼•å¯¼ï¼Œå¹¶æ ¹æ®ç›®æ ‡åŸŸè°ƒæ•´å®ƒä»¬çš„ç›¸å¯¹è´¡çŒ®ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„è®¾ç½®ä¸‹ä¸ä¸åŒçš„æœ€å…ˆè¿›æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„æ¯”è¾ƒï¼Œå¹¶å®ç°äº†å¼ºå¤§çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œä¸ä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šçš„å¹³å‡æé«˜äº†6.5 mIoUã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14231v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Vision Foundation Modelsï¼ˆVFMsï¼‰åœ¨LiDARåŸºäºçš„3Dè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚ç ”ç©¶åˆ©ç”¨é…å¯¹2D-3Dæ•°æ®è¿›è¡Œè·¨åŸŸç‰¹å¾æå–ï¼Œé€šè¿‡VFMè®­ç»ƒ3Déª¨å¹²ç½‘ï¼Œå¹¶å€ŸåŠ©èåˆç½‘ç»œç»“åˆå›¾åƒå’Œç‚¹äº‘æµï¼Œå®ç°æºåˆ°ç›®æ ‡åŸŸçš„é€‚åº”ã€‚è¯¥ç ”ç©¶åœ¨ä¸åŒè®¾ç½®ä¸‹ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¾‹å¦‚åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†6.5 mIoUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Foundation Models (VFMs) å¯ä»¥ä¸ºä¸‹æ¸¸çš„3Dä»»åŠ¡æä¾›æ˜¾è‘—æ•ˆç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ©ç”¨è·¨æ¨¡æ€ä¿¡æ¯çš„æƒ…å†µä¸‹ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†VFMåœ¨LiDAR-based 3Dè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­çš„é€‚åº”æ€§é—®é¢˜ï¼Œä»æ ‡ç­¾æºæ•°æ®é€‚åº”åˆ°æ— æ ‡ç­¾ç›®æ ‡æ•°æ®ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨é…å¯¹2D-3Dæ•°æ®ï¼ˆå›¾åƒå’Œç‚¹äº‘ï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¾èµ–VFMçš„ç¨³å¥è·¨åŸŸç‰¹å¾ã€‚</li>
<li>èåˆç½‘ç»œæ˜¯è¯¥æ–¹æ³•çš„æ ¸å¿ƒï¼Œå®ƒç»“åˆäº†å›¾åƒå’Œç‚¹äº‘æµï¼Œå¹¶æ ¹æ®ç›®æ ‡åŸŸè°ƒæ•´å…¶ç›¸å¯¹è´¡çŒ®ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè®¾ç½®ä¸‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå®ç°äº†å¹³å‡6.5 mIoUçš„æå‡ï¼Œè¿™æ˜¾ç¤ºäº†å…¶æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-27ce45437028e82fa3a9ba27169a5ff5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6507aa3f2859b57e169626fcb89ea150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fb74e4312d20ad331b8ebf36fe0513d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f97d517563656f44a0256f8487103091.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Association-between-nutritional-factors-inflammatory-biomarkers-and-cancer-types-an-analysis-of-NHANES-data-using-machine-learning"><a href="#Association-between-nutritional-factors-inflammatory-biomarkers-and-cancer-types-an-analysis-of-NHANES-data-using-machine-learning" class="headerlink" title="Association between nutritional factors, inflammatory biomarkers and   cancer types: an analysis of NHANES data using machine learning"></a>Association between nutritional factors, inflammatory biomarkers and   cancer types: an analysis of NHANES data using machine learning</h2><p><strong>Authors:Yuqing Liu, Meng Zhao, Guanlan Hu, Yuchen Zhang</strong></p>
<p>Background. Diet and inflammation are critical factors influencing cancer risk. However, the combined impact of nutritional status and inflammatory biomarkers on cancer status and type, using machine learning (ML), remains underexplored.   Objectives. This study investigates the association between nutritional factors, inflammatory biomarkers, and cancer status, and whether these relationships differ across cancer types using National Health and Nutrition Examination Survey (NHANES) data.   Methods. We analyzed 24 macro- and micronutrients, C-reactive protein (CRP), and the advanced lung cancer inflammation index (ALI) in 26,409 NHANES participants (2,120 with cancer). Multivariable logistic regression assessed associations with cancer prevalence. We also examined whether these features differed across the five most common cancer types. To evaluate predictive value, we applied three ML models - Logistic Regression, Random Forest, and XGBoost - on the full feature set.   Results. The cohortâ€™s mean age was 49.1 years; 34.7% were obese. Comorbidities such as anemia and liver conditions, along with nutritional factors like protein and several vitamins, were key predictors of cancer status. Among the models, Random Forest performed best, achieving an accuracy of 0.72.   Conclusions. Higher-quality nutritional intake and lower levels of inflammation may offer protective effects against cancer. These findings highlight the potential of combining nutritional and inflammatory markers with ML to inform cancer prevention strategies. </p>
<blockquote>
<p>èƒŒæ™¯ã€‚é¥®é£Ÿå’Œç‚ç—‡æ˜¯å½±å“ç™Œç—‡é£é™©çš„å…³é”®å› ç´ ã€‚ç„¶è€Œï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç»“åˆè¥å…»çŠ¶å†µå’Œç‚ç—‡ç”Ÿç‰©æ ‡å¿—ç‰©å¯¹ç™Œç—‡çŠ¶å†µå’Œç±»å‹çš„å½±å“çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚ç›®æ ‡ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨è¥å…»å› ç´ ã€ç‚ç—‡ç”Ÿç‰©æ ‡å¿—ç‰©ä¸ç™Œç—‡çŠ¶å†µä¹‹é—´çš„å…³è”ï¼Œå¹¶ç ”ç©¶è¿™äº›å…³ç³»åœ¨ä¸åŒç™Œç—‡ç±»å‹ä¸­æ˜¯å¦æœ‰æ‰€ä¸åŒï¼ŒåŒæ—¶ä½¿ç”¨å›½å®¶å¥åº·å’Œè¥å…»æ£€æŸ¥è°ƒæŸ¥ï¼ˆNHANESï¼‰æ•°æ®è¿›è¡Œåˆ†æã€‚æ–¹æ³•ã€‚æˆ‘ä»¬åˆ†æäº†NHANESçš„26409åå‚ä¸è€…ï¼ˆå…¶ä¸­2120åæ‚£æœ‰ç™Œç—‡ï¼‰çš„24ç§å®é‡å’Œå¾®é‡å…ƒç´ ã€Cååº”è›‹ç™½ï¼ˆCRPï¼‰å’Œé«˜çº§è‚ºç™Œç‚ç—‡æŒ‡æ•°ï¼ˆALIï¼‰ã€‚å¤šå…ƒé€»è¾‘å›å½’ç”¨äºè¯„ä¼°ä¸ç™Œç—‡å‘ç—…ç‡çš„å…³ç³»ã€‚æˆ‘ä»¬è¿˜æ£€æŸ¥äº†è¿™äº›ç‰¹å¾æ˜¯å¦åœ¨äº”ç§æœ€å¸¸è§çš„ç™Œç—‡ç±»å‹ä¸­å­˜åœ¨å·®å¼‚ã€‚ä¸ºäº†è¯„ä¼°é¢„æµ‹ä»·å€¼ï¼Œæˆ‘ä»¬åœ¨æ•´ä¸ªç‰¹å¾é›†ä¸Šåº”ç”¨äº†ä¸‰ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬é€»è¾‘å›å½’ã€éšæœºæ£®æ—å’ŒXGBoostã€‚ç»“æœã€‚è¯¥é˜Ÿåˆ—çš„å¹³å‡å¹´é¾„ä¸º49.1å²ï¼Œå…¶ä¸­34.7%ä¸ºè‚¥èƒ–äººç¾¤ã€‚è´«è¡€å’Œè‚è„ç–¾ç—…ç­‰ä¼´éšç–¾ç—…ï¼Œä»¥åŠè›‹ç™½è´¨å’Œå„ç§ç»´ç”Ÿç´ ç­‰è¥å…»å› ç´ ï¼Œæ˜¯ç™Œç—‡çŠ¶å†µçš„ä¸»è¦é¢„æµ‹å› ç´ ã€‚åœ¨æ¨¡å‹ä¸­ï¼Œéšæœºæ£®æ—è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º0.72ã€‚ç»“è®ºã€‚é«˜è´¨é‡çš„è¥å…»æ‘„å…¥å’Œè¾ƒä½çš„ç‚ç—‡æ°´å¹³å¯èƒ½å¯¹é¢„é˜²ç™Œç—‡å…·æœ‰ä¿æŠ¤ä½œç”¨ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ç»“åˆè¥å…»å’Œç‚ç—‡æ ‡å¿—ç‰©ä¸æœºå™¨å­¦ä¹ æ¥ä¸ºç™Œç—‡é¢„é˜²ç­–ç•¥æä¾›ä¿¡æ¯çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13978v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŒ»å­¦ç ”ç©¶è¡¨æ˜é¥®é£Ÿä¸ç‚ç—‡æ˜¯ç™Œç—‡é£é™©çš„é‡è¦å½±å“å› ç´ ã€‚æœ¬ç ”ç©¶ä½¿ç”¨å…¨å›½å¥åº·å’Œè¥å…»æ™®æŸ¥æ•°æ®ï¼Œè°ƒæŸ¥è¥å…»å› ç´ ã€ç‚ç—‡æ ‡å¿—ç‰©ä¸ç™Œç—‡çŠ¶æ€ä¹‹é—´çš„å…³è”ï¼Œå¹¶æ¢ç´¢è¿™äº›å…³ç³»åœ¨ä¸åŒç™Œç—‡ç±»å‹ä¸­æ˜¯å¦æœ‰å·®å¼‚ã€‚åˆ†ææ˜¾ç¤ºï¼Œè´«è¡€å’Œè‚è„ç–¾ç—…ç­‰å¹¶å‘ç–¾ç—…ä»¥åŠè›‹ç™½è´¨å’Œç»´ç”Ÿç´ ç­‰è¥å…»å› ç´ éƒ½æ˜¯ç™Œç—‡çŠ¶å†µçš„å…³é”®é¢„æµ‹å› å­ã€‚æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆéšæœºæ£®æ—æ¨¡å‹ï¼‰çš„é¢„æµ‹å‡†ç¡®æ€§æœ€é«˜ï¼Œè¾¾åˆ°0.72ã€‚ç ”ç©¶æŒ‡å‡ºä¼˜è´¨è¥å…»æ‘„å…¥å’Œè¾ƒä½çš„ç‚ç—‡æ°´å¹³å¯èƒ½å¯¹é¢„é˜²ç™Œç—‡æœ‰ä¿æŠ¤ä½œç”¨ã€‚ç»“åˆè¥å…»å’Œç‚ç—‡æ ‡å¿—ç‰©ä¸æœºå™¨å­¦ä¹ æœ‰åŠ©äºåˆ¶å®šç™Œç—‡é¢„é˜²ç­–ç•¥ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ul>
<li>é¥®é£Ÿä¸ç‚ç—‡æ˜¯ç™Œç—‡é£é™©çš„å…³é”®å› ç´ ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨å…¨å›½å¥åº·å’Œè¥å…»æ™®æŸ¥æ•°æ®ï¼Œæ¢è®¨äº†è¥å…»å› ç´ ã€ç‚ç—‡æ ‡å¿—ç‰©ä¸ç™Œç—‡çŠ¶æ€çš„å…³ç³»ã€‚</li>
<li>å¹¶å‘ç–¾ç—…å¦‚è´«è¡€å’Œè‚è„ç–¾ç—…ä»¥åŠè¥å…»å› ç´ å¦‚è›‹ç™½è´¨å’Œå¤šç§ç»´ç”Ÿç´ ï¼Œéƒ½æ˜¯é¢„æµ‹ç™Œç—‡çš„å…³é”®å› ç´ ã€‚</li>
<li>æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆéšæœºæ£®æ—æ¨¡å‹ï¼‰æ˜¾ç¤ºå‡ºæœ€é«˜çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>ä¼˜è´¨è¥å…»æ‘„å…¥å’Œè¾ƒä½çš„ç‚ç—‡æ°´å¹³å¯èƒ½æœ‰åŠ©äºé¢„é˜²ç™Œç—‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e15d71d947c86757e8be77ca82415987.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Stochastic-momentum-ADMM-for-nonconvex-and-nonsmooth-optimization-with-application-to-PnP-algorithm"><a href="#Stochastic-momentum-ADMM-for-nonconvex-and-nonsmooth-optimization-with-application-to-PnP-algorithm" class="headerlink" title="Stochastic momentum ADMM for nonconvex and nonsmooth optimization with   application to PnP algorithm"></a>Stochastic momentum ADMM for nonconvex and nonsmooth optimization with   application to PnP algorithm</h2><p><strong>Authors:Kangkang Deng, Shuchang Zhang, Boyu Wang, Jiachen Jin, Juan Zhou, Hongxia Wang</strong></p>
<p>This paper proposes SMADMM, a single-loop Stochastic Momentum Alternating Direction Method of Multipliers for solving a class of nonconvex and nonsmooth composite optimization problems. SMADMM achieves the optimal oracle complexity of $\mathcal{O}(\epsilon^{-3&#x2F;2})$ in the online setting. Unlike previous stochastic ADMM algorithms that require large mini-batches or a double-loop structure, SMADMM uses only $\mathcal{O}(1)$ stochastic gradient evaluations per iteration and avoids costly restarts. To further improve practicality, we incorporate dynamic step sizes and penalty parameters, proving that SMADMM maintains its optimal complexity without the need for large initial batches. We also develop PnP-SMADMM by integrating plug-and-play priors, and establish its theoretical convergence under mild assumptions. Extensive experiments on classification, CT image reconstruction, and phase retrieval tasks demonstrate that our approach outperforms existing stochastic ADMM methods both in accuracy and efficiency, validating our theoretical results. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†SMADMMï¼Œè¿™æ˜¯ä¸€ç§å•ç¯éšæœºåŠ¨é‡äº¤æ›¿æ–¹å‘æ³•ï¼ˆStochastic Momentum Alternating Direction Method of Multipliersï¼‰ï¼Œç”¨äºè§£å†³ä¸€ç±»éå‡¸éå…‰æ»‘å¤åˆä¼˜åŒ–é—®é¢˜ã€‚SMADMMåœ¨åœ¨çº¿è®¾ç½®ä¸‹è¾¾åˆ°äº†æœ€ä¼˜çš„$\mathcal{O}(\epsilon^{-3&#x2F;2})$çš„Oracleå¤æ‚æ€§ã€‚ä¸åŒäºä¹‹å‰éœ€è¦å¤§æ‰¹é‡å¾®æ‰¹å¤„ç†æˆ–åŒé‡å¾ªç¯ç»“æ„çš„éšæœºADMMç®—æ³•ï¼ŒSMADMMæ¯æ¬¡è¿­ä»£åªä½¿ç”¨$\mathcal{O}(1)$ä¸ªéšæœºæ¢¯åº¦è¯„ä¼°ï¼Œé¿å…äº†æ˜‚è´µçš„é‡æ–°å¯åŠ¨ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å®ç”¨æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€æ­¥é•¿å’Œæƒ©ç½šå‚æ•°ï¼Œè¯æ˜äº†SMADMMåœ¨ä¸éœ€è¦å¤§é‡åˆå§‹æ‰¹æ¬¡çš„æƒ…å†µä¸‹ï¼Œä»èƒ½ç»´æŒå…¶æœ€ä¼˜å¤æ‚æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡é›†æˆå³æ’å³ç”¨å…ˆéªŒï¼ˆplug-and-play priorsï¼‰å¼€å‘äº†PnP-SMADMMï¼Œå¹¶åœ¨æ¸©å’Œå‡è®¾ä¸‹å»ºç«‹äº†å…¶ç†è®ºæ”¶æ•›æ€§ã€‚åœ¨åˆ†ç±»ã€CTå›¾åƒé‡å»ºå’Œç›¸ä½æ£€ç´¢ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„éšæœºADMMæ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08223v2">PDF</a> 27 Pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSMADMMçš„å•å¾ªç¯éšæœºåŠ¨é‡äº¤æ›¿æ–¹å‘æ³•ï¼Œç”¨äºè§£å†³ä¸€ç±»éå‡¸å’Œéå…‰æ»‘å¤åˆä¼˜åŒ–é—®é¢˜ã€‚SMADMMåœ¨çº¿ç¯å¢ƒä¸­å®ç°äº†æœ€ä¼˜çš„$\mathcal{O}(\epsilon^{-3&#x2F;2})$å¤æ‚åº¦ã€‚ä¸éœ€è¦å¤§å‹è¿·ä½ æ‰¹æ¬¡æˆ–åŒé‡å¾ªç¯ç»“æ„çš„å…ˆå‰éšæœºADMMç®—æ³•ä¸åŒï¼ŒSMADMMæ¯æ¬¡è¿­ä»£ä»…ä½¿ç”¨$\mathcal{O}(1)$éšæœºæ¢¯åº¦è¯„ä¼°ï¼Œé¿å…äº†æ˜‚è´µçš„é‡æ–°å¯åŠ¨ã€‚é€šè¿‡èå…¥åŠ¨æ€æ­¥é•¿å’Œæƒ©ç½šå‚æ•°ï¼Œæˆ‘ä»¬è¯æ˜SMADMMåœ¨ä¸éœ€è¦å¤§å‹åˆå§‹æ‰¹æ¬¡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒå…¶æœ€ä¼˜å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†PnP-SMADMMï¼Œé€šè¿‡æ•´åˆå³æ’å³ç”¨å…ˆéªŒï¼Œå¹¶åœ¨æ¸©å’Œçš„å‡è®¾ä¸‹å»ºç«‹äº†å…¶ç†è®ºæ”¶æ•›æ€§ã€‚åœ¨åˆ†ç±»ã€CTå›¾åƒé‡å»ºå’Œç›¸ä½æ£€ç´¢ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šéƒ½ä¼˜äºç°æœ‰çš„éšæœºADMMæ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMADMMæ˜¯ä¸€ç§ç”¨äºè§£å†³éå‡¸å’Œéå…‰æ»‘å¤åˆä¼˜åŒ–é—®é¢˜çš„å•å¾ªç¯éšæœºåŠ¨é‡äº¤æ›¿æ–¹å‘æ³•ã€‚</li>
<li>SMADMMå®ç°äº†åœ¨çº¿ç¯å¢ƒä¸­çš„æœ€ä¼˜$\mathcal{O}(\epsilon^{-3&#x2F;2})$å¤æ‚åº¦ã€‚</li>
<li>ä¸å…¶ä»–éšæœºADMMç®—æ³•ä¸åŒï¼ŒSMADMMæ¯æ¬¡è¿­ä»£ä»…ä½¿ç”¨$\mathcal{O}(1)$éšæœºæ¢¯åº¦è¯„ä¼°ï¼Œé¿å…äº†é‡å¯æˆæœ¬ã€‚</li>
<li>SMADMMé€šè¿‡èå…¥åŠ¨æ€æ­¥é•¿å’Œæƒ©ç½šå‚æ•°ï¼Œåœ¨ä¸éœ€è¦å¤§å‹åˆå§‹æ‰¹æ¬¡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒæœ€ä¼˜å¤æ‚åº¦ã€‚</li>
<li>å¼€å‘äº†PnP-SMADMMï¼Œç»“åˆäº†å³æ’å³ç”¨å…ˆéªŒï¼Œå¢å¼ºäº†å®ç”¨æ€§ã€‚</li>
<li>PnP-SMADMMåœ¨åˆ†ç±»ã€CTå›¾åƒé‡å»ºå’Œç›¸ä½æ£€ç´¢ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç†è®ºå’Œå®é™…è¡¨ç°å‡ä¼˜äºç°æœ‰éšæœºADMMæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5098b857af72ca258c87358e7721af1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0434a180cee6a2bd4d09ecca26d885f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MedM-VL-What-Makes-a-Good-Medical-LVLM"><a href="#MedM-VL-What-Makes-a-Good-Medical-LVLM" class="headerlink" title="MedM-VL: What Makes a Good Medical LVLM?"></a>MedM-VL: What Makes a Good Medical LVLM?</h2><p><strong>Authors:Yiming Shi, Shaoshuai Yang, Xun Zhu, Haoyu Wang, Miao Li, Ji Wu</strong></p>
<p>Medical image analysis is essential in modern healthcare. Deep learning has redirected research focus toward complex medical multimodal tasks, including report generation and visual question answering. Traditional task-specific models often fall short in handling these challenges. Large vision-language models (LVLMs) offer new solutions for solving such tasks. In this study, we build on the popular LLaVA framework to systematically explore model architectures and training strategies for both 2D and 3D medical LVLMs. We present extensive empirical findings and practical guidance. To support reproducibility and future research, we release a modular codebase, MedM-VL, and two pre-trained models: MedM-VL-2D for 2D medical image analysis and MedM-VL-CT-Chest for 3D CT-based applications. The code and models are available at: <a target="_blank" rel="noopener" href="https://github.com/MSIIP/MedM-VL">https://github.com/MSIIP/MedM-VL</a> </p>
<blockquote>
<p>ç°ä»£åŒ»å­¦ä¸­ï¼ŒåŒ»å­¦å›¾åƒåˆ†ææ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚æ·±åº¦å­¦ä¹ å°†ç ”ç©¶ç„¦ç‚¹è½¬å‘äº†å¤æ‚çš„åŒ»å­¦å¤šæ¨¡æ€ä»»åŠ¡ï¼ŒåŒ…æ‹¬æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ã€‚ä¼ ç»Ÿçš„é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶å¾€å¾€åŠ›ä¸ä»å¿ƒã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸ºè§£å†³æ­¤ç±»ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»¥æµè¡Œçš„LLaVAæ¡†æ¶ä¸ºåŸºç¡€ï¼Œç³»ç»Ÿåœ°æ¢ç´¢äº†ç”¨äºäºŒç»´å’Œä¸‰ç»´åŒ»å­¦LVLMsçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚æˆ‘ä»¬æä¾›äº†ä¸°å¯Œçš„å®è¯ç»“æœå’Œå®è·µæŒ‡å¯¼ã€‚ä¸ºäº†æ”¯æŒå¯å¤åˆ¶æ€§å’Œæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªæ¨¡å—åŒ–ä»£ç åº“MedM-VLå’Œä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼šç”¨äºäºŒç»´åŒ»å­¦å›¾åƒåˆ†æçš„MedM-VL-2Då’Œç”¨äºåŸºäºä¸‰ç»´CTåº”ç”¨çš„MedM-VL-CT-Chestã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MSIIP/MedM-VL%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MSIIP/MedM-VLè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04323v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦å›¾åƒåˆ†æåœ¨ç°ä»£åŒ»ç–—ä¸­çš„é‡è¦æ€§ï¼Œæ·±åº¦å­¦ä¹ åœ¨è§£å†³å¤æ‚åŒ»å­¦å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ï¼‰æ–¹é¢çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„ä»»åŠ¡ç‰¹å®šæ¨¡å‹åœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜æ—¶å¸¸å¸¸ä¸è¶³ã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸ºè§£å†³è¿™äº›ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶åŸºäºæµè¡Œçš„LLaVAæ¡†æ¶ï¼Œç³»ç»Ÿåœ°æ¢ç´¢äº†äºŒç»´å’Œä¸‰ç»´åŒ»å­¦LVLMçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œå¹¶æä¾›ä¸°å¯Œçš„å®è¯ç ”ç©¶å’Œå®ç”¨æŒ‡å¯¼ã€‚ä¸ºæ”¯æŒå¤ç°å’Œæœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ¨¡å—åŒ–ä»£ç åº“MedM-VLå’Œä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼šç”¨äºäºŒç»´åŒ»å­¦å›¾åƒåˆ†æçš„MedM-VL-2Då’Œç”¨äºä¸‰ç»´CTåº”ç”¨çš„MedM-VL-CT-Chestã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æåœ¨ç°ä»£åŒ»ç–—ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨è§£å†³å¤æ‚åŒ»å­¦å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>ä¼ ç»Ÿä»»åŠ¡ç‰¹å®šæ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚</li>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸ºè§£å†³åŒ»å­¦å¤šæ¨¡æ€ä»»åŠ¡æä¾›æ–°æ–¹æ¡ˆã€‚</li>
<li>åŸºäºLLaVAæ¡†æ¶æ¢ç´¢äº†äºŒç»´å’Œä¸‰ç»´åŒ»å­¦LVLMçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚</li>
<li>æä¾›äº†ä¸°å¯Œçš„å®è¯ç ”ç©¶å’Œå®ç”¨æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32f0d58af5694d5a3ac4d23240d9e7fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63dac66ae36b34b978268f55f87c35ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db9bd0d74f62d09140de88060ea4062f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8825d219ba0dc05386ea28a0e8960ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9394e55c7a6a82a355abf9f9f5ba4b4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-066733183a0212a484260e07652eb2aa.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Radio-pulse-search-from-Aql-X-1"><a href="#Radio-pulse-search-from-Aql-X-1" class="headerlink" title="Radio pulse search from Aql X-1"></a>Radio pulse search from Aql X-1</h2><p><strong>Authors:Long Peng, Zhaosheng Li, Yuanyue Pan, Shanshan Weng, Wengming Yan, Na Wang, Bojun Wang, Shuangqiang Wang</strong></p>
<p>We present 12 observations of the accreting millisecond X-ray pulsar Aql X-1, taken from August 2022 to October 2023 using the Five-hundred-meter Aperture Spherical Radio Telescope at 1250 MHz. These observations covered both the quiescence and X-ray outburst states, as determined by analyzing the X-ray data from the Neutron Star Interior Composition Explorer and the Monitor of All-sky X-ray Image. Periodicity and single-pulse searches were conducted for each observation, but no pulsed signals were detected. The obtained upper limit flux densities are in the range of 2.86-5.73 uJy, which provide the lowest limits to date. We discuss several mechanisms that may prevent detection, suggesting that Aql X-1 may be in the radio-ejection state during quiescence, where the radio pulsed emissions are absorbed by the matter surrounding the system. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨å£å¾„ä¸ºäº”ç™¾ç±³çƒé¢å°„ç”µæœ›è¿œé•œåœ¨1250å…†èµ«ä¸‹ä»2022å¹´8æœˆåˆ°2023å¹´10æœˆå¯¹å¢äº®å‹æ¯«ç§’Xå°„çº¿è„‰å†²æ˜ŸAql X-1è¿›è¡Œçš„12æ¬¡è§‚æµ‹ç»“æœã€‚è¿™äº›è§‚æµ‹æ¶µç›–äº†é™æ­¢æœŸå’ŒXå°„çº¿çˆ†å‘çŠ¶æ€ï¼Œè¿™æ˜¯é€šè¿‡åˆ†ææ¥è‡ªä¸­å­æ˜Ÿå†…éƒ¨ç»“æ„æ¢æµ‹å™¨å’Œå…¨å¤©ç©ºXå°„çº¿å›¾åƒç›‘æµ‹å™¨çš„Xå°„çº¿æ•°æ®æ¥ç¡®å®šçš„ã€‚æˆ‘ä»¬å¯¹æ¯æ¬¡è§‚æµ‹è¿›è¡Œäº†å‘¨æœŸæ€§å’Œå•è„‰å†²æœç´¢ï¼Œä½†æ²¡æœ‰æ£€æµ‹åˆ°è„‰å†²ä¿¡å·ã€‚æ‰€è·å¾—çš„æµé‡å¯†åº¦ä¸Šé™èŒƒå›´ä¸º2.86-5.73å¾®ç„¦è€³&#x2F;å¹³æ–¹åˆ†ç±³ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢çš„æœ€ä½é™åº¦ã€‚æˆ‘ä»¬è®¨è®ºäº†å¯èƒ½é˜»æ­¢æ¢æµ‹çš„å‡ ç§æœºåˆ¶ï¼Œå¹¶æå‡ºAql X-1å¯èƒ½åœ¨é™æ­¢æœŸå¤„äºå°„ç”µå–·å°„çŠ¶æ€ï¼Œå°„ç”µè„‰å†²å‘å°„è¢«ç³»ç»Ÿå‘¨å›´çš„ç‰©è´¨å¸æ”¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05237v3">PDF</a> 8 pages, 2 figures, submitted to ApJ on December 10, 2024; accepted   for publication on March 4, 2025; published on April 3, 2025</p>
<p><strong>Summary</strong></p>
<pre><code> åˆ©ç”¨500ç±³å£å¾„çƒé¢æœ›è¿œé•œåœ¨1250å…†èµ«çš„é¢‘ç‡ä¸‹ï¼Œæˆ‘ä»¬å¯¹æ¸å¢å‹æ¯«ç§’Xå°„çº¿è„‰å†²æ˜ŸAql X-1è¿›è¡Œäº†ä¸ºæœŸä¸€å¹´åŠçš„è§‚å¯Ÿã€‚åˆ†æç»“æœæ¶µç›–å…¶é™æ­¢å’ŒXå°„çº¿çˆ†å‘çŠ¶æ€ï¼Œä½†æœªæ£€æµ‹åˆ°è„‰å†²ä¿¡å·ã€‚è·å¾—çš„ä¸Šé™æµé‡å¯†åº¦èŒƒå›´åœ¨2.86-5.73å¾®ç„¦è€³ä¹‹é—´ï¼Œä¸ºç›®å‰æœ€ä½å€¼ã€‚è®¨è®ºäº†å¯èƒ½é˜»ç¢æ£€æµ‹çš„å‡ ä¸ªæœºåˆ¶ï¼Œæ¨æµ‹Aql X-1åœ¨é™æ­¢æœŸå¯èƒ½å¤„äºæ— çº¿ç”µå–·å°„çŠ¶æ€ï¼Œæ— çº¿ç”µè„‰å†²å‘å°„è¢«ç³»ç»Ÿå‘¨å›´çš„ç‰©è´¨å¸æ”¶ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨500ç±³å£å¾„çƒé¢æœ›è¿œé•œè¿›è¡Œäº†Aql X-1çš„è§‚å¯Ÿã€‚</li>
<li>è§‚å¯Ÿé¢‘ç‡æ˜¯1250å…†èµ«ï¼Œæ—¶é—´ä»2022å¹´8æœˆåˆ°2023å¹´10æœˆã€‚</li>
<li>åˆ†æäº†Aql X-1çš„é™æ­¢å’ŒXå°„çº¿çˆ†å‘çŠ¶æ€ã€‚</li>
<li>æœªæ£€æµ‹åˆ°è„‰å†²ä¿¡å·ã€‚</li>
<li>è·å¾—çš„ä¸Šé™æµé‡å¯†åº¦èŒƒå›´åœ¨2.86-5.73å¾®ç„¦è€³ä¹‹é—´ï¼Œä¸ºç›®å‰æœ€ä½å€¼ã€‚</li>
<li>è®¨è®ºäº†å¯èƒ½é˜»ç¢æ£€æµ‹çš„å‡ ä¸ªæœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f1ee26b187c55cdf4028c9d23bb58ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6729564f6911c0cbf36f33360165b2f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81a363c50f9859856d9d3bcd2ddb7bfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9d18fdc4834a7689ad6fb01ff3d6b24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48362c9587d5524e34f2a2639df9729d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ClinKD-Cross-Modal-Clinical-Knowledge-Distiller-For-Multi-Task-Medical-Images"><a href="#ClinKD-Cross-Modal-Clinical-Knowledge-Distiller-For-Multi-Task-Medical-Images" class="headerlink" title="ClinKD: Cross-Modal Clinical Knowledge Distiller For Multi-Task Medical   Images"></a>ClinKD: Cross-Modal Clinical Knowledge Distiller For Multi-Task Medical   Images</h2><p><strong>Authors:Hongyu Ge, Longkun Hao, Zihui Xu, Zhenxin Lin, Bin Li, Shoujun Zhou, Hongjin Zhao, Yihang Liu</strong></p>
<p>Medical Visual Question Answering (Med-VQA) represents a critical and challenging subtask within the general VQA domain. Despite significant advancements in general Visual Question Answering (VQA), multimodal large language models (MLLMs) still exhibit substantial limitations when handling multi-task VQA scenarios. These limitations manifest through erroneous spatial localization and misinterpretation of medical images, which primarily arise from two fundamental issues: inadequate image-text alignment and insufficient medical knowledge in general-purpose MLLMs for specialized medical applications. To address these issues, we introduce the Cross-Modal Clinical Knowledge Distiller (ClinKD), an innovative framework designed to enhance image-text alignment and establish more effective medical knowledge adaptation mechanisms, which enables MLLMs to adapt to medical knowledge. Our extensive experimental evaluations demonstrate that the ClinKD achieves state-of-the-art performance on the Med-GRIT-270k dataset, a challenging medical benchmark containing fine-grained multi-task QA pairs. The results indicate that our approach not only significantly improves image-text alignment but also effectively enables MLLMs to adapt to the medical knowledge. The source code for ClinKD is available at: <a target="_blank" rel="noopener" href="https://github.com/overloadedHenry/ClinKD">https://github.com/overloadedHenry/ClinKD</a>. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰æ˜¯é€šç”¨VQAé¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„å­ä»»åŠ¡ã€‚å°½ç®¡é€šç”¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤šä»»åŠ¡VQAåœºæ™¯æ—¶ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»ç„¶è¡¨ç°å‡ºæ˜æ˜¾çš„å±€é™æ€§ã€‚è¿™äº›å±€é™æ€§è¡¨ç°ä¸ºåŒ»ç–—å›¾åƒçš„ç©ºé—´å®šä½é”™è¯¯å’Œè¯¯è§£ï¼Œä¸»è¦æºäºä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šå›¾åƒæ–‡æœ¬å¯¹é½ä¸è¶³ä»¥åŠé€šç”¨MLLMsåœ¨ä¸“ç”¨åŒ»ç–—åº”ç”¨ä¸­åŒ»å­¦çŸ¥è¯†çš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ¨¡æ€ä¸´åºŠçŸ¥è¯†è’¸é¦å™¨ï¼ˆClinKDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå›¾åƒæ–‡æœ¬å¯¹é½å¹¶å»ºç«‹æ›´æœ‰æ•ˆçš„åŒ»å­¦çŸ¥è¯†é€‚åº”æœºåˆ¶ï¼Œä½¿MLLMsèƒ½å¤Ÿé€‚åº”åŒ»å­¦çŸ¥è¯†ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒClinKDåœ¨Med-GRIT-270kæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ç²¾ç»†ç²’åº¦å¤šä»»åŠ¡é—®ç­”å¯¹çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æ˜¾è‘—æ”¹å–„äº†å›¾åƒæ–‡æœ¬å¯¹é½ï¼Œè€Œä¸”æœ‰æ•ˆåœ°ä½¿MLLMsé€‚åº”äº†åŒ»å­¦çŸ¥è¯†ã€‚ClinKDçš„æºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/overloadedHenry/ClinKD%E3%80%82">https://github.com/overloadedHenry/ClinKDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05928v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰çš„é‡è¦æ€§åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹å¤šä»»åŠ¡è§†è§‰é—®ç­”åœºæ™¯ä¸­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­˜åœ¨çš„ç©ºé—´å®šä½é”™è¯¯å’Œå¯¹åŒ»ç–—å›¾åƒè¯¯è§£çš„é—®é¢˜ï¼Œæå‡ºäº†è·¨æ¨¡æ€ä¸´åºŠçŸ¥è¯†è’¸é¦å™¨ï¼ˆClinKDï¼‰æ¡†æ¶ã€‚ClinKDé€šè¿‡å¢å¼ºå›¾åƒæ–‡æœ¬å¯¹é½å’Œå»ºç«‹æœ‰æ•ˆçš„åŒ»å­¦çŸ¥è¯†é€‚åº”æœºåˆ¶ï¼Œæé«˜äº†MLLMsåœ¨åŒ»ç–—çŸ¥è¯†æ–¹é¢çš„é€‚åº”èƒ½åŠ›ã€‚åœ¨Med-GRIT-270kæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒClinKDå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-VQAæ˜¯VQAé¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦ä¸”å…·æŒ‘æˆ˜æ€§çš„å­ä»»åŠ¡ã€‚</li>
<li>MLLMsåœ¨å¤„ç†å¤šä»»åŠ¡VQAåœºæ™¯æ—¶å­˜åœ¨ç©ºé—´å®šä½é”™è¯¯å’ŒåŒ»ç–—å›¾åƒè¯¯è§£çš„é—®é¢˜ã€‚</li>
<li>è¿™äº›é—®é¢˜ä¸»è¦æºäºå›¾åƒæ–‡æœ¬å¯¹é½ä¸è¶³å’Œé€šç”¨MLLMsåœ¨ä¸“é—¨åŒ»ç–—åº”ç”¨ä¸­çš„åŒ»å­¦çŸ¥è¯†ä¸è¶³ã€‚</li>
<li>è·¨æ¨¡æ€ä¸´åºŠçŸ¥è¯†è’¸é¦å™¨ï¼ˆClinKDï¼‰æ¡†æ¶æ—¨åœ¨å¢å¼ºå›¾åƒæ–‡æœ¬å¯¹é½å¹¶å»ºç«‹æœ‰æ•ˆçš„åŒ»å­¦çŸ¥è¯†é€‚åº”æœºåˆ¶ã€‚</li>
<li>ClinKDèƒ½æ˜¾è‘—æé«˜MLLMså¯¹åŒ»ç–—çŸ¥è¯†çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>åœ¨Med-GRIT-270kæ•°æ®é›†ä¸Šï¼ŒClinKDå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d932de573470d459873798a4b7e5de6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30e62336f4e1741a8fb1d039604cad91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5aa62887cb209ab8836ec9e1fef8c5e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2b517f25680d4808966966339d2bf2d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Modeling-fast-X-ray-variability-around-an-accreting-black-hole"><a href="#Modeling-fast-X-ray-variability-around-an-accreting-black-hole" class="headerlink" title="Modeling fast X-ray variability around an accreting black hole"></a>Modeling fast X-ray variability around an accreting black hole</h2><p><strong>Authors:Yejing Zhan, Bei You, Adam Ingram, Wenkang Jiang, Fayin Wang</strong></p>
<p>X-ray inter-band time lags are observed during the outbursts of black hole X-ray binaries (BHXRBs). Timing analysis of fast variability in low Fourier frequency bands shows that high-energy photons lag behind low-energy photons, a phenomenon referred to as hard lag. Conversely, in high Fourier frequency bands, low-energy photons lag behind high-energy photons, known as soft lag. This frequency-dependent lag spectrum suggests that the lags arise from different physical processes. Notably, a trend has been observed wherein the lags shift towards shorter timescales during the rising hard state, indicating an evolution in the inner accretion flow. In this study, we simulate these inter-band lags by conducting Monte Carlo simulations of the rapid variability within the geometry of a jet base corona. We consider both inward propagating accretion rate fluctuations and reverberation (light crossing) delays in our simulations. We successfully reproduce both low-frequency hard lags and high-frequency soft lags in a self-consistent manner. We replicate the observed evolution of the frequency-dependent lag spectra by varying the geometrical scale of the corona and the viscous frequency of the disc. Finally, we discuss the potential of a spherical corona and emphasize that polarization observations from the Imaging X-ray Polarimetry Explorer (IXPE) and the enhanced X-ray Timing and Polarimetry mission (eXTP) will be crucial for distinguishing the coronaâ€™s geometry in future studies. </p>
<blockquote>
<p>åœ¨é»‘æ´Xå°„çº¿åŒæ˜Ÿï¼ˆBHXRBsï¼‰çš„çˆ†å‘æœŸé—´ï¼Œè§‚å¯Ÿåˆ°Xå°„çº¿æ³¢æ®µä¹‹é—´çš„æ—¶é—´å»¶è¿Ÿã€‚ä½å‚…é‡Œå¶é¢‘å¸¦å¿«é€Ÿå˜åŒ–çš„å®šæ—¶åˆ†æè¡¨æ˜ï¼Œé«˜èƒ½å…‰å­è½åäºä½èƒ½å…‰å­ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºç¡¬æ»åã€‚ç›¸åï¼Œåœ¨é«˜å‚…é‡Œå¶é¢‘å¸¦ä¸­ï¼Œä½èƒ½å…‰å­è½åäºé«˜èƒ½å…‰å­ï¼Œç§°ä¸ºè½¯æ»åã€‚è¿™ç§ä¸é¢‘ç‡ç›¸å…³çš„æ»åå…‰è°±è¡¨æ˜æ»åæ˜¯ç”±ä¸åŒçš„ç‰©ç†è¿‡ç¨‹å¼•èµ·çš„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå·²ç»è§‚å¯Ÿåˆ°ä¸€ç§è¶‹åŠ¿ï¼Œå³æ»åæ—¶é—´å‘æ›´çŸ­çš„æ—¶é—´å°ºåº¦è½¬å˜ï¼Œè¿™åœ¨å¢å¼ºçš„ç¡¬æ€ä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œè¡¨æ˜å†…ç§¯ç›˜åœ¨å‘ç”Ÿå˜åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿå–·æµåŸºå†•å‡ ä½•ç»“æ„å†…çš„å¿«é€Ÿå˜åŒ–æ¥è¿›è¡Œè’™ç‰¹å¡ç½—æ¨¡æ‹Ÿï¼Œä»¥æ¨¡æ‹Ÿè¿™äº›æ³¢æ®µé—´çš„æ»åç°è±¡ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿä¸­è€ƒè™‘äº†å‘å†…ä¼ æ’­çš„å¸ç§¯ç‡æ³¢åŠ¨å’Œå›å£°ï¼ˆå…‰é€Ÿç©¿è¶Šï¼‰å»¶è¿Ÿã€‚æˆ‘ä»¬æˆåŠŸåœ°ä»¥ä¸€è‡´çš„æ–¹å¼å†ç°äº†ä½é¢‘ç¡¬æ»åå’Œé«˜é¢‘è½¯æ»åã€‚é€šè¿‡æ”¹å˜å†•çš„å‡ ä½•å°ºåº¦å’Œç›˜çš„ç²˜æ€§é¢‘ç‡ï¼Œæˆ‘ä»¬å¤åˆ¶äº†è§‚å¯Ÿåˆ°çš„é¢‘ç‡ç›¸å…³æ»åå…‰è°±çš„æ¼”å˜ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†çƒå½¢å†•çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒæˆåƒXå°„çº¿åæŒ¯ä»ªï¼ˆIXPEï¼‰å’Œå¢å¼ºçš„Xå°„çº¿å®šæ—¶å’ŒåæŒ¯ä»»åŠ¡ï¼ˆeXTPï¼‰çš„åæŒ¯è§‚æµ‹å¯¹äºæœªæ¥ç ”ç©¶ä¸­åŒºåˆ†å†•çš„å‡ ä½•å½¢çŠ¶å°†æ˜¯è‡³å…³é‡è¦çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03995v2">PDF</a> 17 pages, 9 figures, accepted by ApJ</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é»‘æ´Xå°„çº¿åŒæ˜Ÿï¼ˆBHXRBsï¼‰çˆ†å‘æœŸé—´çš„Xå°„çº¿æ³¢æ®µé—´æ—¶é—´å»¶è¿Ÿç°è±¡ã€‚é€šè¿‡è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼ŒæˆåŠŸæ¨¡æ‹Ÿäº†ä½é¢‘ç¡¬æ»åå’Œé«˜é¢‘è½¯æ»åçš„äº§ç”Ÿæœºåˆ¶ï¼Œæ­ç¤ºäº†å»¶è¿Ÿæ˜¯ç”±äºå† å±‚å‡ ä½•ç»“æ„å˜åŒ–å¼•èµ·çš„å‘å†…ä¼ æ’­çš„å¸ç§¯ç‡æ³¢åŠ¨å’Œå›å£°å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è®¨è®ºäº†çƒå½¢å† å±‚çš„å¯èƒ½æ€§ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥ç ”ç©¶ä¸­æåŒ–è§‚æµ‹å¯¹äºåŒºåˆ†å† å±‚å‡ ä½•ç»“æ„çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BHXRBsçˆ†å‘æœŸé—´å­˜åœ¨Xå°„çº¿æ³¢æ®µé—´æ—¶é—´å»¶è¿Ÿç°è±¡ã€‚</li>
<li>é«˜èƒ½å…‰å­åœ¨ä½é¢‘å¸¦çš„æ—¶é—´æ»åäºä½èƒ½å…‰å­ï¼Œäº§ç”Ÿç¡¬æ»åç°è±¡ã€‚</li>
<li>ä½èƒ½å…‰å­åœ¨é«˜é¢‘å¸¦çš„æ—¶é—´æ»åäºé«˜èƒ½å…‰å­ï¼Œäº§ç”Ÿè½¯æ»åç°è±¡ã€‚</li>
<li>æ—¶é—´æ»åç°è±¡åœ¨ä¸åŒé¢‘ç‡ä¸‹çš„è¡¨ç°æ­ç¤ºå‡ºä¸åŒçš„ç‰©ç†è¿‡ç¨‹ã€‚</li>
<li>è’™ç‰¹å¡æ´›æ¨¡æ‹ŸæˆåŠŸæ¨¡æ‹Ÿäº†è¿™ç§æ³¢æ®µé—´çš„æ—¶é—´æ»åç°è±¡ï¼Œä¸è§‚å¯Ÿç»“æœç›¸ç¬¦ã€‚</li>
<li>å»¶è¿Ÿç°è±¡çš„å˜åŒ–ä¸å† å±‚å‡ ä½•ç»“æ„å’Œå¸ç§¯ç›˜çš„ç²˜æ€§é¢‘ç‡æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-034295847653fcd24c76fad6918bf32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c49ddfd64f92033c063a0a0a8cdeff4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="PK-YOLO-Pretrained-Knowledge-Guided-YOLO-for-Brain-Tumor-Detection-in-Multiplanar-MRI-Slices"><a href="#PK-YOLO-Pretrained-Knowledge-Guided-YOLO-for-Brain-Tumor-Detection-in-Multiplanar-MRI-Slices" class="headerlink" title="PK-YOLO: Pretrained Knowledge Guided YOLO for Brain Tumor Detection in   Multiplanar MRI Slices"></a>PK-YOLO: Pretrained Knowledge Guided YOLO for Brain Tumor Detection in   Multiplanar MRI Slices</h2><p><strong>Authors:Ming Kang, Fung Fung Ting, RaphaÃ«l C. -W. Phan, Chee-Ming Ting</strong></p>
<p>Brain tumor detection in multiplane Magnetic Resonance Imaging (MRI) slices is a challenging task due to the various appearances and relationships in the structure of the multiplane images. In this paper, we propose a new You Only Look Once (YOLO)-based detection model that incorporates Pretrained Knowledge (PK), called PK-YOLO, to improve the performance for brain tumor detection in multiplane MRI slices. To our best knowledge, PK-YOLO is the first pretrained knowledge guided YOLO-based object detector. The main components of the new method are a pretrained pure lightweight convolutional neural network-based backbone via sparse masked modeling, a YOLO architecture with the pretrained backbone, and a regression loss function for improving small object detection. The pretrained backbone allows for feature transferability of object queries on individual plane MRI slices into the model encoders, and the learned domain knowledge base can improve in-domain detection. The improved loss function can further boost detection performance on small-size brain tumors in multiplanar two-dimensional MRI slices. Experimental results show that the proposed PK-YOLO achieves competitive performance on the multiplanar MRI brain tumor detection datasets compared to state-of-the-art YOLO-like and DETR-like object detectors. The code is available at <a target="_blank" rel="noopener" href="https://github.com/mkang315/PK-YOLO">https://github.com/mkang315/PK-YOLO</a>. </p>
<blockquote>
<p>åœ¨å¤šå¹³é¢ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ‡ç‰‡ä¸­æ£€æµ‹è„‘è‚¿ç˜¤æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç”±äºå¤šå¹³é¢å›¾åƒçš„ç»“æ„ä¸­çš„ä¸åŒå¤–è§‚å’Œå…³ç³»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºYou Only Look Onceï¼ˆYOLOï¼‰çš„æ£€æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†é¢„è®­ç»ƒçŸ¥è¯†ï¼ˆPKï¼‰ï¼Œç§°ä¸ºPK-YOLOï¼Œä»¥æé«˜åœ¨å¤šå¹³é¢MRIåˆ‡ç‰‡ä¸­æ£€æµ‹è„‘è‚¿ç˜¤çš„æ€§èƒ½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒPK-YOLOæ˜¯ç¬¬ä¸€ä¸ªåŸºäºé¢„è®­ç»ƒçŸ¥è¯†çš„YOLOç›®æ ‡æ£€æµ‹å™¨ã€‚æ–°æ–¹æ³•çš„ ä¸»è¦ç»„ä»¶åŒ…æ‹¬é€šè¿‡ç¨€ç–æ©æ¨¡å»ºæ¨¡çš„é¢„è®­ç»ƒçº¯è½»é‡åŒ–å·ç§¯ç¥ç»ç½‘ç»œä¸»å¹²ã€å¸¦æœ‰é¢„è®­ç»ƒä¸»å¹²çš„YOLOæ¶æ„ï¼Œä»¥åŠç”¨äºæ”¹è¿›å°ç›®æ ‡æ£€æµ‹çš„å›å½’æŸå¤±å‡½æ•°ã€‚é¢„è®­ç»ƒçš„ä¸»å¹²å…è®¸å°†å•ä¸ªå¹³é¢MRIåˆ‡ç‰‡ä¸Šçš„ç›®æ ‡æŸ¥è¯¢çš„ç‰¹å¾å¯è¿ç§»æ€§è½¬ç§»åˆ°æ¨¡å‹ç¼–ç å™¨ï¼Œå¹¶ä¸”å­¦ä¹ åˆ°çš„é¢†åŸŸçŸ¥è¯†åº“å¯ä»¥æé«˜é¢†åŸŸå†…çš„æ£€æµ‹æ€§èƒ½ã€‚æ”¹è¿›çš„æŸå¤±å‡½æ•°å¯ä»¥è¿›ä¸€æ­¥æé«˜åœ¨å¤šå¹³é¢äºŒç»´MRIåˆ‡ç‰‡ä¸­å°å°ºå¯¸è„‘è‚¿ç˜¤çš„æ£€æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ç±»ä¼¼YOLOå’ŒDETRçš„ç›®æ ‡æ£€æµ‹å™¨ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„PK-YOLOåœ¨å¤šå¹³é¢MRIè„‘è‚¿ç˜¤æ£€æµ‹æ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mkang315/PK-YOLO%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mkang315/PK-YOLOä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21822v2">PDF</a> References updated; for example, papers in NeurIPS 2024 proceedings   appeared on 6 Feb 2025 and AAAI 2025 one on 11 Apr 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šå¹³é¢ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„è„‘è‚¿ç˜¤æ£€æµ‹å› å›¾åƒç»“æ„çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹çš„åŸºäºYou Only Look Onceï¼ˆYOLOï¼‰çš„æ£€æµ‹æ¨¡å‹ï¼Œç»“åˆé¢„è®­ç»ƒçŸ¥è¯†ï¼ˆPKï¼‰ï¼Œç§°ä¸ºPK-YOLOï¼Œä»¥æé«˜åœ¨å¤šå¹³é¢MRIåˆ‡ç‰‡ä¸­æ£€æµ‹è„‘è‚¿ç˜¤çš„æ€§èƒ½ã€‚PK-YOLOæ˜¯é¦–ä¸ªåŸºäºé¢„è®­ç»ƒçŸ¥è¯†çš„YOLOç›®æ ‡æ£€æµ‹å™¨ã€‚è¯¥æ–¹æ³•ä¸»è¦åŒ…æ‹¬é€šè¿‡ç¨€ç–æ©æ¨¡å»ºæ¨¡çš„é¢„è®­ç»ƒè½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œä¸»å¹²ã€å¸¦æœ‰é¢„è®­ç»ƒä¸»å¹²çš„YOLOæ¶æ„ï¼Œä»¥åŠç”¨äºæ”¹è¿›å°ç›®æ ‡æ£€æµ‹çš„å›å½’æŸå¤±å‡½æ•°ã€‚é¢„è®­ç»ƒçš„ä¸»å¹²å…è®¸å°†å•ä¸ªå¹³é¢MRIåˆ‡ç‰‡ä¸Šçš„ç›®æ ‡æŸ¥è¯¢çš„ç‰¹å¾è½¬ç§»åˆ°æ¨¡å‹ç¼–ç å™¨ï¼Œè€Œå­¦ä¹ åˆ°çš„é¢†åŸŸçŸ¥è¯†åº“å¯ä»¥æé«˜é¢†åŸŸå†…çš„æ£€æµ‹æ€§èƒ½ã€‚æ”¹è¿›çš„æŸå¤±å‡½æ•°å¯ä»¥è¿›ä¸€æ­¥æé«˜åœ¨å¤šå¹³é¢äºŒç»´MRIåˆ‡ç‰‡ä¸­æ£€æµ‹å°å°ºå¯¸è„‘è‚¿ç˜¤çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°çš„YOLOå’ŒDETRç±»ä¼¼çš„ç›®æ ‡æ£€æµ‹å™¨ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„PK-YOLOåœ¨å¤šå¹³é¢MRIè„‘è‚¿ç˜¤æ£€æµ‹æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PK-YOLOæ˜¯ä¸€ä¸ªç»“åˆé¢„è®­ç»ƒçŸ¥è¯†çš„YOLOç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œé€‚ç”¨äºå¤šå¹³é¢MRIåˆ‡ç‰‡ä¸­çš„è„‘è‚¿ç˜¤æ£€æµ‹ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨é¢„è®­ç»ƒçš„è½»é‡çº§å·ç§¯ç¥ç»ç½‘ç»œä¸»å¹²ï¼Œé€šè¿‡ç¨€ç–æ©æ¨¡å»ºæ¨¡å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>YOLOæ¶æ„ç»“åˆé¢„è®­ç»ƒä¸»å¹²ï¼Œèƒ½æœ‰æ•ˆå¤„ç†MRIåˆ‡ç‰‡ä¸­çš„å¤æ‚å›¾åƒç»“æ„ã€‚</li>
<li>å›å½’æŸå¤±å‡½æ•°çš„æ”¹è¿›æé«˜äº†å¯¹å°å°ºå¯¸è„‘è‚¿ç˜¤çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>é¢„è®­ç»ƒçš„ä¸»å¹²èƒ½å¤Ÿå®ç°ç‰¹å¾çš„å¯è½¬ç§»æ€§ï¼Œæé«˜æ£€æµ‹æ•ˆæœã€‚</li>
<li>PK-YOLOåœ¨å¤šå¹³é¢MRIè„‘è‚¿ç˜¤æ£€æµ‹æ•°æ®é›†ä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21822">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c1f9e85b1ecaf3daa2947f6265c91221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63cf796e80e8c8a1ae2cb7083b996fd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb972d5299f25cc2cbaf7d95fa067ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afe598334e81bf6a3bba91dc42f43588.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4ccc28111ab3ab5d32d984f6d302307.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Adaptive-Mix-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Adaptive-Mix-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Adaptive Mix for Semi-Supervised Medical Image Segmentation"></a>Adaptive Mix for Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Zhiqiang Shen, Peng Cao, Junming Su, Jinzhu Yang, Osmar R. Zaiane</strong></p>
<p>Mix-up is a key technique for consistency regularization-based semi-supervised learning methods, blending two or more images to generate strong-perturbed samples for strong-weak pseudo supervision. Existing mix-up operations are performed either randomly or with predefined fixed rules, such as replacing low-confidence patches with high-confidence ones. The former lacks control over the perturbation degree, leading to overfitting on randomly perturbed samples, while the latter tends to generate images with trivial perturbations, both of which limit the effectiveness of consistency regularization. This paper aims to answer the following question: How can image mix-up perturbation be adaptively performed during training? To this end, we propose an Adaptive Mix algorithm (AdaMix) for image mix-up in a self-paced learning manner. Given that, in general, a modelâ€™s performance gradually improves during training, AdaMix is equipped with a self-paced curriculum that, in the initial training stage, provides relatively simple perturbed samples and then gradually increases the difficulty of perturbed images by adaptively controlling the perturbation degree based on the modelâ€™s learning state estimated by a self-paced regularize. We develop three frameworks with our AdaMix, i.e., AdaMix-ST, AdaMix-MT, and AdaMix-CT, for semi-supervised medical image segmentation. Extensive experiments on three public datasets show that the proposed frameworks can achieve superior performance. For example, compared with the state-of-the-art, AdaMix-CT achieves relative improvements of 2.62% in Dice similarity coefficient and 48.25% in average surface distance on the ACDC dataset with 10% labeled data. The results demonstrate that mix-up operations with dynamically adjusted perturbation strength based on the segmentation modelâ€™s state can significantly enhance the effectiveness of consistency regularization. </p>
<blockquote>
<p>æ··åˆæ˜¯åŠç›‘ç£å­¦ä¹ æ–¹æ³•ä¸­çš„ä¸€ç§å…³é”®æŠ€æœ¯ï¼Œå®ƒç»“åˆäº†ä¸¤ç§æˆ–å¤šç§å›¾åƒæ¥ç”Ÿæˆå¼ºçƒˆå¹²æ‰°çš„æ ·æœ¬ä»¥è¿›è¡Œå¼ºå¼±ä¼ªç›‘ç£ã€‚ç°æœ‰çš„æ··åˆæ“ä½œæ˜¯éšæœºæˆ–æŒ‰é¢„å®šçš„å›ºå®šè§„åˆ™æ‰§è¡Œï¼Œä¾‹å¦‚ç”¨é«˜ç½®ä¿¡åº¦çš„è¡¥ä¸æ›¿æ¢ä½ç½®ä¿¡åº¦çš„è¡¥ä¸ã€‚å‰è€…ç¼ºä¹å¯¹å¹²æ‰°ç¨‹åº¦çš„æ§åˆ¶ï¼Œå¯¼è‡´éšæœºå¹²æ‰°æ ·æœ¬çš„è¿‡åº¦æ‹Ÿåˆï¼Œè€Œåè€…å€¾å‘äºç”Ÿæˆå…·æœ‰è½»å¾®å¹²æ‰°çš„å›¾åƒã€‚ä¸¤è€…éƒ½é™åˆ¶äº†ä¸€è‡´æ€§æ­£åˆ™åŒ–çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡æ—¨åœ¨å›ç­”ä»¥ä¸‹é—®é¢˜ï¼šå¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°æ‰§è¡Œå›¾åƒæ··åˆå¹²æ‰°ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ··åˆç®—æ³•ï¼ˆAdaMixï¼‰ç”¨äºä»¥è‡ªæˆ‘å®‰æ’çš„å­¦ä¹ æ–¹å¼è¿›è¡Œå›¾åƒæ··åˆã€‚é‰´äºæ¨¡å‹çš„æ€§èƒ½é€šå¸¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸æé«˜ï¼ŒAdaMixé…å¤‡äº†ä¸€ç§è‡ªæˆ‘å®‰æ’è¯¾ç¨‹ï¼Œåœ¨åˆå§‹è®­ç»ƒé˜¶æ®µæä¾›ç›¸å¯¹ç®€å•çš„å¹²æ‰°æ ·æœ¬ï¼Œç„¶åé€šè¿‡è‡ªé€‚åº”æ§åˆ¶å¹²æ‰°ç¨‹åº¦æ¥é€æ¸å¢åŠ å¹²æ‰°å›¾åƒçš„éš¾åº¦ã€‚å¹²æ‰°ç¨‹åº¦çš„æ§åˆ¶åŸºäºæ¨¡å‹çš„å­¦ä¹ çŠ¶æ€ï¼Œç”±è‡ªæˆ‘å®‰æ’çš„æ­£åˆ™åŒ–è¿›è¡Œä¼°ç®—ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸‰ç§ä½¿ç”¨AdaMixçš„æ¡†æ¶ï¼Œå³AdaMix-STã€AdaMix-MTå’ŒAdaMix-CTï¼Œç”¨äºåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶å¯ä»¥å–å¾—å“è¶Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒAdaMix-CTåœ¨åªæœ‰10%æ ‡è®°æ•°æ®çš„ACDCæ•°æ®é›†ä¸Šï¼Œåœ¨Diceç›¸ä¼¼ç³»æ•°ä¸Šå®ç°äº†ç›¸å¯¹æé«˜2.62%ï¼Œåœ¨å¹³å‡è¡¨é¢è·ç¦»ä¸Šå®ç°äº†ç›¸å¯¹æé«˜48.25%ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºåˆ†å‰²æ¨¡å‹çŠ¶æ€çš„åŠ¨æ€è°ƒæ•´æ··åˆæ“ä½œçš„å¹²æ‰°å¼ºåº¦å¯ä»¥æ˜¾ç€æé«˜ä¸€è‡´æ€§æ­£åˆ™åŒ–çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21586v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŒ»å­¦å›¾åƒåŠç›‘ç£å­¦ä¹ ä¸­çš„è‡ªé€‚åº”æ··åˆç®—æ³•ç ”ç©¶ã€‚é’ˆå¯¹ç°æœ‰æ··åˆæ“ä½œçš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºä¸€ç§è‡ªé€‚åº”æ··åˆç®—æ³•ï¼ˆAdaMixï¼‰ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨¡å‹çš„å­¦ä¹ çŠ¶æ€è‡ªé€‚åº”è°ƒæ•´æ‰°åŠ¨ç¨‹åº¦ã€‚å¼€å‘ä¸‰ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œå¹¶åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œæ˜¾ç¤ºå‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†åŒ»å­¦å›¾åƒåŠç›‘ç£å­¦ä¹ ä¸­æ··åˆæŠ€æœ¯çš„é‡è¦æ€§ã€‚</li>
<li>åˆ†æç°æœ‰æ··åˆæ“ä½œçš„å±€é™æ€§ï¼ŒåŒ…æ‹¬éšæœºæ“ä½œå¯¼è‡´çš„è¿‡åº¦æ‹Ÿåˆå’Œå›ºå®šè§„åˆ™äº§ç”Ÿçš„è½»å¾®æ‰°åŠ¨ã€‚</li>
<li>æå‡ºè‡ªé€‚åº”æ··åˆç®—æ³•ï¼ˆAdaMixï¼‰ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨¡å‹çš„å­¦ä¹ çŠ¶æ€åŠ¨æ€è°ƒæ•´æ‰°åŠ¨å¼ºåº¦ã€‚</li>
<li>AdaMixé…å¤‡è‡ªæˆ‘è¿›åº¦çš„è¯¾ç¨‹ï¼Œåœ¨è®­ç»ƒåˆæœŸæä¾›ç®€å•çš„æ‰°åŠ¨æ ·æœ¬ï¼Œç„¶åé€æ¸å¢åŠ æ‰°åŠ¨å›¾åƒçš„éš¾åº¦ã€‚</li>
<li>å¼€å‘ä¸‰ä¸ªåŸºäºAdaMixçš„æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.21586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-599d6e7f266fc9c85f6ca67da3918475.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e25b11c3179dd565a8aff036d781038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-152fade80688fa87ae16c2bc2b2a140d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-886decc67f42eada6a8c210682940421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-860191ac6986e3e0369470f1421ce3bd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="O2V-Mapping-Online-Open-Vocabulary-Mapping-with-Neural-Implicit-Representation"><a href="#O2V-Mapping-Online-Open-Vocabulary-Mapping-with-Neural-Implicit-Representation" class="headerlink" title="O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit   Representation"></a>O2V-Mapping: Online Open-Vocabulary Mapping with Neural Implicit   Representation</h2><p><strong>Authors:Muer Tie, Julong Wei, Zhengjun Wang, Ke Wu, Shansuai Yuan, Kaizhao Zhang, Jie Jia, Jieru Zhao, Zhongxue Gan, Wenchao Ding</strong></p>
<p>Online construction of open-ended language scenes is crucial for robotic applications, where open-vocabulary interactive scene understanding is required. Recently, neural implicit representation has provided a promising direction for online interactive mapping. However, implementing open-vocabulary scene understanding capability into online neural implicit mapping still faces three challenges: lack of local scene updating ability, blurry spatial hierarchical semantic segmentation and difficulty in maintaining multi-view consistency. To this end, we proposed O2V-mapping, which utilizes voxel-based language and geometric features to create an open-vocabulary field, thus allowing for local updates during online training process. Additionally, we leverage a foundational model for image segmentation to extract language features on object-level entities, achieving clear segmentation boundaries and hierarchical semantic features. For the purpose of preserving consistency in 3D object properties across different viewpoints, we propose a spatial adaptive voxel adjustment mechanism and a multi-view weight selection method. Extensive experiments on open-vocabulary object localization and semantic segmentation demonstrate that O2V-mapping achieves online construction of language scenes while enhancing accuracy, outperforming the previous SOTA method. </p>
<blockquote>
<p>åœ¨çº¿æ„å»ºå¼€æ”¾å¼è¯­è¨€åœºæ™¯å¯¹æœºå™¨äººåº”ç”¨è‡³å…³é‡è¦ï¼Œè¿™äº›åº”ç”¨éœ€è¦å¼€æ”¾å¼è¯æ±‡äº¤äº’åœºæ™¯ç†è§£ã€‚æœ€è¿‘ï¼Œç¥ç»éšå¼è¡¨ç¤ºæ–¹æ³•ä¸ºåœ¨çº¿äº¤äº’å¼æ˜ å°„æä¾›äº†æœ‰å‰é€”çš„æ–¹å‘ã€‚ç„¶è€Œï¼Œå°†å¼€æ”¾å¼è¯æ±‡åœºæ™¯ç†è§£èƒ½åŠ›èå…¥åœ¨çº¿ç¥ç»éšå¼æ˜ å°„ä»ç„¶é¢ä¸´ä¸‰ä¸ªæŒ‘æˆ˜ï¼šç¼ºä¹å±€éƒ¨åœºæ™¯æ›´æ–°èƒ½åŠ›ã€ç©ºé—´å±‚æ¬¡è¯­ä¹‰åˆ†å‰²æ¨¡ç³Šä»¥åŠç»´æŒå¤šè§†è§’ä¸€è‡´æ€§å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†O2V-mappingï¼Œå®ƒåˆ©ç”¨åŸºäºä½“ç´ çš„è¯­è¨€å’Œå‡ ä½•ç‰¹å¾æ¥åˆ›å»ºå¼€æ”¾å¼è¯æ±‡åœºï¼Œä»è€Œå…è®¸åœ¨çº¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„å±€éƒ¨æ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å›¾åƒåˆ†å‰²çš„åŸºç¡€æ¨¡å‹æ¥æå–å¯¹è±¡çº§å®ä½“çš„è¯­è¨€ç‰¹å¾ï¼Œå®ç°æ¸…æ™°çš„åˆ†å‰²è¾¹ç•Œå’Œå±‚æ¬¡è¯­ä¹‰ç‰¹å¾ã€‚ä¸ºäº†ä¿æŒä¸åŒè§†è§’ä¸‹3Då¯¹è±¡å±æ€§çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç©ºé—´è‡ªé€‚åº”ä½“ç´ è°ƒæ•´æœºåˆ¶å’Œä¸€ç§å¤šè§†è§’æƒé‡é€‰æ‹©æ–¹æ³•ã€‚åœ¨å¼€æ”¾å¼è¯æ±‡å¯¹è±¡å®šä½å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒO2V-mappingå®ç°äº†è¯­è¨€åœºæ™¯çš„åœ¨çº¿æ„å»ºï¼Œæé«˜äº†å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä¼˜æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.06836v2">PDF</a> ECCV2024</p>
<p><strong>Summary</strong></p>
<p>åœ¨çº¿æ„å»ºå¼€æ”¾è¯æ±‡è¯­è¨€åœºæ™¯å¯¹æœºå™¨äººåº”ç”¨è‡³å…³é‡è¦ï¼Œå…¶ä¸­è¦æ±‚å¼€æ”¾è¯æ±‡äº’åŠ¨åœºæ™¯ç†è§£ã€‚è¿‘æœŸï¼Œç¥ç»éšå¼è¡¨ç¤ºæ³•ä¸ºåœ¨çº¿äº’åŠ¨æ˜ å°„æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä½†åœ¨å®ç°å¼€æ”¾è¯æ±‡åœºæ™¯ç†è§£èƒ½åŠ›åˆ°åœ¨çº¿ç¥ç»éšå¼æ˜ å°„ä»é¢ä¸´ä¸‰ä¸ªæŒ‘æˆ˜ï¼šç¼ºä¹å±€éƒ¨åœºæ™¯æ›´æ–°èƒ½åŠ›ã€ç©ºé—´å±‚æ¬¡è¯­ä¹‰åˆ†å‰²æ¨¡ç³Šä»¥åŠå¤šè§†è§’ä¸€è‡´æ€§ç»´æŠ¤å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†O2V-mappingï¼Œåˆ©ç”¨åŸºäºä½“ç´ çš„è¯­è¨€å’Œå‡ ä½•ç‰¹å¾åˆ›å»ºå¼€æ”¾è¯æ±‡åœºï¼Œå…è®¸åœ¨çº¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„å±€éƒ¨æ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹æå–å¯¹è±¡çº§åˆ«çš„è¯­è¨€ç‰¹å¾ï¼Œå®ç°æ¸…æ™°çš„åˆ†å‰²è¾¹ç•Œå’Œå±‚æ¬¡è¯­ä¹‰ç‰¹å¾ã€‚ä¸ºäº†ä¿æŒä¸åŒè§†è§’ä¸‹ç‰©ä½“å±æ€§çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç©ºé—´è‡ªé€‚åº”ä½“ç´ è°ƒæ•´æœºåˆ¶å’Œä¸€ç§å¤šè§†è§’æƒé‡é€‰æ‹©æ–¹æ³•ã€‚åœ¨å¼€æ”¾è¯æ±‡å¯¹è±¡å®šä½å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒO2V-mappingå¯å®ç°è¯­è¨€åœºæ™¯çš„åœ¨çº¿æ„å»ºï¼Œæé«˜äº†å‡†ç¡®æ€§ï¼Œè¶…è¿‡äº†ä»¥å‰çš„æœ€ä¼˜æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿æ„å»ºå¼€æ”¾è¯æ±‡è¯­è¨€åœºæ™¯å¯¹æœºå™¨äººåº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ç¥ç»éšå¼è¡¨ç¤ºæ³•ä¸ºåœ¨çº¿äº’åŠ¨æ˜ å°„æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</li>
<li>å®ç°å¼€æ”¾è¯æ±‡åœºæ™¯ç†è§£èƒ½åŠ›åˆ°åœ¨çº¿ç¥ç»éšå¼æ˜ å°„é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>O2V-mappingåˆ©ç”¨åŸºäºä½“ç´ çš„è¯­è¨€å’Œå‡ ä½•ç‰¹å¾åˆ›å»ºå¼€æ”¾è¯æ±‡åœºï¼Œæ”¯æŒå±€éƒ¨æ›´æ–°ã€‚</li>
<li>é‡‡ç”¨å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹å®ç°æ¸…æ™°åˆ†å‰²è¾¹ç•Œå’Œå±‚æ¬¡è¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>æå‡ºç©ºé—´è‡ªé€‚åº”ä½“ç´ è°ƒæ•´æœºåˆ¶ä»¥ç»´æŠ¤å¤šè§†è§’ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.06836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b172286d9984de1d80a2273e09f29ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f86def0a652befd8026d884b6e2aeb0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c97a0a28538fcbfa48eff138b44eb51.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="HyperFusion-A-Hypernetwork-Approach-to-Multimodal-Integration-of-Tabular-and-Medical-Imaging-Data-for-Predictive-Modeling"><a href="#HyperFusion-A-Hypernetwork-Approach-to-Multimodal-Integration-of-Tabular-and-Medical-Imaging-Data-for-Predictive-Modeling" class="headerlink" title="HyperFusion: A Hypernetwork Approach to Multimodal Integration of   Tabular and Medical Imaging Data for Predictive Modeling"></a>HyperFusion: A Hypernetwork Approach to Multimodal Integration of   Tabular and Medical Imaging Data for Predictive Modeling</h2><p><strong>Authors:Daniel Duenias, Brennan Nichyporuk, Tal Arbel, Tammy Riklin Raviv</strong></p>
<p>The integration of diverse clinical modalities such as medical imaging and the tabular data extracted from patientsâ€™ Electronic Health Records (EHRs) is a crucial aspect of modern healthcare. Integrative analysis of multiple sources can provide a comprehensive understanding of the clinical condition of a patient, improving diagnosis and treatment decision. Deep Neural Networks (DNNs) consistently demonstrate outstanding performance in a wide range of multimodal tasks in the medical domain. However, the complex endeavor of effectively merging medical imaging with clinical, demographic and genetic information represented as numerical tabular data remains a highly active and ongoing research pursuit.   We present a novel framework based on hypernetworks to fuse clinical imaging and tabular data by conditioning the image processing on the EHRâ€™s values and measurements. This approach aims to leverage the complementary information present in these modalities to enhance the accuracy of various medical applications. We demonstrate the strength and generality of our method on two different brain Magnetic Resonance Imaging (MRI) analysis tasks, namely, brain age prediction conditioned by subjectâ€™s sex and multi-class Alzheimerâ€™s Disease (AD) classification conditioned by tabular data. We show that our framework outperforms both single-modality models and state-of-the-art MRI tabular data fusion methods. A link to our code can be found at <a target="_blank" rel="noopener" href="https://github.com/daniel4725/HyperFusion">https://github.com/daniel4725/HyperFusion</a> </p>
<blockquote>
<p>å°†åŒ»å­¦æˆåƒä¸ä»æ‚£è€…ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸­æå–çš„è¡¨æ ¼æ•°æ®ç­‰å¤šæ ·ä¸´åºŠæ¨¡å¼çš„æ•´åˆæ˜¯ç°ä»£åŒ»ç–—ä¿å¥çš„ä¸€ä¸ªé‡è¦æ–¹é¢ã€‚å¤šæºæ•´åˆåˆ†æå¯ä»¥å…¨é¢ç†è§£æ‚£è€…çš„ä¸´åºŠçŠ¶å†µï¼Œæé«˜è¯Šæ–­å’Œæ²»ç–—å†³ç­–ã€‚æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„å¤šæ¨¡å¼ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°å°†åŒ»å­¦æˆåƒä¸ä¸´åºŠã€äººå£ç»Ÿè®¡å­¦å’Œé—ä¼ ä¿¡æ¯åˆå¹¶çš„å¤æ‚å·¥ä½œï¼Œè¡¨ç°ä¸ºæ•°å€¼è¡¨æ ¼æ•°æ®çš„å½¢å¼ï¼Œä»æ˜¯æ´»è·ƒä¸”æŒç»­çš„ç ”ç©¶è¿½æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¶…ç½‘ç»œçš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ä»¥ç”µå­å¥åº·è®°å½•ä¸­çš„å€¼å’Œæµ‹é‡ä¸ºæ¡ä»¶ï¼Œèåˆä¸´åºŠæˆåƒå’Œè¡¨æ ¼æ•°æ®ã€‚è¯¥æ–¹æ³•æ—¨åœ¨åˆ©ç”¨è¿™äº›æ¨¡å¼ä¸­å­˜åœ¨çš„è¡¥å……ä¿¡æ¯ï¼Œæé«˜å„ç§åŒ»ç–—åº”ç”¨çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„è„‘éƒ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ†æä»»åŠ¡ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜åŠ¿å’Œé€šç”¨æ€§ï¼Œå³æ ¹æ®å—è¯•è€…æ€§åˆ«è¿›è¡Œè„‘é¾„é¢„æµ‹ï¼Œä»¥åŠæ ¹æ®è¡¨æ ¼æ•°æ®è¿›è¡Œçš„å¤šç±»é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰åˆ†ç±»ã€‚æˆ‘ä»¬å±•ç¤ºæˆ‘ä»¬çš„æ¡†æ¶è¶…è¶Šäº†å•æ¨¡å¼æ¨¡å‹å’Œæœ€å…ˆè¿›çš„MRIè¡¨æ ¼æ•°æ®èåˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç é“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/daniel4725/HyperFusion">https://github.com/daniel4725/HyperFusion</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13319v3">PDF</a> 20 pages, 11 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦æˆåƒä¸ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰è¡¨æ ¼æ•°æ®çš„èåˆæ˜¯ç°ä»£åŒ»ç–—çš„å…³é”®ã€‚æ·±åº¦å­¦ä¹ ç½‘ç»œï¼ˆDNNsï¼‰åœ¨æ­¤é¢†åŸŸçš„å¤šæ¨¡å¼ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºè¶…ç½‘ç»œçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡æ¡ä»¶å›¾åƒå¤„ç†å°†ä¸´åºŠæˆåƒä¸è¡¨æ ¼æ•°æ®èåˆï¼Œåˆ©ç”¨è¿™ä¸¤ç§æ¨¡å¼çš„äº’è¡¥ä¿¡æ¯æé«˜åŒ»ç–—åº”ç”¨çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åœ¨è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ†æä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒä¸ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰è¡¨æ ¼æ•°æ®çš„èåˆå¯¹ç°ä»£åŒ»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ç½‘ç»œï¼ˆDNNsï¼‰åœ¨å¤šæ¨¡å¼ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>æå‡ºåŸºäºè¶…ç½‘ç»œçš„æ–°æ¡†æ¶ï¼ŒèåˆåŒ»å­¦æˆåƒä¸è¡¨æ ¼æ•°æ®ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ¡ä»¶å›¾åƒå¤„ç†ï¼Œæ—¨åœ¨åˆ©ç”¨ä¸¤ç§æ¨¡å¼çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>æ¡†æ¶åœ¨è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ†æä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¦‚è„‘é¾„é¢„æµ‹å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»ã€‚</li>
<li>æ–°æ¡†æ¶ä¼˜äºå•æ¨¡å¼æ¨¡å‹å’Œå…ˆè¿›çš„MRIè¡¨æ ¼æ•°æ®èåˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0402b6ae252e60a02f8f9b3397eba01d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb63a3d0f162aec8027e0bb6095f2d14.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-002eb4732c94829603a70282f00a7dbc.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  SOLIDO A Robust Watermarking Method for Speech Synthesis via Low-Rank   Adaptation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-76770f72e5c3422b29c3bf5aa446794c.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  DSPO Direct Semantic Preference Optimization for Real-World Image   Super-Resolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
