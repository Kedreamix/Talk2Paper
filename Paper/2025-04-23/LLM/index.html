<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  Seeing from Another Perspective Evaluating Multi-View Understanding in   MLLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-fdf18cf6727a5ef0291e281ed754117b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-23-æ›´æ–°"><a href="#2025-04-23-æ›´æ–°" class="headerlink" title="2025-04-23 æ›´æ–°"></a>2025-04-23 æ›´æ–°</h1><h2 id="Seeing-from-Another-Perspective-Evaluating-Multi-View-Understanding-in-MLLMs"><a href="#Seeing-from-Another-Perspective-Evaluating-Multi-View-Understanding-in-MLLMs" class="headerlink" title="Seeing from Another Perspective: Evaluating Multi-View Understanding in   MLLMs"></a>Seeing from Another Perspective: Evaluating Multi-View Understanding in   MLLMs</h2><p><strong>Authors:Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Rouyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, Yi Ma</strong></p>
<p>Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test modelâ€™s geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at <a target="_blank" rel="noopener" href="https://danielchyeh.github.io/All-Angles-Bench/">https://danielchyeh.github.io/All-Angles-Bench/</a>. </p>
<blockquote>
<p>å¤šè§†è§’ç†è§£æ˜¯åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ä½œä¸ºå®ä½“ä»£ç†åº”ç”¨æ—¶é¢ä¸´çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ã€‚è¿™ç§èƒ½åŠ›èƒ½å¤Ÿåœ¨ä¸åŒè§†è§’é—´åè°ƒè§†è§‰ä¿¡æ¯ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¯¼èˆªã€æ“ä½œå’Œ3Dåœºæ™¯ç†è§£ã€‚è™½ç„¶æœ€è¿‘çš„MLLMsåœ¨é«˜å±‚æ¬¡æ¨ç†å’Œè§„åˆ’æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†åœ¨é¢å¯¹å¤šè§†è§’å‡ ä½•ä¸€è‡´æ€§å’Œè·¨è§†è§’å¯¹åº”é—®é¢˜æ—¶ï¼Œå®ƒä»¬ç»å¸¸æ˜¾å¾—èƒ½åŠ›ä¸è¶³ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°MLLMsåœ¨å¤šè§†è§’åœºæ™¯æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–¹ä½åŸºå‡†æµ‹è¯•ï¼ˆAll-Angles Benchï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡2100ä¸ªäººç±»ç²¾å¿ƒæ ‡æ³¨çš„å¤šè§†è§’é—®ç­”å¯¹ï¼Œæ¶µç›–90ä¸ªå¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œåœºæ™¯ã€‚æˆ‘ä»¬çš„å…­ä¸ªä»»åŠ¡ï¼ˆè®¡æ•°ã€å±æ€§è¯†åˆ«ã€ç›¸å¯¹è·ç¦»ã€ç›¸å¯¹æ–¹å‘ã€å¯¹è±¡æ“ä½œã€ç›¸æœºå§¿æ€ä¼°è®¡ï¼‰ä¸“é—¨æµ‹è¯•æ¨¡å‹çš„å‡ ä½•å¯¹åº”èƒ½åŠ›ä»¥åŠåœ¨å„ä¸ªè§†è§’é—´ä¸€è‡´å¯¹é½ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹27ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„MLLMsè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬Gemini-2.0-Flashã€Claude-3.7-Sonnetå’ŒGPT-4ä¸äººç±»è¯„ä¼°è€…çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œè¿™è¡¨æ˜å½“å‰MLLMsè·ç¦»äººç±»æ°´å¹³è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚é€šè¿‡æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å‘ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼ŒMLLMsè¡¨ç°å°¤ä¸ºä¸è¶³ï¼šï¼ˆ1ï¼‰å¯¹éƒ¨åˆ†é®æŒ¡è§†å›¾çš„è·¨è§†è§’å¯¹åº”ï¼›ï¼ˆ2ï¼‰å»ºç«‹ç²—ç•¥çš„ç›¸æœºå§¿æ€ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ç‰¹å®šé¢†åŸŸç»†åŒ–æˆ–åµŒå…¥æ›´å¼ºå¤šè§†è§’æ„è¯†çš„æ¨¡å—å¿…è¦æ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„å…¨æ–¹ä½åŸºå‡†æµ‹è¯•æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶ä¸ºå¼¥åˆMLLMså’Œäººç±»æ°´å¹³çš„å¤šè§†è§’ç†è§£ä¹‹é—´çš„å·®è·åšå‡ºè´¡çŒ®ã€‚è¯¥é¡¹ç›®å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://danielchyeh.github.io/All-Angles-Bench/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://danielchyeh.github.io/All-Angles-Bench/å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15280v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://danielchyeh.github.io/All-Angles-Bench/">https://danielchyeh.github.io/All-Angles-Bench/</a></p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤šè§†è§’ç†è§£æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå³èåˆä¸åŒè§†è§’çš„ä¿¡æ¯ä»¥å®ç°æœ‰æ•ˆçš„å¯¼èˆªã€æ“ä½œå’Œ3Dåœºæ™¯ç†è§£ã€‚æå‡ºAll-Angles BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«2,100å¤šä¸ªç»è¿‡äººç±»ç²¾å¿ƒæ ‡æ³¨çš„å¤šè§†è§’é—®ç­”å¯¹ï¼Œä»¥å…¨é¢è¯„ä¼°MLLMsåœ¨å¤šè§†è§’åœºæ™¯æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰MLLMsä¸äººç±»æ°´å¹³å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨éƒ¨åˆ†é®æŒ¡è§†è§’å’Œç²—ç•¥ç›¸æœºå§¿æ€ä¼°è®¡æ–¹é¢ã€‚éœ€è¦é¢†åŸŸç‰¹å®šçš„ä¼˜åŒ–æˆ–æ¨¡å—æ¥å¢å¼ºå¤šè§†è§’æ„ŸçŸ¥èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šè§†è§’ç†è§£æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦èåˆä¸åŒè§†è§’çš„ä¿¡æ¯ä»¥å®ç°æœ‰æ•ˆå¯¼èˆªã€æ“ä½œå’Œ3Dåœºæ™¯ç†è§£ã€‚</li>
<li>æå‡ºAll-Angles BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºå…¨é¢è¯„ä¼°MLLMsåœ¨å¤šè§†è§’åœºæ™¯æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>MLLMsåœ¨è·¨è§†è§’å¯¹åº”å’Œéƒ¨åˆ†é®æŒ¡è§†å›¾æ–¹é¢çš„è¡¨ç°è¾ƒå¼±ã€‚</li>
<li>å®éªŒä¸­åŒ…æ‹¬å¤šç§ä»»åŠ¡ï¼Œå¦‚è®¡æ•°ã€å±æ€§è¯†åˆ«ã€ç›¸å¯¹è·ç¦»ã€ç›¸å¯¹æ–¹å‘ã€å¯¹è±¡æ“ä½œã€ç›¸æœºå§¿æ€ä¼°è®¡ç­‰ï¼Œä¸“é—¨æµ‹è¯•æ¨¡å‹çš„å‡ ä½•å¯¹åº”èƒ½åŠ›å’Œè·¨è§†è§’ä¿¡æ¯ä¸€è‡´æ€§ã€‚</li>
<li>ä¸äººç±»è¯„ä¼°è€…ç›¸æ¯”ï¼Œå½“å‰MLLMså­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</li>
<li>éœ€è¦é¢†åŸŸç‰¹å®šçš„ä¼˜åŒ–æˆ–æ¨¡å—æ¥å¢å¼ºMLLMsçš„å¤šè§†è§’æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de96a1b5391a6203e47a2141cee70711.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1c963fdb06bd0857edfb899ec883bdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fce2418249b6d6d9b3fb1e092e209fc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7bfb7d59d238a8750ac6d7ad442678bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9c365e592613d2fdcb393e94f0ce04c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Stop-Summation-Min-Form-Credit-Assignment-Is-All-Process-Reward-Model-Needs-for-Reasoning"><a href="#Stop-Summation-Min-Form-Credit-Assignment-Is-All-Process-Reward-Model-Needs-for-Reasoning" class="headerlink" title="Stop Summation: Min-Form Credit Assignment Is All Process Reward Model   Needs for Reasoning"></a>Stop Summation: Min-Form Credit Assignment Is All Process Reward Model   Needs for Reasoning</h2><p><strong>Authors:Jie Cheng, Ruixi Qiao, Lijun Li, Chao Guo, Junle Wang, Gang Xiong, Yisheng Lv, Fei-Yue Wang</strong></p>
<p>Process reward models (PRMs) have proven effective for test-time scaling of Large Language Models (LLMs) on challenging reasoning tasks. However, reward hacking issues with PRMs limit their successful application in reinforcement fine-tuning. In this paper, we identify the main cause of PRM-induced reward hacking: the canonical summation-form credit assignment in reinforcement learning (RL), which defines the value as cumulative gamma-decayed future rewards, easily induces LLMs to hack steps with high rewards. To address this, we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation of PURE is a min-form credit assignment that formulates the value function as the minimum of future rewards. This method significantly alleviates reward hacking by limiting the value function range and distributing advantages more reasonably. Through extensive experiments on 3 base models, we show that PRM-based approaches enabling min-form credit assignment achieve comparable reasoning performance to verifiable reward-based methods within only 30% steps. In contrast, the canonical sum-form credit assignment collapses training even at the beginning! Additionally, when we supplement PRM-based fine-tuning with just 10% verifiable rewards, we further alleviate reward hacking and produce the best fine-tuned model based on Qwen2.5-Math-7B in our experiments, achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5 benchmarks. Moreover, we summarize the observed reward hacking cases and analyze the causes of training collapse. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/CJReinforce/PURE">https://github.com/CJReinforce/PURE</a>. </p>
<blockquote>
<p>æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨æŒ‘æˆ˜æ€§æ¨ç†ä»»åŠ¡ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æµ‹è¯•æ—¶é—´æ‰©å±•è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼ŒPRMä¸­çš„å¥–åŠ±ç ´è§£é—®é¢˜é™åˆ¶äº†å…¶åœ¨å¼ºåŒ–å¾®è°ƒä¸­çš„æˆåŠŸåº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†PRMå¼•èµ·çš„å¥–åŠ±ç ´è§£çš„ä¸»è¦åŸå› ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„è§„èŒƒæ€»å’Œå½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œå®ƒå°†ä»·å€¼å®šä¹‰ä¸ºç´¯ç§¯çš„æœªæ¥å¥–åŠ±çš„gammaè¡°å‡ï¼Œå®¹æ˜“è¯±å¯¼LLMç ´è§£é«˜å¥–åŠ±çš„æ­¥éª¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PUREï¼šæµç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ã€‚PUREçš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºæœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œå®ƒå°†ä»·å€¼å‡½æ•°åˆ¶å®šä¸ºæœªæ¥å¥–åŠ±çš„æœ€å°å€¼ã€‚è¿™ç§æ–¹æ³•é€šè¿‡é™åˆ¶ä»·å€¼å‡½æ•°èŒƒå›´å’Œæ›´åˆç†åœ°åˆ†é…ä¼˜åŠ¿æ¥æ˜¾è‘—ç¼“è§£å¥–åŠ±ç ´è§£é—®é¢˜ã€‚é€šè¿‡åœ¨ä¸‰å¥—åŸºç¡€æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†åŸºäºPRMçš„æ–¹æ³•ï¼Œä½¿èƒ½æœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œåœ¨ä»…30%çš„æ­¥éª¤å†…å®ç°äº†ä¸å¯éªŒè¯å¥–åŠ±æ–¹æ³•ç›¸å½“çš„æ¨ç†æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§„èŒƒæ€»å’Œå½¢å¼çš„ä¿¡ç”¨åˆ†é…ç”šè‡³åœ¨è®­ç»ƒå¼€å§‹æ—¶å°±ä¼šå´©æºƒï¼æ­¤å¤–ï¼Œå½“æˆ‘ä»¬ä»…åœ¨PRMå¾®è°ƒä¸­åŠ å…¥10%çš„å¯éªŒè¯å¥–åŠ±æ—¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç¼“è§£äº†å¥–åŠ±ç ´è§£é—®é¢˜ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„å®éªŒä¸­åŸºäºQwen2.5-Math-7Bç”Ÿæˆäº†æœ€ä½³å¾®è°ƒæ¨¡å‹ï¼Œåœ¨AMC23ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°82.5%ï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä¸º53.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ€»ç»“äº†è§‚å¯Ÿåˆ°çš„å¥–åŠ±ç ´è§£æ¡ˆä¾‹ï¼Œå¹¶åˆ†æäº†è®­ç»ƒå´©æºƒçš„åŸå› ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CJReinforce/PURE">https://github.com/CJReinforce/PURE</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹é¢çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨å¼ºåŒ–å¾®è°ƒä¸­é¢ä¸´çš„å¥–åŠ±é»‘å®¢æ”»å‡»é—®é¢˜ã€‚é’ˆå¯¹PRMè¯±å¯¼çš„å¥–åŠ±é»‘å®¢æ”»å‡»çš„ä¸»è¦åŸå› ï¼Œå³å¼ºåŒ–å­¦ä¹ ä¸­çš„è§„èŒƒæ±‚å’Œå½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œæœ¬æ–‡æå‡ºäº†PUREï¼šè¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ã€‚PUREçš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºé‡‡ç”¨æœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…ï¼Œå°†ä»·å€¼å‡½æ•°å®šä¹‰ä¸ºæœªæ¥å¥–åŠ±çš„æœ€å°å€¼ï¼Œä»è€Œæœ‰æ•ˆç¼“è§£å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºPRMçš„æ–¹æ³•èƒ½å¤Ÿå®ç°ä¸å¯éªŒè¯å¥–åŠ±æ–¹æ³•ç›¸å½“çš„åŸç†æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä»…30%çš„æ­¥éª¤å†…å®ç°ã€‚åŒæ—¶ï¼Œå½“å°†åŸºäºPRMçš„å¾®è°ƒä¸ä»…10%çš„å¯éªŒè¯å¥–åŠ±ç›¸ç»“åˆæ—¶ï¼Œèƒ½å¤Ÿåœ¨å®éªŒä¸­çš„Qwen2.5-Math-7Bæ¨¡å‹ä¸Šè·å¾—æœ€ä½³å¾®è°ƒæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨æµ‹è¯•æ—¶é—´æ‰©å±•LLMæ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>PRMåœ¨å¼ºåŒ–å¾®è°ƒä¸­é¢ä¸´å¥–åŠ±é»‘å®¢æ”»å‡»é—®é¢˜ã€‚</li>
<li>PRMè¯±å¯¼çš„å¥–åŠ±é»‘å®¢æ”»å‡»çš„ä¸»è¦åŸå› æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„è§„èŒƒæ±‚å’Œå½¢å¼çš„ä¿¡ç”¨åˆ†é…ã€‚</li>
<li>æå‡ºäº†PUREï¼šè¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼Œé‡‡ç”¨æœ€å°å½¢å¼çš„ä¿¡ç”¨åˆ†é…æ¥ç¼“è§£å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚</li>
<li>åŸºäºPRMçš„æ–¹æ³•åœ¨ä»…30%çš„æ­¥éª¤å†…å®ç°äº†ä¸å¯éªŒè¯å¥–åŠ±æ–¹æ³•ç›¸å½“çš„åŸç†æ€§èƒ½ã€‚</li>
<li>ç»“åˆ10%çš„å¯éªŒè¯å¥–åŠ±ä¸åŸºäºPRMçš„å¾®è°ƒï¼Œåœ¨ç‰¹å®šæ¨¡å‹ä¸Šè·å¾—äº†æœ€ä½³å¾®è°ƒæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2f136d345fb5af45f095e0fd2b0afad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb3d2c25a98f8c0492e973cc72b2ad8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0266eddd8ad0fc488116fa4e89873aff.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CRUST-Bench-A-Comprehensive-Benchmark-for-C-to-safe-Rust-Transpilation"><a href="#CRUST-Bench-A-Comprehensive-Benchmark-for-C-to-safe-Rust-Transpilation" class="headerlink" title="CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation"></a>CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation</h2><p><strong>Authors:Anirudh Khatry, Robert Zhang, Jia Pan, Ziteng Wang, Qiaochu Chen, Greg Durrett, Isil Dillig</strong></p>
<p>C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at <a target="_blank" rel="noopener" href="https://github.com/anirudhkhatry/CRUST-bench">https://github.com/anirudhkhatry/CRUST-bench</a>. </p>
<blockquote>
<p>Cåˆ°Rustçš„è½¬è¯‘å¯¹äºç°ä»£åŒ–é—ç•™Cä»£ç è‡³å…³é‡è¦ï¼ŒåŒæ—¶è¿˜å¯ä»¥æé«˜ä¸ç°ä»£Rustç”Ÿæ€ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œäº’æ“ä½œæ€§ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸å­˜åœ¨ç”¨äºè¯„ä¼°ç³»ç»Ÿæ˜¯å¦èƒ½å°†Cè½¬è¯‘ä¸ºå®‰å…¨Rustå¹¶èƒ½é€šè¿‡ä¸€ç»„æµ‹è¯•ç”¨ä¾‹çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å¼•å…¥äº†CRUST-Benchæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«100ä¸ªCä»£ç åº“ï¼Œæ¯ä¸ªåº“éƒ½é…æœ‰äººå·¥ç¼–å†™çš„å®‰å…¨Rustæ¥å£ä»¥åŠå¯ç”¨äºéªŒè¯è½¬è¯‘æ­£ç¡®æ€§çš„æµ‹è¯•ç”¨ä¾‹ã€‚é€šè¿‡è€ƒè™‘æ•´ä¸ªä»£ç åº“è€Œä¸æ˜¯å­¤ç«‹çš„å‡½æ•°ï¼ŒCRUST-Benchæ•æ‰åˆ°äº†ç¿»è¯‘å…·æœ‰è·¨å¤šä¸ªæ–‡ä»¶ä¾èµ–å…³ç³»çš„å¤æ‚é¡¹ç›®æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ‰€æä¾›çš„Rustæ¥å£æä¾›äº†æ˜ç¡®çš„è§„èŒƒï¼Œç¡®ä¿éµå¾ªæƒ¯ç”¨ä¸”å†…å­˜å®‰å…¨çš„Rustæ¨¡å¼ï¼Œè€Œéšé™„çš„æµ‹è¯•ç”¨ä¾‹åˆ™å¼ºåˆ¶æ‰§è¡ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨æ­¤ä»»åŠ¡ä¸Šè¯„ä¼°äº†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå‘ç°å®‰å…¨å’Œæƒ¯ç”¨Rustçš„ç”Ÿæˆå¯¹äºå„ç§æœ€æ–°æ–¹æ³•å’ŒæŠ€æœ¯æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æ·±å…¥æ¢è®¨äº†LLMåœ¨å°†Cä»£ç è½¬è¯‘ä¸ºå®‰å…¨Rustæ—¶é€šå¸¸å‡ºç°çš„é”™è¯¯ã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹OpenAI o1åœ¨å•å›åˆè®¾ç½®ä¸­ä»…èƒ½å®Œæˆ15é¡¹ä»»åŠ¡ã€‚åœ¨CRUST-Benchä¸Šçš„æ”¹è¿›å°†å¯¼è‡´æ›´å…ˆè¿›çš„è½¬è¯‘ç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚åœºæ™¯å¹¶å¸®åŠ©å°†é—ç•™ä»£ç åº“ä»Cè¿ç§»åˆ°åƒRustè¿™æ ·çš„ç¡®ä¿å†…å­˜å®‰å…¨çš„ç¼–ç¨‹è¯­è¨€ã€‚æ‚¨å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/anirudhkhatry/CRUST-bench">https://github.com/anirudhkhatry/CRUST-bench</a>æ‰¾åˆ°æ•°æ®é›†å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15254v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    Cè½¬Rustçš„è½¬æ¢æ˜¯ç°ä»£åŒ–é—ç•™Cä»£ç çš„å…³é”®ï¼Œèƒ½æé«˜å®‰å…¨æ€§å’Œä¸Rustç”Ÿæ€ç³»ç»Ÿçš„äº’æ“ä½œæ€§ã€‚ç„¶è€Œï¼Œå½“å‰ç¼ºä¹è¯„ä¼°ç³»ç»Ÿèƒ½å¦å°†Cå®‰å…¨è½¬æ¢ä¸ºRustçš„æ•°æ®åº“é›†ã€‚æˆ‘ä»¬æ¨å‡ºCRUST-Benchæ•°æ®é›†ï¼ŒåŒ…å«100ä¸ªCä»£ç åº“ï¼Œæ¯ä¸ªåº“éƒ½é…å¤‡æœ‰æ‰‹åŠ¨ç¼–å†™çš„å®‰å…¨Rustæ¥å£å’Œæµ‹è¯•ç”¨ä¾‹ï¼Œç”¨äºéªŒè¯è½¬æ¢çš„æ­£ç¡®æ€§ã€‚è¯¥æ•°æ®é›†è€ƒè™‘æ•´ä¸ªä»£ç åº“è€Œéå­¤ç«‹å‡½æ•°ï¼Œèƒ½åæ˜ ç¿»è¯‘å…·æœ‰è·¨å¤šä¸ªæ–‡ä»¶ä¾èµ–çš„å¤æ‚é¡¹ç›®çš„æŒ‘æˆ˜ã€‚æä¾›çš„Rustæ¥å£æä¾›æ˜ç¡®çš„è§„èŒƒï¼Œç¡®ä¿éµå¾ªä¹ æƒ¯æ€§å’Œå†…å­˜å®‰å…¨çš„Rustæ¨¡å¼ï¼Œè€Œé™„å¸¦æµ‹è¯•æ¡ˆä¾‹åˆ™å¼ºåˆ¶å®è¡ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿™ä¸€ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°ç”Ÿæˆå®‰å…¨ä¸”ä¹ æƒ¯æ€§çš„Rustä»£ç ä»ç„¶æ˜¯å„ç§æœ€æ–°æ–¹æ³•å’ŒæŠ€æœ¯çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å…³äºLLMåœ¨å°†ä»£ç ä»Cè½¬å®‰å…¨Rustæ—¶å¸¸è§é”™è¯¯çš„è§è§£ã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹OpenAI o1ä»…èƒ½åœ¨å•è½®ä»»åŠ¡ä¸­è§£å†³15ä¸ªä»»åŠ¡ã€‚å¯¹CRUST-Benchçš„æ”¹è¿›å°†å¯¼è‡´èƒ½å¤„ç†å¤æ‚åœºæ™¯çš„æ”¹è¿›è½¬æ¢ç³»ç»Ÿï¼Œæœ‰åŠ©äºå°†é—ç•™ä»£ç åº“ä»Cè¿ç§»åˆ°ç¡®ä¿å†…å­˜å®‰å…¨çš„Rustç­‰è¯­è¨€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Cè½¬Rustè½¬æ¢æ˜¯ç°ä»£åŒ–é—ç•™Cä»£ç çš„å…³é”®è¿‡ç¨‹ï¼Œæœ‰åŠ©äºæé«˜å®‰å…¨æ€§å’Œä¸Rustç”Ÿæ€ç³»ç»Ÿçš„äº’æ“ä½œæ€§ã€‚</li>
<li>å½“å‰ç¼ºä¹è¯„ä¼°Cè½¬Rustè½¬æ¢ç³»ç»Ÿæ€§èƒ½çš„æ•°æ®åº“é›†ã€‚</li>
<li>CRUST-Benchæ•°æ®é›†åŒ…å«100ä¸ªCä»£ç åº“ï¼Œæ¯ä¸ªéƒ½é…å¤‡æœ‰æ‰‹åŠ¨ç¼–å†™çš„å®‰å…¨Rustæ¥å£å’Œæµ‹è¯•ç”¨ä¾‹ï¼Œç”¨äºéªŒè¯è½¬æ¢çš„æ­£ç¡®æ€§ã€‚</li>
<li>CRUST-Benchæ•°æ®é›†èƒ½åæ˜ ç¿»è¯‘å…·æœ‰è·¨å¤šä¸ªæ–‡ä»¶ä¾èµ–çš„å¤æ‚é¡¹ç›®çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå®‰å…¨ä¸”ä¹ æƒ¯æ€§çš„Rustä»£ç æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>LLMåœ¨å°†ä»£ç ä»Cè½¬å®‰å…¨Rustæ—¶å¸¸è§é”™è¯¯ç±»å‹åŒ…æ‹¬è¯­æ³•é”™è¯¯ã€è¯­ä¹‰è¯¯è§£å’Œå†…å­˜å®‰å…¨é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d3b161e1896babf1bd66ed4d9834198a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cf6d2d64f9e5e02787b119e21dd9f4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f5c3cabfcdf2ab9a52339626bccf0c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b97845c18b472f0dd1ee397910a0a88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d965328e1eed4e73d136237fffdd801.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evaluating-Judges-as-Evaluators-The-JETTS-Benchmark-of-LLM-as-Judges-as-Test-Time-Scaling-Evaluators"><a href="#Evaluating-Judges-as-Evaluators-The-JETTS-Benchmark-of-LLM-as-Judges-as-Test-Time-Scaling-Evaluators" class="headerlink" title="Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as   Test-Time Scaling Evaluators"></a>Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as   Test-Time Scaling Evaluators</h2><p><strong>Authors:Yilun Zhou, Austin Xu, Peifeng Wang, Caiming Xiong, Shafiq Joty</strong></p>
<p>Scaling test-time computation, or affording a generator large language model (LLM) extra compute during inference, typically employs the help of external non-generative evaluators (i.e., reward models). Concurrently, LLM-judges, models trained to generate evaluations and critiques (explanations) in natural language, are becoming increasingly popular in automatic evaluation. Despite judge empirical successes, their effectiveness as evaluators in test-time scaling settings is largely unknown. In this paper, we introduce the Judge Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge performance in three domains (math reasoning, code generation, and instruction following) under three task settings: response reranking, step-level beam search, and critique-based response refinement. We evaluate 10 different judge models (7B-70B parameters) for 8 different base generator models (6.7B-72B parameters). Our benchmark shows that while judges are competitive with outcome reward models in reranking, they are consistently worse than process reward models in beam search procedures. Furthermore, though unique to LLM-judges, their natural language critiques are currently ineffective in guiding the generator towards better responses. </p>
<blockquote>
<p>åœ¨æµ‹è¯•æ—¶æ‰©å¤§è®¡ç®—è§„æ¨¡æˆ–ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå™¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­æä¾›é¢å¤–çš„è®¡ç®—æ”¯æŒï¼Œé€šå¸¸å€ŸåŠ©å¤–éƒ¨éç”Ÿæˆè¯„ä¼°å™¨ï¼ˆå³å¥–åŠ±æ¨¡å‹ï¼‰çš„å¸®åŠ©ã€‚åŒæ—¶ï¼Œè®­ç»ƒä»¥è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°ä¸æ‰¹è¯„ï¼ˆè§£é‡Šï¼‰çš„LLMåˆ¤æ–­æ¨¡å‹åœ¨è‡ªåŠ¨è¯„ä¼°ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚å°½ç®¡åˆ¤æ–­æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­æœ‰æˆåŠŸçš„ç»éªŒï¼Œä½†ä½œä¸ºæµ‹è¯•æ—¶é—´è§„æ¨¡è®¾ç½®ä¸­çš„è¯„ä¼°è€…çš„æœ‰æ•ˆæ€§ä»çŸ¥ä¹‹ç”šå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æµ‹è¯•æ—¶é—´è§„æ¨¡åˆ¤æ–­ï¼ˆJETTSï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åœ¨ä¸‰ä¸ªé¢†åŸŸï¼ˆæ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’ŒæŒ‡ä»¤éµå¾ªï¼‰ä¸‹ï¼Œå¯¹ä¸‰ç§ä»»åŠ¡è®¾ç½®ä¸­çš„åˆ¤æ–­æ€§èƒ½è¿›è¡Œè¯„ä»·ï¼šå“åº”é‡æ–°æ’åºã€æ­¥éª¤çº§å…‰æŸæœç´¢å’ŒåŸºäºæ‰¹è¯„çš„å“åº”ä¼˜åŒ–ã€‚æˆ‘ä»¬å¯¹8ä¸ªä¸åŒåŸºç¡€ç”Ÿæˆæ¨¡å‹çš„10ä¸ªä¸åŒåˆ¤æ–­æ¨¡å‹ï¼ˆå‚æ•°ä»7Båˆ°70Bï¼‰è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè™½ç„¶åœ¨é‡æ–°æ’åºä¸­ï¼Œåˆ¤æ–­æ¨¡å‹ä¸ç»“æœå¥–åŠ±æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œä½†åœ¨å…‰æŸæœç´¢è¿‡ç¨‹ä¸­ï¼Œå®ƒä»¬å§‹ç»ˆä¸å¦‚è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè™½ç„¶LLMåˆ¤æ–­å…·æœ‰ç‹¬ç‰¹æ€§ï¼Œä½†å…¶è‡ªç„¶è¯­è¨€æ‰¹è¯„ç›®å‰åœ¨å¼•å¯¼ç”Ÿæˆå™¨ç”Ÿæˆæ›´å¥½å“åº”æ–¹é¢æ•ˆæœä¸ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15253v1">PDF</a> The first two authors contributed equally. The codebase is at   <a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/jetts-benchmark">https://github.com/SalesforceAIResearch/jetts-benchmark</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æµ‹è¯•æ—¶é—´å°ºåº¦ä¸Šæ³•å®˜è¯„ä»·ï¼ˆLLM-judgeï¼‰çš„è¡¨ç°ã€‚æ–‡ä¸­å»ºç«‹äº†ä¸€ä¸ªåä¸ºJETTSçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œä»¥è¯„ä¼°æ³•å®˜åœ¨ä¸‰ä¸ªé¢†åŸŸï¼ˆæ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆå’ŒæŒ‡ä»¤éµå¾ªï¼‰ä¸‹çš„ä¸‰ç§ä»»åŠ¡è®¾ç½®ï¼ˆå“åº”é‡æ’ã€æ­¥éª¤çº§æŸæœç´¢å’ŒåŸºäºæ‰¹è¯„çš„å“åº”ä¼˜åŒ–ï¼‰ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è¯„ä»·äº†ä¸åŒè§„æ¨¡çš„æ³•å®˜æ¨¡å‹å’ŒåŸºç¡€ç”Ÿæˆå™¨æ¨¡å‹çš„æ€§èƒ½ï¼Œå‘ç°æ³•å®˜åœ¨é‡æ’ä»»åŠ¡ä¸­ä¸ç»“æœå¥–åŠ±æ¨¡å‹ç«äº‰ï¼Œä½†åœ¨æŸæœç´¢è¿‡ç¨‹ä¸­å§‹ç»ˆä¸å¦‚è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå°½ç®¡LLMæ³•å®˜çš„è‡ªç„¶è¯­è¨€æ‰¹è¯„æ˜¯ç‹¬ç‰¹çš„ï¼Œä½†ç›®å‰å°šä¸èƒ½æœ‰æ•ˆåœ°æŒ‡å¯¼ç”Ÿæˆå™¨ç”Ÿæˆæ›´å¥½çš„å“åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´å°ºåº¦ä¸Šå¼•å…¥LLM-judgeè¯„ä»·ï¼Œé€šè¿‡JETTSåŸºå‡†æµ‹è¯•å¹³å°è¯„ä¼°å…¶åœ¨ä¸‰ä¸ªé¢†åŸŸçš„è¡¨ç°ã€‚</li>
<li>æ³•å®˜æ¨¡å‹åœ¨å“åº”é‡æ’ä»»åŠ¡ä¸­ä¸ç»“æœå¥–åŠ±æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚</li>
<li>åœ¨æŸæœç´¢è¿‡ç¨‹ä¸­ï¼Œæ³•å®˜æ¨¡å‹çš„è¡¨ç°å§‹ç»ˆä¸å¦‚è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>LLM-judgeçš„è‡ªç„¶è¯­è¨€æ‰¹è¯„åœ¨å¼•å¯¼ç”Ÿæˆå™¨ç”Ÿæˆæ›´å¥½å“åº”æ–¹é¢ç›®å‰å°šä¸æœ‰æ•ˆã€‚</li>
<li>ç ”ç©¶æ¶‰åŠå¤šç§è§„æ¨¡çš„æ³•å®˜æ¨¡å‹å’ŒåŸºç¡€ç”Ÿæˆå™¨æ¨¡å‹çš„æ€§èƒ½è¯„ä»·ã€‚</li>
<li>JETTSåŸºå‡†æµ‹è¯•å¹³å°å¯ç”¨äºè¯„ä¼°æ³•å®˜åœ¨ä¸åŒä»»åŠ¡è®¾ç½®å’Œé¢†åŸŸä¸­çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d45b2cd9cb466616ee545d38873b2d83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-179c5acecbd726c45839edf647360039.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-726d440b0ee0d8db977f4997992b1e83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d34a9c3cb18b32082a911d94fee55f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78c37cb88e30b94872d511d6ae3d5b49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14e2ab4aa49a4aa7c2cf74593d3a5244.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MR-Guard-Multilingual-Reasoning-Guardrail-using-Curriculum-Learning"><a href="#MR-Guard-Multilingual-Reasoning-Guardrail-using-Curriculum-Learning" class="headerlink" title="MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning"></a>MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning</h2><p><strong>Authors:Yahan Yang, Soham Dan, Shuo Li, Dan Roth, Insup Lee</strong></p>
<p>Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®¹æ˜“å—åˆ°å¦‚è¶Šç‹±ç­‰å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œä»è€Œå¼•å‘æœ‰å®³æˆ–ä¸å®‰å…¨çš„è¡Œä¸ºã€‚è¿™ç§è„†å¼±æ€§åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­å°¤ä¸ºåŠ å‰§ï¼Œå› ä¸ºå¤šè¯­è¨€å®‰å…¨å¯¹é½çš„æ•°æ®é€šå¸¸æœ‰é™ã€‚å› æ­¤ï¼Œå¼€å‘ä¸€ç§èƒ½å¤Ÿåœ¨å¤šç§è¯­è¨€ä¸­æ£€æµ‹å’Œè¿‡æ»¤ä¸å®‰å…¨å†…å®¹çš„æŠ¤æ ï¼Œå¯¹äºåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­éƒ¨ç½²LLMè‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ¨ç†çš„å¤šè¯­è¨€æŠ¤æ æ„å»ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åˆæˆå¤šè¯­è¨€æ•°æ®ç”Ÿæˆï¼Œèå…¥æ–‡åŒ–å’Œè¯­è¨€ä¸Šçš„ç»†å¾®å·®å¼‚å˜ä½“ï¼Œï¼ˆ2ï¼‰ç›‘ç£å¾®è°ƒï¼Œä»¥åŠï¼ˆ3ï¼‰è¯¾ç¨‹å¼•å¯¼å¼ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¡†æ¶ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šè¯­è¨€æŠ¤æ åœ¨åŸŸå†…å’ŒåŸŸå¤–è¯­è¨€ä¸Šå‡æŒç»­è¶…è¶Šè¿‘æœŸåŸºçº¿ã€‚æˆ‘ä»¬çš„æŠ¤æ å…·å¤‡å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šè¯­è¨€è§£é‡Šï¼Œå¯¹äºç†è§£å¤šè¯­è¨€å†…å®¹å®¡æ ¸ä¸­çš„è¯­è¨€ç‰¹å®šé£é™©å’Œæ¨¡ç³Šæ€§ç‰¹åˆ«æœ‰ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15241v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜“å—åˆ°å¦‚è¶Šç‹±æ”»å‡»ç­‰æ•Œå¯¹æ”»å‡»çš„å½±å“ï¼Œå¯èƒ½å¼•å‘æœ‰å®³æˆ–ä¸å®‰å…¨çš„è¡Œä¸ºã€‚åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ï¼Œè¿™ç§è„†å¼±æ€§å› ç¼ºä¹å¤šè¯­è¨€å®‰å…¨å¯¹é½æ•°æ®è€ŒåŠ å‰§ã€‚å› æ­¤ï¼Œå¼€å‘ä¸€ç§èƒ½å¤Ÿæ£€æµ‹å¹¶è¿‡æ»¤å¤šç§è¯­è¨€ä¸å®‰å…¨å†…å®¹çš„é˜²æŠ¤æ ï¼Œå¯¹äºåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­éƒ¨ç½²LLMsè‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆæ¨ç†çš„å¤šè¯­è¨€é˜²æŠ¤æ æ„å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åˆæˆåŒ…å«æ–‡åŒ–å’Œè¯­è¨€ç»†å¾®å˜åŒ–çš„å¤šè¯­è¨€æ•°æ®ç”Ÿæˆï¼Œï¼ˆ2ï¼‰ç›‘ç£å¾®è°ƒï¼Œä»¥åŠï¼ˆ3ï¼‰è¯¾ç¨‹å¼•å¯¼å¼ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¡†æ¶ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šè¯­è¨€é˜²æŠ¤æ åœ¨åŸŸå†…å’ŒåŸŸå¤–è¯­è¨€ä¸Šå‡ä¸€è‡´ä¼˜äºæœ€è¿‘çš„åŸºçº¿ã€‚é˜²æŠ¤æ çš„å¤šè¯­è¨€æ¨ç†èƒ½åŠ›èƒ½å¤Ÿç”Ÿæˆå¤šè¯­è¨€è§£é‡Šï¼Œå¯¹äºç†è§£å¤šè¯­è¨€å†…å®¹ç®¡ç†ä¸­çš„è¯­è¨€ç‰¹å®šé£é™©å’Œæ¨¡ç³Šæ€§ç‰¹åˆ«æœ‰ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨é¢ä¸´è¶Šç‹±æ”»å‡»æ—¶å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œå¯èƒ½äº§ç”Ÿæœ‰å®³è¡Œä¸ºã€‚</li>
<li>å¤šè¯­è¨€ç¯å¢ƒä¸­LLMsçš„è„†å¼±æ€§å› ç¼ºä¹å¤šè¯­è¨€å®‰å…¨æ•°æ®è€ŒåŠ å‰§ã€‚</li>
<li>å¼€å‘èƒ½æ£€æµ‹å¹¶è¿‡æ»¤å¤šè¯­è¨€ä¸å®‰å…¨å†…å®¹çš„é˜²æŠ¤æ å¯¹LLMçš„ç°å®åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šè¯­è¨€é˜²æŠ¤æ æ„å»ºæ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®ç”Ÿæˆã€ç›‘ç£å¾®è°ƒå’ŒGRPOæ¡†æ¶ã€‚</li>
<li>å¤šè¯­è¨€é˜²æŠ¤æ åœ¨åŸŸå†…å’ŒåŸŸå¤–è¯­è¨€ä¸Šçš„æ€§èƒ½å‡ä¼˜äºåŸºçº¿ã€‚</li>
<li>é˜²æŠ¤æ å…·å¤‡å¤šè¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œèƒ½ç”Ÿæˆå¤šè¯­è¨€è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-56ef2758021ba491a7c809fae596eb73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8a0dc7d6c6a338f0c4ecdb11f16a2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-276b9e5d9c5321fd3232e97bb04f7155.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdf18cf6727a5ef0291e281ed754117b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Integrating-Symbolic-Execution-into-the-Fine-Tuning-of-Code-Generating-LLMs"><a href="#Integrating-Symbolic-Execution-into-the-Fine-Tuning-of-Code-Generating-LLMs" class="headerlink" title="Integrating Symbolic Execution into the Fine-Tuning of Code-Generating   LLMs"></a>Integrating Symbolic Execution into the Fine-Tuning of Code-Generating   LLMs</h2><p><strong>Authors:Marina Sakharova, Abhinav Anand, Mira Mezini</strong></p>
<p>Code-generating Large Language Models (LLMs) have become essential tools in modern software development, enhancing productivity and accelerating development. This paper aims to investigate the fine-tuning of code-generating LLMs using Reinforcement Learning and Direct Preference Optimization, further improving their performance. To achieve this, we enhance the training data for the reward model with the help of symbolic execution techniques, ensuring more comprehensive and objective data. With symbolic execution, we create a custom dataset that better captures the nuances in code evaluation. Our reward models, fine-tuned on this dataset, demonstrate significant improvements over the baseline, CodeRL, in estimating the quality of generated code. Our code-generating LLMs, trained with the help of reward model feedback, achieve similar results compared to the CodeRL benchmark. </p>
<blockquote>
<p>ä»£ç ç”Ÿæˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆä¸ºç°ä»£è½¯ä»¶å¼€å‘ä¸­çš„å¿…å¤‡å·¥å…·ï¼Œæé«˜äº†ç”Ÿäº§åŠ›å¹¶åŠ é€Ÿäº†å¼€å‘è¿›ç¨‹ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œç›´æ¥åå¥½ä¼˜åŒ–å¯¹ä»£ç ç”ŸæˆLLMè¿›è¡Œå¾®è°ƒï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å€ŸåŠ©ç¬¦å·æ‰§è¡ŒæŠ€æœ¯å¢å¼ºå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œç¡®ä¿æ›´å…¨é¢ã€å®¢è§‚çš„æ•°æ®ã€‚é€šè¿‡ç¬¦å·æ‰§è¡Œï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè‡ªå®šä¹‰æ•°æ®é›†ï¼Œæ›´å¥½åœ°æ•æ‰ä»£ç è¯„ä¼°ä¸­çš„ç»†å¾®å·®åˆ«ã€‚åœ¨æ­¤æ•°æ®é›†ä¸Šå¾®è°ƒçš„å¥–åŠ±æ¨¡å‹åœ¨ä¼°è®¡ç”Ÿæˆä»£ç çš„è´¨é‡æ–¹é¢ç›¸æ¯”åŸºçº¿CodeRLæ˜¾ç¤ºå‡ºæ˜¾ç€æ”¹è¿›ã€‚å€ŸåŠ©å¥–åŠ±æ¨¡å‹åé¦ˆè¿›è¡Œè®­ç»ƒçš„ä»£ç ç”ŸæˆLLMè¾¾åˆ°äº†ä¸CodeRLåŸºå‡†ç›¸ä¼¼çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15210v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä»£ç ç”Ÿæˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­å·²æˆä¸ºå¿…ä¸å¯å°‘çš„å·¥å…·ï¼Œå¯ä»¥æé«˜ç”Ÿäº§åŠ›å’ŒåŠ å¿«å¼€å‘é€Ÿåº¦ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œç›´æ¥åå¥½ä¼˜åŒ–å¯¹ä»£ç ç”ŸæˆLLMè¿›è¡Œå¾®è°ƒï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ç¬¦å·æ‰§è¡ŒæŠ€æœ¯å¢å¼ºå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œç¡®ä¿æ›´å…¨é¢ã€å®¢è§‚çš„æ•°æ®ã€‚ç¬¦å·æ‰§è¡Œå¸®åŠ©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰æ•°æ®é›†ï¼Œæ›´å¥½åœ°æ•æ‰ä»£ç è¯„ä¼°çš„ç»†å¾®å·®åˆ«ã€‚åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒå¥–åŠ±æ¨¡å‹ï¼Œåœ¨ä¼°è®¡ç”Ÿæˆä»£ç çš„è´¨é‡æ–¹é¢ï¼Œç›¸æ¯”åŸºçº¿CodeRLï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚å€ŸåŠ©å¥–åŠ±æ¨¡å‹åé¦ˆè®­ç»ƒçš„ä»£ç ç”ŸæˆLLMï¼Œè¾¾åˆ°äº†ä¸CodeRLåŸºå‡†ç›¸ä¼¼çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç ç”ŸæˆLLMåœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­çš„é‡è¦æ€§ï¼šæé«˜ç”Ÿäº§åŠ›å’ŒåŠ é€Ÿå¼€å‘ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œç›´æ¥åå¥½ä¼˜åŒ–å¯¹ä»£ç ç”ŸæˆLLMè¿›è¡Œå¾®è°ƒã€‚</li>
<li>ç¬¦å·æ‰§è¡ŒæŠ€æœ¯ç”¨äºå¢å¼ºå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>ç¬¦å·æ‰§è¡Œåˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ï¼Œæ›´å‡†ç¡®åœ°æ•æ‰ä»£ç è¯„ä¼°çš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹åœ¨ä¼°è®¡ç”Ÿæˆä»£ç è´¨é‡æ–¹é¢ç›¸æ¯”CodeRLæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>é€šè¿‡å¥–åŠ±æ¨¡å‹åé¦ˆè®­ç»ƒçš„ä»£ç ç”ŸæˆLLMè¾¾åˆ°ä¸CodeRLåŸºå‡†ç›¸ä¼¼çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0e0dcae67f64ff167803b27f51c3f5d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-643cf2206bf9bc7b6bc8ea859f2a1ccc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b8e833454e9a59d09ef135011062c69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b294a8b2c773f17222b35d9f16945aae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59d867c50175c7acfa87afef42c1313a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Compute-Optimal-LLMs-Provably-Generalize-Better-With-Scale"><a href="#Compute-Optimal-LLMs-Provably-Generalize-Better-With-Scale" class="headerlink" title="Compute-Optimal LLMs Provably Generalize Better With Scale"></a>Compute-Optimal LLMs Provably Generalize Better With Scale</h2><p><strong>Authors:Marc Finzi, Sanyam Kapoor, Diego Granziol, Anming Gu, Christopher De Sa, J. Zico Kolter, Andrew Gordon Wilson</strong></p>
<p>Why do larger language models generalize better? To investigate this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function. This generalization bound can be decomposed into three interpretable components: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate. As compute-optimal language models are scaled up, the number of parameters per data point remains constant; however, both the loss variance and the quantization error decrease, implying that larger models should have smaller generalization gaps. We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows more slowly than their capacity on the compute-optimal frontier. From these findings we produce a scaling law for the generalization gap, with bounds that become predictably stronger with scale. </p>
<blockquote>
<p>ä¸ºä»€ä¹ˆæ›´å¤§çš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–ï¼Ÿä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨è®¡ç®—æœ€ä¼˜çŠ¶æ€ä¸‹å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒç›®æ ‡åˆ¶å®šäº†æ³›åŒ–è¾¹ç•Œï¼Œå¦‚é‡‘é›€èŠ±æ‰©å±•å®šå¾‹æ‰€è¿°ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ã€å®Œå…¨åŸºäºç»éªŒçš„Freedmanå‹é©¬å°”å¯å¤«æµ“åº¦ä¸ç­‰å¼ï¼Œå®ƒé€šè¿‡è€ƒè™‘æŸå¤±å‡½æ•°çš„æ–¹å·®æ¥æ”¶ç´§ç°æœ‰è¾¹ç•Œã€‚è¿™ä¸ªæ³›åŒ–è¾¹ç•Œå¯ä»¥åˆ†è§£ä¸ºä¸‰ä¸ªå¯è§£é‡Šçš„æˆåˆ†ï¼šæ¯æ ‡è®°çš„å‚æ•°æ•°é‡ã€æŸå¤±æ–¹å·®å’Œåœ¨å›ºå®šæ¯”ç‰¹ç‡ä¸‹çš„é‡åŒ–è¯¯å·®ã€‚åœ¨è®¡ç®—æœ€ä¼˜è¯­è¨€æ¨¡å‹æ—¶ï¼Œæ¯æ•°æ®ç‚¹çš„å‚æ•°æ•°é‡ä¿æŒä¸å˜ï¼›ç„¶è€Œï¼ŒæŸå¤±æ–¹å·®å’Œé‡åŒ–è¯¯å·®éƒ½ä¼šå‡å°‘ï¼Œè¿™æ„å‘³ç€æ›´å¤§çš„æ¨¡å‹åº”è¯¥å…·æœ‰è¾ƒå°çš„æ³›åŒ–å·®è·ã€‚æˆ‘ä»¬ä»ä¿¡æ¯ç†è®ºçš„è§’åº¦åˆ†æä¸ºä»€ä¹ˆæ›´å¤§çš„æ¨¡å‹æ›´å®¹æ˜“é‡åŒ–ï¼Œæ˜¾ç¤ºå®ƒä»¬æ•´åˆæ–°ä¿¡æ¯çš„é€Ÿç‡åœ¨æœ€ä¼˜è®¡ç®—è¾¹ç•Œä¸Šçš„å¢é•¿æ¯”å…¶å®¹é‡è¦æ…¢ã€‚æ ¹æ®è¿™äº›å‘ç°ï¼Œæˆ‘ä»¬ä¸ºæ³›åŒ–å·®è·åˆ¶å®šäº†ä¸€ä¸ªæ‰©å±•å®šå¾‹ï¼Œéšç€è§„æ¨¡çš„æ‰©å¤§ï¼Œè¿™äº›è¾¹ç•Œå˜å¾—æ›´åŠ å¯é¢„æµ‹å’Œç‰¢å›ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15208v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸ºä½•æ›´å¼ºï¼Ÿæœ¬æ–‡é€šè¿‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒç›®æ ‡è¿›è¡Œæ³›åŒ–è¾¹ç•Œç ”ç©¶ï¼Œæ¢è®¨äº†è¿™ä¸€é—®é¢˜ã€‚åœ¨Chinchillaè§„æ¨¡å®šå¾‹æè¿°çš„è®¡ç®—æœºæœ€ä¼˜çŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„ã€å®Œå…¨å®è¯çš„Freedmanå‹é©¬å°”å¯å¤«æµ“åº¦ä¸ç­‰å¼ï¼Œè¯¥ä¸ç­‰å¼é€šè¿‡è€ƒè™‘æŸå¤±å‡½æ•°çš„æ–¹å·®æ¥ç¼©å°ç°æœ‰è¾¹ç•Œã€‚è¿™ä¸ªæ³›åŒ–è¾¹ç•Œå¯ä»¥åˆ†è§£ä¸ºä¸‰ä¸ªå¯è§£é‡Šçš„æˆåˆ†ï¼šæ¯ä»¤ç‰Œçš„å‚æ•°æ•°é‡ã€æŸå¤±æ–¹å·®å’Œåœ¨å›ºå®šæ¯”ç‰¹ç‡ä¸‹çš„é‡åŒ–è¯¯å·®ã€‚éšç€è®¡ç®—æœºæœ€ä¼˜è¯­è¨€æ¨¡å‹çš„æ‰©å±•ï¼Œæ¯æ•°æ®ç‚¹çš„å‚æ•°æ•°é‡ä¿æŒä¸å˜ï¼Œä½†æŸå¤±æ–¹å·®å’Œé‡åŒ–è¯¯å·®å‡å‡å°‘ï¼Œè¿™æ„å‘³ç€æ›´å¤§çš„æ¨¡å‹åº”å…·æœ‰è¾ƒå°çš„æ³›åŒ–è¯¯å·®ã€‚æˆ‘ä»¬ä»ä¿¡æ¯ç†è®ºçš„è§’åº¦åˆ†æä¸ºä½•æ›´å¤§çš„æ¨¡å‹æ›´å®¹æ˜“é‡åŒ–ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬æ•´åˆæ–°ä¿¡æ¯çš„é€Ÿåº¦ä¸è®¡ç®—æœ€ä¼˜è¾¹ç•Œä¸Šçš„å®¹é‡ä¹‹é—´çš„å…³ç³»ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬ä¸ºæ³›åŒ–è¯¯å·®æå‡ºäº†ä¸€ä¸ªè§„æ¨¡å®šå¾‹ï¼Œéšç€è§„æ¨¡çš„æ‰©å¤§ï¼Œè¾¹ç•Œå˜å¾—æ›´å…·é¢„æµ‹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¾—ç›Šäºå¯¹é¢„è®­ç»ƒç›®æ ‡çš„æ³›åŒ–è¾¹ç•Œç ”ç©¶ã€‚</li>
<li>å¼•å…¥äº†Freedmanå‹é©¬å°”å¯å¤«æµ“åº¦ä¸ç­‰å¼æ¥æ”¹è¿›ç°æœ‰è¾¹ç•Œã€‚</li>
<li>æ³›åŒ–è¾¹ç•ŒåŒ…æ‹¬æ¯ä»¤ç‰Œçš„å‚æ•°æ•°é‡ã€æŸå¤±æ–¹å·®å’Œé‡åŒ–è¯¯å·®ä¸‰ä¸ªå¯è§£é‡Šçš„æˆåˆ†ã€‚</li>
<li>éšç€è¯­è¨€æ¨¡å‹çš„æ‰©å±•ï¼ŒæŸå¤±æ–¹å·®å’Œé‡åŒ–è¯¯å·®å‡å°‘ã€‚</li>
<li>å¤§å‹æ¨¡å‹æ›´å®¹æ˜“é‡åŒ–ï¼Œä»ä¿¡æ¯ç†è®ºè§’åº¦åˆ†æäº†å…¶åŸå› ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹æ•´åˆæ–°ä¿¡æ¯çš„é€Ÿåº¦ä¸å®¹é‡ä¹‹é—´çš„å…³ç³»è¢«æ­ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8965566ddf9742a7173f67d81d0aaf15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec0ece45d3ba8a1a6403bda24ab6ac19.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Synergistic-Weak-Strong-Collaboration-by-Aligning-Preferences"><a href="#Synergistic-Weak-Strong-Collaboration-by-Aligning-Preferences" class="headerlink" title="Synergistic Weak-Strong Collaboration by Aligning Preferences"></a>Synergistic Weak-Strong Collaboration by Aligning Preferences</h2><p><strong>Authors:Yizhu Jiao, Xuchao Zhang, Zhaoyang Wang, Yubo Ma, Zhun Deng, Rujia Wang, Chetan Bansal, Saravan Rajmohan, Jiawei Han, Huaxiu Yao</strong></p>
<p>Current Large Language Models (LLMs) excel in general reasoning yet struggle with specialized tasks requiring proprietary or domain-specific knowledge. Fine-tuning large models for every niche application is often infeasible due to black-box constraints and high computational overhead. To address this, we propose a collaborative framework that pairs a specialized weak model with a general strong model. The weak model, tailored to specific domains, produces initial drafts and background information, while the strong model leverages its advanced reasoning to refine these drafts, extending LLMsâ€™ capabilities to critical yet specialized tasks. To optimize this collaboration, we introduce a collaborative feedback to fine-tunes the weak model, which quantifies the influence of the weak modelâ€™s contributions in the collaboration procedure and establishes preference pairs to guide preference tuning of the weak model. We validate our framework through experiments on three domains. We find that the collaboration significantly outperforms each model alone by leveraging complementary strengths. Moreover, aligning the weak model with the collaborative preference further enhances overall performance. </p>
<blockquote>
<p>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸€èˆ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦ä¸“ä¸šçŸ¥è¯†æˆ–ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„ä¸“ä¸šåŒ–ä»»åŠ¡æ–¹é¢å´é‡åˆ°äº†å›°éš¾ã€‚ç”±äºé»‘ç®±çº¦æŸå’Œè®¡ç®—å¼€é”€é«˜æ˜‚ï¼Œä¸ºæ¯ä¸€ä¸ªä¸“ä¸šåº”ç”¨å¾®è°ƒå¤§å‹æ¨¡å‹é€šå¸¸å¹¶ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä½œæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ä¸€ä¸ªä¸“ä¸šåŒ–çš„å¼±æ¨¡å‹ä¸ä¸€ä¸ªé€šç”¨çš„å¼ºæ¨¡å‹é…å¯¹ã€‚å¼±æ¨¡å‹é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œå®šåˆ¶ï¼Œç”¨äºç”Ÿæˆåˆæ­¥è‰æ¡ˆå’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œè€Œå¼ºæ¨¡å‹åˆ™åˆ©ç”¨å…¶å…ˆè¿›çš„æ¨ç†èƒ½åŠ›å¯¹è¿™äº›è‰æ¡ˆè¿›è¡Œæ”¹è¿›ï¼Œä»è€Œå°†LLMçš„èƒ½åŠ›æ‰©å±•åˆ°å…³é”®ä½†ä¸“ä¸šåŒ–çš„ä»»åŠ¡ã€‚ä¸ºäº†ä¼˜åŒ–è¿™ç§åä½œï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä½œåé¦ˆæ¥å¾®è°ƒå¼±æ¨¡å‹ï¼Œè¯¥åé¦ˆé‡åŒ–å¼±æ¨¡å‹åœ¨åä½œè¿‡ç¨‹ä¸­çš„è´¡çŒ®ï¼Œå¹¶å»ºç«‹åå¥½é…å¯¹æ¥æŒ‡å¯¼å¼±æ¨¡å‹çš„åå¥½è°ƒæ•´ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªé¢†åŸŸè¿›è¡Œå®éªŒéªŒè¯æˆ‘ä»¬çš„æ¡†æ¶ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡åˆ©ç”¨å„è‡ªçš„äº’è¡¥ä¼˜åŠ¿ï¼Œè¿™ç§åä½œæ–¹å¼æ˜¾è‘—ä¼˜äºå•ç‹¬ä½¿ç”¨ä»»ä¸€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä½¿å¼±æ¨¡å‹ä¸åä½œåå¥½ä¿æŒä¸€è‡´ï¼Œå¯è¿›ä¸€æ­¥æé«˜æ•´ä½“æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15188v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦ä¸“ä¸šçŸ¥è¯†æˆ–ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„ä¸“ä¸šé¢†åŸŸä»»åŠ¡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ç”±äºé»‘ç®±çº¦æŸå’Œè®¡ç®—å¼€é”€çš„é™åˆ¶ï¼Œä¸ºæ¯ä¸€ä¸ªä¸“ä¸šé¢†åŸŸåº”ç”¨å¾®è°ƒå¤§å‹æ¨¡å‹é€šå¸¸å¹¶ä¸å¯è¡Œã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä½œæ¡†æ¶ï¼Œå°†ä¸“ä¸šé¢†åŸŸçš„å¼±æ¨¡å‹ä¸é€šç”¨é¢†åŸŸçš„å¼ºæ¨¡å‹é…å¯¹ã€‚å¼±æ¨¡å‹è´Ÿè´£ç”Ÿæˆåˆæ­¥è‰æ¡ˆå’ŒèƒŒæ™¯ä¿¡æ¯ï¼Œè€Œå¼ºæ¨¡å‹åˆ™åˆ©ç”¨å…¶é«˜çº§æ¨ç†èƒ½åŠ›è¿›è¡Œæ”¹è¿›ï¼Œä»è€Œæ‰©å±•LLMåœ¨å…³é”®ä¸“ä¸šé¢†åŸŸä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚ä¸ºä¼˜åŒ–è¿™ä¸€åä½œè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä½œåé¦ˆæ¥å¾®è°ƒå¼±æ¨¡å‹ï¼Œé‡åŒ–å¼±æ¨¡å‹åœ¨åä½œè¿‡ç¨‹ä¸­çš„è´¡çŒ®ï¼Œå¹¶å»ºç«‹åå¥½é…å¯¹ä»¥æŒ‡å¯¼å¼±æ¨¡å‹çš„åå¥½è°ƒæ•´ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼Œè¯¥åä½œæ¡†æ¶åœ¨ä¸‰ä¸ªé¢†åŸŸä¸­çš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºå•ä¸€æ¨¡å‹ã€‚è¿›ä¸€æ­¥ä¸åä½œåå¥½å¯¹é½çš„å¼±æ¨¡å‹ï¼Œæ›´æé«˜äº†æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸“ä¸šé¢†åŸŸä»»åŠ¡ä¸Šé‡åˆ°å›°éš¾ã€‚</li>
<li>æå‡ºä¸€ç§åä½œæ¡†æ¶ï¼Œç»“åˆå¼±æ¨¡å‹å’Œå¼ºæ¨¡å‹çš„ä¼˜ç‚¹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å¼±æ¨¡å‹é’ˆå¯¹ç‰¹å®šé¢†åŸŸç”Ÿæˆåˆæ­¥è‰æ¡ˆå’ŒèƒŒæ™¯ä¿¡æ¯ã€‚</li>
<li>å¼ºæ¨¡å‹åˆ©ç”¨é«˜çº§æ¨ç†èƒ½åŠ›å¯¹å¼±æ¨¡å‹ç”Ÿæˆçš„åˆæ­¥æˆæœè¿›è¡Œæ”¹è¿›ã€‚</li>
<li>å¼•å…¥åä½œåé¦ˆæœºåˆ¶æ¥å¾®è°ƒå¼±æ¨¡å‹ï¼Œæé«˜å…¶æ€§èƒ½ã€‚</li>
<li>é‡åŒ–å¼±æ¨¡å‹åœ¨åä½œä¸­çš„è´¡çŒ®ï¼Œå¹¶å»ºç«‹åå¥½é…å¯¹ä»¥ä¼˜åŒ–åä½œè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa7f32fd1a2ded60edd4c1cb023c16d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b94379c9d3860ee630e6256856754362.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7364d178c21d76e69f8e849f94b21fdc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DSPO-Direct-Semantic-Preference-Optimization-for-Real-World-Image-Super-Resolution"><a href="#DSPO-Direct-Semantic-Preference-Optimization-for-Real-World-Image-Super-Resolution" class="headerlink" title="DSPO: Direct Semantic Preference Optimization for Real-World Image   Super-Resolution"></a>DSPO: Direct Semantic Preference Optimization for Real-World Image   Super-Resolution</h2><p><strong>Authors:Miaomiao Cai, Simiao Li, Wei Li, Xudong Huang, Hanting Chen, Jie Hu, Yunhe Wang</strong></p>
<p>Recent advances in diffusion models have improved Real-World Image Super-Resolution (Real-ISR), but existing methods lack human feedback integration, risking misalignment with human preference and may leading to artifacts, hallucinations and harmful content generation. To this end, we are the first to introduce human preference alignment into Real-ISR, a technique that has been successfully applied in Large Language Models and Text-to-Image tasks to effectively enhance the alignment of generated outputs with human preferences. Specifically, we introduce Direct Preference Optimization (DPO) into Real-ISR to achieve alignment, where DPO serves as a general alignment technique that directly learns from the human preference dataset. Nevertheless, unlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR are difficult to reconcile with the image-level preferences of DPO, which can lead to the DPO being overly sensitive to local anomalies, leading to reduced generation quality. To resolve this dichotomy, we propose Direct Semantic Preference Optimization (DSPO) to align instance-level human preferences by incorporating semantic guidance, which is through two strategies: (a) semantic instance alignment strategy, implementing instance-level alignment to ensure fine-grained perceptual consistency, and (b) user description feedback strategy, mitigating hallucinations through semantic textual feedback on instance-level images. As a plug-and-play solution, DSPO proves highly effective in both one-step and multi-step SR frameworks. </p>
<blockquote>
<p>æœ€æ–°çš„æ‰©æ•£æ¨¡å‹è¿›å±•å·²ç»æé«˜äº†çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰çš„æ•ˆæœï¼Œä½†ç°æœ‰æ–¹æ³•ç¼ºä¹äººç±»åé¦ˆæ•´åˆï¼Œå­˜åœ¨ä¸äººç±»åå¥½ä¸ä¸€è‡´çš„é£é™©ï¼Œå¹¶å¯èƒ½å¯¼è‡´å‡ºç°ä¼ªå½±ã€å¹»è§‰å’Œæœ‰å®³å†…å®¹ç”Ÿæˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†äººç±»åå¥½å¯¹é½æŠ€æœ¯å¼•å…¥Real-ISRã€‚è¯¥æŠ€æœ¯å·²æˆåŠŸåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ï¼Œå¯æœ‰æ•ˆæé«˜ç”Ÿæˆè¾“å‡ºä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¼•å…¥Real-ISRä»¥å®ç°å¯¹é½ï¼Œå…¶ä¸­DPOä½œä¸ºä¸€ç§é€šç”¨çš„å¯¹é½æŠ€æœ¯ï¼Œç›´æ¥ä»äººç±»åå¥½æ•°æ®é›†ä¸­å­¦ä¹ ã€‚ç„¶è€Œï¼Œä¸åŒäºé«˜çº§ä»»åŠ¡ï¼ŒReal-ISRçš„åƒç´ çº§é‡å»ºç›®æ ‡ä¸DPOçš„å›¾åƒçº§åå¥½å¾ˆéš¾åè°ƒï¼Œè¿™å¯èƒ½å¯¼è‡´DPOå¯¹å±€éƒ¨å¼‚å¸¸è¿‡äºæ•æ„Ÿï¼Œä»è€Œé™ä½ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€çŸ›ç›¾ï¼Œæˆ‘ä»¬æå‡ºç›´æ¥è¯­ä¹‰åå¥½ä¼˜åŒ–ï¼ˆDSPOï¼‰ï¼Œé€šè¿‡èå…¥è¯­ä¹‰æŒ‡å¯¼æ¥å¯¹é½å®ä¾‹çº§äººç±»åå¥½ï¼Œè¿™é€šè¿‡ä¸¤ä¸ªç­–ç•¥æ¥å®ç°ï¼šï¼ˆaï¼‰è¯­ä¹‰å®ä¾‹å¯¹é½ç­–ç•¥ï¼Œå®ç°å®ä¾‹çº§å¯¹é½ä»¥ç¡®ä¿ç²¾ç»†çš„æ„ŸçŸ¥ä¸€è‡´æ€§ï¼›ï¼ˆbï¼‰ç”¨æˆ·æè¿°åé¦ˆç­–ç•¥ï¼Œé€šè¿‡å®ä¾‹çº§å›¾åƒçš„è¯­ä¹‰æ–‡æœ¬åé¦ˆæ¥ç¼“è§£å¹»è§‰ã€‚ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„è§£å†³æ–¹æ¡ˆï¼ŒDSPOåœ¨å•æ­¥å’Œå¤šæ­¥SRæ¡†æ¶ä¸­éƒ½è¯æ˜äº†å…¶é«˜åº¦æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15176v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ€æ–°æ‰©æ•£æ¨¡å‹æ”¹è¿›äº†çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯ï¼ˆReal-ISRï¼‰ï¼Œä½†ç°æœ‰æ–¹æ³•æœªèå…¥äººç±»åé¦ˆï¼Œå¯èƒ½å¯¼è‡´ä¸äººä¸ºåå¥½çš„é”™ä½ä»¥åŠå›¾åƒç”Ÿæˆæ—¶äº§ç”Ÿçš„ä¼ªå½±ã€å¹»è±¡å’Œæœ‰å®³å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†äººç±»åå¥½å¯¹é½æŠ€æœ¯å¼•å…¥Real-ISRï¼Œè¯¥æŠ€æœ¯å·²æˆåŠŸåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ï¼Œèƒ½æœ‰æ•ˆæå‡ç”Ÿæˆè¾“å‡ºä¸äººä¸ºåå¥½çš„å¯¹é½åº¦ã€‚æˆ‘ä»¬å¼•å…¥ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å®ç°å¯¹é½ï¼Œå®ƒæ˜¯ä¸€ç§é€šç”¨å¯¹é½æŠ€æœ¯ï¼Œå¯ç›´æ¥ä»äººç±»åå¥½æ•°æ®é›†ä¸­å­¦ä¹ ã€‚ç„¶è€Œï¼Œä¸åŒäºé«˜çº§ä»»åŠ¡ï¼ŒReal-ISRçš„åƒç´ çº§é‡å»ºç›®æ ‡ä¸DPOçš„å›¾åƒçº§åå¥½éš¾ä»¥åè°ƒï¼Œå¯èƒ½å¯¼è‡´DPOè¿‡äºæ•æ„Ÿäºå±€éƒ¨å¼‚å¸¸ï¼Œé™ä½ç”Ÿæˆè´¨é‡ã€‚ä¸ºè§£å†³è¿™ä¸€çŸ›ç›¾ï¼Œæˆ‘ä»¬æå‡ºç›´æ¥è¯­ä¹‰åå¥½ä¼˜åŒ–ï¼ˆDSPOï¼‰ï¼Œé€šè¿‡èå…¥è¯­ä¹‰æŒ‡å¯¼æ¥å¯¹é½å®ä¾‹çº§äººä¸ºåå¥½ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªç­–ç•¥ï¼šä¸€æ˜¯è¯­ä¹‰å®ä¾‹å¯¹é½ç­–ç•¥ï¼Œå®ç°å®ä¾‹çº§å¯¹é½ä»¥ç¡®ä¿ç²¾ç»†ç²’åº¦æ„ŸçŸ¥ä¸€è‡´æ€§ï¼›äºŒæ˜¯ç”¨æˆ·æè¿°åé¦ˆç­–ç•¥ï¼Œé€šè¿‡å®ä¾‹çº§å›¾åƒçš„è¯­ä¹‰æ–‡æœ¬åé¦ˆæ¥å‡å°‘å¹»è±¡ã€‚DSPOä½œä¸ºä¸€ç§å³æ’å³ç”¨è§£å†³æ–¹æ¡ˆï¼Œåœ¨å•æ­¥å’Œå¤šæ­¥SRæ¡†æ¶ä¸­éƒ½è¡¨ç°å‡ºé«˜åº¦æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²æ”¹è¿›äº†Real-ISRæŠ€æœ¯ï¼Œä½†ç¼ºä¹äººç±»åé¦ˆé›†æˆå¯èƒ½å¯¼è‡´ä¸äººä¸ºåå¥½çš„é”™ä½åŠç”Ÿæˆé—®é¢˜ã€‚</li>
<li>äººç±»åå¥½å¯¹é½æŠ€æœ¯é¦–æ¬¡è¢«å¼•å…¥Real-ISRï¼Œæå‡ç”Ÿæˆè¾“å‡ºä¸äººä¸ºåå¥½çš„å¯¹é½åº¦ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä¸€ç§é€šç”¨å¯¹é½æŠ€æœ¯ï¼Œå¯ä»äººç±»åå¥½æ•°æ®é›†ä¸­å­¦ä¹ ã€‚</li>
<li>Real-ISRçš„åƒç´ çº§é‡å»ºç›®æ ‡ä¸DPOçš„å›¾åƒçº§åå¥½å­˜åœ¨åè°ƒå›°éš¾ï¼Œå¯èƒ½å¯¼è‡´DPOè¿‡äºæ•æ„Ÿã€‚</li>
<li>ç›´æ¥è¯­ä¹‰åå¥½ä¼˜åŒ–ï¼ˆDSPOï¼‰é€šè¿‡èå…¥è¯­ä¹‰æŒ‡å¯¼è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå®ç°å¯¹å®ä¾‹çº§çš„åå¥½å¯¹é½ã€‚</li>
<li>DSPOåŒ…å«ä¸¤ä¸ªç­–ç•¥ï¼šè¯­ä¹‰å®ä¾‹å¯¹é½å’Œç”¨æˆ·æè¿°åé¦ˆç­–ç•¥ï¼Œåˆ†åˆ«ç¡®ä¿æ„ŸçŸ¥ä¸€è‡´æ€§å’Œå‡å°‘å¹»è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f8c7d046bf5732c6b24f67c04bcca98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b0157178f017a2ad3b8d8c69f41821e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b55ed96bfa69e3a68a99e512026c58.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-Synthetic-Imputation-Approach-Generating-Optimal-Synthetic-Texts-For-Underrepresented-Categories-In-Supervised-Classification-Tasks"><a href="#The-Synthetic-Imputation-Approach-Generating-Optimal-Synthetic-Texts-For-Underrepresented-Categories-In-Supervised-Classification-Tasks" class="headerlink" title="The Synthetic Imputation Approach: Generating Optimal Synthetic Texts   For Underrepresented Categories In Supervised Classification Tasks"></a>The Synthetic Imputation Approach: Generating Optimal Synthetic Texts   For Underrepresented Categories In Supervised Classification Tasks</h2><p><strong>Authors:Joan C. Timoneda</strong></p>
<p>Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa, require that all categories in an annotation task be sufficiently represented in the training data for optimal performance. However, it is often difficult to find sufficient examples for all categories in a task when building a high-quality training set. In this article, I describe this problem and propose a solution, the synthetic imputation approach. Leveraging a generative LLM (GPT-4o), this approach generates synthetic texts based on careful prompting and five original examples drawn randomly with replacement from the sample. This approach ensures that new synthetic texts are sufficiently different from the original texts to reduce overfitting, but retain the underlying substantive meaning of the examples to maximize out-of-sample performance. With 75 original examples or more, synthetic imputationâ€™s performance is on par with a full sample of original texts, and overfitting remains low, predictable and correctable with 50 original samples. The synthetic imputation approach provides a novel role for generative LLMs in research and allows applied researchers to balance their datasets for best performance. </p>
<blockquote>
<p>ç¼–ç å™¨-è§£ç å™¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚BERTå’ŒRoBERTaï¼Œè¦æ±‚åœ¨æ ‡æ³¨ä»»åŠ¡çš„ç±»åˆ«åœ¨è®­ç»ƒæ•°æ®ä¸­èƒ½å¤Ÿå¾—åˆ°å……åˆ†çš„è¡¨ç¤ºï¼Œä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨æ„å»ºé«˜è´¨é‡è®­ç»ƒé›†æ—¶ï¼Œé€šå¸¸å¾ˆéš¾ä¸ºä»»åŠ¡ä¸­çš„æ‰€æœ‰ç±»åˆ«æ‰¾åˆ°è¶³å¤Ÿçš„ç¤ºä¾‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘æè¿°äº†è¿™ä¸ªé—®é¢˜å¹¶æå‡ºäº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œå³åˆæˆæ’è¡¥æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿæˆå¼LLMï¼ˆGPT-4oï¼‰ç”ŸæˆåŸºäºç²¾å¿ƒæç¤ºçš„åˆæˆæ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨äº”ä¸ªéšæœºæŠ½å–ï¼ˆå…è®¸é‡å¤ï¼‰çš„åŸå§‹ç¤ºä¾‹ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿æ–°ç”Ÿæˆçš„åˆæˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬è¶³å¤Ÿä¸åŒï¼Œä»¥å‡å°‘è¿‡åº¦æ‹Ÿåˆï¼ŒåŒæ—¶ä¿ç•™ç¤ºä¾‹çš„åŸºæœ¬å®é™…æ„ä¹‰ï¼Œä»¥æœ€å¤§åŒ–æ ·æœ¬å¤–çš„æ€§èƒ½ã€‚ä½¿ç”¨75ä¸ªåŸå§‹ç¤ºä¾‹æˆ–æ›´å¤šæ—¶ï¼Œåˆæˆæ’è¡¥æ–¹æ³•çš„æ€§èƒ½ä¸å®Œæ•´æ ·æœ¬çš„åŸå§‹æ–‡æœ¬ç›¸å½“ï¼Œè¿‡åº¦æ‹Ÿåˆä»ç„¶ä¿æŒä½æ°´å¹³ã€å¯é¢„æµ‹å¹¶å¯å€ŸåŠ©åŸå§‹æ ·æœ¬è¿›è¡Œä¿®æ­£ã€‚åˆæˆæ’è¡¥æ–¹æ³•ä¸ºç”Ÿæˆå¼LLMçš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„è§’è‰²ï¼Œå¹¶å…è®¸åº”ç”¨ç ”ç©¶äººå‘˜ä¸ºäº†è·å¾—æœ€ä½³æ€§èƒ½è€Œå¹³è¡¡ä»–ä»¬çš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15160v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡æ³¨ä»»åŠ¡ä¸­é‡åˆ°çš„ç±»åˆ«æ•°æ®ä¸å……åˆ†çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†åˆæˆå¡«å……æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåˆ©ç”¨ç”Ÿæˆå¼LLMï¼ˆå¦‚GPT-4oï¼‰åŸºäºç²¾å¿ƒè®¾è®¡çš„æç¤ºå’Œäº”ä¸ªåŸå§‹æ ·æœ¬ç”Ÿæˆåˆæˆæ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†æ–°çš„åˆæˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬è¶³å¤Ÿä¸åŒï¼Œä»¥å‡å°‘è¿‡åº¦æ‹Ÿåˆï¼ŒåŒæ—¶ä¿ç•™æ ·æœ¬çš„å®è´¨æ€§æ„ä¹‰ï¼Œä»¥æœ€å¤§åŒ–æ ·æœ¬å¤–çš„æ€§èƒ½ã€‚ä½¿ç”¨75ä¸ªä»¥ä¸Šçš„åŸå§‹æ ·æœ¬æ—¶ï¼Œåˆæˆå¡«å……çš„æ€§èƒ½ä¸å…¨æ ·æœ¬çš„åŸå§‹æ–‡æœ¬ç›¸å½“ï¼Œå¹¶ä¸”è¿‡åº¦æ‹Ÿåˆä¿æŒåœ¨ä½æ°´å¹³ä¸”å¯é¢„æµ‹å’Œå¯çº æ­£ã€‚åˆæˆå¡«å……æ–¹æ³•ä¸ºç”Ÿæˆå¼LLMåœ¨ç ”ç©¶ä¸­æä¾›äº†æ–°çš„ä½œç”¨ï¼Œå¹¶å…è®¸åº”ç”¨ç ”ç©¶äººå‘˜å¹³è¡¡ä»–ä»¬çš„æ•°æ®é›†ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ ‡æ³¨ä»»åŠ¡ä¸­éœ€è¦å……åˆ†ä»£è¡¨æ‰€æœ‰ç±»åˆ«ä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>å½“æ„å»ºé«˜è´¨é‡è®­ç»ƒé›†æ—¶ï¼Œä¸ºæ‰€æœ‰ç±»åˆ«æ‰¾åˆ°è¶³å¤Ÿçš„ä¾‹å­é€šå¸¸å¾ˆå›°éš¾ã€‚</li>
<li>åˆæˆå¡«å……æ–¹æ³•é€šè¿‡ä½¿ç”¨ç”Ÿæˆå¼LLMï¼ˆå¦‚GPT-4oï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åŸºäºç²¾å¿ƒè®¾è®¡çš„æç¤ºå’Œäº”ä¸ªéšæœºé€‰æ‹©çš„åŸå§‹æ ·æœ¬ç”Ÿæˆåˆæˆæ–‡æœ¬ã€‚</li>
<li>åˆæˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬è¶³å¤Ÿä¸åŒï¼Œä»¥å‡å°‘è¿‡åº¦æ‹Ÿåˆï¼ŒåŒæ—¶ä¿ç•™æ ·æœ¬çš„å®è´¨æ€§æ„ä¹‰ã€‚</li>
<li>ä½¿ç”¨75ä¸ªä»¥ä¸Šçš„åŸå§‹æ ·æœ¬æ—¶ï¼Œåˆæˆå¡«å……çš„æ€§èƒ½ä¸å…¨æ ·æœ¬çš„åŸå§‹æ–‡æœ¬ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7659cfb8f7f571b4ae59195d29fb531a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="KGMEL-Knowledge-Graph-Enhanced-Multimodal-Entity-Linking"><a href="#KGMEL-Knowledge-Graph-Enhanced-Multimodal-Entity-Linking" class="headerlink" title="KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking"></a>KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking</h2><p><strong>Authors:Juyeon Kim, Geon Lee, Taeuk Kim, Kijung Shin</strong></p>
<p>Entity linking (EL) aligns textual mentions with their corresponding entities in a knowledge base, facilitating various applications such as semantic search and question answering. Recent advances in multimodal entity linking (MEL) have shown that combining text and images can reduce ambiguity and improve alignment accuracy. However, most existing MEL methods overlook the rich structural information available in the form of knowledge-graph (KG) triples. In this paper, we propose KGMEL, a novel framework that leverages KG triples to enhance MEL. Specifically, it operates in three stages: (1) Generation: Produces high-quality triples for each mention by employing vision-language models based on its text and images. (2) Retrieval: Learns joint mention-entity representations, via contrastive learning, that integrate text, images, and (generated or KG) triples to retrieve candidate entities for each mention. (3) Reranking: Refines the KG triples of the candidate entities and employs large language models to identify the best-matching entity for the mention. Extensive experiments on benchmark datasets demonstrate that KGMEL outperforms existing methods. Our code and datasets are available at: <a target="_blank" rel="noopener" href="https://github.com/juyeonnn/KGMEL">https://github.com/juyeonnn/KGMEL</a>. </p>
<blockquote>
<p>å®ä½“é“¾æ¥ï¼ˆELï¼‰å°†æ–‡æœ¬æåŠä¸çŸ¥è¯†åº“ä¸­çš„ç›¸åº”å®ä½“å¯¹é½ï¼Œä¿ƒè¿›äº†è¯­ä¹‰æœç´¢å’Œé—®ç­”ç­‰å„ç§åº”ç”¨ã€‚å¤šæ¨¡æ€å®ä½“é“¾æ¥ï¼ˆMELï¼‰çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œç»“åˆæ–‡æœ¬å’Œå›¾åƒå¯ä»¥å‡å°‘æ­§ä¹‰ï¼Œæé«˜å¯¹é½å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„MELæ–¹æ³•å¿½è§†äº†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸‰å…ƒç»„ä¸­ä¸°å¯Œçš„ç»“æ„ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†KGMELï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨KGä¸‰å…ƒç»„å¢å¼ºMELçš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰ç”Ÿæˆï¼šé€šè¿‡åŸºäºæ–‡æœ¬å’Œå›¾åƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰å…ƒç»„ã€‚ï¼ˆ2ï¼‰æ£€ç´¢ï¼šé€šè¿‡å¯¹æ¯”å­¦ä¹ å­¦ä¹ è”åˆæåŠå®ä½“è¡¨ç¤ºï¼Œå°†æ–‡æœ¬ã€å›¾åƒå’Œï¼ˆç”Ÿæˆæˆ–çŸ¥è¯†å›¾è°±ï¼‰ä¸‰å…ƒç»„åˆå¹¶ï¼Œä»¥æ£€ç´¢æ¯ä¸ªæåŠçš„å€™é€‰å®ä½“ã€‚ï¼ˆ3ï¼‰é‡æ–°æ’åºï¼šç²¾ç‚¼å€™é€‰å®ä½“çš„KGä¸‰å…ƒç»„ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«ä¸æåŠå†…å®¹æœ€ä½³åŒ¹é…çš„å®ä½“ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒKGMELä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/juyeonnn/KGMEL%E3%80%82">https://github.com/juyeonnn/KGMELã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15135v1">PDF</a> SIGIR 2025 (Short)</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†å®ä½“é“¾æ¥ï¼ˆELï¼‰å’Œå¤šæ¨¡æ€å®ä½“é“¾æ¥ï¼ˆMELï¼‰çš„åŸºæœ¬æ¦‚å¿µåŠå…¶åœ¨çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MELæ–¹æ³•å¿½ç•¥äº†çŸ¥è¯†å›¾è°±ä¸­çš„ä¸°å¯Œç»“æ„ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶KGMELï¼Œåˆ©ç”¨KGä¸‰å…ƒç»„æ¥å¢å¼ºMELæ€§èƒ½ã€‚KGMELåŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šç”Ÿæˆé«˜è´¨é‡çš„ä¸‰å…ƒç»„ï¼Œå­¦ä¹ è”åˆæåŠå®ä½“è¡¨ç¤ºä»¥åŠé‡æ–°æ’åºå€™é€‰å®ä½“ä»¥è¯†åˆ«æœ€ä½³åŒ¹é…å®ä½“ã€‚å®éªŒè¯æ˜ï¼ŒKGMELåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ä½“é“¾æ¥ï¼ˆELï¼‰æ˜¯å°†æ–‡æœ¬ä¸­çš„æåŠä¸çŸ¥è¯†åº“ä¸­çš„å¯¹åº”å®ä½“è¿›è¡Œå¯¹é½çš„æŠ€æœ¯ï¼Œå¤šæ¨¡æ€å®ä½“é“¾æ¥ï¼ˆMELï¼‰ç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒæ¥å‡å°‘æ­§ä¹‰å¹¶æé«˜å¯¹é½å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰MELæ–¹æ³•å¿½è§†äº†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„ç»“æ„ä¿¡æ¯ã€‚</li>
<li>KGMELæ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨KGä¸‰å…ƒç»„æ¥å¢å¼ºMELæ€§èƒ½ã€‚</li>
<li>KGMELåŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šç”Ÿæˆé«˜è´¨é‡ä¸‰å…ƒç»„ã€å­¦ä¹ è”åˆæåŠå®ä½“è¡¨ç¤ºå’Œé‡æ–°æ’åºå€™é€‰å®ä½“ã€‚</li>
<li>KGMELé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯ç”Ÿæˆå’Œæ£€ç´¢é«˜è´¨é‡çš„ä¸‰å…ƒç»„ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒKGMELçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„MELæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0013392c1ed80c0ef27aa8843df5325e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2670bac72d41d31f5f403076d79a1678.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3303a6e7d03959f1e7bc2a5411f509a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94021f898603f6d781977bc43dbd4a87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81ec62b0be4373a9b32fff9c7e265b02.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EasyEdit2-An-Easy-to-use-Steering-Framework-for-Editing-Large-Language-Models"><a href="#EasyEdit2-An-Easy-to-use-Steering-Framework-for-Editing-Large-Language-Models" class="headerlink" title="EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language   Models"></a>EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language   Models</h2><p><strong>Authors:Ziwen Xu, Shuxun Wang, Kewei Xu, Haoming Xu, Mengru Wang, Xinle Deng, Yunzhi Yao, Guozhou Zheng, Huajun Chen, Ningyu Zhang</strong></p>
<p>In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the modelâ€™s behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the modelâ€™s responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/EasyEdit">https://github.com/zjunlp/EasyEdit</a> along with a demonstration notebook. In addition, we provide a demo video at <a target="_blank" rel="noopener" href="https://zjunlp.github.io/project/EasyEdit2/video">https://zjunlp.github.io/project/EasyEdit2/video</a> for a quick introduction. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†EasyEdit2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å®ç°å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡Œä¸ºè¿›è¡Œå³æ’å³ç”¨è°ƒæ•´çš„æ¡†æ¶ã€‚EasyEdit2æ”¯æŒå¹¿æ³›çš„æµ‹è¯•æ—¶é—´å¹²é¢„ï¼ŒåŒ…æ‹¬å®‰å…¨ã€æƒ…æ„Ÿã€ä¸ªæ€§ã€æ¨ç†æ¨¡å¼ã€çœŸå®æ€§å’Œè¯­è¨€ç‰¹å¾ã€‚ä¸åŒäºå…¶å‰èº«ï¼ŒEasyEdit2é‡‡ç”¨ä¸“é—¨è®¾è®¡ç”¨äºæ— ç¼æ¨¡å‹å¼•å¯¼çš„æ–°æ¶æ„ã€‚å®ƒåŒ…å«å…³é”®æ¨¡å—ï¼Œå¦‚å¼•å¯¼å‘é‡ç”Ÿæˆå™¨å’Œå¼•å¯¼å‘é‡åº”ç”¨å™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå’Œåº”ç”¨å¼•å¯¼å‘é‡ï¼Œä»¥å½±å“æ¨¡å‹çš„è¡Œä¸ºï¼Œè€Œæ— éœ€ä¿®æ”¹å…¶å‚æ•°ã€‚EasyEdit2çš„ä¸»è¦ä¼˜ç‚¹ä¹‹ä¸€æ˜¯ä½¿ç”¨æ–¹ä¾¿-ç”¨æˆ·æ— éœ€å…·å¤‡å¹¿æ³›çš„æŠ€æœ¯çŸ¥è¯†ã€‚åªéœ€ä¸€ä¸ªç¤ºä¾‹ï¼Œä»–ä»¬å°±å¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼å’Œè°ƒæ•´æ¨¡å‹çš„å“åº”ï¼Œä½¿ç²¾ç¡®æ§åˆ¶å˜å¾—æ—¢æ–¹ä¾¿åˆé«˜æ•ˆã€‚æˆ‘ä»¬é€šè¿‡å®éªŒæŠ¥å‘Šäº†åœ¨ä¸åŒLLMä¸Šçš„æ¨¡å‹å¼•å¯¼æ€§èƒ½ï¼Œè¯æ˜äº†è¿™äº›æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å·²åœ¨GitHubä¸Šå‘å¸ƒäº†æºä»£ç ï¼Œç½‘å€ä¸º<a target="_blank" rel="noopener" href="https://github.com/zjunlp/EasyEdit%EF%BC%8C%E5%B9%B6%E9%99%84%E5%B8%A6%E4%B8%80%E4%B8%AA%E6%BC%94%E7%A4%BA%E7%AC%94%E8%AE%B0%E6%9C%AC%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E6%88%91%E4%BB%AC%E8%BF%98%E6%8F%90%E4%BE%9B%E4%BA%86https://zjunlp.github.io/project/EasyEdit2/video%E6%BC%94%E7%A4%BA%E8%A7%86%E9%A2%91%EF%BC%8C%E4%BB%A5%E4%BE%9B%E5%BF%AB%E9%80%9F%E4%BB%8B%E7%BB%8D%E3%80%82">https://github.com/zjunlp/EasyEditï¼Œå¹¶é™„å¸¦ä¸€ä¸ªæ¼”ç¤ºç¬”è®°æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†https://zjunlp.github.io/project/EasyEdit2/videoæ¼”ç¤ºè§†é¢‘ï¼Œä»¥ä¾›å¿«é€Ÿä»‹ç»ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15133v1">PDF</a> Work in progress. Demo:   <a target="_blank" rel="noopener" href="https://zjunlp.github.io/project/EasyEdit2/video">https://zjunlp.github.io/project/EasyEdit2/video</a>; code:   <a target="_blank" rel="noopener" href="https://github.com/zjunlp/EasyEdit">https://github.com/zjunlp/EasyEdit</a></p>
<p><strong>Summary</strong></p>
<p>EasyEdit2æ¡†æ¶æ—¨åœ¨å®ç°å³æ’å³ç”¨å¯è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡Œä¸ºæ§åˆ¶ã€‚å®ƒæ”¯æŒå¹¿æ³›çš„å®‰å…¨ã€æƒ…æ„Ÿã€ä¸ªæ€§ã€æ¨ç†æ¨¡å¼ã€äº‹å®å’Œè¯­è¨€ç‰¹å¾çš„æµ‹è¯•æ—¶é—´å¹²é¢„ã€‚æ–°æ¶æ„å…·å¤‡æ— ç¼æ¨¡å‹è½¬å‘åŠŸèƒ½ï¼ŒåŒ…å«è½¬å‘çŸ¢é‡ç”Ÿæˆå™¨å’Œè½¬å‘çŸ¢é‡åº”ç”¨å™¨ç­‰å…³é”®æ¨¡å—ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆå’Œåº”ç”¨è½¬å‘çŸ¢é‡ä»¥å½±å“æ¨¡å‹è¡Œä¸ºï¼Œæ— éœ€ä¿®æ”¹å‚æ•°ã€‚EasyEdit2æ˜“äºä½¿ç”¨ï¼Œç”¨æˆ·æ— éœ€æ·±å…¥äº†è§£æŠ€æœ¯ç»†èŠ‚ï¼Œåªéœ€ä¸€ä¸ªç¤ºä¾‹å³å¯æœ‰æ•ˆå¼•å¯¼å’Œè°ƒæ•´æ¨¡å‹å“åº”ã€‚ç»éªŒæŠ¥å‘Šæ˜¾ç¤ºåœ¨ä¸åŒLLMä¸­æ¨¡å‹è½¬å‘æ€§èƒ½æ•ˆæœæ˜¾è‘—ã€‚æºç å·²å‘å¸ƒåœ¨GitHubä¸Šï¼Œå¹¶é™„æœ‰æ¼”ç¤ºç¬”è®°æœ¬å’Œè§†é¢‘ä»‹ç»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EasyEdit2æ˜¯ä¸€ä¸ªç”¨äºæ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹è¡Œä¸ºçš„æ¡†æ¶ã€‚</li>
<li>å®ƒæ”¯æŒå¤šç§æµ‹è¯•æ—¶é—´å¹²é¢„ï¼ŒåŒ…æ‹¬å®‰å…¨ã€æƒ…æ„Ÿã€ä¸ªæ€§ç­‰ã€‚</li>
<li>æ–°æ¶æ„å…·æœ‰æ— ç¼æ¨¡å‹è½¬å‘åŠŸèƒ½ã€‚</li>
<li>å«æœ‰è½¬å‘çŸ¢é‡ç”Ÿæˆå™¨å’Œåº”ç”¨å™¨ç­‰å…³é”®æ¨¡å—ã€‚</li>
<li>EasyEdit2èƒ½è‡ªåŠ¨ç”Ÿæˆå’Œåº”ç”¨è½¬å‘çŸ¢é‡ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ã€‚</li>
<li>è¯¥æ¡†æ¶æ˜“äºä½¿ç”¨ï¼Œç”¨æˆ·åªéœ€ç®€å•ç¤ºä¾‹å³å¯è°ƒæ•´æ¨¡å‹å“åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc68d74c6c4f92dc5b012e51ea599177.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02fc5dfb3e8cd3ac6ec012446f84238d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-841d6bd06c7da009ea2859bb8f5dc03f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95c4000f5d11ae57e98c5bd8f9fe36b4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Kuwain-1-5B-An-Arabic-SLM-via-Language-Injection"><a href="#Kuwain-1-5B-An-Arabic-SLM-via-Language-Injection" class="headerlink" title="Kuwain 1.5B: An Arabic SLM via Language Injection"></a>Kuwain 1.5B: An Arabic SLM via Language Injection</h2><p><strong>Authors:Khalil Hennara, Sara Chrouf, Mohamed Motaism Hamed, Zeina Aldallal, Omar Hadid, Safwan AlModhayan</strong></p>
<p>Enhancing existing models with new knowledge is a crucial aspect of AI development. This paper introduces a novel method for integrating a new language into a large language model (LLM). Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge. We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English. Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the modelâ€™s existing knowledge with a minimum amount of the original modelâ€™s data. This offers a cost-effective alternative to training a comprehensive model in both English and Arabic. The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes. </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½å‘å±•ä¸­ï¼Œç”¨æ–°çŸ¥è¯†å¢å¼ºç°æœ‰æ¨¡å‹æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„æ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å°†æ–°è¯­è¨€é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤ŸæˆåŠŸåœ°å°†ä¸€ç§å…ˆå‰æœªè§çš„ç›®æ ‡è¯­è¨€ï¼ˆé˜¿æ‹‰ä¼¯è¯­è¨€ï¼‰å¹¶å…¥åˆ°ç°æœ‰LLMä¸­ï¼Œè€Œä¸ä¼šæŸå®³å…¶åŸæœ‰çŸ¥è¯†ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸»è¦ä¸ºè‹±è¯­çš„å°å‹å¼€æºæ¨¡å‹ä¸­æ³¨å…¥é˜¿æ‹‰ä¼¯è¯­ï¼Œè®­ç»ƒäº†ä¸€ä¸ªåä¸ºâ€œåº“ç“¦å› â€ï¼ˆKuwainï¼‰çš„å°å‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ‹¥æœ‰1.5äº¿å‚æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨é˜¿æ‹‰ä¼¯è¯­æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†8%ï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹åŸæœ‰çš„çŸ¥è¯†å¹¶ä½¿ç”¨äº†æœ€å°çš„åŸå§‹æ¨¡å‹æ•°æ®é‡ã€‚è¿™ä¸ºåœ¨è‹±è¯­å’Œé˜¿æ‹‰ä¼¯è¯­æ–¹é¢è¿›è¡Œå…¨é¢æ¨¡å‹è®­ç»ƒæä¾›äº†ç»æµé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç»“æœçªæ˜¾äº†é«˜æ•ˆã€æœ‰é’ˆå¯¹æ€§çš„è¯­è¨€æ¨¡å‹æ‰©å±•çš„æ½œåŠ›ï¼Œæ— éœ€è¿›è¡Œå¤§è§„æ¨¡é‡æ–°è®­ç»ƒæˆ–èµ„æºå¯†é›†å‹è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15120v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å°†æ–°è¯­è¨€èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸æŸå¤±åŸæœ‰çŸ¥è¯†çš„å‰æä¸‹ï¼ŒæˆåŠŸå°†ä¹‹å‰æœªè§çš„ç›®æ ‡è¯­è¨€èå…¥ç°æœ‰LLMä¸­ã€‚é€šè¿‡å‘ä¸»è¦è‹±è¯­è®­ç»ƒçš„å°å‹å¼€æºæ¨¡å‹ä¸­æ³¨å…¥é˜¿æ‹‰ä¼¯è¯­ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªä»…æœ‰1.5äº¿å‚æ•°çš„å¾®å‹æ¨¡å‹â€”â€”Kuwainã€‚è¯¥æ–¹æ³•åœ¨é˜¿æ‹‰ä¼¯è¯­è¡¨ç°ä¸Šæ˜¾è‘—æé«˜ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜8%ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨å°‘é‡åŸå§‹æ¨¡å‹æ•°æ®å°±ä¿ç•™ä½äº†æ¨¡å‹çš„ç°æœ‰çŸ¥è¯†ã€‚è¿™ä¸ºåœ¨è‹±è¯­å’Œé˜¿æ‹‰ä¼¯è¯­å…¨é¢è®­ç»ƒæ¨¡å‹æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç»“æœçªæ˜¾äº†é«˜æ•ˆã€æœ‰é’ˆå¯¹æ€§çš„è¯­è¨€æ¨¡å‹æ‰©å±•çš„æ½œåŠ›ï¼Œæ— éœ€å¤§é‡é‡æ–°è®­ç»ƒæˆ–èµ„æºå¯†é›†å‹è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§å°†æ–°è¯­è¨€èå…¥å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸æŸå¤±åŸæœ‰çŸ¥è¯†çš„å‰æä¸‹èå…¥æ–°è¯­è¨€ã€‚</li>
<li>é€šè¿‡å‘ä¸»è¦è‹±è¯­è®­ç»ƒçš„å°å‹å¼€æºæ¨¡å‹ä¸­æ³¨å…¥é˜¿æ‹‰ä¼¯è¯­ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåä¸ºKuwainçš„å¾®å‹æ¨¡å‹ã€‚</li>
<li>Kuwainæ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­è¡¨ç°ä¸Šæ˜¾è‘—æé«˜ï¼Œå¹³å‡æé«˜8%ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨å°‘é‡åŸå§‹æ¨¡å‹æ•°æ®å°±å®ç°äº†çŸ¥è¯†çš„ä¿ç•™ã€‚</li>
<li>è¿™ä¸ºåœ¨è‹±è¯­å’Œé˜¿æ‹‰ä¼¯è¯­å…¨é¢è®­ç»ƒæ¨¡å‹æä¾›äº†æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15120">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77fbd4a091c46f8f8edcd350e9b69379.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4d969dbb8119bae0bec53d9ee0b2c18.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Empowering-AI-to-Generate-Better-AI-Code-Guided-Generation-of-Deep-Learning-Projects-with-LLMs"><a href="#Empowering-AI-to-Generate-Better-AI-Code-Guided-Generation-of-Deep-Learning-Projects-with-LLMs" class="headerlink" title="Empowering AI to Generate Better AI Code: Guided Generation of Deep   Learning Projects with LLMs"></a>Empowering AI to Generate Better AI Code: Guided Generation of Deep   Learning Projects with LLMs</h2><p><strong>Authors:Chen Xie, Mingsheng Jiao, Xiaodong Gu, Beijun Shen</strong></p>
<p>While large language models (LLMs) have been widely applied to code generation, they struggle with generating entire deep learning projects, which are characterized by complex structures, longer functions, and stronger reliance on domain knowledge than general-purpose code. An open-domain LLM often lacks coherent contextual guidance and domain expertise for specific projects, making it challenging to produce complete code that fully meets user requirements.   In this paper, we propose a novel planning-guided code generation method, DLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a structured solution plan, offering global guidance for LLMs to generate the project. The generated plan is then leveraged to retrieve semantically analogous code samples and subsequently abstract a code template. To effectively integrate these multiple retrieval-augmented techniques, a comparative learning mechanism is designed to generate the final code. We validate the effectiveness of our approach on a dataset we build for deep learning code generation. Experimental results demonstrate that DLCodeGen outperforms other baselines, achieving improvements of 9.7% in CodeBLEU and 3.6% in human evaluation metrics. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å¹¿æ³›åº”ç”¨äºä»£ç ç”Ÿæˆï¼Œä½†å®ƒä»¬åœ¨ç”Ÿæˆæ•´ä¸ªæ·±åº¦å­¦ä¹ é¡¹ç›®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æ·±åº¦å­¦ä¹ é¡¹ç›®å…·æœ‰å¤æ‚ç»“æ„ã€æ›´é•¿çš„åŠŸèƒ½å’Œæ¯”é€šç”¨ä»£ç æ›´å¼ºçƒˆçš„é¢†åŸŸçŸ¥è¯†ä¾èµ–ã€‚å¼€æ”¾é¢†åŸŸçš„LLMé€šå¸¸ç¼ºä¹è¿è´¯çš„ä¸Šä¸‹æ–‡æŒ‡å¯¼å’Œç‰¹å®šé¡¹ç›®çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå› æ­¤éš¾ä»¥ç”Ÿæˆå®Œå…¨æ»¡è¶³ç”¨æˆ·éœ€æ±‚çš„å®Œæ•´ä»£ç ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è§„åˆ’å¼•å¯¼ä»£ç ç”Ÿæˆæ–¹æ³•DLCodeGenï¼Œä¸“é—¨ç”¨äºç”Ÿæˆæ·±åº¦å­¦ä¹ é¡¹ç›®ã€‚DLCodeGené¢„æµ‹ç»“æ„åŒ–è§£å†³æ–¹æ¡ˆè®¡åˆ’ï¼Œä¸ºLLMç”Ÿæˆé¡¹ç›®æä¾›å…¨å±€æŒ‡å¯¼ã€‚ç”Ÿæˆçš„è®¡åˆ’éšåè¢«ç”¨æ¥æ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„ä»£ç æ ·æœ¬ï¼Œç„¶åæŠ½è±¡å‡ºä»£ç æ¨¡æ¿ã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ•´åˆè¿™äº›å¤šé‡æ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œè®¾è®¡äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æœºåˆ¶æ¥ç”Ÿæˆæœ€ç»ˆä»£ç ã€‚æˆ‘ä»¬åœ¨ä¸ºæ·±åº¦å­¦ä¹ ä»£ç ç”Ÿæˆæ„å»ºçš„æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDLCodeGenä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œåœ¨CodeBLEUæŒ‡æ ‡ä¸Šæé«˜äº†9.7%ï¼Œåœ¨äººç±»è¯„ä»·æŒ‡æ ‡ä¸Šæé«˜äº†3.6%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œé’ˆå¯¹ç”Ÿæˆæ•´ä¸ªæ·±åº¦å­¦ä¹ é¡¹ç›®æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹è§„åˆ’å¼•å¯¼çš„ä»£ç ç”Ÿæˆæ–¹æ³•DLCodeGenã€‚DLCodeGenèƒ½å¤Ÿé¢„æµ‹ç»“æ„åŒ–è§£å†³æ–¹æ¡ˆè®¡åˆ’ï¼Œä¸ºLLMç”Ÿæˆé¡¹ç›®æä¾›å…¨å±€æŒ‡å¯¼ã€‚é€šè¿‡åˆ©ç”¨ç”Ÿæˆçš„è®¡åˆ’ï¼Œæ£€ç´¢è¯­ä¹‰ç›¸ä¼¼çš„ä»£ç æ ·æœ¬å¹¶æŠ½è±¡å‡ºä»£ç æ¨¡æ¿ã€‚ä¸ºäº†æœ‰æ•ˆæ•´åˆå¤šç§æ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œè®¾è®¡äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æœºåˆ¶ä»¥ç”Ÿæˆæœ€ç»ˆä»£ç ã€‚åœ¨è‡ªå»ºçš„æ·±åº¦å­¦ä¹ ä»£ç ç”Ÿæˆæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœè¡¨æ˜DLCodeGenä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œåœ¨CodeBLEUæŒ‡æ ‡ä¸Šæé«˜äº†9.7%ï¼Œåœ¨äººç±»è¯„ä¼°æŒ‡æ ‡ä¸Šæé«˜äº†3.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å·²å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨ç”Ÿæˆæ·±åº¦å­¦ä¹ é¡¹ç›®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›é¡¹ç›®å…·æœ‰å¤æ‚ç»“æ„ã€æ›´é•¿çš„åŠŸèƒ½å’Œæ›´å¼ºçš„é¢†åŸŸçŸ¥è¯†ä¾èµ–ã€‚</li>
<li>DLCodeGenæ˜¯ä¸€ç§é’ˆå¯¹æ·±åº¦å­¦ä¹ é¡¹ç›®çš„è§„åˆ’å¼•å¯¼ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿé¢„æµ‹ç»“æ„åŒ–è§£å†³æ–¹æ¡ˆè®¡åˆ’ï¼Œä¸ºLLMç”Ÿæˆé¡¹ç›®æä¾›å…¨å±€æŒ‡å¯¼ã€‚</li>
<li>DLCodeGené€šè¿‡åˆ©ç”¨ç”Ÿæˆçš„è®¡åˆ’ï¼Œç»“åˆæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œä»è¯­ä¹‰ç›¸ä¼¼çš„ä»£ç æ ·æœ¬ä¸­æŠ½è±¡å‡ºä»£ç æ¨¡æ¿ã€‚</li>
<li>ä¸ºäº†æ•´åˆå¤šç§æ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼ŒDLCodeGenè®¾è®¡äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ æœºåˆ¶ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDLCodeGenåœ¨æ·±åº¦å­¦ä¹ ä»£ç ç”Ÿæˆæ–¹é¢ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</li>
<li>DLCodeGenåœ¨CodeBLEUæŒ‡æ ‡ä¸Šæé«˜äº†9.7%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc444001ee4e6a50a72d7349fc378212.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1b301abb20029797ed06a74f71beb4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eba3d370b5b4b527a583673ef770508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-006e88705bbb1290ba734f618ddcba3e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ParaPO-Aligning-Language-Models-to-Reduce-Verbatim-Reproduction-of-Pre-training-Data"><a href="#ParaPO-Aligning-Language-Models-to-Reduce-Verbatim-Reproduction-of-Pre-training-Data" class="headerlink" title="ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of   Pre-training Data"></a>ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of   Pre-training Data</h2><p><strong>Authors:Tong Chen, Faeze Brahman, Jiacheng Liu, Niloofar Mireshghallah, Weijia Shi, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi</strong></p>
<p>Language models (LMs) can memorize and reproduce segments from their pretraining data verbatim even in non-adversarial settings, raising concerns about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to reduce unintentional regurgitation while preserving their overall utility. ParaPO trains LMs to prefer paraphrased versions of memorized segments over the original verbatim content from the pretraining data. To maintain the ability to recall famous quotations when appropriate, we develop a variant of ParaPO that uses system prompts to control regurgitation behavior. In our evaluation on Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative writing), whereas unlearning methods used in prior work to mitigate regurgitation are less effective outside their targeted unlearned domain (from 17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO with system prompting successfully preserves famous quotation recall while reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the model not to regurgitate produces only a marginal reduction (8.7 to 8.4). </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰èƒ½å¤Ÿåœ¨éå¯¹æŠ—æ€§ç¯å¢ƒä¸­é€å­—é€å¥åœ°è®°å¿†å’Œå¤åˆ¶å…¶é¢„è®­ç»ƒæ•°æ®ä¸­çš„ç‰‡æ®µï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹ç‰ˆæƒã€å‰½çªƒã€éšç§å’Œåˆ›é€ åŠ›çš„æ‹…å¿§ã€‚æˆ‘ä»¬å¼•å…¥äº†Paraphrase Preference Optimizationï¼ˆParaPOï¼‰è¿™ä¸€åè®­ç»ƒæ³•ï¼Œå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘æ— æ„è¯†çš„å¤è¿°ç°è±¡ï¼ŒåŒæ—¶ä¿ç•™å…¶æ•´ä½“æ•ˆç”¨ã€‚ParaPOè®­ç»ƒè¯­è¨€æ¨¡å‹ä¼˜å…ˆé€‰æ‹©å¯¹è®°å¿†ç‰‡æ®µè¿›è¡ŒåŒä¹‰æ”¹å†™ï¼Œè€Œéä½¿ç”¨é¢„è®­ç»ƒæ•°æ®ä¸­çš„åŸå§‹å†…å®¹ã€‚ä¸ºäº†ä¿æŒé€‚æ—¶å›å¿†è‘—åå¼•è¯­çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä½¿ç”¨ç³»ç»Ÿæç¤ºæ¥æ§åˆ¶å¤è¿°è¡Œä¸ºçš„ParaPOå˜ä½“ã€‚åœ¨å¯¹Llama3.1-8Bçš„è¯„ä¼°ä¸­ï¼ŒParaPOåœ¨æ‰€æœ‰æµ‹è¯•æ•°æ®é›†ä¸Šéƒ½èƒ½æŒç»­å‡å°‘å¤è¿°ç°è±¡ï¼ˆä¾‹å¦‚ï¼Œåœ¨åˆ›é€ æ€§å†™ä½œæ–¹é¢å°†å¤è¿°æŒ‡æ ‡ä»17.3é™è‡³12.9ï¼‰ï¼Œè€Œå…ˆå‰å·¥ä½œä¸­ç”¨äºç¼“è§£å¤è¿°çš„å»å­¦ä¹ æ³•åœ¨å…¶æœªé’ˆå¯¹çš„é¢†åŸŸå¤–æ•ˆæœè¾ƒå·®ï¼ˆä»17.3é™è‡³16.9ï¼‰ã€‚å½“åº”ç”¨äºæŒ‡ä»¤è°ƒä¼˜çš„Tulu3-8Bæ¨¡å‹æ—¶ï¼Œå¸¦æœ‰ç³»ç»Ÿæç¤ºçš„ParaPOæˆåŠŸä¿ç•™äº†è‘—åå¼•è¯­çš„å›å¿†èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†æ— æ„è¯†çš„å¤è¿°ç°è±¡ï¼ˆåœ¨åˆ›é€ æ€§å†™ä½œæ–¹é¢ä»8.7é™è‡³6.3ï¼‰ï¼Œå½“æç¤ºä¸è¦å¤è¿°æ—¶æ•ˆæœæ›´ä½³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ²¡æœ‰ä½¿ç”¨ParaPOè°ƒä¼˜ï¼Œä»…æç¤ºæ¨¡å‹ä¸è¦å¤è¿°ï¼Œæ•ˆæœä»…ç•¥æœ‰é™ä½ï¼ˆä»8.7é™è‡³8.4ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14452v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰å³ä½¿åœ¨éå¯¹æŠ—æ€§ç¯å¢ƒä¸­ä¹Ÿèƒ½è®°å¿†å¹¶å†ç°é¢„è®­ç»ƒæ•°æ®ä¸­çš„ç‰‡æ®µï¼Œå¼•å‘äº†å…³äºç‰ˆæƒã€å‰½çªƒã€éšç§å’Œåˆ›é€ åŠ›çš„æ‹…å¿§ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºParaphrase Preference Optimizationï¼ˆParaPOï¼‰çš„åè®­ç»ƒæ³•ï¼Œæ—¨åœ¨é€šè¿‡å¾®è°ƒLMæ¥å‡å°‘æ— æ„è¯†çš„å¤è¿°ï¼ŒåŒæ—¶ä¿æŒå…¶æ•´ä½“æ•ˆç”¨ã€‚ParaPOè®­ç»ƒLMæ›´å€¾å‘äºä½¿ç”¨é¢„è®­ç»ƒæ•°æ®ä¸­è®°å¿†ç‰‡æ®µçš„åŒä¹‰ç‰ˆæœ¬è€ŒéåŸæ ·å†…å®¹ã€‚ä¸ºäº†èƒ½åœ¨é€‚å½“æ—¶å€™å›å¿†è‘—åå¼•æ–‡ï¼Œæ–‡ç« è¿˜å¼€å‘äº†å¸¦æœ‰ç³»ç»Ÿæç¤ºåŠŸèƒ½çš„ParaPOå˜ä½“ã€‚åœ¨Llama3.1-8Bä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒParaPOåœ¨æ‰€æœ‰æµ‹è¯•æ•°æ®é›†ä¸Šéƒ½èƒ½æœ‰æ•ˆå‡å°‘å¤è¿°ï¼ˆä¾‹å¦‚ï¼Œåœ¨åˆ›é€ æ€§å†™ä½œä¸­å°†å¤è¿°æŒ‡æ ‡ä»17.3é™è‡³12.9ï¼‰ï¼Œè€Œå…ˆå‰å·¥ä½œä¸­ç”¨äºå‡è½»å¤è¿°çš„é—å¿˜æ–¹æ³•åœ¨éç›®æ ‡é—å¿˜é¢†åŸŸä¹‹å¤–çš„æ•ˆæœè¾ƒå·®ï¼ˆä»17.3é™è‡³16.9ï¼‰ã€‚å½“åº”ç”¨äºæŒ‡ä»¤è°ƒæ•´çš„Tulu3-8Bæ¨¡å‹æ—¶ï¼Œå¸¦æœ‰ç³»ç»Ÿæç¤ºçš„ParaPOæˆåŠŸä¿ç•™äº†è‘—åå¼•æ–‡çš„å›å¿†ï¼ŒåŒæ—¶å‡å°‘äº†æ— æ„è¯†çš„å¤è¿°ï¼ˆåœ¨åˆ›é€ æ€§å†™ä½œä¸­ä»8.7é™è‡³6.3ï¼‰ï¼Œè€Œä¸ä½¿ç”¨ParaPOæç¤ºæ¨¡å‹å‡å°‘å¤è¿°åªäº§ç”Ÿè½»å¾®çš„æ•ˆæœï¼ˆä»8.7é™è‡³8.4ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰èƒ½è®°å¿†å¹¶å†ç°é¢„è®­ç»ƒæ•°æ®ä¸­çš„ç‰‡æ®µï¼Œå¼•å‘å…³äºç‰ˆæƒã€å‰½çªƒç­‰é—®é¢˜çš„æ‹…å¿§ã€‚</li>
<li>æå‡ºParaphrase Preference Optimizationï¼ˆParaPOï¼‰åè®­ç»ƒæ³•ï¼Œæ—¨åœ¨å‡å°‘LMçš„æ— æ„è¯†å¤è¿°ï¼ŒåŒæ—¶ä¿æŒå…¶æ•´ä½“æ•ˆèƒ½ã€‚</li>
<li>ParaPOé€šè¿‡è®­ç»ƒLMæ›´å€¾å‘äºä½¿ç”¨åŒä¹‰ç‰ˆæœ¬è€ŒéåŸæ ·å†…å®¹æ¥å‡å°‘å¤è¿°ã€‚</li>
<li>å¼€å‘äº†å¸¦æœ‰ç³»ç»Ÿæç¤ºåŠŸèƒ½çš„ParaPOå˜ä½“ï¼Œä»¥åœ¨é€‚å½“æ—¶å€™å›å¿†è‘—åå¼•æ–‡ã€‚</li>
<li>åœ¨Llama3.1-8Bä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒParaPOèƒ½æœ‰æ•ˆå‡å°‘å¤è¿°ã€‚</li>
<li>ä¸å…ˆå‰çš„é—å¿˜æ–¹æ³•ç›¸æ¯”ï¼ŒParaPOåœ¨å‡å°‘å¤è¿°æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86b0d1a6a7e7972b2f833622d88053c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f59ac590ac858aa610f7d265d2504092.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1eeec3091d1d92bb0016a6233de89af5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a02b6d208c8a4f3dad67ac556d6cd3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Empirical-Evaluation-of-Knowledge-Distillation-from-Transformers-to-Subquadratic-Language-Models"><a href="#Empirical-Evaluation-of-Knowledge-Distillation-from-Transformers-to-Subquadratic-Language-Models" class="headerlink" title="Empirical Evaluation of Knowledge Distillation from Transformers to   Subquadratic Language Models"></a>Empirical Evaluation of Knowledge Distillation from Transformers to   Subquadratic Language Models</h2><p><strong>Authors:Patrick Haller, Jonas Golde, Alan Akbik</strong></p>
<p>Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures, leveraging softmax attention for sequence modeling. However, the quadratic complexity of self-attention at inference time remains a significant bottleneck, motivating the exploration of subquadratic alternatives such as structured state-space models (SSMs), linear attention, and recurrent architectures. In this work, we systematically evaluate the transferability of knowledge distillation from a Transformer teacher to nine subquadratic student architectures. Our study aims to determine which subquadratic model best aligns with the teacherâ€™s learned representations and how different architectural constraints influence the distillation process. We also investigate the impact of intelligent initialization strategies, including matrix mixing and query-key-value (QKV) copying, on the adaptation process. Our empirical results on multiple NLP benchmarks provide insights into the trade-offs between efficiency and performance, highlighting key factors for successful knowledge transfer to subquadratic architectures. </p>
<blockquote>
<p>çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŠ€æœ¯ï¼Œé€šè¿‡è®­ç»ƒè¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹æ¥æ¨¡ä»¿è¾ƒå¤§çš„æ•™å¸ˆæ¨¡å‹ã€‚é€šå¸¸ï¼Œæ•™å¸ˆå’Œå­¦ç”Ÿçš„æ¶æ„éƒ½æ˜¯åŸºäºTransformerçš„ï¼Œåˆ©ç”¨softmaxæ³¨æ„åŠ›è¿›è¡Œåºåˆ—å»ºæ¨¡ã€‚ç„¶è€Œï¼Œæ¨ç†æ—¶è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚æ€§ä»ç„¶æ˜¯æ˜¾è‘—ç“¶é¢ˆï¼Œè¿™ä¿ƒä½¿äººä»¬æ¢ç´¢æ¬¡äºŒæ¬¡æ›¿ä»£æ–¹æ¡ˆï¼Œå¦‚ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ã€çº¿æ€§æ³¨æ„åŠ›å’Œå¾ªç¯æ¶æ„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä»Transformeræ•™å¸ˆåˆ°ä¹ç§å­äºŒæ¬¡å­¦ç”Ÿæ¶æ„çš„çŸ¥è¯†è’¸é¦çš„è¿ç§»æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨ç¡®å®šå“ªä¸ªå­äºŒæ¬¡æ¨¡å‹æœ€èƒ½ä¸æ•™å¸ˆçš„å·²å­¦è¡¨ç¤ºå¯¹é½ï¼Œä»¥åŠä¸åŒçš„æ¶æ„çº¦æŸå¦‚ä½•å½±å“è’¸é¦è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ™ºèƒ½åˆå§‹åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬çŸ©é˜µæ··åˆå’ŒæŸ¥è¯¢-é”®å€¼ï¼ˆQKVï¼‰å¤åˆ¶å¯¹é€‚åº”è¿‡ç¨‹çš„å½±å“ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªNLPåŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯ç»“æœä¸ºæ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡æä¾›äº†è§è§£ï¼Œå¹¶çªå‡ºäº†æˆåŠŸè½¬ç§»çŸ¥è¯†åˆ°å­äºŒæ¬¡æ¶æ„çš„å…³é”®å› ç´ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14366v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§å¹¿æ³›ç”¨äºå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŠ€æœ¯ï¼Œé€šè¿‡è®­ç»ƒå°å‹çš„å­¦ç”Ÿæ¨¡å‹æ¥æ¨¡ä»¿å¤§å‹çš„æ•™å¸ˆæ¨¡å‹ã€‚é€šå¸¸ï¼Œæ•™å¸ˆå’Œå­¦ç”Ÿéƒ½æ˜¯åŸºäºTransformerçš„æ¶æ„ï¼Œåˆ©ç”¨softmaxæ³¨æ„åŠ›è¿›è¡Œåºåˆ—å»ºæ¨¡ã€‚ç„¶è€Œï¼Œè‡ªæ³¨æ„åŠ›åœ¨æ¨ç†æ—¶é—´ä¸Šçš„äºŒæ¬¡å¤æ‚æ€§ä»æ˜¯æ˜¾è‘—ç“¶é¢ˆï¼Œä¿ƒä½¿äººä»¬æ¢ç´¢æ¬¡äºŒæ¬¡æ›¿ä»£æ–¹æ¡ˆï¼Œå¦‚ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ã€çº¿æ€§æ³¨æ„åŠ›å’Œé€’å½’æ¶æ„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä»Transformeræ•™å¸ˆåˆ°ä¹ç§å­äºŒæ¬¡å­¦ç”Ÿæ¶æ„çš„çŸ¥è¯†è’¸é¦çš„è¿ç§»æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨ç¡®å®šå“ªä¸ªæ¬¡äºŒæ¬¡æ¨¡å‹æœ€èƒ½ä¸æ•™å¸ˆçš„è¡¨ç¤ºå¯¹é½ï¼Œä»¥åŠä¸åŒçš„æ¶æ„çº¦æŸå¦‚ä½•å½±å“è’¸é¦è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ™ºèƒ½åˆå§‹åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬çŸ©é˜µæ··åˆå’ŒæŸ¥è¯¢-é”®-å€¼ï¼ˆQKVï¼‰å¤åˆ¶å¯¹é€‚åº”è¿‡ç¨‹çš„å½±å“ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªNLPåŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯ç»“æœä¸ºæ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡æä¾›äº†è§è§£ï¼Œå¹¶çªå‡ºäº†æˆåŠŸå°†çŸ¥è¯†è½¬ç§»åˆ°æ¬¡äºŒæ¬¡æ¶æ„çš„å…³é”®å› ç´ ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†è’¸é¦æ˜¯å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæŠ€æœ¯ï¼Œé€šè¿‡è®­ç»ƒå°å‹æ¨¡å‹æ¥æ¨¡ä»¿å¤§å‹æ¨¡å‹ã€‚</li>
<li>ç›®å‰å­˜åœ¨æ¨ç†æ—¶é—´è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚æ€§ç“¶é¢ˆï¼Œéœ€è¦æ¢ç´¢æ¬¡äºŒæ¬¡æ¨¡å‹æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°äº†çŸ¥è¯†è’¸é¦ä»Transformeræ•™å¸ˆæ¨¡å‹åˆ°å¤šç§æ¬¡äºŒæ¬¡å­¦ç”Ÿæ¨¡å‹çš„è¿ç§»æ€§ã€‚</li>
<li>ç ”ç©¶äº†ä¸åŒæ¶æ„çº¦æŸå¯¹è’¸é¦è¿‡ç¨‹çš„å½±å“ã€‚</li>
<li>æ™ºèƒ½åˆå§‹åŒ–ç­–ç•¥ï¼Œå¦‚çŸ©é˜µæ··åˆå’ŒQKVå¤åˆ¶ï¼Œå¯¹é€‚åº”è¿‡ç¨‹æœ‰é‡è¦å½±å“ã€‚</li>
<li>å®è¯ç»“æœæ­ç¤ºäº†æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14366">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b49fc8209c23a671594f0f6940bd5a39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-390f55ffa251de198c9f751a084dbc5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6881999fb9f0124b8c1948295df35047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1420fdf99b30b1c92f96063630d5c9b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Using-customized-GPT-to-develop-prompting-proficiency-in-architectural-AI-generated-images"><a href="#Using-customized-GPT-to-develop-prompting-proficiency-in-architectural-AI-generated-images" class="headerlink" title="Using customized GPT to develop prompting proficiency in architectural   AI-generated images"></a>Using customized GPT to develop prompting proficiency in architectural   AI-generated images</h2><p><strong>Authors:Juan David Salazar Rodriguez, Sam Conrad Joyce, Julfendi Julfendi</strong></p>
<p>This research investigates the use of customized GPT models to enhance prompting proficiency among architecture students when generating AI-driven images. Prompt engineering is increasingly essential in architectural education due to the widespread adoption of generative AI tools. This study utilized a mixed-methods experimental design involving architecture students divided into three distinct groups: a control group receiving no structured support, a second group provided with structured prompting guides, and a third group supported by both structured guides and interactive AI personas. Students engaged in reverse engineering tasks, first guessing provided image prompts and then generating their own prompts, aiming to boost critical thinking and prompting skills. Variables examined included time spent prompting, word count, prompt similarity, and concreteness. Quantitative analysis involved correlation assessments between these variables and a one-way ANOVA to evaluate differences across groups. While several correlations showed meaningful relationships, not all were statistically significant. ANOVA results indicated statistically significant improvements in word count, similarity, and concreteness, especially in the group supported by AI personas and structured prompting guides. Qualitative feedback complemented these findings, revealing enhanced confidence and critical thinking skills in students. These results suggest tailored GPT interactions substantially improve studentsâ€™ ability to communicate architectural concepts clearly and effectively. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å®šåˆ¶çš„GPTæ¨¡å‹åœ¨æé«˜å»ºç­‘å­¦ç”Ÿåœ¨ç”ŸæˆAIé©±åŠ¨å›¾åƒæ—¶çš„æç¤ºèƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚ç”±äºç”Ÿæˆå¼AIå·¥å…·çš„å¹¿æ³›åº”ç”¨ï¼Œæç¤ºå·¥ç¨‹åœ¨å»ºç­‘æ•™è‚²ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬ç ”ç©¶é‡‡ç”¨æ··åˆæ–¹æ³•å®éªŒè®¾è®¡ï¼Œå°†å»ºç­‘å­¦ç”Ÿåˆ†ä¸ºä¸‰ç»„ï¼šå¯¹ç…§ç»„æ— ç»“æ„åŒ–æ”¯æŒï¼Œç¬¬äºŒç»„æä¾›ç»“æ„åŒ–æç¤ºæŒ‡å—ï¼Œç¬¬ä¸‰ç»„ç”±ç»“æ„åŒ–æŒ‡å—å’Œäº¤äº’å¼AIäººæ ¼æä¾›æ”¯æŒã€‚å­¦ç”Ÿä»¬å‚ä¸äº†é€†å‘å·¥ç¨‹ä»»åŠ¡ï¼Œé¦–å…ˆçŒœæµ‹æä¾›çš„å›¾åƒæç¤ºï¼Œç„¶åç”Ÿæˆè‡ªå·±çš„æç¤ºï¼Œæ—¨åœ¨æé«˜æ‰¹åˆ¤æ€ç»´å’Œæç¤ºæŠ€èƒ½ã€‚ç ”ç©¶çš„å˜é‡åŒ…æ‹¬æç¤ºæ‰€èŠ±è´¹çš„æ—¶é—´ã€å­—æ•°ã€æç¤ºçš„ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§ã€‚å®šé‡åˆ†æåŒ…æ‹¬è¿™äº›å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§è¯„ä¼°ï¼Œä»¥åŠå•å› ç´ æ–¹å·®åˆ†æï¼Œä»¥è¯„ä¼°å„ç»„ä¹‹é—´çš„å·®å¼‚ã€‚è™½ç„¶æœ‰å‡ ä¸ªç›¸å…³æ€§æ˜¾ç¤ºå‡ºæœ‰æ„ä¹‰çš„å…³ç³»ï¼Œä½†å¹¶éæ‰€æœ‰å…³ç³»åœ¨ç»Ÿè®¡ä¸Šéƒ½æ˜¯æ˜¾è‘—çš„ã€‚æ–¹å·®åˆ†æç»“æœè¡¨æ˜ï¼Œåœ¨å­—æ•°ã€ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§æ–¹é¢æœ‰ç»Ÿè®¡æ˜¾è‘—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯é‚£äº›å—åˆ°AIäººæ ¼å’Œç»“æ„åŒ–æç¤ºæŒ‡å—æ”¯æŒçš„å°ç»„ã€‚å®šæ€§åé¦ˆè¡¥å……äº†è¿™äº›å‘ç°ï¼Œæ˜¾ç¤ºå­¦ç”Ÿçš„è‡ªä¿¡å’Œæ‰¹åˆ¤æ€ç»´èƒ½åŠ›æœ‰æ‰€æé«˜ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé‡èº«å®šåˆ¶çš„GPTäº’åŠ¨èƒ½æ˜¾è‘—æé«˜å­¦ç”Ÿæ¸…æ™°æœ‰æ•ˆåœ°ä¼ è¾¾å»ºç­‘æ¦‚å¿µçš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13948v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å®šåˆ¶GPTæ¨¡å‹åœ¨æé«˜å»ºç­‘ä¸“ä¸šå­¦ç”Ÿç”ŸæˆAIé©±åŠ¨å›¾åƒæ—¶çš„æç¤ºèƒ½åŠ›ã€‚éšç€ç”Ÿæˆå¼AIå·¥å…·çš„å¹¿æ³›åº”ç”¨ï¼Œæç¤ºå·¥ç¨‹åœ¨å»ºç­‘è®¾è®¡æ•™è‚²ä¸­çš„é‡è¦æ€§æ—¥ç›Šå¢åŠ ã€‚è¯¥ç ”ç©¶é‡‡ç”¨æ··åˆæ–¹æ³•å®éªŒè®¾è®¡ï¼Œå°†å»ºç­‘ä¸“ä¸šå­¦ç”Ÿåˆ†ä¸ºä¸‰ç»„ï¼šå¯¹ç…§ç»„æ— ç»“æ„åŒ–æ”¯æŒï¼Œç¬¬äºŒç»„æä¾›ç»“æ„åŒ–æç¤ºæŒ‡å—ï¼Œç¬¬ä¸‰ç»„ç”±ç»“æ„åŒ–æŒ‡å—å’Œäº¤äº’å¼AIäººæ ¼æä¾›æ”¯æŒã€‚å­¦ç”Ÿä»¬å‚ä¸é€†å‘å·¥ç¨‹ä»»åŠ¡ï¼Œé¦–å…ˆçŒœæµ‹æä¾›çš„å›¾åƒæç¤ºï¼Œç„¶åç”Ÿæˆè‡ªå·±çš„æç¤ºï¼Œæ—¨åœ¨æé«˜æ‰¹åˆ¤æ€ç»´å’Œæç¤ºæŠ€èƒ½ã€‚ç ”ç©¶çš„å˜é‡åŒ…æ‹¬æç¤ºèŠ±è´¹çš„æ—¶é—´ã€å­—æ•°ã€æç¤ºçš„ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§ã€‚å®šé‡åˆ†æäº†è¿™äº›å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶é€šè¿‡å•å‘æ–¹å·®åˆ†æè¯„ä¼°äº†å„ç»„ä¹‹é—´çš„å·®å¼‚ã€‚è™½ç„¶ä¸€äº›ç›¸å…³æ€§æ˜¾ç¤ºå‡ºæœ‰æ„ä¹‰çš„å…³ç³»ï¼Œä½†å¹¶ééƒ½å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚æ–¹å·®åˆ†æç»“æœæ˜¾ç¤ºï¼Œåœ¨å­—æ•°ã€ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§æ–¹é¢å­˜åœ¨ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ”¹å–„ï¼Œç‰¹åˆ«æ˜¯åœ¨AIäººæ ¼å’Œç»“æ„åŒ–æç¤ºæŒ‡å—çš„æ”¯æŒä¸‹ã€‚å®šæ€§åé¦ˆè¯å®äº†è¿™äº›å‘ç°ï¼Œå­¦ç”Ÿä»¬æ˜¾ç¤ºå‡ºå¢å¼ºçš„ä¿¡å¿ƒå’Œæ‰¹åˆ¤æ€ç»´èƒ½åŠ›ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå®šåˆ¶GPTäº¤äº’èƒ½æ˜¾è‘—æé«˜å­¦ç”Ÿæ¸…æ™°æœ‰æ•ˆåœ°ä¼ è¾¾å»ºç­‘æ¦‚å¿µçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶èšç„¦åœ¨åˆ©ç”¨å®šåˆ¶GPTæ¨¡å‹æå‡å»ºç­‘ä¸“ä¸šå­¦ç”Ÿä½¿ç”¨AIå·¥å…·æ—¶çš„æç¤ºèƒ½åŠ›ã€‚</li>
<li>å®éªŒä¸­ï¼Œå­¦ç”Ÿåˆ†ä¸ºä¸‰ç»„ï¼Œåˆ†åˆ«æ¥å—ä¸åŒæ°´å¹³çš„æ”¯æŒï¼ˆæ— æ”¯æŒã€ç»“æ„åŒ–æŒ‡å—æ”¯æŒã€ç»“æ„åŒ–æŒ‡å—ä¸AIäººæ ¼æ”¯æŒï¼‰ã€‚</li>
<li>å­¦ç”Ÿä»¬å‚ä¸åå‘å·¥ç¨‹ä»»åŠ¡ï¼Œé”»ç‚¼çŒœæµ‹å’Œç”Ÿæˆæç¤ºçš„èƒ½åŠ›ï¼Œæ—¨åœ¨æå‡æ‰¹åˆ¤æ€ç»´å’Œæç¤ºæŠ€èƒ½ã€‚</li>
<li>ç ”ç©¶å®šé‡åˆ†æäº†å­¦ç”Ÿæç¤ºèŠ±è´¹çš„æ—¶é—´ã€å­—æ•°ã€æç¤ºçš„ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§ç­‰å˜é‡ã€‚</li>
<li>æ–¹å·®åˆ†ææ˜¾ç¤ºï¼ŒAIæ”¯æŒå’Œç»“æ„åŒ–æŒ‡å—ä¸‹çš„å­¦ç”Ÿåœ¨å­—æ•°ã€æç¤ºçš„ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§æ–¹é¢æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>å®šæ€§åé¦ˆè¡¨æ˜ï¼Œå­¦ç”Ÿä»¬çš„è‡ªä¿¡å’Œæ‰¹åˆ¤æ€ç»´èƒ½åŠ›æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-162460206b5b463c0104c89e2656a7c8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Causal-Copilot-An-Autonomous-Causal-Analysis-Agent"><a href="#Causal-Copilot-An-Autonomous-Causal-Analysis-Agent" class="headerlink" title="Causal-Copilot: An Autonomous Causal Analysis Agent"></a>Causal-Copilot: An Autonomous Causal Analysis Agent</h2><p><strong>Authors:Xinyue Wang, Kun Zhou, Wenyi Wu, Har Simrat Singh, Fang Nan, Songyao Jin, Aryan Philip, Saloni Patnaik, Hou Zhu, Shivam Singh, Parjanya Prashant, Qian Shen, Biwei Huang</strong></p>
<p>Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data â€“ including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle â€“ expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at <a target="_blank" rel="noopener" href="https://causalcopilot.com/">https://causalcopilot.com/</a>. </p>
<blockquote>
<p>å› æœåˆ†æåœ¨ç§‘å­¦å‘ç°å’Œå¯é å†³ç­–åˆ¶å®šä¸­å‘æŒ¥ç€åŸºç¡€ä½œç”¨ï¼Œç„¶è€Œç”±äºå…¶æ¦‚å¿µå’Œç®—æ³•çš„å¤æ‚æ€§ï¼Œé¢†åŸŸä¸“å®¶å¾ˆéš¾è·å¾—è¿™ä¸€çŸ¥è¯†ã€‚å› æœæ–¹æ³•è®ºä¸å®é™…ä½¿ç”¨ä¹‹é—´çš„è„±èŠ‚å¸¦æ¥äº†åŒé‡æŒ‘æˆ˜ï¼šé¢†åŸŸä¸“å®¶æ— æ³•åˆ©ç”¨å› æœå­¦ä¹ æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œè€Œå› æœç ”ç©¶è€…ç¼ºä¹å¹¿æ³›çš„ç°å®ä¸–ç•Œéƒ¨ç½²æ¥æµ‹è¯•å’Œå®Œå–„ä»–ä»¬çš„æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å› æœCopilotï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶å†…æ“ä½œä¸“å®¶çº§å› æœåˆ†æçš„è‡ªä¸»ä½“ã€‚å› æœCopilotè‡ªåŠ¨å®Œæˆå› æœåˆ†æçš„æ•´ä¸ªæµç¨‹ï¼Œæ— è®ºæ˜¯è¡¨æ ¼æ•°æ®è¿˜æ˜¯æ—¶é—´åºåˆ—æ•°æ®ï¼ŒåŒ…æ‹¬å› æœå‘ç°ã€å› æœæ¨ç†ã€ç®—æ³•é€‰æ‹©ã€è¶…å‚æ•°ä¼˜åŒ–ã€ç»“æœè§£é‡Šå’Œç”Ÿæˆå¯æ“ä½œè§è§£ã€‚å®ƒé€šè¿‡è‡ªç„¶è¯­è¨€æ”¯æŒäº¤äº’å¼æ”¹è¿›ï¼Œé™ä½äº†éä¸“ä¸šäººå£«çš„é—¨æ§›ï¼ŒåŒæ—¶ä¿æŒäº†æ–¹æ³•çš„ä¸¥è°¨æ€§ã€‚é€šè¿‡é›†æˆè¶…è¿‡20é¡¹æœ€å…ˆè¿›çš„å› æœåˆ†ææŠ€æœ¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå½¢æˆäº†ä¸€ä¸ªè‰¯æ€§å¾ªç¯â€”â€”æ‰©å¤§é¢†åŸŸä¸“å®¶å¯¹é«˜çº§å› æœæ–¹æ³•çš„è®¿é—®æƒé™ï¼ŒåŒæ—¶ç”Ÿæˆä¸°å¯Œçš„ç°å®ä¸–ç•Œåº”ç”¨ç¨‹åºï¼Œä¸ºå› æœç†è®ºæä¾›ä¿¡æ¯å’Œä¿ƒè¿›å…¶å‘å±•ã€‚å®è¯ç ”ç©¶è¯æ˜ï¼Œä¸ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼Œå› æœCopilotæ€§èƒ½å“è¶Šï¼Œæä¾›äº†ä¸€ç§å¯é ã€å¯æ‰©å±•å’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œç¼©å°äº†ç†è®ºå¤æ‚æ€§å’Œç°å®ä¸–ç•Œåº”ç”¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚å› æœCopilotçš„å®æ—¶äº¤äº’å¼æ¼”ç¤ºç½‘ç«™ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://causalcopilot.com/]">https://causalcopilot.com/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13263v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å› æœåˆ†æçš„é‡è¦æ€§å’ŒæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å…¶åœ¨ç§‘å­¦å‘ç°å’Œå¯é å†³ç­–åˆ¶å®šä¸­çš„åŸºç¡€ä½œç”¨ï¼Œä»¥åŠé¢†åŸŸä¸“å®¶éš¾ä»¥åˆ©ç”¨å› æœå­¦ä¹ æ–¹æ³•çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºCausal-Copilotçš„è‡ªä¸»ä»£ç†æ–¹æ¡ˆï¼Œå¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶å†…è¿›è¡Œä¸“å®¶çº§åˆ«çš„å› æœåˆ†æã€‚è¯¥æ–¹æ¡ˆå¯è‡ªåŠ¨åŒ–å› æœåˆ†æçš„å…¨æµç¨‹ï¼ŒåŒ…æ‹¬å› æœå‘ç°ã€å› æœæ¨ç†ã€ç®—æ³•é€‰æ‹©ã€è¶…å‚æ•°ä¼˜åŒ–ã€ç»“æœè§£è¯»å’Œè¡ŒåŠ¨å»ºè®®ç”Ÿæˆç­‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ”¯æŒé€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤äº’å¼æ”¹è¿›ï¼Œé™ä½äº†éä¸“ä¸šäººå£«çš„é—¨æ§›ï¼ŒåŒæ—¶ä¿æŒäº†æ–¹æ³•è®ºä¸Šçš„ä¸¥è°¨æ€§ã€‚é€šè¿‡æ•´åˆè¶…è¿‡20é¡¹æœ€å…ˆè¿›çš„å› æœåˆ†ææŠ€æœ¯ï¼Œè¯¥ç³»ç»Ÿå½¢æˆäº†ä¸€ä¸ªè‰¯æ€§å¾ªç¯ï¼Œæ‰©å¤§äº†é¢†åŸŸä¸“å®¶å¯¹é«˜çº§å› æœæ–¹æ³•çš„è®¿é—®æƒé™ï¼ŒåŒæ—¶ç”Ÿæˆä¸°å¯Œçš„å®é™…åº”ç”¨æ¡ˆä¾‹ä»¥æ¨åŠ¨å› æœç†è®ºçš„å‘å±•ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒCausal-Copilotç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ¡ˆè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºå› æœåˆ†æé¢†åŸŸæ­å»ºäº†ç†è®ºå…ˆè¿›æ€§ä¸å®é™…åº”ç”¨ä¹‹é—´çš„æ¡¥æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å› æœåˆ†æåœ¨ç§‘å­¦å‘ç°å’Œå†³ç­–åˆ¶å®šä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†é¢†åŸŸä¸“å®¶éš¾ä»¥åˆ©ç”¨æœ€æ–°çš„å› æœå­¦ä¹ æ–¹æ³•ã€‚</li>
<li>Causal-Copilotè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡è‡ªä¸»ä»£ç†å®ç°äº†ä¸“å®¶çº§åˆ«çš„å› æœåˆ†æã€‚</li>
<li>Causal-Copilotå¯ä»¥è‡ªåŠ¨åŒ–å› æœåˆ†æçš„å…¨æµç¨‹ï¼Œä»å› æœå‘ç°åˆ°è¡ŒåŠ¨å»ºè®®ç”Ÿæˆã€‚</li>
<li>è¯¥ç³»ç»Ÿæ”¯æŒè‡ªç„¶è¯­è¨€äº¤äº’ï¼Œæ—¢ç®€åŒ–äº†éä¸“ä¸šäººå£«çš„ä½¿ç”¨éš¾åº¦ï¼Œåˆä¿æŒäº†ä¸“ä¸šä¸¥è°¨æ€§ã€‚</li>
<li>ç³»ç»Ÿæ•´åˆäº†å¤šç§æœ€å…ˆè¿›çš„å› æœåˆ†ææŠ€æœ¯ï¼Œä¿ƒè¿›äº†ç†è®ºä¸å®è·µçš„è‰¯æ€§å¾ªç¯ã€‚</li>
<li>Causal-Copilotåœ¨å®è¯è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰åŸºçº¿æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84f03acd7ba6d9be98256cc26190023d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61250f09e027593c1cd474189a2b0cd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Forecasting-from-Clinical-Textual-Time-Series-Adaptations-of-the-Encoder-and-Decoder-Language-Model-Families"><a href="#Forecasting-from-Clinical-Textual-Time-Series-Adaptations-of-the-Encoder-and-Decoder-Language-Model-Families" class="headerlink" title="Forecasting from Clinical Textual Time Series: Adaptations of the   Encoder and Decoder Language Model Families"></a>Forecasting from Clinical Textual Time Series: Adaptations of the   Encoder and Decoder Language Model Families</h2><p><strong>Authors:Shahriar Noroozizadeh, Sayantan Kumar, Jeremy C. Weiss</strong></p>
<p>Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings â€“ extracted via an LLM-assisted annotation pipeline â€“ serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use. </p>
<blockquote>
<p>ä¸´åºŠç—…ä¾‹æŠ¥å‘ŠåŒ…å«äº†ä¸°å¯Œçš„ã€éšæ—¶é—´å˜åŒ–çš„æ‚£è€…è½¨è¿¹ä¿¡æ¯ï¼Œè€Œä¼ ç»Ÿçš„ä¾èµ–ç»“æ„åŒ–æ•°æ®çš„æœºå™¨å­¦ä¹ æ–¹æ³•å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»æ–‡æœ¬æ—¶é—´åºåˆ—ä¸­å¼•å…¥äº†é¢„æµ‹é—®é¢˜ï¼Œå…¶ä¸­é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©çš„æ ‡æ³¨ç®¡é“æå–çš„æ—¶é—´æˆ³ä¸´åºŠå‘ç°ä½œä¸ºé¢„æµ‹çš„ä¸»è¦è¾“å…¥ã€‚æˆ‘ä»¬å¯¹ä¸€ç³»åˆ—æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼ŒåŒ…æ‹¬å¾®è°ƒè¿‡çš„åŸºäºè§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºç¼–ç å™¨çš„è½¬æ¢å™¨ï¼Œç”¨äºäº‹ä»¶å‘ç”Ÿçš„é¢„æµ‹ã€æ—¶é—´é¡ºåºå’Œç”Ÿå­˜åˆ†æä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºç¼–ç å™¨çš„æ¨¡å‹åœ¨çŸ­æœŸå’Œé•¿æœŸäº‹ä»¶é¢„æµ‹ä¸­å§‹ç»ˆè·å¾—æ›´é«˜çš„F1åˆ†æ•°å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œè€Œç»è¿‡å¾®è°ƒè¿‡çš„æ©ç æ–¹æ³•æé«˜äº†æ’åæ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„è§£ç å™¨æ¨¡å‹åœ¨ç”Ÿå­˜åˆ†æä¸­æ˜¾ç¤ºå‡ºç›¸å¯¹ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸé¢„åç¯å¢ƒä¸­ã€‚æˆ‘ä»¬çš„æ•æ„Ÿæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜æ—¶é—´é¡ºåºçš„é‡è¦æ€§ï¼Œè¿™éœ€è¦ä¸´åºŠæ—¶é—´åºåˆ—çš„æ„å»ºï¼Œä¸æ–‡æœ¬é¡ºåºç›¸å¯¹ï¼Œåè€…æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ç»å…¸è®­ç»ƒä¸­è¾“å…¥æ–‡æœ¬çš„æ ¼å¼ã€‚è¿™çªæ˜¾äº†ä»æ—¶é—´é¡ºåºè¯­æ–™åº“ä¸­è·å¾—çš„é¢å¤–å¥½å¤„ï¼Œå¯¹å¹¿æ³›ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ—¶ä»£çš„æ—¶åºä»»åŠ¡å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10340v2">PDF</a> Machine Learning for Healthcare (MLHC 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸´åºŠç—…ä¾‹æŠ¥å‘Šä¸­çš„æ—¶é—´åºè´¯æ€§æ•°æ®é¢„æµ‹é—®é¢˜ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©çš„æ ‡æ³¨ç®¡é“æå–çš„æ—¶é—´æˆ³ä¸´åºŠå‘ç°ä½œä¸ºä¸»è¦è¾“å…¥è¿›è¡Œé¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œç¼–ç å™¨æ¨¡å‹åœ¨äº‹ä»¶å‘ç”Ÿçš„çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä¸­ï¼ŒF1åˆ†æ•°è¾ƒé«˜ä¸”æ—¶é—´ä¸€è‡´æ€§æ›´ä¼˜ï¼›å¾®è°ƒåçš„æ©ç æ–¹æ³•èƒ½æé«˜æ’åºæ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒæŒ‡ä»¤å¾®è°ƒè§£ç å™¨æ¨¡å‹åœ¨ç”Ÿå­˜åˆ†æä¸­è¡¨ç°å‡ºç›¸å¯¹ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸé¢„åè®¾ç½®ä¸­ã€‚åŒæ—¶å¼ºè°ƒäº†æ—¶é—´é¡ºåºçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬è¾“å…¥æ ¼å¼ä¸Šæ›´ä¾§é‡äºæ–‡æœ¬é¡ºåºè€Œéæ—¶é—´é¡ºåºçš„é™åˆ¶ã€‚å› æ­¤ï¼Œæ„å»ºæ—¶é—´é¡ºåºè¯­æ–™åº“å¯¹äºæ—¶ä»£ä¸­æ—¶é—´ä»»åŠ¡å°¤ä¸ºé‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠç—…ä¾‹æŠ¥å‘ŠåŒ…å«ä¸°å¯Œçš„æ—¶åºæ€§æ‚£è€…è½¨è¿¹ä¿¡æ¯ï¼Œä½†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©æ ‡æ³¨ç®¡é“æå–æ—¶é—´æˆ³ä¸´åºŠå‘ç°ï¼Œä¸ºé¢„æµ‹æä¾›ä¸»è¦è¾“å…¥ã€‚</li>
<li>ç¼–ç å™¨æ¨¡å‹åœ¨äº‹ä»¶é¢„æµ‹ã€æ—¶åºæ’åºå’Œç”Ÿå­˜åˆ†æä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>å¾®è°ƒåçš„æ©ç æ–¹æ³•èƒ½æé«˜æ’åºæ€§èƒ½ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒè§£ç å™¨æ¨¡å‹åœ¨ç”Ÿå­˜åˆ†æä¸­å…·æœ‰ç›¸å¯¹ä¼˜åŠ¿ã€‚</li>
<li>æ—¶é—´é¡ºåºå¯¹é¢„æµ‹ç»“æœè‡³å…³é‡è¦ï¼Œéœ€è¦æ„å»ºä¸´åºŠæ—¶é—´åºåˆ—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4a74fc5667f3a1dacc8e6fbbfc84654.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67239e927ee377beb8f2e9f7ff182aec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e7e190e13e361146f311c6f16860043.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="STI-Bench-Are-MLLMs-Ready-for-Precise-Spatial-Temporal-World-Understanding"><a href="#STI-Bench-Are-MLLMs-Ready-for-Precise-Spatial-Temporal-World-Understanding" class="headerlink" title="STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World   Understanding?"></a>STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World   Understanding?</h2><p><strong>Authors:Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, Bo Zhao</strong></p>
<p>The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution for Embodied AI and Autonomous Driving has become a prevailing trend. While MLLMs have been extensively studied for visual semantic understanding tasks, their ability to perform precise and quantitative spatial-temporal understanding in real-world applications remains largely unexamined, leading to uncertain prospects. To evaluate modelsâ€™ Spatial-Temporal Intelligence, we introduce STI-Bench, a benchmark designed to evaluate MLLMsâ€™ spatial-temporal understanding through challenging tasks such as estimating and predicting the appearance, pose, displacement, and motion of objects. Our benchmark encompasses a wide range of robot and vehicle operations across desktop, indoor, and outdoor scenarios. The extensive experiments reveals that the state-of-the-art MLLMs still struggle in real-world spatial-temporal understanding, especially in tasks requiring precise distance estimation and motion analysis. </p>
<blockquote>
<p>ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºåµŒå…¥å¼äººå·¥æ™ºèƒ½å’Œè‡ªåŠ¨é©¾é©¶çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆå·²æˆä¸ºä¸€ç§æµè¡Œè¶‹åŠ¿ã€‚è™½ç„¶MLLMsåœ¨è§†è§‰è¯­ä¹‰ç†è§£ä»»åŠ¡æ–¹é¢å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†å®ƒä»¬åœ¨ç°å®åº”ç”¨ä¸­è¿›è¡Œç²¾ç¡®å’Œå®šé‡æ—¶ç©ºç†è§£çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ£€éªŒï¼Œå› æ­¤å­˜åœ¨ä¸ç¡®å®šçš„å‰æ™¯ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„æ—¶ç©ºæ™ºèƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†STI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡ä¼°è®¡å’Œé¢„æµ‹ç‰©ä½“çš„å¤–è§‚ã€å§¿æ€ã€ä½ç§»å’Œè¿åŠ¨ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡æ¥è¯„ä¼°MLLMsçš„æ—¶ç©ºç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†æ¡Œé¢ã€å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸­çš„å¹¿æ³›æœºå™¨äººå’Œè½¦è¾†æ“ä½œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„MLLMsåœ¨çœŸå®ä¸–ç•Œçš„æ—¶ç©ºç†è§£æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç¡®è·ç¦»ä¼°è®¡å’Œè¿åŠ¨åˆ†æçš„ä»»åŠ¡ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23765v3">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ™ºèƒ½ä½“å’Œè‡ªåŠ¨é©¾é©¶æä¾›äº†æµè¡Œè¶‹åŠ¿ã€‚è™½ç„¶MLLMsåœ¨è§†è§‰è¯­ä¹‰ç†è§£ä»»åŠ¡æ–¹é¢å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å®ç°ç²¾ç¡®å’Œå®šé‡æ—¶ç©ºç†è§£çš„èƒ½åŠ›ä»æœ‰å¾…ç ”ç©¶ï¼Œå¯¼è‡´å‰æ™¯ä¸æ˜æœ—ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„æ—¶ç©ºæ™ºèƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†STI-BenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºé€šè¿‡è¯¸å¦‚ä¼°è®¡å’Œé¢„æµ‹ç‰©ä½“çš„å¤–è§‚ã€å§¿æ€ã€ä½ç§»å’Œè¿åŠ¨ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡æ¥è¯„ä¼°MLLMsçš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•å¹³å°æ¶µç›–äº†æ¡Œé¢ã€å®¤å†…å’Œå®¤å¤–åœºæ™¯çš„å¹¿æ³›æœºå™¨äººå’Œè½¦è¾†æ“ä½œèŒƒå›´ã€‚å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„MLLMsåœ¨çœŸå®ä¸–ç•Œçš„æ—¶ç©ºç†è§£æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç¡®è·ç¦»ä¼°è®¡å’Œè¿åŠ¨åˆ†æçš„ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²æˆä¸ºæ™ºèƒ½ä½“å’Œè‡ªåŠ¨é©¾é©¶çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆçš„æµè¡Œè¶‹åŠ¿ã€‚</li>
<li>MLLMsåœ¨è§†è§‰è¯­ä¹‰ç†è§£æ–¹é¢å·²æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„æ—¶ç©ºç†è§£æ–¹é¢ä»å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>å¼•å…¥STI-BenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°MLLMsçš„æ—¶ç©ºç†è§£èƒ½åŠ›ã€‚</li>
<li>STI-Benchæ¶µç›–å„ç§æœºå™¨äººå’Œè½¦è¾†æ“ä½œä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¡Œé¢ã€å®¤å†…å’Œå®¤å¤–åœºæ™¯ã€‚</li>
<li>è¯„ä¼°åŒ…æ‹¬ä¼°è®¡å’Œé¢„æµ‹ç‰©ä½“çš„å¤–è§‚ã€å§¿æ€ã€ä½ç§»å’Œè¿åŠ¨ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºæœ€å…ˆè¿›çš„MLLMsåœ¨ç²¾ç¡®è·ç¦»ä¼°è®¡å’Œè¿åŠ¨åˆ†ææ–¹é¢ä»æœ‰å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23765">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-102ff4e7f4880f91ab33827ad6e00293.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640f11efd73c81fcbcabb10ff72d0d0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fa585f9874bad00b396113203c59bc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34d7f4df85f2110d9cd6811514f1da70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9aa73b2729f26067cdd1b132f9989cc9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3b7326e352312fc14c4f2e02b43f5add.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  FlowReasoner Reinforcing Query-Level Meta-Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2b2a88e5bcbadd645ade413c80e756a4.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  Stop Summation Min-Form Credit Assignment Is All Process Reward Model   Needs for Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
