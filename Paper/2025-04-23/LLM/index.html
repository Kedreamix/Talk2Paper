<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-04-23  Seeing from Another Perspective Evaluating Multi-View Understanding in   MLLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-fdf18cf6727a5ef0291e281ed754117b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-23-更新"><a href="#2025-04-23-更新" class="headerlink" title="2025-04-23 更新"></a>2025-04-23 更新</h1><h2 id="Seeing-from-Another-Perspective-Evaluating-Multi-View-Understanding-in-MLLMs"><a href="#Seeing-from-Another-Perspective-Evaluating-Multi-View-Understanding-in-MLLMs" class="headerlink" title="Seeing from Another Perspective: Evaluating Multi-View Understanding in   MLLMs"></a>Seeing from Another Perspective: Evaluating Multi-View Understanding in   MLLMs</h2><p><strong>Authors:Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Rouyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, Yi Ma</strong></p>
<p>Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model’s geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at <a target="_blank" rel="noopener" href="https://danielchyeh.github.io/All-Angles-Bench/">https://danielchyeh.github.io/All-Angles-Bench/</a>. </p>
<blockquote>
<p>多视角理解是在多模态大型语言模型（MLLMs）中作为实体代理应用时面临的一项基本挑战。这种能力能够在不同视角间协调视觉信息，以实现有效的导航、操作和3D场景理解。虽然最近的MLLMs在高层次推理和规划方面取得了令人印象深刻的进展，但在面对多视角几何一致性和跨视角对应问题时，它们经常显得能力不足。为了全面评估MLLMs在多视角场景推理方面的挑战，我们提出了全方位基准测试（All-Angles Bench），这是一个包含超过2100个人类精心标注的多视角问答对，涵盖90个多样化的真实世界场景。我们的六个任务（计数、属性识别、相对距离、相对方向、对象操作、相机姿态估计）专门测试模型的几何对应能力以及在各个视角间一致对齐信息的能力。我们对27个具有代表性的MLLMs进行了广泛的实验，包括Gemini-2.0-Flash、Claude-3.7-Sonnet和GPT-4与人类评估者的基准测试，揭示了显著的性能差距，这表明当前MLLMs距离人类水平还有很长的路要走。通过深入分析，我们发现在两个方面，MLLMs表现尤为不足：（1）对部分遮挡视图的跨视角对应；（2）建立粗略的相机姿态。这些发现强调了特定领域细化或嵌入更强多视角意识的模块必要性。我们相信，我们的全方位基准测试提供了宝贵的见解，并为弥合MLLMs和人类水平的多视角理解之间的差距做出贡献。该项目和基准测试可在<a target="_blank" rel="noopener" href="https://danielchyeh.github.io/All-Angles-Bench/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://danielchyeh.github.io/All-Angles-Bench/公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15280v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://danielchyeh.github.io/All-Angles-Bench/">https://danielchyeh.github.io/All-Angles-Bench/</a></p>
<p><strong>Summary</strong><br>     多模态大型语言模型（MLLMs）在处理多视角理解方面存在挑战，即融合不同视角的信息以实现有效的导航、操作和3D场景理解。提出All-Angles Bench基准测试，包含2,100多个经过人类精心标注的多视角问答对，以全面评估MLLMs在多视角场景推理方面的挑战。实验结果显示，当前MLLMs与人类水平存在显著性能差距，特别是在部分遮挡视角和粗略相机姿态估计方面。需要领域特定的优化或模块来增强多视角感知能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在多视角理解方面面临挑战，需要融合不同视角的信息以实现有效导航、操作和3D场景理解。</li>
<li>提出All-Angles Bench基准测试，用于全面评估MLLMs在多视角场景推理方面的性能。</li>
<li>MLLMs在跨视角对应和部分遮挡视图方面的表现较弱。</li>
<li>实验中包括多种任务，如计数、属性识别、相对距离、相对方向、对象操作、相机姿态估计等，专门测试模型的几何对应能力和跨视角信息一致性。</li>
<li>与人类评估者相比，当前MLLMs存在显著性能差距。</li>
<li>需要领域特定的优化或模块来增强MLLMs的多视角感知能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15280">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-de96a1b5391a6203e47a2141cee70711.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1c963fdb06bd0857edfb899ec883bdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fce2418249b6d6d9b3fb1e092e209fc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7bfb7d59d238a8750ac6d7ad442678bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9c365e592613d2fdcb393e94f0ce04c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Stop-Summation-Min-Form-Credit-Assignment-Is-All-Process-Reward-Model-Needs-for-Reasoning"><a href="#Stop-Summation-Min-Form-Credit-Assignment-Is-All-Process-Reward-Model-Needs-for-Reasoning" class="headerlink" title="Stop Summation: Min-Form Credit Assignment Is All Process Reward Model   Needs for Reasoning"></a>Stop Summation: Min-Form Credit Assignment Is All Process Reward Model   Needs for Reasoning</h2><p><strong>Authors:Jie Cheng, Ruixi Qiao, Lijun Li, Chao Guo, Junle Wang, Gang Xiong, Yisheng Lv, Fei-Yue Wang</strong></p>
<p>Process reward models (PRMs) have proven effective for test-time scaling of Large Language Models (LLMs) on challenging reasoning tasks. However, reward hacking issues with PRMs limit their successful application in reinforcement fine-tuning. In this paper, we identify the main cause of PRM-induced reward hacking: the canonical summation-form credit assignment in reinforcement learning (RL), which defines the value as cumulative gamma-decayed future rewards, easily induces LLMs to hack steps with high rewards. To address this, we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation of PURE is a min-form credit assignment that formulates the value function as the minimum of future rewards. This method significantly alleviates reward hacking by limiting the value function range and distributing advantages more reasonably. Through extensive experiments on 3 base models, we show that PRM-based approaches enabling min-form credit assignment achieve comparable reasoning performance to verifiable reward-based methods within only 30% steps. In contrast, the canonical sum-form credit assignment collapses training even at the beginning! Additionally, when we supplement PRM-based fine-tuning with just 10% verifiable rewards, we further alleviate reward hacking and produce the best fine-tuned model based on Qwen2.5-Math-7B in our experiments, achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5 benchmarks. Moreover, we summarize the observed reward hacking cases and analyze the causes of training collapse. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/CJReinforce/PURE">https://github.com/CJReinforce/PURE</a>. </p>
<blockquote>
<p>流程奖励模型（PRM）在挑战性推理任务中对大型语言模型（LLM）的测试时间扩展表现出了有效性。然而，PRM中的奖励破解问题限制了其在强化微调中的成功应用。在本文中，我们确定了PRM引起的奖励破解的主要原因：强化学习（RL）中的规范总和形式的信用分配，它将价值定义为累积的未来奖励的gamma衰减，容易诱导LLM破解高奖励的步骤。为了解决这个问题，我们提出了PURE：流程监督强化学习。PURE的关键创新之处在于最小形式的信用分配，它将价值函数制定为未来奖励的最小值。这种方法通过限制价值函数范围和更合理地分配优势来显著缓解奖励破解问题。通过在三套基础模型上的大量实验，我们证明了基于PRM的方法，使能最小形式的信用分配，在仅30%的步骤内实现了与可验证奖励方法相当的推理性能。相比之下，规范总和形式的信用分配甚至在训练开始时就会崩溃！此外，当我们仅在PRM微调中加入10%的可验证奖励时，我们进一步缓解了奖励破解问题，并在我们的实验中基于Qwen2.5-Math-7B生成了最佳微调模型，在AMC23上的准确率达到82.5%，在五个基准测试上的平均准确率为53.3%。此外，我们还总结了观察到的奖励破解案例，并分析了训练崩溃的原因。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/CJReinforce/PURE">https://github.com/CJReinforce/PURE</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了过程奖励模型（PRM）在大规模语言模型（LLM）测试时间扩展方面的有效性及其在强化微调中面临的奖励黑客攻击问题。针对PRM诱导的奖励黑客攻击的主要原因，即强化学习中的规范求和形式的信用分配，本文提出了PURE：过程监督强化学习。PURE的关键创新之处在于采用最小形式的信用分配，将价值函数定义为未来奖励的最小值，从而有效缓解奖励黑客攻击。实验表明，基于PRM的方法能够实现与可验证奖励方法相当的原理性能，并且在仅30%的步骤内实现。同时，当将基于PRM的微调与仅10%的可验证奖励相结合时，能够在实验中的Qwen2.5-Math-7B模型上获得最佳微调效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>过程奖励模型（PRM）在测试时间扩展LLM方面表现出有效性。</li>
<li>PRM在强化微调中面临奖励黑客攻击问题。</li>
<li>PRM诱导的奖励黑客攻击的主要原因是强化学习中的规范求和形式的信用分配。</li>
<li>提出了PURE：过程监督强化学习，采用最小形式的信用分配来缓解奖励黑客攻击。</li>
<li>基于PRM的方法在仅30%的步骤内实现了与可验证奖励方法相当的原理性能。</li>
<li>结合10%的可验证奖励与基于PRM的微调，在特定模型上获得了最佳微调效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d2f136d345fb5af45f095e0fd2b0afad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb3d2c25a98f8c0492e973cc72b2ad8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0266eddd8ad0fc488116fa4e89873aff.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CRUST-Bench-A-Comprehensive-Benchmark-for-C-to-safe-Rust-Transpilation"><a href="#CRUST-Bench-A-Comprehensive-Benchmark-for-C-to-safe-Rust-Transpilation" class="headerlink" title="CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation"></a>CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation</h2><p><strong>Authors:Anirudh Khatry, Robert Zhang, Jia Pan, Ziteng Wang, Qiaochu Chen, Greg Durrett, Isil Dillig</strong></p>
<p>C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at <a target="_blank" rel="noopener" href="https://github.com/anirudhkhatry/CRUST-bench">https://github.com/anirudhkhatry/CRUST-bench</a>. </p>
<blockquote>
<p>C到Rust的转译对于现代化遗留C代码至关重要，同时还可以提高与现代Rust生态系统的安全性和互操作性。然而，目前尚不存在用于评估系统是否能将C转译为安全Rust并能通过一组测试用例的数据集。我们引入了CRUST-Bench数据集，其中包含100个C代码库，每个库都配有人工编写的安全Rust接口以及可用于验证转译正确性的测试用例。通过考虑整个代码库而不是孤立的函数，CRUST-Bench捕捉到了翻译具有跨多个文件依赖关系的复杂项目所面临的挑战。所提供的Rust接口提供了明确的规范，确保遵循惯用且内存安全的Rust模式，而随附的测试用例则强制执行功能正确性。我们在此任务上评估了最新的大型语言模型（LLM），发现安全和惯用Rust的生成对于各种最新方法和技术来说仍然是一个具有挑战性的问题。我们还深入探讨了LLM在将C代码转译为安全Rust时通常出现的错误。表现最佳的模型OpenAI o1在单回合设置中仅能完成15项任务。在CRUST-Bench上的改进将导致更先进的转译系统，能够处理复杂场景并帮助将遗留代码库从C迁移到像Rust这样的确保内存安全的编程语言。您可以在<a target="_blank" rel="noopener" href="https://github.com/anirudhkhatry/CRUST-bench">https://github.com/anirudhkhatry/CRUST-bench</a>找到数据集和代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15254v1">PDF</a> </p>
<p><strong>摘要</strong><br>    C转Rust的转换是现代化遗留C代码的关键，能提高安全性和与Rust生态系统的互操作性。然而，当前缺乏评估系统能否将C安全转换为Rust的数据库集。我们推出CRUST-Bench数据集，包含100个C代码库，每个库都配备有手动编写的安全Rust接口和测试用例，用于验证转换的正确性。该数据集考虑整个代码库而非孤立函数，能反映翻译具有跨多个文件依赖的复杂项目的挑战。提供的Rust接口提供明确的规范，确保遵循习惯性和内存安全的Rust模式，而附带测试案例则强制实行功能正确性。我们评估了最新的大型语言模型（LLM）在这一任务上的表现，发现生成安全且习惯性的Rust代码仍然是各种最新方法和技术的挑战性问题。我们还提供了关于LLM在将代码从C转安全Rust时常见错误的见解。表现最佳的模型OpenAI o1仅能在单轮任务中解决15个任务。对CRUST-Bench的改进将导致能处理复杂场景的改进转换系统，有助于将遗留代码库从C迁移到确保内存安全的Rust等语言。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>C转Rust转换是现代化遗留C代码的关键过程，有助于提高安全性和与Rust生态系统的互操作性。</li>
<li>当前缺乏评估C转Rust转换系统性能的数据库集。</li>
<li>CRUST-Bench数据集包含100个C代码库，每个都配备有手动编写的安全Rust接口和测试用例，用于验证转换的正确性。</li>
<li>CRUST-Bench数据集能反映翻译具有跨多个文件依赖的复杂项目的挑战。</li>
<li>最新大型语言模型（LLM）在生成安全且习惯性的Rust代码方面存在挑战。</li>
<li>LLM在将代码从C转安全Rust时常见错误类型包括语法错误、语义误解和内存安全问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15254">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d3b161e1896babf1bd66ed4d9834198a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cf6d2d64f9e5e02787b119e21dd9f4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f5c3cabfcdf2ab9a52339626bccf0c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b97845c18b472f0dd1ee397910a0a88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d965328e1eed4e73d136237fffdd801.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evaluating-Judges-as-Evaluators-The-JETTS-Benchmark-of-LLM-as-Judges-as-Test-Time-Scaling-Evaluators"><a href="#Evaluating-Judges-as-Evaluators-The-JETTS-Benchmark-of-LLM-as-Judges-as-Test-Time-Scaling-Evaluators" class="headerlink" title="Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as   Test-Time Scaling Evaluators"></a>Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as   Test-Time Scaling Evaluators</h2><p><strong>Authors:Yilun Zhou, Austin Xu, Peifeng Wang, Caiming Xiong, Shafiq Joty</strong></p>
<p>Scaling test-time computation, or affording a generator large language model (LLM) extra compute during inference, typically employs the help of external non-generative evaluators (i.e., reward models). Concurrently, LLM-judges, models trained to generate evaluations and critiques (explanations) in natural language, are becoming increasingly popular in automatic evaluation. Despite judge empirical successes, their effectiveness as evaluators in test-time scaling settings is largely unknown. In this paper, we introduce the Judge Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge performance in three domains (math reasoning, code generation, and instruction following) under three task settings: response reranking, step-level beam search, and critique-based response refinement. We evaluate 10 different judge models (7B-70B parameters) for 8 different base generator models (6.7B-72B parameters). Our benchmark shows that while judges are competitive with outcome reward models in reranking, they are consistently worse than process reward models in beam search procedures. Furthermore, though unique to LLM-judges, their natural language critiques are currently ineffective in guiding the generator towards better responses. </p>
<blockquote>
<p>在测试时扩大计算规模或为大型语言模型（LLM）生成器在推理过程中提供额外的计算支持，通常借助外部非生成评估器（即奖励模型）的帮助。同时，训练以自然语言生成评估与批评（解释）的LLM判断模型在自动评估中越来越受欢迎。尽管判断模型在实际应用中有成功的经验，但作为测试时间规模设置中的评估者的有效性仍知之甚少。在本文中，我们介绍了测试时间规模判断（JETTS）基准测试，该测试在三个领域（数学推理、代码生成和指令遵循）下，对三种任务设置中的判断性能进行评价：响应重新排序、步骤级光束搜索和基于批评的响应优化。我们对8个不同基础生成模型的10个不同判断模型（参数从7B到70B）进行了评估。我们的基准测试表明，虽然在重新排序中，判断模型与结果奖励模型具有竞争力，但在光束搜索过程中，它们始终不如过程奖励模型。此外，虽然LLM判断具有独特性，但其自然语言批评目前在引导生成器生成更好响应方面效果不佳。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15253v1">PDF</a> The first two authors contributed equally. The codebase is at   <a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/jetts-benchmark">https://github.com/SalesforceAIResearch/jetts-benchmark</a></p>
<p><strong>Summary</strong></p>
<p>该文介绍了测试时间尺度上法官评价（LLM-judge）的表现。文中建立了一个名为JETTS的基准测试平台，以评估法官在三个领域（数学推理、代码生成和指令遵循）下的三种任务设置（响应重排、步骤级束搜索和基于批评的响应优化）中的表现。该研究评价了不同规模的法官模型和基础生成器模型的性能，发现法官在重排任务中与结果奖励模型竞争，但在束搜索过程中始终不如过程奖励模型。此外，尽管LLM法官的自然语言批评是独特的，但目前尚不能有效地指导生成器生成更好的响应。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时间尺度上引入LLM-judge评价，通过JETTS基准测试平台评估其在三个领域的表现。</li>
<li>法官模型在响应重排任务中与结果奖励模型表现相当。</li>
<li>在束搜索过程中，法官模型的表现始终不如过程奖励模型。</li>
<li>LLM-judge的自然语言批评在引导生成器生成更好响应方面目前尚不有效。</li>
<li>研究涉及多种规模的法官模型和基础生成器模型的性能评价。</li>
<li>JETTS基准测试平台可用于评估法官在不同任务设置和领域中的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d45b2cd9cb466616ee545d38873b2d83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-179c5acecbd726c45839edf647360039.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-726d440b0ee0d8db977f4997992b1e83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d34a9c3cb18b32082a911d94fee55f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78c37cb88e30b94872d511d6ae3d5b49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14e2ab4aa49a4aa7c2cf74593d3a5244.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MR-Guard-Multilingual-Reasoning-Guardrail-using-Curriculum-Learning"><a href="#MR-Guard-Multilingual-Reasoning-Guardrail-using-Curriculum-Learning" class="headerlink" title="MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning"></a>MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning</h2><p><strong>Authors:Yahan Yang, Soham Dan, Shuo Li, Dan Roth, Insup Lee</strong></p>
<p>Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation. </p>
<blockquote>
<p>大型语言模型（LLM）容易受到如越狱等对抗性攻击的影响，从而引发有害或不安全的行为。这种脆弱性在多语言环境中尤为加剧，因为多语言安全对齐的数据通常有限。因此，开发一种能够在多种语言中检测和过滤不安全内容的护栏，对于在现实世界应用中部署LLM至关重要。在这项工作中，我们提出了一种结合推理的多语言护栏构建方法。我们的方法包括：（1）合成多语言数据生成，融入文化和语言上的细微差异变体，（2）监督微调，以及（3）课程引导式群组相对策略优化（GRPO）框架，进一步提高性能。实验结果表明，我们的多语言护栏在域内和域外语言上均持续超越近期基线。我们的护栏具备多语言推理能力，能够生成多语言解释，对于理解多语言内容审核中的语言特定风险和模糊性特别有用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15241v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）易受到如越狱攻击等敌对攻击的影响，可能引发有害或不安全的行为。在多语言环境中，这种脆弱性因缺乏多语言安全对齐数据而加剧。因此，开发一种能够检测并过滤多种语言不安全内容的防护栏，对于在现实世界应用中部署LLMs至关重要。本研究提出了一种结合推理的多语言防护栏构建方法。该方法包括：（1）合成包含文化和语言细微变化的多语言数据生成，（2）监督微调，以及（3）课程引导式群组相对策略优化（GRPO）框架，进一步提高性能。实验结果表明，我们的多语言防护栏在域内和域外语言上均一致优于最近的基线。防护栏的多语言推理能力能够生成多语言解释，对于理解多语言内容管理中的语言特定风险和模糊性特别有用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在面临越狱攻击时存在安全隐患，可能产生有害行为。</li>
<li>多语言环境中LLMs的脆弱性因缺乏多语言安全数据而加剧。</li>
<li>开发能检测并过滤多语言不安全内容的防护栏对LLM的现实应用至关重要。</li>
<li>提出了一种多语言防护栏构建方法，包括数据生成、监督微调和GRPO框架。</li>
<li>多语言防护栏在域内和域外语言上的性能均优于基线。</li>
<li>防护栏具备多语言推理能力，能生成多语言解释。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15241">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-56ef2758021ba491a7c809fae596eb73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8a0dc7d6c6a338f0c4ecdb11f16a2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-276b9e5d9c5321fd3232e97bb04f7155.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdf18cf6727a5ef0291e281ed754117b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Integrating-Symbolic-Execution-into-the-Fine-Tuning-of-Code-Generating-LLMs"><a href="#Integrating-Symbolic-Execution-into-the-Fine-Tuning-of-Code-Generating-LLMs" class="headerlink" title="Integrating Symbolic Execution into the Fine-Tuning of Code-Generating   LLMs"></a>Integrating Symbolic Execution into the Fine-Tuning of Code-Generating   LLMs</h2><p><strong>Authors:Marina Sakharova, Abhinav Anand, Mira Mezini</strong></p>
<p>Code-generating Large Language Models (LLMs) have become essential tools in modern software development, enhancing productivity and accelerating development. This paper aims to investigate the fine-tuning of code-generating LLMs using Reinforcement Learning and Direct Preference Optimization, further improving their performance. To achieve this, we enhance the training data for the reward model with the help of symbolic execution techniques, ensuring more comprehensive and objective data. With symbolic execution, we create a custom dataset that better captures the nuances in code evaluation. Our reward models, fine-tuned on this dataset, demonstrate significant improvements over the baseline, CodeRL, in estimating the quality of generated code. Our code-generating LLMs, trained with the help of reward model feedback, achieve similar results compared to the CodeRL benchmark. </p>
<blockquote>
<p>代码生成大型语言模型（LLM）已成为现代软件开发中的必备工具，提高了生产力并加速了开发进程。本文旨在研究使用强化学习和直接偏好优化对代码生成LLM进行微调，以进一步提高其性能。为此，我们借助符号执行技术增强奖励模型的训练数据，确保更全面、客观的数据。通过符号执行，我们创建了一个自定义数据集，更好地捕捉代码评估中的细微差别。在此数据集上微调的奖励模型在估计生成代码的质量方面相比基线CodeRL显示出显着改进。借助奖励模型反馈进行训练的代码生成LLM达到了与CodeRL基准相似的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15210v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>代码生成大型语言模型（LLM）在现代软件开发中已成为必不可少的工具，可以提高生产力和加快开发速度。本文旨在研究使用强化学习和直接偏好优化对代码生成LLM进行微调，以进一步提高其性能。我们通过符号执行技术增强奖励模型的训练数据，确保更全面、客观的数据。符号执行帮助我们创建一个自定义数据集，更好地捕捉代码评估的细微差别。在自定义数据集上微调奖励模型，在估计生成代码的质量方面，相比基线CodeRL，显示出显著改进。借助奖励模型反馈训练的代码生成LLM，达到了与CodeRL基准相似的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码生成LLM在现代软件开发中的重要性：提高生产力和加速开发。</li>
<li>本文研究使用强化学习和直接偏好优化对代码生成LLM进行微调。</li>
<li>符号执行技术用于增强奖励模型的训练数据。</li>
<li>符号执行创建自定义数据集，更准确地捕捉代码评估的细微差别。</li>
<li>奖励模型在估计生成代码质量方面相比CodeRL有显著改善。</li>
<li>通过奖励模型反馈训练的代码生成LLM达到与CodeRL基准相似的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15210">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0e0dcae67f64ff167803b27f51c3f5d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-643cf2206bf9bc7b6bc8ea859f2a1ccc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b8e833454e9a59d09ef135011062c69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b294a8b2c773f17222b35d9f16945aae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59d867c50175c7acfa87afef42c1313a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Compute-Optimal-LLMs-Provably-Generalize-Better-With-Scale"><a href="#Compute-Optimal-LLMs-Provably-Generalize-Better-With-Scale" class="headerlink" title="Compute-Optimal LLMs Provably Generalize Better With Scale"></a>Compute-Optimal LLMs Provably Generalize Better With Scale</h2><p><strong>Authors:Marc Finzi, Sanyam Kapoor, Diego Granziol, Anming Gu, Christopher De Sa, J. Zico Kolter, Andrew Gordon Wilson</strong></p>
<p>Why do larger language models generalize better? To investigate this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function. This generalization bound can be decomposed into three interpretable components: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate. As compute-optimal language models are scaled up, the number of parameters per data point remains constant; however, both the loss variance and the quantization error decrease, implying that larger models should have smaller generalization gaps. We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows more slowly than their capacity on the compute-optimal frontier. From these findings we produce a scaling law for the generalization gap, with bounds that become predictably stronger with scale. </p>
<blockquote>
<p>为什么更大的语言模型能够更好地泛化？为了研究这个问题，我们在计算最优状态下对大型语言模型（LLM）的预训练目标制定了泛化边界，如金雀花扩展定律所述。我们引入了一种新颖的、完全基于经验的Freedman型马尔可夫浓度不等式，它通过考虑损失函数的方差来收紧现有边界。这个泛化边界可以分解为三个可解释的成分：每标记的参数数量、损失方差和在固定比特率下的量化误差。在计算最优语言模型时，每数据点的参数数量保持不变；然而，损失方差和量化误差都会减少，这意味着更大的模型应该具有较小的泛化差距。我们从信息理论的角度分析为什么更大的模型更容易量化，显示它们整合新信息的速率在最优计算边界上的增长比其容量要慢。根据这些发现，我们为泛化差距制定了一个扩展定律，随着规模的扩大，这些边界变得更加可预测和牢固。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15208v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型的泛化能力为何更强？本文通过对大型语言模型（LLM）的预训练目标进行泛化边界研究，探讨了这一问题。在Chinchilla规模定律描述的计算机最优状态下，我们引入了一种全新的、完全实证的Freedman型马尔可夫浓度不等式，该不等式通过考虑损失函数的方差来缩小现有边界。这个泛化边界可以分解为三个可解释的成分：每令牌的参数数量、损失方差和在固定比特率下的量化误差。随着计算机最优语言模型的扩展，每数据点的参数数量保持不变，但损失方差和量化误差均减少，这意味着更大的模型应具有较小的泛化误差。我们从信息理论的角度分析为何更大的模型更容易量化，并展示了它们整合新信息的速度与计算最优边界上的容量之间的关系。基于这些发现，我们为泛化误差提出了一个规模定律，随着规模的扩大，边界变得更具预测性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的泛化能力得益于对预训练目标的泛化边界研究。</li>
<li>引入了Freedman型马尔可夫浓度不等式来改进现有边界。</li>
<li>泛化边界包括每令牌的参数数量、损失方差和量化误差三个可解释的成分。</li>
<li>随着语言模型的扩展，损失方差和量化误差减少。</li>
<li>大型模型更容易量化，从信息理论角度分析了其原因。</li>
<li>大型语言模型整合新信息的速度与容量之间的关系被揭示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15208">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8965566ddf9742a7173f67d81d0aaf15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec0ece45d3ba8a1a6403bda24ab6ac19.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Synergistic-Weak-Strong-Collaboration-by-Aligning-Preferences"><a href="#Synergistic-Weak-Strong-Collaboration-by-Aligning-Preferences" class="headerlink" title="Synergistic Weak-Strong Collaboration by Aligning Preferences"></a>Synergistic Weak-Strong Collaboration by Aligning Preferences</h2><p><strong>Authors:Yizhu Jiao, Xuchao Zhang, Zhaoyang Wang, Yubo Ma, Zhun Deng, Rujia Wang, Chetan Bansal, Saravan Rajmohan, Jiawei Han, Huaxiu Yao</strong></p>
<p>Current Large Language Models (LLMs) excel in general reasoning yet struggle with specialized tasks requiring proprietary or domain-specific knowledge. Fine-tuning large models for every niche application is often infeasible due to black-box constraints and high computational overhead. To address this, we propose a collaborative framework that pairs a specialized weak model with a general strong model. The weak model, tailored to specific domains, produces initial drafts and background information, while the strong model leverages its advanced reasoning to refine these drafts, extending LLMs’ capabilities to critical yet specialized tasks. To optimize this collaboration, we introduce a collaborative feedback to fine-tunes the weak model, which quantifies the influence of the weak model’s contributions in the collaboration procedure and establishes preference pairs to guide preference tuning of the weak model. We validate our framework through experiments on three domains. We find that the collaboration significantly outperforms each model alone by leveraging complementary strengths. Moreover, aligning the weak model with the collaborative preference further enhances overall performance. </p>
<blockquote>
<p>当前的大型语言模型（LLM）在一般推理方面表现出色，但在需要专业知识或特定领域知识的专业化任务方面却遇到了困难。由于黑箱约束和计算开销高昂，为每一个专业应用微调大型模型通常并不可行。为了解决这一问题，我们提出了一种协作框架，该框架将一个专业化的弱模型与一个通用的强模型配对。弱模型针对特定领域进行定制，用于生成初步草案和背景信息，而强模型则利用其先进的推理能力对这些草案进行改进，从而将LLM的能力扩展到关键但专业化的任务。为了优化这种协作，我们引入了一种协作反馈来微调弱模型，该反馈量化弱模型在协作过程中的贡献，并建立偏好配对来指导弱模型的偏好调整。我们在三个领域进行实验验证我们的框架。我们发现，通过利用各自的互补优势，这种协作方式显著优于单独使用任一模型。此外，使弱模型与协作偏好保持一致，可进一步提高整体性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15188v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLM）在通用推理方面表现出色，但在需要专业知识或特定领域知识的专业领域任务方面存在挑战。由于黑箱约束和计算开销的限制，为每一个专业领域应用微调大型模型通常并不可行。为解决此问题，我们提出了一种协作框架，将专业领域的弱模型与通用领域的强模型配对。弱模型负责生成初步草案和背景信息，而强模型则利用其高级推理能力进行改进，从而扩展LLM在关键专业领域任务上的能力。为优化这一协作过程，我们引入了协作反馈来微调弱模型，量化弱模型在协作过程中的贡献，并建立偏好配对以指导弱模型的偏好调整。通过实验验证，该协作框架在三个领域中的表现均显著优于单一模型。进一步与协作偏好对齐的弱模型，更提高了整体性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在通用推理方面表现出色，但在专业领域任务上遇到困难。</li>
<li>提出一种协作框架，结合弱模型和强模型的优点来解决这一问题。</li>
<li>弱模型针对特定领域生成初步草案和背景信息。</li>
<li>强模型利用高级推理能力对弱模型生成的初步成果进行改进。</li>
<li>引入协作反馈机制来微调弱模型，提高其性能。</li>
<li>量化弱模型在协作中的贡献，并建立偏好配对以优化协作过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15188">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aa7f32fd1a2ded60edd4c1cb023c16d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b94379c9d3860ee630e6256856754362.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7364d178c21d76e69f8e849f94b21fdc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DSPO-Direct-Semantic-Preference-Optimization-for-Real-World-Image-Super-Resolution"><a href="#DSPO-Direct-Semantic-Preference-Optimization-for-Real-World-Image-Super-Resolution" class="headerlink" title="DSPO: Direct Semantic Preference Optimization for Real-World Image   Super-Resolution"></a>DSPO: Direct Semantic Preference Optimization for Real-World Image   Super-Resolution</h2><p><strong>Authors:Miaomiao Cai, Simiao Li, Wei Li, Xudong Huang, Hanting Chen, Jie Hu, Yunhe Wang</strong></p>
<p>Recent advances in diffusion models have improved Real-World Image Super-Resolution (Real-ISR), but existing methods lack human feedback integration, risking misalignment with human preference and may leading to artifacts, hallucinations and harmful content generation. To this end, we are the first to introduce human preference alignment into Real-ISR, a technique that has been successfully applied in Large Language Models and Text-to-Image tasks to effectively enhance the alignment of generated outputs with human preferences. Specifically, we introduce Direct Preference Optimization (DPO) into Real-ISR to achieve alignment, where DPO serves as a general alignment technique that directly learns from the human preference dataset. Nevertheless, unlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR are difficult to reconcile with the image-level preferences of DPO, which can lead to the DPO being overly sensitive to local anomalies, leading to reduced generation quality. To resolve this dichotomy, we propose Direct Semantic Preference Optimization (DSPO) to align instance-level human preferences by incorporating semantic guidance, which is through two strategies: (a) semantic instance alignment strategy, implementing instance-level alignment to ensure fine-grained perceptual consistency, and (b) user description feedback strategy, mitigating hallucinations through semantic textual feedback on instance-level images. As a plug-and-play solution, DSPO proves highly effective in both one-step and multi-step SR frameworks. </p>
<blockquote>
<p>最新的扩散模型进展已经提高了真实世界图像超分辨率（Real-ISR）的效果，但现有方法缺乏人类反馈整合，存在与人类偏好不一致的风险，并可能导致出现伪影、幻觉和有害内容生成。为此，我们首次将人类偏好对齐技术引入Real-ISR。该技术已成功应用于大型语言模型和文本到图像任务，可有效提高生成输出与人类偏好的对齐程度。具体来说，我们将直接偏好优化（DPO）引入Real-ISR以实现对齐，其中DPO作为一种通用的对齐技术，直接从人类偏好数据集中学习。然而，不同于高级任务，Real-ISR的像素级重建目标与DPO的图像级偏好很难协调，这可能导致DPO对局部异常过于敏感，从而降低生成质量。为了解决这一矛盾，我们提出直接语义偏好优化（DSPO），通过融入语义指导来对齐实例级人类偏好，这通过两个策略来实现：（a）语义实例对齐策略，实现实例级对齐以确保精细的感知一致性；（b）用户描述反馈策略，通过实例级图像的语义文本反馈来缓解幻觉。作为一种即插即用的解决方案，DSPO在单步和多步SR框架中都证明了其高度有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15176v1">PDF</a> </p>
<p><strong>Summary</strong><br>    最新扩散模型改进了真实世界图像超分辨率技术（Real-ISR），但现有方法未融入人类反馈，可能导致与人为偏好的错位以及图像生成时产生的伪影、幻象和有害内容。为此，我们首次将人类偏好对齐技术引入Real-ISR，该技术已成功应用于大型语言模型和文本到图像任务，能有效提升生成输出与人为偏好的对齐度。我们引入直接偏好优化（DPO）实现对齐，它是一种通用对齐技术，可直接从人类偏好数据集中学习。然而，不同于高级任务，Real-ISR的像素级重建目标与DPO的图像级偏好难以协调，可能导致DPO过于敏感于局部异常，降低生成质量。为解决这一矛盾，我们提出直接语义偏好优化（DSPO），通过融入语义指导来对齐实例级人为偏好，包括两个策略：一是语义实例对齐策略，实现实例级对齐以确保精细粒度感知一致性；二是用户描述反馈策略，通过实例级图像的语义文本反馈来减少幻象。DSPO作为一种即插即用解决方案，在单步和多步SR框架中都表现出高度有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的最新进展已改进了Real-ISR技术，但缺乏人类反馈集成可能导致与人为偏好的错位及生成问题。</li>
<li>人类偏好对齐技术首次被引入Real-ISR，提升生成输出与人为偏好的对齐度。</li>
<li>直接偏好优化（DPO）是一种通用对齐技术，可从人类偏好数据集中学习。</li>
<li>Real-ISR的像素级重建目标与DPO的图像级偏好存在协调困难，可能导致DPO过于敏感。</li>
<li>直接语义偏好优化（DSPO）通过融入语义指导解决上述问题，实现对实例级的偏好对齐。</li>
<li>DSPO包含两个策略：语义实例对齐和用户描述反馈策略，分别确保感知一致性和减少幻象。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15176">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5f8c7d046bf5732c6b24f67c04bcca98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b0157178f017a2ad3b8d8c69f41821e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b55ed96bfa69e3a68a99e512026c58.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-Synthetic-Imputation-Approach-Generating-Optimal-Synthetic-Texts-For-Underrepresented-Categories-In-Supervised-Classification-Tasks"><a href="#The-Synthetic-Imputation-Approach-Generating-Optimal-Synthetic-Texts-For-Underrepresented-Categories-In-Supervised-Classification-Tasks" class="headerlink" title="The Synthetic Imputation Approach: Generating Optimal Synthetic Texts   For Underrepresented Categories In Supervised Classification Tasks"></a>The Synthetic Imputation Approach: Generating Optimal Synthetic Texts   For Underrepresented Categories In Supervised Classification Tasks</h2><p><strong>Authors:Joan C. Timoneda</strong></p>
<p>Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa, require that all categories in an annotation task be sufficiently represented in the training data for optimal performance. However, it is often difficult to find sufficient examples for all categories in a task when building a high-quality training set. In this article, I describe this problem and propose a solution, the synthetic imputation approach. Leveraging a generative LLM (GPT-4o), this approach generates synthetic texts based on careful prompting and five original examples drawn randomly with replacement from the sample. This approach ensures that new synthetic texts are sufficiently different from the original texts to reduce overfitting, but retain the underlying substantive meaning of the examples to maximize out-of-sample performance. With 75 original examples or more, synthetic imputation’s performance is on par with a full sample of original texts, and overfitting remains low, predictable and correctable with 50 original samples. The synthetic imputation approach provides a novel role for generative LLMs in research and allows applied researchers to balance their datasets for best performance. </p>
<blockquote>
<p>编码器-解码器大型语言模型（LLM），如BERT和RoBERTa，要求在标注任务的类别在训练数据中能够得到充分的表示，以实现最佳性能。然而，在构建高质量训练集时，通常很难为任务中的所有类别找到足够的示例。在本文中，我描述了这个问题并提出了一个解决方案，即合成插补方法。该方法利用生成式LLM（GPT-4o）生成基于精心提示的合成文本，并使用五个随机抽取（允许重复）的原始示例。这种方法确保新生成的合成文本与原始文本足够不同，以减少过度拟合，同时保留示例的基本实际意义，以最大化样本外的性能。使用75个原始示例或更多时，合成插补方法的性能与完整样本的原始文本相当，过度拟合仍然保持低水平、可预测并可借助原始样本进行修正。合成插补方法为生成式LLM的研究提供了一个新的角色，并允许应用研究人员为了获得最佳性能而平衡他们的数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15160v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了在大规模语言模型（LLM）的标注任务中遇到的类别数据不充分的问题。作者提出了合成填充方法来解决这个问题，利用生成式LLM（如GPT-4o）基于精心设计的提示和五个原始样本生成合成文本。这种方法确保了新的合成文本与原始文本足够不同，以减少过度拟合，同时保留样本的实质性意义，以最大化样本外的性能。使用75个以上的原始样本时，合成填充的性能与全样本的原始文本相当，并且过度拟合保持在低水平且可预测和可纠正。合成填充方法为生成式LLM在研究中提供了新的作用，并允许应用研究人员平衡他们的数据集以获得最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模语言模型（LLM）在标注任务中需要充分代表所有类别以达到最佳性能。</li>
<li>当构建高质量训练集时，为所有类别找到足够的例子通常很困难。</li>
<li>合成填充方法通过使用生成式LLM（如GPT-4o）来解决这个问题。</li>
<li>该方法基于精心设计的提示和五个随机选择的原始样本生成合成文本。</li>
<li>合成文本与原始文本足够不同，以减少过度拟合，同时保留样本的实质性意义。</li>
<li>使用75个以上的原始样本时，合成填充的性能与全样本的原始文本相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7659cfb8f7f571b4ae59195d29fb531a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="KGMEL-Knowledge-Graph-Enhanced-Multimodal-Entity-Linking"><a href="#KGMEL-Knowledge-Graph-Enhanced-Multimodal-Entity-Linking" class="headerlink" title="KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking"></a>KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking</h2><p><strong>Authors:Juyeon Kim, Geon Lee, Taeuk Kim, Kijung Shin</strong></p>
<p>Entity linking (EL) aligns textual mentions with their corresponding entities in a knowledge base, facilitating various applications such as semantic search and question answering. Recent advances in multimodal entity linking (MEL) have shown that combining text and images can reduce ambiguity and improve alignment accuracy. However, most existing MEL methods overlook the rich structural information available in the form of knowledge-graph (KG) triples. In this paper, we propose KGMEL, a novel framework that leverages KG triples to enhance MEL. Specifically, it operates in three stages: (1) Generation: Produces high-quality triples for each mention by employing vision-language models based on its text and images. (2) Retrieval: Learns joint mention-entity representations, via contrastive learning, that integrate text, images, and (generated or KG) triples to retrieve candidate entities for each mention. (3) Reranking: Refines the KG triples of the candidate entities and employs large language models to identify the best-matching entity for the mention. Extensive experiments on benchmark datasets demonstrate that KGMEL outperforms existing methods. Our code and datasets are available at: <a target="_blank" rel="noopener" href="https://github.com/juyeonnn/KGMEL">https://github.com/juyeonnn/KGMEL</a>. </p>
<blockquote>
<p>实体链接（EL）将文本提及与知识库中的相应实体对齐，促进了语义搜索和问答等各种应用。多模态实体链接（MEL）的最新进展表明，结合文本和图像可以减少歧义，提高对齐准确性。然而，大多数现有的MEL方法忽视了知识图谱（KG）三元组中丰富的结构信息。在本文中，我们提出了KGMEL，这是一个利用KG三元组增强MEL的新型框架。具体来说，它分为三个阶段：（1）生成：通过基于文本和图像的视觉语言模型生成高质量的三元组。（2）检索：通过对比学习学习联合提及实体表示，将文本、图像和（生成或知识图谱）三元组合并，以检索每个提及的候选实体。（3）重新排序：精炼候选实体的KG三元组，并利用大型语言模型识别与提及内容最佳匹配的实体。在基准数据集上的广泛实验表明，KGMEL优于现有方法。我们的代码和数据集可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/juyeonnn/KGMEL%E3%80%82">https://github.com/juyeonnn/KGMEL。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15135v1">PDF</a> SIGIR 2025 (Short)</p>
<p><strong>Summary</strong><br>文本介绍了实体链接（EL）和多模态实体链接（MEL）的基本概念及其在知识图谱（KG）中的应用。然而，现有的MEL方法忽略了知识图谱中的丰富结构信息。本文提出了一种新的框架KGMEL，利用KG三元组来增强MEL性能。KGMEL包含三个主要阶段：生成高质量的三元组，学习联合提及实体表示以及重新排序候选实体以识别最佳匹配实体。实验证明，KGMEL在基准数据集上的表现优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>实体链接（EL）是将文本中的提及与知识库中的对应实体进行对齐的技术，多模态实体链接（MEL）结合了文本和图像来减少歧义并提高对齐准确性。</li>
<li>现有MEL方法忽视了知识图谱（KG）中的结构信息。</li>
<li>KGMEL是一个新的框架，利用KG三元组来增强MEL性能。</li>
<li>KGMEL包含三个阶段：生成高质量三元组、学习联合提及实体表示和重新排序候选实体。</li>
<li>KGMEL通过视觉语言模型和对比学习技术生成和检索高质量的三元组。</li>
<li>在基准数据集上的实验证明，KGMEL的性能优于现有的MEL方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15135">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0013392c1ed80c0ef27aa8843df5325e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2670bac72d41d31f5f403076d79a1678.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3303a6e7d03959f1e7bc2a5411f509a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94021f898603f6d781977bc43dbd4a87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81ec62b0be4373a9b32fff9c7e265b02.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EasyEdit2-An-Easy-to-use-Steering-Framework-for-Editing-Large-Language-Models"><a href="#EasyEdit2-An-Easy-to-use-Steering-Framework-for-Editing-Large-Language-Models" class="headerlink" title="EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language   Models"></a>EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language   Models</h2><p><strong>Authors:Ziwen Xu, Shuxun Wang, Kewei Xu, Haoming Xu, Mengru Wang, Xinle Deng, Yunzhi Yao, Guozhou Zheng, Huajun Chen, Ningyu Zhang</strong></p>
<p>In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model’s behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model’s responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/EasyEdit">https://github.com/zjunlp/EasyEdit</a> along with a demonstration notebook. In addition, we provide a demo video at <a target="_blank" rel="noopener" href="https://zjunlp.github.io/project/EasyEdit2/video">https://zjunlp.github.io/project/EasyEdit2/video</a> for a quick introduction. </p>
<blockquote>
<p>本文介绍了EasyEdit2，这是一个旨在实现对大型语言模型（LLM）行为进行即插即用调整的框架。EasyEdit2支持广泛的测试时间干预，包括安全、情感、个性、推理模式、真实性和语言特征。不同于其前身，EasyEdit2采用专门设计用于无缝模型引导的新架构。它包含关键模块，如引导向量生成器和引导向量应用器，能够自动生成和应用引导向量，以影响模型的行为，而无需修改其参数。EasyEdit2的主要优点之一是使用方便-用户无需具备广泛的技术知识。只需一个示例，他们就可以有效地引导和调整模型的响应，使精确控制变得既方便又高效。我们通过实验报告了在不同LLM上的模型引导性能，证明了这些技术的有效性。我们已在GitHub上发布了源代码，网址为<a target="_blank" rel="noopener" href="https://github.com/zjunlp/EasyEdit%EF%BC%8C%E5%B9%B6%E9%99%84%E5%B8%A6%E4%B8%80%E4%B8%AA%E6%BC%94%E7%A4%BA%E7%AC%94%E8%AE%B0%E6%9C%AC%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E6%88%91%E4%BB%AC%E8%BF%98%E6%8F%90%E4%BE%9B%E4%BA%86https://zjunlp.github.io/project/EasyEdit2/video%E6%BC%94%E7%A4%BA%E8%A7%86%E9%A2%91%EF%BC%8C%E4%BB%A5%E4%BE%9B%E5%BF%AB%E9%80%9F%E4%BB%8B%E7%BB%8D%E3%80%82">https://github.com/zjunlp/EasyEdit，并附带一个演示笔记本。此外，我们还提供了https://zjunlp.github.io/project/EasyEdit2/video演示视频，以供快速介绍。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15133v1">PDF</a> Work in progress. Demo:   <a target="_blank" rel="noopener" href="https://zjunlp.github.io/project/EasyEdit2/video">https://zjunlp.github.io/project/EasyEdit2/video</a>; code:   <a target="_blank" rel="noopener" href="https://github.com/zjunlp/EasyEdit">https://github.com/zjunlp/EasyEdit</a></p>
<p><strong>Summary</strong></p>
<p>EasyEdit2框架旨在实现即插即用可调整的大型语言模型（LLM）行为控制。它支持广泛的安全、情感、个性、推理模式、事实和语言特征的测试时间干预。新架构具备无缝模型转向功能，包含转向矢量生成器和转向矢量应用器等关键模块，可自动生成和应用转向矢量以影响模型行为，无需修改参数。EasyEdit2易于使用，用户无需深入了解技术细节，只需一个示例即可有效引导和调整模型响应。经验报告显示在不同LLM中模型转向性能效果显著。源码已发布在GitHub上，并附有演示笔记本和视频介绍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EasyEdit2是一个用于控制大型语言模型行为的框架。</li>
<li>它支持多种测试时间干预，包括安全、情感、个性等。</li>
<li>新架构具有无缝模型转向功能。</li>
<li>含有转向矢量生成器和应用器等关键模块。</li>
<li>EasyEdit2能自动生成和应用转向矢量，无需修改模型参数。</li>
<li>该框架易于使用，用户只需简单示例即可调整模型响应。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15133">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dc68d74c6c4f92dc5b012e51ea599177.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02fc5dfb3e8cd3ac6ec012446f84238d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-841d6bd06c7da009ea2859bb8f5dc03f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95c4000f5d11ae57e98c5bd8f9fe36b4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Kuwain-1-5B-An-Arabic-SLM-via-Language-Injection"><a href="#Kuwain-1-5B-An-Arabic-SLM-via-Language-Injection" class="headerlink" title="Kuwain 1.5B: An Arabic SLM via Language Injection"></a>Kuwain 1.5B: An Arabic SLM via Language Injection</h2><p><strong>Authors:Khalil Hennara, Sara Chrouf, Mohamed Motaism Hamed, Zeina Aldallal, Omar Hadid, Safwan AlModhayan</strong></p>
<p>Enhancing existing models with new knowledge is a crucial aspect of AI development. This paper introduces a novel method for integrating a new language into a large language model (LLM). Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge. We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English. Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the model’s existing knowledge with a minimum amount of the original model’s data. This offers a cost-effective alternative to training a comprehensive model in both English and Arabic. The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes. </p>
<blockquote>
<p>在人工智能发展中，用新知识增强现有模型是一个至关重要的方面。本文介绍了一种将新语言集成到大型语言模型（LLM）中的新方法。我们的方法能够成功地将一种先前未见的目标语言（阿拉伯语言）并入到现有LLM中，而不会损害其原有知识。我们通过在主要为英语的小型开源模型中注入阿拉伯语，训练了一个名为“库瓦因”（Kuwain）的小型模型，该模型拥有1.5亿参数。我们的方法在阿拉伯语性能上取得了显著的提升，在各种基准测试中平均提高了8%，同时保留了模型原有的知识并使用了最小的原始模型数据量。这为在英语和阿拉伯语方面进行全面模型训练提供了经济高效的替代方案。结果突显了高效、有针对性的语言模型扩展的潜力，无需进行大规模重新训练或资源密集型过程。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15120v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种将新语言融入大型语言模型（LLM）的新方法。该方法能够在不损失原有知识的前提下，成功将之前未见的目标语言融入现有LLM中。通过向主要英语训练的小型开源模型中注入阿拉伯语，训练出一个仅有1.5亿参数的微型模型——Kuwain。该方法在阿拉伯语表现上显著提高，在各种基准测试中平均提高8%，并且仅使用少量原始模型数据就保留住了模型的现有知识。这为在英语和阿拉伯语全面训练模型提供了一种成本效益高的替代方案。结果突显了高效、有针对性的语言模型扩展的潜力，无需大量重新训练或资源密集型过程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种将新语言融入大型语言模型的新方法。</li>
<li>该方法能够在不损失原有知识的前提下融入新语言。</li>
<li>通过向主要英语训练的小型开源模型中注入阿拉伯语，创建了一个名为Kuwain的微型模型。</li>
<li>Kuwain模型在阿拉伯语表现上显著提高，平均提高8%。</li>
<li>该方法使用少量原始模型数据就实现了知识的保留。</li>
<li>这为在英语和阿拉伯语全面训练模型提供了成本效益高的替代方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15120">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77fbd4a091c46f8f8edcd350e9b69379.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4d969dbb8119bae0bec53d9ee0b2c18.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Empowering-AI-to-Generate-Better-AI-Code-Guided-Generation-of-Deep-Learning-Projects-with-LLMs"><a href="#Empowering-AI-to-Generate-Better-AI-Code-Guided-Generation-of-Deep-Learning-Projects-with-LLMs" class="headerlink" title="Empowering AI to Generate Better AI Code: Guided Generation of Deep   Learning Projects with LLMs"></a>Empowering AI to Generate Better AI Code: Guided Generation of Deep   Learning Projects with LLMs</h2><p><strong>Authors:Chen Xie, Mingsheng Jiao, Xiaodong Gu, Beijun Shen</strong></p>
<p>While large language models (LLMs) have been widely applied to code generation, they struggle with generating entire deep learning projects, which are characterized by complex structures, longer functions, and stronger reliance on domain knowledge than general-purpose code. An open-domain LLM often lacks coherent contextual guidance and domain expertise for specific projects, making it challenging to produce complete code that fully meets user requirements.   In this paper, we propose a novel planning-guided code generation method, DLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a structured solution plan, offering global guidance for LLMs to generate the project. The generated plan is then leveraged to retrieve semantically analogous code samples and subsequently abstract a code template. To effectively integrate these multiple retrieval-augmented techniques, a comparative learning mechanism is designed to generate the final code. We validate the effectiveness of our approach on a dataset we build for deep learning code generation. Experimental results demonstrate that DLCodeGen outperforms other baselines, achieving improvements of 9.7% in CodeBLEU and 3.6% in human evaluation metrics. </p>
<blockquote>
<p>虽然大型语言模型（LLM）已经广泛应用于代码生成，但它们在生成整个深度学习项目时面临挑战。深度学习项目具有复杂结构、更长的功能和比通用代码更强烈的领域知识依赖。开放领域的LLM通常缺乏连贯的上下文指导和特定项目的专业知识，因此难以生成完全满足用户需求的完整代码。在本文中，我们提出了一种新型的规划引导代码生成方法DLCodeGen，专门用于生成深度学习项目。DLCodeGen预测结构化解决方案计划，为LLM生成项目提供全局指导。生成的计划随后被用来检索语义相似的代码样本，然后抽象出代码模板。为了有效地整合这些多重检索增强技术，设计了一种对比学习机制来生成最终代码。我们在为深度学习代码生成构建的数据集上验证了我们的方法的有效性。实验结果表明，DLCodeGen优于其他基线方法，在CodeBLEU指标上提高了9.7%，在人类评价指标上提高了3.6%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）在代码生成中的广泛应用，针对生成整个深度学习项目所面临的挑战，本文提出了一种新型规划引导的代码生成方法DLCodeGen。DLCodeGen能够预测结构化解决方案计划，为LLM生成项目提供全局指导。通过利用生成的计划，检索语义相似的代码样本并抽象出代码模板。为了有效整合多种检索增强技术，设计了一种对比学习机制以生成最终代码。在自建的深度学习代码生成数据集上进行验证，实验结果表明DLCodeGen优于其他基线方法，在CodeBLEU指标上提高了9.7%，在人类评估指标上提高了3.6%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在代码生成方面已得到广泛应用，但在生成深度学习项目时面临挑战，因为这些项目具有复杂结构、更长的功能和更强的领域知识依赖。</li>
<li>DLCodeGen是一种针对深度学习项目的规划引导代码生成方法，能够预测结构化解决方案计划，为LLM生成项目提供全局指导。</li>
<li>DLCodeGen通过利用生成的计划，结合检索增强技术，从语义相似的代码样本中抽象出代码模板。</li>
<li>为了整合多种检索增强技术，DLCodeGen设计了一种对比学习机制。</li>
<li>实验结果表明，DLCodeGen在深度学习代码生成方面优于其他基线方法。</li>
<li>DLCodeGen在CodeBLEU指标上提高了9.7%，显示出其在代码生成方面的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cc444001ee4e6a50a72d7349fc378212.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1b301abb20029797ed06a74f71beb4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eba3d370b5b4b527a583673ef770508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-006e88705bbb1290ba734f618ddcba3e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ParaPO-Aligning-Language-Models-to-Reduce-Verbatim-Reproduction-of-Pre-training-Data"><a href="#ParaPO-Aligning-Language-Models-to-Reduce-Verbatim-Reproduction-of-Pre-training-Data" class="headerlink" title="ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of   Pre-training Data"></a>ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of   Pre-training Data</h2><p><strong>Authors:Tong Chen, Faeze Brahman, Jiacheng Liu, Niloofar Mireshghallah, Weijia Shi, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi</strong></p>
<p>Language models (LMs) can memorize and reproduce segments from their pretraining data verbatim even in non-adversarial settings, raising concerns about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to reduce unintentional regurgitation while preserving their overall utility. ParaPO trains LMs to prefer paraphrased versions of memorized segments over the original verbatim content from the pretraining data. To maintain the ability to recall famous quotations when appropriate, we develop a variant of ParaPO that uses system prompts to control regurgitation behavior. In our evaluation on Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative writing), whereas unlearning methods used in prior work to mitigate regurgitation are less effective outside their targeted unlearned domain (from 17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO with system prompting successfully preserves famous quotation recall while reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the model not to regurgitate produces only a marginal reduction (8.7 to 8.4). </p>
<blockquote>
<p>语言模型（LM）能够在非对抗性环境中逐字逐句地记忆和复制其预训练数据中的片段，这引发了人们对版权、剽窃、隐私和创造力的担忧。我们引入了Paraphrase Preference Optimization（ParaPO）这一后训练法，对语言模型进行微调，以减少无意识的复述现象，同时保留其整体效用。ParaPO训练语言模型优先选择对记忆片段进行同义改写，而非使用预训练数据中的原始内容。为了保持适时回忆著名引语的能力，我们开发了一种使用系统提示来控制复述行为的ParaPO变体。在对Llama3.1-8B的评估中，ParaPO在所有测试数据集上都能持续减少复述现象（例如，在创造性写作方面将复述指标从17.3降至12.9），而先前工作中用于缓解复述的去学习法在其未针对的领域外效果较差（从17.3降至16.9）。当应用于指令调优的Tulu3-8B模型时，带有系统提示的ParaPO成功保留了著名引语的回忆能力，同时减少了无意识的复述现象（在创造性写作方面从8.7降至6.3），当提示不要复述时效果更佳。相比之下，没有使用ParaPO调优，仅提示模型不要复述，效果仅略有降低（从8.7降至8.4）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14452v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了语言模型（LMs）即使在非对抗性环境中也能记忆并再现预训练数据中的片段，引发了关于版权、剽窃、隐私和创造力的担忧。为此，文章提出了一种名为Paraphrase Preference Optimization（ParaPO）的后训练法，旨在通过微调LM来减少无意识的复述，同时保持其整体效用。ParaPO训练LM更倾向于使用预训练数据中记忆片段的同义版本而非原样内容。为了能在适当时候回忆著名引文，文章还开发了带有系统提示功能的ParaPO变体。在Llama3.1-8B上的评估显示，ParaPO在所有测试数据集上都能有效减少复述（例如，在创造性写作中将复述指标从17.3降至12.9），而先前工作中用于减轻复述的遗忘方法在非目标遗忘领域之外的效果较差（从17.3降至16.9）。当应用于指令调整的Tulu3-8B模型时，带有系统提示的ParaPO成功保留了著名引文的回忆，同时减少了无意识的复述（在创造性写作中从8.7降至6.3），而不使用ParaPO提示模型减少复述只产生轻微的效果（从8.7降至8.4）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型（LMs）能记忆并再现预训练数据中的片段，引发关于版权、剽窃等问题的担忧。</li>
<li>提出Paraphrase Preference Optimization（ParaPO）后训练法，旨在减少LM的无意识复述，同时保持其整体效能。</li>
<li>ParaPO通过训练LM更倾向于使用同义版本而非原样内容来减少复述。</li>
<li>开发了带有系统提示功能的ParaPO变体，以在适当时候回忆著名引文。</li>
<li>在Llama3.1-8B上的评估显示，ParaPO能有效减少复述。</li>
<li>与先前的遗忘方法相比，ParaPO在减少复述方面表现出更好的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14452">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-86b0d1a6a7e7972b2f833622d88053c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f59ac590ac858aa610f7d265d2504092.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1eeec3091d1d92bb0016a6233de89af5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a02b6d208c8a4f3dad67ac556d6cd3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Empirical-Evaluation-of-Knowledge-Distillation-from-Transformers-to-Subquadratic-Language-Models"><a href="#Empirical-Evaluation-of-Knowledge-Distillation-from-Transformers-to-Subquadratic-Language-Models" class="headerlink" title="Empirical Evaluation of Knowledge Distillation from Transformers to   Subquadratic Language Models"></a>Empirical Evaluation of Knowledge Distillation from Transformers to   Subquadratic Language Models</h2><p><strong>Authors:Patrick Haller, Jonas Golde, Alan Akbik</strong></p>
<p>Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures, leveraging softmax attention for sequence modeling. However, the quadratic complexity of self-attention at inference time remains a significant bottleneck, motivating the exploration of subquadratic alternatives such as structured state-space models (SSMs), linear attention, and recurrent architectures. In this work, we systematically evaluate the transferability of knowledge distillation from a Transformer teacher to nine subquadratic student architectures. Our study aims to determine which subquadratic model best aligns with the teacher’s learned representations and how different architectural constraints influence the distillation process. We also investigate the impact of intelligent initialization strategies, including matrix mixing and query-key-value (QKV) copying, on the adaptation process. Our empirical results on multiple NLP benchmarks provide insights into the trade-offs between efficiency and performance, highlighting key factors for successful knowledge transfer to subquadratic architectures. </p>
<blockquote>
<p>知识蒸馏是一种广泛应用于压缩大型语言模型（LLM）的技术，通过训练较小的学生模型来模仿较大的教师模型。通常，教师和学生的架构都是基于Transformer的，利用softmax注意力进行序列建模。然而，推理时自注意力的二次复杂性仍然是显著瓶颈，这促使人们探索次二次替代方案，如结构化状态空间模型（SSMs）、线性注意力和循环架构。在这项工作中，我们系统地评估了从Transformer教师到九种子二次学生架构的知识蒸馏的迁移性。我们的研究旨在确定哪个子二次模型最能与教师的已学表示对齐，以及不同的架构约束如何影响蒸馏过程。我们还研究了智能初始化策略，包括矩阵混合和查询-键值（QKV）复制对适应过程的影响。我们在多个NLP基准测试上的实证结果为效率和性能之间的权衡提供了见解，并突出了成功转移知识到子二次架构的关键因素。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14366v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>知识蒸馏是一种广泛用于压缩大型语言模型（LLM）的技术，通过训练小型的学生模型来模仿大型的教师模型。通常，教师和学生都是基于Transformer的架构，利用softmax注意力进行序列建模。然而，自注意力在推理时间上的二次复杂性仍是显著瓶颈，促使人们探索次二次替代方案，如结构化状态空间模型（SSM）、线性注意力和递归架构。在这项工作中，我们系统地评估了从Transformer教师到九种子二次学生架构的知识蒸馏的迁移性。我们的研究旨在确定哪个次二次模型最能与教师的表示对齐，以及不同的架构约束如何影响蒸馏过程。我们还研究了智能初始化策略，包括矩阵混合和查询-键-值（QKV）复制对适应过程的影响。我们在多个NLP基准测试上的实证结果为效率和性能之间的权衡提供了见解，并突出了成功将知识转移到次二次架构的关键因素。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>知识蒸馏是压缩大型语言模型的有效技术，通过训练小型模型来模仿大型模型。</li>
<li>目前存在推理时间自注意力的二次复杂性瓶颈，需要探索次二次模型替代方案。</li>
<li>系统评估了知识蒸馏从Transformer教师模型到多种次二次学生模型的迁移性。</li>
<li>研究了不同架构约束对蒸馏过程的影响。</li>
<li>智能初始化策略，如矩阵混合和QKV复制，对适应过程有重要影响。</li>
<li>实证结果揭示了效率和性能之间的权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14366">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b49fc8209c23a671594f0f6940bd5a39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-390f55ffa251de198c9f751a084dbc5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6881999fb9f0124b8c1948295df35047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1420fdf99b30b1c92f96063630d5c9b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Using-customized-GPT-to-develop-prompting-proficiency-in-architectural-AI-generated-images"><a href="#Using-customized-GPT-to-develop-prompting-proficiency-in-architectural-AI-generated-images" class="headerlink" title="Using customized GPT to develop prompting proficiency in architectural   AI-generated images"></a>Using customized GPT to develop prompting proficiency in architectural   AI-generated images</h2><p><strong>Authors:Juan David Salazar Rodriguez, Sam Conrad Joyce, Julfendi Julfendi</strong></p>
<p>This research investigates the use of customized GPT models to enhance prompting proficiency among architecture students when generating AI-driven images. Prompt engineering is increasingly essential in architectural education due to the widespread adoption of generative AI tools. This study utilized a mixed-methods experimental design involving architecture students divided into three distinct groups: a control group receiving no structured support, a second group provided with structured prompting guides, and a third group supported by both structured guides and interactive AI personas. Students engaged in reverse engineering tasks, first guessing provided image prompts and then generating their own prompts, aiming to boost critical thinking and prompting skills. Variables examined included time spent prompting, word count, prompt similarity, and concreteness. Quantitative analysis involved correlation assessments between these variables and a one-way ANOVA to evaluate differences across groups. While several correlations showed meaningful relationships, not all were statistically significant. ANOVA results indicated statistically significant improvements in word count, similarity, and concreteness, especially in the group supported by AI personas and structured prompting guides. Qualitative feedback complemented these findings, revealing enhanced confidence and critical thinking skills in students. These results suggest tailored GPT interactions substantially improve students’ ability to communicate architectural concepts clearly and effectively. </p>
<blockquote>
<p>本研究探讨了使用定制的GPT模型在提高建筑学生在生成AI驱动图像时的提示能力方面的应用。由于生成式AI工具的广泛应用，提示工程在建筑教育中变得越来越重要。本研究采用混合方法实验设计，将建筑学生分为三组：对照组无结构化支持，第二组提供结构化提示指南，第三组由结构化指南和交互式AI人格提供支持。学生们参与了逆向工程任务，首先猜测提供的图像提示，然后生成自己的提示，旨在提高批判思维和提示技能。研究的变量包括提示所花费的时间、字数、提示的相似性和具体性。定量分析包括这些变量之间的相关性评估，以及单因素方差分析，以评估各组之间的差异。虽然有几个相关性显示出有意义的关系，但并非所有关系在统计上都是显著的。方差分析结果表明，在字数、相似性和具体性方面有统计显著的改进，特别是那些受到AI人格和结构化提示指南支持的小组。定性反馈补充了这些发现，显示学生的自信和批判思维能力有所提高。这些结果表明，量身定制的GPT互动能显著提高学生清晰有效地传达建筑概念的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13948v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了使用定制GPT模型在提高建筑专业学生生成AI驱动图像时的提示能力。随着生成式AI工具的广泛应用，提示工程在建筑设计教育中的重要性日益增加。该研究采用混合方法实验设计，将建筑专业学生分为三组：对照组无结构化支持，第二组提供结构化提示指南，第三组由结构化指南和交互式AI人格提供支持。学生们参与逆向工程任务，首先猜测提供的图像提示，然后生成自己的提示，旨在提高批判思维和提示技能。研究的变量包括提示花费的时间、字数、提示的相似性和具体性。定量分析了这些变量之间的相关性，并通过单向方差分析评估了各组之间的差异。虽然一些相关性显示出有意义的关系，但并非都具有统计学意义。方差分析结果显示，在字数、相似性和具体性方面存在统计学上的显著改善，特别是在AI人格和结构化提示指南的支持下。定性反馈证实了这些发现，学生们显示出增强的信心和批判思维能力。这些结果表明，定制GPT交互能显著提高学生清晰有效地传达建筑概念的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究聚焦在利用定制GPT模型提升建筑专业学生使用AI工具时的提示能力。</li>
<li>实验中，学生分为三组，分别接受不同水平的支持（无支持、结构化指南支持、结构化指南与AI人格支持）。</li>
<li>学生们参与反向工程任务，锻炼猜测和生成提示的能力，旨在提升批判思维和提示技能。</li>
<li>研究定量分析了学生提示花费的时间、字数、提示的相似性和具体性等变量。</li>
<li>方差分析显示，AI支持和结构化指南下的学生在字数、提示的相似性和具体性方面有显著改善。</li>
<li>定性反馈表明，学生们的自信和批判思维能力有所提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13948">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-162460206b5b463c0104c89e2656a7c8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Causal-Copilot-An-Autonomous-Causal-Analysis-Agent"><a href="#Causal-Copilot-An-Autonomous-Causal-Analysis-Agent" class="headerlink" title="Causal-Copilot: An Autonomous Causal Analysis Agent"></a>Causal-Copilot: An Autonomous Causal Analysis Agent</h2><p><strong>Authors:Xinyue Wang, Kun Zhou, Wenyi Wu, Har Simrat Singh, Fang Nan, Songyao Jin, Aryan Philip, Saloni Patnaik, Hou Zhu, Shivam Singh, Parjanya Prashant, Qian Shen, Biwei Huang</strong></p>
<p>Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data – including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle – expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at <a target="_blank" rel="noopener" href="https://causalcopilot.com/">https://causalcopilot.com/</a>. </p>
<blockquote>
<p>因果分析在科学发现和可靠决策制定中发挥着基础作用，然而由于其概念和算法的复杂性，领域专家很难获得这一知识。因果方法论与实际使用之间的脱节带来了双重挑战：领域专家无法利用因果学习方面的最新进展，而因果研究者缺乏广泛的现实世界部署来测试和完善他们的方法。为了解决这一问题，我们引入了因果Copilot，这是一个在大语言模型框架内操作专家级因果分析的自主体。因果Copilot自动完成因果分析的整个流程，无论是表格数据还是时间序列数据，包括因果发现、因果推理、算法选择、超参数优化、结果解释和生成可操作见解。它通过自然语言支持交互式改进，降低了非专业人士的门槛，同时保持了方法的严谨性。通过集成超过20项最先进的因果分析技术，我们的系统形成了一个良性循环——扩大领域专家对高级因果方法的访问权限，同时生成丰富的现实世界应用程序，为因果理论提供信息和促进其发展。实证研究证明，与现有基准相比，因果Copilot性能卓越，提供了一种可靠、可扩展和可扩展的解决方案，缩小了理论复杂性和现实世界应用之间的鸿沟。因果Copilot的实时交互式演示网站为：[<a target="_blank" rel="noopener" href="https://causalcopilot.com/]">https://causalcopilot.com/]</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13263v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了因果分析的重要性和挑战，包括其在科学发现和可靠决策制定中的基础作用，以及领域专家难以利用因果学习方法的问题。为解决这一问题，文章提出了一种名为Causal-Copilot的自主代理方案，可在大型语言模型框架内进行专家级别的因果分析。该方案可自动化因果分析的全流程，包括因果发现、因果推理、算法选择、超参数优化、结果解读和行动建议生成等。此外，它还支持通过自然语言进行交互式改进，降低了非专业人士的门槛，同时保持了方法论上的严谨性。通过整合超过20项最先进的因果分析技术，该系统形成了一个良性循环，扩大了领域专家对高级因果方法的访问权限，同时生成丰富的实际应用案例以推动因果理论的发展。实证评估表明，Causal-Copilot相较于现有基线方案表现出卓越性能，为因果分析领域搭建了理论先进性与实际应用之间的桥梁。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>因果分析在科学发现和决策制定中具有重要作用，但领域专家难以利用最新的因果学习方法。</li>
<li>Causal-Copilot解决了这一问题，通过自主代理实现了专家级别的因果分析。</li>
<li>Causal-Copilot可以自动化因果分析的全流程，从因果发现到行动建议生成。</li>
<li>该系统支持自然语言交互，既简化了非专业人士的使用难度，又保持了专业严谨性。</li>
<li>系统整合了多种最先进的因果分析技术，促进了理论与实践的良性循环。</li>
<li>Causal-Copilot在实证评估中表现出卓越性能，超越了现有基线方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13263">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84f03acd7ba6d9be98256cc26190023d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61250f09e027593c1cd474189a2b0cd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Forecasting-from-Clinical-Textual-Time-Series-Adaptations-of-the-Encoder-and-Decoder-Language-Model-Families"><a href="#Forecasting-from-Clinical-Textual-Time-Series-Adaptations-of-the-Encoder-and-Decoder-Language-Model-Families" class="headerlink" title="Forecasting from Clinical Textual Time Series: Adaptations of the   Encoder and Decoder Language Model Families"></a>Forecasting from Clinical Textual Time Series: Adaptations of the   Encoder and Decoder Language Model Families</h2><p><strong>Authors:Shahriar Noroozizadeh, Sayantan Kumar, Jeremy C. Weiss</strong></p>
<p>Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings – extracted via an LLM-assisted annotation pipeline – serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use. </p>
<blockquote>
<p>临床病例报告包含了丰富的、随时间变化的患者轨迹信息，而传统的依赖结构化数据的机器学习方法往往未能充分利用这些信息。在这项研究中，我们从文本时间序列中引入了预测问题，其中通过大型语言模型辅助的标注管道提取的时间戳临床发现作为预测的主要输入。我们对一系列模型进行了系统评估，包括微调过的基于解码器的大型语言模型和基于编码器的转换器，用于事件发生的预测、时间顺序和生存分析任务。我们的实验表明，基于编码器的模型在短期和长期事件预测中始终获得更高的F1分数和时间一致性，而经过微调过的掩码方法提高了排名性能。相比之下，经过指令训练的解码器模型在生存分析中显示出相对优势，特别是在早期预后环境中。我们的敏感性分析进一步表明时间顺序的重要性，这需要临床时间序列的构建，与文本顺序相对，后者是大型语言模型经典训练中输入文本的格式。这突显了从时间顺序语料库中获得的额外好处，对广泛使用大型语言模型的时代的时序任务具有启示意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10340v2">PDF</a> Machine Learning for Healthcare (MLHC 2025)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了临床病例报告中的时间序贯性数据预测问题。利用大型语言模型辅助的标注管道提取的时间戳临床发现作为主要输入进行预测。实验表明，编码器模型在事件发生的短期和长期预测中，F1分数较高且时间一致性更优；微调后的掩码方法能提高排序性能。相比之下，指令微调解码器模型在生存分析中表现出相对优势，特别是在早期预后设置中。同时强调了时间顺序的重要性，并指出传统的大型语言模型在文本输入格式上更侧重于文本顺序而非时间顺序的限制。因此，构建时间顺序语料库对于时代中时间任务尤为重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>临床病例报告包含丰富的时序性患者轨迹信息，但传统机器学习方法往往未能充分利用。</li>
<li>利用大型语言模型辅助标注管道提取时间戳临床发现，为预测提供主要输入。</li>
<li>编码器模型在事件预测、时序排序和生存分析任务中表现优越。</li>
<li>微调后的掩码方法能提高排序性能。</li>
<li>指令微调解码器模型在生存分析中具有相对优势。</li>
<li>时间顺序对预测结果至关重要，需要构建临床时间序列。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10340">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d4a74fc5667f3a1dacc8e6fbbfc84654.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67239e927ee377beb8f2e9f7ff182aec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e7e190e13e361146f311c6f16860043.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="STI-Bench-Are-MLLMs-Ready-for-Precise-Spatial-Temporal-World-Understanding"><a href="#STI-Bench-Are-MLLMs-Ready-for-Precise-Spatial-Temporal-World-Understanding" class="headerlink" title="STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World   Understanding?"></a>STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World   Understanding?</h2><p><strong>Authors:Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, Bo Zhao</strong></p>
<p>The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution for Embodied AI and Autonomous Driving has become a prevailing trend. While MLLMs have been extensively studied for visual semantic understanding tasks, their ability to perform precise and quantitative spatial-temporal understanding in real-world applications remains largely unexamined, leading to uncertain prospects. To evaluate models’ Spatial-Temporal Intelligence, we introduce STI-Bench, a benchmark designed to evaluate MLLMs’ spatial-temporal understanding through challenging tasks such as estimating and predicting the appearance, pose, displacement, and motion of objects. Our benchmark encompasses a wide range of robot and vehicle operations across desktop, indoor, and outdoor scenarios. The extensive experiments reveals that the state-of-the-art MLLMs still struggle in real-world spatial-temporal understanding, especially in tasks requiring precise distance estimation and motion analysis. </p>
<blockquote>
<p>使用多模态大型语言模型（MLLMs）作为嵌入式人工智能和自动驾驶的端到端解决方案已成为一种流行趋势。虽然MLLMs在视觉语义理解任务方面已被广泛研究，但它们在现实应用中进行精确和定量时空理解的能力尚未得到充分检验，因此存在不确定的前景。为了评估模型的时空智能，我们引入了STI-Bench，这是一个旨在通过估计和预测物体的外观、姿态、位移和运动等具有挑战性的任务来评估MLLMs的时空理解能力的基准测试。我们的基准测试涵盖了桌面、室内和室外场景中的广泛机器人和车辆操作。大量实验表明，最先进的MLLMs在真实世界的时空理解方面仍然存在困难，特别是在需要精确距离估计和运动分析的任务中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23765v3">PDF</a> </p>
<p><strong>总结</strong></p>
<p>使用多模态大型语言模型（MLLMs）作为端到端的解决方案，为智能体和自动驾驶提供了流行趋势。虽然MLLMs在视觉语义理解任务方面已经得到了广泛的研究，但在现实世界应用中实现精确和定量时空理解的能力仍有待研究，导致前景不明朗。为了评估模型的时空智能，我们引入了STI-Bench基准测试平台，用于通过诸如估计和预测物体的外观、姿态、位移和运动等具有挑战性的任务来评估MLLMs的时空理解能力。该基准测试平台涵盖了桌面、室内和室外场景的广泛机器人和车辆操作范围。实验表明，最先进的MLLMs在真实世界的时空理解方面仍然存在困难，特别是在需要精确距离估计和运动分析的任务中。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）已成为智能体和自动驾驶的端到端解决方案的流行趋势。</li>
<li>MLLMs在视觉语义理解方面已有广泛应用，但在现实世界的时空理解方面仍存在不确定性。</li>
<li>引入STI-Bench基准测试平台，用于评估MLLMs的时空理解能力。</li>
<li>STI-Bench涵盖各种机器人和车辆操作任务，包括桌面、室内和室外场景。</li>
<li>评估包括估计和预测物体的外观、姿态、位移和运动等具有挑战性的任务。</li>
<li>实验显示最先进的MLLMs在精确距离估计和运动分析方面仍有困难。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23765">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-102ff4e7f4880f91ab33827ad6e00293.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640f11efd73c81fcbcabb10ff72d0d0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fa585f9874bad00b396113203c59bc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34d7f4df85f2110d9cd6811514f1da70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9aa73b2729f26067cdd1b132f9989cc9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3b7326e352312fc14c4f2e02b43f5add.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-23  FlowReasoner Reinforcing Query-Level Meta-Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2b2a88e5bcbadd645ade413c80e756a4.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-23  Stop Summation Min-Form Credit Assignment Is All Process Reward Model   Needs for Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
