<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-04-23  FlowReasoner Reinforcing Query-Level Meta-Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3b7326e352312fc14c4f2e02b43f5add.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    71 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-23-更新"><a href="#2025-04-23-更新" class="headerlink" title="2025-04-23 更新"></a>2025-04-23 更新</h1><h2 id="FlowReasoner-Reinforcing-Query-Level-Meta-Agents"><a href="#FlowReasoner-Reinforcing-Query-Level-Meta-Agents" class="headerlink" title="FlowReasoner: Reinforcing Query-Level Meta-Agents"></a>FlowReasoner: Reinforcing Query-Level Meta-Agents</h2><p><strong>Authors:Hongcheng Gao, Yue Liu, Yufei He, Longxu Dou, Chao Du, Zhijie Deng, Bryan Hooi, Min Lin, Tianyu Pang</strong></p>
<p>This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/FlowReasoner">https://github.com/sail-sg/FlowReasoner</a>. </p>
<blockquote>
<p>本文提出了一种名为FlowReasoner的查询级元代理，用于自动化设计查询级多代理系统，即针对每个用户查询设计一个系统。我们的核心思想是通过外部执行反馈来激励基于推理的元代理。具体来说，我们通过借鉴DeepSeek R1，首先赋予FlowReasoner关于生成多代理系统的基本推理能力。然后，我们借助强化学习（RL）和外部执行反馈进一步增强其能力。设计了一个多用途奖励来从性能、复杂性和效率方面引导RL训练。通过这种方式，FlowReasoner能够通过审慎推理为每个用户查询生成个性化的多代理系统。工程和竞赛代码基准上的实验证明了FlowReasoner的优越性。值得注意的是，它在三个基准测试上的准确率超越了o1-mini达10.52%。代码可从<a target="_blank" rel="noopener" href="https://github.com/sail-sg/FlowReasoner%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/sail-sg/FlowReasoner获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15257v1">PDF</a> </p>
<p><strong>Summary</strong><br>自动化查询级多智能体系统设计的新方法。提出使用名为FlowReasoner的查询级元智能体，根据用户查询生成个性化系统。结合深度分析和强化学习，实现高效、性能优越的系统生成。代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlowReasoner被提出用于自动化设计查询级多智能体系统。</li>
<li>核心思想是通过外部执行反馈来激励基于推理的元智能体。</li>
<li>结合DeepSeek R1的基本推理能力，增强FlowReasoner的多智能体系统生成能力。</li>
<li>使用强化学习进行进一步改进。</li>
<li>设计了一个多功能的奖励来指导强化学习的训练，涉及性能、复杂性和效率方面。</li>
<li>FlowReasoner能够通过深思熟虑的推理为每个用户查询生成个性化的多智能体系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15257">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c43bc2b712d2f2f76a65c8e53b4c9283.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d39282ca023d88917336c94ee4dcdc44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c19b1efb806f2614556aaa93d497978.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93cb223284b6f0eae915449e14d0b82e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33cb3ed3256e3763d4943bca45a84278.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Self-Improving-Coding-Agent"><a href="#A-Self-Improving-Coding-Agent" class="headerlink" title="A Self-Improving Coding Agent"></a>A Self-Improving Coding Agent</h2><p><strong>Authors:Maxime Robeyns, Martin Szummer, Laurence Aitchison</strong></p>
<p>We demonstrate that an LLM coding agent, equipped with basic coding tools, can autonomously edit itself, and thereby improve its performance on benchmark tasks. We find performance gains from 17% to 53% on a random subset of SWE Bench Verified, with additional performance gains on LiveCodeBench, as well as synthetically generated agent benchmarks. Our work represents an advancement in the automated and open-ended design of agentic systems, and provides a reference agent framework for those seeking to post-train LLMs on tool use and other agentic tasks. </p>
<blockquote>
<p>我们展示了一个配备基本编码工具的大型语言模型（LLM）编码代理，可以自主进行编辑，从而在基准任务上提高性能。我们在SWE Bench Verified的随机子集上实现了17%到53%的性能提升，在LiveCodeBench以及合成生成的代理基准测试上实现了额外的性能提升。我们的工作代表了自动化和开放设计的智能系统的一种进步，并为那些寻求在工具使用和其他智能任务上对大型语言模型进行后期训练的人提供了一个参考代理框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15228v1">PDF</a> Published at an ICLR 2025 workshop on Scaling Self-Improving   Foundation Models</p>
<p><strong>Summary</strong></p>
<p>一个配备基本编码工具的大型语言模型编码智能体可以自主编辑自身，从而提高基准任务的性能。在SWE Bench Verified随机子集上，性能提升了17%至53%，LiveCodeBench以及合成生成的智能体基准上也有额外的性能提升。本研究代表了自动化开放式智能系统设计的一个进步，为寻求对工具使用和其他智能任务的后期训练的语言模型提供了参考框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型编码智能体具备自主编辑能力。</li>
<li>智能体通过自主编辑能显著提高基准任务的性能。</li>
<li>在SWE Bench Verified随机子集上的性能提升范围在17%至53%。</li>
<li>智能体在LiveCodeBench上也有额外的性能提升。</li>
<li>研究展示了自动化开放式智能系统设计的进步。</li>
<li>为寻求工具使用和其他智能任务后期训练的语言模型提供了参考框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15228">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-187ec85ccc96478f6371c03567c5f798.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96514236a97e6c81743d872bd8413269.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b611950b5e84a05296a4a8b72183d48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d46c3a57a58cb88296b3384fd2af592a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Neural-ATTF-A-Scalable-Solution-to-Lifelong-Multi-Agent-Path-Planning"><a href="#Neural-ATTF-A-Scalable-Solution-to-Lifelong-Multi-Agent-Path-Planning" class="headerlink" title="Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning"></a>Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning</h2><p><strong>Authors:Kushal Shah, Jihyun Park, Seung-Kyum Choi</strong></p>
<p>Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics, particularly in applications such as warehouse automation and logistics. Existing solutions often face challenges in scalability, adaptability, and efficiency, limiting their applicability in dynamic environments with real-time planning requirements. This paper presents Neural ATTF (Adaptive Task Token Framework), a new algorithm that combines a Priority Guided Task Matching (PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning method. Neural STA* enhances path planning by enabling rapid exploration of the search space through guided learned heuristics and ensures collision avoidance under dynamic constraints. PGTM prioritizes delayed agents and dynamically assigns tasks by prioritizing agents nearest to these tasks, optimizing both continuity and system throughput. Experimental evaluations against state-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and LNS-wPBS, demonstrate the superior scalability, solution quality, and computational efficiency of Neural ATTF. These results highlight the framework’s potential for addressing the critical demands of complex, real-world multi-agent systems operating in high-demand, unpredictable settings. </p>
<blockquote>
<p>多智能体拾取与交付（MAPD）是机器人技术中的一个基本问题，特别是在仓库自动化和物流等应用中。现有解决方案在可扩展性、适应性和效率方面经常面临挑战，限制了它们在具有实时规划要求的动态环境中的适用性。本文提出了Neural ATTF（自适应任务令牌框架），这是一种新的算法，它将优先级引导任务匹配（PGTM）模块与Neural STA<em>（时空A</em>）相结合，这是一种数据驱动的路径规划方法。Neural STA*通过引导学习启发式方法，能够迅速探索搜索空间，增强路径规划能力，并确保在动态约束下避免碰撞。PGTM对延迟的智能体进行优先级排序，并根据最接近这些任务的智能体动态分配任务，优化连续性和系统吞吐量。与最新最先进的MAPD算法（包括TPTS、CENTRAL、RMCA、LNS-PBS和LNS-wPBS）的实验评估表明，Neural ATTF在可扩展性、解决方案质量和计算效率方面具有优势。这些结果突显了该框架在满足复杂、现实世界多智能体系统在高度需求、不可预测环境中需求方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15130v1">PDF</a> 13 Pages, 5 Figures, 5 Tables</p>
<p><strong>Summary</strong><br>     神经网络ATTF（自适应任务令牌框架）是一种结合了优先级引导任务匹配模块与神经网络STA<em>（时空A</em>）数据驱动路径规划方法的算法。它提高了路径规划的速度，保证了动态约束下的避障能力，并优化了多智能体的连续性和系统吞吐量。实验评估表明，神经网络ATTF在解决多智能体拾取与交付问题的性能优于其他先进算法，具有更好的可扩展性、解决方案质量和计算效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络ATTF是一种针对多智能体拾取与交付问题的新型算法。</li>
<li>该算法结合了优先级引导任务匹配模块与神经网络STA<em>（时空A</em>）路径规划方法。</li>
<li>神经网络STA*能够实现快速搜索空间，通过引导学习到的启发式方法确保动态约束下的避障。</li>
<li>优先级引导任务匹配模块能够优先处理延迟的智能体并动态分配任务，优化智能体的连续性和系统吞吐量。</li>
<li>实验评估表明，神经网络ATTF在解决多智能体拾取与交付问题方面具有出色的可扩展性、解决方案质量和计算效率。</li>
<li>该算法在仓库自动化和物流等应用中具有潜在的应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15130">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3296c28e18dd82e66d58aa0bf852a80f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93158b72e7a3f8f23181f5d27a9cdf19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a459f7ac268daf28dcc586c01e8b3d6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef04a1fd063f59fb2906ad6d336b6eaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea56276ad4a426dc0f79631152ae38a1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Text-to-Decision-Agent-Learning-Generalist-Policies-from-Natural-Language-Supervision"><a href="#Text-to-Decision-Agent-Learning-Generalist-Policies-from-Natural-Language-Supervision" class="headerlink" title="Text-to-Decision Agent: Learning Generalist Policies from Natural   Language Supervision"></a>Text-to-Decision Agent: Learning Generalist Policies from Natural   Language Supervision</h2><p><strong>Authors:Shilin Zhang, Zican Hu, Wenhao Wu, Xinyi Xie, Jianxiang Tang, Chunlin Chen, Daoyi Dong, Yu Cheng, Zhenhong Sun, Zhi Wang</strong></p>
<p>RL systems usually tackle generalization by inferring task beliefs from high-quality samples or warmup explorations. The restricted form limits their generality and usability since these supervision signals are expensive and even infeasible to acquire in advance for unseen tasks. Learning directly from the raw text about decision tasks is a promising alternative to leverage a much broader source of supervision. In the paper, we propose Text-to-Decision Agent (T2DA), a simple and scalable framework that supervises generalist policy learning with natural language. We first introduce a generalized world model to encode multi-task decision data into a dynamics-aware embedding space. Then, inspired by CLIP, we predict which textual description goes with which decision embedding, effectively bridging their semantic gap via contrastive language-decision pre-training and aligning the text embeddings to comprehend the environment dynamics. After training the text-conditioned generalist policy, the agent can directly realize zero-shot text-to-decision generation in response to language instructions. Comprehensive experiments on MuJoCo and Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot generalization and outperforms various types of baselines. </p>
<blockquote>
<p>强化学习系统通常通过从高质量样本或预热探索中推断任务信念来解决泛化问题。这种有限的形式限制了它们的通用性和可用性，因为这些监督信号是昂贵的，甚至对于未见过的任务，提前获取也是不可行的。直接从关于决策任务的原始文本中学习是一个很有前途的替代方案，可以充分利用更广泛的监督来源。在论文中，我们提出了文本到决策代理（T2DA），这是一个简单且可扩展的框架，用自然语言监督通用策略学习。我们首先引入一个通用世界模型，将多任务决策数据编码到一个动态感知的嵌入空间。然后，受到CLIP的启发，我们预测哪种文本描述与哪种决策嵌入相匹配，通过对比语言决策预训练有效地弥合了它们的语义差距，并将文本嵌入对齐以理解环境动态。在训练文本条件下的通用策略之后，代理可以直接实现零射击文本到决策生成，以响应语言指令。在MuJoCo和Meta-World基准测试上的综合实验表明，T2DA促进了高容量零射击泛化，并超越了各种类型的基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15046v1">PDF</a> 18 pages, 8 figures</p>
<p><strong>Summary</strong>：<br>论文提出了一种名为Text-to-Decision Agent（T2DA）的框架，能够通过自然语言直接进行决策任务学习，以广泛的监督源提高通用策略学习的效率。该框架引入了通用世界模型，将多任务决策数据编码为动态感知的嵌入空间，并通过对比语言决策预训练，缩小文本描述与决策嵌入之间的语义差距，实现对环境动态的理解。训练后，该智能体可直接根据语言指令进行零射击决策生成。在MuJoCo和Meta-World基准测试上，T2DA表现出优异的高容量零射击泛化能力，超越了各种基线。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>T2DA框架允许智能体通过自然语言直接进行决策任务学习，提高通用策略学习的效率。</li>
<li>通过引入通用世界模型，将多任务决策数据编码为动态感知的嵌入空间。</li>
<li>采用对比语言决策预训练，缩小文本描述与决策嵌入之间的语义差距。</li>
<li>通过预训练，智能体能够理解环境动态。</li>
<li>T2DA实现了零射击决策生成，即智能体能够直接根据语言指令进行决策。</li>
<li>在MuJoCo和Meta-World基准测试中，T2DA表现出优异的高容量零射击泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15046">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3b7326e352312fc14c4f2e02b43f5add.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd9abdf4287c4d797227d3dfb0a7413d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-611a662efb30523fd04de70433b6296b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Event-triggered-optimal-formation-control-for-nonlinear-multi-agent-systems-under-Denial-of-Service-attacks"><a href="#Event-triggered-optimal-formation-control-for-nonlinear-multi-agent-systems-under-Denial-of-Service-attacks" class="headerlink" title="Event triggered optimal formation control for nonlinear multi-agent   systems under Denial-of-Service attacks"></a>Event triggered optimal formation control for nonlinear multi-agent   systems under Denial-of-Service attacks</h2><p><strong>Authors:Jianqiang Zhang, Kaijun Yang</strong></p>
<p>This paper investigates the optimal formation control problem of a class of nonlinear multi-agent systems(MASs) under Denial-of-Service(DoS) attacks. We design the optimal formation control law using an event-triggered control scheme to achieve formation objectives under DoS attacks. Critic neural network (NN)-based approach is employed to achieve the optimal control policy under DoS attacks. Event-triggered mechanism is introduced to ensure the saving of control resources. Additionally, Lyapunov stability theory is utilized to demonstrate that the local neighborhood formation error exhibits exponential stability and the estimation error of weights are uniformly ultimately bounded. Finally, the effectiveness of the control algorithm is validated through matlab simulations. The results indicate that under DoS attacks, the nonlinear MAS successfully achieves the desired formation for the MAS. </p>
<blockquote>
<p>本文研究DoS攻击下一类非线性多智能体系统（MAS）的最优编队控制问题。我们采用事件触发控制方案，设计最优编队控制律，以实现DoS攻击下的编队目标。采用基于批判神经网络（NN）的方法，实现DoS攻击下的最优控制策略。引入事件触发机制，确保控制资源的节约。此外，利用Lyapunov稳定性理论证明了局部邻域编队误差呈指数稳定，权重估计误差是一致有界的。最后，通过matlab仿真验证了控制算法的有效性。结果表明，在DoS攻击下，非线性MAS成功实现了期望的编队。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14874v1">PDF</a> </p>
<p><strong>Summary</strong><br>该论文研究了在拒绝服务攻击下的一类非线性多智能体系统（MASs）的最优编队控制问题。通过使用基于事件触发的控制方案，实现了DoS攻击下的编队目标。利用批判神经网络（NN）的方法实现了DoS攻击下的最优控制策略。引入事件触发机制可节约控制资源。同时利用Lyapunov稳定性理论证明了局部邻域编队误差呈指数级稳定，权重估计误差一致有界。最后通过Matlab仿真验证了控制算法的有效性。结果显示，在DoS攻击下，非线性MAS成功实现了期望的编队。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了非线性多智能体系统在拒绝服务攻击下的最优编队控制问题。</li>
<li>设计了基于事件触发的最优编队控制律以实现DoS攻击下的编队目标。</li>
<li>采用了批判神经网络方法以制定DoS攻击下的最优控制策略。</li>
<li>事件触发机制用于节约控制资源。</li>
<li>利用Lyapunov稳定性理论证明了局部邻域编队误差的稳定性及权重估计误差的有界性。</li>
<li>通过Matlab仿真验证了控制算法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14874">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d179c1e72257b8446b7dc7e6a35c7d02.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ADL-A-Declarative-Language-for-Agent-Based-Chatbots"><a href="#ADL-A-Declarative-Language-for-Agent-Based-Chatbots" class="headerlink" title="ADL: A Declarative Language for Agent-Based Chatbots"></a>ADL: A Declarative Language for Agent-Based Chatbots</h2><p><strong>Authors:Sirui Zeng, Xifeng Yan</strong></p>
<p>There are numerous agent frameworks capable of creating and orchestrating agents to address complex tasks. However, these frameworks are often too complicated for customer service professionals, who may not have much programming experience but still need an easy way to create chatbots with rich business logic. In this work, we introduce ADL, a Declarative Language for Agent-Based Chatbots. ADL simplifies chatbot development by using natural language programming at its core, making it easier for a broad audience to customize and build task-oriented chatbots. It includes four types of agents and supports integration with custom functions, tool use, and third-party agents. ADL abstracts away implementation details, offering a declarative way to define agents and their interactions, which could ease prompt engineering, testing and debugging. MICA, a multi-agent system designed to interpret and execute ADL programs, has been developed and is now available as an open-source project at <a target="_blank" rel="noopener" href="https://github.com/Mica-labs/MICA">https://github.com/Mica-labs/MICA</a>. Its user documentation can be found at <a target="_blank" rel="noopener" href="https://mica-labs.github.io/">https://mica-labs.github.io/</a>. </p>
<blockquote>
<p>存在许多能够创建和协调代理以处理复杂任务的代理框架。然而，这些框架通常对于客户服务专业人员来说过于复杂，他们可能没有太多的编程经验，但仍然需要一种简单的创建聊天机器人的方法，该机器人具有丰富的业务逻辑。在此工作中，我们介绍了基于代理聊天机器人的声明性语言ADL。ADL使用自然语言编程作为其核心，简化了聊天机器人的开发过程，使得广泛的受众群体更容易定制和构建面向任务的聊天机器人。它包括四种类型的代理并支持自定义功能集成、工具使用和第三方代理。ADL抽象了实现细节，提供了一种声明的方式来定义代理及其交互，这可以简化提示工程、测试和调试。MICA是一个多代理系统，旨在解释和执行ADL程序，已经被开发并且现在作为开源项目在<a target="_blank" rel="noopener" href="https://github.com/Mica-labs/MICA">https://github.com/Mica-labs/MICA</a>上可用。其用户文档可以在<a target="_blank" rel="noopener" href="https://mica-labs.github.io/">https://mica-labs.github.io/</a>上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14787v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AD（自然语言声明性框架）解决了客户服务专业人员对构建和管理复杂任务导向型聊天机器人的需求。它简化了聊天机器人的开发过程，采用自然语言编程为核心，使得广大用户能够轻松地自定义和构建任务导向型聊天机器人。MICA系统能够解释并执行AD程序，现在作为一个开源项目可用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>存在许多代理框架能够创建和协调代理来解决复杂的任务，但对于缺乏编程经验的客户服务专业人员来说，这些框架通常过于复杂。</li>
<li>AD（自然语言声明性框架）被引入以解决这一问题，它简化了聊天机器人的开发过程，并采用自然语言编程为核心。</li>
<li>AD支持四种类型的代理，并可与自定义功能、工具使用和第三方代理集成。</li>
<li>AD通过提供声明性方式来定义代理及其交互，从而简化了提示工程、测试和调试。</li>
<li>MICA是一个多代理系统，用于解释和执行AD程序。</li>
<li>MICA现在作为一个开源项目可用，用户文档也已提供。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14787">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d6276c91b41356d94775a489c7274b6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ff09083255de6ba66985f498e3f3c48.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3422b8852b0f45371523e8e120e65d40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad5c0b85b01021f8b3624d5c55aad311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-216f1772f0177dc267f0c9c4d83ca601.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Exploring-Collaborative-GenAI-Agents-in-Synchronous-Group-Settings-Eliciting-Team-Perceptions-and-Design-Considerations-for-the-Future-of-Work"><a href="#Exploring-Collaborative-GenAI-Agents-in-Synchronous-Group-Settings-Eliciting-Team-Perceptions-and-Design-Considerations-for-the-Future-of-Work" class="headerlink" title="Exploring Collaborative GenAI Agents in Synchronous Group Settings:   Eliciting Team Perceptions and Design Considerations for the Future of Work"></a>Exploring Collaborative GenAI Agents in Synchronous Group Settings:   Eliciting Team Perceptions and Design Considerations for the Future of Work</h2><p><strong>Authors:Janet G. Johnson, Macarena Peralta, Mansanjam Kaur, Ruijie Sophia Huang, Sheng Zhao, Ruijia Guan, Shwetha Rajaram, Michael Nebeling</strong></p>
<p>While generative artificial intelligence (GenAI) is finding increased adoption in workplaces, current tools are primarily designed for individual use. Prior work established the potential for these tools to enhance personal creativity and productivity towards shared goals; however, we don’t know yet how to best take into account the nuances of group work and team dynamics when deploying GenAI in work settings. In this paper, we investigate the potential of collaborative GenAI agents to augment teamwork in synchronous group settings through an exploratory study that engaged 25 professionals across 6 teams in speculative design workshops and individual follow-up interviews. Our workshops included a mixed reality provotype to simulate embodied collaborative GenAI agents capable of actively participating in group discussions. Our findings suggest that, if designed well, collaborative GenAI agents offer valuable opportunities to enhance team problem-solving by challenging groupthink, bridging communication gaps, and reducing social friction. However, teams’ willingness to integrate GenAI agents depended on its perceived fit across a number of individual, team, and organizational factors. We outline the key design tensions around agent representation, social prominence, and engagement and highlight the opportunities spatial and immersive technologies could offer to modulate GenAI influence on team outcomes and strike a balance between augmentation and agency. </p>
<blockquote>
<p>随着生成式人工智能（GenAI）在工作场所的普及程度不断提高，当前的主要工具设计主要是为了个人使用。先前的研究已经证明了这些工具在促进个人创造力和生产力以实现共同目标方面的潜力；然而，我们仍然不知道如何在部署GenAI时考虑到团队合作中的细微差别和团队动态。本文旨在探究协作式GenAI代理在同步团队环境中的潜力，通过一项涉及25名专业人士、跨越六个团队的探索性研究，包括参与式设计研讨会和个人跟进访谈。我们的研讨会包括一个混合现实原型，模拟能够积极参与小组讨论的协作式GenAI代理。我们的研究发现，如果设计得当，协作式GenAI代理通过挑战群体思维、弥沟通鸿沟和减少社会摩擦，为增强团队解决问题的能力提供了宝贵的机会。然而，团队愿意整合GenAI代理的程度取决于个人、团队和组织层面多个因素的感知契合度。我们概述了关于代理表示、社会突出性和参与性的关键设计冲突，并强调空间和沉浸式技术可以在调节GenAI对团队结果的影响以及平衡增强和代理之间找到平衡方面提供的机会。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14779v1">PDF</a> To be published in ACM Conference on Computer-Supported Cooperative   Work and Social Computing (CSCW 2025). 33 pages, 11 figures, 1 table</p>
<p><strong>Summary</strong><br>人工智能辅助团队合作能够提升团队解决问题的能力。研究通过模拟协作式人工智能参与团队讨论发现，设计良好的协作式人工智能可以挑战团队思维，缩小沟通差距并减少社会摩擦。但是团队接受度取决于多个因素如个体因素、团队因素和组织因素等。对此文章给出了关于智能代理展现形式的关键设计难题及社交、沉浸技术的优势并总结了未来的发展趋势。强调了设计这种协作工具在时空环境中需要的权衡点以实现支持和支持对象的自然交融。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式人工智能（GenAI）工具在设计上主要面向个人使用，但在团队协作中的应用潜力尚未得到充分研究。</li>
<li>协作式GenAI代理工具可以在团队讨论中积极参与并提升团队解决问题的能力，包括挑战团队思维、缩小沟通差距和减少社会摩擦。</li>
<li>团队协作中采用GenAI工具的接受度受到多个因素影响，包括个体、团队和组织因素。这些因素需要综合考虑以确保工具的顺利融入。</li>
<li>设计协作式GenAI工具的关键设计难题包括智能代理的展现形式、社交显著性和参与度等方面。这些问题的解决将有助于实现更自然的团队协作体验。 </li>
<li>空间沉浸式技术有助于调整人工智能对团队结果的影响，实现增强与自主之间的平衡。这种技术可以为协作式GenAI工具的设计提供新的可能性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14779">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1391547030c0ec9a394f3784f86f8ae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21adc62875340967bdddfc1c04d48b9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-130e384a7436da66cd697d96b6e2b190.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="An-LLM-enabled-Multi-Agent-Autonomous-Mechatronics-Design-Framework"><a href="#An-LLM-enabled-Multi-Agent-Autonomous-Mechatronics-Design-Framework" class="headerlink" title="An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework"></a>An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework</h2><p><strong>Authors:Zeyu Wang, Frank P. -W. Lo, Qian Chen, Yongqi Zhang, Chen Lin, Xu Chen, Zhenhua Yu, Alexander J. Thompson, Eric M. Yeatman, Benny P. L. Lo</strong></p>
<p>Existing LLM-enabled multi-agent frameworks are predominantly limited to digital or simulated environments and confined to narrowly focused knowledge domain, constraining their applicability to complex engineering tasks that require the design of physical embodiment, cross-disciplinary integration, and constraint-aware reasoning. This work proposes a multi-agent autonomous mechatronics design framework, integrating expertise across mechanical design, optimization, electronics, and software engineering to autonomously generate functional prototypes with minimal direct human design input. Operating primarily through a language-driven workflow, the framework incorporates structured human feedback to ensure robust performance under real-world constraints. To validate its capabilities, the framework is applied to a real-world challenge involving autonomous water-quality monitoring and sampling, where traditional methods are labor-intensive and ecologically disruptive. Leveraging the proposed system, a fully functional autonomous vessel was developed with optimized propulsion, cost-effective electronics, and advanced control. The design process was carried out by specialized agents, including a high-level planning agent responsible for problem abstraction and dedicated agents for structural, electronics, control, and software development. This approach demonstrates the potential of LLM-based multi-agent systems to automate real-world engineering workflows and reduce reliance on extensive domain expertise. </p>
<blockquote>
<p>现有的大型语言模型赋能的多智能体框架主要局限于数字或模拟环境，并局限于狭窄的知识域，这限制了它们在需要物理实体设计、跨学科集成和约束感知推理的复杂工程任务中的应用。本文提出了一种多智能体自主机电设计框架，该框架整合机械设计、优化、电子和软件工程的专长，以自主生成功能原型，并尽量减少直接人为设计输入。该框架主要通过语言驱动的工作流程进行操作，并融入结构化的人类反馈，以确保在现实世界的约束下具有稳健的性能。为了验证其能力，该框架被应用于涉及水质自主监测和采样的现实世界挑战中，传统的方法是劳动密集型的并且具有生态破坏性。利用所提出的系统，开发了一种功能齐全的全自主船只，具有优化的推进能力、成本效益高的电子设备和先进的控制功能。设计过程由专职智能体完成，包括负责问题抽象的高级规划智能体以及结构、电子、控制和软件开发的专用智能体。这种方法展示了基于大型语言模型的多智能体系统在自动化现实世界工程工作流程和减少对广泛领域专业知识的依赖方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14681v1">PDF</a> Accepted by CVPR 2025 Workshop</p>
<p><strong>Summary</strong><br>大規模語言模型（LLM）赋能的多智能体框架目前主要局限于数字或模拟环境，并仅限于狭窄的知识域，限制了其在复杂工程任务中的应用，这些任务需要设计物理实体、跨学科整合和约束感知推理。本研究提出一种多智能体自主机电一体化设计框架，该框架整合机械设计、优化、电子和软件工程的专长，通过语言驱动的工作流程自主生成功能原型，并最小化直接人工设计输入。该框架通过结构化的人类反馈来确保在现实世界的约束下实现稳健性能。为了验证其能力，该框架被应用于涉及水质自主监测和采样的现实挑战中。通过使用该系统，开发了一种具有优化推进、经济电子元件和先进控制功能的完全自主船只。设计过程由专门智能体完成，包括负责问题抽象的高级规划智能体以及负责结构、电子、控制和软件开发的专门智能体。该方法展示了大规模语言模型为基础的多智能体系统在自动化现实工程工作流程方面的潜力，并减少了我们对广泛领域专业知识的依赖。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM赋能的多智能体框架主要局限于数字或模拟环境，且主要适用于狭窄的知识领域。</li>
<li>提出一种多智能体自主机电一体化设计框架，旨在自主生成功能原型，减少对直接人工设计输入的依赖。</li>
<li>框架采用语言驱动的工作流程，并结合结构化的人类反馈确保稳健性能。</li>
<li>框架应用于现实挑战，如自主水质监测和采样，展示了其潜力。</li>
<li>通过开发具有优化推进、经济电子元件和先进控制功能的自主船只，验证了框架的实际应用能力。</li>
<li>设计过程由多个专门智能体完成，包括高级规划智能体和负责不同工程领域的智能体。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0ea92db07ca9275c97a1ef17ca5963e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8415b16b3876f4b841e97e6e715745f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-325244df30b8c5564c7dfee9853cdcb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be51c793f5faae9e8d3fc2f8da66c4bc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Optimal-Circuit-Generation-Multi-Agent-Collaboration-Meets-Collective-Intelligence"><a href="#Towards-Optimal-Circuit-Generation-Multi-Agent-Collaboration-Meets-Collective-Intelligence" class="headerlink" title="Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets   Collective Intelligence"></a>Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets   Collective Intelligence</h2><p><strong>Authors:Haiyan Qin, Jiahao Feng, Xiaotong Feng, Wei W. Xing, Wang Kang</strong></p>
<p>Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38%–1075% higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem – a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6% of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25% of human experts without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at <a target="_blank" rel="noopener" href="https://github.com/BUAA-CLab/CircuitMind">https://github.com/BUAA-CLab/CircuitMind</a>. </p>
<blockquote>
<p>大规模语言模型（LLM）已经改变了代码生成，但它们在硬件设计中的应用产生了比人类设计高38%~1075%的门计数。我们提出了CircuitMind，这是一个多代理框架，通过三个关键创新实现了与人类竞争的效率：语法锁定（将生成限制在基本逻辑门内）、检索增强生成（实现知识驱动设计）和双奖励优化（平衡正确性与效率）。为了评估我们的方法，我们引入了TC-Bench，这是第一个利用TuringComplete生态系统集体智慧的门级基准测试——一个拥有数十万玩家的竞争电路设计平台。实验表明，CircuitMind使55.6%的模型实现能够在组合效率指标上匹配或超过顶级人类专家。值得注意的是，我们的框架提升了14B Phi-4模型的表现，使其超越了GPT-4o mini和Gemini 2.0 Flash，达到了与顶级人类专家效率相当的水平，而无需进行专门培训。这些创新为硬件优化建立了新的范式，其中协作AI系统利用人类的集体专业知识来实现最佳电路设计。我们的模型、数据和代码均在<a target="_blank" rel="noopener" href="https://github.com/BUAA-CLab/CircuitMind%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/BUAA-CLab/CircuitMind开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14625v1">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong><br>大语言模型在代码生成方面有着广泛的应用，但在硬件设计领域的应用会导致门数增加高达38%~1075%。为此，研究团队提出了CircuitMind多智能体框架，通过语法锁定、检索增强生成和双奖励优化等三大创新技术，实现了与人类竞争的效率。通过引入TC-Bench基准测试平台，实验证明CircuitMind使模型实现的效率达到或超过顶级人类专家的复合指标高达55.6%。CircuitMind框架提升了一个拥有超过百万玩家的电路竞争平台TuringComplete生态系统中的集体智能水平。该研究为硬件优化领域开创了新的范式，实现了人工智能系统与人类专家协作的最佳电路设计。相关模型和代码已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大语言模型在硬件设计中的应用会导致门数增加，与人类设计相比效率较低。</li>
<li>CircuitMind框架通过三大创新技术实现与人类竞争的效率，包括语法锁定、检索增强生成和双奖励优化。</li>
<li>TC-Bench基准测试平台用于评估CircuitMind框架的性能。</li>
<li>实验证明CircuitMind框架使模型实现的效率达到或超过顶级人类专家的复合指标高达55.6%。</li>
<li>CircuitMind框架成功提升了电路竞争平台TuringComplete生态系统中的集体智能水平。</li>
<li>CircuitMind框架适用于各种语言模型，并成功提升了小型语言模型的性能。</li>
<li>该研究为硬件优化领域开创了新的范式，实现了人工智能系统与人类专家协作的最佳电路设计。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14625">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d3b2adc5b0133231fced0aa3ba75851.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-804f107ec0d462d0b9fff9cf35cac1bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97e0e2a16be9d577dc2bc66aa502baf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aee2e68d2ca48dd03319850822c9130.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2e25b5eb179e0da930efe0ff558c9e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5752bdeac7b58855ef4a7fc24b10a8e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0632376885d52c18e440032fc5c788f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b4483dc91ea55869efdef15a9308abe.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BookWorld-From-Novels-to-Interactive-Agent-Societies-for-Creative-Story-Generation"><a href="#BookWorld-From-Novels-to-Interactive-Agent-Societies-for-Creative-Story-Generation" class="headerlink" title="BookWorld: From Novels to Interactive Agent Societies for Creative Story   Generation"></a>BookWorld: From Novels to Interactive Agent Societies for Creative Story   Generation</h2><p><strong>Authors:Yiting Ran, Xintao Wang, Tian Qiu, Jiaqing Liang, Yanghua Xiao, Deqing Yang</strong></p>
<p>Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld’s design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: <a target="_blank" rel="noopener" href="https://bookworld2025.github.io/">https://bookworld2025.github.io/</a>. </p>
<blockquote>
<p>最近的大型语言模型（LLM）的进步已经能够通过多智能体系统实现社会模拟。之前的研究主要集中在从零开始创建智能体社会，并为智能体分配新定义的人格。然而，尽管模拟成熟的虚构世界和角色具有重要的实用价值，但对其进行的研究仍然远远不够。在本文中，我们介绍了BookWorld，这是一个用于构建和模拟基于书籍的多智能体社会的综合系统。BookWorld的设计涵盖了现实世界中的复杂细节，包括多样且动态的角色、虚构的世界观、地理约束和变化等。BookWorld支持多种应用，包括故事生成、互动游戏和社会模拟，为探索心爱的虚构作品提供了新的途径。通过广泛的实验，我们证明了BookWorld能够生成具有创意且高质量的故事，同时保持对原书的忠实度，其胜率达到了75.36%，超过了之前的方法。该论文的代码可以在项目页面找到：<a target="_blank" rel="noopener" href="https://bookworld2025.github.io./">https://bookworld2025.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14538v1">PDF</a> 19 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的最新进展，通过多智能体系统实现了社会模拟。以往的研究主要集中在从零开始创建智能体社会并为智能体分配新定义的个性。然而，模拟现有的虚构世界和角色方面仍存在大量未解决的问题，尽管具有重大实际意义。本文介绍了一个名为BookWorld的综合系统，该系统可以构建和模拟基于书籍的多智能体社会。BookWorld的设计涵盖了现实世界中的复杂细节，包括多样化和动态的角色、虚构的世界观、地理约束和变化等。BookWorld支持多种应用程序，包括故事生成、互动游戏和社会模拟，为探索心爱的虚构作品提供了新颖的方式。通过广泛的实验，我们证明了BookWorld在保持对原书的忠实度的同时，能够生成有创造力和高质量的故事，并且以75.36%的胜率超越了以前的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）可用于通过多智能体系统实现社会模拟。</li>
<li>BookWorld系统用于构建和模拟基于书籍的多智能体社会。</li>
<li>BookWorld涵盖了现实世界中的复杂细节，包括角色、世界观、地理约束等。</li>
<li>BookWorld支持故事生成、互动游戏和社会模拟等多种应用程序。</li>
<li>BookWorld能够生成高质量、有创造力的故事，并保持对原书的忠实度。</li>
<li>通过实验证明，BookWorld在模拟现有虚构世界方面表现出色，超越了先前的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14538">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-65b41f99c9fbe6d9efc19016595eaa2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1bd96aeaa08984b48ae8db8fb832d72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0903d7e5422ccbda46bc2865a5ca919e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DialogueAgents-A-Hybrid-Agent-Based-Speech-Synthesis-Framework-for-Multi-Party-Dialogue"><a href="#DialogueAgents-A-Hybrid-Agent-Based-Speech-Synthesis-Framework-for-Multi-Party-Dialogue" class="headerlink" title="DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for   Multi-Party Dialogue"></a>DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for   Multi-Party Dialogue</h2><p><strong>Authors:Xiang Li, Duyi Pan, Hongru Xiao, Jiale Han, Jing Tang, Jiabao Ma, Wei Wang, Bo Cheng</strong></p>
<p>Speech synthesis is crucial for human-computer interaction, enabling natural and intuitive communication. However, existing datasets involve high construction costs due to manual annotation and suffer from limited character diversity, contextual scenarios, and emotional expressiveness. To address these issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis framework, which integrates three specialized agents – a script writer, a speech synthesizer, and a dialogue critic – to collaboratively generate dialogues. Grounded in a diverse character pool, the framework iteratively refines dialogue scripts and synthesizes speech based on speech review, boosting emotional expressiveness and paralinguistic features of the synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a bilingual, multi-party, multi-turn speech dialogue dataset covering diverse topics. Extensive experiments demonstrate the effectiveness of our framework and the high quality of the MultiTalk dataset. We release the dataset and code <a target="_blank" rel="noopener" href="https://github.com/uirlx/DialogueAgents">https://github.com/uirlx/DialogueAgents</a> to facilitate future research on advanced speech synthesis models and customized data generation. </p>
<blockquote>
<p>语音合成对于人机交互至关重要，能够实现自然直观的沟通。然而，现有数据集由于手动标注而涉及较高的构建成本，并且存在字符多样性、上下文情境和情绪表达等方面的局限性。为了解决这些问题，我们提出了DialogueAgents，这是一个基于混合代理的新型语音合成框架。它集成了三个专业代理——剧本作者、语音合成器和对话评论家，共同生成对话。该框架基于多样化的字符池，通过语音评审迭代优化对话脚本并合成语音，提高了合成对话的情感表达和非语言特征。使用DialogueAgent，我们创建了MultiTalk数据集，这是一个涵盖各种话题的双语、多方、多轮语音对话数据集。大量实验证明了我们框架的有效性和MultiTalk数据集的高质量。我们已将数据集和代码发布在<a target="_blank" rel="noopener" href="https://github.com/uirlx/DialogueAgents%E4%B8%8A%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E6%9C%AA%E6%9D%A5%E5%AF%B9%E5%85%88%E8%BF%9B%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%AE%9A%E5%88%B6%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/uirlx/DialogueAgents上，以促进未来对先进语音合成模型和定制数据生成的研究。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14482v1">PDF</a> Accepted by ICME 2025. Dataset and code are publicly available:   <a target="_blank" rel="noopener" href="https://github.com/uirlx/DialogueAgents">https://github.com/uirlx/DialogueAgents</a></p>
<p><strong>Summary</strong></p>
<p>对话合成在人机交互中扮演重要角色，可实现自然、直观的沟通。然而，现有数据集存在高构建成本问题，如手动标注，且存在字符多样性、上下文场景和情感表达有限等缺陷。为解决这些问题，我们提出DialogueAgents这一新型基于代理的对话合成框架，集成了脚本编写器、语音合成器和对话评论家三个专业代理，共同生成对话。该框架基于丰富的角色库，通过对话脚本迭代优化和基于语音评价的合成，提高了合成对话的情感表达和非语言特征。我们还贡献了MultiTalk这一双语、多方、多轮对话数据集，涵盖各种主题。实验结果充分验证了框架的有效性和数据集的高质量。我们公开了数据集和代码，以推动先进的语音合成模型和定制数据生成的未来研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对话合成在人机交互中具有重要作用。</li>
<li>现有数据集存在高构建成本、字符多样性、上下文场景和情感表达有限等问题。</li>
<li>DialogueAgents是一个基于代理的对话合成框架，集成了脚本编写器、语音合成器和对话评论家。</li>
<li>该框架通过迭代优化和语音评价，提高了合成对话的情感表达和非语言特征。</li>
<li>MultiTalk数据集是一个双语、多方、多轮对话数据集，涵盖各种主题。</li>
<li>实验结果证明了DialogueAgents框架和MultiTalk数据集的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-595d08a14f93e5430e1705792ae4f1a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32924dc7b8e04c642cfa67a4347911db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-860f9d11268503f74144dd86c6e89872.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-274d7ebf63cfc2227438caaa7be1f2d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8b03c3e6e3a809e2cb769e6ba23095c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94c85c3aa5f51ad2c175ecee682bedf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f26e25e7478e8170a3d2aaf6a90697e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Optimizing-SIA-Development-A-Case-Study-in-User-Centered-Design-for-Estuary-a-Multimodal-Socially-Interactive-Agent-Framework"><a href="#Optimizing-SIA-Development-A-Case-Study-in-User-Centered-Design-for-Estuary-a-Multimodal-Socially-Interactive-Agent-Framework" class="headerlink" title="Optimizing SIA Development: A Case Study in User-Centered Design for   Estuary, a Multimodal Socially Interactive Agent Framework"></a>Optimizing SIA Development: A Case Study in User-Centered Design for   Estuary, a Multimodal Socially Interactive Agent Framework</h2><p><strong>Authors:Spencer Lin, Miru Jun, Basem Rizk, Karen Shieh, Scott Fisher, Sharon Mozgai</strong></p>
<p>This case study presents our user-centered design model for Socially Intelligent Agent (SIA) development frameworks through our experience developing Estuary, an open source multimodal framework for building low-latency real-time socially interactive agents. We leverage the Rapid Assessment Process (RAP) to collect the thoughts of leading researchers in the field of SIAs regarding the current state of the art for SIA development as well as their evaluation of how well Estuary may potentially address current research gaps. We achieve this through a series of end-user interviews conducted by a fellow researcher in the community. We hope that the findings of our work will not only assist the continued development of Estuary but also guide the development of other future frameworks and technologies for SIAs. </p>
<blockquote>
<p>本案研究通过开发 Estuary（一个开源的多模式框架，用于构建低延迟的实时社交交互代理）的经验，展示了我们以用户为中心的社会智能代理（SIA）设计模型的开发框架。我们利用快速评估流程（RAP）收集社会智能领域领军研究者对当前技术状况的看法以及他们对Estuary如何可能解决当前研究空白进行的评估。我们通过社区内的一位研究员进行的一系列终端用户访谈来实现这一点。我们希望我们的研究结果不仅能帮助Estuary的持续发展，还能为其他未来的SIA框架和技术的发展提供指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14427v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了通过开发Estuary这一开源多模式框架，以用户为中心的设计模型在智能社交代理（SIA）开发框架中的应用。通过快速评估流程（RAP），收集领域内的领先研究者对SIA当前发展状况的看法，并对Estuary如何潜在解决当前研究差距进行评估。通过一系列社区研究者进行的终端用户访谈实现这一目标。本文的研究成果不仅有助于Estuary的持续发展，而且也为SIA的其他未来框架和技术的发展提供了指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了以用户为中心的设计模型在智能社交代理（SIA）开发框架中的应用。</li>
<li>通过开源多模式框架Estuary实现上述设计模型。</li>
<li>采用快速评估流程（RAP）收集专家对SIA当前状况的看法及研究差距。</li>
<li>通过终端用户访谈评估Estuary的潜力。</li>
<li>Estuary的开发旨在为其他未来SIA框架和技术的发展提供指导。</li>
<li>强调了了解用户需求和研究现状对于智能社交代理发展的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14427">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-68118124a0c6f63c8b55ed4528e03793.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Hydra-An-Agentic-Reasoning-Approach-for-Enhancing-Adversarial-Robustness-and-Mitigating-Hallucinations-in-Vision-Language-Models"><a href="#Hydra-An-Agentic-Reasoning-Approach-for-Enhancing-Adversarial-Robustness-and-Mitigating-Hallucinations-in-Vision-Language-Models" class="headerlink" title="Hydra: An Agentic Reasoning Approach for Enhancing Adversarial   Robustness and Mitigating Hallucinations in Vision-Language Models"></a>Hydra: An Agentic Reasoning Approach for Enhancing Adversarial   Robustness and Mitigating Hallucinations in Vision-Language Models</h2><p><strong>Authors: Chung-En,  Yu,  Hsuan-Chih,  Chen, Brian Jalaian, Nathaniel D. Bastian</strong></p>
<p>To develop trustworthy Vision-Language Models (VLMs), it is essential to address adversarial robustness and hallucination mitigation, both of which impact factual accuracy in high-stakes applications such as defense and healthcare. Existing methods primarily focus on either adversarial defense or hallucination post-hoc correction, leaving a gap in unified robustness strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that enhances plug-in VLMs through iterative reasoning, structured critiques, and cross-model verification, improving both resilience to adversarial perturbations and intrinsic model errors. Hydra employs an Action-Critique Loop, where it retrieves and critiques visual information, leveraging Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to both adversarial manipulations and intrinsic model errors, making it robust to malicious perturbations and hallucination-related inaccuracies. We evaluate Hydra on four VLMs, three hallucination benchmarks, two adversarial attack strategies, and two adversarial defense methods, assessing performance on both clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs and state-of-the-art (SOTA) dehallucination methods, even without explicit adversarial defenses, demonstrating enhanced robustness and factual consistency. By bridging adversarial resistance and hallucination mitigation, Hydra provides a scalable, training-free solution for improving the reliability of VLMs in real-world applications. </p>
<blockquote>
<p>要开发可信赖的视觉语言模型（VLMs），解决对抗性稳健和幻觉减轻问题至关重要，两者都会影响高风险应用（如国防和医疗保健）的事实准确性。现有方法主要关注对抗性防御或幻觉事后修正，在统一稳健性策略方面存在差距。我们引入了Hydra，这是一种自适应智能框架，它通过迭代推理、结构批评和跨模型验证增强即插即用型VLMs，提高了对抗性扰动和内在模型错误的抵抗力。Hydra采用行动-批评循环，检索并批评视觉信息，利用思维链（CoT）和上下文学习（ICL）技术动态优化输出。与静态事后修正方法不同，Hydra能够适应对抗性操纵和内在模型错误，使其对恶意扰动和幻觉相关的不准确具有稳健性。我们在四种VLMs、三个幻觉基准测试、两种对抗性攻击策略和两种对抗性防御方法上评估了Hydra的性能，在干净和对抗性输入上的表现。结果表明，即使在没有任何明确的对抗性防御措施的情况下，Hydra也超过了即插即用型VLMs和最新去幻觉方法，显示出增强的稳健性和事实一致性。通过桥接对抗性抵抗和幻觉减轻，Hydra为提高VLM在现实世界应用中的可靠性提供了可伸缩、无需培训的训练后解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14395v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注于开发可靠的视觉语言模型（VLMs），特别强调了对抗性鲁棒性和消除幻觉的重要性，两者都影响高风险应用（如国防和医疗保健）中的事实准确性。文章提出了一种名为Hydra的适应性代理框架，它通过迭代推理、结构化批评和跨模型验证来增强即插即用型VLMs的鲁棒性。Hydra采用行动-评论循环，利用思维链和上下文学习技术动态完善输出。相较于静态的后校正方法，Hydra能够适应对抗性操作和内在模型错误，从而增强鲁棒性对抗恶意干扰和幻觉相关的不准确性。实验结果显示，Hydra在不进行显式对抗性防御的情况下，在清洁和对抗性输入上的性能超过了即插即用型VLMs和最新消除幻觉方法，展现出增强的鲁棒性和事实一致性。总的来说，Hydra以一种可伸缩的方式在非训练环境中改善了VLMs在现实世界的可靠性问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开发可靠的视觉语言模型（VLMs）需要解决对抗性鲁棒性和消除幻觉两大关键问题。这两个问题对于高风险的场景应用如国防和医疗保健尤为重要。</li>
<li>现有方法主要关注对抗性防御或幻觉后校正，缺乏统一的稳健性策略。因此提出了一种名为Hydra的适应性代理框架作为解决方案。它不仅能够对抗外部干扰和修正内部模型错误，还可以提升VLMs的事实一致性。它运用了一种被称为行动-评论循环的机制，这种机制结合了思维链和上下文学习技术以动态优化输出。此外，通过集成跨模型验证的结构化批评流程提高性能评估标准有助于实现对内嵌入模型中隐性和显性问题更为全面且精准的检测与纠正能力。同时相较于传统静态后校正方法具备更高的适应灵活性这一点值得关注后续利用中的优势和价值，这表明它提供了一种通用的增强安全性和健壮性的有效策略即使在新型攻击手段层出不穷的情况下也能保持其适用性并应对未来可能出现的未知威胁和挑战。同时这一框架具备训练外可扩展性特点使得其在实际应用中的部署和维护成本得以显著降低并提高了系统的容错能力和容错效能以适应多变的市场需求和多样化的复杂场景使其更符合高效自动化的发展趋势；特别是在复杂环境下利用其鲁棒性和稳定性来减少潜在风险和成本增加长期运营的安全保障这一环节上扮演至关重要的角色以更好地应对实际应用中所面临的挑战并不断提升客户对产品和服务的使用体验和满意度优化。这些信息展示了其良好的实际应用前景以及它在视觉语言模型领域中的巨大潜力与商业价值前景值得期待未来进一步的研发与应用推广以及行业内的广泛应用与认可等后续的发展动向及其对社会和人类发展的积极贡献意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14395">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5e50cd3455d2637e42cb4507dca65d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa558f0407a85c0fd29c66cce9d210ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12f8a538f1dcfee5370231de7774985f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Causal-Copilot-An-Autonomous-Causal-Analysis-Agent"><a href="#Causal-Copilot-An-Autonomous-Causal-Analysis-Agent" class="headerlink" title="Causal-Copilot: An Autonomous Causal Analysis Agent"></a>Causal-Copilot: An Autonomous Causal Analysis Agent</h2><p><strong>Authors:Xinyue Wang, Kun Zhou, Wenyi Wu, Har Simrat Singh, Fang Nan, Songyao Jin, Aryan Philip, Saloni Patnaik, Hou Zhu, Shivam Singh, Parjanya Prashant, Qian Shen, Biwei Huang</strong></p>
<p>Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data – including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle – expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at <a target="_blank" rel="noopener" href="https://causalcopilot.com/">https://causalcopilot.com/</a>. </p>
<blockquote>
<p>因果分析在科学发现和可靠决策制定中发挥着基础性作用，然而由于其在概念和算法上的复杂性，领域专家很难接触到它。因果方法论与实际应用之间的脱节带来了双重挑战：领域专家无法利用因果学习方面的最新进展，而因果研究者缺乏广泛的真实世界部署来测试和精进他们的方法。为解决这一问题，我们引入了因果协同飞行员（Causal-Copilot），这是一个在大语言模型框架内运行专家级因果分析的自主体。因果协同飞行员自动化了因果分析的完整流程，无论是表格数据还是时间序列数据，包括因果发现、因果推断、算法选择、超参数优化、结果解读和可实施洞察的生成。它通过自然语言支持交互式改进，降低了非专业人士的门槛，同时保持了方法的严谨性。通过整合超过20项最先进的因果分析技术，我们的系统形成了一个良性循环——扩大了领域专家对高级因果方法的访问权限，同时生成了丰富、真实的应用程序来推动和发展因果理论。实证评估表明，与现有基线相比，因果协同飞行员取得了卓越的性能表现，提供了一个可靠、可扩展和可扩展的解决方案，缩小了理论复杂性和真实世界应用之间在因果分析方面的差距。因果协同飞行员的实时互动演示可在<a target="_blank" rel="noopener" href="https://causalcopilot.com/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://causalcopilot.com/上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13263v2">PDF</a> </p>
<p><strong>Summary</strong><br>     因果分析在科学发现和可靠决策中起着基础作用，但由于其概念和算法的复杂性，领域专家很难接触和应用它。为此，我们推出因果推理辅助系统（Causal-Copilot），这是一个自主的智能体，能够在大型语言模型框架中实现专家级的因果分析。该系统自动化因果分析的全流程，包括因果发现、因果推理、算法选择、超参数优化、结果解读和行动建议等。它支持通过自然语言进行交互式改进，降低了非专业人士的门槛，同时保持了方法论严谨性。实证评估表明，Causal-Copilot与现有基线相比性能优越，为实现理论与实践相结合的因果分析提供了一个可靠、可扩展和可扩展的解决方案。有关Causal-Copilot的实时交互式演示，请访问：[<a target="_blank" rel="noopener" href="https://causalcopilot.com/]">https://causalcopilot.com/]</a> 。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>因果分析的重要性和实践困难：虽然因果分析在科学发现和决策制定中起着关键作用，但由于其复杂的概念和算法，领域专家难以应用。</li>
<li>Causal-Copilot系统的介绍：这是一个自主的智能体，能够在大型语言模型框架内实现专家级的因果分析。</li>
<li>Causal-Copilot的自动化功能：该系统自动化了因果分析的全流程，包括因果发现、推理、算法选择等。</li>
<li>交互式改进和自然语言支持：Causal-Copilot支持通过自然语言进行交互式改进，便于非专业人士使用。</li>
<li>系统整合了多种先进因果分析方法：整合超过20种最先进的因果分析方法，为领域专家提供更广泛的访问途径。</li>
<li>实证评估结果：Causal-Copilot在性能上优于现有基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13263">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84f03acd7ba6d9be98256cc26190023d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61250f09e027593c1cd474189a2b0cd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Cross-environment-Cooperation-Enables-Zero-shot-Multi-agent-Coordination"><a href="#Cross-environment-Cooperation-Enables-Zero-shot-Multi-agent-Coordination" class="headerlink" title="Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination"></a>Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination</h2><p><strong>Authors:Kunal Jha, Wilka Carvalho, Yancheng Liang, Simon S. Du, Max Kleiman-Weiner, Natasha Jaques</strong></p>
<p>Zero-shot coordination (ZSC), the ability to adapt to a new partner in a cooperative task, is a critical component of human-compatible AI. While prior work has focused on training agents to cooperate on a single task, these specialized models do not generalize to new tasks, even if they are highly similar. Here, we study how reinforcement learning on a distribution of environments with a single partner enables learning general cooperative skills that support ZSC with many new partners on many new problems. We introduce two Jax-based, procedural generators that create billions of solvable coordination challenges. We develop a new paradigm called Cross-Environment Cooperation (CEC), and show that it outperforms competitive baselines quantitatively and qualitatively when collaborating with real people. Our findings suggest that learning to collaborate across many unique scenarios encourages agents to develop general norms, which prove effective for collaboration with different partners. Together, our results suggest a new route toward designing generalist cooperative agents capable of interacting with humans without requiring human data. </p>
<blockquote>
<p>零拍摄协调（ZSC）是适应合作任务中新伙伴的能力，是人类兼容人工智能的关键组成部分。虽然之前的工作主要集中在训练代理人在单一任务上进行合作，但这些专用模型并不适用于新任务，即使它们高度相似。在这里，我们研究了在单一伙伴的环境分布上进行强化学习如何使代理学习到通用的合作技能，这些技能支持其与许多新伙伴在许多新问题上进行零拍摄协调。我们介绍了两个基于Jax的程式生成器，可以创建数十亿的可解决协调挑战。我们开发了一种称为“跨环境合作”（CEC）的新范式，并证明它在与真实人类合作时，在数量和质量上都优于竞争对手的基线。我们的研究结果表明，在多种独特场景下学习合作鼓励代理人发展通用规范，这些规范在与不同伙伴合作时证明是有效的。总之，我们的结果为人机交互领域开辟了一条新途径，即设计通用合作代理，无需人类数据即可与人类进行交互。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12714v2">PDF</a> Accepted to CogSci 2025, In-review for ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了零射击协调（ZSC）在人工智能与人类合作中的重要性。通过强化学习在单一伙伴环境中的任务分布，学习通用的合作技能，支持与新伙伴在新问题上的零射击协调。引入基于Jax的程序生成器创建数十亿可解决的协调挑战，并展示了一种名为跨环境合作（CEC）的新方法在实际协作中的表现优于竞争对手。这表明在多个独特场景下学习合作鼓励代理人建立通用的规范，这些规范在与不同伙伴协作时非常有效。这为设计能够与人类互动而无需人类数据的通用合作代理人开辟了一条新途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习在多种环境下的单一伙伴合作有助于学习通用的合作技能。</li>
<li>零射击协调（ZSC）是人工智能与人类合作的关键组成部分。</li>
<li>跨环境合作（CEC）是一种新方法，能有效协作并优于竞争对手。</li>
<li>使用基于Jax的程序生成器创建数十亿可解决的协调挑战。</li>
<li>与真实人的协作证明CEC方法在实际环境中的有效性。</li>
<li>在多个独特场景下学习合作鼓励代理人建立通用规范。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-53d563d2b972afc167c3e63f43951df5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-470c8076a8216ba0987ffb785ad13890.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3144b9b1996349a2c29a0cf58fbf8563.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3342337384c37ed3a6a6b8d47d13d4ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a83d1212e9912421c64f10dc3efec6f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26ed93c18608ec8aadd5e464567f736c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CHARMS-A-Cognitive-Hierarchical-Agent-for-Reasoning-and-Motion-Stylization-in-Autonomous-Driving"><a href="#CHARMS-A-Cognitive-Hierarchical-Agent-for-Reasoning-and-Motion-Stylization-in-Autonomous-Driving" class="headerlink" title="CHARMS: A Cognitive Hierarchical Agent for Reasoning and Motion   Stylization in Autonomous Driving"></a>CHARMS: A Cognitive Hierarchical Agent for Reasoning and Motion   Stylization in Autonomous Driving</h2><p><strong>Authors:Jingyi Wang, Duanfeng Chu, Zejian Deng, Liping Lu, Pan Zhou</strong></p>
<p>To address the challenges of limited behavioral intelligence and overly simplified vehicle behavior modeling in autonomous driving simulations, this paper proposes the Cognitive Hierarchical Agent for Reasoning and Motion Stylization (CHARMS). Leveraging Level-k game theory, we model human driver decision-making using reinforcement learning pretraining and supervised fine-tuning. This enables the resulting models to exhibit diverse behaviors, improving the intelligence and realism of surrounding vehicles in simulation. Building upon this capability, we further develop a scenario generation framework that utilizes the Poisson cognitive hierarchy theory to control the distribution of vehicles with different driving styles through Poisson and binomial sampling. Experimental results demonstrate that CHARMS is capable of both making intelligent decisions as an ego vehicle and generating diverse, realistic driving scenarios as surrounding vehicles. The code for CHARMS will be released at <a target="_blank" rel="noopener" href="https://github.com/WUTAD-Wjy/CHARMS">https://github.com/WUTAD-Wjy/CHARMS</a>. </p>
<blockquote>
<p>针对自动驾驶模拟中行为智能有限和车辆行为建模过于简单化的挑战，本文提出了用于推理和动作风格化的认知分层代理（CHARMS）。我们利用Level-k博弈理论，使用强化学习进行预训练和监督微调，对人类驾驶员的决策制定进行建模。这使得模型能够表现出多种行为，提高了模拟中周围车辆的智能和真实感。在此基础上，我们进一步开发了一个情景生成框架，该框架利用Poisson认知层次理论来控制具有不同驾驶风格的车辆分布，通过Poisson和二项采样实现。实验结果表明，CHARMS不仅能作为一辆车做出智能决策，还能生成多样化、真实的驾驶场景作为周围车辆。CHARMS的代码将在<a target="_blank" rel="noopener" href="https://github.com/WUTAD-Wjy/CHARMS%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/WUTAD-Wjy/CHARMS发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02450v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为CHARMS的认知层次化智能体模型，用于解决自主驾驶模拟中的行为智能有限和车辆行为建模过于简单的问题。CHARMS模型结合了Level-k博弈理论，利用强化学习进行预训练和监督微调来模拟人类驾驶决策过程，使模型展现出多样化的行为，提高了模拟中车辆智能和真实感。此外，还开发了一个基于Poisson认知层次理论的场景生成框架，通过Poisson和二元抽样控制不同驾驶风格的车辆分布。实验结果表明，CHARMS不仅能作为自主车辆做出智能决策，还能生成多样且真实的驾驶场景。CHARMS代码已发布在：<a target="_blank" rel="noopener" href="https://github.com/WUTAD-Wjy/CHARMS">https://github.com/WUTAD-Wjy/CHARMS</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CHARMS模型结合了Level-k博弈理论来解决自主驾驶模拟中的行为智能问题。</li>
<li>通过强化学习预训练和监督微调，CHARMS模型能够模拟人类驾驶决策过程。</li>
<li>CHARMS模型展现出多样化的行为，提高了模拟中车辆的智能和真实感。</li>
<li>利用Poisson认知层次理论开发了一个场景生成框架。</li>
<li>该框架通过Poisson和二元抽样控制不同驾驶风格的车辆分布。</li>
<li>实验证明CHARMS模型能作为自主车辆做出智能决策并生成真实多样的驾驶场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02450">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1d3aa1f2dc390b921bfbc2e61f82fa2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85a52c76716ed20bcb7a21f9bedef057.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f2dcc9fbcfbb5b52c8f00362555bdc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79f8501b9908b74925f3a8a1888be96a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a76296a07001f4913bcd437425a5d78a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LLM-Agents-That-Act-Like-Us-Accurate-Human-Behavior-Simulation-with-Real-World-Data"><a href="#LLM-Agents-That-Act-Like-Us-Accurate-Human-Behavior-Simulation-with-Real-World-Data" class="headerlink" title="LLM Agents That Act Like Us: Accurate Human Behavior Simulation with   Real-World Data"></a>LLM Agents That Act Like Us: Accurate Human Behavior Simulation with   Real-World Data</h2><p><strong>Authors:Yuxuan Lu, Jing Huang, Yan Han, Bennet Bei, Yaochen Xie, Dakuo Wang, Jessie Wang, Qi He</strong></p>
<p>Recent research shows that LLMs can simulate <code>believable&#39;&#39; human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating and improving LLM&#39;s objective </code>accuracy’’ rather than the subjective &#96;&#96;believability’’ in the web action generation task, leveraging a large-scale, real-world dataset collected from online shopping human actions. We present the first comprehensive quantitative evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action generation. Our results show that fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasoning traces into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work establishes a new benchmark for evaluating LLMs in behavior simulation and offers actionable insights into how real-world action data and reasoning augmentation can enhance the fidelity of LLM agents. </p>
<blockquote>
<p>最近的研究表明，大型语言模型（LLMs）可以通过仅提示的方法模拟“可信”的人类行为来为LLM代理提供动力。在这项工作中，我们专注于评估和改进LLM在网页动作生成任务中的客观“准确性”，而非主观的“可信度”，我们利用从在线购物人类动作中收集的大规模现实数据集。我们对最先进的LLMs（如DeepSeek-R1、Llama和Claude）在网页动作生成任务上进行了首次全面的定量评估。我们的结果表明，与仅提示的方法相比，对现实行为数据进行微调会显著增强LLM生成动作的能力。此外，在模型训练中融入合成推理轨迹会带来额外的性能提升，这证明了明确理由在行为建模中的价值。这项工作为评估LLMs在行为模拟方面的表现建立了新的基准，并提供了一些实际见解，说明如何借助现实世界的行动数据和推理增强来提高LLM代理的保真度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20749v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>最新研究表明，LLMs可以通过仅使用提示的方法模拟“可信”的人类行为来为LLM代理提供动力。本文重点关注在网页动作生成任务中LLM的客观“准确性”而不是主观的“可信度”，并利用从在线购物人类动作收集的大规模现实世界数据集进行研究和评估。我们对先进LLMs（如DeepSeek-R1、Llama和Claude）在网页动作生成任务上的表现进行了首次全面的定量评估。结果表明，与仅使用提示的方法相比，在现实世界行为数据上微调LLMs可以显著提高其生成动作的能力。此外，将合成推理轨迹纳入模型训练会带来额外的性能提升，证明了明确理由在行为建模中的价值。本文为评估LLM在行为模拟方面的表现建立了新的基准，并提供实际行动数据和推理增强如何增强LLM代理的逼真度的可操作见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs能够模拟可信的人类行为以驱动代理。</li>
<li>研究集中在评估LLMs在网页动作生成任务中的准确性，而非仅关注其可信度。</li>
<li>利用大规模现实世界数据集进行评估。</li>
<li>微调LLMs在现实世界行为数据上能显著提高其在网页动作生成任务上的表现。</li>
<li>将合成推理轨迹纳入模型训练能带来额外性能提升。</li>
<li>明确理由在行为建模中具有价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20749">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee6c7683b7306e183cb7132fe0e0b30a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fcaa1453568ce56829342994a08d37e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Prompt-Flow-Integrity-to-Prevent-Privilege-Escalation-in-LLM-Agents"><a href="#Prompt-Flow-Integrity-to-Prevent-Privilege-Escalation-in-LLM-Agents" class="headerlink" title="Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents"></a>Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents</h2><p><strong>Authors:Juhee Kim, Woohyuk Choi, Byoungyoung Lee</strong></p>
<p>Large Language Models (LLMs) are combined with tools to create powerful LLM agents that provide a wide range of services. Unlike traditional software, LLM agent’s behavior is determined at runtime by natural language prompts from either user or tool’s data. This flexibility enables a new computing paradigm with unlimited capabilities and programmability, but also introduces new security risks, vulnerable to privilege escalation attacks. Moreover, user prompts are prone to be interpreted in an insecure way by LLM agents, creating non-deterministic behaviors that can be exploited by attackers. To address these security risks, we propose Prompt Flow Integrity (PFI), a system security-oriented solution to prevent privilege escalation in LLM agents. Analyzing the architectural characteristics of LLM agents, PFI features three mitigation techniques – i.e., agent isolation, secure untrusted data processing, and privilege escalation guardrails. Our evaluation result shows that PFI effectively mitigates privilege escalation attacks while successfully preserving the utility of LLM agents. </p>
<blockquote>
<p>大型语言模型（LLM）与工具相结合，可以创建出强大的LLM代理，提供广泛的服务。与传统的软件不同，LLM代理的行为在运行时由用户或工具数据的自然语言提示来确定。这种灵活性为实现具有无限能力和可编程性的新型计算范式提供了可能，但同时也带来了新的安全风险，容易受到权限提升攻击的影响。此外，用户提示容易被LLM代理以不安全的方式解释，从而产生非确定性行为，攻击者可以利用这些行为。为了解决这些安全风险，我们提出了面向系统安全的Prompt Flow Integrity（PFI）解决方案，以防止LLM代理中的权限提升。通过分析LLM代理的架构特性，PFI包含三种缓解技术，即代理隔离、安全的不受信任数据处理和权限提升护栏。我们的评估结果表明，PFI在有效缓解权限提升攻击的同时，成功保留了LLM代理的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15547v2">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLM）与工具结合，形成强大的LLM代理，提供广泛的服务。LLM代理的行为在运行时由用户或工具的数据的自然语言提示决定，具有灵活性和无限能力与可编程性，但也带来新安全风险，易受权限提升攻击。为解决这些安全风险，提出面向系统安全的Prompt Flow Integrity（PFI）解决方案，防止LLM代理中的权限提升。PFI包含三种缓解技术：代理隔离、安全非信任数据处理和权限提升护栏。评估结果显示，PFI有效缓解权限提升攻击，同时成功保留LLM代理的实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs结合工具形成强大的LLM代理，提供广泛服务。</li>
<li>LLM代理的行为由自然语言提示决定，具有灵活性和无限能力与可编程性。</li>
<li>LLM代理存在安全风险，易受到权限提升攻击。</li>
<li>Prompt Flow Integrity (PFI)是一种解决LLM代理安全问题的系统安全解决方案。</li>
<li>PFI包含三种缓解技术：代理隔离、安全非信任数据处理和权限提升护栏。</li>
<li>PFI能有效缓解权限提升攻击。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15547">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-19d7bd5341a4717ea01aea8b698adf71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a472e03b7e12364cd33207e04104554c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e6f55ac54f4619050e0d105370b2858.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5edf6bebf177390117e11da8c43c47f8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MaCTG-Multi-Agent-Collaborative-Thought-Graph-for-Automatic-Programming"><a href="#MaCTG-Multi-Agent-Collaborative-Thought-Graph-for-Automatic-Programming" class="headerlink" title="MaCTG: Multi-Agent Collaborative Thought Graph for Automatic Programming"></a>MaCTG: Multi-Agent Collaborative Thought Graph for Automatic Programming</h2><p><strong>Authors:Zixiao Zhao, Jing Sun, Zhe Hou, Zhiyuan Wei, Cheng-Hao Cai, Miao Qiao, Jin Song Dong</strong></p>
<p>With the rapid advancement of Large Language Models (LLMs), LLM-based approaches have demonstrated strong problem-solving capabilities across various domains. However, in automatic programming, a single LLM is typically limited to function-level code generation, while multi-agent systems composed of multiple LLMs often suffer from inefficient task planning. This lack of structured coordination can lead to cascading hallucinations, where accumulated errors across agents result in suboptimal workflows and excessive computational costs. To overcome these challenges, we introduce MaCTG (Multi-Agent Collaborative Thought Graph), a novel multi-agent framework that employs a dynamic graph structure to facilitate precise task allocation and controlled collaboration among LLM agents. MaCTG autonomously assigns agent roles based on programming requirements, dynamically refines task distribution through context-aware adjustments, and systematically verifies and integrates project-level code, effectively reducing hallucination errors and improving overall accuracy. MaCTG enhances cost-effectiveness by implementing a hybrid LLM deployment, where proprietary models handle complex reasoning, while open-source models are used for routine coding and validation tasks. To evaluate MaCTG’s effectiveness, we applied it to traditional image processing auto-programming tasks, achieving a state-of-the-art accuracy of 83.33%. Additionally, by leveraging its hybrid LLM configuration, MaCTG significantly reduced operational costs by 89.09% compared to existing multi-agent frameworks, demonstrating its efficiency, scalability, and real-world applicability. </p>
<blockquote>
<p>随着大型语言模型（LLM）的快速发展，基于LLM的方法已在各个领域展现出强大的问题解决能力。然而，在自动编程领域，单个LLM通常仅限于函数级别的代码生成，而由多个LLM组成的多智能体系统则常常面临任务规划效率低下的问题。这种缺乏结构化协调可能会导致级联幻觉，即智能体之间的累积错误导致工作流程不佳和计算成本过高。为了克服这些挑战，我们引入了MaCTG（多智能体协作思维图），这是一种新型的多智能体框架，采用动态图形结构来促进LLM智能体之间的精确任务分配和控制协作。MaCTG根据编程要求自主分配智能体角色，通过上下文感知调整动态优化任务分配，并系统地验证和集成项目级代码，有效地减少幻觉错误并提高整体准确性。MaCTG通过实施混合LLM部署来提高成本效益，其中专有模型负责复杂推理，而开源模型则用于常规编码和验证任务。为了评估MaCTG的有效性，我们将其应用于传统的图像处理自动编程任务，实现了最先进的83.33%的准确率。此外，通过利用其混合LLM配置，MaCTG与现有的多智能体框架相比，将运营成本降低了89.09%，证明了其效率、可扩展性和现实世界的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19245v2">PDF</a> </p>
<p><strong>Summary</strong><br>LLMs在自动编程领域展现出强大的问题解决能力，但仍存在功能级代码生成和多智能体系统任务规划低效的问题。为解决这些问题，提出MaCTG框架，采用动态图结构促进智能体间的精确任务分配和协作。MaCTG自主分配编程需求的智能体角色，动态调整任务分配并验证集成项目级代码，提高了准确性和成本效益。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在自动编程中展现出强大的问题解决能力，但仍存在功能级代码生成和多智能体系统任务规划低效的问题。</li>
<li>MaCTG框架采用动态图结构促进智能体间的精确任务分配和协作，以提高任务执行效率。</li>
<li>MaCTG自主分配编程需求的智能体角色，通过动态调整任务分配提高代码生成的准确性。</li>
<li>MaCTG通过验证和集成项目级代码，有效减少错误和降低成本。</li>
<li>MaCTG利用混合LLM部署实现复杂推理和常规编码验证任务的分离，进一步提高成本效益。</li>
<li>MaCTG在图像处理自动编程任务中取得最先进的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19245">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-912a586673677f5bca55d062ef881f0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82727d385eaa2f6351fde9203429c555.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3c7394dc5eb88a78cb4e7f2c92e7ae3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a33a66b8eeb5aa92bf6f7136e0120c40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1578cfeb42c09b81782d1a7206572139.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-23/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-198ba6e2b646fbf10214dabbcb289805.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-23  Automated Measurement of Eczema Severity with Self-Supervised Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fdf18cf6727a5ef0291e281ed754117b.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-23  Seeing from Another Perspective Evaluating Multi-View Understanding in   MLLMs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
