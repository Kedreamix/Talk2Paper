<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  SAC Neural Speech Codec with Semantic-Acoustic Dual-Stream Quantization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6bf208ea47f2b4ef56bfa79ac10f85cc')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    59 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-17-æ›´æ–°"><a href="#2025-11-17-æ›´æ–°" class="headerlink" title="2025-11-17 æ›´æ–°"></a>2025-11-17 æ›´æ–°</h1><h2 id="SAC-Neural-Speech-Codec-with-Semantic-Acoustic-Dual-Stream-Quantization"><a href="#SAC-Neural-Speech-Codec-with-Semantic-Acoustic-Dual-Stream-Quantization" class="headerlink" title="SAC: Neural Speech Codec with Semantic-Acoustic Dual-Stream Quantization"></a>SAC: Neural Speech Codec with Semantic-Acoustic Dual-Stream Quantization</h2><p><strong>Authors:Wenxi Chen, Xinsheng Wang, Ruiqi Yan, Yushen Chen, Zhikang Niu, Ziyang Ma, Xiquan Li, Yuzhe Liang, Hanlin Wen, Shunshun Yin, Ming Tao, Xie Chen</strong></p>
<p>Speech codecs that convert continuous speech signals into discrete tokens have become essential for speech language models (SLMs). However, existing codecs struggle to balance high-quality reconstruction with semantically rich representations, limiting their effectiveness in both generative and understanding tasks. In this work, we propose SAC, a neural speech codec with semantic-acoustic dual-stream quantization. By disentangling semantic and acoustic modeling into two dedicated streams, SAC enables each to be optimized for its respective role. Comprehensive evaluations show that SAC achieves strong reconstruction performance across diverse bitrates under both clean and noisy conditions, with particularly high scores on UTMOS and WER, demonstrating superior perceptual quality and intelligibility. Moreover, SAC substantially outperforms state-of-the-art codecs in semantic representation, achieving a level comparable to that of self-supervised learning (SSL) continuous embeddings. Finally, our analysis of speech disentanglement highlights the effectiveness of the dual-stream design, offering new potential for controllable speech applications.</p>
<blockquote>
<p>è¯­éŸ³ç¼–è§£ç å™¨å°†è¿ç»­çš„è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºç¦»æ•£çš„ç¬¦å·ï¼Œå¯¹äºè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰è€Œè¨€å·²ç»å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¼–è§£ç å™¨åœ¨é«˜è´¨é‡é‡å»ºä¸è¯­ä¹‰ä¸°å¯Œè¡¨ç¤ºä¹‹é—´éš¾ä»¥å–å¾—å¹³è¡¡ï¼Œåœ¨ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SACï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰è¯­ä¹‰å£°åŒæµä¼ é‡åŒ–çš„ç¥ç»è¯­éŸ³ç¼–è§£ç å™¨ã€‚é€šè¿‡å°†è¯­ä¹‰å’Œå£°éŸ³å»ºæ¨¡åˆ†ç¦»ä¸ºä¸¤ä¸ªä¸“ç”¨æµï¼ŒSACä½¿æ¯ä¸ªæµéƒ½èƒ½é’ˆå¯¹å…¶ç‰¹å®šè§’è‰²è¿›è¡Œä¼˜åŒ–ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒSACåœ¨å¹²å‡€å’Œå˜ˆæ‚çš„æ¡ä»¶ä¸‹ï¼Œåœ¨ä¸åŒæ¯”ç‰¹ç‡ä¸‹å®ç°äº†å¼ºå¤§çš„é‡å»ºæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨UTMOSå’ŒWERä¸Šçš„å¾—åˆ†å¾ˆé«˜ï¼Œè¯æ˜äº†å…¶åœ¨æ„ŸçŸ¥è´¨é‡å’Œæ¸…æ™°åº¦æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼ŒSACåœ¨è¯­ä¹‰è¡¨ç¤ºæ–¹é¢å¤§å¤§ä¼˜äºæœ€å…ˆè¿›çš„ç¼–è§£ç å™¨ï¼Œè¾¾åˆ°äº†ä¸è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¿ç»­åµŒå…¥ç›¸å½“çš„æ°´å¹³ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹è¯­éŸ³è§£ç¼ çš„åˆ†æçªå‡ºäº†åŒæµä¼ æ„Ÿè®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¯æ§è¯­éŸ³åº”ç”¨æä¾›äº†æ–°çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.16841v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨SACè¢«æå‡ºç”¨äºè§£å†³ç°æœ‰è¯­éŸ³ç¼–è§£ç å™¨åœ¨é‡å»ºè´¨é‡å’Œè¯­ä¹‰è¡¨ç¤ºæ–¹é¢çš„ä¸è¶³ã€‚SACå…·æœ‰è¯­ä¹‰å£°åŒæµä¼ åŒ–é‡åŒ–åŠŸèƒ½ï¼Œèƒ½å°†è¯­ä¹‰å’Œå£°éŸ³å»ºæ¨¡åˆ†å¼€å¤„ç†å¹¶ä¼˜åŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSACåœ¨æ¸…æ´å’Œå˜ˆæ‚æ¡ä»¶ä¸‹è·¨ä¸åŒæ¯”ç‰¹ç‡çš„é‡å»ºæ€§èƒ½å‡ºè‰²ï¼Œä¸”è¯­ä¹‰è¡¨ç¤ºæ˜æ˜¾ä¼˜äºå½“å‰é¢†å…ˆçš„ç¼–è§£ç å™¨ã€‚åŒæµä¼ è®¾è®¡å¯å¤§å¹…æé«˜è¯­éŸ³çš„è§£è€¦æ•ˆæœï¼Œä¸ºå¯æ§è¯­éŸ³åº”ç”¨æä¾›äº†æ–°çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç¥ç»ç½‘ç»œè¯­éŸ³ç¼–è§£ç å™¨SACè§£å†³äº†ç°æœ‰ç¼–è§£ç å™¨åœ¨é‡å»ºè´¨é‡å’Œè¯­ä¹‰è¡¨ç¤ºæ–¹é¢çš„éš¾é¢˜ã€‚</li>
<li>SACé‡‡ç”¨è¯­ä¹‰å£°åŒæµä¼ åŒ–é‡åŒ–ï¼Œå°†è¯­ä¹‰å’Œå£°éŸ³å»ºæ¨¡åˆ†å¼€å¤„ç†ã€‚</li>
<li>SACåœ¨å¤šç§æ¡ä»¶ä¸‹çš„é‡å»ºæ€§èƒ½å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨UTMOSå’ŒWERä¸Šçš„å¾—åˆ†è¾ƒé«˜ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„æ„ŸçŸ¥è´¨é‡å’Œæ¸…æ™°åº¦ã€‚</li>
<li>SACåœ¨è¯­ä¹‰è¡¨ç¤ºæ–¹é¢æ˜æ˜¾ä¼˜äºå½“å‰é¢†å…ˆçš„ç¼–è§£ç å™¨ï¼Œè¾¾åˆ°äº†ä¸è‡ªç›‘ç£å­¦ä¹ è¿ç»­åµŒå…¥ç›¸å½“çš„æ°´å¹³ã€‚</li>
<li>åŒæµä¼ è®¾è®¡ä½¿å¾—SACèƒ½å¤Ÿæœ‰æ•ˆè§£è€¦è¯­éŸ³ï¼Œä¸ºå¯æ§è¯­éŸ³åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
<li>SACçš„è®¾è®¡ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ¯”ç‰¹ç‡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.16841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf1ce8d40bd810b0d53fd8eb776b8fc9" align="middle">
<img src="https://picx.zhimg.com/v2-58433523bde6441a86a5d511b8796dca" align="middle">
<img src="https://picx.zhimg.com/v2-7bfe772bd70db38bb71c25f90a748b4a" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WavJEPA-Semantic-learning-unlocks-robust-audio-foundation-models-for-raw-waveforms"><a href="#WavJEPA-Semantic-learning-unlocks-robust-audio-foundation-models-for-raw-waveforms" class="headerlink" title="WavJEPA: Semantic learning unlocks robust audio foundation models for raw waveforms"></a>WavJEPA: Semantic learning unlocks robust audio foundation models for raw waveforms</h2><p><strong>Authors:Goksenin Yuksel, Pierre Guetschel, Michael Tangermann, Marcel van Gerven, Kiki van der Heijden</strong></p>
<p>Learning audio representations from raw waveforms overcomes key limitations of spectrogram-based audio representation learning, such as the long latency of spectrogram computation and the loss of phase information. Yet, while self-supervised speech representation learning from raw waveforms has been remarkably successful, these approaches have not achieved similar feats for general-purpose audio representation learning from waveforms. Here, we propose WavJEPA, a waveform-based version of the Joint-Embedding Predictive Architecture. WavJEPA leverages high-level semantic representation learning to tackle the shortcomings of representation learning at the speech unit or token level. We show that this approach substantially outperforms state-of-the-art time-domain audio foundation models across a wide variety of downstream benchmark tasks, while requiring considerably fewer computational resources. Additionally, to overcome the performance drop that time-domain models typically exhibit in noisy and reverberant real-world acoustic environments, we present WavJEPA-Nat. WavJEPA-Nat is a multi-channel extension of the WavJEPA architecture trained on simulated naturalistic scenes. We find that WavJEPA-Nat is highly robust to reverberation and noise. These results highlight the feasibility and computational efficiency of general-purpose audio representation learning from raw waveforms, showcasing the potential for low-latency, robust time-domain audio foundation models for real-world applications.</p>
<blockquote>
<p>ä»åŸå§‹æ³¢å½¢ä¸­å­¦ä¹ éŸ³é¢‘è¡¨ç¤ºå…‹æœäº†åŸºäºé¢‘è°±å›¾çš„éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„ä¸»è¦å±€é™æ€§ï¼Œå¦‚é¢‘è°±å›¾è®¡ç®—çš„é•¿å»¶è¿Ÿå’Œç›¸ä½ä¿¡æ¯çš„æŸå¤±ã€‚ç„¶è€Œï¼Œè™½ç„¶ä»åŸå§‹æ³¢å½¢ä¸­è¿›è¡Œè‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨ç¤ºå­¦ä¹ å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†è¿™äº›æ–¹æ³•åœ¨æ³¢å½¢ä¸Šè¿›è¡Œé€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ æ–¹é¢å°šæœªå®ç°ç±»ä¼¼çš„æˆå°±ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ³¢å½¢çš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„çš„æ³¢å½¢ç‰ˆæœ¬â€”â€”WavJEPAã€‚WavJEPAåˆ©ç”¨é«˜çº§è¯­ä¹‰è¡¨ç¤ºå­¦ä¹ æ¥è§£å†³è¯­éŸ³å•å…ƒæˆ–æ ‡è®°çº§åˆ«è¡¨ç¤ºå­¦ä¹ çš„ä¸è¶³ä¹‹å¤„ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä¸‹æ¸¸åŸºå‡†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°çš„æ—¶åŸŸéŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ŒåŒæ—¶éœ€è¦æ›´å°‘çš„è®¡ç®—èµ„æºã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœæ—¶åŸŸæ¨¡å‹é€šå¸¸åœ¨å˜ˆæ‚å’Œæ··å“çš„ç°å®ä¸–ç•Œå£°å­¦ç¯å¢ƒä¸­è¡¨ç°å‡ºçš„æ€§èƒ½ä¸‹é™ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WavJEPAçš„è‡ªç„¶ç‰ˆæœ¬â€”â€”WavJEPA-Natã€‚WavJEPA-Natæ˜¯WavJEPAæ¶æ„çš„å¤šé€šé“æ‰©å±•ï¼Œç»è¿‡æ¨¡æ‹Ÿçš„è‡ªç„¶åœºæ™¯è®­ç»ƒã€‚æˆ‘ä»¬å‘ç°WavJEPA-Natå¯¹æ··å“å’Œå™ªå£°å…·æœ‰é«˜åº¦çš„é²æ£’æ€§ã€‚è¿™äº›ç»“æœçªå‡ºäº†ä»åŸå§‹æ³¢å½¢è¿›è¡Œé€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„å¯è¡Œæ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œå±•ç¤ºäº†æ—¶åŸŸéŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å®ç°ä½å»¶è¿Ÿã€ç¨³å¥æ€§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23238v3">PDF</a> Still under review</p>
<p><strong>Summary</strong>ï¼šä»åŸå§‹æ³¢å½¢å­¦ä¹ éŸ³é¢‘è¡¨ç¤ºå…‹æœäº†åŸºäºå…‰è°±å›¾çš„éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„å…³é”®é™åˆ¶ï¼Œå¦‚å…‰è°±å›¾è®¡ç®—çš„é•¿å»¶è¿Ÿå’Œç›¸ä½ä¿¡æ¯çš„ä¸¢å¤±ã€‚è™½ç„¶è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºå­¦ä¹ ä»åŸå§‹æ³¢å½¢ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†è¿™äº›æ–¹æ³•å¹¶æœªåœ¨é€šç”¨éŸ³é¢‘è¡¨ç¤ºçš„æ³¢å½¢å­¦ä¹ ä¸­å–å¾—ç±»ä¼¼æˆæœã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†WavJEPAï¼Œä¸€ä¸ªåŸºäºæ³¢å½¢çš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ã€‚WavJEPAåˆ©ç”¨é«˜çº§è¯­ä¹‰è¡¨ç¤ºå­¦ä¹ æ¥è§£å†³åœ¨è¯­éŸ³å•å…ƒæˆ–ä»¤ç‰Œçº§åˆ«çš„è¡¨ç¤ºå­¦ä¹ çš„ä¸è¶³ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæ—¶åŸŸéŸ³é¢‘åŸºç¡€æ¨¡å‹çš„æœ€æ–°æŠ€æœ¯ï¼Œåœ¨å„ç§ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶æ‰€éœ€çš„è®¡ç®—èµ„æºå¤§å¤§å‡å°‘ã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœæ—¶åŸŸæ¨¡å‹åœ¨å˜ˆæ‚å’Œæ··å“çš„ç°å®ä¸–ç•Œå£°å­¦ç¯å¢ƒä¸­é€šå¸¸å‡ºç°çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WavJEPA-Natã€‚WavJEPA-Natæ˜¯WavJEPAæ¶æ„çš„å¤šé€šé“æ‰©å±•ï¼Œç»è¿‡æ¨¡æ‹Ÿçš„è‡ªç„¶åœºæ™¯è®­ç»ƒã€‚æˆ‘ä»¬å‘ç°WavJEPA-Natå¯¹æ··å“å’Œå™ªå£°å…·æœ‰é«˜åº¦çš„é²æ£’æ€§ã€‚è¿™äº›ç»“æœçªå‡ºäº†ä»åŸå§‹æ³¢å½¢è¿›è¡Œé€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„å¯è¡Œæ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œå±•ç¤ºäº†æ—¶åŸŸéŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ä½å»¶è¿Ÿå’Œç¨³å¥æ€§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å­¦ä¹ éŸ³é¢‘è¡¨ç¤ºä»åŸå§‹æ³¢å½¢å…‹æœäº†åŸºäºå…‰è°±å›¾çš„è¡¨ç¤ºå­¦ä¹ çš„é™åˆ¶ï¼Œå¦‚è®¡ç®—å»¶è¿Ÿå’Œç›¸ä½ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è¯­éŸ³è¡¨ç¤ºçš„æ³¢å½¢å­¦ä¹ ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨é€šç”¨éŸ³é¢‘è¡¨ç¤ºçš„æ³¢å½¢å­¦ä¹ ä¸­ä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>WavJEPAæ˜¯ä¸€ä¸ªåŸºäºæ³¢å½¢çš„è”åˆåµŒå…¥é¢„æµ‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³è¯­éŸ³å•å…ƒæˆ–ä»¤ç‰Œçº§åˆ«çš„è¡¨ç¤ºå­¦ä¹ ä¸­çš„ä¸è¶³ã€‚</li>
<li>WavJEPAåœ¨å¤šç§ä¸‹æ¸¸åŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºæ—¶åŸŸéŸ³é¢‘åŸºç¡€æ¨¡å‹çš„æœ€æ–°æŠ€æœ¯ï¼Œä¸”è®¡ç®—èµ„æºæ¶ˆè€—è¾ƒå°‘ã€‚</li>
<li>WavJEPA-Natæ˜¯WavJEPAçš„å¤šé€šé“æ‰©å±•ï¼Œå¯¹æ··å“å’Œå™ªå£°å…·æœ‰é«˜åº¦çš„é²æ£’æ€§ã€‚</li>
<li>ä»åŸå§‹æ³¢å½¢è¿›è¡Œé€šç”¨éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ å…·æœ‰å¯è¡Œæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23238">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-409d0fa6d1e92e4f9e8744946e844e1b" align="middle">
<img src="https://picx.zhimg.com/v2-c735173da4a88f1d28eb6c057e57b140" align="middle">
<img src="https://picx.zhimg.com/v2-05c651684ff51c55d73f5108723f1104" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hybrid-Pruning-In-Situ-Compression-of-Self-Supervised-Speech-Models-for-Speaker-Verification-and-Anti-Spoofing"><a href="#Hybrid-Pruning-In-Situ-Compression-of-Self-Supervised-Speech-Models-for-Speaker-Verification-and-Anti-Spoofing" class="headerlink" title="Hybrid Pruning: In-Situ Compression of Self-Supervised Speech Models for Speaker Verification and Anti-Spoofing"></a>Hybrid Pruning: In-Situ Compression of Self-Supervised Speech Models for Speaker Verification and Anti-Spoofing</h2><p><strong>Authors:Junyi Peng, Lin Zhang, Jiangyu Han, OldÅ™ich Plchot, Johan Rohdin, Themos Stafylakis, Shuai Wang, Jan ÄŒernockÃ½</strong></p>
<p>Although large-scale self-supervised learning (SSL) models like WavLM have achieved state-of-the-art performance in speech processing, their significant size impedes deployment on resource-constrained devices. While structured pruning is a key technique for model compression, existing methods typically separate it from task-specific fine-tuning. This multi-stage approach struggles to create optimal architectures tailored for diverse downstream tasks. In this work, we introduce a unified framework that integrates structured pruning into the downstream fine-tuning process. Our framework unifies these steps, jointly optimizing for task performance and model sparsity in a single stage. This allows the model to learn a compressed architecture specifically for the end task, eliminating the need for complex multi-stage pipelines and knowledge distillation. Our pruned models achieve up to a 70% parameter reduction with negligible performance degradation on large-scale datasets, achieving equal error rates of 0.7%, 0.8%, and 1.6% on Vox1-O, -E, and -H, respectively. Furthermore, our approach demonstrates improved generalization in low-resource scenarios, reducing overfitting and achieving a state-of-the-art 3.7% EER on ASVspoof5.</p>
<blockquote>
<p>è™½ç„¶åƒWavLMè¿™æ ·çš„å¤§è§„æ¨¡è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„å¤§è§„æ¨¡éƒ¨ç½²åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå­˜åœ¨å›°éš¾ã€‚è™½ç„¶ç»“æ„åŒ–å‰ªææ˜¯æ¨¡å‹å‹ç¼©çš„å…³é”®æŠ€æœ¯ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸å°†å…¶ä¸ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒåˆ†å¼€ã€‚è¿™ç§å¤šé˜¶æ®µçš„æ–¹æ³•å¾ˆéš¾ä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡åˆ›å»ºæœ€ä½³æ¶æ„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç»“æ„åŒ–å‰ªæé›†æˆåˆ°ä¸‹æ¸¸å¾®è°ƒè¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»Ÿä¸€äº†è¿™äº›æ­¥éª¤ï¼Œè”åˆä¼˜åŒ–ä»»åŠ¡æ€§èƒ½å’Œæ¨¡å‹ç¨€ç–æ€§ï¼Œåœ¨ä¸€ä¸ªé˜¶æ®µå†…å®Œæˆã€‚è¿™å…è®¸æ¨¡å‹ä¸ºç»ˆç«¯ä»»åŠ¡å­¦ä¹ ç‰¹å®šçš„å‹ç¼©æ¶æ„ï¼Œä»è€Œæ¶ˆé™¤äº†å¤æ‚çš„å¤šé˜¶æ®µç®¡é“å’ŒçŸ¥è¯†è’¸é¦çš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„ä¿®å‰ªæ¨¡å‹å®ç°äº†é«˜è¾¾70%çš„å‚æ•°å‡å°‘ï¼Œåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸‹é™å¾®ä¹å…¶å¾®ï¼Œåœ¨Vox1-Oã€-Eå’Œ-Hä¸Šçš„é”™è¯¯ç‡åˆ†åˆ«ä¸º0.7%ã€0.8%å’Œ1.6%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨èµ„æºç¨€ç¼ºçš„åœºæ™¯ä¸­å±•ç¤ºå‡ºäº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘äº†è¿‡æ‹Ÿåˆç°è±¡ï¼Œå¹¶åœ¨ASVspoof5ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„3.7%çš„EERï¼ˆç­‰é”™è¯¯ç‡ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16232v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†ç»“æ„åŒ–å‰ªæé›†æˆåˆ°ä¸‹æ¸¸å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œä»¥é’ˆå¯¹ç»ˆç«¯ä»»åŠ¡å­¦ä¹ å‹ç¼©æ¶æ„ã€‚è¯¥æ–¹æ³•å®ç°äº†åœ¨å•é˜¶æ®µä¸­å¯¹ä»»åŠ¡æ€§èƒ½å’Œæ¨¡å‹ç¨€ç–æ€§çš„è”åˆä¼˜åŒ–ï¼Œæ¶ˆé™¤äº†å¤æ‚çš„å¤šé˜¶æ®µç®¡é“å’ŒçŸ¥è¯†è’¸é¦çš„éœ€æ±‚ã€‚ç»è¿‡å‰ªæçš„æ¨¡å‹åœ¨å¤§å‹æ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾70%çš„å‚æ•°å‡å°‘ï¼ŒåŒæ—¶æ€§èƒ½å‡ ä¹æ²¡æœ‰é™ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºç»Ÿä¸€æ¡†æ¶ï¼Œå°†ç»“æ„åŒ–å‰ªæä¸ä¸‹æ¸¸å¾®è°ƒè¿‡ç¨‹ç»“åˆï¼Œå®ç°æ¨¡å‹å‹ç¼©ã€‚</li>
<li>æ¡†æ¶åœ¨å•é˜¶æ®µè”åˆä¼˜åŒ–ä»»åŠ¡æ€§èƒ½å’Œæ¨¡å‹ç¨€ç–æ€§ã€‚</li>
<li>å‰ªææ¨¡å‹åœ¨å¤§å‹æ•°æ®é›†ä¸Šå®ç°é«˜è¾¾70%çš„å‚æ•°å‡å°‘ã€‚</li>
<li>å‰ªææ¨¡å‹åœ¨è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­æ€§èƒ½å‡ ä¹æ²¡æœ‰é™ä½ã€‚</li>
<li>æ¡†æ¶æ¶ˆé™¤äº†å¤æ‚çš„å¤šé˜¶æ®µç®¡é“å’ŒçŸ¥è¯†è’¸é¦çš„éœ€æ±‚ã€‚</li>
<li>æ¡†æ¶åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬Vox1-Oã€-Eå’Œ-Hç­‰æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1c4bd73b5e52cb80018b220f1afaefd" align="middle">
<img src="https://picx.zhimg.com/v2-85431f472b9e5381983bfb1efbd30e6d" align="middle">
<img src="https://picx.zhimg.com/v2-011abdd956d7b0fa3ed2a353670554bf" align="middle">
<img src="https://picx.zhimg.com/v2-44f15007470aba4db055a8906bf11852" align="middle">
<img src="https://picx.zhimg.com/v2-2f89376e8347605503ba39f376a4f2c4" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions"><a href="#MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions" class="headerlink" title="MiDashengLM: Efficient Audio Understanding with General Audio Captions"></a>MiDashengLM: Efficient Audio Understanding with General Audio Captions</h2><p><strong>Authors:Heinrich Dinkel, Gang Li, Jizhong Liu, Jian Luan, Yadong Niu, Xingwei Sun, Tianzi Wang, Qiyang Xiao, Junbo Zhang, Jiahao Zhou</strong></p>
<p>Current approaches for large audio language models (LALMs) often rely on closed data sources or proprietary models, limiting their generalization and accessibility. This paper introduces MiDashengLM, a novel open audio-language model designed for efficient and comprehensive audio understanding through the use of general audio captions using our novel ACAVCaps training dataset. MiDashengLM exclusively relies on publicly available pretraining and supervised fine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At its core, MiDashengLM integrates Dasheng, an open-source audio encoder, specifically engineered to process diverse auditory information effectively. Unlike previous works primarily focused on Automatic Speech Recognition (ASR) based audio-text alignment, our strategy centers on general audio captions, fusing speech, sound and music information into one textual representation, enabling a holistic textual representation of complex audio scenes. Lastly, MiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT) and up to 20x higher throughput than comparable models. Checkpoints are available online at <a target="_blank" rel="noopener" href="https://huggingface.co/mispeech/midashenglm-7b">https://huggingface.co/mispeech/midashenglm-7b</a> and <a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/dasheng-lm">https://github.com/xiaomi-research/dasheng-lm</a>.</p>
<blockquote>
<p>å½“å‰çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå°é—­çš„æ•°æ®æºæˆ–ä¸“æœ‰æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†MiDashengLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼€æ”¾éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨æˆ‘ä»¬åˆ›æ–°æ€§çš„ACAVCapsè®­ç»ƒæ•°æ®é›†ï¼Œè®¾è®¡ç”¨äºé«˜æ•ˆä¸”å…¨é¢çš„éŸ³é¢‘ç†è§£ã€‚MiDashengLMä»…ä¾èµ–äºå…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œç¡®ä¿å®Œå…¨é€æ˜å’Œå¯é‡å¤æ€§ã€‚å…¶æ ¸å¿ƒç»“åˆäº†Dashengè¿™ä¸€å¼€æºéŸ³é¢‘ç¼–ç å™¨ï¼Œä¸“é—¨ç”¨äºæœ‰æ•ˆå¤„ç†å„ç§å¬è§‰ä¿¡æ¯ã€‚ä¸åŒäºä¸»è¦å…³æ³¨åŸºäºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„éŸ³é¢‘æ–‡æœ¬å¯¹é½çš„å…ˆå‰å·¥ä½œï¼Œæˆ‘ä»¬çš„ç­–ç•¥ä¾§é‡äºé€šç”¨éŸ³é¢‘å­—å¹•ï¼Œå°†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯èåˆåˆ°ä¸€ä¸ªæ–‡æœ¬è¡¨ç¤ºä¸­ï¼Œå®ç°å¯¹å¤æ‚éŸ³é¢‘åœºæ™¯çš„æ•´ä½“æ–‡æœ¬è¡¨ç¤ºã€‚æœ€åï¼ŒMiDashengLMåœ¨é¦–æ¬¡å‡ºè¯æ—¶é—´ï¼ˆTTFTï¼‰æ–¹é¢æä¾›äº†é«˜è¾¾4å€çš„åŠ é€Ÿï¼Œå¹¶ä¸”åœ¨ååé‡æ–¹é¢é«˜å‡ºåŒç±»æ¨¡å‹é«˜è¾¾20å€ã€‚æ£€æŸ¥ç‚¹æ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/mispeech/midashenglm-7b%E5%92%8Chttps://github.com/xiaomi-research/dasheng-lm%E5%9C%A8%E7%BA%BF%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/mispeech/midashenglm-7bå’Œhttps://github.com/xiaomi-research/dasheng-lmåœ¨çº¿è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03983v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MiDashengLMæ˜¯ä¸€ä¸ªåŸºäºå…¬å¼€æ•°æ®é›†çš„å¼€æ”¾éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨ACAVCapsè®­ç»ƒæ•°æ®é›†å®ç°é«˜æ•ˆä¸”å…¨é¢çš„éŸ³é¢‘ç†è§£ã€‚å…¶æ ¸å¿ƒç‰¹ç‚¹æ˜¯ä½¿ç”¨å¼€æºéŸ³é¢‘ç¼–ç å™¨Dashengå¤„ç†å¤šæ ·åŒ–çš„å¬è§‰ä¿¡æ¯ï¼Œå¹¶ä¸“æ³¨äºä¸€èˆ¬éŸ³é¢‘å­—å¹•ï¼Œå°†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯èåˆåˆ°ä¸€ç§æ–‡æœ¬è¡¨ç¤ºä¸­ã€‚ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒMiDashengLMæä¾›æ›´å¿«çš„é¦–æ¬¡ä»¤ç‰Œæ—¶é—´å’Œæ›´é«˜çš„ååé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MiDashengLMæ˜¯ä¸€ä¸ªå¼€æ”¾çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºå…¬å¼€æ•°æ®é›†æ„å»ºï¼Œç¡®ä¿äº†é€æ˜æ€§å’Œå¯é‡å¤æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨ACAVCapsè®­ç»ƒæ•°æ®é›†ï¼Œæå‡äº†éŸ³é¢‘ç†è§£çš„æ•ˆç‡å’Œå…¨é¢æ€§ã€‚</li>
<li>MiDashengLMçš„æ ¸å¿ƒæ˜¯å¼€æºéŸ³é¢‘ç¼–ç å™¨Dashengï¼Œèƒ½æœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–çš„å¬è§‰ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¨¡å‹ä¸“æ³¨äºä¸€èˆ¬éŸ³é¢‘å­—å¹•ï¼Œèƒ½å¤Ÿèåˆè¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯åˆ°ä¸€ç§æ–‡æœ¬è¡¨ç¤ºä¸­ã€‚</li>
<li>MiDashengLMæä¾›æ›´å¿«çš„é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼Œç›¸æ¯”å…¶ä»–æ¨¡å‹æœ€é«˜å¯è¾¾4å€é€Ÿæå‡ã€‚</li>
<li>MiDashengLMçš„ååé‡ç›¸æ¯”å…¶ä»–æ¨¡å‹æœ€é«˜å¯è¾¾20å€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e37337f9abb41aa7c4e63b8ae10331c" align="middle">
<img src="https://picx.zhimg.com/v2-2c0e11fb8976c04eb43489994f04d7c9" align="middle">
<img src="https://picx.zhimg.com/v2-9ef6a5f5d7ceb7f73e56b094400723bf" align="middle">
<img src="https://picx.zhimg.com/v2-c0dc06c5f2be19ac784c4dd6086ec922" align="middle">
<img src="https://picx.zhimg.com/v2-b5c1921345c64b85390c1096caba949c" align="middle">
<img src="https://picx.zhimg.com/v2-4db33b1d59e2ab45c1c29d909f9b7e01" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Hearing-More-with-Less-Multi-Modal-Retrieval-and-Selection-Augmented-Conversational-LLM-Based-ASR"><a href="#Hearing-More-with-Less-Multi-Modal-Retrieval-and-Selection-Augmented-Conversational-LLM-Based-ASR" class="headerlink" title="Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented Conversational LLM-Based ASR"></a>Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented Conversational LLM-Based ASR</h2><p><strong>Authors:Bingshen Mu, Hexin Liu, Hongfei Xue, Kun Wei, Lei Xie</strong></p>
<p>Automatic Speech Recognition (ASR) aims to convert human speech content into corresponding text. In conversational scenarios, effectively utilizing context can enhance its accuracy. Large Language Modelsâ€™ (LLMs) exceptional long-context understanding and reasoning abilities enable LLM-based ASR (LLM-ASR) to leverage historical context for recognizing conversational speech, which has a high degree of contextual relevance. However, existing conversational LLM-ASR methods use a fixed number of preceding utterances or the entire conversation history as context, resulting in significant ASR confusion and computational costs due to massive irrelevant and redundant information. This paper proposes a multi-modal retrieval-and-selection method named MARS that augments conversational LLM-ASR by enabling it to retrieve and select the most relevant acoustic and textual historical context for the current utterance. Specifically, multi-modal retrieval obtains a set of candidate historical contexts, each exhibiting high acoustic or textual similarity to the current utterance. Multi-modal selection calculates the acoustic and textual similarities for each retrieved candidate historical context and, by employing our proposed near-ideal ranking method to consider both similarities, selects the best historical context. Evaluations on the Interspeech 2025 Multilingual Conversational Speech Language Model Challenge dataset show that the LLM-ASR, when trained on only 1.5K hours of data and equipped with the MARS, outperforms the state-of-the-art top-ranking system trained on 179K hours of data.</p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ—¨åœ¨å°†äººç±»è¯­éŸ³å†…å®¹è½¬æ¢ä¸ºç›¸åº”çš„æ–‡æœ¬ã€‚åœ¨å¯¹è¯åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨ä¸Šä¸‹æ–‡å¯ä»¥æé«˜å…¶å‡†ç¡®æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡ºè‰²çš„é•¿æ–‡æœ¬ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œä½¿åŸºäºLLMçš„ASRï¼ˆLLM-ASRï¼‰èƒ½å¤Ÿåˆ©ç”¨å†å²ä¸Šä¸‹æ–‡æ¥è¯†åˆ«å¯¹è¯è¯­éŸ³ï¼Œè¿™äº›è¯­éŸ³å…·æœ‰é«˜åº¦çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¯¹è¯å¼LLM-ASRæ–¹æ³•ä½¿ç”¨å›ºå®šæ•°é‡çš„å…ˆå‰å‘è¨€æˆ–æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œè¿™ä¼šå¯¼è‡´å¤§é‡çš„æ— å…³å’Œå†—ä½™ä¿¡æ¯ï¼Œä»è€Œå¯¼è‡´ASRæ··æ·†å’Œè®¡ç®—æˆæœ¬ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMARSçš„å¤šæ¨¡æ€æ£€ç´¢ä¸é€‰æ‹©æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ£€ç´¢å’Œé€‰æ‹©ä¸å½“å‰å‘è¨€æœ€ç›¸å…³çš„å£°éŸ³å’Œæ–‡å­—å†å²ä¸Šä¸‹æ–‡ï¼Œå¢å¼ºäº†å¯¹è¯å¼LLM-ASRçš„åŠŸèƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œå¤šæ¨¡æ€æ£€ç´¢ä¼šè·å–ä¸€ç»„å€™é€‰å†å²ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸ªä¸Šä¸‹æ–‡éƒ½è¡¨ç°å‡ºä¸å½“å‰å‘è¨€é«˜åº¦ç›¸ä¼¼çš„å£°éŸ³æˆ–æ–‡å­—ã€‚å¤šæ¨¡æ€é€‰æ‹©ä¼šè®¡ç®—æ¯ä¸ªæ£€ç´¢åˆ°çš„å€™é€‰å†å²ä¸Šä¸‹æ–‡çš„è¯­éŸ³å’Œæ–‡å­—ç›¸ä¼¼æ€§ï¼Œå¹¶é€šè¿‡é‡‡ç”¨æˆ‘ä»¬æå‡ºçš„ç†æƒ³æ’åæ–¹æ³•ï¼Œç»¼åˆè€ƒè™‘è¿™ä¸¤ç§ç›¸ä¼¼æ€§ï¼Œé€‰æ‹©æœ€ä½³å†å²ä¸Šä¸‹æ–‡ã€‚åœ¨Interspeech 2025å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä»…ç»è¿‡1.5Kå°æ—¶æ•°æ®è®­ç»ƒçš„LLM-ASRï¼Œé…å¤‡MARSåï¼Œå…¶æ€§èƒ½ä¼˜äºåœ¨17.9ä¸‡å°æ—¶æ•°æ®ä¸Šè®­ç»ƒçš„æœ€æ–°é¡¶å°–ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01166v2">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯åœ¨å¯¹è¯åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨å›ºå®šæ•°é‡çš„å‰é¢çš„è¯è¯­æˆ–æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´ASRæ··æ·†å’Œè®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMARSçš„å¤šæ¨¡æ€æ£€ç´¢ä¸é€‰æ‹©æ–¹æ³•ï¼Œèƒ½å¤Ÿæ£€ç´¢å¹¶é€‰æ‹©å½“å‰è¯è¯­æœ€ç›¸å…³çš„å£°éŸ³å’Œæ–‡å­—å†å²ä¸Šä¸‹æ–‡ã€‚åœ¨Interspeech 2025å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œé…å¤‡MARSçš„LLM-ASRè¡¨ç°ä¼˜äºåœ¨17.9ä¸‡å°æ—¶æ•°æ®ä¸Šè®­ç»ƒçš„å½“å‰é¡¶å°–ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ASRæŠ€æœ¯å¯å°†äººç±»è¯­éŸ³å†…å®¹è½¬åŒ–ä¸ºç›¸åº”çš„æ–‡æœ¬ã€‚</li>
<li>åœ¨å¯¹è¯åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡å¯ä»¥æé«˜ASRçš„å‡†ç¡®æ€§ã€‚</li>
<li>LLMå…·æœ‰å‡ºè‰²çš„é•¿è¯­å¢ƒç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œä½¿LLM-ASRèƒ½å¤Ÿåˆ©ç”¨å†å²è¯­å¢ƒæ¥è¯†åˆ«å…·æœ‰é«˜åº¦è¯­å¢ƒç›¸å…³æ€§çš„å¯¹è¯è¯­éŸ³ã€‚</li>
<li>ç°æœ‰å¯¹è¯LLM-ASRæ–¹æ³•ä½¿ç”¨å›ºå®šæ•°é‡çš„å‰é¢çš„è¯è¯­æˆ–æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œè¿™ä¼šå¯¼è‡´ASRæ··æ·†å’Œè®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>MARSæ–¹æ³•é€šè¿‡å¤šæ¨¡æ€æ£€ç´¢ä¸é€‰æ‹©ï¼Œèƒ½å¤Ÿé€‰æ‹©å½“å‰è¯è¯­æœ€ç›¸å…³çš„å£°éŸ³å’Œæ–‡å­—å†å²ä¸Šä¸‹æ–‡ã€‚</li>
<li>å¤šæ¨¡æ€æ£€ç´¢è·å–ä¸€ç»„å€™é€‰å†å²ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸ªå€™é€‰ä¸Šä¸‹æ–‡ä¸å½“å‰è¯è¯­åœ¨å£°éŸ³æˆ–æ–‡å­—ä¸Šé«˜åº¦ç›¸ä¼¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16f5be1a0dad019970cdc22e38c06708" align="middle">
<img src="https://picx.zhimg.com/v2-ce34ae2941c363b5996f20139cd63ed1" align="middle">
<img src="https://picx.zhimg.com/v2-4eedb0429d9d7288f99daab123ec79e8" align="middle">
<img src="https://picx.zhimg.com/v2-a916004a416ae4350e46e1a76c262466" align="middle">
<img src="https://picx.zhimg.com/v2-606873d63a2b23067cd656c9e6b8ee1e" align="middle">
<img src="https://picx.zhimg.com/v2-6bf208ea47f2b4ef56bfa79ac10f85cc" align="middle">
<img src="https://picx.zhimg.com/v2-b85faaffc2716532775e54bc6fb191b0" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Post-training-for-Deepfake-Speech-Detection"><a href="#Post-training-for-Deepfake-Speech-Detection" class="headerlink" title="Post-training for Deepfake Speech Detection"></a>Post-training for Deepfake Speech Detection</h2><p><strong>Authors:Wanying Ge, Xin Wang, Xuechen Liu, Junichi Yamagishi</strong></p>
<p>We introduce a post-training approach that adapts self-supervised learning (SSL) models for deepfake speech detection by bridging the gap between general pre-training and domain-specific fine-tuning. We present AntiDeepfake models, a series of post-trained models developed using a large-scale multilingual speech dataset containing over 56,000 hours of genuine speech and 18,000 hours of speech with various artifacts in over one hundred languages. Experimental results show that the post-trained models already exhibit strong robustness and generalization to unseen deepfake speech. When they are further fine-tuned on the Deepfake-Eval-2024 dataset, these models consistently surpass existing state-of-the-art detectors that do not leverage post-training. Model checkpoints and source code are available online.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åè®­ç»ƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼¥åˆé€šç”¨é¢„è®­ç»ƒå’Œç‰¹å®šé¢†åŸŸå¾®è°ƒä¹‹é—´çš„å·®è·ï¼Œä½¿è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹é€‚åº”äºæ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†AntiDeepfakeæ¨¡å‹ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä½¿ç”¨å¤§è§„æ¨¡å¤šè¯­è¨€è¯­éŸ³æ•°æ®é›†å¼€å‘çš„åè®­ç»ƒæ¨¡å‹ï¼ŒåŒ…å«è¶…è¿‡5.6ä¸‡å°æ—¶çš„çœŸå®è¯­éŸ³å’ŒåŒ…å«å„ç§ä¼ªé€ çš„è¶…è¿‡ä¸€ä¸‡å…«åƒå°æ—¶çš„è¯­éŸ³ï¼Œæ¶‰åŠä¸€ç™¾å¤šç§è¯­è¨€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›åè®­ç»ƒæ¨¡å‹å·²ç»è¡¨ç°å‡ºå¯¹æœªè§è¿‡çš„æ·±åº¦ä¼ªé€ è¯­éŸ³çš„å¼ºå¤§é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å½“å®ƒä»¬åœ¨Deepfake-Eval-2024æ•°æ®é›†ä¸Šè¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒæ—¶ï¼Œè¿™äº›æ¨¡å‹çš„æ€§èƒ½å§‹ç»ˆè¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ£€æµ‹å™¨ï¼Œè€Œè¿™äº›æ£€æµ‹å™¨å¹¶æœªåˆ©ç”¨åè®­ç»ƒã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å’Œæºä»£ç å‡å¯åœ¨ç½‘ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21090v4">PDF</a> Corrected previous implementation of EER calculation. Slight numerical changes in some of the results</p>
<p><strong>Summary</strong>ï¼šæå‡ºäº†ä¸€ç§ç»“åˆè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„æ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹çš„åè®­ç»ƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ—¨åœ¨ç¼©å°é€šç”¨é¢„è®­ç»ƒå’Œç‰¹å®šé¢†åŸŸå¾®è°ƒä¹‹é—´çš„å·®è·ã€‚é‡‡ç”¨å¤§å‹å¤šè¯­ç§è¯­éŸ³æ•°æ®é›†å¼€å‘çš„AntiDeepfakeç³»åˆ—åè®­ç»ƒæ¨¡å‹å¯æŠµæŠ—å„ç§ä¼ªé€ è¯­éŸ³æ”»å‡»ã€‚åˆæ­¥å®éªŒç»“æœè¯æ˜åè®­ç»ƒæ¨¡å‹å…·æœ‰è‰¯å¥½çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å½“è¿›ä¸€æ­¥åœ¨Deepfake-Eval-2024æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œè¯¥æ¨¡å‹è¡¨ç°è¶…è¿‡å½“å‰ä¸åˆ©ç”¨åè®­ç»ƒçš„æœ€å…ˆè¿›æ£€æµ‹å™¨ã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å’Œæºä»£ç å·²åœ¨çº¿å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹çš„æ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹çš„åè®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>å¼€å‘äº†AntiDeepfakeç³»åˆ—åè®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨å¤§å‹å¤šè¯­ç§è¯­éŸ³æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ¨¡å‹èƒ½åº”å¯¹å¤šç§è¯­è¨€ä¸‹çš„çœŸå®å’Œä¼ªé€ è¯­éŸ³æ ·æœ¬ã€‚</li>
<li>åè®­ç»ƒæ¨¡å‹å·²å±•ç¤ºå‡ºè‰²çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰å‰æ²¿æŠ€æœ¯ç›¸æ¯”ï¼Œç»è¿‡å¾®è°ƒçš„åè®­ç»ƒæ¨¡å‹æ€§èƒ½æ›´ä½³ã€‚</li>
<li>æ¨¡å‹æ£€æŸ¥ç‚¹å’Œæºä»£ç å·²å…¬å¼€å¯ä¾›ä½¿ç”¨ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¸ºæ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4336e5c34f57526fa80eb2b2dd479b4" align="middle">
<img src="https://picx.zhimg.com/v2-ae3c3ae8820b688b86d3a381193cd60e" align="middle">
<img src="https://picx.zhimg.com/v2-7dd6710abdf5786be68a40a535b03637" align="middle">
<img src="https://picx.zhimg.com/v2-22b182ca852b17813fcd5181fa86e1cd" align="middle">
<img src="https://picx.zhimg.com/v2-8b041095b00beae0a66dfb16c8794c29" align="middle">
<img src="https://picx.zhimg.com/v2-0ceb4106b864b8ba9fd01cb40641af51" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation"><a href="#UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation" class="headerlink" title="UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation"></a>UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation</h2><p><strong>Authors:Jinting Wang, Shan Yang, Chenxing Li, Dong Yu, Li Liu</strong></p>
<p>Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics.</p>
<blockquote>
<p>æç¤ºè¯­éŸ³ï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œæä¾›è§†è§‰è¯­éŸ³çº¿ç´¢ï¼Œæ”¯æŒå¬åŠ›å—æŸè€…å‡†ç¡®æ„ŸçŸ¥è¯­éŸ³ã€‚CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰çš„ä»»åŠ¡æ—¨åœ¨å°†CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚ç›®å‰å¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨CSè¯†åˆ«ï¼ˆCSRï¼‰ä¸Šï¼Œå³å°†è§†é¢‘å†…å®¹è½¬å½•ä¸ºæ–‡æœ¬ã€‚å› æ­¤ï¼ŒCSV2Sçš„å¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯å°†CSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç»“åˆèµ·æ¥ã€‚ç„¶è€Œï¼Œè¿™ä¸ªæµç¨‹ä¾èµ–äºæ–‡æœ¬ä½œä¸ºä¸­é—´åª’ä»‹ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯¯å·®ä¼ æ’­ä»¥åŠè¯­éŸ³å’ŒCSè§†é¢‘åŠ¨æ€ä¹‹é—´çš„æ—¶é—´ä¸å¯¹é½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›´æ¥ä»CSè§†é¢‘ç”ŸæˆéŸ³é¢‘è¯­éŸ³ï¼ˆç›´æ¥CSV2Sï¼‰å¾€å¾€å—åˆ°å›ºæœ‰çš„å¤šæ¨¡å¼å¤æ‚æ€§å’Œæœ‰é™çš„CSæ•°æ®å¯ç”¨æ€§çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniCUEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºCSV2Sçš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚UniCUEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ•´åˆäº†ä¸€ä¸ªç†è§£ä»»åŠ¡ï¼ˆCSRï¼‰ï¼Œæä¾›ç²¾ç»†çš„CSè§†è§‰è¯­ä¹‰çº¿ç´¢æ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒUniCUEèå…¥äº†ä¸€ä¸ªå§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€ä¸€ä¸ªè¯­ä¹‰å¯¹é½æ± ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®è§†è§‰è¯­ä¹‰æ˜ å°„ï¼Œä»¥åŠä¸€ä¸ªVisioPhoneticé€‚é…å™¨ï¼Œèƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¶æ„å†…æ¶èµ·ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†UniCUE-HIï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ™®é€šè¯CSæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª14åæ‰“æ‰‹åŠ¿è€…çš„11282ä¸ªè§†é¢‘ï¼Œå…¶ä¸­åŒ…æ‹¬å¬éšœäººå£«å’Œå¬åŠ›æ­£å¸¸çš„äººã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniCUEåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04134v4">PDF</a> 13 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰‹è¯­ç¼–ç çš„æç¤ºéŸ³ï¼ˆCued Speechï¼Œç®€ç§°CSï¼‰èƒ½å¤Ÿå¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œä¸ºå¬åŠ›å—æŸäººå£«æä¾›è§†è§‰éŸ³ç´ æç¤ºæ¥æ”¯æŒç²¾ç¡®æ„ŸçŸ¥è¨€è¯­ã€‚é’ˆå¯¹CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·çš„ä»»åŠ¡ï¼ˆå³CSè§†é¢‘è‡³è¯­éŸ³ç”Ÿæˆï¼‰ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹è§£å†³æ–¹æ¡ˆâ€”â€”UniCUEã€‚è¿™æ˜¯é¦–ä¸ªä¸éœ€è¦ä¾èµ–ä¸­é—´æ–‡æœ¬çš„ç›´æ¥ç”Ÿæˆè¯­éŸ³çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡é›†æˆç»†ç²’åº¦çš„CSè§†è§‰è¯­ä¹‰æç¤ºï¼Œç›´æ¥ç”Ÿæˆè¯­éŸ³ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºç»“åˆä¸€ä¸ªç†è§£ä»»åŠ¡ï¼ˆCSè¯†åˆ«ï¼‰ï¼Œæä¾›äº†ä¸€ä¸ªå§¿åŠ¿æ„ŸçŸ¥çš„è§†è§‰å¤„ç†å™¨ã€ç²¾ç¡®çš„è¯­ä¹‰æ˜ å°„æ± ä»¥åŠè¿æ¥ç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„VisioPhoneticé€‚é…å™¨ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ™®é€šè¯CSæ•°æ®é›†UniCUE-HIæ¥æ”¯æŒè¯¥æ¡†æ¶çš„åº”ç”¨ã€‚å®éªŒè¯æ˜ï¼ŒUniCUEåœ¨å¤šä¸ªè¯„ä»·æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cued Speechï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œä¸ºå¬åŠ›å—æŸäººå£«æä¾›è§†è§‰éŸ³ç´ æç¤ºã€‚</li>
<li>CSè§†é¢‘è‡³è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰ä»»åŠ¡æ—¨åœ¨å°†CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚</li>
<li>å½“å‰ç ”ç©¶å¤šèšç„¦äºCSè¯†åˆ«ï¼ˆCSRï¼‰ï¼Œå³å°†è§†é¢‘å†…å®¹è½¬å½•ä¸ºæ–‡æœ¬ã€‚</li>
<li>UniCUEæ˜¯é¦–ä¸ªç›´æ¥ç”Ÿæˆè¯­éŸ³çš„CSV2Sæ¡†æ¶ï¼Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚</li>
<li>UniCUEé›†æˆäº†ç»†ç²’åº¦çš„CSè§†è§‰è¯­ä¹‰æç¤ºï¼Œé€šè¿‡å§¿åŠ¿æ„ŸçŸ¥çš„è§†è§‰å¤„ç†å™¨ã€ç²¾ç¡®çš„è¯­ä¹‰æ˜ å°„æ± ä»¥åŠVisioPhoneticé€‚é…å™¨å®Œæˆä»»åŠ¡ã€‚</li>
<li>ä¸ºæ”¯æŒUniCUEæ¡†æ¶çš„åº”ç”¨ï¼Œæ„å»ºäº†å¤§è§„æ¨¡çš„æ™®é€šè¯CSæ•°æ®é›†UniCUE-HIã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3cf65a969e61b9e5e3b2089a86a6224b" align="middle">
<img src="https://picx.zhimg.com/v2-8d5d9e30a85565ceb0c67c2191b00a00" align="middle">
<img src="https://picx.zhimg.com/v2-603aaf8c59f2e204535672abe1c21315" align="middle">
<img src="https://picx.zhimg.com/v2-5028efd4477eab37955f93669f8fc6ee" align="middle">
<img src="https://picx.zhimg.com/v2-734b1fbcf79a02b66dac1912d553c271" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GRAM-Spatial-general-purpose-audio-representation-models-for-real-world-applications"><a href="#GRAM-Spatial-general-purpose-audio-representation-models-for-real-world-applications" class="headerlink" title="GRAM: Spatial general-purpose audio representation models for real-world applications"></a>GRAM: Spatial general-purpose audio representation models for real-world applications</h2><p><strong>Authors:Goksenin Yuksel, Marcel van Gerven, Kiki van der Heijden</strong></p>
<p>Although audio foundations models have seen great progress on a wide variety of tasks, their application in real-world acoustic environments with reverberation and noise has been less successful. Moreover, as audio foundation models are typically trained on dry, single-channel audio clips, the inherent spatial nature of real-world sound scenes is overlooked and tasks involving sound localization ruled out. To address these limitations, we propose GRAM: a General-purpose Real-world Audio Model utilizing a multi-channel masked auto-encoder approach to efficiently learn spatial audio representations from high-quality simulated real-world scenes. To evaluate the performance of GRAM and other audio foundation models in real-world sound scenes, we release Nat-HEAR: A naturalistic version of the HEAR benchmark suite comprising a simulated real-world version, as well as two new sound localization tasks. We show that the performance of GRAM surpasses all state-of-the-art self-supervised audio foundation models and speech models on both HEAR and Nat-HEAR, while using only a fraction of the training data. GRAM also showcases state-of-the-art localization performance, surpassing even supervised sound localization approaches, and can be flexibly applied either to a two-channel, binaural sound format or a four-channel, Ambisonics format. Validating GRAMâ€™s performance on real-world sound recordings demonstrates robust transfer to real-world scenes. Taken together, GRAM presents a significant advancement towards robust, spatial audio foundation models for real-world applications.</p>
<blockquote>
<p>å°½ç®¡éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†å·¨å¤§è¿›æ­¥ï¼Œä½†å®ƒä»¬åœ¨å…·æœ‰æ··å“å’Œå™ªå£°çš„ç°å®ä¸–ç•Œå£°å­¦ç¯å¢ƒä¸­çš„åº”ç”¨å´ä¸å¤ªæˆåŠŸã€‚æ­¤å¤–ï¼Œç”±äºéŸ³é¢‘åŸºç¡€æ¨¡å‹é€šå¸¸æ˜¯åœ¨å¹²ç‡¥çš„å•å£°é“éŸ³é¢‘ç‰‡æ®µä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œå› æ­¤å¿½ç•¥äº†ç°å®ä¸–ç•Œå£°éŸ³åœºæ™¯å›ºæœ‰çš„ç©ºé—´ç‰¹æ€§ï¼Œå¹¶ä¸”æ’é™¤äº†æ¶‰åŠå£°éŸ³å®šä½çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GRAMï¼šä¸€ä¸ªåˆ©ç”¨å¤šé€šé“æ©ç è‡ªç¼–ç å™¨æ–¹æ³•çš„é«˜æ•ˆç°å®ä¸–ç•ŒéŸ³é¢‘æ¨¡å‹ï¼Œä»é«˜è´¨é‡æ¨¡æ‹Ÿç°å®åœºæ™¯ä¸­å­¦ä¹ ç©ºé—´éŸ³é¢‘è¡¨ç¤ºã€‚ä¸ºäº†è¯„ä¼°GRAMå’Œå…¶ä»–éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œå£°éŸ³åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å‘å¸ƒäº†Nat-HEARï¼šä¸€ä¸ªè‡ªç„¶ä¸»ä¹‰çš„HEARåŸºå‡†æµ‹è¯•å¥—ä»¶ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿçš„ç°å®ä¸–ç•Œç‰ˆæœ¬ä»¥åŠä¸¤ä¸ªæ–°çš„å£°éŸ³å®šä½ä»»åŠ¡ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒGRAMçš„æ€§èƒ½è¶…è¿‡äº†æ‰€æœ‰æœ€å…ˆè¿›çš„è‡ªç›‘ç£éŸ³é¢‘åŸºç¡€æ¨¡å‹å’Œè¯­éŸ³æ¨¡å‹åœ¨HEARå’ŒNat-HEARä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶ä»…ä½¿ç”¨äº†å°‘é‡çš„è®­ç»ƒæ•°æ®ã€‚GRAMè¿˜å±•ç¤ºäº†æœ€å…ˆè¿›çš„å®šä½æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ç›‘ç£å¼å£°éŸ³å®šä½æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥çµæ´»åœ°åº”ç”¨äºåŒå£°é“ç«‹ä½“å£°æ ¼å¼æˆ–å››å£°é“çš„Ambisonicsæ ¼å¼ã€‚é€šè¿‡åœ¨å®é™…ä¸–ç•Œå£°éŸ³å½•åˆ¶ä¸ŠéªŒè¯GRAMçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶å¯¹äºçœŸå®åœºæ™¯çš„ç¨³å¥æ€§è½¬ç§»ã€‚æ€»çš„æ¥è¯´ï¼ŒGRAMåœ¨æ„å»ºç”¨äºå®é™…åº”ç”¨çš„ç¨³å¥ç©ºé—´éŸ³é¢‘åŸºç¡€æ¨¡å‹æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00934v4">PDF</a> Still under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå£°å­¦ç¯å¢ƒä¸­é¢å¯¹æ··å“å’Œå™ªå£°æ—¶çš„åº”ç”¨ä¸è¶³ï¼Œä»¥åŠå¿½ç•¥ç©ºé—´éŸ³é¢‘ç‰¹æ€§ç­‰é—®é¢˜ï¼Œæå‡ºäº†GRAMæ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤šé€šé“æ©ç è‡ªç¼–ç å™¨æ–¹æ³•ï¼Œä»é«˜è´¨é‡æ¨¡æ‹Ÿçš„çœŸå®åœºæ™¯ä¸­é«˜æ•ˆå­¦ä¹ ç©ºé—´éŸ³é¢‘è¡¨å¾ã€‚ä¸ºäº†è¯„ä¼°GRAMå’Œå…¶ä»–éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå£°éŸ³åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œå‘å¸ƒäº†Nat-HEARæ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¨¡æ‹ŸçœŸå®åœºæ™¯ç‰ˆæœ¬å’Œä¸¤ä¸ªæ–°çš„å£°éŸ³å®šä½ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒGRAMåœ¨HEARå’ŒNat-HEARä¸Šçš„æ€§èƒ½å‡è¶…è¿‡äº†å…¶ä»–å…ˆè¿›çš„è‡ªç›‘ç£éŸ³é¢‘åŸºç¡€æ¨¡å‹å’Œè¯­éŸ³æ¨¡å‹ï¼Œä¸”ä½¿ç”¨è®­ç»ƒæ•°æ®é‡å¾ˆå°‘ã€‚æ­¤å¤–ï¼ŒGRAMè¿˜å…·æœ‰å‡ºè‰²çš„å®šä½æ€§èƒ½ï¼Œå¯ä»¥çµæ´»åº”ç”¨äºåŒé€šé“åŒè€³å£°æ ¼å¼å’Œå››é€šé“Ambisonicsæ ¼å¼ã€‚åœ¨çœŸå®ä¸–ç•Œå½•éŸ³ä¸Šçš„è¡¨ç°è¯æ˜äº†å…¶å¯¹çœŸå®åœºæ™¯çš„ç¨³å¥æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒGRAMæ˜¯æœç€ç¨³å¥çš„ç©ºé—´éŸ³é¢‘åŸºç¡€æ¨¡å‹è¿›è¡ŒçœŸå®ä¸–ç•Œåº”ç”¨çš„é‡è¦è¿›å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GRAMæ¨¡å‹è§£å†³äº†éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå£°å­¦ç¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ··å“å’Œå™ªå£°æ¡ä»¶ä¸‹çš„æ€§èƒ½ä¸è¶³ã€‚</li>
<li>GRAMåˆ©ç”¨å¤šé€šé“æ©ç è‡ªç¼–ç å™¨æ–¹æ³•ï¼Œä»æ¨¡æ‹Ÿçš„çœŸå®åœºæ™¯ä¸­å­¦ä¹ ç©ºé—´éŸ³é¢‘è¡¨å¾ã€‚</li>
<li>å‘å¸ƒæ–°çš„æ•°æ®é›†Nat-HEARï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå£°éŸ³åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>GRAMåœ¨HEARå’ŒNat-HEARä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›çš„è‡ªç›‘ç£éŸ³é¢‘åŸºç¡€æ¨¡å‹å’Œè¯­éŸ³æ¨¡å‹ã€‚</li>
<li>GRAMå…·æœ‰å‡ºè‰²çš„å£°éŸ³å®šä½æ€§èƒ½ï¼Œå¯çµæ´»åº”ç”¨äºä¸åŒæ ¼å¼çš„éŸ³é¢‘ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œå½•éŸ³ä¸Šçš„å®éªŒè¯æ˜äº†GRAMå¯¹çœŸå®åœºæ™¯çš„ç¨³å¥æ€§ã€‚</li>
<li>GRAMæ˜¯éŸ³é¢‘åŸºç¡€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„é‡å¤§è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d472030a1ca8c64f19007ab43a0451a" align="middle">
<img src="https://picx.zhimg.com/v2-0b6265e8ce6dc5a591f86d63cf1cb8cf" align="middle">
<img src="https://picx.zhimg.com/v2-5c89cd6bd7e4b129881451c64fbc8d37" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhancing-Large-Language-Models-for-Detecting-Mental-Manipulation-via-Annotation-Free-Data-Augmentation-and-Anti-Curriculum-Distillation"><a href="#Enhancing-Large-Language-Models-for-Detecting-Mental-Manipulation-via-Annotation-Free-Data-Augmentation-and-Anti-Curriculum-Distillation" class="headerlink" title="Enhancing Large Language Models for Detecting Mental Manipulation via Annotation-Free Data Augmentation and Anti-Curriculum Distillation"></a>Enhancing Large Language Models for Detecting Mental Manipulation via Annotation-Free Data Augmentation and Anti-Curriculum Distillation</h2><p><strong>Authors:Yuansheng Gao, Han Bao, Tong Zhang, Bin Li, Jixiang Luo, Ronghao Chen, Zonghui Wang, Wenzhi Chen</strong></p>
<p>Mental manipulation is a subtle yet pervasive form of psychological abuse that poses serious threats to mental health. Nevertheless, detecting mental manipulation remains a largely underexplored research problem. The field faces three major challenges: (i) insufficient and hard-to-obtain training data; (ii) the covert nature of mental manipulation, which hinders detection; and (iii) the lack of real-world datasets. To address these challenges, we propose MentalMAC, a novel framework that enhances large language modelsâ€™ ability to detect elements of mental manipulation in multi-turn dialogue. Our approach consists of three key components: EvoSA, an annotation-free data augmentation method based on evolutionary operations and speech act theory; teacher-model-generated multi-task supervision; and progressive task-level anti-curriculum distillation. We then constructed the ReaMent dataset, comprising 5,000 real-world dialogue samples, utilizing MentalMAC-distilled models to aid in human annotation. Vast experiments show that MentalMAC achieves up to 25.9% improvement in F1mac and 8.1% in accuracy over the best-performing baseline, outperforming commercial LLMs such as GPT-4 and Claude-3.5-Sonnet. Warning: This paper contains content that may be offensive to the reader.</p>
<blockquote>
<p>å¿ƒç†æ“æ§æ˜¯ä¸€ç§å¾®å¦™è€Œæ™®éçš„å¿ƒç†è™å¾…å½¢å¼ï¼Œå¯¹å¿ƒç†å¥åº·æ„æˆä¸¥é‡å¨èƒã€‚ç„¶è€Œï¼Œæ£€æµ‹å¿ƒç†æ“æ§ä»ç„¶æ˜¯ä¸€ä¸ªè¢«å¤§å¤§å¿½è§†çš„ç ”ç©¶é—®é¢˜ã€‚è¯¥é¢†åŸŸé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼š(i)è®­ç»ƒæ•°æ®ä¸è¶³ä¸”éš¾ä»¥è·å–ï¼›(ii)å¿ƒç†æ“æ§çš„éšè”½æ€§ï¼Œé˜»ç¢äº†æ£€æµ‹ï¼›(iii)ç¼ºä¹çœŸå®ä¸–ç•Œçš„æ•°æ®é›†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MentalMACï¼Œä¸€ä¸ªå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­æ£€æµ‹å¿ƒç†æ“æ§å…ƒç´ çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šEvoSAï¼Œä¸€ç§åŸºäºè¿›åŒ–æ“ä½œå’Œè¨€è¯­è¡Œä¸ºç†è®ºçš„æ— æ³¨è§£æ•°æ®å¢å¼ºæ–¹æ³•ï¼›æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„å¤šä»»åŠ¡ç›‘ç£ï¼›ä»¥åŠæ¸è¿›çš„ä»»åŠ¡çº§åè¯¾ç¨‹è’¸é¦ã€‚æˆ‘ä»¬éšåæ„å»ºäº†RealMentæ•°æ®é›†ï¼ŒåŒ…å«5000ä¸ªçœŸå®ä¸–ç•Œå¯¹è¯æ ·æœ¬ï¼Œåˆ©ç”¨MentalMACç²¾é¦æ¨¡å‹è¾…åŠ©äººå·¥æ ‡æ³¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMentalMACåœ¨F1macæŒ‡æ ‡ä¸Šæœ€é«˜æå‡äº†25.9%ï¼Œå‡†ç¡®ç‡æå‡äº†8.1%ï¼Œè¶…è¿‡äº†æœ€ä½³åŸºçº¿è¡¨ç°ï¼Œå¹¶ä¸”åœ¨å•†ä¸šå¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPT-4å’ŒClaude-3.5-Sonnetçš„å¯¹æ¯”ä¸­è¡¨ç°å‡ºè‰²ã€‚è­¦å‘Šï¼šæœ¬æ–‡å«æœ‰å¯èƒ½å¯¹è¯»è€…é€ æˆä¸é€‚çš„å†…å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15255v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†å¿ƒç†æ“çºµè¿™ä¸€å¾®å¦™è€Œæ™®éå­˜åœ¨çš„å¿ƒç†è™å¾…å½¢å¼å¯¹å¿ƒç†å¥åº·çš„ä¸¥é‡å¨èƒã€‚æ£€æµ‹å¿ƒç†æ“çºµæ˜¯ä¸€ä¸ªå°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶çš„éš¾é¢˜ï¼Œé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šè®­ç»ƒæ•°æ®ä¸è¶³ä¸”éš¾ä»¥è·å–ã€å¿ƒç†æ“çºµçš„éšè”½æ€§é˜»ç¢æ£€æµ‹ä»¥åŠç¼ºä¹çœŸå®ä¸–ç•Œæ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MentalMACæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­æ£€æµ‹å¿ƒç†æ“çºµå…ƒç´ çš„èƒ½åŠ›ã€‚è¿™ä¸‰ä¸ªç»„ä»¶åŒ…æ‹¬åŸºäºè¿›åŒ–æ“ä½œå’Œè¨€è¯­è¡Œä¸ºç†è®ºçš„æ³¨é‡Šå…è´¹æ•°æ®å¢å¼ºæ–¹æ³•EvoSAã€æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„å¤šä»»åŠ¡ç›‘ç£ä»¥åŠæ¸è¿›çš„ä»»åŠ¡çº§åè¯¾ç¨‹è’¸é¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºäº†RealMentæ•°æ®é›†ï¼ŒåŒ…å«5000ä¸ªçœŸå®å¯¹è¯æ ·æœ¬ï¼Œåˆ©ç”¨MentalMACè’¸é¦æ¨¡å‹è¾…åŠ©äººå·¥æ ‡æ³¨ã€‚å®éªŒè¡¨æ˜ï¼ŒMentalMACç›¸è¾ƒäºæœ€ä½³åŸºçº¿æ¨¡å‹åœ¨F1macä¸Šæé«˜äº†25.9%ï¼Œå‡†ç¡®ç‡æé«˜äº†8.1%ï¼Œè¶…è¶Šäº†GPT-4å’ŒClaude-3.5-Sonnetç­‰å•†ä¸šå¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œæœ¬æ–‡å†…å®¹å¯èƒ½å¯¹æŸäº›è¯»è€…å…·æœ‰å†’çŠ¯æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒç†æ“çºµæ˜¯ä¸€ç§ä¸¥é‡çš„å¿ƒç†è™å¾…å½¢å¼ï¼Œå¯¹å¿ƒç†å¥åº·æ„æˆå¨èƒï¼Œä½†æ£€æµ‹å¿ƒç†æ“çºµæ˜¯ä¸€ä¸ªå°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶çš„éš¾é¢˜ã€‚</li>
<li>ç›®å‰é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬è®­ç»ƒæ•°æ®ä¸è¶³å’Œéš¾ä»¥è·å–ã€å¿ƒç†æ“çºµçš„éšè”½æ€§ä»¥åŠç¼ºä¹çœŸå®ä¸–ç•Œæ•°æ®é›†ã€‚</li>
<li>MentalMACæ¡†æ¶é€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šEvoSAæ•°æ®å¢å¼ºæ–¹æ³•ã€æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„å¤šä»»åŠ¡ç›‘ç£ä»¥åŠä»»åŠ¡çº§åè¯¾ç¨‹è’¸é¦ã€‚</li>
<li>æ„å»ºäº†RealMentæ•°æ®é›†ï¼Œç”¨äºè¾…åŠ©æ£€æµ‹å¿ƒç†æ“çºµçš„çœŸå®å¯¹è¯æ ·æœ¬æ ‡æ³¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMentalMACç›¸è¾ƒäºç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œä¸”è¶…è¶Šäº†ä¸€äº›å•†ä¸šå¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>æœ¬æ–‡è­¦å‘Šï¼šåŒ…å«å¯èƒ½å†’çŠ¯æŸäº›è¯»è€…çš„å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f74c744b26d20e0d80409fefbfe2dab" align="middle">
<img src="https://picx.zhimg.com/v2-343df35db79f873e16642f19dfa22df6" align="middle">
<img src="https://picx.zhimg.com/v2-6e1e92bb7df10ac2fee85de8bddfa8e9" align="middle">
<img src="https://picx.zhimg.com/v2-8f55d77323ad08edb5c0d567907a4c3b" align="middle">
<img src="https://picx.zhimg.com/v2-0494144824d73360025d5e0767be1af6" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speech-to-Speech-Dialogue-Modeling-with-End-to-End-Retrieval-Augmented-Generation"><a href="#Enhancing-Speech-to-Speech-Dialogue-Modeling-with-End-to-End-Retrieval-Augmented-Generation" class="headerlink" title="Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation"></a>Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation</h2><p><strong>Authors:Pengchao Feng, Ziyang Ma, Wenxi Chen, Yao Li, Sheng Wang, Kai Yu, Xie Chen</strong></p>
<p>End-to-end speech-to-speech (S2S) dialogue systems have recently garnered increasing research attention for their lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration of information. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind the SOTA cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. Our code and dataset are released.</p>
<blockquote>
<p>ç«¯åˆ°ç«¯è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰å¯¹è¯ç³»ç»Ÿå› å…¶è¾ƒä½çš„å»¶è¿Ÿå’Œæ›´è‡ªç„¶åœ°æ•´åˆéè¨€è¯­çº¿ç´¢ï¼ˆå¦‚æƒ…æ„Ÿå’Œè¯´è¯è€…èº«ä»½ï¼‰è€Œæœ€è¿‘å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿé¢ä¸´å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•´åˆå¤–éƒ¨çŸ¥è¯†æ–¹é¢ï¼Œè¿™æ˜¯æ–‡æœ¬åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šå¸¸è§£å†³çš„é—®é¢˜ã€‚æ ¸å¿ƒå›°éš¾åœ¨äºè¾“å…¥è¯­éŸ³å’Œæ£€ç´¢åˆ°çš„æ–‡æœ¬çŸ¥è¯†ä¹‹é—´çš„æ¨¡æ€å·®è·ï¼Œè¿™é˜»ç¢äº†ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯RAGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥ä»è¯­éŸ³æŸ¥è¯¢ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æœ¬çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç«¯åˆ°ç«¯S2Så¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ä¸Šæœ‰äº†æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„æ£€ç´¢æ•ˆç‡ã€‚å°½ç®¡æ€»ä½“æ€§èƒ½ä»ç„¶è½åäºæœ€å…ˆè¿›çš„çº§è”æ¨¡å‹ï¼Œä½†æˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¢å¼ºç«¯åˆ°ç«¯S2Sç³»ç»Ÿä¸­çš„çŸ¥è¯†æ•´åˆæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å·²ç»å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00028v2">PDF</a> Accepted to EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†ç«¯å¯¹ç«¯è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰å¯¹è¯ç³»ç»Ÿä¸­èå…¥å¤–éƒ¨çŸ¥è¯†çš„é—®é¢˜ã€‚ä¸ºç¼©å°è¾“å…¥è¯­éŸ³å’Œæ£€ç´¢æ–‡æœ¬çŸ¥è¯†é—´çš„æ¨¡æ€å·®è·ï¼Œæå‡ºä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œå¯ç›´æ¥ä»è¯­éŸ³æŸ¥è¯¢ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜S2Så¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ï¼Œå¹¶è¾¾åˆ°è¾ƒé«˜çš„æ£€ç´¢æ•ˆç‡ã€‚å°½ç®¡æ€»ä½“æ€§èƒ½ä»è½åäºå…ˆè¿›çš„çº§è”æ¨¡å‹ï¼Œä½†è¯¥æ¡†æ¶ä¸ºå¢å¼ºç«¯å¯¹ç«¯S2Sç³»ç»Ÿä¸­çš„çŸ¥è¯†èåˆæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯å¯¹ç«¯è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰å¯¹è¯ç³»ç»Ÿè¿‘å¹´æ¥å—åˆ°ç ”ç©¶å…³æ³¨ï¼Œå…¶ä½å»¶è¿Ÿå’Œæ›´è‡ªç„¶èå…¥éè¨€è¯­çº¿ç´¢ï¼ˆå¦‚æƒ…æ„Ÿå’Œè¯´è¯äººèº«ä»½ï¼‰çš„ç‰¹ç‚¹ä½¿å…¶å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>S2Sç³»ç»Ÿé¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œé€šå¸¸é€šè¿‡æ–‡æœ¬å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥è§£å†³ã€‚</li>
<li>æ¨¡æ€å·®è·æ˜¯S2Sç³»ç»Ÿé¢ä¸´çš„æ ¸å¿ƒé—®é¢˜ï¼Œä½“ç°åœ¨è¾“å…¥è¯­éŸ³å’Œæ£€ç´¢æ–‡æœ¬çŸ¥è¯†ä¹‹é—´ï¼Œé˜»ç¢äº†ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯RAGæ¡†æ¶ï¼Œå¯ç›´æ¥ä»è¯­éŸ³æŸ¥è¯¢ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬çŸ¥è¯†ï¼Œæ˜¾è‘—æé«˜S2Så¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½å’Œæ£€ç´¢æ•ˆç‡ã€‚</li>
<li>å°½ç®¡æ€»ä½“æ€§èƒ½ä»è½åäºå…ˆè¿›çš„çº§è”æ¨¡å‹ï¼Œä½†è¯¥æ¡†æ¶ä¸ºå¢å¼ºç«¯å¯¹ç«¯S2Sç³»ç»Ÿä¸­çš„çŸ¥è¯†èåˆæä¾›äº†æ–°æ–¹å‘ã€‚</li>
<li>é‡Šæ”¾ä»£ç å’Œæ•°æ®é›†ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2932519ab27ab5fc89d46b369ebe0457" align="middle">
<img src="https://picx.zhimg.com/v2-1ae1a7d681c82eabae8df366e112243c" align="middle">
<img src="https://picx.zhimg.com/v2-18b529b2b32009ba5044debfb0dbddb8" align="middle">
<img src="https://picx.zhimg.com/v2-f456b538c57c6cb91bbe43c021502568" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MultiMed-ST-Large-scale-Many-to-many-Multilingual-Medical-Speech-Translation"><a href="#MultiMed-ST-Large-scale-Many-to-many-Multilingual-Medical-Speech-Translation" class="headerlink" title="MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation"></a>MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation</h2><p><strong>Authors:Khai Le-Duc, Tuyen Tran, Bach Phan Tat, Nguyen Kim Hai Bui, Quan Dang, Hung-Phong Tran, Thanh-Thuy Nguyen, Ly Nguyen, Tuan-Minh Phan, Thi Thu Phuong Tran, Chris Ngo, Nguyen X. Khanh, Thanh Nguyen-Tang</strong></p>
<p>Multilingual speech translation (ST) and machine translation (MT) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, and Simplified&#x2F;Traditional Chinese, together with the models. With 290,000 samples, this is the largest medical MT dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most comprehensive ST analysis in the fieldâ€™s history, to our best knowledge, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed-ST">https://github.com/leduckhai/MultiMed-ST</a></p>
<blockquote>
<p>åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSTï¼‰å’Œæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰é€šè¿‡çªç ´è¯­è¨€éšœç¢å®ç°é«˜æ•ˆæ²Ÿé€šã€ç¼“è§£ä¸“ä¸šåŠ³åŠ¨åŠ›çŸ­ç¼ºä»¥åŠä¿ƒè¿›è¯Šæ–­å’Œæ²»ç–—çš„æ”¹å–„ï¼Œç‰¹åˆ«æ˜¯åœ¨ç–«æƒ…æœŸé—´ï¼Œå¢å¼ºäº†æ‚£è€…æŠ¤ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¯¹åŒ»ç–—STè¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œé€šè¿‡å‘å¸ƒMultiMed-STæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯åŒ»ç–—é¢†åŸŸçš„å¤§è§„æ¨¡STæ•°æ®é›†ï¼Œæ¶µç›–äº”ä¸ªè¯­è¨€çš„æ‰€æœ‰ç¿»è¯‘æ–¹å‘ï¼šè¶Šå—è¯­ã€è‹±è¯­ã€å¾·è¯­ã€æ³•è¯­å’Œç®€ä½“ä¸­æ–‡&#x2F;ç¹ä½“ä¸­æ–‡ï¼Œä»¥åŠç›¸åº”çš„æ¨¡å‹ã€‚åŒ…å«29ä¸‡ä¸ªæ ·æœ¬ï¼Œè¿™æ˜¯æœ€å¤§çš„åŒ»ç–—MTæ•°æ®é›†å’Œæ‰€æœ‰é¢†åŸŸä¸­æœ€å¤§çš„å¤šè¯­ç§STæ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†è¯¥é¢†åŸŸå†å²ä¸Šæœ€å…¨é¢çš„STåˆ†æï¼ŒåŒ…æ‹¬ï¼šå®è¯ç ”ç©¶åŸºå‡†çº¿ã€åŒè¯­-å¤šè¯­å¯¹æ¯”ç ”ç©¶ã€ç«¯åˆ°ç«¯ä¸çº§è”å¯¹æ¯”ç ”ç©¶ã€ä»»åŠ¡ç‰¹å®šä¸å¤šä»»åŠ¡åºåˆ—åˆ°åºåˆ—å¯¹æ¯”ç ”ç©¶ã€ä»£ç åˆ‡æ¢åˆ†æå’Œå®šé‡-å®šæ€§è¯¯å·®åˆ†æã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‡å¯åœ¨ç½‘ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed-ST%E3%80%82">https://github.com/leduckhai/MultiMed-STã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03546v3">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»ç–—é¢†åŸŸä¸­çš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSTï¼‰å’Œæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æŠ€æœ¯ã€‚é€šè¿‡å‘å¸ƒMultiMed-STå¤§è§„æ¨¡åŒ»ç–—STæ•°æ®é›†ï¼Œæœ¬æ–‡æä¾›äº†åŒ»ç–—STçš„é¦–ä¸ªç³»ç»Ÿæ€§ç ”ç©¶ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº”ç§è¯­è¨€çš„æ‰€æœ‰ç¿»è¯‘æ–¹å‘ï¼Œå¹¶é™„å¸¦æœ‰æ¨¡å‹ã€‚æ•°æ®é›†åŒ…å«29ä¸‡ä¸ªæ ·æœ¬ï¼Œæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åŒ»ç–—æœºå™¨ç¿»è¯‘æ•°æ®é›†ä»¥åŠè·¨é¢†åŸŸæœ€å¤šçš„å¤šè¯­ç§ç¿»è¯‘æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†è¯¥é¢†åŸŸå†å²ä¸Šæœ€å…¨é¢çš„STåˆ†æï¼ŒåŒ…æ‹¬å®è¯åŸºå‡†ã€åŒè¯­å¤šè¯­è¨€å¯¹æ¯”åˆ†æç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—é¢†åŸŸçš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«ç¿»è¯‘å’Œæœºå™¨ç¿»è¯‘æŠ€æœ¯é€šè¿‡å…‹æœè¯­è¨€éšœç¢ï¼Œæå‡æ‚£è€…æŠ¤ç†æ•ˆç‡ã€‚</li>
<li>å‘å¸ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒ»ç–—STæ•°æ®é›†MultiMed-STï¼ŒåŒ…å«äº”ç§è¯­è¨€çš„29ä¸‡ä¸ªæ ·æœ¬ã€‚</li>
<li>MultiMed-STæ•°æ®é›†æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åŒ»ç–—æœºå™¨ç¿»è¯‘æ•°æ®é›†å’Œè·¨é¢†åŸŸæœ€å¤šçš„å¤šè¯­ç§ç¿»è¯‘æ•°æ®é›†ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†åŒ»ç–—STçš„é¦–ä¸ªç³»ç»Ÿæ€§ç ”ç©¶ï¼ŒåŒ…æ‹¬å¯¹å¤šç§ç¿»è¯‘æ–¹æ³•ï¼ˆå¦‚å®è¯åŸºå‡†ã€åŒè¯­å¤šè¯­è¨€å¯¹æ¯”åˆ†æç­‰ï¼‰çš„è¯¦å°½åˆ†æã€‚</li>
<li>é€šè¿‡åœ¨çº¿å¹³å°åˆ†äº«äº†æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</li>
<li>ç ”ç©¶çš„é‡ç‚¹è¿˜åŒ…æ‹¬å¯¹ç«¯åˆ°ç«¯ä¸çº§è”ç¿»è¯‘çš„æ¯”è¾ƒç ”ç©¶ï¼Œä»¥åŠä»»åŠ¡ç‰¹å®šä¸å¤šä»»åŠ¡åºåˆ—åˆ°åºåˆ—çš„æ¯”è¾ƒç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2df1fadd41950e64032d19494b953f3f" align="middle">
<img src="https://picx.zhimg.com/v2-34b25e4889b717b026935494ce929d8d" align="middle">
<img src="https://picx.zhimg.com/v2-2247af5c09629ce4fe3c3b333d75ecd1" align="middle">
<img src="https://picx.zhimg.com/v2-e383c9c6d34921104f097b44d113d902" align="middle">
<img src="https://picx.zhimg.com/v2-80247dd2ae739a2cc27cfad94b02e9a4" align="middle">
<img src="https://picx.zhimg.com/v2-bf4374239637be2f1dfacfbadaff8b29" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FocalCodec-Low-Bitrate-Speech-Coding-via-Focal-Modulation-Networks"><a href="#FocalCodec-Low-Bitrate-Speech-Coding-via-Focal-Modulation-Networks" class="headerlink" title="FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks"></a>FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks</h2><p><strong>Authors:Luca Della Libera, Francesco Paissan, Cem Subakan, Mirco Ravanelli</strong></p>
<p>Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples and code are available at <a target="_blank" rel="noopener" href="https://lucadellalib.github.io/focalcodec-web/">https://lucadellalib.github.io/focalcodec-web/</a>.</p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é€šè¿‡æµ·é‡æ•°æ®çš„è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œå·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¼å±€ã€‚å—æ­¤æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶äººå‘˜å·²ç»æ¢ç´¢äº†ä½¿ç”¨ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨å°†è¿ç»­éŸ³é¢‘ç¦»æ•£åŒ–ä¸ºä»¤ç‰Œï¼Œä»¥é€‚åº”è¯­éŸ³å¤„ç†çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼ŒåŒ…æ‹¬æ¯”ç‰¹ç‡é«˜ã€è¯­ä¹‰æˆ–å£°å­¦ä¿¡æ¯ä¸¢å¤±ï¼Œä»¥åŠåœ¨å°è¯•æ•è·ä¸¤è€…æ—¶ä¾èµ–å¤šç æœ¬è®¾è®¡ï¼Œè¿™å¢åŠ äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ¶æ„å¤æ‚æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FocalCodecï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç„¦ç‚¹è°ƒåˆ¶çš„æœ‰æ•ˆä½æ¯”ç‰¹ç‡ç¼–è§£ç å™¨ï¼Œå®ƒåˆ©ç”¨å•ä¸ªäºŒè¿›åˆ¶ç æœ¬åœ¨0.16è‡³0.65kbpsä¹‹é—´å‹ç¼©è¯­éŸ³ã€‚FocalCodecåœ¨è¾ƒä½çš„æ¯”ç‰¹ç‡ä¸‹å®ç°äº†ä¸å½“å‰æœ€ä½³æ°´å¹³ç›¸å½“çš„è¯­éŸ³å†åˆæˆå’Œè¯­éŸ³è½¬æ¢æ€§èƒ½ï¼ŒåŒæ—¶æœ‰æ•ˆå¤„ç†å¤šè¯­è¨€è¯­éŸ³å’Œå˜ˆæ‚ç¯å¢ƒã€‚ä¸‹æ¸¸ä»»åŠ¡çš„è¯„ä¼°è¡¨æ˜ï¼ŒFocalCodecæˆåŠŸä¿ç•™äº†è¶³å¤Ÿçš„è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯ï¼ŒåŒæ—¶éå¸¸é€‚åˆç”Ÿæˆå»ºæ¨¡ã€‚æ¼”ç¤ºæ ·æœ¬å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://lucadellalib.github.io/focalcodec-web/%E6%89%BE%E5%88%B0%E3%80%82">https://lucadellalib.github.io/focalcodec-web/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04465v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå‘ç”Ÿäº†é©å‘½æ€§çš„å˜åŒ–ã€‚å—è¿™ä¸€æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶äººå‘˜å°è¯•å°†è¿™ç§æ–¹æ³•é€‚åº”åˆ°è¯­éŸ³é¢†åŸŸï¼Œé€šè¿‡å°†è¿ç»­éŸ³é¢‘ç¦»æ•£åŒ–ä¸ºä»¤ç‰Œï¼Œä½¿ç”¨ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨è¿›è¡Œå¤„ç†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´é«˜æ¯”ç‰¹ç‡ã€è¯­ä¹‰æˆ–å£°å­¦ä¿¡æ¯ä¸¢å¤±ä»¥åŠè¯•å›¾åŒæ—¶æ•æ‰ä¸¤è€…æ—¶ä¾èµ–å¤šç¼–ç ç°¿è®¾è®¡ç­‰é—®é¢˜ï¼Œè¿™å¢åŠ äº†ä¸‹æ¸¸ä»»åŠ¡çš„æ¶æ„å¤æ‚æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FocalCodecï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç„¦ç‚¹è°ƒåˆ¶çš„æœ‰æ•ˆä½æ¯”ç‰¹ç‡ç¼–è§£ç å™¨ï¼Œå®ƒåˆ©ç”¨å•ä¸ªäºŒè¿›åˆ¶ç¼–ç ç°¿ä»¥0.16è‡³0.65kbpsçš„é€Ÿç‡å‹ç¼©è¯­éŸ³ã€‚FocalCodecåœ¨è¾ƒä½æ¯”ç‰¹ç‡ä¸‹æä¾›å‡ºè‰²çš„è¯­éŸ³é‡å»ºå’Œè¯­éŸ³è½¬æ¢æ€§èƒ½ï¼ŒåŒæ—¶æœ‰æ•ˆå¤„ç†å¤šè¯­ç§å’Œå˜ˆæ‚ç¯å¢ƒä¸‹çš„è¯­éŸ³ã€‚ä¸‹æ¸¸ä»»åŠ¡çš„è¯„ä¼°è¡¨æ˜ï¼ŒFocalCodecæˆåŠŸä¿ç•™äº†è¶³å¤Ÿçš„è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯ï¼ŒåŒæ—¶éå¸¸é€‚åˆç”Ÿæˆå»ºæ¨¡ã€‚æ¼”ç¤ºæ ·å“å’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://lucadellalib.github.io/focalcodec-web/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://lucadellalib.github.io/focalcodec-web/]ä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è‡ªç›‘ç£é¢„è®­ç»ƒåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>ç ”ç©¶äººå‘˜æ­£åœ¨æ¢ç´¢å°†è¿™ç§æ–¹æ³•é€‚åº”åˆ°è¯­éŸ³é¢†åŸŸï¼Œé€šè¿‡ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨å¤„ç†è¯­éŸ³ã€‚</li>
<li>ç°æœ‰è¯­éŸ³ç¼–è§£ç æ–¹æ³•é¢ä¸´é«˜æ¯”ç‰¹ç‡ã€ä¿¡æ¯ä¸¢å¤±å’Œå¤šç¼–ç ç°¿è®¾è®¡å¤æ‚æ€§ç­‰é—®é¢˜ã€‚</li>
<li>FocalCodecæ˜¯ä¸€ç§æœ‰æ•ˆçš„ä½æ¯”ç‰¹ç‡ç¼–è§£ç å™¨ï¼Œèƒ½è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¹¶æˆåŠŸåœ¨è¯­éŸ³é‡å»ºå’Œè¯­éŸ³è½¬æ¢æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>FocalCodecèƒ½åœ¨è¾ƒä½æ¯”ç‰¹ç‡ä¸‹å·¥ä½œï¼ŒåŒæ—¶å¤„ç†å¤šè¯­ç§å’Œå˜ˆæ‚ç¯å¢ƒä¸‹çš„è¯­éŸ³ã€‚</li>
<li>FocalCodecèƒ½æœ‰æ•ˆä¿ç•™è¯­éŸ³çš„è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯ï¼Œé€‚åˆç”¨äºç”Ÿæˆå»ºæ¨¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d79570239d32b05f2ca0dc6021f3fc6" align="middle">
<img src="https://picx.zhimg.com/v2-df9f91e946bfe1931f0e93c2e9f6802d" align="middle">
<img src="https://picx.zhimg.com/v2-a60ce77176fb2dcb65af184bd5505b92" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Unmasking-Deepfakes-Leveraging-Augmentations-and-Features-Variability-for-Deepfake-Speech-Detection"><a href="#Unmasking-Deepfakes-Leveraging-Augmentations-and-Features-Variability-for-Deepfake-Speech-Detection" class="headerlink" title="Unmasking Deepfakes: Leveraging Augmentations and Features Variability for Deepfake Speech Detection"></a>Unmasking Deepfakes: Leveraging Augmentations and Features Variability for Deepfake Speech Detection</h2><p><strong>Authors:Inbal Rimon, Oren Gal, Haim Permuter</strong></p>
<p>Deepfake speech detection presents a growing challenge as generative audio technologies continue to advance. We propose a hybrid training framework that advances detection performance through novel augmentation strategies. First, we introduce a dual-stage masking approach that operates both at the spectrogram level (MaskedSpec) and within the latent feature space (MaskedFeature), providing complementary regularization that improves tolerance to localized distortions and enhances generalization learning. Second, we introduce compression-aware strategy during self-supervised to increase variability in low-resource scenarios while preserving the integrity of learned representations, thereby improving the suitability of pretrained features for deepfake detection. The framework integrates a learnable self-supervised feature extractor with a ResNet classification head in a unified training pipeline, enabling joint adaptation of acoustic representations and discriminative patterns. On the ASVspoof5 Challenge (Track~1), the system achieves state-of-the-art results with an Equal Error Rate (EER) of 4.08% under closed conditions, further reduced to 2.71% through fusion of models with diverse pretrained feature extractors. when trained on ASVspoof2019, our system obtaining leading performance on the ASVspoof2019 evaluation set (0.18% EER) and the ASVspoof2021 DF task (2.92% EER).</p>
<blockquote>
<p>éšç€ç”ŸæˆéŸ³é¢‘æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹é¢ä¸´è¶Šæ¥è¶Šå¤§çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ–°é¢–çš„æ•°æ®å¢å¼ºç­–ç•¥æ¥æå‡æ£€æµ‹æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒé˜¶æ®µæ©ç æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ—¢åœ¨é¢‘è°±å›¾å±‚é¢ï¼ˆMaskedSpecï¼‰è¿è¡Œï¼Œä¹Ÿåœ¨æ½œåœ¨ç‰¹å¾ç©ºé—´å†…ï¼ˆMaskedFeatureï¼‰è¿è¡Œï¼Œæä¾›äº†è¡¥å……æ€§æ­£åˆ™åŒ–ï¼Œæ”¹å–„äº†å±€éƒ¨å¤±çœŸçš„å®¹å¿åº¦ï¼Œå¹¶å¢å¼ºäº†æ³›åŒ–å­¦ä¹ ã€‚å…¶æ¬¡ï¼Œåœ¨è‡ªæˆ‘ç›‘ç£è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‹ç¼©æ„ŸçŸ¥ç­–ç•¥ï¼Œä»¥å¢åŠ ä½èµ„æºåœºæ™¯ä¸­çš„å˜é‡ï¼ŒåŒæ—¶ä¿æŒå­¦ä¹ è¡¨ç¤ºçš„å®Œæ•´æ€§ï¼Œä»è€Œæé«˜äº†é¢„è®­ç»ƒç‰¹å¾ç”¨äºæ·±åº¦ä¼ªé€ æ£€æµ‹çš„é€‚ç”¨æ€§ã€‚è¯¥æ¡†æ¶å°†å¯å­¦ä¹ çš„è‡ªæˆ‘ç›‘ç£ç‰¹å¾æå–å™¨ä¸ResNetåˆ†ç±»å¤´é›†æˆåœ¨ä¸€ä¸ªç»Ÿä¸€çš„è®­ç»ƒç®¡é“ä¸­ï¼Œèƒ½å¤Ÿå®ç°å£°å­¦è¡¨ç¤ºå’Œåˆ¤åˆ«æ¨¡å¼çš„è”åˆé€‚åº”ã€‚åœ¨ASVspoof5æŒ‘æˆ˜ï¼ˆTrack~1ï¼‰ä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨å°é—­æ¡ä»¶ä¸‹ä»¥4.08%çš„ç­‰è¯¯ç‡ï¼ˆEqual Error Rateï¼‰å–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœï¼Œé€šè¿‡èåˆå…·æœ‰ä¸åŒé¢„è®­ç»ƒç‰¹å¾æå–å™¨çš„æ¨¡å‹ï¼Œè¿›ä¸€æ­¥å°†ç­‰è¯¯ç‡é™ä½åˆ°2.71%ã€‚åœ¨ASVspoof2019è®­ç»ƒçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ASVspoof2019è¯„ä¼°é›†ï¼ˆ0.18% EERï¼‰å’ŒASVspoof2021 DFä»»åŠ¡ï¼ˆ2.92% EERï¼‰ä¸Šå‡å–å¾—äº†é¢†å…ˆæ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05545v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§æ··åˆè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ–°å‹æ•°æ®å¢å¼ºç­–ç•¥æå‡å‡éŸ³é¢‘æ£€æµ‹æ€§èƒ½ã€‚å¼•å…¥åŒé‡é˜¶æ®µæ©ç æ–¹æ³•ï¼Œåœ¨é¢‘è°±å›¾å’Œæ½œåœ¨ç‰¹å¾ç©ºé—´è¿›è¡Œæ“ä½œï¼Œæé«˜æ¨¡å‹å¯¹å±€éƒ¨å¤±çœŸçš„å®¹å¿åº¦å¹¶å¢å¼ºæ³›åŒ–å­¦ä¹ èƒ½åŠ›ã€‚åŒæ—¶ï¼Œåœ¨è‡ªç›‘ç£å­¦ä¹ ä¸­é‡‡ç”¨å‹ç¼©æ„ŸçŸ¥ç­–ç•¥ï¼Œæé«˜ä½èµ„æºåœºæ™¯ä¸­çš„å˜é‡æ€§å¹¶ä¿ç•™å­¦ä¹ è¡¨å¾çš„å®Œæ•´æ€§ã€‚è¯¥æ¡†æ¶åœ¨ASVspoofæŒ‘æˆ˜ä¸­å–å¾—æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„æ··åˆè®­ç»ƒæ¡†æ¶æ—¨åœ¨é€šè¿‡æ–°å‹æ•°æ®å¢å¼ºç­–ç•¥æå‡å‡éŸ³é¢‘æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åŒé‡é˜¶æ®µæ©ç æ–¹æ³•åŒ…æ‹¬åœ¨é¢‘è°±å›¾å’Œæ½œåœ¨ç‰¹å¾ç©ºé—´çš„æ“ä½œï¼Œæé«˜æ¨¡å‹å¯¹å±€éƒ¨å¤±çœŸçš„å®¹å¿åº¦å’Œæ³›åŒ–å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥è‡ªç›‘ç£å­¦ä¹ ä¸­çš„å‹ç¼©æ„ŸçŸ¥ç­–ç•¥ï¼Œä»¥æé«˜ä½èµ„æºåœºæ™¯ä¸­çš„å˜é‡æ€§å¹¶ä¿ç•™å­¦ä¹ è¡¨å¾çš„å®Œæ•´æ€§ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†å¯å­¦ä¹ çš„è‡ªç›‘ç£ç‰¹å¾æå–å™¨å’ŒResNetåˆ†ç±»å¤´ï¼Œåœ¨ç»Ÿä¸€è®­ç»ƒç®¡é“ä¸­å®ç°äº†å£°å­¦è¡¨å¾å’Œåˆ¤åˆ«æ¨¡å¼çš„è”åˆé€‚åº”ã€‚</li>
<li>åœ¨ASVspoofæŒ‘æˆ˜ä¸­ï¼Œè¯¥ç³»ç»Ÿçš„æ€§èƒ½è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ç­‰é”™è¯¯ç‡é™è‡³4.08%ã€‚</li>
<li>é€šè¿‡èåˆå…·æœ‰ä¸åŒé¢„è®­ç»ƒç‰¹å¾æå–å™¨çš„æ¨¡å‹ï¼Œç­‰é”™è¯¯ç‡è¿›ä¸€æ­¥é™è‡³2.71%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-309e4033282311473bddaafd59962420" align="middle">
<img src="https://picx.zhimg.com/v2-6bff6ad18d2e2af233da7dcf4b50b85a" align="middle">
<img src="https://picx.zhimg.com/v2-683f9358a16070a6c8888d8c2538a324" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Compositional-Phoneme-Approximation-for-L1-Grounded-L2-Pronunciation-Training"><a href="#Compositional-Phoneme-Approximation-for-L1-Grounded-L2-Pronunciation-Training" class="headerlink" title="Compositional Phoneme Approximation for L1-Grounded L2 Pronunciation Training"></a>Compositional Phoneme Approximation for L1-Grounded L2 Pronunciation Training</h2><p><strong>Authors:Jisang Park, Minu Kim, DaYoung Hong, Jongha Lee</strong></p>
<p>Learners of a second language (L2) often map non-native phonemes to similar native-language (L1) phonemes, making conventional L2-focused training slow and effortful. To address this, we propose an L1-grounded pronunciation training method based on compositional phoneme approximation (CPA), a feature-based representation technique that approximates L2 sounds with sequences of L1 phonemes. Evaluations with 20 Korean non-native English speakers show that CPA-based training achieves a 76% in-box formant rate in acoustic analysis, 17.6% relative improvement in phoneme recognition accuracy, and over 80% of speech being rated as more native-like, with minimal training. Project page: <a target="_blank" rel="noopener" href="https://gsanpark.github.io/CPA-Pronunciation">https://gsanpark.github.io/CPA-Pronunciation</a>.</p>
<blockquote>
<p>ç¬¬äºŒè¯­è¨€ï¼ˆL2ï¼‰å­¦ä¹ è€…å¾€å¾€å°†éæ¯è¯­éŸ³ç´ æ˜ å°„åˆ°ç›¸ä¼¼çš„æ¯è¯­ï¼ˆL1ï¼‰éŸ³ç´ ï¼Œè¿™ä½¿å¾—ä¼ ç»Ÿçš„ä»¥L2ä¸ºä¸­å¿ƒçš„è®­ç»ƒå˜å¾—ç¼“æ…¢ä¸”è´¹åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¯è¯­å‘éŸ³è®­ç»ƒçš„ç»„æˆéŸ³ç´ è¿‘ä¼¼æ³•ï¼ˆCPAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç‰¹å¾è¡¨ç¤ºçš„æŠ€æœ¯ï¼Œç”¨L1éŸ³ç´ çš„åºåˆ—æ¥è¿‘ä¼¼L2çš„å£°éŸ³ã€‚å¯¹20åéè‹±è¯­æ¯è¯­éŸ©å›½äººçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒåŸºäºCPAçš„è®­ç»ƒæ–¹æ³•åœ¨å£°å­¦åˆ†æä¸­è¾¾åˆ°äº†76%çš„ç®±å¼å…±æŒ¯å³°ç‡ï¼ŒéŸ³ç´ è¯†åˆ«å‡†ç¡®ç‡æé«˜äº†17.6%ï¼Œè¶…è¿‡80%çš„è¯­éŸ³è¢«è®¤ä¸ºæ›´åƒæ¯è¯­ï¼Œä¸”è®­ç»ƒæ—¶é—´æœ€å°‘ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://gsanpark.github.io/CPA-Pronunciation%E3%80%82">https://gsanpark.github.io/CPA-Pronunciationã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10927v5">PDF</a> Accepted to IJCNLP-AACL 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>äºŒè¯­å­¦ä¹ è€…å¸¸å¸¸å°†éæ¯è¯­éŸ³ç´ æ˜ å°„åˆ°ç±»ä¼¼æ¯è¯­éŸ³ç´ ä¸Šï¼Œå¯¼è‡´å¸¸è§„çš„ç¬¬äºŒè¯­è¨€å‘éŸ³è®­ç»ƒå˜å¾—ç¼“æ…¢ä¸”è´¹åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åŸºäºæ¯è¯­ï¼ˆL1ï¼‰çš„å‘éŸ³è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºç»„åˆéŸ³ç´ è¿‘ä¼¼ï¼ˆCPAï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç‰¹å¾çš„è¡¨è¾¾æ–¹å¼ï¼Œèƒ½å¤Ÿç”¨æ¯è¯­éŸ³ç´ çš„åºåˆ—æ¥è¿‘ä¼¼è¡¨è¾¾ç¬¬äºŒè¯­è¨€çš„å‘éŸ³ã€‚å¯¹20åéŸ©å›½éè‹±è¯­æ¯è¯­è€…çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒåŸºäºCPAçš„è®­ç»ƒæ–¹æ³•è¾¾åˆ°äº†76%çš„å£°å­¦åˆ†æå‡†ç¡®ç‡ï¼Œè¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡æé«˜äº†17.6%ï¼Œè¶…è¿‡80%çš„è¯­éŸ³è¢«è¯„ä»·ä¸ºæ›´æ¥è¿‘æ¯è¯­æ°´å¹³ï¼Œä¸”è®­ç»ƒæ—¶é—´è¾ƒçŸ­ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äºŒè¯­å­¦ä¹ è€…åœ¨å‘éŸ³è®­ç»ƒæ—¶ï¼Œå€¾å‘äºå°†éæ¯è¯­éŸ³ç´ æ˜ å°„åˆ°ç±»ä¼¼æ¯è¯­éŸ³ç´ ä¸Šï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸å°½äººæ„ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°çš„L1-groundedå‘éŸ³è®­ç»ƒæ–¹æ³•ï¼ŒåŸºäºç»„åˆéŸ³ç´ è¿‘ä¼¼ï¼ˆCPAï¼‰æŠ€æœ¯ã€‚</li>
<li>CPAæŠ€æœ¯ç”¨æ¯è¯­éŸ³ç´ çš„åºåˆ—æ¥è¿‘ä¼¼è¡¨è¾¾ç¬¬äºŒè¯­è¨€çš„å‘éŸ³ã€‚</li>
<li>å¯¹éŸ©å›½éè‹±è¯­æ¯è¯­è€…çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒåŸºäºCPAçš„è®­ç»ƒæ–¹æ³•åœ¨å£°å­¦åˆ†æã€è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡æ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>è¶…è¿‡80%çš„è¯­éŸ³ç»è¿‡è¿™ç§è®­ç»ƒåè¢«è¯„ä»·ä¸ºæ›´æ¥è¿‘æ¯è¯­æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•çš„è®­ç»ƒæ—¶é—´ç›¸å¯¹è¾ƒçŸ­ï¼Œæ•ˆç‡è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13f14d563b6b9f032741c741fc9036d3" align="middle">
<img src="https://picx.zhimg.com/v2-5a1dd64f9e1981e4f2fb3332935fa08b" align="middle">
<img src="https://picx.zhimg.com/v2-79541bbce8308538e1a6f45ed5ff6d3a" align="middle">
<img src="https://picx.zhimg.com/v2-64f45fe19a13c653efe0ae30fe02388f" align="middle">
<img src="https://picx.zhimg.com/v2-50771a45b0427770202d8ec86353c728" align="middle">
<img src="https://picx.zhimg.com/v2-17e186988e56b76a32a40e0ef0b57040" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Error-Correction-in-Radiology-Reports-A-Knowledge-Distillation-Based-Multi-Stage-Framework"><a href="#Error-Correction-in-Radiology-Reports-A-Knowledge-Distillation-Based-Multi-Stage-Framework" class="headerlink" title="Error Correction in Radiology Reports: A Knowledge Distillation-Based Multi-Stage Framework"></a>Error Correction in Radiology Reports: A Knowledge Distillation-Based Multi-Stage Framework</h2><p><strong>Authors:Jinge Wu, Zhaolong Wu, Ruizhe Li, Tong Chen, Abul Hasan, Yunsoo Kim, Jason P. Y. Cheung, Teng Zhang, Honghan Wu</strong></p>
<p>The increasing complexity and workload of clinical radiology leads to inevitable oversights and mistakes in their use as diagnostic tools, causing delayed treatments and sometimes life-threatening harm to patients. While large language models (LLMs) have shown remarkable progress in many tasks, their utilities in detecting and correcting errors in radiology reporting are limited. This paper proposes a novel dual-knowledge infusion framework that enhances LLMsâ€™ capability for radiology report proofreading through systematic integration of medical expertise. Specifically, the knowledge infusion combines medical knowledge graph distillation (MKGD) with external knowledge retrieval (EXKR), enabling an effective automated approach in tackling mistakes in radiology reporting. By decomposing the complex proofreading task into three specialized stages of detection, localization, and correction, our method mirrors the systematic review process employed by expert radiologists, ensuring both precision and clinical interpretability. To perform a robust, clinically relevant evaluation, a comprehensive benchmark is also proposed using real-world radiology reports with real-world error patterns, including speech recognition confusions, terminology ambiguities, and template-related inconsistencies. Extensive evaluations across multiple LLM architectures demonstrate substantial improvements of our approach: up to 31.56% increase in error detection accuracy and 37.4% reduction in processing time. Human evaluation by radiologists confirms superior clinical relevance and factual consistency compared to existing approaches.</p>
<blockquote>
<p>ä¸´åºŠæ”¾å°„å­¦çš„å¤æ‚æ€§å’Œå·¥ä½œé‡ä¸æ–­å¢åŠ ï¼Œå¯¼è‡´å…¶ä½œä¸ºè¯Šæ–­å·¥å…·æ—¶ä¸å¯é¿å…åœ°ä¼šå‡ºç°ç–å¿½å’Œé”™è¯¯ï¼Œä»è€Œå¯¼è‡´æ²»ç–—å»¶è¿Ÿï¼Œæœ‰æ—¶ç”šè‡³ä¼šå¯¹æ‚£è€…é€ æˆå±åŠç”Ÿå‘½çš„ä¼¤å®³ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨æ£€æµ‹å’Œçº æ­£æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„é”™è¯¯æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒçŸ¥è¯†èåˆæ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ•´åˆåŒ»å­¦ä¸“ä¸šçŸ¥è¯†ï¼Œå¢å¼ºäº†LLMåœ¨æ”¾å°„å­¦æŠ¥å‘Šæ ¡å¯¹æ–¹é¢çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒçŸ¥è¯†èåˆç»“åˆäº†åŒ»å­¦çŸ¥è¯†å›¾è°±è’¸é¦ï¼ˆMKGDï¼‰å’Œå¤–éƒ¨çŸ¥è¯†æ£€ç´¢ï¼ˆEXKRï¼‰ï¼Œä¸ºå®ç°æ”¾å°„å­¦æŠ¥å‘Šä¸­é”™è¯¯çš„è‡ªåŠ¨åŒ–å¤„ç†æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚é€šè¿‡å°†å¤æ‚çš„æ ¡å¯¹ä»»åŠ¡åˆ†è§£ä¸ºæ£€æµ‹ã€å®šä½å’Œæ ¡æ­£ä¸‰ä¸ªä¸“ä¸šé˜¶æ®µï¼Œæˆ‘ä»¬çš„æ–¹æ³•åæ˜ äº†ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿæ‰€é‡‡ç”¨çš„ç³»ç»Ÿå®¡æŸ¥è¿‡ç¨‹ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œä¸´åºŠè§£é‡Šæ€§ã€‚ä¸ºäº†è¿›è¡Œç¨³å¥ä¸”ä¸ä¸´åºŠç›¸å…³çš„è¯„ä¼°ï¼Œè¿˜ä½¿ç”¨å…·æœ‰çœŸå®ä¸–ç•Œé”™è¯¯æ¨¡å¼çš„çœŸå®ä¸–ç•Œæ”¾å°„å­¦æŠ¥å‘Šæå‡ºäº†ä¸€é¡¹å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«æ··æ·†ã€æœ¯è¯­æ¨¡ç³Šå’Œæ¨¡æ¿ç›¸å…³çš„ä¸ä¸€è‡´æ€§ã€‚å¯¹å¤šä¸ªLLMæ¶æ„çš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„å¤§å¹…æ”¹è¿›ï¼šé”™è¯¯æ£€æµ‹å‡†ç¡®ç‡æé«˜äº†31.56%ï¼Œå¤„ç†æ—¶é—´å‡å°‘äº†37.4%ã€‚æ”¾å°„ç§‘åŒ»ç”Ÿçš„äººç±»è¯„ä¼°è¯å®ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå…¶åœ¨ä¸´åºŠç›¸å…³æ€§å’Œäº‹å®ä¸€è‡´æ€§æ–¹é¢æ›´ä¸ºä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15045v3">PDF</a> Accepted to AAAI 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸï¼Œå°¤å…¶æ˜¯åœ¨ä¸´åºŠæ”¾å°„å­¦çš„é”™è¯¯æ£€æµ‹å’Œä¿®æ­£ä¸­ä»æœ‰å±€é™æ€§ã€‚ä¸ºæé«˜å…¶åœ¨æ”¾å°„æŠ¥å‘Šæ ¡è®¢ä¸­çš„èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„åŒçŸ¥è¯†èåˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç³»ç»Ÿæ•´åˆä¸“ä¸šçŸ¥è¯†å®ç°äº†å¯¹æ”¾å°„æŠ¥å‘Šæ ¡å¯¹è¿‡ç¨‹çš„æŠ€æœ¯æå‡ã€‚åŒçŸ¥è¯†èåˆåŒ…å«åŒ»å­¦çŸ¥è¯†å›¾è°±ç²¾ç‚¼ä¸å¤–éƒ¨çŸ¥è¯†æ£€ç´¢çš„ç»“åˆï¼Œæ¨¡æ‹Ÿä¸“å®¶æ”¾å°„å¸ˆçš„å®¡æŸ¥è¿‡ç¨‹ï¼Œå°†å¤æ‚çš„æ ¡å¯¹ä»»åŠ¡åˆ†è§£ä¸ºæ£€æµ‹ã€å®šä½å’Œä¿®æ­£ä¸‰ä¸ªé˜¶æ®µã€‚åŒæ—¶ï¼Œåˆ©ç”¨çœŸå®ä¸–ç•Œæ”¾å°„æŠ¥å‘Šå’ŒçœŸå®é”™è¯¯æ¨¡å¼æ„å»ºå…¨é¢çš„åŸºå‡†æµ‹è¯•é›†ï¼Œè¿›è¡Œä¸¥è°¨çš„ä¸´åºŠè¯„ä¼°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ–°æ–¹æ³•åœ¨é”™è¯¯æ£€æµ‹ç²¾åº¦ä¸Šæé«˜äº†é«˜è¾¾31.56%ï¼Œå¤„ç†æ—¶é—´å‡å°‘äº†37.4%ã€‚æ”¾å°„å¸ˆçš„è¯„ä¼°ä¹Ÿè¯æ˜äº†å…¶ä¼˜è¶Šçš„ä¸´åºŠç›¸å…³æ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•çš„å®æ–½æœ‰æœ›æ”¹å–„ä¸´åºŠæ”¾å°„å­¦ä¸­çš„å¤æ‚æ€§å’Œå·¥ä½œé‡é—®é¢˜ï¼Œå‡å°‘ç–æ¼å’Œé”™è¯¯ï¼Œæé«˜æ‚£è€…æ²»ç–—æ•ˆæœå’Œå®‰å…¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¸´åºŠæ”¾å°„å­¦å·¥ä½œé‡æ—¥ç›Šå¢åŠ å¯¼è‡´è¯Šæ–­å·¥å…·ä¸­çš„ç–æ¼å’Œé”™è¯¯å¢åŠ ï¼Œå¯èƒ½å¯¹æ‚£è€…é€ æˆå»¶è¿Ÿæ²»ç–—å’Œç”Ÿå‘½å¨èƒçš„ä¼¤å®³ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ”¾å°„å­¦é”™è¯¯æ£€æµ‹å’Œæ ¡æ­£æ–¹é¢çš„åº”ç”¨ä»æœ‰é™åˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹åŒçŸ¥è¯†èåˆæ¡†æ¶ï¼Œæ•´åˆåŒ»å­¦ä¸“ä¸šçŸ¥è¯†ï¼Œæé«˜äº†LLMsåœ¨æ”¾å°„æŠ¥å‘Šæ ¡å¯¹æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡åˆ†è§£æ ¡å¯¹ä»»åŠ¡ä¸ºæ£€æµ‹ã€å®šä½å’Œä¿®æ­£ä¸‰ä¸ªé˜¶æ®µï¼Œæ¨¡æ‹Ÿä¸“å®¶æ”¾å°„å¸ˆçš„å®¡æŸ¥è¿‡ç¨‹ã€‚</li>
<li>åˆ©ç”¨çœŸå®ä¸–ç•Œæ”¾å°„æŠ¥å‘Šå’Œé”™è¯¯æ¨¡å¼è¿›è¡Œä¸¥æ ¼çš„ä¸´åºŠè¯„ä¼°ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«çš„æ··æ·†ã€æœ¯è¯­æ­§ä¹‰å’Œæ¨¡æ¿ä¸ä¸€è‡´ç­‰ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨é”™è¯¯æ£€æµ‹ç²¾åº¦ä¸Šæ˜¾è‘—æé«˜ï¼Œå¤„ç†æ—¶é—´å¤§å¹…å‡å°‘ã€‚</li>
<li>é€šè¿‡æ”¾å°„å¸ˆçš„è¯„ä¼°ç¡®è®¤äº†å…¶ä¸´åºŠç›¸å…³æ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa32e20a70caadba2ee3f0ef151eb274" align="middle">
<img src="https://picx.zhimg.com/v2-0ee701f56f719a75bfc75ed551f8a56d" align="middle">
<img src="https://picx.zhimg.com/v2-e3817ccc68e0fa05fa13a94af14f24bc" align="middle">
<img src="https://picx.zhimg.com/v2-4af64997da5e05135e9267f25f0d50f2" align="middle">
<img src="https://picx.zhimg.com/v2-7a53bdc74b5119078f857781203e33f9" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-18a3919ebb20bedf87432f0f2cac6324" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian Responses
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-20b55b2a0407f46d189ead06a85443aa" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
