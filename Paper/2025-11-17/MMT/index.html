<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="MMT">
    <meta name="description" content="MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  HI-TransPA Hearing Impairments Translation Personal Assistant">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>MMT | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e11e35bef5f1a07f1fd5fd285a73917a')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">MMT</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/MMT/">
                                <span class="chip bg-color">MMT</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                MMT
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-17-æ›´æ–°"><a href="#2025-11-17-æ›´æ–°" class="headerlink" title="2025-11-17 æ›´æ–°"></a>2025-11-17 æ›´æ–°</h1><h2 id="HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant"><a href="#HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant" class="headerlink" title="HI-TransPA: Hearing Impairments Translation Personal Assistant"></a>HI-TransPA: Hearing Impairments Translation Personal Assistant</h2><p><strong>Authors:Zhiming Ma, Shiyu Gan, Junhao Zhao, Xianming Li, Qingyun Pan, Peidong Wang, Mingjun Pan, Yuhao Mo, Jiajie Cheng, Chengxin Chen, Zhonglun Cao, Chonghan Liu, Shi Cheng</strong></p>
<p>To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.</p>
<blockquote>
<p>ä¸ºäº†ä¸ºå¬åŠ›å—æŸäººå£«çš„æ—¥å¸¸äº¤æµæä¾›ä¸€ä¸ªç»Ÿä¸€ä¸”çµæ´»è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å°†Omni-ModelèŒƒå¼å¼•å…¥è¾…åŠ©æŠ€æœ¯ï¼Œå¹¶æ¨å‡ºHI-TransPAï¼Œè¿™æ˜¯ä¸€æ¬¾æŒ‡ä»¤é©±åŠ¨çš„è§†å¬ä¸ªäººåŠ©ç†ã€‚è¯¥æ¨¡å‹èåˆäº†ä¸æ¸…æ™°è¯­éŸ³ä¸é«˜å¸§ç‡å”‡åŠ¨æ€ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€çš„å¤šæ¨¡å¼æ¡†æ¶å†…å®ç°ç¿»è¯‘å’Œå¯¹è¯ã€‚ä¸ºäº†è§£å†³åŸå§‹æ•°æ®ä¸­çš„å™ªå£°å’Œå¼‚è´¨æ€§ä»¥åŠç°æœ‰Omni-Modelså¯¹å¬åŠ›å—æŸè¯­éŸ³çš„é€‚åº”æ€§æœ‰é™çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„é¢„å¤„ç†å’Œç­›é€‰ç®¡é“ï¼Œç”¨äºæ£€æµ‹é¢éƒ¨ç‰¹å¾ç‚¹ã€éš”ç¦»å’Œç¨³å®šå”‡éƒ¨åŒºåŸŸï¼Œå¹¶å®šé‡è¯„ä¼°å¤šæ¨¡å¼æ ·æœ¬è´¨é‡ã€‚è¿™äº›è´¨é‡åˆ†æ•°å¼•å¯¼äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é¦–å…ˆä½¿ç”¨å¹²å‡€ã€é«˜ç½®ä¿¡åº¦çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€æ­¥åŠ å…¥æ›´å›°éš¾çš„æ¡ˆä¾‹ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨SigLIPç¼–ç å™¨ç»“åˆUnified 3D-Resampleré«˜æ•ˆç¼–ç é«˜å¸§ç‡å”‡åŠ¨ã€‚åœ¨æˆ‘ä»¬ä¸“é—¨æ„å»ºçš„HI-Dialogueæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHI-TransPAåœ¨å­—é¢å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è¿™é¡¹å·¥ä½œä¸ºå°†Omni-Modelsåº”ç”¨äºè¾…åŠ©é€šä¿¡æŠ€æœ¯å¥ å®šäº†åŸºç¡€ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ç«¯åˆ°ç«¯çš„å»ºæ¨¡æ¡†æ¶å’Œå¿…è¦çš„å¤„ç†å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09915v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å°†Omni-ModelèŒƒå¼å¼•å…¥è¾…åŠ©æŠ€æœ¯ï¼Œä¸ºå¬åŠ›å—æŸäººå£«æä¾›ç»Ÿä¸€ã€çµæ´»çš„æ—¥å¸¸æ²Ÿé€šè§£å†³æ–¹æ¡ˆã€‚æå‡ºçš„HI-TransPAæŒ‡ä»¤é©±åŠ¨è§†å¬ä¸ªäººåŠ©ç†ï¼Œèƒ½å¤Ÿèåˆæ¨¡ç³Šè¯­éŸ³ä¸é«˜å¸§ç‡å”‡åŠ¨æ€ï¼Œåœ¨å•ä¸€å¤šæ¨¡å¼æ¡†æ¶å†…å®ç°ç¿»è¯‘å’Œå¯¹è¯ã€‚ä¸ºè§£å†³å™ªå£°å’Œå¼‚è´¨åŸå§‹æ•°æ®æŒ‘æˆ˜ï¼Œä»¥åŠç°æœ‰Omni-Modelå¯¹å¬åŠ›å—æŸè¯­éŸ³çš„æœ‰é™é€‚åº”æ€§ï¼Œæ„å»ºäº†å…¨é¢çš„é¢„å¤„ç†å’Œç­›é€‰ç®¡é“ï¼ŒåŒ…æ‹¬æ£€æµ‹é¢éƒ¨åœ°æ ‡ã€éš”ç¦»å’Œç¨³å®šå”‡éƒ¨åŒºåŸŸï¼Œå¹¶å®šé‡è¯„ä¼°å¤šæ¨¡å¼æ ·æœ¬è´¨é‡ã€‚è´¨é‡åˆ†æ•°å¼•å¯¼ä¸€ç§å¾ªåºæ¸è¿›å­¦ä¹ ç­–ç•¥ï¼Œå…ˆè®­ç»ƒæ¸…æ´ã€é«˜ä¿¡å¿ƒæ ·æœ¬ï¼Œé€æ­¥èå…¥æ›´å›°éš¾æ¡ˆä¾‹ä»¥å¢å¼ºæ¨¡å‹ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒHI-TransPAåœ¨å­—é¢å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ä¸ºå°†Omni-Modelåº”ç”¨äºè¾…åŠ©é€šä¿¡æŠ€æœ¯æä¾›äº†åŸºç¡€ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ç«¯åˆ°ç«¯çš„å»ºæ¨¡æ¡†æ¶å’Œå¿…è¦çš„å¤„ç†å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni-ModelèŒƒå¼è¢«å¼•å…¥è¾…åŠ©æŠ€æœ¯ï¼Œä¸ºå¬åŠ›å—æŸäººå£«æä¾›æ—¥å¸¸æ²Ÿé€šçš„ç»Ÿä¸€ã€çµæ´»è§£å†³æ–¹æ¡ˆã€‚</li>
<li>HI-TransPAæ˜¯ä¸€ä¸ªæŒ‡ä»¤é©±åŠ¨çš„è§†å¬ä¸ªäººåŠ©ç†ï¼Œèƒ½èåˆæ¨¡ç³Šè¯­éŸ³å’Œé«˜å¸§ç‡å”‡åŠ¨æ€ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„é¢„å¤„ç†å’Œç­›é€‰ç®¡é“ï¼Œä»¥å¤„ç†å™ªå£°å’Œå¼‚è´¨åŸå§‹æ•°æ®ï¼Œå¹¶æé«˜æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>é€šè¿‡æ£€æµ‹é¢éƒ¨åœ°æ ‡ã€éš”ç¦»å’Œç¨³å®šå”‡éƒ¨åŒºåŸŸï¼Œå®šé‡è¯„ä¼°å¤šæ¨¡å¼æ ·æœ¬è´¨é‡ã€‚</li>
<li>é‡‡ç”¨äº†è´¨é‡åˆ†æ•°å¼•å¯¼çš„æ¸è¿›å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å…ˆè®­ç»ƒæ¸…æ´ã€é«˜ä¿¡å¿ƒæ ·æœ¬æ¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>HI-TransPAåœ¨å­—é¢å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17f6d527a09b05f27d65ecd776cac619" align="middle">
<img src="https://picx.zhimg.com/v2-7bdbafb5b6cb0deae546215572971c4e" align="middle">
<img src="https://picx.zhimg.com/v2-39b1e5664eab3cd18ce0474fdfe71e3d" align="middle">
<img src="https://picx.zhimg.com/v2-dc926741fa46e4bd0b6db86f7281859d" align="middle">
<img src="https://picx.zhimg.com/v2-1bfa9b94382767ca6363eab686894e94" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ACT-as-Human-Multimodal-Large-Language-Model-Data-Annotation-with-Critical-Thinking"><a href="#ACT-as-Human-Multimodal-Large-Language-Model-Data-Annotation-with-Critical-Thinking" class="headerlink" title="ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking"></a>ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking</h2><p><strong>Authors:Lequan Lin, Dai Shi, Andi Han, Feng Chen, Qiuzheng Chen, Jiawen Li, Zhaoyang Li, Jiyuan Li, Zhenbang Sun, Junbin Gao</strong></p>
<p>Supervised learning relies on high-quality labeled data, but obtaining such data through human annotation is both expensive and time-consuming. Recent work explores using large language models (LLMs) for annotation, but LLM-generated labels still fall short of human-level quality. To address this problem, we propose the Annotation with Critical Thinking (ACT) data pipeline, where LLMs serve not only as annotators but also as judges to critically identify potential errors. Human effort is then directed towards reviewing only the most â€œsuspiciousâ€ cases, significantly improving the human annotation efficiency. Our major contributions are as follows: (1) ACT is applicable to a wide range of domains, including natural language processing (NLP), computer vision (CV), and multimodal understanding, by leveraging multimodal-LLMs (MLLMs). (2) Through empirical studies, we derive 7 insights on how to enhance annotation quality while efficiently reducing the human cost, and then translate these findings into user-friendly guidelines. (3) We theoretically analyze how to modify the loss function so that models trained on ACT data achieve similar performance to those trained on fully human-annotated data. Our experiments show that the performance gap can be reduced to less than 2% on most benchmark datasets while saving up to 90% of human costs.</p>
<blockquote>
<p>ç›‘ç£å­¦ä¹ ä¾èµ–äºé«˜è´¨é‡çš„æœ‰æ ‡ç­¾æ•°æ®ï¼Œä½†é€šè¿‡äººå·¥æ ‡æ³¨è·å–æ­¤ç±»æ•°æ®æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚æœ€è¿‘çš„å·¥ä½œæ¢ç´¢ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ ‡æ³¨ï¼Œä½†LLMç”Ÿæˆçš„æ ‡ç­¾ä»ç„¶è¾¾ä¸åˆ°äººç±»æ°´å¹³çš„å“è´¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ‰¹åˆ¤æ€§æ€ç»´æ ‡æ³¨â€ï¼ˆACTï¼‰æ•°æ®ç®¡é“ï¼Œå…¶ä¸­LLMä¸ä»…ä½œä¸ºæ ‡æ³¨å™¨ï¼Œè¿˜ä½œä¸ºè£åˆ¤æ¥æ‰¹åˆ¤æ€§åœ°è¯†åˆ«æ½œåœ¨é”™è¯¯ã€‚ç„¶åï¼Œäººå·¥åŠªåŠ›åªé’ˆå¯¹æœ€â€œå¯ç–‘â€çš„æ¡ˆä¾‹è¿›è¡Œå®¡æŸ¥ï¼Œä»è€Œæå¤§åœ°æé«˜äº†äººå·¥æ ‡æ³¨çš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼šï¼ˆ1ï¼‰é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼ŒACTé€‚ç”¨äºåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å’Œå¤šæ¨¡æ€ç†è§£åœ¨å†…çš„å¹¿æ³›é¢†åŸŸã€‚ï¼ˆ2ï¼‰é€šè¿‡å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å¾—å‡ºäº†å¦‚ä½•æé«˜æ ‡æ³¨è´¨é‡å¹¶æœ‰æ•ˆå‡å°‘äººåŠ›æˆæœ¬çš„7ä¸ªè§è§£ï¼Œå¹¶å°†è¿™äº›å‘ç°è½¬åŒ–ä¸ºç”¨æˆ·å‹å¥½çš„æŒ‡å—ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬ä»ç†è®ºä¸Šåˆ†æäº†å¦‚ä½•ä¿®æ”¹æŸå¤±å‡½æ•°ï¼Œä»¥ä¾¿åœ¨ACTæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹è¾¾åˆ°ä¸å®Œå…¨ç”±äººç±»æ ‡æ³¨çš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸ä¼¼çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°åŸºå‡†æ•°æ®é›†ä¸Šï¼Œæ€§èƒ½å·®è·å¯ä»¥ç¼©å°åˆ°ä¸åˆ°2%ï¼ŒåŒæ—¶èŠ‚çœé«˜è¾¾90%çš„äººåŠ›æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09833v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è™½å¯ç”¨äºæ ‡æ³¨æ•°æ®ï¼Œä½†å…¶ç”Ÿæˆçš„æ ‡ç­¾è´¨é‡éš¾ä»¥è¾¾åˆ°äººç±»æ°´å¹³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåŸºäºæ‰¹åˆ¤æ€§æ€ç»´ï¼ˆACTï¼‰çš„æ•°æ®ç®¡é“ï¼Œè®©è¯­è¨€æ¨¡å‹ä¸ä»…ä½œä¸ºæ ‡æ³¨å™¨ï¼Œè¿˜ä½œä¸ºåˆ¤æ–­å™¨æ¥è¯†åˆ«æ½œåœ¨é”™è¯¯ã€‚äººç±»åªéœ€é‡ç‚¹å®¡æŸ¥æœ€â€œå¯ç–‘â€çš„æ¡ˆä¾‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äººå·¥æ ‡æ³¨æ•ˆç‡ã€‚ACTé€‚ç”¨äºNLPã€è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€ç†è§£ç­‰é¢†åŸŸï¼Œé€šè¿‡å®è¯ç ”ç©¶å’Œç”¨æˆ·å‹å¥½æŒ‡å—æé«˜æ ‡æ³¨è´¨é‡å¹¶é™ä½äººåŠ›æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒACTè®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ä¸å…¨äººå·¥æ ‡æ³¨æ•°æ®çš„æ€§èƒ½å·®è·å°äº2%ï¼ŒåŒæ—¶èŠ‚çœäº†é«˜è¾¾90%çš„äººåŠ›æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºAnnotation with Critical Thinking (ACT)æ•°æ®ç®¡é“è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ ‡æ³¨æ•°æ®è´¨é‡é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ï¼Œæé«˜æ ‡æ³¨è´¨é‡å¹¶é™ä½äººå·¥å®¡æŸ¥å·¥ä½œé‡ã€‚</li>
<li>ACTé€‚ç”¨äºNLPã€è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€ç†è§£ç­‰å¤šä¸ªé¢†åŸŸã€‚</li>
<li>é€šè¿‡å®è¯ç ”ç©¶å¾—å‡ºæé«˜æ ‡æ³¨è´¨é‡å’Œé™ä½äººåŠ›æˆæœ¬çš„7ä¸ªå…³é”®è§è§£ã€‚</li>
<li>æå‡ºä¿®æ”¹æŸå¤±å‡½æ•°çš„æ–¹æ¡ˆï¼Œç¼©å°æ¨¡å‹æ€§èƒ½å·®è·ï¼ŒåŒæ—¶é™ä½æˆæœ¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f912c1121913684fd7d3dffa6ed93e30" align="middle">
<img src="https://picx.zhimg.com/v2-94c5d37f8605452b1fb8a0142b692379" align="middle">
<img src="https://picx.zhimg.com/v2-298c0c924bd9957eb2b5c1fa2c255f33" align="middle">
<img src="https://picx.zhimg.com/v2-362a9c87393a44d2b318e915a5b7416d" align="middle">
<img src="https://picx.zhimg.com/v2-56462932cfc55f9728de5265e6b42366" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CrochetBench-Can-Vision-Language-Models-Move-from-Describing-to-Doing-in-Crochet-Domain"><a href="#CrochetBench-Can-Vision-Language-Models-Move-from-Describing-to-Doing-in-Crochet-Domain" class="headerlink" title="CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?"></a>CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?</h2><p><strong>Authors:Peiyu Li, Xiaobao Huang, Nitesh V. Chawla</strong></p>
<p>We present CrochetBench, a benchmark for evaluating the ability of multimodal large language models to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks that focus on high-level description or visual question answering, CrochetBench shifts the emphasis from describing to doing: models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. We adopt the CrochetPARADE DSL as our intermediate representation, enabling structural validation and functional evaluation via execution. The benchmark covers tasks including stitch classification, instruction grounding, and both natural language and image-to-DSL translation. Across all tasks, performance sharply declines as the evaluation shifts from surface-level similarity to executable correctness, exposing limitations in long-range symbolic reasoning and 3D-aware procedural synthesis. CrochetBench offers a new lens for assessing procedural competence in multimodal models and highlights the gap between surface-level understanding and executable precision in real-world creative domains. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Peiyu-Georgia-Li/crochetBench">https://github.com/Peiyu-Georgia-Li/crochetBench</a>.</p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†CrochetBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é’©ç¼–é¢†åŸŸè¿›è¡Œç²¾ç»†ã€ä½çº§çš„ç¨‹åºæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸åŒäºä»¥å¾€ä¸“æ³¨äºé«˜çº§æè¿°æˆ–è§†è§‰é—®ç­”çš„åŸºå‡†æµ‹è¯•ï¼ŒCrochetBenchå°†é‡ç‚¹ä»æè¿°è½¬å‘æ“ä½œï¼šæ¨¡å‹éœ€è¦è¯†åˆ«é’ˆè„šã€é€‰æ‹©ç»“æ„é€‚å½“çš„æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆå¯ç¼–è¯‘çš„é’©ç¼–ç¨‹åºã€‚æˆ‘ä»¬é‡‡ç”¨CrochetPARADE DSLä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œé€šè¿‡æ‰§è¡Œè¿›è¡Œç»“æ„éªŒè¯å’ŒåŠŸèƒ½è¯„ä¼°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬é’ˆè„šåˆ†ç±»ã€æŒ‡ä»¤æ¥åœ°ä»¥åŠè‡ªç„¶è¯­è¨€å›¾åƒåˆ°DSLçš„ç¿»è¯‘ä»»åŠ¡ã€‚éšç€è¯„ä¼°ä»è¡¨é¢ç›¸ä¼¼æ€§è½¬å‘å¯æ‰§è¡Œæ­£ç¡®æ€§ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œæš´éœ²äº†è¿œç¨‹ç¬¦å·æ¨ç†å’Œ3Dæ„ŸçŸ¥ç¨‹åºåˆæˆæ–¹é¢çš„å±€é™æ€§ã€‚CrochetBenchä¸ºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„ç¨‹åºèƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶çªå‡ºäº†ç°å®ä¸–ç•Œåˆ›æ„é¢†åŸŸä¸­è¡¨é¢ç†è§£ä¸å¯æ‰§è¡Œç²¾åº¦ä¹‹é—´çš„å·®è·ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Peiyu-Georgia-Li/crochetBench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Peiyu-Georgia-Li/crochetBenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09483v1">PDF</a> code available at <a target="_blank" rel="noopener" href="https://github.com/Peiyu-Georgia-Li/crochetBench">https://github.com/Peiyu-Georgia-Li/crochetBench</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CrochetBenchï¼Œä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é’©ç¼–é¢†åŸŸå†…è¿›è¡Œç²¾ç»†ã€ä½çº§çš„ç¨‹åºæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸åŒäºä»¥å¾€ä¾§é‡äºé«˜çº§æè¿°æˆ–è§†è§‰é—®ç­”çš„åŸºå‡†æµ‹è¯•ï¼ŒCrochetBenchå°†é‡ç‚¹ä»æè¿°è½¬å‘æ“ä½œï¼šæ¨¡å‹éœ€è¦è¯†åˆ«é’ˆè„šã€é€‰æ‹©ç»“æ„é€‚å½“çš„æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆå¯ç¼–è¯‘çš„é’©ç¼–ç¨‹åºã€‚é‡‡ç”¨CrochetPARADE DSLä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œé€šè¿‡æ‰§è¡Œå®ç°ç»“æ„éªŒè¯å’ŒåŠŸèƒ½è¯„ä¼°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬é’ˆè„šåˆ†ç±»ã€æŒ‡ä»¤æ¥åœ°ä»¥åŠè‡ªç„¶è¯­è¨€ä¸å›¾åƒåˆ°DSLçš„ç¿»è¯‘ä»»åŠ¡ã€‚åœ¨æ‰€æœ‰è¿™äº›ä»»åŠ¡ä¸­ï¼Œéšç€è¯„ä¼°ä»è¡¨é¢å±‚æ¬¡çš„ç›¸ä¼¼æ€§è½¬å‘å¯æ‰§è¡Œçš„æ­£ç¡®æ€§ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œæš´éœ²äº†é•¿è·ç¦»ç¬¦å·æ¨ç†å’Œ3Dæ„ŸçŸ¥ç¨‹åºåˆæˆçš„å±€é™æ€§ã€‚CrochetBenchä¸ºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„ç¨‹åºèƒ½åŠ›æä¾›äº†æ–°è§†è§’ï¼Œå¹¶çªå‡ºäº†ç°å®ä¸–ç•Œä¸­åˆ›æ„é¢†åŸŸè¡¨é¢å±‚æ¬¡ç†è§£ä¸å¯æ‰§è¡Œç²¾åº¦ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CrochetBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºç²¾ç»†ã€ä½çº§çš„ç¨‹åºæ¨ç†èƒ½åŠ›åœ¨é’©ç¼–é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>ä¸å…¶ä»–åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒCrochetBenchå¼ºè°ƒæ“ä½œè€Œéæè¿°ï¼Œè¦æ±‚æ¨¡å‹è¯†åˆ«é’ˆè„šã€é€‰æ‹©é€‚å½“æŒ‡ä»¤å¹¶ç”Ÿæˆå¯ç¼–è¯‘ç¨‹åºã€‚</li>
<li>é‡‡ç”¨CrochetPARADE DSLä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œå®ç°ç»“æ„éªŒè¯å’ŒåŠŸèƒ½è¯„ä¼°ã€‚</li>
<li>åŸºå‡†æµ‹è¯•åŒ…æ‹¬é’ˆè„šåˆ†ç±»ã€æŒ‡ä»¤æ¥åœ°ä»¥åŠè‡ªç„¶è¯­è¨€ä¸å›¾åƒåˆ°DSLçš„ç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>éšç€è¯„ä¼°ä»è¡¨é¢å±‚æ¬¡çš„ç›¸ä¼¼æ€§è½¬å‘å¯æ‰§è¡Œæ­£ç¡®æ€§ï¼Œæ¨¡å‹æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œè¡¨æ˜å­˜åœ¨é•¿è·ç¦»ç¬¦å·æ¨ç†å’Œ3Dæ„ŸçŸ¥ç¨‹åºåˆæˆçš„å±€é™æ€§ã€‚</li>
<li>CrochetBenchä¸ºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„ç¨‹åºèƒ½åŠ›æä¾›äº†æ–°è§†è§’ã€‚</li>
<li>å¼ºè°ƒäº†ç°å®ä¸–ç•Œä¸­åˆ›æ„é¢†åŸŸè¡¨é¢å±‚æ¬¡ç†è§£ä¸å¯æ‰§è¡Œç²¾åº¦ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2cab8fc5c6e274c20384aebeea9ef67" align="middle">
<img src="https://picx.zhimg.com/v2-20b55b2a0407f46d189ead06a85443aa" align="middle">
<img src="https://picx.zhimg.com/v2-4c17dab7e6c30a2c5721e8dc5618f78e" align="middle">
<img src="https://picx.zhimg.com/v2-82d6fe732c516b0b46990444a551fdc9" align="middle">
<img src="https://picx.zhimg.com/v2-b743f2a36335ed71122ab071c31b00a2" align="middle">
<img src="https://picx.zhimg.com/v2-13a9e15fe1af8d8eb26e8ee467b415cc" align="middle">
<img src="https://picx.zhimg.com/v2-259a9702de514035a2ab2dbc9c19330d" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation"><a href="#WebVIA-A-Web-based-Vision-Language-Agentic-Framework-for-Interactive-and-Verifiable-UI-to-Code-Generation" class="headerlink" title="WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation"></a>WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation</h2><p><strong>Authors:Mingde Xu, Zhen Yang, Wenyi Hong, Lihang Pan, Xinyue Fan, Yan Wang, Xiaotao Gu, Bin Xu, Jie Tang</strong></p>
<p>User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML&#x2F;CSS&#x2F;JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML&#x2F;CSS&#x2F;JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at \href{<a target="_blank" rel="noopener" href="https://zheny2751-dotcom.github.io/webvia.github.io/%7D%7B/texttt%7Bhttps://webvia.github.io%7D%7D">https://zheny2751-dotcom.github.io/webvia.github.io/}{\texttt{https://webvia.github.io}}</a>.</p>
<blockquote>
<p>ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰å¼€å‘éœ€è¦å°†è®¾è®¡è‰å›¾è½¬åŒ–ä¸ºåŠŸèƒ½ä»£ç ï¼Œè¿™ä¸€è¿‡ç¨‹ä»ç„¶é‡å¤ä¸”åŠ³åŠ¨å¯†é›†ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯ä»¥è‡ªåŠ¨è¿›è¡ŒUIåˆ°ä»£ç çš„ç”Ÿæˆï¼Œä½†å®ƒä»¬åªèƒ½ç”Ÿæˆç¼ºä¹äº¤äº’æ€§çš„é™æ€HTML&#x2F;CSS&#x2F;JavaScriptå¸ƒå±€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WebVIAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºäº¤äº’å¼UIåˆ°ä»£ç ç”Ÿæˆå’ŒéªŒè¯çš„ä»£ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªç»„ä»¶ï¼š1ï¼‰ä¸€ä¸ªæ¢ç´¢ä»£ç†ï¼Œç”¨äºæ•è·å¤šçŠ¶æ€UIæˆªå›¾ï¼›2ï¼‰ä¸€ä¸ªUI2Codeæ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå¯æ‰§è¡Œçš„äº¤äº’å¼ä»£ç ï¼›3ï¼‰ä¸€ä¸ªéªŒè¯æ¨¡å—ï¼Œç”¨äºéªŒè¯äº¤äº’æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒWebVIAä»£ç†åœ¨UIæ¢ç´¢æ–¹é¢å®ç°äº†æ¯”é€šç”¨ä»£ç†ï¼ˆä¾‹å¦‚Gemini-2.5-Proï¼‰æ›´ç¨³å®šå’Œå‡†ç¡®çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¾®è°ƒåçš„WebVIA-UI2Codeæ¨¡å‹åœ¨ç”Ÿæˆå¯æ‰§è¡Œçš„äº¤äº’å¼HTML&#x2F;CSS&#x2F;JavaScriptä»£ç æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œåœ¨äº¤äº’å¼å’Œé™æ€UI2CodeåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå…¶åŸºç¡€ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://webvia.github.ioä¸Šæ‰¾åˆ°./">https://webvia.github.ioä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06251v1">PDF</a> 36 pages, 30 figures</p>
<p><strong>Summary</strong></p>
<p>ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå°†è®¾è®¡è‰å›¾è½¬åŒ–ä¸ºåŠŸèƒ½æ€§ä»£ç ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤ä¸”åŠ³åŠ¨å¯†é›†å‹çš„ä»»åŠ¡ã€‚è™½ç„¶ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿå®ç°UIåˆ°ä»£ç çš„è‡ªåŠ¨ç”Ÿæˆï¼Œä½†å®ƒä»¬ä¸»è¦ç”Ÿæˆé™æ€çš„HTML&#x2F;CSS&#x2F;JavaScriptå¸ƒå±€ï¼Œç¼ºä¹äº¤äº’æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WebVIAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºäº¤äº’å¼UIåˆ°ä»£ç ç”Ÿæˆå’ŒéªŒè¯çš„agenticæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªç»„ä»¶ï¼š1ï¼‰æ¢ç´¢agentï¼Œç”¨äºæ•è·å¤šçŠ¶æ€UIæˆªå›¾ï¼›2ï¼‰UI2Codeæ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå¯æ‰§è¡Œçš„äº¤äº’å¼ä»£ç ï¼›3ï¼‰éªŒè¯æ¨¡å—ï¼Œç”¨äºéªŒè¯äº¤äº’æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒWebVIA-Agentåœ¨UIæ¢ç´¢æ–¹é¢å®ç°äº†æ¯”ä¸€èˆ¬ç”¨é€”agentï¼ˆå¦‚Gemini-2.5-Proï¼‰æ›´ç¨³å®šå’Œå‡†ç¡®çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¾®è°ƒåçš„WebVIA-UI2Codeæ¨¡å‹åœ¨ç”Ÿæˆå¯æ‰§è¡Œå’Œäº¤äº’å¼HTML&#x2F;CSS&#x2F;JavaScriptä»£ç æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œåœ¨äº¤äº’å¼å’Œé™æ€UI2CodeåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://webvia.github.ioè®¿é—®./">https://webvia.github.ioè®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>UIå¼€å‘è¿‡ç¨‹ä¸­ï¼Œè®¾è®¡åˆ°ä»£ç çš„è½¬åŒ–ä»ç„¶æœ‰å¾ˆé«˜çš„é‡å¤æ€§å’ŒåŠ³åŠ¨å¼ºåº¦ã€‚</li>
<li>å½“å‰VLMsè™½å¯å®ç°UIåˆ°ä»£ç çš„è‡ªåŠ¨ç”Ÿæˆï¼Œä½†ç”Ÿæˆçš„ä»£ç ç¼ºä¹äº¤äº’æ€§ã€‚</li>
<li>WebVIAæ˜¯é¦–ä¸ªç”¨äºäº¤äº’å¼UIåˆ°ä»£ç ç”Ÿæˆå’ŒéªŒè¯çš„agenticæ¡†æ¶ã€‚</li>
<li>WebVIAæ¡†æ¶åŒ…å«æ¢ç´¢agentã€UI2Codeæ¨¡å‹å’ŒéªŒè¯æ¨¡å—ä¸‰ä¸ªä¸»è¦ç»„ä»¶ã€‚</li>
<li>WebVIA-Agentåœ¨UIæ¢ç´¢æ–¹é¢æ€§èƒ½ç¨³å®šä¸”å‡†ç¡®ï¼Œä¼˜äºä¸€èˆ¬ç”¨é€”çš„agentã€‚</li>
<li>WebVIA-UI2Codeæ¨¡å‹åœ¨ç”Ÿæˆäº¤äº’å¼HTML&#x2F;CSS&#x2F;JavaScriptä»£ç æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-291d7eb3adf01314ac39ff5bc2c00ca0" align="middle">
<img src="https://picx.zhimg.com/v2-18e7cf61e4d50bc8fe257714c11e7602" align="middle">
<img src="https://picx.zhimg.com/v2-f7f9d3e80d7cdf5e400daeb4629a33ea" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TRACE-Textual-Reasoning-for-Affordance-Coordinate-Extraction"><a href="#TRACE-Textual-Reasoning-for-Affordance-Coordinate-Extraction" class="headerlink" title="TRACE: Textual Reasoning for Affordance Coordinate Extraction"></a>TRACE: Textual Reasoning for Affordance Coordinate Extraction</h2><p><strong>Authors:Sangyun Park, Jin Kim, Yuchen Cui, Matthew S. Brown</strong></p>
<p>Vision-Language Models (VLMs) struggle to translate high-level instructions into the precise spatial affordances required for robotic manipulation. While visual Chain-of-Thought (CoT) methods exist, they are often computationally intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance Coordinate Extraction), a novel methodology that integrates a textual Chain of Reasoning (CoR) into the affordance prediction process. We use this methodology to create the TRACE dataset, a large-scale collection created via an autonomous pipeline that pairs instructions with explicit textual rationales. By fine-tuning a VLM on this data, our model learns to externalize its spatial reasoning before acting. Our experiments show that our TRACE-tuned model achieves state-of-the-art performance, reaching 48.1% accuracy on the primary Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more challenging W2P(h) subset. Crucially, an ablation study demonstrates that performance scales directly with the amount of reasoning data used, confirming the CoRâ€™s effectiveness. Furthermore, analysis of the modelâ€™s attention maps reveals an interpretable reasoning process where focus shifts dynamically across reasoning steps. This work shows that training VLMs to generate a textual CoR is an effective and robust strategy for enhancing the precision, reliability, and interpretability of VLM-based robot control. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/jink-ucla/TRACE">https://github.com/jink-ucla/TRACE</a></p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å°†é«˜çº§æŒ‡ä»¤è½¬åŒ–ä¸ºæœºå™¨äººæ“ä½œæ‰€éœ€çš„ç²¾ç¡®ç©ºé—´é€‚åº”æ€§æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡å­˜åœ¨è§†è§‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•ï¼Œä½†å®ƒä»¬é€šå¸¸è®¡ç®—é‡å¤§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†TRACEï¼ˆç”¨äºé€‚åº”æ€§åæ ‡æå–çš„æ–‡æœ¬æ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†æ–‡æœ¬æ€ç»´é“¾ï¼ˆCoRï¼‰èå…¥é€‚åº”æ€§é¢„æµ‹è¿‡ç¨‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤æ–¹æ³•åˆ›å»ºäº†TRACEæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é›†åˆï¼Œé€šè¿‡è‡ªä¸»ç®¡é“åˆ›å»ºï¼Œè¯¥ç®¡é“å°†æŒ‡ä»¤ä¸æ˜ç¡®çš„æ–‡æœ¬ç†ç”±é…å¯¹ã€‚é€šè¿‡åœ¨æ­¤æ•°æ®ä¸Šå¾®è°ƒVLMï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¼šäº†åœ¨é‡‡å–è¡ŒåŠ¨ä¹‹å‰è¿›è¡Œå¤–éƒ¨ç©ºé—´æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„TRACEè°ƒæ•´æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä¸»è¦çš„Where2Placeï¼ˆW2Pï¼‰åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†48.1%çš„å‡†ç¡®ç‡ï¼ˆç›¸å¯¹æé«˜äº†9.6%ï¼‰ï¼Œè€Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„W2Pï¼ˆhï¼‰å­é›†ä¸Šè¾¾åˆ°äº†55.0%çš„å‡†ç¡®ç‡ã€‚å…³é”®çš„æ˜¯ï¼Œä¸€é¡¹æ¶ˆèç ”ç©¶è¯æ˜æ€§èƒ½ä¸æ‰€ä½¿ç”¨çš„æ¨ç†æ•°æ®é‡ç›´æ¥ç›¸å…³ï¼Œè¯å®äº†CoRçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå¯¹æ¨¡å‹çš„æ³¨æ„åŠ›å›¾çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œå…¶ä¸­ç„¦ç‚¹åœ¨æ¨ç†æ­¥éª¤ä¹‹é—´åŠ¨æ€å˜åŒ–ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œè®­ç»ƒVLMç”Ÿæˆæ–‡æœ¬CoRæ˜¯ä¸€ç§æœ‰æ•ˆä¸”ç¨³å¥çš„ç­–ç•¥ï¼Œå¯ä»¥æé«˜åŸºäºVLMçš„æœºå™¨äººæ§åˆ¶çš„ç²¾ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jink-ucla/TRACE%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/jink-ucla/TRACEè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01999v1">PDF</a> ICCV 2025. *Equal contribution. â€ Corresponding author</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TRACEï¼ˆç”¨äºé…å¯¹çš„æ–‡æœ¬æ¨ç†æ–¹æ³•ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å°†æ–‡æœ¬æ¨ç†é“¾ï¼ˆCoRï¼‰æ•´åˆåˆ°æ“æ§è¿‡ç¨‹çš„æ¡†æ¶ã€‚æˆ‘ä»¬åˆ›å»ºäº†TRACEæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„é€šè¿‡è‡ªä¸»ç®¡é“ç”Ÿæˆçš„é…å¯¹æŒ‡ä»¤ä¸æ˜ç¡®çš„æ–‡æœ¬æ¨ç†é›†åˆã€‚é€šè¿‡å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨è¡ŒåŠ¨å‰è¿›è¡Œå¤–éƒ¨ç©ºé—´æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨TRACEè®­ç»ƒçš„æ¨¡å‹åœ¨Where2Placeï¼ˆW2Pï¼‰åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†48.1%ï¼ˆç›¸å¯¹æé«˜äº†9.6%ï¼‰ï¼Œå¹¶åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„W2Pï¼ˆhï¼‰å­é›†ä¸Šè¾¾åˆ°äº†55.0%ã€‚åˆ†ææ¨¡å‹çš„æ³¨æ„åŠ›åœ°å›¾æ­ç¤ºäº†å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œå…¶ä¸­ç„¦ç‚¹åœ¨æ¨ç†æ­¥éª¤é—´åŠ¨æ€å˜åŒ–ã€‚è¿™è¡¨æ˜è®­ç»ƒVLMç”Ÿæˆæ–‡æœ¬æ¨ç†é“¾æ˜¯ä¸€ç§æœ‰æ•ˆä¸”ç¨³å¥çš„ç­–ç•¥ï¼Œå¯ä»¥æé«˜åŸºäºVLMçš„æœºå™¨äººæ§åˆ¶çš„ç²¾ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>TRACEæ–¹æ³•ç»“åˆäº†æ–‡æœ¬æ¨ç†é“¾ï¼ˆCoRï¼‰ï¼Œä»¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æœºå™¨äººæ“æ§ä¸­çš„ç²¾ç¡®ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†TRACEæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡è‡ªä¸»ç®¡é“ç”Ÿæˆï¼Œå°†æŒ‡ä»¤ä¸æ˜ç¡®çš„æ–‡æœ¬ç†ç”±é…å¯¹ã€‚</li>
<li>é€šè¿‡åœ¨TRACEæ•°æ®é›†ä¸Šå¾®è°ƒVLMï¼Œæ¨¡å‹å­¦ä¼šäº†åœ¨è¡ŒåŠ¨å‰è¿›è¡Œå¤–éƒ¨ç©ºé—´æ¨ç†ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨TRACEè®­ç»ƒçš„æ¨¡å‹åœ¨Where2Placeï¼ˆW2Pï¼‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œç›¸å¯¹æé«˜äº†9.6%çš„å‡†ç¡®ç‡ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜ï¼Œæ€§èƒ½ä¸ä½¿ç”¨çš„æ¨ç†æ•°æ®é‡ç›´æ¥ç›¸å…³ï¼Œè¯å®äº†CoRçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åˆ†ææ¨¡å‹çš„æ³¨æ„åŠ›åœ°å›¾æ­ç¤ºäº†å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ï¼Œå…¶ä¸­ç„¦ç‚¹åœ¨æ¨ç†æ­¥éª¤é—´åŠ¨æ€ç§»åŠ¨ã€‚</li>
<li>æ•´ä½“è€Œè¨€ï¼ŒTRACEæ–¹æ³•æ˜¯ä¸€ç§æœ‰æ•ˆä¸”ç¨³å¥çš„ç­–ç•¥ï¼Œå¯å¢å¼ºVLMåœ¨æœºå™¨äººæ§åˆ¶ä¸­çš„ç²¾ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f9968946acbcf01b24c522c8696777b" align="middle">
<img src="https://picx.zhimg.com/v2-6f3615619cef82d9e29d95b27701f3d0" align="middle">
<img src="https://picx.zhimg.com/v2-3d1e09ec7f6b57f6f97d364ece4b9694" align="middle">
<img src="https://picx.zhimg.com/v2-bc939ea03d8d6dd2a406940aea3e74a3" align="middle">
<img src="https://picx.zhimg.com/v2-b3fddd1b713a4aae8d90d98e7cb0ecee" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Pelican-VL-1-0-A-Foundation-Brain-Model-for-Embodied-Intelligence"><a href="#Pelican-VL-1-0-A-Foundation-Brain-Model-for-Embodied-Intelligence" class="headerlink" title="Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence"></a>Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</h2><p><strong>Authors:Yi Zhang, Che Liu, Xiancong Ren, Hanchu Ni, Shuai Zhang, Zeyuan Ding, Jiayu Hu, Hanzhe Shan, Zhenwei Niu, Zhaoyang Liu, Yue Zhao, Junbo Qi, Qinfan Zhang, Dengjie Li, Yidong Wang, Jiachen Luo, Yong Dai, Jian Tang, Xiaozhu Ju</strong></p>
<p>This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.</p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Pelican-VL 1.0ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼€æºå®ä½“è„‘æ¨¡å‹å®¶æ—ï¼Œå‚æ•°è§„æ¨¡ä»7äº¿åˆ°72äº¿ä¸ç­‰ã€‚æˆ‘ä»¬çš„æ˜ç¡®ä½¿å‘½æ˜¯ï¼šå°†å¼ºå¤§çš„æ™ºèƒ½åµŒå…¥å„ç§å®ä½“ã€‚Pelican-VL 1.0ç›®å‰æ˜¯æœ€å¤§è§„æ¨¡çš„å¼€æºå®ä½“å¤šæ¨¡æ€è„‘æ¨¡å‹ã€‚å®ƒçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæ·±åº¦æ•´åˆæ•°æ®èƒ½åŠ›å’Œæ™ºèƒ½è‡ªé€‚åº”å­¦ä¹ æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œmetaloopä»åŒ…å«4äº¿å¤šä¸ªä»¤ç‰Œçš„åŸæ•°æ®é›†ä¸­æç‚¼å‡ºé«˜è´¨é‡çš„æ•°æ®é›†ã€‚Pelican-VL 1.0æ˜¯åœ¨ç”±1000å¤šä¸ªA800 GPUç»„æˆçš„å¤§è§„æ¨¡é›†ç¾¤ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œæ¯ä¸ªæ£€æŸ¥ç‚¹æ¶ˆè€—è¶…è¿‡5ä¸‡å¤šä¸ªA800 GPUå°æ—¶ã€‚è¿™ä½¿å…¶åŸºç¡€æ¨¡å‹çš„æ€§èƒ½æå‡äº†20.3%ï¼Œå¹¶åœ¨ä¼—æ‰€å‘¨çŸ¥çš„å®ä½“åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†100Bçº§åˆ«çš„å¼€æºç«äº‰å¯¹æ‰‹ï¼Œæé«˜äº†10.6%ï¼Œä¸é¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿä¸ç›¸ä¸Šä¸‹ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶DPPOï¼ˆæ·±æ€ç†Ÿè™‘çš„ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œå—äººç±»å…ƒè®¤çŸ¥çš„å¯å‘æ¥è®­ç»ƒPelican-VL 1.0ã€‚æˆ‘ä»¬å°†å…¶æ“ä½œåŒ–ä¸ºä¸€ä¸ªmetaloopï¼Œæ•™ä¼šäººå·¥æ™ºèƒ½æœ‰æ„è¯†åœ°ç»ƒä¹ ï¼Œè¿™æ˜¯ä¸€ä¸ªRL-Refine-Diagnose-SFTå¾ªç¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00108v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä½©åˆ©è‚¯-VL 1.0æ˜¯ä¸€æ¬¾æ–°æ¨å‡ºçš„å¼€æºå®ä½“æ¨¡å‹å®¶æ—ï¼Œæ‹¥æœ‰ä»7äº¿åˆ°72äº¿çš„å‚æ•°è§„æ¨¡ã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæ·±åº¦æ•´åˆæ•°æ®èƒ½åŠ›ä¸æ™ºèƒ½è‡ªé€‚åº”å­¦ä¹ æœºåˆ¶ã€‚å®ƒä½¿ç”¨å¤§å‹GPUé›†ç¾¤è¿›è¡Œè®­ç»ƒï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œä¸é¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿä¸ç›¸ä¸Šä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½©åˆ©è‚¯-VL 1.0æ˜¯ä¸€ä¸ªæ–°çš„å¼€æºå®ä½“æ¨¡å‹å®¶æ—ï¼Œå‚æ•°è§„æ¨¡ä»7äº¿åˆ°72äº¿ä¸ç­‰ã€‚</li>
<li>å®ƒçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæ·±åº¦æ•´åˆæ•°æ®èƒ½åŠ›ä¸æ™ºèƒ½è‡ªé€‚åº”å­¦ä¹ æœºåˆ¶ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨å¤§å‹GPUé›†ç¾¤è¿›è¡Œè®­ç»ƒï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>ä½©åˆ©è‚¯-VL 1.0è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†åä¸ºDPPOï¼ˆæ·±æ€ç†Ÿè™‘çš„ç­–ç•¥ä¼˜åŒ–ï¼‰çš„æ–°æ¡†æ¶ã€‚</li>
<li>DPPOæ¡†æ¶å€Ÿé‰´äº†äººç±»å…ƒè®¤çŸ¥çš„ç†å¿µï¼Œè®©AIå­¦ä¼šåˆ»æ„ç»ƒä¹ ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨è‘—åçš„å®ä½“åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä¸é¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿä¸ç›¸ä¸Šä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1f0c873cf4841731db3fb26f1ae3097" align="middle">
<img src="https://picx.zhimg.com/v2-f1b298b690c452c26dfc25e071e6b14e" align="middle">
<img src="https://picx.zhimg.com/v2-e071c7cd06f5a0f70c304fe8b96775b7" align="middle">
<img src="https://picx.zhimg.com/v2-af9ee46c4a04d11fcfc100f70e9a08ba" align="middle">
<img src="https://picx.zhimg.com/v2-ab391a8e2dd9da342aed84bd4e674ea6" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="World-Simulation-with-Video-Foundation-Models-for-Physical-AI"><a href="#World-Simulation-with-Video-Foundation-Models-for-Physical-AI" class="headerlink" title="World Simulation with Video Foundation Models for Physical AI"></a>World Simulation with Video Foundation Models for Physical AI</h2><p><strong>Authors: NVIDIA,  :, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, Wei-Cheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, Yuke Zhu</strong></p>
<p>We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at <a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-predict2.5">https://github.com/nvidia-cosmos/cosmos-predict2.5</a> and <a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-transfer2.5">https://github.com/nvidia-cosmos/cosmos-transfer2.5</a>. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†[Cosmos-Predict2.5]ï¼Œè¿™æ˜¯å®‡å®™ä¸–ç•ŒåŸºé‡‘ä¼šç‰©ç†äººå·¥æ™ºèƒ½æ¨¡å‹çš„æœ€æ–°ä¸€ä»£äº§å“ã€‚åŸºäºæµæ¶æ„æ„å»ºï¼Œ[Cosmos-Predict2.5]åœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ç»Ÿä¸€äº†Text2Worldã€Image2Worldå’ŒVideo2Worldçš„ç”Ÿæˆï¼Œå¹¶åˆ©ç”¨[Cosmos-Reason1]è¿™ä¸€ç‰©ç†äººå·¥æ™ºèƒ½è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæä¾›æ›´ä¸°å¯Œçš„æ–‡æœ¬åŸºç¡€å’Œæ›´ç²¾ç»†çš„ä¸–ç•Œæ¨¡æ‹Ÿæ§åˆ¶ã€‚ç»è¿‡åœ¨2äº¿ä¸ªç²¾é€‰çš„è§†é¢‘ç‰‡æ®µä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡åŸºäºå¼ºåŒ–å­¦ä¹ çš„åæœŸè®­ç»ƒè¿›è¡Œå¾®è°ƒï¼Œ[Cosmos-Predict2.5]åœ¨è§†é¢‘è´¨é‡å’ŒæŒ‡ä»¤å¯¹é½æ–¹é¢è¾ƒä¹‹å‰çš„æ¨¡å‹å®ç°äº†æ˜¾è‘—æå‡ã€‚ç›®å‰å‘å¸ƒæ¨¡å‹è§„æ¨¡ä¸ºä¸¤åƒäº¿å’Œä¸€ä¸‡å››åƒäº¿ã€‚è¿™äº›åŠŸèƒ½èƒ½å¤Ÿæ”¯æŒæ›´å¯é çš„ç»¼åˆæ•°æ®ç”Ÿæˆã€ç­–ç•¥è¯„ä¼°å’Œé—­ç¯æ¨¡æ‹Ÿï¼Œåœ¨æœºå™¨äººå’Œè‡ªä¸»ç³»ç»Ÿä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¨å‡ºäº†å®¶æ—äº§å“[Cosmos-Transfer2.5]ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºSim2Realå’ŒReal2Realä¸–ç•Œç¿»è¯‘çš„æ§åˆ¶ç½‘ç»œé£æ ¼æ¡†æ¶ã€‚å°½ç®¡ä½“ç§¯æ¯”ä¸Šä¸€ä»£äº§å“å°3.5å€ï¼Œ[Cosmos-Transfer2.5]å´å®ç°äº†æ›´é«˜ä¿çœŸåº¦å’Œç¨³å¥çš„é•¿æœŸè§†é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œ[Cosmos-Predict2.5]å’Œ[Cosmos-Transfer2.5]çš„è¿›æ­¥ä½¿å…¶æˆä¸ºæ™ºèƒ½ä½“æŠ€æœ¯çš„é€šç”¨å·¥å…·ã€‚ä¸ºäº†åŠ é€Ÿç‰©ç†äººå·¥æ™ºèƒ½çš„ç ”ç©¶å’Œåº”ç”¨éƒ¨ç½²ï¼Œæˆ‘ä»¬åœ¨NVIDIAå¼€æ”¾æ¨¡å‹è®¸å¯ä¸‹å‘å¸ƒäº†æºä»£ç ã€é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å’Œç²¾é€‰åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œç½‘å€ä¸º<a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-predict2.5%E5%92%8Chttps://github.com/nvidia-cosmos/cosmos-transfer2.5%E3%80%82%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E8%BF%99%E4%BA%9B%E5%BC%80%E6%94%BE%E8%B5%84%E6%BA%90%E8%83%BD%E5%A4%9F%E9%99%8D%E4%BD%8E%E9%87%87%E7%94%A8%E9%97%A8%E6%A7%9B%E5%B9%B6%E4%BF%83%E8%BF%9B%E5%88%9B%E6%96%B0%EF%BC%8C%E5%85%B1%E5%90%8C%E6%8E%A8%E5%8A%A8%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%99%BA%E8%83%BD%E4%BD%93%E6%8A%80%E6%9C%AF%E7%9A%84%E5%8F%91%E5%B1%95%E3%80%82">https://github.com/nvidia-cosmos/cosmos-predict2.5å’Œhttps://github.com/nvidia-cosmos/cosmos-transfer2.5ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å¼€æ”¾èµ„æºèƒ½å¤Ÿé™ä½é‡‡ç”¨é—¨æ§›å¹¶ä¿ƒè¿›åˆ›æ–°ï¼Œå…±åŒæ¨åŠ¨ä¸‹ä¸€ä»£æ™ºèƒ½ä½“æŠ€æœ¯çš„å‘å±•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00062v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>[Cosmos-Predict2.5]æ˜¯å®‡å®™ä¸–ç•ŒåŸºé‡‘ä¼šæ¨¡å‹çš„æœ€æ–°ä¸€ä»£ç‰©ç†äººå·¥æ™ºèƒ½ã€‚å®ƒé‡‡ç”¨æµæ¶æ„ï¼Œç»Ÿä¸€äº†Text2Worldã€Image2Worldå’ŒVideo2Worldç”Ÿæˆï¼Œåœ¨ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­å®ç°äº†æ–‡æœ¬æ¥åœ°æ›´ä¸°å¯Œã€ä¸–ç•Œæ¨¡æ‹Ÿæ§åˆ¶æ›´ç²¾ç»†çš„[Cosmos-Reason1]ç‰©ç†äººå·¥æ™ºèƒ½è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ç»è¿‡åœ¨2äº¿ä¸ªç²¾é€‰è§†é¢‘ç‰‡æ®µä¸Šçš„è®­ç»ƒï¼Œå¹¶åˆ©ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒè¿›è¡Œç²¾ç‚¼ï¼Œ[Cosmos-Predict2.5]åœ¨è§†é¢‘è´¨é‡å’ŒæŒ‡ä»¤å¯¹é½æ–¹é¢ç›¸å¯¹äº[Cosmos-Predict1]å®ç°äº†å®è´¨æ€§æ”¹è¿›ï¼Œå‘å¸ƒè§„æ¨¡ä¸º2Bå’Œ14Bçš„æ¨¡å‹ã€‚è¿™äº›åŠŸèƒ½ä½¿æ›´å¯é åˆæˆæ•°æ®ç”Ÿæˆã€æ”¿ç­–è¯„ä¼°ã€æœºå™¨äººå’Œè‡ªä¸»ç³»ç»Ÿçš„é—­ç¯æ¨¡æ‹Ÿæˆä¸ºå¯èƒ½ã€‚[Cosmos-Transfer2.5]æ˜¯æ§åˆ¶ç½‘ç»œé£æ ¼çš„æ¡†æ¶ï¼Œç”¨äºSim2Realå’ŒReal2Realä¸–ç•Œç¿»è¯‘ã€‚å°½ç®¡å®ƒæ¯”[Cosmos-Transfer1]å°3.5å€ï¼Œä½†æä¾›äº†æ›´é«˜ä¿çœŸå’Œç¨³å¥çš„é•¿æœŸè§†é¢‘ç”Ÿæˆã€‚è¿™äº›è¿›æ­¥ä½¿[Cosmos-Predict2.5]å’Œ[Cosmos-Transfer2.5]æˆä¸ºæ¨åŠ¨æ™ºèƒ½èº«ä½“è§„æ¨¡åŒ–çš„å¤šåŠŸèƒ½å·¥å…·ã€‚ä¸ºåŠ é€Ÿç‰©ç†äººå·¥æ™ºèƒ½çš„ç ”ç©¶å’Œéƒ¨ç½²ï¼Œæˆ‘ä»¬åœ¨NVIDIAå¼€æ”¾æ¨¡å‹è®¸å¯è¯ä¸‹å‘å¸ƒæºä»£ç ã€é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å’Œç²¾é€‰åŸºå‡†æµ‹è¯•ï¼Œå¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-predict2.5">https://github.com/nvidia-cosmos/cosmos-predict2.5</a>å’Œ<a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-transfer2.5%E3%80%82%E6%88%91%E4%BB%AC%E5%B9%BF%E6%9C%9F%E7%AE%97%E6%B3%9B%E7%9D%A9%E6%BA%A9%E7%9A%84%E5%AF%B8%E9%BB%8Dtv'%E7'%A3%'BHxvuvUQq7rV&open_new_window=true">https://github.com/nvidia-cosmos/cosmos-transfer2.5ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å¼€æ”¾èµ„æºèƒ½é™ä½é‡‡ç”¨é—¨æ§›å¹¶ä¿ƒè¿›åˆ›æ–°ï¼Œä»¥æ„å»ºä¸‹ä¸€ä»£æ™ºèƒ½èº«ä½“ã€‚</a>ã€‚è¿™äº›èµ„æºæ—¨åœ¨æ¨åŠ¨ç‰©ç†äººå·¥æ™ºèƒ½é¢†åŸŸçš„ç ”ç©¶å’Œåˆ›æ–°ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™äº›æ¨¡å‹çš„å¼€æºå°†ä¿ƒè¿›æ›´å¤šç ”ç©¶äººå‘˜å’Œä¼ä¸šå‚ä¸åˆ°ç‰©ç†äººå·¥æ™ºèƒ½çš„ç ”å‘ä¸­æ¥ï¼Œæ¨åŠ¨æŠ€æœ¯è¿›æ­¥å’Œåˆ›æ–°åº”ç”¨çš„å‘å±•ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿå°†æ›´åŠ ä¾èµ–äºç‰©ç†ä¸–ç•Œçš„ç†è§£å’Œæ¨¡æ‹Ÿèƒ½åŠ›ï¼Œè€Œè¿™äº›æ¨¡å‹å°†ä¸ºè¿™ä¸€é¢†åŸŸçš„å‘å±•æä¾›å¼ºå¤§çš„æ”¯æŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›è¿™äº›å¼€æ”¾èµ„æºèƒ½å¤Ÿå¸®åŠ©é™ä½å¼€å‘é—¨æ§›ï¼Œä½¿å¾—æ›´å¤šçš„å¼€å‘è€…å’Œç ”ç©¶äººå‘˜èƒ½å¤Ÿæ›´å®¹æ˜“åœ°è·å–å’Œä½¿ç”¨è¿™äº›æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨æ™ºèƒ½ç§‘æŠ€é¢†åŸŸçš„å‘å±•ã€‚æ€»çš„æ¥è¯´ï¼Œ[Cosmos-Predict2.5]å’Œ[Cosmos-Transfer2.5]çš„å¼€æºå°†ä¸ºç‰©ç†äººå·¥æ™ºèƒ½çš„å‘å±•æ³¨å…¥æ–°çš„æ´»åŠ›ï¼Œå¹¶æ¨åŠ¨æ™ºèƒ½ç§‘æŠ€é¢†åŸŸçš„è¿›æ­¥ã€‚æˆ‘ä»¬ä¹ŸæœŸå¾…ç€è¿™ä¸ªé¢†åŸŸçš„æœªæ¥å‘å±•ã€‚æˆ‘ä»¬ä¹Ÿæ¬¢è¿å¹¿å¤§ç”¨æˆ·ç§¯æå‚ä¸æˆ‘ä»¬çš„å¼€æºé¡¹ç›®ï¼Œå…±åŒæ¨åŠ¨ç‰©ç†äººå·¥æ™ºèƒ½çš„å‘å±•å’Œåº”ç”¨è½åœ°ã€‚è®©æˆ‘ä»¬ä¸€èµ·æºæ‰‹æ„å»ºä¸€ä¸ªæ›´æ™ºèƒ½çš„ä¸–ç•Œï¼<strong>å…³é”®è§è§£</strong></p>
<ol>
<li>[Cosmos-Predict2.5]æ˜¯æœ€æ–°ä¸€ä»£å®‡å®™ä¸–ç•ŒåŸºé‡‘ä¼šæ¨¡å‹ï¼Œç”¨äºç‰©ç†äººå·¥æ™ºèƒ½ã€‚</li>
<li>åŸºäºæµæ¶æ„ï¼Œå®ƒç»Ÿä¸€äº†Text2Worldã€Image2Worldå’ŒVideo2Worldç”Ÿæˆã€‚</li>
<li>å¼•å…¥[Cosmos-Reason1]ï¼Œæä¾›ä¸°å¯Œçš„æ–‡æœ¬æ¥åœ°å’Œç²¾ç»†çš„ä¸–ç•Œæ¨¡æ‹Ÿæ§åˆ¶ã€‚</li>
<li>ç»è¿‡åœ¨å¤§é‡è§†é¢‘ç‰‡æ®µä¸Šçš„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼Œè§†é¢‘è´¨é‡å’ŒæŒ‡ä»¤å¯¹é½å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
<li>æ¨å‡º[Cosmos-Transfer2.5]ï¼Œç”¨äºSim2Realå’ŒReal2Realä¸–ç•Œç¿»è¯‘çš„æ§åˆ¶ç½‘ç»œé£æ ¼æ¡†æ¶ã€‚</li>
<li>å°½ç®¡è§„æ¨¡ç¼©å°ï¼Œä½†[Cosmos-Transfer2.5]å®ç°äº†æ›´é«˜ä¿çœŸå’Œé•¿æœŸè§†é¢‘ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af080989f0ac7b2687535ce7398a41aa" align="middle">
<img src="https://picx.zhimg.com/v2-e7a765920dcf1ac0733697443fdeaccd" align="middle">
<img src="https://picx.zhimg.com/v2-ed20cfe8fba886fcaa60d99cddcfe0de" align="middle">
<img src="https://picx.zhimg.com/v2-86b2ed38c96c1d2a85c3c88183149463" align="middle">
<img src="https://picx.zhimg.com/v2-0b0bdfca675aa4145919a14a4e3cb00a" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs"><a href="#SeeingEye-Agentic-Information-Flow-Unlocks-Multimodal-Reasoning-In-Text-only-LLMs" class="headerlink" title="SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs"></a>SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs</h2><p><strong>Authors:Weijia Zhang, Zijia Liu, Haoru Li, Haoqi Chen, Jiaxuan You</strong></p>
<p>Recent advances in text-only large language models (LLMs), such as DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models remain fragile or entirely incapable when extended to multi-modal tasks. Existing approaches largely rely on single-form captions, which lack diversity and often fail to adapt across different types of Visual Question Answering (VQA) benchmarks. As a result, they provide no principled or efficient channel for transmitting fine-grained visual information. We introduce Seeing Eye, a modular framework that unlocks multimodal reasoning in text-only LLMs through an agent-based small VLM translator. This translator acts as a perception agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. These SIRs are then passed to the text-only LLM, which serves as a reasoning agent. Crucially, the translator and reasoner engage in multi-round feedback and interaction, enabling the extraction of targeted visual details and yielding more confident answers. Experiments on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate that Seeing Eye not only reduces inference cost but also surpasses much larger end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision translator with an 8B-parameter language reasoner outperforms a monolithic 32B VLM on challenging knowledge-based questions. Our results highlight that decoupling perception from reasoning via agent information flow offers a scalable and plug-and-play pathway to multimodal reasoning, allowing strong text-only LLMs to fully leverage their reasoning capabilities. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/SeeingEye">https://github.com/ulab-uiuc/SeeingEye</a></p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚DeepSeek-R1å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“æ‰©å±•åˆ°å¤šæ¨¡æ€ä»»åŠ¡æ—¶ï¼Œè¿™äº›æ¨¡å‹ä»ç„¶æ˜¾å¾—è„†å¼±æˆ–å®Œå…¨æ— æ³•åº”å¯¹ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºå•ä¸€å½¢å¼çš„å­—å¹•ï¼Œç¼ºä¹å¤šæ ·æ€§ï¼Œä¸”å¾€å¾€æ— æ³•é€‚åº”ä¸åŒç±»å‹çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ã€‚å› æ­¤ï¼Œå®ƒä»¬æ²¡æœ‰æä¾›ä¼ è¾“ç²¾ç»†ç²’åº¦è§†è§‰ä¿¡æ¯çš„åŸåˆ™æ€§æˆ–æœ‰æ•ˆæ€§æ¸ é“ã€‚æˆ‘ä»¬æ¨å‡ºäº†Seeing Eyeï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºä»£ç†çš„å°å‹VLMç¿»è¯‘å™¨åœ¨æ–‡æœ¬å‹LLMä¸­å®ç°å¤šæ¨¡æ€æ¨ç†ã€‚è¯¥ç¿»è¯‘å™¨å……å½“æ„ŸçŸ¥ä»£ç†ï¼šå®ƒå¯ä»¥è°ƒç”¨ä¸“ä¸šå·¥å…·ï¼ˆä¾‹å¦‚OCRå’Œè£å‰ªï¼‰ï¼Œå¹¶è¿­ä»£åœ°å°†å¤šæ¨¡æ€è¾“å…¥è½¬åŒ–ä¸ºé’ˆå¯¹é—®é¢˜çš„ç»“æ„åŒ–ä¸­é—´è¡¨ç¤ºï¼ˆSIRï¼‰ã€‚ç„¶åï¼Œè¿™äº›SIRä¼ é€’ç»™æ–‡æœ¬å‹LLMï¼Œå……å½“æ¨ç†ä»£ç†ã€‚å…³é”®çš„æ˜¯ï¼Œç¿»è¯‘å™¨å’Œæ¨ç†å™¨è¿›è¡Œå¤šè½®åé¦ˆå’Œäº¤äº’ï¼Œèƒ½å¤Ÿæå–æœ‰é’ˆå¯¹æ€§çš„è§†è§‰ç»†èŠ‚å¹¶äº§ç”Ÿæ›´ç¡®å®šçš„ç­”æ¡ˆã€‚åœ¨çŸ¥è¯†å¯†é›†å‹VQAåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬MMMUå’ŒMIA-Benchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSeeing Eyeä¸ä»…é™ä½äº†æ¨ç†æˆæœ¬ï¼Œè€Œä¸”è¶…è¶Šäº†æ›´å¤§çš„ç«¯åˆ°ç«¯VLMã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç»“åˆäº†3Bå‚æ•°è§†è§‰ç¿»è¯‘å™¨å’Œ8Bå‚æ•°è¯­è¨€æ¨ç†å™¨çš„å®ä¾‹ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çŸ¥è¯†æ€§é—®é¢˜ä¸Šè¡¨ç°ä¼˜äº32Bå•ä¸€VLMã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œé€šè¿‡ä»£ç†ä¿¡æ¯æµç¨‹å°†æ„ŸçŸ¥ä¸æ¨ç†è§£è€¦ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†æä¾›äº†å¯æ‰©å±•çš„å³æ’å³ç”¨è·¯å¾„ï¼Œä½¿å¼ºå¤§çš„æ–‡æœ¬å‹LLMèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/SeeingEye">https://github.com/ulab-uiuc/SeeingEye</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25092v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºæ–‡æœ¬å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³çš„ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†Seeing Eyeæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŸºäºä»£ç†çš„å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç¿»è¯‘å™¨è§£é”äº†æ–‡æœ¬å‹LLMsçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯¥ç¿»è¯‘å™¨èƒ½å¤Ÿè°ƒç”¨ä¸“ä¸šå·¥å…·å¤„ç†è§†è§‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºé€‚åˆé—®é¢˜çš„ç»“æ„åŒ–ä¸­é—´è¡¨ç¤ºï¼ˆSIRï¼‰ã€‚è¿™äº›SIRséšåä¼ é€’ç»™æ–‡æœ¬å‹LLMè¿›è¡Œæ¨ç†ã€‚ä¸¤è€…ä¹‹é—´çš„å¤šè½®åé¦ˆå’Œäº¤äº’ä½¿å¾—ç­”æ¡ˆæ›´åŠ ç²¾å‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒSeeing Eyeä¸ä»…é™ä½äº†æ¨ç†æˆæœ¬ï¼Œè€Œä¸”åœ¨çŸ¥è¯†å¯†é›†å‹å¤šæ¨¡æ€é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¸€äº›å¤§å‹çš„ç«¯åˆ°ç«¯VLMsã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ã€‚</li>
<li>Seeing Eyeæ¡†æ¶é€šè¿‡å¼•å…¥åŸºäºä»£ç†çš„å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç¿»è¯‘å™¨è§£é”äº†æ–‡æœ¬å‹LLMsçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥ç¿»è¯‘å™¨å¯ä»¥è°ƒç”¨ä¸“ä¸šå·¥å…·å¤„ç†è§†è§‰ä¿¡æ¯å¹¶è½¬åŒ–ä¸ºç»“æ„åŒ–ä¸­é—´è¡¨ç¤ºï¼ˆSIRï¼‰ã€‚</li>
<li>Seeing Eyeæ¡†æ¶é€šè¿‡å¤šè½®åé¦ˆå’Œäº¤äº’æé«˜äº†ç­”æ¡ˆçš„ç²¾å‡†åº¦ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSeeing Eyeåœ¨çŸ¥è¯†å¯†é›†å‹å¤šæ¨¡æ€é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†éƒ¨åˆ†å¤§å‹ç«¯åˆ°ç«¯VLMsã€‚</li>
<li>Seeing Eyeæ¡†æ¶é™ä½äº†æ¨ç†æˆæœ¬ã€‚</li>
<li>ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å¯ä¾›æŸ¥é˜…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-738186cff0d222733b6c9a6b5016f943" align="middle">
<img src="https://picx.zhimg.com/v2-d0ae8839d767cfa45e5bcc14ae52ca8a" align="middle">
<img src="https://picx.zhimg.com/v2-f2709d9e93ffeb73a4a8c54c002d7ffc" align="middle">
<img src="https://picx.zhimg.com/v2-c40d4a66dc10322a7bbd7a9038368ed7" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PISA-Bench-The-PISA-Index-as-a-Multilingual-and-Multimodal-Metric-for-the-Evaluation-of-Vision-Language-Models"><a href="#PISA-Bench-The-PISA-Index-as-a-Multilingual-and-Multimodal-Metric-for-the-Evaluation-of-Vision-Language-Models" class="headerlink" title="PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models"></a>PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models</h2><p><strong>Authors:Patrick Haller, Fabio Barth, Jonas Golde, Georg Rehm, Alan Akbik</strong></p>
<p>Vision-language models (VLMs) have demonstrated remarkable progress in multimodal reasoning. However, existing benchmarks remain limited in terms of high-quality, human-verified examples. Many current datasets rely on synthetically generated content by large language models (LLMs). Furthermore, most datasets are limited to English, as manual quality assurance of translated samples is time-consuming and costly. To fill this gap, we introduce PISA-Bench, a multilingual benchmark derived from English examples of the expert-created PISA tests, a unified framework for the assessment of student competencies in over eighty countries. Each example consists of human-extracted instructions, questions, answer options, and images, enriched with question type categories, and has been translated from English into five additional languages (Spanish, German, Chinese, French, and Italian), resulting in a fully parallel corpus covering six languages. We evaluate state-of-the-art vision-language models on PISA-Bench and find that especially small models (&lt;20B parameters) fail to achieve high test scores. We further find substantial performance degradation on non-English splits as well as high error-rates when models are tasked with spatial and geometric reasoning. By releasing the dataset and evaluation framework, we provide a resource for advancing research on multilingual multimodal reasoning.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨é«˜è´¨é‡ã€ç»äººå·¥éªŒè¯çš„æ ·æœ¬æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚ç›®å‰è®¸å¤šæ•°æ®é›†ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆæˆç”Ÿæˆçš„å†…å®¹ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æ•°æ®é›†ä»…é™äºè‹±è¯­ï¼Œå› ä¸ºå¯¹ç¿»è¯‘æ ·æœ¬è¿›è¡Œäººå·¥è´¨é‡ä¿éšœæ˜¯è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PISA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæºäºä¸“å®¶åˆ›å»ºçš„PISAæµ‹è¯•è‹±è¯­æ ·æœ¬çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ã€‚PISAæµ‹è¯•æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å…«åå¤šä¸ªå›½å®¶å­¦ç”Ÿçš„èƒ½åŠ›ã€‚æ¯ä¸ªæ ·æœ¬éƒ½ç”±äººå·¥æå–çš„æŒ‡ä»¤ã€é—®é¢˜ã€ç­”æ¡ˆé€‰é¡¹å’Œå›¾åƒç»„æˆï¼Œä¸°å¯Œäº†é—®é¢˜ç±»å‹ç±»åˆ«ï¼Œå¹¶ä»è‹±è¯­ç¿»è¯‘æˆäº†å¦å¤–äº”ç§è¯­è¨€ï¼ˆè¥¿ç­ç‰™è¯­ã€å¾·è¯­ã€ä¸­æ–‡ã€æ³•è¯­å’Œæ„å¤§åˆ©è¯­ï¼‰ï¼Œä»è€Œå½¢æˆäº†ä¸€ä¸ªæ¶µç›–å…­ç§è¯­è¨€çš„å®Œå…¨å¹³è¡Œè¯­æ–™åº“ã€‚æˆ‘ä»¬åœ¨PISA-Benchä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å°¤å…¶æ˜¯å°äº20Bå‚æ•°çš„å°æ¨¡å‹éš¾ä»¥å–å¾—è¾ƒé«˜çš„æµ‹è¯•æˆç»©ã€‚æˆ‘ä»¬è¿˜å‘ç°åœ¨éè‹±è¯­åˆ†å‰²ä¸Šçš„æ€§èƒ½å¤§å¹…ä¸‹é™ï¼Œä»¥åŠåœ¨æ¨¡å‹å’Œç©ºé—´åŠå‡ ä½•æ¨ç†ä»»åŠ¡ä¸­çš„é«˜é”™è¯¯ç‡ã€‚æˆ‘ä»¬é€šè¿‡å‘å¸ƒæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œä¸ºæ¨è¿›å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24792v2">PDF</a> 8 pages, 11 tables and figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºPISAæµ‹è¯•çš„å¤šè¯­è¨€åŸºå‡†æ•°æ®é›†PISA-Benchå¡«è¡¥äº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†é¢†åŸŸçš„ç©ºç™½ã€‚è¯¥æ•°æ®é›†ä¸ä»…åŒ…å«äº†è‹±è¯­æ ·æœ¬ï¼Œè¿˜ç¿»è¯‘æˆäº†äº”ç§å…¶ä»–è¯­è¨€ï¼Œå½¢æˆäº†ä¸€ä¸ªå…¨é¢çš„å¹³è¡Œè¯­æ–™åº“ã€‚è¯„ä¼°å‘ç°ï¼Œå°å‹æ¨¡å‹åœ¨éè‹±è¯­åˆ†æ”¯ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶åœ¨ç©ºé—´å‡ ä½•æ¨ç†æ–¹é¢é”™è¯¯ç‡è¾ƒé«˜ã€‚æ­¤æ•°æ®é›†çš„å‘å¸ƒä¸ºæ¨è¿›å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PISA-Benchæ˜¯ä¸€ä¸ªåŸºäºPISAæµ‹è¯•çš„å¤šè¯­è¨€åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è‹±è¯­ä»¥åŠå…¶ä»–äº”ç§è¯­è¨€çš„å¹³è¡Œè¯­æ–™ï¼Œä¸°å¯Œäº†æ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶æœ‰åŠ©äºç ”ç©¶å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨PISA-Benchä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯å°å‹æ¨¡å‹ã€‚</li>
<li>éè‹±è¯­åˆ†æ”¯ä¸Šçš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¡¨æ˜æ¨¡å‹å¯¹ä¸åŒè¯­è¨€çš„é€‚åº”æ€§æœ‰å¾…æé«˜ã€‚</li>
<li>æ¨¡å‹åœ¨ç©ºé—´å‡ ä½•æ¨ç†æ–¹é¢çš„é”™è¯¯ç‡è¾ƒé«˜ï¼Œéœ€è¦è¿›ä¸€æ­¥åŠ å¼ºç›¸å…³ç ”ç©¶ã€‚</li>
<li>PISA-Benchçš„å‘å¸ƒä¸ºæ¨è¿›å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24792">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55adffce2ed82117b298f5d703bc27e5" align="middle">
<img src="https://picx.zhimg.com/v2-c17893b4a5da046157dfd2da811e9a86" align="middle">
<img src="https://picx.zhimg.com/v2-24e69397f2b60d46dfaec1b86d7a79e8" align="middle">
<img src="https://picx.zhimg.com/v2-a91fa7f12361a809204cd2eaa081a8ef" align="middle">
<img src="https://picx.zhimg.com/v2-b6e61c619902db031dcad741b9d2db12" align="middle">
<img src="https://picx.zhimg.com/v2-cc1fa247d05beadfa78553dbe06c9090" align="middle">
<img src="https://picx.zhimg.com/v2-984c307e3d2d64ed43b69285d5e582f8" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs"><a href="#Latent-Sketchpad-Sketching-Visual-Thoughts-to-Elicit-Multimodal-Reasoning-in-MLLMs" class="headerlink" title="Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs"></a>Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</h2><p><strong>Authors:Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending modelâ€™s textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: <a target="_blank" rel="noopener" href="https://latent-sketchpad.github.io/">https://latent-sketchpad.github.io/</a>.</p>
<blockquote>
<p>è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦è§†è§‰è§„åˆ’å’Œæƒ³è±¡åŠ›çš„å¤æ‚åœºæ™¯ä¸­ï¼Œå®ƒä»¬å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚å—äººç±»ä½¿ç”¨è‰å›¾ä½œä¸ºè§†è§‰æ€ç»´æ¥å‘å±•å’Œäº¤æµæ€æƒ³æ–¹å¼çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨è‰å›¾æ¿ï¼ˆLatent Sketchpadï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ºMLLMsé…å¤‡äº†å†…éƒ¨è§†è§‰è‰ç¨¿æ¿ã€‚MLLMsçš„å†…éƒ¨è§†è§‰è¡¨ç¤ºä¼ ç»Ÿä¸Šä»…é™äºæ„ŸçŸ¥ç†è§£ã€‚æˆ‘ä»¬é‡æ–°å®šä½å®ƒä»¬ä»¥æ”¯æŒç”Ÿæˆå¼è§†è§‰æ€ç»´ï¼Œè€Œä¸ä¼šæŸå®³æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºå‰æ²¿çš„MLLMsæ„å»ºï¼Œå°†è§†è§‰ç”Ÿæˆç›´æ¥é›†æˆåˆ°å…¶å›ºæœ‰çš„è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ã€‚å®ƒå…è®¸æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†å’Œè§†è§‰æ½œåœ¨ç”Ÿæˆä¹‹é—´è¿›è¡Œäº¤æ›¿ã€‚è¿™äº›æ½œåœ¨æ€§æŒ‡å¯¼å†…éƒ¨æ€ç»´è¿‡ç¨‹ï¼Œå¹¶å¯è½¬åŒ–ä¸ºè‰å›¾å›¾åƒä»¥å®ç°å¯è§£é‡Šæ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªç»„ä»¶ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰å¤´ï¼ˆContext-Aware Vision Headï¼‰è‡ªå›å½’åœ°ç”Ÿæˆè§†è§‰è¡¨ç¤ºï¼Œé¢„è®­ç»ƒçš„è‰å›¾è§£ç å™¨ï¼ˆSketch Decoderï¼‰å°†è¿™äº›è¡¨ç¤ºå‘ˆç°ä¸ºäººç±»å¯ç†è§£çš„å›¾åƒã€‚æˆ‘ä»¬åœ¨æ–°çš„æ•°æ®é›†è¿·å®«è§„åˆ’ï¼ˆMazePlanningï¼‰ä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶ã€‚è·¨å„ç§MLLMsçš„å®éªŒè¡¨æ˜ï¼Œæ½œåœ¨è‰å›¾æ¿æ¡†æ¶åœ¨æ¨ç†æ€§èƒ½ä¸Šè¡¨ç°ç›¸å½“ç”šè‡³æ›´èƒœä¸€ç­¹ã€‚å®ƒè¿›ä¸€æ­¥é€‚ç”¨äºä¸åŒçš„å‰æ²¿MLLMsï¼ŒåŒ…æ‹¬Gemma3å’ŒQwen2.5-VLã€‚é€šè¿‡å°†æ¨¡å‹çš„æ–‡æœ¬æ¨ç†æ‰©å±•åˆ°è§†è§‰æ€ç»´ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ºæ›´ä¸°å¯Œçš„äººæœºäº¤äº’å’Œæ›´å¹¿æ³›çš„åº”ç”¨æä¾›äº†æ–°çš„æœºä¼šã€‚æ›´å¤šç»†èŠ‚å’Œèµ„æºè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://latent-sketchpad.github.io/">https://latent-sketchpad.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24514v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººç±»åˆ©ç”¨è‰å›¾ä½œä¸ºè§†è§‰æ€è€ƒçš„æ–¹å¼æ¥å‘å±•å’Œäº¤æµæ€æƒ³ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†Latent Sketchpadæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é…å¤‡äº†å†…éƒ¨è§†è§‰è‰å›¾æ¿ã€‚æ­¤æ¡†æ¶ä½¿MLLMsçš„å†…éƒ¨è§†è§‰è¡¨å¾ä¸å†ä»…é™äºæ„ŸçŸ¥ç†è§£ï¼Œè€Œæ˜¯æ”¯æŒç”Ÿæˆæ€§çš„è§†è§‰æ€è€ƒè€Œä¸æŸå®³å…¶æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é›†æˆäº†å‰æ²¿çš„MLLMsï¼Œå°†è§†è§‰ç”Ÿæˆç›´æ¥çº³å…¥å…¶å¤©ç”Ÿçš„è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ã€‚è¯¥æ¡†æ¶å…è®¸æ¨¡å‹å°†æ–‡æœ¬æ¨ç†ä¸è§†è§‰æ½œåœ¨ç”Ÿæˆäº¤æ›¿è¿›è¡Œï¼Œè¿™äº›æ½œåœ¨ç”ŸæˆæŒ‡å¯¼å†…éƒ¨æ€ç»´è¿‡ç¨‹å¹¶å¯è½¬åŒ–ä¸ºè‰å›¾å›¾åƒä»¥ä¾¿è§£é‡Šã€‚ä¸ºç°å®æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Context-Aware Vision Headè‡ªå›å½’åœ°ç”Ÿæˆè§†è§‰è¡¨å¾å’Œé¢„è®­ç»ƒçš„Sketch Decoderå°†å…¶è½¬åŒ–ä¸ºäººç±»å¯ç†è§£çš„å›¾åƒä¸¤ä¸ªç»„ä»¶ã€‚æˆ‘ä»¬åœ¨æ–°æ•°æ®é›†MazePlanningä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œå®éªŒè¡¨æ˜ï¼ŒLatent Sketchpadåœ¨å¤šç§MLLMsä¸Šçš„æ¨ç†æ€§èƒ½ä¸å…¶ä¸»å¹²ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼Œå¹¶èƒ½å¹¿æ³›åº”ç”¨äºä¸åŒçš„å‰æ²¿MLLMsï¼ŒåŒ…æ‹¬Gemma3å’ŒQwen2.5-VLã€‚é€šè¿‡æ‰©å±•æ¨¡å‹çš„æ–‡æœ¬æ¨ç†è‡³è§†è§‰æ€è€ƒï¼Œè¯¥æ¡†æ¶ä¸ºæ›´ä¸°å¯Œçš„äººæœºäº¤äº’å’Œæ›´å¹¿æ³›çš„åº”ç”¨æ‰“å¼€äº†æ–°çš„æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨éœ€è¦è§†è§‰è§„åˆ’å’Œæƒ³è±¡åŠ›çš„å¤æ‚åœºæ™¯ä¸­å¸¸å¸¸è¡¨ç°æŒ£æ‰ã€‚</li>
<li>äººç±»åˆ©ç”¨è‰å›¾ä½œä¸ºè§†è§‰æ€è€ƒçš„æ–¹å¼å¯å‘ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†Latent Sketchpadæ¡†æ¶ã€‚</li>
<li>Latent Sketchpadä¸ºMLLMsé…å¤‡äº†å†…éƒ¨è§†è§‰è‰å›¾æ¿ï¼Œæ”¯æŒç”Ÿæˆæ€§è§†è§‰æ€è€ƒè€Œä¸æŸå®³å…¶æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†å‰æ²¿çš„MLLMsï¼Œå°†è§†è§‰ç”Ÿæˆçº³å…¥å…¶è‡ªå›å½’æ¨ç†è¿‡ç¨‹ä¸­ã€‚</li>
<li>Latent Sketchpadå…è®¸æ¨¡å‹äº¤æ›¿è¿›è¡Œæ–‡æœ¬æ¨ç†ä¸è§†è§‰æ½œåœ¨ç”Ÿæˆï¼Œè¿™äº›æ½œåœ¨ç”Ÿæˆå¯è½¬åŒ–ä¸ºè‰å›¾å›¾åƒä»¥ä¾¿è§£é‡Šã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šContext-Aware Vision Headå’ŒSketch Decoderã€‚</li>
<li>åœ¨MazePlanningæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLatent Sketchpadåœ¨å¤šç§MLLMsä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå¹¶èƒ½å¹¿æ³›åº”ç”¨äºä¸åŒæ¨¡å‹ã€‚è¯¥æ¡†æ¶æœ‰åŠ©äºæ›´ä¸°å¯Œçš„äººæœºäº¤äº’å’Œæ›´å¹¿æ³›çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58cb3b6cfc2b075a068e47d809d6c7bd" align="middle">
<img src="https://picx.zhimg.com/v2-125df9d57d4798ee1929636060895bcb" align="middle">
<img src="https://picx.zhimg.com/v2-170fd4ccef3b6b3ac7230db060c55522" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Seeing-Across-Views-Benchmarking-Spatial-Reasoning-of-Vision-Language-Models-in-Robotic-Scenes"><a href="#Seeing-Across-Views-Benchmarking-Spatial-Reasoning-of-Vision-Language-Models-in-Robotic-Scenes" class="headerlink" title="Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes"></a>Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</h2><p><strong>Authors:Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo</strong></p>
<p>Vision-language models (VLMs) are essential to Embodied AI, enabling robots to perceive, reason, and act in complex environments. They also serve as the foundation for the recent Vision-Language-Action (VLA) models. Yet most evaluations of VLMs focus on single-view settings, leaving their ability to integrate multi-view information underexplored. At the same time, multi-camera setups are increasingly standard in robotic platforms, as they provide complementary perspectives to mitigate occlusion and depth ambiguity. Whether VLMs can effectively leverage such multi-view inputs for robotic reasoning therefore remains an open question. To bridge this gap, we introduce MV-RoboBench, a benchmark specifically designed to evaluate the multi-view spatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench consists of 1.7k manually curated QA items across eight subtasks, divided into two primary categories: spatial understanding and robotic execution. We evaluate a diverse set of existing VLMs, including both open-source and closed-source models, along with enhanced versions incorporating CoT-inspired techniques. The results show that state-of-the-art models remain far below human performance, underscoring the substantial challenges VLMs face in multi-view robotic perception. Additionally, our analysis uncovers two key findings: (i) spatial intelligence and robotic task execution are positively correlated in multi-view robotic scenarios; and (ii) strong performance on existing general-purpose single-view spatial understanding benchmarks does not reliably translate to success in the robotic spatial tasks assessed by our benchmark. We release MV-RoboBench as an open resource to foster progress in spatially grounded VLMs and VLAs, providing not only data but also a standardized evaluation protocol for multi-view embodied reasoning.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹äºåµŒå…¥å¼äººå·¥æ™ºèƒ½ï¼ˆEmbodied AIï¼‰è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿè®©æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œæ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ã€‚å®ƒä»¬ä¹Ÿæ˜¯æœ€è¿‘è§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å¯¹VLMsçš„è¯„ä¼°éƒ½é›†ä¸­åœ¨å•è§†å›¾è®¾ç½®ä¸Šï¼Œå¿½è§†äº†å®ƒä»¬æ•´åˆå¤šè§†å›¾ä¿¡æ¯çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¤šç›¸æœºè®¾ç½®åœ¨æœºå™¨äººå¹³å°ä¸Šè¶Šæ¥è¶Šæ ‡å‡†ï¼Œå› ä¸ºå®ƒä»¬æä¾›äº†è¡¥å……è§†è§’ï¼Œå¯ä»¥ç¼“è§£é®æŒ¡å’Œæ·±åº¦æ­§ä¹‰ã€‚å› æ­¤ï¼ŒVLMsæ˜¯å¦èƒ½æœ‰æ•ˆåˆ©ç”¨è¿™ç§å¤šè§†å›¾è¾“å…¥è¿›è¡Œæœºå™¨äººæ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MV-RoboBenchåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°VLMsåœ¨å¤šè§†å›¾ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœºå™¨äººæ“ä½œã€‚MV-RoboBenchåŒ…å«1700ä¸ªç»è¿‡äººå·¥ç­›é€‰çš„é—®ç­”é¡¹ç›®ï¼Œåˆ†ä¸ºå…«ä¸ªå­ä»»åŠ¡ï¼Œåˆ†ä¸ºä¸¤ä¸ªä¸»è¦ç±»åˆ«ï¼šç©ºé—´ç†è§£å’Œæœºå™¨äººæ‰§è¡Œã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—ç°æœ‰çš„VLMsï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹ï¼Œä»¥åŠèå…¥è®¤çŸ¥å›¾çµï¼ˆCoTï¼‰å¯å‘æŠ€æœ¯çš„å¢å¼ºç‰ˆæœ¬ã€‚ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½ä»ç„¶è¿œè¿œä½äºäººç±»æ°´å¹³ï¼Œå¼ºè°ƒäº†VLMsåœ¨å¤šè§†å›¾æœºå™¨äººæ„ŸçŸ¥æ–¹é¢æ‰€é¢ä¸´çš„å·¨å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¿˜å‘ç°äº†ä¸¤ä¸ªå…³é”®å‘ç°ï¼šï¼ˆiï¼‰åœ¨å¤šè§†å›¾æœºå™¨äººåœºæ™¯ä¸­ï¼Œç©ºé—´æ™ºèƒ½å’Œæœºå™¨äººä»»åŠ¡æ‰§è¡Œæ˜¯æ­£ç›¸å…³çš„ï¼›ï¼ˆiiï¼‰åœ¨ç°æœ‰çš„é€šç”¨å•è§†å›¾ç©ºé—´ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å‡ºè‰²è¡¨ç°å¹¶ä¸ä¸€å®šèƒ½è½¬åŒ–ä¸ºåœ¨æˆ‘ä»¬åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°çš„æœºå™¨äººç©ºé—´ä»»åŠ¡çš„æˆåŠŸã€‚æˆ‘ä»¬å°†MV-RoboBenchä½œä¸ºå¼€æ”¾èµ„æºå‘å¸ƒï¼Œä»¥ä¿ƒè¿›åŸºäºç©ºé—´çš„è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œä¸ä»…æä¾›æ•°æ®ï¼Œè¿˜æä¾›æ ‡å‡†åŒ–çš„å¤šè§†å›¾åµŒå…¥å¼æ¨ç†è¯„ä¼°åè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19400v1">PDF</a> The project and benchmark are publicly available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/MV-RoboBench">https://github.com/microsoft/MV-RoboBench</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½ï¼ˆEmbodied AIï¼‰ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå¤§å¤šæ•°è¯„ä¼°ä¸»è¦å…³æ³¨å•ä¸€è§†è§’è®¾ç½®ï¼Œå¿½è§†äº†å®ƒä»¬å¯¹å¤šè§†è§’ä¿¡æ¯çš„æ•´åˆèƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡å¼•å…¥äº†MV-RoboBenchåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°VLMsåœ¨å¤šè§†è§’ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨MV-RoboBenchåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¿œä½äºäººç±»æ°´å¹³ï¼Œå‡¸æ˜¾å‡ºVLMåœ¨å¤šè§†è§’æœºå™¨äººæ„ŸçŸ¥æ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜å‘ç°äº†ä¸¤ä¸ªå…³é”®è§‚ç‚¹ï¼šåœ¨å¤šè§†è§’æœºå™¨äººåœºæ™¯ä¸­ï¼Œç©ºé—´æ™ºèƒ½å’Œæœºå™¨äººä»»åŠ¡æ‰§è¡Œèƒ½åŠ›å‘ˆæ­£ç›¸å…³ï¼›åœ¨å•ä¸€è§†è§’ç©ºé—´ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½çš„æ¨¡å‹å¹¶ä¸ä¸€å®šèƒ½æˆåŠŸåº”å¯¹æœºå™¨äººç©ºé—´ä»»åŠ¡è¯„ä¼°ã€‚æ–‡ç« å¸Œæœ›ä¿ƒè¿›åœ¨ç©ºé—´è¯­å¢ƒä¸‹çš„VLMå’ŒVLAé¢†åŸŸçš„ç ”ç©¶è¿›æ­¥ï¼Œæä¾›æ•°æ®å’Œæ ‡å‡†åŒ–çš„å¤šè§†è§’å®ä½“æ¨ç†è¯„ä¼°åè®®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½ä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ã€‚</li>
<li>å½“å‰å¯¹VLMsçš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨å•ä¸€è§†è§’è®¾ç½®ä¸Šï¼Œç¼ºä¹å¯¹å…¶å¤šè§†è§’ä¿¡æ¯æ•´åˆèƒ½åŠ›çš„æ¢ç´¢ã€‚</li>
<li>MV-RoboBenchåŸºå‡†æµ‹è¯•çš„å¼•å…¥ï¼Œæ—¨åœ¨è¯„ä¼°VLMsåœ¨å¤šè§†è§’ç©ºé—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨MV-RoboBenchä¸Šçš„è¡¨ç°è¿œä½äºäººç±»æ°´å¹³ï¼Œçªæ˜¾å‡ºVLMåœ¨å¤šè§†è§’æœºå™¨äººæ„ŸçŸ¥æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>ç©ºé—´æ™ºèƒ½å’Œæœºå™¨äººä»»åŠ¡æ‰§è¡Œèƒ½åŠ›åœ¨å¤šè§†è§’æœºå™¨äººåœºæ™¯ä¸­å‘ˆæ­£ç›¸å…³ã€‚</li>
<li>å•ä¸€è§†è§’ç©ºé—´ç†è§£åŸºå‡†æµ‹è¯•çš„è¡¨ç°å¹¶ä¸ç­‰åŒäºåœ¨æœºå™¨äººç©ºé—´ä»»åŠ¡è¯„ä¼°ä¸­çš„æˆåŠŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d99f518a87b1388c69bfb9a864e77f0" align="middle">
<img src="https://picx.zhimg.com/v2-95877ba1cdceb803928901b625105138" align="middle">
<img src="https://picx.zhimg.com/v2-e11e35bef5f1a07f1fd5fd285a73917a" align="middle">
<img src="https://picx.zhimg.com/v2-4e0e3645fd74efbab81e0b093eb625aa" align="middle">
<img src="https://picx.zhimg.com/v2-5050d1321d36a5cda1d15317e56448dd" align="middle">
<img src="https://picx.zhimg.com/v2-69bd6de0c2e941e7224d9cdfa116d6a8" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Multimodal-Safety-Is-Asymmetric-Cross-Modal-Exploits-Unlock-Black-Box-MLLMs-Jailbreaks"><a href="#Multimodal-Safety-Is-Asymmetric-Cross-Modal-Exploits-Unlock-Black-Box-MLLMs-Jailbreaks" class="headerlink" title="Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks"></a>Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks</h2><p><strong>Authors:Xinkai Wang, Beibei Li, Zerui Shao, Ao Liu, Shouling Ji</strong></p>
<p>Multimodal large language models (MLLMs) have demonstrated significant utility across diverse real-world applications. But MLLMs remain vulnerable to jailbreaks, where adversarial inputs can collapse their safety constraints and trigger unethical responses. In this work, we investigate jailbreaks in the text-vision multimodal setting and pioneer the observation that visual alignment imposes uneven safety constraints across modalities in MLLMs, thereby giving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a black-box jailbreak method grounded in reinforcement learning. Initially, we probe the modelâ€™s attention dynamics and latent representation space, assessing how visual inputs reshape cross-modal information flow and diminish the modelâ€™s ability to separate harmful from benign inputs, thereby exposing exploitable vulnerabilities. On this basis, we systematize them into generalizable and reusable operational rules that constitute a structured library of Atomic Strategy Primitives, which translate harmful intents into jailbreak inputs through step-wise transformations. Guided by the primitives, PolyJailbreak employs a multi-agent optimization process that automatically adapts inputs against the target models. We conduct comprehensive evaluations on a variety of open-source and closed-source MLLMs, demonstrating that PolyJailbreak outperforms state-of-the-art baselines.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§çœŸå®ä¸–ç•Œåº”ç”¨ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„å®ç”¨æ€§ã€‚ä½†æ˜¯ï¼ŒMLLMsä»ç„¶å®¹æ˜“å—åˆ°â€œè¶Šç‹±æ”»å‡»â€ï¼Œå¯¹æŠ—æ€§è¾“å…¥å¯èƒ½ä¼šç ´åå…¶å®‰å…¨çº¦æŸå¹¶è§¦å‘ä¸é“å¾·çš„å“åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ–‡æœ¬-è§†è§‰å¤šæ¨¡æ€è®¾ç½®ä¸­çš„â€œè¶Šç‹±æ”»å‡»â€ï¼Œå¹¶ç‡å…ˆè§‚å¯Ÿåˆ°è§†è§‰å¯¹é½åœ¨MLLMsä¸­ä¼šå¯¹ä¸åŒæ¨¡æ€æ–½åŠ ä¸å‡åŒ€çš„å®‰å…¨çº¦æŸï¼Œä»è€Œäº§ç”Ÿå¤šæ¨¡æ€å®‰å…¨ä¸å¯¹ç§°æ€§ã€‚éšåï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„PolyJailbreaké»‘ç›’è¶Šç‹±æ–¹æ³•ã€‚æœ€åˆï¼Œæˆ‘ä»¬æ¢ç´¢æ¨¡å‹çš„æ³¨æ„åŠ›åŠ¨æ€å’Œæ½œåœ¨è¡¨ç¤ºç©ºé—´ï¼Œè¯„ä¼°è§†è§‰è¾“å…¥å¦‚ä½•é‡å¡‘è·¨æ¨¡æ€ä¿¡æ¯æµå¹¶å‰Šå¼±æ¨¡å‹åŒºåˆ†æœ‰å®³å’Œè‰¯æ€§è¾“å…¥çš„èƒ½åŠ›ï¼Œä»è€Œæš´éœ²å¯åˆ©ç”¨çš„æ¼æ´ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†å®ƒä»¬ç³»ç»ŸåŒ–ï¼Œå½¢æˆå¯æ¦‚æ‹¬å’Œé‡å¤ä½¿ç”¨çš„æ“ä½œè§„åˆ™ï¼Œæ„æˆåŸå­ç­–ç•¥åŸå§‹å…ƒç´ çš„ç»“æ„åŒ–åº“ï¼Œé€šè¿‡é€æ­¥è½¬æ¢å°†æœ‰å®³æ„å›¾è½¬åŒ–ä¸ºè¶Šç‹±è¾“å…¥ã€‚åœ¨åŸå§‹å…ƒç´ çš„æŒ‡å¯¼ä¸‹ï¼ŒPolyJailbreaké‡‡ç”¨å¤šæ™ºèƒ½ä½“ä¼˜åŒ–è¿‡ç¨‹ï¼Œè‡ªåŠ¨é€‚åº”ç›®æ ‡æ¨¡å‹ã€‚æˆ‘ä»¬å¯¹å„ç§å¼€æºå’Œé—­æºMLLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœè¡¨æ˜PolyJailbreakä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17277v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ½œåœ¨æ¼æ´é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰å¯¹é½åœ¨MLLMsä¸­é€ æˆäº†è·¨æ¨¡æ€çš„å®‰å…¨çº¦æŸä¸å‡è¡¡ï¼Œå¼•å‘äº†å¤šæ¨¡æ€å®‰å…¨ä¸å¯¹ç§°é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„é»‘ç›’è¶Šç‹±æ–¹æ³•PolyJailbreakã€‚è¯¥æ–¹æ³•é€šè¿‡æ¢æŸ¥æ¨¡å‹çš„æ³¨æ„åŠ›åŠ¨æ€å’Œæ½œåœ¨è¡¨ç¤ºç©ºé—´ï¼Œå°†æœ‰å®³æ„å›¾è½¬åŒ–ä¸ºè¶Šç‹±è¾“å…¥ï¼Œå®ç°å¯¹ç›®æ ‡æ¨¡å‹çš„è‡ªåŠ¨é€‚åº”ã€‚ç»è¿‡åœ¨å¤šç§å¼€æºå’Œé—­æºMLLMsä¸Šçš„ç»¼åˆè¯„ä¼°ï¼Œè¯æ˜PolyJailbreakä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ•ˆç”¨ï¼Œä½†å­˜åœ¨è¢«æ¶æ„è¾“å…¥ç ´åå®‰å…¨çº¦æŸå¹¶è§¦å‘éé“å¾·å›åº”çš„æ¼æ´ï¼Œå³â€œjailbreakâ€ç°è±¡ã€‚</li>
<li>è§†è§‰å¯¹é½åœ¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­å¼•å‘è·¨æ¨¡æ€å®‰å…¨çº¦æŸä¸å‡è¡¡ï¼Œå½¢æˆå¤šæ¨¡æ€å®‰å…¨ä¸å¯¹ç§°é—®é¢˜ã€‚</li>
<li>PolyJailbreakæ–¹æ³•é€šè¿‡æ¢æŸ¥æ¨¡å‹æ³¨æ„åŠ›åŠ¨æ€å’Œæ½œåœ¨è¡¨ç¤ºç©ºé—´æ¥è¯„ä¼°æ¨¡å‹è„†å¼±æ€§ï¼Œå°†æœ‰å®³æ„å›¾è½¬åŒ–ä¸ºè¶Šç‹±è¾“å…¥ã€‚</li>
<li>PolyJailbreaké‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„é»‘ç›’è¶Šç‹±ç­–ç•¥ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“ä¼˜åŒ–è¿‡ç¨‹è‡ªåŠ¨é€‚åº”ç›®æ ‡æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å…¨é¢è¯„ä¼°å¤šç§å¼€æºå’Œé—­æºMLLMsï¼Œè¯æ˜PolyJailbreakæ–¹æ³•ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¢å¼ºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc68b534af565fa38eb1499498eed7bb" align="middle">
<img src="https://picx.zhimg.com/v2-f4aebff1e30285e47305583a34f417e5" align="middle">
<img src="https://picx.zhimg.com/v2-842887c1b728bb0bc4efcbd7895e8650" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="From-Watch-to-Imagine-Steering-Long-horizon-Manipulation-via-Human-Demonstration-and-Future-Envisionment"><a href="#From-Watch-to-Imagine-Steering-Long-horizon-Manipulation-via-Human-Demonstration-and-Future-Envisionment" class="headerlink" title="From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment"></a>From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment</h2><p><strong>Authors:Ke Ye, Jiaming Zhou, Yuanfeng Qiu, Jiayi Liu, Shihui Zhou, Kun-Yu Lin, Junwei Liang</strong></p>
<p>Generalizing to long-horizon manipulation tasks in a zero-shot setting remains a central challenge in robotics. Current multimodal foundation based approaches, despite their capabilities, typically fail to decompose high-level commands into executable action sequences from static visual input alone. To address this challenge, we introduce Super-Mimic, a hierarchical framework that enables zero-shot robotic imitation by directly inferring procedural intent from unscripted human demonstration videos. Our framework is composed of two sequential modules. First, a Human Intent Translator (HIT) parses the input video using multimodal reasoning to produce a sequence of language-grounded subtasks. These subtasks then condition a Future Dynamics Predictor (FDP), which employs a generative model that synthesizes a physically plausible video rollout for each step. The resulting visual trajectories are dynamics-aware, explicitly modeling crucial object interactions and contact points to guide the low-level controller. We validate this approach through extensive experiments on a suite of long-horizon manipulation tasks, where Super-Mimic significantly outperforms state-of-the-art zero-shot methods by over 20%. These results establish that coupling video-driven intent parsing with prospective dynamics modeling is a highly effective strategy for developing general-purpose robotic systems.</p>
<blockquote>
<p>åœ¨é›¶æ ·æœ¬è®¾å®šä¸‹å°†æŠ€æœ¯æ¨å¹¿è‡³é•¿æœŸæ“ä½œä»»åŠ¡ä»ç„¶æ˜¯æœºå™¨äººæŠ€æœ¯é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚å°½ç®¡å½“å‰çš„å¤šæ¨¡æ€åŸºç¡€æ–¹æ³•å…·æœ‰æŸäº›èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸æ— æ³•ä»…ä»é™æ€è§†è§‰è¾“å…¥ä¸­åˆ†è§£é«˜çº§å‘½ä»¤ä¸ºå¯æ‰§è¡Œçš„è¡ŒåŠ¨åºåˆ—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Super-Mimicï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å±‚æ¡†æ¶ï¼Œå¯ä»¥é€šè¿‡ç›´æ¥ä»éè„šæœ¬çš„äººç±»æ¼”ç¤ºè§†é¢‘ä¸­æ¨æ–­ç¨‹åºæ„å›¾æ¥å®ç°é›¶æ ·æœ¬æœºå™¨äººæ¨¡ä»¿ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç”±ä¸¤ä¸ªé¡ºåºæ¨¡å—ç»„æˆã€‚é¦–å…ˆï¼Œäººç±»æ„å›¾ç¿»è¯‘å™¨ï¼ˆHITï¼‰ä½¿ç”¨å¤šæ¨¡æ€æ¨ç†è§£æè¾“å…¥è§†é¢‘ï¼Œä»¥äº§ç”Ÿä¸€ç³»åˆ—åŸºäºè¯­è¨€çš„å­ä»»åŠ¡ã€‚è¿™äº›å­ä»»åŠ¡ç„¶åä¸ºæœªæ¥åŠ¨æ€é¢„æµ‹å™¨ï¼ˆFDPï¼‰æä¾›æ¡ä»¶ï¼Œè¯¥é¢„æµ‹å™¨é‡‡ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œé’ˆå¯¹æ¯ä¸€æ­¥åˆæˆç‰©ç†ä¸Šåˆç†çš„è§†é¢‘æ»šåŠ¨ã€‚ç»“æœä¸­çš„è§†è§‰è½¨è¿¹æ˜¯åŠ¨æ€çš„ï¼Œå¯ä»¥æ˜ç¡®åœ°æ¨¡æ‹Ÿå…³é”®ç‰©ä½“ä¹‹é—´çš„äº¤äº’å’Œæ¥è§¦ç‚¹ï¼Œä»¥æŒ‡å¯¼ä½çº§æ§åˆ¶å™¨ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—é•¿æœŸæ“ä½œä»»åŠ¡çš„å¤§é‡å®éªŒéªŒè¯äº†è¿™ç§æ–¹æ³•ï¼ŒSuper-Mimicåœ¨é›¶æ ·æœ¬æ–¹æ³•ä¸­è¡¨ç°å‡ºè¶…è¿‡20%çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†è§†é¢‘é©±åŠ¨çš„æ„å›¾è§£æä¸å‰ç»æ€§åŠ¨æ€å»ºæ¨¡ç›¸ç»“åˆæ˜¯ä¸€ç§å¼€å‘é€šç”¨æœºå™¨äººç³»ç»Ÿçš„é«˜æ•ˆç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22205v2">PDF</a> More details and videos can be found at: <a target="_blank" rel="noopener" href="https://yipko.com/super-mimic">https://yipko.com/super-mimic</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSuper-Mimicçš„åˆ†å±‚æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç›´æ¥ä»éè„šæœ¬çš„äººç±»æ¼”ç¤ºè§†é¢‘ä¸­æ¨æ–­ç¨‹åºæ„å›¾ï¼Œè§£å†³äº†é›¶æ ·æœ¬æœºå™¨äººæ¨¡ä»¿ä¸­çš„é•¿æœŸæ“ä½œä»»åŠ¡æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼Œé¦–å…ˆé€šè¿‡äººç±»æ„å›¾ç¿»è¯‘å™¨è§£æè¾“å…¥è§†é¢‘å¹¶ç”ŸæˆåŸºäºè¯­è¨€çš„å­ä»»åŠ¡åºåˆ—ï¼Œç„¶åè¿™äº›å­ä»»åŠ¡æ¡ä»¶åŒ–æœªæ¥åŠ¨æ€é¢„æµ‹å™¨ï¼Œé‡‡ç”¨ç”Ÿæˆæ¨¡å‹å¯¹æ¯ä¸€æ­¥è¿›è¡Œç‰©ç†å¯è¡Œæ€§è§†é¢‘æ»šåŠ¨åˆæˆã€‚è¿™ä¸€æ–¹æ³•é€šè¿‡ä¸€ç³»åˆ—é•¿æœŸæ“ä½œä»»åŠ¡çš„å®éªŒéªŒè¯ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰é›¶æ ·æœ¬æ–¹æ³•ï¼Œè¡¨æ˜è§†é¢‘é©±åŠ¨çš„æ„å›¾è§£æä¸å‰ç»æ€§åŠ¨æ€å»ºæ¨¡çš„ç»“åˆæ˜¯å¼€å‘é€šç”¨æœºå™¨äººç³»ç»Ÿçš„é«˜æ•ˆç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ¨¡å¼åŸºç¡€æ–¹æ³•éš¾ä»¥ä»é™æ€è§†è§‰è¾“å…¥ä¸­åˆ†è§£é«˜çº§å‘½ä»¤ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œåºåˆ—ã€‚</li>
<li>Super-Mimicæ¡†æ¶é€šè¿‡ç›´æ¥è§£æéè„šæœ¬äººç±»æ¼”ç¤ºè§†é¢‘æ¥æ¨æ–­ç¨‹åºæ„å›¾ï¼Œè§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>Super-MimicåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šäººç±»æ„å›¾ç¿»è¯‘å™¨å’Œæœªæ¥åŠ¨æ€é¢„æµ‹å™¨ã€‚</li>
<li>äººç±»æ„å›¾ç¿»è¯‘å™¨ä»è¾“å…¥è§†é¢‘ä¸­è§£æå‡ºåŸºäºè¯­è¨€çš„å­ä»»åŠ¡åºåˆ—ã€‚</li>
<li>æœªæ¥åŠ¨æ€é¢„æµ‹å™¨é‡‡ç”¨ç”Ÿæˆæ¨¡å‹å¯¹æ¯ä¸€æ­¥è¿›è¡Œç‰©ç†å¯è¡Œæ€§è§†é¢‘æ»šåŠ¨åˆæˆã€‚</li>
<li>Super-Mimicåœ¨å¤šä¸ªé•¿æœŸæ“ä½œä»»åŠ¡çš„å®éªŒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰é›¶æ ·æœ¬æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39215c67dbf51a7634d5cd48ba38743e" align="middle">
<img src="https://picx.zhimg.com/v2-43ea75dffd454f525e770bed439ac96c" align="middle">
<img src="https://picx.zhimg.com/v2-18fac49cc6a68d023a15d214099b901d" align="middle">
<img src="https://picx.zhimg.com/v2-4ad5af72dda362116c78992c7ed5053b" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ScreenCoder-Advancing-Visual-to-Code-Generation-for-Front-End-Automation-via-Modular-Multimodal-Agents"><a href="#ScreenCoder-Advancing-Visual-to-Code-Generation-for-Front-End-Automation-via-Modular-Multimodal-Agents" class="headerlink" title="ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents"></a>ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents</h2><p><strong>Authors:Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael R. Lyu, Xiangyu Yue</strong></p>
<p>Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While multimodal large language models (MLLMs) can translate images to code, they often fail on complex UIs, struggling to unify visual perception, layout planning, and code synthesis within a single monolithic model, which leads to frequent perception and planning errors. To address this, we propose ScreenCoder, a modular multi-agent framework that decomposes the task into three interpretable stages: grounding, planning, and generation. By assigning these distinct responsibilities to specialized agents, our framework achieves significantly higher robustness and fidelity than end-to-end approaches. Furthermore, ScreenCoder serves as a scalable data engine, enabling us to generate high-quality image-code pairs. We use this data to fine-tune open-source MLLM via a dual-stage pipeline of supervised fine-tuning and reinforcement learning, demonstrating substantial gains in its UI generation capabilities. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at <a target="_blank" rel="noopener" href="https://github.com/leigest519/ScreenCoder">https://github.com/leigest519/ScreenCoder</a>.</p>
<blockquote>
<p>è‡ªåŠ¨åŒ–å°†ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰è®¾è®¡è½¬åŒ–ä¸ºå‰ç«¯ä»£ç ï¼Œå¯¹äºåŠ é€Ÿè½¯ä»¶å¼€å‘å’Œæ™®åŠè®¾è®¡å·¥ä½œæµç¨‹å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯ä»¥å°†å›¾åƒè½¬æ¢ä¸ºä»£ç ï¼Œä½†åœ¨å¤æ‚çš„ç”¨æˆ·ç•Œé¢ä¸Šå¸¸å¸¸ä¼šå‡ºç°å¤±è´¥ï¼Œå› ä¸ºå®ƒä»¬åœ¨ä¸€ä¸ªå•ä¸€çš„å•ä½“æ¨¡å‹ä¸­ç»Ÿä¸€è§†è§‰æ„ŸçŸ¥ã€å¸ƒå±€è§„åˆ’å’Œä»£ç åˆæˆæ—¶é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´é¢‘ç¹çš„æ„ŸçŸ¥å’Œè§„åˆ’é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ScreenCoderï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºä¸‰ä¸ªå¯è§£é‡Šçš„é˜¶æ®µï¼šæ¥åœ°ã€è§„åˆ’å’Œç”Ÿæˆã€‚é€šè¿‡å°†è¿™äº›ç‹¬ç‰¹çš„è´£ä»»åˆ†é…ç»™ä¸“ä¸šæ™ºèƒ½ä½“ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ¯”ç«¯åˆ°ç«¯çš„æ–¹æ³•å®ç°äº†æ›´é«˜çš„ç¨³å¥æ€§å’Œä¿çœŸåº¦ã€‚æ­¤å¤–ï¼ŒScreenCoderä½œä¸ºä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾ç‰‡-ä»£ç å¯¹ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›æ•°æ®é€šè¿‡ç›‘ç£ç²¾ç»†è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ çš„åŒé˜¶æ®µç®¡é“å¯¹å¼€æºMLLMè¿›è¡Œå¾®è°ƒï¼Œè¯æ˜äº†å…¶åœ¨UIç”Ÿæˆèƒ½åŠ›ä¸Šçš„æ˜¾è‘—æã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¸ƒå±€å‡†ç¡®æ€§ã€ç»“æ„è¿è´¯æ€§å’Œä»£ç æ­£ç¡®æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/leigest519/ScreenCoder%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/leigest519/ScreenCoderä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22827v2">PDF</a> ScreenCoder-v2</p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨åŒ–ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰è®¾è®¡è½¬åŒ–ä¸ºå‰ç«¯ä»£ç çš„è¿‡ç¨‹å…·æœ‰åŠ é€Ÿè½¯ä»¶å¼€å‘å’Œæ™®åŠè®¾è®¡å·¥ä½œæµç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰èƒ½å¤Ÿç¿»è¯‘å›¾åƒåˆ°ä»£ç ï¼Œä½†åœ¨å¤æ‚UIä¸Šå¸¸å¸¸å¤±è´¥ï¼Œéš¾ä»¥åœ¨ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ç»Ÿä¸€è§†è§‰æ„ŸçŸ¥ã€å¸ƒå±€è§„åˆ’å’Œä»£ç åˆæˆï¼Œå¯¼è‡´é¢‘ç¹å‡ºç°æ„ŸçŸ¥å’Œè§„åˆ’é”™è¯¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ScreenCoderæ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†ä»»åŠ¡åˆ†è§£æˆå¯è§£é‡Šçš„ä¸‰ä¸ªé˜¶ï¼šæ¥åœ°ã€è§„åˆ’å’Œç”Ÿæˆã€‚é€šè¿‡å°†è¿™äº›ç‹¬ç‰¹è´£ä»»åˆ†é…ç»™ä¸“ä¸šæ™ºèƒ½ä½“ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ¯”ç«¯åˆ°ç«¯æ–¹æ³•æ›´å…·é²æ£’æ€§å’Œä¿çœŸåº¦ã€‚æ­¤å¤–ï¼ŒScreenCoderè¿˜æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒ-ä»£ç å¯¹ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›æ•°æ®é€šè¿‡ç›‘ç£å¾®è°ƒçš„ä¸¤é˜¶æ®µç®¡é“å’Œå¼ºåŒ–å­¦ä¹ æ¥å¾®è°ƒå¼€æºMLLMï¼Œåœ¨UIç”Ÿæˆèƒ½åŠ›æ–¹é¢å–å¾—äº†å®è´¨æ€§è¿›å±•ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå…·æœ‰å¸ƒå±€å‡†ç¡®æ€§ã€ç»“æ„è¿è´¯æ€§å’Œä»£ç æ­£ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/leigest519/ScreenCoder%E3%80%82">https://github.com/leigest519/ScreenCoderã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–UIè®¾è®¡è½¬åŒ–ä¸ºå‰ç«¯ä»£ç æœ‰åŠ©äºåŠ é€Ÿè½¯ä»¶å¼€å‘å’Œæ™®åŠè®¾è®¡å·¥ä½œæµç¨‹ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤æ‚UIä¸Šå­˜åœ¨æ„ŸçŸ¥å’Œè§„åˆ’é”™è¯¯çš„é—®é¢˜ã€‚</li>
<li>ScreenCoderæ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶é€šè¿‡å°†ä»»åŠ¡åˆ†è§£ä¸ºä¸‰ä¸ªé˜¶æ®µæ¥æé«˜é²æ£’æ€§å’Œä¿çœŸåº¦ï¼šæ¥åœ°ã€è§„åˆ’å’Œç”Ÿæˆã€‚</li>
<li>ScreenCoderæ¡†æ¶ä½œä¸ºå¯æ‰©å±•çš„æ•°æ®å¼•æ“ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒ-ä»£ç å¯¹ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒçš„ä¸¤é˜¶æ®µç®¡é“å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨ScreenCoderç”Ÿæˆçš„æ•°æ®å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œæé«˜äº†UIç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ScreenCoderæ–¹æ³•å®ç°äº†å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬å¸ƒå±€å‡†ç¡®æ€§ã€ç»“æ„è¿è´¯æ€§å’Œä»£ç æ­£ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31e25a0e3e08b3bd35de829ca2b78f3c" align="middle">
<img src="https://picx.zhimg.com/v2-8620bf7e5c384a8ca7d34e64e5154b68" align="middle">
<img src="https://picx.zhimg.com/v2-d156435140e12e3b20f4a42ca023e1c4" align="middle">
<img src="https://picx.zhimg.com/v2-fe52ea0bfeb22c03277acf8d6ba367ec" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Doctor-Approved-Generating-Medically-Accurate-Skin-Disease-Images-through-AI-Expert-Feedback"><a href="#Doctor-Approved-Generating-Medically-Accurate-Skin-Disease-Images-through-AI-Expert-Feedback" class="headerlink" title="Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback"></a>Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback</h2><p><strong>Authors:Janet Wang, Yunbei Zhang, Zhengming Ding, Jihun Hamm</strong></p>
<p>Paucity of medical data severely limits the generalizability of diagnostic ML models, as the full spectrum of disease variability can not be represented by a small clinical dataset. To address this, diffusion models (DMs) have been considered as a promising avenue for synthetic image generation and augmentation. However, they frequently produce medically inaccurate images, deteriorating the model performance. Expert domain knowledge is critical for synthesizing images that correctly encode clinical information, especially when data is scarce and quality outweighs quantity. Existing approaches for incorporating human feedback, such as reinforcement learning (RL) and Direct Preference Optimization (DPO), rely on robust reward functions or demand labor-intensive expert evaluations. Recent progress in Multimodal Large Language Models (MLLMs) reveals their strong visual reasoning capabilities, making them adept candidates as evaluators. In this work, we propose a novel framework, coined MAGIC (Medically Accurate Generation of Images through AI-Expert Collaboration), that synthesizes clinically accurate skin disease images for data augmentation. Our method creatively translates expert-defined criteria into actionable feedback for image synthesis of DMs, significantly improving clinical accuracy while reducing the direct human workload. Experiments demonstrate that our method greatly improves the clinical quality of synthesized skin disease images, with outputs aligning with dermatologist assessments. Additionally, augmenting training data with these synthesized images improves diagnostic accuracy by +9.02% on a challenging 20-condition skin disease classification task, and by +13.89% in the few-shot setting.</p>
<blockquote>
<p>åŒ»ç–—æ•°æ®çš„åŒ®ä¹ä¸¥é‡é™åˆ¶äº†è¯Šæ–­æœºå™¨å­¦ä¹ æ¨¡å‹çš„é€šç”¨æ€§ï¼Œå› ä¸ºå°å‹çš„ä¸´åºŠæ•°æ®é›†æ— æ³•ä»£è¡¨ç–¾ç—…çš„å…¨éƒ¨å˜å¼‚è°±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²è¢«è§†ä¸ºåˆæˆå›¾åƒç”Ÿæˆå’Œå¢å¼ºçš„æœ‰å‰é€”çš„é€”å¾„ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸äº§ç”ŸåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒï¼Œä»è€Œé™ä½äº†æ¨¡å‹æ€§èƒ½ã€‚åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨è´¨é‡é«˜äºæ•°é‡çš„æƒ…å¢ƒä¸‹ï¼Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†å¯¹äºåˆæˆæ­£ç¡®ç¼–ç ä¸´åºŠä¿¡æ¯çš„å›¾åƒè‡³å…³é‡è¦ã€‚ç°æœ‰çš„ç»“åˆäººç±»åé¦ˆçš„æ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œä¾èµ–äºç¨³å¥çš„å¥–åŠ±åŠŸèƒ½æˆ–éœ€è¦å¤§é‡çš„äººå·¥è¯„ä¼°ã€‚æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºå‡ºäº†å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºè¯„ä¼°è€…çš„åˆé€‚å€™é€‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºMAGICï¼ˆé€šè¿‡AI-ä¸“å®¶åä½œè¿›è¡ŒåŒ»å­¦å‡†ç¡®çš„å›¾åƒç”Ÿæˆï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºåˆæˆç”¨äºæ•°æ®å¢å¼ºçš„ä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç—…å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ›é€ æ€§åœ°å°†ä¸“å®¶å®šä¹‰çš„æ ‡å‡†è½¬åŒ–ä¸ºå¯¹æ‰©æ•£æ¨¡å‹å›¾åƒåˆæˆçš„å¯æ“ä½œåé¦ˆï¼Œæ˜¾è‘—æé«˜äº†ä¸´åºŠå‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†ç›´æ¥äººåŠ›å·¥ä½œé‡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§æé«˜äº†åˆæˆçš®è‚¤ç—…å›¾åƒçš„ä¸´åºŠè´¨é‡ï¼Œè¾“å‡ºç»“æœä¸çš®è‚¤ç§‘åŒ»ç”Ÿçš„è¯„ä¼°ç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è¿™äº›åˆæˆå›¾åƒå¢å¼ºè®­ç»ƒæ•°æ®ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„20ç§çš®è‚¤ç—…åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯Šæ–­å‡†ç¡®ç‡æé«˜äº†9.02%ï¼Œåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹æé«˜äº†13.89%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12323v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºåŒ»ç–—æ•°æ®çš„åŒ®ä¹ä¸¥é‡é™åˆ¶äº†è¯Šæ–­æœºå™¨å­¦ä¹ æ¨¡å‹çš„é€šç”¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å°è¯•åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç”Ÿæˆå’Œå¢å¼ºï¼Œä½†å®ƒä»¬å¸¸äº§ç”ŸåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆäººå·¥æ™ºèƒ½ä¸ä¸“å®¶çŸ¥è¯†çš„æ–°å‹æ¡†æ¶MAGICï¼Œæ—¨åœ¨åˆæˆä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç–¾ç—…å›¾åƒç”¨äºæ•°æ®å¢å¼ºã€‚é€šè¿‡ä¸“å®¶å®šä¹‰çš„å‡†åˆ™è½¬åŒ–ä¸ºå¯¹å›¾åƒåˆæˆçš„å¯æ“ä½œåé¦ˆï¼Œæ˜¾è‘—æé«˜äº†ä¸´åºŠå‡†ç¡®æ€§å¹¶é™ä½äº†äººå·¥å·¥ä½œé‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å¤§å¤§æé«˜äº†åˆæˆçš®è‚¤ç–¾ç—…å›¾åƒçš„ä¸´åºŠè´¨é‡ï¼Œå¹¶ä¸çš®è‚¤ç§‘åŒ»ç”Ÿçš„è¯„ä¼°ç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è¿™äº›åˆæˆå›¾åƒå¢å¼ºè®­ç»ƒæ•°æ®ï¼Œåœ¨20ç§çš®è‚¤ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸­æé«˜äº†9.02%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹æé«˜äº†13.89%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—æ•°æ®çš„åŒ®ä¹é™åˆ¶äº†è¯Šæ–­æœºå™¨å­¦ä¹ æ¨¡å‹çš„é€šç”¨æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç”Ÿæˆå’Œå¢å¼ºä¸­æœ‰æ½œåŠ›ï¼Œä½†ä¼šäº§ç”ŸåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒã€‚</li>
<li>ä¸“å®¶çŸ¥è¯†å¯¹äºåˆæˆæ­£ç¡®ç¼–ç ä¸´åºŠä¿¡æ¯çš„å›¾åƒè‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚</li>
<li>ç°æœ‰èå…¥äººç±»åé¦ˆçš„æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œéœ€è¦ç¨³å¥çš„å¥–åŠ±å‡½æ•°æˆ–ç¹é‡çš„ä¸“å®¶è¯„ä¼°ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå¯ä½œä¸ºè¯„ä¼°è€…ã€‚</li>
<li>æå‡ºçš„MAGICæ¡†æ¶ç»“åˆäººå·¥æ™ºèƒ½ä¸ä¸“å®¶çŸ¥è¯†ï¼Œèƒ½åˆæˆä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç–¾ç—…å›¾åƒç”¨äºæ•°æ®å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c50aa8ed00c0f2d6f2b631ab5ced920" align="middle">
<img src="https://picx.zhimg.com/v2-4a3fb120011fcfafaa2851c5fb95f845" align="middle">
<img src="https://picx.zhimg.com/v2-2fe52b39bc4308923cd78ed128ec6d42" align="middle">
<img src="https://picx.zhimg.com/v2-89706c45d139275fd6d1e54199726085" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Caption-This-Reason-That-VLMs-Caught-in-the-Middle"><a href="#Caption-This-Reason-That-VLMs-Caught-in-the-Middle" class="headerlink" title="Caption This, Reason That: VLMs Caught in the Middle"></a>Caption This, Reason That: VLMs Caught in the Middle</h2><p><strong>Authors:Zihan Weng, Lucas Gomez, Taylor Whittington Webb, Pouya Bashivan</strong></p>
<p>Vision-Language Models (VLMs) have shown remarkable progress in visual understanding in recent years. Yet, they still lag behind human capabilities in specific visual tasks such as counting or relational reasoning. To understand the underlying limitations, we adopt methodologies from cognitive science, analyzing VLM performance along core cognitive axes: Perception, Attention, and Memory. Using a suite of tasks targeting these abilities, we evaluate state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct cognitive profiles: while advanced models approach ceiling performance on some tasks (e.g. category identification), a significant gap persists, particularly in tasks requiring spatial understanding or selective attention. Investigating the source of these failures and potential methods for improvement, we employ a vision-text decoupling analysis, finding that models struggling with direct visual reasoning show marked improvement when reasoning over their own generated text captions. These experiments reveal a strong need for improved VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed human performance. Furthermore, we demonstrate the potential of targeted fine-tuning on composite visual reasoning tasks and show that fine-tuning smaller VLMs substantially improves core cognitive abilities. While this improvement does not translate to large enhancements on challenging, out-of-distribution benchmarks, we show broadly that VLM performance on our datasets strongly correlates with performance on these other benchmarks. Our work provides a detailed analysis of VLM cognitive strengths and weaknesses and identifies key bottlenecks in simultaneous perception and reasoning while also providing an effective and simple solution.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿‘å¹´æ¥åœ¨è§†è§‰ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç‰¹å®šçš„è§†è§‰ä»»åŠ¡ï¼ˆå¦‚è®¡æ•°æˆ–å…³ç³»æ¨ç†ï¼‰æ–¹é¢ä»ç„¶è½åäºäººç±»çš„èƒ½åŠ›ã€‚ä¸ºäº†äº†è§£æ½œåœ¨çš„å±€é™æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨è®¤çŸ¥ç§‘å­¦çš„æ–¹æ³•ï¼Œåˆ†æVLMåœ¨æ ¸å¿ƒè®¤çŸ¥è½´ï¼ˆæ„ŸçŸ¥ã€æ³¨æ„åŠ›å’Œè®°å¿†ï¼‰ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡ä¸€ç³»åˆ—é’ˆå¯¹è¿™äº›èƒ½åŠ›çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„VLMsï¼ŒåŒ…æ‹¬GPT-4oã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸åŒçš„è®¤çŸ¥ç‰¹å¾ï¼šè™½ç„¶å…ˆè¿›æ¨¡å‹åœ¨æŸäº›ä»»åŠ¡ä¸Šæ¥è¿‘å¤©èŠ±æ¿æ€§èƒ½ï¼ˆä¾‹å¦‚ç±»åˆ«è¯†åˆ«ï¼‰ï¼Œä½†åœ¨éœ€è¦ç©ºé—´ç†è§£æˆ–é€‰æ‹©æ€§æ³¨æ„åŠ›çš„ä»»åŠ¡ä¸­ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚é€šè¿‡è°ƒæŸ¥å¤±è´¥çš„åŸå› å’Œæ½œåœ¨çš„æ”¹è¿›æ–¹æ³•ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è§†è§‰æ–‡æœ¬è§£è€¦åˆ†æï¼Œå‘ç°æ¨¡å‹åœ¨ç›´æ¥è§†è§‰æ¨ç†æ–¹é¢é‡åˆ°å›°éš¾æ—¶ï¼Œé€šè¿‡å¯¹ä»–ä»¬è‡ªå·±ç”Ÿæˆçš„æ–‡æœ¬æè¿°è¿›è¡Œæ¨ç†ä¼šæ˜¾è‘—æ”¹å–„ã€‚è¿™äº›å®éªŒæ­ç¤ºäº†å¯¹æ”¹è¿›VLMé“¾å¼æ€ç»´ï¼ˆCoTï¼‰èƒ½åŠ›çš„å¼ºçƒˆéœ€æ±‚ï¼Œå³ä½¿åœ¨é‚£äº›å§‹ç»ˆè¶…è¶Šäººç±»æ€§èƒ½çš„æ¨¡å‹ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é’ˆå¯¹å¤åˆè§†è§‰æ¨ç†ä»»åŠ¡è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒæ½œåŠ›ï¼Œå¹¶è¡¨æ˜å¯¹è¾ƒå°çš„VLMsè¿›è¡Œå¾®è°ƒä¼šæå¤§åœ°æé«˜å…¶æ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›ã€‚è™½ç„¶è¿™ç§æ”¹è¿›å¹¶ä¸è½¬åŒ–ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„åˆ†å¸ƒå¤–åŸºå‡†æµ‹è¯•çš„å¤§å¹…æå‡ï¼Œä½†æˆ‘ä»¬æ€»ä½“ä¸Šå‘ç°æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šçš„VLMæ€§èƒ½ä¸è¿™äº›å…¶ä»–åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œå¯¹VLMçš„è®¤çŸ¥ä¼˜åŠ¿å’ŒåŠ£åŠ¿è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œå¹¶ç¡®å®šäº†åœ¨åŒæ—¶æ„ŸçŸ¥å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„å…³é”®ç“¶é¢ˆï¼ŒåŒæ—¶æä¾›äº†ä¸€ç§æœ‰æ•ˆè€Œç®€å•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21538v2">PDF</a> Paper accepted by nips 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰ç†è§£æ–¹é¢çš„è¿›å±•ä¸å±€é™ã€‚è™½ç„¶VLMsåœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¦‚ç±»åˆ«è¯†åˆ«ï¼Œä½†åœ¨éœ€è¦ç©ºé—´ç†è§£æˆ–é€‰æ‹©æ€§æ³¨æ„çš„ä»»åŠ¡ä¸Šä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ç ”ç©¶é€šè¿‡è®¤çŸ¥ç§‘å­¦çš„æ–¹æ³•ï¼Œå¯¹VLMçš„è®¤çŸ¥èƒ½åŠ›è¿›è¡Œåˆ†æï¼Œå‘ç°æ¨¡å‹åœ¨è§†è§‰æ¨ç†æ–¹é¢å­˜åœ¨è–„å¼±ç¯èŠ‚ã€‚é€šè¿‡æ–‡æœ¬è§†è§‰è§£è€¦åˆ†æï¼Œå‘ç°æ¨¡å‹åœ¨åŸºäºè‡ªèº«ç”Ÿæˆçš„æ–‡æœ¬æè¿°è¿›è¡Œæ¨ç†æ—¶ï¼Œæ€§èƒ½æœ‰æ‰€æ”¹å–„ã€‚ç ”ç©¶è¿˜å¼ºè°ƒäº†VLMé“¾å¼æ€ç»´ï¼ˆCoTï¼‰èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†é’ˆå¯¹æ€§å¾®è°ƒå¯¹å¤åˆè§†è§‰æ¨ç†ä»»åŠ¡çš„æ½œåœ¨å½±å“ã€‚æœ¬æ–‡è¯¦ç»†åˆ†æäº†VLMçš„è®¤çŸ¥ä¼˜åŠ¿å’Œå¼±ç‚¹ï¼Œå¹¶æä¾›äº†æœ‰æ•ˆçš„ç®€å•è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨è§†è§‰ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç‰¹å®šè§†è§‰ä»»åŠ¡ä¸Šä»è½åäºäººç±»ï¼Œå¦‚è®¡æ•°å’Œå…³ç³»æ¨ç†ã€‚</li>
<li>é€šè¿‡è®¤çŸ¥ç§‘å­¦çš„æ–¹æ³•ï¼Œå‘ç°VLMåœ¨æ„ŸçŸ¥ã€æ³¨æ„åŠ›å’Œè®°å¿†ç­‰æ ¸å¿ƒè®¤çŸ¥é¢†åŸŸå­˜åœ¨å±€é™ã€‚</li>
<li>åœ¨éœ€è¦ç©ºé—´ç†è§£æˆ–é€‰æ‹©æ€§æ³¨æ„çš„ä»»åŠ¡ä¸Šï¼ŒVLMså­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>VLMsåœ¨è§†è§‰æ¨ç†æ–¹é¢å­˜åœ¨è–„å¼±ç¯èŠ‚ï¼Œä½†é€šè¿‡æ–‡æœ¬è§†è§‰è§£è€¦åˆ†æï¼Œå‘ç°æ¨¡å‹åœ¨åŸºäºè‡ªèº«ç”Ÿæˆçš„æ–‡æœ¬æè¿°è¿›è¡Œæ¨ç†æ—¶æ€§èƒ½æ”¹å–„ã€‚</li>
<li>VLMé“¾å¼æ€ç»´ï¼ˆCoTï¼‰èƒ½åŠ›å¯¹æ€§èƒ½æœ‰å½±å“ï¼Œå³ä½¿æ˜¯è¶…è¿‡äººç±»æ€§èƒ½è¡¨ç°çš„æ¨¡å‹ä¹Ÿéœ€è¦å¼ºåŒ–æ­¤èƒ½åŠ›ã€‚</li>
<li>é’ˆå¯¹æ€§å¾®è°ƒå¯¹å¤åˆè§†è§‰æ¨ç†ä»»åŠ¡æœ‰æ½œåœ¨å½±å“ï¼Œå¯ä»¥æ˜¾è‘—æé«˜VLMçš„æ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21538">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b4d8cee34fdba6a41746440414baf68" align="middle">
<img src="https://picx.zhimg.com/v2-2237f55220e226262314eaaf21f38276" align="middle">
<img src="https://picx.zhimg.com/v2-86c5677d4df168faf31e5f269f736af1" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MELLM-Exploring-LLM-Powered-Micro-Expression-Understanding-Enhanced-by-Subtle-Motion-Perception"><a href="#MELLM-Exploring-LLM-Powered-Micro-Expression-Understanding-Enhanced-by-Subtle-Motion-Perception" class="headerlink" title="MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception"></a>MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception</h2><p><strong>Authors:Sirui Zhao, Zhengye Zhang, Shifeng Liu, Xinglong Mao, Shukang Yin, Chaoyou Fu, Tong Xu, Enhong Chen</strong></p>
<p>Micro-expressions (MEs), brief and low-intensity facial movements revealing concealed emotions, are crucial for affective computing. Despite notable progress in ME recognition, existing methods are largely confined to discrete emotion classification, lacking the capacity for comprehensive ME Understanding (MEU), particularly in interpreting subtle facial dynamics and underlying emotional cues. While Multimodal Large Language Models (MLLMs) offer potential for MEU with their advanced reasoning abilities, they still struggle to perceive such subtle facial affective behaviors. To bridge this gap, we propose a ME Large Language Model (MELLM) that integrates optical flow-based sensitivity to subtle facial motions with the powerful inference ability of LLMs. Specifically, an iterative, warping-based optical-flow estimator, named MEFlowNet, is introduced to precisely capture facial micro-movements. For its training and evaluation, we construct MEFlowDataset, a large-scale optical-flow dataset with 54,611 onset-apex image pairs spanning diverse identities and subtle facial motions. Subsequently, we design a Flow-Guided Micro-Expression Understanding paradigm. Under this framework, the optical flow signals extracted by MEFlowNet are leveraged to build MEU-Instruct, an instruction-tuning dataset for MEU. MELLM is then fine-tuned on MEU-Instruct, enabling it to translate subtle motion patterns into human-readable descriptions and generate corresponding emotional inferences. Experiments demonstrate that MEFlowNet significantly outperforms existing optical flow methods in facial and ME-flow estimation, while MELLM achieves state-of-the-art accuracy and generalization across multiple ME benchmarks. To the best of our knowledge, this work presents two key contributions: MEFlowNet as the first dedicated ME flow estimator, and MELLM as the first LLM tailored for MEU.</p>
<blockquote>
<p>å¾®è¡¨æƒ…ï¼ˆMEsï¼‰æ˜¯çŸ­æš‚ä¸”ä½å¼ºåº¦çš„é¢éƒ¨è¿åŠ¨ï¼Œèƒ½å¤Ÿæ­ç¤ºéšè—çš„æƒ…æ„Ÿï¼Œå¯¹äºæƒ…æ„Ÿè®¡ç®—è‡³å…³é‡è¦ã€‚å°½ç®¡åœ¨å¾®è¡¨æƒ…è¯†åˆ«æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å¤§å¤šå±€é™äºç¦»æ•£æƒ…ç»ªåˆ†ç±»ï¼Œç¼ºä¹å…¨é¢çš„å¾®è¡¨æƒ…ç†è§£ï¼ˆMEUï¼‰èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£é‡Šå¾®å¦™çš„é¢éƒ¨åŠ¨æ€å’Œæ½œåœ¨çš„æƒ…æ„Ÿçº¿ç´¢æ–¹é¢ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‡­å€Ÿå…¶å…ˆè¿›çš„æ¨ç†èƒ½åŠ›åœ¨MEUæ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥æ„ŸçŸ¥åˆ°è¿™äº›å¾®å¦™çš„é¢éƒ¨æƒ…æ„Ÿè¡Œä¸ºã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¾®è¡¨æƒ…å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMELLMï¼‰ï¼Œå®ƒå°†åŸºäºå…‰æµçš„å¾®å¦™é¢éƒ¨è¿åŠ¨æ•æ„Ÿæ€§ä¸LLMsçš„å¼ºå¤§æ¨ç†èƒ½åŠ›ç›¸ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºè¿­ä»£ã€warpingçš„å…‰æµä¼°è®¡å™¨ï¼Œåä¸ºMEFlowNetï¼Œå¯ä»¥ç²¾ç¡®æ•æ‰é¢éƒ¨çš„å¾®è¡¨æƒ…è¿åŠ¨ã€‚ä¸ºäº†å¯¹å…¶è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†MEFlowDatasetï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å…‰æµæ•°æ®é›†ï¼ŒåŒ…å«54611ä¸ªèµ·å§‹-é¡¶ç‚¹å›¾åƒå¯¹ï¼Œæ¶µç›–äº†å¤šç§èº«ä»½å’Œå¾®å¦™çš„é¢éƒ¨è¿åŠ¨ã€‚éšåï¼Œæˆ‘ä»¬è®¾è®¡äº†Flow-Guided Micro-Expression UnderstandingèŒƒå¼ã€‚åœ¨æ­¤æ¡†æ¶ä¸‹ï¼Œåˆ©ç”¨MEFlowNetæå–çš„å…‰æµä¿¡å·æ„å»ºMEU-Instructï¼Œè¿™æ˜¯ç”¨äºå¾®è¡¨æƒ…ç†è§£çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨MEU-Instructä¸Šå¯¹MELLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿå°†å¾®å¦™çš„è¿åŠ¨æ¨¡å¼è½¬åŒ–ä¸ºäººç±»å¯è¯»çš„æè¿°å¹¶ç”Ÿæˆç›¸åº”çš„æƒ…æ„Ÿæ¨æ–­ã€‚å®éªŒè¡¨æ˜ï¼ŒMEFlowNetåœ¨é¢éƒ¨å’Œå¾®è¡¨æƒ…æµä¼°è®¡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰å…‰æµæ–¹æ³•ï¼Œè€ŒMELLMåœ¨å¤šä¸ªå¾®è¡¨æƒ…åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œæœ‰ä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼šä¸€æ˜¯é¦–åˆ›çš„å¾®è¡¨æƒ…æµä¼°è®¡å™¨MEFlowNetï¼ŒäºŒæ˜¯é’ˆå¯¹å¾®è¡¨æƒ…ç†è§£é‡èº«å®šåˆ¶çš„é¦–ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹MELLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07007v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢éƒ¨å¾®è¡¨æƒ…ï¼ˆMEsï¼‰åœ¨æƒ…æ„Ÿè®¡ç®—ä¸­çš„é‡è¦æ€§ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å±€é™äºç¦»æ•£æƒ…ç»ªåˆ†ç±»ï¼Œç¼ºä¹å¯¹é¢éƒ¨å¾®è¡¨æƒ…çš„ç»¼åˆç†è§£ï¼ˆMEUï¼‰ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªMEå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMELLMï¼‰ï¼Œç»“åˆå…‰å­¦æµåŠ¨å¯¹ç»†å¾®é¢éƒ¨è¿åŠ¨çš„æ•æ„Ÿæ€§ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯å¼•å…¥äº†åä¸ºMEFlowNetçš„åŸºäºå…‰æµçš„ä¼°è®¡å™¨æ¥ç²¾ç¡®æ•æ‰é¢éƒ¨å¾®è¿åŠ¨ï¼Œå¹¶æ„å»ºäº†MEFlowDatasetæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚æœ€åï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºå…‰å­¦æµåŠ¨çš„å¾®è¡¨æƒ…ç†è§£èŒƒå¼ï¼Œé€šè¿‡MEFlowNetæå–çš„å…‰æµä¿¡å·å»ºç«‹MEU-Instructæ•°æ®é›†ï¼Œå¯¹MELLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿè½¬åŒ–å¾®å¦™çš„è¿åŠ¨æ¨¡å¼ä¸ºäººç±»å¯è¯»çš„æè¿°å¹¶äº§ç”Ÿç›¸åº”çš„æƒ…æ„Ÿæ¨æ–­ã€‚è¯¥ç ”ç©¶ä¸ºé¢éƒ¨å¾®è¡¨æƒ…çš„ç†è§£æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨å¾®è¡¨æƒ…ï¼ˆMEsï¼‰åœ¨æƒ…æ„Ÿè®¡ç®—ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å±€é™äºç¦»æ•£æƒ…ç»ªåˆ†ç±»ï¼Œç¼ºä¹ç»¼åˆç†è§£ï¼ˆMEUï¼‰ã€‚</li>
<li>MEFlowNetè¢«å¼•å…¥ä½œä¸ºé¦–ä¸ªä¸“é—¨é’ˆå¯¹MEçš„å…‰æµä¼°è®¡å™¨ï¼Œèƒ½å¤Ÿç²¾ç¡®æ•æ‰é¢éƒ¨å¾®è¿åŠ¨ã€‚</li>
<li>æ„å»ºäº†å¤§å‹å…‰å­¦æµåŠ¨æ•°æ®é›†MEFlowDatasetï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°MEFlowNetã€‚</li>
<li>æå‡ºäº†åŸºäºå…‰å­¦æµåŠ¨çš„å¾®è¡¨æƒ…ç†è§£èŒƒå¼ï¼Œåˆ©ç”¨å…‰æµä¿¡å·å»ºç«‹MEU-Instructæ•°æ®é›†ã€‚</li>
<li>MELLMæ¨¡å‹è¢«è®¾è®¡ä¸ºé¦–ä¸ªé’ˆå¯¹MEUçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“åˆå…‰å­¦æµåŠ¨æ•æ„Ÿæ€§ä¸LLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MELLMç»è¿‡åœ¨MEU-Instructæ•°æ®é›†ä¸Šçš„å¾®è°ƒï¼Œèƒ½å¤Ÿè½¬åŒ–å¾®å¦™çš„è¿åŠ¨æ¨¡å¼ä¸ºäººç±»å¯è¯»çš„æè¿°ï¼Œå¹¶äº§ç”Ÿç›¸åº”çš„æƒ…æ„Ÿæ¨æ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c90903a7292b9eb41f1b40444b889bc9" align="middle">
<img src="https://picx.zhimg.com/v2-a81eef5bfe36b7ae3f6c8204b4d2c29f" align="middle">
<img src="https://picx.zhimg.com/v2-1e86f4ec6367a8a12bad4b5fa60fd107" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Nexus-An-Omni-Perceptive-And-Interactive-Model-for-Language-Audio-And-Vision"><a href="#Nexus-An-Omni-Perceptive-And-Interactive-Model-for-Language-Audio-And-Vision" class="headerlink" title="Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio, And Vision"></a>Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio, And Vision</h2><p><strong>Authors:Che Liu, Yingji Zhang, Dong Zhang, Weijie Zhang, Chenggong Gong, Yu Lu, Shilin Zhou, Ziliang Gan, Ziao Wang, Haipang Wu, Ji Liu, AndrÃ© Freitas, Qifan Wang, Zenglin Xu, Rongjuncheng Zhang, Yong Dai</strong></p>
<p>This work proposes an industry-level omni-modal large language model (LLM) pipeline that integrates auditory, visual, and linguistic modalities to overcome challenges such as limited tri-modal datasets, high computational costs, and complex feature alignments. Our pipeline consists of three main components: First, a modular framework enabling flexible configuration of various encoder-LLM-decoder architectures. Second, a lightweight training strategy that pre-trains audio-language alignment on the state-of-the-art vision-language model Qwen2.5-VL, thus avoiding the costly pre-training of vision-specific modalities. Third, an audio synthesis pipeline that generates high-quality audio-text data from diverse real-world scenarios, supporting applications such as Automatic Speech Recognition and Speech-to-Speech chat. To this end, we introduce an industry-level omni-modal LLM, Nexus. Extensive experiments validate the efficacy of our pipeline, yielding the following key findings:(1) In the visual understanding task, Nexus exhibits superior performance compared with its backbone model - Qwen2.5-VL-7B, validating the efficiency of our training strategy. (2) Within the English Spoken Question-Answering task, the model achieves better accuracy than the same-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In our real-world ASR testset, Nexus achieves outstanding performance, indicating its robustness in real scenarios. (4) In the Speech-to-Text Translation task, our model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task, based on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), Nexus is comparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth analysis of tri-modal alignment reveals that incorporating the audio modality enhances representational alignment between vision and language.</p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨è¡Œä¸šçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç®¡é“ï¼Œè¯¥ç®¡é“èåˆäº†å¬è§‰ã€è§†è§‰å’Œè¯­è¨€å­¦æ¨¡å¼ï¼Œä»¥å…‹æœå¦‚æœ‰é™çš„ä¸‰æ¨¡æ€æ•°æ®é›†ã€é«˜è®¡ç®—æˆæœ¬å’Œå¤æ‚çš„ç‰¹å¾å¯¹é½ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç®¡é“ä¸»è¦ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šé¦–å…ˆï¼Œä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿçµæ´»é…ç½®å„ç§ç¼–ç å™¨-LLM-è§£ç å™¨æ¶æ„ã€‚å…¶æ¬¡ï¼Œä¸€ç§è½»é‡çº§çš„è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å¯¹æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VLè¿›è¡Œé¢„è®­ç»ƒï¼Œå®ç°éŸ³é¢‘è¯­è¨€å¯¹é½ï¼Œä»è€Œé¿å…äº†é’ˆå¯¹ç‰¹å®šè§†è§‰æ¨¡å¼çš„æ˜‚è´µé¢„è®­ç»ƒã€‚æœ€åï¼Œä¸€ä¸ªéŸ³é¢‘åˆæˆç®¡é“ï¼Œå…¶ä»å„ç§çœŸå®åœºæ™¯ä¸­ç”Ÿæˆé«˜è´¨é‡éŸ³é¢‘æ–‡æœ¬æ•°æ®ï¼Œæ”¯æŒå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆ°è¯­éŸ³èŠå¤©ç­‰åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†è·¨è¡Œä¸šçš„å¤šæ¨¡æ€LLMâ€”â€”Nexusã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œå¾—å‡ºä»¥ä¸‹å…³é”®å‘ç°ï¼šï¼ˆ1ï¼‰åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­ï¼ŒNexusç›¸è¾ƒäºå…¶åŸºç¡€æ¨¡å‹Qwen2.5-VL-7Bå±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†æˆ‘ä»¬è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰åœ¨è‹±è¯­å£è¯­é—®ç­”ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹çš„å‡†ç¡®æ€§é«˜äºåŒæœŸç«äº‰å¯¹æ‰‹ï¼ˆå³LLaMA Q. benchmarkä¸­çš„MiniCPM-o2.6-7Bï¼‰ã€‚ï¼ˆ3ï¼‰åœ¨æˆ‘ä»¬çš„çœŸå®ASRæµ‹è¯•é›†ä¸­ï¼ŒNexusè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¡¨æ˜å…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚ï¼ˆ4ï¼‰åœ¨è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºQwen2-Audio-Instruct-7Bã€‚ï¼ˆ5ï¼‰åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ä»»åŠ¡ä¸­ï¼ŒåŸºäºé¢„è®­ç»ƒçš„vocoderï¼ˆä¾‹å¦‚Fishspeech1.4æˆ–CosyVoice2.0ï¼‰ï¼ŒNexusåœ¨Seed-TTS benchmarkä¸Šçš„è¡¨ç°ä¸å…¶åŸºç¡€vocoderç›¸å½“ã€‚ï¼ˆ6ï¼‰å¯¹ä¸‰æ¨¡æ€å¯¹é½çš„æ·±å…¥åˆ†æè¡¨æ˜ï¼Œèå…¥éŸ³é¢‘æ¨¡å¼å¢å¼ºäº†è§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„ä»£è¡¨æ€§å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01879v4">PDF</a> Project: <a target="_blank" rel="noopener" href="https://github.com/HiThink-Research/NEXUS-O">https://github.com/HiThink-Research/NEXUS-O</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨è¡Œä¸šçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç®¡é“ï¼Œè¯¥ç®¡é“èåˆäº†å¬è§‰ã€è§†è§‰å’Œè¯­è¨€å­¦ä¸‰å¤§æ¨¡æ€ï¼Œä»¥å…‹æœå¦‚æœ‰é™çš„ä¸‰æ¨¡æ€æ•°æ®é›†ã€é«˜æ˜‚çš„è®¡ç®—æˆæœ¬å’Œå¤æ‚çš„ç‰¹å¾å¯¹é½ç­‰æŒ‘æˆ˜ã€‚è¯¥ç®¡é“åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šä¸€æ˜¯æ¨¡å—åŒ–æ¡†æ¶ï¼Œå¯ä»¥çµæ´»é…ç½®å„ç§ç¼–ç å™¨-LLM-è§£ç å™¨æ¶æ„ï¼›äºŒæ˜¯è½»é‡çº§è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VLè¿›è¡ŒéŸ³é¢‘è¯­è¨€å¯¹é½çš„é¢„è®­ç»ƒï¼›ä¸‰æ˜¯éŸ³é¢‘åˆæˆç®¡é“ï¼Œå¯ä»å„ç§çœŸå®åœºæ™¯ç”Ÿæˆé«˜è´¨é‡çš„éŸ³é¢‘æ–‡æœ¬æ•°æ®ï¼Œæ”¯æŒå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆ°è¯­éŸ³èŠå¤©ç­‰åº”ç”¨ã€‚é€šè¿‡å¼•å…¥è¡Œä¸šçº§åˆ«çš„å¤šæ¨¡æ€LLM Nexusï¼Œå®éªŒè¯æ˜è¯¥ç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å®ç°äº†åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç®¡é“ï¼Œé›†æˆäº†å¬è§‰ã€è§†è§‰å’Œè¯­è¨€å­¦ä¸‰å¤§æ¨¡æ€ã€‚</li>
<li>ç®¡é“åŒ…æ‹¬æ¨¡å—åŒ–æ¡†æ¶ã€è½»é‡çº§è®­ç»ƒç­–ç•¥å’ŒéŸ³é¢‘åˆæˆç®¡é“ä¸‰ä¸ªä¸»è¦ç»„ä»¶ã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡ŒéŸ³é¢‘è¯­è¨€å¯¹é½çš„é¢„è®­ç»ƒï¼Œé¿å…äº†é’ˆå¯¹ç‰¹å®šè§†è§‰æ¨¡æ€çš„æ˜‚è´µé¢„è®­ç»ƒã€‚</li>
<li>åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­ï¼ŒNexusç›¸è¾ƒäºå…¶åŸºç¡€æ¨¡å‹Qwen2.5-VL-7Bè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨è‹±è¯­å£è¯­é—®ç­”ä»»åŠ¡ä¸­ï¼ŒNexusçš„å‡†ç¡®ç‡è¶…è¿‡äº†åŒæœŸç«äº‰å¯¹æ‰‹ã€‚</li>
<li>åœ¨çœŸå®åœºæ™¯çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æµ‹è¯•é›†ä¸­ï¼ŒNexusè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78125e7a9aa3a72152b27a5ac462bba8" align="middle">
<img src="https://picx.zhimg.com/v2-88f1d29a330302a2f6c76f3fc5c0107f" align="middle">
<img src="https://picx.zhimg.com/v2-6acec14c2780b4d7beced11d34f417e8" align="middle">
<img src="https://picx.zhimg.com/v2-7476f59f20929343b9491403e165d160" align="middle">
<img src="https://picx.zhimg.com/v2-6da19ab65f5bc992fa31c8fb735286ba" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Physics-Context-Builders-A-Modular-Framework-for-Physical-Reasoning-in-Vision-Language-Models"><a href="#Physics-Context-Builders-A-Modular-Framework-for-Physical-Reasoning-in-Vision-Language-Models" class="headerlink" title="Physics Context Builders: A Modular Framework for Physical Reasoning in Vision-Language Models"></a>Physics Context Builders: A Modular Framework for Physical Reasoning in Vision-Language Models</h2><p><strong>Authors:Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, Rahul G. Krishnan</strong></p>
<p>Physical reasoning remains a significant challenge for Vision-Language Models (VLMs). This limitation arises from an inability to translate learned knowledge into predictions about physical behavior. Although continual fine-tuning can mitigate this issue, it is expensive for large models and impractical to perform repeatedly for every task. This necessitates the creation of modular and scalable ways to teach VLMs about physical reasoning. To that end, we introduce Physics Context Builders (PCBs), a modular framework where specialized smaller VLMs are fine-tuned to generate detailed physical scene descriptions. These can be used as physical contexts to enhance the reasoning capabilities of larger VLMs. PCBs enable the separation of visual perception from reasoning, allowing us to analyze their relative contributions to physical understanding. We perform experiments on CLEVRER and on Falling Tower, a stability detection dataset with both simulated and real-world scenes, to demonstrate that PCBs provide substantial performance improvements, increasing average accuracy by up to 13.8% on complex physical reasoning tasks. Notably, PCBs also show strong Sim2Real transfer, successfully generalizing from simulated training data to real-world scenes.</p>
<blockquote>
<p>ç‰©ç†æ¨ç†å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è¿™ä¸€é™åˆ¶æºäºæ— æ³•å°†å­¦åˆ°çš„çŸ¥è¯†è½¬åŒ–ä¸ºå¯¹ç‰©ç†è¡Œä¸ºçš„é¢„æµ‹ã€‚è™½ç„¶æŒç»­å¾®è°ƒå¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½†å¯¹äºå¤§å‹æ¨¡å‹æ¥è¯´æˆæœ¬å¾ˆé«˜ï¼Œå¹¶ä¸”å¯¹äºæ¯ä¸ªä»»åŠ¡éƒ½é‡å¤è¿›è¡Œå¾®è°ƒå¹¶ä¸å®é™…ã€‚å› æ­¤ï¼Œå¿…é¡»åˆ›å»ºæ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„æ–¹æ³•æ¥æ•™æˆVLMsè¿›è¡Œç‰©ç†æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‰©ç†ä¸Šä¸‹æ–‡æ„å»ºå™¨ï¼ˆPCBsï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå…¶ä¸­ä¸“é—¨çš„å°å‹VLMsç»è¿‡å¾®è°ƒä»¥ç”Ÿæˆè¯¦ç»†çš„ç‰©ç†åœºæ™¯æè¿°ã€‚è¿™äº›æè¿°å¯ä»¥ç”¨ä½œç‰©ç†ä¸Šä¸‹æ–‡ï¼Œä»¥å¢å¼ºå¤§å‹VLMsçš„æ¨ç†èƒ½åŠ›ã€‚PCBsä½¿è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†åˆ†ç¦»ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ†æå®ƒä»¬å¯¹ç‰©ç†ç†è§£çš„ç›¸å¯¹è´¡çŒ®ã€‚æˆ‘ä»¬åœ¨CLEVRERå’ŒFalling Towerï¼ˆä¸€ä¸ªåŒ…å«æ¨¡æ‹Ÿå’ŒçœŸå®åœºæ™¯çš„ç¨³å®šæ£€æµ‹æ•°æ®é›†ï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œä»¥è¯æ˜PCBsæä¾›äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œåœ¨å¤æ‚çš„ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šå¹³å‡å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾13.8%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPCBsè¿˜æ˜¾ç¤ºå‡ºå¼ºå¤§çš„Sim2Realè¿ç§»èƒ½åŠ›ï¼ŒæˆåŠŸåœ°ä»æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®æ¨å¹¿åˆ°çœŸå®åœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08619v3">PDF</a> </p>
<p><strong>Summary</strong><br>ç‰©ç†æ¨ç†ä»ç„¶æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢ä¸´çš„ä¸€å¤§æŒ‘æˆ˜ã€‚è¯¥æŒ‘æˆ˜æºäºæ— æ³•å°†å­¦åˆ°çš„çŸ¥è¯†è½¬åŒ–ä¸ºå¯¹ç‰©ç†è¡Œä¸ºçš„é¢„æµ‹ã€‚è™½ç„¶æŒç»­å¾®è°ƒå¯ä»¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å¯¹äºå¤§å‹æ¨¡å‹è€Œè¨€æˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥é’ˆå¯¹æ¯ä¸ªä»»åŠ¡é‡å¤æ‰§è¡Œã€‚å› æ­¤ï¼Œéœ€è¦åˆ›å»ºæ¨¡å—åŒ–å’Œå¯æ‰©å±•åŒ–çš„æ–¹æ³•æ¥æ•™æˆVLMsç‰©ç†æ¨ç†ã€‚æˆ‘ä»¬ä¸ºæ­¤å¼•å…¥äº†ç‰©ç†åœºæ™¯æ„å»ºå™¨ï¼ˆPCBsï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå…¶ä¸­ä¸“é—¨çš„å°å‹VLMsç»è¿‡å¾®è°ƒä»¥ç”Ÿæˆè¯¦ç»†çš„ç‰©ç†åœºæ™¯æè¿°ã€‚è¿™äº›æè¿°å¯ä»¥ç”¨ä½œç‰©ç†ä¸Šä¸‹æ–‡ï¼Œä»¥å¢å¼ºå¤§å‹VLMsçš„æ¨ç†èƒ½åŠ›ã€‚PCBså®ç°äº†è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†çš„åˆ†ç¦»ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ†æå®ƒä»¬åœ¨ç†è§£ç‰©ç†æ–¹é¢çš„ç›¸å¯¹è´¡çŒ®ã€‚æˆ‘ä»¬åœ¨CLEVRERå’ŒFalling Towerï¼ˆä¸€ä¸ªåŒ…å«æ¨¡æ‹Ÿå’ŒçœŸå®åœºæ™¯çš„ç¨³å®šæ€§æ£€æµ‹æ•°æ®é›†ï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯æ˜PCBsåœ¨å¤æ‚çš„ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šæä¾›äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾13.8%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPCBsè¿˜è¡¨ç°å‡ºå¼ºå¤§çš„Sim2Realè¿ç§»èƒ½åŠ›ï¼ŒæˆåŠŸåœ°ä»æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®æ¨å¹¿åˆ°çœŸå®åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©ç†æ¨ç†æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é‡è¦æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éš¾ä»¥å°†å­¦åˆ°çš„çŸ¥è¯†è½¬åŒ–ä¸ºç‰©ç†è¡Œä¸ºçš„é¢„æµ‹ã€‚</li>
<li>æŒç»­å¾®è°ƒè™½ç„¶å¯ä»¥æ”¹å–„è¿™ä¸€é—®é¢˜ï¼Œä½†å¯¹å¤§å‹æ¨¡å‹è€Œè¨€æˆæœ¬é«˜æ˜‚ï¼Œä¸”éš¾ä»¥é‡å¤æ‰§è¡Œã€‚</li>
<li>å¼•å…¥ç‰©ç†åœºæ™¯æ„å»ºå™¨ï¼ˆPCBsï¼‰è¿™ä¸€æ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä¸“é—¨çš„å°å‹VLMsç”Ÿæˆè¯¦ç»†çš„ç‰©ç†åœºæ™¯æè¿°ï¼Œå¢å¼ºå¤§å‹VLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>PCBså®ç°äº†è§†è§‰æ„ŸçŸ¥ä¸æ¨ç†çš„åˆ†ç¦»ï¼Œä¾¿äºåˆ†æä¸¤è€…çš„ç›¸å¯¹è´¡çŒ®ã€‚</li>
<li>åœ¨CLEVRERå’ŒFalling Toweræ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPCBsåœ¨ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡æå‡é«˜è¾¾13.8%ã€‚</li>
<li>PCBså…·æœ‰å¼ºå¤§çš„Sim2Realè¿ç§»èƒ½åŠ›ï¼Œèƒ½åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®åœºæ™¯ä¹‹é—´å®ç°æœ‰æ•ˆçš„æ¨å¹¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3b276bafa34a26d5cbf63777e21a667" align="middle">
<img src="https://picx.zhimg.com/v2-afc3c52bd2a7138abfcd2677e07c8d08" align="middle">
<img src="https://picx.zhimg.com/v2-69b7fb74f9504866b7d23abdd2c7af35" align="middle">
<img src="https://picx.zhimg.com/v2-a7929e16d4247ec39b270b3a90723275" align="middle">
<img src="https://picx.zhimg.com/v2-c0a15e125e1b5f712ddb7f9f3fa04009" align="middle">
<img src="https://picx.zhimg.com/v2-099760a2400b374d65bcdb07f75404f0" align="middle">
<img src="https://picx.zhimg.com/v2-1146ec95e9c5bc49666462d01197c05c" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/MMT/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/MMT/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/MMT/">
                                    <span class="chip bg-color">MMT</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f94fc05d96989a2c6ff57ef1daa9a909" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Bi-Level Contextual Bandits for Individualized Resource Allocation under Delayed Feedback
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1f6452c3b68d6ea66c075c46913538e2" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Explaining Decentralized Multi-Agent Reinforcement Learning Policies
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
