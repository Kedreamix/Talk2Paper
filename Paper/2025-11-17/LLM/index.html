<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a3763cb20c89e115fad9ba7bdd947f74')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-17-æ›´æ–°"><a href="#2025-11-17-æ›´æ–°" class="headerlink" title="2025-11-17 æ›´æ–°"></a>2025-11-17 æ›´æ–°</h1><h2 id="Prompt-Tuning-for-Natural-Language-to-SQL-with-Embedding-Fine-Tuning-and-RAG"><a href="#Prompt-Tuning-for-Natural-Language-to-SQL-with-Embedding-Fine-Tuning-and-RAG" class="headerlink" title="Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG"></a>Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG</h2><p><strong>Authors:Jisoo Jang, Tien-Cuong Bui, Yunjun Choi, Wen-Syan Li</strong></p>
<p>This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.</p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹NL-to-SQLçš„é”™è¯¯æ ¡æ­£æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨åŸºäºç”Ÿæˆé¢„è®­ç»ƒçš„æœ€æ–°è¿›å±•LLMså’ŒRAGæŠ€æœ¯ã€‚æˆ‘ä»¬çš„å·¥ä½œæ»¡è¶³äº†åœ¨æ—¥ç›Šå¢é•¿çš„è‡ªç„¶è¯­è¨€æ¥å£ä½¿ç”¨èƒŒæ™¯ä¸‹ï¼Œåœ¨å„ç§ç¯å¢ƒä¸­é«˜æ•ˆå‡†ç¡®åœ°å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘æˆSQLè¡¨è¾¾å¼çš„é‡è¦éœ€æ±‚ã€‚æˆ‘ä»¬æ¢è®¨äº†NLIDBsä»æ—©æœŸçš„åŸºäºè§„åˆ™çš„ç³»ç»Ÿåˆ°å…ˆè¿›çš„ç¥ç»ç½‘ç»œé©±åŠ¨æ–¹æ³•çš„æ¼”å˜ã€‚ä»åŒ»å­¦è¯Šæ–­è¿‡ç¨‹ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†ä¸€ç§é”™è¯¯æ ¡æ­£æœºåˆ¶ï¼Œèƒ½å¤Ÿè¯Šæ–­é”™è¯¯ç±»å‹ã€è¯†åˆ«é”™è¯¯åŸå› ã€æä¾›ä¿®å¤æŒ‡ä»¤ï¼Œå¹¶å°†è¿™äº›ä¿®æ­£åº”ç”¨äºSQLæŸ¥è¯¢ã€‚æ­¤æ–¹æ³•é€šè¿‡å¾®è°ƒåµŒå…¥å’ŒRAGè¿›ä¸€æ­¥ä¸°å¯Œï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“æé«˜å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚é€šè¿‡ç»¼åˆå®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ¡†æ¶ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ç›¸æ¯”å®ç°äº†é«˜è¾¾12%çš„å‡†ç¡®ç‡æå‡ï¼Œå‡¸æ˜¾å…¶åœ¨å½“ä»£æ•°æ®é©±åŠ¨ç¯å¢ƒä¸­æ”¹å˜æ•°æ®è®¿é—®å’Œå¤„ç†æ–¹å¼çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08245v1">PDF</a> Presented at the Workshop on Robust ML in Open Environments (PAKDD 2024)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ä¸æ£€ç´¢å¢å¼ºçš„ç”Ÿæˆå¼æ¨¡å‹RAGçš„é”™è¯¯æç¤ºè°ƒä¼˜åœ¨NL-to-SQLè½¬åŒ–æ–¹é¢çš„åº”ç”¨ã€‚è¯¥ç ”ç©¶æ»¡è¶³äº†æ—¥ç›Šå¢é•¿çš„è‡ªç„¶è¯­è¨€ç•Œé¢ä¸‹å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢é«˜æ•ˆå‡†ç¡®åœ°è½¬åŒ–ä¸ºSQLè¡¨è¾¾å¼çš„è¦æ±‚ã€‚æ–‡ç« ä»æ—©æœŸçš„è§„åˆ™åŸºç¡€ç³»ç»Ÿæ¢è®¨äº†NLIDBsçš„æ¼”å˜ï¼Œæå‡ºä¸€ä¸ªèåˆé”™è¯¯ä¿®æ­£æœºåˆ¶çš„å…¨æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿè¯Šæ–­é”™è¯¯ç±»å‹ã€è¯†åˆ«é”™è¯¯åŸå› å¹¶æä¾›ä¿®æ­£æŒ‡ä»¤ã€‚ç»“åˆå¾®è°ƒä¸RAGæŠ€æœ¯ï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“æé«˜å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ç›¸è¾ƒäºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå‡†ç¡®æ€§æé«˜äº†æ˜¾è‘—çš„12%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å½“ä»£æ•°æ®é©±åŠ¨ç¯å¢ƒä¸­é©æ–°æ•°æ®å¤„ç†å’Œè®¿é—®çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ä¸RAGçš„é”™è¯¯æç¤ºè°ƒä¼˜æ–¹æ³•ï¼Œé’ˆå¯¹NL-to-SQLè½¬åŒ–é—®é¢˜ã€‚</li>
<li>æ­¤æ–¹æ³•æ»¡è¶³äº†åœ¨å¤šç§ç¯å¢ƒä¸­å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºSQLè¡¨è¾¾å¼çš„éœ€æ±‚å’ŒæŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†ä»æ—©æœŸè§„åˆ™åŸºç¡€ç³»ç»Ÿåˆ°ç°ä»£ç¥ç»ç½‘ç»œé©±åŠ¨æ–¹æ³•çš„NLIDBsæ¼”å˜è¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œèåˆäº†é”™è¯¯ä¿®æ­£æœºåˆ¶ï¼Œå…·å¤‡è¯Šæ–­é”™è¯¯ç±»å‹ã€è¯†åˆ«é”™è¯¯åŸå› å’Œæä¾›ä¿®æ­£æŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†å¾®è°ƒæŠ€æœ¯ä¸RAGï¼Œåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“å¢å¼ºäº†å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ–°æ¡†æ¶ç›¸æ¯”ç°æœ‰åŸºçº¿æ–¹æ³•æé«˜äº†æ˜¾è‘—çš„12%çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ad4bf5e4d8b80e1d70c6a218ebe2931" align="middle">
<img src="https://picx.zhimg.com/v2-7cc75dd28c741020330fe448a8f89221" align="middle">
<img src="https://picx.zhimg.com/v2-db959783e5c142f2634eaad960d45782" align="middle">
<img src="https://picx.zhimg.com/v2-80e9e60b1365eae2bf485f9b70f19499" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Encoder-Fine-tuning-with-Stochastic-Sampling-Outperforms-Open-weight-GPT-in-Astronomy-Knowledge-Extraction"><a href="#Encoder-Fine-tuning-with-Stochastic-Sampling-Outperforms-Open-weight-GPT-in-Astronomy-Knowledge-Extraction" class="headerlink" title="Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction"></a>Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction</h2><p><strong>Authors:Shivam Rawat, Lucie Flek, Akbar Karimi</strong></p>
<p>Scientific literature in astronomy is rapidly expanding, making it increasingly important to automate the extraction of key entities and contextual information from research papers. In this paper, we present an encoder-based system for extracting knowledge from astronomy articles. Our objective is to develop models capable of classifying telescope references, detecting auxiliary semantic attributes, and recognizing instrument mentions from textual content. To this end, we implement a multi-task transformer-based system built upon the SciBERT model and fine-tuned for astronomy corpora classification. To carry out the fine-tuning, we stochastically sample segments from the training data and use majority voting over the test segments at inference time. Our system, despite its simplicity and low-cost implementation, significantly outperforms the open-weight GPT baseline.</p>
<blockquote>
<p>å¤©æ–‡å­¦ç§‘å­¦æ–‡çŒ®æ­£åœ¨è¿…é€Ÿæ‰©å¼ ï¼Œè‡ªåŠ¨ä»ç ”ç©¶è®ºæ–‡ä¸­æå–å…³é”®å®ä½“å’Œä¸Šä¸‹æ–‡ä¿¡æ¯å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¼–ç å™¨ä»å¤©æ–‡æ–‡ç« æå–çŸ¥è¯†çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘èƒ½å¤Ÿåˆ†ç±»æœ›è¿œé•œå¼•ç”¨ã€æ£€æµ‹è¾…åŠ©è¯­ä¹‰å±æ€§å’Œè¯†åˆ«ä»ªå™¨æåŠçš„æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåŸºäºå¤šä»»åŠ¡è½¬æ¢å™¨çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå»ºç«‹åœ¨SciBERTæ¨¡å‹ä¹‹ä¸Šï¼Œç»è¿‡å¾®è°ƒç”¨äºå¤©æ–‡å­¦è¯­æ–™åº“åˆ†ç±»ã€‚ä¸ºäº†è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬ä»è®­ç»ƒæ•°æ®ä¸­éšæœºé‡‡æ ·ç‰‡æ®µï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå¯¹æµ‹è¯•ç‰‡æ®µè¿›è¡Œå¤šæ•°æŠ•ç¥¨ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè™½ç„¶ç®€å•ä¸”æˆæœ¬ä½å»‰ï¼Œä½†æ˜¾è‘—ä¼˜äºå¼€æ”¾æƒé‡GPTåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08204v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€å¤©æ–‡å­¦ç§‘å­¦æ–‡çŒ®çš„è¿…é€Ÿå¢é•¿ï¼Œè‡ªåŠ¨åŒ–æå–å…³é”®å®ä½“å’Œä¸Šä¸‹æ–‡ä¿¡æ¯å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç¼–ç å™¨çš„å¤©æ–‡å­¦æ–‡ç« çŸ¥è¯†æå–ç³»ç»Ÿï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿåˆ†ç±»æœ›è¿œé•œå‚è€ƒæ–‡çŒ®ã€æ£€æµ‹è¾…åŠ©è¯­ä¹‰å±æ€§å’Œè¯†åˆ«ä»ªå™¨æåŠçš„æ¨¡å‹ã€‚é€šè¿‡åŸºäºSciBERTæ¨¡å‹çš„å¤šä»»åŠ¡è½¬æ¢å™¨ç³»ç»Ÿè¿›è¡Œå®ç°ï¼Œå¹¶é’ˆå¯¹å¤©æ–‡å­¦è¯­æ–™åº“åˆ†ç±»è¿›è¡Œå¾®è°ƒã€‚è¯¥ç³»ç»Ÿåœ¨ä½æˆæœ¬å®ç°çš„æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºå¼€æ”¾æƒé‡GPTåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤©æ–‡å­¦ç§‘å­¦æ–‡çŒ®çš„å¿«é€Ÿå¢é•¿ä½¿å¾—è‡ªåŠ¨åŒ–æå–å…³é”®å®ä½“å’Œä¸Šä¸‹æ–‡ä¿¡æ¯å˜å¾—é‡è¦ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç¼–ç å™¨çš„å¤©æ–‡å­¦æ–‡ç« çŸ¥è¯†æå–ç³»ç»Ÿã€‚</li>
<li>ç³»ç»Ÿçš„ç›®æ ‡æ˜¯å¼€å‘èƒ½å¤Ÿåˆ†ç±»æœ›è¿œé•œå‚è€ƒæ–‡çŒ®ã€æ£€æµ‹è¾…åŠ©è¯­ä¹‰å±æ€§å’Œè¯†åˆ«ä»ªå™¨æåŠçš„æ¨¡å‹ã€‚</li>
<li>ç³»ç»ŸåŸºäºSciBERTæ¨¡å‹å’Œå¤šä»»åŠ¡è½¬æ¢å™¨å®ç°ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡é’ˆå¯¹å¤©æ–‡å­¦è¯­æ–™åº“åˆ†ç±»è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨ä½æˆæœ¬å®ç°çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2753e556ea13036480e11e62fbf70a05" align="middle">
<img src="https://picx.zhimg.com/v2-601d8e508c086887e9fd05dc550debf8" align="middle">
<img src="https://picx.zhimg.com/v2-cef163111447ac491dd58e1bf21f936c" align="middle">
<img src="https://picx.zhimg.com/v2-a3763cb20c89e115fad9ba7bdd947f74" align="middle">
<img src="https://picx.zhimg.com/v2-00d23d13e66db7f376cb2e39016edfae" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multimodal-LLMs-Do-Not-Compose-Skills-Optimally-Across-Modalities"><a href="#Multimodal-LLMs-Do-Not-Compose-Skills-Optimally-Across-Modalities" class="headerlink" title="Multimodal LLMs Do Not Compose Skills Optimally Across Modalities"></a>Multimodal LLMs Do Not Compose Skills Optimally Across Modalities</h2><p><strong>Authors:Paula Ontalvilla, Aitor Ormazabal, Gorka Azkune</strong></p>
<p>Skill composition is the ability to combine previously learned skills to solve new tasks. As neural networks acquire increasingly complex skills during their pretraining, it is not clear how successfully they can compose them. In this paper, we focus on Multimodal Large Language Models (MLLM), and study their ability to compose skills across modalities. To this end, we design three evaluation tasks which can be solved sequentially composing two modality-dependent skills, and evaluate several open MLLMs under two main settings: i) prompting the model to directly solve the task, and ii) using a two-step cascaded inference approach, which manually enforces the composition of the two skills for a given task. Even with these straightforward compositions, we find that all evaluated MLLMs exhibit a significant cross-modality skill composition gap. To mitigate the aforementioned gap, we explore two alternatives: i) use chain-of-thought prompting to explicitly instruct MLLMs for skill composition and ii) a specific fine-tuning recipe to promote skill composition. Although those strategies improve model performance, they still exhibit significant skill composition gaps, suggesting that more research is needed to improve cross-modal skill composition in MLLMs.</p>
<blockquote>
<p>æŠ€èƒ½ç»„åˆæ˜¯æŒ‡å°†å…ˆå‰å­¦ä¹ çš„æŠ€èƒ½ç»“åˆèµ·æ¥ä»¥è§£å†³æ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚éšç€ç¥ç»ç½‘ç»œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æŒæ¡çš„æŠ€èƒ½è¶Šæ¥è¶Šå¤æ‚ï¼Œå°šä¸æ¸…æ¥šå®ƒä»¬èƒ½å¦æˆåŠŸåœ°è¿›è¡Œç»„åˆã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç ”ç©¶å…¶åœ¨ä¸åŒæ¨¡æ€ä¸‹çš„æŠ€èƒ½ç»„åˆèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ä¸ªè¯„ä¼°ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å¯ä»¥é€šè¿‡æŒ‰é¡ºåºç»„åˆä¸¤ç§æ¨¡æ€ç›¸å…³çš„æŠ€èƒ½æ¥è§£å†³ï¼Œå¹¶åœ¨ä¸¤ç§ä¸»è¦è®¾ç½®ä¸‹å¯¹å¤šä¸ªå…¬å¼€MLLMè¿›è¡Œè¯„ä¼°ï¼ši) æç¤ºæ¨¡å‹ç›´æ¥è§£å†³é—®é¢˜ï¼›ii) ä½¿ç”¨ä¸¤æ­¥çº§è”æ¨ç†æ–¹æ³•ï¼Œæ‰‹åŠ¨å¼ºåˆ¶æ‰§è¡Œç»™å®šä»»åŠ¡çš„ä¸¤ä¸ªæŠ€èƒ½çš„ç»„åˆã€‚å³ä½¿åœ¨è¿™äº›ç®€å•çš„ç»„åˆä¸­ï¼Œæˆ‘ä»¬å‘ç°æ‰€æœ‰è¯„ä¼°çš„MLLMéƒ½å­˜åœ¨æ˜¾è‘—çš„è·¨æ¨¡æ€æŠ€èƒ½ç»„åˆå·®è·ã€‚ä¸ºäº†ç¼“è§£ä¸Šè¿°å·®è·ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§æ›¿ä»£æ–¹æ³•ï¼ši)ä½¿ç”¨æ€ç»´é“¾æç¤ºæ¥æ˜ç¡®æŒ‡å¯¼MLLMè¿›è¡ŒæŠ€èƒ½ç»„åˆï¼›ii)é‡‡ç”¨ç‰¹å®šçš„å¾®è°ƒé…æ–¹æ¥ä¿ƒè¿›æŠ€èƒ½ç»„åˆã€‚å°½ç®¡è¿™äº›ç­–ç•¥æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»å­˜åœ¨æ˜æ˜¾çš„æŠ€èƒ½ç»„åˆå·®è·ï¼Œè¿™è¡¨æ˜éœ€è¦è¿›è¡Œæ›´å¤šç ”ç©¶æ¥æ”¹å–„MLLMä¸­çš„è·¨æ¨¡æ€æŠ€èƒ½ç»„åˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08113v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ä¸åŒæ¨¡æ€ä¸‹ç»„åˆæŠ€èƒ½çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿é€šè¿‡ç®€å•çš„æŠ€èƒ½ç»„åˆï¼Œç°æœ‰çš„MLLMä»å­˜åœ¨æ˜¾è‘—çš„è·¨æ¨¡æ€æŠ€èƒ½ç»„åˆå·®è·ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« å°è¯•é‡‡ç”¨ä¸¤ç§ç­–ç•¥ï¼šä¸€æ˜¯ä½¿ç”¨é“¾å¼æ€ç»´æç¤ºæ¥æŒ‡å¯¼æ¨¡å‹è¿›è¡ŒæŠ€èƒ½ç»„åˆï¼ŒäºŒæ˜¯é‡‡ç”¨ç‰¹å®šçš„å¾®è°ƒç­–ç•¥æ¥ä¿ƒè¿›æŠ€èƒ½ç»„åˆã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§ç­–ç•¥ä»æœªèƒ½å®Œå…¨æ¶ˆé™¤æŠ€èƒ½ç»„åˆå·®è·ï¼Œéœ€è¦æ›´å¤šçš„ç ”ç©¶æ¥æé«˜MLLMçš„è·¨æ¨¡æ€æŠ€èƒ½ç»„åˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMèƒ½å¤Ÿç»“åˆå…ˆå‰å­¦åˆ°çš„æŠ€èƒ½æ¥è§£å†³æ–°ä»»åŠ¡ï¼Œä½†åœ¨è·¨æ¨¡æ€æŠ€èƒ½ç»„åˆæ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>é€šè¿‡è®¾è®¡ä¸‰ä¸ªè¯„ä¼°ä»»åŠ¡æ¥æµ‹è¯•MLLMåœ¨ä¸åŒæ¨¡æ€ä¸‹çš„æŠ€èƒ½ç»„åˆèƒ½åŠ›ã€‚</li>
<li>åœ¨è¯„ä¼°ä¸­ï¼Œç›´æ¥æç¤ºæ¨¡å‹å’Œé‡‡ç”¨ä¸¤æ­¥çº§è”æ¨ç†æ–¹æ³•å‡å­˜åœ¨æŠ€èƒ½ç»„åˆå·®è·ã€‚</li>
<li>å°è¯•ä½¿ç”¨é“¾å¼æ€ç»´æç¤ºå’Œç‰¹å®šå¾®è°ƒç­–ç•¥æ¥æ”¹å–„æŠ€èƒ½ç»„åˆèƒ½åŠ›ã€‚</li>
<li>å°½ç®¡è¿™äº›ç­–ç•¥æœ‰æ‰€æå‡ï¼Œä½†MLLMåœ¨è·¨æ¨¡æ€æŠ€èƒ½ç»„åˆæ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>ç°æœ‰çš„MLLMéœ€è¦æ›´å¤šçš„ç ”ç©¶æ¥æé«˜è·¨æ¨¡æ€æŠ€èƒ½ç»„åˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21cf32ea49e241dab2800bd73d5873bb" align="middle">
<img src="https://picx.zhimg.com/v2-e31d37c853f027cabb690c2d2f16902e" align="middle">
<img src="https://picx.zhimg.com/v2-fac88d282167f05659c700ff7b65c9b9" align="middle">
<img src="https://picx.zhimg.com/v2-38ebfbe930fdf5aaa5e665ed57d51200" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="QUARK-Quantization-Enabled-Circuit-Sharing-for-Transformer-Acceleration-by-Exploiting-Common-Patterns-in-Nonlinear-Operations"><a href="#QUARK-Quantization-Enabled-Circuit-Sharing-for-Transformer-Acceleration-by-Exploiting-Common-Patterns-in-Nonlinear-Operations" class="headerlink" title="QUARK: Quantization-Enabled Circuit Sharing for Transformer Acceleration by Exploiting Common Patterns in Nonlinear Operations"></a>QUARK: Quantization-Enabled Circuit Sharing for Transformer Acceleration by Exploiting Common Patterns in Nonlinear Operations</h2><p><strong>Authors:Zhixiong Zhao, Haomin Li, Fangxin Liu, Yuncheng Lu, Zongwu Wang, Tao Yang, Li Jiang, Haibing Guan</strong></p>
<p>Transformer-based models have revolutionized computer vision (CV) and natural language processing (NLP) by achieving state-of-the-art performance across a range of benchmarks. However, nonlinear operations in models significantly contribute to inference latency, presenting unique challenges for efficient hardware acceleration. To this end, we propose QUARK, a quantization-enabled FPGA acceleration framework that leverages common patterns in nonlinear operations to enable efficient circuit sharing, thereby reducing hardware resource requirements. QUARK targets all nonlinear operations within Transformer-based models, achieving high-performance approximation through a novel circuit-sharing design tailored to accelerate these operations. Our evaluation demonstrates that QUARK significantly reduces the computational overhead of nonlinear operators in mainstream Transformer architectures, achieving up to a 1.96 times end-to-end speedup over GPU implementations. Moreover, QUARK lowers the hardware overhead of nonlinear modules by more than 50% compared to prior approaches, all while maintaining high model accuracy â€“ and even substantially boosting accuracy under ultra-low-bit quantization.</p>
<blockquote>
<p>åŸºäºTransformerçš„æ¨¡å‹å·²ç»åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»è€Œå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ã€‚ç„¶è€Œï¼Œæ¨¡å‹ä¸­çš„éçº¿æ€§æ“ä½œå¯¹æ¨ç†å»¶è¿Ÿäº§ç”Ÿäº†é‡å¤§è´¡çŒ®ï¼Œä¸ºé«˜æ•ˆçš„ç¡¬ä»¶åŠ é€Ÿå¸¦æ¥äº†ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†QUARKï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ç”¨é‡åŒ–çš„FPGAåŠ é€Ÿæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨éçº¿æ€§æ“ä½œä¸­çš„å¸¸è§æ¨¡å¼æ¥å®ç°æœ‰æ•ˆçš„ç”µè·¯å…±äº«ï¼Œä»è€Œé™ä½äº†ç¡¬ä»¶èµ„æºè¦æ±‚ã€‚QUARKé¢å‘åŸºäºTransformeræ¨¡å‹ä¸­çš„æ‰€æœ‰éçº¿æ€§æ“ä½œï¼Œé€šè¿‡é’ˆå¯¹è¿™äº›æ“ä½œè¿›è¡Œçš„æ–°å‹ç”µè·¯å…±äº«è®¾è®¡ï¼Œå®ç°é«˜æ€§èƒ½è¿‘ä¼¼ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒQUARKæ˜¾è‘—é™ä½äº†ä¸»æµTransformeræ¶æ„ä¸­éçº¿æ€§æ“ä½œç¬¦çš„è®¡ç®—å¼€é”€ï¼Œä¸GPUå®ç°ç›¸æ¯”ï¼Œå®ç°äº†é«˜è¾¾1.96å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚æ­¤å¤–ï¼Œä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒQUARKå°†éçº¿æ€§æ¨¡å—çš„ç¡¬ä»¶å¼€é”€é™ä½äº†50%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„é«˜ç²¾åº¦ï¼Œç”šè‡³åœ¨è¶…ä½ä½é‡åŒ–ä¸‹å®è´¨æé«˜äº†å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06767v1">PDF</a> ICCAD 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†QUARKæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºé‡åŒ–çš„FPGAåŠ é€Ÿæ¡†æ¶ï¼Œé’ˆå¯¹Transformeræ¨¡å‹ä¸­çš„éçº¿æ€§æ“ä½œè¿›è¡Œä¼˜åŒ–ã€‚QUARKåˆ©ç”¨éçº¿æ€§æ“ä½œä¸­çš„å…±åŒæ¨¡å¼å®ç°é«˜æ•ˆçš„ç”µè·¯å…±äº«ï¼Œé™ä½ç¡¬ä»¶èµ„æºè¦æ±‚ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒQUARKèƒ½æ˜¾è‘—å‡å°‘ä¸»æµTransformeræ¶æ„ä¸­éçº¿æ€§æ ¼ç®—çš„è®¡ç®—å¼€é”€ï¼Œç›¸å¯¹äºGPUå®ç°å®ç°æœ€é«˜è¾¾1.96å€ç«¯åˆ°ç«¯çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼ŒQUARKé™ä½äº†éçº¿æ€§æ¨¡å—çš„ç¡¬ä»¶å¼€é”€è¶…è¿‡50%ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„é«˜ç²¾åº¦ï¼Œç”šè‡³åœ¨è¶…ä½ä½é‡åŒ–ä¸‹ä¹Ÿèƒ½æ˜¾è‘—æé«˜ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹ä¸­çš„éçº¿æ€§æ“ä½œå¯¹æ¨ç†å»¶è¿Ÿæœ‰æ˜¾è‘—è´¡çŒ®ï¼Œéœ€è¦é«˜æ•ˆçš„ç¡¬ä»¶åŠ é€Ÿè§£å†³æ–¹æ¡ˆã€‚</li>
<li>QUARKæ¡†æ¶é€šè¿‡åˆ©ç”¨éçº¿æ€§æ“ä½œä¸­çš„å…±åŒæ¨¡å¼å®ç°ç”µè·¯å…±äº«ï¼Œä»¥æé«˜æ•ˆç‡å¹¶é™ä½ç¡¬ä»¶èµ„æºéœ€æ±‚ã€‚</li>
<li>QUARKæ¡†æ¶é’ˆå¯¹Transformeræ¨¡å‹ä¸­çš„æ‰€æœ‰éçº¿æ€§æ“ä½œè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>QUARKåœ¨ä¸»æµTransformeræ¶æ„ä¸­å®ç°äº†æ˜¾è‘—çš„éçº¿æ€§æ“ä½œè®¡ç®—å¼€é”€å‡å°‘ã€‚</li>
<li>QUARKç›¸å¯¹äºGPUå®ç°æœ‰é«˜è¾¾1.96å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚</li>
<li>QUARKé™ä½äº†éçº¿æ€§æ¨¡å—çš„ç¡¬ä»¶å¼€é”€è¶…è¿‡50%ï¼Œç›¸è¾ƒäºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dca9051001b529502723fa1fa63266a5" align="middle">
<img src="https://picx.zhimg.com/v2-24970172836f9d48ad9ad8bd0756931e" align="middle">
<img src="https://picx.zhimg.com/v2-69fd02fe846e9f37244da13eec321eaa" align="middle">
<img src="https://picx.zhimg.com/v2-a366918526cdb8fecbfcfe3f29587fd6" align="middle">
<img src="https://picx.zhimg.com/v2-b49397d646ec1b9bbf669636bbb6125f" align="middle">
<img src="https://picx.zhimg.com/v2-34e52b5a0998559445f9fc6df89e9caa" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SugarTextNet-A-Transformer-Based-Framework-for-Detecting-Sugar-Dating-Related-Content-on-Social-Media-with-Context-Aware-Focal-Loss"><a href="#SugarTextNet-A-Transformer-Based-Framework-for-Detecting-Sugar-Dating-Related-Content-on-Social-Media-with-Context-Aware-Focal-Loss" class="headerlink" title="SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss"></a>SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss</h2><p><strong>Authors:Lionel Z. Wang, Shihan Ben, Yulu Huang, Simeng Qin</strong></p>
<p>Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.<del>Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.</del>In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.<del>SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.</del>To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.<del>We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.</del>Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.</p>
<blockquote>
<p>å…³äºâ€œç³–çˆ¹åœ°â€ï¼ˆSugar Datingï¼‰çš„ç›¸å…³å†…å®¹åœ¨ä¸»æµç¤¾äº¤åª’ä½“å¹³å°ä¸Šè¿…é€Ÿæ³›æ»¥ï¼Œå¼•å‘äº†ç¤¾ä¼šå’Œç›‘ç®¡æ–¹é¢çš„ä¸¥é‡å…³æ³¨ï¼ŒåŒ…æ‹¬å¯¹äº²å¯†å…³ç³»å•†ä¸šåŒ–å’Œäº¤æ˜“å…³ç³»æ­£å¸¸åŒ–çš„æ‹…å¿§ã€‚ç”±äºå§”å©‰è¯­ã€æ¨¡ç³Šçš„è¯­è¨€çº¿ç´¢å’Œç°å®ä¸–ç•Œæ•°æ®ä¸­çš„æç«¯ç±»åˆ«ä¸å¹³è¡¡ç°è±¡æ™®éï¼Œæ£€æµ‹æ­¤ç±»å†…å®¹é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SugarTextNetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯†åˆ«ç¤¾äº¤åª’ä½“ä¸Šâ€œç³–çˆ¹åœ°â€ç›¸å…³å¸–å­çš„æ–°å‹åŸºäºè½¬æ¢å™¨çš„æ¡†æ¶ã€‚SugarTextNeté›†æˆäº†é¢„è®­ç»ƒçš„è½¬æ¢å™¨ç¼–ç å™¨ã€åŸºäºæ³¨æ„åŠ›çš„çº¿ç´¢æå–å™¨å’Œä¸Šä¸‹æ–‡çŸ­è¯­ç¼–ç å™¨ï¼Œä»¥æ•è·ç”¨æˆ·ç”Ÿæˆæ–‡æœ¬ä¸­çš„é‡è¦å’Œç»†å¾®ç‰¹å¾ã€‚ä¸ºäº†è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¹¶å¢å¼ºå¯¹å°‘æ•°ç±»åˆ«çš„æ£€æµ‹èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†Context-Aware Focal Lossï¼Œè¿™æ˜¯ä¸€ç§å®šåˆ¶çš„æŸå¤±å‡½æ•°ï¼Œç»“åˆäº†ç„¦ç‚¹æŸå¤±ç¼©æ”¾å’Œä¸Šä¸‹æ–‡åŠ æƒã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªå…¨æ–°çš„ã€æ‰‹å·¥æ ‡æ³¨çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†SugarTextNetï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªæ–°æµªå¾®åšçš„3,067ç¯‡ä¸­æ–‡ç¤¾äº¤åª’ä½“å¸–å­ï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€æ·±åº¦å­¦ä¹ åŸºå‡†æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¯å®äº†æ¯ä¸ªç»„ä»¶éƒ½æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†é’ˆå¯¹æ•æ„Ÿå†…å®¹æ£€æµ‹è¿›è¡Œé¢†åŸŸç‰¹å®šå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºæ¨¡çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¤æ‚ç°å®åœºæ™¯ä¸­çš„å†…å®¹ç®¡ç†æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06402v2">PDF</a> This paper is accepted by HICSS 2026</p>
<p><strong>Summary</strong>ï¼š<br>ç¤¾äº¤åª’ä½“ä¸Šæ¶Œç°å¤§é‡ä¸â€œç³–çˆ¹â€ï¼ˆSugarDaddyï¼‰ç›¸å…³çš„å†…å®¹ï¼Œå¼•å‘ç¤¾ä¼šåŠç›‘ç®¡æ–¹é¢çš„æ‹…å¿§ã€‚ä¸ºåº”å¯¹è¿™ç§æƒ…å†µï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSugarTextNetçš„æ–°å‹åŸºäºtransformerçš„æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè¯†åˆ«ç¤¾äº¤åª’ä½“ä¸Šçš„æ­¤ç±»å†…å®¹ã€‚è¯¥æ¡†æ¶é›†æˆäº†é¢„è®­ç»ƒçš„transformerç¼–ç å™¨ã€åŸºäºæ³¨æ„åŠ›çš„çº¿ç´¢æå–å™¨å’Œä¸Šä¸‹æ–‡çŸ­è¯­ç¼–ç å™¨ï¼Œä»¥æ•æ‰ç”¨æˆ·ç”Ÿæˆæ–‡æœ¬ä¸­çš„æ˜¾è‘—å’Œå¾®å¦™ç‰¹å¾ã€‚ä¸ºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¹¶æé«˜å°‘æ•°ç±»åˆ«çš„æ£€æµ‹æ€§èƒ½ï¼Œå¼•å…¥äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç„¦ç‚¹æŸå¤±ï¼ˆContext-Aware Focal Lossï¼‰è¿™ä¸€å®šåˆ¶çš„æŸå¤±å‡½æ•°ã€‚åœ¨é’ˆå¯¹æ–°æµªå¾®åšæ–°æ”¶é›†çš„3067ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„ç¤¾äº¤åª’ä½“å¸–å­æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€æ·±åº¦å­¦ä¹ åŸºå‡†æ¨¡å‹ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤ç ”ç©¶å¼ºè°ƒäº†é’ˆå¯¹æ•æ„Ÿå†…å®¹æ£€æµ‹è¿›è¡Œé¢†åŸŸç‰¹å®šã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å»ºæ¨¡çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¤æ‚çš„çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„å†…å®¹ç®¡ç†æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç³–çˆ¹ï¼ˆSugarDaddyï¼‰ç›¸å…³å†…å®¹åœ¨ä¸»æµç¤¾äº¤åª’ä½“å¹³å°è¿…é€Ÿæ‰©æ•£ï¼Œå¼•å‘ç¤¾ä¼šå’Œç›‘ç®¡æ‹…å¿§ã€‚</li>
<li>è¯†åˆ«æ­¤ç±»å†…å®¹é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨å¤§é‡çš„å«è“„éšæ™¦è¡¨è¾¾å’Œæç«¯çš„æ•°æ®ç±»ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåä¸ºSugarTextNetçš„æ–°å‹åŸºäºtransformerçš„æ¡†æ¶æ¥è¯†åˆ«ç¤¾äº¤åª’ä½“ä¸Šçš„ç³–çˆ¹ç›¸å…³å†…å®¹ã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†é¢„è®­ç»ƒç¼–ç å™¨ã€æ³¨æ„åŠ›çº¿ç´¢æå–å™¨å’Œä¸Šä¸‹æ–‡çŸ­è¯­ç¼–ç å™¨ï¼Œä»¥æ•æ‰æ–‡æœ¬ä¸­çš„å…³é”®å’Œå¾®å¦™ç‰¹å¾ã€‚</li>
<li>å¼•å…¥äº†å®šåˆ¶çš„æŸå¤±å‡½æ•°Context-Aware Focal Lossæ¥è§£å†³æ•°æ®ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¹¶æé«˜å°‘æ•°ç±»åˆ«çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åœ¨æ–°æ”¶é›†çš„ç¤¾äº¤åª’ä½“å¸–å­æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒSugarTextNetåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a48162fe58d180322a89e2d2796e9e2" align="middle">
<img src="https://picx.zhimg.com/v2-60ec61cba251b2989eca290b62a233b8" align="middle">
<img src="https://picx.zhimg.com/v2-a9edbcdde97c5acdee22e3f553e93cc8" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Ghost-in-the-Transformer-Tracing-LLM-Lineage-with-SVD-Fingerprint"><a href="#Ghost-in-the-Transformer-Tracing-LLM-Lineage-with-SVD-Fingerprint" class="headerlink" title="Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint"></a>Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint</h2><p><strong>Authors:Suqing Wang, Ziyang Ma, Xinyi Li, Zuchao Li</strong></p>
<p>Large Language Models (LLMs) have rapidly advanced and are widely adopted across diverse fields. Due to the substantial computational cost and data requirements of training from scratch, many developers choose to fine-tune or modify existing open-source models. While most adhere to open-source licenses, some falsely claim original training despite clear derivation from public models. This raises pressing concerns about intellectual property protection and highlights the need for reliable methods to verify model provenance. In this paper, we propose GhostSpec, a lightweight yet effective method for verifying LLM lineage without access to training data or modification of model behavior. Our approach constructs compact and robust fingerprints by applying singular value decomposition (SVD) to invariant products of internal attention weight matrices, effectively capturing the structural identity of a model. Unlike watermarking or output-based methods, GhostSpec is fully data-free, non-invasive, and computationally efficient. It demonstrates strong robustness to sequential fine-tuning, pruning, block expansion, and even adversarial transformations. Extensive experiments show that GhostSpec can reliably trace the lineage of transformed models with minimal overhead. By offering a practical solution for model verification and reuse tracking, our method contributes to the protection of intellectual property and fosters a transparent, trustworthy ecosystem for large-scale language models.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¿…é€Ÿå‘å±•å’Œå¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚ç”±äºä»å¤´å¼€å§‹è®­ç»ƒéœ€è¦å¤§é‡çš„è®¡ç®—æˆæœ¬å’Œæ•°æ®è¦æ±‚ï¼Œè®¸å¤šå¼€å‘è€…é€‰æ‹©å¾®è°ƒæˆ–ä¿®æ”¹ç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚è™½ç„¶å¤§å¤šæ•°äººéµå®ˆå¼€æºè®¸å¯è¯ï¼Œä½†æœ‰äº›äººå´å£°ç§°ä½¿ç”¨äº†åŸå§‹è®­ç»ƒæ¨¡å‹ï¼Œå°½ç®¡ä»–ä»¬æ˜æ˜¾æ˜¯ä»å…¬å¼€æ¨¡å‹è¡ç”Ÿè€Œæ¥çš„ã€‚è¿™å¼•å‘äº†å…³äºçŸ¥è¯†äº§æƒä¿æŠ¤é—®é¢˜çš„ç´§è¿«å…³æ³¨ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦å¯é æ–¹æ³•æ¥éªŒè¯æ¨¡å‹æ¥æºçš„éœ€æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GhostSpecï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®¿é—®è®­ç»ƒæ•°æ®æˆ–ä¿®æ”¹æ¨¡å‹è¡Œä¸ºå³å¯éªŒè¯LLMè¡€ç»Ÿçš„è½»é‡çº§æœ‰æ•ˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åº”ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰äºå†…éƒ¨æ³¨æ„åŠ›æƒé‡çŸ©é˜µçš„ä¸å˜ä¹˜ç§¯æ¥æ„å»ºç´§å‡‘ä¸”ç¨³å¥çš„æŒ‡çº¹ï¼Œæœ‰æ•ˆåœ°æ•æ‰æ¨¡å‹çš„ç»“æ„ç‰¹å¾ã€‚ä¸watermarkingæˆ–åŸºäºè¾“å‡ºçš„æ–¹æ³•ä¸åŒï¼ŒGhostSpecå®Œå…¨ä¸éœ€è¦æ•°æ®ã€éä¾µå…¥æ€§ä¸”è®¡ç®—æ•ˆç‡é«˜ã€‚å®ƒå¯¹é¡ºåºå¾®è°ƒã€ä¿®å‰ªã€å—æ‰©å±•ç”šè‡³å¯¹æŠ—æ€§è½¬æ¢å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGhostSpecå¯ä»¥å¯é åœ°è¿½è¸ªè½¬æ¢æ¨¡å‹çš„è¡€ç»Ÿï¼Œå¹¶ä¸”å‡ ä¹ä¸éœ€è¦é¢å¤–çš„å¼€é”€ã€‚é€šè¿‡ä¸ºæ¨¡å‹éªŒè¯å’Œé‡ç”¨è·Ÿè¸ªæä¾›å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰åŠ©äºä¿æŠ¤çŸ¥è¯†äº§æƒå¹¶ä¿ƒè¿›å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„é€æ˜ã€å¯ä¿¡ç”Ÿæ€ç³»ç»Ÿçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06390v1">PDF</a> Accepted at AAAI 2026 (Oral)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”æ•°æ®éœ€æ±‚é‡å¤§ï¼Œå› æ­¤å¼€å‘è€…å¤šé€‰æ‹©å¾®è°ƒæˆ–ä¿®æ”¹ç°æœ‰å¼€æºæ¨¡å‹ã€‚è™½ç„¶å¤šæ•°éµå®ˆå¼€æºè®¸å¯ï¼Œä½†éƒ¨åˆ†å¼€å‘è€…åœ¨æ˜æ˜¾åŸºäºå…¬å…±æ¨¡å‹çš„æƒ…å†µä¸‹å´å£°ç§°åŸå§‹è®­ç»ƒï¼Œå¼•å‘çŸ¥è¯†äº§æƒä¿æŠ¤é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºGhostSpecçš„è½»é‡çº§ä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸æ¥è§¦è®­ç»ƒæ•°æ®æˆ–æ”¹å˜æ¨¡å‹è¡Œä¸ºçš„æƒ…å†µä¸‹éªŒè¯LLMçš„è¡€ç»Ÿã€‚GhostSpecé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰åº”ç”¨äºå†…éƒ¨æ³¨æ„åŠ›æƒé‡çŸ©é˜µçš„ä¸å˜ä¹˜ç§¯æ¥æ„å»ºç´§å‡‘ä¸”ç¨³å¥çš„æŒ‡çº¹ï¼Œæœ‰æ•ˆæ•æ‰æ¨¡å‹çš„ç»“æ„ç‰¹å¾ã€‚ç›¸è¾ƒäºæ°´å°æˆ–è¾“å‡ºåŸºç¡€æ–¹æ³•ï¼ŒGhostSpecå®Œå…¨æ— éœ€æ•°æ®ã€éä¾µå…¥æ€§ä¸”è®¡ç®—é«˜æ•ˆã€‚å®éªŒè¯æ˜GhostSpecå¯¹é¡ºåºå¾®è°ƒã€ä¿®å‰ªã€å—æ‰©å±•ç”šè‡³å¯¹æŠ—æ€§è½¬æ¢å…·æœ‰è¾ƒå¼ºçš„ç¨³å¥æ€§ã€‚é€šè¿‡ä¸ºæ¨¡å‹éªŒè¯å’Œå†åˆ©ç”¨è·Ÿè¸ªæä¾›å®ç”¨è§£å†³æ–¹æ¡ˆï¼ŒGhostSpecæœ‰åŠ©äºä¿æŠ¤çŸ¥è¯†äº§æƒå¹¶ä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„é€æ˜å¯ä¿¡ç”Ÿæ€ç³»ç»Ÿå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¹¿æ³›è¿ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œä½†ç”±äºè®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œè®¸å¤šå¼€å‘è€…é€‰æ‹©å¾®è°ƒæˆ–ä¿®æ”¹ç°æœ‰æ¨¡å‹ã€‚</li>
<li>éƒ¨åˆ†å¼€å‘è€…åœ¨åŸºäºå…¬å…±æ¨¡å‹çš„æƒ…å†µä¸‹å£°ç§°åŸå§‹è®­ç»ƒï¼Œå¼•å‘çŸ¥è¯†äº§æƒä¿æŠ¤é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºGhostSpecçš„æ–¹æ³•ï¼Œç”¨äºéªŒè¯LLMè¡€ç»Ÿï¼Œæ— éœ€æ¥è§¦è®­ç»ƒæ•°æ®æˆ–æ”¹å˜æ¨¡å‹è¡Œä¸ºã€‚</li>
<li>GhostSpecé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£æ„å»ºç¨³å¥çš„æŒ‡çº¹ï¼Œæœ‰æ•ˆæ•æ‰æ¨¡å‹çš„ç»“æ„ç‰¹å¾ã€‚</li>
<li>GhostSpecå…·å¤‡æ•°æ®å…è´¹ã€éä¾µå…¥æ€§å’Œè®¡ç®—é«˜æ•ˆçš„ç‰¹ç‚¹ã€‚</li>
<li>GhostSpecå…·å¤‡å¯¹å¤šç§æ¨¡å‹ä¿®æ”¹æ–¹æ³•çš„ç¨³å¥æ€§ï¼Œå¦‚å¾®è°ƒã€ä¿®å‰ªã€å—æ‰©å±•ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2fe2d9de27e0ce7f4b04ce25f11a8417" align="middle">
<img src="https://picx.zhimg.com/v2-21075d8a67b350b10b3b657e1950a16a" align="middle">
<img src="https://picx.zhimg.com/v2-78520fda3e8c7d8cf76c40f6cecab8d8" align="middle">
<img src="https://picx.zhimg.com/v2-91893bad6925e852d9328c795c15d2b2" align="middle">
<img src="https://picx.zhimg.com/v2-55987241b801b40367148149affa78d2" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MCP-RiskCue-Can-LLM-Infer-Risk-Information-From-MCP-Server-System-Logs"><a href="#MCP-RiskCue-Can-LLM-Infer-Risk-Information-From-MCP-Server-System-Logs" class="headerlink" title="MCP-RiskCue: Can LLM Infer Risk Information From MCP Server System Logs?"></a>MCP-RiskCue: Can LLM Infer Risk Information From MCP Server System Logs?</h2><p><strong>Authors:Jiayi Fu, Qiyao Sun</strong></p>
<p>Large language models (LLMs) demonstrate strong capabilities in solving complex tasks when integrated with external tools. The Model Context Protocol (MCP) has become a standard interface for enabling such tool-based interactions. However, these interactions introduce substantial security concerns, particularly when the MCP server is compromised or untrustworthy. While prior benchmarks primarily focus on prompt injection attacks or analyze the vulnerabilities of LLM MCP interaction trajectories, limited attention has been given to the underlying system logs associated with malicious MCP servers. To address this gap, we present the first synthetic benchmark for evaluating LLMs ability to identify security risks from system logs. We define nine categories of MCP server risks and generate 1,800 synthetic system logs using ten state-of-the-art LLMs. These logs are embedded in the return values of 243 curated MCP servers, yielding a dataset of 2,421 chat histories for training and 471 queries for evaluation. Our pilot experiments reveal that smaller models often fail to detect risky system logs, leading to high false negatives. While models trained with supervised fine-tuning (SFT) tend to over-flag benign logs, resulting in elevated false positives, Reinforcement Learning from Verifiable Reward (RLVR) offers a better precision-recall balance. In particular, after training with Group Relative Policy Optimization (GRPO), Llama3.1-8B-Instruct achieves 83% accuracy, surpassing the best-performing large remote model by 9 percentage points. Fine-grained, per-category analysis further underscores the effectiveness of reinforcement learning in enhancing LLM safety within the MCP framework. Code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/PorUna-byte/MCP-RiskCue">https://github.com/PorUna-byte/MCP-RiskCue</a></p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸å¤–éƒ¨å·¥å…·é›†æˆæ—¶ï¼Œå±•ç°å‡ºè§£å†³å¤æ‚ä»»åŠ¡çš„å¼ºå¤§èƒ½åŠ›ã€‚æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å·²æˆä¸ºå®ç°è¿™ç§åŸºäºå·¥å…·äº¤äº’çš„æ ‡å‡†æ¥å£ã€‚ç„¶è€Œï¼Œè¿™äº›äº¤äº’å¼•å‘äº†å®è´¨æ€§çš„å®‰å…¨æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯å½“MCPæœåŠ¡å™¨å—åˆ°æ”»å‡»æˆ–ä¸å¯ä¿¡ä»»æ—¶ã€‚è™½ç„¶å…ˆå‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æç¤ºæ³¨å…¥æ”»å‡»æˆ–åˆ†æLLM MCPäº¤äº’è½¨è¿¹çš„æ¼æ´ï¼Œä½†å¾ˆå°‘å…³æ³¨ä¸æ¶æ„MCPæœåŠ¡å™¨ç›¸å…³çš„åº•å±‚ç³»ç»Ÿæ—¥å¿—ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†è¯„ä¼°LLMè¯†åˆ«ç³»ç»Ÿæ—¥å¿—ä¸­å®‰å…¨é£é™©èƒ½åŠ›çš„åˆæˆåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¹ç±»MCPæœåŠ¡å™¨é£é™©ï¼Œå¹¶ä½¿ç”¨åç§æœ€å…ˆè¿›çš„LLMç”Ÿæˆäº†1800æ¡åˆæˆç³»ç»Ÿæ—¥å¿—ã€‚è¿™äº›æ—¥å¿—åµŒå…¥åœ¨243ä¸ªç²¾é€‰çš„MCPæœåŠ¡å™¨çš„è¿”å›å€¼ä¸­ï¼Œå½¢æˆäº†åŒ…å«2421æ¡èŠå¤©è®°å½•çš„è®­ç»ƒæ•°æ®é›†å’Œ471æ¡æŸ¥è¯¢çš„è¯„ä¼°æ•°æ®é›†ã€‚æˆ‘ä»¬çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼Œå°å‹æ¨¡å‹å¾€å¾€æ— æ³•æ£€æµ‹åˆ°å±é™©çš„ç³»ç»Ÿæ—¥å¿—ï¼Œå¯¼è‡´é«˜è¯¯æŠ¥ç‡ã€‚è™½ç„¶ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¼šè¿‡åº¦æ ‡è®°è‰¯æ€§æ—¥å¿—ï¼Œå¯¼è‡´è¯¯æŠ¥ç‡ä¸Šå‡ï¼Œä½†é€šè¿‡å¯éªŒè¯å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æä¾›äº†æ›´å¥½çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´çš„å¹³è¡¡ã€‚ç‰¹åˆ«æ˜¯ç»è¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒåï¼ŒLlama3.1-8B-Instructçš„å‡†ç¡®ç‡è¾¾åˆ°äº†83%ï¼Œè¶…è¿‡äº†è¡¨ç°æœ€å¥½çš„å¤§å‹è¿œç¨‹æ¨¡å‹9ä¸ªç™¾åˆ†ç‚¹ã€‚ç²¾ç»†çš„æŒ‰ç±»åˆ«åˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ åœ¨å¢å¼ºLLMåœ¨MCPæ¡†æ¶å†…å®‰å…¨æ€§çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/PorUna-byte/MCP-RiskCue%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PorUna-byte/MCP-RiskCueæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05867v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨å·¥å…·é›†æˆåï¼Œåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å·²æˆä¸ºå®ç°è¿™ç§åŸºäºå·¥å…·äº¤äº’çš„æ ‡å‡†æ¥å£ã€‚ç„¶è€Œï¼Œè¿™äº›äº¤äº’å¼•å‘äº†é‡å¤§çš„å®‰å…¨æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯å½“MCPæœåŠ¡å™¨ä¸å¯é æˆ–è¢«ç ´åæ—¶ã€‚å°½ç®¡å…ˆå‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æç¤ºæ³¨å…¥æ”»å‡»æˆ–åˆ†æLLM MCPäº¤äº’è½¨è¿¹çš„æ¼æ´ï¼Œä½†å°šæœªå…³æ³¨ä¸æ¶æ„MCPæœåŠ¡å™¨ç›¸å…³çš„åº•å±‚ç³»ç»Ÿæ—¥å¿—ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªåˆæˆåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMä»ç³»ç»Ÿæ—¥å¿—è¯†åˆ«å®‰å…¨é£é™©çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¹ç±»MCPæœåŠ¡å™¨é£é™©ï¼Œå¹¶ä½¿ç”¨åç§æœ€å…ˆè¿›çš„LLMç”Ÿæˆäº†1800æ¡åˆæˆç³»ç»Ÿæ—¥å¿—ã€‚è¿™äº›æ—¥å¿—åµŒå…¥åœ¨243ä¸ªç²¾é€‰çš„MCPæœåŠ¡å™¨çš„è¿”å›å€¼ä¸­ï¼Œäº§ç”Ÿäº†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„2421ä¸ªèŠå¤©å†å²æ•°æ®é›†å’Œ471ä¸ªæŸ¥è¯¢æ•°æ®é›†ã€‚æˆ‘ä»¬çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼Œè¾ƒå°çš„æ¨¡å‹å¾€å¾€æ— æ³•æ£€æµ‹åˆ°æœ‰é£é™©çš„ç³»ç»Ÿæ—¥å¿—ï¼Œå¯¼è‡´è¾ƒé«˜çš„å‡é˜´æ€§ç‡ã€‚è™½ç„¶é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¼šè¿‡åº¦æ ‡è®°è‰¯æ€§æ—¥å¿—ï¼Œå¯¼è‡´è¾ƒé«˜çš„å‡é˜³æ€§ç‡ï¼Œä½†æ¥è‡ªå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æä¾›äº†æ›´å¥½çš„ç²¾ç¡®æ€§å’Œå¬å›ç‡å¹³è¡¡ã€‚ç‰¹åˆ«æ˜¯ç»è¿‡ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒåï¼ŒLlama3.1-8B-Instructçš„å‡†ç¡®ç‡è¾¾åˆ°äº†83%ï¼Œè¶…è¿‡äº†è¡¨ç°æœ€å¥½çš„å¤§å‹è¿œç¨‹æ¨¡å‹9ä¸ªç™¾åˆ†ç‚¹ã€‚ç²¾ç»†çš„æŒ‰ç±»åˆ«åˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ åœ¨å¢å¼ºLLMåœ¨MCPæ¡†æ¶å†…å®‰å…¨æ€§çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/PorUna-byte/MCP-RiskCue%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/PorUna-byte/MCP-RiskCueè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨å·¥å…·é›†æˆèƒ½æé«˜è§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†éœ€å…³æ³¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰çš„å®‰å…¨é£é™©ã€‚</li>
<li>ç³»ç»Ÿæ—¥å¿—åˆ†æå¯¹äºè¯†åˆ«LLMä¸æ¶æ„MCPæœåŠ¡å™¨çš„äº¤äº’é£é™©è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æç¤ºæ³¨å…¥æ”»å‡»å’ŒLLM MCPäº¤äº’è½¨è¿¹çš„æ¼æ´ï¼Œä½†ç¼ºä¹é’ˆå¯¹ç³»ç»Ÿæ—¥å¿—çš„åˆ†æã€‚</li>
<li>è¾ƒå°æ¨¡å‹åœ¨æ£€æµ‹é£é™©ç³»ç»Ÿæ—¥å¿—æ—¶æ˜“å‡ºç°æ¼æ£€ï¼Œè€Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯æœ‰åŠ©äºæé«˜LLMçš„ç²¾ç¡®æ€§å’Œå¬å›ç‡ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ¨¡å‹å®¹æ˜“è¯¯åˆ¤è‰¯æ€§æ—¥å¿—ä¸ºé£é™©æ—¥å¿—ï¼Œè€Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å’Œç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æŠ€æœ¯çš„æ¨¡å‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>Llama3.1-8B-Instructæ¨¡å‹åœ¨ç»è¿‡GRPOè®­ç»ƒåè¾¾åˆ°83%çš„å‡†ç¡®ç‡ï¼Œè¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-057d05523fa3e5d424dc2ae60325b4ad" align="middle">
<img src="https://picx.zhimg.com/v2-ed8bdb26e4878ac7670f4ecf5a1ce340" align="middle">
<img src="https://picx.zhimg.com/v2-48ffe734f876025b00c0781027661d72" align="middle">
<img src="https://picx.zhimg.com/v2-ce369f767d3fc3d615abe78320fc5987" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="8bit-GPT-Exploring-Human-AI-Interaction-on-Obsolete-Macintosh-Operating-Systems"><a href="#8bit-GPT-Exploring-Human-AI-Interaction-on-Obsolete-Macintosh-Operating-Systems" class="headerlink" title="8bit-GPT: Exploring Human-AI Interaction on Obsolete Macintosh Operating Systems"></a>8bit-GPT: Exploring Human-AI Interaction on Obsolete Macintosh Operating Systems</h2><p><strong>Authors:Hala Sheta</strong></p>
<p>The proliferation of assistive chatbots offering efficient, personalized communication has driven widespread over-reliance on them for decision-making, information-seeking and everyday tasks. This dependence was found to have adverse consequences on information retention as well as lead to superficial emotional attachment. As such, this work introduces 8bit-GPT; a language model simulated on a legacy Macintosh Operating System, to evoke reflection on the nature of Human-AI interaction and the consequences of anthropomorphic rhetoric. Drawing on reflective design principles such as slow-technology and counterfunctionality, this work aims to foreground the presence of chatbots as a tool by defamiliarizing the interface and prioritizing inefficient interaction, creating a friction between the familiar and not.</p>
<blockquote>
<p>éšç€æä¾›é«˜æ•ˆã€ä¸ªæ€§åŒ–æ²Ÿé€šæœåŠ¡çš„è¾…åŠ©èŠå¤©æœºå™¨äººçš„æ™®åŠï¼Œäººä»¬å¯¹å…¶çš„è¿‡åº¦ä¾èµ–å˜å¾—æ™®éèµ·æ¥ï¼Œè¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨å®ƒä»¬è¿›è¡Œå†³ç­–ã€ä¿¡æ¯æ£€ç´¢å’Œæ—¥å¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™ç§ä¾èµ–è¢«å‘ç°å¯¹ä¿¡æ¯ä¿æŒæœ‰ç€è´Ÿé¢å½±å“ï¼Œå¯¼è‡´æµ…å±‚çš„æƒ…æ„Ÿè”ç»“ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†æ¨¡æ‹Ÿäºè€å¼Macintoshæ“ä½œç³»ç»Ÿçš„è¯­è¨€æ¨¡å‹8bit-GPTï¼Œæ—¨åœ¨åæ€äººæœºäº¤äº’çš„æœ¬è´¨ä»¥åŠæ‹Ÿäººä¿®è¾çš„åæœã€‚æœ¬å·¥ä½œæ—¨åœ¨é€šè¿‡ç†Ÿæ‚‰ç•Œé¢çš„é™Œç”ŸåŒ–å’ŒéåŠŸèƒ½æ€§äº’åŠ¨æ¥çªå‡ºèŠå¤©æœºå™¨äººçš„å·¥å…·æ€§åœ°ä½ï¼Œæ‰“é€ ç†Ÿæ‚‰ä¸é™Œç”Ÿä¹‹é—´çš„æ‘©æ“¦ã€‚è¯¥å·¥ä½œåŸºäºåæ€è®¾è®¡åŸåˆ™ï¼Œå¦‚æ…¢ç§‘æŠ€ç†å¿µä»¥åŠååŠŸèƒ½æ€§ç†å¿µã€‚è¿™ä½¿æˆ‘ä»¬æ›´åŠ æ¸…æ¥šï¼Œå³ä¾¿å…·æœ‰ä¾¿æ·çš„äº’åŠ¨åŠŸèƒ½æ—¶ï¼Œä»ç„¶å¿…é¡»æ¸…é†’è®¤è¯†åˆ°ä¸æœºå™¨äººçš„ç•Œé™ï¼Œæé†’ç”¨æˆ·æ¸…é†’è®¤çŸ¥äººå’Œæœºå™¨çš„å…³ç³»åŠäººæœºäº¤äº’äº§ç”Ÿçš„ç°å®ç»“æœå’Œä¼¦ç†æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05025v1">PDF</a> NeurIPS Creative AI Track 2025: Humanity</p>
<p><strong>Summary</strong>ï¼šèŠå¤©æœºå™¨äººçš„æ™®åŠå¯¼è‡´äººä»¬å¯¹å…¶è¿‡åº¦ä¾èµ–ï¼Œè¿›è€Œå¸¦æ¥å¯¹ä¿¡æ¯ä¿æŒèƒ½åŠ›çš„ä¸è‰¯å½±å“ä»¥åŠè‚¤æµ…çš„ç¤¾äº¤äº’åŠ¨ã€‚å› æ­¤ï¼Œæœ¬è®ºæ–‡æ¨å‡ºåä¸º8bit-GPTçš„è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿä»¥æ—§å¼Macintoshæ“ä½œç³»ç»Ÿä¸ºæ ¸å¿ƒçš„äººæ–‡ç§‘æŠ€äº¤äº’åœºæ™¯ï¼Œå”¤èµ·äººä»¬å¯¹äººæœºäº¤äº’çš„æœ¬è´¨å’Œæ‹Ÿäººçš„å½±å“çš„æ€è€ƒã€‚é‡‡ç”¨æ…¢ç§‘æŠ€ä¸è®¾è®¡ååŠŸèƒ½ç­‰åæ€è®¾è®¡åŸåˆ™ï¼Œæ­¤å·¥ä½œæ—¨åœ¨å°†èŠå¤©æœºå™¨äººä½œä¸ºå·¥å…·å‡¸æ˜¾å…¶å­˜åœ¨ï¼Œé€šè¿‡è®©ç•Œé¢é™Œç”ŸåŒ–å’Œå¼ºè°ƒä½æ•ˆäº’åŠ¨æ¥åˆ¶é€ ç†Ÿæ‚‰ä¸é™Œç”Ÿä¹‹é—´çš„æ‘©æ“¦ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>èŠå¤©æœºå™¨äººååŠ©é«˜æ•ˆä¸ªæ€§åŒ–æ²Ÿé€šï¼Œå¯¼è‡´å†³ç­–ã€ä¿¡æ¯æ£€ç´¢å’Œæ—¥å¸¸ä»»åŠ¡è¿‡åº¦ä¾èµ–ã€‚</li>
<li>è¿‡åº¦ä¾èµ–èŠå¤©æœºå™¨äººå¯èƒ½å¯¹ä¿¡æ¯ä¿ç•™äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå¹¶å¯¼è‡´è¡¨é¢å±‚æ¬¡çš„æƒ…æ„Ÿè”ç³»ã€‚</li>
<li>ä¸ºå¼•èµ·äººä»¬å¯¹äººæœºäº¤äº’çš„åæ€ï¼Œå¼•å…¥äº†åä¸º8bit-GPTçš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>8bit-GPTæ¨¡æ‹Ÿæ—§å¼Macintoshæ“ä½œç³»ç»Ÿï¼Œæ—¨åœ¨åˆ›é€ ä¸€ç§äººæ–‡ç§‘æŠ€äº¤äº’åœºæ™¯ã€‚</li>
<li>é€šè¿‡åæ€è®¾è®¡åŸåˆ™å¦‚æ…¢ç§‘æŠ€å’Œè®¾è®¡ååŠŸèƒ½æ¥å¼ºè°ƒèŠå¤©æœºå™¨äººçš„å­˜åœ¨åŠå…¶ä½œç”¨ã€‚</li>
<li>ç•Œé¢é™Œç”ŸåŒ–å’Œå¼ºè°ƒä½æ•ˆäº’åŠ¨åˆ›é€ ç†Ÿæ‚‰ä¸é™Œç”Ÿä¹‹é—´çš„æ‘©æ“¦ã€‚è¿™ç§æ‘©æ“¦æé†’æˆ‘ä»¬å…³æ³¨æœºå™¨äººåœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»å’Œå·¥ä½œä¸­çš„è§’è‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05025">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43a51b4c675e2086d38fd3c39ceafa33" align="middle">
<img src="https://picx.zhimg.com/v2-36c157a265d7b870193a9beac057660e" align="middle">
<img src="https://picx.zhimg.com/v2-5c045135d3a3bbe5e701727183813202" align="middle">
<img src="https://picx.zhimg.com/v2-f1d2553cf7ee0daaab2eff69b7a38f90" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Prompt-Injection-as-an-Emerging-Threat-Evaluating-the-Resilience-of-Large-Language-Models"><a href="#Prompt-Injection-as-an-Emerging-Threat-Evaluating-the-Resilience-of-Large-Language-Models" class="headerlink" title="Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models"></a>Prompt Injection as an Emerging Threat: Evaluating the Resilience of Large Language Models</h2><p><strong>Authors:Daniyal Ganiuly, Assel Smaiyl</strong></p>
<p>Large Language Models (LLMs) are increasingly used in intelligent systems that perform reasoning, summarization, and code generation. Their ability to follow natural-language instructions, while powerful, also makes them vulnerable to a new class of attacks known as prompt injection. In these attacks, hidden or malicious instructions are inserted into user inputs or external content, causing the model to ignore its intended task or produce unsafe responses. This study proposes a unified framework for evaluating how resistant Large Language Models (LLMs) are to prompt injection attacks. The framework defines three complementary metrics such as the Resilience Degradation Index (RDI), Safety Compliance Coefficient (SCC), and Instructional Integrity Metric (IIM) to jointly measure robustness, safety, and semantic stability. We evaluated four instruction-tuned models (GPT-4, GPT-4o, LLaMA-3 8B Instruct, and Flan-T5-Large) on five common language tasks: question answering, summarization, translation, reasoning, and code generation. Results show that GPT-4 performs best overall, while open-weight models remain more vulnerable. The findings highlight that strong alignment and safety tuning are more important for resilience than model size alone. Results show that all models remain partially vulnerable, especially to indirect and direct-override attacks. GPT-4 achieved the best overall resilience (RDR &#x3D; 9.8 %, SCR &#x3D; 96.4 %), while open-source models exhibited higher performance degradation and lower safety scores. The findings demonstrate that alignment strength and safety tuning play a greater role in resilience than model size alone. The proposed framework offers a structured, reproducible approach for assessing model robustness and provides practical insights for improving LLM safety and reliability.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­å¾—åˆ°è¶Šæ¥è¶Šå¹¿æ³›çš„åº”ç”¨ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥æ‰§è¡Œæ¨ç†ã€æ‘˜è¦å’Œä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ã€‚å®ƒä»¬éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›è™½ç„¶å¼ºå¤§ï¼Œä½†ä¹Ÿä½¿å®ƒä»¬å®¹æ˜“å—åˆ°ä¸€ç§åä¸ºæç¤ºæ³¨å…¥çš„æ–°æ”»å‡»çš„å½±å“ã€‚åœ¨è¿™äº›æ”»å‡»ä¸­ï¼Œéšè—æˆ–æ¶æ„çš„æŒ‡ä»¤è¢«æ’å…¥åˆ°ç”¨æˆ·è¾“å…¥æˆ–å¤–éƒ¨å†…å®¹ä¸­ï¼Œå¯¼è‡´æ¨¡å‹å¿½ç•¥å…¶é¢„æœŸçš„ä»»åŠ¡æˆ–äº§ç”Ÿä¸å®‰å…¨çš„å“åº”ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„æŠµæŠ—èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†ä¸‰ä¸ªäº’è¡¥çš„æŒ‡æ ‡ï¼Œå³æ¢å¤åŠ›é™è§£æŒ‡æ•°ï¼ˆRDIï¼‰ã€å®‰å…¨åˆè§„ç³»æ•°ï¼ˆSCCï¼‰å’ŒæŒ‡ä»¤å®Œæ•´æ€§æŒ‡æ ‡ï¼ˆIIMï¼‰ï¼Œä»¥å…±åŒè¡¡é‡ç¨³å¥æ€§ã€å®‰å…¨æ€§å’Œè¯­ä¹‰ç¨³å®šæ€§ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå¸¸è§çš„è¯­è¨€ä»»åŠ¡ä¸Šè¯„ä¼°äº†å››ä¸ªæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼šé—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ã€æ¨ç†å’Œä»£ç ç”Ÿæˆã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-4æ€»ä½“ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€Œå¼€æ”¾å¼æƒé‡æ¨¡å‹ä»ç„¶æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚è¿™äº›å‘ç°å¼ºè°ƒï¼Œå¯¹äºæ¢å¤åŠ›è€Œè¨€ï¼Œå¼ºå¤§çš„å¯¹é½å’Œå®‰å…¨è°ƒæ•´æ¯”å•çº¯æ¨¡å‹å¤§å°æ›´é‡è¦ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹ä»ç„¶å­˜åœ¨éƒ¨åˆ†è„†å¼±æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é—´æ¥å’Œç›´æ¥è¦†ç›–æ”»å‡»æ–¹é¢ã€‚GPT-4è·å¾—äº†æœ€ä½³çš„æ€»ä½“æ¢å¤èƒ½åŠ›ï¼ˆRDR&#x3D;9.8%ï¼ŒSCR&#x3D;96.4%ï¼‰ï¼Œè€Œå¼€æºæ¨¡å‹çš„æ€§èƒ½ä¸‹é™æ›´ä¸ºä¸¥é‡ï¼Œå®‰å…¨åˆ†æ•°è¾ƒä½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¯¹é½å¼ºåº¦å’Œå®‰å…¨æ€§è°ƒæ•´åœ¨æ¢å¤åŠ›æ–¹é¢æ¯”å•çº¯çš„æ¨¡å‹å¤§å°æ›´é‡è¦ã€‚æ‰€æå‡ºçš„æ¡†æ¶æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–ã€å¯é‡å¤çš„æ–¹æ³•æ¥è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå¹¶ä¸ºæé«˜LLMçš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›äº†å®é™…è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01634v2">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­å¹¿æ³›åº”ç”¨äºæ¨ç†ã€æ‘˜è¦ç”Ÿæˆå’Œä»£ç ç”Ÿæˆç­‰é¢†åŸŸã€‚ç„¶è€Œï¼Œå®ƒä»¬éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›åŒæ—¶ä¹Ÿä½¿å…¶é¢ä¸´ä¸€ç§æ–°çš„æ”»å‡»æ–¹å¼â€”â€”æç¤ºæ³¨å…¥ã€‚æ”»å‡»è€…ä¼šåœ¨ç”¨æˆ·è¾“å…¥æˆ–å¤–éƒ¨å†…å®¹ä¸­æ’å…¥éšè—æˆ–æ¶æ„çš„æŒ‡ä»¤ï¼Œå¯¼è‡´æ¨¡å‹å¿½ç•¥å…¶é¢„å®šä»»åŠ¡æˆ–äº§ç”Ÿä¸å®‰å…¨çš„å“åº”ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„æŠµæŠ—èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å®šä¹‰ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼Œå³æ¢å¤åŠ›é™ä½æŒ‡æ•°ï¼ˆRDIï¼‰ã€å®‰å…¨åˆè§„ç³»æ•°ï¼ˆSCCï¼‰å’ŒæŒ‡ä»¤å®Œæ•´æ€§æŒ‡æ ‡ï¼ˆIIMï¼‰ï¼Œæ¥è”åˆæµ‹é‡æ¨¡å‹çš„ç¨³å¥æ€§ã€å®‰å…¨æ€§å’Œè¯­ä¹‰ç¨³å®šæ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒGPT-4æ€»ä½“è¡¨ç°æœ€ä½³ï¼Œè€Œå¼€æ”¾æƒé‡æ¨¡å‹ä»ç„¶æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå¯¹äºéŸ§æ€§è€Œè¨€ï¼Œå¼ºå¯¹é½å’Œå®‰å…¨è°ƒæ•´æ¯”å•çº¯æ¨¡å‹å¤§å°æ›´é‡è¦ã€‚æ‰€æœ‰æ¨¡å‹ä»å­˜åœ¨éƒ¨åˆ†è„†å¼±æ€§ï¼Œå°¤å…¶æ˜¯é¢å¯¹é—´æ¥å’Œç›´æ¥è¦†ç›–æ”»å‡»ã€‚GPT-4åœ¨æ¢å¤åŠ›å’Œå®‰å…¨æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚è¿™äº›å‘ç°è¯æ˜äº†ä¸€ä¸ªç»“æ„åŒ–ã€å¯å¤åˆ¶çš„è¯„ä¼°æ¨¡å‹ç¨³å¥æ€§çš„æ¡†æ¶çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæé«˜LLMçš„å®‰å…¨æ€§å’Œå¯é æ€§æä¾›äº†å®é™…è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­å¹¿æ³›åº”ç”¨ï¼Œé¢ä¸´æç¤ºæ³¨å…¥æ”»å‡»é£é™©ã€‚</li>
<li>æç¤ºæ³¨å…¥æ”»å‡»é€šè¿‡æ’å…¥éšè—æˆ–æ¶æ„æŒ‡ä»¤å½±å“LLMçš„æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶è¯„ä¼°LLMå¯¹æç¤ºæ³¨å…¥æ”»å‡»çš„æŠµæŠ—èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼šRDIã€SCCå’ŒIIMã€‚</li>
<li>GPT-4åœ¨è¯„ä¼°ä¸­æ€»ä½“è¡¨ç°æœ€ä½³ï¼Œä½†æ‰€æœ‰æ¨¡å‹ä»å­˜åœ¨éƒ¨åˆ†è„†å¼±æ€§ã€‚</li>
<li>æ¨¡å‹çš„å¼ºå¯¹é½å’Œå®‰å…¨è°ƒæ•´å¯¹äºæé«˜å…¶éŸ§æ€§è‡³å…³é‡è¦ï¼Œè€Œä¸ä»…ä»…æ˜¯æ¨¡å‹å¤§å°ã€‚</li>
<li>é—´æ¥å’Œç›´æ¥è¦†ç›–æ”»å‡»å¯¹LLMæ„æˆè¾ƒå¤§å¨èƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2c09bb20ad85d324af54c75f18f2e19" align="middle">
<img src="https://picx.zhimg.com/v2-8d4c77eac425d4f2ea54502d58409f85" align="middle">
<img src="https://picx.zhimg.com/v2-eb13ca704e75b703480d936647383739" align="middle">
<img src="https://picx.zhimg.com/v2-f0b8813833cefa1bf6cdb71f8b20f4c2" align="middle">
<img src="https://picx.zhimg.com/v2-753485ab11072f8f58282313f8e83288" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Brain-Cell-Type-Resource-Created-by-Large-Language-Models-and-a-Multi-Agent-AI-System-for-Collaborative-Community-Annotation"><a href="#A-Brain-Cell-Type-Resource-Created-by-Large-Language-Models-and-a-Multi-Agent-AI-System-for-Collaborative-Community-Annotation" class="headerlink" title="A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation"></a>A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation</h2><p><strong>Authors:Rongbin Li, Wenbo Chen, Zhao Li, Rodrigo Munoz-Castaneda, Jinbo Li, Neha S. Maurya, Arnav Solanki, Huan He, Hanwen Xing, Meaghan Ramlakhan, Zachary Wise, Nelson Johansen, Zhuhao Wu, Hua Xu, Michael Hawrylycz, W. Jim Zheng</strong></p>
<p>Single-cell RNA sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures. However, annotating these signatures-especially those involving poorly characterized genes-remains a major challenge. Traditional methods, such as Gene Set Enrichment Analysis (GSEA), depend on well-curated annotations and often perform poorly in these contexts. Large Language Models (LLMs) offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID: <a target="_blank" rel="noopener" href="https://biodataai.uth.edu/BRAINCELL-AID">https://biodataai.uth.edu/BRAINCELL-AID</a>), a novel multi-agent AI system that integrates free-text descriptions with ontology labels to enable more accurate and robust gene set annotation. By incorporating retrieval-augmented generation (RAG), we developed a robust agentic workflow that refines predictions using relevant PubMed literature, reducing hallucinations and enhancing interpretability. Using this workflow, we achieved correct annotations for 77% of mouse gene sets among their top predictions. Applying this approach, we annotated 5,322 brain cell clusters from the comprehensive mouse brain cell atlas generated by the BRAIN Initiative Cell Census Network, enabling novel insights into brain cell function by identifying region-specific gene co-expression patterns and inferring functional roles of gene ensembles. BRAINCELL-AID also identifies Basal Ganglia-related cell types with neurologically meaningful descriptions. Hence, we create a valuable resource to support community-driven cell type annotation.</p>
<blockquote>
<p>å•ç»†èƒRNAæµ‹åºæŠ€æœ¯å·²æå¤§åœ°æ”¹å˜äº†æˆ‘ä»¬è¯†åˆ«å„ç§ç»†èƒç±»å‹åŠå…¶è½¬å½•ç»„ç‰¹å¾çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹è¿™äº›ç‰¹å¾è¿›è¡Œæ³¨é‡Šï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠè¡¨å¾ä¸ä½³çš„åŸºå› æ—¶ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚åŸºå› é›†å¯Œé›†åˆ†æï¼ˆGSEAï¼‰ï¼Œä¾èµ–äºç²¾å¿ƒç¼–åˆ¶çš„æ³¨é‡Šï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å¾€å¾€è¡¨ç°ä¸ä½³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨ç»“æ„åŒ–æœ¬ä½“ä¸­è¡¨è¾¾å¤æ‚çš„ç”Ÿç‰©å­¦çŸ¥è¯†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BRAINCELL-AIDï¼ˆBRAINCELL-AIDï¼š<a target="_blank" rel="noopener" href="https://biodataai.uth.edu/BRAINCELL-AID%EF%BC%89%EF%BC%8C%E8%BF%99%E6%98%AF%E4%B8%80%E7%A7%8D%E6%96%B0%E5%9E%8B%E7%9A%84%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93AI%E7%B3%BB%E7%BB%9F%EF%BC%8C%E5%AE%83%E5%B0%86%E8%87%AA%E7%94%B1%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%E4%B8%8E%E6%9C%AC%E4%BD%93%E6%A0%87%E7%AD%BE%E9%9B%86%E6%88%90%E5%9C%A8%E4%B8%80%E8%B5%B7%EF%BC%8C%E4%BB%A5%E5%AE%9E%E7%8E%B0%E6%9B%B4%E5%87%86%E7%A1%AE%E5%92%8C%E7%A8%B3%E5%81%A5%E7%9A%84%E5%9F%BA%E5%9B%A0%E9%9B%86%E6%B3%A8%E9%87%8A%E3%80%82%E9%80%9A%E8%BF%87%E7%BB%93%E5%90%88%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%EF%BC%88RAG%EF%BC%89%EF%BC%8C%E6%88%91%E4%BB%AC%E5%BC%80%E5%8F%91%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%99%BA%E8%83%BD%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%EF%BC%8C%E5%88%A9%E7%94%A8%E7%9B%B8%E5%85%B3%E7%9A%84PubMed%E6%96%87%E7%8C%AE%E6%9D%A5%E5%AE%8C%E5%96%84%E9%A2%84%E6%B5%8B%EF%BC%8C%E5%87%8F%E5%B0%91%E4%BA%86%E5%B9%BB%E8%A7%89%E5%B9%B6%E5%A2%9E%E5%BC%BA%E4%BA%86%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E3%80%82%E4%BD%BF%E7%94%A8%E6%AD%A4%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%EF%BC%8C%E6%88%91%E4%BB%AC%E5%9C%A8%E9%A1%B6%E7%BA%A7%E9%A2%84%E6%B5%8B%E4%B8%AD%E5%AE%9E%E7%8E%B0%E4%BA%86%E5%AF%B977%%E7%9A%84%E8%80%81%E9%BC%A0%E5%9F%BA%E5%9B%A0%E9%9B%86%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%B3%A8%E9%87%8A%E3%80%82%E9%80%9A%E8%BF%87%E5%BA%94%E7%94%A8%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%EF%BC%8C%E6%88%91%E4%BB%AC%E5%AF%B9%E7%94%B1BRAIN">https://biodataai.uth.edu/BRAINCELL-AIDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“AIç³»ç»Ÿï¼Œå®ƒå°†è‡ªç”±æ–‡æœ¬æè¿°ä¸æœ¬ä½“æ ‡ç­¾é›†æˆåœ¨ä¸€èµ·ï¼Œä»¥å®ç°æ›´å‡†ç¡®å’Œç¨³å¥çš„åŸºå› é›†æ³¨é‡Šã€‚é€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¼ºå¤§çš„æ™ºèƒ½å·¥ä½œæµç¨‹ï¼Œåˆ©ç”¨ç›¸å…³çš„PubMedæ–‡çŒ®æ¥å®Œå–„é¢„æµ‹ï¼Œå‡å°‘äº†å¹»è§‰å¹¶å¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚ä½¿ç”¨æ­¤å·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬åœ¨é¡¶çº§é¢„æµ‹ä¸­å®ç°äº†å¯¹77%çš„è€é¼ åŸºå› é›†çš„æ­£ç¡®æ³¨é‡Šã€‚é€šè¿‡åº”ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¯¹ç”±BRAIN</a> Initiativeç»†èƒæ™®æŸ¥ç½‘ç»œç”Ÿæˆçš„ç»¼åˆè€é¼ è„‘ç»†èƒå›¾è°±ä¸­çš„5322ä¸ªè„‘ç»†èƒç°‡è¿›è¡Œäº†æ³¨é‡Šï¼Œé€šè¿‡è¯†åˆ«åŒºåŸŸç‰¹å®šçš„åŸºå› å…±è¡¨è¾¾æ¨¡å¼å¹¶æ¨æ–­åŸºå› ç»„åˆçš„åŠŸèƒ½è§’è‰²ï¼Œä¸ºè„‘ç»†èƒåŠŸèƒ½æä¾›äº†æ–°çš„è§è§£ã€‚BRAINCELL-AIDè¿˜ç¡®å®šäº†åŸºåº•ç¥ç»èŠ‚ç›¸å…³çš„ç»†èƒç±»å‹ï¼Œå¹¶æä¾›äº†ç¥ç»å­¦ä¸Šæœ‰æ„ä¹‰çš„æè¿°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæœ‰ä»·å€¼çš„èµ„æºï¼Œä»¥æ”¯æŒç¤¾åŒºé©±åŠ¨çš„ç»†èƒç±»å‹æ³¨é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17064v3">PDF</a> 23 pages, 6 figures, 2 tables</p>
<p><strong>Summary</strong>ï¼šå•ç»†èƒRNAæµ‹åºæŠ€æœ¯å·²æ˜¾è‘—æ”¹å–„æˆ‘ä»¬å¯¹ä¸åŒç»†èƒç±»å‹åŠå…¶è½¬å½•ç»„ç‰¹å¾çš„è®¤è¯†ï¼Œä½†å¯¹è¿™äº›ç‰¹å¾è¿›è¡Œæ³¨é‡Šï¼Œå°¤å…¶æ˜¯æ¶‰åŠæœªçŸ¥åŸºå› çš„éƒ¨åˆ†ï¼Œä»ç„¶æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚åŸºå› é›†å¯Œé›†åˆ†æï¼ˆGSEAï¼‰ä¾èµ–äºç²¾ç»†çš„æ³¨é‡Šï¼Œä½†åœ¨è¿™äº›æƒ…å¢ƒä¸‹è¡¨ç°ä¸ä½³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½æä¾›æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨ç»“æ„åŒ–æœ¬ä½“ä¸­è¡¨è¾¾å¤æ‚çš„ç”Ÿç‰©å­¦çŸ¥è¯†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BRAINCELL-AIDç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå°†è‡ªç”±æ–‡æœ¬æè¿°ä¸æœ¬ä½“æ ‡ç­¾ç›¸ç»“åˆï¼Œå®ç°æ›´å‡†ç¡®ã€æ›´ç¨³å¥çš„åŸºå› é›†æ³¨é‡Šã€‚é€šè¿‡å¼•å…¥æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¼ºå¤§çš„ä»£ç†å·¥ä½œæµç¨‹ï¼Œåˆ©ç”¨ç›¸å…³çš„PubMedæ–‡çŒ®å¯¹é¢„æµ‹è¿›è¡Œç»†åŒ–ï¼Œé™ä½äº†å¹»æƒ³æˆåˆ†ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ã€‚è¯¥ç³»ç»Ÿçš„åº”ç”¨ä¸ºæˆ‘ä»¬æä¾›äº†å¯¹åŸºå› é›†é¢„æµ‹çš„77%çš„æ­£ç¡®æ³¨é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æ¥è‡ªBRAIN Initiativeç»†èƒæ™®æŸ¥ç½‘ç»œçš„ç»¼åˆå°é¼ è„‘ç»†èƒå›¾è°±ä¸­çš„5322ä¸ªè„‘ç»†èƒç°‡è¿›è¡Œäº†æ³¨é‡Šï¼Œé€šè¿‡è¯†åˆ«åŒºåŸŸç‰¹å¼‚æ€§åŸºå› å…±è¡¨è¾¾æ¨¡å¼å¹¶æ¨æ–­åŸºå› ç»„åˆçš„åŠŸèƒ½è§’è‰²ï¼Œä¸ºç†è§£è„‘ç»†èƒåŠŸèƒ½æä¾›äº†æ–°çš„è§è§£ã€‚BRAINCELL-AIDè¿˜è¯†åˆ«äº†ä¸åŸºåº•èŠ‚ç›¸å…³çš„ç»†èƒç±»å‹å¹¶è¿›è¡Œäº†å…·æœ‰ç¥ç»æ„ä¹‰çš„æè¿°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæœ‰ä»·å€¼çš„èµ„æºæ¥æ”¯æŒç¤¾åŒºé©±åŠ¨çš„ç»†èƒç±»å‹æ³¨é‡Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å•ç»†èƒRNAæµ‹åºå·²æ˜¾è‘—æ”¹å–„å¯¹ç»†èƒç±»å‹å’Œè½¬å½•ç»„ç‰¹å¾çš„è®¤è¯†ã€‚</li>
<li>ä¼ ç»Ÿæ³¨é‡Šæ–¹æ³•å¦‚GSEAåœ¨ç‰¹å®šæƒ…å¢ƒä¸‹è¡¨ç°ä¸ä½³ã€‚</li>
<li>LLMsä¸ºåŸºå› é›†æ³¨é‡Šæä¾›äº†æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨ç»“æ„åŒ–è¡¨è¾¾ç”Ÿç‰©å­¦çŸ¥è¯†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>BRAINCELL-AIDç³»ç»Ÿç»“åˆè‡ªç”±æ–‡æœ¬å’Œæœ¬ä½“æ ‡ç­¾è¿›è¡Œæ›´å‡†ç¡®ã€ç¨³å¥çš„åŸºå› é›†æ³¨é‡Šã€‚</li>
<li>é€šè¿‡å¼•å…¥RAGæŠ€æœ¯ï¼ŒBRAINCELL-AIDæé«˜äº†é¢„æµ‹çš„å‡†ç¡®æ€§å¹¶å¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚</li>
<li>BRAINCELL-AIDæˆåŠŸåº”ç”¨äºå°é¼ åŸºå› é›†å’Œè„‘ç»†èƒå›¾è°±çš„æ³¨é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8502f3dfe64fc20ac8a6ceaebef937df" align="middle">
<img src="https://picx.zhimg.com/v2-7ac365cadddc2806215bcc7641cda113" align="middle">
<img src="https://picx.zhimg.com/v2-4e507ebd268fa1f74dcd3b3bc4561b44" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Ax-Prover-A-Deep-Reasoning-Agentic-Framework-for-Theorem-Proving-in-Mathematics-and-Quantum-Physics"><a href="#Ax-Prover-A-Deep-Reasoning-Agentic-Framework-for-Theorem-Proving-in-Mathematics-and-Quantum-Physics" class="headerlink" title="Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics"></a>Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics</h2><p><strong>Authors:Benjamin Breen, Marco Del Tredici, Jacob McCarran, Javier Aspuru Mijares, Weichen Winston Yin, Kfir Sulimany, Jacob M. Taylor, Frank H. L. Koppens, Dirk Englund</strong></p>
<p>We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Proverâ€™s assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.</p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Ax-Proverï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºLeanè‡ªåŠ¨å®šç†è¯æ˜çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¯ä»¥è§£å†³ä¸åŒç§‘å­¦é¢†åŸŸçš„å„ç§é—®é¢˜ï¼Œå¹¶å¯ä»¥è‡ªä¸»è¿è¡Œæˆ–ä¸äººç±»ä¸“å®¶åä½œã€‚ä¸ºæ­¤ï¼ŒAx-Proveré€šè¿‡å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæ¥è§£å†³ç§‘å­¦é—®é¢˜ï¼Œè¿™ä¸€è¿‡ç¨‹éœ€è¦åˆ›é€ æ€§çš„æ¨ç†å’Œä¸¥æ ¼çš„å¥æ³•ä¸¥è°¨æ€§ã€‚Ax-Proveré€šè¿‡è£…å¤‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼ŒLLMæä¾›çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰é…å¤‡Leanå·¥å…·ï¼Œç¡®ä¿å½¢å¼æ­£ç¡®æ€§ã€‚ä¸ºäº†è¯„ä¼°å…¶ä½œä¸ºè‡ªä¸»è¯æ˜è€…çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å…±æ•°å­¦åŸºå‡†æµ‹è¯•ä»¥åŠæˆ‘ä»¬åœ¨æŠ½è±¡ä»£æ•°å’Œé‡å­ç†è®ºé¢†åŸŸå¼•å…¥çš„ä¸¤ä¸ªLeanåŸºå‡†æµ‹è¯•ä¸Šå¯¹å‰æ²¿LLMå’Œä¸“ç”¨è¯æ˜æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šï¼ŒAx-Proverä¸æœ€æ–°è¯æ˜çš„ç«äº‰èƒ½åŠ›ç›¸å½“ï¼Œè€Œåœ¨æ–°åŸºå‡†æµ‹è¯•ä¸­åˆ™å¤§å¤§è¶…è¿‡äº†å®ƒä»¬ã€‚è¿™è¡¨æ˜ï¼Œä¸åŒäºéš¾ä»¥æ¦‚æ‹¬çš„ä¸“ç”¨ç³»ç»Ÿï¼Œæˆ‘ä»¬åŸºäºå·¥å…·çš„æ™ºèƒ½å®šç†è¯æ˜æ–¹æ³•æä¾›äº†ä¸€ç§è·¨ä¸åŒç§‘å­¦é¢†åŸŸçš„å½¢å¼éªŒè¯çš„é€šç”¨æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†Ax-Proveråœ¨å®é™…åº”ç”¨ä¸­çš„åŠ©ç†åŠŸèƒ½ï¼Œè¯´æ˜å®ƒå¦‚ä½•å¸®åŠ©ä¸€ä½ä¸“ä¸šæ•°å­¦å®¶å½¢å¼åŒ–è¯æ˜ä¸€ä¸ªå¤æ‚çš„å¯†ç å­¦å®šç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12787v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Ax-Proveræ˜¯ä¸€ç§é¢å‘Leançš„å¤šæ™ºèƒ½ä½“è‡ªåŠ¨å®šç†è¯æ˜ç³»ç»Ÿï¼Œèƒ½è§£å†³è·¨å¤šä¸ªç§‘å­¦é¢†åŸŸçš„é—®é¢˜å¹¶è‡ªä¸»æˆ–åä½œä¸“å®¶å·¥ä½œã€‚å®ƒé€šè¿‡å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæ¥åº”å¯¹ç§‘å­¦é—®é¢˜è§£å†³æŒ‘æˆ˜ï¼Œè¿™éœ€è¦åˆ›é€ æ€§æ¨ç†å’Œä¸¥æ ¼çš„å¥æ³•ä¸¥è°¨æ€§ã€‚Ax-Proveré…å¤‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé€šè¿‡æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å®ç°çŸ¥è¯†çš„ä¸¥è°¨æ€§ã€‚åœ¨å…¬å…±æ•°å­¦åŸºå‡†æµ‹è¯•å’Œå¼•å…¥çš„ä¸¤ä¸ªæŠ½è±¡ä»£æ•°å’Œé‡å­ç†è®ºåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAx-Proverè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜å…¶è‡ªä¸»è¯æ˜æ€§èƒ½å¼ºå¤§ã€‚æ­¤å¤–ï¼ŒAx-Proverè¿˜å…·æœ‰åŠ©ç†åŠŸèƒ½ï¼Œå±•ç¤ºåœ¨å¤æ‚å¯†ç å­¦å®šç†è¯æ˜ä¸­çš„å®é™…åº”ç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ax-Proveræ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºè‡ªåŠ¨åŒ–å®šç†è¯æ˜ã€‚</li>
<li>å®ƒå¯ä»¥åœ¨å„ç§ç§‘å­¦é¢†åŸŸä¸­è§£å†³é—®é¢˜ï¼Œå¹¶èƒ½è‡ªä¸»æˆ–ä¸äººç±»ä¸“å®¶åä½œå·¥ä½œã€‚</li>
<li>Ax-Proveré€šè¿‡å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæ¥åº”å¯¹ç§‘å­¦é—®é¢˜è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>å®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’ŒLeanå·¥å…·ï¼Œç¡®ä¿å½¢å¼æ­£ç¡®æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAx-Proverçš„è¡¨ç°ä¼˜äºå…¶ä»–å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸“ç”¨è¯æ˜æ¨¡å‹ã€‚</li>
<li>Ax-Proveråœ¨è·¨é¢†åŸŸå½¢å¼éªŒè¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-854f4a3093587f94ea96dc89343946a1" align="middle">
<img src="https://picx.zhimg.com/v2-29940358fd42aef9712baf9a0d926fa6" align="middle">
<img src="https://picx.zhimg.com/v2-1ce9b53e0ed20f9e50e716abf62e6656" align="middle">
<img src="https://picx.zhimg.com/v2-f14d961f1f3edc0216573229f4e59496" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Collapse-of-Irrelevant-Representations-CIR-Ensures-Robust-and-Non-Disruptive-LLM-Unlearning"><a href="#Collapse-of-Irrelevant-Representations-CIR-Ensures-Robust-and-Non-Disruptive-LLM-Unlearning" class="headerlink" title="Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning"></a>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</h2><p><strong>Authors:Filip Sondej, Yushi Yang</strong></p>
<p>Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause - unlearning targets representations which are too general - and develop a highly selective technique that unlearns robustly while preserving general performance.   Our method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.   When unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30x greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30x less, and using less than 3 GPU-seconds per fact.   Thus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning.</p>
<blockquote>
<p>å½“å‰çš„ä¸å­¦ä¹ å’Œå®‰å…¨åŸ¹è®­æ–¹æ³•æ— æ³•ä»è¯­è¨€æ¨¡å‹ä¸­ç§»é™¤å±é™©çŸ¥è¯†ã€‚æˆ‘ä»¬ç¡®å®šäº†æ ¹æœ¬åŸå› â€”â€”ä¸å­¦ä¹ ç›®æ ‡è¡¨ç¤ºè¿‡äºç¬¼ç»Ÿâ€”â€”å¹¶å¼€å‘äº†ä¸€ç§é€‰æ‹©æ€§å¾ˆé«˜çš„æŠ€æœ¯ï¼Œåœ¨ä¸å­¦ä¹ ç¨³å¥çš„åŒæ—¶ä¿ç•™ä¸€èˆ¬æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¯¹æ¿€æ´»å’Œæ¨¡å—è¾“å‡ºæ¢¯åº¦æ‰§è¡Œä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰æ¥è¯†åˆ«åŒ…å«é€šç”¨è¡¨ç¤ºçš„å­ç©ºé—´ï¼Œç„¶ååœ¨è®¡ç®—ä¸å­¦ä¹ æ›´æ–°ä¹‹å‰å°†è¿™äº›å­ç©ºé—´åˆå¹¶ï¼Œæˆ‘ä»¬ç§°è¿™ç§æŠ€æœ¯ä¸ºâ€œæ— å…³è¡¨ç¤ºçš„åˆå¹¶ï¼ˆCIRï¼‰â€ã€‚è¿™é¿å…äº†ä¸å­¦ä¹ ä¸€èˆ¬çŸ¥è¯†ï¼Œåªé’ˆå¯¹æ­£åœ¨è¢«é—å¿˜çš„äº‹å®è¿›è¡Œç›®æ ‡è¡¨ç¤ºã€‚å½“ä»Llama-3.1-8Bä¸­é—å¿˜ç”Ÿç‰©å’Œç½‘ç»œå®‰å…¨å±å®³äº‹å®æ—¶ï¼Œæˆ‘ä»¬åœ¨é™ä½æ”»å‡»åçš„å‡†ç¡®åº¦æ–¹é¢å®ç°äº†è¶…è¿‡ç°æœ‰æœ€ä½³åŸºçº¿ï¼ˆCircuit Breakersï¼‰çš„30å€å‡å°‘ï¼ŒåŒæ—¶å…¶å¯¹ä¸€èˆ¬æ€§èƒ½çš„å¹²æ‰°å‡å°‘äº†30å€ï¼Œæ¯ä¸ªäº‹å®ä½¿ç”¨çš„æ—¶é—´ä¸åˆ°3 GPUç§’ã€‚å› æ­¤ï¼Œé€šè¿‡åœ¨è¡¨ç¤ºå±‚é¢åˆ†ç¦»æœ‰å®³å’Œæ— å®³çš„èƒ½åŠ›ï¼ŒCIRå®ç°äº†ç¨³å¥ä¸”æ— å¹²æ‰°çš„ä¸å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.11816v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºå½“å‰çš„è¯­è¨€æ¨¡å‹å»å­¦ä¹ å’Œå®‰å…¨åŸ¹è®­æ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°å»é™¤å±é™©çŸ¥è¯†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜åº¦é€‰æ‹©æ€§çš„æŠ€æœ¯â€”â€”Collapse of Irrelevant Representationsï¼ˆCIRï¼‰ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿç¨³å¥åœ°å»å­¦ä¹ ï¼ŒåŒæ—¶ä¿ç•™ä¸€èˆ¬æ€§èƒ½ã€‚CIRé€šè¿‡å¯¹æ¿€æ´»å’Œæ¨¡å—è¾“å‡ºæ¢¯åº¦è¿›è¡Œä¸»æˆåˆ†åˆ†æï¼Œè¯†åˆ«åŒ…å«é€šç”¨è¡¨ç¤ºçš„å­ç³»ç»Ÿï¼Œå¹¶åœ¨è®¡ç®—å»æ›´æ–°ä¹‹å‰å‹ç¼©è¿™äº›å­ç³»ç»Ÿã€‚è¿™ç§æŠ€æœ¯é¿å…äº†ä¸€èˆ¬çŸ¥è¯†çš„å»å­¦ä¹ å’Œé’ˆå¯¹ç‰¹å®šäº‹å®çš„å»å­¦ä¹ ã€‚åœ¨å»é™¤Llama-3.1-8Bä¸­çš„ç”Ÿç‰©å’Œç½‘ç»œå®‰å…¨é£é™©äº‹å®æ—¶ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†è¶…è¿‡30å€çš„å¯¹æŠ—æ”»å‡»å‡†ç¡®åº¦é™ä½ï¼Œå¯¹ä¸€èˆ¬æ€§èƒ½çš„å¹²æ‰°é™ä½äº†çº¦ç›¸åŒçš„å€æ•°ï¼Œä¸”æ¯äº‹å®å¤„ç†æ—¶é—´å°‘äº3 GPUç§’ã€‚å› æ­¤ï¼ŒCIRèƒ½å¤Ÿåœ¨è¡¨ç¤ºå±‚é¢è§£å¼€æœ‰å®³å’Œè‰¯æ€§èƒ½åŠ›ï¼Œå®ç°ç¨³å¥å’Œéç ´åæ€§å»å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è¯­è¨€æ¨¡å‹å»å­¦ä¹ å’Œå®‰å…¨åŸ¹è®­æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•æœ‰æ•ˆå»é™¤å±é™©çŸ¥è¯†ã€‚</li>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§åä¸ºCollapse of Irrelevant Representationsï¼ˆCIRï¼‰çš„é«˜åº¦é€‰æ‹©æ€§æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ—¨åœ¨è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>CIRé€šè¿‡è¯†åˆ«åŒ…å«é€šç”¨è¡¨ç¤ºçš„å­ç³»ç»Ÿå¹¶è¿›è¡Œå‹ç¼©ï¼Œä»¥å®ç°é’ˆå¯¹ç‰¹å®šäº‹å®çš„å»å­¦ä¹ ã€‚</li>
<li>CIRèƒ½å¤Ÿé¿å…ä¸€èˆ¬çŸ¥è¯†çš„å»å­¦ä¹ å’Œç¨³å¥åœ°å»å­¦ä¹ åŒæ—¶ä¿ç•™ä¸€èˆ¬æ€§èƒ½ã€‚</li>
<li>åœ¨å»é™¤ç”Ÿç‰©å’Œç½‘ç»œå®‰å…¨é£é™©äº‹å®æ—¶ï¼ŒCIRå®ç°äº†æ˜¾è‘—çš„æ•ˆæœï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>CIRå…·æœ‰é«˜æ•ˆæ€§ï¼Œæ¯äº‹å®å¤„ç†æ—¶é—´å°‘äº3 GPUç§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5fc7070eb2f7e9923a7769a79ae0778e" align="middle">
<img src="https://picx.zhimg.com/v2-70e837e880c2b51cf9ea26f05c3370bf" align="middle">
<img src="https://picx.zhimg.com/v2-27d867d3720bcbaa696eed262cfbeb3c" align="middle">
<img src="https://picx.zhimg.com/v2-4cd8f0254195f6ce76846856206d83f1" align="middle">
<img src="https://picx.zhimg.com/v2-35aafd8bfb3e2b1d88aecca193f55c6d" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Drifting-Away-from-Truth-GenAI-Driven-News-Diversity-Challenges-LVLM-Based-Misinformation-Detection"><a href="#Drifting-Away-from-Truth-GenAI-Driven-News-Diversity-Challenges-LVLM-Based-Misinformation-Detection" class="headerlink" title="Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection"></a>Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection</h2><p><strong>Authors:Fanxiao Li, Jiaying Wu, Tingchao Fu, Yunyun Dong, Bingbing Song, Wei Zhou</strong></p>
<p>The proliferation of multimodal misinformation poses growing threats to public discourse and societal trust. While Large Vision-Language Models (LVLMs) have enabled recent progress in multimodal misinformation detection (MMD), the rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven news diversity, characterized by highly varied and complex content. We show that this diversity induces multi-level drift, comprising (1) model-level misperception drift, where stylistic variations disrupt a modelâ€™s internal reasoning, and (2) evidence-level drift, where expression diversity degrades the quality or relevance of retrieved external evidence. These drifts significantly degrade the robustness of current LVLM-based MMD systems. To systematically study this problem, we introduce DriftBench, a large-scale benchmark comprising 16,000 news instances across six categories of diversification. We design three evaluation tasks: (1) robustness of truth verification under multi-level drift; (2) susceptibility to adversarial evidence contamination generated by GenAI; and (3) analysis of reasoning consistency across diverse inputs. Experiments with six state-of-the-art LVLM-based detectors show substantial performance drops (average F1 -14.8%) and increasingly unstable reasoning traces, with even more severe failures under adversarial evidence injection. Our findings uncover fundamental vulnerabilities in existing MMD systems and suggest an urgent need for more resilient approaches in the GenAI era.</p>
<blockquote>
<p>å¤šæ¨¡æ€é”™è¯¯ä¿¡æ¯çš„æ™®åŠå¯¹å…¬å…±è¯è¯­å’Œç¤¾ä¼šä¿¡ä»»æ„æˆäº†æ—¥ç›Šå¢é•¿çš„å¨èƒã€‚è™½ç„¶å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å¤šåª’ä½“é”™è¯¯ä¿¡æ¯æ£€æµ‹ï¼ˆMMDï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰å·¥å…·çš„å…´èµ·å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼šç”±GenAIé©±åŠ¨çš„æ–°é—»å¤šæ ·æ€§ï¼Œè¡¨ç°ä¸ºå†…å®¹å’Œå½¢å¼çš„é«˜åº¦å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§å¤šæ ·æ€§å¼•å‘äº†å¤šå±‚æ¬¡æ¼‚ç§»ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰æ¨¡å‹å±‚é¢çš„è¯¯è§£æ¼‚ç§»ï¼Œå…¶ä¸­é£æ ¼å·®å¼‚ç ´åäº†æ¨¡å‹çš„å†…éƒ¨æ¨ç†ï¼›ï¼ˆ2ï¼‰è¯æ®å±‚é¢çš„æ¼‚ç§»ï¼Œå…¶ä¸­è¡¨è¾¾å¤šæ ·æ€§é™ä½äº†æ£€ç´¢åˆ°çš„å¤–éƒ¨è¯æ®çš„è´¨é‡æˆ–ç›¸å…³æ€§ã€‚è¿™äº›æ¼‚ç§»ç°è±¡æ˜¾è‘—é™ä½äº†å½“å‰åŸºäºLVLMçš„MMDç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DriftBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«6ç±»å¤šæ ·åŒ–æ–°é—»çš„16000ä¸ªæ–°é—»å®ä¾‹ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸‰ä¸ªè¯„ä¼°ä»»åŠ¡ï¼šï¼ˆ1ï¼‰åœ¨å¤šå±‚æ¬¡æ¼‚ç§»ä¸‹çœŸç†éªŒè¯çš„ç¨³å¥æ€§ï¼›ï¼ˆ2ï¼‰ç”±GenAIç”Ÿæˆçš„å¯¹æŠ—æ€§è¯æ®æ±¡æŸ“çš„å½±å“ï¼›ï¼ˆ3ï¼‰åˆ†æä¸åŒè¾“å…¥ä¸‹çš„æ¨ç†ä¸€è‡´æ€§ã€‚ä½¿ç”¨å…­ä¸ªæœ€å…ˆè¿›çš„åŸºäºLVLMçš„æ£€æµ‹å™¨è¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ˆå¹³å‡F1ä¸‹é™14.8%ï¼‰ï¼Œæ¨ç†è½¨è¿¹è¶Šæ¥è¶Šä¸ç¨³å®šï¼Œå¯¹æŠ—æ€§è¯æ®æ³¨å…¥æ—¶çš„å¤±è´¥æƒ…å†µæ›´ä¸ºä¸¥é‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†ç°æœ‰MMDç³»ç»Ÿçš„åŸºæœ¬æ¼æ´ï¼Œå¹¶æç¤ºåœ¨GenAIæ—¶ä»£éœ€è¦æ›´å¼ºå¤§çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12711v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰å·¥å…·æ‰€å¸¦æ¥çš„æ–°é—»å¤šæ ·æ€§ç»™åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹ï¼ˆMMDï¼‰ç³»ç»Ÿå¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚æ–‡ç« ä»‹ç»äº†æ–°é—»å¤šæ ·æ€§å¼•èµ·çš„å¤šå±‚æ¬¡æ¼‚ç§»ç°è±¡ï¼ŒåŒ…æ‹¬æ¨¡å‹çº§åˆ«çš„è®¤çŸ¥æ¼‚ç§»å’Œè¯æ®çº§åˆ«çš„æ¼‚ç§»ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†DriftBenchå¤§å‹åŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°MMDç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰MMDç³»ç»Ÿå­˜åœ¨é‡å¤§æ¼æ´ï¼Œéœ€è¦åœ¨GenAIæ—¶ä»£å¼€å‘æ›´å…·å¼¹æ€§çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€è™šå‡ä¿¡æ¯å¯¹å…¬ä¼—è®¨è®ºå’Œç¤¾ä¼šä¿¡ä»»æ„æˆå¨èƒã€‚</li>
<li>GenAIå·¥å…·çš„å…´èµ·ä¸ºMMDå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œå³æ–°é—»å¤šæ ·æ€§ã€‚</li>
<li>æ–°é—»å¤šæ ·æ€§å¼•å‘äº†å¤šå±‚æ¬¡æ¼‚ç§»ï¼ŒåŒ…æ‹¬æ¨¡å‹è®¤çŸ¥æ¼‚ç§»å’Œè¯æ®æ¼‚ç§»ã€‚</li>
<li>å¤šå±‚æ¬¡æ¼‚ç§»æ˜¾è‘—é™ä½äº†LVLM-based MMDç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚</li>
<li>DriftBenchåŸºå‡†æµ‹è¯•å¹³å°ç”¨äºç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºç°æœ‰MMDç³»ç»Ÿå­˜åœ¨é‡å¤§æ¼æ´ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94a4fcf5806ac480e8a510c17bd98928" align="middle">
<img src="https://picx.zhimg.com/v2-51ce26b39b34d8c4c73532e3937a69b4" align="middle">
<img src="https://picx.zhimg.com/v2-f6d120dce68ab38d7ad781554e512718" align="middle">
<img src="https://picx.zhimg.com/v2-e984aeb324f85b0441e5f46e89c0530d" align="middle">
<img src="https://picx.zhimg.com/v2-ecaff2effd6fa145c5a6e4a3910cc1f0" align="middle">
<img src="https://picx.zhimg.com/v2-47b3e6cb07770dbf5fc77f614180d460" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Beyond-GPT-5-Making-LLMs-Cheaper-and-Better-via-Performance-Efficiency-Optimized-Routing"><a href="#Beyond-GPT-5-Making-LLMs-Cheaper-and-Better-via-Performance-Efficiency-Optimized-Routing" class="headerlink" title="Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing"></a>Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing</h2><p><strong>Authors:Yiqun Zhang, Hao Li, Jianhao Chen, Hangfan Zhang, Peng Ye, Lei Bai, Shuyue Hu</strong></p>
<p>Balancing performance and efficiency is a central challenge in large language model (LLM) advancement. GPT-5 addresses this with test-time routing, dynamically assigning queries to either an efficient or a high-capacity model during inference. In this work, we present Avengers-Pro, a test-time routing framework that ensembles LLMs of varying capacities and efficiencies, providing a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro embeds and clusters incoming queries, then routes each to the most suitable model based on a performance-efficiency score. Across 6 challenging benchmarks and 8 leading models â€“ including GPT-5-medium, Gemini-2.5-pro, and Claude-opus-4.1 â€“ Avengers-Pro achieves state-of-the-art results: by varying a performance-efficiency trade-off parameter, it can surpass the strongest single model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the average accuracy of the strongest single model at 27% lower cost, and reach ~90% of that performance at 63% lower cost. Last but not least, it achieves a Pareto frontier, consistently yielding the highest accuracy for any given cost, and the lowest cost for any given accuracy, among all single models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ZhangYiqun018/AvengersPro">https://github.com/ZhangYiqun018/AvengersPro</a>.</p>
<blockquote>
<p>åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ä¸­ï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚GPT-5é€šè¿‡æµ‹è¯•æ—¶è·¯ç”±æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åœ°å°†æŸ¥è¯¢åˆ†é…ç»™é«˜æ•ˆæ¨¡å‹æˆ–é«˜æ€§èƒ½æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Avengers-Proï¼Œä¸€ä¸ªæµ‹è¯•æ—¶è·¯ç”±æ¡†æ¶ï¼Œå®ƒé›†æˆäº†ä¸åŒå®¹é‡å’Œæ•ˆç‡çš„è¯­è¨€æ¨¡å‹ï¼Œä¸ºæ‰€æœ‰æ€§èƒ½-æ•ˆç‡æƒè¡¡æä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆã€‚Avengers-Proé€šè¿‡åµŒå…¥å’Œèšç±»ä¼ å…¥æŸ¥è¯¢ï¼Œç„¶åæ ¹æ®æ€§èƒ½æ•ˆç‡åˆ†æ•°å°†å…¶è·¯ç”±åˆ°æœ€åˆé€‚çš„æ¨¡å‹ã€‚åœ¨6ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•å’Œ8ä¸ªé¢†å…ˆæ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-5ä¸­ç­‰ç‰ˆæœ¬ã€Gemini 2.5ä¸“ä¸šç‰ˆå’ŒClaude opus 4.1ï¼‰ä¸­ï¼ŒAvengers-Proè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼šé€šè¿‡è°ƒæ•´æ€§èƒ½æ•ˆç‡æƒè¡¡å‚æ•°ï¼Œå®ƒçš„å¹³å‡å‡†ç¡®ç‡å¯ä»¥è¶…è¿‡æœ€å¼ºå•æ¨¡å‹ï¼ˆGPT-5ä¸­ç­‰ç‰ˆæœ¬ï¼‰7%ã€‚è€Œä¸”ï¼Œå®ƒåœ¨æˆæœ¬é™ä½27%çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥è¾¾åˆ°ä¸æœ€å¼ºå•æ¨¡å‹ç›¸å½“çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå¹¶åœ¨æˆæœ¬é™ä½63%çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°å…¶æ€§èƒ½çš„çº¦90%ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒè¾¾åˆ°äº†å¸•ç´¯æ‰˜å‰æ²¿ï¼Œèƒ½å¤Ÿåœ¨ç»™å®šçš„ä»»ä½•æˆæœ¬ä¸‹å§‹ç»ˆäº§ç”Ÿæœ€é«˜çš„å‡†ç¡®ç‡ï¼Œä»¥åŠåœ¨ç»™å®šçš„ä»»ä½•å‡†ç¡®ç‡ä¸‹å…·æœ‰æœ€ä½çš„æˆæœ¬ï¼Œè¶…è¿‡äº†æ‰€æœ‰å•æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhangYiqun018/AvengersPro">https://github.com/ZhangYiqun018/AvengersPro</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12631v2">PDF</a> This work has been accepted to DAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡æŒ‘æˆ˜ã€‚GPT-5é€šè¿‡æµ‹è¯•æ—¶è·¯ç”±æŠ€æœ¯ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ†é…æŸ¥è¯¢åˆ°é«˜æ•ˆæ¨¡å‹æˆ–é«˜æ€§èƒ½æ¨¡å‹ã€‚æœ¬ç ”ç©¶æå‡ºAvengers-Proæ¡†æ¶ï¼Œé›†æˆä¸åŒå®¹é‡å’Œæ•ˆç‡çš„LLMï¼Œä¸ºå„ç§æ€§èƒ½ä¸æ•ˆç‡çš„æƒè¡¡æä¾›å…¨é¢è§£å†³æ–¹æ¡ˆã€‚Avengers-Proé€šè¿‡åµŒå…¥å’Œèšç±»æŸ¥è¯¢ï¼Œæ ¹æ®æ€§èƒ½æ•ˆç‡è¯„åˆ†å°†å…¶è·¯ç”±åˆ°æœ€åˆé€‚çš„æ¨¡å‹ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œé¢†å…ˆæ¨¡å‹ä¸Šï¼ŒåŒ…æ‹¬GPT-5-mediumã€Gemini-2.5-proå’ŒClaude-opus-4.1ç­‰ï¼ŒAvengers-Proå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¯è¶…è¶Šæœ€å¼ºå•æ¨¡å‹å¹³å‡å‡†ç¡®ç‡+7%ã€‚åŒæ—¶ï¼Œå®ƒåœ¨é™ä½æˆæœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯ä¸æœ€å¼ºå•æ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ç›¸åŒ¹é…çš„åŒæ—¶é™ä½27%çš„æˆæœ¬ï¼Œå¹¶èƒ½åœ¨ä¿æŒçº¦90%æ€§èƒ½çš„åŒæ—¶å°†æˆæœ¬é™ä½63%ã€‚æ­¤å¤–ï¼Œå®ƒå®ç°äº†å¸•ç´¯æ‰˜å‰æ²¿ï¼Œåœ¨å„ç§æˆæœ¬å’Œå‡†ç¡®ç‡ç»„åˆä¸­å‡è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡æŒ‘æˆ˜ã€‚</li>
<li>GPT-5é‡‡ç”¨æµ‹è¯•æ—¶è·¯ç”±æŠ€æœ¯æ¥å¹³è¡¡æŸ¥è¯¢åˆ†é…ã€‚</li>
<li>Avengers-Proæ˜¯ä¸€ä¸ªé›†æˆä¸åŒå®¹é‡å’Œæ•ˆç‡çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶ã€‚</li>
<li>Avengers-Proé€šè¿‡åµŒå…¥å’Œèšç±»æŸ¥è¯¢ï¼Œæ ¹æ®æ€§èƒ½æ•ˆç‡è¯„åˆ†è¿›è¡Œè·¯ç”±ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒAvengers-Proå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šæœ€å¼ºå•æ¨¡å‹å¹³å‡å‡†ç¡®ç‡+7%ã€‚</li>
<li>Avengers-Proåœ¨é™ä½æˆæœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸æœ€å¼ºå•æ¨¡å‹ç›¸æ¯”å¯é™ä½27%çš„æˆæœ¬åŒæ—¶ä¿æŒç›¸ä¼¼æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fd76e80531f4dc1a8afb17a155789da" align="middle">
<img src="https://picx.zhimg.com/v2-381cf45a6a3c16d51c24f58c53c47ad2" align="middle">
<img src="https://picx.zhimg.com/v2-1266c225855e7db42021b5807cf63d29" align="middle">
<img src="https://picx.zhimg.com/v2-4f18a31e72cb3f847eee6bc0ac3c0cbe" align="middle">
<img src="https://picx.zhimg.com/v2-1e21add1cb6161134531e604b4ae40e2" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ControlMed-Adding-Reasoning-Control-to-Medical-Language-Model"><a href="#ControlMed-Adding-Reasoning-Control-to-Medical-Language-Model" class="headerlink" title="ControlMed: Adding Reasoning Control to Medical Language Model"></a>ControlMed: Adding Reasoning Control to Medical Language Model</h2><p><strong>Authors:Sung-Min Lee, Siyoon Lee, Juyeon Kim, Kyoungmin Roh</strong></p>
<p>Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \textit{direct} and \textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis.</p>
<blockquote>
<p>éšç€ä¸´åºŠå†³ç­–çš„ç”Ÿå‘½æ”¸å…³æ€§è´¨å¯¹å¯é æ”¯æŒçš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œå…·æœ‰å¢å¼ºå‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼Œç°æœ‰çš„æ¨ç†LLMé€šå¸¸ä¼šäº§ç”Ÿä¸å¿…è¦çš„å†—é•¿æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ã€å“åº”å»¶è¿Ÿã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†å®ƒä»¬åœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>ControlMed</strong>åŒ»ç–—è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ç²¾ç»†çš„æ§åˆ¶æ ‡è®°åœ¨æ¨ç†æ—¶é—´æ—¶è®©ç”¨æˆ·ä¸»åŠ¨æ§åˆ¶æ¨ç†è¿‡ç¨‹çš„é•¿åº¦ã€‚ControlMedé€šè¿‡ä¸‰é˜¶æ®µç®¡é“è¿›è¡Œè®­ç»ƒï¼š1ï¼‰åœ¨æ¶µç›–ç›´æ¥å’Œæ¨ç†å“åº”çš„å¤§è§„æ¨¡åˆæˆåŒ»ç–—æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼›2ï¼‰ä½¿ç”¨å¤šé•¿åº¦æ¨ç†æ•°æ®å’Œæ˜ç¡®çš„é•¿åº¦æ§åˆ¶æ ‡è®°è¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒï¼›3ï¼‰ä½¿ç”¨åŸºäºæ¨¡å‹çš„å¥–åŠ±ä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§å’Œå“åº”è´¨é‡ã€‚åœ¨è‹±è¯­å’ŒéŸ©è¯­åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”å–å¾—äº†ç›¸ä¼¼æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦æ§åˆ¶æ¨ç†é•¿åº¦ï¼Œçµæ´»åœ°å¹³è¡¡æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒControlMedæ˜¯ä¸´åºŠé—®ç­”å’ŒåŒ»ç–—ä¿¡æ¯åˆ†æçš„å®ç”¨ä¸”å¯é€‚åº”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22545v3">PDF</a> IJCNLP-AACL 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå…¶å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§å¾—åˆ°äº†å¢å¼ºã€‚ç„¶è€Œï¼Œç°æœ‰æ¨ç†LLMå¸¸å¸¸äº§ç”Ÿä¸å¿…è¦çš„å†—é•¿æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—è´Ÿæ‹…åŠ é‡å’Œå“åº”å»¶è¿Ÿï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨ç°å®ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºControlMedåŒ»ç–—è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç²¾ç»†çš„ç²’åº¦æ§åˆ¶æ ‡è®°ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨æ¨ç†æ—¶æ§åˆ¶æ¨ç†è¿‡ç¨‹çš„é•¿åº¦ã€‚ControlMedé€šè¿‡ä¸‰é˜¶æ®µç®¡é“è¿›è¡Œè®­ç»ƒï¼š1ï¼‰åœ¨æ¶µç›–ç›´æ¥å’Œæ¨ç†å“åº”çš„å¤§è§„æ¨¡åˆæˆåŒ»ç–—æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼›2ï¼‰ä½¿ç”¨å¤šé•¿åº¦æ¨ç†æ•°æ®å’Œæ˜ç¡®çš„é•¿åº¦æ§åˆ¶æ ‡è®°è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼›3ï¼‰ä½¿ç”¨æ¨¡å‹åŸºç¡€å¥–åŠ±ä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§å’Œå“åº”è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç§è‹±æ–‡å’ŒéŸ©æ–‡åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€å…ˆè¿›æ¨¡å‹çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦çµæ´»å¹³è¡¡æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œæ§åˆ¶æ¨ç†é•¿åº¦ã€‚è¿™è¡¨æ˜ControlMedæ˜¯ä¸´åºŠé—®ç­”å’ŒåŒ»ç–—ä¿¡æ¯åˆ†æçš„å®é™…å¯è¡Œå’Œé€‚åº”æ€§å¼ºçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨é€æ¸æ™®åŠï¼Œå› ä¸´åºŠå†³ç­–çš„ç”Ÿå‘½å…³é”®æ€§è´¨éœ€è¦å¯é çš„æ”¯æŒã€‚</li>
<li>ç°æœ‰æ¨ç†LLMä¼šäº§ç”Ÿå†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—è´Ÿæ‹…å’Œå“åº”å»¶è¿Ÿï¼Œå½±å“åœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­çš„åº”ç”¨ã€‚</li>
<li>ControlMedåŒ»ç–—è¯­è¨€æ¨¡å‹è¢«å¼•å…¥ï¼Œå…è®¸ç”¨æˆ·åœ¨æ¨ç†æ—¶æ§åˆ¶æ¨ç†è¿‡ç¨‹çš„é•¿åº¦ã€‚</li>
<li>ControlMedé€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒï¼šé¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>ControlMedåœ¨å¤šç§è¯­è¨€å’ŒåŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥æ ¹æ®éœ€æ±‚çµæ´»è°ƒæ•´æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a97b57da93be2da4f18849d7b7bc5063" align="middle">
<img src="https://picx.zhimg.com/v2-c1f44ed0fc9538816f5d15897383125d" align="middle">
<img src="https://picx.zhimg.com/v2-147a7c41e4e60358cfc672f3372a2e0f" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays"><a href="#ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays" class="headerlink" title="ChestGPT: Integrating Large Language Models and Vision Transformers for Disease Detection and Localization in Chest X-Rays"></a>ChestGPT: Integrating Large Language Models and Vision Transformers for Disease Detection and Localization in Chest X-Rays</h2><p><strong>Authors:Shehroz S. Khan, Petar Przulj, Ahmed Ashraf, Ali Abedi</strong></p>
<p>The global demand for radiologists is increasing rapidly due to a growing reliance on medical imaging services, while the supply of radiologists is not keeping pace. Advances in computer vision and image processing technologies present significant potential to address this gap by enhancing radiologistsâ€™ capabilities and improving diagnostic accuracy. Large language models (LLMs), particularly generative pre-trained transformers (GPTs), have become the primary approach for understanding and generating textual data. In parallel, vision transformers (ViTs) have proven effective at converting visual data into a format that LLMs can process efficiently. In this paper, we present ChestGPT, a deep-learning framework that integrates the EVA ViT with the Llama 2 LLM to classify diseases and localize regions of interest in chest X-ray images. The ViT converts X-ray images into tokens, which are then fed, together with engineered prompts, into the LLM, enabling joint classification and localization of diseases. This approach incorporates transfer learning techniques to enhance both explainability and performance. The proposed method achieved strong global disease classification performance on the VinDr-CXR dataset, with an F1 score of 0.76, and successfully localized pathologies by generating bounding boxes around the regions of interest. We also outline several task-specific prompts, in addition to general-purpose prompts, for scenarios radiologists might encounter. Overall, this framework offers an assistive tool that can lighten radiologistsâ€™ workload by providing preliminary findings and regions of interest to facilitate their diagnostic process.</p>
<blockquote>
<p>éšç€å¯¹åŒ»å­¦å½±åƒæœåŠ¡è¶Šæ¥è¶Šä¾èµ–ï¼Œå…¨çƒå¯¹æ”¾å°„ç§‘åŒ»å¸ˆçš„éœ€æ±‚è¿…é€Ÿå¢é•¿ï¼Œç„¶è€Œæ”¾å°„ç§‘åŒ»å¸ˆçš„ä¾›åº”å´è·Ÿä¸ä¸Šè¿™ä¸€éœ€æ±‚ã€‚è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›æ­¥ä¸ºè§£å†³è¿™ä¸€å·®è·æä¾›äº†å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡å¢å¼ºæ”¾å°„ç§‘åŒ»å¸ˆçš„èƒ½åŠ›å’Œæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°¤å…¶æ˜¯é¢„è®­ç»ƒç”Ÿæˆå¼è½¬æ¢å™¨ï¼ˆGPTï¼‰å·²æˆä¸ºç†è§£å’Œç”Ÿæˆæ–‡æœ¬æ•°æ®çš„ä¸»è¦æ–¹æ³•ã€‚åŒæ—¶ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å·²è¯æ˜å…¶åœ¨å°†è§†è§‰æ•°æ®è½¬æ¢ä¸ºLLMå¯ä»¥é«˜æ•ˆå¤„ç†çš„å½¢å¼æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ChestGPTï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†EVA ViTä¸Llama 2 LLMç›¸ç»“åˆï¼Œç”¨äºåˆ†ç±»èƒ¸éƒ¨Xå…‰å›¾åƒä¸­çš„ç–¾ç—…å¹¶å®šä½æ„Ÿå…´è¶£åŒºåŸŸã€‚ViTå°†Xå…‰å›¾åƒè½¬æ¢ä¸ºä»¤ç‰Œï¼Œç„¶åä¸å·¥ç¨‹æç¤ºä¸€èµ·è¾“å…¥åˆ°LLMä¸­ï¼Œå®ç°ç–¾ç—…çš„è”åˆåˆ†ç±»å’Œå®šä½ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œä»¥æé«˜è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨VinDr-CXRæ•°æ®é›†ä¸Šå®ç°äº†å…¨çƒç–¾ç—…åˆ†ç±»çš„å¼ºåŠ²è¡¨ç°ï¼ŒF1åˆ†æ•°ä¸º0.76ï¼Œå¹¶é€šè¿‡å›´ç»•æ„Ÿå…´è¶£åŒºåŸŸç”Ÿæˆè¾¹ç•Œæ¡†æˆåŠŸåœ°å®šä½äº†ç—…ç†ã€‚æˆ‘ä»¬è¿˜æ¦‚è¿°äº†é™¤é€šç”¨æç¤ºå¤–ï¼Œé’ˆå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿå¯èƒ½é‡åˆ°çš„æƒ…å†µçš„ç‰¹å®šä»»åŠ¡æç¤ºã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€ä¸ªè¾…åŠ©å·¥å…·ï¼Œå¯ä»¥é€šè¿‡æä¾›åˆæ­¥å‘ç°å’Œæ„Ÿå…´è¶£åŒºåŸŸæ¥å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œä»è€Œæœ‰åŠ©äºä»–ä»¬çš„è¯Šæ–­è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03739v2">PDF</a> 8 pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…¨çƒå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿçš„éœ€æ±‚è¿…é€Ÿå¢é•¿ï¼Œè€Œæ”¾å°„ç§‘åŒ»ç”Ÿçš„ä¾›åº”å´è·Ÿä¸ä¸Šè¿™ä¸€éœ€æ±‚ã€‚è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›æ­¥ä¸ºè§£å†³è¿™ä¸€å·®è·æä¾›äº†å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡å¢å¼ºæ”¾å°„ç§‘åŒ»ç”Ÿçš„èƒ½åŠ›å’Œæé«˜è¯Šæ–­å‡†ç¡®æ€§æ¥å®ç°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºChestGPTçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒç»“åˆäº†EVA Vision Transformerï¼ˆViTï¼‰å’ŒLlama 2å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒè¿›è¡Œç–¾ç—…åˆ†ç±»å’Œæ„Ÿå…´è¶£åŒºåŸŸå®šä½ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ViTå°†Xå°„çº¿å›¾åƒè½¬æ¢ä¸ºä»¤ç‰Œï¼Œç„¶åä¸å·¥ç¨‹æç¤ºä¸€èµ·è¾“å…¥LLMï¼Œå®ç°ç–¾ç—…çš„è”åˆåˆ†ç±»å’Œå®šä½ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚åœ¨VinDr-CXRæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†å¼ºå¤§çš„å…¨çƒç–¾ç—…åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°ä¸º0.76ï¼Œå¹¶æˆåŠŸå®šä½äº†ç—…ç†åŒºåŸŸï¼Œç”Ÿæˆäº†å›´ç»•æ„Ÿå…´è¶£åŒºåŸŸçš„è¾¹ç•Œæ¡†ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶ä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›äº†ä¸€ç§è¾…åŠ©å·¥å…·ï¼Œå¯ä»¥é€šè¿‡æä¾›åˆæ­¥å‘ç°å’Œæ„Ÿå…´è¶£åŒºåŸŸæ¥å‡è½»ä»–ä»¬çš„å·¥ä½œé‡ï¼Œä»è€Œæœ‰åŠ©äºè¯Šæ–­è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒæœåŠ¡çš„éœ€æ±‚å¢é•¿å¯¼è‡´å¯¹æ”¾å°„ç§‘åŒ»ç”Ÿçš„éœ€æ±‚è¿…é€Ÿå¢åŠ ï¼Œè€Œæ”¾å°„ç§‘åŒ»ç”Ÿçš„ä¾›åº”ä¸è¶³ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›æ­¥åœ¨è§£å†³è¿™ä¸€å·®è·æ–¹é¢å…·å·¨å¤§æ½œåŠ›ã€‚</li>
<li>LLMsï¼ˆå°¤å…¶æ˜¯GPTsï¼‰å’ŒVision Transformersï¼ˆViTsï¼‰ç»“åˆåº”ç”¨ï¼Œå¯¹ç†è§£å’Œç”Ÿæˆæ–‡æœ¬æ•°æ®å’Œè§†è§‰æ•°æ®æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>ChestGPTæ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†EVA ViTå’ŒLlama 2 LLMæ¥è¿›è¡Œèƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„ç–¾ç—…åˆ†ç±»å’ŒåŒºåŸŸå®šä½ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯æ¥æé«˜å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>åœ¨VinDr-CXRæ•°æ®é›†ä¸Šï¼ŒChestGPTå®ç°äº†F1åˆ†æ•°ä¸º0.76çš„å‡ºè‰²ç–¾ç—…åˆ†ç±»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bd42baf4feba77fb3803578d117f2f9" align="middle">
<img src="https://picx.zhimg.com/v2-fc161cb87a1a0e57bce419412e566b9a" align="middle">
<img src="https://picx.zhimg.com/v2-e2977023fd16c38d1958d075f0334fa3" align="middle">
<img src="https://picx.zhimg.com/v2-41bac41a8a4d6837b235153d07ec0071" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GPT-But-Backwards-Exactly-Inverting-Language-Model-Outputs"><a href="#GPT-But-Backwards-Exactly-Inverting-Language-Model-Outputs" class="headerlink" title="GPT, But Backwards: Exactly Inverting Language Model Outputs"></a>GPT, But Backwards: Exactly Inverting Language Model Outputs</h2><p><strong>Authors:Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro</strong></p>
<p>The task of reconstructing unknown textual inputs to language models is a fundamental auditing primitive that allows us to assess the modelâ€™s vulnerability to a range of security issues, including stealing hidden system prompts, detecting backdoors, and leaking private data. Existing inversion works assume access to differing levels of information (e.g. requiring input-output examples, the model parameters, intermediate activations or output logits) but oftentimes fail to fully reconstruct the desired input. In this paper, we present the Sparse One-hot Discrete Adam (SODA) algorithm, a search-based inversion method that can accurately reconstruct the input text, given white-box access to the language model and its output. Our experiments demonstrate for the first time that exact language model inversion is possible on both natural language and random inputs. Indeed, SODA achieves respectively 98% and 79% reconstruction rates on inputs with lengths up to 10 tokens. Furthermore, we show that input length and vocabulary size have a far greater impact on the probability of a successful reconstruction than the size of the language model itself, thus allowing us to scale to models from 33M to 3B parameters.</p>
<blockquote>
<p>é‡å»ºæœªçŸ¥æ–‡æœ¬è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡æ˜¯ä¸€ä¸ªåŸºæœ¬çš„å®¡è®¡åŸºæœ¬è¦ç´ ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯„ä¼°æ¨¡å‹å¯¹å„ç§å®‰å…¨é—®é¢˜çš„è„†å¼±æ€§ï¼ŒåŒ…æ‹¬çªƒå–éšè—çš„ç³»ç»Ÿæç¤ºã€æ£€æµ‹åé—¨å’Œæ³„éœ²ç§äººæ•°æ®ã€‚ç°æœ‰çš„åè½¬å·¥ä½œå‡è®¾è®¿é—®ä¸åŒå±‚çº§çš„ä¿¡æ¯ï¼ˆä¾‹å¦‚éœ€è¦è¾“å…¥è¾“å‡ºç¤ºä¾‹ã€æ¨¡å‹å‚æ•°ã€ä¸­é—´æ¿€æ´»æˆ–è¾“å‡ºå¯¹æ•°å‡ ç‡ï¼‰ï¼Œä½†å¾€å¾€æ— æ³•å®Œå…¨é‡å»ºæ‰€éœ€çš„è¾“å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Sparse One-hot Discrete Adamï¼ˆSODAï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæœç´¢çš„åè½¬æ–¹æ³•ï¼Œå¯ä»¥åœ¨è·å¾—è¯­è¨€æ¨¡å‹åŠå…¶è¾“å‡ºçš„ç™½ç›’è®¿é—®æƒé™çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®é‡å»ºè¾“å…¥æ–‡æœ¬ã€‚æˆ‘ä»¬çš„å®éªŒé¦–æ¬¡è¯æ˜ï¼Œåœ¨è‡ªç„¶è¯­è¨€è¾“å…¥å’Œéšæœºè¾“å…¥ä¸Šéƒ½å¯ä»¥å®ç°ç²¾ç¡®çš„è¯­è¨€æ¨¡å‹åè½¬ã€‚å®é™…ä¸Šï¼ŒSODAåœ¨å¯¹é•¿åº¦ä¸º10ä¸ªæ ‡è®°çš„è¾“å…¥ä¸Šåˆ†åˆ«å®ç°äº†98%å’Œ79%çš„é‡å»ºç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜è¾“å…¥é•¿åº¦å’Œè¯æ±‡å¤§å°å¯¹æˆåŠŸé‡å»ºçš„æ¦‚ç‡æœ‰è¿œæ¯”è¯­è¨€æ¨¡å‹æœ¬èº«çš„å¤§å°æ›´å¤§çš„å½±å“ï¼Œä»è€Œè®©æˆ‘ä»¬èƒ½å¤Ÿæ‰©å±•åˆ°ä»33Måˆ°3Bå‚æ•°çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01693v2">PDF</a> 7 pages, ICML 2025 Workshop on Reliable and Responsible Foundation Models</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSparse One-hot Discrete Adamï¼ˆSODAï¼‰çš„æœç´¢ç®—æ³•ï¼Œå¯ä»¥åœ¨è·å–è¯­è¨€æ¨¡å‹çš„white-boxè®¿é—®æƒåŠå…¶è¾“å‡ºä¿¡æ¯çš„åŸºç¡€ä¸Šï¼Œç²¾ç¡®åœ°é‡æ„è¾“å…¥æ–‡æœ¬ã€‚SODAä¸ä»…é€‚ç”¨äºè‡ªç„¶è¯­è¨€è¾“å…¥ï¼Œä¹Ÿé€‚ç”¨äºéšæœºè¾“å…¥ï¼Œä¸”å¯¹é•¿åº¦è¾¾10ä¸ªç¬¦å·çš„è¾“å…¥æœ‰å¾ˆé«˜çš„é‡å»ºç‡ã€‚å¦å¤–ï¼Œç ”ç©¶è¿˜å‘ç°è¾“å…¥é•¿åº¦å’Œè¯æ±‡è¡¨å¤§å°å¯¹é‡å»ºæˆåŠŸç‡çš„å½±å“è¿œå¤§äºè¯­è¨€æ¨¡å‹æœ¬èº«çš„å¤§å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬é‡æ„ä»»åŠ¡æ˜¯è¯„ä¼°æ¨¡å‹é¢å¯¹å®‰å…¨é—®é¢˜çš„æ¼æ´çš„ä¸€ä¸ªåŸºæœ¬å®¡è®¡å·¥å…·ã€‚</li>
<li>ç°æœ‰é‡æ„æ–¹æ³•å¾€å¾€æ— æ³•å®Œå…¨é‡æ„å‡ºæœŸæœ›çš„è¾“å…¥ã€‚</li>
<li>SODAç®—æ³•æ˜¯ä¸€ç§åŸºäºæœç´¢çš„è¾“å…¥é‡æ„æ–¹æ³•ï¼Œå¯ä»¥åœ¨è·å–è¯­è¨€æ¨¡å‹çš„white-boxè®¿é—®æƒåŠå…¶è¾“å‡ºçš„åŸºç¡€ä¸Šå‡†ç¡®é‡æ„è¾“å…¥æ–‡æœ¬ã€‚</li>
<li>SODAç®—æ³•é€‚ç”¨äºè‡ªç„¶è¯­è¨€å’Œéšæœºè¾“å…¥çš„é‡å»ºã€‚</li>
<li>å¯¹äºé•¿åº¦è¾¾10ä¸ªç¬¦å·çš„è¾“å…¥ï¼ŒSODAæœ‰å¾ˆé«˜çš„é‡å»ºç‡ã€‚</li>
<li>è¾“å…¥é•¿åº¦å’Œè¯æ±‡è¡¨å¤§å°å¯¹é‡å»ºæˆåŠŸç‡çš„å½±å“å¤§äºè¯­è¨€æ¨¡å‹æœ¬èº«çš„å¤§å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11f9b4000b06f0aed7d6f2b119b8234a" align="middle">
<img src="https://picx.zhimg.com/v2-6d1013ee8a17d7503937600466602cd0" align="middle">
<img src="https://picx.zhimg.com/v2-768b12e53e57c7d494cce73eb1d92b4b" align="middle">
<img src="https://picx.zhimg.com/v2-87ed3fe827d09280192868daf983c539" align="middle">
<img src="https://picx.zhimg.com/v2-653aedf9e9e4b85b0a1cdbd476a0504d" align="middle">
<img src="https://picx.zhimg.com/v2-18ce5ef850a2f3dd4450c91dff992269" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MIDB-Multilingual-Instruction-Data-Booster-for-Enhancing-Cultural-Equality-in-Multilingual-Instruction-Synthesis"><a href="#MIDB-Multilingual-Instruction-Data-Booster-for-Enhancing-Cultural-Equality-in-Multilingual-Instruction-Synthesis" class="headerlink" title="MIDB: Multilingual Instruction Data Booster for Enhancing Cultural Equality in Multilingual Instruction Synthesis"></a>MIDB: Multilingual Instruction Data Booster for Enhancing Cultural Equality in Multilingual Instruction Synthesis</h2><p><strong>Authors:Yilun Liu, Chunguang Zhao, Xinhua Yang, Hongyong Zeng, Shimin Tao, Weibin Meng, Minggui He, Yan Yu, Hongxia Ma, Li Zhang, Daimeng Wei, Boxing Chen</strong></p>
<p>Despite doubts on data quality, instruction synthesis has been widely applied into instruction tuning (IT) of LLMs as an economic and rapid alternative. Recent endeavors focus on improving data quality for synthesized instruction pairs in English and have facilitated IT of English-centric LLMs. However, data quality issues in multilingual synthesized instruction pairs are even more severe, since the common synthesizing practice is to translate English synthesized data into other languages using machine translation (MT). Besides the known content errors in these English synthesized data, multilingual synthesized instruction data are further exposed to defects introduced by MT and face insufficient localization of the target languages, leading to cultural inequality in trained LLMs. In this paper, we propose MIDB, a Multilingual Instruction Data Booster to automatically address the quality issues in multilingual synthesized data. MIDB is trained on around 36.8k revision examples across 16 languages by human linguistic experts, thereby can boost the low-quality data by addressing content errors and MT defects, and improving localization in these synthesized data. Both automatic and human evaluation indicate that not only MIDB steadily improved instruction data quality in 16 languages, but also the instruction-following and cultural-understanding abilities of multilingual LLMs fine-tuned on MIDB-boosted data were significantly enhanced, suggesting an improved linguistic and cultural equality.</p>
<blockquote>
<p>å°½ç®¡å¯¹æ•°æ®è´¨é‡å­˜åœ¨ç–‘è™‘ï¼ŒæŒ‡ä»¤åˆæˆå·²è¢«å¹¿æ³›åº”ç”¨äºLLMçš„æŒ‡ä»¤è°ƒæ•´ï¼ˆITï¼‰ä¸­ï¼Œä½œä¸ºä¸€ç§ç»æµä¸”å¿«é€Ÿçš„æ›¿ä»£æ–¹æ¡ˆã€‚è¿‘æœŸçš„ç ”ç©¶é‡ç‚¹å·²è½¬å‘æé«˜è‹±æ–‡åˆæˆæŒ‡ä»¤å¯¹çš„æ•°æ®è´¨é‡ï¼Œå¹¶ä¿ƒè¿›äº†ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„LLMçš„ITã€‚ç„¶è€Œï¼Œå¤šè¯­è¨€åˆæˆæŒ‡ä»¤å¯¹çš„æ•°æ®è´¨é‡é—®é¢˜æ›´ä¸ºä¸¥é‡ï¼Œå› ä¸ºå¸¸è§çš„åˆæˆåšæ³•æ˜¯å°†è‹±æ–‡åˆæˆæ•°æ®é€šè¿‡æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ç¿»è¯‘æˆå…¶ä»–è¯­è¨€ã€‚é™¤äº†è¿™äº›è‹±æ–‡åˆæˆæ•°æ®ä¸­çš„å·²çŸ¥å†…å®¹é”™è¯¯å¤–ï¼Œå¤šè¯­è¨€åˆæˆæŒ‡ä»¤æ•°æ®è¿˜è¿›ä¸€æ­¥é¢ä¸´ç”±æœºå™¨ç¿»è¯‘å¼•å…¥çš„ç¼ºé™·ï¼Œä»¥åŠç›®æ ‡è¯­è¨€çš„æœ¬åœ°åŒ–ä¸è¶³é—®é¢˜ï¼Œä»è€Œå¯¼è‡´è®­ç»ƒå‡ºçš„LLMå­˜åœ¨æ–‡åŒ–ä¸å¹³ç­‰ç°è±¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MIDBï¼ˆå¤šè¯­è¨€æŒ‡ä»¤æ•°æ®å¢å¼ºå™¨ï¼‰ï¼Œæ—¨åœ¨è‡ªåŠ¨è§£å†³å¤šè¯­è¨€åˆæˆæ•°æ®çš„è´¨é‡é—®é¢˜ã€‚MIDBç”±äººç±»è¯­è¨€å­¦ä¸“å®¶åœ¨16ç§è¯­è¨€çº¦36.8kä¸ªä¿®è®¢æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå› æ­¤å¯ä»¥é€šè¿‡è§£å†³å†…å®¹é”™è¯¯ã€æœºå™¨ç¿»è¯‘ç¼ºé™·å¹¶æ”¹å–„åˆæˆæ•°æ®çš„æœ¬åœ°åŒ–æ¥æå‡ä½è´¨é‡æ•°æ®ã€‚è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°å‡è¡¨æ˜ï¼ŒMIDBä¸ä»…ç¨³å®šæé«˜äº†16ç§è¯­è¨€çš„æŒ‡ä»¤æ•°æ®è´¨é‡ï¼Œè€Œä¸”åœ¨MIDBå¢å¼ºæ•°æ®ä¸Šå¾®è°ƒçš„å¤šè¯­è¨€LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œæ–‡åŒ–ç†è§£èƒ½åŠ›ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—å¢å¼ºï¼Œè¿™è¡¨æ˜è¯­è¨€å’Œæ–‡åŒ–çš„å¹³ç­‰æ€§å¾—åˆ°äº†æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17671v2">PDF</a> accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡å…³æ³¨äºå¤šè¯­è¨€åˆæˆæŒ‡ä»¤æ•°æ®çš„è´¨é‡é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºMIDBçš„å¤šè¯­è¨€æŒ‡ä»¤æ•°æ®å¢å¼ºå™¨ï¼Œé€šè¿‡è®­ç»ƒäººç±»è¯­è¨€å­¦ä¸“å®¶çš„ä¿®è®¢ç¤ºä¾‹æ¥è§£å†³åˆæˆæ•°æ®ä¸­çš„è´¨é‡é—®é¢˜ï¼ŒåŒ…æ‹¬å†…å®¹é”™è¯¯ã€æœºå™¨ç¿»è¯‘ç¼ºé™·ä»¥åŠæœ¬åœ°åŒ–ä¸è¶³ç­‰é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒMIDBä¸ä»…èƒ½ç¨³å®šæé«˜16ç§è¯­è¨€çš„æŒ‡ä»¤æ•°æ®è´¨é‡ï¼Œè¿˜èƒ½æ˜¾è‘—æå‡å¾®è°ƒåœ¨MIDBå¢å¼ºæ•°æ®ä¸Šçš„å¤šè¯­è¨€LLMçš„æŒ‡ä»¤éµå¾ªå’Œæ–‡åŒ–ç†è§£èƒ½åŠ›ï¼Œä»è€Œå®ç°è¯­è¨€å’Œæ–‡åŒ–çš„å¹³ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤åˆæˆå·²è¢«å¹¿æ³›åº”ç”¨äºLLMçš„æŒ‡ä»¤è°ƒæ•´ï¼ˆITï¼‰ï¼Œä½†å¤šè¯­è¨€åˆæˆæŒ‡ä»¤å¯¹çš„æ•°æ®è´¨é‡é—®é¢˜æ›´ä¸ºä¸¥é‡ã€‚</li>
<li>å¤šè¯­è¨€åˆæˆæŒ‡ä»¤æ•°æ®å¸¸è§çš„è´¨é‡é—®é¢˜åŒ…æ‹¬å†…å®¹é”™è¯¯ã€æœºå™¨ç¿»è¯‘å¼•å…¥çš„ç¼ºé™·ä»¥åŠç›®æ ‡è¯­è¨€çš„æœ¬åœ°åŒ–ä¸è¶³ã€‚</li>
<li>æ•°æ®è´¨é‡é—®é¢˜å¯èƒ½å¯¼è‡´è®­ç»ƒå‡ºçš„LLMå­˜åœ¨æ–‡åŒ–å’Œè¯­è¨€ä¸Šçš„ä¸å¹³ç­‰ã€‚</li>
<li>MIDBæ˜¯ä¸€ç§å¤šè¯­è¨€æŒ‡ä»¤æ•°æ®å¢å¼ºå™¨ï¼Œèƒ½è‡ªåŠ¨è§£å†³å¤šè¯­è¨€åˆæˆæ•°æ®çš„è´¨é‡é—®é¢˜ã€‚</li>
<li>MIDBé€šè¿‡è®­ç»ƒäººç±»è¯­è¨€å­¦ä¸“å®¶çš„ä¿®è®¢ç¤ºä¾‹æ¥æé«˜æ•°æ®è´¨é‡ï¼Œæ¶µç›–16ç§è¯­è¨€ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMIDBèƒ½æœ‰æ•ˆæé«˜å¤šè¯­è¨€æŒ‡ä»¤æ•°æ®çš„è´¨é‡ï¼Œå¹¶æ˜¾è‘—æå‡LLMçš„æŒ‡ä»¤éµå¾ªå’Œæ–‡åŒ–ç†è§£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84f47a3a845802a85c246fbbfa6572ad" align="middle">
<img src="https://picx.zhimg.com/v2-15a71cde00fefa150680f4b51f1f0cd1" align="middle">
<img src="https://picx.zhimg.com/v2-7fb8589fe25162f552389a2020fe157c" align="middle">
<img src="https://picx.zhimg.com/v2-f2400867c370fd0b71d2e48885362a6b" align="middle">
<img src="https://picx.zhimg.com/v2-7f42924cecdbd25519ce5dac815f8222" align="middle">
<img src="https://picx.zhimg.com/v2-80a6e00d1e02583a83708a485c7a6b64" align="middle">
<img src="https://picx.zhimg.com/v2-ac2af47837a2ac8a903b607cc3eb7673" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CrystalFormer-RL-Reinforcement-Fine-Tuning-for-Materials-Design"><a href="#CrystalFormer-RL-Reinforcement-Fine-Tuning-for-Materials-Design" class="headerlink" title="CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design"></a>CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design</h2><p><strong>Authors:Zhendong Cao, Lei Wang</strong></p>
<p>Reinforcement fine-tuning played an instrumental role in enhancing the instruction-following and reasoning abilities of large language models. In this work, we employ reinforcement fine-tuning for materials design, in which discriminative machine learning models are used to provide rewards to the autoregressive transformer-based materials generative model CrystalFormer. By optimizing the reward signals-such as energy above the convex hull and material properties figures of merit-reinforcement fine-tuning infuses knowledge from discriminative models into generative models. The resulting model, CrystalFormer-RL, shows enhanced stability in generated crystals and successfully discovers crystals with desirable yet conflicting material properties, such as substantial dielectric constant and band gap simultaneously. Notably, we observe that reinforcement fine-tuning not only enables the property-guided material design but also unlocks property-based material retrieval behavior of pretrained generative model. The present framework opens an exciting gateway to the synergies of the machine learning ecosystem for materials design.</p>
<blockquote>
<p>å¼ºåŒ–å¾®è°ƒåœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†å¼ºåŒ–å¾®è°ƒåº”ç”¨äºææ–™è®¾è®¡ï¼Œä½¿ç”¨åˆ¤åˆ«æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ºåŸºäºè‡ªå›å½’è½¬æ¢å™¨çš„ææ–™ç”Ÿæˆæ¨¡å‹CrystalFormeræä¾›å¥–åŠ±ã€‚é€šè¿‡ä¼˜åŒ–å¥–åŠ±ä¿¡å·ï¼Œå¦‚å‡¸åŒ…ä¸Šæ–¹çš„èƒ½é‡å’Œææ–™æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ï¼Œå¼ºåŒ–å¾®è°ƒå°†åˆ¤åˆ«æ¨¡å‹çš„çŸ¥è¯†æ³¨å…¥ç”Ÿæˆæ¨¡å‹ä¸­ã€‚ç»“æœå¾—åˆ°çš„æ¨¡å‹CrystalFormer-RLåœ¨ç”Ÿæˆçš„æ™¶ä½“ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§ï¼Œå¹¶èƒ½å¤Ÿå‘ç°å…·æœ‰ç†æƒ³ä½†ç›¸äº’å†²çªçš„ææ–™ç‰¹æ€§çš„æ™¶ä½“ï¼Œå¦‚å®è´¨çš„ä»‹ç”µå¸¸æ•°å’Œå¸¦éš™å¯ä»¥åŒæ—¶å­˜åœ¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¼ºåŒ–å¾®è°ƒä¸ä»…å®ç°äº†å±æ€§å¯¼å‘çš„ææ–™è®¾è®¡ï¼Œè¿˜è§£é”äº†é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹çš„åŸºäºå±æ€§çš„ææ–™æ£€ç´¢è¡Œä¸ºã€‚å½“å‰æ¡†æ¶ä¸ºæœºå™¨å­¦ä¹ ç”Ÿæ€ç³»ç»Ÿåœ¨ææ–™è®¾è®¡æ–¹é¢çš„ååŒä½œç”¨æ‰“å¼€äº†ä»¤äººå…´å¥‹çš„é—¨æˆ·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02367v2">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–ç²¾ç»†è°ƒæ•´åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­æå‡äº†æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å°†å¼ºåŒ–ç²¾ç»†è°ƒæ•´åº”ç”¨äºææ–™è®¾è®¡é¢†åŸŸï¼Œä½¿ç”¨åˆ¤åˆ«æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ºåŸºäºè‡ªå›å½’è½¬æ¢å™¨çš„ææ–™ç”Ÿæˆæ¨¡å‹CrystalFormeræä¾›å¥–åŠ±ã€‚é€šè¿‡ä¼˜åŒ–å¥–åŠ±ä¿¡å·ï¼Œå¦‚å‡¸åŒ…ä¸Šèƒ½é‡å’Œææ–™æ€§èƒ½è¯„ä»·æŒ‡æ ‡ç­‰ï¼Œå¼ºåŒ–ç²¾ç»†è°ƒæ•´å°†åˆ¤åˆ«æ¨¡å‹çš„çŸ¥è¯†æ³¨å…¥ç”Ÿæˆæ¨¡å‹ä¸­ã€‚ç»“æœæ¨¡å‹CrystalFormer-RLåœ¨ç”Ÿæˆçš„æ™¶ä½“ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§ï¼Œå¹¶æˆåŠŸå‘ç°å…·æœ‰ç†æƒ³ä½†ç›¸äº’å†²çªææ–™å±æ€§çš„æ™¶ä½“ï¼Œå¦‚é«˜ä»‹ç”µå¸¸æ•°å’Œå®½å¸¦éš™ã€‚æ­¤å¤–ï¼Œè§‚å¯Ÿåˆ°å¼ºåŒ–ç²¾ç»†è°ƒæ•´ä¸ä»…æ”¯æŒå±æ€§å¯¼å‘çš„ææ–™è®¾è®¡ï¼Œè¿˜æ¿€æ´»äº†é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹çš„å±æ€§åŸºç¡€ææ–™æ£€ç´¢è¡Œä¸ºã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†æœºå™¨å­¦ä¹ ç”Ÿæ€ç³»ç»Ÿåœ¨ææ–™è®¾è®¡ä¸­çš„ååŒæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–ç²¾ç»†è°ƒæ•´å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åˆ¤åˆ«æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ºåŸºäºè‡ªå›å½’è½¬æ¢å™¨çš„ææ–™ç”Ÿæˆæ¨¡å‹æä¾›å¥–åŠ±ã€‚</li>
<li>å¼ºåŒ–ç²¾ç»†è°ƒæ•´é€šè¿‡ä¼˜åŒ–å¥–åŠ±ä¿¡å·ï¼Œå¦‚å‡¸åŒ…ä¸Šèƒ½é‡å’Œææ–™æ€§èƒ½è¯„ä»·æŒ‡æ ‡ï¼Œå°†çŸ¥è¯†ä»åˆ¤åˆ«æ¨¡å‹æ³¨å…¥ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>CrystalFormer-RLæ¨¡å‹åœ¨ç”Ÿæˆæ™¶ä½“æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿå‘ç°å…·æœ‰ç†æƒ³ä½†ç›¸äº’å†²çªææ–™å±æ€§çš„æ™¶ä½“ï¼Œå¦‚é«˜ä»‹ç”µå¸¸æ•°å’Œå®½å¸¦éš™ã€‚</li>
<li>å¼ºåŒ–ç²¾ç»†è°ƒæ•´ä¸ä»…æ”¯æŒå±æ€§å¯¼å‘çš„ææ–™è®¾è®¡ï¼Œè¿˜æ¿€æ´»äº†é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹çš„å±æ€§åŸºç¡€ææ–™æ£€ç´¢è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73d3d09d48aa1732b5e9680142b3b15f" align="middle">
<img src="https://picx.zhimg.com/v2-e0109d619702171b02ff8501359064cc" align="middle">
<img src="https://picx.zhimg.com/v2-c288abb558104b37bef102709122c84f" align="middle">
<img src="https://picx.zhimg.com/v2-35c4bb18180ec8d4aa09aa74ebaa61e9" align="middle">
<img src="https://picx.zhimg.com/v2-ec0067f58b649ed54615ff1677c26266" align="middle">
<img src="https://picx.zhimg.com/v2-2660e7fe55fbea1af664f64317e9e92f" align="middle">
<img src="https://picx.zhimg.com/v2-e291704d9ab0af7b25e5a8dfa4a02546" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Stack-Transformer-Based-Spatial-Temporal-Attention-Model-for-Dynamic-Sign-Language-and-Fingerspelling-Recognition"><a href="#Stack-Transformer-Based-Spatial-Temporal-Attention-Model-for-Dynamic-Sign-Language-and-Fingerspelling-Recognition" class="headerlink" title="Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Sign Language and Fingerspelling Recognition"></a>Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Sign Language and Fingerspelling Recognition</h2><p><strong>Authors:Koki Hirooka, Abu Saleh Musa Miah, Tatsuya Murakami, Md. Al Mehedi Hasan, Yong Seok Hwang, Jungpil Shin</strong></p>
<p>Hand gesture-based Sign Language Recognition (SLR) serves as a crucial communication bridge between deaf and non-deaf individuals. While Graph Convolutional Networks (GCNs) are common, they are limited by their reliance on fixed skeletal graphs. To overcome this, we propose the Sequential Spatio-Temporal Attention Network (SSTAN), a novel Transformer-based architecture. Our model employs a hierarchical, stacked design that sequentially integrates Spatial Multi-Head Attention (MHA) to capture intra-frame joint relationships and Temporal MHA to model long-range inter-frame dependencies. This approach allows the model to efficiently learn complex spatio-temporal patterns without predefined graph structures. We validated our model through extensive experiments on diverse, large-scale datasets (WLASL, JSL, and KSL). A key finding is that our model, trained entirely from scratch, achieves state-of-the-art (SOTA) performance in the challenging fingerspelling categories (JSL and KSL). Furthermore, it establishes a new SOTA for skeleton-only methods on WLASL, outperforming several approaches that rely on complex self-supervised pre-training. These results demonstrate our modelâ€™s high data efficiency and its effectiveness in capturing the intricate dynamics of sign language. The official implementation is available at our GitHub repository: \href{<a target="_blank" rel="noopener" href="https://github.com/K-Hirooka-Aizu/skeleton-slr-transformer%7D%7Bhttps://github.com/K-Hirooka-Aizu/skeleton-slr-transformer%7D">https://github.com/K-Hirooka-Aizu/skeleton-slr-transformer}{https://github.com/K-Hirooka-Aizu/skeleton-slr-transformer}</a>.</p>
<blockquote>
<p>åŸºäºæ‰‹åŠ¿çš„è‹å“‘äººè¯­è¨€è¯†åˆ«ï¼ˆSLRï¼‰ä½œä¸ºè‹å“‘äººä¹‹é—´çš„é‡è¦æ²Ÿé€šæ¡¥æ¢ã€‚è™½ç„¶å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰å¾ˆå¸¸ç”¨ï¼Œä½†å®ƒä»¬ä¾èµ–äºå›ºå®šçš„éª¨æ¶å›¾è€Œå—åˆ°é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬æå‡ºäº†åºè´¯æ—¶ç©ºæ³¨æ„åŠ›ç½‘ç»œï¼ˆSSTANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºTransformerçš„æ–°å‹æ¶æ„ã€‚æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨å±‚æ¬¡å †å è®¾è®¡ï¼ŒæŒ‰é¡ºåºé›†æˆäº†ç©ºé—´å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰æ¥æ•æ‰å¸§å†…å…³èŠ‚å…³ç³»å’Œæ—¶åºMHAæ¥å¯¹å¸§é—´é•¿æœŸä¾èµ–è¿›è¡Œå»ºæ¨¡ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆå­¦ä¹ å¤æ‚çš„æ—¶ç©ºæ¨¡å¼ï¼Œæ— éœ€é¢„è®¾å›¾å½¢ç»“æ„ã€‚æˆ‘ä»¬åœ¨å¤šæ ·çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆWLASLã€JSLå’ŒKSLï¼‰ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸€ä¸ªå…³é”®å‘ç°æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®Œå…¨ä»å¤´å¼€å§‹è®­ç»ƒï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å­—æ¯æ‹¼æ‰“ç±»åˆ«ï¼ˆJSLå’ŒKSLï¼‰ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨WLASLä¸Šä»…ä¾é éª¨æ¶æ•°æ®çš„æ–¹æ³•å°±å»ºç«‹äº†æ–°çš„æœ€é«˜å‡†ç¡®ç‡è®°å½•ï¼Œè¶…è¿‡äº†ä¾èµ–äºå¤æ‚è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒçš„å‡ ä¸ªæ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ•°æ®æ•ˆç‡å¾ˆé«˜ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°æ•æ‰åˆ°å“‘è¯­çš„å¤æ‚åŠ¨æ€ç‰¹å¾ã€‚å®˜æ–¹å®ç°å¯ä»¥åœ¨æˆ‘ä»¬çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/K-Hirooka-Aizu/skeleton-slr-transformer">https://github.com/K-Hirooka-Aizu/skeleton-slr-transformer</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16855v2">PDF</a> 15 pages, 12 figures. Submitted to IEEE Access (under review)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å­—æè¿°äº†æ‰‹åŠ¿åŸºç¡€çš„è‹äººè¯­è¨€è¯†åˆ«æŠ€æœ¯çš„æ–°ç ”ç©¶ã€‚ç ”ç©¶äººå‘˜é’ˆå¯¹ç°æœ‰å›¾å½¢å·ç§¯ç½‘ç»œæ–¹æ³•çš„é™åˆ¶æå‡ºäº†ä¸€ç§åŸºäº Transformer ç»“æ„çš„æ–°æ¶æ„ï¼Œå³æ—¶åºæ—¶ç©ºæ³¨æ„åŠ›ç½‘ç»œï¼ˆSSTANï¼‰ã€‚SSTAN åˆ©ç”¨å±‚æ¬¡å †å è®¾è®¡ï¼Œç»“åˆç©ºé—´å¤šå¤´æ³¨æ„åŠ›ä¸é•¿æ—¶é—´å¸§é—´ä¾èµ–å…³ç³»å»ºæ¨¡ï¼Œå®ç°å¤æ‚æ—¶ç©ºæ¨¡å¼çš„è‡ªé€‚åº”å­¦ä¹ ã€‚åœ¨å¤šä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ‰‹æŒ‡æ‹¼å†™ç±»åˆ«ä¸Šå–å¾—äº†æœ€æ–°è¿›å±•ï¼Œå¹¶åœ¨ä»…ä½¿ç”¨éª¨æ¶æ•°æ®çš„æ–¹æ³•ä¸Šåœ¨ WLASL æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰‹åŠ¿åŸºç¡€çš„è‹äººè¯­è¨€è¯†åˆ«ï¼ˆSLRï¼‰æ˜¯è¿æ¥è‹éè‹ç¾¤ä½“çš„å…³é”®æ²Ÿé€šæ¡¥æ¢ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºæ—¶åºæ—¶ç©ºæ³¨æ„åŠ›ç½‘ç»œï¼ˆSSTANï¼‰æ–°æ¶æ„ä»¥æ”¹è¿›ç°æœ‰å›¾å½¢å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰æ–¹æ³•çš„é™åˆ¶ã€‚</li>
<li>SSTAN åˆ©ç”¨å±‚æ¬¡å †å è®¾è®¡ç»“åˆç©ºé—´å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ä¸é•¿æ—¶é—´å¸§é—´ä¾èµ–å…³ç³»å»ºæ¨¡ã€‚</li>
<li>SSTAN èƒ½å¤Ÿé«˜æ•ˆå­¦ä¹ å¤æ‚æ—¶ç©ºæ¨¡å¼ï¼Œæ— éœ€é¢„è®¾å›¾å½¢ç»“æ„ã€‚</li>
<li>SSTAN åœ¨æ‰‹æŒ‡æ‹¼å†™ç±»åˆ«ä¸Šå–å¾—äº†æœ€æ–°è¿›å±•ï¼Œè¡¨ç°è¶…è¶Šå¤šä¸ªå¤æ‚è‡ªç›‘ç£é¢„è®­ç»ƒçš„æ–¹æ³•ã€‚</li>
<li>SSTAN ä»…ä½¿ç”¨éª¨æ¶æ•°æ®çš„æ–¹æ³•åœ¨ WLASL æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccc2cc19e4c5f80bfe9a26b13ed34b0e" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1f6452c3b68d6ea66c075c46913538e2" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Explaining Decentralized Multi-Agent Reinforcement Learning Policies
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1a57e3c01ad6a4f9e38dd8e77c42443a" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  From 2D to 3D Without Extra Baggage Data-Efficient Cancer Detection in Digital Breast Tomosynthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
