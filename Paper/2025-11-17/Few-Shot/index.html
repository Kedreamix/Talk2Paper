<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Bi-Level Contextual Bandits for Individualized Resource Allocation under Delayed Feedback">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f94fc05d96989a2c6ff57ef1daa9a909')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-17-æ›´æ–°"><a href="#2025-11-17-æ›´æ–°" class="headerlink" title="2025-11-17 æ›´æ–°"></a>2025-11-17 æ›´æ–°</h1><h2 id="Bi-Level-Contextual-Bandits-for-Individualized-Resource-Allocation-under-Delayed-Feedback"><a href="#Bi-Level-Contextual-Bandits-for-Individualized-Resource-Allocation-under-Delayed-Feedback" class="headerlink" title="Bi-Level Contextual Bandits for Individualized Resource Allocation under Delayed Feedback"></a>Bi-Level Contextual Bandits for Individualized Resource Allocation under Delayed Feedback</h2><p><strong>Authors:Mohammadsina Almasi, Hadis Anahideh</strong></p>
<p>Equitably allocating limited resources in high-stakes domains-such as education, employment, and healthcare-requires balancing short-term utility with long-term impact, while accounting for delayed outcomes, hidden heterogeneity, and ethical constraints. However, most learning-based allocation frameworks either assume immediate feedback or ignore the complex interplay between individual characteristics and intervention dynamics. We propose a novel bi-level contextual bandit framework for individualized resource allocation under delayed feedback, designed to operate in real-world settings with dynamic populations, capacity constraints, and time-sensitive impact. At the meta level, the model optimizes subgroup-level budget allocations to satisfy fairness and operational constraints. At the base level, it identifies the most responsive individuals within each group using a neural network trained on observational data, while respecting cooldown windows and delayed treatment effects modeled via resource-specific delay kernels. By explicitly modeling temporal dynamics and feedback delays, the algorithm continually refines its policy as new data arrive, enabling more responsive and adaptive decision-making. We validate our approach on two real-world datasets from education and workforce development, showing that it achieves higher cumulative outcomes, better adapts to delay structures, and ensures equitable distribution across subgroups. Our results highlight the potential of delay-aware, data-driven decision-making systems to improve institutional policy and social welfare.</p>
<blockquote>
<p>åœ¨è¯¸å¦‚æ•™è‚²ã€å°±ä¸šå’ŒåŒ»ç–—ä¿å¥ç­‰é«˜é£é™©é¢†åŸŸä¸­å…¬å¹³åˆ†é…æœ‰é™èµ„æºï¼Œéœ€è¦åœ¨æƒè¡¡çŸ­æœŸæ•ˆç”¨ä¸é•¿æœŸå½±å“çš„åŒæ—¶ï¼Œè€ƒè™‘åˆ°ç»“æœå»¶è¿Ÿã€éšæ€§å¼‚è´¨æ€§å’Œé“å¾·çº¦æŸã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºå­¦ä¹ çš„åˆ†é…æ¡†æ¶è¦ä¹ˆå‡è®¾å³æ—¶åé¦ˆï¼Œè¦ä¹ˆå¿½è§†ä¸ªä½“ç‰¹å¾ä¸å¹²é¢„åŠ¨æ€ä¹‹é—´çš„å¤æ‚ç›¸äº’ä½œç”¨ã€‚æˆ‘ä»¬é’ˆå¯¹å»¶è¿Ÿåé¦ˆç¯å¢ƒä¸‹çš„ä¸ªæ€§åŒ–èµ„æºåˆ†é…é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°å‹çš„ä¸¤çº§ä¸Šä¸‹æ–‡å¼ºç›—æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨å…·æœ‰åŠ¨æ€äººç¾¤ã€å®¹é‡çº¦æŸå’Œæ—¶é—´æ•æ„Ÿå½±å“çš„ç°å®ç¯å¢ƒä¸­è¿è¡Œã€‚åœ¨å…ƒçº§åˆ«ä¸Šï¼Œè¯¥æ¨¡å‹ä¼˜åŒ–ç¾¤ä½“å±‚é¢çš„é¢„ç®—åˆ†é…ä»¥æ»¡è¶³å…¬å¹³æ€§å’Œæ“ä½œçº¦æŸã€‚åœ¨åŸºç¡€å±‚é¢ä¸Šï¼Œå®ƒåˆ©ç”¨åŸºäºè§‚æµ‹æ•°æ®è®­ç»ƒçš„ç¥ç»ç½‘ç»œè¯†åˆ«æ¯ç»„ä¸­æœ€æœ‰ååº”çš„ä¸ªäººï¼ŒåŒæ—¶å°Šé‡å†·å´çª—å£å’Œé€šè¿‡ç‰¹å®šèµ„æºå»¶è¿Ÿå†…æ ¸å»ºæ¨¡çš„å»¶è¿Ÿæ²»ç–—æ•ˆæœã€‚é€šè¿‡æ˜¾å¼å»ºæ¨¡æ—¶é—´åŠ¨æ€å’Œåé¦ˆå»¶è¿Ÿï¼Œè¯¥ç®—æ³•éšç€æ–°æ•°æ®çš„åˆ°æ¥ä¸æ–­å®Œå–„å…¶æ”¿ç­–ï¼Œä½¿å†³ç­–æ›´å…·å“åº”æ€§å’Œé€‚åº”æ€§ã€‚æˆ‘ä»¬é€šè¿‡æ•™è‚²å’Œå·¥ä½œåŠ›å‘å±•é¢†åŸŸçš„ä¸¤ä¸ªçœŸå®æ•°æ®é›†éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜å®ƒå®ç°äº†æ›´é«˜çš„ç´¯ç§¯æˆæœï¼Œæ›´å¥½åœ°é€‚åº”äº†å»¶è¿Ÿç»“æ„ï¼Œå¹¶ç¡®ä¿äº†åœ¨å„ç¾¤ä½“ä¹‹é—´çš„å…¬å¹³åˆ†é…ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†å»¶è¿Ÿæ„ŸçŸ¥ã€æ•°æ®é©±åŠ¨çš„å†³ç­–ç³»ç»Ÿçš„æ½œåŠ›ï¼Œå¯ä»¥æ”¹è¿›æœºæ„æ”¿ç­–å¹¶æé«˜ç¤¾ä¼šç¦åˆ©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10572v1">PDF</a> Accepted at AAAI-26 (AISI Track). Final version to appear in the Proceedings of the AAAI Conference on Artificial Intelligence (AAAI-26), 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ä¸¤çº§ä¸Šä¸‹æ–‡å¼ºç›—æ¡†æ¶ï¼Œç”¨äºå»¶è¿Ÿåé¦ˆä¸‹çš„ä¸ªæ€§åŒ–èµ„æºåˆ†é…ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æ»¡è¶³å…¬å¹³æ€§å’Œæ“ä½œçº¦æŸæ¡ä»¶çš„åŒæ—¶ï¼Œè§£å†³ç°å®ä¸–ç•Œç¯å¢ƒä¸­çš„åŠ¨æ€äººç¾¤å’Œèµ„æºåˆ†é…é—®é¢˜ã€‚å®ƒé€šè¿‡ç¥ç»ç½‘ç»œè®­ç»ƒè§‚å¯Ÿæ•°æ®æ¥è¯†åˆ«æœ€å…·å“åº”æ€§çš„ä¸ªä½“ï¼ŒåŒæ—¶å»ºç«‹å»¶è¿Ÿå¤„ç†æ•ˆæœæ¨¡å‹ã€‚è¿™ç§ç®—æ³•å¯éšç€æ–°æ•°æ®çš„åˆ°æ¥ä¸æ–­ä¼˜åŒ–å…¶ç­–ç•¥ï¼Œå®ç°æ›´å¿«é€Ÿã€çµæ´»çš„å†³ç­–åˆ¶å®šã€‚é€šè¿‡å¯¹æ•™è‚²å’ŒåŠ³åŠ¨åŠ›å‘å±•ä¸¤ä¸ªçœŸå®æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œè¯æ˜è¯¥æ¡†æ¶å¯å®ç°æ›´é«˜çš„ç´¯ç§¯æˆæœã€æ›´å¥½çš„å»¶è¿Ÿé€‚åº”æ€§ä»¥åŠè·¨å­ç¾¤çš„å…¬å¹³åˆ†é…ã€‚è¿™å‡¸æ˜¾äº†å»¶è¿Ÿæ„ŸçŸ¥æ•°æ®é©±åŠ¨å†³ç­–ç³»ç»Ÿçš„æ½œåŠ›ï¼Œæœ‰åŠ©äºæ”¹å–„æœºæ„æ”¿ç­–å’Œç¤¾ä¼šç¦åˆ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å¼ºè°ƒèµ„æºåˆ†é…çš„å¤æ‚æ€§ï¼Œæ¶‰åŠçŸ­æœŸæ•ˆç”¨ä¸é•¿æœŸå½±å“çš„å¹³è¡¡ï¼Œéœ€è¦è€ƒè™‘å»¶è¿Ÿç»“æœã€éšæ€§å¼‚è´¨æ€§åŠä¼¦ç†é™åˆ¶ç­‰å› ç´ ã€‚</li>
<li>æå‡ºæ–°å‹ä¸¤çº§ä¸Šä¸‹æ–‡å¼ºç›—æ¡†æ¶å¤„ç†ä¸ªæ€§åŒ–èµ„æºåˆ†é…é—®é¢˜ï¼Œé€‚åº”ç°å®ä¸–ç•Œçš„åŠ¨æ€ç¯å¢ƒåŠèµ„æºé™åˆ¶ã€‚</li>
<li>é€šè¿‡ç¥ç»ç½‘ç»œè¯†åˆ«å¯¹å¹²é¢„æªæ–½ååº”æœ€å¼ºçƒˆçš„ä¸ªä½“ï¼ŒåŒæ—¶è€ƒè™‘èµ„æºåˆ†é…çš„å…¬å¹³æ€§å’Œæ“ä½œçº¦æŸã€‚</li>
<li>æ¨¡å‹å»ºç«‹å»¶è¿Ÿå¤„ç†æ•ˆæœçš„æœºåˆ¶ï¼Œä»¥åæ˜ çœŸå®ä¸–ç•Œçš„åé¦ˆå»¶è¿Ÿå’Œä¸´æ—¶åŠ¨æ€å˜åŒ–ã€‚</li>
<li>é€šè¿‡æ•™è‚²åŠåŠ³åŠ¨åŠ›å‘å±•é¢†åŸŸçš„çœŸå®æ•°æ®é›†éªŒè¯ï¼Œè¯æ˜è¯¥æ¡†æ¶åœ¨ç´¯ç§¯æˆæœã€å»¶è¿Ÿé€‚åº”æ€§åŠè·¨å­ç¾¤å…¬å¹³åˆ†é…æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>å»¶è¿Ÿæ„ŸçŸ¥æ•°æ®é©±åŠ¨å†³ç­–ç³»ç»Ÿå¯æ”¹å–„æœºæ„æ”¿ç­–å’Œç¤¾ä¼šç¦åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6c3ef48c77bf44186965b6df18ca825" align="middle">
<img src="https://picx.zhimg.com/v2-9d4c5fcaced11ebe6b9990f89d994128" align="middle">
<img src="https://picx.zhimg.com/v2-a687724cf75d035996ea46d5125a23cd" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Beyond-Elicitation-Provision-based-Prompt-Optimization-for-Knowledge-Intensive-Tasks"><a href="#Beyond-Elicitation-Provision-based-Prompt-Optimization-for-Knowledge-Intensive-Tasks" class="headerlink" title="Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks"></a>Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks</h2><p><strong>Authors:Yunzhe Xu, Zhuosheng Zhang, Zhe Liu</strong></p>
<p>While prompt optimization has emerged as a critical technique for enhancing language model performance, existing approaches primarily focus on elicitation-based strategies that search for optimal prompts to activate modelsâ€™ capabilities. These methods exhibit fundamental limitations when addressing knowledge-intensive tasks, as they operate within fixed parametric boundaries rather than providing the factual knowledge, terminology precision, and reasoning patterns required in specialized domains. To address these limitations, we propose Knowledge-Provision-based Prompt Optimization (KPPO), a framework that reformulates prompt optimization as systematic knowledge integration rather than potential elicitation. KPPO introduces three key innovations: 1) a knowledge gap filling mechanism for knowledge gap identification and targeted remediation; 2) a batch-wise candidate evaluation approach that considers both performance improvement and distributional stability; 3) an adaptive knowledge pruning strategy that balances performance and token efficiency, reducing up to 29% token usage. Extensive evaluation on 15 knowledge-intensive benchmarks from various domains demonstrates KPPOâ€™s superiority over elicitation-based methods, with an average performance improvement of ~6% over the strongest baseline while achieving comparable or lower token consumption. Code at: <a target="_blank" rel="noopener" href="https://github.com/xyz9911/KPPO">https://github.com/xyz9911/KPPO</a>.</p>
<blockquote>
<p>è™½ç„¶æç¤ºä¼˜åŒ–å·²ä½œä¸ºå¢å¼ºè¯­è¨€æ¨¡å‹æ€§èƒ½çš„å…³é”®æŠ€æœ¯å‡ºç°ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åŸºäºæ¿€å‘çš„ç­–ç•¥ä¸Šï¼Œå¯»æ‰¾æœ€ä¼˜æç¤ºæ¥æ¿€æ´»æ¨¡å‹çš„èƒ½åŠ›ã€‚è¿™äº›æ–¹æ³•åœ¨åº”å¯¹çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶å­˜åœ¨æ ¹æœ¬æ€§å±€é™ï¼Œå› ä¸ºå®ƒä»¬ä»…åœ¨å›ºå®šçš„å‚æ•°è¾¹ç•Œå†…æ“ä½œï¼Œè€Œä¸æ˜¯æä¾›ç‰¹å®šé¢†åŸŸæ‰€éœ€çš„äº‹å®çŸ¥è¯†ã€æœ¯è¯­ç²¾åº¦å’Œæ¨ç†æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçŸ¥è¯†æä¾›çš„æç¤ºä¼˜åŒ–ï¼ˆKPPOï¼‰æ¡†æ¶ï¼Œå°†æç¤ºä¼˜åŒ–é‡æ–°å®šä¹‰ä¸ºç³»ç»ŸçŸ¥è¯†é›†æˆï¼Œè€Œéæ½œåœ¨æ¿€å‘ã€‚KPPOå¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼š1) çŸ¥è¯†ç¼ºå£å¡«å……æœºåˆ¶ï¼Œç”¨äºè¯†åˆ«å’Œç›®æ ‡åŒ–çŸ¥è¯†ç¼ºå£ï¼›2) æ‰¹é‡å€™é€‰è¯„ä¼°æ–¹æ³•ï¼ŒåŒæ—¶è€ƒè™‘æ€§èƒ½æå‡å’Œåˆ†å¸ƒç¨³å®šæ€§ï¼›3) è‡ªé€‚åº”çŸ¥è¯†ä¿®å‰ªç­–ç•¥ï¼Œå¹³è¡¡æ€§èƒ½å’Œä»¤ç‰Œæ•ˆç‡ï¼Œå‡å°‘æœ€å¤šè¾¾29%çš„ä»¤ç‰Œä½¿ç”¨ã€‚åœ¨15ä¸ªæ¥è‡ªä¸åŒé¢†åŸŸçš„çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒKPPOä¼˜äºåŸºäºæ¿€å‘çš„æ–¹æ³•ï¼Œåœ¨æœ€å¼ºåŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æ€§èƒ½æé«˜çº¦6%ï¼ŒåŒæ—¶å®ç°ç›¸å½“æˆ–æ›´ä½çš„ä»¤ç‰Œæ¶ˆè€—ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/xyz9911/KPPO">https://github.com/xyz9911/KPPO</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10465v1">PDF</a> 16 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºçŸ¥è¯†ä¾›ç»™çš„æç¤ºä¼˜åŒ–ï¼ˆKPPOï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç³»ç»Ÿæ•´åˆçŸ¥è¯†ï¼Œè€Œéä»…ä¾èµ–æ½œåœ¨æç¤ºï¼Œå¼•å…¥ä¸‰é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šå¡«è¡¥çŸ¥è¯†å·®è·çš„æœºåˆ¶ã€æ‰¹é‡å€™é€‰è¯„ä¼°æ–¹æ³•å’Œè‡ªé€‚åº”çŸ¥è¯†ä¿®å‰ªç­–ç•¥ã€‚åœ¨å¤šä¸ªçŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒKPPOæ˜¾è‘—ä¼˜äºåŸºäºæç¤ºçš„æ–¹æ³•ï¼Œå¹³å‡æ€§èƒ½æé«˜çº¦6%ï¼ŒåŒæ—¶å®ç°è¾ƒä½æˆ–ç›¸å½“çš„ä»¤ç‰Œæ¶ˆè€—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KPPOæ¡†æ¶è§£å†³äº†ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚</li>
<li>KPPOé€šè¿‡ç³»ç»Ÿæ•´åˆçŸ¥è¯†è€Œéä»…ä¾èµ–æ½œåœ¨æç¤ºè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>KPPOå¼•å…¥ä¸‰é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šçŸ¥è¯†å·®è·å¡«è¡¥æœºåˆ¶ã€æ‰¹é‡å€™é€‰è¯„ä¼°æ–¹æ³•å’Œè‡ªé€‚åº”çŸ¥è¯†ä¿®å‰ªç­–ç•¥ã€‚</li>
<li>çŸ¥è¯†å·®è·å¡«è¡¥æœºåˆ¶å¯è¯†åˆ«å¹¶é’ˆå¯¹æ€§è§£å†³çŸ¥è¯†å·®è·ã€‚</li>
<li>æ‰¹é‡å€™é€‰è¯„ä¼°æ–¹æ³•åŒæ—¶è€ƒè™‘æ€§èƒ½æå‡å’Œåˆ†å¸ƒç¨³å®šæ€§ã€‚</li>
<li>è‡ªé€‚åº”çŸ¥è¯†ä¿®å‰ªç­–ç•¥å¹³è¡¡äº†æ€§èƒ½å’Œä»¤ç‰Œæ•ˆç‡ï¼Œæœ€å¤šå¯å‡å°‘29%çš„ä»¤ç‰Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-899c12b13fe0005d2df451c8debf0bc9" align="middle">
<img src="https://picx.zhimg.com/v2-88eb149ad7cf016074061754bd24f75e" align="middle">
<img src="https://picx.zhimg.com/v2-6c668b0ee84bd72cfd4b1e6e51a37f63" align="middle">
<img src="https://picx.zhimg.com/v2-878342177da068c67210b4ec0b608994" align="middle">
<img src="https://picx.zhimg.com/v2-7a74ed15d165f5fbf2b1b0704ebefe44" align="middle">
<img src="https://picx.zhimg.com/v2-2ba46e8e0fd7f28094788770bf4b453c" align="middle">
<img src="https://picx.zhimg.com/v2-22dc501c4d8587bc311ad0370e400bea" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BhashaKritika-Building-Synthetic-Pretraining-Data-at-Scale-for-Indic-Languages"><a href="#BhashaKritika-Building-Synthetic-Pretraining-Data-at-Scale-for-Indic-Languages" class="headerlink" title="BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages"></a>BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages</h2><p><strong>Authors:Guduru Manoj, Neel Prabhanjan Rachamalla, Ashish Kulkarni, Gautam Rajeev, Jay Piplodiya, Arul Menezes, Shaharukh Khan, Souvik Rana, Manya Sah, Chandra Khatri, Shubham Agarwal</strong></p>
<p>In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora.</p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒèƒŒæ™¯ä¸‹ï¼Œåˆæˆæ•°æ®ä½œä¸ºç”Ÿæˆå¤§è§„æ¨¡é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®çš„ä¸€ç§æ›¿ä»£æ–¹æ³•åº”è¿è€Œç”Ÿã€‚è¿™åœ¨èµ„æºè´«ä¹çš„è¯­è¨€ç¯å¢ƒä¸­ç‰¹åˆ«æœ‰ç›Šï¼Œå› ä¸ºåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼Œæœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹æ‰€å¸¦æ¥çš„å¥½å¤„åœ¨ä¸åŒçš„è¯­è¨€ä¸­åˆ†å¸ƒä¸å‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹å°åº¦è¯­è¨€åˆæˆå¤šè¯­è¨€é¢„è®­ç»ƒæ•°æ®çš„ç”Ÿæˆå’Œè¯„ä¼°è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚æˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡åˆæˆæ•°æ®é›†BhashaKritikaï¼Œè¯¥æ•°æ®é›†ç”±ä½¿ç”¨äº”ç§ä¸åŒæŠ€æœ¯ç”Ÿæˆçš„æ¶µç›–åç§è¯­è¨€çš„54äº¿ä¸ªä»¤ç‰Œç»„æˆã€‚æˆ‘ä»¬æ¢è®¨äº†ä»¥æ–‡æ¡£ã€äººç‰©å’Œä¸»é¢˜ä¸ºæ ¹æ®ç”Ÿæˆçš„å½±å“ã€‚æˆ‘ä»¬åˆ†æäº†æç¤ºæŒ‡ä»¤å’Œæ–‡æ¡£ä¾æ®ä¸­çš„è¯­è¨€é€‰æ‹©æ˜¯å¦‚ä½•å½±å“æ•°æ®è´¨é‡çš„ï¼Œå¹¶æ¯”è¾ƒäº†è‹±è¯­å†…å®¹çš„ç¿»è¯‘å’Œåœ¨å°åº¦è¯­ä¸­çš„æœ¬åœ°ç”Ÿæˆã€‚ä¸ºäº†æ”¯æŒå¯æ‰©å±•æ€§å’Œè¯­è¨€æ•æ„Ÿæ€§çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡å—åŒ–è´¨é‡è¯„ä¼°ç®¡é“ï¼Œè¯¥ç®¡é“é›†æˆäº†è„šæœ¬å’Œè¯­è¨€æ£€æµ‹ã€å…ƒæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ã€n-gramé‡å¤åˆ†æä»¥åŠä½¿ç”¨KenLMæ¨¡å‹çš„åŸºäºå›°æƒ‘åº¦çš„è¿‡æ»¤ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨å„ç§è„šæœ¬å’Œè¯­è¨€ç¯å¢ƒä¸­å®ç°ç¨³å¥çš„è´¨é‡æ§åˆ¶ã€‚é€šè¿‡æ¨¡å‹è¿è¡Œå¾—åˆ°çš„å®è¯ç»“æœæ­ç¤ºäº†ç”Ÿæˆç­–ç•¥çš„å…³é”®æƒè¡¡ï¼Œå¹¶å¼ºè°ƒäº†æ„å»ºæœ‰æ•ˆå¤šè¯­è¨€è¯­æ–™åº“çš„æœ€ä½³å®è·µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10338v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åˆæˆæ•°æ®åœ¨é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹çš„ä¼˜åŠ¿ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹å°åº¦è¯­è¨€ç”Ÿæˆå¹¶è¯„ä¼°äº†å¤§è§„æ¨¡åˆæˆé¢„è®­ç»ƒæ•°æ®é›†BhashaKritikaï¼ŒåŒ…å«540Bæ ‡è®°ï¼Œä½¿ç”¨5ç§æŠ€æœ¯ä¸º10ç§è¯­è¨€ç”Ÿæˆã€‚è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºæ–‡æ¡£ã€äººç‰©å’Œä¸»é¢˜çš„ç”Ÿæˆå½±å“ï¼Œåˆ†æäº†æŒ‡ä»¤è¯­è¨€å’Œæ–‡æ¡£åŸºç¡€çš„è¯­è¨€é€‰æ‹©å¯¹æ•°æ®è´¨é‡çš„å½±å“ï¼Œå¹¶æ¯”è¾ƒäº†è‹±è¯­å†…å®¹çš„ç¿»è¯‘ä¸å°åº¦è¯­è¨€çš„æœ¬åœ°ç”Ÿæˆã€‚ä¸ºæ”¯æŒå¯æ‰©å±•å’Œè¯­è¨€æ•æ„Ÿçš„è¯„ä»·ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥æ¨¡å—åŒ–è´¨é‡è¯„ä¼°ç®¡é“ï¼Œæ•´åˆè„šæœ¬å’Œè¯­è¨€æ£€æµ‹ã€å…ƒæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ã€n-gramé‡å¤åˆ†æå’ŒåŸºäºKenLMæ¨¡å‹çš„å›°æƒ‘åº¦è¿‡æ»¤ã€‚æ­¤æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸åŒè„šæœ¬å’Œè¯­è¨€ç¯å¢ƒä¸‹å®ç°ç¨³å¥çš„è´¨é‡æ§åˆ¶ã€‚å®è¯ç»“æœæ­ç¤ºäº†ç”Ÿæˆç­–ç•¥çš„å…³é”®æƒè¡¡ï¼Œå¹¶å¼ºè°ƒäº†æ„å»ºæœ‰æ•ˆå¤šè¯­è¨€è¯­æ–™åº“çš„æœ€ä½³å®è·µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆæ•°æ®ä½œä¸ºå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†é’ˆå¯¹å°åº¦è¯­è¨€çš„åˆæˆå¤šè¯­è¨€é¢„è®­ç»ƒæ•°æ®é›†BhashaKritikaï¼ŒåŒ…å«540Bæ ‡è®°ï¼Œä½¿ç”¨5ç§æŠ€æœ¯ä¸º10ç§è¯­è¨€ç”Ÿæˆã€‚</li>
<li>æ¢è®¨äº†åŸºäºæ–‡æ¡£ã€äººç‰©å’Œä¸»é¢˜çš„ç”Ÿæˆæ–¹æ³•çš„å½±å“ï¼Œå¹¶åˆ†æäº†è¯­è¨€é€‰æ‹©å¯¹æ•°æ®è´¨é‡çš„å½±å“ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†è‹±è¯­å†…å®¹çš„ç¿»è¯‘ä¸å°åº¦è¯­è¨€çš„æœ¬åœ°ç”Ÿæˆã€‚</li>
<li>å¼•å…¥æ¨¡å—åŒ–è´¨é‡è¯„ä¼°ç®¡é“ï¼ŒåŒ…æ‹¬è„šæœ¬å’Œè¯­è¨€æ£€æµ‹ã€å…ƒæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ç­‰ï¼Œå®ç°ç¨³å¥çš„è´¨é‡æ§åˆ¶ã€‚</li>
<li>å®è¯ç»“æœæ­ç¤ºäº†ç”Ÿæˆç­–ç•¥çš„å…³é”®æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc9d72dd9c73dc4f48d85466d29b7ea7" align="middle">
<img src="https://picx.zhimg.com/v2-8bca1f66d9b556853a894e17488db31a" align="middle">
<img src="https://picx.zhimg.com/v2-f99fffa254a0a462eeb3ba3b725e3698" align="middle">
<img src="https://picx.zhimg.com/v2-17e842ff6356c253a2c1feae130f3d7d" align="middle">
<img src="https://picx.zhimg.com/v2-89acea9b3788e69c9338000740865fdd" align="middle">
<img src="https://picx.zhimg.com/v2-35d205c63b74cf575a82ce25d992c01f" align="middle">
<img src="https://picx.zhimg.com/v2-10a61ae9d14b55c78f61a20a84baac17" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Speech-Audio-Compositional-Attacks-on-Multimodal-LLMs-and-Their-Mitigation-with-SALMONN-Guard"><a href="#Speech-Audio-Compositional-Attacks-on-Multimodal-LLMs-and-Their-Mitigation-with-SALMONN-Guard" class="headerlink" title="Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard"></a>Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard</h2><p><strong>Authors:Yudong Yang, Xuezhen Zhang, Zhifeng Han, Siyin Wang, Jimin Zhuang, Zengrui Jin, Jing Shao, Guangzhi Sun, Chao Zhang</strong></p>
<p>Recent progress in large language models (LLMs) has enabled understanding of both speech and non-speech audio, but exposing new safety risks emerging from complex audio inputs that are inadequately handled by current safeguards. We introduce SACRED-Bench (Speech-Audio Composition for RED-teaming) to evaluate the robustness of LLMs under complex audio-based attacks. Unlike existing perturbation-based methods that rely on noise optimization or white-box access, SACRED-Bench exploits speech-audio composition mechanisms. SACRED-Bench adopts three mechanisms: (a) speech overlap and multi-speaker dialogue, which embeds harmful prompts beneath or alongside benign speech; (b) speech-audio mixture, which imply unsafe intent via non-speech audio alongside benign speech or audio; and (c) diverse spoken instruction formats (open-ended QA, yes&#x2F;no) that evade text-only filters. Experiments show that, even Gemini 2.5 Pro, the state-of-the-art proprietary LLM, still exhibits 66% attack success rate in SACRED-Bench test set, exposing vulnerabilities under cross-modal, speech-audio composition attacks. To bridge this gap, we propose SALMONN-Guard, a safeguard LLM that jointly inspects speech, audio, and text for safety judgments, reducing attack success down to 20%. Our results highlight the need for audio-aware defenses for the safety of multimodal LLMs. The benchmark and SALMONN-Guard checkpoints can be found at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench">https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench</a>. Warning: this paper includes examples that may be offensive or harmful.</p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ä½¿å¾—ç†è§£å’Œå¤„ç†è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘æˆä¸ºå¯èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿæš´éœ²å‡ºå½“å‰ä¿éšœæªæ–½æœªèƒ½å……åˆ†åº”å¯¹å¤æ‚éŸ³é¢‘è¾“å…¥æ‰€å¸¦æ¥çš„æ–°å®‰å…¨å¨èƒã€‚æˆ‘ä»¬æ¨å‡ºSACRED-Benchï¼ˆç”¨äºçº¢é˜Ÿå¯¹æŠ—çš„è¯­éŸ³éŸ³é¢‘ç»„åˆï¼‰ï¼Œä»¥è¯„ä¼°å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹LLMçš„ç¨³å¥æ€§ã€‚ä¸ç°æœ‰çš„åŸºäºæ‰°åŠ¨çš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå™ªå£°ä¼˜åŒ–æˆ–ç™½ç›’è®¿é—®ï¼ŒSACRED-Benchåˆ™åˆ©ç”¨è¯­éŸ³éŸ³é¢‘ç»„åˆæœºåˆ¶ã€‚SACRED-Benché‡‡ç”¨ä¸‰ç§æœºåˆ¶ï¼šï¼ˆaï¼‰è¯­éŸ³é‡å å’Œå¤šäººå¯¹è¯ï¼Œåœ¨è‰¯æ€§è¯­éŸ³ä¹‹ä¸‹æˆ–æ—è¾¹åµŒå…¥æœ‰å®³æç¤ºï¼›ï¼ˆbï¼‰è¯­éŸ³éŸ³é¢‘æ··åˆï¼Œé€šè¿‡éè¯­éŸ³éŸ³é¢‘æš—ç¤ºä¸å®‰å…¨æ„å›¾ï¼Œä¼´éšè‰¯æ€§è¯­éŸ³æˆ–éŸ³é¢‘ï¼›ï¼ˆcï¼‰å¤šæ ·çš„å£å¤´æŒ‡ä»¤æ ¼å¼ï¼ˆå¼€æ”¾å¼é—®ç­”ã€æ˜¯éé¢˜ï¼‰è§„é¿ä»…æ–‡æœ¬è¿‡æ»¤å™¨ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨æœ€æ–°å…ˆè¿›çš„ä¸“æœ‰LLMâ€”â€”Gemini 2.5 Proä¸­ï¼Œåœ¨SACRED-Benchæµ‹è¯•é›†ä¸­æ”»å‡»æˆåŠŸç‡ä»é«˜è¾¾66%ï¼Œæš´éœ²å‡ºè·¨æ¨¡æ€å’Œè¯­éŸ³éŸ³é¢‘ç»„åˆæ”»å‡»ä¸‹çš„æ¼æ´ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºSALMONN-Guardï¼Œä¸€ä¸ªå®‰å…¨ä¿æŠ¤çš„LLMï¼Œèƒ½åŒæ—¶æ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬ä»¥åšå‡ºå®‰å…¨åˆ¤æ–­ï¼Œå°†æ”»å‡»æˆåŠŸç‡é™ä½åˆ°ä»…20%ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†ä¸ºå¤šåª’ä½“LLMçš„å®‰å…¨æ„å»ºéŸ³é¢‘æ„ŸçŸ¥é˜²å¾¡çš„å¿…è¦æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•å’ŒSALMONN-Guardæ£€æŸ¥ç‚¹å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench%E6%89%BE%E5%88%B0%E3%80%82%E8%AD%A6%E5%91%8A%EF%BC%9A%E6%9C%AC%E7%BA%BA%E4%BB%A3%E5%8C%BA%E5%8D%B7%E4%BB%BD%E6%AF%94%E8%BE%BD%E5%AF%BC%E5%8F%AF%E8%83%BD%E5%A4%9F%E6%AD%A3%%E3%80%82">https://huggingface.co/datasets/tsinghua-ee/SACRED-Benchæ‰¾åˆ°ã€‚è­¦å‘Šï¼šæœ¬è®ºæ–‡åŒ…å«å¯èƒ½å…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³çš„ç¤ºä¾‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10222v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘æ–¹é¢çš„æœ€æ–°è¿›å±•æ‰€å¼•å‘çš„æ–°å®‰å…¨é£é™©ï¼Œå¹¶å¼•å…¥äº†SACRED-BenchåŸºå‡†æµ‹è¯•å¹³å°æ¥è¯„ä¼°LLMsåœ¨å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹çš„ç¨³å¥æ€§ã€‚æ–‡ç« è¿˜æå‡ºäº†SALMONN-Guardå®‰å…¨ä¿æŠ¤æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆè”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬ä»¥è¿›è¡Œå®‰å…¨åˆ¤æ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç†è§£è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘ï¼Œä½†å­˜åœ¨æ–°çš„å®‰å…¨é£é™©ã€‚</li>
<li>SACRED-Benchç”¨äºè¯„ä¼°LLMsåœ¨å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹çš„ç¨³å¥æ€§ï¼Œé‡‡ç”¨ä¸‰ç§æœºåˆ¶ï¼šè¯­éŸ³é‡å å’Œå¤šè¯´è¯è€…å¯¹è¯ã€è¯­éŸ³éŸ³é¢‘æ··åˆä»¥åŠå¤šæ ·çš„å£è¯­æŒ‡ä»¤æ ¼å¼ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMï¼ˆå¦‚Gemini 2.5 Proï¼‰åœ¨SACRED-Benchæµ‹è¯•é›†ä¸Šçš„æ”»å‡»æˆåŠŸç‡ä¹Ÿè¾¾åˆ°66%ï¼Œæš´éœ²å‡ºè·¨æ¨¡æ€å’Œè¯­éŸ³éŸ³é¢‘ç»„åˆæ”»å‡»ä¸‹çš„æ¼æ´ã€‚</li>
<li>SALMONN-Guardæ˜¯ä¸€ç§å®‰å…¨ä¿æŠ¤æ–¹æ¡ˆï¼Œèƒ½å¤Ÿè”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬ä»¥è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼Œé™ä½äº†æ”»å‡»æˆåŠŸç‡è‡³20%ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†å¯¹äºå¤šæ¨¡å¼LLMsçš„éŸ³é¢‘æ„ŸçŸ¥é˜²å¾¡çš„å¿…è¦æ€§ã€‚</li>
<li>SACRED-Benchå’ŒSALMONN-Guardçš„æ£€æŸ¥ç‚¹å¯ä»¥åœ¨Hugging Faceä¸Šæ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eec535246af685e907211e048b9a24e8" align="middle">
<img src="https://picx.zhimg.com/v2-6639799ebe64da9d7aecf974300000fa" align="middle">
<img src="https://picx.zhimg.com/v2-44e23b1b9e1e925cd24c70b552902557" align="middle">
<img src="https://picx.zhimg.com/v2-2a9120299be6e2c693cf76d5dda63146" align="middle">
<img src="https://picx.zhimg.com/v2-b497c205ce19b296f6ab26f0faad2bbd" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GraphIF-Enhancing-Multi-Turn-Instruction-Following-for-Large-Language-Models-with-Relation-Graph-Prompt"><a href="#GraphIF-Enhancing-Multi-Turn-Instruction-Following-for-Large-Language-Models-with-Relation-Graph-Prompt" class="headerlink" title="GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt"></a>GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt</h2><p><strong>Authors:Zhenhe Li, Can Lin, Ling Zheng, Wen-Da Wei, Junli Liang, Qi Song</strong></p>
<p>Multi-turn instruction following is essential for building intelligent conversational systems that can consistently adhere to instructions across dialogue turns. However, existing approaches to enhancing multi-turn instruction following primarily rely on collecting or generating large-scale multi-turn dialogue datasets to fine-tune large language models (LLMs), which treat each response generation as an isolated task and fail to explicitly incorporate multi-turn instruction following into the optimization objectives. As a result, instruction-tuned LLMs often struggle with complex long-distance constraints. In multi-turn dialogues, relational constraints across turns can be naturally modeled as labeled directed edges, making graph structures particularly suitable for modeling multi-turn instruction following. Despite this potential, leveraging graph structures to enhance the multi-turn instruction following capabilities of LLMs remains unexplored. To bridge this gap, we propose GraphIF, a plug-and-play framework that models multi-turn dialogues as directed relation graphs and leverages graph prompts to enhance the instruction following capabilities of LLMs. GraphIF comprises three key components: (1) an agent-based relation extraction module that captures inter-turn semantic relations via action-triggered mechanisms to construct structured graphs; (2) a relation graph prompt generation module that converts structured graph information into natural language prompts; and (3) a response rewriting module that refines initial LLM outputs using the generated graph prompts. Extensive experiments on two long multi-turn dialogue datasets demonstrate that GraphIF can be seamlessly integrated into instruction-tuned LLMs and leads to significant improvements across all four multi-turn instruction-following evaluation metrics.</p>
<blockquote>
<p>å¤šè½®æŒ‡ä»¤è·Ÿéšå¯¹äºæ„å»ºèƒ½å¤Ÿå§‹ç»ˆåœ¨å¯¹è¯å›åˆä¸­éµå¾ªæŒ‡ä»¤çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¢å¼ºå¤šè½®æŒ‡ä»¤è·Ÿéšçš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ”¶é›†æˆ–ç”Ÿæˆå¤§è§„æ¨¡çš„å¤šè½®å¯¹è¯æ•°æ®é›†æ¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¿™äº›æ–¹æ³•å°†æ¯ä¸ªå“åº”ç”Ÿæˆè§†ä¸ºä¸€é¡¹ç‹¬ç«‹ä»»åŠ¡ï¼Œå¹¶æœªæ˜¾å¼åœ°å°†å¤šè½®æŒ‡ä»¤è·Ÿéšçº³å…¥ä¼˜åŒ–ç›®æ ‡ä¸­ã€‚å› æ­¤ï¼ŒæŒ‡ä»¤è°ƒæ•´è¿‡çš„LLMsé€šå¸¸é¢ä¸´å¤æ‚çš„é•¿æœŸçº¦æŸæŒ‘æˆ˜ã€‚åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œè·¨å›åˆçš„å…³ç³»çº¦æŸå¯ä»¥è‡ªç„¶åœ°å»ºæ¨¡ä¸ºæœ‰æ ‡ç­¾çš„æœ‰å‘è¾¹ï¼Œä½¿å›¾ç»“æ„ç‰¹åˆ«é€‚åˆå»ºæ¨¡å¤šè½®æŒ‡ä»¤è·Ÿéšã€‚å°½ç®¡å­˜åœ¨è¿™ç§æ½œåŠ›ï¼Œä½†åˆ©ç”¨å›¾ç»“æ„å¢å¼ºLLMsçš„å¤šè½®æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ä»å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†GraphIFï¼Œè¿™æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œå®ƒå°†å¤šè½®å¯¹è¯å»ºæ¨¡ä¸ºæœ‰å‘å…³ç³»å›¾ï¼Œå¹¶åˆ©ç”¨å›¾æç¤ºå¢å¼ºLLMsçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚GraphIFåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŸºäºä»£ç†çš„å…³ç³»æå–æ¨¡å—ï¼Œé€šè¿‡åŠ¨ä½œè§¦å‘æœºåˆ¶æ•è·å›åˆé—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»¥æ„å»ºç»“æ„åŒ–çš„å›¾ï¼›ï¼ˆ2ï¼‰å…³ç³»å›¾æç¤ºç”Ÿæˆæ¨¡å—ï¼Œå°†ç»“æ„åŒ–å›¾ä¿¡æ¯è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æç¤ºï¼›ï¼ˆ3ï¼‰å“åº”é‡å†™æ¨¡å—ï¼Œä½¿ç”¨ç”Ÿæˆçš„å›¾æç¤ºå¯¹åˆå§‹LLMè¾“å‡ºè¿›è¡Œç²¾ç‚¼ã€‚åœ¨ä¸¤ä¸ªé•¿çš„å¤šè½®å¯¹è¯æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGraphIFå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°æŒ‡ä»¤è°ƒæ•´è¿‡çš„LLMsä¸­ï¼Œå¹¶åœ¨æ‰€æœ‰å››ä¸ªå¤šè½®æŒ‡ä»¤è·Ÿéšè¯„ä¼°æŒ‡æ ‡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10051v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒåœ¨æ„å»ºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿæ—¶ï¼Œå¤šè½®æŒ‡ä»¤è·Ÿéšçš„é‡è¦æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ”¶é›†æˆ–ç”Ÿæˆå¤§è§„æ¨¡å¤šè½®å¯¹è¯æ•°æ®é›†æ¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½†å®ƒä»¬æœªèƒ½æ˜¾å¼åœ°å°†å¤šè½®æŒ‡ä»¤è·Ÿéšçº³å…¥ä¼˜åŒ–ç›®æ ‡ã€‚å› æ­¤ï¼Œæå‡ºGraphIFæ¡†æ¶ï¼Œå°†å¤šè½®å¯¹è¯å»ºæ¨¡ä¸ºæœ‰å‘å…³ç³»å›¾ï¼Œå¹¶åˆ©ç”¨å›¾æç¤ºå¢å¼ºLLMsçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶å¯æ— ç¼é›†æˆåˆ°æŒ‡ä»¤è°ƒæ•´è¿‡çš„LLMsä¸­ï¼Œå¹¶åœ¨æ‰€æœ‰å››é¡¹å¤šè½®æŒ‡ä»¤è·Ÿè¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè½®æŒ‡ä»¤è·Ÿéšå¯¹äºæ„å»ºèƒ½å¤Ÿå§‹ç»ˆéµå¾ªæŒ‡ä»¤çš„å¯¹è¯ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½†æœªå……åˆ†è€ƒè™‘å¤šè½®æŒ‡ä»¤è·Ÿéšã€‚</li>
<li>GraphIFæ¡†æ¶å°†å¤šè½®å¯¹è¯å»ºæ¨¡ä¸ºå…³ç³»å›¾ï¼Œä»¥æ›´å¥½åœ°å¤„ç†è·¨è½®çš„å…³ç³»çº¦æŸã€‚</li>
<li>GraphIFåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå…³ç³»æå–æ¨¡å—ã€å…³ç³»å›¾æç¤ºç”Ÿæˆæ¨¡å—å’Œå“åº”é‡å†™æ¨¡å—ã€‚</li>
<li>é€šè¿‡åœ¨ä¸¤ä¸ªé•¿å¤šè½®å¯¹è¯æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œè¯æ˜äº†GraphIFå¯ä»¥æ— ç¼é›†æˆåˆ°æŒ‡ä»¤è°ƒæ•´è¿‡çš„LLMsä¸­ã€‚</li>
<li>GraphIFåœ¨å››é¡¹å¤šè½®æŒ‡ä»¤è·Ÿè¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-221dc36f1fdaa208180b25c02e1e2502" align="middle">
<img src="https://picx.zhimg.com/v2-3b908dbce967c5b39229f9432fa0f80f" align="middle">
<img src="https://picx.zhimg.com/v2-bb3f5213991eaba559a4045aa2568f6a" align="middle">
<img src="https://picx.zhimg.com/v2-4f1aa7c2bc88e11e7c1e929980b40bc9" align="middle">
<img src="https://picx.zhimg.com/v2-a0a1fd3be81afdbed17d6d842d2c13da" align="middle">
<img src="https://picx.zhimg.com/v2-ac71a046d29c0c65184e64f9b64059a7" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples"><a href="#MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples" class="headerlink" title="MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples"></a>MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples</h2><p><strong>Authors:Xurui Li, Feng Xue, Yu Zhou</strong></p>
<p>Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC&#x2F;AS, which flexibly supports single 2D&#x2F;3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D&#x2F;3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a $\textbf{+23.7%}$ AP gain on the MVTec 3D-AD dataset and a $\textbf{+19.3%}$ boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at \href{<a target="_blank" rel="noopener" href="https://github.com/HUST-SLOW/MuSc-V2%7D%7Bhttps://github.com/HUST-SLOW/MuSc-V2%7D">https://github.com/HUST-SLOW/MuSc-V2}{https://github.com/HUST-SLOW/MuSc-V2}</a>.</p>
<blockquote>
<p>é›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»ï¼ˆACï¼‰å’Œåˆ†å‰²ï¼ˆASï¼‰æ–¹æ³•æ—¨åœ¨ä¸ä½¿ç”¨ä»»ä½•æ ‡è®°æ ·æœ¬æ¥è¯†åˆ«å’Œæè¿°ç¼ºé™·ã€‚æœ¬æ–‡æ­ç¤ºäº†ç°æœ‰æ–¹æ³•å¿½ç•¥çš„å…³é”®å±æ€§ï¼šå·¥ä¸šäº§å“ä¸­çš„æ­£å¸¸å›¾åƒæ–‘å—é€šå¸¸å¯ä»¥æ‰¾åˆ°è®¸å¤šå…¶ä»–ç›¸ä¼¼çš„æ–‘å—ï¼Œä¸ä»…åœ¨äºŒç»´å¤–è§‚ä¸Šï¼Œè€Œä¸”åœ¨ä¸‰ç»´å½¢çŠ¶ä¸Šï¼Œè€Œå¼‚å¸¸å€¼ä»ç„¶å¤šæ ·ä¸”å­¤ç«‹ã€‚ä¸ºäº†æ˜ç¡®åˆ©ç”¨è¿™ç§åˆ¤åˆ«å±æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºé›¶æ ·æœ¬AC&#x2F;ASçš„MuSc-V2æ¡†æ¶ï¼Œè¯¥æ¡†æ¶çµæ´»æ”¯æŒå•æ¨¡æ€çš„äºŒç»´&#x2F;ä¸‰ç»´æˆ–å¤šæ¨¡æ€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡è¿­ä»£ç‚¹åˆ†ç»„ï¼ˆIPGï¼‰æ”¹è¿›ä¸‰ç»´è¡¨ç¤ºï¼Œä»è€Œå‡å°‘æ¥è‡ªä¸è¿ç»­è¡¨é¢çš„è¯¯æŠ¥ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å…·æœ‰å¤šåº¦çš„ç›¸ä¼¼æ€§é‚»åŸŸèšåˆï¼ˆSNAMDï¼‰æ¥èåˆäºŒç»´&#x2F;ä¸‰ç»´é‚»åŸŸçº¿ç´¢ï¼Œä»¥ç”Ÿæˆæ›´å…·åˆ¤åˆ«åŠ›çš„å¤šå°ºåº¦æ–‘å—ç‰¹å¾æ¥è¿›è¡Œç›¸äº’è¯„åˆ†ã€‚æ ¸å¿ƒéƒ¨åˆ†åŒ…æ‹¬ç›¸äº’è¯„åˆ†æœºåˆ¶ï¼ˆMSMï¼‰ï¼Œå®ƒå…è®¸æ¯ä¸ªæ¨¡æ€å†…çš„æ ·æœ¬ç›¸äº’è¯„åˆ†ï¼Œä»¥åŠè·¨æ¨¡æ€å¼‚å¸¸å¢å¼ºï¼ˆCAEï¼‰ï¼Œå®ƒèåˆäº†äºŒç»´å’Œä¸‰ç»´åˆ†æ•°æ¥æ¢å¤ç‰¹å®šæ¨¡æ€ç¼ºå¤±çš„å¼‚å¸¸å€¼ã€‚æœ€åï¼Œé€šè¿‡çº¦æŸé‚»åŸŸè¿›è¡Œå†æ¬¡è¯„åˆ†ï¼ˆRsConï¼‰æŠ‘åˆ¶äº†åŸºäºä¸æ›´å…·ä»£è¡¨æ€§æ ·æœ¬ç›¸ä¼¼æ€§çš„è¯¯åˆ†ç±»ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å®Œæ•´æ•°æ®é›†å’Œå°å­é›†ä¸Šéƒ½èƒ½çµæ´»å·¥ä½œï¼Œå…·æœ‰æŒç»­ç¨³å®šçš„æ€§èƒ½ï¼Œç¡®ä¿åœ¨ä¸åŒäº§å“çº¿ä¹‹é—´æ— ç¼é€‚åº”ã€‚å€ŸåŠ©æ–°å‹æ¡†æ¶MuSc-V2å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼šåœ¨MVTec 3D-ADæ•°æ®é›†ä¸Šæé«˜äº†+23.7%çš„APå€¼ï¼Œåœ¨Eyecandiesæ•°æ®é›†ä¸Šæé«˜äº†+19.3%çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ï¼Œç”šè‡³è¶…è¶Šäº†å¤§å¤šæ•°å°æ ·æ–¹æ³•ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/HUST-SLOW/MuSc-V2">https://github.com/HUST-SLOW/MuSc-V2</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10047v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºMutual Scoringæ¡†æ¶ï¼ˆMuSc-V2ï¼‰çš„é›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»ä¸åˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨æ­£å¸¸å›¾åƒå—åœ¨å·¥ä¸šå“ä¸­çš„ç›¸ä¼¼æ€§å’Œè¿ç»­æ€§ï¼Œé€šè¿‡è¿­ä»£ç‚¹åˆ†ç»„å’Œæ”¹è¿›çš„å¤šå°ºåº¦ç›¸ä¼¼æ€§é‚»åŸŸèšåˆï¼Œæé«˜äº†å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å…·æœ‰è·¨æ¨¡æ€é€‚åº”æ€§ï¼Œå¯åœ¨ä¸åŒæ¨¡æ€çš„æ•°æ®é›†ä¸Šå®ç°è‰¯å¥½çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMuSc-V2åœ¨MVTec 3D-ADå’ŒEyecandiesæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¶…è¿‡äº†é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ï¼Œç”šè‡³ä¼˜äºå¤§å¤šæ•°å°‘æ ·æœ¬æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºMutual Scoringæ¡†æ¶ï¼ˆMuSc-V2ï¼‰çš„é›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»ä¸åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>å‘ç°äº†æ­£å¸¸å›¾åƒå—åœ¨å·¥ä¸šå“ä¸­çš„ç›¸ä¼¼æ€§å’Œè¿ç»­æ€§è¢«å¿½ç•¥çš„é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è¿™ä¸€ç‚¹æé«˜äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡è¿­ä»£ç‚¹åˆ†ç»„å’Œæ”¹è¿›çš„å¤šå°ºåº¦ç›¸ä¼¼æ€§é‚»åŸŸèšåˆï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶æ”¯æŒå•æ¨¡æ€ï¼ˆ2D&#x2F;3Dï¼‰å’Œå¤šæ¨¡æ€æ•°æ®ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>MuSc-V2æ¡†æ¶åœ¨MVTec 3D-ADå’ŒEyecandiesæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>MuSc-V2çš„æ€§èƒ½è¶…è¿‡äº†é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ï¼Œç”šè‡³ä¼˜äºå¤§å¤šæ•°å°‘æ ·æœ¬æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e0d72a6bad08efacf16adb64ed406dd" align="middle">
<img src="https://picx.zhimg.com/v2-26ed1aed70b801eb31422ff2b241964d" align="middle">
<img src="https://picx.zhimg.com/v2-64bdea85898a2a23c36adae03167e3d3" align="middle">
<img src="https://picx.zhimg.com/v2-8a9012993a2481f8ef1cb0bab65f4558" align="middle">
<img src="https://picx.zhimg.com/v2-3fe52fc29768f5e885d3f39128a7bf24" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-agent-In-context-Coordination-via-Decentralized-Memory-Retrieval"><a href="#Multi-agent-In-context-Coordination-via-Decentralized-Memory-Retrieval" class="headerlink" title="Multi-agent In-context Coordination via Decentralized Memory Retrieval"></a>Multi-agent In-context Coordination via Decentralized Memory Retrieval</h2><p><strong>Authors:Tao Jiang, Zichuan Lin, Lihe Li, Yi-Chen Li, Cong Guan, Lei Yuan, Zongzhang Zhang, Yang Yu, Deheng Ye</strong></p>
<p>Large transformer models, trained on diverse datasets, have demonstrated impressive few-shot performance on previously unseen tasks without requiring parameter updates. This capability has also been explored in Reinforcement Learning (RL), where agents interact with the environment to retrieve context and maximize cumulative rewards, showcasing strong adaptability in complex settings. However, in cooperative Multi-Agent Reinforcement Learning (MARL), where agents must coordinate toward a shared goal, decentralized policy deployment can lead to mismatches in task alignment and reward assignment, limiting the efficiency of policy adaptation. To address this challenge, we introduce Multi-agent In-context Coordination via Decentralized Memory Retrieval (MAICC), a novel approach designed to enhance coordination by fast adaptation. Our method involves training a centralized embedding model to capture fine-grained trajectory representations, followed by decentralized models that approximate the centralized one to obtain team-level task information. Based on the learned embeddings, relevant trajectories are retrieved as context, which, combined with the agentsâ€™ current sub-trajectories, inform decision-making. During decentralized execution, we introduce a novel memory mechanism that effectively balances test-time online data with offline memory. Based on the constructed memory, we propose a hybrid utility score that incorporates both individual- and team-level returns, ensuring credit assignment across agents. Extensive experiments on cooperative MARL benchmarks, including Level-Based Foraging (LBF) and SMAC (v1&#x2F;v2), show that MAICC enables faster adaptation to unseen tasks compared to existing methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/LAMDA-RL/MAICC">https://github.com/LAMDA-RL/MAICC</a>.</p>
<blockquote>
<p>å¤§å‹è½¬æ¢å™¨æ¨¡å‹åœ¨å¤šç§æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œåœ¨æœªè§è¿‡çš„æ–°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å°‘é‡æ ·æœ¬æ€§èƒ½ï¼Œæ— éœ€è¿›è¡Œå‚æ•°æ›´æ–°ã€‚è¿™ç§èƒ½åŠ›åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ä¹Ÿå¾—åˆ°äº†æ¢ç´¢ï¼Œå…¶ä¸­æ™ºèƒ½ä½“ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ä»¥è·å–ä¸Šä¸‹æ–‡å¹¶æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚ç¯å¢ƒä¸­çš„å¼ºå¤§é€‚åº”æ€§ã€‚ç„¶è€Œï¼Œåœ¨åˆä½œå‹å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­ï¼Œæ™ºèƒ½ä½“å¿…é¡»æœç€å…±åŒçš„ç›®æ ‡è¿›è¡Œåè°ƒï¼Œå»ä¸­å¿ƒåŒ–ç­–ç•¥éƒ¨ç½²å¯èƒ½å¯¼è‡´ä»»åŠ¡å¯¹é½å’Œå¥–åŠ±åˆ†é…ä¸åŒ¹é…çš„é—®é¢˜ï¼Œé™åˆ¶äº†ç­–ç•¥é€‚åº”çš„æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå»ä¸­å¿ƒåŒ–è®°å¿†æ£€ç´¢çš„å¤šæ™ºèƒ½ä½“ä¸Šä¸‹æ–‡åè°ƒï¼ˆMAICCï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¿«é€Ÿé€‚åº”å¢å¼ºåè°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬è®­ç»ƒä¸€ä¸ªé›†ä¸­å¼åµŒå…¥æ¨¡å‹æ¥æ•æ‰ç²¾ç»†çš„è½¨è¿¹è¡¨ç¤ºï¼Œéšåä½¿ç”¨å»ä¸­å¿ƒåŒ–æ¨¡å‹æ¥è¿‘ä¼¼é›†ä¸­å¼æ¨¡å‹ä»¥è·å¾—å›¢é˜Ÿçº§åˆ«çš„ä»»åŠ¡ä¿¡æ¯ã€‚åŸºäºå­¦ä¹ çš„åµŒå…¥ï¼Œæˆ‘ä»¬æ£€ç´¢ç›¸å…³çš„è½¨è¿¹ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä¸æ™ºèƒ½ä½“çš„å½“å‰å­è½¨è¿¹ç›¸ç»“åˆï¼Œä¸ºå†³ç­–æä¾›ä¾æ®ã€‚åœ¨å»ä¸­å¿ƒåŒ–æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„è®°å¿†æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥å¹³è¡¡æµ‹è¯•æ—¶çš„åœ¨çº¿æ•°æ®å’Œç¦»çº¿è®°å¿†ã€‚åŸºäºæ„å»ºçš„è®°å¿†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ··åˆæ•ˆç”¨è¯„åˆ†ï¼Œè¯¥è¯„åˆ†ç»“åˆäº†ä¸ªäººå’Œå›¢é˜Ÿå±‚é¢çš„å›æŠ¥ï¼Œç¡®ä¿æ™ºèƒ½ä½“ä¹‹é—´çš„ä¿¡ç”¨åˆ†é…ã€‚åœ¨åˆä½œå‹MARLåŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬åŸºäºçº§åˆ«çš„è§…é£Ÿï¼ˆLBFï¼‰å’ŒSMACï¼ˆv1&#x2F;v2ï¼‰ï¼Œè¡¨æ˜MAICCä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°é€‚åº”æœªè§è¿‡çš„ä»»åŠ¡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LAMDA-RL/MAICC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LAMDA-RL/MAICCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10030v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¤§å‹è½¬æ¢å™¨æ¨¡å‹åœ¨æœªè§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„å°‘æ ·æœ¬æ€§èƒ½ã€‚æ–‡ç« è¿˜æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„å¤šæ™ºèƒ½ä½“ååŒé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MAICCï¼ˆMulti-agent In-context Coordination via Decentralized Memory Retrievalï¼‰æ¥å¢å¼ºåè°ƒå¹¶åŠ é€Ÿé€‚åº”ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒä¸­å¤®åµŒå…¥æ¨¡å‹æ¥æ•æ‰ç²¾ç»†è½¨è¿¹è¡¨ç¤ºï¼Œç„¶åé€šè¿‡åˆ†æ•£æ¨¡å‹è¿‘ä¼¼ä¸­å¤®æ¨¡å‹ä»¥è·å–å›¢é˜Ÿçº§ä»»åŠ¡ä¿¡æ¯ã€‚åŸºäºå­¦ä¹ çš„åµŒå…¥ï¼Œæ£€ç´¢ç›¸å…³è½¨è¿¹ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œç»“åˆæ™ºèƒ½ä½“çš„å½“å‰å­è½¨è¿¹æ¥åšå‡ºå†³ç­–ã€‚åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„è®°å¿†æœºåˆ¶ï¼Œæœ‰æ•ˆå¹³è¡¡åœ¨çº¿æ•°æ®å’Œç¦»çº¿è®°å¿†ã€‚å®éªŒè¡¨æ˜ï¼ŒMAICCåœ¨åˆä½œMARLåŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œèƒ½å¤Ÿé€‚åº”æ›´å¿«çš„æ–°ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è½¬æ¢å™¨æ¨¡å‹åœ¨æœªè§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸­çš„å¤šæ™ºèƒ½ä½“ååŒé¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³ä»»åŠ¡å¯¹é½å’Œå¥–åŠ±åˆ†é…çš„é—®é¢˜ã€‚</li>
<li>MAICCæ–¹æ³•é€šè¿‡è®­ç»ƒä¸­å¤®åµŒå…¥æ¨¡å‹æ¥æ•æ‰ç²¾ç»†è½¨è¿¹è¡¨ç¤ºï¼Œæé«˜ååŒèƒ½åŠ›ã€‚</li>
<li>MAICCé€šè¿‡åˆ†æ•£æ¨¡å‹è¿‘ä¼¼ä¸­å¤®æ¨¡å‹è·å–å›¢é˜Ÿçº§ä»»åŠ¡ä¿¡æ¯ã€‚</li>
<li>ç›¸å…³è½¨è¿¹è¢«æ£€ç´¢ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œç»“åˆæ™ºèƒ½ä½“çš„å½“å‰å­è½¨è¿¹è¿›è¡Œå†³ç­–ã€‚</li>
<li>MAICCå¼•å…¥æ–°çš„è®°å¿†æœºåˆ¶ï¼Œæœ‰æ•ˆå¹³è¡¡åœ¨çº¿æ•°æ®å’Œç¦»çº¿è®°å¿†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8793668defaf33d2b276010f75b2418e" align="middle">
<img src="https://picx.zhimg.com/v2-538d80731247e3448e012717732f9da2" align="middle">
<img src="https://picx.zhimg.com/v2-462fa8dd46889eb97846d38fb41bfc4c" align="middle">
<img src="https://picx.zhimg.com/v2-62b73a758993ad8102f09174f688f261" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models"><a href="#AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models" class="headerlink" title="AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models"></a>AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models</h2><p><strong>Authors:Xinyi Wang, Xun Yang, Yanlong Xu, Yuchen Wu, Zhen Li, Na Zhao</strong></p>
<p>Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.</p>
<blockquote>
<p>å®ç°ç‰©ç†ç¯å¢ƒä¸­äººç±»ä¸æ™ºèƒ½ä½“ä¹‹é—´çš„æœ‰æ•ˆåä½œï¼Œä¸ä»…è¦æ±‚ç†è§£å¦‚ä½•è¡ŒåŠ¨ï¼Œè¿˜éœ€è¦ç†è§£å¯æ“ä½œå…ƒç´ çš„ä½ç½®ä»¥åŠå¦‚ä½•ä¸ä¹‹äº’åŠ¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»…åœ¨å¯¹è±¡çº§åˆ«è¿›è¡Œæ“ä½œï¼Œæˆ–è€…æ–­ç»­åœ°è¿›è¡Œç²¾ç»†çš„å¯ç”¨æ€§æ¨ç†ï¼Œç¼ºä¹è¿è´¯çš„ã€æŒ‡ä»¤é©±åŠ¨çš„æ¥åœ°å’Œæ¨ç†ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼šç²¾ç»†3DåµŒå…¥å¼æ¨ç†ï¼Œè¦æ±‚æ™ºèƒ½ä½“æ ¹æ®ä»»åŠ¡æŒ‡ä»¤ï¼Œé¢„æµ‹3Dåœºæ™¯ä¸­æ¯ä¸ªå‚è€ƒå¯ç”¨æ€§å…ƒç´ çš„ç»“æ„åŒ–ä¸‰å…ƒç»„ï¼ŒåŒ…æ‹¬å…¶ç©ºé—´ä½ç½®ã€è¿åŠ¨ç±»å‹å’Œè¿åŠ¨è½´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AffordBotï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸é‡èº«å®šåˆ¶çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èŒƒå¼ç›¸ç»“åˆçš„æ–°æ¡†æ¶ã€‚ä¸ºäº†å¼¥åˆ3Dè¾“å…¥ä¸2Då…¼å®¹çš„MLLMsä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬å¯¹åœºæ™¯è¿›è¡Œç¯ç»•è§†å›¾æ¸²æŸ“ï¼Œå¹¶å°†3Då…ƒç´ å€™é€‰è€…æŠ•å½±åˆ°è¿™äº›è§†å›¾ä¸­ï¼Œå½¢æˆä¸åœºæ™¯å‡ ä½•ç»“æ„å¯¹é½çš„ä¸°å¯Œè§†è§‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„CoTç®¡é“å§‹äºä¸»åŠ¨æ„ŸçŸ¥é˜¶æ®µï¼Œæç¤ºMLLMæ ¹æ®æŒ‡ä»¤é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„è§‚ç‚¹ï¼Œç„¶åè¿›è¡Œé€æ­¥æ¨ç†ä»¥å®šä½å¯ç”¨æ€§å…ƒç´ å¹¶æ¨æ–­å¯èƒ½çš„äº¤äº’åŠ¨ä½œã€‚åœ¨SceneFun3Dæ•°æ®é›†ä¸Šè¯„ä¼°AffordBotæ—¶ï¼Œå®ƒå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…åœ¨è¾“å…¥3Dç‚¹äº‘å’ŒMLLMsçš„æƒ…å†µä¸‹ï¼Œå°±å±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œç‰©ç†æ¥åœ°æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10017v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Fine-grained 3D Embodied Reasoningä»»åŠ¡ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨3Dåœºæ™¯ä¸­ï¼ŒåŸºäºä»»åŠ¡æŒ‡ä»¤é¢„æµ‹æ¯ä¸ªå‚è€ƒçš„å¯æ“ä½œå…ƒç´ çš„ç»“æ„åŒ–ä¸‰å…ƒç»„ï¼ŒåŒ…æ‹¬å…¶ç©ºé—´ä½ç½®ã€è¿åŠ¨ç±»å‹å’Œè¿åŠ¨è½´ã€‚ä¸ºè§£å†³æ­¤ä»»åŠ¡ï¼Œæå‡ºäº†AffordBotæ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œå®šåˆ¶åŒ–çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†èŒƒå¼ã€‚é€šè¿‡æ¸²æŸ“åœºæ™¯å‘¨è¾¹è§†å›¾å¹¶å°†3Då…ƒç´ å€™é€‰è€…æŠ•å½±åˆ°è¿™äº›è§†å›¾ä¸­ï¼Œå½¢æˆä¸åœºæ™¯å‡ ä½•ç»“æ„å¯¹é½çš„ä¸°å¯Œè§†è§‰è¡¨å¾ï¼Œä»¥ç¼©å°3Dè¾“å…¥å’Œ2Då…¼å®¹çš„MLLMsä¹‹é—´çš„å·®è·ã€‚AffordBotåœ¨SceneFun3Dæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–å’Œç‰©ç†æ¨ç†èƒ½åŠ›ï¼Œä»…ä½¿ç”¨3Dç‚¹äº‘è¾“å…¥å’ŒMLLMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fine-grained 3D Embodied Reasoningä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“é¢„æµ‹3Dåœºæ™¯ä¸­æ¯ä¸ªå‚è€ƒçš„å¯æ“ä½œå…ƒç´ çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚</li>
<li>AffordBotæ¡†æ¶ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ€ç»´é“¾æ¨ç†èŒƒå¼ã€‚</li>
<li>ä¸ºäº†é€‚åº”2Då…¼å®¹çš„MLLMsï¼Œé€šè¿‡æ¸²æŸ“åœºæ™¯å‘¨è¾¹è§†å›¾å¹¶æŠ•å½±3Då…ƒç´ ã€‚</li>
<li>AffordBotå…·å¤‡ä¸»åŠ¨æ„ŸçŸ¥é˜¶æ®µï¼Œæ ¹æ®æŒ‡ä»¤é€‰æ‹©æœ€å…·æœ‰ä¿¡æ¯é‡çš„è§†è§’ã€‚</li>
<li>AffordBoté€šè¿‡é€æ­¥æ¨ç†æ¥å®šä½å¯æ“ä½œå…ƒç´ å¹¶æ¨æ–­å¯èƒ½çš„äº¤äº’åŠ¨ä½œã€‚</li>
<li>åœ¨SceneFun3Dæ•°æ®é›†ä¸Šï¼ŒAffordBotå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72e0a707f23140f6b36e7e8faa290070" align="middle">
<img src="https://picx.zhimg.com/v2-d8e6772aa6d323a4643011f32e6d26d7" align="middle">
<img src="https://picx.zhimg.com/v2-e631d2a3ff0f1c02345fa7b7eab36227" align="middle">
<img src="https://picx.zhimg.com/v2-047fbe16e0b0fb2c2f11526b16346025" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Beyond-Cosine-Similarity-Magnitude-Aware-CLIP-for-No-Reference-Image-Quality-Assessment"><a href="#Beyond-Cosine-Similarity-Magnitude-Aware-CLIP-for-No-Reference-Image-Quality-Assessment" class="headerlink" title="Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment"></a>Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment</h2><p><strong>Authors:Zhicheng Liao, Dongxu Wu, Zhenshan Shi, Sijie Mai, Hanwei Zhu, Lingyu Zhu, Yuncheng Jiang, Baoliang Chen</strong></p>
<p>Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as â€œa good photoâ€ or â€œa bad photo.â€ However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.</p>
<blockquote>
<p>æœ€è¿‘çš„åŠªåŠ›å°è¯•å°†å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹é‡æ–°ç”¨äºæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆNR-IQAï¼‰ï¼Œé€šè¿‡æµ‹é‡å›¾åƒåµŒå…¥å’Œæ–‡æœ¬æç¤ºï¼ˆå¦‚â€œå¥½ç…§ç‰‡â€æˆ–â€œåç…§ç‰‡â€ï¼‰ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥å®ç°ã€‚ç„¶è€Œï¼Œè¿™ç§è¯­ä¹‰ç›¸ä¼¼æ€§å¿½ç•¥äº†è‡³å…³é‡è¦å´è¢«å¿½ç•¥çš„çº¿ç´¢ï¼šCLIPå›¾åƒç‰¹å¾çš„å¤§å°ï¼Œæˆ‘ä»¬å®è¯å‘ç°å…¶ä¸æ„ŸçŸ¥è´¨é‡å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹è‡ªé€‚åº”èåˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”¨æ„ŸçŸ¥è´¨é‡çº¿ç´¢è¡¥å……ä½™å¼¦ç›¸ä¼¼æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå–CLIPå›¾åƒç‰¹å¾çš„ç»å¯¹å€¼ï¼Œå¹¶åº”ç”¨Box-Coxè½¬æ¢å¯¹ç‰¹å¾åˆ†å¸ƒè¿›è¡Œç»Ÿè®¡å½’ä¸€åŒ–ï¼Œå‡è½»è¯­ä¹‰æ•æ„Ÿæ€§ã€‚å¾—åˆ°çš„æ ‡é‡æ‘˜è¦ä½œä¸ºè¯­ä¹‰å½’ä¸€åŒ–çš„è¾…åŠ©çº¿ç´¢ï¼Œè¡¥å……åŸºäºä½™å¼¦çš„æç¤ºåŒ¹é…ã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ•´åˆè¿™ä¸¤ç§çº¿ç´¢ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„èåˆæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæ ¹æ®ç›¸å¯¹å¼ºåº¦è‡ªé€‚åº”åœ°æƒè¡¡æ¯é¡¹ã€‚åœ¨å¤šä¸ªåŸºå‡†IQAæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºåŸºäºæ ‡å‡†CLIPçš„IQAå’Œæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œä¸”æ— éœ€è¿›è¡Œä»»ä½•ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09948v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æè¿°äº†ä¸€ç§åˆ©ç”¨CLIPæ¨¡å‹è¿›è¡Œæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆNR-IQAï¼‰çš„æ–°æ–¹æ³•ã€‚å®ƒå¼•å…¥äº†å›¾åƒç‰¹å¾çš„å¹…åº¦ä½œä¸ºä¸€ä¸ªé‡è¦çš„è´¨é‡çº¿ç´¢ï¼Œä¸åŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„è¯­ä¹‰ç›¸ä¼¼æ€§ç›¸ç»“åˆã€‚é€šè¿‡åº”ç”¨Box-Coxå˜æ¢æ¥æ ‡å‡†åŒ–ç‰¹å¾åˆ†å¸ƒï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºç½®ä¿¡åº¦çš„èåˆæ–¹æ¡ˆï¼Œå°†ä¸¤ç§çº¿ç´¢æœ‰æ•ˆåœ°ç»“åˆåœ¨ä¸€èµ·ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªIQAæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒå³å¯è¶…è¶Šæ ‡å‡†CLIP-based IQAå’Œç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨CLIPæ¨¡å‹è¿›è¡ŒNR-IQAçš„æ–°æ–¹æ³•ç»“åˆäº†å›¾åƒç‰¹å¾çš„å¹…åº¦ä½œä¸ºè´¨é‡çº¿ç´¢ã€‚</li>
<li>é€šè¿‡Box-Coxå˜æ¢æ ‡å‡†åŒ–CLIPå›¾åƒç‰¹å¾åˆ†å¸ƒï¼Œå‡å°‘è¯­ä¹‰æ•æ„Ÿæ€§ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºç½®ä¿¡åº¦çš„èåˆæ–¹æ¡ˆï¼Œè‡ªé€‚åº”åœ°ç»“åˆä½™å¼¦ç›¸ä¼¼æ€§å’Œå¹…åº¦çº¿ç´¢ã€‚</li>
<li>æ–°æ–¹æ³•åœ¨ä¸åŒIQAæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºæ ‡å‡†CLIP-based IQAå’Œç°æœ‰åŸºçº¿ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•æ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚</li>
<li>æ­¤æ–¹æ³•ç»“åˆäº†è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå¹…åº¦çº¿ç´¢ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46d3dcd84d527d27409eb1bc421a7ea8" align="middle">
<img src="https://picx.zhimg.com/v2-a6a4ec235840b4bf90a8343c27af2bc6" align="middle">
<img src="https://picx.zhimg.com/v2-52314e1f8e9e5dc2437e59ed56047481" align="middle">
<img src="https://picx.zhimg.com/v2-8f57555be2b6fb168b80fc4148dfbfec" align="middle">
<img src="https://picx.zhimg.com/v2-e2413cf9a7524b934a53941fa6323e90" align="middle">
<img src="https://picx.zhimg.com/v2-e043081dfcc28bad2b52c53630e98e62" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Identifying-Knowledge-Components"><a href="#Leveraging-Large-Language-Models-for-Identifying-Knowledge-Components" class="headerlink" title="Leveraging Large Language Models for Identifying Knowledge Components"></a>Leveraging Large Language Models for Identifying Knowledge Components</h2><p><strong>Authors:Canwen Wang, Jionghao Lin, Kenneth R. Koedinger</strong></p>
<p>Knowledge Components (KCs) are foundational to adaptive learning systems, but their manual identification by domain experts is a significant bottleneck. While Large Language Models (LLMs) offer a promising avenue for automating this process, prior research has been limited to small datasets and has been shown to produce superfluous, redundant KC labels. This study addresses these limitations by first scaling a â€œsimulated textbookâ€ LLM prompting strategy (using GPT-4o-mini) to a larger dataset of 646 multiple-choice questions. We found that this initial automated approach performed significantly worse than an expert-designed KC model (RMSE 0.4285 vs. 0.4206) and generated an excessive number of KCs (569 vs. 101). To address the issue of redundancy, we proposed and evaluated a novel method for merging semantically similar KC labels based on their cosine similarity. This merging strategy significantly improved the modelâ€™s performance; a model using a cosine similarity threshold of 0.8 achieved the best result, reducing the KC count to 428 and improving the RMSE to 0.4259. This demonstrates that while scaled LLM generation alone is insufficient, combining it with a semantic merging technique offers a viable path toward automating and refining KC identification.</p>
<blockquote>
<p>çŸ¥è¯†ç»„ä»¶ï¼ˆKCsï¼‰æ˜¯è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿçš„æ ¸å¿ƒåŸºç¡€ï¼Œä½†å…¶é€šè¿‡é¢†åŸŸä¸“å®¶è¿›è¡Œçš„æ‰‹åŠ¨è¯†åˆ«æ˜¯ä¸€ä¸ªé‡å¤§ç“¶é¢ˆã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ï¼Œä½†å…ˆå‰çš„ç ”ç©¶ä»…é™äºå°å‹æ•°æ®é›†ï¼Œå¹¶æ˜¾ç¤ºå‡ºä¼šäº§ç”Ÿå¤šä½™ã€å†—ä½™çš„KCæ ‡ç­¾ã€‚æœ¬ç ”ç©¶é€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³äº†è¿™äº›é™åˆ¶ï¼šé¦–å…ˆï¼Œå°†ä¸€ç§æ¨¡æ‹Ÿæ•™ç§‘ä¹¦çš„LLMæç¤ºç­–ç•¥ï¼ˆä½¿ç”¨GPT-4o-miniï¼‰æ‰©å¤§åˆ°åŒ…å«646é“é€‰æ‹©é¢˜çš„æ›´å¤§æ•°æ®é›†ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¿™ç§åˆæ­¥çš„è‡ªåŠ¨åŒ–æ–¹æ³•è¡¨ç°å¾—æ˜æ˜¾ä¸å¦‚ä¸“å®¶è®¾è®¡çš„KCæ¨¡å‹ï¼ˆRMSE 0.4285ä¸RMSE 0.4206ï¼‰ï¼Œå¹¶äº§ç”Ÿäº†è¿‡å¤šçš„KCæ•°é‡ï¼ˆ569ä¸ªä¸101ä¸ªï¼‰ã€‚ä¸ºäº†è§£å†³å†—ä½™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä½™å¼¦ç›¸ä¼¼åº¦åˆå¹¶è¯­ä¹‰ç›¸ä¼¼KCæ ‡ç­¾çš„æ–°æ–¹æ³•å¹¶å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚è¿™ç§åˆå¹¶ç­–ç•¥æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼›ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼ä¸º0.8çš„æ¨¡å‹å–å¾—äº†æœ€ä½³ç»“æœï¼Œå°†KCæ•°é‡å‡å°‘åˆ°428ä¸ªï¼Œå¹¶å°†RMSEæé«˜åˆ°0.4259ã€‚è¿™è¡¨æ˜å•ç‹¬æ‰©å¤§LLMç”Ÿæˆæ˜¯ä¸å¤Ÿçš„ï¼Œä½†å°†å…¶ä¸è¯­ä¹‰åˆå¹¶æŠ€æœ¯ç›¸ç»“åˆä¸ºè‡ªåŠ¨åŒ–å’Œç»†åŒ–KCè¯†åˆ«æä¾›äº†å¯è¡Œçš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09935v1">PDF</a> Accepted as an extended abstract in The International Conference on Learning Analytics &amp; Knowledge (LAKâ€™25) Workshop: LLMs for Qualitative Analysis in Education</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†çŸ¥è¯†ç»„ä»¶ï¼ˆKCsï¼‰åœ¨è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿä¸­çš„é‡è¦åœ°ä½ï¼Œä»¥åŠé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨åŒ–è¯†åˆ«KCsçš„æ½œåŠ›ã€‚ç ”ç©¶é€šè¿‡GPT-4o-miniæ¨¡æ‹Ÿæ•™ç§‘ä¹¦æç¤ºç­–ç•¥ï¼Œåœ¨646é“é€‰æ‹©é¢˜çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œå°è¯•ï¼Œå‘ç°åˆå§‹è‡ªåŠ¨åŒ–æ–¹æ³•è¡¨ç°ä¸å¦‚ä¸“å®¶è®¾è®¡çš„KCæ¨¡å‹ï¼Œå¹¶äº§ç”Ÿè¿‡å¤šçš„KCã€‚ä¸ºè§£å†³å†—ä½™é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†åŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„åˆå¹¶è¯­ä¹‰ç›¸ä¼¼KCæ ‡ç­¾çš„æ–°æ–¹æ³•ã€‚ç»“åˆè¯­ä¹‰åˆå¹¶æŠ€æœ¯ï¼Œå±•ç°äº†è‡ªåŠ¨åŒ–å’Œç»†åŒ–KCè¯†åˆ«çš„å¯è¡Œè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†ç»„ä»¶ï¼ˆKCsï¼‰æ˜¯è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿçš„åŸºçŸ³ï¼Œä½†å…¶ç”±é¢†åŸŸä¸“å®¶æ‰‹åŠ¨è¯†åˆ«å­˜åœ¨ç“¶é¢ˆã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºè‡ªåŠ¨åŒ–è¯†åˆ«KCsæä¾›å¸Œæœ›ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶å—é™äºå°æ•°æ®é›†ï¼Œä¸”äº§ç”Ÿçš„KCæ ‡ç­¾å¤šä½™ä¸”å†—ä½™ã€‚</li>
<li>åˆæ­¥å°è¯•ä½¿ç”¨GPT-4o-miniæ¨¡æ‹Ÿæ•™ç§‘ä¹¦æç¤ºç­–ç•¥åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¸ä¸“å®¶è®¾è®¡çš„KCæ¨¡å‹ç›¸æ¯”å­˜åœ¨å·®è·ã€‚</li>
<li>åˆå§‹è‡ªåŠ¨åŒ–æ–¹æ³•äº§ç”Ÿè¿‡å¤šçš„KCæ ‡ç­¾ã€‚</li>
<li>ä¸ºè§£å†³å†—ä½™é—®é¢˜ï¼Œæå‡ºäº†åŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„åˆå¹¶è¯­ä¹‰ç›¸ä¼¼KCæ ‡ç­¾çš„æ–¹æ³•ã€‚</li>
<li>ç»“åˆè¯­ä¹‰åˆå¹¶æŠ€æœ¯æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼æ€§é˜ˆå€¼ä¸º0.8çš„æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6a5a14d04b51b2785b72c14754b413d" align="middle">
<img src="https://picx.zhimg.com/v2-2ce893eee4d557e9d5122130e9f2605a" align="middle">
<img src="https://picx.zhimg.com/v2-f94fc05d96989a2c6ff57ef1daa9a909" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Debiased-Dual-Invariant-Defense-for-Adversarially-Robust-Person-Re-Identification"><a href="#Debiased-Dual-Invariant-Defense-for-Adversarially-Robust-Person-Re-Identification" class="headerlink" title="Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification"></a>Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification</h2><p><strong>Authors:Yuhang Zhou, Yanxiang Zhao, Zhongyun Hua, Zhipu Liu, Zhaoquan Gu, Qing Liao, Leo Yu Zhang</strong></p>
<p>Person re-identification (ReID) is a fundamental task in many real-world applications such as pedestrian trajectory tracking. However, advanced deep learning-based ReID models are highly susceptible to adversarial attacks, where imperceptible perturbations to pedestrian images can cause entirely incorrect predictions, posing significant security threats. Although numerous adversarial defense strategies have been proposed for classification tasks, their extension to metric learning tasks such as person ReID remains relatively unexplored. Moreover, the several existing defenses for person ReID fail to address the inherent unique challenges of adversarially robust ReID. In this paper, we systematically identify the challenges of adversarial defense in person ReID into two key issues: model bias and composite generalization requirements. To address them, we propose a debiased dual-invariant defense framework composed of two main phases. In the data balancing phase, we mitigate model bias using a diffusion-model-based data resampling strategy that promotes fairness and diversity in training data. In the bi-adversarial self-meta defense phase, we introduce a novel metric adversarial training approach incorporating farthest negative extension softening to overcome the robustness degradation caused by the absence of classifier. Additionally, we introduce an adversarially-enhanced self-meta mechanism to achieve dual-generalization for both unseen identities and unseen attack types. Experiments demonstrate that our method significantly outperforms existing state-of-the-art defenses.</p>
<blockquote>
<p>è¡Œäººå†è¯†åˆ«ï¼ˆReIDï¼‰æ˜¯è®¸å¤šç°å®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚è¡Œäººè½¨è¿¹è·Ÿè¸ªï¼‰ä¸­çš„åŸºæœ¬ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå…ˆè¿›çš„åŸºäºæ·±åº¦å­¦ä¹ çš„ReIDæ¨¡å‹å¾ˆå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œå…¶ä¸­è¡Œäººå›¾åƒçš„å¾®å°æ‰°åŠ¨å¯èƒ½å¯¼è‡´å®Œå…¨é”™è¯¯çš„é¢„æµ‹ï¼Œæ„æˆé‡å¤§å®‰å…¨å¨èƒã€‚è™½ç„¶é’ˆå¯¹åˆ†ç±»ä»»åŠ¡å·²ç»æå‡ºäº†è®¸å¤šå¯¹æŠ—æ€§é˜²å¾¡ç­–ç•¥ï¼Œä½†å®ƒä»¬æ‰©å±•åˆ°åº¦é‡å­¦ä¹ ä»»åŠ¡ï¼ˆå¦‚è¡ŒäººReIDï¼‰ä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ã€‚è€Œä¸”ï¼Œç°æœ‰çš„é’ˆå¯¹è¡ŒäººReIDçš„å‡ ç§é˜²å¾¡æ–¹æ³•æœªèƒ½è§£å†³å¯¹æŠ—é²æ£’ReIDçš„å›ºæœ‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹æŠ—æ€§é˜²å¾¡åœ¨è¡ŒäººReIDä¸­çš„æŒ‘æˆ˜ç³»ç»Ÿåœ°å½’ä¸ºä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šæ¨¡å‹åè§å’Œå¤åˆæ³›åŒ–è¦æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå»ååŒä¸å˜é˜²å¾¡æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªé˜¶æ®µã€‚åœ¨æ•°æ®å¹³è¡¡é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®é‡é‡‡æ ·ç­–ç•¥æ¥ç¼“è§£æ¨¡å‹åè§ï¼Œä¿ƒè¿›è®­ç»ƒæ•°æ®çš„å…¬å¹³æ€§å’Œå¤šæ ·æ€§ã€‚åœ¨åŒå‘å¯¹æŠ—æ€§è‡ªæˆ‘å…ƒé˜²å¾¡é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åº¦é‡å¯¹æŠ—è®­ç»ƒæ–¹æ³•æ¥å…‹æœç”±äºç¼ºä¹åˆ†ç±»å™¨è€Œå¯¼è‡´çš„é²æ£’æ€§ä¸‹é™ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æœ€è¿œçš„è´Ÿæ‰©å±•è½¯åŒ–ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¯¹æŠ—å¢å¼ºçš„è‡ªæˆ‘å…ƒæœºåˆ¶ï¼Œä»¥å®ç°æœªè§èº«ä»½å’Œæœªè§æ”»å‡»ç±»å‹çš„åŒé‡æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„é˜²å¾¡æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09933v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹äººç‰©å†è¯†åˆ«ï¼ˆReIDï¼‰ä»»åŠ¡ä¸­æ¨¡å‹æ˜“å—å¯¹æŠ—æ”»å‡»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå»ååŒä¸å˜é˜²å¾¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ•°æ®å¹³è¡¡å’ŒåŒå‘å¯¹æŠ—è‡ªå…ƒé˜²å¾¡ä¸¤ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹åå·®å’Œå¤åˆæ³›åŒ–è¦æ±‚ç­‰æŒ‘æˆ˜ã€‚é€šè¿‡æ‰©æ•£æ¨¡å‹åŸºç¡€ä¸Šçš„æ•°æ®é‡é‡‡æ ·ç­–ç•¥ï¼Œå‡è½»æ¨¡å‹åå·®ï¼Œä¿ƒè¿›è®­ç»ƒæ•°æ®çš„å…¬å¹³æ€§å’Œå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œå¼•å…¥åº¦é‡å¯¹æŠ—è®­ç»ƒæ–¹æ³•å’Œå¯¹æŠ—å¢å¼ºè‡ªå…ƒæœºåˆ¶ï¼Œå®ç°å¯¹æœªè§èº«ä»½å’Œæœªè§æ”»å‡»ç±»å‹çš„åŒé‡æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„é˜²å¾¡æ‰‹æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç‰©å†è¯†åˆ«ï¼ˆReIDï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­éå¸¸é‡è¦ï¼Œä½†æ·±åº¦æ¨¡å‹æ˜“å—å¯¹æŠ—æ”»å‡»ã€‚</li>
<li>å¯¹æŠ—é˜²å¾¡åœ¨ReIDä¸­çš„æŒ‘æˆ˜åŒ…æ‹¬æ¨¡å‹åå·®å’Œå¤åˆæ³›åŒ–è¦æ±‚ã€‚</li>
<li>æå‡ºçš„å»ååŒä¸å˜é˜²å¾¡æ¡†æ¶åŒ…æ‹¬æ•°æ®å¹³è¡¡å’ŒåŒå‘å¯¹æŠ—è‡ªå…ƒé˜²å¾¡ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>æ•°æ®å¹³è¡¡é˜¶æ®µé€šè¿‡æ‰©æ•£æ¨¡å‹åŸºç¡€ä¸Šçš„æ•°æ®é‡é‡‡æ ·ç­–ç•¥ï¼Œå‡è½»æ¨¡å‹åå·®ã€‚</li>
<li>åŒå‘å¯¹æŠ—è‡ªå…ƒé˜²å¾¡é˜¶æ®µå¼•å…¥åº¦é‡å¯¹æŠ—è®­ç»ƒæ–¹æ³•å’Œå¯¹æŠ—å¢å¼ºè‡ªå…ƒæœºåˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å¯¹æœªè§èº«ä»½å’Œæœªè§æ”»å‡»ç±»å‹çš„åŒé‡æ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-655f1643249e31293f53c4222a15bfb7" align="middle">
<img src="https://picx.zhimg.com/v2-5d317d779900c0e6ed6b65f6bdc506b8" align="middle">
<img src="https://picx.zhimg.com/v2-588b3f7cb16d2c9fcebed4d1f99409f2" align="middle">
<img src="https://picx.zhimg.com/v2-ed22a87c1df582e975cd3c13ce0e0ab9" align="middle">
<img src="https://picx.zhimg.com/v2-3432838e6a70b4e0536afd9f2414ce4a" align="middle">
<img src="https://picx.zhimg.com/v2-634178881122f7cc094f066755a80f76" align="middle">
<img src="https://picx.zhimg.com/v2-9c9da430b0338f0afef4c8b904b4e4cf" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Boosting-In-Silicon-Directed-Evolution-with-Fine-Tuned-Protein-Language-Model-and-Tree-Search"><a href="#Boosting-In-Silicon-Directed-Evolution-with-Fine-Tuned-Protein-Language-Model-and-Tree-Search" class="headerlink" title="Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search"></a>Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search</h2><p><strong>Authors:Yaodong Yang, Yang Wang, Jinpeng Li, Pei Guo, Da Han, Guangyong Chen, Pheng-Ann Heng</strong></p>
<p>Protein evolution through amino acid sequence mutations is a cornerstone of life sciences. While current in-silicon directed evolution algorithms focus on designing search strategies, they overlook how to utilize the transformative protein language models, which encode rich evolutionary patterns, to guide search. To bridge this gap, we propose AlphaDE, a novel framework to evolve protein sequences by harnessing the innovative paradigms of large language models. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility for the interested protein class. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. An interesting case study further shows that AlphaDE supports condensing the protein sequence space through computational evolution.</p>
<blockquote>
<p>è›‹ç™½è´¨é€šè¿‡æ°¨åŸºé…¸åºåˆ—çªå˜è¿›è¡Œè¿›åŒ–æ˜¯ç”Ÿå‘½ç§‘å­¦çš„æ ¸å¿ƒå†…å®¹ã€‚è™½ç„¶å½“å‰çš„ç¡…åŸºå®šå‘è¿›åŒ–ç®—æ³•ä¾§é‡äºè®¾è®¡æœç´¢ç­–ç•¥ï¼Œä½†å®ƒä»¬å¿½è§†äº†å¦‚ä½•åˆ©ç”¨è•´å«ä¸°å¯Œè¿›åŒ–æ¨¡å¼çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹æ¥æŒ‡å¯¼æœç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†AlphaDEï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åˆ›æ–°èŒƒå¼è¿›åŒ–è›‹ç™½è´¨åºåˆ—çš„æ–°å‹æ¡†æ¶ã€‚é¦–å…ˆï¼ŒAlphaDEé€šè¿‡ä½¿ç”¨åŒæºè›‹ç™½è´¨åºåˆ—çš„æ©ç è¯­è¨€å»ºæ¨¡å¯¹é¢„è®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ¿€æ´»æ‰€å…³æ³¨è›‹ç™½è´¨ç±»åˆ«çš„è¿›åŒ–å¯èƒ½æ€§ã€‚å…¶æ¬¡ï¼ŒAlphaDEå¼•å…¥åŸºäºè’™ç‰¹å¡ç½—æ ‘æœç´¢çš„æµ‹è¯•æ—¶æ¨ç†ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨å¾®è°ƒåçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹æä¾›çš„è¿›åŒ–æŒ‡å¯¼æ¥è¿›åŒ–è›‹ç™½è´¨ã€‚å¤§é‡åŸºå‡†å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å°‘é‡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒAlphaDEä¹Ÿæ˜¾è‘—ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ä¸€ä¸ªæœ‰è¶£çš„æ¡ˆä¾‹ç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒAlphaDEæ”¯æŒé€šè¿‡è®¡ç®—è¿›åŒ–æ¥å‡èšè›‹ç™½è´¨åºåˆ—ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09900v1">PDF</a> working in progress, 23 pages, 6 figures, 15 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è›‹ç™½è´¨è¿›åŒ–æ¡†æ¶AlphaDEï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è›‹ç™½è´¨è¯­è¨€æ¨¡å‹çš„åˆ›æ–°èŒƒå¼ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹å¹¶åˆ©ç”¨è’™ç‰¹å¡ç½—æ ‘æœç´¢è¿›è¡Œæ¨ç†ï¼Œå®ç°è›‹ç™½è´¨åºåˆ—çš„è¿›åŒ–ã€‚AlphaDEæ˜¾è‘—æé«˜äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œå¹¶æ”¯æŒé€šè¿‡è®¡ç®—è¿›åŒ–æ¥å‡èšè›‹ç™½è´¨åºåˆ—ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AlphaDEæ¡†æ¶åˆ©ç”¨è›‹ç™½è´¨è¯­è¨€æ¨¡å‹æ¥æŒ‡å¯¼è›‹ç™½è´¨åºåˆ—è¿›åŒ–çš„æœç´¢ç­–ç•¥ã€‚</li>
<li>AlphaDEé€šè¿‡åœ¨åŒæºè›‹ç™½è´¨åºåˆ—ä¸Šåº”ç”¨æ©ç è¯­è¨€å»ºæ¨¡æ¥å¾®è°ƒé¢„è®­ç»ƒçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼Œä»è€Œæ¿€æ´»å¯¹ç›®æ ‡è›‹ç™½è´¨ç±»çš„è¿›åŒ–å¯èƒ½æ€§ã€‚</li>
<li>AlphaDEå¼•å…¥åŸºäºè’™ç‰¹å¡ç½—æ ‘æœç´¢çš„æµ‹è¯•æ—¶æ¨ç†ï¼Œå®ç°è›‹ç™½è´¨çš„æœ‰æ•ˆè¿›åŒ–ã€‚</li>
<li>AlphaDEåœ¨å°‘æ•°æƒ…å†µä¸‹çš„å¾®è°ƒä¹Ÿèƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>AlphaDEæ”¯æŒé€šè¿‡è®¡ç®—è¿›åŒ–æ¥å‡èšè›‹ç™½è´¨åºåˆ—ç©ºé—´ï¼Œå±•ç¤ºäº†ä¸€ä¸ªæœ‰è¶£çš„æ¡ˆä¾‹ç ”ç©¶ã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹äºè›‹ç™½è´¨è¿›åŒ–ç ”ç©¶å…·æœ‰é‡è¦çš„æ¨åŠ¨ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b0cc8f00169196de3425cfc1da76d99" align="middle">
<img src="https://picx.zhimg.com/v2-c12744eae5bb3ed79c61224e0ab2ae72" align="middle">
<img src="https://picx.zhimg.com/v2-13231bd996e42a0c8d7c02cc9e18f939" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="How-Small-Can-You-Go-Compact-Language-Models-for-On-Device-Critical-Error-Detection-in-Machine-Translation"><a href="#How-Small-Can-You-Go-Compact-Language-Models-for-On-Device-Critical-Error-Detection-in-Machine-Translation" class="headerlink" title="How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation"></a>How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation</h2><p><strong>Authors:Muskaan Chopra, Lorenz Sparrenberg, Sarthak Khanna, Rafet Sifa</strong></p>
<p>Large Language Models (LLMs) excel at evaluating machine translation (MT), but their scale and cost hinder deployment on edge devices and in privacy-sensitive workflows. We ask: how small can you get while still detecting meaning-altering translation errors? Focusing on English-&gt;German Critical Error Detection (CED), we benchmark sub-2B models (LFM2-350M, Qwen-3-0.6B&#x2F;1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across WMT21, WMT22, and SynCED-EnDe-2025. Our framework standardizes prompts, applies lightweight logit-bias calibration and majority voting, and reports both semantic quality (MCC, F1-ERR&#x2F;F1-NOT) and compute metrics (VRAM, latency, throughput). Results reveal a clear sweet spot around one billion parameters: Gemma-3-1B provides the best quality-efficiency trade-off, reaching MCC&#x3D;0.77 with F1-ERR&#x3D;0.98 on SynCED-EnDe-2025 after merged-weights fine-tuning, while maintaining 400 ms single-sample latency on a MacBook Pro M4 Pro (24 GB). At larger scale, Qwen-3-1.7B attains the highest absolute MCC (+0.11 over Gemma) but with higher compute cost. In contrast, ultra-small models (0.6B) remain usable with few-shot calibration yet under-detect entity and number errors. Overall, compact, instruction-tuned LLMs augmented with lightweight calibration and small-sample supervision can deliver trustworthy, on-device CED for MT, enabling private, low-cost error screening in real-world translation pipelines. All datasets, prompts, and scripts are publicly available at our GitHub repository.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿è¯„ä¼°æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰çš„æ•ˆæœï¼Œä½†å…¶è§„æ¨¡å’Œæˆæœ¬é˜»ç¢äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡å’Œéšç§æ•æ„Ÿå·¥ä½œæµç¨‹ä¸­çš„éƒ¨ç½²ã€‚æˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šåœ¨ä»èƒ½æ£€æµ‹æ„ä¹‰æ”¹å˜çš„ç¿»è¯‘é”™è¯¯çš„æƒ…å†µä¸‹ï¼Œä½ èƒ½åšåˆ°å¤šå°ï¼Ÿæˆ‘ä»¬ä¸“æ³¨äºè‹±è¯­åˆ°å¾·è¯­çš„ä¸´ç•Œé”™è¯¯æ£€æµ‹ï¼ˆCEDï¼‰ï¼Œå¯¹sub-2Bæ¨¡å‹ï¼ˆLFM2-350Mã€Qwen-3-0.6B&#x2F;1.7Bã€Llama-3.2-1B-Instructã€Gemma-3-1Bï¼‰åœ¨WMT21ã€WMT22å’ŒSynCED-EnDe-2025ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ ‡å‡†åŒ–äº†æç¤ºï¼Œåº”ç”¨äº†è½»é‡çº§çš„logit-biasæ ¡å‡†å’Œå¤šæ•°æŠ•ç¥¨ï¼Œå¹¶æŠ¥å‘Šäº†è¯­ä¹‰è´¨é‡ï¼ˆMCCã€F1-ERR&#x2F;F1-NOTï¼‰å’Œè®¡ç®—æŒ‡æ ‡ï¼ˆVRAMã€å»¶è¿Ÿã€ååé‡ï¼‰ã€‚ç»“æœæ­ç¤ºäº†ä¸€ä¸ªæ˜ç¡®çš„ç”œèœœç‚¹ï¼Œå¤§çº¦åäº¿å‚æ•°ï¼šGemma-3-1Båœ¨åˆå¹¶æƒé‡å¾®è°ƒåï¼Œåœ¨SynCED-EnDe-2025ä¸Šè¾¾åˆ°MCC&#x3D;0.77ï¼ŒF1-ERR&#x3D;0.98ï¼Œæä¾›äº†æœ€ä½³çš„è´¨é‡æ•ˆç‡æƒè¡¡ï¼ŒåŒæ—¶åœ¨MacBook Pro M4 Proï¼ˆ24GBï¼‰ä¸Šä¿æŒ400æ¯«ç§’çš„å•æ ·æœ¬å»¶è¿Ÿã€‚åœ¨æ›´å¤§è§„æ¨¡ä¸Šï¼ŒQwen-3-1.7Bè¾¾åˆ°äº†æœ€é«˜çš„ç»å¯¹MCCå€¼ï¼ˆæ¯”Gemmaé«˜0.11ï¼‰ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¶…å°å‹æ¨¡å‹ï¼ˆ0.6Bï¼‰åœ¨å°‘é‡æ ¡å‡†ä¸‹ä»ç„¶å¯ç”¨ï¼Œä½†åœ¨æ£€æµ‹å®ä½“å’Œæ•°å­—é”™è¯¯æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚æ€»ä½“è€Œè¨€ï¼Œç´§å‡‘ã€æŒ‡ä»¤è°ƒæ•´çš„LLMsè¾…ä»¥è½»é‡çº§æ ¡å‡†å’Œå°æ ·æœ¬ç›‘ç£ï¼Œå¯ä»¥å®ç°åœ¨çº¿è®¾å¤‡ä¸Šçš„MT CEDå¯é æ£€æµ‹ï¼Œä»è€Œèƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œçš„ç¿»è¯‘ç®¡é“ä¸­å®ç°ç§äººåŒ–ã€ä½æˆæœ¬é”™è¯¯ç­›æŸ¥ã€‚æ‰€æœ‰æ•°æ®é›†ã€æç¤ºå’Œè„šæœ¬éƒ½å¯åœ¨æˆ‘ä»¬çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09748v1">PDF</a> Accepted in IEEE BigData 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æ¢è®¨äº†å°å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘é”™è¯¯æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹æ¯”å¤šä¸ªå­2Bæ¨¡å‹åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå‘ç°å›´ç»•åäº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹å…·æœ‰æœ€ä½³çš„è´¨é‡æ•ˆç‡å¹³è¡¡ç‚¹ã€‚ç‰¹å®šæ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šè¾¾åˆ°äº†è¾ƒé«˜çš„è¯­ä¹‰è´¨é‡æŒ‡æ ‡ï¼ŒåŒæ—¶åœ¨MacBook Proä¸Šçš„å»¶è¿Ÿè¾ƒä½ã€‚éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œè™½ç„¶æ€§èƒ½æœ‰æ‰€æé«˜ï¼Œä½†è®¡ç®—æˆæœ¬ä¹Ÿç›¸åº”å¢åŠ ã€‚è€Œè¶…å°å‹æ¨¡å‹è™½ç„¶å¯ä»¥é€šè¿‡å°‘é‡æ ·æœ¬è¿›è¡Œæ ¡å‡†ï¼Œä½†åœ¨æ£€æµ‹å®ä½“å’Œæ•°å­—é”™è¯¯æ–¹é¢ä»æœ‰ä¸è¶³ã€‚æ€»ä½“è€Œè¨€ï¼Œé€šè¿‡è½»é‡çº§æ ¡å‡†å’Œå°æ ·æœ¬ç›‘ç£å¢å¼ºçš„ç´§å‡‘æŒ‡ä»¤è°ƒä¼˜çš„å°å‹è¯­è¨€æ¨¡å‹å¯ä¸ºæœºå™¨ç¿»è¯‘æä¾›å¯é çš„åœ¨è®¾å¤‡é”™è¯¯æ£€æµ‹ï¼Œé€‚ç”¨äºç°å®ä¸–ç•Œçš„ç¿»è¯‘ç®¡é“ä¸­çš„ç§äººã€ä½æˆæœ¬é”™è¯¯ç­›é€‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘é”™è¯¯æ£€æµ‹ä¸­å±•ç°æ½œåŠ›ã€‚</li>
<li>åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šï¼Œå›´ç»•åäº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹è¡¨ç°å‡ºæœ€ä½³çš„è´¨é‡æ•ˆç‡å¹³è¡¡ç‚¹ã€‚</li>
<li>é€šè¿‡æ ¡å‡†å’Œå¤šæ•°æŠ•ç¥¨ç­‰è½»é‡çº§æŠ€æœ¯ï¼Œå¯æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é«˜æ€§èƒ½æ¨¡å‹è™½èƒ½æé«˜è¯­ä¹‰è´¨é‡æŒ‡æ ‡ï¼Œä½†è®¡ç®—æˆæœ¬ä¹Ÿéšä¹‹å¢åŠ ã€‚</li>
<li>è¶…å°å‹æ¨¡å‹åœ¨æ£€æµ‹å®ä½“å’Œæ•°å­—é”™è¯¯æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ç´§å‡‘ã€æŒ‡ä»¤è°ƒä¼˜çš„å°å‹è¯­è¨€æ¨¡å‹ç»“åˆè½»é‡çº§æ ¡å‡†å’Œå°æ ·æœ¬ç›‘ç£ï¼Œå¯å®ç°å¯é çš„æœºå™¨ç¿»è¯‘åœ¨è®¾å¤‡é”™è¯¯æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba89b6d757b4c32e3216c12c9bc044ff" align="middle">
<img src="https://picx.zhimg.com/v2-957e097193495b0a84082cb00e985aac" align="middle">
<img src="https://picx.zhimg.com/v2-89f2e9b87752cb1fb13007bc6055cba1" align="middle">
<img src="https://picx.zhimg.com/v2-469c415a9abf90e7899fe2ab860d2b07" align="middle">
<img src="https://picx.zhimg.com/v2-44b88e438cc0cc8a51965417454a7109" align="middle">
<img src="https://picx.zhimg.com/v2-a35cac53bad461a8768c8a00d112127e" align="middle">
<img src="https://picx.zhimg.com/v2-788decf0feada96c4e6066c829e88602" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SpatialActor-Exploring-Disentangled-Spatial-Representations-for-Robust-Robotic-Manipulation"><a href="#SpatialActor-Exploring-Disentangled-Spatial-Representations-for-Robust-Robotic-Manipulation" class="headerlink" title="SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation"></a>SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation</h2><p><strong>Authors:Hao Shi, Bin Xie, Yingfei Liu, Yang Yue, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, Gao Huang</strong></p>
<p>Robotic manipulation requires precise spatial understanding to interact with objects in the real world. Point-based methods suffer from sparse sampling, leading to the loss of fine-grained semantics. Image-based methods typically feed RGB and depth into 2D backbones pre-trained on 3D auxiliary tasks, but their entangled semantics and geometry are sensitive to inherent depth noise in real-world that disrupts semantic understanding. Moreover, these methods focus on high-level geometry while overlooking low-level spatial cues essential for precise interaction. We propose SpatialActor, a disentangled framework for robust robotic manipulation that explicitly decouples semantics and geometry. The Semantic-guided Geometric Module adaptively fuses two complementary geometry from noisy depth and semantic-guided expert priors. Also, a Spatial Transformer leverages low-level spatial cues for accurate 2D-3D mapping and enables interaction among spatial features. We evaluate SpatialActor on multiple simulation and real-world scenarios across 50+ tasks. It achieves state-of-the-art performance with 87.4% on RLBench and improves by 13.9% to 19.4% under varying noisy conditions, showing strong robustness. Moreover, it significantly enhances few-shot generalization to new tasks and maintains robustness under various spatial perturbations. Project Page: <a target="_blank" rel="noopener" href="https://shihao1895.github.io/SpatialActor">https://shihao1895.github.io/SpatialActor</a></p>
<blockquote>
<p>æœºå™¨äººæ“ä½œéœ€è¦å¯¹ç°å®ä¸–ç•Œä¸­çš„ç‰©ä½“è¿›è¡Œç²¾ç¡®çš„ç©ºé—´ç†è§£ã€‚åŸºäºç‚¹çš„æ–¹æ³•å—åˆ°ç¨€ç–é‡‡æ ·çš„å›°æ‰°ï¼Œå¯¼è‡´ç²¾ç»†è¯­ä¹‰çš„ä¸¢å¤±ã€‚åŸºäºå›¾åƒçš„æ–¹æ³•é€šå¸¸å°†RGBå’Œæ·±åº¦ä¿¡æ¯è¾“å…¥åˆ°é¢„è®­ç»ƒäº3Dè¾…åŠ©ä»»åŠ¡çš„2Déª¨å¹²ç½‘ç»œä¸Šï¼Œä½†å®ƒä»¬çš„è¯­ä¹‰å’Œå‡ ä½•çº ç¼ å¯¹ç°å®ä¸–ç•Œä¸­çš„å›ºæœ‰æ·±åº¦å™ªå£°æ•æ„Ÿï¼Œè¿™ç ´åäº†è¯­ä¹‰ç†è§£ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å…³æ³¨é«˜çº§å‡ ä½•ï¼Œå´å¿½è§†äº†å¯¹ç²¾ç¡®äº¤äº’è‡³å…³é‡è¦çš„ä½çº§ç©ºé—´çº¿ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†SpatialActorï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç¨³å¥æœºå™¨äººæ“ä½œçš„è§£è€¦æ¡†æ¶ï¼Œå®ƒæ˜ç¡®åœ°åˆ†ç¦»äº†è¯­ä¹‰å’Œå‡ ä½•ã€‚è¯­ä¹‰å¼•å¯¼å‡ ä½•æ¨¡å—è‡ªé€‚åº”åœ°èåˆæ¥è‡ªå˜ˆæ‚æ·±åº¦å’Œè¯­ä¹‰å¼•å¯¼ä¸“å®¶å…ˆéªŒçš„ä¸¤ç§äº’è¡¥å‡ ä½•ã€‚æ­¤å¤–ï¼Œç©ºé—´è½¬æ¢å™¨åˆ©ç”¨ä½çº§ç©ºé—´çº¿ç´¢è¿›è¡Œå‡†ç¡®çš„2D-3Dæ˜ å°„ï¼Œå¹¶å®ç°ç©ºé—´ç‰¹å¾ä¹‹é—´çš„äº¤äº’ã€‚æˆ‘ä»¬åœ¨50å¤šä¸ªä»»åŠ¡çš„å¤šä¸ªä»¿çœŸå’ŒçœŸå®åœºæ™¯ä¸­å¯¹SpatialActorè¿›è¡Œäº†è¯„ä¼°ã€‚å®ƒåœ¨RLBenchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¾¾åˆ°87.4%ï¼Œå¹¶åœ¨ä¸åŒçš„å™ªå£°æ¡ä»¶ä¸‹æé«˜äº†13.9%è‡³19.4%ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œå®ƒå¤§å¤§å¢å¼ºäº†å¯¹æ–°ä»»åŠ¡çš„å°‘é‡æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å„ç§ç©ºé—´æ‰°åŠ¨ä¸‹ä¿æŒç¨³å¥æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://shihao1895.github.io/SpatialActor">https://shihao1895.github.io/SpatialActor</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09555v1">PDF</a> AAAI 2026 Oral | Project Page: <a target="_blank" rel="noopener" href="https://shihao1895.github.io/SpatialActor">https://shihao1895.github.io/SpatialActor</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSpatialActorçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³æœºå™¨äººåœ¨çœŸå®ä¸–ç•Œä¸­è¿›è¡Œæ“ä½œæ—¶çš„è¯­ä¹‰å’Œå‡ ä½•é—®é¢˜ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯æ—¶çš„ä¸è¶³ï¼ŒåŒ…æ‹¬ç¨€ç–é‡‡æ ·å¯¼è‡´çš„è¯­ä¹‰ä¸¢å¤±ã€çœŸå®ä¸–ç•Œæ·±åº¦å™ªå£°å¼•èµ·çš„è¯­ä¹‰å’Œå‡ ä½•çº ç¼ é—®é¢˜ï¼Œä»¥åŠå¿½è§†ä½çº§ç©ºé—´çº¿ç´¢çš„é—®é¢˜ã€‚SpatialActoré€šè¿‡è‡ªé€‚åº”èåˆå™ªå£°æ·±åº¦å’Œè¯­ä¹‰å¼•å¯¼å…ˆéªŒä¿¡æ¯ï¼Œå®ç°äº†è¯­ä¹‰æŒ‡å¯¼çš„å‡ ä½•æ¨¡å—ã€‚æ­¤å¤–ï¼Œå®ƒåˆ©ç”¨ä½çº§ç©ºé—´çº¿ç´¢è¿›è¡Œç²¾ç¡®çš„äºŒç»´è‡³ä¸‰ç»´æ˜ å°„ï¼Œå¹¶é€šè¿‡ç©ºé—´å˜æ¢å®ç°ç©ºé—´ç‰¹å¾é—´çš„äº¤äº’ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒSpatialActoråœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å¤šä¸ªåœºæ™¯ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒå™ªå£°æ¡ä»¶ä¸‹çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½æ˜¾è‘—æé«˜å¯¹æ–°ä»»åŠ¡çš„å¿«é€Ÿé€‚åº”èƒ½åŠ›å’Œåœ¨å„ç§ç©ºé—´æ‰°åŠ¨ä¸‹çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SpatialActorè§£å†³äº†æœºå™¨äººæ“ä½œä¸­å­˜åœ¨çš„è¯­ä¹‰å’Œå‡ ä½•é—®é¢˜ï¼Œæé«˜äº†æ“ä½œç²¾åº¦ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”èåˆå™ªå£°æ·±åº¦å’Œè¯­ä¹‰å¼•å¯¼å…ˆéªŒä¿¡æ¯ï¼Œå®ç°äº†è¯­ä¹‰æŒ‡å¯¼çš„å‡ ä½•æ¨¡å—ã€‚</li>
<li>åˆ©ç”¨ä½çº§ç©ºé—´çº¿ç´¢è¿›è¡Œç²¾ç¡®çš„äºŒç»´è‡³ä¸‰ç»´æ˜ å°„ï¼Œæé«˜ç©ºé—´ç‰¹å¾çš„äº¤äº’èƒ½åŠ›ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å¤šä¸ªåœºæ™¯ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨ä¸åŒå™ªå£°æ¡ä»¶ä¸‹çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ã€‚</li>
<li>èƒ½æ˜¾è‘—æé«˜å¯¹æ–°ä»»åŠ¡çš„å¿«é€Ÿé€‚åº”èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f05d31be617ce1182191b85be5bb2913" align="middle">
<img src="https://picx.zhimg.com/v2-befa06f281ab161814c06631f0c667d2" align="middle">
<img src="https://picx.zhimg.com/v2-619e130a5eb3db0d0a59e18c08440fd7" align="middle">
<img src="https://picx.zhimg.com/v2-7ac1d1b6323ae5f4528b134be5ab1a0d" align="middle">
<img src="https://picx.zhimg.com/v2-a83184fecd0d20a74a8ec8d3d9300a97" align="middle">
<img src="https://picx.zhimg.com/v2-3d5da7321cb8ea72c180bcb254df753e" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="vMFCoOp-Towards-Equilibrium-on-a-Unified-Hyperspherical-Manifold-for-Prompting-Biomedical-VLMs"><a href="#vMFCoOp-Towards-Equilibrium-on-a-Unified-Hyperspherical-Manifold-for-Prompting-Biomedical-VLMs" class="headerlink" title="vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs"></a>vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs</h2><p><strong>Authors:Minye Shao, Sihan Guo, Xinrun Li, Xingyu Miao, Haoran Duan, Yang Long</strong></p>
<p>Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through <a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/UniEqui">https://github.com/VinyehShaw/UniEqui</a>.</p>
<blockquote>
<p>æœ€è¿‘ï¼Œä»¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç‚¼çš„åŒ»ç–—è¯­ä¹‰å…ˆéªŒçŸ¥è¯†ä¸ºæŒ‡å¯¼çš„ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆCoOpï¼‰çš„è¿›æ­¥ï¼Œä¸ºåŸºäºCLIPçš„ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é€‚åº”æä¾›äº†æ‰‹åŠ¨æç¤ºå·¥ç¨‹å’Œå®Œå…¨å¾®è°ƒçš„å¯æ‰©å±•æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæç¤ºå­¦ä¹ é¢ä¸´æ¥è‡ªLLMå’ŒCLIPå˜ä½“ä¹‹é—´è¯­ä¹‰ä¸å¯¹é½çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºä¸åŒçš„è®­ç»ƒè¯­æ–™åº“å’Œæ¨¡å‹æ¶æ„å¯¼è‡´çš„ï¼›å®ƒç¼ºä¹åœ¨ä¸æ–­å‘å±•çš„åŸºç¡€æ¨¡å‹å®¶æ—ä¸­çš„å¯æ‰©å±•æ€§ã€‚æ›´å…³é”®çš„æ˜¯ï¼Œé€šè¿‡ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´ä¼˜åŒ–è¿›è¡Œçš„æˆå¯¹å¤šæ¨¡å¼å¯¹é½ç¼ºä¹å»ºæ¨¡ç»Ÿä¸€è¡¨ç¤ºæˆ–åº”ç”¨å±€éƒ¨å‡ ä½•çº¦æŸçš„èƒ½åŠ›ï¼Œè¿™å¾€å¾€ä¼šæ”¾å¤§å¤æ‚ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„æ¨¡å¼å·®è·ï¼Œå¹¶ç ´åå°‘é‡æ•°æ®çš„é€‚åº”æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†vMFCoOpæ¡†æ¶ï¼Œå®ƒé€šè¿‡é€†å‘ä¼°è®¡å…±äº«è¶…çƒæµå½¢ä¸Šçš„von Mises-Fisherï¼ˆvMFï¼‰åˆ†å¸ƒï¼Œé€šè¿‡ç»Ÿä¸€è¯­ä¹‰é”šå¯¹é½ä»»æ„çš„LLMå’ŒCLIPéª¨å¹²ç½‘ï¼Œå®ç°ç¨³å¥çš„ç”Ÿç‰©åŒ»å­¦æç¤ºå’Œå‡ºè‰²çš„å°‘é‡æ•°æ®åˆ†ç±»ã€‚åŸºäºä¸‰ä¸ªäº’è¡¥çš„çº¦æŸï¼ŒvMFCoOpåœ¨14ä¸ªåŒ»ç–—æ•°æ®é›†ã€12ç§åŒ»ç–—æˆåƒæ–¹å¼å’Œ13ä¸ªè§£å‰–åŒºåŸŸä¸Šæ˜¾ç¤ºå‡ºæŒç»­çš„ä¸€è‡´æ€§æ”¹è¿›ï¼Œåœ¨å‡†ç¡®æ€§ã€é€šç”¨æ€§å’Œä¸´åºŠé€‚ç”¨æ€§æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æœ¬å·¥ä½œçš„ç›®æ ‡æ˜¯ä¸æ–­æ‰©å¤§ï¼Œæ¶µç›–æ›´å¤šçš„ä¸‹æ¸¸åº”ç”¨ï¼Œç›¸å…³èµ„æºå°†é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/UniEqui%E8%BF%9B%E8%A1%8C%E5%85%B1%E4%BA%AB%E3%80%82">https://github.com/VinyehShaw/UniEquiè¿›è¡Œå…±äº«ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09540v2">PDF</a> Accepted as an Oral Presentation at AAAI 2026 Main Technical Track (this version is not peer-reviewed; it is the extended version)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç‚¼åŒ»å­¦è¯­ä¹‰å…ˆéªŒçš„ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆCoOpï¼‰è¿›å±•ä¸ºè§£å†³ç”Ÿç‰©åŒ»å­¦CLIPåŸºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é€‚åº”é—®é¢˜æä¾›äº†å¯ä¼¸ç¼©çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¦‚æ‰‹åŠ¨æç¤ºå·¥ç¨‹å’Œå…¨ç²¾ç»†è°ƒæ•´ã€‚ç„¶è€Œï¼Œè¯¥èƒŒæ™¯ä¸‹çš„æç¤ºå­¦ä¹ é¢ä¸´LLMså’ŒCLIPå˜ä½“é—´è¯­ä¹‰ä¸ä¸€è‡´çš„æŒ‘æˆ˜ï¼Œå› ä¸¤è€…æœ‰ä¸åŒçš„è®­ç»ƒè¯­æ–™åº“å’Œæ¨¡å‹æ¶æ„ï¼Œä¸”ç¼ºä¹è·¨ä¸æ–­å‘å±•çš„åŸºç¡€æ¨¡å‹çš„æ‰©å±•æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œé€šè¿‡ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´ä¼˜åŒ–è¿›è¡Œä¸€å¯¹ä¸€çš„å¤šæ¨¡å¼å¯¹é½æ— æ³•å»ºæ¨¡ç»Ÿä¸€è¡¨ç¤ºæˆ–åº”ç”¨å±€éƒ¨å‡ ä½•çº¦æŸï¼Œè¿™åœ¨å¤æ‚çš„ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­ä¼šæ”¾å¤§æ¨¡å¼å·®è·å¹¶ç ´åå°æ ·æœ¬é€‚åº”çš„ç¨³å®šæ€§ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†vMFCoOpæ¡†æ¶ï¼Œå®ƒé€šè¿‡å…±äº«è¶…çƒé¢æµå½¢ä¸Šçš„é€†å‘ä¼°è®¡von Mises-Fisherï¼ˆvMFï¼‰åˆ†å¸ƒï¼Œå¹¶åˆ©ç”¨ç»Ÿä¸€è¯­ä¹‰é”šå¯¹é½ä»»æ„LLMså’ŒCLIPéª¨å¹²ç½‘ä¹‹é—´çš„è¯­ä¹‰åå·®ï¼Œå®ç°äº†ç¨³å¥çš„ç”Ÿç‰©åŒ»å­¦æç¤ºå’Œä¼˜è¶Šçš„å°æ ·æœ¬åˆ†ç±»ã€‚vMFCoOpåœ¨ä¸‰ä¸ªäº’è¡¥çº¦æŸçš„æ”¯æŒä¸‹ï¼Œåœ¨14ä¸ªåŒ»ç–—æ•°æ®é›†ã€12ç§åŒ»ç–—æˆåƒæ–¹å¼å’Œ13ä¸ªè§£å‰–åŒºåŸŸä¸Šå®ç°äº†æ”¹è¿›ï¼Œåœ¨å‡†ç¡®æ€§ã€é€šç”¨æ€§å’Œä¸´åºŠé€‚ç”¨æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å·¥ä½œæ—¨åœ¨ä¸æ–­æ‰©å±•ä»¥æ¶µç›–æ›´å¤šçš„ä¸‹æ¸¸åº”ç”¨ï¼Œç›¸å…³èµ„æºå°†é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/UniEqui%E5%85%B1%E4%BA%AB%E3%80%82">https://github.com/VinyehShaw/UniEquiå…±äº«ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-distilled medical semantic priors guide recent advances in context optimization (CoOp) for biomedical CLIP-based vision-language models (VLMs).</li>
<li>Prompt learning in this context faces challenges due to semantic misalignment between LLMs and CLIP variants.</li>
<li>ä¼ ç»Ÿçš„æ¬§æ°ç©ºé—´ä¼˜åŒ–åœ¨å¤æ‚ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­å­˜åœ¨æ¨¡æ€å·®è·å’Œç¨³å®šæ€§é—®é¢˜ã€‚</li>
<li>vMFCoOpæ¡†æ¶é€šè¿‡å…±äº«è¶…çƒé¢æµå½¢ä¸Šçš„é€†å‘ä¼°è®¡von Mises-Fisherï¼ˆvMFï¼‰åˆ†å¸ƒæ¥å¯¹é½è¯­ä¹‰åå·®ï¼Œå®ç°äº†ç¨³å¥çš„ç”Ÿç‰©åŒ»å­¦æç¤ºå’Œä¼˜è¶Šçš„å°æ ·æœ¬åˆ†ç±»ã€‚</li>
<li>vMFCoOpåœ¨å¤šä¸ªåŒ»ç–—æ•°æ®é›†ã€æˆåƒæ–¹å¼å’Œè§£å‰–åŒºåŸŸä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ­¤å·¥ä½œæ—¨åœ¨é€šè¿‡ä¸æ–­æ‰©å±•ä»¥æ¶µç›–æ›´å¤šä¸‹æ¸¸åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a12f86f9e44d6d45af5fc33a08ce354e" align="middle">
<img src="https://picx.zhimg.com/v2-d1c51fb95f443e62bf65fe82ae6cb003" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-cross-modal-pre-training-framework-with-video-data-for-improving-performance-and-generalization-of-distributed-acoustic-sensing"><a href="#A-cross-modal-pre-training-framework-with-video-data-for-improving-performance-and-generalization-of-distributed-acoustic-sensing" class="headerlink" title="A cross-modal pre-training framework with video data for improving performance and generalization of distributed acoustic sensing"></a>A cross-modal pre-training framework with video data for improving performance and generalization of distributed acoustic sensing</h2><p><strong>Authors:Junyi Duan, Jiageng Chen, Zuyuan He</strong></p>
<p>Fiber-optic distributed acoustic sensing (DAS) has emerged as a critical Internet-of-Things (IoT) sensing technology with broad industrial applications. However, the two-dimensional spatial-temporal morphology of DAS signals presents analytical challenges where conventional methods prove suboptimal, while being well-suited for deep learning approaches. Although our previous work, DAS Masked Autoencoder (DAS-MAE), established state-of-the-art performance and generalization without labels, it is not satisfactory in frequency analysis in temporal-dominated DAS data. Moreover, the limitation of effective training data fails to address the substantial data requirements inherent to Transformer architectures in DAS-MAE. To overcome these limitations, we present an enhanced framework incorporating short-time Fourier transform (STFT) for explicit temporal-frequency feature extraction and pioneering video-to-DAS cross-modal pre-training to mitigate data constraints. This approach learns high-level representations (e.g., event classification) through label-free reconstruction tasks. Experimental results demonstrate transformative improvements: 0.1% error rate in few-shot classification (90.9% relative improvement over DAS-MAE) and 4.7% recognition error in external damage prevention applications (75.4% improvement over from-scratch training). As the first work to pioneer video-to-DAS cross-modal pre-training, available training resources are expanded by bridging computer vision and distributed sensing areas. The enhanced performance and generalization facilitate DAS deployment across diverse industrial scenarios while advancing cross-modal representation learning for industrial IoT sensing.</p>
<blockquote>
<p>å…‰çº¤åˆ†å¸ƒå¼å£°å­¦ä¼ æ„Ÿï¼ˆDASï¼‰å·²ç»ä½œä¸ºä¸€ç§å…³é”®çš„ç‰©è”ç½‘ï¼ˆIoTï¼‰ä¼ æ„ŸæŠ€æœ¯å´­éœ²å¤´è§’ï¼Œåœ¨å·¥ä¸šé¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼ŒDASä¿¡å·çš„äºŒç»´æ—¶ç©ºå½¢æ€ç»™åˆ†æå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œä¼ ç»Ÿçš„æ–¹æ³•è¯æ˜æ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œè€Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åˆ™æ›´åŠ é€‚ç”¨ã€‚å°½ç®¡æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œDASæ©ç è‡ªç¼–ç å™¨ï¼ˆDAS-MAEï¼‰åœ¨æ— æ ‡ç­¾æƒ…å†µä¸‹å»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨ä»¥æ—¶é—´ä¸ºä¸»çš„DASæ•°æ®çš„é¢‘ç‡åˆ†æä¸­å¹¶ä¸ä»¤äººæ»¡æ„ã€‚æ­¤å¤–ï¼Œæœ‰æ•ˆè®­ç»ƒæ•°æ®çš„é™åˆ¶æ— æ³•è§£å†³DAS-MAEä¸­Transformeræ¶æ„å›ºæœ‰çš„å¤§é‡æ•°æ®è¦æ±‚ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºæ¡†æ¶ï¼Œç»“åˆäº†çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰è¿›è¡Œæ˜ç¡®çš„æ—¶ç©ºé¢‘ç‡ç‰¹å¾æå–ï¼Œä»¥åŠå¼€åˆ›æ€§çš„è§†é¢‘åˆ°DASè·¨æ¨¡æ€é¢„è®­ç»ƒæ¥ç¼“è§£æ•°æ®çº¦æŸã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ— æ ‡ç­¾é‡å»ºä»»åŠ¡å­¦ä¹ é«˜çº§è¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼Œäº‹ä»¶åˆ†ç±»ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜å®ç°äº†çªç ´æ€§æ”¹è¿›ï¼šåœ¨å°‘é•œå¤´åˆ†ç±»ä¸­çš„é”™è¯¯ç‡ä¸º0.1%ï¼ˆç›¸å¯¹äºDAS-MAEæœ‰90.9%çš„ç›¸å¯¹æ”¹è¿›ï¼‰ï¼Œåœ¨å¤–éƒ¨æŸä¼¤é¢„é˜²åº”ç”¨ä¸­çš„è¯†åˆ«é”™è¯¯ç‡ä¸º4.7%ï¼ˆç›¸å¯¹äºä»å¤´å¼€å§‹è®­ç»ƒæœ‰75.4%çš„æ”¹è¿›ï¼‰ã€‚ä½œä¸ºå¼€åˆ›è§†é¢‘åˆ°DASè·¨æ¨¡æ€é¢„è®­ç»ƒçš„ç¬¬ä¸€é¡¹å·¥ä½œï¼Œé€šè¿‡è¿æ¥è®¡ç®—æœºè§†è§‰å’Œåˆ†å¸ƒå¼ä¼ æ„Ÿé¢†åŸŸï¼Œæ‰©å¤§äº†å¯ç”¨çš„è®­ç»ƒèµ„æºã€‚å¢å¼ºæ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æœ‰åŠ©äºåœ¨å¤šç§å·¥ä¸šåœºæ™¯ä¸­éƒ¨ç½²DASï¼ŒåŒæ—¶æ¨åŠ¨å·¥ä¸šç‰©è”ç½‘æ„ŸçŸ¥çš„è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09342v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    å…‰çº¤åˆ†å¸ƒå¼å£°å­¦æ„ŸçŸ¥ï¼ˆDASï¼‰æ˜¯ç‰©è”ç½‘ï¼ˆIoTï¼‰çš„å…³é”®ä¼ æ„ŸæŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºå·¥ä¸šé¢†åŸŸã€‚ç„¶è€Œï¼ŒDASä¿¡å·çš„äºŒç»´æ—¶ç©ºå½¢æ€åˆ†æå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¼ ç»Ÿæ–¹æ³•æ•ˆæœæœ‰é™ï¼Œè€Œæ·±åº¦å­¦ä¹ åˆ™å±•ç°å‡ºä¼˜åŠ¿ã€‚è™½ç„¶å…ˆå‰å·¥ä½œDAS Masked Autoencoderï¼ˆDAS-MAEï¼‰åœ¨æ— æ ‡ç­¾æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨ä»¥æ—¶é—´ä¸ºä¸»çš„DASæ•°æ®çš„é¢‘ç‡åˆ†æä¸Šä»æœ‰ä¸è¶³ã€‚æ­¤å¤–ï¼Œå—é™äºæœ‰æ•ˆçš„è®­ç»ƒæ•°æ®æ— æ³•æ»¡è¶³Transformeræ¶æ„çš„æ•°æ®éœ€æ±‚ã€‚ä¸ºå…‹æœè¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªç»“åˆçŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰è¿›è¡Œæ˜¾å¼æ—¶ç©ºé¢‘ç‡ç‰¹å¾æå–çš„å¢å¼ºæ¡†æ¶ï¼Œå¹¶å¼€åˆ›è§†é¢‘åˆ°DASçš„è·¨æ¨¡æ€é¢„è®­ç»ƒä»¥ç¼“è§£æ•°æ®çº¦æŸã€‚è¯¥æ–¹æ³•é€šè¿‡æ— æ ‡ç­¾é‡å»ºä»»åŠ¡å­¦ä¹ é«˜çº§è¡¨ç¤ºï¼ˆå¦‚äº‹ä»¶åˆ†ç±»ï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºå‡ºçªç ´æ€§æ”¹è¿›ï¼šåœ¨å°‘æ ·æœ¬åˆ†ç±»ä¸­çš„é”™è¯¯ç‡ä¸º0.1%ï¼ˆç›¸å¯¹äºDAS-MAEæœ‰90.9%çš„ç›¸å¯¹æ”¹è¿›ï¼‰ï¼Œåœ¨å¤–éƒ¨æŸä¼¤é¢„é˜²åº”ç”¨ä¸­çš„è¯†åˆ«é”™è¯¯ç‡ä¸º4.7%ï¼ˆç›¸å¯¹äºä»å¤´å¼€å§‹è®­ç»ƒæœ‰75.4%çš„æ”¹è¿›ï¼‰ã€‚ä½œä¸ºé¦–æ¬¡å°è¯•è§†é¢‘åˆ°DASè·¨æ¨¡æ€é¢„è®­ç»ƒçš„å·¥ä½œï¼Œæˆ‘ä»¬æ‰©å¤§äº†å¯ç”¨çš„è®­ç»ƒèµ„æºï¼Œé€šè¿‡è¿æ¥è®¡ç®—æœºè§†è§‰å’Œåˆ†å¸ƒå¼æ„ŸçŸ¥é¢†åŸŸã€‚å¢å¼ºæ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æœ‰åŠ©äºåœ¨ä¸åŒå·¥ä¸šåœºæ™¯ä¸­éƒ¨ç½²DASï¼ŒåŒæ—¶æ¨åŠ¨å·¥ä¸šç‰©è”ç½‘æ„ŸçŸ¥çš„è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…‰çº¤åˆ†å¸ƒå¼å£°å­¦æ„ŸçŸ¥ï¼ˆDASï¼‰æ˜¯IoTçš„é‡è¦ä¼ æ„ŸæŠ€æœ¯ï¼Œå…·æœ‰å¹¿æ³›çš„å·¥ä¸šåº”ç”¨ã€‚</li>
<li>DASä¿¡å·çš„äºŒç»´æ—¶ç©ºå½¢æ€åˆ†æå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¼ ç»Ÿæ–¹æ³•è¡¨ç°ä¸ä½³ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•æ›´ä¼˜ã€‚</li>
<li>å…ˆå‰çš„DAS-MAEæ–¹æ³•åœ¨é¢‘ç‡åˆ†æä¸Šä»æœ‰ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥æ—¶é—´ä¸ºä¸»çš„DASæ•°æ®ä¸­ã€‚</li>
<li>æå‡ºç»“åˆçŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰çš„å¢å¼ºæ¡†æ¶ï¼Œè¿›è¡Œæ˜¾å¼çš„æ—¶ç©ºé¢‘ç‡ç‰¹å¾æå–ã€‚</li>
<li>å¼€åˆ›è§†é¢‘åˆ°DASçš„è·¨æ¨¡æ€é¢„è®­ç»ƒï¼Œä»¥ç¼“è§£æ•°æ®çº¦æŸå¹¶æ‰©å¤§è®­ç»ƒèµ„æºã€‚</li>
<li>æ–¹æ³•é€šè¿‡æ— æ ‡ç­¾é‡å»ºä»»åŠ¡å­¦ä¹ é«˜çº§è¡¨ç¤ºï¼Œå¦‚äº‹ä»¶åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-691ec4bff9ff9f373f46e55025001265" align="middle">
<img src="https://picx.zhimg.com/v2-8550c6e32c3de07a02df55b46e863685" align="middle">
<img src="https://picx.zhimg.com/v2-b7e972ea1cb857c82e26ac5cb2267b2e" align="middle">
<img src="https://picx.zhimg.com/v2-0d3496abe59104bedc4b8baea91e07e5" align="middle">
<img src="https://picx.zhimg.com/v2-c091061f1324c0b8acab4de1cbb1edbd" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Zero-Order-Sharpness-Aware-Minimization"><a href="#Zero-Order-Sharpness-Aware-Minimization" class="headerlink" title="Zero-Order Sharpness-Aware Minimization"></a>Zero-Order Sharpness-Aware Minimization</h2><p><strong>Authors:Yao Fu, Yihang Jin, Chunxia Zhang, Junmin Liu, Haishan Ye</strong></p>
<p>Prompt learning has become a key method for adapting large language models to specific tasks with limited data. However, traditional gradient-based optimization methods for tuning prompts are computationally intensive, posing challenges for efficiency. We introduce ZOSA (Zero-Order Sharpness-Aware Minimization), a novel optimization framework that integrates zero-order optimization with sharpness-aware minimization to enhance prompt tuning. ZOSA employs Rademacher perturbation vectors to estimate gradients without requiring backpropagation. By incorporating sharpness-aware principles, it targets flat minima in the loss landscape, improving generalization. An adaptive learning rate, guided by loss variability, further ensures stable convergence. Experiments on few-shot learning tasks, such as text classification and natural language inference, show that ZOSA significantly outperforms existing methods. With its theoretical foundation and computational efficiency, ZOSA offers a practical solution for prompt-based learning in resource-limited settings.</p>
<blockquote>
<p>æç¤ºå­¦ä¹ å·²æˆä¸ºé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç‰¹å®šä»»åŠ¡çš„å…³é”®æ–¹æ³•ï¼Œå°¤å…¶å½“æ•°æ®æœ‰é™æ—¶ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•åœ¨è°ƒæ•´æç¤ºæ—¶è®¡ç®—é‡å¤§ï¼Œå¯¹æ•ˆç‡æ„æˆæŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ZOSAï¼ˆé›¶é˜¶å°–é”åº¦æ„ŸçŸ¥æœ€å°åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–æ¡†æ¶ï¼Œå®ƒå°†é›¶é˜¶ä¼˜åŒ–ä¸å°–é”åº¦æ„ŸçŸ¥æœ€å°åŒ–ç›¸ç»“åˆï¼Œä»¥æé«˜æç¤ºè°ƒæ•´èƒ½åŠ›ã€‚ZOSAä½¿ç”¨Rademacheræ‰°åŠ¨å‘é‡æ¥ä¼°è®¡æ¢¯åº¦ï¼Œæ— éœ€åå‘ä¼ æ’­ã€‚é€šè¿‡ç»“åˆå°–é”åº¦æ„ŸçŸ¥åŸç†ï¼Œå®ƒåœ¨æŸå¤±æ™¯è§‚ä¸­å¯»æ‰¾å¹³å¦çš„æœ€å°å€¼ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚ç”±æŸå¤±å˜åŒ–å¼•å¯¼çš„è‡ªé€‚åº”å­¦ä¹ ç‡è¿›ä¸€æ­¥ç¡®ä¿ç¨³å®šçš„æ”¶æ•›ã€‚åœ¨æ–‡æœ¬åˆ†ç±»å’Œè‡ªç„¶è¯­è¨€æ¨ç†ç­‰å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒZOSAæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å‡­å€Ÿå…¶ç†è®ºåŸºç¡€å’Œè®¡ç®—æ•ˆç‡ï¼ŒZOSAä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„åŸºäºæç¤ºçš„å­¦ä¹ æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09156v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æå‡ºä¸€ç§åä¸ºZOSAçš„æ–°å‹ä¼˜åŒ–æ¡†æ¶ï¼Œç»“åˆé›¶é˜¶ä¼˜åŒ–å’Œå°–é”åº¦æ„ŸçŸ¥æœ€å°åŒ–æŠ€æœ¯ï¼Œç”¨äºæé«˜æç¤ºè°ƒæ•´çš„æ•ˆç‡ã€‚ZOSAé‡‡ç”¨Rademacheræ‰°åŠ¨å‘é‡è¿›è¡Œæ¢¯åº¦ä¼°è®¡ï¼Œä¸éœ€è¦åå‘ä¼ æ’­ã€‚å®ƒé€šè¿‡åœ¨æŸå¤±æ™¯è§‚ä¸­å¯»æ‰¾å¹³å¦çš„æœ€å°å€¼æ¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ–‡æœ¬åˆ†ç±»å’Œè‡ªç„¶è¯­è¨€æ¨ç†ç­‰å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒZOSAæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºèµ„æºæœ‰é™ç¯å¢ƒä¸‹çš„æç¤ºå­¦ä¹ æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ZOSAç»“åˆé›¶é˜¶ä¼˜åŒ–å’Œå°–é”åº¦æ„ŸçŸ¥æœ€å°åŒ–ï¼Œæ—¨åœ¨æé«˜æç¤ºè°ƒæ•´çš„æ•ˆç‡ã€‚</li>
<li>ZOSAé‡‡ç”¨Rademacheræ‰°åŠ¨å‘é‡ä¼°è®¡æ¢¯åº¦ï¼Œæ— éœ€åå‘ä¼ æ’­ã€‚</li>
<li>é€šè¿‡å¯»æ‰¾æŸå¤±æ™¯è§‚ä¸­çš„å¹³å¦æœ€å°å€¼ï¼ŒZOSAæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ZOSAå…·æœ‰è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæ ¹æ®æŸå¤±å˜åŒ–è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œç¡®ä¿ç¨³å®šæ”¶æ•›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒZOSAåœ¨æ–‡æœ¬åˆ†ç±»å’Œè‡ªç„¶è¯­è¨€æ¨ç†ç­‰å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ZOSAå…·æœ‰ç†è®ºæ”¯æ’‘å’Œè®¡ç®—æ•ˆç‡ï¼Œé€‚ç”¨äºèµ„æºæœ‰é™çš„ç¯å¢ƒä¸‹çš„æç¤ºå­¦ä¹ ã€‚</li>
<li>ZOSAä¸ºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºè°ƒå‚æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b96a0802ab773aa5685064c6372d85b9" align="middle">
<img src="https://picx.zhimg.com/v2-926aa61c24bd5a1e399234be4765a7f3" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VietMEAgent-Culturally-Aware-Few-Shot-Multimodal-Explanation-for-Vietnamese-Visual-Question-Answering"><a href="#VietMEAgent-Culturally-Aware-Few-Shot-Multimodal-Explanation-for-Vietnamese-Visual-Question-Answering" class="headerlink" title="VietMEAgent: Culturally-Aware Few-Shot Multimodal Explanation for Vietnamese Visual Question Answering"></a>VietMEAgent: Culturally-Aware Few-Shot Multimodal Explanation for Vietnamese Visual Question Answering</h2><p><strong>Authors:Hai-Dang Nguyen, Minh-Anh Dang, Minh-Tan Le, Minh-Tuan Le</strong></p>
<p>Contemporary Visual Question Answering (VQA) systems remain constrained when confronted with culturally specific content, largely because cultural knowledge is under-represented in training corpora and the reasoning process is not rendered interpretable to end users. This paper introduces VietMEAgent, a multimodal explainable framework engineered for Vietnamese cultural understanding. The method integrates a cultural object detection backbone with a structured program generation layer, yielding a pipeline in which answer prediction and explanation are tightly coupled. A curated knowledge base of Vietnamese cultural entities serves as an explicit source of background information, while a dual-modality explanation module combines attention-based visual evidence with structured, human-readable textual rationales. We further construct a Vietnamese Cultural VQA dataset sourced from public repositories and use it to demonstrate the practicality of programming-based methodologies for cultural AI. The resulting system provides transparent explanations that disclose both the computational rationale and the underlying cultural context, supporting education and cultural preservation with an emphasis on interpretability and cultural sensitivity.</p>
<blockquote>
<p>ç°ä»£è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ç³»ç»Ÿåœ¨é¢å¯¹æ–‡åŒ–ç‰¹å®šå†…å®¹æ—¶ä»å—åˆ°è¯¸å¤šé™åˆ¶ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºè®­ç»ƒè¯­æ–™åº“ä¸­çš„æ–‡åŒ–çŸ¥è¯†å‚¨å¤‡ä¸è¶³ï¼Œä¸”æ¨ç†è¿‡ç¨‹æ— æ³•å‘æœ€ç»ˆç”¨æˆ·è§£é‡Šã€‚æœ¬æ–‡ä»‹ç»äº†VietMEAgentï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è¶Šå—æ–‡åŒ–ç†è§£çš„å¤šæ¨¡å¼å¯è§£é‡Šæ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†æ–‡åŒ–å¯¹è±¡æ£€æµ‹éª¨å¹²ä¸ç»“æ„åŒ–ç¨‹åºç”Ÿæˆå±‚ç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ªç´§å¯†è€¦åˆçš„ç­”æ¡ˆé¢„æµ‹å’Œè§£é‡Šç®¡é“ã€‚è¶Šå—æ–‡åŒ–å®ä½“çš„ç²¾é€‰çŸ¥è¯†åº“ä½œä¸ºèƒŒæ™¯ä¿¡æ¯çš„æ˜ç¡®æ¥æºï¼Œè€ŒåŒæ¨¡å¼è§£é‡Šæ¨¡å—åˆ™ç»“åˆäº†åŸºäºæ³¨æ„åŠ›çš„è§†è§‰è¯æ®ä¸ç»“æ„åŒ–ã€å¯è¯»çš„æ–‡æœ¬ç†ç”±ã€‚æˆ‘ä»¬è¿˜ä»å…¬å…±å­˜å‚¨åº“ä¸­æ„å»ºäº†è¶Šå—æ–‡åŒ–VQAæ•°æ®é›†ï¼Œå¹¶ç”¨å®ƒæ¥è¯æ˜åŸºäºç¼–ç¨‹çš„æ–‡åŒ–äººå·¥æ™ºèƒ½æ–¹æ³•çš„å®ç”¨æ€§ã€‚è¯¥ç³»ç»Ÿæä¾›é€æ˜çš„è§£é‡Šï¼Œæ—¢æ­ç¤ºè®¡ç®—æ¨ç†åˆæ­ç¤ºæ½œåœ¨çš„æ–‡åŒ–èƒŒæ™¯ï¼Œå¼ºè°ƒå¯è§£é‡Šæ€§å’Œæ–‡åŒ–æ•æ„Ÿæ€§ï¼Œæ”¯æŒæ•™è‚²å’Œæ–‡åŒ–ä¿æŠ¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09058v1">PDF</a> 7 pages, 3 figures, 3 tables, FAIR 2025 conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VietMEAgentï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºè¶Šå—æ–‡åŒ–ç†è§£è®¾è®¡çš„å¤šæ¨¡æ€å¯è§£é‡Šæ¡†æ¶ã€‚å®ƒé€šè¿‡æ•´åˆæ–‡åŒ–å¯¹è±¡æ£€æµ‹ä¸»å¹²ä¸ç»“æ„åŒ–ç¨‹åºç”Ÿæˆå±‚ï¼Œå»ºç«‹ä¸€ä¸ªç´§å¯†è€¦åˆçš„ç­”æ¡ˆé¢„æµ‹å’Œè§£é‡Šç®¡é“ã€‚ä½¿ç”¨è¶Šå—æ–‡åŒ–å®ä½“çš„ç²¾é€‰çŸ¥è¯†åº“ä½œä¸ºèƒŒæ™¯ä¿¡æ¯çš„æ˜ç¡®æ¥æºï¼ŒåŒæ—¶ç»“åˆåŸºäºæ³¨æ„åŠ›çš„è§†è§‰è¯æ®å’Œç»“æ„åŒ–ã€å¯è¯»çš„æ–‡æœ¬è§£é‡Šï¼Œæ„æˆåŒæ¨¡æ€è§£é‡Šæ¨¡å—ã€‚æ­¤å¤–ï¼Œä»å…¬å…±ä»“åº“æ„å»ºè¶Šå—æ–‡åŒ–VQAæ•°æ®é›†ï¼Œæ¼”ç¤ºåŸºäºç¼–ç¨‹çš„æ–‡åŒ–AIçš„å®ç”¨æ€§ã€‚è¯¥ç³»ç»Ÿæä¾›é€æ˜çš„è§£é‡Šï¼Œæ—¢æ­ç¤ºè®¡ç®—åŸç†åˆå±•ç°æ–‡åŒ–èƒŒæ™¯ï¼Œæ”¯æŒæ•™è‚²ä¸æ–‡åŒ–ä¿è‚²ï¼Œå¹¶å¼ºè°ƒå¯è§£é‡Šæ€§å’Œæ–‡åŒ–æ•æ„Ÿæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.VietMEAgentæ¡†æ¶æ—¨åœ¨æé«˜VQAç³»ç»Ÿå¯¹è¶Šå—æ–‡åŒ–å†…å®¹çš„ç†è§£ã€‚<br>2.è¯¥æ¡†æ¶ç»“åˆæ–‡åŒ–å¯¹è±¡æ£€æµ‹ä¸ç»“æ„åŒ–ç¨‹åºç”Ÿæˆï¼Œå®ç°ç­”æ¡ˆé¢„æµ‹ä¸è§£é‡Šçš„ç´§å¯†ç»“åˆã€‚<br>3.ä½¿ç”¨è¶Šå—æ–‡åŒ–å®ä½“çš„çŸ¥è¯†åº“ä½œä¸ºèƒŒæ™¯ä¿¡æ¯çš„æ¥æºã€‚<br>4.åŒæ¨¡æ€è§£é‡Šæ¨¡å—ç»“åˆè§†è§‰è¯æ®å’Œæ–‡æœ¬è§£é‡Šï¼Œæé«˜ç³»ç»Ÿçš„å¯è§£é‡Šæ€§ã€‚<br>5.ç ”ç©¶æ„å»ºäº†è¶Šå—æ–‡åŒ–VQAæ•°æ®é›†ï¼Œå±•ç¤ºç¼–ç¨‹æ–¹æ³•åœ¨æ–‡åŒ–AIä¸­çš„å®ç”¨æ€§ã€‚<br>6.è¯¥ç³»ç»Ÿæä¾›çš„è§£é‡Šæ—¢æ¶‰åŠè®¡ç®—åŸç†åˆåŒ…å«æ–‡åŒ–èƒŒæ™¯ï¼Œå¼ºè°ƒæ•™è‚²åŠæ–‡åŒ–ä¿è‚²åŠŸèƒ½ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a02d3055cc01a5077141577a411bb53" align="middle">
<img src="https://picx.zhimg.com/v2-3e33d4210e2cce69f0ef830a14bbfd23" align="middle">
<img src="https://picx.zhimg.com/v2-935a8648c522e78d6a178af21fe09426" align="middle">
<img src="https://picx.zhimg.com/v2-838d12df874b082c474a26aa55b249be" align="middle">
<img src="https://picx.zhimg.com/v2-296fdfabadc99bdf411e267d55043c54" align="middle">
<img src="https://picx.zhimg.com/v2-69910e5ec525e43df4bcf651268bf76f" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Fairness-Aware-Few-Shot-Learning-for-Audio-Visual-Stress-Detection"><a href="#Fairness-Aware-Few-Shot-Learning-for-Audio-Visual-Stress-Detection" class="headerlink" title="Fairness-Aware Few-Shot Learning for Audio-Visual Stress Detection"></a>Fairness-Aware Few-Shot Learning for Audio-Visual Stress Detection</h2><p><strong>Authors:Anushka Sanjay Shelke, Aditya Sneh, Arya Adyasha, Haroon R. Lone</strong></p>
<p>Fairness in AI-driven stress detection is critical for equitable mental healthcare, yet existing models frequently exhibit gender bias, particularly in data-scarce scenarios. To address this, we propose FairM2S, a fairness-aware meta-learning framework for stress detection leveraging audio-visual data. FairM2S integrates Equalized Odds constraints during both meta-training and adaptation phases, employing adversarial gradient masking and fairness-constrained meta-updates to effectively mitigate bias. Evaluated against five state-of-the-art baselines, FairM2S achieves 78.1% accuracy while reducing the Equal Opportunity to 0.06, demonstrating substantial fairness gains. We also release SAVSD, a smartphone-captured dataset with gender annotations, designed to support fairness research in low-resource, real-world contexts. Together, these contributions position FairM2S as a state-of-the-art approach for equitable and scalable few-shot stress detection in mental health AI. We release our dataset and FairM2S publicly with this paper.</p>
<blockquote>
<p>äººå·¥æ™ºèƒ½é©±åŠ¨çš„åº”æ¿€æ£€æµ‹ä¸­çš„å…¬å¹³æ€§å¯¹äºå…¬å¹³çš„å¿ƒç†å¥åº·æŠ¤ç†è‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰çš„æ¨¡å‹ç»å¸¸è¡¨ç°å‡ºæ€§åˆ«åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FairM2Sï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨è§†å¬æ•°æ®çš„åº”æ¿€æ£€æµ‹å…¬å¹³æ€§æ„ŸçŸ¥å…ƒå­¦ä¹ æ¡†æ¶ã€‚FairM2Såœ¨å…ƒè®­ç»ƒå’Œé€‚åº”é˜¶æ®µéƒ½æ•´åˆäº†å‡è¡¡æœºä¼šçº¦æŸï¼Œé‡‡ç”¨å¯¹æŠ—æ€§æ¢¯åº¦æ©è”½å’Œå…¬å¹³æ€§çº¦æŸå…ƒæ›´æ–°æ¥æœ‰æ•ˆå‡è½»åè§ã€‚ä¸äº”ç§æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼ŒFairM2Sè¾¾åˆ°äº†78.1%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å°†å¹³ç­‰æœºä¼šé™ä½åˆ°0.06ï¼Œæ˜¾ç¤ºå‡ºå®è´¨æ€§çš„å…¬å¹³æ”¶ç›Šã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†SAVSDï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰æ€§åˆ«æ³¨é‡Šçš„æ™ºèƒ½æ‰‹æœºæ•æ‰æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒä½èµ„æºã€ç°å®ç¯å¢ƒä¸­çš„å…¬å¹³æ€§ç ”ç©¶ã€‚è¿™äº›è´¡çŒ®å…±åŒå°†FairM2Så®šä½ä¸ºå¿ƒç†å¥åº·äººå·¥æ™ºèƒ½ä¸­å…¬å¹³ä¸”å¯æ‰©å±•çš„å°‘é‡åº”æ¿€æ£€æµ‹çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æˆ‘ä»¬éšè¿™ç¯‡è®ºæ–‡å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†å’ŒFairM2Sã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09039v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„å‹åŠ›æ£€æµ‹ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºæƒ…å¢ƒä¸‹çš„æ€§åˆ«åè§é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†FairM2Sï¼Œä¸€ä¸ªåˆ©ç”¨è§†å¬æ•°æ®çš„å…¬å¹³æ„è¯†å…ƒå­¦ä¹ æ¡†æ¶ã€‚FairM2Såœ¨å…ƒè®­ç»ƒå’Œé€‚åº”é˜¶æ®µéƒ½æ•´åˆäº†å¹³ç­‰æœºä¼šçº¦æŸï¼Œé‡‡ç”¨å¯¹æŠ—æ€§æ¢¯åº¦æ©è”½å’Œå…¬å¹³æ€§çº¦æŸå…ƒæ›´æ–°æ¥æœ‰æ•ˆå‡è½»åè§ã€‚ä¸äº”ç§æœ€å…ˆè¿›åŸºçº¿ç›¸æ¯”ï¼ŒFairM2Sè¾¾åˆ°äº†78.1%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å°†å¹³ç­‰æœºä¼šé™è‡³0.06ï¼Œè¯æ˜äº†å…¶å…¬å¹³æ€§æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†SAVSDæ•°æ®é›†ï¼ŒåŒ…å«æ€§åˆ«æ³¨é‡Šï¼Œç”¨äºæ”¯æŒä½èµ„æºã€ç°å®ç¯å¢ƒä¸‹çš„å…¬å¹³æ€§ç ”ç©¶ã€‚æ€»çš„æ¥è¯´ï¼ŒFairM2Sæˆä¸ºäº†ä¸€ç§å…ˆè¿›çš„æ–¹æ³•ï¼Œå¯å®ç°å¿ƒç†å¥åº·AIä¸­çš„å…¬å¹³å’Œå¯æ‰©å±•çš„å°‘é‡å‹åŠ›æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨å‹åŠ›æ£€æµ‹ä¸­é¢ä¸´å…¬å¹³æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºæƒ…å¢ƒä¸‹çš„æ€§åˆ«åè§é—®é¢˜ã€‚</li>
<li>FairM2Sæ˜¯ä¸€ä¸ªåˆ©ç”¨è§†å¬æ•°æ®çš„å…¬å¹³æ„è¯†å…ƒå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>FairM2Såœ¨å…ƒè®­ç»ƒå’Œé€‚åº”é˜¶æ®µéƒ½æ•´åˆäº†å¹³ç­‰æœºä¼šçº¦æŸï¼Œä»¥å‡è½»åè§ã€‚</li>
<li>FairM2Sè¾¾åˆ°äº†78.1%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å°†å¹³ç­‰æœºä¼šé™è‡³0.06ï¼Œæ˜¾ç¤ºå‡ºå…¶å…¬å¹³æ€§æ˜¾è‘—æé«˜ã€‚</li>
<li>å‘å¸ƒäº†SAVSDæ•°æ®é›†ï¼ŒåŒ…å«æ€§åˆ«æ³¨é‡Šï¼Œç”¨äºæ”¯æŒä½èµ„æºã€ç°å®ç¯å¢ƒä¸‹çš„å…¬å¹³æ€§ç ”ç©¶å’Œæ•°æ®è®­ç»ƒã€‚</li>
<li>FairM2Så¯¹äºå®ç°å¿ƒç†å¥åº·AIä¸­çš„å…¬å¹³å’Œå¯æ‰©å±•çš„å°‘é‡å‹åŠ›æ£€æµ‹æ˜¯ä¸€ç§å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ec14e903eebb1db2e1eb51ac802b30f" align="middle">
<img src="https://picx.zhimg.com/v2-880245b64d2b6a8645b848fc7fc8622e" align="middle">
<img src="https://picx.zhimg.com/v2-31bafba33ab8a0d56e0601db0c2721fe" align="middle">
<img src="https://picx.zhimg.com/v2-11e045d9059fb05d113f9206da276661" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Spatio-Temporal-Data-Enhanced-Vision-Language-Model-for-Traffic-Scene-Understanding"><a href="#Spatio-Temporal-Data-Enhanced-Vision-Language-Model-for-Traffic-Scene-Understanding" class="headerlink" title="Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding"></a>Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding</h2><p><strong>Authors:Jingtian Ma, Jingyuan Wang, Wayne Xin Zhao, Guoping Liu, Xiang Wen</strong></p>
<p>Nowadays, navigation and ride-sharing apps have collected numerous images with spatio-temporal data. A core technology for analyzing such images, associated with spatiotemporal information, is Traffic Scene Understanding (TSU), which aims to provide a comprehensive description of the traffic scene. Unlike traditional spatio-temporal data analysis tasks, the dependence on both spatio-temporal and visual-textual data introduces distinct challenges to TSU task. However, recent research often treats TSU as a common image understanding task, ignoring the spatio-temporal information and overlooking the interrelations between different aspects of the traffic scene. To address these issues, we propose a novel SpatioTemporal Enhanced Model based on CILP (ST-CLIP) for TSU. Our model uses the classic vision-language model, CLIP, as the backbone, and designs a Spatio-temporal Context Aware Multiaspect Prompt (SCAMP) learning method to incorporate spatiotemporal information into TSU. The prompt learning method consists of two components: A dynamic spatio-temporal context representation module that extracts representation vectors of spatio-temporal data for each traffic scene image, and a bi-level ST-aware multi-aspect prompt learning module that integrates the ST-context representation vectors into word embeddings of prompts for the CLIP model. The second module also extracts low-level visual features and image-wise high-level semantic features to exploit interactive relations among different aspects of traffic scenes. To the best of our knowledge, this is the first attempt to integrate spatio-temporal information into visionlanguage models to facilitate TSU task. Experiments on two realworld datasets demonstrate superior performance in the complex scene understanding scenarios with a few-shot learning strategy.</p>
<blockquote>
<p>å¦‚ä»Šï¼Œå¯¼èˆªå’Œæ‹¼è½¦åº”ç”¨ç¨‹åºå·²ç»æ”¶é›†äº†å¤§é‡å¸¦æœ‰æ—¶ç©ºæ•°æ®çš„å›¾åƒã€‚åˆ†æè¿™äº›ä¸æ—¶ç©ºä¿¡æ¯ç›¸å…³çš„å›¾åƒçš„æ ¸å¿ƒæŠ€æœ¯æ˜¯äº¤é€šåœºæ™¯ç†è§£ï¼ˆTSUï¼‰ï¼Œæ—¨åœ¨æä¾›å¯¹äº¤é€šåœºæ™¯çš„ç»¼åˆæè¿°ã€‚ä¸ä¼ ç»Ÿçš„æ—¶ç©ºæ•°æ®åˆ†æä»»åŠ¡ä¸åŒï¼ŒTSUä»»åŠ¡æ—¢ä¾èµ–äºæ—¶ç©ºæ•°æ®ï¼Œåˆä¾èµ–äºè§†è§‰æ–‡æœ¬æ•°æ®ï¼Œè¿™å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å¾€å¾€å°†TSUè§†ä¸ºå¸¸è§çš„å›¾åƒç†è§£ä»»åŠ¡ï¼Œå¿½ç•¥äº†æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶å¿½è§†äº†äº¤é€šåœºæ™¯ä¸åŒæ–¹é¢çš„ç›¸äº’å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºCILPçš„æ–°å‹æ—¶ç©ºå¢å¼ºæ¨¡å‹ï¼ˆST-CLIPï¼‰ç”¨äºTSUã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»¥ç»å…¸çš„è§†è§‰è¯­è¨€æ¨¡å‹CLIPä½œä¸ºéª¨å¹²ç½‘ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ—¶ç©ºä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ–¹é¢æç¤ºï¼ˆSCAMPï¼‰å­¦ä¹ æ–¹æ³•ï¼Œå°†æ—¶ç©ºä¿¡æ¯èå…¥TSUä¸­ã€‚æç¤ºå­¦ä¹ æ–¹æ³•ç”±ä¸¤ä¸ªç»„ä»¶ç»„æˆï¼šä¸€ä¸ªåŠ¨æ€æ—¶ç©ºä¸Šä¸‹æ–‡è¡¨ç¤ºæ¨¡å—ï¼Œç”¨äºæå–æ¯ä¸ªäº¤é€šåœºæ™¯å›¾åƒçš„æ—¶ç©ºæ•°æ®è¡¨ç¤ºå‘é‡ï¼›ä¸€ä¸ªä¸¤çº§STæ„ŸçŸ¥å¤šæ–¹é¢æç¤ºå­¦ä¹ æ¨¡å—ï¼Œå°†STä¸Šä¸‹æ–‡è¡¨ç¤ºå‘é‡é›†æˆåˆ°CLIPæ¨¡å‹çš„æç¤ºè¯åµŒå…¥ä¸­ã€‚ç¬¬äºŒä¸ªæ¨¡å—è¿˜æå–ä½çº§è§†è§‰ç‰¹å¾å’Œå›¾åƒçº§é«˜çº§è¯­ä¹‰ç‰¹å¾ï¼Œä»¥åˆ©ç”¨äº¤é€šåœºæ™¯ä¸åŒæ–¹é¢çš„äº¤äº’å…³ç³»ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•å°†æ—¶ç©ºä¿¡æ¯æ•´åˆåˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥ä¿ƒè¿›TSUä»»åŠ¡ã€‚åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å¤æ‚åœºæ™¯ç†è§£åœºæ™¯ä¸­é‡‡ç”¨å°æ ·æœ¬å­¦ä¹ ç­–ç•¥æ—¶ï¼Œå…¶æ€§èƒ½ä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08978v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºæ—¶ç©ºå¢å¼ºæ¨¡å‹çš„äº¤é€šåœºæ™¯ç†è§£ç ”ç©¶æ—¨åœ¨é€šè¿‡ç»“åˆæ—¶ç©ºä¿¡æ¯å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆCLIPï¼‰æ¥ç†è§£äº¤é€šåœºæ™¯ã€‚è¯¥ç ”ç©¶é€šè¿‡è®¾è®¡æ—¶ç©ºä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ–¹é¢æç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆSCAMPï¼‰ï¼Œå°†æ—¶ç©ºæ•°æ®èå…¥äº¤é€šåœºæ™¯ç†è§£ï¼ˆTSUï¼‰ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å¤æ‚åœºæ™¯ç†è§£åœºæ™¯ä¸­ï¼Œé‡‡ç”¨å°‘é‡å­¦ä¹ ç­–ç•¥çš„è¯¥æ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº¤é€šåœºæ™¯ç†è§£ï¼ˆTSUï¼‰æ˜¯åˆ†æå¸¦æœ‰æ—¶ç©ºä¿¡æ¯çš„å›¾åƒçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ—¨åœ¨å…¨é¢æè¿°äº¤é€šåœºæ™¯ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¸¸å°†TSUè§†ä¸ºæ™®é€šå›¾åƒç†è§£ä»»åŠ¡ï¼Œå¿½ç•¥äº†æ—¶ç©ºä¿¡æ¯ä»¥åŠäº¤é€šåœºæ™¯ä¸­ä¸åŒæ–¹é¢çš„ç›¸äº’å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„æ—¶ç©ºå¢å¼ºæ¨¡å‹ï¼ˆST-CLIPï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ST-CLIPæ¨¡å‹ä½¿ç”¨ç»å…¸çš„è¯­è¨€æ¨¡å‹CLIPä½œä¸ºéª¨å¹²ï¼Œå¹¶è®¾è®¡äº†SCAMPå­¦ä¹ æ–¹æ³•æ¥èå…¥æ—¶ç©ºä¿¡æ¯ã€‚</li>
<li>SCAMPå­¦ä¹ æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªæ¨¡å—ï¼šåŠ¨æ€æ—¶ç©ºä¸Šä¸‹æ–‡è¡¨ç¤ºæ¨¡å—å’ŒåŒçº§STæ„ŸçŸ¥å¤šæ–¹é¢æç¤ºå­¦ä¹ æ¨¡å—ã€‚</li>
<li>è¯¥æ–¹æ³•é¦–æ¬¡å°è¯•å°†æ—¶ç©ºä¿¡æ¯èå…¥è§†è§‰è¯­è¨€æ¨¡å‹ä»¥ä¿ƒè¿›TSUä»»åŠ¡ã€‚</li>
<li>åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å°‘é‡å­¦ä¹ ç­–ç•¥çš„å¤æ‚åœºæ™¯ç†è§£ä¸­ï¼Œè¯¥æ–¹æ³•å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-604dd447dc7dbfdde53e78b77f98e2a5" align="middle">
<img src="https://picx.zhimg.com/v2-23cb1c7413dccc4bbef63a2ba05d2bb6" align="middle">
<img src="https://picx.zhimg.com/v2-369e8156f83f4f110dec8dd14f081d39" align="middle">
<img src="https://picx.zhimg.com/v2-f0121491a168e21fe0fb73ee03f23454" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-20b55b2a0407f46d189ead06a85443aa" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e11e35bef5f1a07f1fd5fd285a73917a" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  HI-TransPA Hearing Impairments Translation Personal Assistant
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
