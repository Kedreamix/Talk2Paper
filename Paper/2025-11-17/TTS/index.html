<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  MOSPA Human Motion Generation Driven by Spatial Audio">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-734b1fbcf79a02b66dac1912d553c271')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    31 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-17-æ›´æ–°"><a href="#2025-11-17-æ›´æ–°" class="headerlink" title="2025-11-17 æ›´æ–°"></a>2025-11-17 æ›´æ–°</h1><h2 id="MOSPA-Human-Motion-Generation-Driven-by-Spatial-Audio"><a href="#MOSPA-Human-Motion-Generation-Driven-by-Spatial-Audio" class="headerlink" title="MOSPA: Human Motion Generation Driven by Spatial Audio"></a>MOSPA: Human Motion Generation Driven by Spatial Audio</h2><p><strong>Authors:Shuyang Xu, Zhiyang Dou, Mingyi Shi, Liang Pan, Leo Ho, Jingbo Wang, Yuan Liu, Cheng Lin, Yuexin Ma, Wenping Wang, Taku Komura</strong></p>
<p>Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA can generate diverse, realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our code and model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation">https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation</a></p>
<blockquote>
<p>å®ç°è™šæ‹Ÿäººç‰©å¯¹å¤šç§å¬è§‰åˆºæ¿€çš„åŠ¨æ€å’ŒçœŸå®ååº”ä»æ˜¯è§’è‰²åŠ¨ç”»ä¸­çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œè¿™éœ€è¦æ„ŸçŸ¥å»ºæ¨¡å’Œè¿åŠ¨åˆæˆçš„é›†æˆã€‚å°½ç®¡å…¶æ„ä¹‰é‡å¤§ï¼Œä½†è¿™é¡¹ä»»åŠ¡ä»è¢«å¤§é‡å¿½è§†ã€‚ä»¥å‰çš„å¤§éƒ¨åˆ†å·¥ä½œä¸»è¦é›†ä¸­åœ¨å°†è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ç­‰æ˜ å°„æ¨¡å¼è½¬åŒ–ä¸ºäººç±»è¿åŠ¨ã€‚ç„¶è€Œï¼Œè¿„ä»Šä¸ºæ­¢ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸å¿½è§†äº†ç©ºé—´éŸ³é¢‘ä¿¡å·æ‰€ç¼–ç çš„ç©ºé—´ç‰¹å¾å¯¹äººç±»è¿åŠ¨çš„å½±å“ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½å¹¶å®ç°é«˜è´¨é‡çš„ç©ºé—´éŸ³é¢‘é©±åŠ¨äººç±»è¿åŠ¨å»ºæ¨¡ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¨å‡ºäº†å…¨é¢çš„ç©ºé—´éŸ³é¢‘é©±åŠ¨äººç±»è¿åŠ¨ï¼ˆSAMï¼‰æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¤šæ ·ä¸”é«˜è´¨é‡çš„ç©ºé—´éŸ³é¢‘å’Œè¿åŠ¨æ•°æ®ã€‚ä¸ºäº†åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºç©ºé—´éŸ³é¢‘é©±åŠ¨çš„äººç±»è¿åŠ¨ç”Ÿæˆï¼Œç§°ä¸ºMOSPAã€‚å®ƒé€šè¿‡ä¸€ä¸ªæœ‰æ•ˆçš„èåˆæœºåˆ¶å¿ å®æ•æ‰èº«ä½“è¿åŠ¨å’Œç©ºé—´éŸ³é¢‘ä¹‹é—´çš„å…³ç³»ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒMOSPAå¯ä»¥æ ¹æ®ä¸åŒçš„ç©ºé—´éŸ³é¢‘è¾“å…¥ç”Ÿæˆå¤šæ ·ä¸”é€¼çœŸçš„è¿åŠ¨ã€‚æˆ‘ä»¬å¯¹æå‡ºçš„æ•°æ®é›†è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥å¹¶è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation">https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11949v2">PDF</a> NeurIPS 2025 (Spotlight)</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€é¡¹å…³äºç©ºé—´éŸ³é¢‘é©±åŠ¨è™šæ‹Ÿäººç±»è¿åŠ¨çš„ç ”ç©¶ã€‚ç”±äºç¼ºå°‘ç›¸åº”çš„æ•°æ®é›†å’Œç ”ç©¶æ¨¡å‹ï¼Œæ­¤å‰å¯¹æ­¤ä»»åŠ¡çš„ç ”ç©¶å°šå¤„äºåˆæ­¥é˜¶æ®µã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†é¦–ä¸ªç©ºé—´éŸ³é¢‘é©±åŠ¨äººä½“è¿åŠ¨ï¼ˆSAMï¼‰æ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡æ•°æ®å»ºæ¨¡æé«˜è™šæ‹Ÿäººç‰©å¯¹ç©ºé—´éŸ³é¢‘çš„ååº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜å¼€å‘äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¡†æ¶MOSPAï¼Œèƒ½å¤Ÿæ ¹æ®ç©ºé—´éŸ³é¢‘ç”Ÿæˆé€¼çœŸçš„è™šæ‹Ÿäººä½“è¿åŠ¨ã€‚è¯¥ç ”ç©¶å¯¹è™šæ‹Ÿè§’è‰²çš„åŠ¨ä½œè¡¨ç°å…·æœ‰é‡è¦çš„æå‡ä½œç”¨ã€‚æ•°æ®é›†å’Œæ¨¡å‹å·²ç»å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€æ„å»ºé¦–ä¸ªç©ºé—´éŸ³é¢‘é©±åŠ¨äººä½“è¿åŠ¨ï¼ˆSAMï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·ä¸”é«˜è´¨é‡çš„ç©ºé—´éŸ³é¢‘å’Œè¿åŠ¨æ•°æ®ã€‚<br>äºŒã€æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¡†æ¶MOSPAï¼Œèƒ½å¤Ÿæ ¹æ®ç©ºé—´éŸ³é¢‘ç”Ÿæˆè™šæ‹Ÿäººä½“è¿åŠ¨ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰ç©ºé—´éŸ³é¢‘ä¸äººä½“è¿åŠ¨ä¹‹é—´çš„å…³ç³»ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9e34dc2f030fae5a59784ade6f0f57d" align="middle">
<img src="https://picx.zhimg.com/v2-35a714aee61052b3a38e9ea8f6ade198" align="middle">
<img src="https://picx.zhimg.com/v2-02a69dbc05a3e51b12c6b1fb00637e59" align="middle">
<img src="https://picx.zhimg.com/v2-cf4a9995c947f2746c494e29be8b27ad" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DARAS-Dynamic-Audio-Room-Acoustic-Synthesis-for-Blind-Room-Impulse-Response-Estimation"><a href="#DARAS-Dynamic-Audio-Room-Acoustic-Synthesis-for-Blind-Room-Impulse-Response-Estimation" class="headerlink" title="DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse Response Estimation"></a>DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse Response Estimation</h2><p><strong>Authors:Chunxi Wang, Maoshen Jia, Wenyu Jin</strong></p>
<p>Room Impulse Responses (RIRs) accurately characterize acoustic properties of indoor environments and play a crucial role in applications such as speech enhancement, speech recognition, and audio rendering in augmented reality (AR) and virtual reality (VR). Existing blind estimation methods struggle to achieve practical accuracy. To overcome this challenge, we propose the dynamic audio-room acoustic synthesis (DARAS) model, a novel deep learning framework that is explicitly designed for blind RIR estimation from monaural reverberant speech signals. First, a dedicated deep audio encoder effectively extracts relevant nonlinear latent space features. Second, the Mamba-based self-supervised blind room parameter estimation (MASS-BRPE) module, utilizing the efficient Mamba state space model (SSM), accurately estimates key room acoustic parameters and features. Third, the system incorporates a hybrid-path cross-attention feature fusion module, enhancing deep integration between audio and room acoustic features. Finally, our proposed dynamic acoustic tuning (DAT) decoder adaptively segments early reflections and late reverberation to improve the realism of synthesized RIRs. Experimental results, including a MUSHRA-based subjective listening study, demonstrate that DARAS substantially outperforms existing baseline models, providing a robust and effective solution for practical blind RIR estimation in real-world acoustic environments.</p>
<blockquote>
<p>å®¤å†…ç¯å¢ƒçš„å£°å­¦ç‰¹æ€§å¯ä»¥é€šè¿‡æˆ¿é—´è„‰å†²å“åº”ï¼ˆRIRsï¼‰æ¥ç²¾å‡†æè¿°ï¼Œå…¶åœ¨è¯­éŸ³å¢å¼ºã€è¯­éŸ³è¯†åˆ«ã€å¢å¼ºç°å®ï¼ˆARï¼‰å’Œè™šæ‹Ÿç°å®ï¼ˆVRï¼‰çš„éŸ³é¢‘æ¸²æŸ“ç­‰åº”ç”¨ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ã€‚ç°æœ‰çš„ç›²ä¼°è®¡æ–¹æ³•å¾ˆéš¾è¾¾åˆ°å®ç”¨æ‰€éœ€çš„å‡†ç¡®åº¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€éŸ³é¢‘æˆ¿é—´å£°å­¦åˆæˆï¼ˆDARASï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºä»å•å£°é“æ··å“è¯­éŸ³ä¿¡å·ä¸­ç›²ä¼°è®¡RIRè€Œè®¾è®¡çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚é¦–å…ˆï¼Œä¸“ç”¨çš„æ·±åº¦éŸ³é¢‘ç¼–ç å™¨æœ‰æ•ˆåœ°æå–äº†ç›¸å…³çš„éçº¿æ€§æ½œåœ¨ç©ºé—´ç‰¹å¾ã€‚å…¶æ¬¡ï¼ŒåŸºäºMambaçš„è‡ªæˆ‘ç›‘ç£ç›²æˆ¿é—´å‚æ•°ä¼°è®¡ï¼ˆMASS-BRPEï¼‰æ¨¡å—ï¼Œåˆ©ç”¨é«˜æ•ˆçš„MambaçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œå‡†ç¡®ä¼°è®¡äº†å…³é”®çš„æˆ¿é—´å£°å­¦å‚æ•°å’Œç‰¹å¾ã€‚ç¬¬ä¸‰ï¼Œç³»ç»Ÿèå…¥äº†ä¸€ä¸ªæ··åˆè·¯å¾„äº¤å‰æ³¨æ„ç‰¹å¾èåˆæ¨¡å—ï¼Œå¢å¼ºäº†éŸ³é¢‘å’Œæˆ¿é—´å£°å­¦ç‰¹å¾ä¹‹é—´çš„æ·±åº¦é›†æˆã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºçš„åŠ¨æ€å£°å­¦è°ƒæ•´ï¼ˆDATï¼‰è§£ç å™¨è‡ªé€‚åº”åœ°åˆ†å‰²äº†æ—©æœŸåå°„å’ŒåæœŸå›å£°ï¼Œæé«˜äº†åˆæˆRIRçš„çœŸå®æ„Ÿã€‚åŒ…æ‹¬åŸºäºMUSHRAçš„ä¸»è§‚å¬è§‰ç ”ç©¶åœ¨å†…çš„å®éªŒç»“æœè¯æ˜ï¼ŒDARASåœ¨çœŸå®ä¸–ç•Œçš„å£°å­¦ç¯å¢ƒä¸­ï¼Œç›¸è¾ƒäºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œå¤§å¹…æé«˜äº†ç›²ä¼°è®¡RIRçš„å®ç”¨æ€§å’Œå‡†ç¡®æ€§ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†ç¨³å¥æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08135v2">PDF</a> 14 pages, 9 figures, accepted for publication in IEEE&#x2F;ACM Transactions on Audio, Speech, and Language Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŠ¨æ€éŸ³é¢‘æˆ¿é—´å£°å­¦åˆæˆï¼ˆDARASï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºä»å•å£°é“æ··å“è¯­éŸ³ä¿¡å·ä¸­ç›²ä¼°è®¡æˆ¿é—´è„‰å†²å“åº”ï¼ˆRIRï¼‰è€Œè®¾è®¡çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¨¡å‹é€šè¿‡æ·±åº¦éŸ³é¢‘ç¼–ç å™¨æå–éçº¿æ€§æ½œåœ¨ç©ºé—´ç‰¹å¾ï¼Œåˆ©ç”¨åŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰è¿›è¡Œæˆ¿é—´å£°å­¦å‚æ•°çš„è‡ªæˆ‘ç›‘ç£ç›²ä¼°è®¡ï¼Œå¹¶ç»“åˆæ··åˆè·¯å¾„äº¤å‰æ³¨æ„åŠ›ç‰¹å¾èåˆæ¨¡å—ï¼Œè‡ªé€‚åº”åˆ†å‰²æ—©æœŸåå°„å’ŒåæœŸæ··å“ï¼Œæé«˜äº†åˆæˆRIRçš„çœŸå®æ„Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDARASåœ¨çœŸå®ä¸–ç•Œå£°å­¦ç¯å¢ƒä¸­å®ç°äº†å¯¹åŸºçº¿æ¨¡å‹çš„æ˜¾è‘—è¶…è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æˆ¿é—´è„‰å†²å“åº”ï¼ˆRIRï¼‰æ˜¯å®¤å†…å£°å­¦ç‰¹æ€§çš„å‡†ç¡®è¡¨å¾ï¼Œåœ¨è¯­éŸ³å¢å¼ºã€è¯­éŸ³è¯†åˆ«ã€å¢å¼ºç°å®ï¼ˆARï¼‰å’Œè™šæ‹Ÿç°å®ï¼ˆVRï¼‰ä¸­çš„éŸ³é¢‘æ¸²æŸ“ç­‰åº”ç”¨ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰ç›²ä¼°è®¡æ–¹æ³•éš¾ä»¥è¾¾åˆ°å®ç”¨ç²¾åº¦ï¼ŒDARASæ¨¡å‹è¢«æå‡ºä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>DARASåŒ…æ‹¬ä¸€ä¸ªæ·±åº¦éŸ³é¢‘ç¼–ç å™¨ç”¨äºæå–éçº¿æ€§æ½œåœ¨ç©ºé—´ç‰¹å¾ï¼Œä»¥åŠåŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹è¿›è¡Œæˆ¿é—´å£°å­¦å‚æ•°çš„è‡ªæˆ‘ç›‘ç£ç›²ä¼°è®¡ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆäº†æ··åˆè·¯å¾„äº¤å‰æ³¨æ„åŠ›ç‰¹å¾èåˆæ¨¡å—ï¼Œå¢å¼ºäº†éŸ³é¢‘å’Œæˆ¿é—´å£°å­¦ç‰¹å¾ä¹‹é—´çš„æ·±åº¦èåˆã€‚</li>
<li>åŠ¨æ€å£°å­¦è°ƒéŸ³ï¼ˆDATï¼‰è§£ç å™¨è‡ªé€‚åº”åˆ†å‰²æ—©æœŸåå°„å’ŒåæœŸæ··å“ï¼Œæé«˜äº†åˆæˆRIRçš„çœŸå®æ„Ÿã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜DARASæ¨¡å‹åœ¨ç›²RIRä¼°è®¡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da4b9c22a084740008cb03497e4f05a1" align="middle">
<img src="https://picx.zhimg.com/v2-21e441330753ef7c5e0d026b0a5c7a68" align="middle">
<img src="https://picx.zhimg.com/v2-3d00637c650262377999b5fa4e0cb9c2" align="middle">
<img src="https://picx.zhimg.com/v2-840f62d1c6c79a1551f62f76285257e3" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DeepOmni-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE"><a href="#DeepOmni-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE" class="headerlink" title="DeepOmni: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE"></a>DeepOmni: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE</h2><p><strong>Authors:Hang Shao, Heting Gao, Yunhang Shen, Jiawei Chen, Zuwei Long, Dong Yang, Ke Li, Xing Sun</strong></p>
<p>Native multimodal large language models (MLLMs) restructure a single large language model (LLM) into a spoken language model (SLM) capable of both speech and text generation. Compared to modular and aligned MLLMs, native MLLMs preserve richer paralinguistic features such as emotion and prosody, and generate speech responses directly within the backbone LLM rather than using a separate speech decoder. This integration also results in lower response latency and smoother interaction. However, native MLLMs suffer from catastrophic forgetting and performance degradation because the available paired speech-text data is insufficient to support the pretraining of MLLMs compared to the vast amount of text data required to pretrain text LLMs. To address this issue, we propose DeepTalk, a framework for adaptive modality expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk first adaptively distinguishes modality experts according to their modality load within the LLM. Each modality expert then undergoes specialized single-modality training, followed by joint multimodal collaborative training. As a result, DeepTalk incurs only a 5.5% performance drop compared to the original LLM, which is significantly lower than the average performance drop of over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within 0.5 seconds, ensuring a seamless and intelligent speech interaction experience. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk">https://github.com/talkking/DeepTalk</a>.</p>
<blockquote>
<p>åŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å°†å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡æ„ä¸ºèƒ½å¤Ÿç”Ÿæˆè¯­éŸ³å’Œæ–‡æœ¬çš„å£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ã€‚ä¸æ¨¡å—åŒ–å’Œå¯¹é½çš„MLLMsç›¸æ¯”ï¼ŒåŸç”ŸMLLMsä¿ç•™äº†æ›´ä¸°å¯Œçš„å‰¯è¯­è¨€ç‰¹å¾ï¼Œå¦‚æƒ…æ„Ÿå’Œè¯­è°ƒï¼Œå¹¶åœ¨ä¸»å¹²LLMå†…ç›´æ¥ç”Ÿæˆè¯­éŸ³å“åº”ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å•ç‹¬çš„è¯­éŸ³è§£ç å™¨ã€‚è¿™ç§é›†æˆè¿˜å¸¦æ¥äº†æ›´ä½çš„å“åº”å»¶è¿Ÿå’Œæ›´æµç•…çš„äº’åŠ¨ã€‚ç„¶è€Œï¼Œç”±äºå¯ç”¨çš„é…å¯¹è¯­éŸ³-æ–‡æœ¬æ•°æ®ä¸é¢„è®­ç»ƒæ–‡æœ¬LLMæ‰€éœ€çš„å¤§é‡æ–‡æœ¬æ•°æ®ç›¸æ¯”ä¸è¶³ï¼ŒåŸç”ŸMLLMsé­å—ç¾éš¾æ€§é—å¿˜å’Œæ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepTalkï¼Œä¸€ä¸ªåŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„è‡ªé€‚åº”æ¨¡æ€ä¸“å®¶å­¦ä¹ æ¡†æ¶ã€‚DeepTalké¦–å…ˆæ ¹æ®LLMå†…çš„æ¨¡æ€è´Ÿè½½è‡ªé€‚åº”åœ°åŒºåˆ†æ¨¡æ€ä¸“å®¶ã€‚ç„¶åï¼Œæ¯ä¸ªæ¨¡æ€ä¸“å®¶æ¥å—ä¸“é—¨çš„å•æ¨¡æ€è®­ç»ƒï¼Œæ¥ç€è¿›è¡Œè”åˆå¤šæ¨¡æ€åä½œè®­ç»ƒã€‚å› æ­¤ï¼Œä¸åŸå§‹LLMç›¸æ¯”ï¼ŒDeepTalkä»…äº§ç”Ÿ5.5%çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™è¿œä½äºåŸç”ŸMLLMsï¼ˆå¦‚GLM-4-Voiceï¼‰é€šå¸¸å‡ºç°çš„è¶…è¿‡20%çš„å¹³å‡æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸æ¨¡å—åŒ–MLLMsç›¸å½“ã€‚åŒæ—¶ï¼Œç«¯åˆ°ç«¯å¯¹è¯å»¶è¿Ÿä¿æŒåœ¨0.5ç§’å†…ï¼Œç¡®ä¿æµç•…æ™ºèƒ½çš„è¯­éŸ³äº¤äº’ä½“éªŒã€‚ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk%E3%80%82">https://github.com/talkking/DeepTalkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21864v3">PDF</a> Under Review</p>
<p><strong>æ‘˜è¦</strong><br>    åŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å°†å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡ç»„ä¸ºèƒ½å¤Ÿç”Ÿæˆè¯­éŸ³å’Œæ–‡æœ¬çš„è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ã€‚ç›¸æ¯”æ¨¡å—åŒ–å’Œå¯¹é½çš„MLLMsï¼ŒåŸç”ŸMLLMsä¿ç•™äº†æ›´ä¸°å¯Œçš„å‰¯è¯­è¨€ç‰¹å¾ï¼Œå¦‚æƒ…æ„Ÿå’Œè¯­è°ƒï¼Œå¹¶åœ¨ä¸»å¹²LLMå†…éƒ¨ç›´æ¥ç”Ÿæˆè¯­éŸ³å“åº”ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å•ç‹¬çš„è¯­éŸ³è§£ç å™¨ã€‚ä½†åŸç”ŸMLLMsé¢ä¸´ç¾éš¾æ€§é—å¿˜å’Œæ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œå› ä¸ºå¯ç”¨çš„é…å¯¹è¯­éŸ³-æ–‡æœ¬æ•°æ®ä¸è¶³ä»¥æ”¯æŒMLLMsçš„é¢„è®­ç»ƒï¼Œä¸éœ€è¦å¤§é‡æ–‡æœ¬æ•°æ®æ¥é¢„è®­ç»ƒæ–‡æœ¬LLMsçš„æƒ…å†µç›¸æ¯”ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„DeepTalkæ¡†æ¶ï¼Œè¿›è¡Œè‡ªé€‚åº”æ¨¡æ€ä¸“å®¶å­¦ä¹ ã€‚DeepTalké¦–å…ˆæ ¹æ®LLMå†…çš„æ¨¡æ€è´Ÿè½½è‡ªé€‚åº”åœ°è¯†åˆ«æ¨¡æ€ä¸“å®¶ã€‚ç„¶åæ¯ä¸ªæ¨¡æ€ä¸“å®¶æ¥å—ä¸“é—¨çš„å•æ¨¡æ€è®­ç»ƒï¼Œæ¥ç€è¿›è¡Œè”åˆå¤šæ¨¡æ€åä½œè®­ç»ƒã€‚ç»“æœï¼ŒDeepTalkä¸åŸå§‹LLMç›¸æ¯”ä»…æ€§èƒ½ä¸‹é™5.5%ï¼Œè¿œä½äºåŸç”ŸMLLMsé€šå¸¸å‡ºç°çš„è¶…è¿‡20%çš„å¹³å‡æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸æ¨¡å—åŒ–MLLMsç›¸å½“ã€‚åŒæ—¶ï¼Œç«¯åˆ°ç«¯å¯¹è¯å»¶è¿Ÿä¿æŒåœ¨0.5ç§’å†…ï¼Œç¡®ä¿æµç•…æ™ºèƒ½çš„è¯­éŸ³äº¤äº’ä½“éªŒã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk%E3%80%82">https://github.com/talkking/DeepTalkã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸç”ŸMLLMsèƒ½å¤Ÿç»“åˆè¯­éŸ³å’Œæ–‡æœ¬ç”Ÿæˆï¼Œä¿ç•™ä¸°å¯Œçš„å‰¯è¯­è¨€ç‰¹å¾ã€‚</li>
<li>ä¸æ¨¡å—åŒ–å’Œå¯¹é½çš„MLLMsç›¸æ¯”ï¼ŒåŸç”ŸMLLMsåœ¨ä¸»å¹²LLMå†…ç›´æ¥ç”Ÿæˆè¯­éŸ³å“åº”ã€‚</li>
<li>åŸç”ŸMLLMsé¢ä¸´æ•°æ®ä¸è¶³å¯¼è‡´çš„æ€§èƒ½é—®é¢˜å’Œç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>DeepTalkæ¡†æ¶é€šè¿‡è‡ªé€‚åº”æ¨¡æ€ä¸“å®¶å­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>DeepTalké€šè¿‡æ¨¡æ€è´Ÿè½½è¯†åˆ«æ¨¡æ€ä¸“å®¶ï¼Œè¿›è¡Œå•æ¨¡æ€å’Œè”åˆå¤šæ¨¡æ€è®­ç»ƒã€‚</li>
<li>DeepTalkä¸åŸå§‹LLMç›¸æ¯”æ€§èƒ½ä¸‹é™è¾ƒå°ï¼Œä¼˜äºå…¶ä»–åŸç”ŸMLLMsï¼Œä¸æ¨¡å—åŒ–MLLMsç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f40abedd2ad73f088c4fd0518dddd9b" align="middle">
<img src="https://picx.zhimg.com/v2-fe8595fb480aa96b73e1a1f9e399111b" align="middle">
<img src="https://picx.zhimg.com/v2-6fecc5d706d591e4f84b7786fed7dcc3" align="middle">
<img src="https://picx.zhimg.com/v2-f18fe3941dc0f81d211bb977c620b94f" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TTSOps-A-Closed-Loop-Corpus-Optimization-Framework-for-Training-Multi-Speaker-TTS-Models-from-Dark-Data"><a href="#TTSOps-A-Closed-Loop-Corpus-Optimization-Framework-for-Training-Multi-Speaker-TTS-Models-from-Dark-Data" class="headerlink" title="TTSOps: A Closed-Loop Corpus Optimization Framework for Training Multi-Speaker TTS Models from Dark Data"></a>TTSOps: A Closed-Loop Corpus Optimization Framework for Training Multi-Speaker TTS Models from Dark Data</h2><p><strong>Authors:Kentaro Seki, Shinnosuke Takamichi, Takaaki Saeki, Hiroshi Saruwatari</strong></p>
<p>This paper presents TTSOps, a fully automated closed-loop framework for constructing multi-speaker text-to-speech (TTS) systems from noisy, uncurated web-scale speech data, often referred to as &#96;&#96;dark data,â€™â€™ such as online videos. Conventional TTS training pipelines require well-curated corpora with high acoustic quality and accurate text-speech alignment, which severely limits scalability, speaker diversity, and real-world applicability. While recent studies have proposed acoustic-quality-based data selection techniques, they often overlook two critical aspects: (1) the inherent robustness of modern TTS models to noise, and (2) the potential contribution of perceptually low-quality yet informative samples. To address these issues, TTSOps introduces a data-centric training pipeline that integrates three core components: (1) automated data collection from dark data sources, (2) utterance-level dynamic selection of data cleansing methods based on training data quality, and (3) evaluation-in-the-loop data selection using automatically predicted mean opinion scores (MOS) to estimate each utteranceâ€™s impact on model performance. Furthermore, TTSOps jointly optimizes the corpus and the TTS model in a closed-loop framework by dynamically adapting both data selection and data cleansing processes to the characteristics of the target TTS model. Extensive experiments on Japanese YouTube data demonstrate that TTSOps outperforms conventional acoustic-quality-based baselines in both the naturalness and speaker diversity of synthesized speech.</p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TTSOpsï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„é—­ç¯æ¡†æ¶ï¼Œç”¨äºä»å˜ˆæ‚çš„ã€æœªæ•´ç†çš„ç½‘é¡µè§„æ¨¡è¯­éŸ³æ•°æ®ï¼ˆé€šå¸¸è¢«ç§°ä¸ºâ€œæš—æ•°æ®â€ï¼Œå¦‚åœ¨çº¿è§†é¢‘ï¼‰æ„å»ºå¤šè¯´è¯äººæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚ä¼ ç»Ÿçš„TTSè®­ç»ƒç®¡é“éœ€è¦é«˜è´¨é‡ã€è¯­éŸ³æ–‡æœ¬å¯¹é½ç²¾ç¡®çš„è¯­æ–™åº“ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å¯æ‰©å±•æ€§ã€è¯´è¯äººå¤šæ ·æ€§å’Œç°å®ä¸–ç•Œçš„é€‚ç”¨æ€§ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶å·²ç»æå‡ºäº†åŸºäºéŸ³è´¨çš„æ•°æ®é€‰æ‹©æŠ€æœ¯ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†ä¸¤ä¸ªå…³é”®æ–¹é¢ï¼šï¼ˆ1ï¼‰ç°ä»£TTSæ¨¡å‹å¯¹å™ªå£°çš„å›ºæœ‰é²æ£’æ€§ï¼›ï¼ˆ2ï¼‰æ„ŸçŸ¥è´¨é‡è¾ƒä½ä½†ä¿¡æ¯ä¸°å¯Œçš„æ ·æœ¬çš„æ½œåœ¨è´¡çŒ®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒTTSOpså¼•å…¥äº†ä¸€ä¸ªä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è®­ç»ƒç®¡é“ï¼Œè¯¥ç®¡é“é›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰ä»æš—æ•°æ®æºè‡ªåŠ¨æ”¶é›†æ•°æ®ï¼Œï¼ˆ2ï¼‰åŸºäºè®­ç»ƒæ•°æ®è´¨é‡çš„è¯­å¥çº§åŠ¨æ€é€‰æ‹©æ•°æ®æ¸…æ´—æ–¹æ³•ï¼Œï¼ˆ3ï¼‰ä½¿ç”¨è‡ªåŠ¨é¢„æµ‹çš„å¹³å‡æ„è§åˆ†æ•°ï¼ˆMOSï¼‰è¿›è¡Œé—­ç¯æ•°æ®é€‰æ‹©ï¼Œä»¥ä¼°è®¡æ¯ä¸ªè¯­å¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼ŒTTSOpsåœ¨ä¸€ä¸ªé—­ç¯æ¡†æ¶ä¸­è”åˆä¼˜åŒ–è¯­æ–™åº“å’ŒTTSæ¨¡å‹ï¼Œé€šè¿‡åŠ¨æ€é€‚åº”æ•°æ®é€‰æ‹©å’Œæ¸…æ´—è¿‡ç¨‹æ¥é€‚åº”ç›®æ ‡TTSæ¨¡å‹çš„ç‰¹å¾ã€‚åœ¨æ—¥è¯­YouTubeæ•°æ®ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTTSOpsåœ¨åˆæˆè¯­éŸ³çš„è‡ªç„¶æ€§å’Œè¯´è¯äººå¤šæ ·æ€§æ–¹é¢ä¼˜äºåŸºäºä¼ ç»ŸéŸ³è´¨è´¨é‡çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15614v3">PDF</a> Accepted to IEEE Transactions on Audio, Speech and Language Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TTSOpsï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„é—­ç¯æ¡†æ¶ï¼Œç”¨äºä»å˜ˆæ‚çš„ã€æœªæ•´ç†çš„ç½‘é¡µè§„æ¨¡è¯­éŸ³æ•°æ®ä¸­æ„å»ºå¤šè¯´è¯è€…æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚ä¸ä¼ ç»ŸTTSè®­ç»ƒç®¡é“éœ€è¦å¤§é‡é«˜è´¨é‡å’Œç²¾ç¡®æ–‡æœ¬-è¯­éŸ³å¯¹é½çš„è¯­æ–™åº“ä¸åŒï¼ŒTTSOpsè§£å†³äº†å¯æ‰©å±•æ€§ã€è¯´è¯è€…å¤šæ ·æ€§å’Œç°å®ä¸–ç•Œåº”ç”¨æ€§çš„é™åˆ¶ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†ã€åŸºäºè®­ç»ƒæ•°æ®è´¨é‡çš„è¯­å¥çº§åˆ«åŠ¨æ€é€‰æ‹©æ•°æ®æ¸…æ´—æ–¹æ³•ï¼Œä»¥åŠä½¿ç”¨è‡ªåŠ¨é¢„æµ‹çš„å¹³å‡æ„è§åˆ†æ•°ï¼ˆMOSï¼‰è¿›è¡Œé—­ç¯æ•°æ®é€‰æ‹©ã€‚TTSOpsåœ¨ç›®æ ‡TTSæ¨¡å‹çš„ç‰¹æ€§ä¸ŠåŠ¨æ€é€‚åº”æ•°æ®é€‰æ‹©å’Œæ¸…æ´—è¿‡ç¨‹ï¼Œå¯¹æ—¥è¯­YouTubeæ•°æ®çš„å®éªŒè¡¨æ˜ï¼ŒTTSOpsåœ¨è‡ªç„¶åº¦å’Œè¯´è¯è€…å¤šæ ·æ€§æ–¹é¢ä¼˜äºåŸºäºå£°å­¦è´¨é‡çš„ä¼ ç»ŸåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSOpsæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„é—­ç¯æ¡†æ¶ï¼Œç”¨äºæ„å»ºå¤šè¯´è¯è€…TTSç³»ç»Ÿã€‚</li>
<li>å®ƒè§£å†³äº†ä¼ ç»ŸTTSè®­ç»ƒç®¡é“åœ¨å¯æ‰©å±•æ€§ã€è¯´è¯è€…å¤šæ ·æ€§å’Œç°å®ä¸–ç•Œåº”ç”¨æ–¹é¢çš„é™åˆ¶ã€‚</li>
<li>TTSOpsä»æ‰€è°“çš„â€œæš—æ•°æ®â€ï¼ˆå¦‚åœ¨çº¿è§†é¢‘ï¼‰ä¸­æ„å»ºTTSç³»ç»Ÿã€‚</li>
<li>å®ƒæ•´åˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†ã€è¯­å¥çº§åŠ¨æ€æ•°æ®æ¸…æ´—å’Œé—­ç¯æ•°æ®é€‰æ‹©ã€‚</li>
<li>TTSOpsä½¿ç”¨è‡ªåŠ¨é¢„æµ‹çš„å¹³å‡æ„è§åˆ†æ•°ï¼ˆMOSï¼‰æ¥ä¼°è®¡æ¯ä¸ªè¯­å¥å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>å®ƒæ ¹æ®ç›®æ ‡TTSæ¨¡å‹çš„ç‰¹æ€§åŠ¨æ€é€‚åº”æ•°æ®é€‰æ‹©å’Œæ¸…æ´—è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3eb2a31e2f20c07090b4597d1ab705b9" align="middle">
<img src="https://picx.zhimg.com/v2-130919b427ab9d9a88cd3e29ab92a621" align="middle">
<img src="https://picx.zhimg.com/v2-4e0869a055f396be111550db567f9aaa" align="middle">
<img src="https://picx.zhimg.com/v2-a861c0af2571d470213c06b8ab91bbb6" align="middle">
<img src="https://picx.zhimg.com/v2-628ac539c4b5ef61ceb6d3e97dcf689b" align="middle">
<img src="https://picx.zhimg.com/v2-1a3665c4636f2d7c9f8a1d66ccb3abb1" align="middle">
<img src="https://picx.zhimg.com/v2-de4905b3f92c6fad6c146eeb4c38ec4e" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation"><a href="#UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation" class="headerlink" title="UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation"></a>UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation</h2><p><strong>Authors:Jinting Wang, Shan Yang, Chenxing Li, Dong Yu, Li Liu</strong></p>
<p>Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics.</p>
<blockquote>
<p>Cued Speechï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œæä¾›è§†è§‰è¯­éŸ³çº¿ç´¢ï¼Œæ”¯æŒå¬åŠ›å—æŸè€…ç²¾ç¡®æ„ŸçŸ¥è¯­éŸ³ã€‚CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰çš„ä»»åŠ¡æ—¨åœ¨å°†CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚ç›®å‰å¤§å¤šæ•°ç ”ç©¶é›†ä¸­åœ¨CSè¯†åˆ«ï¼ˆCSRï¼‰ä¸Šï¼Œå³å°†è§†é¢‘å†…å®¹è½¬å½•ä¸ºæ–‡æœ¬ã€‚å› æ­¤ï¼ŒCSV2Sçš„ä¸€ç§å¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯å°†CSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿé›†æˆåœ¨ä¸€èµ·ã€‚ç„¶è€Œï¼Œè¿™ç§ç®¡é“ä¾èµ–äºæ–‡æœ¬ä½œä¸ºä¸­é—´åª’ä»‹ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯¯å·®ä¼ æ’­å’Œè¯­éŸ³ä¸CSè§†é¢‘åŠ¨æ€ä¹‹é—´çš„æ—¶é—´ä¸åŒ¹é…ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›´æ¥ä»CSè§†é¢‘ç”ŸæˆéŸ³é¢‘è¯­éŸ³ï¼ˆç›´æ¥CSV2Sï¼‰å¸¸å¸¸å—åˆ°å›ºæœ‰çš„å¤šæ¨¡å¼å¤æ‚æ€§å’Œæœ‰é™çš„CSæ•°æ®å¯ç”¨æ€§çš„å›°æ‰°ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniCUEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºCSV2Sçš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚UniCUEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé›†æˆäº†ç†è§£ä»»åŠ¡ï¼ˆCSRï¼‰ï¼Œæä¾›ç²¾ç»†çš„CSè§†è§‰è¯­ä¹‰çº¿ç´¢æ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒUniCUEé‡‡ç”¨å§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€è¯­ä¹‰å¯¹é½æ± ï¼ˆä½¿ç²¾ç¡®çš„è§†è§‰è¯­ä¹‰æ˜ å°„æˆä¸ºå¯èƒ½ï¼‰å’ŒVisioPhoneticé€‚é…å™¨ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¶æ„å†…æ¶èµ·ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚ä¸ºäº†æ”¯æŒæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†UniCUE-HIï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ™®é€šè¯CSæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª14åæ‰“æ‰‹åŠ¿è€…çš„11282ä¸ªè§†é¢‘ï¼Œå…¶ä¸­åŒ…æ‹¬å¬éšœäººå£«å’Œå¬åŠ›æ­£å¸¸çš„äººã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniCUEåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04134v4">PDF</a> 13 pages, 12 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Cued Speechï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œä¸ºå¬åŠ›å—æŸè€…æä¾›è§†è§‰éŸ³ç´ çº¿ç´¢ä»¥æ”¯æŒç²¾ç¡®è¯­éŸ³æ„ŸçŸ¥ã€‚CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰çš„ä»»åŠ¡æ—¨åœ¨å°†CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶é›†ä¸­åœ¨CSè¯†åˆ«ï¼ˆCSRï¼‰ä¸Šï¼Œå°†è§†é¢‘å†…å®¹è½¬å½•ä¸ºæ–‡æœ¬ã€‚å› æ­¤ï¼ŒCSV2Sçš„å¸¸è§è§£å†³æ–¹æ¡ˆæ˜¯å°†CSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç»“åˆã€‚ç„¶è€Œï¼Œè¿™ç§ç®¡é“ä¾èµ–äºä½œä¸ºä¸­é—´ä»‹è´¨çš„æ–‡æœ¬ï¼Œå¯èƒ½å¯¼è‡´è¯¯å·®ä¼ æ’­å’Œè¯­éŸ³ä¸CSè§†é¢‘åŠ¨æ€ä¹‹é—´çš„æ—¶é—´ä¸åŒ¹é…ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›´æ¥ä»CSè§†é¢‘ç”ŸæˆéŸ³é¢‘è¯­éŸ³ï¼ˆç›´æ¥CSV2Sï¼‰å¸¸å¸¸é¢ä¸´å›ºæœ‰çš„å¤šæ¨¡å¼å¤æ‚æ€§å’ŒCSæ•°æ®æœ‰é™å¯ç”¨æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniCUEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºCSV2Sçš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼Œè€Œæ— éœ€ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚UniCUEçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºæ•´åˆäº†CSRè¿™ä¸€ç†è§£ä»»åŠ¡ï¼Œæä¾›ç²¾ç»†çš„CSè§†è§‰è¯­ä¹‰çº¿ç´¢æ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒUniCUEç»“åˆäº†å§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€è¯­ä¹‰å¯¹é½æ± ï¼ˆä½¿ç²¾ç¡®è§†è§‰è¯­ä¹‰æ˜ å°„æˆä¸ºå¯èƒ½ï¼‰å’ŒVisioPhoneticé€‚é…å™¨ï¼ˆåœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¹‹é—´å»ºç«‹æ¡¥æ¢çš„ç»Ÿä¸€æ¶æ„ï¼‰ã€‚ä¸ºäº†æ”¯æŒæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«11282ä¸ªè§†é¢‘å’Œ14ä¸ªæ‰‹è¯­è€…çš„UniCUE-HIå¤§è§„æ¨¡æ™®é€šè¯CSæ•°æ®é›†ã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒUniCUEåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Cued Speechï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯»ï¼Œä¸ºå¬åŠ›å—æŸè€…æä¾›è§†è§‰éŸ³ç´ çº¿ç´¢ã€‚</li>
<li>CSV2Sä»»åŠ¡æ—¨åœ¨å°†CSè§†é¢‘è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ï¼Œé¢ä¸´å¤šæ¨¡å¼å¤æ‚æ€§å’Œæ•°æ®æœ‰é™æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬ä½œä¸ºä¸­é—´ä»‹è´¨ï¼Œå¯èƒ½å¯¼è‡´è¯¯å·®ä¼ æ’­å’Œæ—¶é—´ä¸åŒ¹é…ã€‚</li>
<li>UniCUEæ˜¯ç¬¬ä¸€ä¸ªç›´æ¥ç”Ÿæˆè¯­éŸ³çš„ç»Ÿä¸€æ¡†æ¶ï¼Œä¸ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚</li>
<li>UniCUEæ•´åˆäº†CSRä»»åŠ¡ï¼Œæä¾›ç²¾ç»†çš„CSè§†è§‰è¯­ä¹‰çº¿ç´¢æ¥æŒ‡å¯¼è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>UniCUEç»“åˆäº†å¤šä¸ªåˆ›æ–°ç»„ä»¶ï¼ŒåŒ…æ‹¬å§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ã€è¯­ä¹‰å¯¹é½æ± å’ŒVisioPhoneticé€‚é…å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3cf65a969e61b9e5e3b2089a86a6224b" align="middle">
<img src="https://picx.zhimg.com/v2-8d5d9e30a85565ceb0c67c2191b00a00" align="middle">
<img src="https://picx.zhimg.com/v2-603aaf8c59f2e204535672abe1c21315" align="middle">
<img src="https://picx.zhimg.com/v2-5028efd4477eab37955f93669f8fc6ee" align="middle">
<img src="https://picx.zhimg.com/v2-734b1fbcf79a02b66dac1912d553c271" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speech-to-Speech-Dialogue-Modeling-with-End-to-End-Retrieval-Augmented-Generation"><a href="#Enhancing-Speech-to-Speech-Dialogue-Modeling-with-End-to-End-Retrieval-Augmented-Generation" class="headerlink" title="Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation"></a>Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation</h2><p><strong>Authors:Pengchao Feng, Ziyang Ma, Wenxi Chen, Yao Li, Sheng Wang, Kai Yu, Xie Chen</strong></p>
<p>End-to-end speech-to-speech (S2S) dialogue systems have recently garnered increasing research attention for their lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration of information. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind the SOTA cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. Our code and dataset are released.</p>
<blockquote>
<p>ç«¯åˆ°ç«¯è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰å¯¹è¯ç³»ç»Ÿå› å…¶è¾ƒä½çš„å»¶è¿Ÿå’Œæ›´è‡ªç„¶åœ°æ•´åˆéè¨€è¯­çº¿ç´¢ï¼ˆå¦‚æƒ…æ„Ÿå’Œè¯´è¯è€…èº«ä»½ï¼‰è€Œæœ€è¿‘å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿé¢ä¸´å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•´åˆå¤–éƒ¨çŸ¥è¯†æ–¹é¢ï¼Œè¿™æ˜¯åŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸é€šè¿‡å¢å¼ºæ£€ç´¢ï¼ˆRAGï¼‰æ¥è§£å†³çš„é—®é¢˜ã€‚æ ¸å¿ƒå›°éš¾åœ¨äºè¾“å…¥è¯­éŸ³å’Œæ£€ç´¢åˆ°çš„æ–‡æœ¬çŸ¥è¯†ä¹‹é—´çš„æ¨¡æ€å·®è·ï¼Œè¿™é˜»ç¢äº†ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯RAGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥ä»è¯­éŸ³æŸ¥è¯¢ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æœ¬çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç«¯åˆ°ç«¯S2Så¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ä¸Šæœ‰äº†æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶æé«˜äº†æ£€ç´¢æ•ˆç‡ã€‚å°½ç®¡æ€»ä½“æ€§èƒ½ä»ç„¶è½åäºæœ€å…ˆè¿›çš„çº§è”æ¨¡å‹ï¼Œä½†æˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¢å¼ºç«¯åˆ°ç«¯S2Sç³»ç»Ÿä¸­çš„çŸ¥è¯†æ•´åˆæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å·²ç»å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00028v2">PDF</a> Accepted to EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æ¢è®¨äº†ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰å¯¹è¯ç³»ç»Ÿçš„æœ€æ–°ç ”ç©¶è¶‹åŠ¿å’ŒæŒ‘æˆ˜ã€‚ä¸ºæé«˜çŸ¥è¯†æ•´åˆèƒ½åŠ›ï¼Œæå‡ºä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œå¯ä»è¯­éŸ³æŸ¥è¯¢ä¸­ç›´æ¥æ£€ç´¢ç›¸å…³æ–‡æœ¬çŸ¥è¯†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ç«¯åˆ°ç«¯S2Så¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ï¼ŒåŒæ—¶æé«˜äº†æ£€ç´¢æ•ˆç‡ï¼Œä¸ºå¢å¼ºç«¯åˆ°ç«¯S2Sç³»ç»Ÿä¸­çš„çŸ¥è¯†æ•´åˆæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰å¯¹è¯ç³»ç»Ÿå—åˆ°ç ”ç©¶å…³æ³¨ï¼Œå› å…¶ä½å»¶è¿Ÿå’Œæ›´è‡ªç„¶çš„éè¯­è¨€çº¿ç´¢æ•´åˆèƒ½åŠ›ã€‚</li>
<li>é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œè¿™åœ¨æ–‡æœ¬å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é€šå¸¸é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è§£å†³ã€‚</li>
<li>æ¨¡æ€é—´éš™æ˜¯é˜»ç¢æœ‰æ•ˆæ•´åˆä¿¡æ¯çš„ä¸»è¦éš¾é¢˜ï¼Œå­˜åœ¨äºè¾“å…¥è¯­éŸ³å’Œæ£€ç´¢åˆ°çš„æ–‡æœ¬çŸ¥è¯†ä¹‹é—´ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„RAGæ¡†æ¶ï¼Œç›´æ¥ä»è¯­éŸ³æŸ¥è¯¢ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬çŸ¥è¯†ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•æé«˜äº†S2Så¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½å’Œæ£€ç´¢æ•ˆç‡ã€‚</li>
<li>å°½ç®¡æ€»ä½“æ€§èƒ½ä»è½åäºçº§è”æ¨¡å‹ï¼Œä½†è¯¥æ¡†æ¶ä¸ºå¢å¼ºç«¯åˆ°ç«¯S2Sç³»ç»Ÿä¸­çš„çŸ¥è¯†æ•´åˆæä¾›äº†æœ‰å‰é€”çš„æ–¹å‘ã€‚</li>
<li>ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2932519ab27ab5fc89d46b369ebe0457" align="middle">
<img src="https://picx.zhimg.com/v2-1ae1a7d681c82eabae8df366e112243c" align="middle">
<img src="https://picx.zhimg.com/v2-18b529b2b32009ba5044debfb0dbddb8" align="middle">
<img src="https://picx.zhimg.com/v2-f456b538c57c6cb91bbe43c021502568" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Long-duration-Talking-Video-Synthesis-with-Linear-Diffusion-Transformer-under-Multimodal-Guidance"><a href="#Efficient-Long-duration-Talking-Video-Synthesis-with-Linear-Diffusion-Transformer-under-Multimodal-Guidance" class="headerlink" title="Efficient Long-duration Talking Video Synthesis with Linear Diffusion Transformer under Multimodal Guidance"></a>Efficient Long-duration Talking Video Synthesis with Linear Diffusion Transformer under Multimodal Guidance</h2><p><strong>Authors:Haojie Zhang, Zhihao Liang, Ruibo Fu, Bingyan Liu, Zhengqi Wen, Xuefei Liu, Jianhua Tao, Yaling Liang</strong></p>
<p>Long-duration talking video synthesis faces enduring challenges in achieving high video quality, portrait and temporal consistency, and computational efficiency. As video length increases, issues such as visual degradation, identity inconsistency, temporal incoherence, and error accumulation become increasingly problematic, severely affecting the realism and reliability of the results. To address these challenges, we present LetsTalk, a diffusion transformer framework equipped with multimodal guidance and a novel memory bank mechanism, explicitly maintaining contextual continuity and enabling robust, high-quality, and efficient generation of long-duration talking videos. In particular, LetsTalk introduces a noise-regularized memory bank to alleviate error accumulation and sampling artifacts during extended video generation. To further improve efficiency and spatiotemporal consistency, LetsTalk employs a deep compression autoencoder and a spatiotemporal-aware transformer with linear attention for effective multimodal fusion. We systematically analyze three fusion schemes and show that combining deep (Symbiotic Fusion) for portrait features and shallow (Direct Fusion) for audio achieves superior visual realism and precise speech-driven motion, while preserving diversity of movements. Extensive experiments demonstrate that LetsTalk establishes new state-of-the-art in generation quality, producing temporally coherent and realistic talking videos with enhanced diversity and liveliness, and maintains remarkable efficiency with 8x fewer parameters than previous approaches.</p>
<blockquote>
<p>é•¿æ—¶è¯­éŸ³è§†é¢‘åˆæˆé¢ä¸´ç€å®ç°é«˜è´¨é‡è§†é¢‘ã€è‚–åƒå’Œæ—¶åºä¸€è‡´æ€§ä»¥åŠè®¡ç®—æ•ˆç‡çš„æŒä¹…æŒ‘æˆ˜ã€‚éšç€è§†é¢‘é•¿åº¦çš„å¢åŠ ï¼Œè§†è§‰é€€åŒ–ã€èº«ä»½ä¸ä¸€è‡´ã€æ—¶åºä¸ä¸€è‡´å’Œè¯¯å·®ç´¯ç§¯ç­‰é—®é¢˜å˜å¾—è¶Šæ¥è¶Šä¸¥é‡ï¼Œä¸¥é‡å½±å“äº†ç»“æœçš„çœŸå®æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LetsTalkï¼Œè¿™æ˜¯ä¸€ä¸ªé…å¤‡äº†å¤šæ¨¡å¼æŒ‡å¯¼å’Œæ–°é¢–è®°å¿†åº“æœºåˆ¶çš„æ‰©æ•£å˜å‹å™¨æ¡†æ¶ï¼Œæ˜ç¡®åœ°ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼Œå¹¶èƒ½å¤Ÿå®ç°ç¨³å¥ã€é«˜è´¨é‡å’Œé«˜æ•ˆçš„é•¿æ—¶è¯­éŸ³è§†é¢‘ç”Ÿæˆã€‚ç‰¹åˆ«åœ°ï¼ŒLetsTalkå¼•å…¥äº†ä¸€ä¸ªå™ªå£°æ­£åˆ™åŒ–è®°å¿†åº“ï¼Œä»¥å‡è½»æ‰©å±•è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è¯¯å·®ç´¯ç§¯å’Œé‡‡æ ·ä¼ªå½±ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ•ˆç‡å’Œæ—¶ç©ºä¸€è‡´æ€§ï¼ŒLetsTalké‡‡ç”¨æ·±åº¦å‹ç¼©è‡ªåŠ¨ç¼–ç å™¨å’Œå…·æœ‰çº¿æ€§æ³¨æ„åŠ›çš„æ—¶ç©ºæ„ŸçŸ¥å˜å‹å™¨ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¤šæ¨¡å¼èåˆã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†ä¸‰ç§èåˆæ–¹æ¡ˆï¼Œå¹¶è¡¨æ˜æ·±åº¦ï¼ˆå…±ç”Ÿèåˆï¼‰ç”¨äºè‚–åƒç‰¹å¾å’Œæµ…å±‚ï¼ˆç›´æ¥èåˆï¼‰ç”¨äºéŸ³é¢‘çš„ç»“åˆå®ç°äº†å“è¶Šçš„è§†é¢‘çœŸå®æ„Ÿå’Œç²¾ç¡®çš„è¯­è¨€é©±åŠ¨è¿åŠ¨ï¼ŒåŒæ—¶ä¿æŒè¿åŠ¨çš„å¤šæ ·æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLetsTalkåœ¨ç”Ÿæˆè´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œç”Ÿæˆäº†æ—¶åºä¸€è‡´å’Œç°å®æ„Ÿå¼ºçš„è¯­éŸ³è§†é¢‘ï¼Œå¢å¼ºäº†å¤šæ ·æ€§å’Œç”ŸåŠ¨æ€§ï¼Œå¹¶ä¸”åœ¨å‚æ•°æ–¹é¢æ¯”ä»¥å‰çš„æ–¹æ³•å°‘äº†8å€ï¼Œä»ç„¶ä¿æŒäº†æƒŠäººçš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16748v4">PDF</a> 10 pages, 7 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é•¿æ—¶ç¨‹å¯¹è¯è§†é¢‘åˆæˆåœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ—¶é¢ä¸´å¤šæ–¹é¢çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å®ç°é«˜è§†é¢‘è´¨é‡ã€ä¿æŒäººåƒä¸æ—¶é—´çš„è¿è´¯ä¸€è‡´æ€§ä»¥åŠè®¡ç®—æ•ˆç‡çš„æå‡ã€‚éšç€è§†é¢‘é•¿åº¦çš„å¢åŠ ï¼Œè§†è§‰å¤±çœŸã€èº«ä»½ä¸ä¸€è‡´ã€æ—¶é—´ä¸ä¸€è‡´å’Œè¯¯å·®ç´¯ç§¯ç­‰é—®é¢˜æ„ˆå‘ä¸¥é‡ï¼Œä¸¥é‡å½±å“äº†ç»“æœçš„é€¼çœŸåº¦å’Œå¯é æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºLetsTalkçš„æ‰©æ•£å˜æ¢æ¡†æ¶ï¼Œå®ƒé…å¤‡äº†å¤šæ¨¡æ€æŒ‡å¯¼å’Œä¸€ç§æ–°å‹è®°å¿†åº“æœºåˆ¶ï¼Œèƒ½å¤Ÿæ˜ç¡®åœ°ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼Œä»è€Œå®ç°äº†ç¨³å¥ã€é«˜è´¨é‡å’Œé«˜æ•ˆçš„é•¿æ—¶ç¨‹å¯¹è¯è§†é¢‘ç”Ÿæˆã€‚ç‰¹åˆ«åœ°ï¼ŒLetsTalkå¼•å…¥äº†ä¸€ç§å™ªå£°æ­£åˆ™åŒ–è®°å¿†åº“ï¼Œä»¥å‡è½»åœ¨ç”Ÿæˆé•¿è§†é¢‘è¿‡ç¨‹ä¸­çš„è¯¯å·®ç´¯ç§¯å’Œé‡‡æ ·ä¼ªå½±é—®é¢˜ã€‚ä¸ºè¿›ä¸€æ­¥æ”¹å–„æ•ˆç‡å’Œæ—¶ç©ºä¸€è‡´æ€§ï¼ŒLetsTalké‡‡ç”¨æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨ä»¥åŠå…·æœ‰çº¿æ€§æ³¨æ„åŠ›çš„æ—¶ç©ºæ„ŸçŸ¥å˜æ¢å™¨ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†ä¸‰ç§èåˆæ–¹æ¡ˆï¼Œå¹¶å‘ç°é‡‡ç”¨æ·±åº¦èåˆï¼ˆå…±ç”Ÿèåˆï¼‰è¿›è¡Œäººåƒç‰¹å¾èåˆå’Œæµ…å±‚èåˆï¼ˆç›´æ¥èåˆï¼‰è¿›è¡ŒéŸ³é¢‘èåˆçš„æ–¹æ³•ï¼Œå¯å®ç°æœ€ä½³çš„è§†è§‰é€¼çœŸåº¦å’Œç²¾ç¡®çš„è¯­éŸ³é©±åŠ¨è¿åŠ¨æ•ˆæœï¼ŒåŒæ—¶ä¿æŒåŠ¨ä½œå¤šæ ·æ€§çš„ä¿ç•™ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLetsTalkåœ¨ç”Ÿæˆè´¨é‡æ–¹é¢æ ‘ç«‹äº†æ–°çš„ä¸šç•Œæ ‡æ†ï¼Œèƒ½å¤Ÿç”Ÿæˆæ—¶é—´è¿è´¯ä¸”é€¼çœŸçš„å¯¹è¯è§†é¢‘ï¼Œå¢å¼ºäº†å¤šæ ·æ€§å’Œç”ŸåŠ¨æ€§ï¼Œå¹¶ä¸”åœ¨å‚æ•°ä½¿ç”¨ä¸Šè¾ƒä¹‹å‰çš„æ–¹æ³•å‡å°‘äº†8å€ï¼Œä¿æŒäº†æ˜¾è‘—çš„é«˜æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é•¿æ—¶ç¨‹å¯¹è¯è§†é¢‘åˆæˆé¢ä¸´é«˜è§†é¢‘è´¨é‡ã€äººåƒå’Œæ—¶é—´è¿è´¯ä¸€è‡´æ€§ä»¥åŠè®¡ç®—æ•ˆç‡çš„ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>éšç€è§†é¢‘é•¿åº¦å¢åŠ ï¼Œè§†è§‰å¤±çœŸã€èº«ä»½ä¸ä¸€è‡´ç­‰é—®é¢˜æ„ˆå‘ä¸¥é‡ã€‚</li>
<li>LetsTalké‡‡ç”¨æ‰©æ•£å˜æ¢æ¡†æ¶ï¼Œé…å¤‡å¤šæ¨¡æ€æŒ‡å¯¼å’Œè®°å¿†åº“æœºåˆ¶åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>å™ªå£°æ­£åˆ™åŒ–è®°å¿†åº“ç”¨äºå‡è½»è¯¯å·®ç´¯ç§¯å’Œé‡‡æ ·ä¼ªå½±ã€‚</li>
<li>æ·±åº¦èåˆä¸æµ…å±‚èåˆç»“åˆæ–¹æ³•å®ç°æœ€ä½³è§†è§‰é€¼çœŸåº¦å’Œè¯­éŸ³é©±åŠ¨è¿åŠ¨æ•ˆæœã€‚</li>
<li>LettsTalkåœ¨ç”Ÿæˆè´¨é‡ä¸Šæ ‘ç«‹æ–°æ ‡æ†ï¼Œç”Ÿæˆè§†é¢‘å…·æœ‰æ—¶é—´è¿è´¯æ€§ã€é€¼çœŸåº¦ã€å¤šæ ·æ€§å’Œç”ŸåŠ¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a093f785b24fbc41e2fd4b92a69df74e" align="middle">
<img src="https://picx.zhimg.com/v2-1d1da715a10edbe23b1d08fff46b814e" align="middle">
<img src="https://picx.zhimg.com/v2-b83a97512c6120fd09ae516f07aead4b" align="middle">
<img src="https://picx.zhimg.com/v2-afa26eec912757fa41f1692f7a9da2e7" align="middle">
<img src="https://picx.zhimg.com/v2-30ac3b5962f32623eb45e4ff2f40f17a" align="middle">
<img src="https://picx.zhimg.com/v2-24758ed84dfef215f6a1c603c127b63e" align="middle">
<img src="https://picx.zhimg.com/v2-8cf821eb2878698ad5af1011535d8e9a" align="middle">
<img src="https://picx.zhimg.com/v2-aad7a47730924b7cbc97ce809e10725d" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3d57943303de825dfe353d07827e2920" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Normality and the Turing Test
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e42cd283f8e0da422c423a77083561e0" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Second-order spatial analysis of shapes of tumor cell nuclei
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
