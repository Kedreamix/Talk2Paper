<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Second-order spatial analysis of shapes of tumor cell nuclei">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e42cd283f8e0da422c423a77083561e0')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-17-æ›´æ–°"><a href="#2025-11-17-æ›´æ–°" class="headerlink" title="2025-11-17 æ›´æ–°"></a>2025-11-17 æ›´æ–°</h1><h2 id="Second-order-spatial-analysis-of-shapes-of-tumor-cell-nuclei"><a href="#Second-order-spatial-analysis-of-shapes-of-tumor-cell-nuclei" class="headerlink" title="Second-order spatial analysis of shapes of tumor cell nuclei"></a>Second-order spatial analysis of shapes of tumor cell nuclei</h2><p><strong>Authors:Ye Jin Choi, Sebastian Kurtek, Simeng Zhu, Karthik Bharath</strong></p>
<p>Intra-tumor heterogeneity driving disease progression is characterized by distinct growth and spatial proliferation patterns of cells and their nuclei within tumor and non-tumor tissues. A widely accepted hypothesis is that these spatial patterns are correlated with morphology of the cells and their nuclei. Nevertheless, tools to quantify the correlation, with uncertainty, are scarce, and the state-of-the-art is based on low-dimensional numerical summaries of the shapes that are inadequate to fully encode shape information. To this end, we propose a marked point process framework to assess spatial correlation among shapes of planar closed curves, which represent cell or nuclei outlines. With shapes of curves as marks, the framework is based on a mark-weighted $K$ function, a second-order spatial statistic that accounts for the marksâ€™ variation by using test functions that capture only the shapes of cells and their nuclei. We then develop local and global hypothesis tests for spatial dependence between the marks using the $K$ function. The framework is brought to bear on the cell nuclei extracted from histopathology images of breast cancer, where we uncover distinct correlation patterns that are consistent with clinical expectations.</p>
<blockquote>
<p>è‚¿ç˜¤å†…å¼‚è´¨æ€§æ¨åŠ¨ç–¾ç—…è¿›å±•çš„ç‰¹ç‚¹æ˜¯è‚¿ç˜¤å’Œéè‚¿ç˜¤ç»„ç»‡å†…ç»†èƒåŠå…¶ç»†èƒæ ¸çš„ç‹¬ç‰¹ç”Ÿé•¿å’Œç©ºé—´å¢æ®–æ¨¡å¼ã€‚ä¸€ä¸ªè¢«å¹¿æ³›æ¥å—çš„å‡è®¾æ˜¯ï¼Œè¿™äº›ç©ºé—´æ¨¡å¼ä¸ç»†èƒå’Œç»†èƒæ ¸çš„å½¢æ€æœ‰å…³ã€‚ç„¶è€Œï¼Œç”¨æ¥é‡åŒ–è¿™ç§ä¸ç¡®å®šå…³è”çš„å·¥å…·å´å¾ˆç¨€ç¼ºï¼Œå½“å‰çš„æŠ€æœ¯æ˜¯åŸºäºå¯¹å½¢çŠ¶çš„ä½ç»´æ•°å€¼æ‘˜è¦ï¼Œè¿™ä¸è¶³ä»¥å®Œå…¨ç¼–ç å½¢çŠ¶ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ ‡è®°ç‚¹è¿‡ç¨‹æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¹³é¢é—­åˆæ›²çº¿å½¢çŠ¶ä¹‹é—´çš„ç©ºé—´ç›¸å…³æ€§ï¼Œè¿™äº›æ›²çº¿ä»£è¡¨ç»†èƒæˆ–ç»†èƒæ ¸çš„è½®å»“ã€‚ä»¥æ›²çº¿å½¢çŠ¶ä¸ºæ ‡è®°ï¼Œè¯¥æ¡†æ¶åŸºäºæ ‡è®°åŠ æƒKå‡½æ•°ï¼Œè¿™æ˜¯ä¸€ä¸ªäºŒé˜¶ç©ºé—´ç»Ÿè®¡é‡ï¼Œé€šè¿‡ä½¿ç”¨ä»…æ•æ‰ç»†èƒå’Œç»†èƒæ ¸å½¢çŠ¶çš„æµ‹è¯•å‡½æ•°æ¥è€ƒè™‘æ ‡è®°çš„å˜åŒ–ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¸ºæ ‡è®°ä¹‹é—´ä½¿ç”¨Kå‡½æ•°è¿›è¡Œå±€éƒ¨å’Œå…¨å±€å‡è®¾æ£€éªŒç©ºé—´ä¾èµ–æ€§ã€‚è¯¥æ¡†æ¶åº”ç”¨äºä»ä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­æå–çš„ç»†èƒæ ¸ä¸Šï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº›æ˜æ˜¾çš„å…³è”æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼ä¸ä¸´åºŠé¢„æœŸç›¸ç¬¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09023v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‚¿ç˜¤å†…å¼‚è´¨æ€§é©±åŠ¨ç–¾ç—…è¿›å±•çš„ç‰¹å¾ï¼Œè¡¨ç°ä¸ºè‚¿ç˜¤å’Œéè‚¿ç˜¤ç»„ç»‡å†…ç»†èƒåŠå…¶ç»†èƒæ ¸çš„ç”Ÿé•¿å’Œç©ºé—´å¢æ®–æ¨¡å¼çš„å·®å¼‚ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºæ ‡è®°ç‚¹è¿‡ç¨‹æ¡†æ¶çš„æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¹³é¢é—­åˆæ›²çº¿å½¢çŠ¶ä¹‹é—´çš„ç©ºé—´ç›¸å…³æ€§ï¼Œè¿™äº›æ›²çº¿ä»£è¡¨ç»†èƒæˆ–ç»†èƒæ ¸çš„è½®å»“ã€‚è¯¥æ–¹æ³•åŸºäºæ ‡è®°åŠ æƒçš„Kå‡½æ•°ï¼Œæ˜¯ä¸€ç§äºŒé˜¶ç©ºé—´ç»Ÿè®¡é‡ï¼Œé€šè¿‡ä½¿ç”¨ä»…æ•æ‰ç»†èƒå’Œç»†èƒæ ¸å½¢çŠ¶çš„æµ‹è¯•å‡½æ•°æ¥è€ƒè™‘æ ‡è®°çš„å˜åŒ–ã€‚ç„¶åï¼Œä½¿ç”¨Kå‡½æ•°å¯¹æ ‡è®°ä¹‹é—´çš„ç©ºé—´ä¾èµ–æ€§è¿›è¡Œå±€éƒ¨å’Œå…¨å±€å‡è®¾æ£€éªŒã€‚è¯¥æ¡†æ¶åº”ç”¨äºä»ä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­æå–çš„ç»†èƒæ ¸ï¼Œæ­ç¤ºäº†ä¸ä¸´åºŠé¢„æœŸä¸€è‡´çš„ä¸åŒç›¸å…³æ€§æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚¿ç˜¤å†…å¼‚è´¨æ€§æ˜¯ç–¾ç—…è¿›å±•çš„å…³é”®ç‰¹å¾ï¼Œæ¶‰åŠç»†èƒå’Œç»†èƒæ ¸çš„ç©ºé—´å¢æ®–æ¨¡å¼å·®å¼‚ã€‚</li>
<li>ç›®å‰ç¼ºä¹é‡åŒ–ç»†èƒå½¢æ€å’Œç©ºé—´æ¨¡å¼ç›¸å…³æ€§çš„å·¥å…·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ ‡è®°ç‚¹è¿‡ç¨‹æ¡†æ¶çš„æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¹³é¢é—­åˆæ›²çº¿ï¼ˆä»£è¡¨ç»†èƒæˆ–ç»†èƒæ ¸è½®å»“ï¼‰å½¢çŠ¶ä¹‹é—´çš„ç©ºé—´ç›¸å…³æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨æ ‡è®°åŠ æƒçš„Kå‡½æ•°ï¼Œè¿™æ˜¯ä¸€ç§äºŒé˜¶ç©ºé—´ç»Ÿè®¡é‡ï¼Œé€šè¿‡æµ‹è¯•å‡½æ•°æ•æ‰ç»†èƒå’Œç»†èƒæ ¸çš„å½¢çŠ¶å˜åŒ–ã€‚</li>
<li>å¼€å‘äº†å±€éƒ¨å’Œå…¨å±€å‡è®¾æ£€éªŒï¼Œä»¥æ£€éªŒæ ‡è®°ï¼ˆå¦‚ç»†èƒå’Œç»†èƒæ ¸ï¼‰ä¹‹é—´çš„ç©ºé—´ä¾èµ–æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åº”ç”¨äºä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒçš„ç»†èƒæ ¸åˆ†æï¼Œå‘ç°äº†ä¸ä¸´åºŠé¢„æœŸç›¸ç¬¦çš„ç›¸å…³æ€§æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a4ad7200d3e8950b12060a7e3688430" align="middle">
<img src="https://picx.zhimg.com/v2-7b6a79f9730f432e10e54f622ac87861" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SAMora-Enhancing-SAM-through-Hierarchical-Self-Supervised-Pre-Training-for-Medical-Images"><a href="#SAMora-Enhancing-SAM-through-Hierarchical-Self-Supervised-Pre-Training-for-Medical-Images" class="headerlink" title="SAMora: Enhancing SAM through Hierarchical Self-Supervised Pre-Training for Medical Images"></a>SAMora: Enhancing SAM through Hierarchical Self-Supervised Pre-Training for Medical Images</h2><p><strong>Authors:Shuhang Chen, Hangjie Yuan, Pengwei Liu, Hanxue Gu, Tao Feng, Dong Ni</strong></p>
<p>The Segment Anything Model (SAM) has demonstrated significant potential in medical image segmentation. Yet, its performance is limited when only a small amount of labeled data is available, while there is abundant valuable yet often overlooked hierarchical information in medical data. To address this limitation, we draw inspiration from self-supervised learning and propose SAMora, an innovative framework that captures hierarchical medical knowledge by applying complementary self-supervised learning objectives at the image, patch, and pixel levels. To fully exploit the complementarity of hierarchical knowledge within LoRAs, we introduce HL-Attn, a hierarchical fusion module that integrates multi-scale features while maintaining their distinct characteristics. SAMora is compatible with various SAM variants, including SAM2, SAMed, and H-SAM. Experimental results on the Synapse, LA, and PROMISE12 datasets demonstrate that SAMora outperforms existing SAM variants. It achieves state-of-the-art performance in both few-shot and fully supervised settings while reducing fine-tuning epochs by 90%. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ShChen233/SAMora">https://github.com/ShChen233/SAMora</a>.</p>
<blockquote>
<p>Segment Anything Modelï¼ˆSAMï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“åªæœ‰å°‘é‡æ ‡è®°æ•°æ®æ—¶ï¼Œå…¶æ€§èƒ½å—åˆ°é™åˆ¶ï¼Œè€ŒåŒ»å­¦æ•°æ®ä¸­å­˜åœ¨ç€å¤§é‡æœ‰ä»·å€¼ä½†è¢«å¿½ç•¥çš„å±‚çº§ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬å—åˆ°è‡ªæˆ‘ç›‘ç£å­¦ä¹ çš„å¯å‘ï¼Œæå‡ºäº†SAMoraï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°æ¡†æ¶ï¼Œå®ƒé€šè¿‡åº”ç”¨å›¾åƒã€è¡¥ä¸å’Œåƒç´ çº§åˆ«çš„äº’è¡¥è‡ªæˆ‘ç›‘ç£å­¦ä¹ ç›®æ ‡æ¥æ•è·åŒ»å­¦çŸ¥è¯†çš„å±‚æ¬¡ç»“æ„ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨LoRAså†…éƒ¨å±‚æ¬¡çŸ¥è¯†çš„äº’è¡¥æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†HL-Attnï¼Œè¿™æ˜¯ä¸€ä¸ªå±‚æ¬¡èåˆæ¨¡å—ï¼Œèƒ½å¤Ÿæ•´åˆå¤šå°ºåº¦ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒå…¶ç‹¬ç‰¹ç‰¹æ€§ã€‚SAMoraå¯ä»¥ä¸å„ç§SAMå˜ä½“å…¼å®¹ï¼ŒåŒ…æ‹¬SAM2ã€SAMedå’ŒH-SAMã€‚åœ¨Synapseã€LAå’ŒPROMISE12æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSAMoraä¼˜äºç°æœ‰çš„SAMå˜ä½“ã€‚å®ƒåœ¨å°æ ·æœ¬å’Œå®Œå…¨ç›‘ç£è®¾ç½®ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†å¾®è°ƒå‘¨æœŸè¾¾90%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShChen233/SAMora%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ShChen233/SAMoraè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08626v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAMoraæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚å®ƒé€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ•è·åŒ»å­¦çŸ¥è¯†çš„å±‚æ¬¡ç»“æ„ï¼Œå¹¶åœ¨å›¾åƒã€è¡¥ä¸å’Œåƒç´ çº§åˆ«åº”ç”¨äº’è¡¥çš„è‡ªå­¦ç›®æ ‡ã€‚æ­¤å¤–ï¼Œå¼•å…¥HL-Attnæ¨¡å—å®ç°å¤šå±‚æ¬¡ç‰¹å¾èåˆï¼Œæå‡æ€§èƒ½ã€‚SAMoraä¸å¤šç§SAMå˜ä½“å…¼å®¹ï¼Œå¹¶åœ¨Synapseã€LAå’ŒPROMISE12æ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œå‡å°‘å¾®è°ƒå‘¨æœŸ90%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMoraæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>åœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒSAMoraé€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ æé«˜æ€§èƒ½ã€‚</li>
<li>SAMoraé€šè¿‡åº”ç”¨äº’è¡¥çš„è‡ªå­¦ç›®æ ‡åœ¨å›¾åƒã€è¡¥ä¸å’Œåƒç´ çº§åˆ«æ•è·åŒ»å­¦çŸ¥è¯†çš„å±‚æ¬¡ç»“æ„ã€‚</li>
<li>HL-Attnæ¨¡å—çš„å¼•å…¥å®ç°äº†å¤šå±‚æ¬¡ç‰¹å¾èåˆï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</li>
<li>SAMoraä¸å¤šç§SAMå˜ä½“å…¼å®¹ã€‚</li>
<li>åœ¨Synapseã€LAå’ŒPROMISE12æ•°æ®é›†ä¸Šï¼ŒSAMoraå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>SAMoraå‡å°‘äº†å¾®è°ƒå‘¨æœŸ90%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3101c176ca500836d54d7e7263398168" align="middle">
<img src="https://picx.zhimg.com/v2-d97c500b50ede95c4285f47a689af113" align="middle">
<img src="https://picx.zhimg.com/v2-fca079dde309f95d744f117cf64e2e3b" align="middle">
<img src="https://picx.zhimg.com/v2-9abfb159406d56a9c1583b48ac0526dd" align="middle">
<img src="https://picx.zhimg.com/v2-53cbc0b9e7d091d3dd445b0eaafb50b3" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SENCA-st-Integrating-Spatial-Transcriptomics-and-Histopathology-with-Cross-Attention-Shared-Encoder-for-Region-Identification-in-Cancer-Pathology"><a href="#SENCA-st-Integrating-Spatial-Transcriptomics-and-Histopathology-with-Cross-Attention-Shared-Encoder-for-Region-Identification-in-Cancer-Pathology" class="headerlink" title="SENCA-st: Integrating Spatial Transcriptomics and Histopathology with Cross Attention Shared Encoder for Region Identification in Cancer Pathology"></a>SENCA-st: Integrating Spatial Transcriptomics and Histopathology with Cross Attention Shared Encoder for Region Identification in Cancer Pathology</h2><p><strong>Authors:Shanaka Liyanaarachchi, Chathurya Wijethunga, Shihab Aaqil Ahamed, Akthas Absar, Ranga Rodrigo</strong></p>
<p>Spatial transcriptomics is an emerging field that enables the identification of functional regions based on the spatial distribution of gene expression. Integrating this functional information present in transcriptomic data with structural data from histopathology images is an active research area with applications in identifying tumor substructures associated with cancer drug resistance. Current histopathology-spatial-transcriptomic region segmentation methods suffer due to either making spatial transcriptomics prominent by using histopathology features just to assist processing spatial transcriptomics data or using vanilla contrastive learning that make histopathology images prominent due to only promoting common features losing functional information. In both extremes, the model gets either lost in the noise of spatial transcriptomics or overly smoothed, losing essential information. Thus, we propose our novel architecture SENCA-st (Shared Encoder with Neighborhood Cross Attention) that preserves the features of both modalities. More importantly, it emphasizes regions that are structurally similar in histopathology but functionally different on spatial transcriptomics using cross-attention. We demonstrate the superior performance of our model that surpasses state-of-the-art methods in detecting tumor heterogeneity and tumor micro-environment regions, a clinically crucial aspect.</p>
<blockquote>
<p>ç©ºé—´è½¬å½•ç»„å­¦æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸï¼Œå®ƒå¯ä»¥æ ¹æ®åŸºå› è¡¨è¾¾çš„ç©ºé—´åˆ†å¸ƒæ¥è¯†åˆ«åŠŸèƒ½åŒºåŸŸã€‚å°†è½¬å½•ç»„æ•°æ®ä¸­çš„åŠŸèƒ½ä¿¡æ¯ä¸ç»„ç»‡ç—…ç†å­¦å›¾åƒçš„ç»“æ„æ•°æ®ç›¸ç»“åˆï¼Œæ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œåœ¨è¯†åˆ«ä¸ç™Œç—‡è¯ç‰©æŠµæŠ—ç›¸å…³çš„è‚¿ç˜¤äºšç»“æ„æ–¹é¢å…·æœ‰åº”ç”¨å‰æ™¯ã€‚å½“å‰çš„ç»„ç»‡ç—…ç†å­¦-ç©ºé—´è½¬å½•ç»„åŒºåŸŸåˆ†å‰²æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œè¦ä¹ˆè¿‡åˆ†å¼ºè°ƒç©ºé—´è½¬å½•ç»„å­¦ï¼Œä»…ä½¿ç”¨ç»„ç»‡ç—…ç†å­¦ç‰¹å¾è¾…åŠ©å¤„ç†ç©ºé—´è½¬å½•ç»„æ•°æ®ï¼›è¦ä¹ˆä½¿ç”¨æ™®é€šçš„å¯¹æ¯”å­¦ä¹ ï¼Œä½¿å¾—ç»„ç»‡ç—…ç†å­¦å›¾åƒå› ä»…å¼ºè°ƒå…±åŒç‰¹å¾è€Œå¤±å»åŠŸèƒ½ä¿¡æ¯ã€‚åœ¨è¿™ä¸¤ç§æç«¯æƒ…å†µä¸‹ï¼Œæ¨¡å‹è¦ä¹ˆè¿·å¤±åœ¨å™ªå£°çš„ç©ºé—´è½¬å½•ç»„ä¸­ï¼Œè¦ä¹ˆè¿‡äºå¹³æ»‘è€Œå¤±å»å…³é”®ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°é¢–æ¶æ„SENCA-stï¼ˆå…·æœ‰é‚»åŸŸäº¤å‰æ³¨æ„åŠ›çš„å…±äº«ç¼–ç å™¨ï¼‰ï¼Œèƒ½å¤Ÿä¿ç•™ä¸¤ç§æ¨¡å¼çš„ç‰¹å¾ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å¼ºè°ƒåœ¨ç»„ç»‡ç—…ç†å­¦ç»“æ„ä¸Šç›¸ä¼¼ä½†åœ¨ç©ºé—´è½¬å½•ç»„å­¦ä¸ŠåŠŸèƒ½ä¸åŒçš„åŒºåŸŸã€‚æˆ‘ä»¬å±•ç¤ºäº†æ¨¡å‹åœ¨æ£€æµ‹è‚¿ç˜¤å¼‚è´¨æ€§å’Œè‚¿ç˜¤å¾®ç¯å¢ƒåŒºåŸŸæ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œè¿™æ˜¯ä¸´åºŠä¸Šè‡³å…³é‡è¦çš„æ–¹é¢ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08573v1">PDF</a> Accepted at WACV 2026</p>
<p><strong>Summary</strong></p>
<p>ç©ºé—´è½¬å½•ç»„å­¦æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸï¼Œèƒ½å¤Ÿé€šè¿‡åŸºå› è¡¨è¾¾çš„ç©ºé—´åˆ†å¸ƒæ¥è¯†åˆ«åŠŸèƒ½åŒºåŸŸã€‚æœ¬æ–‡å°†ç©ºé—´è½¬å½•ç»„æ•°æ®ä¸ç»„ç»‡ç—…ç†å­¦å›¾åƒçš„ç»“æ„æ•°æ®ç›¸ç»“åˆï¼Œæå‡ºä¸€ç§æ–°å‹æ¶æ„SENCA-stï¼Œæ—¢ä¿ç•™äº†ä¸¤ç§æ¨¡æ€çš„ç‰¹å¾ï¼Œåˆå¼ºè°ƒåœ¨ç»„ç»‡ç—…ç†å­¦ä¸Šç»“æ„ç›¸ä¼¼ä½†åœ¨ç©ºé—´è½¬å½•ç»„å­¦ä¸ŠåŠŸèƒ½ä¸åŒçš„åŒºåŸŸã€‚è¯¥æ¨¡å‹åœ¨æ£€æµ‹è‚¿ç˜¤å¼‚è´¨æ€§å’Œè‚¿ç˜¤å¾®ç¯å¢ƒåŒºåŸŸæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·æœ‰ä¸´åºŠåº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´è½¬å½•ç»„å­¦èƒ½å¤ŸåŸºäºåŸºå› è¡¨è¾¾çš„ç©ºé—´åˆ†å¸ƒè¯†åˆ«åŠŸèƒ½åŒºåŸŸã€‚</li>
<li>ç»“åˆç©ºé—´è½¬å½•ç»„å­¦ä¸ç»„ç»‡ç—…ç†å­¦å›¾åƒå…·æœ‰è¯†åˆ«è‚¿ç˜¤å­ç»“æ„ä¸åº”ç”¨åœ¨æŠ—ç™Œè¯ç‰©æŠµæŠ—æ€§ç ”ç©¶ä¸­çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œè¦ä¹ˆè¿‡åº¦ä¾èµ–ç©ºé—´è½¬å½•ç»„å­¦æ•°æ®ï¼Œè¦ä¹ˆè¿‡äºå¼ºè°ƒç»„ç»‡ç—…ç†å­¦å›¾åƒçš„å…±åŒç‰¹å¾è€Œå¿½ç•¥åŠŸèƒ½ä¿¡æ¯ã€‚</li>
<li>SENCA-stæ¶æ„æ—¨åœ¨å¹³è¡¡ä¸¤ç§æ¨¡æ€æ•°æ®çš„ç‰¹å¾ï¼ŒåŒæ—¶å¼ºè°ƒåœ¨ç»„ç»‡å­¦ä¸Šç»“æ„ç›¸ä¼¼ä½†åŠŸèƒ½ä¸Šä¸åŒçš„åŒºåŸŸã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶æ¥è¯†åˆ«ç©ºé—´è½¬å½•ç»„å­¦å’Œç»„ç»‡ç—…ç†å­¦å›¾åƒä¹‹é—´çš„å…³è”ã€‚</li>
<li>SENCA-stæ¨¡å‹åœ¨æ£€æµ‹è‚¿ç˜¤å¼‚è´¨æ€§å’Œè‚¿ç˜¤å¾®ç¯å¢ƒåŒºåŸŸæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f52e8923e6990f8930d4a42aae7e459d" align="middle">
<img src="https://picx.zhimg.com/v2-db4c2d3109aeac2a517a2df7bd903ed2" align="middle">
<img src="https://picx.zhimg.com/v2-ee3b7341059999a64d87936c57751b5b" align="middle">
<img src="https://picx.zhimg.com/v2-aa6a6a135d57a76e49096eca811db030" align="middle">
<img src="https://picx.zhimg.com/v2-4582aec3cf8ef86df2fe5d2da02a6314" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cross-pyramid-consistency-regularization-for-semi-supervised-medical-image-segmentation"><a href="#Cross-pyramid-consistency-regularization-for-semi-supervised-medical-image-segmentation" class="headerlink" title="Cross-pyramid consistency regularization for semi-supervised medical image segmentation"></a>Cross-pyramid consistency regularization for semi-supervised medical image segmentation</h2><p><strong>Authors:Matus Bojko, Maros Kollar, Marek Jakab, Wanda Benesova</strong></p>
<p>Semi-supervised learning (SSL) enables training of powerful models with the assumption of limited, carefully labelled data and a large amount of unlabeled data to support the learning. In this paper, we propose a hybrid consistency learning approach to effectively exploit unlabeled data for semi-supervised medical image segmentation by leveraging Cross-Pyramid Consistency Regularization (CPCR) between two decoders. First, we design a hybrid Dual Branch Pyramid Network (DBPNet), consisting of an encoder and two decoders that differ slightly, each producing a pyramid of perturbed auxiliary predictions across multiple resolution scales. Second, we present a learning strategy for this network named CPCR that combines existing consistency learning and uncertainty minimization approaches on the main output predictions of decoders with our novel regularization term. More specifically, in this term, we extend the soft-labeling setting to pyramid predictions across decoders to support knowledge distillation in deep hierarchical features. Experimental results show that DBPNet with CPCR outperforms five state-of-the-art self-supervised learning methods and has comparable performance with recent ones on a public benchmark dataset.</p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰èƒ½å¤Ÿåœ¨æœ‰é™ä¸”ç²¾å¿ƒæ ‡æ³¨çš„æ•°æ®å’Œå¤§é‡æ— æ ‡ç­¾æ•°æ®æ”¯æŒçš„å‡è®¾ä¸‹ï¼Œè®­ç»ƒå‡ºå¼ºå¤§çš„æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆä¸€è‡´æ€§å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ä¸¤ä¸ªè§£ç å™¨ä¹‹é—´çš„è·¨é‡‘å­—å¡”ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCPCRï¼‰ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®è¿›è¡ŒåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ··åˆåŒåˆ†æ”¯é‡‘å­—å¡”ç½‘ç»œï¼ˆDBPNetï¼‰ï¼Œå®ƒç”±ç•¥æœ‰ä¸åŒçš„ä¸€ä¸ªç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨ç»„æˆï¼Œæ¯ä¸ªè§£ç å™¨éƒ½å¯ä»¥äº§ç”Ÿä¸åŒåˆ†è¾¨ç‡çš„é‡‘å­—å¡”è¾…åŠ©é¢„æµ‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é’ˆå¯¹è¯¥ç½‘ç»œæå‡ºäº†ä¸€ç§åä¸ºCPCRçš„å­¦ä¹ ç­–ç•¥ï¼Œå®ƒå°†ç°æœ‰çš„ä¸€è‡´æ€§å­¦ä¹ å’Œä¸ç¡®å®šæ€§æœ€å°åŒ–æ–¹æ³•ç»“åˆåœ¨è§£ç å™¨çš„ä¸»è¦è¾“å‡ºé¢„æµ‹ä¸Šï¼Œå¹¶ä¸æˆ‘ä»¬æ–°é¢–çš„æ­£åˆ™åŒ–é¡¹ç›¸ç»“åˆã€‚æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨è¿™ä¸€é¡¹ä¸­ï¼Œæˆ‘ä»¬å°†è½¯æ ‡ç­¾è®¾ç½®æ‰©å±•åˆ°è§£ç å™¨ä¹‹é—´çš„é‡‘å­—å¡”é¢„æµ‹ï¼Œä»¥æ”¯æŒæ·±åº¦å±‚æ¬¡ç‰¹å¾ä¸­çš„çŸ¥è¯†è’¸é¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¸¦æœ‰CPCRçš„DBPNetä¼˜äºäº”ç§æœ€å…ˆè¿›çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå¹¶åœ¨å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸æœ€æ–°æ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08435v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŠç›‘ç£å­¦ä¹ ç”¨äºè®­ç»ƒåŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®å’Œå°‘é‡ä»”ç»†æ ‡è®°çš„æ•°æ®ï¼Œå®ç°æ›´é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆä¸€è‡´æ€§å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ä¸¤ä¸ªè§£ç å™¨ä¹‹é—´çš„è·¨é‡‘å­—å¡”ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCPCRï¼‰æ¥æœ‰æ•ˆåˆ©ç”¨æœªæ ‡è®°æ•°æ®ã€‚è®¾è®¡äº†ä¸€ç§æ··åˆåŒåˆ†æ”¯é‡‘å­—å¡”ç½‘ç»œï¼ˆDBPNetï¼‰ï¼Œå¹¶ç»“åˆç°æœ‰çš„ä¸€è‡´æ€§å­¦ä¹ æ–¹æ³•å’Œä¸ç¡®å®šæ€§æœ€å°åŒ–ç­–ç•¥ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCPCRçš„å­¦ä¹ ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDBPNetä¸CPCRç›¸æ¯”äº”ç§æœ€å…ˆè¿›çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå¹¶åœ¨å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸æœ€æ–°æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æœ‰é™æ ‡è®°æ•°æ®å’Œå¤§é‡æœªæ ‡è®°æ•°æ®ã€‚</li>
<li>æå‡ºäº†æ··åˆä¸€è‡´æ€§å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨è·¨é‡‘å­—å¡”ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCPCRï¼‰æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è®¾è®¡äº†æ··åˆåŒåˆ†æ”¯é‡‘å­—å¡”ç½‘ç»œï¼ˆDBPNetï¼‰ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç¼–ç å™¨åŠä¸¤ä¸ªè½»å¾®ä¸åŒçš„è§£ç å™¨ï¼Œäº§ç”Ÿå¤šä¸ªåˆ†è¾¨ç‡å°ºåº¦çš„æ‰°åŠ¨è¾…åŠ©é¢„æµ‹ã€‚</li>
<li>CPCRç»“åˆäº†ç°æœ‰çš„ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥å’Œä¸ç¡®å®šæ€§æœ€å°åŒ–æ–¹æ³•ï¼Œå¯¹è§£ç å™¨çš„ä¸»è¦è¾“å‡ºé¢„æµ‹è¿›è¡Œæ­£åˆ™åŒ–ã€‚</li>
<li>å°†è½¯æ ‡ç­¾è®¾ç½®æ‰©å±•åˆ°é‡‘å­—å¡”é¢„æµ‹ï¼Œä»¥æ”¯æŒæ·±åº¦å±‚æ¬¡ç‰¹å¾ä¸­çš„çŸ¥è¯†è’¸é¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDBPNetä¸CPCRåœ¨å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºäº”ç§æœ€å…ˆè¿›çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>æ‰€ææ–¹æ³•ä¸æœ€æ–°æ–¹æ³•çš„æ€§èƒ½ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a1f2165f7f5ea36bcd9d19328c5168a" align="middle">
<img src="https://picx.zhimg.com/v2-b47df7f5da4e8a3e5f9a3f4876a43fb6" align="middle">
<img src="https://picx.zhimg.com/v2-96af99855cedab8bf12169077f95b28f" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Impact-of-Longitudinal-Mammogram-Alignment-on-Breast-Cancer-Risk-Assessment"><a href="#The-Impact-of-Longitudinal-Mammogram-Alignment-on-Breast-Cancer-Risk-Assessment" class="headerlink" title="The Impact of Longitudinal Mammogram Alignment on Breast Cancer Risk Assessment"></a>The Impact of Longitudinal Mammogram Alignment on Breast Cancer Risk Assessment</h2><p><strong>Authors:Solveig Thrun, Stine Hansen, Zijun Sun, Nele Blum, Suaiba A. Salahuddin, Xin Wang, Kristoffer WickstrÃ¸m, Elisabeth Wetzer, Robert Jenssen, Maik Stille, Michael Kampffmeyer</strong></p>
<p>Regular mammography screening is crucial for early breast cancer detection. By leveraging deep learning-based risk models, screening intervals can be personalized, especially for high-risk individuals. While recent methods increasingly incorporate longitudinal information from prior mammograms, accurate spatial alignment across time points remains a key challenge. Misalignment can obscure meaningful tissue changes and degrade model performance. In this study, we provide insights into various alignment strategies, image-based registration, feature-level (representation space) alignment with and without regularization, and implicit alignment methods, for their effectiveness in longitudinal deep learning-based risk modeling. Using two large-scale mammography datasets, we assess each method across key metrics, including predictive accuracy, precision, recall, and deformation field quality.   Our results show that image-based registration consistently outperforms the more recently favored feature-based and implicit approaches across all metrics, enabling more accurate, temporally consistent predictions and generating smooth, anatomically plausible deformation fields. Although regularizing the deformation field improves deformation quality, it reduces the risk prediction performance of feature-level alignment. Applying image-based deformation fields within the feature space yields the best risk prediction performance.   These findings underscore the importance of image-based deformation fields for spatial alignment in longitudinal risk modeling, offering improved prediction accuracy and robustness. This approach has strong potential to enhance personalized screening and enable earlier interventions for high-risk individuals. The code is available at <a target="_blank" rel="noopener" href="https://github.com/sot176/Mammogram_Alignment_Study_Risk_Prediction.git">https://github.com/sot176/Mammogram_Alignment_Study_Risk_Prediction.git</a>, allowing full reproducibility of the results.</p>
<blockquote>
<p>è§„å¾‹æ€§çš„ä¹³è…ºé’¼é¶ç­›æŸ¥å¯¹äºæ—©æœŸä¹³è…ºç™Œçš„å‘ç°è‡³å…³é‡è¦ã€‚é€šè¿‡åˆ©ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„é£é™©æ¨¡å‹ï¼Œå¯ä»¥å®ç°ä¸ªæ€§åŒ–çš„ç­›æŸ¥é—´éš”ï¼Œå°¤å…¶é€‚ç”¨äºé«˜é£é™©ä¸ªä½“ã€‚è™½ç„¶è¿‘æœŸçš„æ–¹æ³•è¶Šæ¥è¶Šå¤šåœ°èå…¥äº†å…ˆå‰é’¼é¶æ‘„å½±çš„çºµå‘ä¿¡æ¯ï¼Œä½†æ—¶é—´ç‚¹ä¸Šå‡†ç¡®çš„ç©ºé—´å¯¹é½ä»æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚é”™ä½å¯èƒ½ä¼šæ©ç›–æœ‰æ„ä¹‰çš„ç»„ç»‡å˜åŒ–ï¼Œå¹¶é™ä½æ¨¡å‹æ€§èƒ½ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†å„ç§å¯¹é½ç­–ç•¥ï¼ŒåŒ…æ‹¬åŸºäºå›¾åƒçš„æ³¨å†Œã€ç‰¹å¾çº§ï¼ˆè¡¨ç¤ºç©ºé—´ï¼‰å¯¹é½ï¼ˆæœ‰&#x2F;æ— æ­£åˆ™åŒ–ï¼‰ä»¥åŠéšå¼å¯¹é½æ–¹æ³•ï¼Œå®ƒä»¬åœ¨åŸºäºæ·±åº¦å­¦ä¹ çš„çºµå‘é£é™©æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨ä¸¤ä¸ªå¤§è§„æ¨¡çš„é’¼é¶æ‘„å½±æ•°æ®é›†ï¼Œæˆ‘ä»¬æ ¹æ®å…³é”®æŒ‡æ ‡è¯„ä¼°äº†æ¯ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬é¢„æµ‹ç²¾åº¦ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œå˜å½¢åœºè´¨é‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºå›¾åƒçš„æ³¨å†Œåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºæœ€è¿‘æ›´å—æ¬¢è¿çš„åŸºäºç‰¹å¾å’Œéšå¼çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°æ›´ç²¾ç¡®ã€æ—¶é—´ä¸Šä¸€è‡´çš„é¢„æµ‹ï¼Œå¹¶ç”Ÿæˆå¹³æ»‘ã€è§£å‰–ä¸Šåˆç†çš„å˜å½¢åœºã€‚è™½ç„¶å¯¹å˜å½¢åœºè¿›è¡Œæ­£åˆ™åŒ–æé«˜äº†å˜å½¢è´¨é‡ï¼Œä½†å®ƒé™ä½äº†ç‰¹å¾çº§å¯¹é½çš„é£é™©é¢„æµ‹æ€§èƒ½ã€‚åœ¨ç‰¹å¾ç©ºé—´å†…åº”ç”¨åŸºäºå›¾åƒçš„å˜å½¢åœºå¯ä»¥è·å¾—æœ€ä½³çš„é£é™©é¢„æµ‹æ€§èƒ½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åŸºäºå›¾åƒçš„å˜å½¢åœºåœ¨çºµå‘é£é™©æ¨¡å‹ä¸­çš„ç©ºé—´å¯¹é½çš„é‡è¦æ€§ï¼Œæé«˜äº†é¢„æµ‹ç²¾åº¦å’Œç¨³å¥æ€§ã€‚è¿™ç§æ–¹æ³•æœ‰æœ›æé«˜ä¸ªæ€§åŒ–ç­›æŸ¥ï¼Œå¹¶ä¸ºé«˜é£é™©ä¸ªä½“æ›´æ—©åœ°é‡‡å–å¹²é¢„æªæ–½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sot176/Mammogram_Alignment_Study_Risk_Prediction.git%E4%B8%8A%E8%8E%B7%E5%8F%96%EF%BC%8C">https://github.com/sot176/Mammogram_Alignment_Study_Risk_Prediction.gitä¸Šè·å–ï¼Œ</a>ä»¥å®ç°ç»“æœçš„å®Œå…¨å¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08328v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯é’ˆå¯¹ä¹³è…ºç™Œçš„ç­›æŸ¥å’Œæ—©æœŸæ£€æµ‹è¿›è¡Œä¸ªæ€§åŒ–åˆ†æã€‚ç ”ç©¶é€šè¿‡ä¸åŒçš„çºµå‘å›¾åƒå¯¹é½ç­–ç•¥ï¼Œå¦‚å›¾åƒæ³¨å†Œã€ç‰¹å¾çº§åˆ«å¯¹é½å’Œéšå¼å¯¹é½æ–¹æ³•ï¼Œå‘ç°å›¾åƒåŸºç¡€çš„æ³¨å†Œæ–¹æ³•åœ¨é¢„æµ‹ç²¾åº¦ã€æ—¶é—´ä¸€è‡´æ€§å’Œç”Ÿæˆå¹³æ»‘å˜å½¢åœºæ–¹é¢è¡¨ç°æœ€ä¼˜ã€‚é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹é›†æˆæ­¤ç­–ç•¥èƒ½æ›´å‡†ç¡®åœ°è¯„ä¼°ä¹³è…ºç™Œé£é™©å¹¶æœ‰åŠ©äºæé«˜æ£€æµ‹å‡†ç¡®ç‡ã€‚è¿™å°†æœ‰åŠ©äºæé«˜ä¸ªäººåŒ–çš„ç­›æŸ¥é—´éš”é€‰æ‹©ä»¥åŠå¯¹é«˜å±äººç¾¤çš„æ—©æœŸå¹²é¢„èƒ½åŠ›ã€‚æœ‰å…³ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å¯ä¾›å¤åˆ¶å’ŒéªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æ·±åº¦å­¦ä¹ å¯¹ä¹³è…ºç™Œé£é™©è¿›è¡Œä¸ªæ€§åŒ–é¢„æµ‹çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶å¯¹ä¸åŒçš„çºµå‘å›¾åƒå¯¹é½ç­–ç•¥è¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ï¼ŒåŒ…æ‹¬å›¾åƒæ³¨å†Œã€ç‰¹å¾çº§åˆ«å¯¹é½å’Œéšå¼å¯¹é½æ–¹æ³•ã€‚</li>
<li>å›¾åƒåŸºç¡€çš„æ³¨å†Œæ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä¼˜ï¼ŒåŒ…æ‹¬é¢„æµ‹ç²¾åº¦ã€æ—¶é—´ä¸€è‡´æ€§ç­‰ã€‚</li>
<li>é›†æˆå›¾åƒåŸºç¡€çš„å˜å½¢åœºèƒ½æé«˜é£é™©é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å›¾åƒåŸºç¡€çš„å˜å½¢åœºåœ¨çºµå‘é£é™©æ¨¡å‹ä¸­çš„ç©ºé—´å¯¹é½ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶çš„æ–¹æ³•å…·æœ‰æé«˜ä¹³è…ºç™Œæ—©æœŸæ£€æµ‹å‡†ç¡®ç‡å’Œä¸ªæ€§åŒ–ç­›æŸ¥çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-797f38071bbe1bc283b909a25702a75a" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Federated-CLIP-for-Resource-Efficient-Heterogeneous-Medical-Image-Classification"><a href="#Federated-CLIP-for-Resource-Efficient-Heterogeneous-Medical-Image-Classification" class="headerlink" title="Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification"></a>Federated CLIP for Resource-Efficient Heterogeneous Medical Image Classification</h2><p><strong>Authors:Yihang Wu, Ahmad Chaddad</strong></p>
<p>Despite the remarkable performance of deep models in medical imaging, they still require source data for training, which limits their potential in light of privacy concerns. Federated learning (FL), as a decentralized learning framework that trains a shared model with multiple hospitals (a.k.a., FL clients), provides a feasible solution. However, data heterogeneity and resource costs hinder the deployment of FL models, especially when using vision language models (VLM). To address these challenges, we propose a novel contrastive language-image pre-training (CLIP) based FL approach for medical image classification (FedMedCLIP). Specifically, we introduce a masked feature adaptation module (FAM) as a communication module to reduce the communication load while freezing the CLIP encoders to reduce the computational overhead. Furthermore, we propose a masked multi-layer perceptron (MLP) as a private local classifier to adapt to the client tasks. Moreover, we design an adaptive Kullback-Leibler (KL) divergence-based distillation regularization method to enable mutual learning between FAM and MLP. Finally, we incorporate model compression to transmit the FAM parameters while using ensemble predictions for classification. Extensive experiments on four publicly available medical datasets demonstrate that our model provides feasible performance (e.g., 8% higher compared to second best baseline on ISIC2019) with reasonable resource cost (e.g., 120$\times$ faster than FedAVG).</p>
<blockquote>
<p>å°½ç®¡æ·±åº¦æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶éœ€è¦æºæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™è€ƒè™‘åˆ°éšç§é—®é¢˜ï¼Œé™åˆ¶äº†å…¶æ½œåŠ›ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä½œä¸ºä¸€ç§åˆ†æ•£å¼å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥ä¸å¤šå®¶åŒ»é™¢ï¼ˆå³FLå®¢æˆ·ç«¯ï¼‰å…±åŒè®­ç»ƒå…±äº«æ¨¡å‹ï¼Œä¸ºæ­¤æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ•°æ®å¼‚è´¨æ€§å’Œèµ„æºæˆæœ¬é˜»ç¢äº†è”é‚¦å­¦ä¹ æ¨¡å‹çš„éƒ¨ç½²ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ—¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ï¼ˆFedMedCLIPï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ©ç ç‰¹å¾é€‚é…æ¨¡å—ï¼ˆFAMï¼‰ä½œä¸ºé€šä¿¡æ¨¡å—ï¼Œä»¥å‡å°‘é€šä¿¡è´Ÿè½½ï¼ŒåŒæ—¶å†»ç»“CLIPç¼–ç å™¨ä»¥å‡å°‘è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ©ç å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä½œä¸ºç§æœ‰æœ¬åœ°åˆ†ç±»å™¨ï¼Œä»¥é€‚åº”å®¢æˆ·ç«¯ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºè‡ªé€‚åº”Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦çš„è’¸é¦æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥å®ç°FAMå’ŒMLPä¹‹é—´çš„ç›¸äº’å­¦ä¹ ã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆäº†æ¨¡å‹å‹ç¼©æ¥ä¼ è¾“FAMå‚æ•°ï¼ŒåŒæ—¶ä½¿ç”¨é›†æˆé¢„æµ‹è¿›è¡Œåˆ†ç±»ã€‚åœ¨å››ä¸ªå…¬å¼€çš„åŒ»å­¦æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæä¾›äº†å¯è¡Œçš„ç»“æœï¼ˆä¾‹å¦‚åœ¨ISIC2019ä¸Šæ¯”ç¬¬äºŒååŸºå‡†é«˜å‡º8%ï¼‰ï¼ŒåŒæ—¶èµ„æºæˆæœ¬åˆç†ï¼ˆä¾‹å¦‚æ¯”FedAVGå¿«120å€ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07929v1">PDF</a> Accepted in AAAI 2026 Main track. Code is available at <a target="_blank" rel="noopener" href="https://github.com/AIPMLab/FedMedCLIP">https://github.com/AIPMLab/FedMedCLIP</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®éšç§ã€æ•°æ®å¼‚è´¨æ€§å’Œèµ„æºæˆæœ¬é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„è”é‚¦å­¦ä¹ æ–¹æ¡ˆï¼ˆFedMedCLIPï¼‰ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¯¥æ–¹æ³•å¼•å…¥ç‰¹å¾é€‚åº”æ¨¡å—ï¼ˆFAMï¼‰å‡å°‘é€šä¿¡è´Ÿè½½å¹¶è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”KLæ•£åº¦è’¸é¦æ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œå®ç°äº†åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½èµ„æºæˆæœ¬çš„ç›®æ ‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªå…¬å¼€åŒ»å­¦æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ æ˜¯è§£å†³åŒ»å­¦æˆåƒä¸­æ•°æ®éšç§é—®é¢˜çš„å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ•°æ®å¼‚è´¨æ€§å’Œèµ„æºæˆæœ¬æ˜¯éƒ¨ç½²è”é‚¦å­¦ä¹ æ¨¡å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åŸºäºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„è”é‚¦å­¦ä¹ æ–¹æ¡ˆï¼ˆFedMedCLIPï¼‰ç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ã€‚</li>
<li>FedMedCLIPé€šè¿‡å¼•å…¥ç‰¹å¾é€‚åº”æ¨¡å—ï¼ˆFAMï¼‰å‡å°‘é€šä¿¡è´Ÿè½½å¹¶å†»ç»“CLIPç¼–ç å™¨ä»¥é™ä½è®¡ç®—å¼€é”€ã€‚</li>
<li>è®¾è®¡äº†è‡ªé€‚åº”KLæ•£åº¦è’¸é¦æ­£åˆ™åŒ–çš„æ–¹æ³•ä»¥å®ç°ç‰¹å¾é€‚åº”æ¨¡å—ï¼ˆFAMï¼‰å’Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¹‹é—´çš„äº’å­¦ä¹ ã€‚</li>
<li>é‡‡ç”¨äº†æ¨¡å‹å‹ç¼©æŠ€æœ¯ä»¥ä¼ è¾“FAMå‚æ•°å¹¶åˆ©ç”¨é›†æˆé¢„æµ‹è¿›è¡Œåˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07929">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f25881300734258c83224ba52d396517" align="middle">
<img src="https://picx.zhimg.com/v2-366eacc007cafbc7aead7ce318c3ceee" align="middle">
<img src="https://picx.zhimg.com/v2-ff9bcfbe3a12b236fc223f216b5880f2" align="middle">
<img src="https://picx.zhimg.com/v2-62ed7d29a16af27b7871b12cadd9dd95" align="middle">
<img src="https://picx.zhimg.com/v2-cc9008813795224cd97d900601ff3e6e" align="middle">
<img src="https://picx.zhimg.com/v2-430cdb496c866f82b914f6c79b0510bb" align="middle">
<img src="https://picx.zhimg.com/v2-021d82a3e91ac246a81907344e350fb6" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cancer-Net-PCa-MultiSeg-Multimodal-Enhancement-of-Prostate-Cancer-Lesion-Segmentation-Using-Synthetic-Correlated-Diffusion-Imaging"><a href="#Cancer-Net-PCa-MultiSeg-Multimodal-Enhancement-of-Prostate-Cancer-Lesion-Segmentation-Using-Synthetic-Correlated-Diffusion-Imaging" class="headerlink" title="Cancer-Net PCa-MultiSeg: Multimodal Enhancement of Prostate Cancer Lesion Segmentation Using Synthetic Correlated Diffusion Imaging"></a>Cancer-Net PCa-MultiSeg: Multimodal Enhancement of Prostate Cancer Lesion Segmentation Using Synthetic Correlated Diffusion Imaging</h2><p><strong>Authors:Jarett Dewbury, Chi-en Amy Tai, Alexander Wong</strong></p>
<p>Current deep learning approaches for prostate cancer lesion segmentation achieve limited performance, with Dice scores of 0.32 or lower in large patient cohorts. To address this limitation, we investigate synthetic correlated diffusion imaging (CDI$^s$) as an enhancement to standard diffusion-based protocols. We conduct a comprehensive evaluation across six state-of-the-art segmentation architectures using 200 patients with co-registered CDI$^s$, diffusion-weighted imaging (DWI) and apparent diffusion coefficient (ADC) sequences. We demonstrate that CDI$^s$ integration reliably enhances or preserves segmentation performance in 94% of evaluated configurations, with individual architectures achieving up to 72.5% statistically significant relative improvement over baseline modalities. CDI$^s$ + DWI emerges as the safest enhancement pathway, achieving significant improvements in half of evaluated architectures with zero instances of degradation. Since CDI$^s$ derives from existing DWI acquisitions without requiring additional scan time or architectural modifications, it enables immediate deployment in clinical workflows. Our results establish validated integration pathways for CDI$^s$ as a practical drop-in enhancement for PCa lesion segmentation tasks across diverse deep learning architectures.</p>
<blockquote>
<p>å½“å‰æ·±åº¦å­¦ä¹ åœ¨å‰åˆ—è…ºç™Œç—…ç¶åˆ†å‰²æ–¹é¢çš„è¡¨ç°æœ‰é™ï¼Œåœ¨å¤§è§„æ¨¡æ‚£è€…é˜Ÿåˆ—ä¸­çš„Diceå¾—åˆ†ä½äºæˆ–ç­‰äº0.32ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI$^s$ï¼‰ä½œä¸ºåŸºäºæ ‡å‡†æ‰©æ•£åè®®çš„å¢å¼ºæ‰‹æ®µã€‚æˆ‘ä»¬ä½¿ç”¨äº†200ä½æ‚£è€…çš„æ•°æ®è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œè¿™äº›æ‚£è€…éƒ½æœ‰å…±æ³¨å†Œçš„CDI$^s$ã€æ‰©æ•£åŠ æƒæˆåƒï¼ˆDWIï¼‰å’Œè¡¨è§‚æ‰©æ•£ç³»æ•°ï¼ˆADCï¼‰åºåˆ—ã€‚ç ”ç©¶æ¶µç›–äº†å…­ç§æœ€å…ˆè¿›çš„åˆ†å‰²æ¶æ„ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨94%çš„è¯„ä¼°é…ç½®ä¸­ï¼ŒCDI$^s$çš„é›†æˆå¯é åœ°å¢å¼ºäº†æˆ–ä¿æŒäº†åˆ†å‰²æ€§èƒ½ï¼Œä¸ªåˆ«æ¶æ„ç›¸å¯¹äºåŸºçº¿æ¨¡å¼å®ç°äº†é«˜è¾¾72.5%çš„ç»Ÿè®¡æ˜¾è‘—ç›¸å¯¹æ”¹å–„ã€‚CDI$^s$+DWIè¢«è®¤ä¸ºæ˜¯æœ€å®‰å…¨çš„å¢å¼ºé€”å¾„ï¼Œåœ¨è¯„ä¼°çš„ä¸€åŠæ¶æ„ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä¸”æ²¡æœ‰å‡ºç°æ€§èƒ½ä¸‹é™çš„æƒ…å†µã€‚ç”±äºCDI$^s$æºäºç°æœ‰çš„DWIé‡‡é›†ï¼Œæ— éœ€é¢å¤–çš„æ‰«ææ—¶é—´æˆ–æ¶æ„ä¿®æ”¹ï¼Œå› æ­¤å¯ä»¥ç«‹å³éƒ¨ç½²åœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ã€‚æˆ‘ä»¬çš„ç»“æœéªŒè¯äº†CDI$^s$ä½œä¸ºå¤šç§æ·±åº¦å­¦ä¹ æ¶æ„ä¸­PCaç—…ç¶åˆ†å‰²ä»»åŠ¡çš„å®ç”¨å³æ—¶å¢å¼ºæ–¹æ³•çš„é›†æˆé€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07816v1">PDF</a> Accepted at ML4H 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢ç©¶äº†åˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI^sï¼‰å¯¹å‰åˆ—è…ºç™Œç—…ç¶åˆ†å‰²çš„æ·±åº¦å­¦ä¹ çš„æ€§èƒ½æå‡æ•ˆæœã€‚ç ”ç©¶è¯„ä¼°äº†å…­ç§æœ€å…ˆè¿›çš„åˆ†å‰²æ¶æ„ï¼Œæ¶‰åŠ200åæ‚£è€…çš„CDI^sã€æ‰©æ•£åŠ æƒæˆåƒï¼ˆDWIï¼‰å’Œè¡¨è§‚æ‰©æ•£ç³»æ•°ï¼ˆADCï¼‰åºåˆ—ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒCDI^sé›†æˆåœ¨94%çš„è¯„ä¼°é…ç½®ä¸­å¯é åœ°æå‡äº†æˆ–ä¿æŒäº†åˆ†å‰²æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æŸäº›æ¶æ„ä¸­ç›¸å¯¹äºåŸºå‡†æ¨¡æ€å®ç°äº†é«˜è¾¾72.5%çš„ç»Ÿè®¡å­¦æ˜¾è‘—ç›¸å¯¹æ”¹è¿›ã€‚ç”±äºCDI^sæºäºç°æœ‰çš„DWIé‡‡é›†ï¼Œæ— éœ€é¢å¤–çš„æ‰«ææ—¶é—´æˆ–æ¶æ„ä¿®æ”¹ï¼Œå› æ­¤å¯ç«‹å³éƒ¨ç½²åœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ã€‚æœ¬ç ”ç©¶ç»“æœä¸ºCDI^sä½œä¸ºå®ç”¨å³æ’å³ç”¨å¢å¼ºåŠŸèƒ½åœ¨å¤šç§æ·±åº¦å­¦ä¹ æ¶æ„ä¸­çš„å‰åˆ—è…ºç™Œç—…ç¶åˆ†å‰²ä»»åŠ¡ä¸­çš„é›†æˆé€”å¾„æä¾›äº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†åˆæˆç›¸å…³æ‰©æ•£æˆåƒï¼ˆCDI^sï¼‰å¯¹å‰åˆ—è…ºç™Œç—…ç¶åˆ†å‰²æ·±åº¦å­¦ä¹ çš„æ€§èƒ½å½±å“ã€‚</li>
<li>åœ¨å¤§å‹æ‚£è€…é˜Ÿåˆ—ä¸­ï¼Œå½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•æ€§èƒ½æœ‰é™ï¼ŒDiceåˆ†æ•°ä½äº0.32ã€‚</li>
<li>é€šè¿‡é›†æˆCDI^sï¼Œç ”ç©¶å‘ç°åœ¨94%çš„è¯„ä¼°é…ç½®ä¸­ï¼Œåˆ†å‰²æ€§èƒ½æœ‰æ‰€æå‡æˆ–ä¿æŒä¸å˜ã€‚</li>
<li>åœ¨æŸäº›æ¶æ„ä¸­ï¼Œä¸åŸºçº¿æ¨¡æ€ç›¸æ¯”ï¼ŒCDI^sé›†æˆå®ç°äº†é«˜è¾¾72.5%çš„æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>CDI^sä¸DWIç»“åˆè¡¨ç°å‡ºæœ€å®‰å…¨çš„æ•ˆæœï¼Œåœ¨å¤šä¸ªæ¶æ„ä¸­å®ç°äº†æ”¹è¿›ï¼Œæ²¡æœ‰æ€§èƒ½ä¸‹é™çš„æƒ…å†µã€‚</li>
<li>CDI^sæºäºç°æœ‰çš„DWIé‡‡é›†ï¼Œæ— éœ€é¢å¤–çš„æ‰«ææ—¶é—´æˆ–å¤æ‚çš„æ¶æ„ä¿®æ”¹ï¼Œå¯è½»æ¾èå…¥å½“å‰ä¸´åºŠå·¥ä½œæµç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3361135ed266ba941c24a83e44d8cf98" align="middle">
<img src="https://picx.zhimg.com/v2-df05252dc6d5e424b96957416e73ef04" align="middle">
<img src="https://picx.zhimg.com/v2-870324acf05a55790f8157f85979d0b8" align="middle">
<img src="https://picx.zhimg.com/v2-41903a77a889f24b6df70ad01a8cb9ed" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Class-Incremental-Medical-Image-Segmentation-via-Prototype-Guided-Calibration-and-Dual-Aligned-Distillation"><a href="#Class-Incremental-Medical-Image-Segmentation-via-Prototype-Guided-Calibration-and-Dual-Aligned-Distillation" class="headerlink" title="Class Incremental Medical Image Segmentation via Prototype-Guided Calibration and Dual-Aligned Distillation"></a>Class Incremental Medical Image Segmentation via Prototype-Guided Calibration and Dual-Aligned Distillation</h2><p><strong>Authors:Shengqian Zhu, Chengrong Yu, Qiang Wang, Ying Song, Guangjun Li, Jiafei Wu, Xiaogang Xu, Zhang Yi, Junjie Hu</strong></p>
<p>Class incremental medical image segmentation (CIMIS) aims to preserve knowledge of previously learned classes while learning new ones without relying on old-class labels. However, existing methods 1) either adopt one-size-fits-all strategies that treat all spatial regions and feature channels equally, which may hinder the preservation of accurate old knowledge, 2) or focus solely on aligning local prototypes with global ones for old classes while overlooking their local representations in new data, leading to knowledge degradation. To mitigate the above issues, we propose Prototype-Guided Calibration Distillation (PGCD) and Dual-Aligned Prototype Distillation (DAPD) for CIMIS in this paper. Specifically, PGCD exploits prototype-to-feature similarity to calibrate class-specific distillation intensity in different spatial regions, effectively reinforcing reliable old knowledge and suppressing misleading information from old classes. Complementarily, DAPD aligns the local prototypes of old classes extracted from the current model with both global prototypes and local prototypes, further enhancing segmentation performance on old categories. Comprehensive evaluations on two widely used multi-organ segmentation benchmarks demonstrate that our method outperforms state-of-the-art methods, highlighting its robustness and generalization capabilities.</p>
<blockquote>
<p>ç±»å¢é‡åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCIMISï¼‰æ—¨åœ¨ä¿ç•™å…ˆå‰å­¦ä¹ ç±»åˆ«çš„çŸ¥è¯†ï¼Œåœ¨å­¦ä¹ æ–°ç±»åˆ«æ—¶ä¸éœ€è¦ä¾èµ–æ—§ç±»åˆ«çš„æ ‡ç­¾ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•1ï¼‰é‡‡ç”¨ä¸€åˆ€åˆ‡ç­–ç•¥ï¼Œå¹³ç­‰å¯¹å¾…æ‰€æœ‰ç©ºé—´åŒºåŸŸå’Œç‰¹å¾é€šé“ï¼Œè¿™å¯èƒ½é˜»ç¢å‡†ç¡®æ—§çŸ¥è¯†çš„ä¿ç•™ï¼›2ï¼‰æˆ–è€…åªå…³æ³¨å±€éƒ¨åŸå‹ä¸æ—§ç±»çš„å…¨å±€åŸå‹å¯¹é½ï¼Œè€Œå¿½è§†æ–°æ•°æ®ä¸­å®ƒä»¬çš„å±€éƒ¨è¡¨ç¤ºï¼Œå¯¼è‡´çŸ¥è¯†é€€åŒ–ã€‚ä¸ºäº†ç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç”¨äºCIMISçš„åŸå‹å¼•å¯¼æ ¡å‡†è’¸é¦ï¼ˆPGCDï¼‰å’ŒåŒå¯¹é½åŸå‹è’¸é¦ï¼ˆDAPDï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒPGCDåˆ©ç”¨åŸå‹åˆ°ç‰¹å¾çš„ç›¸ä¼¼æ€§æ¥æ ¡å‡†ä¸åŒç©ºé—´åŒºåŸŸå†…çš„ç±»ç‰¹å®šè’¸é¦å¼ºåº¦ï¼Œæœ‰æ•ˆåœ°å¼ºåŒ–äº†å¯é çš„æ—§çŸ¥è¯†ï¼ŒæŠ‘åˆ¶äº†æ—§ç±»çš„è¯¯å¯¼ä¿¡æ¯ã€‚ä½œä¸ºè¡¥å……ï¼ŒDAPDå°†å½“å‰æ¨¡å‹ä¸­æ—§ç±»çš„å±€éƒ¨åŸå‹ä¸å…¨å±€åŸå‹å’Œå±€éƒ¨åŸå‹å¯¹é½ï¼Œè¿›ä¸€æ­¥æé«˜æ—§ç±»åˆ«çš„åˆ†å‰²æ€§èƒ½ã€‚åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¤šå™¨å®˜åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œçªå‡ºäº†å…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07749v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ç±»å¢é‡å­¦ä¹ é—®é¢˜ï¼ˆCIMISï¼‰çš„è§£å†³æ–¹æ¡ˆã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†åŸå‹å¼•å¯¼æ ¡å‡†è’¸é¦ï¼ˆPGCDï¼‰å’ŒåŒå¯¹é½åŸå‹è’¸é¦ï¼ˆDAPDï¼‰ã€‚å‰è€…åˆ©ç”¨åŸå‹ä¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œåœ¨ä¸åŒç©ºé—´åŒºåŸŸæ ¡å‡†ç±»ç‰¹å®šçš„è’¸é¦å¼ºåº¦ï¼Œå¼ºåŒ–æ—§çŸ¥è¯†å¹¶æŠ‘åˆ¶è¯¯å¯¼ä¿¡æ¯ã€‚åè€…åˆ™å¯¹é½æ—§ç±»çš„å±€éƒ¨åŸå‹ï¼Œè¿›ä¸€æ­¥æé«˜æ—§ç±»åˆ«çš„åˆ†å‰²æ€§èƒ½ã€‚ç»å¤šå™¨å®˜åˆ†å‰²åŸºå‡†æµ‹è¯•éªŒè¯ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œè¡¨ç°å‡ºç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Class incremental medical image segmentation (CIMIS) æ—¨åœ¨å­¦ä¹ æ–°ç±»æ—¶ä¿ç•™æ—§ç±»çš„çŸ¥è¯†ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼šé‡‡ç”¨ä¸€åˆ€åˆ‡ç­–ç•¥æˆ–è¿‡åˆ†å…³æ³¨å±€éƒ¨åŸå‹ä¸å…¨å±€çš„å¯¹é½ï¼Œå¿½è§†æ–°æ•°æ®ä¸­çš„å±€éƒ¨è¡¨ç¤ºã€‚</li>
<li>åŸå‹å¼•å¯¼æ ¡å‡†è’¸é¦ï¼ˆPGCDï¼‰åˆ©ç”¨åŸå‹ä¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œåœ¨ä¸åŒç©ºé—´åŒºåŸŸæ ¡å‡†ç±»ç‰¹å®šçš„è’¸é¦å¼ºåº¦ã€‚</li>
<li>åŒå¯¹é½åŸå‹è’¸é¦ï¼ˆDAPDï¼‰å¯¹é½æ—§ç±»çš„å±€éƒ¨åŸå‹ä¸å…¨å±€åŸå‹ï¼Œè¿›ä¸€æ­¥æé«˜æ—§ç±»åˆ«çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•é€šè¿‡å¼ºåŒ–æ—§çŸ¥è¯†å’ŒæŠ‘åˆ¶è¯¯å¯¼ä¿¡æ¯ï¼Œå®ç°å¯¹å¯é æ—§çŸ¥è¯†çš„æœ‰æ•ˆä¿ç•™ã€‚</li>
<li>åœ¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¤šå™¨å®˜åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºå½“å‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02dbeaee1446fc34b5c3bed0dd6fe307" align="middle">
<img src="https://picx.zhimg.com/v2-85b5415e5788a8a1182e4187fd1d4659" align="middle">
<img src="https://picx.zhimg.com/v2-4d892bb0b46b9af55394c3b26cb0cc60" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Integrating-Epigenetic-and-Phenotypic-Features-for-Biological-Age-Estimation-in-Cancer-Patients-via-Multimodal-Learning"><a href="#Integrating-Epigenetic-and-Phenotypic-Features-for-Biological-Age-Estimation-in-Cancer-Patients-via-Multimodal-Learning" class="headerlink" title="Integrating Epigenetic and Phenotypic Features for Biological Age Estimation in Cancer Patients via Multimodal Learning"></a>Integrating Epigenetic and Phenotypic Features for Biological Age Estimation in Cancer Patients via Multimodal Learning</h2><p><strong>Authors:Shuyue Jiang, Wenjing Ma, Shaojun Yu, Chang Su, Runze Yan, Jiaying Lu</strong></p>
<p>Biological age, which may be older or younger than chronological age due to factors such as genetic predisposition, environmental exposures, serves as a meaningful biomarker of aging processes and can inform risk stratification, treatment planning, and survivorship care in cancer patients. We propose EpiCAge, a multimodal framework that integrates epigenetic and phenotypic data to improve biological age prediction. Evaluated on eight internal and four external cancer cohorts, EpiCAge consistently outperforms existing epigenetic and phenotypic age clocks. Our analyses show that EpiCAge identifies biologically relevant markers, and its derived age acceleration is significantly associated with mortality risk. These results highlight EpiCAge as a promising multimodal machine learning tool for biological age assessment in oncology.</p>
<blockquote>
<p>ç”Ÿç‰©å¹´é¾„å¯èƒ½å› é—ä¼ å€¾å‘ã€ç¯å¢ƒæš´éœ²ç­‰å› ç´ è€Œé«˜äºæˆ–ä½äºå®é™…å¹´é¾„ã€‚ç”Ÿç‰©å¹´é¾„æ˜¯è¡°è€è¿‡ç¨‹çš„é‡è¦ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œèƒ½ä¸ºç™Œç—‡æ‚£è€…çš„é£é™©è¯„ä¼°ã€æ²»ç–—è®¡åˆ’å’Œç”Ÿå­˜æŠ¤ç†æä¾›ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†EpiCAgeï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å¼æ¡†æ¶ï¼Œèåˆäº†è¡¨è§‚é—ä¼ å’Œè¡¨å‹æ•°æ®ï¼Œä»¥æé«˜ç”Ÿç‰©å¹´é¾„çš„é¢„æµ‹èƒ½åŠ›ã€‚åœ¨å…«ä¸ªå†…éƒ¨å’Œå››ä¸ªå¤–éƒ¨ç™Œç—‡é˜Ÿåˆ—ä¸­è¿›è¡Œè¯„ä¼°ï¼ŒEpiCAgeå§‹ç»ˆä¼˜äºç°æœ‰çš„è¡¨è§‚é—ä¼ å’Œè¡¨å‹å¹´é¾„æ—¶é’Ÿã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼ŒEpiCAgeèƒ½å¤Ÿè¯†åˆ«å‡ºå…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„æ ‡è®°ç‰©ï¼Œå…¶è¡ç”Ÿçš„å¹´é¾„åŠ é€Ÿä¸æ­»äº¡é£é™©æœ‰æ˜¾ç€å…³è”ã€‚è¿™äº›ç»“æœçªå‡ºäº†EpiCAgeåœ¨è‚¿ç˜¤ç”Ÿç‰©å­¦å¹´é¾„è¯„ä¼°ä¸­å…·æœ‰å‰æ™¯çš„å¤šæ¨¡å¼æœºå™¨å­¦ä¹ å·¥å…·çš„åœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07219v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºEpiCAgeçš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ•´åˆè¡¨è§‚é—ä¼ å’Œè¡¨å‹æ•°æ®ä»¥æ”¹è¿›ç”Ÿç‰©å¹´é¾„é¢„æµ‹ã€‚é€šè¿‡è¯„ä¼°å¤šä¸ªç™Œç—‡é˜Ÿåˆ—ï¼Œå‘ç°EpiCAgeæ¯”ç°æœ‰çš„å…¶ä»–é¢„æµ‹å·¥å…·è¡¨ç°æ›´ä¼˜ç§€ï¼Œä¸”èƒ½å¤Ÿè¯†åˆ«ä¸ç”Ÿç‰©å­¦ç›¸å…³çš„æ ‡å¿—ç‰©ï¼Œå…¶è¡ç”Ÿå‡ºçš„å¹´é¾„åŠ é€Ÿä¸æ­»äº¡é£é™©æ˜¾è‘—ç›¸å…³ã€‚è¿™è¡¨æ˜EpiCAgeåœ¨è‚¿ç˜¤ç”Ÿç‰©å­¦å¹´é¾„è¯„ä¼°ä¸­å…·æœ‰å¹¿é˜”åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿç‰©å¹´é¾„æ˜¯åæ˜ è¡°è€è¿‡ç¨‹çš„é‡è¦ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œå¯åº”ç”¨äºç™Œç—‡æ‚£è€…çš„é£é™©è¯„ä¼°ã€æ²»ç–—è®¡åˆ’å’Œç”Ÿå­˜æŠ¤ç†ã€‚</li>
<li>EpiCAgeæ˜¯ä¸€ç§å¤šæ¨¡å¼æ¡†æ¶ï¼Œç»“åˆäº†è¡¨è§‚é—ä¼ å’Œè¡¨å‹æ•°æ®ï¼Œæ—¨åœ¨æé«˜ç”Ÿç‰©å¹´é¾„çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>EpiCAgeåœ¨å†…éƒ¨å’Œå¤–éƒ¨ç™Œç—‡é˜Ÿåˆ—ä¸­çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„ç”Ÿç‰©å¹´é¾„é¢„æµ‹å·¥å…·ã€‚</li>
<li>EpiCAgeèƒ½å¤Ÿè¯†åˆ«ç”Ÿç‰©å­¦ç›¸å…³çš„æ ‡å¿—ç‰©ï¼Œè¿™äº›æ ‡å¿—ç‰©å¯¹äºç†è§£ç™Œç—‡å’Œå…¶ä»–ç–¾ç—…çš„å‘å±•è¿‡ç¨‹å¯èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>EpiCAgeçš„è¡ç”Ÿå¹´é¾„åŠ é€Ÿä¸æ­»äº¡é£é™©æ˜¾è‘—ç›¸å…³ï¼Œæç¤ºå…¶åœ¨è¯„ä¼°æ‚£è€…é¢„åå’Œåˆ¶å®šä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆä¸­çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
<li>EpiCAgeå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯èƒ½ç”¨äºè‚¿ç˜¤ç”Ÿç‰©å­¦ã€æµè¡Œç—…å­¦å’Œå…¶ä»–ç›¸å…³é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b1d42a4b1697269a3be8c5b99b5e94c" align="middle">
<img src="https://picx.zhimg.com/v2-09d20ef52e2dfd6e9e3545205a005d99" align="middle">
<img src="https://picx.zhimg.com/v2-192abd12d7ef53765a7d89b9962d2d43" align="middle">
<img src="https://picx.zhimg.com/v2-be8eda01c02ffc482bfbd61d66ffa06a" align="middle">
<img src="https://picx.zhimg.com/v2-06d113fc9d9efe5daaf3599c3e11ecd2" align="middle">
<img src="https://picx.zhimg.com/v2-4bae9f6c780896a03cbd3febdb7a9cf7" align="middle">
<img src="https://picx.zhimg.com/v2-ea5cb8035e9794930ecc0d427faab997" align="middle">
<img src="https://picx.zhimg.com/v2-f64d78a118006f10de1e79ad6bb57a65" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Ambiguity-aware-Truncated-Flow-Matching-for-Ambiguous-Medical-Image-Segmentation"><a href="#Ambiguity-aware-Truncated-Flow-Matching-for-Ambiguous-Medical-Image-Segmentation" class="headerlink" title="Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation"></a>Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation</h2><p><strong>Authors:Fanding Li, Xiangyu Li, Xianghe Su, Xingyu Qiu, Suyu Dong, Wei Wang, Kuanquan Wang, Gongning Luo, Shuo Li</strong></p>
<p>A simultaneous enhancement of accuracy and diversity of predictions remains a challenge in ambiguous medical image segmentation (AMIS) due to the inherent trade-offs. While truncated diffusion probabilistic models (TDPMs) hold strong potential with a paradigm optimization, existing TDPMs suffer from entangled accuracy and diversity of predictions with insufficient fidelity and plausibility. To address the aforementioned challenges, we propose Ambiguity-aware Truncated Flow Matching (ATFM), which introduces a novel inference paradigm and dedicated model components. Firstly, we propose Data-Hierarchical Inference, a redefinition of AMIS-specific inference paradigm, which enhances accuracy and diversity at data-distribution and data-sample level, respectively, for an effective disentanglement. Secondly, Gaussian Truncation Representation (GTR) is introduced to enhance both fidelity of predictions and reliability of truncation distribution, by explicitly modeling it as a Gaussian distribution at $T_{\text{trunc}}$ instead of using sampling-based approximations.Thirdly, Segmentation Flow Matching (SFM) is proposed to enhance the plausibility of diverse predictions by extending semantic-aware flow transformation in Flow Matching (FM). Comprehensive evaluations on LIDC and ISIC3 datasets demonstrate that ATFM outperforms SOTA methods and simultaneously achieves a more efficient inference. ATFM improves GED and HM-IoU by up to $12%$ and $7.3%$ compared to advanced methods.</p>
<blockquote>
<p>åœ¨æ¨¡ç³ŠåŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆAMISï¼‰ä¸­ï¼Œç”±äºå›ºæœ‰çš„æƒè¡¡ï¼Œé¢„æµ‹å‡†ç¡®æ€§å’Œå¤šæ ·æ€§çš„åŒæ—¶æé«˜ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è™½ç„¶æˆªæ–­æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆTDPMï¼‰åœ¨èŒƒå¼ä¼˜åŒ–æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰çš„TDPMåœ¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€å¤šæ ·æ€§å’Œå¯ä¿¡åº¦æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡ç³Šæ„ŸçŸ¥æˆªæ–­æµåŒ¹é…ï¼ˆATFMï¼‰ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§æ–°çš„æ¨ç†èŒƒå¼å’Œä¸“é—¨çš„æ¨¡å‹ç»„ä»¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†æ•°æ®åˆ†å±‚æ¨ç†ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹AMISçš„ç‰¹å®šæ¨ç†èŒƒå¼çš„é‡æ–°å®šä¹‰ï¼Œåˆ†åˆ«åœ¨æ•°æ®åˆ†å¸ƒå’Œæ ·æœ¬å±‚é¢æé«˜äº†å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ï¼Œä»¥å®ç°æœ‰æ•ˆçš„è§£çº ç¼ ã€‚å…¶æ¬¡ï¼Œå¼•å…¥é«˜æ–¯æˆªæ–­è¡¨ç¤ºï¼ˆGTRï¼‰ï¼Œé€šè¿‡å°†æˆªæ–­åˆ†å¸ƒæ˜¾å¼å»ºæ¨¡ä¸ºæˆªæ–­æ—¶é—´Ttçš„é«˜æ–¯åˆ†å¸ƒï¼Œè€Œä¸æ˜¯ä½¿ç”¨åŸºäºé‡‡æ ·çš„è¿‘ä¼¼å€¼ï¼Œä»è€Œæé«˜é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæˆªæ–­åˆ†å¸ƒçš„å¯ä¿¡åº¦ã€‚ç¬¬ä¸‰ï¼Œæå‡ºäº†åˆ†å‰²æµåŒ¹é…ï¼ˆSFMï¼‰ï¼Œé€šè¿‡æ‰©å±•æµåŒ¹é…ï¼ˆFMï¼‰ä¸­çš„è¯­ä¹‰æ„ŸçŸ¥æµå˜æ¢ï¼Œä»¥æé«˜å¤šæ ·é¢„æµ‹çš„å¯ä¿¡æ€§ã€‚åœ¨LIDCå’ŒISIC3æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒATFMä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶å®ç°äº†æ›´æœ‰æ•ˆçš„æ¨ç†ã€‚ä¸é«˜çº§æ–¹æ³•ç›¸æ¯”ï¼ŒATFMåœ¨GEDå’ŒHM-IoUæ–¹é¢æé«˜äº†é«˜è¾¾12%å’Œ7.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06857v1">PDF</a> 13 pages, 10 figures, extended version of AAAI-26 paper</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹å…·æœ‰å†…åœ¨çŸ›ç›¾çš„åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜æå‡ºäº†æŒ‘æˆ˜æ€§é—®é¢˜ï¼šåœ¨åˆ†å‰²å…·æœ‰ä¸ç¡®å®šæ€§çš„åŒ»å­¦å›¾åƒæ—¶å¦‚ä½•åŒæ—¶æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§åŠå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³æ­¤æŒ‘æˆ˜ï¼Œå¼•å…¥äº†ä¸€ç§å…¨æ–°çš„æ¨ç†æ¡†æ¶å’Œç›¸å…³æ¨¡å‹ç»„ä»¶æ„å»ºâ€œæ­§ä¹‰æ„ŸçŸ¥æˆªæ–­æµåŒ¹é…â€ï¼ˆATFMï¼‰ã€‚é€šè¿‡æ•°æ®å±‚æ¬¡æ¨ç†æ–¹æ³•ï¼Œæé«˜äº†é¢„æµ‹å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ï¼›é€šè¿‡é«˜æ–¯æˆªæ–­è¡¨ç¤ºæ³•ï¼Œæé«˜äº†é¢„æµ‹å‡†ç¡®æ€§å’Œæˆªæ–­åˆ†å¸ƒçš„å¯é æ€§ï¼›é€šè¿‡åˆ†å‰²æµåŒ¹é…æŠ€æœ¯ï¼Œå¢å¼ºäº†é¢„æµ‹å¤šæ ·æ€§çš„åˆç†æ€§ã€‚åœ¨LIDCå’ŒISIC3æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒATFMåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜æ•ˆæ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆAMISï¼‰é¢ä¸´ç€æé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œå¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æˆªæ–­æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆTDPMï¼‰ä»å­˜åœ¨ä¸€å®šç¼ºé™·ã€‚</li>
<li>æå‡ºçš„æ­§ä¹‰æ„ŸçŸ¥æˆªæ–­æµåŒ¹é…ï¼ˆATFMï¼‰åŒ…å«æ–°é¢–æ¨ç†æ¡†æ¶å’Œç‰¹å®šæ¨¡å‹ç»„ä»¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ•°æ®å±‚æ¬¡æ¨ç†æ–¹æ³•å¢å¼ºé¢„æµ‹å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>é«˜æ–¯æˆªæ–­è¡¨ç¤ºæ³•æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§å’Œæˆªæ–­åˆ†å¸ƒçš„å¯é æ€§ã€‚</li>
<li>åˆ†å‰²æµåŒ¹é…æŠ€æœ¯å¢å¼ºäº†é¢„æµ‹å¤šæ ·æ€§çš„åˆç†æ€§ã€‚</li>
<li>åœ¨LIDCå’ŒISIC3æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒATFMåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜å…¨å±€ç¼–è¾‘è·ç¦»å’Œåˆ†å±‚äº¤å¹¶æ¯”æŒ‡æ ‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-528d7f32d10a42ee4421a4d8731f9f30" align="middle">
<img src="https://picx.zhimg.com/v2-dedbc1b10d4fb74c32b3e5a422990eb6" align="middle">
<img src="https://picx.zhimg.com/v2-415754b81e9e523cd53988723c03c54b" align="middle">
<img src="https://picx.zhimg.com/v2-9ca4d40a53a1e0399893c6ecf82a52a0" align="middle">
<img src="https://picx.zhimg.com/v2-5a521bf0480f07c0b515101641d96ed3" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Beyond-Plain-Demos-A-Demo-centric-Anchoring-Paradigm-for-In-Context-Learning-in-Alzheimerâ€™s-Disease-Detection"><a href="#Beyond-Plain-Demos-A-Demo-centric-Anchoring-Paradigm-for-In-Context-Learning-in-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimerâ€™s Disease Detection"></a>Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimerâ€™s Disease Detection</h2><p><strong>Authors:Puzhen Su, Haoran Yin, Yongzhu Miao, Jintao Tang, Shasha Li, Ting Wang</strong></p>
<p>Detecting Alzheimerâ€™s disease (AD) from narrative transcripts challenges large language models (LLMs): pre-training rarely covers this out-of-distribution task, and all transcript demos describe the same scene, producing highly homogeneous contexts. These factors cripple both the modelâ€™s built-in task knowledge (\textbf{task cognition}) and its ability to surface subtle, class-discriminative cues (\textbf{contextual perception}). Because cognition is fixed after pre-training, improving in-context learning (ICL) for AD detection hinges on enriching perception through better demonstration (demo) sets. We demonstrate that standard ICL quickly saturates, its demos lack diversity (context width) and fail to convey fine-grained signals (context depth), and that recent task vector (TV) approaches improve broad task adaptation by injecting TV into the LLMsâ€™ hidden states (HSs), they are ill-suited for AD detection due to the mismatch of injection granularity, strength and position. To address these bottlenecks, we introduce \textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands context width via \emph{\textbf{Diverse and Contrastive Retrieval}} (DCR) and deepens each demoâ€™s signal via \emph{\textbf{Projected Vector Anchoring}} (PVA) at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large, stable gains over both ICL and TV baselines, charting a new paradigm for fine-grained, OOD and low-resource LLM adaptation.</p>
<blockquote>
<p>ä»å™äº‹è®°å½•ä¸­æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼šé¢„è®­ç»ƒå¾ˆå°‘æ¶‰åŠè¿™ç§åç¦»åˆ†å¸ƒçš„ä»»åŠ¡ï¼Œæ‰€æœ‰çš„è½¬å½•ç¤ºä¾‹éƒ½æè¿°åŒä¸€åœºæ™¯ï¼Œå¯¼è‡´é«˜åº¦åŒè´¨çš„ä¸Šä¸‹æ–‡ç¯å¢ƒã€‚è¿™äº›å› ç´ ä¸ä»…å½±å“æ¨¡å‹å¯¹å†…ç½®ä»»åŠ¡çŸ¥è¯†ï¼ˆä»»åŠ¡è®¤çŸ¥ï¼‰çš„è®¤çŸ¥èƒ½åŠ›ï¼Œè¿˜å½±å“å…¶è¯†åˆ«ç»†å¾®ä¸”ç±»åˆ«å¯è¾¨è¯†çº¿ç´¢ï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰çš„èƒ½åŠ›ã€‚ç”±äºè®¤çŸ¥åœ¨é¢„è®­ç»ƒåå·²å›ºå®šï¼Œå› æ­¤æé«˜é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹èƒ½åŠ›çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¾èµ–äºé€šè¿‡æ›´å¥½çš„æ¼”ç¤ºé›†æ¥ä¸°å¯Œæ„ŸçŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜æ ‡å‡†ICLå¾ˆå¿«è¾¾åˆ°é¥±å’ŒçŠ¶æ€ï¼Œå…¶æ¼”ç¤ºç¼ºä¹å¤šæ ·æ€§ï¼ˆä¸Šä¸‹æ–‡å®½åº¦ï¼‰ä¸”æ— æ³•ä¼ é€’ç²¾ç»†ä¿¡å·ï¼ˆä¸Šä¸‹æ–‡æ·±åº¦ï¼‰ï¼Œè€Œæœ€è¿‘çš„åŸºäºä»»åŠ¡å‘é‡ï¼ˆTVï¼‰çš„æ–¹æ³•é€šè¿‡å‘LLMçš„éšè—çŠ¶æ€ï¼ˆHSsï¼‰æ³¨å…¥TVæ”¹å–„äº†å¹¿æ³›çš„ä»»åŠ¡é€‚åº”æ€§ï¼Œä½†ç”±äºæ³¨å…¥çš„ç²’åº¦ã€å¼ºåº¦å’Œä½ç½®çš„åŒ¹é…é—®é¢˜ï¼Œå®ƒä»¬ä¸é€‚ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ£€æµ‹ã€‚ä¸ºäº†å…‹æœè¿™äº›ç“¶é¢ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»¥æ¼”ç¤ºä¸ºä¸­å¿ƒçš„é”šå®šæ¡†æ¶<strong>DA4ICL</strong>ï¼Œå®ƒé€šè¿‡é€šè¿‡ä¸åŒæ–¹å¼å’Œå¯¹æ¯”æ£€ç´¢æ¥åŒæ—¶æ‰©å±•ä¸Šä¸‹æ–‡å®½åº¦å’Œä¸åŒæ·±åº¦æ¥æ·±åŒ–æ¯ä¸ªæ¼”ç¤ºä¿¡å·çš„æ„ŸçŸ¥æŠ•å½±å‘é‡é”šå®šæ³•ï¼Œè¯¥æ¡†æ¶åœ¨æ¯ä¸ªTransformerå±‚ä¸Šæ‰§è¡Œè¿™ä¸¤ä¸ªæ“ä½œã€‚åœ¨ä¸‰ä¸ªé˜¿å°”èŒ¨æµ·é»˜ç—…åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDA4ICLä¸ICLå’ŒTVåŸºçº¿ç›¸æ¯”å–å¾—äº†ç¨³å®šçš„å¤§å¹…å¢é•¿ï¼Œä¸ºç²¾ç»†ã€åç¦»æ­£å¸¸æƒ…å¢ƒå’Œä½èµ„æºè¯­è¨€æ¨¡å‹é€‚åº”é¢†åŸŸåˆ›é€ äº†æ–°çš„èŒƒä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06826v1">PDF</a> Accepted to the 40th Annual AAAI Conference on Artificial Intelligence (2026) - Main Technical Track (Oral)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»å™äº‹è½¬å½•æœ¬ä¸­æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„æŒ‘æˆ˜ã€‚ç”±äºé¢„è®­ç»ƒå¾ˆå°‘æ¶‰åŠæ­¤ç±»ä»»åŠ¡ï¼Œä¸”æ‰€æœ‰è½¬å½•æœ¬æè¿°çš„åœºæ™¯ç›¸åŒï¼Œä½¿å¾—æ¨¡å‹çš„å†…ç½®ä»»åŠ¡çŸ¥è¯†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›å—é™ã€‚æé«˜æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆICLï¼‰çš„å…³é”®åœ¨äºé€šè¿‡æ›´å¥½çš„æ¼”ç¤ºé›†ä¸°å¯Œæ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ ‡å‡†ICLå¿«é€Ÿé¥±å’Œï¼Œæ¼”ç¤ºç¼ºä¹å¤šæ ·æ€§ä¸”æ— æ³•ä¼ é€’ç²¾ç»†ä¿¡å·ã€‚å°½ç®¡è¿‘æœŸä»»åŠ¡å‘é‡ï¼ˆTVï¼‰æ–¹æ³•é€šè¿‡æ³¨å…¥TVæé«˜ä»»åŠ¡é€‚åº”æ€§ï¼Œä½†ç”±äºæ³¨å…¥ç²’åº¦ã€å¼ºåº¦å’Œä½ç½®çš„å·®å¼‚ï¼Œå®ƒä»¬åœ¨ADæ£€æµ‹ä¸­å¹¶ä¸é€‚ç”¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ä»¥æ¼”ç¤ºä¸ºä¸­å¿ƒçš„é”šå®šæ¡†æ¶DA4ICLï¼Œé€šè¿‡å¤šæ ·åŒ–å’Œå¯¹æ¯”æ£€ç´¢æ‰©å¤§ä¸Šä¸‹æ–‡å®½åº¦ï¼Œå¹¶é€šè¿‡æŠ•å½±å‘é‡é”šå®šæŠ€æœ¯æ·±åŒ–æ¯ä¸ªæ¼”ç¤ºçš„ä¿¡å·ã€‚åœ¨ä¸‰ä¸ªADåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDA4ICLç›¸è¾ƒäºICLå’ŒTVåŸºçº¿å®ç°äº†æ˜¾è‘—ä¸”ç¨³å®šçš„æå‡ï¼Œä¸ºç»†ç²’åº¦ã€å¼‚å¸¸æ£€æµ‹ä»¥åŠä½èµ„æºLLMé€‚åº”è®¾å®šäº†æ–°çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„å™äº‹è½¬å½•æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºé¢„è®­ç»ƒå¾ˆå°‘æ¶‰åŠæ­¤ç±»ä»»åŠ¡ï¼Œä¸”è¯­å¢ƒé«˜åº¦åŒè´¨åŒ–ã€‚</li>
<li>æ¨¡å‹åœ¨ä»»åŠ¡è®¤çŸ¥å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹é¢å­˜åœ¨å±€é™ï¼Œéœ€è¦é€šè¿‡æ›´å¥½çš„æ¼”ç¤ºé›†æ¥ä¸°å¯Œæ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>æ ‡å‡†ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹æ³•å¿«é€Ÿé¥±å’Œï¼Œæ¼”ç¤ºç¼ºä¹å¤šæ ·æ€§å’Œç²¾ç»†ä¿¡å·ä¼ é€’èƒ½åŠ›ã€‚</li>
<li>ä»»åŠ¡å‘é‡ï¼ˆTVï¼‰æ–¹æ³•è™½ç„¶èƒ½æé«˜å¹¿æ³›ä»»åŠ¡é€‚åº”æ€§ï¼Œä½†åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ä¸­å› æ³¨å…¥ç²’åº¦ã€å¼ºåº¦å’Œä½ç½®çš„å·®å¼‚è€Œä¸é€‚ç”¨ã€‚</li>
<li>å¼•å…¥çš„DA4ICLæ¡†æ¶é€šè¿‡å¤šæ ·åŒ–å’Œå¯¹æ¯”æ£€ç´¢æ‰©å¤§ä¸Šä¸‹æ–‡å®½åº¦ï¼Œå¹¶é€šè¿‡æŠ•å½±å‘é‡é”šå®šæŠ€æœ¯æ·±åŒ–æ¯ä¸ªæ¼”ç¤ºçš„ä¿¡å·ã€‚</li>
<li>DA4ICLåœ¨ä¸‰ä¸ªé˜¿å°”èŒ¨æµ·é»˜ç—…åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ˜¾è‘—ä¸”ç¨³å®šçš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df36352bbd304db85716b829437a1ac8" align="middle">
<img src="https://picx.zhimg.com/v2-5df1ab4498e58f66f569c85d6b4fbaa1" align="middle">
<img src="https://picx.zhimg.com/v2-6d45c328f4d1f4f9e46ae08f027790cb" align="middle">
<img src="https://picx.zhimg.com/v2-f1da25e445f95b9ad697fb35c55bdbd6" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Dutch-Metaphor-Extraction-from-Cancer-Patientsâ€™-Interviews-and-Forum-Data-using-LLMs-and-Human-in-the-Loop"><a href="#Dutch-Metaphor-Extraction-from-Cancer-Patientsâ€™-Interviews-and-Forum-Data-using-LLMs-and-Human-in-the-Loop" class="headerlink" title="Dutch Metaphor Extraction from Cancer Patientsâ€™ Interviews and Forum Data using LLMs and Human in the Loop"></a>Dutch Metaphor Extraction from Cancer Patientsâ€™ Interviews and Forum Data using LLMs and Human in the Loop</h2><p><strong>Authors:Lifeng Han, David Lindevelt, Sander Puts, Erik van Mulligen, Suzan Verberne</strong></p>
<p>Metaphors and metaphorical language (MLs) play an important role in healthcare communication between clinicians, patients, and patientsâ€™ family members. In this work, we focus on Dutch language data from cancer patients. We extract metaphors used by patients using two data sources: (1) cancer patient storytelling interview data and (2) online forum data, including patientsâ€™ posts, comments, and questions to professionals. We investigate how current state-of-the-art large language models (LLMs) perform on this task by exploring different prompting strategies such as chain of thought reasoning, few-shot learning, and self-prompting. With a human-in-the-loop setup, we verify the extracted metaphors and compile the outputs into a corpus named HealthQuote.NL. We believe the extracted metaphors can support better patient care, for example shared decision making, improved communication between patients and clinicians, and enhanced patient health literacy. They can also inform the design of personalized care pathways. We share prompts and related resources at <a target="_blank" rel="noopener" href="https://github.com/aaronlifenghan/HealthQuote.NL">https://github.com/aaronlifenghan/HealthQuote.NL</a></p>
<blockquote>
<p>éšå–»å’Œéšå–»è¯­è¨€ï¼ˆMLsï¼‰åœ¨åŒ»ç”Ÿã€æ‚£è€…å’Œæ‚£è€…å®¶åº­æˆå‘˜ä¹‹é—´çš„åŒ»ç–—å«ç”Ÿæ²Ÿé€šä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨æ¥è‡ªç™Œç—‡æ‚£è€…çš„è·å…°è¯­æ•°æ®ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªæ•°æ®æºæå–æ‚£è€…ä½¿ç”¨çš„éšå–»ï¼šï¼ˆ1ï¼‰ç™Œç—‡æ‚£è€…è®²æ•…äº‹è®¿è°ˆæ•°æ®ï¼›ï¼ˆ2ï¼‰åœ¨çº¿è®ºå›æ•°æ®ï¼ŒåŒ…æ‹¬æ‚£è€…çš„å¸–å­ã€è¯„è®ºå’Œå‘ä¸“ä¸šäººå£«æå‡ºçš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡æ¢ç´¢ä¸åŒçš„æç¤ºç­–ç•¥ï¼Œå¦‚æ€ç»´é“¾æ¨ç†ã€å°æ ·æœ¬å­¦ä¹ å’Œè‡ªæˆ‘æç¤ºï¼Œæ¥ç ”ç©¶å½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡äººå·¥å¾ªç¯è®¾ç½®ï¼Œæˆ‘ä»¬éªŒè¯äº†æå–çš„éšå–»ï¼Œå¹¶å°†è¾“å‡ºç¼–è¯‘æˆåä¸ºHealthQuote.NLçš„è¯­æ–™åº“ã€‚æˆ‘ä»¬ç›¸ä¿¡æå–çš„éšå–»å¯ä»¥æ”¹å–„æ‚£è€…æŠ¤ç†ï¼Œä¾‹å¦‚æ”¯æŒå…±åŒå†³ç­–ã€æ”¹å–„æ‚£è€…ä¸ä¸´åºŠåŒ»ç”Ÿä¹‹é—´çš„æ²Ÿé€šä»¥åŠæé«˜æ‚£è€…çš„å¥åº·ç´ å…»ã€‚å®ƒä»¬è¿˜å¯ä»¥ä¸ºä¸ªæ€§åŒ–æŠ¤ç†è·¯å¾„çš„è®¾è®¡æä¾›ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/aaronlifenghan/HealthQuote.NL">https://github.com/aaronlifenghan/HealthQuote.NL</a>åˆ†äº«æç¤ºå’Œç›¸å…³èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06427v1">PDF</a> Ongoing project report, on behalf of 4D PICTURE <a target="_blank" rel="noopener" href="https://4dpicture.eu/">https://4dpicture.eu/</a></p>
<p><strong>Summary</strong></p>
<p>ä»è¿™æ®µæ–‡æœ¬ä¸­å¯ä»¥çœ‹å‡ºï¼Œç ”ç©¶äººå‘˜åœ¨è·å…°è¯­æ•°æ®ä¸­ç ”ç©¶éšå–»åŠå…¶è¯­è¨€ï¼ˆMLsï¼‰åœ¨åŒ»æ‚£åŠå®¶åº­æˆå‘˜ä¹‹é—´çš„æ²Ÿé€šé‡è¦æ€§ã€‚é€šè¿‡é‡‡ç”¨ç™Œç—‡æ‚£è€…çš„ä¸¤ç§æ•°æ®æºï¼šç—…äººè®²è¿°æ•…äº‹çš„é¢è°ˆæ•°æ®ä»¥åŠåœ¨çº¿è®ºå›æ•°æ®ï¼ˆåŒ…æ‹¬æ‚£è€…å¸–å­ã€è¯„è®ºå’Œé—®é¢˜ï¼‰ï¼Œç ”ç©¶äººå‘˜æ¢è®¨äº†å½“å‰æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éšå–»æå–ä»»åŠ¡æ—¶çš„è¡¨ç°ã€‚ä»–ä»¬éªŒè¯å¹¶ç¼–è¯‘äº†æå–å‡ºçš„éšå–»ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªåä¸ºHealthQuote.NLçš„è¯­æ–™åº“ã€‚æå–å‡ºçš„éšå–»æœ‰æœ›æ”¯æŒæ›´å¥½çš„ç—…äººæŠ¤ç†ï¼Œå¦‚å†³ç­–å…±äº«ã€æ”¹å–„åŒ»æ‚£æ²Ÿé€šä»¥åŠæé«˜æ‚£è€…å¥åº·ç´ å…»ç­‰ã€‚åŒæ—¶ï¼Œå®ƒä»¬è¿˜å¯ä»¥ä¸ºä¸ªæ€§åŒ–æŠ¤ç†è·¯å¾„çš„è®¾è®¡æä¾›ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšå–»å’Œéšå–»æ€§è¯­è¨€åœ¨åŒ»ç–—ä¿å¥æ²Ÿé€šä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»æ‚£åŠå®¶åº­æˆå‘˜ä¹‹é—´ã€‚</li>
<li>ç ”ç©¶äººå‘˜é‡‡ç”¨è·å…°è¯­æ•°æ®æ¥æ¢è®¨è‡ªç„¶è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éšå–»æå–ä»»åŠ¡æ—¶çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨ä¸¤ç§æ•°æ®æºæå–ç™Œç—‡æ‚£è€…çš„éšå–»ï¼šé¢è°ˆæ•°æ®å’Œåœ¨çº¿è®ºå›æ•°æ®ã€‚</li>
<li>æ¢è®¨äº†å¤šç§è¯­è¨€æ¨¡å‹æç¤ºç­–ç•¥ï¼Œå¦‚é“¾å¼æ€ç»´æ¨ç†ã€å°‘æ ·æœ¬å­¦ä¹ å’Œè‡ªæˆ‘æç¤ºã€‚</li>
<li>äººç±»å‚ä¸éªŒè¯å’Œç¼–è¯‘æå–å‡ºçš„éšå–»ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåä¸ºHealthQuote.NLçš„è¯­æ–™åº“ã€‚</li>
<li>æå–å‡ºçš„éšå–»èƒ½å¤Ÿæ”¯æŒæ›´å¥½çš„ç—…äººæŠ¤ç†ï¼Œå¦‚å…±äº«å†³ç­–ã€æ”¹å–„æ²Ÿé€šå’Œæé«˜æ‚£è€…å¥åº·ç´ å…»ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-692a87260fbf766488b662de60a7ad36" align="middle">
<img src="https://picx.zhimg.com/v2-3e612f1a89093cc0ce03d54b7f715927" align="middle">
<img src="https://picx.zhimg.com/v2-2dd3beb58357c729bd2ab77a09be54fc" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Mixture-of-Experts-Framework-with-Log-Logistic-Components-for-Survival-Analysis-on-Histopathology-Images"><a href="#A-Mixture-of-Experts-Framework-with-Log-Logistic-Components-for-Survival-Analysis-on-Histopathology-Images" class="headerlink" title="A Mixture-of-Experts Framework with Log-Logistic Components for Survival Analysis on Histopathology Images"></a>A Mixture-of-Experts Framework with Log-Logistic Components for Survival Analysis on Histopathology Images</h2><p><strong>Authors:Ardhendu Sekhar, Vasu Soni, Keshav Aske, Shivam Madnoorkar, Pranav Jeevan, Amit Sethi</strong></p>
<p>We propose a modular framework for predicting cancer specific survival from whole slide pathology images (WSIs). The method integrates four components: (i) Quantile Gated Patch Selection via quantile based thresholding to isolate prognostically informative tissue regions; (ii) Graph Guided Clustering using a k nearest neighbor graph to capture phenotype level heterogeneity through spatial and morphological coherence; (iii) Hierarchical Context Attention to learn intra and inter cluster interactions; and (iv) an Expert Driven Mixture of Log logistics framework to estimate complex survival distributions using Log logistics distributions. The model attains a concordance index of 0.644 on TCGA LUAD, 0.751 on TCGA KIRC, and 0.752 on TCGA BRCA respectively, outperforming existing state of the art approaches.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºä¸€ç§æ¨¡å—åŒ–æ¡†æ¶ï¼Œç”¨äºä»å…¨ç—…ç†åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰é¢„æµ‹ç™Œç—‡ç‰¹å¼‚æ€§ç”Ÿå­˜ã€‚è¯¥æ–¹æ³•èåˆäº†å››ä¸ªç»„ä»¶ï¼šï¼ˆiï¼‰åŸºäºåˆ†ä½æ•°é˜ˆå€¼çš„Quantile Gated Patch Selectionï¼Œç”¨äºéš”ç¦»é¢„åä¿¡æ¯ä¸°å¯Œçš„ç»„ç»‡åŒºåŸŸï¼›ï¼ˆiiï¼‰ä½¿ç”¨kè¿‘é‚»å›¾çš„Graph Guided Clusteringï¼Œé€šè¿‡ç©ºé—´å½¢æ€ä¸€è‡´æ€§æ•æ‰è¡¨å‹æ°´å¹³å¼‚è´¨æ€§ï¼›ï¼ˆiiiï¼‰å±‚æ¬¡ä¸Šä¸‹æ–‡æ³¨æ„åŠ›ï¼ˆHierarchical Context Attentionï¼‰ï¼Œå­¦ä¹ é›†ç¾¤å†…éƒ¨å’Œå¤–éƒ¨çš„äº¤äº’ä½œç”¨ï¼›ï¼ˆivï¼‰ä¸“å®¶é©±åŠ¨çš„æ··åˆå¯¹æ•°é€»è¾‘æ¡†æ¶ï¼ˆExpert Driven Mixture of Log logistics frameworkï¼‰ï¼Œåˆ©ç”¨å¯¹æ•°é€»è¾‘åˆ†å¸ƒä¼°è®¡å¤æ‚çš„ç”Ÿå­˜åˆ†å¸ƒã€‚è¯¥æ¨¡å‹åœ¨TCGA LUADä¸Šè¾¾åˆ°0.644çš„å¥‘åˆæŒ‡æ•°ï¼Œåœ¨TCGA KIRCä¸Šè¾¾åˆ°0.751ï¼Œåœ¨TCGA BRCAä¸Šè¾¾åˆ°0.752ï¼Œè¶…è¿‡äº†ç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06266v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å—åŒ–æ¡†æ¶çš„é¢„æµ‹ç™Œç—‡ç‰¹å¼‚æ€§ç”Ÿå­˜çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å…¨å¹»ç¯ç‰‡ç—…ç†å›¾åƒï¼ˆWSIsï¼‰ï¼Œæ•´åˆäº†å››é¡¹æŠ€æœ¯ï¼šå®šé‡é—¨æ§è¡¥ä¸é€‰æ‹©ã€å›¾å¼•å¯¼èšç±»ã€å±‚æ¬¡ä¸Šä¸‹æ–‡æ³¨æ„åŠ›å’Œä¸“å®¶é©±åŠ¨çš„æ··åˆé€»è¾‘å›å½’æ¡†æ¶ã€‚è¯¥æ¨¡å‹åœ¨TCGAè‚ºç™Œã€è‚¾ç™Œå’Œä¹³è…ºç™Œæ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„é¢„æµ‹ä¸€è‡´æ€§æŒ‡æ•°ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–æ¡†æ¶ç”¨äºé¢„æµ‹ç™Œç—‡ç‰¹å¼‚æ€§ç”Ÿå­˜ã€‚</li>
<li>åˆ©ç”¨å…¨å¹»ç¯ç‰‡ç—…ç†å›¾åƒï¼ˆWSIsï¼‰è¿›è¡Œåˆ†æã€‚</li>
<li>æ•´åˆäº†å››é¡¹æŠ€æœ¯ï¼šå®šé‡é—¨æ§è¡¥ä¸é€‰æ‹©ã€å›¾å¼•å¯¼èšç±»ã€å±‚æ¬¡ä¸Šä¸‹æ–‡æ³¨æ„åŠ›å’Œä¸“å®¶é©±åŠ¨çš„æ··åˆé€»è¾‘å›å½’æ¡†æ¶ã€‚</li>
<li>æ¨¡å‹åœ¨TCGAè‚ºç™Œã€è‚¾ç™Œå’Œä¹³è…ºç™Œæ•°æ®é›†ä¸Šè·å¾—è¾ƒé«˜é¢„æµ‹ä¸€è‡´æ€§æŒ‡æ•°ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿæ•æ‰é¢„åä¿¡æ¯ä¸­çš„ç»„ç»‡åŒºåŸŸã€ç°è±¡å‹æ°´å¹³çš„å¼‚è´¨æ€§ä»¥åŠç°‡å†…å’Œç°‡é—´çš„äº¤äº’ä½œç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69dde83b4ebbc46ce8a5ca61d2f29fae" align="middle">
<img src="https://picx.zhimg.com/v2-7fa035f0233321e86abbd7ce41b4bce6" align="middle">
<img src="https://picx.zhimg.com/v2-518565fe5b56dee15ce9c1a3674e401d" align="middle">
<img src="https://picx.zhimg.com/v2-aa3d16f600a143c64d2e5f39c6cfad11" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Explicit-Knowledge-Guided-In-Context-Learning-for-Early-Detection-of-Alzheimerâ€™s-Disease"><a href="#Explicit-Knowledge-Guided-In-Context-Learning-for-Early-Detection-of-Alzheimerâ€™s-Disease" class="headerlink" title="Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimerâ€™s Disease"></a>Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimerâ€™s Disease</h2><p><strong>Authors:Puzhen Su, Yongzhu Miao, Chunxi Guo, Jintao Tang, Shasha Li, Ting Wang</strong></p>
<p>Detecting Alzheimerâ€™s Disease (AD) from narrative transcripts remains a challenging task for large language models (LLMs), particularly under out-of-distribution (OOD) and data-scarce conditions. While in-context learning (ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL approaches often suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, issues that are amplified in clinical domains like AD detection. We propose Explicit Knowledge In-Context Learners (EK-ICL), a novel framework that integrates structured explicit knowledge to enhance reasoning stability and task alignment in ICL. EK-ICL incorporates three knowledge components: confidence scores derived from small language models (SLMs) to ground predictions in task-relevant patterns, parsing feature scores to capture structural differences and improve demo selection, and label word replacement to resolve semantic misalignment with LLM priors. In addition, EK-ICL employs a parsing-based retrieval strategy and ensemble prediction to mitigate the effects of semantic homogeneity in AD transcripts. Extensive experiments across three AD datasets demonstrate that EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that ICL performance in AD detection is highly sensitive to the alignment of label semantics and task-specific context, underscoring the importance of explicit knowledge in clinical reasoning under low-resource conditions.</p>
<blockquote>
<p>ä»å™äº‹æ–‡æœ¬ä¸­æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´ï¼ˆOODï¼‰å’Œç¼ºä¹æ•°æ®çš„æƒ…å†µä¸‹ã€‚è™½ç„¶ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸ºå¾®è°ƒæä¾›äº†ä¸€ç§å‚æ•°æ•ˆç‡é«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„ICLæ–¹æ³•å¾€å¾€å­˜åœ¨ä»»åŠ¡è¯†åˆ«å¤±è´¥ã€æ¼”ç¤ºé€‰æ‹©ä¸ä½³ä»¥åŠæ ‡ç­¾è¯ä¸ä»»åŠ¡ç›®æ ‡ä¹‹é—´çš„ä¸åŒ¹é…ç­‰é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ç­‰ä¸´åºŠé¢†åŸŸå°¤ä¸ºçªå‡ºã€‚æˆ‘ä»¬æå‡ºäº†æ˜¾å¼çŸ¥è¯†ä¸Šä¸‹æ–‡å­¦ä¹ è€…ï¼ˆEK-ICLï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆç»“æ„åŒ–æ˜¾å¼çŸ¥è¯†ä»¥å¢å¼ºä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ¨ç†ç¨³å®šæ€§å’Œä»»åŠ¡å¯¹é½çš„æ–°å‹æ¡†æ¶ã€‚EK-ICLç»“åˆäº†ä¸‰ç§çŸ¥è¯†æˆåˆ†ï¼šä»å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ´¾ç”Ÿçš„ç½®ä¿¡åº¦åˆ†æ•°ï¼Œä»¥å°†é¢„æµ‹ä¸ä»»åŠ¡ç›¸å…³æ¨¡å¼ç›¸ç»“åˆï¼›è§£æç‰¹å¾åˆ†æ•°ï¼Œä»¥æ•æ‰ç»“æ„å·®å¼‚å¹¶æ”¹è¿›æ¼”ç¤ºé€‰æ‹©ï¼›æ ‡ç­¾è¯æ›¿æ¢ï¼Œä»¥è§£å†³ä¸LLMå…ˆéªŒçŸ¥è¯†çš„è¯­ä¹‰ä¸åŒ¹é…é—®é¢˜ã€‚æ­¤å¤–ï¼ŒEK-ICLé‡‡ç”¨åŸºäºè§£æçš„æ£€ç´¢ç­–ç•¥å’Œé›†æˆé¢„æµ‹ï¼Œä»¥ç¼“è§£é˜¿å°”èŒ¨æµ·é»˜ç—…è½¬å½•ä¸­çš„è¯­ä¹‰ä¸€è‡´æ€§å¯¹é¢„æµ‹ç»“æœçš„å¹²æ‰°ã€‚åœ¨ä¸‰ä¸ªé˜¿å°”èŒ¨æµ·é»˜ç—‡æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEK-ICLæ˜¾è‘—ä¼˜äºæœ€æ–°çš„å¾®è°ƒåŠICLåŸºçº¿æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œé˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹ä¸­ICLçš„æ€§èƒ½é«˜åº¦ä¾èµ–äºæ ‡ç­¾è¯­ä¹‰å’Œä»»åŠ¡ç‰¹å®šä¸Šä¸‹æ–‡çš„åŒ¹é…åº¦ï¼Œè¿™å‡¸æ˜¾äº†åœ¨èµ„æºæœ‰é™çš„æ¡ä»¶ä¸‹ä¸´åºŠæ¨ç†ä¸­æ˜¾å¼çŸ¥è¯†çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06215v1">PDF</a> This paper was accepted by IEEE BIBM 2025 conference</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬æè¿°ï¼Œæå‡ºä¸€ç§åä¸ºEK-ICLçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆç»“æ„åŒ–æ˜ç¡®çŸ¥è¯†ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ¨ç†ç¨³å®šæ€§ä¸ä»»åŠ¡å¯¹é½ã€‚é€šè¿‡å¼•å…¥å°è¯­è¨€æ¨¡å‹çš„ç½®ä¿¡åº¦è¯„åˆ†ã€è§£æç‰¹å¾è¯„åˆ†å’Œæ ‡ç­¾è¯æ›¿æ¢ä¸‰ä¸ªçŸ¥è¯†ç»„ä»¶æ¥è§£å†³ä»»åŠ¡è¯†åˆ«å¤±è´¥ã€ç¤ºèŒƒé€‰æ‹©ä¸ä½³å’Œæ ‡ç­¾è¯ä¸ä»»åŠ¡ç›®æ ‡é—´çš„å¯¹é½é—®é¢˜ã€‚å¹¶é€šè¿‡è§£æå¼æ£€ç´¢ç­–ç•¥å’Œé›†æˆé¢„æµ‹ç¼“è§£ADè½¬å½•æœ¬çš„è¯­ä¹‰åŒè´¨æ€§é—®é¢˜ã€‚åœ¨ä¸‰ä¸ªADæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒEK-ICLæ˜¾è‘—ä¼˜äºå…ˆè¿›çš„å¾®è°ƒä¸ä¸Šä¸‹æ–‡å­¦ä¹ åŸºçº¿ã€‚åˆ†æè¡¨æ˜ï¼ŒADæ£€æµ‹ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½é«˜åº¦ä¾èµ–äºæ ‡ç­¾è¯­ä¹‰ä¸ä»»åŠ¡ç‰¹å®šä¸Šä¸‹æ–‡çš„å¯¹é½ï¼Œçªæ˜¾åœ¨ä½èµ„æºæ¡ä»¶ä¸‹ä¸´åºŠæ¨ç†ä¸­æ˜ç¡®çŸ¥è¯†çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ä»å™äº‹è½¬å½•å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰å’Œæ•°æ®ç¨€ç¼ºçš„æ¡ä»¶ä¸‹ã€‚</li>
<li>ç°æœ‰ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–¹æ³•ç»å¸¸é¢ä¸´ä»»åŠ¡è¯†åˆ«å¤±è´¥ã€ç¤ºèŒƒé€‰æ‹©ä¸ä½³å’Œæ ‡ç­¾è¯ä¸ä»»åŠ¡ç›®æ ‡é—´å¯¹é½é—®é¢˜ã€‚</li>
<li>EK-ICLæ¡†æ¶ç»“åˆäº†ç»“æ„åŒ–æ˜ç¡®çŸ¥è¯†æ¥æé«˜æ¨ç†ç¨³å®šæ€§å’Œä»»åŠ¡å¯¹é½ã€‚åŒ…æ‹¬å°è¯­è¨€æ¨¡å‹çš„ç½®ä¿¡åº¦è¯„åˆ†ã€è§£æç‰¹å¾è¯„åˆ†å’Œæ ‡ç­¾è¯æ›¿æ¢ä¸‰ä¸ªçŸ¥è¯†ç»„ä»¶ã€‚</li>
<li>EK-ICLä½¿ç”¨è§£æå¼æ£€ç´¢ç­–ç•¥å’Œé›†æˆé¢„æµ‹æ¥ç¼“è§£ADè½¬å½•æœ¬çš„è¯­ä¹‰åŒè´¨æ€§é—®é¢˜ã€‚</li>
<li>åœ¨ä¸‰ä¸ªADæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEK-ICLæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>ICLåœ¨ADæ£€æµ‹ä¸­çš„æ€§èƒ½é«˜åº¦ä¾èµ–äºæ ‡ç­¾è¯­ä¹‰ä¸ä»»åŠ¡ç‰¹å®šä¸Šä¸‹æ–‡çš„å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85c7f0319ee35eece35beae62bccd7e2" align="middle">
<img src="https://picx.zhimg.com/v2-ec3b763cf921cc7eb868dbf9f5d41eae" align="middle">
<img src="https://picx.zhimg.com/v2-d4b4664100efeaa17982389a5525385f" align="middle">
<img src="https://picx.zhimg.com/v2-69f71eb93799ae4b68ad5f31d3bd8c02" align="middle">
<img src="https://picx.zhimg.com/v2-12253477604f049470e077e1da0e5e4a" align="middle">
<img src="https://picx.zhimg.com/v2-d37251e0090bfe3767a642ca0fe55dda" align="middle">
<img src="https://picx.zhimg.com/v2-5b288de0578ed40eae5bf9eaa10d4b78" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling"><a href="#NURBGen-High-Fidelity-Text-to-CAD-Generation-through-LLM-Driven-NURBS-Modeling" class="headerlink" title="NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS Modeling"></a>NURBGen: High-Fidelity Text-to-CAD Generation through LLM-Driven NURBS Modeling</h2><p><strong>Authors:Muhammad Usama, Mohammad Sadil Khan, Didier Stricker, Muhammad Zeshan Afzal</strong></p>
<p>Generating editable 3D CAD models from natural language remains challenging, as existing text-to-CAD systems either produce meshes or rely on scarce design-history data. We present NURBGen, the first framework to generate high-fidelity 3D CAD models directly from text using Non-Uniform Rational B-Splines (NURBS). To achieve this, we fine-tune a large language model (LLM) to translate free-form texts into JSON representations containing NURBS surface parameters (\textit{i.e}, control points, knot vectors, degrees, and rational weights) which can be directly converted into BRep format using Python. We further propose a hybrid representation that combines untrimmed NURBS with analytic primitives to handle trimmed surfaces and degenerate regions more robustly, while reducing token complexity. Additionally, we introduce partABC, a curated subset of the ABC dataset consisting of individual CAD components, annotated with detailed captions using an automated annotation pipeline. NURBGen demonstrates strong performance on diverse prompts, surpassing prior methods in geometric fidelity and dimensional accuracy, as confirmed by expert evaluations. Code and dataset will be released publicly.</p>
<blockquote>
<p>ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå¯ç¼–è¾‘çš„3D CADæ¨¡å‹ä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰çš„æ–‡æœ¬åˆ°CADç³»ç»Ÿè¦ä¹ˆäº§ç”Ÿç½‘æ ¼ï¼Œè¦ä¹ˆä¾èµ–äºç¨€ç¼ºçš„è®¾è®¡å†å²æ•°æ®ã€‚æˆ‘ä»¬æ¨å‡ºäº†NURBGenï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨éå‡åŒ€æœ‰ç†Bæ ·æ¡ï¼ˆNURBSï¼‰ç›´æ¥ä»æ–‡æœ¬ç”Ÿæˆé«˜ä¿çœŸ3D CADæ¨¡å‹çš„é¦–ä¸ªæ¡†æ¶ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥å°†è‡ªç”±å½¢å¼çš„æ–‡æœ¬ç¿»è¯‘æˆåŒ…å«NURBSæ›²é¢å‚æ•°çš„JSONè¡¨ç¤ºï¼ˆå³æ§åˆ¶ç‚¹ã€ç»“å‘é‡ã€åº¦æ•°å’Œæœ‰ç†æƒé‡ï¼‰ï¼Œç„¶åå¯ä»¥ç›´æ¥ä½¿ç”¨Pythonå°†å…¶è½¬æ¢ä¸ºBRepæ ¼å¼ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ··åˆè¡¨ç¤ºæ³•ï¼Œå®ƒå°†æœªä¿®å‰ªçš„NURBSä¸è§£æåŸå§‹è¯­ç›¸ç»“åˆï¼Œä»¥æ›´ç¨³å¥åœ°å¤„ç†ä¿®å‰ªè¡¨é¢å’Œé€€åŒ–åŒºåŸŸï¼ŒåŒæ—¶é™ä½ä»¤ç‰Œå¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†partABCï¼Œè¿™æ˜¯ABCæ•°æ®é›†çš„ä¸€ä¸ªç²¾é€‰å­é›†ï¼Œç”±å•ä¸ªCADç»„ä»¶ç»„æˆï¼Œå¹¶ä½¿ç”¨è‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“è¿›è¡Œäº†è¯¦ç»†çš„æ ‡é¢˜æ³¨é‡Šã€‚NURBGenåœ¨ä¸åŒæç¤ºä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨å‡ ä½•ä¿çœŸåº¦å’Œå°ºå¯¸å‡†ç¡®æ€§æ–¹é¢è¶…è¿‡äº†å…ˆå‰çš„æ–¹æ³•ï¼Œè¿™å¾—åˆ°äº†ä¸“å®¶è¯„ä¼°çš„è¯å®ã€‚ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06194v1">PDF</a> Accepted in AAAI 2026</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºéå‡åŒ€æœ‰ç†Bæ ·æ¡ï¼ˆNURBSï¼‰æŠ€æœ¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ¡†æ¶NURBGenï¼Œå®ƒèƒ½ç›´æ¥ä»æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´CADæ¨¡å‹ã€‚é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°†è‡ªç”±å½¢å¼çš„æ–‡æœ¬è½¬æ¢ä¸ºåŒ…å«NURBSæ›²é¢å‚æ•°çš„JSONè¡¨ç¤ºå½¢å¼ï¼Œè¿›è€Œé€šè¿‡Pythonç›´æ¥è½¬æ¢ä¸ºBRepæ ¼å¼ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ··åˆè¡¨ç¤ºæ³•ï¼Œç»“åˆäº†æœªä¿®å‰ªçš„NURBSå’Œè§£æåŸºæœ¬ä½“ï¼Œä»¥æ›´ç¨³å¥åœ°å¤„ç†ä¿®å‰ªæ›²é¢å’Œé€€åŒ–åŒºåŸŸï¼ŒåŒæ—¶é™ä½æ ‡è®°å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å«CADç»„ä»¶çš„partABCæ•°æ®é›†å­é›†ï¼Œé‡‡ç”¨è‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“å¯¹å…¶å®æ–½è¯¦ç»†çš„æ³¨é‡Šã€‚åœ¨å¤šç§æç¤ºä¸Šè¡¨ç°å‡ºè‰²ï¼Œå‡ ä½•ä¿çœŸåº¦å’Œå°ºå¯¸ç²¾åº¦å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸“å®¶è¯„ä¼°ç»“æœè¯å®äº†è¿™ä¸€ç‚¹ã€‚ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºéå‡åŒ€æœ‰ç†Bæ ·æ¡ï¼ˆNURBGenï¼‰æŠ€æœ¯çš„å…¨æ–°æ¡†æ¶ï¼Œç›´æ¥ä»æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´CADæ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°æ–‡æœ¬åˆ°CADæ¨¡å‹çš„è½¬æ¢ã€‚</li>
<li>åˆ©ç”¨JSONè¡¨ç¤ºå½¢å¼æ¥å­˜å‚¨NURBSæ›²é¢å‚æ•°ï¼Œå¹¶é€šè¿‡Pythonè½¬æ¢ä¸ºBRepæ ¼å¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ··åˆè¡¨ç¤ºæ³•ï¼Œç»“åˆäº†æœªä¿®å‰ªçš„NURBSå’Œè§£æåŸºæœ¬ä½“ï¼Œæé«˜äº†å¤„ç†å¤æ‚æ›²é¢çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†åŒ…å«è¯¦ç»†æ³¨é‡Šçš„CADç»„ä»¶æ•°æ®é›†partABCã€‚</li>
<li>NURBGenåœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¾—åˆ°ä¸“å®¶çš„è‚¯å®šè¯„ä»·ã€‚</li>
<li>ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ä¸è¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a86e3550700c66bbb1da4fdfccc08308" align="middle">
<img src="https://picx.zhimg.com/v2-e42cd283f8e0da422c423a77083561e0" align="middle">
<img src="https://picx.zhimg.com/v2-93d3d49622e13cfabfeb6c0dce1162fb" align="middle">
<img src="https://picx.zhimg.com/v2-0b1cdb465c767a803fefc6c74e30ddef" align="middle">
<img src="https://picx.zhimg.com/v2-8df2a8f45749ecc8b172306171fd1abc" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Distributed-Deep-Learning-for-Medical-Image-Denoising-with-Data-Obfuscation"><a href="#Distributed-Deep-Learning-for-Medical-Image-Denoising-with-Data-Obfuscation" class="headerlink" title="Distributed Deep Learning for Medical Image Denoising with Data Obfuscation"></a>Distributed Deep Learning for Medical Image Denoising with Data Obfuscation</h2><p><strong>Authors:Sulaimon Oyeniyi Adebayo, Ayaz H. Khan</strong></p>
<p>Medical image denoising is essential for improving image quality while minimizing the exposure of sensitive information, particularly when working with large-scale clinical datasets. This study explores distributed deep learning for denoising chest X-ray images from the NIH Chest X-ray14 dataset, using additive Gaussian noise as a lightweight obfuscation technique. We implement and evaluate U-Net and U-Net++ architectures under single-GPU, standard multi-GPU (DataParallel), and optimized multi-GPU training configurations using PyTorchâ€™s DistributedDataParallel (DDP) and Automatic Mixed Precision (AMP). Our results show that U-Net++ consistently delivers superior denoising performance, achieving competitive Peak Signal to Noise Ratio (PSNR) and Structured Similarity Index Method (SSIM) scores, though with less performance in Learned Perceptual Image Patch Similarity (LPIPS) compared to U-Net under low and moderate noise levels. This indicates U-Net++â€™s enhanced structural fidelity and low perceptual similarity. Meanwhile, our optimized training pipeline reduces training time by over 60% for both models compared to single-GPU training, and outperforms standard DataParallel by over 40%, with only a minor accuracy drop for both models (trading some accuracy for speed). These findings highlight the effectiveness of software-level optimization in distributed learning for medical imaging. This work demonstrates the practical viability of combining architectural design, lightweight obfuscation, and advanced distributed training strategies to accelerate and enhance medical image processing pipelines in real-world clinical and research environments. The full implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Suadey/medical-image-denoising-ddp">https://github.com/Suadey/medical-image-denoising-ddp</a>.</p>
<blockquote>
<p>åŒ»å­¦å›¾åƒå»å™ªå¯¹äºæé«˜å›¾åƒè´¨é‡å¹¶æœ€å°åŒ–æ•æ„Ÿä¿¡æ¯çš„æš´éœ²è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡ä¸´åºŠæ•°æ®é›†æ—¶ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ åœ¨NIH Chest X-ray14æ•°æ®é›†èƒ¸éƒ¨Xå°„çº¿å›¾åƒå»å™ªæ–¹é¢çš„åº”ç”¨ï¼Œé‡‡ç”¨é«˜æ–¯å™ªå£°ä½œä¸ºè½»é‡çº§æ¨¡ç³ŠæŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨å•ä¸ªGPUã€æ ‡å‡†å¤šGPUï¼ˆDataParallelï¼‰å’Œä¼˜åŒ–å¤šGPUè®­ç»ƒé…ç½®ä¸‹å®ç°äº†U-Netå’ŒU-Net++æ¶æ„ï¼Œä½¿ç”¨PyTorchçš„DistributedDataParallelï¼ˆDDPï¼‰å’Œè‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒU-Net++åœ¨å»å™ªæ€§èƒ½æ–¹é¢å§‹ç»ˆè¡¨ç°æ›´ä¼˜ï¼Œåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°æ–¹æ³•ï¼ˆSSIMï¼‰æ–¹é¢å–å¾—äº†æœ‰ç«äº‰åŠ›çš„æˆç»©ï¼Œä½†åœ¨ä½å™ªå£°å’Œä¸­å™ªå£°æ°´å¹³ä¸‹ï¼Œå…¶åœ¨å­¦ä¹ çš„æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰æ–¹é¢çš„è¡¨ç°ç•¥é€ŠäºU-Netã€‚è¿™è¡¨æ˜U-Net++å…·æœ‰æ›´é«˜çš„ç»“æ„ä¿çœŸåº¦å’Œè¾ƒä½çš„æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„ä¼˜åŒ–è®­ç»ƒç®¡é“å°†ä¸¤ä¸ªæ¨¡å‹ç›¸å¯¹äºå•ä¸ªGPUè®­ç»ƒçš„æ—¶é—´ç¼©çŸ­äº†è¶…è¿‡60%ï¼Œå¹¶ä¸”ç›¸è¾ƒäºæ ‡å‡†çš„DataParallelæé«˜äº†è¶…è¿‡40%ï¼ŒåŒæ—¶ä¸¤ä¸ªæ¨¡å‹çš„ç²¾åº¦ç•¥æœ‰ä¸‹é™ï¼ˆä»¥ç‰ºç‰²ä¸€äº›å‡†ç¡®æ€§æ¢å–é€Ÿåº¦ï¼‰ã€‚è¿™äº›å‘ç°çªæ˜¾äº†è½¯ä»¶çº§ä¼˜åŒ–åœ¨åŒ»å­¦æˆåƒåˆ†å¸ƒå¼å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ç»“åˆæ¶æ„è®¾è®¡ã€è½»é‡çº§æ¨¡ç³ŠæŠ€æœ¯å’Œå…ˆè¿›çš„åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥åŠ é€Ÿå’Œæ”¹è¿›çœŸå®ä¸´åºŠå’Œç ”ç©¶ç¯å¢ƒä¸­çš„åŒ»å­¦å›¾åƒå¤„ç†æµç¨‹çš„å®é™…å¯è¡Œæ€§ã€‚å®Œæ•´çš„å®ç°å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Suadey/medical-image-denoising-ddp%E3%80%82">https://github.com/Suadey/medical-image-denoising-ddpã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06006v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºåˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒå»å™ªæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„åº”ç”¨ã€‚ç ”ç©¶é‡‡ç”¨äº†NIH Chest X-ray14æ•°æ®é›†ï¼Œä½¿ç”¨æ·»åŠ é«˜æ–¯å™ªå£°ä½œä¸ºè½»é‡çº§æ··æ·†æŠ€æœ¯ã€‚ç ”ç©¶å®ç°äº†U-Netå’ŒU-Net++æ¶æ„ï¼Œå¹¶åœ¨å•GPUã€æ ‡å‡†å¤šGPUå’Œä¼˜åŒ–å¤šGPUè®­ç»ƒé…ç½®ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒU-Net++åœ¨å»å™ªæ€§èƒ½ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼Œåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°æ–¹æ³•ï¼ˆSSIMï¼‰æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä½†åœ¨ä½å™ªå£°å’Œä¸­å™ªå£°æ°´å¹³ä¸‹æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰æ–¹é¢çš„è¡¨ç°ç•¥é€ŠäºU-Netã€‚è¿™è¡¨æ˜U-Net++å…·æœ‰æ›´é«˜çš„ç»“æ„ä¿çœŸåº¦å’Œè¾ƒä½çš„æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚åŒæ—¶ï¼Œä¼˜åŒ–è®­ç»ƒç®¡é“å°†ä¸¤ä¸ªæ¨¡å‹çš„è®­ç»ƒæ—¶é—´å‡å°‘äº†60ï¼…ä»¥ä¸Šï¼Œä¸å•GPUè®­ç»ƒç›¸æ¯”æé«˜äº†æ€§èƒ½ï¼Œå¹¶ä¼˜äºæ ‡å‡†DataParallelè®­ç»ƒï¼Œä½†ä»¥è½»å¾®çš„æ€§èƒ½ä¸‹é™ä¸ºä»£ä»·ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†ç»“åˆæ¶æ„è®¾è®¡ã€è½»é‡çº§æ··æ·†å’Œé«˜çº§åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥åœ¨å®é™…ä¸´åºŠå’Œç ”ç©¶ç¯å¢ƒä¸­åŠ é€Ÿå’Œæ”¹è¿›åŒ»å­¦å›¾åƒå¤„ç†æµç¨‹çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢ç´¢äº†åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒå»å™ªä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒã€‚</li>
<li>ä½¿ç”¨NIH Chest X-ray14æ•°æ®é›†å’Œæ·»åŠ é«˜æ–¯å™ªå£°ä½œä¸ºè½»é‡çº§æ··æ·†æŠ€æœ¯ã€‚</li>
<li>U-Net++åœ¨å»å™ªæ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨PSNRå’ŒSSIMæ–¹é¢ã€‚</li>
<li>åœ¨ä½å™ªå£°å’Œä¸­å™ªå£°æ°´å¹³ä¸‹ï¼ŒU-Net++è¡¨ç°å‡ºè¾ƒé«˜çš„ç»“æ„ä¿çœŸåº¦å’Œè¾ƒä½çš„æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚</li>
<li>ä¼˜åŒ–è®­ç»ƒç®¡é“æ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>ä¼˜åŒ–è®­ç»ƒåœ¨åˆ†å¸ƒå¼å­¦ä¹ ä¸­è½¯ä»¶çº§ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3798e07a6303b299e9f766e768c21cf" align="middle">
<img src="https://picx.zhimg.com/v2-7115697785ca255dce0aec9cdb4f2985" align="middle">
<img src="https://picx.zhimg.com/v2-c1db917974858404ee7ed48efddc69d1" align="middle">
<img src="https://picx.zhimg.com/v2-69424223e543febfa708b4ea40eb098c" align="middle">
<img src="https://picx.zhimg.com/v2-11200ff9869f2cb503eea2d7373cc607" align="middle">
<img src="https://picx.zhimg.com/v2-5f7173165dde32b81b223667c8dd9d12" align="middle">
<img src="https://picx.zhimg.com/v2-09a566ae78d0d634ced15e7918c7363d" align="middle">
<img src="https://picx.zhimg.com/v2-9c30c8fb795d020d2bf1fe2ccae50eb6" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Dual-Mode-ViT-Conditioned-Diffusion-Framework-with-an-Adaptive-Conditioning-Bridge-for-Breast-Cancer-Segmentation"><a href="#A-Dual-Mode-ViT-Conditioned-Diffusion-Framework-with-an-Adaptive-Conditioning-Bridge-for-Breast-Cancer-Segmentation" class="headerlink" title="A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation"></a>A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation</h2><p><strong>Authors:Prateek Singh, Moumita Dholey, P. K. Vinod</strong></p>
<p>In breast ultrasound images, precise lesion segmentation is essential for early diagnosis; however, low contrast, speckle noise, and unclear boundaries make this difficult. Even though deep learning models have demonstrated potential, standard convolutional architectures frequently fall short in capturing enough global context, resulting in segmentations that are anatomically inconsistent. To overcome these drawbacks, we suggest a flexible, conditional Denoising Diffusion Model that combines an enhanced UNet-based generative decoder with a Vision Transformer (ViT) encoder for global feature extraction. We introduce three primary innovations: 1) an Adaptive Conditioning Bridge (ACB) for efficient, multi-scale fusion of semantic features; 2) a novel Topological Denoising Consistency (TDC) loss component that regularizes training by penalizing structural inconsistencies during denoising; and 3) a dual-head architecture that leverages the denoising objective as a powerful regularizer, enabling a lightweight auxiliary head to perform rapid and accurate inference on smaller datasets and a noise prediction head. Our framework establishes a new state-of-the-art on public breast ultrasound datasets, achieving Dice scores of 0.96 on BUSI, 0.90 on BrEaST and 0.97 on BUS-UCLM. Comprehensive ablation studies empirically validate that the model components are critical for achieving these results and for producing segmentations that are not only accurate but also anatomically plausible.</p>
<blockquote>
<p>åœ¨ä¹³è…ºè¶…å£°å›¾åƒä¸­ï¼Œç²¾ç¡®çš„ç—…å˜åˆ†å‰²å¯¹äºæ—©æœŸè¯Šæ–­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºå¯¹æ¯”åº¦ä½ã€æ–‘ç‚¹å™ªå£°å’Œè¾¹ç•Œä¸æ¸…ç­‰é—®é¢˜ï¼Œè¿™ä¸€ä»»åŠ¡å˜å¾—å›°éš¾ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†æ ‡å‡†çš„å·ç§¯æ¶æ„é€šå¸¸éš¾ä»¥æ•è·è¶³å¤Ÿçš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´åˆ†å‰²ç»“æœè§£å‰–ä¸Šä¸è¿è´¯ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„ã€æœ‰æ¡ä»¶çš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†åŸºäºå¢å¼ºå‹UNetçš„ç”Ÿæˆè§£ç å™¨å’ŒVision Transformerï¼ˆViTï¼‰ç¼–ç å™¨è¿›è¡Œå…¨å±€ç‰¹å¾æå–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªä¸»è¦åˆ›æ–°ç‚¹ï¼š1ï¼‰è‡ªé€‚åº”æ¡ä»¶æ¡¥ï¼ˆACBï¼‰å®ç°è¯­ä¹‰ç‰¹å¾çš„é«˜æ•ˆå¤šå°ºåº¦èåˆï¼›2ï¼‰ä¸€ç§æ–°çš„æ‹“æ‰‘å»å™ªä¸€è‡´æ€§ï¼ˆTDCï¼‰æŸå¤±ç»„ä»¶ï¼Œé€šè¿‡æƒ©ç½šå»å™ªè¿‡ç¨‹ä¸­çš„ç»“æ„ä¸ä¸€è‡´æ€§æ¥è§„èŒƒè®­ç»ƒï¼›3ï¼‰åŒå¤´æ¶æ„åˆ©ç”¨å»å™ªç›®æ ‡ä½œä¸ºå¼ºå¤§çš„æ­£åˆ™åŒ–å™¨ï¼Œä½¿è½»é‡çº§è¾…åŠ©å¤´èƒ½å¤Ÿåœ¨è¾ƒå°æ•°æ®é›†ä¸Šæ‰§è¡Œå¿«é€Ÿè€Œå‡†ç¡®çš„æ¨ç†ï¼Œä»¥åŠä¸€ä¸ªå™ªå£°é¢„æµ‹å¤´ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨BUSIä¸Šå®ç°äº†0.96çš„Diceå¾—åˆ†ï¼Œåœ¨BrEaSTä¸Šå®ç°äº†0.90çš„Diceå¾—åˆ†ï¼Œä»¥åŠåœ¨BUS-UCLMä¸Šå®ç°äº†0.97çš„Diceå¾—åˆ†ã€‚ç»¼åˆæ¶ˆèç ”ç©¶ç»éªŒæ€§åœ°éªŒè¯äº†æ¨¡å‹ç»„ä»¶å¯¹äºå®ç°è¿™äº›ç»“æœå’Œäº§ç”Ÿæ—¢å‡†ç¡®åˆè§£å‰–ä¸Šåˆç†åˆ†å‰²çš„å…³é”®ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05989v1">PDF</a> 5 pages, 2 figures, 3 tables, submitted to ISBI 2026</p>
<p><strong>Summary</strong><br>    é’ˆå¯¹ä¹³è…ºè¶…å£°å›¾åƒä¸­çš„ç—…ç¶åˆ†å‰²éš¾é¢˜ï¼Œå¦‚ä½å¯¹æ¯”åº¦ã€æ–‘ç‚¹å™ªå£°å’Œè¾¹ç•Œä¸æ¸…ç­‰é—®é¢˜ï¼Œæå‡ºä¸€ç§ç»“åˆå¢å¼ºå‹UNetç”Ÿæˆè§£ç å™¨ä¸Vision Transformerï¼ˆViTï¼‰ç¼–ç å™¨çš„çµæ´»æ¡ä»¶å»å™ªæ‰©æ•£æ¨¡å‹ã€‚æ¨¡å‹å¼•å…¥ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”æ¡ä»¶æ¡¥ï¼ˆACBï¼‰ã€æ‹“æ‰‘å»å™ªä¸€è‡´æ€§ï¼ˆTDCï¼‰æŸå¤±ç»„ä»¶å’ŒåŒå¤´æ¶æ„ï¼Œå®ç°é«˜æ•ˆå¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾èåˆã€ç»“æ„ä¸ä¸€è‡´æ€§çš„è®­ç»ƒæƒ©ç½šä»¥åŠå¿«é€Ÿå‡†ç¡®çš„å°æ•°æ®é›†æ¨ç†ã€‚åœ¨å…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šå–å¾—æœ€æ–°æˆæœï¼ŒDiceå¾—åˆ†è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¹³è…ºè¶…å£°å›¾åƒä¸­çš„ç—…ç¶åˆ†å‰²å¯¹äºæ—©æœŸè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨ä½å¯¹æ¯”åº¦ã€æ–‘ç‚¹å™ªå£°å’Œè¾¹ç•Œä¸æ¸…ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¹³è…ºè¶…å£°å›¾åƒç—…ç¶åˆ†å‰²ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†æ ‡å‡†å·ç§¯æ¶æ„é€šå¸¸æ— æ³•æ•è·è¶³å¤Ÿçš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„çµæ´»æ¡ä»¶å»å™ªæ‰©æ•£æ¨¡å‹ç»“åˆäº†å¢å¼ºå‹UNetç”Ÿæˆè§£ç å™¨å’ŒVision Transformerï¼ˆViTï¼‰ç¼–ç å™¨ï¼Œä»¥æé«˜åˆ†å‰²ç²¾åº¦å’Œæ•ˆç‡ã€‚</li>
<li>æ¨¡å‹å¼•å…¥ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯ï¼šè‡ªé€‚åº”æ¡ä»¶æ¡¥ï¼ˆACBï¼‰ã€æ‹“æ‰‘å»å™ªä¸€è‡´æ€§ï¼ˆTDCï¼‰æŸå¤±ç»„ä»¶å’ŒåŒå¤´æ¶æ„ã€‚</li>
<li>ACBå®ç°å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾èåˆï¼Œæé«˜åˆ†å‰²æ•ˆç‡ã€‚</li>
<li>TDCæŸå¤±ç»„ä»¶é€šè¿‡æƒ©ç½šç»“æ„ä¸ä¸€è‡´æ€§æ¥æ­£åˆ™åŒ–è®­ç»ƒï¼Œæé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŒå¤´æ¶æ„åˆ©ç”¨å»å™ªç›®æ ‡ä½œä¸ºå¼ºå¤§æ­£åˆ™åŒ–å™¨ï¼Œå…è®¸åœ¨å°å‹æ•°æ®é›†ä¸Šè¿›è¡Œå¿«é€Ÿå‡†ç¡®çš„æ¨ç†ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒDiceå¾—åˆ†é¢†å…ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfc1fb58eecbd74b4b2f1df70397dce5" align="middle">
<img src="https://picx.zhimg.com/v2-e9ebbfae91e260c2e641ad9cb67908c3" align="middle">
<img src="https://picx.zhimg.com/v2-6796816b46bd7dcff5228928a50bec80" align="middle">
<img src="https://picx.zhimg.com/v2-a371c8ca0bdff79cd1f4fe2dec66e4aa" align="middle">
<img src="https://picx.zhimg.com/v2-a28b29948c60f73fa27cf18feefa0063" align="middle">
<img src="https://picx.zhimg.com/v2-01572a57df002f872fd868117dc290f0" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DiA-gnostic-VLVAE-Disentangled-Alignment-Constrained-Vision-Language-Variational-AutoEncoder-for-Robust-Radiology-Reporting-with-Missing-Modalities"><a href="#DiA-gnostic-VLVAE-Disentangled-Alignment-Constrained-Vision-Language-Variational-AutoEncoder-for-Robust-Radiology-Reporting-with-Missing-Modalities" class="headerlink" title="DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities"></a>DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities</h2><p><strong>Authors:Nagur Shareef Shaik, Teja Krishna Cherukuri, Adnan Masood, Dong Hye Ye</strong></p>
<p>The integration of medical images with clinical context is essential for generating accurate and clinically interpretable radiology reports. However, current automated methods often rely on resource-heavy Large Language Models (LLMs) or static knowledge graphs and struggle with two fundamental challenges in real-world clinical data: (1) missing modalities, such as incomplete clinical context , and (2) feature entanglement, where mixed modality-specific and shared information leads to suboptimal fusion and clinically unfaithful hallucinated findings. To address these challenges, we propose the DiA-gnostic VLVAE, which achieves robust radiology reporting through Disentangled Alignment. Our framework is designed to be resilient to missing modalities by disentangling shared and modality-specific features using a Mixture-of-Experts (MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained optimization objective enforces orthogonality and alignment between these latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder then uses these disentangled representations to generate reports efficiently. On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4 scores of 0.266 and 0.134, respectively. Experimental results show that the proposed method significantly outperforms state-of-the-art models.</p>
<blockquote>
<p>å°†åŒ»å­¦å›¾åƒä¸ä¸´åºŠèƒŒæ™¯ç›¸ç»“åˆå¯¹äºç”Ÿæˆå‡†ç¡®ä¸”å¯ä¸´åºŠè§£è¯»çš„æ”¾å°„å­¦æŠ¥å‘Šè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è‡ªåŠ¨åŒ–æ–¹æ³•å¸¸å¸¸ä¾èµ–äºèµ„æºå¯†é›†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æˆ–é™æ€çŸ¥è¯†å›¾è°±ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®é¢ä¸´ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç¼ºå¤±æ¨¡æ€ï¼Œä¾‹å¦‚ä¸å®Œæ•´çš„ä¸´åºŠèƒŒæ™¯ï¼›ï¼ˆ2ï¼‰ç‰¹å¾çº ç¼ ï¼Œå…¶ä¸­æ··åˆçš„æ¨¡æ€ç‰¹å®šå’Œå…±äº«ä¿¡æ¯å¯¼è‡´æ¬¡ä¼˜èåˆå’Œä¸´åºŠä¸Šä¸çœŸå®çš„å¹»æƒ³å‘ç°ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯Šæ–­å¼VLVAEModelï¼ˆDiA-gnostic VLVAEï¼‰ï¼Œå®ƒé€šè¿‡è§£çº ç¼ å¯¹é½å®ç°äº†ç¨³å¥çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¡†æ¶æ—¨åœ¨é€šè¿‡ä¸“å®¶æ··åˆï¼ˆMoEï¼‰çš„è§†è¯­è¨€å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVision-Language Variational Autoencoderï¼ŒVLAVAEï¼‰æ¥è§£çº ç¼ å…±äº«å’Œæ¨¡æ€ç‰¹å®šç‰¹å¾ï¼Œä»è€Œå¯¹ç¼ºå¤±çš„æ¨¡æ€å…·æœ‰éŸ§æ€§ã€‚çº¦æŸä¼˜åŒ–ç›®æ ‡å¼ºåˆ¶æ‰§è¡Œè¿™äº›æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„æ­£äº¤æ€§å’Œå¯¹é½ï¼Œä»¥é˜²æ­¢æ¬¡ä¼˜èåˆã€‚ç„¶åï¼Œä¸€ä¸ªç´§å‡‘çš„LLaMA-Xè§£ç å™¨ä½¿ç”¨è¿™äº›è§£çº ç¼ çš„è¡¨ç¤ºæœ‰æ•ˆåœ°ç”ŸæˆæŠ¥å‘Šã€‚åœ¨IU Xå…‰ç‰‡å’ŒMIMIC-CXRæ•°æ®é›†ä¸Šï¼ŒDiAå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„BLEU@4å¾—åˆ†åˆ†åˆ«ä¸º0.266å’Œ0.134ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05968v1">PDF</a> Accepted for Oral Presentation at the 40th AAAI Conference on Artificial Intelligence (AAAI-26), Main Technical Track</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒåŒ»å­¦å›¾åƒä¸ä¸´åºŠèƒŒæ™¯çš„èåˆå¯¹ç”Ÿæˆå‡†ç¡®ä¸”å¯è§£é‡Šçš„æ”¾å°„å­¦æŠ¥å‘Šçš„é‡è¦æ€§ã€‚é’ˆå¯¹ç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•é¢ä¸´å¦‚ç¼ºå¤±æ¨¡æ€å’Œç‰¹å¾çº ç¼ ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†DiA-gnostic VLVAEæ¡†æ¶ï¼Œé€šè¿‡è§£çº ç¼ å¯¹é½å®ç°ç¨³å¥çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºMoEçš„Vision-Language Variational Autoencoder (VLVAE)æ¥åˆ†ç¦»å…±äº«å’Œæ¨¡æ€ç‰¹å®šç‰¹å¾ï¼Œå¹¶é€šè¿‡çº¦æŸä¼˜åŒ–ç›®æ ‡å®ç°è¿™äº›æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„æ­£äº¤æ€§å’Œå¯¹é½ï¼Œä»è€Œé˜²æ­¢æ¬¡ä¼˜èåˆã€‚ä½¿ç”¨LLaMA-Xè§£ç å™¨ç”ŸæˆæŠ¥å‘Šã€‚åœ¨IU X-Rayå’ŒMIMIC-CXRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸ä¸´åºŠèƒŒæ™¯çš„æ•´åˆå¯¹ç”Ÿæˆå‡†ç¡®ä¸”å¯è§£é‡Šçš„æ”¾å°„å­¦æŠ¥å‘Šè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è‡ªåŠ¨åŒ–æ–¹æ³•é¢ä¸´ç¼ºå¤±æ¨¡æ€å’Œç‰¹å¾çº ç¼ çš„æŒ‘æˆ˜ã€‚</li>
<li>DiA-gnostic VLVAEæ¡†æ¶é€šè¿‡è§£çº ç¼ å¯¹é½è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°ç¨³å¥çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨MoEçš„Vision-Language Variational Autoencoder (VLVAE)åˆ†ç¦»å…±äº«å’Œæ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚</li>
<li>é€šè¿‡çº¦æŸä¼˜åŒ–ç›®æ ‡å®ç°æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„æ­£äº¤æ€§å’Œå¯¹é½ï¼Œé˜²æ­¢æ¬¡ä¼˜èåˆã€‚</li>
<li>ä½¿ç”¨LLaMA-Xè§£ç å™¨é«˜æ•ˆç”ŸæˆæŠ¥å‘Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79ff4efead2f6d2b1a5ba8dea013c915" align="middle">
<img src="https://picx.zhimg.com/v2-6d5e9ab637114323cd0d9f5fba1557d1" align="middle">
<img src="https://picx.zhimg.com/v2-14d7a0b4b185d7fbd7ecbbe8acbdf449" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-734b1fbcf79a02b66dac1912d553c271" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  MOSPA Human Motion Generation Driven by Spatial Audio
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-18a3919ebb20bedf87432f0f2cac6324" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian Responses
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
