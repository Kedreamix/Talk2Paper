<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  From 2D to 3D Without Extra Baggage Data-Efficient Cancer Detection in Digital Breast Tomosynthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1a57e3c01ad6a4f9e38dd8e77c42443a')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-17-æ›´æ–°"><a href="#2025-11-17-æ›´æ–°" class="headerlink" title="2025-11-17 æ›´æ–°"></a>2025-11-17 æ›´æ–°</h1><h2 id="From-2D-to-3D-Without-Extra-Baggage-Data-Efficient-Cancer-Detection-in-Digital-Breast-Tomosynthesis"><a href="#From-2D-to-3D-Without-Extra-Baggage-Data-Efficient-Cancer-Detection-in-Digital-Breast-Tomosynthesis" class="headerlink" title="From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis"></a>From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis</h2><p><strong>Authors:Yen Nhi Truong Vu, Dan Guo, Sripad Joshi, Harshit Kumar, Jason Su, Thomas Paul Matthews</strong></p>
<p>Digital Breast Tomosynthesis (DBT) enhances finding visibility for breast cancer detection by providing volumetric information that reduces the impact of overlapping tissues; however, limited annotated data has constrained the development of deep learning models for DBT. To address data scarcity, existing methods attempt to reuse 2D full-field digital mammography (FFDM) models by either flattening DBT volumes or processing slices individually, thus discarding volumetric information. Alternatively, 3D reasoning approaches introduce complex architectures that require more DBT training data. Tackling these drawbacks, we propose M&amp;M-3D, an architecture that enables learnable 3D reasoning while remaining parameter-free relative to its FFDM counterpart, M&amp;M. M&amp;M-3D constructs malignancy-guided 3D features, and 3D reasoning is learned through repeatedly mixing these 3D features with slice-level information. This is achieved by modifying operations in M&amp;M without adding parameters, thus enabling direct weight transfer from FFDM. Extensive experiments show that M&amp;M-3D surpasses 2D projection and 3D slice-based methods by 11-54% for localization and 3-10% for classification. Additionally, M&amp;M-3D outperforms complex 3D reasoning variants by 20-47% for localization and 2-10% for classification in the low-data regime, while matching their performance in high-data regime. On the popular BCS-DBT benchmark, M&amp;M-3D outperforms previous top baseline by 4% for classification and 10% for localization.</p>
<blockquote>
<p>æ•°å­—ä¹³è…ºæ–­å±‚åˆæˆæŠ€æœ¯ï¼ˆDBTï¼‰é€šè¿‡æä¾›ä½“ç§¯ä¿¡æ¯å‡å°‘äº†é‡å ç»„ç»‡çš„å½±å“ï¼Œæé«˜äº†ä¹³è…ºç™Œæ£€æµ‹çš„å¯è§æ€§ã€‚ç„¶è€Œï¼Œæ ‡æ³¨æ•°æ®çš„æœ‰é™é™åˆ¶äº†DBTæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡é‡æ–°ä½¿ç”¨äºŒç»´å…¨è§†é‡æ•°å­—ä¹³è…ºæ‘„å½±ï¼ˆFFDMï¼‰æ¨¡å‹æ¥åˆ©ç”¨DBTæ•°æ®ï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆå°†DBTä½“ç§¯å‹å¹³ï¼Œè¦ä¹ˆå•ç‹¬å¤„ç†åˆ‡ç‰‡ï¼Œä»è€Œä¸¢å¼ƒäº†ä½“ç§¯ä¿¡æ¯ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯é‡‡ç”¨ä¸‰ç»´æ¨ç†æ–¹æ³•ï¼Œä½†è¿™éœ€è¦å¼•å…¥æ›´å¤æ‚çš„æ¶æ„å’Œæ›´å¤šçš„DBTè®­ç»ƒæ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†M&amp;M-3Dæ¶æ„ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸‰ç»´ç©ºé—´è¿›è¡Œæ¨ç†å­¦ä¹ ï¼ŒåŒæ—¶ç›¸å¯¹äºå…¶FFDMå¯¹åº”æ¨¡å‹M&amp;Mè€Œè¨€æ— éœ€å‚æ•°ã€‚M&amp;M-3Dæ„å»ºäº†ä»¥æ¶æ€§ä¸ºå¼•å¯¼çš„ä¸‰ç»´ç‰¹å¾ï¼Œå¹¶é€šè¿‡åå¤æ··åˆè¿™äº›ä¸‰ç»´ç‰¹å¾ä¸åˆ‡ç‰‡çº§åˆ«çš„ä¿¡æ¯æ¥å­¦ä¹ ä¸‰ç»´æ¨ç†ã€‚è¿™æ˜¯é€šè¿‡åœ¨M&amp;Mä¸­ä¿®æ”¹æ“ä½œè€Œæ— éœ€æ·»åŠ å‚æ•°æ¥å®ç°çš„ï¼Œä»è€Œå®ç°äº†ä»FFDMçš„ç›´æ¥æƒé‡è½¬ç§»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å®šä½å’Œåˆ†ç±»æ–¹é¢ï¼ŒM&amp;M-3Dè¶…è¶Šäº†äºŒç»´æŠ•å½±å’ŒåŸºäºä¸‰ç»´åˆ‡ç‰‡çš„æ–¹æ³•ï¼Œåˆ†åˆ«æé«˜äº†11-54%å’Œ3-10%ã€‚æ­¤å¤–ï¼Œåœ¨ä½æ•°æ®æƒ…å†µä¸‹ï¼ŒM&amp;M-3Dåœ¨å®šä½å’Œåˆ†ç±»æ–¹é¢åˆ†åˆ«ä¼˜äºå¤æ‚çš„ä¸‰ç»´æ¨ç†å˜ä½“20-47%å’Œ2-10%ï¼Œè€Œåœ¨é«˜æ•°æ®æƒ…å†µä¸‹åˆ™ä¸ä¹‹ç›¸åŒ¹é…ã€‚åœ¨æµè¡Œçš„BCS-DBTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒM&amp;M-3Dåœ¨åˆ†ç±»æ–¹é¢ä¼˜äºå…ˆå‰çš„æœ€ä½³åŸºå‡†æ¨¡å‹4%ï¼Œåœ¨å®šä½æ–¹é¢æé«˜äº†10%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10597v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    DBTæŠ€æœ¯æé«˜äº†ä¹³è…ºç™Œæ£€æµ‹çš„å¯è§æ€§ï¼Œä½†æ ‡æ³¨æ•°æ®æœ‰é™åˆ¶çº¦äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸ºè§£å†‘æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæœ‰äººå°è¯•å¤ç”¨2Då…¨æ•°å­—ä¹³è…ºé’¼é¶æ‘„å½±ï¼ˆFFDMï¼‰æ¨¡å‹ï¼Œä½†ä¼šä¸¢å¤±ä½“ç§¯ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºM&amp;M-3Dæ¶æ„ï¼Œå¯åœ¨æ— éœ€æ–°å¢å‚æ•°çš„æƒ…å†µä¸‹å®ç°ä¸‰ç»´æ¨ç†å­¦ä¹ ï¼Œä¸”ç›¸å¯¹FFDMå¯¹åº”æ¶æ„M&amp;Må…·æœ‰ä¼˜å¼‚æ€§èƒ½ã€‚M&amp;M-3Dé€šè¿‡æ„å»ºæ¶æ€§å¼•å¯¼çš„ä¸‰ç»´ç‰¹å¾ï¼Œå¹¶åå¤æ··åˆè¿™äº›ç‰¹å¾ä¸åˆ‡ç‰‡ä¿¡æ¯æ¥å­¦ä¹ ä¸‰ç»´æ¨ç†ã€‚å®éªŒè¯æ˜ï¼ŒM&amp;M-3Dåœ¨å®šä½å’Œåˆ†ç±»ä»»åŠ¡ä¸Šåˆ†åˆ«è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•è¾¾11-54%å’Œ3-10%ã€‚åœ¨ä½æ•°æ®åœºæ™¯ä¸‹ï¼Œå…¶æ€§èƒ½ä¼˜äºå¤æ‚çš„ä¸‰ç»´æ¨ç†å˜ä½“è¾¾20-47%å’Œ2-10%ï¼Œä½†åœ¨é«˜æ•°æ®åœºæ™¯ä¸‹ä¸ä¹‹åŒ¹é…ã€‚åœ¨BCS-DBTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒM&amp;M-3Dçš„åˆ†ç±»å’Œå®šä½æ€§èƒ½å‡ä¼˜äºç°æœ‰æœ€ä½³åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DBTæŠ€æœ¯é€šè¿‡æä¾›ä½“ç§¯ä¿¡æ¯æé«˜ä¹³è…ºç™Œæ£€æµ‹çš„å¯è§æ€§ï¼Œä½†æ ‡æ³¨æ•°æ®æœ‰é™é™åˆ¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å¤ç”¨FFDMæ¨¡å‹æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä½†è¿™æ ·åšä¼šä¸¢å¤±ä½“ç§¯ä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„M&amp;M-3Dæ¶æ„å¯åœ¨æ— éœ€æ–°å¢å‚æ•°çš„æƒ…å†µä¸‹å®ç°ä¸‰ç»´æ¨ç†å­¦ä¹ ï¼Œå…·æœ‰ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>M&amp;M-3Dé€šè¿‡æ„å»ºæ¶æ€§å¼•å¯¼çš„ä¸‰ç»´ç‰¹å¾ï¼Œå¹¶åå¤æ··åˆè¿™äº›ç‰¹å¾ä¸åˆ‡ç‰‡ä¿¡æ¯æ¥å­¦ä¹ ä¸‰ç»´æ¨ç†ã€‚</li>
<li>M&amp;M-3Dåœ¨å®šä½å’Œåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ä»¥åŠå¤æ‚çš„ä¸‰ç»´æ¨ç†å˜ä½“ã€‚</li>
<li>åœ¨BCS-DBTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒM&amp;M-3Dçš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€ä½³åŸºçº¿æ¨¡å‹ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºä¹³è…ºç™Œæ£€æµ‹æä¾›äº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ï¼Œå…·æœ‰æ½œåœ¨çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97b8c73981863feeeb179b3b248b3356" align="middle">
<img src="https://picx.zhimg.com/v2-de10952de19c806c96c07850f887a62a" align="middle">
<img src="https://picx.zhimg.com/v2-9ba232ac7ed3d1251ae82c72b3670b81" align="middle">
<img src="https://picx.zhimg.com/v2-29a024020e29765c9caa56f3ffe3de4e" align="middle">
<img src="https://picx.zhimg.com/v2-2bca68b65d97a370b3ab92c2fb2209e5" align="middle">
<img src="https://picx.zhimg.com/v2-e943d1fe3641b9507715cfde9379f6a6" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Right-Looks-Wrong-Reasons-Compositional-Fidelity-in-Text-to-Image-Generation"><a href="#Right-Looks-Wrong-Reasons-Compositional-Fidelity-in-Text-to-Image-Generation" class="headerlink" title="Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation"></a>Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation</h2><p><strong>Authors:Mayank Vatsa, Aparna Bharati, Richa Singh</strong></p>
<p>The architectural blueprint of todayâ€™s leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.</p>
<blockquote>
<p>å½“å‰é¢†å…ˆçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ¶æ„è“å›¾å­˜åœ¨ä¸€ä¸ªåŸºæœ¬ç¼ºé™·ï¼šæ— æ³•å¤„ç†é€»è¾‘ç»„åˆã€‚è¿™ç¯‡ç»¼è¿°è°ƒæŸ¥äº†ä¸‰å¤§æ ¸å¿ƒåŸå§‹é—®é¢˜â€”â€”å¦å®šã€è®¡æ•°å’Œç©ºé—´å…³ç³»çš„è¿™ä¸ªç¼ºé™·ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†æ€§èƒ½çš„å¤§å¹…å´©æºƒï¼šåœ¨å•ä¸ªåŸå§‹é—®é¢˜ä¸Šæ˜¯å‡†ç¡®çš„æ¨¡å‹ï¼Œåœ¨è¿™äº›é—®é¢˜ç»“åˆèµ·æ¥æ—¶å´è¿…é€Ÿå¤±è´¥ï¼Œæš´éœ²äº†ä¸¥é‡çš„å¹²æ‰°ã€‚æˆ‘ä»¬å°†è¿™ç§å¤±è´¥è¿½æŸ¥åˆ°ä¸‰ä¸ªå…³é”®å› ç´ ã€‚é¦–å…ˆï¼Œè®­ç»ƒæ•°æ®å‡ ä¹å®Œå…¨ç¼ºä¹æ˜ç¡®çš„å¦å®šã€‚å…¶æ¬¡ï¼Œè¿ç»­æ³¨æ„åŠ›æ¶æ„ä»æ ¹æœ¬ä¸Šä¸é€‚åˆç¦»æ•£é€»è¾‘ã€‚ç¬¬ä¸‰ï¼Œè¯„ä¼°æŒ‡æ ‡å¥–åŠ±è§†è§‰å¯ä¿¡åº¦è€Œä¸æ˜¯çº¦æŸæ»¡è¶³ã€‚é€šè¿‡åˆ†ææœ€è¿‘çš„åŸºå‡†æµ‹è¯•å’Œæ–¹æ³•ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œå½“å‰è§£å†³æ–¹æ¡ˆå’Œç®€å•æ‰©å±•æ— æ³•å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œå®ç°çœŸæ­£çš„ç»„åˆæ€§å°†éœ€è¦è¡¨ç¤ºå’Œæ¨ç†æ–¹é¢çš„æ ¹æœ¬æ€§è¿›æ­¥ï¼Œè€Œä¸æ˜¯å¯¹ç°æœ‰æ¶æ„çš„å¢é‡è°ƒæ•´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10136v1">PDF</a> Accepted in AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« æŒ‡å‡ºå½“å‰é¢†å…ˆçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å­˜åœ¨æ ¹æœ¬ç¼ºé™·ï¼Œå³æ— æ³•å¤„ç†é€»è¾‘ç»„åˆã€‚è°ƒæŸ¥åˆ†æäº†ä¸‰ä¸ªæ ¸å¿ƒåŸå§‹é—®é¢˜ï¼šå¦å®šã€è®¡æ•°å’Œç©ºé—´å…³ç³»ã€‚åˆ†ææ˜¾ç¤ºæ¨¡å‹åœ¨å•ä¸€åŸå§‹é—®é¢˜ä¸Šçš„å‡†ç¡®æ€§åœ¨ç»„åˆæ—¶æ€¥å‰§ä¸‹é™ï¼Œæš´éœ²å‡ºä¸¥é‡å¹²æ‰°ã€‚æ–‡ç« è¿½è¸ªäº†è¿™ç§å¤±è´¥çš„ä¸‰ä¸ªå…³é”®å› ç´ ï¼šè®­ç»ƒæ•°æ®ç¼ºä¹æ˜ç¡®å¦å®šï¼›è¿ç»­æ³¨æ„åŠ›æ¶æ„ä¸é€‚åˆç¦»æ•£é€»è¾‘ï¼›è¯„ä¼°æŒ‡æ ‡æ›´é‡è§†è§†è§‰å¯ä¿¡åº¦è€Œéçº¦æŸæ»¡è¶³ã€‚æ–‡ç« è¡¨æ˜ï¼Œå®ç°çœŸæ­£çš„ç»„åˆæ€§éœ€è¦ä»£è¡¨å’Œæ¨ç†æ–¹é¢çš„æ ¹æœ¬æ€§è¿›å±•ï¼Œè€Œéå¯¹ç°æœ‰æ¶æ„çš„å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰é¢†å…ˆçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å­˜åœ¨æ— æ³•å¤„ç†é€»è¾‘ç»„åˆçš„æ ¹æœ¬ç¼ºé™·ã€‚</li>
<li>æ¨¡å‹åœ¨ç»„åˆå¦å®šã€è®¡æ•°å’Œç©ºé—´å…³ç³»ç­‰æ ¸å¿ƒåŸå§‹é—®é¢˜æ—¶è¡¨ç°æ€¥å‰§ä¸‹é™ã€‚</li>
<li>è®­ç»ƒæ•°æ®å‡ ä¹å®Œå…¨ç¼ºä¹æ˜ç¡®çš„å¦å®šä¿¡æ¯ã€‚</li>
<li>è¿ç»­æ³¨æ„åŠ›æ¶æ„ä¸é€‚åˆå¤„ç†ç¦»æ•£é€»è¾‘é—®é¢˜ã€‚</li>
<li>è¯„ä¼°æŒ‡æ ‡æ›´æ³¨é‡è§†è§‰å¯ä¿¡åº¦ï¼Œè€Œéçº¦æŸæ»¡è¶³ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆå’Œç®€å•æ‰©å±•æ— æ³•å¼¥è¡¥è¿™ä¸€å·®è·ã€‚</li>
<li>å®ç°çœŸæ­£çš„ç»„åˆæ€§éœ€è¦ä»£è¡¨å’Œæ¨ç†æ–¹é¢çš„æ ¹æœ¬æ€§è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4011900b4b9fbe144cfffbf284a4a14" align="middle">
<img src="https://picx.zhimg.com/v2-a87751d364d32028496c717c1b436b20" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Opinion-Towards-Unified-Expressive-Policy-Optimization-for-Robust-Robot-Learning"><a href="#Opinion-Towards-Unified-Expressive-Policy-Optimization-for-Robust-Robot-Learning" class="headerlink" title="Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning"></a>Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning</h2><p><strong>Authors:Haidong Huang, Haiyue Zhu. Jiayu Song, Xixin Zhao, Yaohua Zhou, Jiayi Zhang, Yuze Zhai, Xiaocong Li</strong></p>
<p>Offline-to-online reinforcement learning (O2O-RL) has emerged as a promising paradigm for safe and efficient robotic policy deployment but suffers from two fundamental challenges: limited coverage of multimodal behaviors and distributional shifts during online adaptation. We propose UEPO, a unified generative framework inspired by large language model pretraining and fine-tuning strategies. Our contributions are threefold: (1) a multi-seed dynamics-aware diffusion policy that efficiently captures diverse modalities without training multiple models; (2) a dynamic divergence regularization mechanism that enforces physically meaningful policy diversity; and (3) a diffusion-based data augmentation module that enhances dynamics model generalization. On the D4RL benchmark, UEPO achieves +5.9% absolute improvement over Uni-O4 on locomotion tasks and +12.4% on dexterous manipulation, demonstrating strong generalization and scalability.</p>
<blockquote>
<p>ç¦»çº¿åˆ°åœ¨çº¿çš„å¼ºåŒ–å­¦ä¹ ï¼ˆO2O-RLï¼‰å·²ç»æˆä¸ºå®‰å…¨æœ‰æ•ˆçš„æœºå™¨äººç­–ç•¥éƒ¨ç½²çš„å¾ˆæœ‰å‰æ™¯çš„æ¨¡å¼ï¼Œä½†å®ƒé¢ä¸´ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šå¤šæ¨¡å¼è¡Œä¸ºçš„è¦†ç›–æœ‰é™å’Œåœ¨çº¿é€‚åº”è¿‡ç¨‹ä¸­çš„åˆ†å¸ƒåç§»ã€‚æˆ‘ä»¬æå‡ºäº†UEPOï¼Œè¿™æ˜¯ä¸€ä¸ªå—å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥å¯å‘çš„ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸‰ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€ç§å¤šç§å­åŠ¨åŠ›å­¦æ„ŸçŸ¥æ‰©æ•£ç­–ç•¥ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•è·å¤šç§æ¨¡å¼ï¼Œè€Œæ— éœ€è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼›ï¼ˆ2ï¼‰ä¸€ç§åŠ¨æ€å‘æ•£æ­£åˆ™åŒ–æœºåˆ¶ï¼Œå¼ºåˆ¶å®æ–½å…·æœ‰ç‰©ç†æ„ä¹‰çš„ç­–ç•¥å¤šæ ·æ€§ï¼›ï¼ˆ3ï¼‰ä¸€ç§åŸºäºæ‰©æ•£çš„æ•°æ®å¢å¼ºæ¨¡å—ï¼Œæé«˜äº†åŠ¨åŠ›å­¦æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUEPOåœ¨æ­¥æ€ä»»åŠ¡ä¸Šç›¸å¯¹äºUni-O4å®ç°äº†+5.9%çš„ç»å¯¹æ”¹è¿›ï¼Œåœ¨ç²¾ç»†æ“ä½œä»»åŠ¡ä¸Šå®ç°äº†+12.4%çš„æ”¹è¿›ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10087v1">PDF</a> Accepted by NeurIPS 2025 Workshop on Embodied World Models for Decision Making</p>
<p><strong>Summary</strong>:<br>ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆO2O-RLï¼‰åœ¨æœºå™¨äººæ”¿ç­–éƒ¨ç½²ä¸­è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´å¤šæ¨¡æ€è¡Œä¸ºè¦†ç›–æœ‰é™å’Œåœ¨çº¿é€‚åº”è¿‡ç¨‹ä¸­çš„åˆ†å¸ƒåç§»ä¸¤å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºUEPOï¼Œä¸€ä¸ªå—å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥å¯å‘çš„ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶ã€‚è´¡çŒ®æœ‰ä¸‰ç‚¹ï¼šä¸€æ˜¯å¤šç§å­åŠ¨æ€æ„ŸçŸ¥æ‰©æ•£ç­–ç•¥ï¼Œèƒ½é«˜æ•ˆæ•è·å¤šç§æ¨¡å¼è€Œæ— éœ€è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼›äºŒæ˜¯åŠ¨æ€å‘æ•£æ­£åˆ™åŒ–æœºåˆ¶ï¼Œå¼ºåˆ¶å®æ–½å…·æœ‰ç‰©ç†æ„ä¹‰çš„æ”¿ç­–å¤šæ ·æ€§ï¼›ä¸‰æ˜¯åŸºäºæ‰©æ•£çš„æ•°æ®å¢å¼ºæ¨¡å—ï¼Œæé«˜åŠ¨æ€æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚åœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­ï¼ŒUEPOåœ¨æ­¥æ€ä»»åŠ¡ä¸Šç›¸å¯¹äºUni-O4çš„ç»å¯¹æ”¹è¿›ç‡ä¸º+5.9%ï¼Œåœ¨ç²¾ç»†æ“ä½œä»»åŠ¡ä¸Šä¸º+12.4%ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>O2O-RLé¢ä¸´å¤šæ¨¡æ€è¡Œä¸ºè¦†ç›–æœ‰é™å’Œåœ¨çº¿é€‚åº”åˆ†å¸ƒåç§»çš„æŒ‘æˆ˜ã€‚</li>
<li>UEPOæ˜¯ä¸€ä¸ªç»Ÿä¸€ç”Ÿæˆæ¡†æ¶ï¼Œå—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥çš„å¯å‘ã€‚</li>
<li>UEPOé‡‡ç”¨å¤šç§å­åŠ¨æ€æ„ŸçŸ¥æ‰©æ•£ç­–ç•¥ï¼Œèƒ½é«˜æ•ˆæ•è·å¤šç§æ¨¡å¼ã€‚</li>
<li>UEPOå®æ–½åŠ¨æ€å‘æ•£æ­£åˆ™åŒ–æœºåˆ¶ï¼Œä¿ƒè¿›æ”¿ç­–å¤šæ ·æ€§ã€‚</li>
<li>UEPOåŒ…å«åŸºäºæ‰©æ•£çš„æ•°æ®å¢å¼ºæ¨¡å—ï¼Œæé«˜åŠ¨æ€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>UEPOåœ¨D4RLåŸºå‡†æµ‹è¯•ä¸­çš„æ­¥æ€ä»»åŠ¡æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42ef0d74bcc9e7d5a86ac8a556d1ff04" align="middle">
<img src="https://picx.zhimg.com/v2-21dfdaac9b3e337cfb58a1804bd5581c" align="middle">
<img src="https://picx.zhimg.com/v2-71e5519814690cedc1374f936c59cf5e" align="middle">
<img src="https://picx.zhimg.com/v2-f2fb17fce0af4653c0d8276fb379e7d4" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Format-Matters-The-Robustness-of-Multimodal-LLMs-in-Reviewing-Evidence-from-Tables-and-Charts"><a href="#Format-Matters-The-Robustness-of-Multimodal-LLMs-in-Reviewing-Evidence-from-Tables-and-Charts" class="headerlink" title="Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts"></a>Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts</h2><p><strong>Authors:Xanh Ho, Yun-Ang Wu, Sunisth Kumar, Florian Boudin, Atsuhiro Takasu, Akiko Aizawa</strong></p>
<p>With the growing number of submitted scientific papers, there is an increasing demand for systems that can assist reviewers in evaluating research claims. Experimental results are a core component of scientific work, often presented in varying formats such as tables or charts. Understanding how robust current multimodal large language models (multimodal LLMs) are at verifying scientific claims across different evidence formats remains an important and underexplored challenge. In this paper, we design and conduct a series of experiments to assess the ability of multimodal LLMs to verify scientific claims using both tables and charts as evidence. To enable this evaluation, we adapt two existing datasets of scientific papers by incorporating annotations and structures necessary for a multimodal claim verification task. Using this adapted dataset, we evaluate 12 multimodal LLMs and find that current models perform better with table-based evidence while struggling with chart-based evidence. We further conduct human evaluations and observe that humans maintain strong performance across both formats, unlike the models. Our analysis also reveals that smaller multimodal LLMs (under 8B) show weak correlation in performance between table-based and chart-based tasks, indicating limited cross-modal generalization. These findings highlight a critical gap in current modelsâ€™ multimodal reasoning capabilities. We suggest that future multimodal LLMs should place greater emphasis on improving chart understanding to better support scientific claim verification.</p>
<blockquote>
<p>éšç€æäº¤çš„ç§‘ç ”è®ºæ–‡æ•°é‡ä¸æ–­å¢åŠ ï¼Œå¯¹èƒ½å¤Ÿå¸®åŠ©å®¡ç¨¿äººè¯„ä¼°ç ”ç©¶è®ºç‚¹çš„ç³»ç»Ÿçš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚å®éªŒç»“æœæ˜¯ç§‘ç ”å·¥ä½œçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œé€šå¸¸ä»¥è¡¨æ ¼æˆ–å›¾è¡¨ç­‰å¤šç§å½¢å¼å‘ˆç°ã€‚äº†è§£å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆmultimodal LLMsï¼‰åœ¨ä¸åŒè¯æ®æ ¼å¼ä¸‹éªŒè¯ç§‘å­¦è®ºæ–­çš„ç¨³å¥æ€§ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦ä¸”å°šæœªè¢«å……åˆ†ç ”ç©¶çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡å¹¶å®æ–½äº†ä¸€ç³»åˆ—å®éªŒï¼Œä»¥è¯„ä¼°å¤šæ¨¡æ€LLMsä½¿ç”¨è¡¨æ ¼å’Œå›¾è¡¨ä½œä¸ºè¯æ®æ¥éªŒè¯ç§‘å­¦è®ºæ–­çš„èƒ½åŠ›ã€‚ä¸ºäº†è¿›è¡Œæ­¤è¯„ä¼°ï¼Œæˆ‘ä»¬é€‚åº”äº†ä¸¤å¥—ç§‘å­¦è®ºæ–‡æ•°æ®é›†ï¼Œå¹¶èå…¥äº†è¿›è¡Œå¤šæ¨¡æ€è®ºæ–­éªŒè¯ä»»åŠ¡æ‰€éœ€çš„æ³¨è§£å’Œç»“æ„ã€‚ä½¿ç”¨è¿™ä¸ªé€‚åº”åçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†12ä¸ªå¤šæ¨¡æ€LLMsï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨å¤„ç†åŸºäºè¡¨æ ¼çš„è¯æ®æ—¶è¡¨ç°æ›´å¥½ï¼Œè€Œåœ¨å¤„ç†åŸºäºå›¾è¡¨çš„è¯æ®æ—¶åˆ™è¾ƒä¸ºå›°éš¾ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†äººç±»è¯„ä¼°ï¼Œå¹¶è§‚å¯Ÿåˆ°äººç±»åœ¨è¿™ä¸¤ç§æ ¼å¼ä¸‹çš„è¡¨ç°éƒ½å¾ˆå‡ºè‰²ï¼Œä¸æ¨¡å‹ä¸åŒã€‚æˆ‘ä»¬çš„åˆ†æè¿˜æ˜¾ç¤ºï¼Œè¾ƒå°çš„å¤šæ¨¡æ€LLMsï¼ˆå°äº8Bï¼‰åœ¨åŸºäºè¡¨æ ¼å’ŒåŸºäºå›¾è¡¨çš„ä»»åŠ¡ä¹‹é—´æ€§èƒ½ç›¸å…³æ€§è¾ƒå¼±ï¼Œè¡¨æ˜è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®å·®è·ã€‚æˆ‘ä»¬å»ºè®®æœªæ¥çš„å¤šæ¨¡æ€LLMsåº”æ›´åŠ é‡è§†æ”¹è¿›å›¾è¡¨ç†è§£ï¼Œä»¥æ›´å¥½åœ°æ”¯æŒç§‘å­¦è®ºæ–­éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10075v1">PDF</a> Accepted at AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç§‘å­¦è®ºæ–‡è¯„å®¡ä¸­å¯¹ç ”ç©¶ä¸»å¼ çš„éªŒè¯éœ€æ±‚ï¼Œå¹¶é‡ç‚¹ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆmultimodal LLMsï¼‰åœ¨å¤„ç†ä¸åŒæ ¼å¼è¯æ®ï¼ˆå¦‚è¡¨å’Œå›¾è¡¨ï¼‰æ—¶çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨å¤„ç†åŸºäºå›¾è¡¨çš„è¯æ®æ—¶è¡¨ç°è¾ƒå¼±ï¼Œè€Œåœ¨å¤„ç†åŸºäºè¡¨æ ¼çš„è¯æ®æ—¶è¡¨ç°è¾ƒå¥½ã€‚åŒæ—¶ï¼Œå°å‹å¤šæ¨¡æ€LLMåœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°å‘ˆç°å‡ºå¼±ç›¸å…³æ€§ã€‚æœªæ¥å¤šæ¨¡æ€LLMåº”æ›´åŠ é‡è§†å›¾è¡¨ç†è§£èƒ½åŠ›çš„æå‡ï¼Œä»¥æ›´å¥½åœ°æ”¯æŒç§‘å­¦ä¸»å¼ çš„éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§‘å­¦è®ºæ–‡è¯„å®¡ä¸­å¯¹ç ”ç©¶ä¸»å¼ çš„éªŒè¯éœ€æ±‚å¢é•¿ï¼Œéœ€è¦ç³»ç»Ÿè¾…åŠ©è¯„å®¡ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆmultimodal LLMsï¼‰åœ¨å¤„ç†ç§‘å­¦è¯æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨å¤„ç†åŸºäºå›¾è¡¨çš„è¯æ®æ—¶è¡¨ç°è¾ƒå¼±ï¼Œå¤„ç†åŸºäºè¡¨æ ¼çš„è¯æ®æ—¶è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>å°å‹å¤šæ¨¡æ€LLMåœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°å‘ˆç°å‡ºå¼±ç›¸å…³æ€§ã€‚</li>
<li>äººç±»åœ¨ä¸¤ç§æ ¼å¼çš„è¯æ®ä¸Šéƒ½èƒ½ä¿æŒå¼ºè¡¨ç°ã€‚</li>
<li>æœªæ¥å¤šæ¨¡æ€LLMéœ€è¦æå‡å›¾è¡¨ç†è§£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00239502bf893ad16a19d1de5ae3d9ef" align="middle">
<img src="https://picx.zhimg.com/v2-aeddb1cd9f1c89e4bdbf45c4def7a4e7" align="middle">
<img src="https://picx.zhimg.com/v2-300a121cf75c53848a0f9189690045af" align="middle">
<img src="https://picx.zhimg.com/v2-acf37a4c34a6e3707639a8d35af23e6a" align="middle">
<img src="https://picx.zhimg.com/v2-1a57e3c01ad6a4f9e38dd8e77c42443a" align="middle">
<img src="https://picx.zhimg.com/v2-21682cd0995bec480df9127ea075f6db" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="When-Eyes-and-Ears-Disagree-Can-MLLMs-Discern-Audio-Visual-Confusion"><a href="#When-Eyes-and-Ears-Disagree-Can-MLLMs-Discern-Audio-Visual-Confusion" class="headerlink" title="When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?"></a>When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?</h2><p><strong>Authors:Qilang Ye, Wei Zeng, Meng Liu, Jie Zhang, Yupeng Hu, Zitong Yu, Yu Zhou</strong></p>
<p>Can Multimodal Large Language Models (MLLMs) discern confused objects that are visually present but audio-absent? To study this, we introduce a new benchmark, AV-ConfuseBench, which simulates an &#96;&#96;Audio-Visual Confusionâ€™â€™ scene by modifying the corresponding sound of an object in the video, e.g., mute the sounding object and ask MLLMs Is there a&#x2F;an muted-object soundâ€™â€™. Experimental results reveal that MLLMs, such as Qwen2.5-Omni and Gemini 2.5, struggle to discriminate non-existent audio due to visually dominated reasoning. Motivated by this observation, we introduce RL-CoMM, a Reinforcement Learning-based Collaborative Multi-MLLM that is built upon the Qwen2.5-Omni foundation. RL-CoMM includes two stages: 1) To alleviate visually dominated ambiguities, we introduce an external model, a Large Audio Language Model (LALM), as the reference model to generate audio-only reasoning. Then, we design a Step-wise Reasoning Reward function that enables MLLMs to self-improve audio-visual reasoning with the audio-only reference. 2) To ensure an accurate answer prediction, we introduce Answer-centered Confidence Optimization to reduce the uncertainty of potential heterogeneous reasoning differences. Extensive experiments on audio-visual question answering and audio-visual hallucination show that RL-CoMM improves the accuracy by 10~30% over the baseline model with limited training data. Follow: <a target="_blank" rel="noopener" href="https://github.com/rikeilong/AVConfusion">https://github.com/rikeilong/AVConfusion</a>.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¦åŒºåˆ†è§†è§‰ä¸Šå­˜åœ¨ä½†å¬è§‰ä¸å­˜åœ¨çš„æ··æ·†å¯¹è±¡ï¼Ÿä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•AV-ConfuseBenchï¼Œå®ƒé€šè¿‡ä¿®æ”¹è§†é¢‘ä¸­çš„å¯¹è±¡å£°éŸ³æ¥æ¨¡æ‹Ÿâ€œè§†å¬æ··æ·†â€åœºæ™¯ï¼Œä¾‹å¦‚ï¼Œé™éŸ³å‘å£°å¯¹è±¡å¹¶è¯¢é—®MLLMsâ€œæ˜¯å¦æœ‰&#x2F;ä¸€ä¸ªé™éŸ³å¯¹è±¡çš„å£°éŸ³â€ã€‚å®éªŒç»“æœè¡¨æ˜¾ç¤ºï¼ŒMLLMsï¼ˆå¦‚Qwen2.5-Omniå’ŒGemini 2.5ï¼‰ç”±äºè§†è§‰ä¸»å¯¼çš„æ¨ç†æœºåˆ¶è€Œæ— æ³•åŒºåˆ†ä¸å­˜åœ¨çš„éŸ³é¢‘ã€‚å—æ­¤è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„åä½œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹RL-CoMMï¼Œè¯¥æ¨¡å‹ä»¥Qwen2.5-Omniä¸ºåŸºç¡€æ„å»ºã€‚RL-CoMMåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼š1ï¼‰ä¸ºäº†å‡è½»è§†è§‰ä¸»å¯¼çš„æ­§ä¹‰æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤–éƒ¨æ¨¡å‹â€”â€”å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ä½œä¸ºå‚è€ƒæ¨¡å‹ï¼Œä»¥ç”Ÿæˆä»…éŸ³é¢‘çš„æ¨ç†ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é€æ­¥æ¨ç†å¥–åŠ±å‡½æ•°ï¼Œä½¿MLLMsèƒ½å¤Ÿå€ŸåŠ©ä»…éŸ³é¢‘çš„å‚è€ƒè¿›è¡Œè‡ªæˆ‘æå‡çš„è§†å¬æ¨ç†ã€‚2ï¼‰ä¸ºäº†ç¡®ä¿å‡†ç¡®çš„ç­”æ¡ˆé¢„æµ‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»¥ç­”æ¡ˆä¸ºä¸­å¿ƒçš„ç½®ä¿¡ä¼˜åŒ–ï¼Œä»¥å‡å°‘æ½œåœ¨å¼‚è´¨æ¨ç†å·®å¼‚çš„ä¸ç¡®å®šæ€§ã€‚åœ¨è§†å¬é—®ç­”å’Œè§†å¬å¹»è§‰æ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨æœ‰é™è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒRL-CoMMçš„å‡†ç¡®ç‡è¾ƒåŸºçº¿æ¨¡å‹æé«˜äº†10~30%ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/rikeilong/AVConfusion%E3%80%82">https://github.com/rikeilong/AVConfusionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10059v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å­˜åœ¨ä½†éŸ³é¢‘ç¼ºå¤±çš„æ··æ·†å¯¹è±¡è¾¨è¯†æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•AV-ConfuseBenchï¼Œé€šè¿‡ä¿®æ”¹è§†é¢‘ä¸­çš„å¯¹è±¡å£°éŸ³æ¥æ¨¡æ‹Ÿâ€œè§†å¬æ··æ·†â€åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMLLMsåœ¨è¾¨è¯†ä¸å­˜åœ¨çš„éŸ³é¢‘æ—¶å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬å€¾å‘äºè§†è§‰ä¸»å¯¼çš„æ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šMLLMåä½œæ–¹æ³•RL-CoMMã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œå¼•å…¥å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ä½œä¸ºå‚è€ƒæ¨¡å‹ï¼Œç”Ÿæˆä»…éŸ³é¢‘çš„æ¨ç†ï¼Œä»¥ç¼“è§£è§†è§‰ä¸»å¯¼çš„æ­§ä¹‰ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ç§é€æ­¥æ¨ç†å¥–åŠ±å‡½æ•°ï¼Œä½¿MLLMsèƒ½å¤Ÿé€šè¿‡ä»…éŸ³é¢‘çš„å‚è€ƒè¿›è¡Œè‡ªæˆ‘æ”¹è¿›çš„éŸ³é¢‘-è§†è§‰æ¨ç†ã€‚å…¶æ¬¡ï¼Œä¸ºäº†ç¡®ä¿å‡†ç¡®çš„ç­”æ¡ˆé¢„æµ‹ï¼Œå¼•å…¥äº†ä»¥ç­”æ¡ˆä¸ºä¸­å¿ƒçš„ä¿¡å¿ƒä¼˜åŒ–ï¼Œä»¥å‡å°‘æ½œåœ¨çš„ä¸åŒæ¨ç†æ–¹å¼çš„ä¸ç¡®å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒRL-CoMMåœ¨éŸ³é¢‘è§†è§‰é—®ç­”å’ŒéŸ³é¢‘è§†è§‰å¹»è§‰æ–¹é¢ï¼Œç›¸è¾ƒäºåŸºå‡†æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†10~30%ï¼Œå¹¶ä¸”åœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹è¡¨ç°æ›´ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¾¨è¯†è§†è§‰ä¸Šå­˜åœ¨ä½†éŸ³é¢‘ç¼ºå¤±çš„æ··æ·†å¯¹è±¡æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•AV-ConfuseBenchï¼Œæ¨¡æ‹Ÿâ€œè§†å¬æ··æ·†â€åœºæ™¯ã€‚</li>
<li>MLLMsåœ¨è¾¨è¯†ä¸å­˜åœ¨çš„éŸ³é¢‘æ—¶å—è§†è§‰ä¸»å¯¼çš„æ¨ç†å½±å“ã€‚</li>
<li>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šMLLMåä½œæ–¹æ³•RL-CoMMã€‚</li>
<li>RL-CoMMåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šå¼•å…¥å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ä½œä¸ºå‚è€ƒï¼Œç”Ÿæˆä»…éŸ³é¢‘çš„æ¨ç†ï¼›è®¾è®¡é€æ­¥æ¨ç†å¥–åŠ±å‡½æ•°ï¼Œæé«˜MLLMsçš„éŸ³é¢‘-è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RL-CoMMé€šè¿‡ç­”æ¡ˆä¸ºä¸­å¿ƒçš„ä¿¡å¿ƒä¼˜åŒ–ï¼Œå‡å°‘ä¸åŒæ¨ç†æ–¹å¼çš„ä¸ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04a37592458d1c70ce5e9cb4fe02dead" align="middle">
<img src="https://picx.zhimg.com/v2-81a328464f3ff8eb84474d0295849917" align="middle">
<img src="https://picx.zhimg.com/v2-e6ec94de782389df9097905221b600ef" align="middle">
<img src="https://picx.zhimg.com/v2-04403ee2c4148928544921e6b923cf59" align="middle">
<img src="https://picx.zhimg.com/v2-482bacade6d4ed60ef43d699a4e29906" align="middle">
<img src="https://picx.zhimg.com/v2-d8708efa60de076bc6c70ab6920f6406" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-ReAct-A-Planner-Centric-Framework-for-Complex-Tool-Augmented-LLM-Reasoning"><a href="#Beyond-ReAct-A-Planner-Centric-Framework-for-Complex-Tool-Augmented-LLM-Reasoning" class="headerlink" title="Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning"></a>Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning</h2><p><strong>Authors:Xiaolong Wei, Yuehu Dong, Xingliang Wang, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin</strong></p>
<p>Existing tool-augmented large language models (LLMs) encounter significant challenges when processing complex queries. Current frameworks such as ReAct are prone to local optimization traps due to their reliance on incremental decision-making processes. To address these limitations, we propose a novel Planner-centric Plan-Execute paradigm that fundamentally resolves local optimization bottlenecks through architectural innovation. Central to our approach is a novel Planner model that performs global Directed Acyclic Graph (DAG) planning for complex queries, enabling optimized execution beyond conventional tool coordination. We also introduce ComplexTool-Plan, a large-scale benchmark dataset featuring complex queries that demand sophisticated multi-tool composition and coordination capabilities. Additionally, we develop a two-stage training methodology that integrates Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), systematically enhancing the Plannerâ€™s tool selection accuracy and global planning awareness through structured DAG-based planning. When integrated with a capable executor, our framework achieves state-of-the-art performance on the StableToolBench benchmark for complex user queries, demonstrating superior end-to-end execution capabilities and robust handling of intricate multi-tool workflows.</p>
<blockquote>
<p>ç°æœ‰çš„å·¥å…·å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰æ¡†æ¶ï¼ˆå¦‚ReActï¼‰ç”±äºä¾èµ–äºå¢é‡å†³ç­–è¿‡ç¨‹ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨ä¼˜åŒ–é™·é˜±ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä»¥è§„åˆ’è€…ä¸ºä¸­å¿ƒçš„è§„åˆ’-æ‰§è¡ŒèŒƒå¼ï¼Œé€šè¿‡æ¶æ„åˆ›æ–°ä»æ ¹æœ¬ä¸Šè§£å†³å±€éƒ¨ä¼˜åŒ–ç“¶é¢ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°å‹è§„åˆ’å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹å¤æ‚æŸ¥è¯¢è¿›è¡Œå…¨å±€æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è§„åˆ’ï¼Œä½¿ä¼˜åŒ–æ‰§è¡Œè¶…è¶Šä¼ ç»Ÿçš„å·¥å…·åè°ƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ComplexTool-Planï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«éœ€è¦ç²¾ç»†çš„å¤šå·¥å…·ç»„åˆå’Œåè°ƒèƒ½åŠ›çš„å¤æ‚æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œå°†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡åŸºäºç»“æ„çš„DAGè§„åˆ’ï¼Œç³»ç»Ÿåœ°æé«˜äº†è§„åˆ’å™¨çš„å·¥å…·é€‰æ‹©ç²¾åº¦å’Œå…¨å±€è§„åˆ’æ„è¯†ã€‚å½“ä¸åŠŸèƒ½å¼ºå¤§çš„æ‰§è¡Œå™¨ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨StableToolBenchåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¤æ‚ç”¨æˆ·æŸ¥è¯¢çš„å“è¶Šæ€§èƒ½ï¼Œå±•ç¤ºäº†å‡ºè‰²çš„ç«¯åˆ°ç«¯æ‰§è¡Œèƒ½åŠ›å’Œå¯¹å¤æ‚å¤šå·¥å…·å·¥ä½œæµç¨‹çš„ç¨³å¥å¤„ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10037v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹ç°æœ‰å·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„Planner-centric Plan-ExecuteèŒƒå¼ã€‚è¯¥èŒƒå¼é€šè¿‡æ¶æ„åˆ›æ–°ä»æ ¹æœ¬ä¸Šè§£å†³å±€éƒ¨ä¼˜åŒ–ç“¶é¢ˆï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„Planneræ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹å¤æ‚æŸ¥è¯¢è¿›è¡Œå…¨å±€æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è§„åˆ’ï¼Œä»¥å®ç°ä¼˜äºä¼ ç»Ÿå·¥å…·åè°ƒçš„ä¼˜åŒ–æ‰§è¡Œã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†å¤§å‹åŸºå‡†æµ‹è¯•æ•°æ®é›†ComplexTool-Planï¼Œä»¥åŠä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæé«˜äº†Plannerçš„å·¥å…·é€‰æ‹©å‡†ç¡®æ€§å’Œå…¨å±€è§„åˆ’æ„è¯†ã€‚ä¸åŠŸèƒ½å¼ºå¤§çš„æ‰§è¡Œå™¨ç»“åˆæ—¶ï¼Œè¯¥æ¡†æ¶åœ¨StableToolBenchåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå±•ç°å‡ºå‡ºè‰²çš„ç«¯åˆ°ç«¯æ‰§è¡Œèƒ½åŠ›å’Œå¤„ç†å¤æ‚å¤šå·¥å…·å·¥ä½œæµçš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„Planner-centric Plan-ExecuteèŒƒå¼ï¼Œé€šè¿‡æ¶æ„åˆ›æ–°è§£å†³å±€éƒ¨ä¼˜åŒ–ç“¶é¢ˆã€‚</li>
<li>å¼•å…¥äº†å…¨å±€æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è§„åˆ’çš„æ–°é¢–Planneræ¨¡å‹ï¼Œä¼˜åŒ–æ‰§è¡Œå¤æ‚æŸ¥è¯¢ã€‚</li>
<li>æ¨å‡ºäº†ComplexTool-Planå¤§å‹åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œæ¨¡æ‹Ÿå¤æ‚æŸ¥è¯¢éœ€æ±‚ã€‚</li>
<li>é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæé«˜Plannerçš„å·¥å…·é€‰æ‹©å‡†ç¡®æ€§å’Œå…¨å±€è§„åˆ’èƒ½åŠ›ã€‚</li>
<li>ä¸åŠŸèƒ½å¼ºå¤§çš„æ‰§è¡Œå™¨ç»“åˆæ—¶ï¼Œè¯¥æ¡†æ¶åœ¨StableToolBenchåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15543e81b15b4a0ea53087f38ed55c93" align="middle">
<img src="https://picx.zhimg.com/v2-79115e6d9dc041d63cef97edca328fef" align="middle">
<img src="https://picx.zhimg.com/v2-23826a8e0bc143dc3c67bfb451962950" align="middle">
<img src="https://picx.zhimg.com/v2-88155cd74f6e6f794742f0c62b09c694" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models"><a href="#AffordBot-3D-Fine-grained-Embodied-Reasoning-via-Multimodal-Large-Language-Models" class="headerlink" title="AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models"></a>AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models</h2><p><strong>Authors:Xinyi Wang, Xun Yang, Yanlong Xu, Yuchen Wu, Zhen Li, Na Zhao</strong></p>
<p>Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.</p>
<blockquote>
<p>æœ‰æ•ˆçš„äººæœºåä½œåœ¨ç‰©ç†ç¯å¢ƒä¸­ä¸ä»…è¦æ±‚ç†è§£è¦é‡‡å–çš„è¡ŒåŠ¨ï¼Œè¿˜è¦æ±‚ç†è§£å¯æ“ä½œå…ƒç´ çš„ä½ç½®ä»¥åŠå¦‚ä½•ä¸å®ƒä»¬äº’åŠ¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¼šåœ¨å¯¹è±¡å±‚é¢æ“ä½œæˆ–ä»¥é›¶æ•£çš„æ–¹å¼å¤„ç†ç²¾ç»†ç²’åº¦çš„å¯ç”¨æ€§æ¨ç†ï¼Œç¼ºä¹è¿è´¯çš„ã€æŒ‡ä»¤é©±åŠ¨çš„æ¥åœ°å’Œæ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼šç²¾ç»†çš„3Då®ä½“æ¨ç†ï¼Œè¦æ±‚ä»£ç†æ ¹æ®ä»»åŠ¡æŒ‡ä»¤ï¼Œé¢„æµ‹3Dåœºæ™¯ä¸­æ¯ä¸ªå¼•ç”¨çš„å¯ç”¨æ€§å…ƒç´ çš„ç»“æ„åŒ–ä¸‰å…ƒç»„ï¼ŒåŒ…æ‹¬å…¶ç©ºé—´ä½ç½®ã€è¿åŠ¨ç±»å‹å’Œè¿åŠ¨è½´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AffordBotï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒå°†å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸é‡èº«å®šåˆ¶çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èŒƒå¼ç›¸ç»“åˆã€‚ä¸ºäº†å¼¥åˆ3Dè¾“å…¥å’Œ2Då…¼å®¹MLLMä¹‹é—´çš„é¸¿æ²Ÿï¼Œæˆ‘ä»¬å‘ˆç°äº†åœºæ™¯çš„ç¯ç»•è§†å›¾å›¾åƒï¼Œå¹¶å°†3Då…ƒç´ å€™é€‰è€…æŠ•å½±åˆ°è¿™äº›è§†å›¾ä¸­ï¼Œå½¢æˆä¸åœºæ™¯å‡ ä½•ç»“æ„å¯¹é½çš„ä¸°å¯Œè§†è§‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„CoTç®¡é“å§‹äºä¸»åŠ¨æ„ŸçŸ¥é˜¶æ®µï¼Œæç¤ºMLLMæ ¹æ®æŒ‡ä»¤é€‰æ‹©æœ€å…·æœ‰ä¿¡æ¯é‡çš„è§‚ç‚¹ï¼Œç„¶åè¿›è¡Œé€æ­¥æ¨ç†ä»¥å®šä½å¯ç”¨æ€§å…ƒç´ å¹¶æ¨æ–­åˆç†çš„äº¤äº’åŠ¨ä½œã€‚åœ¨SceneFun3Dæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒAffordBotè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–å’Œç‰©ç†æ¥åœ°æ¨ç†èƒ½åŠ›ï¼Œä»…ä½¿ç”¨3Dç‚¹äº‘è¾“å…¥å’ŒMLLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10017v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ç²¾ç»†åŒ–çš„ä¸‰ç»´ä½“ç°æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚æ™ºèƒ½ä½“åŸºäºä»»åŠ¡æŒ‡ä»¤é¢„æµ‹æ¯ä¸ªå‚ç…§çš„å¯åˆ©ç”¨å…ƒç´ åœ¨ä¸‰ç»´åœºæ™¯ä¸­çš„ç©ºé—´ä½ç½®ã€è¿åŠ¨ç±»å‹å’Œè¿åŠ¨è½´çš„ç»“æ„åŒ–ä¸‰å…ƒç»„ã€‚ä¸ºè§£å†³æ­¤ä»»åŠ¡ï¼Œæå‡ºäº†AffordBotæ¡†æ¶ï¼Œèåˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸é‡èº«å®šåˆ¶çš„é“¾å¼æ€ç»´æ¨ç†æ¨¡å¼ã€‚ä¸ºè§£å†³ä¸‰ç»´è¾“å…¥ä¸äºŒç»´å…¼å®¹çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œé€šè¿‡å¯¹åœºæ™¯è¿›è¡Œç¯ç»•è§†å›¾æ¸²æŸ“ï¼Œå°†ä¸‰ç»´å…ƒç´ å€™é€‰è€…æŠ•å½±åˆ°è¿™äº›è§†å›¾ä¸­ï¼Œå½¢æˆä¸åœºæ™¯å‡ ä½•å¯¹é½çš„ä¸°å¯Œè§†è§‰è¡¨å¾ã€‚åœ¨SceneFun3Dæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒAffordBotä»…é€šè¿‡ä¸‰ç»´ç‚¹äº‘è¾“å…¥å’Œå¤§å‹è¯­è¨€æ¨¡å‹å°±å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç‰©ç†åŸºç¡€æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ç²¾ç»†åŒ–ä¸‰ç»´ä½“ç°æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚æ™ºèƒ½ä½“é¢„æµ‹æ¯ä¸ªå‚ç…§ç‰©çš„ç»“æ„åŒ–ä¿¡æ¯ï¼ˆç©ºé—´ä½ç½®ã€è¿åŠ¨ç±»å‹å’Œè¿åŠ¨è½´ï¼‰ã€‚</li>
<li>AffordBotæ¡†æ¶èåˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œé“¾å¼æ€ç»´æ¨ç†æ¨¡å¼ï¼Œç”¨äºè§£å†³æ­¤ä»»åŠ¡ã€‚</li>
<li>è§£å†³äº†ä¸‰ç»´è¾“å…¥ä¸äºŒç»´è¯­è¨€æ¨¡å‹ä¹‹é—´çš„å…¼å®¹æ€§é—®é¢˜ï¼Œé€šè¿‡åœºæ™¯ç¯ç»•è§†å›¾æ¸²æŸ“å’ŒæŠ•å½±æŠ€æœ¯ã€‚</li>
<li>AffordBotåœ¨SceneFun3Dæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>ä»…é€šè¿‡ä¸‰ç»´ç‚¹äº‘è¾“å…¥å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç‰©ç†åŸºç¡€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é¦–æ¬¡ç»“åˆäº†è‡ªç„¶è¯­è¨€ç†è§£å’Œä¸‰ç»´åœºæ™¯ç†è§£çš„ä»»åŠ¡è¦æ±‚ï¼Œå¼ºè°ƒäº†å¯¹ç¯å¢ƒçš„åŠ¨æ€æ„ŸçŸ¥å’Œå¯¹æŒ‡ä»¤çš„æ·±å…¥ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72e0a707f23140f6b36e7e8faa290070" align="middle">
<img src="https://picx.zhimg.com/v2-d8e6772aa6d323a4643011f32e6d26d7" align="middle">
<img src="https://picx.zhimg.com/v2-e631d2a3ff0f1c02345fa7b7eab36227" align="middle">
<img src="https://picx.zhimg.com/v2-047fbe16e0b0fb2c2f11526b16346025" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Trustworthiness-in-Multimodal-Emotional-Support-Systems"><a href="#Reinforcing-Trustworthiness-in-Multimodal-Emotional-Support-Systems" class="headerlink" title="Reinforcing Trustworthiness in Multimodal Emotional Support Systems"></a>Reinforcing Trustworthiness in Multimodal Emotional Support Systems</h2><p><strong>Authors:Huy M. Le, Dat Tien Nguyen, Ngan T. T. Vo, Tuan D. Q. Nguyen, Nguyen Binh Le, Duy Minh Ho Nguyen, Daniel Sonntag, Lizi Liao, Binh T. Nguyen</strong></p>
<p>In todayâ€™s world, emotional support is increasingly essential, yet it remains challenging for both those seeking help and those offering it. Multimodal approaches to emotional support show great promise by integrating diverse data sources to provide empathetic, contextually relevant responses, fostering more effective interactions. However, current methods have notable limitations, often relying solely on text or converting other data types into text, or providing emotion recognition only, thus overlooking the full potential of multimodal inputs. Moreover, many studies prioritize response generation without accurately identifying critical emotional support elements or ensuring the reliability of outputs. To overcome these issues, we introduce \textsc{ MultiMood}, a new framework that (i) leverages multimodal embeddings from video, audio, and text to predict emotional components and to produce responses responses aligned with professional therapeutic standards. To improve trustworthiness, we (ii) incorporate novel psychological criteria and apply Reinforcement Learning (RL) to optimize large language models (LLMs) for consistent adherence to these standards. We also (iii) analyze several advanced LLMs to assess their multimodal emotional support capabilities. Experimental results show that MultiMood achieves state-of-the-art on MESC and DFEW datasets while RL-driven trustworthiness improvements are validated through human and LLM evaluations, demonstrating its superior capability in applying a multimodal framework in this domain.</p>
<blockquote>
<p>åœ¨å¦‚ä»Šè¿™ä¸ªæ—¶ä»£ï¼Œæƒ…æ„Ÿæ”¯æŒå˜å¾—æ„ˆå‘é‡è¦ï¼Œç„¶è€Œå¯¹äºå¯»æ±‚å¸®åŠ©å’Œæä¾›å¸®åŠ©çš„äººæ¥è¯´éƒ½é¢ä¸´æŒ‘æˆ˜ã€‚é€šè¿‡æ•´åˆå„ç§æ•°æ®æºï¼Œé‡‡ç”¨å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒæ–¹æ³•èƒ½å¤Ÿå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œæä¾›å¯Œæœ‰åŒæƒ…å¿ƒå’Œè¯­å¢ƒç›¸å…³çš„å›åº”ï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„äº’åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ï¼Œå¾€å¾€ä»…ä¾èµ–æ–‡æœ¬æˆ–å°†å…¶ä»–æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œæˆ–ä»…æä¾›æƒ…ç»ªè¯†åˆ«ï¼Œä»è€Œå¿½ç•¥äº†å¤šæ¨¡å¼è¾“å…¥çš„å…¨é¢æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè®¸å¤šç ”ç©¶ä¼˜å…ˆä¾§é‡å“åº”ç”Ÿæˆï¼Œè€Œæ²¡æœ‰å‡†ç¡®è¯†åˆ«å…³é”®çš„æƒ…æ„Ÿæ”¯æŒè¦ç´ æˆ–ç¡®ä¿è¾“å‡ºçš„å¯é æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†\textsc{MultiMood}è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒï¼ˆiï¼‰åˆ©ç”¨è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„è·¨æ¨¡å¼åµŒå…¥æ¥é¢„æµ‹æƒ…æ„Ÿæˆåˆ†ï¼Œå¹¶äº§ç”Ÿç¬¦åˆä¸“ä¸šæ²»ç–—æ ‡å‡†çš„å“åº”ã€‚ä¸ºäº†æé«˜å¯ä¿¡åº¦ï¼Œæˆ‘ä»¬ï¼ˆiiï¼‰èå…¥æ–°é¢–çš„å¿ƒç†æ ‡å‡†å’Œè¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿å…¶å§‹ç»ˆç¬¦åˆè¿™äº›æ ‡å‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ï¼ˆiiiï¼‰åˆ†æäº†å¤šä¸ªå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è¯„ä¼°å…¶å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMultiMoodåœ¨MESCå’ŒDFEWæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè€Œç”±å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„å¯ä¿¡æ€§æ”¹è¿›åˆ™é€šè¿‡äººç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨è¯¥é¢†åŸŸåº”ç”¨å¤šæ¨¡å¼æ¡†æ¶çš„å“è¶Šèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10011v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¦‚ä»Šçš„ä¸–ç•Œï¼Œæƒ…æ„Ÿæ”¯æŒå˜å¾—æ—¥ç›Šé‡è¦ï¼Œä½†å¯¹äºå¯»æ±‚å¸®åŠ©è€…å’Œæä¾›å¸®åŠ©è€…æ¥è¯´ä»ç„¶æ˜¯ä¸ªæŒ‘æˆ˜ã€‚å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒæ–¹æ³•é€šè¿‡æ•´åˆå„ç§æ•°æ®æºæ¥æä¾›ä½“æ¤ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„å›åº”ï¼Œå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„äº’åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•æœ‰ç€æ˜æ˜¾çš„å±€é™æ€§ï¼Œå¦‚ä»…ä¾èµ–æ–‡æœ¬æˆ–å°†å…¶ä»–æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œæˆ–åªæä¾›æƒ…ç»ªè¯†åˆ«ï¼Œä»è€Œå¿½ç•¥äº†å¤šæ¨¡å¼è¾“å…¥çš„å…¨é¢æ½œåŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MultiMoodæ¡†æ¶ï¼Œå®ƒï¼ˆä¸€ï¼‰åˆ©ç”¨è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„å¤šåª’ä½“åµŒå…¥æ¥é¢„æµ‹æƒ…ç»ªæˆåˆ†ï¼Œå¹¶äº§ç”Ÿä¸ä¸“ä¸šçš„æ²»ç–—æ ‡å‡†å¯¹é½çš„å›åº”ã€‚ï¼ˆäºŒï¼‰é€šè¿‡èå…¥æ–°çš„å¿ƒç†æ ‡å‡†å’Œåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥æé«˜å…¶ä¸€è‡´æ€§ã€‚ï¼ˆä¸‰ï¼‰åˆ†æå¤šä¸ªå…ˆè¿›çš„LLMä»¥è¯„ä¼°å…¶å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMultiMoodåœ¨MESCå’ŒDFEWæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè€Œç”±RLé©±åŠ¨çš„ä¿¡ä»»åº¦æ”¹è¿›åˆ™é€šè¿‡äººç±»å’ŒLLMè¯„ä¼°å¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨è¯¥é¢†åŸŸåº”ç”¨å¤šåª’ä½“æ¡†æ¶çš„å“è¶Šèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿæ”¯æŒåœ¨å½“ä»Šä¸–ç•Œçš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡å¼æ–¹æ³•åœ¨æƒ…æ„Ÿæ”¯æŒä¸­çš„æ½œåŠ›ä»¥åŠå½“å‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>MultiMoodæ¡†æ¶é€šè¿‡åˆ©ç”¨å¤šåª’ä½“åµŒå…¥æ¥é¢„æµ‹æƒ…ç»ªæˆåˆ†å¹¶äº§ç”Ÿä¸“ä¸šå¯¹é½çš„å›åº”ã€‚</li>
<li>MultiMoodæ¡†æ¶åˆ©ç”¨æ–°çš„å¿ƒç†æ ‡å‡†å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>MultiMoodæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ°´å¹³çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡äººç±»å’ŒLLMè¯„ä¼°éªŒè¯äº†MultiMoodæ¡†æ¶çš„ä¿¡ä»»åº¦æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-981a56c0085f01c82780cc4eee11405b" align="middle">
<img src="https://picx.zhimg.com/v2-fa1b85e6bd0e01fde72e10565094b7bc" align="middle">
<img src="https://picx.zhimg.com/v2-9093b195770eee5e6b22cd46940dcf9b" align="middle">
<img src="https://picx.zhimg.com/v2-0c2703c6b802b1193664bc8caa69159e" align="middle">
<img src="https://picx.zhimg.com/v2-12611d4f7057fa539d97e15f3af67f40" align="middle">
<img src="https://picx.zhimg.com/v2-2dba4d36757c09dea058137fda73cda9" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HierRouter-Coordinated-Routing-of-Specialized-Large-Language-Models-via-Reinforcement-Learning"><a href="#HierRouter-Coordinated-Routing-of-Specialized-Large-Language-Models-via-Reinforcement-Learning" class="headerlink" title="HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning"></a>HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning</h2><p><strong>Authors:Nikunj Gupta, Bill Guo, Rajgopal Kannan, Viktor K. Prasanna</strong></p>
<p>Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings. To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models. Formulated as a finite-horizon Markov Decision Process (MDP), our approach trains a Proximal Policy Optimization (PPO)-based reinforcement learning agent to iteratively select which models to invoke at each stage of multi-hop inference. The agent conditions on the evolving context and accumulated cost to make context-aware routing decisions. Experiments with three open-source candidate LLMs across six benchmarks, including QA, code generation, and mathematical reasoning, show that HierRouter improves response quality by up to 2.4x compared to using individual models independently, while incurring only a minimal additional inference cost on average. These results highlight the promise of hierarchical routing for cost-efficient, high-performance LLM inference. All codes can be found here <a target="_blank" rel="noopener" href="https://github.com/">https://github.com/</a> Nikunj-Gupta&#x2F;hierouter.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å¸¦æ¥äº†è¾ƒé«˜çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™æˆ–å®æ—¶ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HierRouterï¼Œè¿™æ˜¯ä¸€ç§åˆ†å±‚è·¯ç”±æ–¹æ³•ï¼Œå®ƒå¯ä»¥ä»ä¸€ç»„ä¸“ç”¨çš„ã€è½»å‹çš„è¯­è¨€æ¨¡å‹ä¸­åŠ¨æ€åœ°ç»„åˆæ¨ç†ç®¡é“ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•è¡¨è¿°ä¸ºä¸€ä¸ªæœ‰é™è§†ç•Œçš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶é€šè¿‡åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ä»£ç†è¿›è¡Œè®­ç»ƒï¼Œä»¥è¿­ä»£åœ°é€‰æ‹©åœ¨å¤šè·³æ¨ç†çš„æ¯ä¸ªé˜¶æ®µåº”è°ƒç”¨å“ªäº›æ¨¡å‹ã€‚è¯¥ä»£ç†æ ¹æ®ä¸æ–­å˜åŒ–çš„ä¸Šä¸‹æ–‡å’Œç´¯ç§¯çš„æˆæœ¬æ¥åšå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è·¯ç”±å†³ç­–ã€‚åœ¨é—®ç­”ã€ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†ç­‰å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹ä¸‰ä¸ªå¼€æºå€™é€‰å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸å•ç‹¬ä½¿ç”¨å„ä¸ªæ¨¡å‹ç›¸æ¯”ï¼ŒHierRouterçš„å“åº”è´¨é‡æé«˜äº†é«˜è¾¾2.4å€ï¼ŒåŒæ—¶å¹³å‡å¸¦æ¥çš„é¢å¤–æ¨ç†æˆæœ¬å¾®ä¹å…¶å¾®ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åˆ†å±‚è·¯ç”±åœ¨é«˜æ•ˆã€é«˜æ€§èƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¸­çš„æ½œåŠ›ã€‚æ‰€æœ‰ä»£ç å‡å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Nikunj-Gupta/hierouter">https://github.com/Nikunj-Gupta/hierouter</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09873v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶è®¡ç®—ä¸å†…å­˜æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åœ¨èµ„æºå—é™æˆ–å®æ—¶åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HierRouterï¼Œä¸€ç§å±‚æ¬¡åŒ–è·¯ç”±æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŠ¨æ€åœ°ä»ä¸“ç”¨ã€è½»é‡çº§è¯­è¨€æ¨¡å‹çš„æ± ä¸­ç»„è£…æ¨ç†ç®¡é“ã€‚æˆ‘ä»¬å°†æ­¤æ–¹æ³•è¡¨è¿°ä¸ºä¸€ä¸ªæœ‰é™æœŸé™çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œå¹¶è®­ç»ƒåŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œä»¥åœ¨å¤šè·³æ¨ç†çš„æ¯ä¸ªé˜¶æ®µé€‰æ‹©è°ƒç”¨å“ªäº›æ¨¡å‹ã€‚ä»£ç†ä¼šæ ¹æ®ä¸æ–­å˜åŒ–çš„ä¸Šä¸‹æ–‡å’Œç´¯ç§¯çš„æˆæœ¬æ¥åšå‡ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è·¯ç”±å†³ç­–ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨é—®ç­”ã€ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†ç­‰å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨ä¸‰ä¸ªå¼€æºå€™é€‰LLMsçš„HierRouterï¼Œåœ¨æé«˜å“åº”è´¨é‡çš„åŒæ—¶ï¼Œå¹³å‡æ¨ç†æˆæœ¬ä»…ç•¥æœ‰å¢åŠ ã€‚è¿™çªæ˜¾äº†åˆ†å±‚è·¯ç”±åœ¨é«˜æ•ˆã€é«˜æ€§èƒ½LLMæ¨ç†ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†è®¡ç®—ä¸å†…å­˜æˆæœ¬é«˜ã€‚</li>
<li>HierRouteræ˜¯ä¸€ç§å±‚æ¬¡åŒ–è·¯ç”±æ–¹æ³•ï¼Œèƒ½å¤ŸåŠ¨æ€ç»„è£…æ¨ç†ç®¡é“ï¼Œä»ä¸“ç”¨è½»é‡çº§è¯­è¨€æ¨¡å‹ä¸­é€‰æ‹©ã€‚</li>
<li>HierRouteræ–¹æ³•è¡¨è¿°ä¸ºæœ‰é™æœŸé™çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚</li>
<li>ä½¿ç”¨åŸºäºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ä»£ç†è¿›è¡Œè·¯ç”±å†³ç­–ã€‚</li>
<li>ä»£ç†æ ¹æ®ä¸Šä¸‹æ–‡å’Œç´¯ç§¯æˆæœ¬åšå‡ºå†³ç­–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒHierRouteråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†å“åº”è´¨é‡ï¼Œå¹¶é™ä½äº†æ¨ç†æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e34a3b6a6bb76662d8205cc3838298a3" align="middle">
<img src="https://picx.zhimg.com/v2-29cdaa4bd7d5bc7ab8967ec7f24b3875" align="middle">
<img src="https://picx.zhimg.com/v2-8a94ba1cc7ba59a4172525d43fe9867d" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="In-Token-Rationality-Optimization-Towards-Accurate-and-Concise-LLM-Reasoning-via-Self-Feedback"><a href="#In-Token-Rationality-Optimization-Towards-Accurate-and-Concise-LLM-Reasoning-via-Self-Feedback" class="headerlink" title="In-Token Rationality Optimization: Towards Accurate and Concise LLM Reasoning via Self-Feedback"></a>In-Token Rationality Optimization: Towards Accurate and Concise LLM Reasoning via Self-Feedback</h2><p><strong>Authors:Mingye Zhu, Yi Liu, Zheren Fu, Quan Wang, Yongdong Zhang</strong></p>
<p>Training Large Language Models (LLMs) for chain-of-thought reasoning presents a significant challenge: supervised fine-tuning on a single â€œgoldenâ€ rationale hurts generalization as it penalizes equally valid alternatives, whereas reinforcement learning with verifiable rewards struggles with credit assignment and prohibitive computational cost. To tackle these limitations, we introduce InTRO (In-Token Rationality Optimization), a new framework that enables both token-level exploration and self-feedback for accurate and concise reasoning. Instead of directly optimizing an intractable objective over all valid reasoning paths, InTRO leverages correction factors-token-wise importance weights estimated by the information discrepancy between the generative policy and its answer-conditioned counterpart, for informative next token selection. This approach allows the model to perform token-level exploration and receive self-generated feedback within a single forward pass, ultimately encouraging accurate and concise rationales. Across six math-reasoning benchmarks, InTRO consistently outperforms other baselines, raising solution accuracy by up to 20% relative to the base model. Its chains of thought are also notably more concise, exhibiting reduced verbosity. Beyond this, InTRO enables cross-domain transfer, successfully adapting to out-of-domain reasoning tasks that extend beyond the realm of mathematics, demonstrating robust generalization.</p>
<blockquote>
<p>è®­ç»ƒç”¨äºæ€ç»´é“¾æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼šå¯¹å•ä¸€â€œé»„é‡‘â€ç†ç”±çš„ç›‘ç£å¾®è°ƒä¼šæŸå®³æ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä¼šå¹³ç­‰åœ°æƒ©ç½šæœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œè€Œä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åˆ™é¢ä¸´ä¿¡ç”¨åˆ†é…å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†InTROï¼ˆIn-Token Rationality Optimizationï¼Œå†…éƒ¨æ ‡è®°ç†æ€§ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿä¸ºç²¾ç¡®å’Œç®€æ´çš„æ¨ç†æä¾›æ ‡è®°çº§åˆ«çš„æ¢ç´¢å’Œè‡ªæˆ‘åé¦ˆã€‚InTROä¸æ˜¯ç›´æ¥ä¼˜åŒ–æ‰€æœ‰æœ‰æ•ˆæ¨ç†è·¯å¾„ä¸Šä¸å¯è¡Œçš„ç›®æ ‡ï¼Œè€Œæ˜¯åˆ©ç”¨æ ¡æ­£å› å­â€”â€”åŸºäºç”Ÿæˆç­–ç•¥ä¸å…¶ç­”æ¡ˆæ¡ä»¶å¯¹åº”ç­–ç•¥ä¹‹é—´çš„ä¿¡æ¯å·®å¼‚ä¼°è®¡çš„æ ‡è®°çº§é‡è¦æ€§æƒé‡ï¼Œæ¥è¿›è¡Œä¿¡æ¯ä¸°å¯Œçš„ä¸‹ä¸€ä¸ªæ ‡è®°é€‰æ‹©ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­è¿›è¡Œæ ‡è®°çº§åˆ«çš„æ¢ç´¢å¹¶æ¥æ”¶è‡ªæˆ‘ç”Ÿæˆçš„åé¦ˆï¼Œæœ€ç»ˆé¼“åŠ±å‡†ç¡®ä¸”ç®€æ´çš„æ¨ç†ã€‚åœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒInTROæŒç»­ä¼˜äºå…¶ä»–åŸºå‡†æµ‹è¯•ï¼Œç›¸å¯¹äºåŸºç¡€æ¨¡å‹ï¼Œè§£å†³æ–¹æ¡ˆçš„å‡†ç¡®æ€§æé«˜äº†é«˜è¾¾20%ã€‚å®ƒçš„æ€ç»´é“¾ä¹Ÿæ›´åŠ ç®€æ´ï¼Œå‡å°‘äº†å†—ä½™ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒInTROå®ç°äº†è·¨åŸŸè¿ç§»ï¼ŒæˆåŠŸé€‚åº”è¶…å‡ºæ•°å­¦é¢†åŸŸçš„åŸŸå¤–æ¨ç†ä»»åŠ¡ï¼Œå±•ç°äº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09865v1">PDF</a> AAAI 2026 Oral</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé“¾å¼æ€ç»´æ¨ç†é¢ä¸´æŒ‘æˆ˜ï¼šå•ä¸€â€œé»„é‡‘â€ç†ç”±çš„ç›‘ç£å¾®è°ƒä¼šæŸå®³æ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä¼šå¹³ç­‰åœ°æƒ©ç½šæœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œè€Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åˆ™é¢ä¸´ä¿¡ç”¨åˆ†é…å’Œç¦æ­¢çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†InTROï¼ˆIn-Token Rationality Optimizationï¼‰æ–°æ¡†æ¶ï¼Œå®ƒæ”¯æŒæ ‡è®°çº§åˆ«çš„æ¢ç´¢å’Œè‡ªæˆ‘åé¦ˆï¼Œä»¥å®ç°å‡†ç¡®å’Œç®€æ´çš„æ¨ç†ã€‚InTROä¸æ˜¯ç›´æ¥ä¼˜åŒ–æ‰€æœ‰æœ‰æ•ˆæ¨ç†è·¯å¾„çš„ä¸å¯è¡Œç›®æ ‡ï¼Œè€Œæ˜¯åˆ©ç”¨æ ¡æ­£å› å­â€”â€”åŸºäºç”Ÿæˆç­–ç•¥ä¸å…¶ç­”æ¡ˆæ¡ä»¶å¯¹åº”ç­–ç•¥ä¹‹é—´çš„ä¿¡æ¯å·®å¼‚ä¼°è®¡çš„æ ‡è®°çº§é‡è¦æ€§æƒé‡ï¼Œç”¨äºé€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹åœ¨å•ä¸ªå‰å‘ä¼ é€’ä¸­è¿›è¡Œæ ‡è®°çº§åˆ«çš„æ¢ç´¢å’Œè‡ªæˆ‘ç”Ÿæˆçš„åé¦ˆï¼Œä»è€Œé¼“åŠ±å‡†ç¡®å’Œç®€æ´çš„ç†ç”±ã€‚åœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒInTROå§‹ç»ˆä¼˜äºå…¶ä»–åŸºå‡†æµ‹è¯•ï¼Œç›¸å¯¹äºåŸºç¡€æ¨¡å‹æé«˜äº†é«˜è¾¾20%çš„è§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒInTROçš„æ¨ç†é“¾æ›´åŠ ç®€æ´ï¼Œå‡å°‘äº†å†—ä½™ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒInTROå®ç°äº†è·¨åŸŸè¿ç§»ï¼ŒæˆåŠŸé€‚åº”è¶…å‡ºæ•°å­¦é¢†åŸŸçš„åŸŸå¤–æ¨ç†ä»»åŠ¡ï¼Œå±•ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é“¾å¼æ€ç»´æ¨ç†ä¸Šå­˜æŒ‘æˆ˜ï¼Œå› ç›‘ç£å¾®è°ƒæ˜“æŸå®³æ³›åŒ–èƒ½åŠ›ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™é¢ä¸´ä¿¡ç”¨åˆ†é…å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>InTROæ¡†æ¶é€šè¿‡å¼•å…¥æ ¡æ­£å› å­è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œåˆ©ç”¨ç”Ÿæˆç­–ç•¥ä¸ç­”æ¡ˆæ¡ä»¶é—´çš„ä¿¡æ¯å·®å¼‚ä¼°è®¡æ ‡è®°çº§é‡è¦æ€§æƒé‡ã€‚</li>
<li>InTROå…è®¸æ ‡è®°çº§åˆ«çš„æ¢ç´¢å’Œè‡ªæˆ‘ç”Ÿæˆçš„åé¦ˆï¼Œé¼“åŠ±å‡†ç¡®å’Œç®€æ´çš„ç†ç”±ã€‚</li>
<li>InTROåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œè§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§æé«˜æ˜¾è‘—ã€‚</li>
<li>InTROæ¨ç†é“¾æ›´ç®€æ´ï¼Œå‡å°‘å†—ä½™ã€‚</li>
<li>InTROå®ç°è·¨åŸŸè¿ç§»ï¼Œé€‚åº”è¶…å‡ºæ•°å­¦é¢†åŸŸçš„åŸŸå¤–æ¨ç†ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d337f0b8a6e71c29032a77f983ec16a" align="middle">
<img src="https://picx.zhimg.com/v2-bf8429ce4f7723e65fbafaa81805f3c9" align="middle">
<img src="https://picx.zhimg.com/v2-0a6768668784f678d5513fc53ce97525" align="middle">
<img src="https://picx.zhimg.com/v2-60400fe8c7e95a69ac0e7ee31745184f" align="middle">
<img src="https://picx.zhimg.com/v2-3fd07a9eb0a0f1385a134169d0185f78" align="middle">
<img src="https://picx.zhimg.com/v2-425b053ac4a3351f7ff6c06cb513d76a" align="middle">
<img src="https://picx.zhimg.com/v2-b156ac251ee1d6339b55828e8f4706b2" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Guided-Checkpoint-Selection-for-Reinforcement-Finetuning-of-Large-Language-Models"><a href="#Uncertainty-Guided-Checkpoint-Selection-for-Reinforcement-Finetuning-of-Large-Language-Models" class="headerlink" title="Uncertainty-Guided Checkpoint Selection for Reinforcement Finetuning of Large Language Models"></a>Uncertainty-Guided Checkpoint Selection for Reinforcement Finetuning of Large Language Models</h2><p><strong>Authors:Manh Nguyen, Dung Nguyen, Dai Do, Svetha Venkatesh, Hung Le</strong></p>
<p>Reinforcement learning (RL) finetuning is crucial to aligning large language models (LLMs), but the process is notoriously unstable and exhibits high variance across model checkpoints. In practice, selecting the best checkpoint is challenging: evaluating checkpoints on the validation set during training is computationally expensive and requires a good validation set, while relying on the final checkpoint provides no guarantee of good performance. We introduce an uncertainty-guided approach for checkpoint selection (UGCS) that avoids these pitfalls. Our method identifies hard question-answer pairs using per-sample uncertainty and ranks checkpoints by how well they handle these challenging cases. By averaging the rewards of the top-uncertain samples over a short training window, our method produces a stable and discriminative signal without additional forward passes or significant computation overhead. Experiments across three datasets and three LLMs demonstrate that it consistently identifies checkpoints with stronger generalization, outperforming traditional strategies such as relying on training or validation performance. These results highlight that models solving their hardest tasks with low uncertainty are the most reliable overall.</p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒå¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œä½†è¿™ä¸ªè¿‡ç¨‹éå¸¸ä¸ç¨³å®šï¼Œå¹¶ä¸”åœ¨æ¨¡å‹æ£€æŸ¥ç‚¹ä¹‹é—´è¡¨ç°å‡ºè¾ƒé«˜çš„æ–¹å·®ã€‚åœ¨å®è·µä¸­ï¼Œé€‰æ‹©æœ€ä½³æ£€æŸ¥ç‚¹å…·æœ‰æŒ‘æˆ˜æ€§ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ£€æŸ¥ç‚¹è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”éœ€è¦ä¸€ä¸ªè‰¯å¥½çš„éªŒè¯é›†ï¼Œè€Œä¾èµ–æœ€ç»ˆæ£€æŸ¥ç‚¹æ— æ³•ä¿è¯è‰¯å¥½æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„æ£€æŸ¥ç‚¹é€‰æ‹©æ–¹æ³•ï¼ˆUGCSï¼‰ï¼Œé¿å…äº†è¿™äº›é™·é˜±ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨æ¯ä¸ªæ ·æœ¬çš„ä¸ç¡®å®šæ€§æ¥ç¡®å®šéš¾ä»¥å›ç­”çš„é—®é¢˜å¯¹ï¼Œå¹¶æŒ‰æ£€æŸ¥ç‚¹è§£å†³è¿™äº›å›°éš¾æ¡ˆä¾‹çš„èƒ½åŠ›å¯¹å…¶è¿›è¡Œæ’åã€‚é€šè¿‡å¹³å‡çŸ­æ—¶é—´å†…é¡¶éƒ¨ä¸ç¡®å®šæ ·æœ¬çš„å¥–åŠ±ï¼Œæˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†ä¸€ä¸ªç¨³å®šä¸”å…·æœ‰åŒºåˆ†åº¦çš„ä¿¡å·ï¼Œæ— éœ€é¢å¤–çš„å‰å‘ä¼ é€’æˆ–é‡å¤§è®¡ç®—å¼€é”€ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒå§‹ç»ˆèƒ½å¤Ÿè¯†åˆ«å…·æœ‰æ›´å¼ºæ³›åŒ–èƒ½åŠ›çš„æ£€æŸ¥ç‚¹ï¼Œå¹¶ä¼˜äºä¼ ç»Ÿç­–ç•¥ï¼Œå¦‚ä¾èµ–è®­ç»ƒæˆ–éªŒè¯æ€§èƒ½ã€‚è¿™äº›ç»“æœå¼ºè°ƒï¼Œä»¥ä½ä¸ç¡®å®šæ€§è§£å†³å…¶æœ€å›°éš¾ä»»åŠ¡çš„æ¨¡å‹æ˜¯æœ€å¯é çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09864v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œé€‰æ‹©æœ€ä½³çš„æ£€æŸ¥ç‚¹æ˜¯ä¸€ä¸ªé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹å¼è®¡ç®—é‡å¤§ä¸”æ— æ³•ç¡®ä¿å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„æ£€æŸ¥ç‚¹é€‰æ‹©æ–¹æ³•ï¼ˆUGCSï¼‰ï¼Œé€šè¿‡è¯†åˆ«éš¾ä»¥å›ç­”çš„é—®é¢˜å¯¹å¹¶åˆ©ç”¨æ¯æ ·æœ¬çš„ä¸ç¡®å®šæ€§æ¥æ’åæ£€æŸ¥ç‚¹ï¼Œä»è€Œé¿å…è¿™äº›é—®é¢˜ã€‚é€šè¿‡å¹³å‡çŸ­æœŸå†…é¡¶éƒ¨ä¸ç¡®å®šæ ·æœ¬çš„å¥–åŠ±ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ç¨³å®šä¸”å…·æœ‰åŒºåˆ†åº¦çš„ä¿¡å·ï¼Œæ— éœ€é¢å¤–çš„å‰å‘ä¼ é€’æˆ–å¤§é‡è®¡ç®—å¼€é”€ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå‡èƒ½æœ‰æ•ˆè¯†åˆ«å…·æœ‰æ›´å¼ºæ³›åŒ–èƒ½åŠ›çš„æ£€æŸ¥ç‚¹ï¼Œä¼˜äºä¾èµ–è®­ç»ƒæˆ–éªŒè¯æ€§èƒ½çš„ä¼ ç»Ÿç­–ç•¥ã€‚è¿™è¡¨æ˜è§£å†³å…¶æœ€å›°éš¾ä»»åŠ¡ä¸”ä¸ç¡®å®šæ€§ä½çš„æ¨¡å‹æ˜¯æœ€å¯é çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶å­˜åœ¨é€‰æ‹©æœ€ä½³æ£€æŸ¥ç‚¹çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æ–¹å¼è®¡ç®—é‡å¤§ä¸”æ— æ³•ç¡®ä¿å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºçš„UGCSæ–¹æ³•é€šè¿‡è¯†åˆ«éš¾ä»¥å›ç­”çš„é—®é¢˜å¯¹å¹¶åŸºäºæ¯æ ·æœ¬çš„ä¸ç¡®å®šæ€§æ¥æ’åæ£€æŸ¥ç‚¹ã€‚</li>
<li>UGCSæ–¹æ³•é€šè¿‡å¹³å‡çŸ­æœŸå†…é¡¶éƒ¨ä¸ç¡®å®šæ ·æœ¬çš„å¥–åŠ±ï¼Œæä¾›ç¨³å®šä¸”å…·æœ‰åŒºåˆ†åº¦çš„ä¿¡å·ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„å‰å‘ä¼ é€’æˆ–å¤§é‡è®¡ç®—å¼€é”€ã€‚</li>
<li>å®éªŒè¡¨æ˜UGCSæ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’Œè¯­è¨€æ¨¡å‹ä¸Šèƒ½æœ‰æ•ˆè¯†åˆ«å…·æœ‰æ›´å¼ºæ³›åŒ–èƒ½åŠ›çš„æ£€æŸ¥ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-830dc86ce4f41d4c3b4046942fd9aa8b" align="middle">
<img src="https://picx.zhimg.com/v2-8abdde0f3e1149122ecbeefa13f721ac" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ConstrainedSQL-Training-LLMs-for-Text2SQL-via-Constrained-Reinforcement-Learning"><a href="#ConstrainedSQL-Training-LLMs-for-Text2SQL-via-Constrained-Reinforcement-Learning" class="headerlink" title="ConstrainedSQL: Training LLMs for Text2SQL via Constrained Reinforcement Learning"></a>ConstrainedSQL: Training LLMs for Text2SQL via Constrained Reinforcement Learning</h2><p><strong>Authors:Weiqin Chen, Nhan Huu Pham, Michael Robert Glass, Long Hai Vu, Gaetano Rossiello, Dharmashankar Subramanian, Santiago Paternain</strong></p>
<p>Reinforcement learning (RL) has demonstrated significant promise in enhancing the reasoning capabilities of Text2SQL LLMs, especially with advanced algorithms such as GRPO and DAPO. However, the performance of these methods is highly sensitive to the design of reward functions. Inappropriate rewards can lead to reward hacking, where models exploit loopholes in the reward structure to achieve high scores without genuinely solving the task. This work considers a constrained RL framework for Text2SQL that incorporates natural and interpretable reward and constraint signals, while dynamically balancing trade-offs among them during the training. We establish the theoretical guarantees of our constrained RL framework and our numerical experiments on the well-known Text2SQL datasets substantiate the improvement of our approach over the state-of-the-art RL-trained LLMs.</p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡Text2SQLå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯é‡‡ç”¨GRPOå’ŒDAPOç­‰é«˜çº§ç®—æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„æ€§èƒ½é«˜åº¦ä¾èµ–äºå¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€‚ä¸æ°å½“çš„å¥–åŠ±ä¼šå¯¼è‡´å¥–åŠ±æ»¥ç”¨ï¼Œå³æ¨¡å‹ä¼šåˆ©ç”¨å¥–åŠ±ç»“æ„ä¸­çš„æ¼æ´æ¥è·å¾—é«˜åˆ†ï¼Œè€Œå¹¶æœªçœŸæ­£å®Œæˆä»»åŠ¡ã€‚æœ¬ç ”ç©¶è€ƒè™‘äº†ä¸€ä¸ªç”¨äºText2SQLçš„çº¦æŸæ€§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è‡ªç„¶å’Œå¯è§£é‡Šçš„å¥–åŠ±å’Œçº¦æŸä¿¡å·ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å¹³è¡¡å®ƒä»¬ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„çº¦æŸæ€§å¼ºåŒ–å­¦ä¹ æ¡†æ¶å»ºç«‹äº†ç†è®ºä¿è¯ï¼Œå¹¶åœ¨è‘—åçš„Text2SQLæ•°æ®é›†ä¸Šè¿›è¡Œçš„æ•°å€¼å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°çš„RLè®­ç»ƒLLMæœ‰æ‰€æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09693v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ åœ¨æå‡Text2SQLå¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†æ€§èƒ½é«˜åº¦ä¾èµ–äºå¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€‚ä¸æ°å½“çš„å¥–åŠ±å¯èƒ½å¯¼è‡´æ¨¡å‹å¥–åŠ±æ»¥ç”¨ï¼Œå³æ¨¡å‹åˆ©ç”¨å¥–åŠ±ç»“æ„çš„æ¼æ´å®ç°é«˜åˆ†è€ŒæœªçœŸæ­£å®Œæˆä»»åŠ¡ã€‚æœ¬ç ”ç©¶é‡‡ç”¨çº¦æŸå¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡ŒText2SQLä»»åŠ¡ï¼Œç»“åˆè‡ªç„¶ã€å¯è§£é‡Šçš„å¥–åŠ±å’Œçº¦æŸä¿¡å·ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å¹³è¡¡è¿™äº›ä¿¡å·ä¹‹é—´çš„æƒè¡¡ã€‚ç†è®ºä¿è¯å’Œæ•°å€¼å®éªŒå‡è¯æ˜è¯¥æ¡†æ¶ç›¸è¾ƒäºç°æœ‰å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤§æ¨¡å‹æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨Text2SQLå¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æå‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨GRPOå’ŒDAPOç­‰é«˜çº§ç®—æ³•ã€‚</li>
<li>ä¸æ°å½“çš„å¥–åŠ±å‡½æ•°è®¾è®¡å¯èƒ½å¯¼è‡´æ¨¡å‹å¥–åŠ±æ»¥ç”¨ï¼Œä½¿æ¨¡å‹é€šè¿‡åˆ©ç”¨å¥–åŠ±ç»“æ„æ¼æ´æ¥è·å¾—é«˜åˆ†è€ŒæœªçœŸæ­£å®Œæˆä»»åŠ¡ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§çº¦æŸå¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡ŒText2SQLä»»åŠ¡ï¼Œè¯¥æ¡†æ¶ç»“åˆè‡ªç„¶ã€å¯è§£é‡Šçš„å¥–åŠ±å’Œçº¦æŸä¿¡å·ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å¹³è¡¡å„ç§ä¿¡å·ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>ç ”ç©¶å»ºç«‹äº†è¯¥çº¦æŸå¼ºåŒ–å­¦ä¹ æ¡†æ¶çš„ç†è®ºä¿è¯ã€‚</li>
<li>æ•°å€¼å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨çŸ¥åText2SQLæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤§æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccab66ead1df97d5c41f647d90b3e458" align="middle">
<img src="https://picx.zhimg.com/v2-17408244f0f193efed6f026a2ac54120" align="middle">
<img src="https://picx.zhimg.com/v2-80f1418d34e768442601b894b2f55eb8" align="middle">
<img src="https://picx.zhimg.com/v2-8ccbbe7875ff72e421aaaf9b2d00b727" align="middle">
<img src="https://picx.zhimg.com/v2-13adaf6af3014142af009955cb194d2f" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MMaDA-Parallel-Multimodal-Large-Diffusion-Language-Models-for-Thinking-Aware-Editing-and-Generation"><a href="#MMaDA-Parallel-Multimodal-Large-Diffusion-Language-Models-for-Thinking-Aware-Editing-and-Generation" class="headerlink" title="MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation"></a>MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</h2><p><strong>Authors:Ye Tian, Ling Yang, Jiongfan Yang, Anran Wang, Yu Tian, Jiani Zheng, Haochen Wang, Zhiyang Teng, Zhuochen Wang, Yinjie Wang, Yunhai Tong, Mengdi Wang, Xiangtai Li</strong></p>
<p>While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/tyfeld/MMaDA-Parallel">https://github.com/tyfeld/MMaDA-Parallel</a></p>
<blockquote>
<p>è™½ç„¶æ€è€ƒæ„ŸçŸ¥ç”Ÿæˆæ—¨åœ¨æé«˜å¤æ‚ä»»åŠ¡çš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸€ç§å…³é”®çš„å¤±è´¥æ¨¡å¼ï¼Œå³ç°æœ‰çš„åºåˆ—ã€è‡ªå›å½’æ–¹æ³•ä¼šç”±äºè¯¯å·®ä¼ æ’­è€Œæ„å¤–åœ°é™ä½æ€§èƒ½ã€‚ä¸ºäº†ç³»ç»Ÿåœ°åˆ†æè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ParaBenchï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ–‡æœ¬å’Œå›¾åƒè¾“å‡ºæ¨¡å¼çš„æ–°åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨ParaBenchçš„åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ€§èƒ½ä¸‹é™ä¸ç”Ÿæˆæ¨ç†å’Œæœ€ç»ˆå›¾åƒä¹‹é—´çš„å¯¹é½ä¸è‰¯æœ‰ç€å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09611v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://tyfeld.github.io/mmadaparellel.github.io/">https://tyfeld.github.io/mmadaparellel.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸­æåˆ°ï¼Œæ€è€ƒæ„ŸçŸ¥ç”Ÿæˆæ—¨åœ¨æé«˜å¤æ‚ä»»åŠ¡çš„æ€§èƒ½ï¼Œä½†ç°æœ‰åºã€è‡ªå›å½’æ–¹æ³•å­˜åœ¨ä¸€ç§å…³é”®å¤±æ•ˆæ¨¡å¼ï¼Œå³é”™è¯¯ä¼ æ’­å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºç³»ç»Ÿåˆ†æè¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ParaBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬å’Œå›¾åƒè¾“å‡ºæ¨¡å¼ã€‚åˆ†ææ˜¾ç¤ºæ€§èƒ½ä¸‹é™ä¸ç”Ÿæˆæ¨ç†å’Œæœ€ç»ˆå›¾åƒä¹‹é—´çš„å¯¹é½ä¸è‰¯æœ‰å…³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œä½œè€…æå‡ºäº†å¹¶è¡Œå¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶MMaDA-Parallelï¼Œåœ¨æ•´ä¸ªå»å™ªè½¨è¿¹ä¸­å®ç°æ–‡æœ¬å’Œå›¾åƒçš„æŒç»­åŒå‘äº¤äº’ã€‚è¯¥æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒè¿›è¡Œè®­ç»ƒï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡Parallel Reinforcement Learningï¼ˆParaRLï¼‰ä¼˜åŒ–ï¼Œæ²¿è½¨è¿¹åº”ç”¨è¯­ä¹‰å¥–åŠ±æ¥å¼ºåˆ¶æ‰§è¡Œè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜è¯¥æ¨¡å‹æ˜¾è‘—æé«˜äº†è·¨æ¨¡æ€å¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œåœ¨ParaBenchä¸Šçš„è¾“å‡ºå¯¹é½åº¦ç›¸æ¯”æœ€æ–°æ¨¡å‹Bagelæé«˜äº†6.9%ï¼Œä¸ºæ€è€ƒæ„ŸçŸ¥å›¾åƒåˆæˆå»ºç«‹äº†æ›´ç¨³å¥çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è‡ªå›å½’æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­å­˜åœ¨é”™è¯¯ä¼ æ’­å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>ParaBenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°æ–‡æœ¬å’Œå›¾åƒè¾“å‡ºæ¨¡å¼ï¼Œæ­ç¤ºäº†ç”Ÿæˆæ¨ç†ä¸å›¾åƒå¯¹é½é—®é¢˜ã€‚</li>
<li>MMaDA-Parallelæ¡†æ¶å®ç°æ–‡æœ¬å’Œå›¾åƒçš„æŒç»­åŒå‘äº¤äº’ï¼Œæé«˜è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>MMaDA-Parallelé€šè¿‡ç›‘ç£å¾®è°ƒè¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨Parallel Reinforcement Learningï¼ˆParaRLï¼‰ä¼˜åŒ–ã€‚</li>
<li>ParaRLé€šè¿‡æ²¿è½¨è¿¹åº”ç”¨è¯­ä¹‰å¥–åŠ±æ¥å¼ºåŒ–è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜MMaDA-Parallelæ¨¡å‹åœ¨ParaBenchä¸Šæ˜¾è‘—æé«˜è¾“å‡ºå¯¹é½åº¦ï¼Œç›¸å¯¹Bagelæ¨¡å‹æœ‰6.9%çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7bbf5398f9ab3ddc1462c21b84fddfdc" align="middle">
<img src="https://picx.zhimg.com/v2-d60d55e15b807652b0a47e5cd811faf4" align="middle">
<img src="https://picx.zhimg.com/v2-84fd4c7d240f3acd55f25cb46fbb39fb" align="middle">
<img src="https://picx.zhimg.com/v2-23b5121eaf775eb73b369eb65d47504e" align="middle">
<img src="https://picx.zhimg.com/v2-f32b92308ede5f0449e0f4317581be18" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Scaling-Environments-for-LLM-Agents-in-the-Era-of-Learning-from-Interaction-A-Survey"><a href="#Scaling-Environments-for-LLM-Agents-in-the-Era-of-Learning-from-Interaction-A-Survey" class="headerlink" title="Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey"></a>Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey</h2><p><strong>Authors:Yuchen Huang, Sijia Li, Minghao Liu, Wei Liu, Shijue Huang, Zhiyuan Fan, Hou Pong Chan, Yi R. Fung</strong></p>
<p>LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agentsâ€™ actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze benchmarks, implementation strategies, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.</p>
<blockquote>
<p>åŸºäºLLMçš„ä»£ç†å¯ä»¥è‡ªä¸»å®Œæˆå„ç§é¢†åŸŸçš„å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¸ºäº†è¿›ä¸€æ­¥åŸ¹å…»é€‚åº”è¡Œä¸ºå’Œé•¿æœŸå†³ç­–åˆ¶å®šç­‰èƒ½åŠ›ï¼Œä»…ä¾é å¯¹äººç±»çŸ¥è¯†æ°´å¹³é™æ€æ•°æ®é›†çš„è®­ç»ƒæ˜¯ä¸è¶³çš„ã€‚è¿™äº›æ•°æ®é›†æ„å»ºæˆæœ¬é«˜æ˜‚ï¼Œç¼ºä¹åŠ¨æ€å’ŒçœŸå®æ€§ã€‚è¶Šæ¥è¶Šå¤šçš„å…±è¯†è®¤ä¸ºï¼Œä»£ç†åº”è¯¥ç›´æ¥ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä»ç»éªŒä¸­å­¦ä¹ ã€‚æˆ‘ä»¬å°†è¿™ä¸€è¿­ä»£è¿‡ç¨‹æ­£å¼ç¡®å®šä¸ºç”Ÿæˆ-æ‰§è¡Œ-åé¦ˆï¼ˆGEFï¼‰å¾ªç¯ï¼Œå…¶ä¸­ç¯å¢ƒç”Ÿæˆä»»åŠ¡ä»¥æŒ‘æˆ˜ä»£ç†ï¼Œåœ¨ä»»åŠ¡æ‰§è¡ŒæœŸé—´å¯¹ä»£ç†çš„è¡Œä¸ºåšå‡ºååº”å¹¶è¿”å›è§‚å¯Ÿç»“æœï¼Œå¯¹æ‰§è¡Œç»“æœæä¾›è¯„ä¼°åé¦ˆä»¥ä¾›åç»­å­¦ä¹ ã€‚åœ¨è¿™ä¸€æ¨¡å¼ä¸‹ï¼Œç¯å¢ƒä½œä¸ºç»éªŒæ•°æ®çš„ä¸å¯æˆ–ç¼ºçš„ç”Ÿäº§è€…ï¼Œéœ€è¦æœç€æ›´å¤§çš„å¤æ‚æ€§ã€çœŸå®æ€§å’Œäº¤äº’æ€§è¿›è¡Œæ‰©å±•ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å›é¡¾äº†ä»ä»¥ç¯å¢ƒä¸ºä¸­å¿ƒçš„è§’åº¦è¿›è¡Œç¯å¢ƒè§„æ¨¡æ‰©å±•çš„ä»£è¡¨æ–¹æ³•ï¼Œå¹¶æŒ‰GEFå¾ªç¯çš„é˜¶æ®µï¼ˆå³ä»»åŠ¡ç”Ÿæˆã€ä»»åŠ¡æ‰§è¡Œå’Œåé¦ˆï¼‰å¯¹å…¶è¿›è¡Œæ•´ç†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†åŸºå‡†æµ‹è¯•ã€å®æ–½ç­–ç•¥å’Œåº”ç”¨ï¼Œæ•´åˆäº†ç¢ç‰‡åŒ–çš„è¿›å±•ï¼Œå¹¶æ¦‚è¿°äº†æ™ºèƒ½ä»£ç†çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09586v1">PDF</a> 20 pages, 4 figures, SEA Workshop @ NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ä»£ç†äººåœ¨ä¸åŒé¢†åŸŸè‡ªä¸»å®Œæˆå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä»…é åŸºäºäººç±»çŸ¥è¯†çš„é™æ€æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ— æ³•åŸ¹å…»ä»£ç†äººçš„é€‚åº”è¡Œä¸ºå’Œé•¿æœŸå†³ç­–èƒ½åŠ›ã€‚å› æ­¤ï¼Œæå€¡è®©ä»£ç†äººç›´æ¥ä¸ç¯å¢ƒäº’åŠ¨ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»ç»éªŒä¸­å­¦ä¹ ã€‚æ–‡ç« å°†è¿™ä¸€è¿‡ç¨‹å½¢å¼åŒ–ä¸ºç”Ÿæˆ-æ‰§è¡Œ-åé¦ˆï¼ˆGEFï¼‰å¾ªç¯ï¼Œå¹¶å¼ºè°ƒç¯å¢ƒä½œä¸ºç»éªŒæ•°æ®ä¸å¯æˆ–ç¼ºçš„ç”Ÿäº§è€…ï¼Œéœ€è¦å‘æ›´å¤§çš„å¤æ‚æ€§ã€ç°å®æ€§å’Œäº’åŠ¨æ€§æ‰©å±•ã€‚æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†ç¯å¢ƒæ‰©å±•çš„ä»£è¡¨æ–¹æ³•ï¼Œå¹¶åˆ†æäº†åŸºå‡†æµ‹è¯•ã€å®æ–½ç­–ç•¥å’Œåº”ç”¨ç¨‹åºï¼ŒåŒæ—¶å·©å›ºäº†åˆ†æ•£çš„è¿›å±•å¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based agents can accomplish complex tasks across various domains.</li>
<li>Training on static datasets built from human-level knowledge is insufficient for cultivating capabilities like adaptive behavior and long-term decision-making.</li>
<li>Agents should interact directly with environments and learn from experience through reinforcement learning.</li>
<li>The generation-execution-feedback (GEF) loop formalizes the process of agent learning from environments.</li>
<li>Environments are indispensable for producing experiential data, necessitating scaling them for greater complexity, realism, and interactivity.</li>
<li>This survey systematically reviews methods for environment scaling from a pioneering environment-centric perspective.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ce9dff6fa5462d7f5e1c9edc1f60eaa" align="middle">
<img src="https://picx.zhimg.com/v2-1b6e352a3e32fb9b317ccc300d9f0f98" align="middle">
<img src="https://picx.zhimg.com/v2-3e9ddb8223a6bf26f6de9e67b6374c7d" align="middle">
<img src="https://picx.zhimg.com/v2-4d01b0ecd0c77b0042a574957fedca25" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Efficient-Reasoning-via-Reward-Model"><a href="#Efficient-Reasoning-via-Reward-Model" class="headerlink" title="Efficient Reasoning via Reward Model"></a>Efficient Reasoning via Reward Model</h2><p><strong>Authors:Yuhao Wang, Xiaopeng Li, Cheng Gong, Ziru Liu, Suiyun Zhang, Rui Liu, Xiangyu Zhao</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the methodâ€™s effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CRM">https://anonymous.4open.science/r/CRM</a>.</p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²è¯æ˜å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œå®ç°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼ŒåƒDeepSeek-R1å’ŒOpenAI o1è¿™æ ·çš„LRMç»å¸¸äº§ç”Ÿå†—é•¿ä¸”åŒ…å«å¤šä½™æˆ–æ— å…³æ¨ç†æ­¥éª¤çš„å“åº”â€”â€”è¿™ç§ç°è±¡è¢«ç§°ä¸ºè¿‡åº¦æ€è€ƒï¼Œè¿™æå¤§åœ°å¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚ä¹‹å‰ä¸ºè§£å†³æ­¤é—®é¢˜çš„å°è¯•é€šå¸¸ä¼šåœ¨å¥–åŠ±å‡½æ•°ä¸­åŠ å…¥é•¿åº¦æƒ©ç½šï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒä»¬ç»å¸¸é¢ä¸´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šé•¿åº¦å´©æºƒå’Œè®­ç»ƒå´©æºƒï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è®­ç»ƒç®€æ´æ€§å¥–åŠ±æ¨¡å‹ï¼ˆCRMï¼‰çš„ç®¡é“ï¼Œè¯¥æ¨¡å‹èƒ½å¯¹æ¨ç†è·¯å¾„çš„ç®€æ´æ€§è¿›è¡Œè¯„åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºç®€æ´æ€§å¥–åŠ±å‡½æ•°ï¼ˆCRFï¼‰çš„æ–°å‹å¥–åŠ±å…¬å¼ï¼Œè¯¥å…¬å¼å°†ç»“æœå¥–åŠ±ä¸ç®€æ´æ€§åˆ†æ•°ä¹‹é—´å»ºç«‹äº†æ˜ç¡®çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œä¿ƒè¿›äº†æ›´æœ‰æ•ˆã€æ›´é«˜æ•ˆçš„æ¨ç†ã€‚ä»ç†è®ºä¸Šçœ‹ï¼Œæˆ‘ä»¬ä»é™ä½å·®å¼‚æ€§å’Œæ”¹å–„æ”¶æ•›æ€§ä¸¤ä¸ªè§’åº¦è¯æ˜äº†æ–°å¥–åŠ±çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œåœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ ‡è®°æ•ˆç‡ï¼Œåœ¨Qwen2.5-7Bä¸Šå®ç°äº†8.1%çš„å‡†ç¡®ç‡æå‡å’Œ19.9%çš„å“åº”ä»¤ç‰Œé•¿åº¦å‡å°‘ã€‚è€Œä¸”ï¼Œè¯¥æ–¹æ³•å¯ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°å…¶ä»–LLMï¼ŒåŒ…æ‹¬Llamaå’ŒMistralã€‚ç›¸å…³å®ç°ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€ä»¥ä¾›å¤åˆ¶ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CRM%E3%80%82">https://anonymous.4open.science/r/CRMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09158v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼ŒLRMså¦‚DeepSeek-R1å’ŒOpenAI o1äº§ç”Ÿçš„å“åº”å†—é•¿ï¼ŒåŒ…å«å¤§é‡å†—ä½™æˆ–æ— å…³æ¨ç†æ­¥éª¤ï¼Œå³â€œè¿‡åº¦æ€è€ƒâ€ï¼Œè¿™å¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è®­ç»ƒç®€æ´å¥–åŠ±æ¨¡å‹ï¼ˆCRMï¼‰çš„ç®¡é“ï¼Œè¯¥æ¨¡å‹å¯¹æ¨ç†è·¯å¾„çš„ç®€æ´æ€§è¿›è¡Œè¯„åˆ†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºç®€æ´å¥–åŠ±å‡½æ•°ï¼ˆCRFï¼‰çš„æ–°å‹å¥–åŠ±å…¬å¼ï¼Œå°†ç»“æœå¥–åŠ±ä¸ç®€æ´æ€§è¯„åˆ†ä¹‹é—´å»ºç«‹æ˜ç¡®ä¾èµ–å…³ç³»ï¼Œä»è€Œä¿ƒè¿›æ›´æœ‰æ•ˆã€æ›´é«˜æ•ˆçš„æ¨ç†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ•ˆæœï¼Œå“åº”ä»¤ç‰Œé•¿åº¦å‡å°‘äº†19.9%ï¼Œå‡†ç¡®ç‡æé«˜äº†8.1%ã€‚æ­¤æ–¹æ³•å¯¹å…¶ä»–LLMså¦‚Llamaå’ŒMistralå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRæé«˜äº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨äº†LRMçš„å‘å±•ã€‚</li>
<li>LRMså¦‚DeepSeek-R1å’ŒOpenAI o1å­˜åœ¨ç”Ÿæˆå†—é•¿å“åº”çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡åœ¨å¥–åŠ±å‡½æ•°ä¸­å¼•å…¥é•¿åº¦æƒ©ç½šæ¥å‡è½»è¿™ä¸€é—®é¢˜ï¼Œä½†å­˜åœ¨é•¿åº¦å´©æºƒå’ŒåŸ¹è®­å´©æºƒä¸¤ä¸ªå…³é”®é—®é¢˜ã€‚</li>
<li>æå‡ºè®­ç»ƒç®€æ´å¥–åŠ±æ¨¡å‹ï¼ˆCRMï¼‰æ¥è¯„åˆ†æ¨ç†è·¯å¾„çš„ç®€æ´æ€§ã€‚</li>
<li>å¼•å…¥ç®€æ´å¥–åŠ±å‡½æ•°ï¼ˆCRFï¼‰ï¼Œå»ºç«‹ç»“æœå¥–åŠ±ä¸ç®€æ´æ€§è¯„åˆ†ä¹‹é—´çš„æ˜ç¡®ä¾èµ–å…³ç³»ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æ•°æ®é›†ä¸Šæœ‰æ•ˆï¼Œå“åº”ä»¤ç‰Œé•¿åº¦å‡å°‘ï¼Œå‡†ç¡®ç‡æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d54b7dd80d763cf4e5b0c19f3837d95" align="middle">
<img src="https://picx.zhimg.com/v2-235f7d068c6b17ca35a8d57035282199" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="History-Aware-Reasoning-for-GUI-Agents"><a href="#History-Aware-Reasoning-for-GUI-Agents" class="headerlink" title="History-Aware Reasoning for GUI Agents"></a>History-Aware Reasoning for GUI Agents</h2><p><strong>Authors:Ziwei Wang, Leyang Yang, Xiaoxuan Tang, Sheng Zhou, Dajun Chen, Wei Jiang, Yong Li</strong></p>
<p>Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between usersâ€™ concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æå¤§åœ°ä¿ƒè¿›äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„è‡ªåŠ¨åŒ–ã€‚ä¸ºGUIä»£ç†é…å¤‡å¯é çš„æƒ…æ™¯æ¨ç†èƒ½åŠ›æ˜¯ç¼©å°ç”¨æˆ·ç®€æ´ä»»åŠ¡æè¿°ä¸çœŸå®ä¸–ç•Œæ‰§è¡Œå¤æ‚æ€§ä¹‹é—´å·®è·çš„å…³é”®ã€‚å½“å‰çš„æ–¹æ³•å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ç³»ç»Ÿ2æ€ç»´é“¾ç›¸ç»“åˆï¼Œåœ¨æ¨ç†å¢å¼ºæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚å¯¹äºé•¿å‘¨æœŸGUIä»»åŠ¡ï¼Œå†å²äº¤äº’å°†æ¯ä¸ªå±å¹•ä¸ä»¥ç›®æ ‡ä¸ºå¯¼å‘çš„æƒ…èŠ‚é“¾è¿æ¥èµ·æ¥ï¼Œæœ‰æ•ˆåˆ©ç”¨è¿™äº›çº¿ç´¢å¯¹äºå½“å‰å†³ç­–è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æœ¬æœºGUIä»£ç†åœ¨æ˜¾æ€§æ¨ç†æ–¹é¢è¡¨ç°å‡ºçŸ­æœŸè®°å¿†å¼±çš„ç¼ºé™·ï¼Œä»–ä»¬å°†è¿é”äº’åŠ¨è§£é‡Šä¸ºç¦»æ•£å±å¹•ç†è§£ï¼Œå³ä¸äº†è§£æƒ…èŠ‚å†…çš„å†å²äº¤äº’ã€‚è¿™ç§å¿½è§†å†å²çš„æ¨ç†æŒ‘æˆ˜äº†å®ƒä»¬åœ¨GUIè‡ªåŠ¨åŒ–æ–¹é¢çš„æ€§èƒ½ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€å¼±ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†å†å²æ„ŸçŸ¥æ¨ç†ï¼ˆHARï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¼“åŠ±ä»£ç†åæ€è‡ªå·±çš„é”™è¯¯ï¼Œå¹¶é€šè¿‡å®šåˆ¶çš„ç­–ç•¥ä»é”™è¯¯ä¸­è·å–æƒ…æ™¯æ¨ç†çŸ¥è¯†ï¼Œè¿™äº›ç­–ç•¥å¢å¼ºäº†é•¿å‘¨æœŸäº¤äº’çš„çŸ­æœŸè®°å¿†ã€‚è¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬æ„å»ºåæ€å­¦ä¹ åœºæ™¯ã€ç»¼åˆå®šåˆ¶ä¿®æ­£æŒ‡å—ä»¥åŠè®¾è®¡æ··åˆRLå¥–åŠ±åŠŸèƒ½ã€‚ä½¿ç”¨HARæ¡†æ¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ¬æœºç«¯åˆ°ç«¯æ¨¡å‹HAR-GUI-3Bï¼Œå®ƒå°†å›ºæœ‰çš„æ¨ç†æ¨¡å¼ä»å¿½è§†å†å²è½¬å˜ä¸ºæ„ŸçŸ¥å†å²ï¼Œä¸ºGUIä»£ç†æä¾›ç¨³å®šçš„çŸ­æœŸè®°å¿†å’Œå¯é çš„å±å¹•ç»†èŠ‚æ„ŸçŸ¥ã€‚åœ¨å¤šç§GUIç›¸å…³åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09127v1">PDF</a> Paper accepted to AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä¸ºäº†åœ¨ç”¨æˆ·çš„ç®€æ´ä»»åŠ¡æè¿°ä¸ç°å®ä¸–ç•Œçš„å¤æ‚æ‰§è¡Œä¹‹é—´æ­å»ºæ¡¥æ¢ï¼Œä¸ºGUIä»£ç†é…å¤‡å¯é çš„æƒ…æ™¯æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚å½“å‰æ–¹æ³•å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ç³»ç»Ÿ2æ€ç»´é“¾ç›¸ç»“åˆï¼Œåœ¨æ¨ç†å¢å¼ºæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚å¯¹äºé•¿æœŸGUIä»»åŠ¡ï¼Œå†å²äº’åŠ¨å°†æ¯ä¸ªå±å¹•ä¸ä»¥ç›®æ ‡ä¸ºå¯¼å‘çš„å‰§é›†é“¾ç›¸è¿æ¥ï¼Œæœ‰æ•ˆåˆ©ç”¨è¿™äº›çº¿ç´¢å¯¹å½“å‰çš„å†³ç­–è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æœ¬åœ°GUIä»£ç†åœ¨æ˜¾æ€§æ¨ç†æ–¹é¢å­˜åœ¨çŸ­æœŸè®°å¿†å¼±çš„ç¼ºé™·ï¼Œä»–ä»¬å°†è¿é”äº’åŠ¨è§£é‡Šä¸ºç¦»æ•£å±å¹•ç†è§£ï¼Œå³ä¸äº†è§£å‰§é›†å†…çš„å†å²äº’åŠ¨ã€‚é’ˆå¯¹è¿™ä¸€å†å²æ— çŸ¥æ¨ç†çš„ç¼ºé™·ï¼Œæˆ‘ä»¬æå‡ºäº†å†å²æ„ŸçŸ¥æ¨ç†ï¼ˆHARï¼‰æ¡†æ¶ï¼Œé¼“åŠ±ä»£ç†åæ€è‡ªå·±çš„é”™è¯¯ï¼Œå¹¶é€šè¿‡å®šåˆ¶ç­–ç•¥ä»é”™è¯¯ä¸­è·å–æƒ…æ™¯æ¨ç†çŸ¥è¯†ï¼Œå¢å¼ºé•¿æœŸäº’åŠ¨ä¸­çš„çŸ­æœŸè®°å¿†ã€‚ä½¿ç”¨HARæ¡†æ¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æœ¬åœ°æ¨¡å‹HAR-GUI-3Bï¼Œå®ƒå°†å›ºæœ‰çš„æ¨ç†æ¨¡å¼ä»ç¼ºä¹å†å²æ„ŸçŸ¥è½¬å˜ä¸ºå…·å¤‡å†å²æ„ŸçŸ¥ï¼Œä¸ºGUIä»£ç†é…å¤‡ç¨³å®šçš„çŸ­æœŸè®°å¿†å’Œå¯é çš„å±å¹•ç»†èŠ‚æ„ŸçŸ¥ã€‚å…¨é¢è¯„ä¼°GUIç›¸å…³åŸºå‡†æµ‹è¯•çš„æ•ˆæœï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ˜¾è‘—å¢å¼ºäº†GUIè‡ªåŠ¨åŒ–ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ç³»ç»Ÿ2æ€ç»´é“¾çš„ç»“åˆåœ¨å¢å¼ºGUIä»£ç†çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚</li>
<li>å¯¹äºé•¿æœŸGUIä»»åŠ¡ï¼Œå†å²äº’åŠ¨å¯¹å½“å‰çš„å†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰GUIä»£ç†å­˜åœ¨çŸ­æœŸè®°å¿†å¼±çš„ç¼ºé™·ï¼Œæ— æ³•å……åˆ†ç†è§£å†å²äº’åŠ¨çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†å†å²æ„ŸçŸ¥æ¨ç†ï¼ˆHARï¼‰æ¡†æ¶ï¼Œé¼“åŠ±ä»£ç†ä»é”™è¯¯ä¸­å­¦ä¹ å¹¶å¢å¼ºçŸ­æœŸè®°å¿†ã€‚</li>
<li>ä½¿ç”¨HARæ¡†æ¶å¼€å‘çš„HAR-GUI-3Bæ¨¡å‹å…·å¤‡å†å²æ„ŸçŸ¥çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09127">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d27df4f01f75c28dc40db652f0589f72" align="middle">
<img src="https://picx.zhimg.com/v2-e784d07e616eb9584e8dfbe9d8c3d191" align="middle">
<img src="https://picx.zhimg.com/v2-05cce51a2449234e7f7714a444cee79e" align="middle">
<img src="https://picx.zhimg.com/v2-316655c995444374aa2ca3176ebf6118" align="middle">
<img src="https://picx.zhimg.com/v2-1cc543c3036d38be26cf8354a8e0daef" align="middle">
<img src="https://picx.zhimg.com/v2-ce9d479e7ec1ab274d1e1a9c1e519d88" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="OR-R1-Automating-Modeling-and-Solving-of-Operations-Research-Optimization-Problem-via-Test-Time-Reinforcement-Learning"><a href="#OR-R1-Automating-Modeling-and-Solving-of-Operations-Research-Optimization-Problem-via-Test-Time-Reinforcement-Learning" class="headerlink" title="OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning"></a>OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning</h2><p><strong>Authors:Zezhen Ding, Zhen Tan, Jiheng Zhang, Tianlong Chen</strong></p>
<p>Optimization modeling and solving are fundamental to the application of Operations Research (OR) in real-world decision making, yet the process of translating natural language problem descriptions into formal models and solver code remains highly expertise intensive. While recent advances in large language models (LLMs) have opened new opportunities for automation, the generalization ability and data efficiency of existing LLM-based methods are still limited, asmost require vast amounts of annotated or synthetic data, resulting in high costs and scalability barriers. In this work, we present OR-R1, a data-efficient training framework for automated optimization modeling and solving. OR-R1 first employs supervised fine-tuning (SFT) to help the model acquire the essential reasoning patterns for problem formulation and code generation from limited labeled data. In addition, it improves the capability and consistency through Test-Time Group Relative Policy Optimization (TGRPO). This two-stage design enables OR-R1 to leverage both scarce labeled and abundant unlabeled data for effective learning. Experiments show that OR-R1 achieves state-of-the-art performance with an average solving accuracy of $67.7%$, using only $1&#x2F;10$ the synthetic data required by prior methods such as ORLM, exceeding ORLMâ€™s solving accuracy by up to $4.2%$. Remarkably, OR-R1 outperforms ORLM by over $2.4%$ with just $100$ synthetic samples. Furthermore, TGRPO contributes an additional $3.1%-6.4%$ improvement in accuracy, significantly narrowing the gap between single-attempt (Pass@1) and multi-attempt (Pass@8) performance from $13%$ to $7%$. Extensive evaluations across diverse real-world benchmarks demonstrate that OR-R1 provides a robust, scalable, and cost-effective solution for automated OR optimization problem modeling and solving, lowering the expertise and data barriers for industrial OR applications.</p>
<blockquote>
<p>ä¼˜åŒ–å»ºæ¨¡å’Œæ±‚è§£æ˜¯è¿ç­¹å­¦ï¼ˆORï¼‰åœ¨ç°å®ä¸–ç•Œå†³ç­–åº”ç”¨ä¸­ä¸å¯æˆ–ç¼ºçš„åŸºç¡€ï¼Œç„¶è€Œå°†è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°è½¬åŒ–ä¸ºæ­£å¼æ¨¡å‹å’Œæ±‚è§£å™¨ä»£ç çš„è¿‡ç¨‹ä»ç„¶éœ€è¦é«˜åº¦ä¸“ä¸šåŒ–çš„çŸ¥è¯†ã€‚å°½ç®¡æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºè‡ªåŠ¨åŒ–æä¾›äº†æ–°çš„æœºä¼šï¼Œä½†ç°æœ‰LLMæ–¹æ³•çš„ä¸€èˆ¬åŒ–èƒ½åŠ›å’Œæ•°æ®æ•ˆç‡ä»ç„¶æœ‰é™ï¼Œå› ä¸ºå¤§å¤šæ•°éœ€è¦å¤§é‡çš„æ³¨é‡Šæˆ–åˆæˆæ•°æ®ï¼Œå¯¼è‡´æˆæœ¬é«˜æ˜‚å’Œå¯æ‰©å±•æ€§éšœç¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†OR-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºæ¨¡å’Œæ±‚è§£çš„æ•°æ®é«˜æ•ˆè®­ç»ƒæ¡†æ¶ã€‚OR-R1é¦–å…ˆé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ï¼Œå¸®åŠ©æ¨¡å‹ä»æœ‰é™çš„æœ‰æ ‡ç­¾æ•°æ®ä¸­æŒæ¡é—®é¢˜åˆ¶å®šå’Œä»£ç ç”Ÿæˆçš„åŸºæœ¬æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜é€šè¿‡æµ‹è¯•æ—¶é—´ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTGRPOï¼‰æé«˜äº†èƒ½åŠ›å’Œä¸€è‡´æ€§ã€‚è¿™ç§ä¸¤é˜¶æ®µçš„è®¾è®¡ä½¿OR-R1èƒ½å¤Ÿåˆ©ç”¨ç¨€ç¼ºçš„æœ‰æ ‡ç­¾æ•°æ®å’Œä¸°å¯Œçš„æ— æ ‡ç­¾æ•°æ®è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒOR-R1çš„è§£å†³å‡†ç¡®ç‡è¾¾åˆ°äº†å¹³å‡æ°´å¹³çš„æœ€é«˜æ°´å¹³ä¸ºå¹³å‡$ 67.7%$ ï¼Œåªéœ€è¦å…ˆå‰çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ORLMï¼‰ååˆ†ä¹‹ä¸€çš„æ•°æ®åˆæˆéœ€æ±‚å°±èƒ½å®ç°è¿™ç§æ•ˆæœã€‚æ›´æ˜¾è‘—çš„æ˜¯ï¼Œä»…ç”¨ä¸åˆ°åƒåˆ†ä¹‹ä¸€çš„åˆæˆæ ·æœ¬æ—¶ï¼ŒOR-R1åœ¨ORLMä¸Šå°±å…·æœ‰è¶…è¿‡å››åˆ†ä¹‹ä¸‰ç‚¹çš„å‡†ç¡®ç‡æå‡ä¼˜åŠ¿ã€‚å¦å¤–å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå°½ç®¡å­˜åœ¨æ ‡è®°å°è¯•å’Œéè¿ç»­å¤šæ¬¡å°è¯•çš„ä¸ç¡®å®šæ€§å› ç´ é—®é¢˜ä¸‹TGRPOä¹Ÿä»ç„¶å®ç°äº†å¯¹å‡†ç¡®ç‡çš„å¯è§‚è´¡çŒ®å€¼è¾¾$+ 3.1%- 6.4%$å·¦å³ä½¿å¾—å•ä¸€å°è¯•æˆåŠŸä¸è¿ç»­å…«æ¬¡å°è¯•ä¸­å¤šæ¬¡å°è¯•çš„æˆåŠŸè¡¨ç°ä¹‹é—´çš„ç™¾åˆ†æ¯”å·®è·å¤§å¹…å‡å°‘ï¼ˆç”±åä¸‰ç¼©å°åˆ°ä¸ƒä¸ªç™¾åˆ†ç‚¹ï¼‰ã€‚å…¨é¢è¯„ä¼°å„ç§ç°å®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒOR-R1ä¸ºè‡ªåŠ¨åŒ–çš„è¿ç­¹ä¼˜åŒ–é—®é¢˜å»ºæ¨¡å’Œæ±‚è§£æä¾›äº†ç¨³å¥ã€å¯æ‰©å±•ä¸”ç»æµå®æƒ çš„è§£å†³æ–¹æ¡ˆï¼Œé™ä½äº†å·¥ä¸šè¿ç­¹åº”ç”¨ä¸­çš„ä¸“ä¸šçŸ¥è¯†å’Œæ•°æ®éšœç¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09092v1">PDF</a> 9 pages, 5 figures, AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOR-R1çš„æ•°æ®é«˜æ•ˆè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºæ¨¡å’Œæ±‚è§£ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æŠ€æœ¯ï¼Œå¸®åŠ©æ¨¡å‹ä»æœ‰é™æ ‡è®°æ•°æ®ä¸­è·å–é—®é¢˜è¡¨è¿°å’Œä»£ç ç”Ÿæˆçš„åŸºæœ¬æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨æµ‹è¯•æ—¶é—´ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTGRPOï¼‰æ¥æé«˜èƒ½åŠ›å’Œä¸€è‡´æ€§ã€‚è¿™ç§ä¸¤é˜¶æ®µè®¾è®¡ä½¿OR-R1èƒ½å¤Ÿåˆ©ç”¨ç¨€ç¼ºçš„æ ‡è®°æ•°æ®å’Œå¤§é‡çš„æ— æ ‡è®°æ•°æ®è¿›è¡Œæœ‰æ•ˆå­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒOR-R1åœ¨ä»…ä½¿ç”¨ORLMæ‰€éœ€åˆæˆæ•°æ®çš„ååˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡æ±‚è§£å‡†ç¡®åº¦ä¸º67.7%ï¼Œå¹¶è¶…è¶Šäº†ORLMæœ€å¤šé«˜è¾¾4.2%çš„æ±‚è§£å‡†ç¡®åº¦ã€‚å°¤å…¶ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼ŒOR-R1ä»…éœ€100ä¸ªåˆæˆæ ·æœ¬å°±èƒ½è¶…è¿‡ORLMè‡³å°‘è¶…è¿‡2.4%ã€‚è€Œä¸”ï¼ŒTGRPOè¿›ä¸€æ­¥æé«˜äº†å‡†ç¡®æ€§ï¼Œç¼©å°äº†å•æ¬¡å°è¯•å’Œå¤šæ¬¡å°è¯•ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚å¹¿æ³›çš„çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒOR-R1ä¸ºè‡ªåŠ¨åŒ–è¿ç­¹ä¼˜åŒ–é—®é¢˜çš„å»ºæ¨¡å’Œæ±‚è§£æä¾›äº†ç¨³å¥ã€å¯æ‰©å±•å’Œç»æµçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OR-R1æ˜¯ä¸€ç§æ•°æ®é«˜æ•ˆè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºæ¨¡å’Œæ±‚è§£ã€‚</li>
<li>é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æŠ€æœ¯å¸®åŠ©æ¨¡å‹ä»æœ‰é™æ ‡è®°æ•°æ®ä¸­è·å–åŸºæœ¬æ¨ç†æ¨¡å¼ã€‚</li>
<li>æµ‹è¯•æ—¶é—´ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTGRPOï¼‰æé«˜äº†èƒ½åŠ›å’Œä¸€è‡´æ€§ã€‚</li>
<li>OR-R1é€šè¿‡ä»…ä½¿ç”¨ååˆ†ä¹‹ä¸€çš„æ•°æ®å°±è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒOR-R1åœ¨åˆæˆæ•°æ®éœ€æ±‚æ›´å°‘çš„æƒ…å†µä¸‹å®ç°äº†æ›´é«˜çš„æ±‚è§£å‡†ç¡®åº¦ã€‚</li>
<li>TGRPOå¯¹æ€§èƒ½æœ‰é¢å¤–è´¡çŒ®ï¼Œç¼©å°äº†å•æ¬¡å°è¯•å’Œå¤šæ¬¡å°è¯•ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b18c01cc0de193f1841ad8bfeff9afa9" align="middle">
<img src="https://picx.zhimg.com/v2-9ebb485a54a1ad226197298edf5786a6" align="middle">
<img src="https://picx.zhimg.com/v2-44752117e38b07d28a7744b4bd3d7b88" align="middle">
<img src="https://picx.zhimg.com/v2-be05d97493a4847c7b0c4f58d23b06d7" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Advancing-Autonomous-Emergency-Response-Systems-A-Generative-AI-Perspective"><a href="#Advancing-Autonomous-Emergency-Response-Systems-A-Generative-AI-Perspective" class="headerlink" title="Advancing Autonomous Emergency Response Systems: A Generative AI Perspective"></a>Advancing Autonomous Emergency Response Systems: A Generative AI Perspective</h2><p><strong>Authors:Yousef Emami, Radha Reddy, Azadeh Pourkabirian, Miguel Gutierrez Gaitan</strong></p>
<p>Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.</p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰é€šè¿‡å®ç°æ›´å¿«ã€æ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„å“åº”ï¼Œæ­£æœç€å½»åº•æ”¹å˜åº”æ€¥æœåŠ¡é¢†åŸŸè¿ˆå‡ºåšå®æ­¥ä¼ã€‚è¿™ä¸€å˜é©æ˜¯ç”±äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è¿›æ­¥æ¨åŠ¨çš„ï¼Œç‰¹åˆ«æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¿›æ­¥ï¼Œå¼ºåŒ–å­¦ä¹ ä½¿å¾—è‡ªåŠ¨é©¾é©¶è½¦è¾†èƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å¯¼èˆªå¹¶åœ¨å®æ—¶åšå‡ºå…³é”®å†³ç­–ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ èŒƒå¼é€šå¸¸é¢ä¸´æ ·æœ¬æ•ˆç‡ä½å’ŒåŠ¨æ€åº”æ€¥åœºæ™¯ä¸‹é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚æœ¬æ–‡ç»¼è¿°äº†ä¸‹ä¸€ä»£è‡ªåŠ¨é©¾é©¶ä¼˜åŒ–ç­–ç•¥æ¥è§£å†³è¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬åˆ†æäº†ä»ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ å‘æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å¢å¼ºå‹å¼ºåŒ–å­¦ä¹ çš„è½¬å˜ï¼Œå°½ç®¡è®¡ç®—æˆæœ¬å¢åŠ ï¼Œä½†æ‰©æ•£æ¨¡å‹å¢å¼ºå‹å¼ºåŒ–å­¦ä¹ é€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆæé«˜äº†ç­–ç•¥ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…åŠ©ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„æ–°å…´èŒƒå¼ï¼Œå®ƒé€šè¿‡æ— éœ€é‡æ–°è®­ç»ƒå³å¯å®ç°å¿«é€Ÿå³æ—¶é€‚åº”ï¼Œæä¾›äº†ä¸€ç§è½»ä¾¿ä¸”å¯è§£é‡Šæ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡é€šè¿‡å›é¡¾è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ã€æ‰©æ•£æ¨¡å‹å¢å¼ºå‹å¼ºåŒ–å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©ä¸Šä¸‹æ–‡å­¦ä¹ çš„æœ€æ–°è¿›å±•ï¼Œæä¾›äº†ä¸€ä¸ªä»ç”Ÿæˆäººå·¥æ™ºèƒ½çš„è§’åº¦äº†è§£ä¸‹ä¸€ä»£è‡ªä¸»åº”æ€¥å“åº”ç³»ç»Ÿçš„å…³é”®æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09044v1">PDF</a> 8 pages, 3 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰é€šè¿‡è¿ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°æ›´å¿«ã€æ›´å®‰å…¨ã€æ›´é«˜æ•ˆçš„åº”æ€¥æœåŠ¡å“åº”ï¼Œä»è€Œå¼•é¢†ä¸€åœºé©å‘½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„RLèŒƒå¼åœ¨åº”å¯¹åŠ¨æ€ç´§æ€¥åœºæ™¯æ—¶å­˜åœ¨æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œé€‚åº”æ€§ä¸è¶³çš„å±€é™æ€§ã€‚æœ¬æ–‡è¯„è¿°äº†ä¸‹ä¸€ä»£AVä¼˜åŒ–ç­–ç•¥ï¼Œä»ä¼ ç»Ÿçš„RLè½¬å‘æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å¢å¼ºRLï¼Œé€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆæé«˜ç­–ç•¥ç¨³å¥æ€§ï¼ŒåŒæ—¶æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…åŠ©çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ–°å…´èŒƒå¼ã€‚æœ¬æ–‡ä»ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è§’åº¦ä¸ºç†è§£ä¸‹ä¸€ä»£è‡ªä¸»åº”æ€¥å“åº”ç³»ç»Ÿæä¾›äº†å…³é”®æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰èƒ½æ›´å¿«ã€æ›´å®‰å…¨ã€æ›´é«˜æ•ˆåœ°è¿›è¡Œåº”æ€¥æœåŠ¡å“åº”ã€‚</li>
<li>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç‰¹åˆ«æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯æ˜¯è‡ªåŠ¨é©¾é©¶è½¦è¾†å®ç°é«˜æ•ˆå“åº”çš„å…³é”®ã€‚</li>
<li>ä¼ ç»ŸRLèŒƒå¼åœ¨åº”å¯¹åŠ¨æ€ç´§æ€¥åœºæ™¯æ—¶å­˜åœ¨æ ·æœ¬æ•ˆç‡ä½ä¸‹å’Œé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å¢å¼ºRLæ˜¯æå‡ç­–ç•¥ç¨³å¥æ€§çš„æ–°æ–¹æ³•ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¾…åŠ©çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æä¾›äº†ä¸€ç§è½»ä¾¿ã€å¯è§£é‡Šæ€§å¼ºçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¯å¿«é€Ÿé€‚åº”ä¸éœ€å†è®­ç»ƒã€‚</li>
<li>æœ¬æ–‡ä»ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è§’åº¦æä¾›äº†ç†è§£ä¸‹ä¸€ä»£è‡ªä¸»åº”æ€¥å“åº”ç³»ç»Ÿçš„å…³é”®æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b33ce003c4bb4d1a113aec5083deaf99" align="middle">
<img src="https://picx.zhimg.com/v2-ff3101a5c00c44e686a7bcb090870e1b" align="middle">
<img src="https://picx.zhimg.com/v2-3cc8c546d18536f8d27ad9b1bb1c9790" align="middle">
<img src="https://picx.zhimg.com/v2-99f555b4defaf8803cb90bdc5d34ea26" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SpiralThinker-Latent-Reasoning-through-an-Iterative-Process-with-Text-Latent-Interleaving"><a href="#SpiralThinker-Latent-Reasoning-through-an-Iterative-Process-with-Text-Latent-Interleaving" class="headerlink" title="SpiralThinker: Latent Reasoning through an Iterative Process with Text-Latent Interleaving"></a>SpiralThinker: Latent Reasoning through an Iterative Process with Text-Latent Interleaving</h2><p><strong>Authors:Shengmin Piao, Sanghyun Park</strong></p>
<p>Recent advances in large reasoning models have been driven by reinforcement learning and test-time scaling, accompanied by growing interest in latent rather than purely textual reasoning. However, existing latent reasoning methods lack mechanisms to ensure stable evolution of latent representations and a systematic way to interleave implicit and explicit reasoning. We introduce SpiralThinker, a unified framework that performs iterative updates over latent representations, enabling extended implicit reasoning without generating additional tokens. A progressive alignment objective combined with structured annotations maintains coherence between latent and textual reasoning. Across mathematical, logical, and commonsense reasoning tasks, SpiralThinker achieves the best overall performance among latent reasoning approaches, consistently surpassing previous methods across all benchmarks. Detailed analyses reveal that both iteration and alignment are indispensable, the numbers of latent tokens and iterations exhibit dataset-specific optima, and appropriate alignment proves critical for an effective iterative process. Overall, SpiralThinker bridges iterative computation and latent reasoning, demonstrating that aligned iterative updates can reliably steer reasoning in the latent space.</p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹çš„è¿›æ­¥å¾—ç›Šäºå¼ºåŒ–å­¦ä¹ å’Œæµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯çš„æ¨åŠ¨ï¼ŒåŒæ—¶å¯¹æ½œåœ¨æ¨ç†è€Œéçº¯æ–‡æœ¬æ¨ç†çš„å…´è¶£ä¹Ÿåœ¨å¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ½œåœ¨æ¨ç†æ–¹æ³•ç¼ºä¹ä¿è¯æ½œåœ¨è¡¨ç¤ºç¨³å®šè¿›åŒ–çš„æœºåˆ¶ï¼Œä»¥åŠäº¤æ›¿è¿›è¡Œéšå¼å’Œæ˜¾å¼æ¨ç†çš„ç³»ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†SpiralThinkerï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå¯¹æ½œåœ¨è¡¨ç¤ºè¿›è¡Œè¿­ä»£æ›´æ–°ï¼Œèƒ½å¤Ÿåœ¨ä¸ç”Ÿæˆé¢å¤–ä»¤ç‰Œçš„æƒ…å†µä¸‹è¿›è¡Œæ‰©å±•çš„éšå¼æ¨ç†ã€‚ç»“åˆç»“æ„åŒ–æ³¨é‡Šçš„æ¸è¿›å¯¹é½ç›®æ ‡ï¼Œåœ¨æ½œåœ¨æ¨ç†å’Œæ–‡æœ¬æ¨ç†ä¹‹é—´ä¿æŒä¸€è‡´æ€§ã€‚åœ¨æ¶µç›–æ•°å­¦ã€é€»è¾‘å’Œå¸¸è¯†æ¨ç†çš„ä»»åŠ¡ä¸­ï¼ŒSpiralThinkeråœ¨æ½œåœ¨æ¨ç†æ–¹æ³•ä¸­å®ç°äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¿‡äº†ä»¥å‰çš„æ–¹æ³•ã€‚è¯¦ç»†åˆ†æè¡¨æ˜ï¼Œè¿­ä»£å’Œå¯¹é½éƒ½æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œæ½œåœ¨ä»¤ç‰Œçš„æ•°é‡å’Œè¿­ä»£æ¬¡æ•°è¡¨ç°å‡ºæ•°æ®é›†ç‰¹å®šçš„æœ€ä¼˜çŠ¶æ€ï¼Œé€‚å½“çš„å¯¹é½å¯¹äºæœ‰æ•ˆçš„è¿­ä»£è¿‡ç¨‹è‡³å…³é‡è¦ã€‚æ€»ä½“ä¸Šï¼ŒSpiralThinkeræ¡¥æ¢è¿­ä»£è®¡ç®—å’Œæ½œåœ¨æ¨ç†ï¼Œè¯æ˜å¯¹é½è¿­ä»£æ›´æ–°å¯ä»¥å¯é åœ°å¼•å¯¼æ½œåœ¨ç©ºé—´ä¸­çš„æ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹æ¨ç†æ¨¡å‹çš„è¿›å±•å¾—ç›Šäºå¼ºåŒ–å­¦ä¹ ä¸æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯ï¼ŒåŒæ—¶äººä»¬å¯¹æ½œåœ¨æ¨ç†çš„å…´è¶£æ—¥ç›Šå¢é•¿ï¼Œè€Œéçº¯ç²¹çš„æ–‡æœ¬æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰æ½œåœ¨æ¨ç†æ–¹æ³•ç¼ºä¹ä¿è¯æ½œåœ¨è¡¨ç¤ºç¨³å®šæ¼”åŒ–çš„æœºåˆ¶ï¼Œä»¥åŠæ··åˆéšå¼å’Œæ˜¾å¼æ¨ç†çš„ç³»ç»Ÿæ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpiralThinkeræ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ç”Ÿæˆé¢å¤–ä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼Œå¯¹æ½œåœ¨è¡¨ç¤ºè¿›è¡Œè¿­ä»£æ›´æ–°ï¼Œå®ç°æ‰©å±•çš„éšå¼æ¨ç†ã€‚ç»“åˆç»“æ„åŒ–æ³¨é‡Šï¼Œé€šè¿‡æ¸è¿›å¯¹é½ç›®æ ‡ç»´æŒæ½œåœ¨æ¨ç†ä¸æ–‡æœ¬æ¨ç†ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚åœ¨æ•°å­¦ã€é€»è¾‘å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡æ–¹é¢ï¼ŒSpiralThinkeråœ¨æ½œåœ¨æ¨ç†æ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šä»¥å‰çš„æ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼Œè¿­ä»£å’Œå¯¹é½éƒ½æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œæ½œåœ¨ä»¤ç‰Œçš„æ•°é‡å’Œè¿­ä»£æ¬¡æ•°å±•ç°å‡ºæ•°æ®é›†ç‰¹å®šçš„æœ€ä¼˜çŠ¶æ€ï¼Œé€‚å½“çš„å¯¹é½å¯¹äºæœ‰æ•ˆçš„è¿­ä»£è¿‡ç¨‹è‡³å…³é‡è¦ã€‚æ€»çš„æ¥è¯´ï¼ŒSpiralThinkeræ¡¥æ¢è¿­ä»£è®¡ç®—ä¸æ½œåœ¨æ¨ç†ï¼Œè¯æ˜å¯¹é½è¿­ä»£æ›´æ–°èƒ½å¤Ÿå¯é åœ°å¼•å¯¼æ½œåœ¨ç©ºé—´ä¸­çš„æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå¤§å‹æ¨ç†æ¨¡å‹çš„è¿›å±•å¾—ç›Šäºå¼ºåŒ–å­¦ä¹ ä¸æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰æ½œåœ¨æ¨ç†æ–¹æ³•ç¼ºä¹ä¿è¯æ½œåœ¨è¡¨ç¤ºç¨³å®šæ¼”åŒ–çš„æœºåˆ¶ã€‚</li>
<li>SpiralThinkeræ¡†æ¶èƒ½é€šè¿‡è¿­ä»£æ›´æ–°æ½œåœ¨è¡¨ç¤ºå®ç°æ‰©å±•çš„éšå¼æ¨ç†ã€‚</li>
<li>SpiralThinkerç»“åˆäº†ç»“æ„åŒ–æ³¨é‡Šï¼Œä¿æŒæ½œåœ¨æ¨ç†ä¸æ–‡æœ¬æ¨ç†ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚</li>
<li>SpiralThinkeråœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¶…è¶Šä¹‹å‰çš„æ–¹æ³•ã€‚</li>
<li>åˆ†æå’Œå®éªŒè¡¨æ˜è¿­ä»£å’Œå¯¹é½åœ¨SpiralThinkerä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c9b089c3ee94541c375dc58333e7e80" align="middle">
<img src="https://picx.zhimg.com/v2-96f00709ee3b90f3028058c6973f7a0b" align="middle">
<img src="https://picx.zhimg.com/v2-e57638b5b4c6d67678f20f620db2a3cb" align="middle">
<img src="https://picx.zhimg.com/v2-69cd24a7545960bb8f9e4db2d75cb81d" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds"><a href="#Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds" class="headerlink" title="Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds"></a>Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds</h2><p><strong>Authors:Weihao Tan, Xiangyang Li, Yunhao Fang, Heyuan Yao, Shi Yan, Hao Luo, Tenglong Ao, Huihui Li, Hongbin Ren, Bairen Yi, Yujia Qin, Bo An, Libin Liu, Guang Shi</strong></p>
<p>We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumineâ€™s effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.</p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºLumineï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æ”¾é…æ–¹ï¼Œç”¨äºå¼€å‘èƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„3Då¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®æ—¶å®Œæˆæ•°å°æ—¶å¤æ‚ä»»åŠ¡çš„é€šç”¨æ™ºèƒ½ä½“ã€‚Lumineé‡‡ç”¨ç±»ä¼¼äººç±»çš„äº¤äº’èŒƒå¼ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ç»Ÿä¸€æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ï¼Œç”±è§†è§‰è¯­è¨€æ¨¡å‹é©±åŠ¨ã€‚å®ƒå¤„ç†åŸå§‹åƒç´ çš„é¢‘ç‡ä¸º5Hzï¼Œä»¥äº§ç”Ÿç²¾ç¡®çš„é¢‘ç‡ä¸º30Hzçš„é”®ç›˜é¼ æ ‡åŠ¨ä½œï¼Œå¹¶åœ¨å¿…è¦æ—¶è‡ªé€‚åº”åœ°è§¦å‘æ¨ç†ã€‚åœ¨ã€ŠåŸç¥ã€‹çš„è®­ç»ƒä¸‹ï¼ŒLumineæˆåŠŸå®Œæˆäº†é•¿è¾¾äº”ä¸ªå°æ—¶çš„è’™å¾·ä¸»çº¿ä»»åŠ¡ï¼Œæ•ˆç‡å ªæ¯”äººç±»ï¼Œå¹¶èƒ½æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ‰§è¡Œå¹¿æ³›çš„3Då¼€æ”¾ä¸–ç•Œæ¢ç´¢å’Œ2Då›¾å½¢ç”¨æˆ·ç•Œé¢æ“ä½œä»»åŠ¡ï¼ŒåŒ…æ‹¬æ”¶é›†ã€æˆ˜æ–—ã€è§£è°œå’ŒNPCäº¤äº’ç­‰ã€‚é™¤äº†åŸŸå†…è¡¨ç°å¤–ï¼ŒLumineè¿˜å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¸¸æˆé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ— éœ€å¾®è°ƒï¼Œå®ƒå³å¯åœ¨ã€Šæµªæ½®ç‹‚æ¶Œã€‹ä¸­å®Œæˆé•¿è¾¾100åˆ†é’Ÿçš„ä½¿å‘½ï¼Œä»¥åŠåœ¨ã€Šå´©åï¼šæ˜Ÿç©¹é“é“ã€‹ä¸­å®Œæˆå®Œæ•´çš„ç¬¬ä¸€ç« èŠ‚äº”å°æ—¶ä»»åŠ¡ã€‚è¿™äº›ä»¤äººé¼“èˆçš„ç»“æœå‡¸æ˜¾äº†Lumineåœ¨ä¸åŒä¸–ç•Œå’Œäº¤äº’åŠ¨æ€ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæœç€å¼€å‘å¼€æ”¾ç¯å¢ƒä¸­çš„é€šç”¨æ™ºèƒ½ä½“è¿ˆå‡ºäº†åšå®çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08892v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å…¨æ–°çš„é€šç”¨æ™ºèƒ½ä½“ä»£ç†Lumineé—®ä¸–ï¼Œèƒ½å¤Ÿåœ¨å……æ»¡æŒ‘æˆ˜çš„ä¸‰ç»´å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®Œæˆå®æ—¶é•¿æ—¶ä»»åŠ¡ã€‚Lumineé€šè¿‡åƒç´ çº§åˆ«çš„å¤„ç†åˆ°é”®ç›˜é¼ æ ‡æ“ä½œçš„æ§åˆ¶ä»¥åŠç»“åˆæ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨çš„äº¤äº’æ¨¡å¼æ¥å®ç°ç±»äººæ“ä½œã€‚åœ¨ã€ŠåŸç¥ã€‹çš„è®­ç»ƒä¸‹ï¼ŒLumineæˆåŠŸå®Œæˆäº†é•¿è¾¾äº”å°æ—¶çš„è’™å¾·ä¸»çº¿ä»»åŠ¡ï¼Œæ•ˆç‡å ªæ¯”äººç±»ï¼Œå¹¶èƒ½åœ¨3Då¼€æ”¾ä¸–ç•Œæ¢ç´¢å’Œ2Dç•Œé¢æ“ä½œä¸­æ‰§è¡Œä¸€ç³»åˆ—ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒLumineè¿˜å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¸¸æˆé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœªè¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹åœ¨å…¶ä»–æ¸¸æˆä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lumineæ˜¯é¦–ä¸ªå¼€å‘é€šç”¨æ™ºèƒ½ä½“ä»£ç†çš„å…¬å¼€é…æ–¹ï¼Œèƒ½åœ¨æŒ‘æˆ˜æ€§çš„ä¸‰ç»´å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®Œæˆé•¿æ—¶é—´ä»»åŠ¡ã€‚</li>
<li>Lumineé‡‡ç”¨ç±»äººäº¤äº’æ¨¡å¼ï¼Œç»Ÿä¸€æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ã€‚</li>
<li>Lumineé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹é©±åŠ¨ï¼Œèƒ½å¤„ç†åŸå§‹åƒç´ æ•°æ®å¹¶äº§ç”Ÿç²¾ç¡®æ“ä½œã€‚</li>
<li>åœ¨ã€ŠåŸç¥ã€‹çš„è®­ç»ƒä¸‹ï¼ŒLumineèƒ½é«˜æ•ˆå®Œæˆé•¿è¾¾äº”å°æ—¶çš„ä¸»çº¿ä»»åŠ¡ã€‚</li>
<li>Lumineå…·å¤‡åœ¨ä¸‰ç»´å¼€æ”¾ä¸–ç•Œæ¢ç´¢å’ŒäºŒç»´ç•Œé¢æ“ä½œä¸­çš„å¹¿æ³›ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚</li>
<li>Lumineå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœªè¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹åœ¨å…¶ä»–æ¸¸æˆä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec5f9dbf38453c08b87eec6cccaaa0af" align="middle">
<img src="https://picx.zhimg.com/v2-751e50418ec4b23a01676ff80c8732c2" align="middle">
<img src="https://picx.zhimg.com/v2-5113eb787174867fa85c9a9bd2effe30" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-17/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a3763cb20c89e115fad9ba7bdd947f74" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8e3adb44bf8ed190c0ffe932bf3bbb6d" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Reinforcing Trustworthiness in Multimodal Emotional Support Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
