<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-09-20  Lightweight and Accurate Multi-View Stereo with Confidence-Aware   Diffusion Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-67098d89d2cede73749b941db21f4102~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030892&auth_key=1760030892-0-0-b6f487b65550d82604d5b331e0faa442&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    50 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-20-更新"><a href="#2025-09-20-更新" class="headerlink" title="2025-09-20 更新"></a>2025-09-20 更新</h1><h2 id="Lightweight-and-Accurate-Multi-View-Stereo-with-Confidence-Aware-Diffusion-Model"><a href="#Lightweight-and-Accurate-Multi-View-Stereo-with-Confidence-Aware-Diffusion-Model" class="headerlink" title="Lightweight and Accurate Multi-View Stereo with Confidence-Aware   Diffusion Model"></a>Lightweight and Accurate Multi-View Stereo with Confidence-Aware   Diffusion Model</h2><p><strong>Authors:Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys</strong></p>
<p>To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks &amp; Temples and ETH3D. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/cvg/diffmvs">https://github.com/cvg/diffmvs</a>. </p>
<blockquote>
<p>从校准图像重建3D几何结构时，基于学习的多视图立体（MVS）方法通常执行多视图深度估计，然后将深度图融合为网格或点云。为了提高计算效率，许多方法初始化一个粗略的深度图，然后逐渐在更高分辨率下对其进行优化。最近，扩散模型在生成任务中取得了巨大成功。从随机噪声开始，扩散模型通过迭代去噪过程逐渐恢复样本。本文提出了一种新的MVS框架，该框架引入了MVS中的扩散模型。具体来说，我们将深度优化制定为条件扩散过程。考虑到深度估计的判别特性，我们设计了一个条件编码器来指导扩散过程。为了提高效率，我们提出了一种结合轻量化2D U-Net和卷积GRU的新型扩散网络。此外，我们提出了一种基于信心的采样策略，根据扩散模型估计的信心自适应地采样深度假设。基于我们新的MVS框架，我们提出了两种新的MVS方法，即DiffMVS和CasDiffMVS。DiffMVS在运行时间和GPU内存方面达到了先进的效率；CasDiffMVS在DTU、Tanks &amp; Temples和ETH3D上达到了最新性能。代码可访问于：<a target="_blank" rel="noopener" href="https://github.com/cvg/diffmvs%E3%80%82">https://github.com/cvg/diffmvs。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15220v1">PDF</a> Accepted to IEEE T-PAMI 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/cvg/diffmvs">https://github.com/cvg/diffmvs</a></p>
<p><strong>Summary</strong></p>
<p>本文提出将扩散模型引入多视角立体（MVS）框架中，通过条件扩散过程进行深度细化。设计条件编码器以提高深度估计的判别能力，并提出结合轻量化2D U-Net和卷积GRU的新型扩散网络。此外，还提出了一种基于信心的采样策略，根据扩散模型估计的信心自适应采样深度假设。基于该框架，提出了两种新型MVS方法：DiffMVS和CasDiffMVS，前者在运行时和GPU内存使用方面表现出高效竞争力，后者在DTU、Tanks &amp; Temples和ETH3D上达到最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型被引入多视角立体（MVS）框架中，用于提高从校准图像重建3D几何体的计算效率和性能。</li>
<li>深度细化被表述为条件扩散过程，以提高深度估计的准确性。</li>
<li>设计了条件编码器以引导扩散过程，从而提高深度估计的判别能力。</li>
<li>提出了一种结合轻量化2D U-Net和卷积GRU的新型扩散网络结构。</li>
<li>引入基于信心的采样策略，根据扩散模型的信心估计自适应采样深度假设。</li>
<li>基于新框架，提出了两种MVS方法：DiffMVS和CasDiffMVS。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-eb873c72a43f8cc2ffcf6da8cbd8b3a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030899&auth_key=1760030899-0-0-b869619f7186dc2eb1bc746834fe266e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1879fab1477eb044d091291934dcf591~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030907&auth_key=1760030907-0-0-951ae09ab1c7af117874f808fbeb3cf1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f0f23cbbc1403b6fc42e6aa350e074e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030914&auth_key=1760030914-0-0-75b0da3ddaae7f1bf5c212e7509bb4fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a79ddd4c1a882a039f5ca0f42d5d45a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030921&auth_key=1760030921-0-0-867951c7f2fa1af2dcccefc161ac62ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing"><a href="#AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing" class="headerlink" title="AutoEdit: Automatic Hyperparameter Tuning for Image Editing"></a>AutoEdit: Automatic Hyperparameter Tuning for Image Editing</h2><p><strong>Authors:Chau Pham, Quan Dao, Mahesh Bhosale, Yunjie Tian, Dimitris Metaxas, David Doermann</strong></p>
<p>Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing’s hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world. </p>
<blockquote>
<p>扩散模型的最新进展彻底改变了文本引导的图像编辑，但现有的编辑方法在超参数识别方面面临重大挑战。为了获得合理的编辑性能，这些方法通常要求用户强行调整多个相互依赖的超参数，如反转时间步长和注意力修改等。这一过程由于超参数搜索空间的巨大而产生了高昂的计算成本。我们将寻找最佳编辑超参数视为扩散去噪过程中的一个序贯决策任务。具体来说，我们提出了一种强化学习框架，它建立了一个马尔可夫决策过程，该过程在去噪步骤中动态调整超参数，并将编辑目标整合到奖励函数中。该方法通过近端策略优化实现了时间效率，同时保持了最佳超参数配置。实验表明，与现有的强行方法相比，搜索时间和计算开销大大减少，为基于扩散的图像编辑框架在实际世界中的实际应用提供了进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15031v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于强化学习的扩散模型优化方法，旨在解决文本引导的图像编辑中面临的超参数识别挑战。该方法将超参数搜索视为扩散去噪过程中的序贯决策任务，并提出一个强化学习框架，通过动态调整去噪步骤中的超参数，将编辑目标融入奖励函数，实现时间效率与最优超参数配置的平衡。实验证明，该方法相较于现有暴力搜索方法，显著减少了搜索时间和计算开销，推动了扩散模型图像编辑框架在实际应用中的部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本引导的图像编辑中的最新进展及其面临的挑战。</li>
<li>提出将超参数搜索视为扩散去噪过程中的序贯决策任务。</li>
<li>引入强化学习框架，动态调整去噪步骤中的超参数。</li>
<li>将编辑目标融入奖励函数，实现时间效率与最优超参数配置的平衡。</li>
<li>方法的实验验证，证明其相较于现有方法的优势。</li>
<li>该方法显著减少了搜索时间和计算开销。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15031">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-464f1ed661ac3bbfc5c7b2b86fa86e61~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030928&auth_key=1760030928-0-0-2ab1267f43c4997c0b532907590f5ce5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-514a7f0a65b5d46d97c2eb06a635f5bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030936&auth_key=1760030936-0-0-f9b46a245c5b2994e2eff1bbe0b03264&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-50f5df456c84a723ebc0a9a0cf958d9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030942&auth_key=1760030942-0-0-94aeca0bb791f8206b277cd31dcb3db5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Controllable-Localized-Face-Anonymization-Via-Diffusion-Inpainting"><a href="#Controllable-Localized-Face-Anonymization-Via-Diffusion-Inpainting" class="headerlink" title="Controllable Localized Face Anonymization Via Diffusion Inpainting"></a>Controllable Localized Face Anonymization Via Diffusion Inpainting</h2><p><strong>Authors:Ali Salar, Qing Liu, Guoying Zhao</strong></p>
<p>The growing use of portrait images in computer vision highlights the need to protect personal identities. At the same time, anonymized images must remain useful for downstream computer vision tasks. In this work, we propose a unified framework that leverages the inpainting ability of latent diffusion models to generate realistic anonymized images. Unlike prior approaches, we have complete control over the anonymization process by designing an adaptive attribute-guidance module that applies gradient correction during the reverse denoising process, aligning the facial attributes of the generated image with those of the synthesized target image. Our framework also supports localized anonymization, allowing users to specify which facial regions are left unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ datasets show that our method outperforms state-of-the-art approaches while requiring no additional model training. The source code is available on our page. </p>
<blockquote>
<p>计算机视觉中肖像图像的使用日益增多，这突出了保护个人身份的需要。同时，匿名图像必须仍然对下游计算机视觉任务有用。在这项工作中，我们提出了一个统一的框架，该框架利用潜在扩散模型的补全能力来生成逼真的匿名图像。与以前的方法不同，我们通过设计一个自适应属性引导模块来控制匿名化过程，该模块在反向去噪过程中应用梯度校正，使生成图像的面部属性与合成目标图像的面部属性对齐。我们的框架还支持局部匿名化，允许用户指定哪些面部区域保持不变。在公共CelebA-HQ和FFHQ数据集上进行的广泛实验表明，我们的方法在不需要额外模型训练的情况下优于最先进的方法。源代码可在我们的页面上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14866v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一个利用潜在扩散模型的补全能力生成真实匿名肖像图像的统一框架。该框架通过设计一个自适应属性引导模块，在反向去噪过程中进行梯度校正，使生成图像的面部属性与合成目标图像对齐，从而完全控制匿名化过程。此外，该框架支持局部匿名化，允许用户指定面部区域保持不变。在公共CelebA-HQ和FFHQ数据集上的广泛实验表明，该方法优于现有先进技术，且无需额外模型训练。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用潜在扩散模型的补全能力生成真实匿名肖像图像的统一框架被提出。</li>
<li>通过自适应属性引导模块实现梯度校正，使生成图像的面部属性与目标图像对齐。</li>
<li>支持局部匿名化，允许用户指定面部区域保持不变。</li>
<li>该方法在公共CelebA-HQ和FFHQ数据集上表现优异，优于现有技术。</li>
<li>该方法无需额外模型训练。</li>
<li>源码已公开。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14866">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-675611ceff82a5ddd4baba812c803b7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030950&auth_key=1760030950-0-0-82a4a21bab0f05843c5628ffdf1b852f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bc6af4fe59324b75341c2f977d12627c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030957&auth_key=1760030957-0-0-d5a4099481482085fca054cb096224e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-736b280a0dd145bed0cc192345715028~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030963&auth_key=1760030963-0-0-29855cbc15c6733ef113da156b354f26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d39de97a93c86ecf21e40e771743cdd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030970&auth_key=1760030970-0-0-d6d06e10b6f6e8ee0220b2793ca5adfe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e7e0a4fd743c70c82f0a62f81f00bdd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030977&auth_key=1760030977-0-0-88ead5f1122c382f904f4f3e25faffae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Radiology-Report-Conditional-3D-CT-Generation-with-Multi-Encoder-Latent-diffusion-Model"><a href="#Radiology-Report-Conditional-3D-CT-Generation-with-Multi-Encoder-Latent-diffusion-Model" class="headerlink" title="Radiology Report Conditional 3D CT Generation with Multi Encoder Latent   diffusion Model"></a>Radiology Report Conditional 3D CT Generation with Multi Encoder Latent   diffusion Model</h2><p><strong>Authors:Sina Amirrajab, Zohaib Salahuddin, Sheng Kuang, Henry C. Woodruff, Philippe Lambin</strong></p>
<p>Text to image latent diffusion models have recently advanced medical image synthesis, but applications to 3D CT generation remain limited. Existing approaches rely on simplified prompts, neglecting the rich semantic detail in full radiology reports, which reduces text image alignment and clinical fidelity. We propose Report2CT, a radiology report conditional latent diffusion framework for synthesizing 3D chest CT volumes directly from free text radiology reports, incorporating both findings and impression sections using multiple text encoder. Report2CT integrates three pretrained medical text encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced clinical context. Radiology reports and voxel spacing information condition a 3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset. Model performance was evaluated using Frechet Inception Distance (FID) for real synthetic distributional similarity and CLIP based metrics for semantic alignment, with additional qualitative and quantitative comparisons against GenerateCT model. Report2CT generated anatomically consistent CT volumes with excellent visual quality and text image alignment. Multi encoder conditioning improved CLIP scores, indicating stronger preservation of fine grained clinical details in the free text radiology reports. Classifier free guidance further enhanced alignment with only a minor trade off in FID. We ranked first in the VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved state of the art performance across all evaluation metrics. By leveraging complete radiology reports and multi encoder text conditioning, Report2CT advances 3D CT synthesis, producing clinically faithful and high quality synthetic data. </p>
<blockquote>
<p>文本到图像潜在扩散模型在医学图像合成方面取得了最新进展，但在3D CT生成方面的应用仍然有限。现有方法依赖于简化的提示，忽略了完整放射学报告中的丰富语义细节，这降低了文本图像对齐和临床保真度。我们提出了Report2CT，这是一种基于放射学报告条件的潜在扩散框架，可以直接从自由文本放射学报告中合成3D胸部CT体积，它结合了检查结果和印象部分，使用多个文本编码器。Report2CT集成了三个预训练的医疗文本编码器（BiomedVLP CXR BERT、MedEmbed和ClinicalBERT）来捕捉微妙的临床背景。放射学报告和体素间距信息对CT RATE数据集上经过训练的2万个CT体积的3D潜在扩散模型进行了条件处理。我们通过Frechet Inception Distance（FID）评估模型性能，用于真实合成分布相似性，并使用CLIP相关指标进行语义对齐，与GenerateCT模型进行了定性和定量比较。Report2CT生成的CT体积解剖结构一致，视觉质量优秀，文本图像对齐。多编码器条件改善了CLIP分数，表明精细粒度的临床细节在自由文本放射学报告中得到了更好的保留。无分类器指导进一步增强了对齐性，同时只牺牲了少量的FID。在MICCAI 2025年的VLM3D挑战中，我们在文本条件CT生成方面排名第一，并在所有评估指标上达到了最新技术水平。通过利用完整的放射学报告和多编码器文本条件处理，Report2CT推动了3D CT合成的发展，生成了临床真实且高质量的综合数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14780v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>报告提出了Report2CT，一个基于放射学报告条件的潜在扩散框架，可从自由文本放射学报告直接合成3D胸部CT体积。该框架结合了多种医学文本编码器，捕捉微妙的临床背景。使用来自CT RATE数据集的2万份CT体积训练了3D潜在扩散模型。模型性能通过Frechet Inception Distance (FID)进行真实与合成分布相似性评估，并通过CLIP指标进行语义对齐评估。Report2CT生成了结构一致的CT体积，具有良好的视觉质量和文本图像对齐性。多编码器条件改善了CLIP评分，表明精细粒度的临床细节在自由文本放射学报告中得到了更好的保留。无分类器指导进一步增强了对齐性，仅略微降低了FID。在MICCAI 2025年的VLM3D挑战中，Report2CT在文本条件CT生成方面排名第一，并在所有评估指标上取得了最新技术性能。通过利用完整的放射学报告和多编码器文本条件，Report2CT推进了3D CT的合成，产生了临床真实且高质量的综合数据。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>报告提出了一种新的方法Report2CT，可以从自由文本放射学报告直接合成3D胸部CT体积。</li>
<li>Report2CT结合了多种医学文本编码器以捕捉微妙的临床背景信息。</li>
<li>使用来自CT RATE数据集的CT体积训练了3D潜在扩散模型。</li>
<li>Report2CT生成的CT体积具有良好的视觉质量和文本图像对齐性。</li>
<li>多编码器条件改善了CLIP评分，保留了更精细的临床细节。</li>
<li>无分类器指导增强了文本与图像之间的对齐性，且只带来微小的FID损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14780">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8223982d0023638a0955b9026ee52d63~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030984&auth_key=1760030984-0-0-6b543f02531491e34e5804acf6d0b100&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71e1aa93fa0106dfdb55be1ecb1dfb32~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030991&auth_key=1760030991-0-0-8affee7143e4b5d992a8bbf8fe4f2dd4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-841e74619116d8671e9c18585437ad38~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030998&auth_key=1760030998-0-0-3cbf53469003f98d7408662b1f180b95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Dataset-Distillation-for-Super-Resolution-without-Class-Labels-and-Pre-trained-Models"><a href="#Dataset-Distillation-for-Super-Resolution-without-Class-Labels-and-Pre-trained-Models" class="headerlink" title="Dataset Distillation for Super-Resolution without Class Labels and   Pre-trained Models"></a>Dataset Distillation for Super-Resolution without Class Labels and   Pre-trained Models</h2><p><strong>Authors:Sunwoo Cho, Yejin Jung, Nam Ik Cho, Jae Woong Soh</strong></p>
<p>Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset. </p>
<blockquote>
<p>训练深度神经网络的要求越来越高，需要大量的数据集和重要的计算资源，尤其是随着模型复杂性的提高。旨在提高数据效率的数据蒸馏方法已成为应对这一挑战的有前途的解决方案。在单图像超分辨率（SISR）领域，对大量训练数据集的依赖突显了这些技术的重要性。最近，提出了一种基于生成对抗网络（GAN）反转的SR数据蒸馏框架，显示出更好的数据利用潜力。然而，当前的方法严重依赖于预训练的SR网络和类别特定信息，这限制了其通用性和适用性。为了解决这些问题，我们介绍了一种新的用于图像SR的数据蒸馏方法，该方法不需要类标签或预训练的SR模型。具体来说，我们首先提取高梯度补丁并根据CLIP特征对图像进行分类，然后对所选补丁微调扩散模型以学习其分布并合成蒸馏的训练图像。实验结果表明，我们的方法在实现最新性能的同时，使用更少的训练数据和更短的计算时间。具体来说，当我们仅使用原始数据集的0.68%来训练SR的基线Transformer模型时，性能下降仅为0.3分贝。在这种情况下，扩散模型的微调需要4小时，SR模型的训练在1小时内完成，远远短于使用完整数据集的11小时训练时间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14777v1">PDF</a> </p>
<p><strong>Summary</strong><br>     深度学习网络训练对大数据集和计算资源的需求日益增加，特别是在模型复杂性提升的背景下。数据蒸馏方法旨在提高数据效率，成为应对这一挑战的有前途的解决方案。在单图像超分辨率（SISR）领域，对大量训练数据集的依赖凸显了这些技术的重要性。最近提出了基于生成对抗网络（GAN）反演的SR数据蒸馏框架，显示出更好的数据利用潜力。然而，当前方法严重依赖于预训练的SR网络和类别特定信息，限制了其通用性和适用性。为解决这些问题，我们提出了一种新的图像SR数据蒸馏方法，无需类别标签或预训练的SR模型。我们首先将高梯度补丁提取并分类图像基于CLIP特征，然后在选定区域微调扩散模型以学习其分布并合成蒸馏的训练图像。实验表明，该方法达到了最新的性能水平，同时使用更少的训练数据和更短的计算时间。具体来说，当我们仅使用原始数据集中的0.68%训练SR基线Transformer模型时，性能下降仅为0.3分贝。在这种情况下，扩散模型的微调需要4小时，而SR模型的训练在1小时内完成，远短于使用整个数据集时的训练时间（需时11小时）。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度神经网络的训练需要大量数据集和计算资源。为了解决这一挑战，数据蒸馏方法应运而生以提高数据效率。</li>
<li>在单图像超分辨率领域，数据蒸馏技术尤为重要。当前方法主要依赖于预训练的SR网络和类别特定信息。</li>
<li>提出了一种新的图像SR数据蒸馏方法，无需类别标签或预训练的SR模型。通过提取高梯度补丁并基于CLIP特征分类图像进行训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14777">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-95a5aa1e1edfd5853f63ac3ee73f3db7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031005&auth_key=1760031005-0-0-df348b3957bdb541d3c886f29120478a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9bec12f1d5ca4770a5f4b61146f72257~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031012&auth_key=1760031012-0-0-76ccd152ba2d0a793ebd1be9ddaf592d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c6137d2050dabf879722f20e08ae7c07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031019&auth_key=1760031019-0-0-aa55b3ee64f5947ac478eee2c31959c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d22b532bd39a34d514ec8a1d54eb4dfb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031026&auth_key=1760031026-0-0-a9f1c567dbbca80939c3c37d14a66291&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bb500596bf9716899626a01f467220a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031032&auth_key=1760031032-0-0-c9c29291263053e07a563a1cfe41946c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DICE-Diffusion-Consensus-Equilibrium-for-Sparse-view-CT-Reconstruction"><a href="#DICE-Diffusion-Consensus-Equilibrium-for-Sparse-view-CT-Reconstruction" class="headerlink" title="DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction"></a>DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction</h2><p><strong>Authors:Leon Suarez-Rodriguez, Roman Jacome, Romario Gualdron-Hurtado, Ana Mantilla-Dulcey, Henry Arguello</strong></p>
<p>Sparse-view computed tomography (CT) reconstruction is fundamentally challenging due to undersampling, leading to an ill-posed inverse problem. Traditional iterative methods incorporate handcrafted or learned priors to regularize the solution but struggle to capture the complex structures present in medical images. In contrast, diffusion models (DMs) have recently emerged as powerful generative priors that can accurately model complex image distributions. In this work, we introduce Diffusion Consensus Equilibrium (DICE), a framework that integrates a two-agent consensus equilibrium into the sampling process of a DM. DICE alternates between: (i) a data-consistency agent, implemented through a proximal operator enforcing measurement consistency, and (ii) a prior agent, realized by a DM performing a clean image estimation at each sampling step. By balancing these two complementary agents iteratively, DICE effectively combines strong generative prior capabilities with measurement consistency. Experimental results show that DICE significantly outperforms state-of-the-art baselines in reconstructing high-quality CT images under uniform and non-uniform sparse-view settings of 15, 30, and 60 views (out of a total of 180), demonstrating both its effectiveness and robustness. </p>
<blockquote>
<p>稀疏视图计算机断层扫描（CT）重建面临根本挑战，因为欠采样导致反问题不适定。传统迭代方法结合手工或学习先验来正则化解决方案，但难以捕捉医疗图像中存在的复杂结构。相比之下，扩散模型（DMs）最近作为强大的生成先验出现，可以准确地模拟复杂的图像分布。在这项工作中，我们引入了扩散共识平衡（DICE），这是一个将两智能体共识平衡集成到扩散模型的采样过程中的框架。DICE在以下两个智能体之间交替进行：（i）数据一致性智能体，通过近端算子强制实施测量一致性来实现；（ii）先验智能体，通过每个采样步骤中执行清洁图像估计的扩散模型实现。通过迭代平衡这两个互补的智能体，DICE有效地结合了强大的生成先验能力和测量一致性。实验结果表明，无论是在统一和非统一的稀疏视图设置下，DICE在重建高质量CT图像方面都显著优于最新基线，即在总共180个视图中仅使用15、30和60个视图，证明了其有效性和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14566v1">PDF</a> 8 pages, 4 figures, confenrence</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Diffusion Consensus Equilibrium（DICE）的框架，该框架结合了扩散模型（DMs）的强大生成先验和测量一致性。DICE通过在采样过程中采用双智能体共识均衡机制，实现了数据一致性智能体和先验智能体的交替作用。这种方法在稀疏视图计算机断层扫描（CT）重建中表现优异，显著优于现有基线，能够在均匀和非均匀稀疏视图设置下重建高质量CT图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型（DMs）作为强大的生成先验，能够准确地模拟复杂的图像分布。</li>
<li>DICE框架结合了数据一致性智能体和先验智能体，通过迭代平衡两者来实现高效的CT图像重建。</li>
<li>DICE框架通过采用双智能体共识均衡机制，在采样过程中实现了强大的生成先验与测量一致性结合。</li>
<li>DICE在稀疏视图CT重建中表现出显著优势，能够在不同视图设置下重建高质量图像。</li>
<li>DICE框架在均匀和非均匀稀疏视图设置下均表现出有效性和稳健性。</li>
<li>传统迭代方法在手工艺或学习先验方面存在挑战，无法完全捕捉医疗图像的复杂结构，而DICE框架能够克服这些挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d3adb423d54538b23cc0682885ab8782~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031040&auth_key=1760031040-0-0-e14312b63a9ef6cc28789daa0690b46f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71bb183fbfc59dbcc0d5a4b840cdfc03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031047&auth_key=1760031047-0-0-803ea714543de6265781196f7ed68fac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-906f7a8898badb45208b45e8b63b80c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031054&auth_key=1760031054-0-0-3fb772fd8e52f7e0ac4600dc37d44f38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14d22ae476e583d123c728cfb565d507~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031061&auth_key=1760031061-0-0-468a7770eae9e7b10dbbc2300a103910&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f498f8c04d9a24aac027ed34a7482e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031068&auth_key=1760031068-0-0-c6cd70f3571cb1c5e987e389599f9a35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5eb7fc7256b42dcd3bfff81dba874c60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031075&auth_key=1760031075-0-0-e7441a823f50c7c836671864ef78410d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiffVL-Diffusion-Based-Visual-Localization-on-2D-Maps-via-BEV-Conditioned-GPS-Denoising"><a href="#DiffVL-Diffusion-Based-Visual-Localization-on-2D-Maps-via-BEV-Conditioned-GPS-Denoising" class="headerlink" title="DiffVL: Diffusion-Based Visual Localization on 2D Maps via   BEV-Conditioned GPS Denoising"></a>DiffVL: Diffusion-Based Visual Localization on 2D Maps via   BEV-Conditioned GPS Denoising</h2><p><strong>Authors:Li Gao, Hongyang Sun, Liu Liu, Yunhao Li, Yang Cai</strong></p>
<p>Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Bird’s-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods. </p>
<blockquote>
<p>精确视觉定位对于自动驾驶至关重要。然而，现有方法面临一个基本困境：高清地图虽然提供了高精度的定位参考，但其高昂的构建和维护成本阻碍了可扩展性，这促使研究转向标准定义地图（如OpenStreetMap）。当前基于SD地图的方法主要关注图像和地图之间的鸟瞰图（BEV）匹配，忽视了普遍存在的信号噪声GPS。尽管GPS易于获取，但在城市环境中它受到多径误差的影响。我们提出了DiffVL框架，它是首个将视觉定位重新表述为使用扩散模型的GPS去噪任务的框架。我们的关键见解是，当在有视觉BEV特征和SD地图的条件下，噪声GPS轨迹隐含地编码了真实的姿态分布，这可以通过迭代扩散细化来恢复。DiffVL不同于先前的BEV匹配方法（例如OrienterNet）或基于变压器的注册方法，它通过联合建模GPS、SD地图和视觉信号，学习反向GPS噪声扰动，在不依赖高清地图的情况下实现亚米级精度。在多个数据集上的实验表明，我们的方法达到了与BEV匹配基线相比的领先水平。最重要的是，我们的工作证明了扩散模型可以通过将噪声GPS视为生成先验，实现与传统的基于匹配的方法的模式转变，从而实现可扩展的定位。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于扩散模型的视觉定位新方法，采用GPS去噪方式实现准确视觉定位。该方法利用标准地图和鸟瞰视图特征，通过迭代扩散优化，从噪声GPS轨迹中恢复真实姿态分布，实现亚米级定位精度，无需依赖高精度地图。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有自主驾驶视觉定位方法主要依赖高精度地图，但其高昂的构建和维护成本限制了可扩展性。</li>
<li>提出的DiffVL框架将视觉定位重新定义为GPS去噪任务，利用扩散模型进行处理。</li>
<li>DiffVL利用噪声GPS轨迹、鸟瞰视图特征和标准地图，通过迭代扩散优化，隐式编码真实姿态分布。</li>
<li>DiffVL不同于以往的鸟瞰视图匹配方法或基于变压器的注册方法。</li>
<li>DiffVL学会反向恢复GPS噪声扰动，通过联合建模GPS、标准地图和视觉信号，实现亚米级定位精度。</li>
<li>实验证明，DiffVL在多个数据集上实现了最先进的定位精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14565">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-48da142296e8ed0358f432a2e1313640~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031084&auth_key=1760031084-0-0-ee7c0af435209c217cf975afe2baf27f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-741ce8c769ae34a17bdb4608b58549ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031091&auth_key=1760031091-0-0-74678ecc449f416dbe5add3321923e3c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e42aa573936cd411bed9f92a1669ed0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031098&auth_key=1760031098-0-0-ce5aca68c72f04b8541eaf6e6e11fc38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67098d89d2cede73749b941db21f4102~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031148&auth_key=1760031148-0-0-03eca0d7baa111ddc90508c36abd9ece&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27d195e09ff24a4cfe9e42890183818c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031154&auth_key=1760031154-0-0-1a8ea2c1d952709db193ed124bd350a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ba322526e93c29cadeea0568dcd7c28~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031160&auth_key=1760031160-0-0-32f771120453c7869d65dbac43498200&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TIDE-Achieving-Balanced-Subject-Driven-Image-Generation-via-Target-Instructed-Diffusion-Enhancement"><a href="#TIDE-Achieving-Balanced-Subject-Driven-Image-Generation-via-Target-Instructed-Diffusion-Enhancement" class="headerlink" title="TIDE: Achieving Balanced Subject-Driven Image Generation via   Target-Instructed Diffusion Enhancement"></a>TIDE: Achieving Balanced Subject-Driven Image Generation via   Target-Instructed Diffusion Enhancement</h2><p><strong>Authors:Jibai Lin, Bo Ma, Yating Yang, Xi Zhou, Rong Ma, Turghun Osman, Ahtamjan Ahmat, Rui Dong, Lei Wang</strong></p>
<p>Subject-driven image generation (SDIG) aims to manipulate specific subjects within images while adhering to textual instructions, a task crucial for advancing text-to-image diffusion models. SDIG requires reconciling the tension between maintaining subject identity and complying with dynamic edit instructions, a challenge inadequately addressed by existing methods. In this paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework, which resolves this tension through target supervision and preference learning without test-time fine-tuning. TIDE pioneers target-supervised triplet alignment, modelling subject adaptation dynamics using a (reference image, instruction, target images) triplet. This approach leverages the Direct Subject Diffusion (DSD) objective, training the model with paired “winning” (balanced preservation-compliance) and “losing” (distorted) targets, systematically generated and evaluated via quantitative metrics. This enables implicit reward modelling for optimal preservation-compliance balance. Experimental results on standard benchmarks demonstrate TIDE’s superior performance in generating subject-faithful outputs while maintaining instruction compliance, outperforming baseline methods across multiple quantitative metrics. TIDE’s versatility is further evidenced by its successful application to diverse tasks, including structural-conditioned generation, image-to-image generation, and text-image interpolation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/KomJay520/TIDE">https://github.com/KomJay520/TIDE</a>. </p>
<blockquote>
<p>主题驱动图像生成（SDIG）旨在根据文本指令操作图像中的特定主题，这对于推进文本到图像的扩散模型至关重要。SDIG需要在保持主题身份和遵守动态编辑指令之间找到平衡，现有方法对此挑战解决不足。在本文中，我们引入了目标指导扩散增强（TIDE）框架，通过目标监督和偏好学习解决这一平衡问题，而无需进行测试时微调。TIDE首创目标监督三元组对齐，使用（参考图像、指令、目标图像）三元组对主题适应动态进行建模。该方法利用直接主题扩散（DSD）目标，用配对的“获胜”（平衡保留和合规性）和“失败”（失真）目标训练模型，这些目标通过定量指标进行系统化生成和评估。这为实现最优保留合规性平衡隐式奖励建模提供了可能。在标准基准测试上的实验结果表明，TIDE在生成忠实于主题的输出时表现出卓越性能，同时保持指令合规性，在多个定量指标上优于基线方法。TIDE的通用性进一步体现在其在结构条件生成、图像到图像生成和文本图像插值等多样化任务中的成功应用。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/KomJay520/TIDE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KomJay520/TIDE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06499v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对文本指令驱动图像生成的任务，提出了一种名为TIDE（目标指导扩散增强）的框架。该框架解决了在维持主题身份与遵循动态编辑指令之间的平衡问题，通过目标监督与偏好学习，无需测试时微调即可实现。TIDE采用目标监督的三重对准技术，利用（参考图像、指令、目标图像）三重结构建模主题适应动态。通过Direct Subject Diffusion（DSD）目标进行训练，使用系统生成的配对的“胜出”（平衡保留合规性）和“失败”（失真）目标，并通过定量指标进行评估。实验结果证明，TIDE在生成主题忠实输出同时维持指令合规性的任务上表现卓越，优于基准方法。TIDE的通用性还体现在其在结构条件生成、图像到图像生成和文本图像插值等多样化任务的成功应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIDE框架解决了文本指令驱动图像生成中维持主题身份与遵循编辑指令之间的平衡问题。</li>
<li>TIDE通过目标监督与偏好学习实现这一目标，无需测试时微调。</li>
<li>TIDE采用目标监督的三重对准技术，利用（参考图像、指令、目标图像）三重结构建模主题适应动态。</li>
<li>通过Direct Subject Diffusion（DSD）目标进行训练，使用配对的“胜出”和“失败”目标进行系统评估。</li>
<li>TIDE在多种定量指标上表现优越，能生成主题忠实的输出并维持指令合规性。</li>
<li>TIDE框架具有广泛的应用性，成功应用于结构条件生成、图像到图像生成和文本图像插值等多样化任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06499">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b698d647d6dc96c7bfe4276fc355da19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031168&auth_key=1760031168-0-0-38a7f8bb5dec079e140bf13484cdcc51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98381ae38909c2778d2e7c6f1b553a68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031175&auth_key=1760031175-0-0-cfdb8a12195c78f3252995c82d21fb36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-acb18ee69b3d68a29d12abbf27c31b8c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031182&auth_key=1760031182-0-0-d711c91bbc96aec9efc80f07d743431d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb794b77b221060b1ddf32f16be8145c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031189&auth_key=1760031189-0-0-d301f5904a31bcc5438a3a48fadf9810&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1650c731f8b7315f3f6a588f949be7c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031196&auth_key=1760031196-0-0-1a63351888c6fcc474d2c330f5c09173&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models"><a href="#Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models" class="headerlink" title="Probing the Representational Power of Sparse Autoencoders in Vision   Models"></a>Probing the Representational Power of Sparse Autoencoders in Vision   Models</h2><p><strong>Authors:Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng</strong></p>
<p>Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting the hidden states of large language models (LLMs). By learning to reconstruct activations from a sparse bottleneck layer, SAEs discover interpretable features from the high-dimensional internal representations of LLMs. Despite their popularity with language models, SAEs remain understudied in the visual domain. In this work, we provide an extensive evaluation the representational power of SAEs for vision models using a broad range of image-based tasks. Our experimental results demonstrate that SAE features are semantically meaningful, improve out-of-distribution generalization, and enable controllable generation across three vision model architectures: vision embedding models, multi-modal LMMs and diffusion models. In vision embedding models, we find that learned SAE features can be used for OOD detection and provide evidence that they recover the ontological structure of the underlying model. For diffusion models, we demonstrate that SAEs enable semantic steering through text encoder manipulation and develop an automated pipeline for discovering human-interpretable attributes. Finally, we conduct exploratory experiments on multi-modal LLMs, finding evidence that SAE features reveal shared representations across vision and language modalities. Our study provides a foundation for SAE evaluation in vision models, highlighting their strong potential improving interpretability, generalization, and steerability in the visual domain. </p>
<blockquote>
<p>稀疏自编码器（Sparse Autoencoders，简称SAE）已经成为解读大型语言模型（LLM）隐藏状态的一种流行工具。通过学会从稀疏瓶颈层重建激活，SAE从LLM的高维内部表示中发现可解释的特征。尽管SAE在语言模型中很受欢迎，但在视觉领域，对它们的研究仍然不足。在这项工作中，我们通过对一系列图像任务的大量评估，全面评估了SAE在视觉模型中的表示能力。实验结果表明，SAE特征在语义上是明确的，能够提升模型对分布外的泛化能力，并能实现在三种视觉模型架构下的可控生成：视觉嵌入模型、多模态LLM和扩散模型。在视觉嵌入模型中，我们发现学习到的SAE特征可用于OOD检测，并提供证据表明它们恢复了底层模型的本体结构。对于扩散模型，我们展示了SAE通过文本编码器操控来实现语义引导的能力，并开发了一个自动化流程来发现人类可解释的属性。最后，我们对多模态LLM进行了探索性实验，发现证据表明SAE特征揭示了跨视觉和语言模态的共享表示。我们的研究为SAE在视觉模型中的评估奠定了基础，突显了它们在提高视觉领域的可解释性、泛化能力和操控能力方面的强大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11277v2">PDF</a> ICCV 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>稀疏自编码器（SAE）在解读大型语言模型（LLM）的隐藏状态方面展现出巨大潜力。本研究通过对视觉模型进行广泛评估，证明了SAE在视觉领域的表现能力。实验结果显示，SAE特征语义丰富，能提高模型在分布外的泛化能力，并在三种视觉模型架构中实现可控生成。此外，SAE特征还可用于图像嵌入模型的OOD检测，并在扩散模型中实现语义导向。本研究为SAE在视觉模型中的应用提供了基础，展现了其在提高解释性、泛化能力和可控性方面的强大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAEs用于解读大型语言模型的隐藏状态。</li>
<li>SAE在视觉领域表现能力得到广泛评估。</li>
<li>SAE特征语义丰富，能提高模型在分布外的泛化能力。</li>
<li>SAE在三种视觉模型架构中实现可控生成。</li>
<li>SAE特征可用于图像嵌入模型的OOD检测。</li>
<li>在扩散模型中，SAE实现语义导向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11277">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-17a3bc4e3f3c5292a225aab902b3db42~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031204&auth_key=1760031204-0-0-0341bd62f682c8d8a3fb615ab114df6a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9dc7f6230aef793f5188cc1f68aa0339~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031211&auth_key=1760031211-0-0-5eac8fced3caaa3f6027a92f9cf10608&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a43df17e27a99dd99a7d20823f8fa45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031218&auth_key=1760031218-0-0-44cfecc341ff7922d07d89fe2011a6c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe3261aaac461393420903ec09a57edd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031225&auth_key=1760031225-0-0-aebe25eea5902b30cc7c66ea54a12f5f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Style-Transfer-with-Diffusion-Models-for-Synthetic-to-Real-Domain-Adaptation"><a href="#Style-Transfer-with-Diffusion-Models-for-Synthetic-to-Real-Domain-Adaptation" class="headerlink" title="Style Transfer with Diffusion Models for Synthetic-to-Real Domain   Adaptation"></a>Style Transfer with Diffusion Models for Synthetic-to-Real Domain   Adaptation</h2><p><strong>Authors:Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Thomas Oberlin</strong></p>
<p>Semantic segmentation models trained on synthetic data often perform poorly on real-world images due to domain gaps, particularly in adverse conditions where labeled data is scarce. Yet, recent foundation models enable to generate realistic images without any training. This paper proposes to leverage such diffusion models to improve the performance of vision models when learned on synthetic data. We introduce two novel techniques for semantically consistent style transfer using diffusion models: Class-wise Adaptive Instance Normalization and Cross-Attention (CACTI) and its extension with selective attention Filtering (CACTIF). CACTI applies statistical normalization selectively based on semantic classes, while CACTIF further filters cross-attention maps based on feature similarity, preventing artifacts in regions with weak cross-attention correspondences. Our methods transfer style characteristics while preserving semantic boundaries and structural coherence, unlike approaches that apply global transformations or generate content without constraints. Experiments using GTA5 as source and Cityscapes&#x2F;ACDC as target domains show that our approach produces higher quality images with lower FID scores and better content preservation. Our work demonstrates that class-aware diffusion-based style transfer effectively bridges the synthetic-to-real domain gap even with minimal target domain data, advancing robust perception systems for challenging real-world applications. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/echigot/cactif">https://github.com/echigot/cactif</a>. </p>
<blockquote>
<p>基于合成数据训练的语义分割模型在真实世界图像上的表现往往较差，这主要是由于领域差距造成的，特别是在标记数据稀缺的恶劣条件下。然而，最近的基础模型能够在无需任何训练的情况下生成逼真的图像。本文提出利用此类扩散模型来提高在合成数据上学习的视觉模型的性能。我们引入两种利用扩散模型进行语义一致风格转移的新技术：基于类别的自适应实例归一化和交叉注意力（CACTI），以及其带有选择性注意力过滤的扩展（CACTIF）。CACTI根据语义类别有选择地应用统计归一化，而CACTIF进一步根据特征相似性过滤交叉注意力图，防止在交叉注意力对应关系较弱的区域出现伪影。我们的方法在转移风格特征的同时，保留了语义边界和结构连贯性，不同于应用全局变换或无条件生成内容的方法。使用GTA5作为源域，Cityscapes&#x2F;ACDC作为目标域的实验表明，我们的方法生成了质量更高、FID得分更低、内容保存更好的图像。我们的工作证明，类感知扩散风格转移有效地缩小了合成到真实的领域差距，即使目标领域的数据最少，也为具有挑战性的真实世界应用提供了稳健的感知系统。源代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/echigot/cactif%E3%80%82">https://github.com/echigot/cactif。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16360v2">PDF</a> Published in Computer Vision and Image Understanding, September 2025   (CVIU 2025)</p>
<p><strong>Summary</strong></p>
<p>本文提出利用扩散模型改善在合成数据上训练的语义分割模型在真实世界图像中的表现。针对合成数据与实际场景之间的域差异问题，文章引入两种新的基于扩散模型的技术：Class-wise Adaptive Instance Normalization与Cross-Attention结合的方法（CACTI）及其通过选择性注意力过滤的扩展版本（CACTIF）。CACTI根据语义类别选择性应用统计归一化，而CACTIF则进一步根据特征相似性过滤交叉注意力图，避免了弱交叉注意力对应区域的伪影。实验表明，该方法在风格转移时能够保留语义边界和结构连贯性，使用GTA5作为源域、Cityscapes&#x2F;ACDC作为目标域的实验结果证明了其生成图像质量更高、FID分数更低、内容保留更好。该研究展示了基于类别的扩散模型在桥梁合成到真实域差距方面的有效性，即使在目标域数据极少的情况下也能取得良好效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割模型在真实世界图像中的表现因合成数据和真实数据之间的域差异而受限。</li>
<li>扩散模型被用来改善在合成数据上训练的视觉模型性能。</li>
<li>引入两种新技术：Class-wise Adaptive Instance Normalization与Cross-Attention结合的方法（CACTI）和它的选择性注意力过滤扩展版本（CACTIF）。</li>
<li>CACTI通过选择性应用统计归一化基于语义类别进行风格转移。</li>
<li>CACTIF利用特征相似性过滤交叉注意力图，减少伪影。</li>
<li>实验证明，该方法生成图像质量高，FID分数低，内容保留好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16360">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4f74b667de0ceefb98757fa110ca0fe0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031232&auth_key=1760031232-0-0-2586a7427d2643e8902375b54ce676d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46549f41c04cd58e6bfe357c7adc1e7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031239&auth_key=1760031239-0-0-7266c4f7fec4aaff9455f27c463e672a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PVLM-Parsing-Aware-Vision-Language-Model-with-Dynamic-Contrastive-Learning-for-Zero-Shot-Deepfake-Attribution"><a href="#PVLM-Parsing-Aware-Vision-Language-Model-with-Dynamic-Contrastive-Learning-for-Zero-Shot-Deepfake-Attribution" class="headerlink" title="PVLM: Parsing-Aware Vision Language Model with Dynamic Contrastive   Learning for Zero-Shot Deepfake Attribution"></a>PVLM: Parsing-Aware Vision Language Model with Dynamic Contrastive   Learning for Zero-Shot Deepfake Attribution</h2><p><strong>Authors:Yaning Zhang, Jiahe Zhang, Chunjie Ma, Weili Guan, Tian Gan, Zan Gao</strong></p>
<p>The challenge of tracing the source attribution of forged faces has gained significant attention due to the rapid advancement of generative models. However, existing deepfake attribution (DFA) works primarily focus on the interaction among various domains in vision modality, and other modalities such as texts and face parsing are not fully explored. Besides, they tend to fail to assess the generalization performance of deepfake attributors to unseen advanced generators like diffusion in a fine-grained manner. In this paper, we propose a novel parsing-aware vision language model with dynamic contrastive learning(PVLM) method for zero-shot deepfake attribution (ZS-DFA),which facilitates effective and fine-grained traceability to unseen advanced generators. Specifically, we conduct a novel and fine-grained ZS-DFA benchmark to evaluate the attribution performance of deepfake attributors to unseen advanced generators like diffusion. Besides, we propose an innovative parsing-guided vision language model with dynamic contrastive learning (PVLM) method to capture general and diverse attribution features. We are motivated by the observation that the preservation of source face attributes in facial images generated by GAN and diffusion models varies significantly. We employ the inherent face attributes preservation differences to capture face parsing-aware forgery representations. Therefore, we devise a novel parsing encoder to focus on global face attribute embeddings, enabling parsing-guided DFA representation learning via dynamic vision-parsing matching. Additionally, we present a novel deepfake attribution contrastive center loss to pull relevant generators closer and push irrelevant ones away, which can be introduced into DFA models to enhance traceability. Experimental results show that our model exceeds the state-of-the-art on the ZS-DFA benchmark via various protocol evaluations. </p>
<blockquote>
<p>随着生成模型的快速发展，追踪伪造面孔的源头归属问题获得了广泛关注。然而，现有的深度伪造归属（DFA）主要关注视觉模态中不同域之间的交互，而文本和面部解析等其他模态并未得到充分探索。此外，它们往往无法以精细的方式评估深度伪造归属者对未见的高级生成器（如扩散）的泛化性能。在本文中，我们提出了一种新颖的解析感知视觉语言模型，结合动态对比学习（PVLM）方法进行零样本深度伪造归属（ZS-DFA），有助于对未见的高级生成器进行有效且精细的追踪。具体来说，我们构建了一个新颖且精细的ZS-DFA基准测试，以评估深度伪造归属者对未见的高级生成器的归属性能，如扩散模型。除此之外，我们提出了创新的解析引导视觉语言模型，结合动态对比学习方法来捕捉通用和多样化的归属特征。我们的动机是观察到由GAN和扩散模型生成的面部图像中保留的源脸特征差异很大。我们利用固有的面部特征保留差异来捕捉面部解析感知的伪造表示。因此，我们设计了一种新的解析编码器，专注于全局面部特征嵌入，通过动态视觉解析匹配实现解析引导DFA表示学习。此外，我们提出了一种新的深度伪造归属对比中心损失，将相关生成器拉近并推远不相关的生成器，可以引入到DFA模型中以提高追踪性能。实验结果表明，我们的模型在各种协议评估上的ZS-DFA基准测试上超过了现有技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14129v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文关注于面部伪造溯源的问题，针对现有深度伪造归因（DFA）方法主要关注视觉模态的不足，提出一种结合解析感知与视觉语言模型的动态对比学习（PVLM）方法，用于零样本深度伪造归因（ZS-DFA）。该方法能有效追踪未见的高级生成器如扩散模型。文章建立了一个新颖的细粒度ZS-DFA基准测试平台，评估归因性能。同时，提出解析引导的动态对比学习机制，捕捉全面且多样的归因特征。利用GAN和扩散模型生成面部图像时源脸属性保留程度的差异，设计了一种新颖的解析编码器，专注于全局面部属性嵌入，通过动态视觉解析匹配实现解析引导DFA表示学习。此外，引入深度伪造对比中心损失，拉近相关生成器并远离不相关生成器，以增强追踪能力。实验结果显示，该模型在ZS-DFA基准测试中超过了现有最佳方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>现有深度伪造归因方法主要关注视觉模态的交互，忽视了文本和面部解析等其他模态的作用。</li>
<li>提出了一种新颖的解析感知与视觉语言模型结合的动态对比学习方法（PVLM），用于零样本深度伪造归因（ZS-DFA）。</li>
<li>建立了一个细粒度的ZS-DFA基准测试平台，以评估对未见的高级生成器的归因性能。</li>
<li>利用面部图像生成时源脸属性保留程度的差异，设计了一种新颖的解析编码器。</li>
<li>引入了深度伪造对比中心损失，以增强追踪能力。</li>
<li>实验结果显示，该模型在多种协议评估中的ZS-DFA基准测试上表现优异。</li>
<li>该方法为实现更有效的面部伪造溯源提供了新的思路和工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14129">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-32fafde3f0f23ea621770ef3aa36a126~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031247&auth_key=1760031247-0-0-71da5f950910ef1c021f25de098edb98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-324a4295b240cf74fe2dfe6d195e44fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031255&auth_key=1760031255-0-0-2b437cdd8ac995c3b9c51235c4573549&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a7cb783314a2aeedaef6ef7e35b7c6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031262&auth_key=1760031262-0-0-2b73952b35afdcb2a668f86b869b2140&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-601ca7c1d08273bb9e525e26d29964f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031269&auth_key=1760031269-0-0-762a9167a3d4591b01dce2ad96933ed5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7194d94a7725db2e6161d2790894e0b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031275&auth_key=1760031275-0-0-1a578caef20387b58b6c07b5c2920ef7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Boost-3D-Reconstruction-using-Diffusion-based-Monocular-Camera-Calibration"><a href="#Boost-3D-Reconstruction-using-Diffusion-based-Monocular-Camera-Calibration" class="headerlink" title="Boost 3D Reconstruction using Diffusion-based Monocular Camera   Calibration"></a>Boost 3D Reconstruction using Diffusion-based Monocular Camera   Calibration</h2><p><strong>Authors:Junyuan Deng, Wei Yin, Xiaoyang Guo, Qian Zhang, Xiaotao Hu, Weiqiang Ren, Xiao-Xiao Long, Ping Tan</strong></p>
<p>In this paper, we present DM-Calib, a diffusion-based approach for estimating pinhole camera intrinsic parameters from a single input image. Monocular camera calibration is essential for many 3D vision tasks. However, most existing methods depend on handcrafted assumptions or are constrained by limited training data, resulting in poor generalization across diverse real-world images. Recent advancements in stable diffusion models, trained on massive data, have shown the ability to generate high-quality images with varied characteristics. Emerging evidence indicates that these models implicitly capture the relationship between camera focal length and image content. Building on this insight, we explore how to leverage the powerful priors of diffusion models for monocular pinhole camera calibration. Specifically, we introduce a new image-based representation, termed Camera Image, which losslessly encodes the numerical camera intrinsics and integrates seamlessly with the diffusion framework. Using this representation, we reformulate the problem of estimating camera intrinsics as the generation of a dense Camera Image conditioned on an input image. By fine-tuning a stable diffusion model to generate a Camera Image from a single RGB input, we can extract camera intrinsics via a RANSAC operation. We further demonstrate that our monocular calibration method enhances performance across various 3D tasks, including zero-shot metric depth estimation, 3D metrology, pose estimation and sparse-view reconstruction. Extensive experiments on multiple public datasets show that our approach significantly outperforms baselines and provides broad benefits to 3D vision tasks. </p>
<blockquote>
<p>本文介绍了DM-Calib，这是一种基于扩散的方法，用于从单个输入图像估计针孔相机的内在参数。单目相机标定是许多三维视觉任务的基础。然而，大多数现有方法依赖于手工制作的假设或受限于有限的训练数据，导致在多样化的真实世界图像中的泛化能力较差。最近基于大规模数据的稳定扩散模型的进展表明，它们能够生成具有不同特性的高质量图像。有证据表明，这些模型隐含地捕捉了相机焦距与图像内容之间的关系。基于这一见解，我们探讨了如何利用扩散模型的有力先验进行单目针孔相机标定。具体来说，我们引入了一种新的基于图像的表示方法，称为Camera Image，它无损地编码了相机的数值内在参数，并与扩散框架无缝集成。使用这种表示方法，我们将估计相机内在参数的问题重新表述为根据输入图像生成密集Camera Image的问题。通过微调稳定的扩散模型来从单个RGB输入生成Camera Image，我们可以通过RANSAC操作提取相机内在参数。我们进一步证明，我们的单目标定方法提高了各种三维任务的性能，包括零样本度量深度估计、三维测量、姿态估计和稀疏视图重建。在多个公共数据集上的广泛实验表明，我们的方法显著优于基准线，并为三维视觉任务提供了广泛的益处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17240v3">PDF</a> </p>
<p><strong>Summary</strong><br>DM-Calib是一种基于扩散模型的针孔相机内参估计方法，从单张输入图像出发，利用扩散模型的强大先验信息进行单目相机校准。新方法解决了传统方法受限于手工假设或训练数据不足的问题，提高了在多种真实图像中的泛化能力。通过引入Camera Image表示，将相机内参估计问题转化为基于输入图像生成密集Camera Image的问题。通过微调稳定的扩散模型生成Camera Image，利用RANSAC算法提取相机内参。在多个公共数据集上的实验表明，该方法显著优于基线方法，并为多种三维视觉任务提供了广泛的益处。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DM-Calib是一种基于扩散模型的针孔相机内参估计方法。</li>
<li>该方法解决了传统相机校准方法依赖于手工假设或训练数据不足的问题。</li>
<li>引入Camera Image表示，将相机内参估计问题转化为生成问题。</li>
<li>利用扩散模型的强大先验信息进行相机校准。</li>
<li>通过微调稳定的扩散模型生成Camera Image。</li>
<li>使用RANSAC算法从生成的Camera Image中提取相机内参。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17240">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-9ce7c585df33057620f5f2dcfe5c931f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031283&auth_key=1760031283-0-0-4098f12b26601c3687f1ac20ca539034&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8176a46f91981d5990a77ec56fb7d1dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031291&auth_key=1760031291-0-0-3e05c29d1ed15c110658f6aebd33e7f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6ac516f3a44f6f2bfd92a63351523d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031297&auth_key=1760031297-0-0-87bc52a2ed1de7c24819c41cb06678db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-142a00e278dd0ae16e1bddcd63bef1fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031304&auth_key=1760031304-0-0-2fc6e1fb1c454e4f05d83f26d502c7ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ed5bf5ef103a9408a8b868da8b14736~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031311&auth_key=1760031311-0-0-4f48b46987ddaf28be4e3e580cec3525&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-305bb56c15aae926bb206bc5437d48c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031318&auth_key=1760031318-0-0-c42cfd93581cf4021e9dd425889340a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-20  Semi-Supervised 3D Medical Segmentation from 2D Natural Images   Pretrained Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-2c06628dbcec73febee3836f1ff9f431~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030821&auth_key=1760030821-0-0-2a1593095ab8f42c051a5604f3542017&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-09-20  RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
