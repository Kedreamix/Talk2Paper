<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  Lightweight and Accurate Multi-View Stereo with Confidence-Aware   Diffusion Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-67098d89d2cede73749b941db21f4102~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030892&auth_key=1760030892-0-0-b6f487b65550d82604d5b331e0faa442&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    50 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-20-æ›´æ–°"><a href="#2025-09-20-æ›´æ–°" class="headerlink" title="2025-09-20 æ›´æ–°"></a>2025-09-20 æ›´æ–°</h1><h2 id="Lightweight-and-Accurate-Multi-View-Stereo-with-Confidence-Aware-Diffusion-Model"><a href="#Lightweight-and-Accurate-Multi-View-Stereo-with-Confidence-Aware-Diffusion-Model" class="headerlink" title="Lightweight and Accurate Multi-View Stereo with Confidence-Aware   Diffusion Model"></a>Lightweight and Accurate Multi-View Stereo with Confidence-Aware   Diffusion Model</h2><p><strong>Authors:Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys</strong></p>
<p>To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks &amp; Temples and ETH3D. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/cvg/diffmvs">https://github.com/cvg/diffmvs</a>. </p>
<blockquote>
<p>ä»æ ¡å‡†å›¾åƒé‡å»º3Då‡ ä½•ç»“æ„æ—¶ï¼ŒåŸºäºå­¦ä¹ çš„å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ–¹æ³•é€šå¸¸æ‰§è¡Œå¤šè§†å›¾æ·±åº¦ä¼°è®¡ï¼Œç„¶åå°†æ·±åº¦å›¾èåˆä¸ºç½‘æ ¼æˆ–ç‚¹äº‘ã€‚ä¸ºäº†æé«˜è®¡ç®—æ•ˆç‡ï¼Œè®¸å¤šæ–¹æ³•åˆå§‹åŒ–ä¸€ä¸ªç²—ç•¥çš„æ·±åº¦å›¾ï¼Œç„¶åé€æ¸åœ¨æ›´é«˜åˆ†è¾¨ç‡ä¸‹å¯¹å…¶è¿›è¡Œä¼˜åŒ–ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ä»éšæœºå™ªå£°å¼€å§‹ï¼Œæ‰©æ•£æ¨¡å‹é€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹é€æ¸æ¢å¤æ ·æœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„MVSæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†MVSä¸­çš„æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ·±åº¦ä¼˜åŒ–åˆ¶å®šä¸ºæ¡ä»¶æ‰©æ•£è¿‡ç¨‹ã€‚è€ƒè™‘åˆ°æ·±åº¦ä¼°è®¡çš„åˆ¤åˆ«ç‰¹æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¡ä»¶ç¼–ç å™¨æ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆè½»é‡åŒ–2D U-Netå’Œå·ç§¯GRUçš„æ–°å‹æ‰©æ•£ç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®æ‰©æ•£æ¨¡å‹ä¼°è®¡çš„ä¿¡å¿ƒè‡ªé€‚åº”åœ°é‡‡æ ·æ·±åº¦å‡è®¾ã€‚åŸºäºæˆ‘ä»¬æ–°çš„MVSæ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°çš„MVSæ–¹æ³•ï¼Œå³DiffMVSå’ŒCasDiffMVSã€‚DiffMVSåœ¨è¿è¡Œæ—¶é—´å’ŒGPUå†…å­˜æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›çš„æ•ˆç‡ï¼›CasDiffMVSåœ¨DTUã€Tanks &amp; Templeså’ŒETH3Dä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/cvg/diffmvs%E3%80%82">https://github.com/cvg/diffmvsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15220v1">PDF</a> Accepted to IEEE T-PAMI 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/cvg/diffmvs">https://github.com/cvg/diffmvs</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºå°†æ‰©æ•£æ¨¡å‹å¼•å…¥å¤šè§†è§’ç«‹ä½“ï¼ˆMVSï¼‰æ¡†æ¶ä¸­ï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£è¿‡ç¨‹è¿›è¡Œæ·±åº¦ç»†åŒ–ã€‚è®¾è®¡æ¡ä»¶ç¼–ç å™¨ä»¥æé«˜æ·±åº¦ä¼°è®¡çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œå¹¶æå‡ºç»“åˆè½»é‡åŒ–2D U-Netå’Œå·ç§¯GRUçš„æ–°å‹æ‰©æ•£ç½‘ç»œã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®æ‰©æ•£æ¨¡å‹ä¼°è®¡çš„ä¿¡å¿ƒè‡ªé€‚åº”é‡‡æ ·æ·±åº¦å‡è®¾ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæå‡ºäº†ä¸¤ç§æ–°å‹MVSæ–¹æ³•ï¼šDiffMVSå’ŒCasDiffMVSï¼Œå‰è€…åœ¨è¿è¡Œæ—¶å’ŒGPUå†…å­˜ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºé«˜æ•ˆç«äº‰åŠ›ï¼Œåè€…åœ¨DTUã€Tanks &amp; Templeså’ŒETH3Dä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è¢«å¼•å…¥å¤šè§†è§’ç«‹ä½“ï¼ˆMVSï¼‰æ¡†æ¶ä¸­ï¼Œç”¨äºæé«˜ä»æ ¡å‡†å›¾åƒé‡å»º3Då‡ ä½•ä½“çš„è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>æ·±åº¦ç»†åŒ–è¢«è¡¨è¿°ä¸ºæ¡ä»¶æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥æé«˜æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>è®¾è®¡äº†æ¡ä»¶ç¼–ç å™¨ä»¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ·±åº¦ä¼°è®¡çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆè½»é‡åŒ–2D U-Netå’Œå·ç§¯GRUçš„æ–°å‹æ‰©æ•£ç½‘ç»œç»“æ„ã€‚</li>
<li>å¼•å…¥åŸºäºä¿¡å¿ƒçš„é‡‡æ ·ç­–ç•¥ï¼Œæ ¹æ®æ‰©æ•£æ¨¡å‹çš„ä¿¡å¿ƒä¼°è®¡è‡ªé€‚åº”é‡‡æ ·æ·±åº¦å‡è®¾ã€‚</li>
<li>åŸºäºæ–°æ¡†æ¶ï¼Œæå‡ºäº†ä¸¤ç§MVSæ–¹æ³•ï¼šDiffMVSå’ŒCasDiffMVSã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-eb873c72a43f8cc2ffcf6da8cbd8b3a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030899&auth_key=1760030899-0-0-b869619f7186dc2eb1bc746834fe266e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1879fab1477eb044d091291934dcf591~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030907&auth_key=1760030907-0-0-951ae09ab1c7af117874f808fbeb3cf1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f0f23cbbc1403b6fc42e6aa350e074e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030914&auth_key=1760030914-0-0-75b0da3ddaae7f1bf5c212e7509bb4fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a79ddd4c1a882a039f5ca0f42d5d45a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030921&auth_key=1760030921-0-0-867951c7f2fa1af2dcccefc161ac62ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing"><a href="#AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing" class="headerlink" title="AutoEdit: Automatic Hyperparameter Tuning for Image Editing"></a>AutoEdit: Automatic Hyperparameter Tuning for Image Editing</h2><p><strong>Authors:Chau Pham, Quan Dao, Mahesh Bhosale, Yunjie Tian, Dimitris Metaxas, David Doermann</strong></p>
<p>Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editingâ€™s hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å½»åº•æ”¹å˜äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ï¼Œä½†ç°æœ‰çš„ç¼–è¾‘æ–¹æ³•åœ¨è¶…å‚æ•°è¯†åˆ«æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è·å¾—åˆç†çš„ç¼–è¾‘æ€§èƒ½ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸è¦æ±‚ç”¨æˆ·å¼ºè¡Œè°ƒæ•´å¤šä¸ªç›¸äº’ä¾èµ–çš„è¶…å‚æ•°ï¼Œå¦‚åè½¬æ—¶é—´æ­¥é•¿å’Œæ³¨æ„åŠ›ä¿®æ”¹ç­‰ã€‚è¿™ä¸€è¿‡ç¨‹ç”±äºè¶…å‚æ•°æœç´¢ç©ºé—´çš„å·¨å¤§è€Œäº§ç”Ÿäº†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å°†å¯»æ‰¾æœ€ä½³ç¼–è¾‘è¶…å‚æ•°è§†ä¸ºæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªåºè´¯å†³ç­–ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå»ºç«‹äº†ä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹åœ¨å»å™ªæ­¥éª¤ä¸­åŠ¨æ€è°ƒæ•´è¶…å‚æ•°ï¼Œå¹¶å°†ç¼–è¾‘ç›®æ ‡æ•´åˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å®ç°äº†æ—¶é—´æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†æœ€ä½³è¶…å‚æ•°é…ç½®ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å¼ºè¡Œæ–¹æ³•ç›¸æ¯”ï¼Œæœç´¢æ—¶é—´å’Œè®¡ç®—å¼€é”€å¤§å¤§å‡å°‘ï¼Œä¸ºåŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘æ¡†æ¶åœ¨å®é™…ä¸–ç•Œä¸­çš„å®é™…åº”ç”¨æä¾›äº†è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15031v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ä¸­é¢ä¸´çš„è¶…å‚æ•°è¯†åˆ«æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å°†è¶…å‚æ•°æœç´¢è§†ä¸ºæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­çš„åºè´¯å†³ç­–ä»»åŠ¡ï¼Œå¹¶æå‡ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å»å™ªæ­¥éª¤ä¸­çš„è¶…å‚æ•°ï¼Œå°†ç¼–è¾‘ç›®æ ‡èå…¥å¥–åŠ±å‡½æ•°ï¼Œå®ç°æ—¶é—´æ•ˆç‡ä¸æœ€ä¼˜è¶…å‚æ•°é…ç½®çš„å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æš´åŠ›æœç´¢æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†æœç´¢æ—¶é—´å’Œè®¡ç®—å¼€é”€ï¼Œæ¨åŠ¨äº†æ‰©æ•£æ¨¡å‹å›¾åƒç¼–è¾‘æ¡†æ¶åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ä¸­çš„æœ€æ–°è¿›å±•åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºå°†è¶…å‚æ•°æœç´¢è§†ä¸ºæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­çš„åºè´¯å†³ç­–ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒåŠ¨æ€è°ƒæ•´å»å™ªæ­¥éª¤ä¸­çš„è¶…å‚æ•°ã€‚</li>
<li>å°†ç¼–è¾‘ç›®æ ‡èå…¥å¥–åŠ±å‡½æ•°ï¼Œå®ç°æ—¶é—´æ•ˆç‡ä¸æœ€ä¼˜è¶…å‚æ•°é…ç½®çš„å¹³è¡¡ã€‚</li>
<li>æ–¹æ³•çš„å®éªŒéªŒè¯ï¼Œè¯æ˜å…¶ç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æœç´¢æ—¶é—´å’Œè®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-464f1ed661ac3bbfc5c7b2b86fa86e61~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030928&auth_key=1760030928-0-0-2ab1267f43c4997c0b532907590f5ce5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-514a7f0a65b5d46d97c2eb06a635f5bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030936&auth_key=1760030936-0-0-f9b46a245c5b2994e2eff1bbe0b03264&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-50f5df456c84a723ebc0a9a0cf958d9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030942&auth_key=1760030942-0-0-94aeca0bb791f8206b277cd31dcb3db5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Controllable-Localized-Face-Anonymization-Via-Diffusion-Inpainting"><a href="#Controllable-Localized-Face-Anonymization-Via-Diffusion-Inpainting" class="headerlink" title="Controllable Localized Face Anonymization Via Diffusion Inpainting"></a>Controllable Localized Face Anonymization Via Diffusion Inpainting</h2><p><strong>Authors:Ali Salar, Qing Liu, Guoying Zhao</strong></p>
<p>The growing use of portrait images in computer vision highlights the need to protect personal identities. At the same time, anonymized images must remain useful for downstream computer vision tasks. In this work, we propose a unified framework that leverages the inpainting ability of latent diffusion models to generate realistic anonymized images. Unlike prior approaches, we have complete control over the anonymization process by designing an adaptive attribute-guidance module that applies gradient correction during the reverse denoising process, aligning the facial attributes of the generated image with those of the synthesized target image. Our framework also supports localized anonymization, allowing users to specify which facial regions are left unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ datasets show that our method outperforms state-of-the-art approaches while requiring no additional model training. The source code is available on our page. </p>
<blockquote>
<p>è®¡ç®—æœºè§†è§‰ä¸­è‚–åƒå›¾åƒçš„ä½¿ç”¨æ—¥ç›Šå¢å¤šï¼Œè¿™çªå‡ºäº†ä¿æŠ¤ä¸ªäººèº«ä»½çš„éœ€è¦ã€‚åŒæ—¶ï¼ŒåŒ¿åå›¾åƒå¿…é¡»ä»ç„¶å¯¹ä¸‹æ¸¸è®¡ç®—æœºè§†è§‰ä»»åŠ¡æœ‰ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è¡¥å…¨èƒ½åŠ›æ¥ç”Ÿæˆé€¼çœŸçš„åŒ¿åå›¾åƒã€‚ä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬é€šè¿‡è®¾è®¡ä¸€ä¸ªè‡ªé€‚åº”å±æ€§å¼•å¯¼æ¨¡å—æ¥æ§åˆ¶åŒ¿ååŒ–è¿‡ç¨‹ï¼Œè¯¥æ¨¡å—åœ¨åå‘å»å™ªè¿‡ç¨‹ä¸­åº”ç”¨æ¢¯åº¦æ ¡æ­£ï¼Œä½¿ç”Ÿæˆå›¾åƒçš„é¢éƒ¨å±æ€§ä¸åˆæˆç›®æ ‡å›¾åƒçš„é¢éƒ¨å±æ€§å¯¹é½ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜æ”¯æŒå±€éƒ¨åŒ¿ååŒ–ï¼Œå…è®¸ç”¨æˆ·æŒ‡å®šå“ªäº›é¢éƒ¨åŒºåŸŸä¿æŒä¸å˜ã€‚åœ¨å…¬å…±CelebA-HQå’ŒFFHQæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸éœ€è¦é¢å¤–æ¨¡å‹è®­ç»ƒçš„æƒ…å†µä¸‹ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æºä»£ç å¯åœ¨æˆ‘ä»¬çš„é¡µé¢ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14866v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è¡¥å…¨èƒ½åŠ›ç”ŸæˆçœŸå®åŒ¿åè‚–åƒå›¾åƒçš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡ä¸€ä¸ªè‡ªé€‚åº”å±æ€§å¼•å¯¼æ¨¡å—ï¼Œåœ¨åå‘å»å™ªè¿‡ç¨‹ä¸­è¿›è¡Œæ¢¯åº¦æ ¡æ­£ï¼Œä½¿ç”Ÿæˆå›¾åƒçš„é¢éƒ¨å±æ€§ä¸åˆæˆç›®æ ‡å›¾åƒå¯¹é½ï¼Œä»è€Œå®Œå…¨æ§åˆ¶åŒ¿ååŒ–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒå±€éƒ¨åŒ¿ååŒ–ï¼Œå…è®¸ç”¨æˆ·æŒ‡å®šé¢éƒ¨åŒºåŸŸä¿æŒä¸å˜ã€‚åœ¨å…¬å…±CelebA-HQå’ŒFFHQæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œä¸”æ— éœ€é¢å¤–æ¨¡å‹è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è¡¥å…¨èƒ½åŠ›ç”ŸæˆçœŸå®åŒ¿åè‚–åƒå›¾åƒçš„ç»Ÿä¸€æ¡†æ¶è¢«æå‡ºã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”å±æ€§å¼•å¯¼æ¨¡å—å®ç°æ¢¯åº¦æ ¡æ­£ï¼Œä½¿ç”Ÿæˆå›¾åƒçš„é¢éƒ¨å±æ€§ä¸ç›®æ ‡å›¾åƒå¯¹é½ã€‚</li>
<li>æ”¯æŒå±€éƒ¨åŒ¿ååŒ–ï¼Œå…è®¸ç”¨æˆ·æŒ‡å®šé¢éƒ¨åŒºåŸŸä¿æŒä¸å˜ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å…¬å…±CelebA-HQå’ŒFFHQæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€é¢å¤–æ¨¡å‹è®­ç»ƒã€‚</li>
<li>æºç å·²å…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-675611ceff82a5ddd4baba812c803b7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030950&auth_key=1760030950-0-0-82a4a21bab0f05843c5628ffdf1b852f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bc6af4fe59324b75341c2f977d12627c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030957&auth_key=1760030957-0-0-d5a4099481482085fca054cb096224e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-736b280a0dd145bed0cc192345715028~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030963&auth_key=1760030963-0-0-29855cbc15c6733ef113da156b354f26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d39de97a93c86ecf21e40e771743cdd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030970&auth_key=1760030970-0-0-d6d06e10b6f6e8ee0220b2793ca5adfe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1e7e0a4fd743c70c82f0a62f81f00bdd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030977&auth_key=1760030977-0-0-88ead5f1122c382f904f4f3e25faffae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Radiology-Report-Conditional-3D-CT-Generation-with-Multi-Encoder-Latent-diffusion-Model"><a href="#Radiology-Report-Conditional-3D-CT-Generation-with-Multi-Encoder-Latent-diffusion-Model" class="headerlink" title="Radiology Report Conditional 3D CT Generation with Multi Encoder Latent   diffusion Model"></a>Radiology Report Conditional 3D CT Generation with Multi Encoder Latent   diffusion Model</h2><p><strong>Authors:Sina Amirrajab, Zohaib Salahuddin, Sheng Kuang, Henry C. Woodruff, Philippe Lambin</strong></p>
<p>Text to image latent diffusion models have recently advanced medical image synthesis, but applications to 3D CT generation remain limited. Existing approaches rely on simplified prompts, neglecting the rich semantic detail in full radiology reports, which reduces text image alignment and clinical fidelity. We propose Report2CT, a radiology report conditional latent diffusion framework for synthesizing 3D chest CT volumes directly from free text radiology reports, incorporating both findings and impression sections using multiple text encoder. Report2CT integrates three pretrained medical text encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced clinical context. Radiology reports and voxel spacing information condition a 3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset. Model performance was evaluated using Frechet Inception Distance (FID) for real synthetic distributional similarity and CLIP based metrics for semantic alignment, with additional qualitative and quantitative comparisons against GenerateCT model. Report2CT generated anatomically consistent CT volumes with excellent visual quality and text image alignment. Multi encoder conditioning improved CLIP scores, indicating stronger preservation of fine grained clinical details in the free text radiology reports. Classifier free guidance further enhanced alignment with only a minor trade off in FID. We ranked first in the VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved state of the art performance across all evaluation metrics. By leveraging complete radiology reports and multi encoder text conditioning, Report2CT advances 3D CT synthesis, producing clinically faithful and high quality synthetic data. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œä½†åœ¨3D CTç”Ÿæˆæ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºç®€åŒ–çš„æç¤ºï¼Œå¿½ç•¥äº†å®Œæ•´æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„ä¸°å¯Œè¯­ä¹‰ç»†èŠ‚ï¼Œè¿™é™ä½äº†æ–‡æœ¬å›¾åƒå¯¹é½å’Œä¸´åºŠä¿çœŸåº¦ã€‚æˆ‘ä»¬æå‡ºäº†Report2CTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ”¾å°„å­¦æŠ¥å‘Šæ¡ä»¶çš„æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œå¯ä»¥ç›´æ¥ä»è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šä¸­åˆæˆ3Dèƒ¸éƒ¨CTä½“ç§¯ï¼Œå®ƒç»“åˆäº†æ£€æŸ¥ç»“æœå’Œå°è±¡éƒ¨åˆ†ï¼Œä½¿ç”¨å¤šä¸ªæ–‡æœ¬ç¼–ç å™¨ã€‚Report2CTé›†æˆäº†ä¸‰ä¸ªé¢„è®­ç»ƒçš„åŒ»ç–—æ–‡æœ¬ç¼–ç å™¨ï¼ˆBiomedVLP CXR BERTã€MedEmbedå’ŒClinicalBERTï¼‰æ¥æ•æ‰å¾®å¦™çš„ä¸´åºŠèƒŒæ™¯ã€‚æ”¾å°„å­¦æŠ¥å‘Šå’Œä½“ç´ é—´è·ä¿¡æ¯å¯¹CT RATEæ•°æ®é›†ä¸Šç»è¿‡è®­ç»ƒçš„2ä¸‡ä¸ªCTä½“ç§¯çš„3Dæ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†æ¡ä»¶å¤„ç†ã€‚æˆ‘ä»¬é€šè¿‡Frechet Inception Distanceï¼ˆFIDï¼‰è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç”¨äºçœŸå®åˆæˆåˆ†å¸ƒç›¸ä¼¼æ€§ï¼Œå¹¶ä½¿ç”¨CLIPç›¸å…³æŒ‡æ ‡è¿›è¡Œè¯­ä¹‰å¯¹é½ï¼Œä¸GenerateCTæ¨¡å‹è¿›è¡Œäº†å®šæ€§å’Œå®šé‡æ¯”è¾ƒã€‚Report2CTç”Ÿæˆçš„CTä½“ç§¯è§£å‰–ç»“æ„ä¸€è‡´ï¼Œè§†è§‰è´¨é‡ä¼˜ç§€ï¼Œæ–‡æœ¬å›¾åƒå¯¹é½ã€‚å¤šç¼–ç å™¨æ¡ä»¶æ”¹å–„äº†CLIPåˆ†æ•°ï¼Œè¡¨æ˜ç²¾ç»†ç²’åº¦çš„ä¸´åºŠç»†èŠ‚åœ¨è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šä¸­å¾—åˆ°äº†æ›´å¥½çš„ä¿ç•™ã€‚æ— åˆ†ç±»å™¨æŒ‡å¯¼è¿›ä¸€æ­¥å¢å¼ºäº†å¯¹é½æ€§ï¼ŒåŒæ—¶åªç‰ºç‰²äº†å°‘é‡çš„FIDã€‚åœ¨MICCAI 2025å¹´çš„VLM3DæŒ‘æˆ˜ä¸­ï¼Œæˆ‘ä»¬åœ¨æ–‡æœ¬æ¡ä»¶CTç”Ÿæˆæ–¹é¢æ’åç¬¬ä¸€ï¼Œå¹¶åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚é€šè¿‡åˆ©ç”¨å®Œæ•´çš„æ”¾å°„å­¦æŠ¥å‘Šå’Œå¤šç¼–ç å™¨æ–‡æœ¬æ¡ä»¶å¤„ç†ï¼ŒReport2CTæ¨åŠ¨äº†3D CTåˆæˆçš„å‘å±•ï¼Œç”Ÿæˆäº†ä¸´åºŠçœŸå®ä¸”é«˜è´¨é‡çš„ç»¼åˆæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14780v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æŠ¥å‘Šæå‡ºäº†Report2CTï¼Œä¸€ä¸ªåŸºäºæ”¾å°„å­¦æŠ¥å‘Šæ¡ä»¶çš„æ½œåœ¨æ‰©æ•£æ¡†æ¶ï¼Œå¯ä»è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šç›´æ¥åˆæˆ3Dèƒ¸éƒ¨CTä½“ç§¯ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šç§åŒ»å­¦æ–‡æœ¬ç¼–ç å™¨ï¼Œæ•æ‰å¾®å¦™çš„ä¸´åºŠèƒŒæ™¯ã€‚ä½¿ç”¨æ¥è‡ªCT RATEæ•°æ®é›†çš„2ä¸‡ä»½CTä½“ç§¯è®­ç»ƒäº†3Dæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚æ¨¡å‹æ€§èƒ½é€šè¿‡Frechet Inception Distance (FID)è¿›è¡ŒçœŸå®ä¸åˆæˆåˆ†å¸ƒç›¸ä¼¼æ€§è¯„ä¼°ï¼Œå¹¶é€šè¿‡CLIPæŒ‡æ ‡è¿›è¡Œè¯­ä¹‰å¯¹é½è¯„ä¼°ã€‚Report2CTç”Ÿæˆäº†ç»“æ„ä¸€è‡´çš„CTä½“ç§¯ï¼Œå…·æœ‰è‰¯å¥½çš„è§†è§‰è´¨é‡å’Œæ–‡æœ¬å›¾åƒå¯¹é½æ€§ã€‚å¤šç¼–ç å™¨æ¡ä»¶æ”¹å–„äº†CLIPè¯„åˆ†ï¼Œè¡¨æ˜ç²¾ç»†ç²’åº¦çš„ä¸´åºŠç»†èŠ‚åœ¨è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šä¸­å¾—åˆ°äº†æ›´å¥½çš„ä¿ç•™ã€‚æ— åˆ†ç±»å™¨æŒ‡å¯¼è¿›ä¸€æ­¥å¢å¼ºäº†å¯¹é½æ€§ï¼Œä»…ç•¥å¾®é™ä½äº†FIDã€‚åœ¨MICCAI 2025å¹´çš„VLM3DæŒ‘æˆ˜ä¸­ï¼ŒReport2CTåœ¨æ–‡æœ¬æ¡ä»¶CTç”Ÿæˆæ–¹é¢æ’åç¬¬ä¸€ï¼Œå¹¶åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨å®Œæ•´çš„æ”¾å°„å­¦æŠ¥å‘Šå’Œå¤šç¼–ç å™¨æ–‡æœ¬æ¡ä»¶ï¼ŒReport2CTæ¨è¿›äº†3D CTçš„åˆæˆï¼Œäº§ç”Ÿäº†ä¸´åºŠçœŸå®ä¸”é«˜è´¨é‡çš„ç»¼åˆæ•°æ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æŠ¥å‘Šæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•Report2CTï¼Œå¯ä»¥ä»è‡ªç”±æ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šç›´æ¥åˆæˆ3Dèƒ¸éƒ¨CTä½“ç§¯ã€‚</li>
<li>Report2CTç»“åˆäº†å¤šç§åŒ»å­¦æ–‡æœ¬ç¼–ç å™¨ä»¥æ•æ‰å¾®å¦™çš„ä¸´åºŠèƒŒæ™¯ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨æ¥è‡ªCT RATEæ•°æ®é›†çš„CTä½“ç§¯è®­ç»ƒäº†3Dæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Report2CTç”Ÿæˆçš„CTä½“ç§¯å…·æœ‰è‰¯å¥½çš„è§†è§‰è´¨é‡å’Œæ–‡æœ¬å›¾åƒå¯¹é½æ€§ã€‚</li>
<li>å¤šç¼–ç å™¨æ¡ä»¶æ”¹å–„äº†CLIPè¯„åˆ†ï¼Œä¿ç•™äº†æ›´ç²¾ç»†çš„ä¸´åºŠç»†èŠ‚ã€‚</li>
<li>æ— åˆ†ç±»å™¨æŒ‡å¯¼å¢å¼ºäº†æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„å¯¹é½æ€§ï¼Œä¸”åªå¸¦æ¥å¾®å°çš„FIDæŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8223982d0023638a0955b9026ee52d63~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030984&auth_key=1760030984-0-0-6b543f02531491e34e5804acf6d0b100&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71e1aa93fa0106dfdb55be1ecb1dfb32~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030991&auth_key=1760030991-0-0-8affee7143e4b5d992a8bbf8fe4f2dd4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-841e74619116d8671e9c18585437ad38~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030998&auth_key=1760030998-0-0-3cbf53469003f98d7408662b1f180b95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Dataset-Distillation-for-Super-Resolution-without-Class-Labels-and-Pre-trained-Models"><a href="#Dataset-Distillation-for-Super-Resolution-without-Class-Labels-and-Pre-trained-Models" class="headerlink" title="Dataset Distillation for Super-Resolution without Class Labels and   Pre-trained Models"></a>Dataset Distillation for Super-Resolution without Class Labels and   Pre-trained Models</h2><p><strong>Authors:Sunwoo Cho, Yejin Jung, Nam Ik Cho, Jae Woong Soh</strong></p>
<p>Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset. </p>
<blockquote>
<p>è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„è¦æ±‚è¶Šæ¥è¶Šé«˜ï¼Œéœ€è¦å¤§é‡çš„æ•°æ®é›†å’Œé‡è¦çš„è®¡ç®—èµ„æºï¼Œå°¤å…¶æ˜¯éšç€æ¨¡å‹å¤æ‚æ€§çš„æé«˜ã€‚æ—¨åœ¨æé«˜æ•°æ®æ•ˆç‡çš„æ•°æ®è’¸é¦æ–¹æ³•å·²æˆä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰é¢†åŸŸï¼Œå¯¹å¤§é‡è®­ç»ƒæ•°æ®é›†çš„ä¾èµ–çªæ˜¾äº†è¿™äº›æŠ€æœ¯çš„é‡è¦æ€§ã€‚æœ€è¿‘ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åè½¬çš„SRæ•°æ®è’¸é¦æ¡†æ¶ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ•°æ®åˆ©ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºé¢„è®­ç»ƒçš„SRç½‘ç»œå’Œç±»åˆ«ç‰¹å®šä¿¡æ¯ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„ç”¨äºå›¾åƒSRçš„æ•°æ®è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦ç±»æ ‡ç­¾æˆ–é¢„è®­ç»ƒçš„SRæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå–é«˜æ¢¯åº¦è¡¥ä¸å¹¶æ ¹æ®CLIPç‰¹å¾å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œç„¶åå¯¹æ‰€é€‰è¡¥ä¸å¾®è°ƒæ‰©æ•£æ¨¡å‹ä»¥å­¦ä¹ å…¶åˆ†å¸ƒå¹¶åˆæˆè’¸é¦çš„è®­ç»ƒå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°æœ€æ–°æ€§èƒ½çš„åŒæ—¶ï¼Œä½¿ç”¨æ›´å°‘çš„è®­ç»ƒæ•°æ®å’Œæ›´çŸ­çš„è®¡ç®—æ—¶é—´ã€‚å…·ä½“æ¥è¯´ï¼Œå½“æˆ‘ä»¬ä»…ä½¿ç”¨åŸå§‹æ•°æ®é›†çš„0.68%æ¥è®­ç»ƒSRçš„åŸºçº¿Transformeræ¨¡å‹æ—¶ï¼Œæ€§èƒ½ä¸‹é™ä»…ä¸º0.3åˆ†è´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰©æ•£æ¨¡å‹çš„å¾®è°ƒéœ€è¦4å°æ—¶ï¼ŒSRæ¨¡å‹çš„è®­ç»ƒåœ¨1å°æ—¶å†…å®Œæˆï¼Œè¿œè¿œçŸ­äºä½¿ç”¨å®Œæ•´æ•°æ®é›†çš„11å°æ—¶è®­ç»ƒæ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14777v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ ç½‘ç»œè®­ç»ƒå¯¹å¤§æ•°æ®é›†å’Œè®¡ç®—èµ„æºçš„éœ€æ±‚æ—¥ç›Šå¢åŠ ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡å‹å¤æ‚æ€§æå‡çš„èƒŒæ™¯ä¸‹ã€‚æ•°æ®è’¸é¦æ–¹æ³•æ—¨åœ¨æé«˜æ•°æ®æ•ˆç‡ï¼Œæˆä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰é¢†åŸŸï¼Œå¯¹å¤§é‡è®­ç»ƒæ•°æ®é›†çš„ä¾èµ–å‡¸æ˜¾äº†è¿™äº›æŠ€æœ¯çš„é‡è¦æ€§ã€‚æœ€è¿‘æå‡ºäº†åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åæ¼”çš„SRæ•°æ®è’¸é¦æ¡†æ¶ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ•°æ®åˆ©ç”¨æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºé¢„è®­ç»ƒçš„SRç½‘ç»œå’Œç±»åˆ«ç‰¹å®šä¿¡æ¯ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒSRæ•°æ®è’¸é¦æ–¹æ³•ï¼Œæ— éœ€ç±»åˆ«æ ‡ç­¾æˆ–é¢„è®­ç»ƒçš„SRæ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆå°†é«˜æ¢¯åº¦è¡¥ä¸æå–å¹¶åˆ†ç±»å›¾åƒåŸºäºCLIPç‰¹å¾ï¼Œç„¶ååœ¨é€‰å®šåŒºåŸŸå¾®è°ƒæ‰©æ•£æ¨¡å‹ä»¥å­¦ä¹ å…¶åˆ†å¸ƒå¹¶åˆæˆè’¸é¦çš„è®­ç»ƒå›¾åƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶ä½¿ç”¨æ›´å°‘çš„è®­ç»ƒæ•°æ®å’Œæ›´çŸ­çš„è®¡ç®—æ—¶é—´ã€‚å…·ä½“æ¥è¯´ï¼Œå½“æˆ‘ä»¬ä»…ä½¿ç”¨åŸå§‹æ•°æ®é›†ä¸­çš„0.68%è®­ç»ƒSRåŸºçº¿Transformeræ¨¡å‹æ—¶ï¼Œæ€§èƒ½ä¸‹é™ä»…ä¸º0.3åˆ†è´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰©æ•£æ¨¡å‹çš„å¾®è°ƒéœ€è¦4å°æ—¶ï¼Œè€ŒSRæ¨¡å‹çš„è®­ç»ƒåœ¨1å°æ—¶å†…å®Œæˆï¼Œè¿œçŸ­äºä½¿ç”¨æ•´ä¸ªæ•°æ®é›†æ—¶çš„è®­ç»ƒæ—¶é—´ï¼ˆéœ€æ—¶11å°æ—¶ï¼‰ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®é›†å’Œè®¡ç®—èµ„æºã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæ•°æ®è’¸é¦æ–¹æ³•åº”è¿è€Œç”Ÿä»¥æé«˜æ•°æ®æ•ˆç‡ã€‚</li>
<li>åœ¨å•å›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸï¼Œæ•°æ®è’¸é¦æŠ€æœ¯å°¤ä¸ºé‡è¦ã€‚å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé¢„è®­ç»ƒçš„SRç½‘ç»œå’Œç±»åˆ«ç‰¹å®šä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒSRæ•°æ®è’¸é¦æ–¹æ³•ï¼Œæ— éœ€ç±»åˆ«æ ‡ç­¾æˆ–é¢„è®­ç»ƒçš„SRæ¨¡å‹ã€‚é€šè¿‡æå–é«˜æ¢¯åº¦è¡¥ä¸å¹¶åŸºäºCLIPç‰¹å¾åˆ†ç±»å›¾åƒè¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-95a5aa1e1edfd5853f63ac3ee73f3db7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031005&auth_key=1760031005-0-0-df348b3957bdb541d3c886f29120478a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9bec12f1d5ca4770a5f4b61146f72257~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031012&auth_key=1760031012-0-0-76ccd152ba2d0a793ebd1be9ddaf592d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c6137d2050dabf879722f20e08ae7c07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031019&auth_key=1760031019-0-0-aa55b3ee64f5947ac478eee2c31959c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d22b532bd39a34d514ec8a1d54eb4dfb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031026&auth_key=1760031026-0-0-a9f1c567dbbca80939c3c37d14a66291&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bb500596bf9716899626a01f467220a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031032&auth_key=1760031032-0-0-c9c29291263053e07a563a1cfe41946c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DICE-Diffusion-Consensus-Equilibrium-for-Sparse-view-CT-Reconstruction"><a href="#DICE-Diffusion-Consensus-Equilibrium-for-Sparse-view-CT-Reconstruction" class="headerlink" title="DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction"></a>DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction</h2><p><strong>Authors:Leon Suarez-Rodriguez, Roman Jacome, Romario Gualdron-Hurtado, Ana Mantilla-Dulcey, Henry Arguello</strong></p>
<p>Sparse-view computed tomography (CT) reconstruction is fundamentally challenging due to undersampling, leading to an ill-posed inverse problem. Traditional iterative methods incorporate handcrafted or learned priors to regularize the solution but struggle to capture the complex structures present in medical images. In contrast, diffusion models (DMs) have recently emerged as powerful generative priors that can accurately model complex image distributions. In this work, we introduce Diffusion Consensus Equilibrium (DICE), a framework that integrates a two-agent consensus equilibrium into the sampling process of a DM. DICE alternates between: (i) a data-consistency agent, implemented through a proximal operator enforcing measurement consistency, and (ii) a prior agent, realized by a DM performing a clean image estimation at each sampling step. By balancing these two complementary agents iteratively, DICE effectively combines strong generative prior capabilities with measurement consistency. Experimental results show that DICE significantly outperforms state-of-the-art baselines in reconstructing high-quality CT images under uniform and non-uniform sparse-view settings of 15, 30, and 60 views (out of a total of 180), demonstrating both its effectiveness and robustness. </p>
<blockquote>
<p>ç¨€ç–è§†å›¾è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰é‡å»ºé¢ä¸´æ ¹æœ¬æŒ‘æˆ˜ï¼Œå› ä¸ºæ¬ é‡‡æ ·å¯¼è‡´åé—®é¢˜ä¸é€‚å®šã€‚ä¼ ç»Ÿè¿­ä»£æ–¹æ³•ç»“åˆæ‰‹å·¥æˆ–å­¦ä¹ å…ˆéªŒæ¥æ­£åˆ™åŒ–è§£å†³æ–¹æ¡ˆï¼Œä½†éš¾ä»¥æ•æ‰åŒ»ç–—å›¾åƒä¸­å­˜åœ¨çš„å¤æ‚ç»“æ„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æœ€è¿‘ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒå‡ºç°ï¼Œå¯ä»¥å‡†ç¡®åœ°æ¨¡æ‹Ÿå¤æ‚çš„å›¾åƒåˆ†å¸ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ‰©æ•£å…±è¯†å¹³è¡¡ï¼ˆDICEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†ä¸¤æ™ºèƒ½ä½“å…±è¯†å¹³è¡¡é›†æˆåˆ°æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ä¸­çš„æ¡†æ¶ã€‚DICEåœ¨ä»¥ä¸‹ä¸¤ä¸ªæ™ºèƒ½ä½“ä¹‹é—´äº¤æ›¿è¿›è¡Œï¼šï¼ˆiï¼‰æ•°æ®ä¸€è‡´æ€§æ™ºèƒ½ä½“ï¼Œé€šè¿‡è¿‘ç«¯ç®—å­å¼ºåˆ¶å®æ–½æµ‹é‡ä¸€è‡´æ€§æ¥å®ç°ï¼›ï¼ˆiiï¼‰å…ˆéªŒæ™ºèƒ½ä½“ï¼Œé€šè¿‡æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­æ‰§è¡Œæ¸…æ´å›¾åƒä¼°è®¡çš„æ‰©æ•£æ¨¡å‹å®ç°ã€‚é€šè¿‡è¿­ä»£å¹³è¡¡è¿™ä¸¤ä¸ªäº’è¡¥çš„æ™ºèƒ½ä½“ï¼ŒDICEæœ‰æ•ˆåœ°ç»“åˆäº†å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒèƒ½åŠ›å’Œæµ‹é‡ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ç»Ÿä¸€å’Œéç»Ÿä¸€çš„ç¨€ç–è§†å›¾è®¾ç½®ä¸‹ï¼ŒDICEåœ¨é‡å»ºé«˜è´¨é‡CTå›¾åƒæ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œå³åœ¨æ€»å…±180ä¸ªè§†å›¾ä¸­ä»…ä½¿ç”¨15ã€30å’Œ60ä¸ªè§†å›¾ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14566v1">PDF</a> 8 pages, 4 figures, confenrence</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDiffusion Consensus Equilibriumï¼ˆDICEï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„å¼ºå¤§ç”Ÿæˆå…ˆéªŒå’Œæµ‹é‡ä¸€è‡´æ€§ã€‚DICEé€šè¿‡åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­é‡‡ç”¨åŒæ™ºèƒ½ä½“å…±è¯†å‡è¡¡æœºåˆ¶ï¼Œå®ç°äº†æ•°æ®ä¸€è‡´æ€§æ™ºèƒ½ä½“å’Œå…ˆéªŒæ™ºèƒ½ä½“çš„äº¤æ›¿ä½œç”¨ã€‚è¿™ç§æ–¹æ³•åœ¨ç¨€ç–è§†å›¾è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰é‡å»ºä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œèƒ½å¤Ÿåœ¨å‡åŒ€å’Œéå‡åŒ€ç¨€ç–è§†å›¾è®¾ç½®ä¸‹é‡å»ºé«˜è´¨é‡CTå›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°æ¨¡æ‹Ÿå¤æ‚çš„å›¾åƒåˆ†å¸ƒã€‚</li>
<li>DICEæ¡†æ¶ç»“åˆäº†æ•°æ®ä¸€è‡´æ€§æ™ºèƒ½ä½“å’Œå…ˆéªŒæ™ºèƒ½ä½“ï¼Œé€šè¿‡è¿­ä»£å¹³è¡¡ä¸¤è€…æ¥å®ç°é«˜æ•ˆçš„CTå›¾åƒé‡å»ºã€‚</li>
<li>DICEæ¡†æ¶é€šè¿‡é‡‡ç”¨åŒæ™ºèƒ½ä½“å…±è¯†å‡è¡¡æœºåˆ¶ï¼Œåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­å®ç°äº†å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒä¸æµ‹é‡ä¸€è‡´æ€§ç»“åˆã€‚</li>
<li>DICEåœ¨ç¨€ç–è§†å›¾CTé‡å»ºä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒè§†å›¾è®¾ç½®ä¸‹é‡å»ºé«˜è´¨é‡å›¾åƒã€‚</li>
<li>DICEæ¡†æ¶åœ¨å‡åŒ€å’Œéå‡åŒ€ç¨€ç–è§†å›¾è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>ä¼ ç»Ÿè¿­ä»£æ–¹æ³•åœ¨æ‰‹å·¥è‰ºæˆ–å­¦ä¹ å…ˆéªŒæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œæ— æ³•å®Œå…¨æ•æ‰åŒ»ç–—å›¾åƒçš„å¤æ‚ç»“æ„ï¼Œè€ŒDICEæ¡†æ¶èƒ½å¤Ÿå…‹æœè¿™äº›æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d3adb423d54538b23cc0682885ab8782~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031040&auth_key=1760031040-0-0-e14312b63a9ef6cc28789daa0690b46f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-71bb183fbfc59dbcc0d5a4b840cdfc03~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031047&auth_key=1760031047-0-0-803ea714543de6265781196f7ed68fac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-906f7a8898badb45208b45e8b63b80c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031054&auth_key=1760031054-0-0-3fb772fd8e52f7e0ac4600dc37d44f38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14d22ae476e583d123c728cfb565d507~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031061&auth_key=1760031061-0-0-468a7770eae9e7b10dbbc2300a103910&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f498f8c04d9a24aac027ed34a7482e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031068&auth_key=1760031068-0-0-c6cd70f3571cb1c5e987e389599f9a35&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5eb7fc7256b42dcd3bfff81dba874c60~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031075&auth_key=1760031075-0-0-e7441a823f50c7c836671864ef78410d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiffVL-Diffusion-Based-Visual-Localization-on-2D-Maps-via-BEV-Conditioned-GPS-Denoising"><a href="#DiffVL-Diffusion-Based-Visual-Localization-on-2D-Maps-via-BEV-Conditioned-GPS-Denoising" class="headerlink" title="DiffVL: Diffusion-Based Visual Localization on 2D Maps via   BEV-Conditioned GPS Denoising"></a>DiffVL: Diffusion-Based Visual Localization on 2D Maps via   BEV-Conditioned GPS Denoising</h2><p><strong>Authors:Li Gao, Hongyang Sun, Liu Liu, Yunhao Li, Yang Cai</strong></p>
<p>Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Birdâ€™s-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods. </p>
<blockquote>
<p>ç²¾ç¡®è§†è§‰å®šä½å¯¹äºè‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸€ä¸ªåŸºæœ¬å›°å¢ƒï¼šé«˜æ¸…åœ°å›¾è™½ç„¶æä¾›äº†é«˜ç²¾åº¦çš„å®šä½å‚è€ƒï¼Œä½†å…¶é«˜æ˜‚çš„æ„å»ºå’Œç»´æŠ¤æˆæœ¬é˜»ç¢äº†å¯æ‰©å±•æ€§ï¼Œè¿™ä¿ƒä½¿ç ”ç©¶è½¬å‘æ ‡å‡†å®šä¹‰åœ°å›¾ï¼ˆå¦‚OpenStreetMapï¼‰ã€‚å½“å‰åŸºäºSDåœ°å›¾çš„æ–¹æ³•ä¸»è¦å…³æ³¨å›¾åƒå’Œåœ°å›¾ä¹‹é—´çš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰åŒ¹é…ï¼Œå¿½è§†äº†æ™®éå­˜åœ¨çš„ä¿¡å·å™ªå£°GPSã€‚å°½ç®¡GPSæ˜“äºè·å–ï¼Œä½†åœ¨åŸå¸‚ç¯å¢ƒä¸­å®ƒå—åˆ°å¤šå¾„è¯¯å·®çš„å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†DiffVLæ¡†æ¶ï¼Œå®ƒæ˜¯é¦–ä¸ªå°†è§†è§‰å®šä½é‡æ–°è¡¨è¿°ä¸ºä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„GPSå»å™ªä»»åŠ¡çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå½“åœ¨æœ‰è§†è§‰BEVç‰¹å¾å’ŒSDåœ°å›¾çš„æ¡ä»¶ä¸‹ï¼Œå™ªå£°GPSè½¨è¿¹éšå«åœ°ç¼–ç äº†çœŸå®çš„å§¿æ€åˆ†å¸ƒï¼Œè¿™å¯ä»¥é€šè¿‡è¿­ä»£æ‰©æ•£ç»†åŒ–æ¥æ¢å¤ã€‚DiffVLä¸åŒäºå…ˆå‰çš„BEVåŒ¹é…æ–¹æ³•ï¼ˆä¾‹å¦‚OrienterNetï¼‰æˆ–åŸºäºå˜å‹å™¨çš„æ³¨å†Œæ–¹æ³•ï¼Œå®ƒé€šè¿‡è”åˆå»ºæ¨¡GPSã€SDåœ°å›¾å’Œè§†è§‰ä¿¡å·ï¼Œå­¦ä¹ åå‘GPSå™ªå£°æ‰°åŠ¨ï¼Œåœ¨ä¸ä¾èµ–é«˜æ¸…åœ°å›¾çš„æƒ…å†µä¸‹å®ç°äºšç±³çº§ç²¾åº¦ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¸BEVåŒ¹é…åŸºçº¿ç›¸æ¯”çš„é¢†å…ˆæ°´å¹³ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†æ‰©æ•£æ¨¡å‹å¯ä»¥é€šè¿‡å°†å™ªå£°GPSè§†ä¸ºç”Ÿæˆå…ˆéªŒï¼Œå®ç°ä¸ä¼ ç»Ÿçš„åŸºäºåŒ¹é…çš„æ–¹æ³•çš„æ¨¡å¼è½¬å˜ï¼Œä»è€Œå®ç°å¯æ‰©å±•çš„å®šä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†è§‰å®šä½æ–°æ–¹æ³•ï¼Œé‡‡ç”¨GPSå»å™ªæ–¹å¼å®ç°å‡†ç¡®è§†è§‰å®šä½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ ‡å‡†åœ°å›¾å’Œé¸Ÿç°è§†å›¾ç‰¹å¾ï¼Œé€šè¿‡è¿­ä»£æ‰©æ•£ä¼˜åŒ–ï¼Œä»å™ªå£°GPSè½¨è¿¹ä¸­æ¢å¤çœŸå®å§¿æ€åˆ†å¸ƒï¼Œå®ç°äºšç±³çº§å®šä½ç²¾åº¦ï¼Œæ— éœ€ä¾èµ–é«˜ç²¾åº¦åœ°å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è‡ªä¸»é©¾é©¶è§†è§‰å®šä½æ–¹æ³•ä¸»è¦ä¾èµ–é«˜ç²¾åº¦åœ°å›¾ï¼Œä½†å…¶é«˜æ˜‚çš„æ„å»ºå’Œç»´æŠ¤æˆæœ¬é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚</li>
<li>æå‡ºçš„DiffVLæ¡†æ¶å°†è§†è§‰å®šä½é‡æ–°å®šä¹‰ä¸ºGPSå»å™ªä»»åŠ¡ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚</li>
<li>DiffVLåˆ©ç”¨å™ªå£°GPSè½¨è¿¹ã€é¸Ÿç°è§†å›¾ç‰¹å¾å’Œæ ‡å‡†åœ°å›¾ï¼Œé€šè¿‡è¿­ä»£æ‰©æ•£ä¼˜åŒ–ï¼Œéšå¼ç¼–ç çœŸå®å§¿æ€åˆ†å¸ƒã€‚</li>
<li>DiffVLä¸åŒäºä»¥å¾€çš„é¸Ÿç°è§†å›¾åŒ¹é…æ–¹æ³•æˆ–åŸºäºå˜å‹å™¨çš„æ³¨å†Œæ–¹æ³•ã€‚</li>
<li>DiffVLå­¦ä¼šåå‘æ¢å¤GPSå™ªå£°æ‰°åŠ¨ï¼Œé€šè¿‡è”åˆå»ºæ¨¡GPSã€æ ‡å‡†åœ°å›¾å’Œè§†è§‰ä¿¡å·ï¼Œå®ç°äºšç±³çº§å®šä½ç²¾åº¦ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDiffVLåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å®šä½ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-48da142296e8ed0358f432a2e1313640~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031084&auth_key=1760031084-0-0-ee7c0af435209c217cf975afe2baf27f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-741ce8c769ae34a17bdb4608b58549ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031091&auth_key=1760031091-0-0-74678ecc449f416dbe5add3321923e3c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e42aa573936cd411bed9f92a1669ed0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031098&auth_key=1760031098-0-0-ce5aca68c72f04b8541eaf6e6e11fc38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67098d89d2cede73749b941db21f4102~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031148&auth_key=1760031148-0-0-03eca0d7baa111ddc90508c36abd9ece&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27d195e09ff24a4cfe9e42890183818c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031154&auth_key=1760031154-0-0-1a8ea2c1d952709db193ed124bd350a4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ba322526e93c29cadeea0568dcd7c28~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031160&auth_key=1760031160-0-0-32f771120453c7869d65dbac43498200&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TIDE-Achieving-Balanced-Subject-Driven-Image-Generation-via-Target-Instructed-Diffusion-Enhancement"><a href="#TIDE-Achieving-Balanced-Subject-Driven-Image-Generation-via-Target-Instructed-Diffusion-Enhancement" class="headerlink" title="TIDE: Achieving Balanced Subject-Driven Image Generation via   Target-Instructed Diffusion Enhancement"></a>TIDE: Achieving Balanced Subject-Driven Image Generation via   Target-Instructed Diffusion Enhancement</h2><p><strong>Authors:Jibai Lin, Bo Ma, Yating Yang, Xi Zhou, Rong Ma, Turghun Osman, Ahtamjan Ahmat, Rui Dong, Lei Wang</strong></p>
<p>Subject-driven image generation (SDIG) aims to manipulate specific subjects within images while adhering to textual instructions, a task crucial for advancing text-to-image diffusion models. SDIG requires reconciling the tension between maintaining subject identity and complying with dynamic edit instructions, a challenge inadequately addressed by existing methods. In this paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework, which resolves this tension through target supervision and preference learning without test-time fine-tuning. TIDE pioneers target-supervised triplet alignment, modelling subject adaptation dynamics using a (reference image, instruction, target images) triplet. This approach leverages the Direct Subject Diffusion (DSD) objective, training the model with paired â€œwinningâ€ (balanced preservation-compliance) and â€œlosingâ€ (distorted) targets, systematically generated and evaluated via quantitative metrics. This enables implicit reward modelling for optimal preservation-compliance balance. Experimental results on standard benchmarks demonstrate TIDEâ€™s superior performance in generating subject-faithful outputs while maintaining instruction compliance, outperforming baseline methods across multiple quantitative metrics. TIDEâ€™s versatility is further evidenced by its successful application to diverse tasks, including structural-conditioned generation, image-to-image generation, and text-image interpolation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/KomJay520/TIDE">https://github.com/KomJay520/TIDE</a>. </p>
<blockquote>
<p>ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆï¼ˆSDIGï¼‰æ—¨åœ¨æ ¹æ®æ–‡æœ¬æŒ‡ä»¤æ“ä½œå›¾åƒä¸­çš„ç‰¹å®šä¸»é¢˜ï¼Œè¿™å¯¹äºæ¨è¿›æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ã€‚SDIGéœ€è¦åœ¨ä¿æŒä¸»é¢˜èº«ä»½å’Œéµå®ˆåŠ¨æ€ç¼–è¾‘æŒ‡ä»¤ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œç°æœ‰æ–¹æ³•å¯¹æ­¤æŒ‘æˆ˜è§£å†³ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç›®æ ‡æŒ‡å¯¼æ‰©æ•£å¢å¼ºï¼ˆTIDEï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç›®æ ‡ç›‘ç£å’Œåå¥½å­¦ä¹ è§£å†³è¿™ä¸€å¹³è¡¡é—®é¢˜ï¼Œè€Œæ— éœ€è¿›è¡Œæµ‹è¯•æ—¶å¾®è°ƒã€‚TIDEé¦–åˆ›ç›®æ ‡ç›‘ç£ä¸‰å…ƒç»„å¯¹é½ï¼Œä½¿ç”¨ï¼ˆå‚è€ƒå›¾åƒã€æŒ‡ä»¤ã€ç›®æ ‡å›¾åƒï¼‰ä¸‰å…ƒç»„å¯¹ä¸»é¢˜é€‚åº”åŠ¨æ€è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç›´æ¥ä¸»é¢˜æ‰©æ•£ï¼ˆDSDï¼‰ç›®æ ‡ï¼Œç”¨é…å¯¹çš„â€œè·èƒœâ€ï¼ˆå¹³è¡¡ä¿ç•™å’Œåˆè§„æ€§ï¼‰å’Œâ€œå¤±è´¥â€ï¼ˆå¤±çœŸï¼‰ç›®æ ‡è®­ç»ƒæ¨¡å‹ï¼Œè¿™äº›ç›®æ ‡é€šè¿‡å®šé‡æŒ‡æ ‡è¿›è¡Œç³»ç»ŸåŒ–ç”Ÿæˆå’Œè¯„ä¼°ã€‚è¿™ä¸ºå®ç°æœ€ä¼˜ä¿ç•™åˆè§„æ€§å¹³è¡¡éšå¼å¥–åŠ±å»ºæ¨¡æä¾›äº†å¯èƒ½ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTIDEåœ¨ç”Ÿæˆå¿ å®äºä¸»é¢˜çš„è¾“å‡ºæ—¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæŒ‡ä»¤åˆè§„æ€§ï¼Œåœ¨å¤šä¸ªå®šé‡æŒ‡æ ‡ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚TIDEçš„é€šç”¨æ€§è¿›ä¸€æ­¥ä½“ç°åœ¨å…¶åœ¨ç»“æ„æ¡ä»¶ç”Ÿæˆã€å›¾åƒåˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬å›¾åƒæ’å€¼ç­‰å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„æˆåŠŸåº”ç”¨ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KomJay520/TIDE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KomJay520/TIDEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06499v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬æŒ‡ä»¤é©±åŠ¨å›¾åƒç”Ÿæˆçš„ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTIDEï¼ˆç›®æ ‡æŒ‡å¯¼æ‰©æ•£å¢å¼ºï¼‰çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶è§£å†³äº†åœ¨ç»´æŒä¸»é¢˜èº«ä»½ä¸éµå¾ªåŠ¨æ€ç¼–è¾‘æŒ‡ä»¤ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡ç›®æ ‡ç›‘ç£ä¸åå¥½å­¦ä¹ ï¼Œæ— éœ€æµ‹è¯•æ—¶å¾®è°ƒå³å¯å®ç°ã€‚TIDEé‡‡ç”¨ç›®æ ‡ç›‘ç£çš„ä¸‰é‡å¯¹å‡†æŠ€æœ¯ï¼Œåˆ©ç”¨ï¼ˆå‚è€ƒå›¾åƒã€æŒ‡ä»¤ã€ç›®æ ‡å›¾åƒï¼‰ä¸‰é‡ç»“æ„å»ºæ¨¡ä¸»é¢˜é€‚åº”åŠ¨æ€ã€‚é€šè¿‡Direct Subject Diffusionï¼ˆDSDï¼‰ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨ç³»ç»Ÿç”Ÿæˆçš„é…å¯¹çš„â€œèƒœå‡ºâ€ï¼ˆå¹³è¡¡ä¿ç•™åˆè§„æ€§ï¼‰å’Œâ€œå¤±è´¥â€ï¼ˆå¤±çœŸï¼‰ç›®æ ‡ï¼Œå¹¶é€šè¿‡å®šé‡æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTIDEåœ¨ç”Ÿæˆä¸»é¢˜å¿ å®è¾“å‡ºåŒæ—¶ç»´æŒæŒ‡ä»¤åˆè§„æ€§çš„ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä¼˜äºåŸºå‡†æ–¹æ³•ã€‚TIDEçš„é€šç”¨æ€§è¿˜ä½“ç°åœ¨å…¶åœ¨ç»“æ„æ¡ä»¶ç”Ÿæˆã€å›¾åƒåˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬å›¾åƒæ’å€¼ç­‰å¤šæ ·åŒ–ä»»åŠ¡çš„æˆåŠŸåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIDEæ¡†æ¶è§£å†³äº†æ–‡æœ¬æŒ‡ä»¤é©±åŠ¨å›¾åƒç”Ÿæˆä¸­ç»´æŒä¸»é¢˜èº«ä»½ä¸éµå¾ªç¼–è¾‘æŒ‡ä»¤ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚</li>
<li>TIDEé€šè¿‡ç›®æ ‡ç›‘ç£ä¸åå¥½å­¦ä¹ å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæ— éœ€æµ‹è¯•æ—¶å¾®è°ƒã€‚</li>
<li>TIDEé‡‡ç”¨ç›®æ ‡ç›‘ç£çš„ä¸‰é‡å¯¹å‡†æŠ€æœ¯ï¼Œåˆ©ç”¨ï¼ˆå‚è€ƒå›¾åƒã€æŒ‡ä»¤ã€ç›®æ ‡å›¾åƒï¼‰ä¸‰é‡ç»“æ„å»ºæ¨¡ä¸»é¢˜é€‚åº”åŠ¨æ€ã€‚</li>
<li>é€šè¿‡Direct Subject Diffusionï¼ˆDSDï¼‰ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨é…å¯¹çš„â€œèƒœå‡ºâ€å’Œâ€œå¤±è´¥â€ç›®æ ‡è¿›è¡Œç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>TIDEåœ¨å¤šç§å®šé‡æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œèƒ½ç”Ÿæˆä¸»é¢˜å¿ å®çš„è¾“å‡ºå¹¶ç»´æŒæŒ‡ä»¤åˆè§„æ€§ã€‚</li>
<li>TIDEæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼ŒæˆåŠŸåº”ç”¨äºç»“æ„æ¡ä»¶ç”Ÿæˆã€å›¾åƒåˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬å›¾åƒæ’å€¼ç­‰å¤šæ ·åŒ–ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b698d647d6dc96c7bfe4276fc355da19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031168&auth_key=1760031168-0-0-38a7f8bb5dec079e140bf13484cdcc51&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98381ae38909c2778d2e7c6f1b553a68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031175&auth_key=1760031175-0-0-cfdb8a12195c78f3252995c82d21fb36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-acb18ee69b3d68a29d12abbf27c31b8c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031182&auth_key=1760031182-0-0-d711c91bbc96aec9efc80f07d743431d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb794b77b221060b1ddf32f16be8145c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031189&auth_key=1760031189-0-0-d301f5904a31bcc5438a3a48fadf9810&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1650c731f8b7315f3f6a588f949be7c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031196&auth_key=1760031196-0-0-1a63351888c6fcc474d2c330f5c09173&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models"><a href="#Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models" class="headerlink" title="Probing the Representational Power of Sparse Autoencoders in Vision   Models"></a>Probing the Representational Power of Sparse Autoencoders in Vision   Models</h2><p><strong>Authors:Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng</strong></p>
<p>Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting the hidden states of large language models (LLMs). By learning to reconstruct activations from a sparse bottleneck layer, SAEs discover interpretable features from the high-dimensional internal representations of LLMs. Despite their popularity with language models, SAEs remain understudied in the visual domain. In this work, we provide an extensive evaluation the representational power of SAEs for vision models using a broad range of image-based tasks. Our experimental results demonstrate that SAE features are semantically meaningful, improve out-of-distribution generalization, and enable controllable generation across three vision model architectures: vision embedding models, multi-modal LMMs and diffusion models. In vision embedding models, we find that learned SAE features can be used for OOD detection and provide evidence that they recover the ontological structure of the underlying model. For diffusion models, we demonstrate that SAEs enable semantic steering through text encoder manipulation and develop an automated pipeline for discovering human-interpretable attributes. Finally, we conduct exploratory experiments on multi-modal LLMs, finding evidence that SAE features reveal shared representations across vision and language modalities. Our study provides a foundation for SAE evaluation in vision models, highlighting their strong potential improving interpretability, generalization, and steerability in the visual domain. </p>
<blockquote>
<p>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencodersï¼Œç®€ç§°SAEï¼‰å·²ç»æˆä¸ºè§£è¯»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éšè—çŠ¶æ€çš„ä¸€ç§æµè¡Œå·¥å…·ã€‚é€šè¿‡å­¦ä¼šä»ç¨€ç–ç“¶é¢ˆå±‚é‡å»ºæ¿€æ´»ï¼ŒSAEä»LLMçš„é«˜ç»´å†…éƒ¨è¡¨ç¤ºä¸­å‘ç°å¯è§£é‡Šçš„ç‰¹å¾ã€‚å°½ç®¡SAEåœ¨è¯­è¨€æ¨¡å‹ä¸­å¾ˆå—æ¬¢è¿ï¼Œä½†åœ¨è§†è§‰é¢†åŸŸï¼Œå¯¹å®ƒä»¬çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ä¸€ç³»åˆ—å›¾åƒä»»åŠ¡çš„å¤§é‡è¯„ä¼°ï¼Œå…¨é¢è¯„ä¼°äº†SAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAEç‰¹å¾åœ¨è¯­ä¹‰ä¸Šæ˜¯æ˜ç¡®çš„ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹å¯¹åˆ†å¸ƒå¤–çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½å®ç°åœ¨ä¸‰ç§è§†è§‰æ¨¡å‹æ¶æ„ä¸‹çš„å¯æ§ç”Ÿæˆï¼šè§†è§‰åµŒå…¥æ¨¡å‹ã€å¤šæ¨¡æ€LLMå’Œæ‰©æ•£æ¨¡å‹ã€‚åœ¨è§†è§‰åµŒå…¥æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°å­¦ä¹ åˆ°çš„SAEç‰¹å¾å¯ç”¨äºOODæ£€æµ‹ï¼Œå¹¶æä¾›è¯æ®è¡¨æ˜å®ƒä»¬æ¢å¤äº†åº•å±‚æ¨¡å‹çš„æœ¬ä½“ç»“æ„ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†SAEé€šè¿‡æ–‡æœ¬ç¼–ç å™¨æ“æ§æ¥å®ç°è¯­ä¹‰å¼•å¯¼çš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æµç¨‹æ¥å‘ç°äººç±»å¯è§£é‡Šçš„å±æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹å¤šæ¨¡æ€LLMè¿›è¡Œäº†æ¢ç´¢æ€§å®éªŒï¼Œå‘ç°è¯æ®è¡¨æ˜SAEç‰¹å¾æ­ç¤ºäº†è·¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å…±äº«è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºSAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„è¯„ä¼°å¥ å®šäº†åŸºç¡€ï¼Œçªæ˜¾äº†å®ƒä»¬åœ¨æé«˜è§†è§‰é¢†åŸŸçš„å¯è§£é‡Šæ€§ã€æ³›åŒ–èƒ½åŠ›å’Œæ“æ§èƒ½åŠ›æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11277v2">PDF</a> ICCV 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰åœ¨è§£è¯»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éšè—çŠ¶æ€æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚æœ¬ç ”ç©¶é€šè¿‡å¯¹è§†è§‰æ¨¡å‹è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œè¯æ˜äº†SAEåœ¨è§†è§‰é¢†åŸŸçš„è¡¨ç°èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAEç‰¹å¾è¯­ä¹‰ä¸°å¯Œï¼Œèƒ½æé«˜æ¨¡å‹åœ¨åˆ†å¸ƒå¤–çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸‰ç§è§†è§‰æ¨¡å‹æ¶æ„ä¸­å®ç°å¯æ§ç”Ÿæˆã€‚æ­¤å¤–ï¼ŒSAEç‰¹å¾è¿˜å¯ç”¨äºå›¾åƒåµŒå…¥æ¨¡å‹çš„OODæ£€æµ‹ï¼Œå¹¶åœ¨æ‰©æ•£æ¨¡å‹ä¸­å®ç°è¯­ä¹‰å¯¼å‘ã€‚æœ¬ç ”ç©¶ä¸ºSAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„åº”ç”¨æä¾›äº†åŸºç¡€ï¼Œå±•ç°äº†å…¶åœ¨æé«˜è§£é‡Šæ€§ã€æ³›åŒ–èƒ½åŠ›å’Œå¯æ§æ€§æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAEsç”¨äºè§£è¯»å¤§å‹è¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€ã€‚</li>
<li>SAEåœ¨è§†è§‰é¢†åŸŸè¡¨ç°èƒ½åŠ›å¾—åˆ°å¹¿æ³›è¯„ä¼°ã€‚</li>
<li>SAEç‰¹å¾è¯­ä¹‰ä¸°å¯Œï¼Œèƒ½æé«˜æ¨¡å‹åœ¨åˆ†å¸ƒå¤–çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SAEåœ¨ä¸‰ç§è§†è§‰æ¨¡å‹æ¶æ„ä¸­å®ç°å¯æ§ç”Ÿæˆã€‚</li>
<li>SAEç‰¹å¾å¯ç”¨äºå›¾åƒåµŒå…¥æ¨¡å‹çš„OODæ£€æµ‹ã€‚</li>
<li>åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒSAEå®ç°è¯­ä¹‰å¯¼å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-17a3bc4e3f3c5292a225aab902b3db42~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031204&auth_key=1760031204-0-0-0341bd62f682c8d8a3fb615ab114df6a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9dc7f6230aef793f5188cc1f68aa0339~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031211&auth_key=1760031211-0-0-5eac8fced3caaa3f6027a92f9cf10608&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a43df17e27a99dd99a7d20823f8fa45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031218&auth_key=1760031218-0-0-44cfecc341ff7922d07d89fe2011a6c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe3261aaac461393420903ec09a57edd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031225&auth_key=1760031225-0-0-aebe25eea5902b30cc7c66ea54a12f5f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Style-Transfer-with-Diffusion-Models-for-Synthetic-to-Real-Domain-Adaptation"><a href="#Style-Transfer-with-Diffusion-Models-for-Synthetic-to-Real-Domain-Adaptation" class="headerlink" title="Style Transfer with Diffusion Models for Synthetic-to-Real Domain   Adaptation"></a>Style Transfer with Diffusion Models for Synthetic-to-Real Domain   Adaptation</h2><p><strong>Authors:Estelle Chigot, Dennis G. Wilson, Meriem Ghrib, Thomas Oberlin</strong></p>
<p>Semantic segmentation models trained on synthetic data often perform poorly on real-world images due to domain gaps, particularly in adverse conditions where labeled data is scarce. Yet, recent foundation models enable to generate realistic images without any training. This paper proposes to leverage such diffusion models to improve the performance of vision models when learned on synthetic data. We introduce two novel techniques for semantically consistent style transfer using diffusion models: Class-wise Adaptive Instance Normalization and Cross-Attention (CACTI) and its extension with selective attention Filtering (CACTIF). CACTI applies statistical normalization selectively based on semantic classes, while CACTIF further filters cross-attention maps based on feature similarity, preventing artifacts in regions with weak cross-attention correspondences. Our methods transfer style characteristics while preserving semantic boundaries and structural coherence, unlike approaches that apply global transformations or generate content without constraints. Experiments using GTA5 as source and Cityscapes&#x2F;ACDC as target domains show that our approach produces higher quality images with lower FID scores and better content preservation. Our work demonstrates that class-aware diffusion-based style transfer effectively bridges the synthetic-to-real domain gap even with minimal target domain data, advancing robust perception systems for challenging real-world applications. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/echigot/cactif">https://github.com/echigot/cactif</a>. </p>
<blockquote>
<p>åŸºäºåˆæˆæ•°æ®è®­ç»ƒçš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸Šçš„è¡¨ç°å¾€å¾€è¾ƒå·®ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºé¢†åŸŸå·®è·é€ æˆçš„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡è®°æ•°æ®ç¨€ç¼ºçš„æ¶åŠ£æ¡ä»¶ä¸‹ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„åŸºç¡€æ¨¡å‹èƒ½å¤Ÿåœ¨æ— éœ€ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨æ­¤ç±»æ‰©æ•£æ¨¡å‹æ¥æé«˜åœ¨åˆæˆæ•°æ®ä¸Šå­¦ä¹ çš„è§†è§‰æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥ä¸¤ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè¯­ä¹‰ä¸€è‡´é£æ ¼è½¬ç§»çš„æ–°æŠ€æœ¯ï¼šåŸºäºç±»åˆ«çš„è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–å’Œäº¤å‰æ³¨æ„åŠ›ï¼ˆCACTIï¼‰ï¼Œä»¥åŠå…¶å¸¦æœ‰é€‰æ‹©æ€§æ³¨æ„åŠ›è¿‡æ»¤çš„æ‰©å±•ï¼ˆCACTIFï¼‰ã€‚CACTIæ ¹æ®è¯­ä¹‰ç±»åˆ«æœ‰é€‰æ‹©åœ°åº”ç”¨ç»Ÿè®¡å½’ä¸€åŒ–ï¼Œè€ŒCACTIFè¿›ä¸€æ­¥æ ¹æ®ç‰¹å¾ç›¸ä¼¼æ€§è¿‡æ»¤äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œé˜²æ­¢åœ¨äº¤å‰æ³¨æ„åŠ›å¯¹åº”å…³ç³»è¾ƒå¼±çš„åŒºåŸŸå‡ºç°ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è½¬ç§»é£æ ¼ç‰¹å¾çš„åŒæ—¶ï¼Œä¿ç•™äº†è¯­ä¹‰è¾¹ç•Œå’Œç»“æ„è¿è´¯æ€§ï¼Œä¸åŒäºåº”ç”¨å…¨å±€å˜æ¢æˆ–æ— æ¡ä»¶ç”Ÿæˆå†…å®¹çš„æ–¹æ³•ã€‚ä½¿ç”¨GTA5ä½œä¸ºæºåŸŸï¼ŒCityscapes&#x2F;ACDCä½œä¸ºç›®æ ‡åŸŸçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†è´¨é‡æ›´é«˜ã€FIDå¾—åˆ†æ›´ä½ã€å†…å®¹ä¿å­˜æ›´å¥½çš„å›¾åƒã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œç±»æ„ŸçŸ¥æ‰©æ•£é£æ ¼è½¬ç§»æœ‰æ•ˆåœ°ç¼©å°äº†åˆæˆåˆ°çœŸå®çš„é¢†åŸŸå·®è·ï¼Œå³ä½¿ç›®æ ‡é¢†åŸŸçš„æ•°æ®æœ€å°‘ï¼Œä¹Ÿä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œåº”ç”¨æä¾›äº†ç¨³å¥çš„æ„ŸçŸ¥ç³»ç»Ÿã€‚æºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/echigot/cactif%E3%80%82">https://github.com/echigot/cactifã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16360v2">PDF</a> Published in Computer Vision and Image Understanding, September 2025   (CVIU 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ”¹å–„åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸­çš„è¡¨ç°ã€‚é’ˆå¯¹åˆæˆæ•°æ®ä¸å®é™…åœºæ™¯ä¹‹é—´çš„åŸŸå·®å¼‚é—®é¢˜ï¼Œæ–‡ç« å¼•å…¥ä¸¤ç§æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯ï¼šClass-wise Adaptive Instance Normalizationä¸Cross-Attentionç»“åˆçš„æ–¹æ³•ï¼ˆCACTIï¼‰åŠå…¶é€šè¿‡é€‰æ‹©æ€§æ³¨æ„åŠ›è¿‡æ»¤çš„æ‰©å±•ç‰ˆæœ¬ï¼ˆCACTIFï¼‰ã€‚CACTIæ ¹æ®è¯­ä¹‰ç±»åˆ«é€‰æ‹©æ€§åº”ç”¨ç»Ÿè®¡å½’ä¸€åŒ–ï¼Œè€ŒCACTIFåˆ™è¿›ä¸€æ­¥æ ¹æ®ç‰¹å¾ç›¸ä¼¼æ€§è¿‡æ»¤äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œé¿å…äº†å¼±äº¤å‰æ³¨æ„åŠ›å¯¹åº”åŒºåŸŸçš„ä¼ªå½±ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é£æ ¼è½¬ç§»æ—¶èƒ½å¤Ÿä¿ç•™è¯­ä¹‰è¾¹ç•Œå’Œç»“æ„è¿è´¯æ€§ï¼Œä½¿ç”¨GTA5ä½œä¸ºæºåŸŸã€Cityscapes&#x2F;ACDCä½œä¸ºç›®æ ‡åŸŸçš„å®éªŒç»“æœè¯æ˜äº†å…¶ç”Ÿæˆå›¾åƒè´¨é‡æ›´é«˜ã€FIDåˆ†æ•°æ›´ä½ã€å†…å®¹ä¿ç•™æ›´å¥½ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†åŸºäºç±»åˆ«çš„æ‰©æ•£æ¨¡å‹åœ¨æ¡¥æ¢åˆæˆåˆ°çœŸå®åŸŸå·®è·æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå³ä½¿åœ¨ç›®æ ‡åŸŸæ•°æ®æå°‘çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å–å¾—è‰¯å¥½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸­çš„è¡¨ç°å› åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„åŸŸå·®å¼‚è€Œå—é™ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨æ¥æ”¹å–„åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„è§†è§‰æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥ä¸¤ç§æ–°æŠ€æœ¯ï¼šClass-wise Adaptive Instance Normalizationä¸Cross-Attentionç»“åˆçš„æ–¹æ³•ï¼ˆCACTIï¼‰å’Œå®ƒçš„é€‰æ‹©æ€§æ³¨æ„åŠ›è¿‡æ»¤æ‰©å±•ç‰ˆæœ¬ï¼ˆCACTIFï¼‰ã€‚</li>
<li>CACTIé€šè¿‡é€‰æ‹©æ€§åº”ç”¨ç»Ÿè®¡å½’ä¸€åŒ–åŸºäºè¯­ä¹‰ç±»åˆ«è¿›è¡Œé£æ ¼è½¬ç§»ã€‚</li>
<li>CACTIFåˆ©ç”¨ç‰¹å¾ç›¸ä¼¼æ€§è¿‡æ»¤äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œå‡å°‘ä¼ªå½±ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆå›¾åƒè´¨é‡é«˜ï¼ŒFIDåˆ†æ•°ä½ï¼Œå†…å®¹ä¿ç•™å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4f74b667de0ceefb98757fa110ca0fe0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031232&auth_key=1760031232-0-0-2586a7427d2643e8902375b54ce676d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46549f41c04cd58e6bfe357c7adc1e7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031239&auth_key=1760031239-0-0-7266c4f7fec4aaff9455f27c463e672a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PVLM-Parsing-Aware-Vision-Language-Model-with-Dynamic-Contrastive-Learning-for-Zero-Shot-Deepfake-Attribution"><a href="#PVLM-Parsing-Aware-Vision-Language-Model-with-Dynamic-Contrastive-Learning-for-Zero-Shot-Deepfake-Attribution" class="headerlink" title="PVLM: Parsing-Aware Vision Language Model with Dynamic Contrastive   Learning for Zero-Shot Deepfake Attribution"></a>PVLM: Parsing-Aware Vision Language Model with Dynamic Contrastive   Learning for Zero-Shot Deepfake Attribution</h2><p><strong>Authors:Yaning Zhang, Jiahe Zhang, Chunjie Ma, Weili Guan, Tian Gan, Zan Gao</strong></p>
<p>The challenge of tracing the source attribution of forged faces has gained significant attention due to the rapid advancement of generative models. However, existing deepfake attribution (DFA) works primarily focus on the interaction among various domains in vision modality, and other modalities such as texts and face parsing are not fully explored. Besides, they tend to fail to assess the generalization performance of deepfake attributors to unseen advanced generators like diffusion in a fine-grained manner. In this paper, we propose a novel parsing-aware vision language model with dynamic contrastive learning(PVLM) method for zero-shot deepfake attribution (ZS-DFA),which facilitates effective and fine-grained traceability to unseen advanced generators. Specifically, we conduct a novel and fine-grained ZS-DFA benchmark to evaluate the attribution performance of deepfake attributors to unseen advanced generators like diffusion. Besides, we propose an innovative parsing-guided vision language model with dynamic contrastive learning (PVLM) method to capture general and diverse attribution features. We are motivated by the observation that the preservation of source face attributes in facial images generated by GAN and diffusion models varies significantly. We employ the inherent face attributes preservation differences to capture face parsing-aware forgery representations. Therefore, we devise a novel parsing encoder to focus on global face attribute embeddings, enabling parsing-guided DFA representation learning via dynamic vision-parsing matching. Additionally, we present a novel deepfake attribution contrastive center loss to pull relevant generators closer and push irrelevant ones away, which can be introduced into DFA models to enhance traceability. Experimental results show that our model exceeds the state-of-the-art on the ZS-DFA benchmark via various protocol evaluations. </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè¿½è¸ªä¼ªé€ é¢å­”çš„æºå¤´å½’å±é—®é¢˜è·å¾—äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ·±åº¦ä¼ªé€ å½’å±ï¼ˆDFAï¼‰ä¸»è¦å…³æ³¨è§†è§‰æ¨¡æ€ä¸­ä¸åŒåŸŸä¹‹é—´çš„äº¤äº’ï¼Œè€Œæ–‡æœ¬å’Œé¢éƒ¨è§£æç­‰å…¶ä»–æ¨¡æ€å¹¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¾€å¾€æ— æ³•ä»¥ç²¾ç»†çš„æ–¹å¼è¯„ä¼°æ·±åº¦ä¼ªé€ å½’å±è€…å¯¹æœªè§çš„é«˜çº§ç”Ÿæˆå™¨ï¼ˆå¦‚æ‰©æ•£ï¼‰çš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£ææ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç»“åˆåŠ¨æ€å¯¹æ¯”å­¦ä¹ ï¼ˆPVLMï¼‰æ–¹æ³•è¿›è¡Œé›¶æ ·æœ¬æ·±åº¦ä¼ªé€ å½’å±ï¼ˆZS-DFAï¼‰ï¼Œæœ‰åŠ©äºå¯¹æœªè§çš„é«˜çº§ç”Ÿæˆå™¨è¿›è¡Œæœ‰æ•ˆä¸”ç²¾ç»†çš„è¿½è¸ªã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°é¢–ä¸”ç²¾ç»†çš„ZS-DFAåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ·±åº¦ä¼ªé€ å½’å±è€…å¯¹æœªè§çš„é«˜çº§ç”Ÿæˆå™¨çš„å½’å±æ€§èƒ½ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ›æ–°çš„è§£æå¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç»“åˆåŠ¨æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•æ¥æ•æ‰é€šç”¨å’Œå¤šæ ·åŒ–çš„å½’å±ç‰¹å¾ã€‚æˆ‘ä»¬çš„åŠ¨æœºæ˜¯è§‚å¯Ÿåˆ°ç”±GANå’Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é¢éƒ¨å›¾åƒä¸­ä¿ç•™çš„æºè„¸ç‰¹å¾å·®å¼‚å¾ˆå¤§ã€‚æˆ‘ä»¬åˆ©ç”¨å›ºæœ‰çš„é¢éƒ¨ç‰¹å¾ä¿ç•™å·®å¼‚æ¥æ•æ‰é¢éƒ¨è§£ææ„ŸçŸ¥çš„ä¼ªé€ è¡¨ç¤ºã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„è§£æç¼–ç å™¨ï¼Œä¸“æ³¨äºå…¨å±€é¢éƒ¨ç‰¹å¾åµŒå…¥ï¼Œé€šè¿‡åŠ¨æ€è§†è§‰è§£æåŒ¹é…å®ç°è§£æå¼•å¯¼DFAè¡¨ç¤ºå­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦ä¼ªé€ å½’å±å¯¹æ¯”ä¸­å¿ƒæŸå¤±ï¼Œå°†ç›¸å…³ç”Ÿæˆå™¨æ‹‰è¿‘å¹¶æ¨è¿œä¸ç›¸å…³çš„ç”Ÿæˆå™¨ï¼Œå¯ä»¥å¼•å…¥åˆ°DFAæ¨¡å‹ä¸­ä»¥æé«˜è¿½è¸ªæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§åè®®è¯„ä¼°ä¸Šçš„ZS-DFAåŸºå‡†æµ‹è¯•ä¸Šè¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14129v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å…³æ³¨äºé¢éƒ¨ä¼ªé€ æº¯æºçš„é—®é¢˜ï¼Œé’ˆå¯¹ç°æœ‰æ·±åº¦ä¼ªé€ å½’å› ï¼ˆDFAï¼‰æ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰æ¨¡æ€çš„ä¸è¶³ï¼Œæå‡ºä¸€ç§ç»“åˆè§£ææ„ŸçŸ¥ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„åŠ¨æ€å¯¹æ¯”å­¦ä¹ ï¼ˆPVLMï¼‰æ–¹æ³•ï¼Œç”¨äºé›¶æ ·æœ¬æ·±åº¦ä¼ªé€ å½’å› ï¼ˆZS-DFAï¼‰ã€‚è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè¿½è¸ªæœªè§çš„é«˜çº§ç”Ÿæˆå™¨å¦‚æ‰©æ•£æ¨¡å‹ã€‚æ–‡ç« å»ºç«‹äº†ä¸€ä¸ªæ–°é¢–çš„ç»†ç²’åº¦ZS-DFAåŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯„ä¼°å½’å› æ€§èƒ½ã€‚åŒæ—¶ï¼Œæå‡ºè§£æå¼•å¯¼çš„åŠ¨æ€å¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œæ•æ‰å…¨é¢ä¸”å¤šæ ·çš„å½’å› ç‰¹å¾ã€‚åˆ©ç”¨GANå’Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆé¢éƒ¨å›¾åƒæ—¶æºè„¸å±æ€§ä¿ç•™ç¨‹åº¦çš„å·®å¼‚ï¼Œè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„è§£æç¼–ç å™¨ï¼Œä¸“æ³¨äºå…¨å±€é¢éƒ¨å±æ€§åµŒå…¥ï¼Œé€šè¿‡åŠ¨æ€è§†è§‰è§£æåŒ¹é…å®ç°è§£æå¼•å¯¼DFAè¡¨ç¤ºå­¦ä¹ ã€‚æ­¤å¤–ï¼Œå¼•å…¥æ·±åº¦ä¼ªé€ å¯¹æ¯”ä¸­å¿ƒæŸå¤±ï¼Œæ‹‰è¿‘ç›¸å…³ç”Ÿæˆå™¨å¹¶è¿œç¦»ä¸ç›¸å…³ç”Ÿæˆå™¨ï¼Œä»¥å¢å¼ºè¿½è¸ªèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ZS-DFAåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰æ·±åº¦ä¼ªé€ å½’å› æ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰æ¨¡æ€çš„äº¤äº’ï¼Œå¿½è§†äº†æ–‡æœ¬å’Œé¢éƒ¨è§£æç­‰å…¶ä»–æ¨¡æ€çš„ä½œç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£ææ„ŸçŸ¥ä¸è§†è§‰è¯­è¨€æ¨¡å‹ç»“åˆçš„åŠ¨æ€å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ˆPVLMï¼‰ï¼Œç”¨äºé›¶æ ·æœ¬æ·±åº¦ä¼ªé€ å½’å› ï¼ˆZS-DFAï¼‰ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªç»†ç²’åº¦çš„ZS-DFAåŸºå‡†æµ‹è¯•å¹³å°ï¼Œä»¥è¯„ä¼°å¯¹æœªè§çš„é«˜çº§ç”Ÿæˆå™¨çš„å½’å› æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨é¢éƒ¨å›¾åƒç”Ÿæˆæ—¶æºè„¸å±æ€§ä¿ç•™ç¨‹åº¦çš„å·®å¼‚ï¼Œè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„è§£æç¼–ç å™¨ã€‚</li>
<li>å¼•å…¥äº†æ·±åº¦ä¼ªé€ å¯¹æ¯”ä¸­å¿ƒæŸå¤±ï¼Œä»¥å¢å¼ºè¿½è¸ªèƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§åè®®è¯„ä¼°ä¸­çš„ZS-DFAåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºå®ç°æ›´æœ‰æ•ˆçš„é¢éƒ¨ä¼ªé€ æº¯æºæä¾›äº†æ–°çš„æ€è·¯å’Œå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-32fafde3f0f23ea621770ef3aa36a126~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031247&auth_key=1760031247-0-0-71da5f950910ef1c021f25de098edb98&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-324a4295b240cf74fe2dfe6d195e44fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031255&auth_key=1760031255-0-0-2b437cdd8ac995c3b9c51235c4573549&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a7cb783314a2aeedaef6ef7e35b7c6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031262&auth_key=1760031262-0-0-2b73952b35afdcb2a668f86b869b2140&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-601ca7c1d08273bb9e525e26d29964f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031269&auth_key=1760031269-0-0-762a9167a3d4591b01dce2ad96933ed5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7194d94a7725db2e6161d2790894e0b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031275&auth_key=1760031275-0-0-1a578caef20387b58b6c07b5c2920ef7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Boost-3D-Reconstruction-using-Diffusion-based-Monocular-Camera-Calibration"><a href="#Boost-3D-Reconstruction-using-Diffusion-based-Monocular-Camera-Calibration" class="headerlink" title="Boost 3D Reconstruction using Diffusion-based Monocular Camera   Calibration"></a>Boost 3D Reconstruction using Diffusion-based Monocular Camera   Calibration</h2><p><strong>Authors:Junyuan Deng, Wei Yin, Xiaoyang Guo, Qian Zhang, Xiaotao Hu, Weiqiang Ren, Xiao-Xiao Long, Ping Tan</strong></p>
<p>In this paper, we present DM-Calib, a diffusion-based approach for estimating pinhole camera intrinsic parameters from a single input image. Monocular camera calibration is essential for many 3D vision tasks. However, most existing methods depend on handcrafted assumptions or are constrained by limited training data, resulting in poor generalization across diverse real-world images. Recent advancements in stable diffusion models, trained on massive data, have shown the ability to generate high-quality images with varied characteristics. Emerging evidence indicates that these models implicitly capture the relationship between camera focal length and image content. Building on this insight, we explore how to leverage the powerful priors of diffusion models for monocular pinhole camera calibration. Specifically, we introduce a new image-based representation, termed Camera Image, which losslessly encodes the numerical camera intrinsics and integrates seamlessly with the diffusion framework. Using this representation, we reformulate the problem of estimating camera intrinsics as the generation of a dense Camera Image conditioned on an input image. By fine-tuning a stable diffusion model to generate a Camera Image from a single RGB input, we can extract camera intrinsics via a RANSAC operation. We further demonstrate that our monocular calibration method enhances performance across various 3D tasks, including zero-shot metric depth estimation, 3D metrology, pose estimation and sparse-view reconstruction. Extensive experiments on multiple public datasets show that our approach significantly outperforms baselines and provides broad benefits to 3D vision tasks. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DM-Calibï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œç”¨äºä»å•ä¸ªè¾“å…¥å›¾åƒä¼°è®¡é’ˆå­”ç›¸æœºçš„å†…åœ¨å‚æ•°ã€‚å•ç›®ç›¸æœºæ ‡å®šæ˜¯è®¸å¤šä¸‰ç»´è§†è§‰ä»»åŠ¡çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„å‡è®¾æˆ–å—é™äºæœ‰é™çš„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´åœ¨å¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œå›¾åƒä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æœ€è¿‘åŸºäºå¤§è§„æ¨¡æ•°æ®çš„ç¨³å®šæ‰©æ•£æ¨¡å‹çš„è¿›å±•è¡¨æ˜ï¼Œå®ƒä»¬èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸åŒç‰¹æ€§çš„é«˜è´¨é‡å›¾åƒã€‚æœ‰è¯æ®è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹éšå«åœ°æ•æ‰äº†ç›¸æœºç„¦è·ä¸å›¾åƒå†…å®¹ä¹‹é—´çš„å…³ç³»ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æœ‰åŠ›å…ˆéªŒè¿›è¡Œå•ç›®é’ˆå­”ç›¸æœºæ ‡å®šã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºå›¾åƒçš„è¡¨ç¤ºæ–¹æ³•ï¼Œç§°ä¸ºCamera Imageï¼Œå®ƒæ— æŸåœ°ç¼–ç äº†ç›¸æœºçš„æ•°å€¼å†…åœ¨å‚æ•°ï¼Œå¹¶ä¸æ‰©æ•£æ¡†æ¶æ— ç¼é›†æˆã€‚ä½¿ç”¨è¿™ç§è¡¨ç¤ºæ–¹æ³•ï¼Œæˆ‘ä»¬å°†ä¼°è®¡ç›¸æœºå†…åœ¨å‚æ•°çš„é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæ ¹æ®è¾“å…¥å›¾åƒç”Ÿæˆå¯†é›†Camera Imageçš„é—®é¢˜ã€‚é€šè¿‡å¾®è°ƒç¨³å®šçš„æ‰©æ•£æ¨¡å‹æ¥ä»å•ä¸ªRGBè¾“å…¥ç”ŸæˆCamera Imageï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡RANSACæ“ä½œæå–ç›¸æœºå†…åœ¨å‚æ•°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œæˆ‘ä»¬çš„å•ç›®æ ‡å®šæ–¹æ³•æé«˜äº†å„ç§ä¸‰ç»´ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åº¦é‡æ·±åº¦ä¼°è®¡ã€ä¸‰ç»´æµ‹é‡ã€å§¿æ€ä¼°è®¡å’Œç¨€ç–è§†å›¾é‡å»ºã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºå‡†çº¿ï¼Œå¹¶ä¸ºä¸‰ç»´è§†è§‰ä»»åŠ¡æä¾›äº†å¹¿æ³›çš„ç›Šå¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17240v3">PDF</a> </p>
<p><strong>Summary</strong><br>DM-Calibæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é’ˆå­”ç›¸æœºå†…å‚ä¼°è®¡æ–¹æ³•ï¼Œä»å•å¼ è¾“å…¥å›¾åƒå‡ºå‘ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒä¿¡æ¯è¿›è¡Œå•ç›®ç›¸æœºæ ¡å‡†ã€‚æ–°æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•å—é™äºæ‰‹å·¥å‡è®¾æˆ–è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œæé«˜äº†åœ¨å¤šç§çœŸå®å›¾åƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥Camera Imageè¡¨ç¤ºï¼Œå°†ç›¸æœºå†…å‚ä¼°è®¡é—®é¢˜è½¬åŒ–ä¸ºåŸºäºè¾“å…¥å›¾åƒç”Ÿæˆå¯†é›†Camera Imageçš„é—®é¢˜ã€‚é€šè¿‡å¾®è°ƒç¨³å®šçš„æ‰©æ•£æ¨¡å‹ç”ŸæˆCamera Imageï¼Œåˆ©ç”¨RANSACç®—æ³•æå–ç›¸æœºå†…å‚ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºå¤šç§ä¸‰ç»´è§†è§‰ä»»åŠ¡æä¾›äº†å¹¿æ³›çš„ç›Šå¤„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DM-Calibæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é’ˆå­”ç›¸æœºå†…å‚ä¼°è®¡æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿç›¸æœºæ ¡å‡†æ–¹æ³•ä¾èµ–äºæ‰‹å·¥å‡è®¾æˆ–è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥Camera Imageè¡¨ç¤ºï¼Œå°†ç›¸æœºå†…å‚ä¼°è®¡é—®é¢˜è½¬åŒ–ä¸ºç”Ÿæˆé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒä¿¡æ¯è¿›è¡Œç›¸æœºæ ¡å‡†ã€‚</li>
<li>é€šè¿‡å¾®è°ƒç¨³å®šçš„æ‰©æ•£æ¨¡å‹ç”ŸæˆCamera Imageã€‚</li>
<li>ä½¿ç”¨RANSACç®—æ³•ä»ç”Ÿæˆçš„Camera Imageä¸­æå–ç›¸æœºå†…å‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9ce7c585df33057620f5f2dcfe5c931f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031283&auth_key=1760031283-0-0-4098f12b26601c3687f1ac20ca539034&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8176a46f91981d5990a77ec56fb7d1dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031291&auth_key=1760031291-0-0-3e05c29d1ed15c110658f6aebd33e7f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6ac516f3a44f6f2bfd92a63351523d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031297&auth_key=1760031297-0-0-87bc52a2ed1de7c24819c41cb06678db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-142a00e278dd0ae16e1bddcd63bef1fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031304&auth_key=1760031304-0-0-2fc6e1fb1c454e4f05d83f26d502c7ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ed5bf5ef103a9408a8b868da8b14736~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031311&auth_key=1760031311-0-0-4f48b46987ddaf28be4e3e580cec3525&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-305bb56c15aae926bb206bc5437d48c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760031318&auth_key=1760031318-0-0-c42cfd93581cf4021e9dd425889340a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  Semi-Supervised 3D Medical Segmentation from 2D Natural Images   Pretrained Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-2c06628dbcec73febee3836f1ff9f431~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030821&auth_key=1760030821-0-0-2a1593095ab8f42c051a5604f3542017&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
