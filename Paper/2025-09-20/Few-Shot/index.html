<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-09-20  Seeing 3D Through 2D Lenses 3D Few-Shot Class-Incremental Learning via   Cross-Modal Geometric Rectification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-7e3f8f182fc20e823326aea988214435~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029538&auth_key=1760029538-0-0-764be6934c9a29e0b3cfe82da6e00a9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-20-更新"><a href="#2025-09-20-更新" class="headerlink" title="2025-09-20 更新"></a>2025-09-20 更新</h1><h2 id="Seeing-3D-Through-2D-Lenses-3D-Few-Shot-Class-Incremental-Learning-via-Cross-Modal-Geometric-Rectification"><a href="#Seeing-3D-Through-2D-Lenses-3D-Few-Shot-Class-Incremental-Learning-via-Cross-Modal-Geometric-Rectification" class="headerlink" title="Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via   Cross-Modal Geometric Rectification"></a>Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via   Cross-Modal Geometric Rectification</h2><p><strong>Authors:Xiang Tuo, Xu Xuemiao, Liu Bangzhen, Li Jinyi, Li Yong, He Shengfeng</strong></p>
<p>The rapid growth of 3D digital content necessitates expandable recognition systems for open-world scenarios. However, existing 3D class-incremental learning methods struggle under extreme data scarcity due to geometric misalignment and texture bias. While recent approaches integrate 3D data with 2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by texture-biased projections and indiscriminate fusion of geometric-textural cues, leading to unstable decision prototypes and catastrophic forgetting. To address these issues, we propose Cross-Modal Geometric Rectification (CMGR), a framework that enhances 3D geometric fidelity by leveraging CLIP’s hierarchical spatial semantics. Specifically, we introduce a Structure-Aware Geometric Rectification module that hierarchically aligns 3D part structures with CLIP’s intermediate spatial priors through attention-driven geometric fusion. Additionally, a Texture Amplification Module synthesizes minimal yet discriminative textures to suppress noise and reinforce cross-modal consistency. To further stabilize incremental prototypes, we employ a Base-Novel Discriminator that isolates geometric variations. Extensive experiments demonstrate that our method significantly improves 3D few-shot class-incremental learning, achieving superior geometric coherence and robustness to texture bias across cross-domain and within-domain settings. </p>
<blockquote>
<p>随着三维数字内容的快速增长，开放世界场景需要可扩展的识别系统。然而，现有的三维类增量学习方法在极端数据稀缺的情况下，由于几何失配和纹理偏见，表现挣扎。虽然最近的方法将三维数据与二维基础模型（例如CLIP）相结合，但它们受到纹理偏向投影和几何纹理线索的散漫融合导致的语义模糊的影响，导致决策原型不稳定和灾难性遗忘。为了解决这些问题，我们提出了跨模态几何校正（CMGR）框架，该框架利用CLIP的分层空间语义增强三维几何保真度。具体来说，我们引入了一个结构感知几何校正模块，该模块通过注意力驱动的几何融合层次地与CLIP的中间空间先验对齐三维部分结构。此外，纹理放大模块合成最小但具有区分力的纹理，以抑制噪声并增强跨模态一致性。为了进一步稳定增量原型，我们采用了基础-新颖鉴别器，以隔离几何变化。大量实验表明，我们的方法在三维小样本类增量学习上显著改进，实现了跨域和域内设置中的出色几何连贯性和对纹理偏见的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14958v1">PDF</a> ICCV2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了三维数字内容的快速增长对开放世界场景下的识别系统提出了更高的要求。现有的三维类增量学习方法在极端数据稀缺的情况下存在几何失配和纹理偏见的问题。为解决这些问题，本文提出一种名为Cross-Modal Geometric Rectification（CMGR）的框架，利用CLIP的层次空间语义提高三维几何保真度。通过引入结构感知几何校正模块和纹理放大模块，实现了对三维部分结构与CLIP中间空间先验的层次对齐，并合成最小但具有鉴别力的纹理以抑制噪声并增强跨模态一致性。此外，通过采用基础-新型判别器进一步稳定增量原型，隔离几何变化。实验证明，该方法在三维小样本类增量学习上表现出显著的改进，提高了几何一致性和对纹理偏见的鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关于该文本的关键见解：</p>
<ol>
<li>三维数字内容的增长推动了开放世界场景下识别系统的需求。</li>
<li>现有三维类增量学习方法面临极端数据稀缺时的几何失配和纹理偏见问题。</li>
<li>Cross-Modal Geometric Rectification（CMGR）框架利用CLIP的层次空间语义提高三维几何保真度。</li>
<li>结构感知几何校正模块实现了三维部分结构与CLIP中间空间先验的层次对齐。</li>
<li>纹理放大模块合成最小但具有鉴别力的纹理，以抑制噪声并增强跨模态一致性。</li>
<li>基础-新型判别器用于进一步稳定增量原型，隔离几何变化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-1cbb59b39c82c5ce445f8174b0b79581~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029548&auth_key=1760029548-0-0-e2dba9bce7356458d015f558d5e013a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e01d2c5a3277616a91317d76b754895f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029556&auth_key=1760029556-0-0-cf8e6a01bb8268cb5a1b84da66e72386&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a703653ef39737083a1d7ae0bb07d6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029563&auth_key=1760029563-0-0-dbea69e48c67fda27ef69da4398940e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-251425028bcf6ed3241d02c5eb83e868~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029570&auth_key=1760029570-0-0-912c285bdffe8dc1c95e9dd0782d6513&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Trade-offs-in-Cross-Domain-Generalization-of-Foundation-Model-Fine-Tuned-for-Biometric-Applications"><a href="#Trade-offs-in-Cross-Domain-Generalization-of-Foundation-Model-Fine-Tuned-for-Biometric-Applications" class="headerlink" title="Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned   for Biometric Applications"></a>Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned   for Biometric Applications</h2><p><strong>Authors:Tahar Chettaoui, Naser Damer, Fadi Boutros</strong></p>
<p>Foundation models such as CLIP have demonstrated exceptional zero- and few-shot transfer capabilities across diverse vision tasks. However, when fine-tuned for highly specialized biometric tasks, face recognition (FR), morphing attack detection (MAD), and presentation attack detection (PAD), these models may suffer from over-specialization. Thus, they may lose one of their foundational strengths, cross-domain generalization. In this work, we systematically quantify these trade-offs by evaluating three instances of CLIP fine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the original CLIP baseline on 14 general vision datasets under zero-shot and linear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our results indicate that fine-tuned models suffer from over-specialization, especially when fine-tuned for complex tasks of FR. Also, our results pointed out that task complexity and classification head design, multi-class (FR) vs. binary (MAD and PAD), correlate with the degree of catastrophic forgetting. The FRoundation model with the ViT-L backbone outperforms other approaches on the large-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%. However, it experiences a substantial performance drop on ImageNetV2, reaching only 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover, the larger CLIP architecture consistently preserves more of the model’s original generalization ability than the smaller variant, indicating that increased model capacity may help mitigate over-specialization. </p>
<blockquote>
<p>诸如CLIP等基础模型在多种视觉任务中表现出了出色的零样本和少样本迁移能力。然而，当针对高度专业化的生物识别任务（如人脸识别（FR）、形态攻击检测（MAD）和呈现攻击检测（PAD））进行微调时，这些模型可能会过度专业化。因此，它们可能会丧失其基础优势之一，即跨域泛化。在这项工作中，我们通过评估针对FR、MAD和PAD微调的三例CLIP来系统地量化这些权衡。我们评估每个适配模型以及原始CLIP基准线在零样本和线性探针协议下的14个通用视觉数据集上的表现，同时还有人脸识别、形态攻击检测和呈现攻击检测的常见基准测试。我们的结果表明，微调模型存在过度专业化的问题，特别是在针对复杂的人脸识别任务进行微调时。此外，我们的结果还指出，任务复杂性和分类头部设计（多类（人脸识别）与二元（形态攻击检测和呈现攻击检测））与灾难性遗忘的程度有关。具有ViT-L主干的基础模型在大规模人脸识别基准测试IJB-C上优于其他方法，提高了高达58.52%。然而，它在ImageNetV2上的性能大幅下降，仅达到51.63%，而基线CLIP模型则达到了69.84%。此外，较大的CLIP架构始终保留了模型更多的原始泛化能力，这暗示着增加模型容量可能有助于缓解过度专业化的问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14921v1">PDF</a> Accepted at the IEEE International Joint Conference on Biometrics   2025 (IJCB 2025)</p>
<p><strong>Summary</strong><br>    本研究探讨了CLIP等基础模型在人脸识别（FR）、形态攻击检测（MAD）和呈现攻击检测（PAD）等高度专业化生物识别任务上的微调表现，发现这些模型可能因过度专业化而丧失跨域泛化能力。在多种通用视觉数据集上的评估表明，微调模型在零样本和线性探测协议下出现过度专业化现象，特别是面对复杂的人脸识别任务时。同时，任务复杂度和分类头部设计（多类人脸识别与二元攻击检测）与灾难性遗忘程度相关。此外，具有ViT-L主干的基础模型在大型人脸识别基准测试IJB-C上表现优异，但在ImageNetV2上的性能大幅下降。更大规模的CLIP架构更能保持模型的原始泛化能力，表明增加模型容量可能有助于缓解过度专业化问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基础模型如CLIP在高度专业化的生物识别任务（如人脸识别、形态攻击检测、呈现攻击检测）上经过微调后可能面临过度专业化的问题，导致丧失跨域泛化能力。</li>
<li>在通用视觉数据集上的评估显示，微调模型在零样本和线性探测协议下出现过度专业化现象。</li>
<li>任务复杂度和分类头部设计（多类与二元）与灾难性遗忘程度有关。</li>
<li>具有ViT-L主干的基础模型在人脸识别基准测试上表现良好，但在其他大型图像数据集上的性能可能下降。</li>
<li>更大规模的CLIP架构能够更好地保持模型的原始泛化能力。</li>
<li>增加模型容量可能有助于缓解过度专业化问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14921">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f21e63aa19fa56bc11eefcb9d2ddebcc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029578&auth_key=1760029578-0-0-998f3478eee96ffd997b913b2660494d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-34122b936f0b517a163ae81dd4b6a27a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029585&auth_key=1760029585-0-0-31be44ad9f3a616503504fe1bcd9a2c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c7de623561dbef88f6c8c91254b16bd0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029591&auth_key=1760029591-0-0-187558681f8199329d3f2d8b61bc65b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-Data-and-Parameter-Efficient-Strategies-for-Arabic-Dialect-Identifications"><a href="#Exploring-Data-and-Parameter-Efficient-Strategies-for-Arabic-Dialect-Identifications" class="headerlink" title="Exploring Data and Parameter Efficient Strategies for Arabic Dialect   Identifications"></a>Exploring Data and Parameter Efficient Strategies for Arabic Dialect   Identifications</h2><p><strong>Authors:Vani Kanjirangat, Ljiljana Dolamic, Fabio Rinaldi</strong></p>
<p>This paper discusses our exploration of different data-efficient and parameter-efficient approaches to Arabic Dialect Identification (ADI). In particular, we investigate various soft-prompting strategies, including prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA reparameterizations. For the data-efficient strategy, we analyze hard prompting with zero-shot and few-shot inferences to analyze the dialect identification capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT approaches, we conducted our experiments using Arabic-specific encoder models on several major datasets. We also analyzed the n-shot inferences on open-source decoder-only models, a general multilingual model (Phi-3.5), and an Arabic-specific one(SILMA). We observed that the LLMs generally struggle to differentiate the dialectal nuances in the few-shot or zero-shot setups. The soft-prompted encoder variants perform better, while the LoRA-based fine-tuned models perform best, even surpassing full fine-tuning. </p>
<blockquote>
<p>本文讨论了我们在阿拉伯语方言识别（ADI）方面对不同数据高效和参数高效方法的探索。具体来说，我们研究了各种软提示策略，包括前缀调整、提示调整、P-tuning和P-tuning V2，以及LoRA重新参数化。对于数据高效策略，我们分析了零样本和少样本推断中的硬提示，以分析大型语言模型（LLM）的方言识别能力。对于参数高效的PEFT方法，我们在几个主要数据集上使用了阿拉伯语言特定编码器模型进行实验。我们还分析了开源解码器模型、通用多语言模型（Phi-3.5）和阿拉伯语言特定模型（SILMA）的n次射击推断。我们发现，大型语言模型通常在少样本或零样本设置中难以区分方言细微差别。软提示编码器变体表现更好，而基于LoRA的微调模型表现最佳，甚至超过了全量微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13775v2">PDF</a> 4 main pages, 4 additional, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了数据高效和参数高效的阿拉伯语方言识别（ADI）方法。研究了多种软提示策略，如前缀调整、提示调整、P-tuning和P-tuning V2，以及LoRA重新参数化。对于数据高效策略，我们分析了零样本和少样本推断下的硬提示方法，以评估大型语言模型（LLM）的方言识别能力。对于参数高效的PEFT方法，我们在多个主要数据集上进行了阿拉伯语特定编码器模型的实验，并分析了开源解码器模型、通用多语言模型（Phi-3.5）和阿拉伯语特定模型（SILMA）的n-shot推断结果。观察到大型语言模型在少样本或零样本设置中难以区分方言细微差别。软提示编码器变体表现较好，而基于LoRA的精细调整模型表现最佳，甚至超越全精细调整。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文探讨了数据高效和参数高效的阿拉伯语方言识别方法。</li>
<li>研究了多种软提示策略，包括前缀调整、提示调整等。</li>
<li>在数据高效策略方面，硬提示方法用于评估大型语言模型的方言识别能力。</li>
<li>在参数高效的实验中，使用了阿拉伯语特定编码器模型并进行了实验验证。</li>
<li>LLM在少样本或零样本设置中方言识别能力受限。</li>
<li>软提示编码器变体在方言识别中表现较好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-63ee891ddcf7f6225d9bf665007e14d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029599&auth_key=1760029599-0-0-95579027a5f913ffdf0074599bef30bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1256a228e2b26db63cd06c65b77c479b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029606&auth_key=1760029606-0-0-33d699b56bd4459c8a7b015b374c3a37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9ebb80089163d7e56d48bb3ee763efaa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029612&auth_key=1760029612-0-0-9c9aa69211270907f1dbfdf0a57c6f78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ThinkAct-Vision-Language-Action-Reasoning-via-Reinforced-Visual-Latent-Planning"><a href="#ThinkAct-Vision-Language-Action-Reasoning-via-Reinforced-Visual-Latent-Planning" class="headerlink" title="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent   Planning"></a>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent   Planning</h2><p><strong>Authors:Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang</strong></p>
<p>Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks. </p>
<blockquote>
<p>视觉-语言-行动（VLA）推理任务要求智能体解释多模式指令，进行长期规划，并在动态环境中进行自适应行动。现有方法通常以端到端的方式训练VLA模型，直接将输入映射到行动，而没有明确的推理，这阻碍了它们在多个步骤上进行规划或适应复杂任务变化的能力。在本文中，我们提出了ThinkAct，这是一个双系统框架，它通过强化视觉潜在规划，将高级推理与低级行动执行联系起来。ThinkAct训练了一个多模式大型语言模型，根据目标完成和轨迹一致性生成强化行动对齐的视觉奖励来指导具象化推理计划。这些推理计划被压缩成视觉计划潜在状态，以在目标环境中对下游行动模型进行稳健的行动执行。在具象推理和机器人操作基准测试上的大量实验表明，ThinkAct能够在复杂的嵌入式AI任务中实现少量适应、长期规划和自我修正行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16815v2">PDF</a> NeurIPS 2025. Project page:   <a target="_blank" rel="noopener" href="https://jasper0314-huang.github.io/thinkact-vla/">https://jasper0314-huang.github.io/thinkact-vla/</a></p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种名为ThinkAct的双向系统框架，用于实现视觉语言动作（VLA）推理任务中的高级推理与低级动作执行的桥梁。该框架通过强化视觉潜在规划，生成以行动对齐的视觉奖励为指导的实体推理计划。这些推理计划被压缩成视觉计划潜在状态，以在目标环境中进行稳健的动作执行。ThinkAct能够在复杂的实体AI任务中实现少量适应、长期规划和自我校正行为。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ThinkAct是一个双系统框架，结合了高级推理和低级动作执行。</li>
<li>通过强化视觉潜在规划来实现VLA任务。</li>
<li>生成以行动对齐的视觉奖励为指导的实体推理计划。</li>
<li>推理计划被压缩成视觉计划潜在状态以指导动作执行。</li>
<li>ThinkAct能够在复杂的实体AI任务中实现少量适应。</li>
<li>具备长期规划能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e856f7068800be5947fc00d6c082cf2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029620&auth_key=1760029620-0-0-2b7fa70ea3223f9170a91896f28be578&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-26165280aef05380758055dc6996ac68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029627&auth_key=1760029627-0-0-29f2d9c267a4fb36bc7e140267eb6d21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6087562152b4780163d95b6009a36d66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029635&auth_key=1760029635-0-0-918e6ceafcae88d351de2b62e273f7d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Adapting-Language-Models"><a href="#Self-Adapting-Language-Models" class="headerlink" title="Self-Adapting Language Models"></a>Self-Adapting Language Models</h2><p><strong>Authors:Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, Pulkit Agrawal</strong></p>
<p>Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model’s own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at <a target="_blank" rel="noopener" href="https://jyopari.github.io/posts/seal">https://jyopari.github.io/posts/seal</a>. </p>
<blockquote>
<p>大型语言模型（LLM）虽然强大但静态，缺乏根据新任务、知识或示例调整其权重的机制。我们引入了自适应语言模型（SEAL），这是一个框架，允许LLM通过生成自己的微调数据和更新指令进行自我适应。对于新的输入，模型会产生自我编辑的版本，这个版本可能会以不同的方式重新组织信息，指定优化超参数，或调用数据增强工具和基于梯度的更新工具。通过监督微调（SFT），这些自我编辑会导致持久的权重更新，从而实现持久的适应。为了训练模型产生有效的自我编辑，我们使用强化学习循环，以更新模型的下游性能作为奖励信号。不同于依赖单独适应模块或辅助网络的先前方法，SEAL直接使用模型自身的生成来控制其适应过程。在知识整合和少量样本泛化方面的实验表明，SEAL是朝着能够自我指导适应的语言模型迈出的有前景的一步。我们的网站和代码可通过<a target="_blank" rel="noopener" href="https://jyopari.github.io/posts/seal%E8%AE%BF%E9%97%AE%E3%80%82">https://jyopari.github.io/posts/seal访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10943v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）功能强大但静态不变，无法根据新任务、知识或示例调整自身权重。我们提出Self-Adapting LLMs（SEAL）框架，使LLMs能够通过生成自身的微调数据和更新指令来实现自我适应。对于新输入的信息，模型会产生自我编辑，通过监督微调（SFT）产生持久性的权重更新，实现长期适应。我们通过强化学习循环训练模型产生有效的自我编辑，以更新模型的下游性能作为奖励信号。不同于以往依赖额外适应模块或辅助网络的方法，SEAL直接利用模型自身的生成来控制其适应过程。实验表明，SEAL在知识融合和少量样本泛化方面展现出巨大潜力，是向能够自我定向适应的语言模型迈进的重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs虽然强大，但缺乏根据新任务、知识或示例自我调整的能力。</li>
<li>SEAL框架使LLMs能够通过生成自我编辑数据和指令来实现自我适应。</li>
<li>模型产生的自我编辑可以通过监督微调（SFT）导致持久的权重更新。</li>
<li>强化学习循环用于训练模型产生有效的自我编辑。</li>
<li>模型下游性能作为奖励信号来评估自我编辑的效果。</li>
<li>与其他适应模块或辅助网络不同，SEAL利用模型自身的生成来控制适应过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10943">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-47b41faf0a772c46b5006145033d386e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029642&auth_key=1760029642-0-0-d74eedf92d41d736ba47ad343ae391a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8e44568cb184743b4959907b0f9ce90e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029649&auth_key=1760029649-0-0-a3ebba0daa2767263f53946f4551d963&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a753461583488b92e0b9e22800090050~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029656&auth_key=1760029656-0-0-cd5e157572b1b50c902d3f6eb01e150e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebc40b6d26a274647b2f4bef3d56e414~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029663&auth_key=1760029663-0-0-712070ee30722f94c297571fc38c0541&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MOLE-Metadata-Extraction-and-Validation-in-Scientific-Papers-Using-LLMs"><a href="#MOLE-Metadata-Extraction-and-Validation-in-Scientific-Papers-Using-LLMs" class="headerlink" title="MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs"></a>MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs</h2><p><strong>Authors:Zaid Alyafeai, Maged S. Al-Shaibani, Bernard Ghanem</strong></p>
<p>Metadata extraction is essential for cataloging and preserving datasets, enabling effective research discovery and reproducibility, especially given the current exponential growth in scientific research. While Masader (Alyafeai et al.,2021) laid the groundwork for extracting a wide range of metadata attributes from Arabic NLP datasets’ scholarly articles, it relies heavily on manual annotation. In this paper, we present MOLE, a framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers covering datasets of languages other than Arabic. Our schema-driven methodology processes entire documents across multiple input formats and incorporates robust validation mechanisms for consistent output. Additionally, we introduce a new benchmark to evaluate the research progress on this task. Through systematic analysis of context length, few-shot learning, and web browsing integration, we demonstrate that modern LLMs show promising results in automating this task, highlighting the need for further future work improvements to ensure consistent and reliable performance. We release the code: <a target="_blank" rel="noopener" href="https://github.com/IVUL-KAUST/MOLE">https://github.com/IVUL-KAUST/MOLE</a> and dataset: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/IVUL-KAUST/MOLE">https://huggingface.co/datasets/IVUL-KAUST/MOLE</a> for the research community. </p>
<blockquote>
<p>元数据提取对于数据集编目和保存至关重要，它促进了有效的研究发现和可重复性，尤其是在当前科学研究呈指数级增长的情况下。虽然Masader（Alyafeai等人，2021年）奠定了从阿拉伯语NLP数据集学术论文中提取广泛元数据属性的基础，但它主要依赖于人工标注。在本文中，我们介绍了MOLE框架，该框架利用大型语言模型（LLM）自动从涉及非阿拉伯语数据集的科学论文中提取元数据属性。我们的基于模式的方法处理多种输入格式的整个文档，并包含用于一致输出的稳健验证机制。此外，我们还引入了一个新的基准测试来评估此任务的研究进展。通过系统分析上下文长度、小样本学习和网页浏览集成，我们证明了现代LLM在该任务的自动化方面显示出有前途的结果，并强调了未来需要进一步改进工作以确保性能和可靠性的一致性。我们向研究社区发布代码：<a target="_blank" rel="noopener" href="https://github.com/IVUL-KAUST/MOLE%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9Ahttps://huggingface.co/datasets/IVUL-KAUST/MOLE%E3%80%82">https://github.com/IVUL-KAUST/MOLE和数据集：https://huggingface.co/datasets/IVUL-KAUST/MOLE。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19800v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了MOLE框架，该框架利用大型语言模型（LLMs）自动从非阿拉伯语的科学论文中提取元数据属性。此框架是自动的，可以在多种输入格式下处理整个文档，并包含稳健的验证机制以确保一致输出。该研究还引入了一个新的基准测试来评估此任务的研究进展，并通过上下文长度、小样本学习和网页浏览整合的系统性分析，证明了现代大型语言模型在该任务自动化方面的前景。该研究的代码和数据集已公开发布，供研究社区使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MOLE框架能自动从非阿拉伯语的科学论文中提取元数据属性，极大地减少了手动标注的需求。</li>
<li>MOLE采用了基于schema的方法来处理不同格式的文档。</li>
<li>它包含了稳健的验证机制以确保输出的准确性及一致性。</li>
<li>研究人员引入了一个新的基准测试来评估自动提取元数据属性的研究进展。</li>
<li>通过上下文长度、小样本学习等方面的系统性分析，验证了大型语言模型在自动化提取元数据方面的潜力。</li>
<li>该研究发布了MOLE框架的代码和基准数据集以供研究社区使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-79913c05b3ec704ecf3605a4fb0a3eba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029670&auth_key=1760029670-0-0-a63d0abd0a0249a17488e235b12ea709&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-24e7d3182931e410263f0949695940bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029678&auth_key=1760029678-0-0-5a0b28534afbda7e77f61be424ab9360&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c3c5dde6f3df9e4275ea84e3509be80c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029684&auth_key=1760029684-0-0-b8009afac9f52110dcbd5312914a99ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-59acb9844a43690cc94bd82e5cbb37f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029691&auth_key=1760029691-0-0-bba4c5022532dafb9d2204daf7c989e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2a6d593aef7c8570f9167f9165de5d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029697&auth_key=1760029697-0-0-c12a5bc047d72e1cb6f81d0f391525fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2abf251e611771762ac3f2c5611d769d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029704&auth_key=1760029704-0-0-7e544399d5f036d7d70a08a0b3b1a902&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cea7bb028b15f18c21dd6987afc159ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029711&auth_key=1760029711-0-0-660687f9e47cc5b16055704ebbdc0f72&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-562e52156200bb8315d0176aca8d0151~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029718&auth_key=1760029718-0-0-974c1411858bff8a8ba23d3940cebebc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SNaRe-Domain-aware-Data-Generation-for-Low-Resource-Event-Detection"><a href="#SNaRe-Domain-aware-Data-Generation-for-Low-Resource-Event-Detection" class="headerlink" title="SNaRe: Domain-aware Data Generation for Low-Resource Event Detection"></a>SNaRe: Domain-aware Data Generation for Low-Resource Event Detection</h2><p><strong>Authors:Tanmay Parekh, Yuxuan Dong, Lucas Bandarkar, Artin Kim, I-Hung Hsu, Kai-Wei Chang, Nanyun Peng</strong></p>
<p>Event Detection (ED) – the task of identifying event mentions from natural language text – is critical for enabling reasoning in highly specialized domains such as biomedicine, law, and epidemiology. Data generation has proven to be effective in broadening its utility to wider applications without requiring expensive expert annotations. However, when existing generation approaches are applied to specialized domains, they struggle with label noise, where annotations are incorrect, and domain drift, characterized by a distributional mismatch between generated sentences and the target domain. To address these issues, we introduce SNaRe, a domain-aware synthetic data generation framework composed of three components: Scout, Narrator, and Refiner. Scout extracts triggers from unlabeled target domain data and curates a high-quality domain-specific trigger list using corpus-level statistics to mitigate domain drift. Narrator, conditioned on these triggers, generates high-quality domain-aligned sentences, and Refiner identifies additional event mentions, ensuring high annotation quality. Experimentation on three diverse domain ED datasets reveals how SNaRe outperforms the best baseline, achieving average F1 gains of 3-7% in the zero-shot&#x2F;few-shot settings and 4-20% F1 improvement for multilingual generation. Analyzing the generated trigger hit rate and human evaluation substantiates SNaRe’s stronger annotation quality and reduced domain drift. </p>
<blockquote>
<p>事件检测（ED）——从自然语言文本中识别事件提及的任务——对于在生物医学、法律和流行病学等高度专业化领域进行推理至关重要。数据生成已证明在扩大其在更广泛应用中的效用方面非常有效，而无需昂贵的专家注释。然而，当将现有的生成方法应用于专业领域时，它们会面临标签噪声的问题，即注释不正确，以及领域漂移，表现为生成句子与目标领域之间的分布不匹配。为了解决这些问题，我们引入了SNaRe，这是一个领域感知的合成数据生成框架，由三个组件组成：侦察兵、叙述者和精炼者。侦察兵从目标领域的无标签数据中提取触发器，并使用语料库级别的统计信息来创建高质量的专业特定触发器列表，以减轻领域漂移的问题。叙述者根据这些触发器生成高质量、符合领域要求的句子，而精炼者则负责识别其他事件提及，确保高质量的注释。在三个不同领域的ED数据集上进行实验表明，SNaRe的表现超过了最佳基线，在零样本&#x2F;少样本设置中平均F1得分提高了3-7%，在多语言生成中F1得分提高了4-20%。对生成的触发器命中率和人类评估的分析证实了SNaRe具有更高的注释质量和减少的领域漂移问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17394v3">PDF</a> Accepted at EMNLP 2025 Main</p>
<p><strong>Summary</strong></p>
<p>事件检测（ED）是从自然语言文本中识别事件提及的任务，对于生物医药、法律和流行病学等高度专业化领域中的推理至关重要。数据生成已证明在扩大其应用范围而无需昂贵的专家注释方面非常有效。然而，当现有生成方法应用于专业领域时，它们面临标签噪声（注释不正确）和领域漂移（生成句子与目标领域的分布不匹配）的问题。为解决这些问题，我们引入了SNaRe，这是一个领域感知的合成数据生成框架，由Scout、Narrator和Refiner三个组件组成。通过从目标领域无标签数据中提取触发器并使用语料库级统计来创建高质量特定领域的触发器列表，从而减轻领域漂移。Narrator根据这些触发器生成高质量的领域对齐句子，而Refiner则识别其他事件提及，确保高注释质量。在三个不同的领域ED数据集上的实验表明，SNaRe优于最佳基线，在零样本&#x2F;少样本设置中平均F1得分提高了3-7%，在多语言生成中提高了4-20%。对生成的触发器命中率和人类评估的分析证实了SNaRe更强的注释质量和减少的领域漂移。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>事件检测（ED）在高度专业化领域中至关重要，如生物医药、法律和流行病学。</li>
<li>数据生成方法已被证明对于扩大事件检测的应用范围非常有效。</li>
<li>当前数据生成方法面临标签噪声和领域漂移的挑战。</li>
<li>SNaRe是一个领域感知的合成数据生成框架，由Scout、Narrator和Refiner三个组件组成。</li>
<li>SNaRe通过从目标领域无标签数据中提取触发器来减轻领域漂移问题。</li>
<li>SNaRe在多个领域数据集上的实验表现优于最佳基线，具有显著的效果提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17394">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-111477456dd120accfbe4eac95e00242~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029725&auth_key=1760029725-0-0-cace59a298ea2e4d8ca8d7fb78855011&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e3bc99ffc6591e22bec2c183875be39d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029732&auth_key=1760029732-0-0-4b6cd008fca5e6672c8f2e37a9b44319&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e90d7191f9d168f2eb99570834aea0f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029739&auth_key=1760029739-0-0-4baa125c606df501f68730bf14e1b257&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c54568f5754cd360a88bbc8d284057e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029746&auth_key=1760029746-0-0-fd3d9de3287a05867816413c8a8f42c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de938d790d53e75d2b1833d3b07d68e8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029753&auth_key=1760029753-0-0-8f91f1a3a6a81b9c7de319319eb12404&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6cec76433eb8acf6759c650eb840a286~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029760&auth_key=1760029760-0-0-4fdd8eb159000a81b58b54b6245888fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ReMoBot-Retrieval-Based-Few-Shot-Imitation-Learning-for-Mobile-Manipulation-with-Vision-Foundation-Models"><a href="#ReMoBot-Retrieval-Based-Few-Shot-Imitation-Learning-for-Mobile-Manipulation-with-Vision-Foundation-Models" class="headerlink" title="ReMoBot: Retrieval-Based Few-Shot Imitation Learning for Mobile   Manipulation with Vision Foundation Models"></a>ReMoBot: Retrieval-Based Few-Shot Imitation Learning for Mobile   Manipulation with Vision Foundation Models</h2><p><strong>Authors:Yuying Zhang, Wenyan Yang, Francesco Verdoja, Ville Kyrki, Joni Pajarinen</strong></p>
<p>Imitation learning (IL) algorithms typically distill experience into parametric behavior policies to mimic expert demonstrations. However, with limited demonstrations, existing methods often struggle to generate accurate actions, particularly under partial observability. To address this problem, we introduce a few-shot IL approach, ReMoBot, which directly retrieves information from demonstrations to solve Mobile manipulation tasks with ego-centric visual observations. Given the current observation, ReMoBot utilizes vision foundation models to identify relevant demonstrations, considering visual similarity w.r.t. both individual observations and history trajectories. A motion selection policy then selects the proper command for the robot until the task is successfully completed.   The performance of ReMoBot is evaluated on three mobile manipulation tasks with a Boston Dynamics Spot robot in both simulation and the real world. After benchmarking five approaches in simulation, we compare our method with two baselines in the real world, training directly on the real-world dataset without sim-to-real transfer. With only 20 demonstrations, ReMoBot outperforms the baselines, achieving high success rates in Table Uncover (70%) and Gap Cover (80%), while also showing promising performance on the more challenging Curtain Open task in the real-world setting. Furthermore, ReMoBot demonstrates generalization across varying robot positions, object sizes, and material types. Additional details are available at: <a target="_blank" rel="noopener" href="https://sites.google.com/view/remobot/home">https://sites.google.com/view/remobot/home</a> </p>
<blockquote>
<p>模仿学习（IL）算法通常会将经验转化为参数化的行为策略，以模仿专家演示。然而，在演示有限的情况下，现有方法往往难以生成准确的行为，特别是在部分可观察的情况下。为了解决这个问题，我们引入了一种few-shot IL方法ReMoBot，它直接从演示中检索信息，以解决使用自我中心视觉观察的移动操作任务。给定当前观察，ReMoBot利用视觉基础模型来识别与演示相关的内容，同时考虑个体观察和历史轨迹的视觉相似性。然后，运动选择策略会为机器人选择适当的命令，直到任务成功完成。ReMoBot在模拟和真实环境中对三项移动操作任务进行了评估，使用的是波士顿动力公司的Spot机器人。在模拟环境中对五种方法进行了基准测试后，我们在真实环境中将我们的方法与两个基准方法进行了比较，直接对真实世界数据集进行训练，无需模拟到真实的转移。仅凭20个演示，ReMoBot就超越了基准方法，在“表面揭开”（70%）和“缝隙覆盖”（80%）任务中取得了较高的成功率，同时在更具挑战性的“窗帘打开”任务中也表现出了有前景的性能。此外，ReMoBot展示了在不同机器人位置、物体大小和材质类型上的泛化能力。更多详细信息请访问：<a target="_blank" rel="noopener" href="https://sites.google.com/view/remobot/home%EF%BC%88%E9%93%BE%E6%8E%A5%E5%9C%B0%E5%9D%80%E4%B8%BARemoBot%E6%AD%A3%E5%AE%B6%EF%BC%89">https://sites.google.com/view/remobot/home（链接地址为ReMoBot官网）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15919v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>模仿学习（IL）算法通常将经验转化为参数化的行为策略以模仿专家演示。但在演示有限的情况下，现有方法往往难以生成准确动作，尤其在部分可观测条件下。为解决这一问题，我们提出了一种少样本IL方法ReMoBot，它直接从演示中获取信息，解决移动操作任务时的自我中心视觉观察问题。给定当前观察结果，ReMoBot利用视觉基础模型识别与演示相关的内容，考虑个体观察和历史轨迹的视觉相似性。运动选择策略则为机器人选择适当的命令，直至任务完成。ReMoBot在波士顿动力斑点机器人上的三项移动操作任务中的性能，在模拟和真实世界环境中都得到了评估。在模拟环境中对比五种方法后，我们的方法在真实世界环境中与两个基线进行了比较，直接在真实世界数据集上进行训练，无需模拟到现实的迁移。凭借仅有的20个演示，ReMoBot超越了基线方法，在表覆盖（70%）和间隙覆盖（80%）任务上取得了高成功率，同时在更具挑战性的窗帘打开任务上也表现出了有前景的性能，展现了在不同机器人位置、物体大小和材质类型上的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReMoBot是一种少样本模仿学习（IL）方法，直接从演示中获取信息以解决移动操作任务。</li>
<li>ReMoBot利用视觉基础模型来识别与当前观察结果相关的演示内容。</li>
<li>该方法考虑视觉相似性，既针对个体观察也关注历史轨迹。</li>
<li>通过运动选择策略，ReMoBot为机器人选择适当命令以完成任务。</li>
<li>在模拟和真实环境中，ReMoBot在三项移动操作任务上表现出色。</li>
<li>仅凭20个演示，ReMoBot在表覆盖和间隙覆盖任务上取得了高成功率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-fedaa80bc926eb5d423b2b44418cc742~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029767&auth_key=1760029767-0-0-5ed341b3f38b87fe5af9bcb141e01afd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-12be1533e06f2ffb2b741096727a04a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029774&auth_key=1760029774-0-0-dfc2c83b7e3bc2159322c1d7adfdf56a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-49f9d55befdbf8cd9064130fae33fd11~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029781&auth_key=1760029781-0-0-9479b84ac74f957636e3cf513f3a6756&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7e3f8f182fc20e823326aea988214435~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029788&auth_key=1760029788-0-0-ced58d81cebac0fdd75b98407e9dbfe3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da8710d2138f0317790518032754cf8e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029795&auth_key=1760029795-0-0-5128da30bc03cc33d14ab6af1970d4f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VLM-Agents-Generate-Their-Own-Memories-Distilling-Experience-into-Embodied-Programs-of-Thought"><a href="#VLM-Agents-Generate-Their-Own-Memories-Distilling-Experience-into-Embodied-Programs-of-Thought" class="headerlink" title="VLM Agents Generate Their Own Memories: Distilling Experience into   Embodied Programs of Thought"></a>VLM Agents Generate Their Own Memories: Distilling Experience into   Embodied Programs of Thought</h2><p><strong>Authors:Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, Katerina Fragkiadaki</strong></p>
<p>Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot learning but require high-quality demonstrations. We propose In-Context Abstraction Learning (ICAL), enabling VLM agents to transform suboptimal trajectories into high-quality training data through self-reflection and human feedback. Given imperfect task demonstrations, a VLM abstracts trajectories into generalized strategies and action annotations by correcting inefficiencies and annotating cognitive abstractions: causal relationships, object state changes, temporal subgoals, and task-relevant visual elements. These annotations are iteratively refined through human feedback during execution in similar environments. The resulting examples significantly improve decision-making when used for retrieval-augmented generation or fine-tuning. As the agent’s example library grows, it becomes more efficient at abstracting new examples, requiring less human feedback and fewer environment interactions. ICAL achieves state-of-the-art results across multiple benchmarks. In TEACh dialogue-based instruction following, combining fine-tuning and retrieval on ICAL examples outperforms raw human demonstrations and expert examples by 17.5% in goal-condition success. In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success 1.6x, while fine-tuned Qwen2-VL achieves 2.8x improvement over the base model. In Ego4D action forecasting, we surpass few-shot GPT-4V and remain competitive with supervised models. Our approach scales 2x better than raw demonstrations and significantly reduces manual prompt engineering requirements. </p>
<blockquote>
<p>大规模生成式语言和视觉语言模型（LLM和VLM）在少量样本学习方面表现出色，但需要高质量示范。我们提出上下文抽象学习（ICAL）方法，使VLM代理能够通过自我反思和人类反馈将次优轨迹转化为高质量训练数据。面对不完美的任务示范，VLM通过纠正无效行为并标注认知抽象（包括因果关系、对象状态变化、临时子目标和任务相关视觉元素），将轨迹转化为通用策略和行动注释。这些注释在执行类似环境的过程中通过人类反馈进行迭代优化。当用于增强生成或微调时，这些示例能显著改善决策效果。随着代理示例库的增长，它在抽象新示例方面变得更加高效，需要更少的人类反馈和环境交互。ICAL在多基准测试中实现了最先进的成果。在TEACh对话式指令遵循任务中，以ICAL示例进行微调与检索的结合，在目标条件成功率方面超越了原始人类示范和专业示例17.5%。在VisualWebArena任务中，结合ICAL的GPT-4V增强检索任务成功率提升1.6倍，而经过微调的Qwen2-VL较基础模型实现了2.8倍的提升。在Ego4D动作预测任务中，我们超越了少量样本的GPT-4V，并在监督模型保持竞争力。我们的方法比原始演示扩展了2倍，并显著减少了手动提示工程需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14596v6">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://ical-learning.github.io/">https://ical-learning.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>大规模生成式语言和视觉语言模型（LLMs和VLMs）在少样本学习中表现出色，但需要高质量示范。本文提出一种名为In-Context Abstraction Learning（ICAL）的方法，使VLM代理能够通过自我反思和人类反馈将次优轨迹转化为高质量训练数据。ICAL通过纠正效率不足并注释因果关系、对象状态变化、临时子目标和任务相关视觉元素等认知抽象来抽象化轨迹为通用策略和行动注释。这些注释在执行类似环境的过程中通过人类反馈进行迭代优化。使用ICAL生成的示例在检索增强生成或微调时，显著提高了决策能力。随着代理示例库的增长，它更有效地抽象出新的示例，需要更少的人类反馈和环境交互。ICAL在多个基准测试中实现了最佳结果。在TEACh对话式指令遵循任务中，结合ICAL示例的微调与检索超越了原始人类示范和专业示例的目标条件成功率，达到17.5%。在VisualWebArena中，使用ICAL的检索增强GPT-4V任务成功率提高了1.6倍，而经过调校的Qwen2-VL模型相对于基础模型提高了2.8倍。在Ego4D动作预测任务中，我们超越了少样本GPT-4V并保持与监督模型的竞争力。我们的方法比原始示范更好地扩展了2倍并大大减少了手动提示工程要求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模生成式语言和视觉语言模型（LLMs和VLMs）在少样本学习中表现出色，但需要高质量示范来提升性能。</li>
<li>提出了一种新的方法In-Context Abstraction Learning（ICAL），使VLM代理能够通过自我反思和人类反馈将次优轨迹转化为高质量训练数据。</li>
<li>ICAL通过抽象化轨迹为通用策略和行动注释，包括因果关系、对象状态变化、临时子目标和任务相关视觉元素等。</li>
<li>ICAL生成的示例在多个基准测试中实现了最佳结果，显著提高了决策能力。</li>
<li>随着代理示例库的增长，ICAL方法更有效地抽象出新的示例，并减少了人类反馈和环境交互的需求。</li>
<li>在TEACh对话式指令遵循任务中，结合ICAL的示例的微调与检索超越了原始人类示范和专业示例的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14596">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a309fc6b3817dab3ec02c2fc7473c601~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029802&auth_key=1760029802-0-0-eebe61f585fec00106089d851614b2ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f50a399f8145c12177ffd0d2239e90bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029810&auth_key=1760029810-0-0-622941f386748774046d707d9fa24d33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf3e5fc2d4993a836fd351ecc79180b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029816&auth_key=1760029816-0-0-08a9a7f45dc48a3052ff544b6a24e909&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-f69f1399f3fe648a56e7d18c8c299852~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029823&auth_key=1760029823-0-0-77f5067bf15435e24f7335d14aca11d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-09-20  Roll Your Eyes Gaze Redirection via Explicit 3D Eyeball Rotation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-19f877595158f8e9bd206c4ee7e85cb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029069&auth_key=1760029069-0-0-898edd11119f365ab5fe9bfb7594c753&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-09-20  ScaleCUA Scaling Open-Source Computer Use Agents with Cross-Platform   Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
