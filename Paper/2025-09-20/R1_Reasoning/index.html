<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  Generalizable Geometric Image Caption Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15194v1/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-20-æ›´æ–°"><a href="#2025-09-20-æ›´æ–°" class="headerlink" title="2025-09-20 æ›´æ–°"></a>2025-09-20 æ›´æ–°</h1><h2 id="Generalizable-Geometric-Image-Caption-Synthesis"><a href="#Generalizable-Geometric-Image-Caption-Synthesis" class="headerlink" title="Generalizable Geometric Image Caption Synthesis"></a>Generalizable Geometric Image Caption Synthesis</h2><p><strong>Authors:Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Renjie Pi, Shizhe Diao, Tong Zhang</strong></p>
<p>Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8%\text{-}4.8%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4%\text{-}3.9%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å„ç§å®é™…åº”ç”¨ï¼Œè¿™äº›åº”ç”¨éœ€è¦å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡æœ€è¿‘æœ‰è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨è§£å†³å¤æ‚çš„å‡ ä½•é—®é¢˜æ—¶ä»ç„¶æ„Ÿåˆ°å›°éš¾ã€‚ä¸»è¦æŒ‘æˆ˜æºäºç¼ºä¹ç”¨äºç†è§£å‡ ä½•å›¾åƒçš„é«˜è´¨é‡å›¾åƒæ–‡æœ¬å¯¹æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°åŸºäºæ¨¡æ¿çš„æ•°æ®åˆæˆç®¡é“é€šå¸¸æ— æ³•æ¨å¹¿åˆ°å…¶é¢„å®šä¹‰æ¨¡æ¿ä¹‹å¤–çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰çš„è¡¥å……è¿‡ç¨‹å¼•å…¥æ•°æ®ç”Ÿæˆç®¡é“æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚é€šè¿‡é‡‡ç”¨RLVRæ¥ç²¾ç‚¼ç”±50ç§åŸºæœ¬å‡ ä½•å…³ç³»åˆæˆçš„å‡ ä½•å›¾åƒçš„æ ‡é¢˜ï¼Œå¹¶ä½¿ç”¨æ¥æºäºæ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡çš„å¥–åŠ±ä¿¡å·ï¼Œæˆ‘ä»¬çš„ç®¡é“æˆåŠŸåœ°æ•æ‰åˆ°äº†è§£å†³å‡ ä½•é—®é¢˜çš„å…³é”®ç‰¹å¾ã€‚è¿™å®ç°äº†æ›´å¥½çš„ä»»åŠ¡æ³›åŒ–ï¼Œå¹¶äº§ç”Ÿäº†ä¸å°çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨è¶…å‡ºåˆ†å¸ƒçš„åœºæ™¯ä¸‹ï¼Œç”Ÿæˆçš„æ•°æ®é›†ä¹Ÿèƒ½æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸€èˆ¬æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ•°å­¦è§†é‡ï¼ˆMathVistaï¼‰å’Œæ•°å­¦å®‡å®™ï¼ˆMathVerseï¼‰çš„éå‡ ä½•è¾“å…¥å›¾åƒç»Ÿè®¡ã€ç®—æœ¯ã€ä»£æ•°å’Œæ•°å€¼ä»»åŠ¡ä¸­å‡†ç¡®ç‡æé«˜äº†2.8%~4.8%ï¼Œåœ¨MMUçš„è‰ºæœ¯ã€è®¾è®¡ã€æŠ€æœ¯å’Œå·¥ç¨‹ä»»åŠ¡ä¸­å‡†ç¡®ç‡æé«˜äº†2.4%~3.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15217v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éœ€è¦å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å®é™…åº”ç”¨ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å¤æ‚å‡ ä½•é—®é¢˜æ—¶ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé€šè¿‡ç²¾ç‚¼å‡ ä½•å›¾åƒåˆæˆä¸­çš„å­—å¹•å¹¶ä½¿ç”¨æ¥è‡ªæ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡çš„å¥–åŠ±ä¿¡å·ï¼ŒæˆåŠŸè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚è¿™ä¸ä»…æé«˜äº†æ¨¡å‹çš„å‡ ä½•é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹åœ¨ç»Ÿè®¡ã€ç®—æœ¯ã€ä»£æ•°å’Œæ•°å€¼ä»»åŠ¡ä¸­çš„ä¸€èˆ¬æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚å‡ ä½•é—®é¢˜æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹é«˜è´¨é‡çš„å›¾åƒæ–‡æœ¬å¯¹æ•°æ®é›†æ˜¯ç†è§£å‡ ä½•å›¾åƒçš„ä¸»è¦éšœç¢ä¹‹ä¸€ã€‚</li>
<li>å¤§å¤šæ•°åŸºäºæ¨¡æ¿çš„æ•°æ®åˆæˆç®¡é“æ— æ³•æ¨å¹¿åˆ°è¶…å‡ºå…¶é¢„å®šä¹‰æ¨¡æ¿çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç²¾ç‚¼å‡ ä½•å›¾åƒåˆæˆçš„å­—å¹•ï¼Œè¯¥ç®¡é“æˆåŠŸæ•æ‰äº†è§£å†³å‡ ä½•é—®é¢˜çš„å…³é”®ç‰¹å¾ã€‚</li>
<li>åœ¨æ•°å­¦å’Œéå‡ ä½•å›¾åƒçš„ä»»åŠ¡ä¸­ï¼Œç”Ÿæˆçš„æ•°æ®é›†å¢å¼ºäº†æ¨¡å‹çš„ä¸€èˆ¬æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¸¦æ¥æ˜¾è‘—çš„å‡†ç¡®æ€§æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15217v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15217v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15217v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15217v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation"><a href="#Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation" class="headerlink" title="Evolving Language Models without Labels: Majority Drives Selection,   Novelty Promotes Variation"></a>Evolving Language Models without Labels: Majority Drives Selection,   Novelty Promotes Variation</h2><p><strong>Authors:Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu</strong></p>
<p>Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the modelâ€™s inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRLâ€™s 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­ï¼Œéœ€è¦æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡ç­¾æˆ–å¤–éƒ¨è¯„åˆ¤çš„æƒ…å†µä¸‹è‡ªæˆ‘æ”¹è¿›ã€‚ç°æœ‰çš„æ— æ ‡ç­¾æ–¹æ³•ï¼Œå¦‚ç½®ä¿¡åº¦æœ€å°åŒ–ã€è‡ªæˆ‘ä¸€è‡´æ€§æˆ–å¤šæ•°æŠ•ç¥¨ç›®æ ‡ï¼Œè™½ç„¶å¯ä»¥ç¨³å®šå­¦ä¹ ï¼Œä½†ä¼šé€æ­¥å‡å°‘æ¢ç´¢ï¼Œå¯¼è‡´ç†µå´©æºƒï¼šç”Ÿæˆçš„å†…å®¹å˜å¾—æ›´çŸ­ã€æ›´å°‘æ ·åŒ–å’Œè„†å¼±ã€‚ä¸åŒäºç°æœ‰çš„æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰æ–¹æ³•ï¼Œåè€…ä¸»è¦é€‚åº”æ‰‹å¤´çš„å³æ—¶æ— æ ‡ç­¾æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ›´ä¸ºå¹¿æ³›ï¼šå³åœ¨ä¸å½±å“æ¨¡å‹å›ºæœ‰çš„æ¢ç´¢èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›çš„æƒ…å†µä¸‹å®ç°ä¸€èˆ¬æ€§çš„æ”¹è¿›ï¼Œå³è¿›åŒ–ã€‚æˆ‘ä»¬å½¢å¼åŒ–äº†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶æå‡ºäº†é¢å‘è¿›åŒ–çš„æ— æ ‡ç­¾å¼ºåŒ–å­¦ä¹ ï¼ˆEVOL-RLï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„è§„åˆ™ï¼Œåœ¨æ ‡ç­¾è®¾ç½®ä¸‹å®ç°ç¨³å®šæ€§å’Œå˜å¼‚çš„ç»“åˆã€‚EVOL-RLå°†å¤šæ•°æŠ•ç¥¨çš„ç­”æ¡ˆä½œä¸ºç¨³å®šçš„é”šç‚¹ï¼ˆé€‰æ‹©ï¼‰ï¼ŒåŒæ—¶æ·»åŠ ä¸€ä¸ªæ„è¯†åˆ°æ–°é¢–æ€§çš„å¥–åŠ±ï¼Œè¯¥å¥–åŠ±æœ‰åˆ©äºé‚£äº›æ¨ç†ä¸å·²äº§ç”Ÿçš„ç»“æœä¸åŒçš„å›ç­”ï¼ˆå˜å¼‚ï¼‰ï¼Œå¹¶åœ¨è¯­ä¹‰ç©ºé—´ä¸­è¿›è¡Œæµ‹é‡ã€‚ç»“åˆGPROå®ç°çš„EVOL-RLè¿˜ä½¿ç”¨ä¸å¯¹ç§°è£å‰ªæ¥ä¿ç•™å¼ºä¿¡å·å’Œç†µæ­£åˆ™åŒ–æ¥ç»´æŒæœç´¢ã€‚è¿™ç§â€œå¤šæ•°æŠ•ç¥¨é€‰æ‹©+æ–°å¥‡å˜å¼‚â€çš„è®¾è®¡å¯ä»¥é˜²æ­¢å´©æºƒï¼Œä¿æŒæ›´é•¿çš„ã€æ›´å…·ä¿¡æ¯é‡çš„æ€è€ƒé“¾ï¼Œå¹¶æé«˜äº†pass@1å’Œpass@nã€‚EVOL-RLå§‹ç»ˆä¼˜äºä»…ä½¿ç”¨æŠ•ç¥¨é€‰æ‹©çš„TTRLåŸºçº¿ï¼›ä¾‹å¦‚ï¼Œåœ¨æ— æ ‡ç­¾AIME24ä¸Šè¿›è¡Œè®­ç»ƒä½¿Qwen3-4B-Base AIME25çš„pass@1ä»TTRLçš„4.6%æå‡åˆ°16.4%ï¼Œpass@16ä»18.5%æå‡åˆ°37.9%ã€‚EVOL-RLä¸ä»…é˜²æ­¢äº†å¤šæ ·æ€§å´©æºƒï¼Œè¿˜æé«˜äº†è·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼ˆä¾‹å¦‚GPQAï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†EVOL-RLåœ¨RLVRè®¾ç½®ä¸­ä¹Ÿæå‡äº†æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è¿›è¡Œè®­ç»ƒè¶Šæ¥è¶Šæ™®éï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­éœ€è¦æ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘æ”¹è¿›è€Œæ— éœ€æ ‡ç­¾æˆ–å¤–éƒ¨è¯„åˆ¤ã€‚ç°æœ‰çš„æ— æ ‡ç­¾æ–¹æ³•å¦‚ä¿¡å¿ƒæœ€å°åŒ–ã€è‡ªæˆ‘ä¸€è‡´æ€§æˆ–å¤šæ•°æŠ•ç¥¨ç›®æ ‡ç­‰ï¼Œè™½ç„¶ç¨³å®šäº†å­¦ä¹ ï¼Œä½†ä¼šé€æ­¥å‡å°‘æ¢ç´¢ï¼Œå¯¼è‡´ç†µå´©æºƒï¼Œç”Ÿæˆçš„æ–‡æœ¬å˜å¾—ç®€çŸ­ã€ç¼ºä¹å¤šæ ·æ€§ä¸”è„†å¼±ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºEVOL-RLçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ— æ ‡ç­¾ä¸‹çš„é€šç”¨æ”¹è¿›è€Œä¸ç‰ºç‰²æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚EVOL-RLå°†å¤šæ•°æŠ•ç¥¨çš„ç­”æ¡ˆä½œä¸ºç¨³å®šçš„é”šç‚¹ï¼ˆé€‰æ‹©ï¼‰ï¼ŒåŒæ—¶å¢åŠ ä¸€ä¸ªé‡è§†å“åº”ä¸­ä¸åŒäºå·²ç”Ÿæˆå†…å®¹çš„æ–°é¢–æ€§å¥–åŠ±ï¼ˆå˜å¼‚ï¼‰ï¼Œå¹¶åœ¨è¯­ä¹‰ç©ºé—´ä¸­è¿›è¡Œæµ‹é‡ã€‚è¯¥æ–¹æ³•é˜²æ­¢äº†å´©æºƒï¼Œä¿æŒäº†æ›´é•¿çš„æ€è€ƒé“¾ï¼Œå¹¶æé«˜äº†pass@1å’Œpass@nã€‚EVOL-RLåœ¨å¤šä¸ªå®éªŒä¸Šå‡è¡¨ç°å‡ºè¶…è¶Šä»…ä¾èµ–å¤šæ•°æŠ•ç¥¨çš„åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±è®­ç»ƒè¿‡ç¨‹ä¸­é¢ä¸´ç°å®ä¸–ç•Œçš„éƒ¨ç½²æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ— æ ‡ç­¾æ–¹æ³•ç¨³å®šäº†å­¦ä¹ ä½†å‡å°‘äº†æ¢ç´¢ï¼Œå¯¼è‡´ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§å’Œåˆ›æ–°æ€§ä¸‹é™ã€‚</li>
<li>EVOL-RLæ–¹æ³•æ—¨åœ¨åœ¨æ— æ ‡ç­¾ä¸‹å®ç°æ›´å¹¿æ³›çš„æ”¹è¿›ï¼Œæ—¨åœ¨å¹³è¡¡ç¨³å®šæ€§å’Œæ¢ç´¢èƒ½åŠ›ã€‚</li>
<li>EVOL-RLç»“åˆäº†å¤šæ•°æŠ•ç¥¨ç¨³å®šå’ŒåŸºäºæ–°é¢–æ€§çš„å¥–åŠ±ï¼ˆå˜å¼‚ï¼‰ï¼Œä»¥é˜²æ­¢æ¨¡å‹ç”Ÿæˆå•ä¸€ç­”æ¡ˆå¹¶ä¿æŒè¯­ä¹‰å¤šæ ·æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨GRPOå’Œä¸å¯¹ç§°è£å‰ªç­‰æŠ€æœ¯å®ç°EVOL-RLï¼Œæœ‰æ•ˆä¿ç•™å¼ºä¿¡å·å¹¶ç»´æŒæœç´¢ã€‚</li>
<li>EVOL-RLåœ¨å¤šç§å®éªŒè®¾ç½®ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬æ— æ ‡ç­¾AIME24è®­ç»ƒå’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15194v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15194v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15194v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding"><a href="#Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding" class="headerlink" title="Unleashing the Potential of Multimodal LLMs for Zero-Shot   Spatio-Temporal Video Grounding"></a>Unleashing the Potential of Multimodal LLMs for Zero-Shot   Spatio-Temporal Video Grounding</h2><p><strong>Authors:Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lau</strong></p>
<p>Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the modelâ€™s attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at <a target="_blank" rel="noopener" href="https://github.com/zaiquanyang/LLaVA_Next_STVG">https://github.com/zaiquanyang/LLaVA_Next_STVG</a>. </p>
<blockquote>
<p>æ—¶ç©ºè§†é¢‘å®šä½ï¼ˆSTVGï¼‰æ—¨åœ¨æ ¹æ®è¾“å…¥çš„æ–‡æœ¬æŸ¥è¯¢å®šä½è§†é¢‘ä¸­çš„æ—¶ç©ºç®¡é“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¢ç´¢STVGä¸­çš„é›¶æ ·æœ¬è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æ­ç¤ºäº†å…³äºMLLMsçš„ä¸¤ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰MLLMså€¾å‘äºåŠ¨æ€åˆ†é…ç‰¹æ®Šä»¤ç‰Œï¼Œç§°ä¸ºâ€œå®šä½ä»¤ç‰Œâ€ï¼Œç”¨äºå®šä½æ–‡æœ¬æŸ¥è¯¢ï¼›ï¼ˆ2ï¼‰ç”±äºæ— æ³•å®Œå…¨æ•´åˆæ–‡æœ¬æŸ¥è¯¢ä¸­çš„çº¿ç´¢ï¼ˆä¾‹å¦‚å±æ€§ã€åŠ¨ä½œï¼‰è¿›è¡Œæ¨ç†ï¼ŒMLLMsç»å¸¸é­å—æ¬¡ä¼˜å®šä½ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMLLMçš„é›¶æ ·æœ¬STVGæ¡†æ¶ï¼ŒåŒ…æ‹¬æ–°å‹åˆ†è§£æ—¶ç©ºé«˜äº®ï¼ˆDSTHï¼‰å’Œæ—¶é—´å¢å¼ºè£…é…ï¼ˆTASï¼‰ç­–ç•¥ï¼Œä»¥é‡Šæ”¾MLLMçš„æ¨ç†èƒ½åŠ›ã€‚DSTHç­–ç•¥é¦–å…ˆæ˜¯å°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºå±æ€§å’ŒåŠ¨ä½œå­æŸ¥è¯¢ï¼Œä»¥è¯¢é—®ç›®æ ‡åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šçš„å­˜åœ¨ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨æ–°å‹çš„å¯¹æ•°å¼•å¯¼å†æ³¨æ„ï¼ˆLRAï¼‰æ¨¡å—æ¥å­¦ä¹ ä½œä¸ºç©ºé—´å’Œæ—¶é—´æç¤ºçš„æ½œåœ¨å˜é‡ï¼Œé€šè¿‡è§„èŒƒåŒ–æ¯ä¸ªå­æŸ¥è¯¢çš„ä»¤ç‰Œé¢„æµ‹ã€‚è¿™äº›æç¤ºåˆ†åˆ«çªå‡ºå±æ€§å’ŒåŠ¨ä½œçº¿ç´¢ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ä¸ç©ºé—´å’Œæ—¶é—´ç›¸å…³çš„å¯é è§†è§‰åŒºåŸŸã€‚æ­¤å¤–ï¼Œç”±äºå±æ€§å­æŸ¥è¯¢çš„ç©ºé—´å®šä½åœ¨æ—¶é—´ä¸Šåº”è¯¥æ˜¯ä¸€è‡´çš„ï¼Œæˆ‘ä»¬å¼•å…¥äº†TASç­–ç•¥ï¼Œä½¿ç”¨åŸå§‹è§†é¢‘å¸§å’Œæ—¶é—´å¢å¼ºå¸§ä½œä¸ºè¾“å…¥æ¥ç»„è£…é¢„æµ‹ï¼Œä»¥å¸®åŠ©æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨å„ç§MLLMsä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‰ä¸ªå¸¸è§çš„STVGåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å…¶è¶…è¶Šç°æœ‰æŠ€æœ¯æ–¹æ³•çš„æ•ˆæœã€‚ç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/zaiquanyang/LLaVA_Next_STVG%E3%80%82">https://github.com/zaiquanyang/LLaVA_Next_STVGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15178v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é›¶æ ·æœ¬æ—¶ç©ºè§†é¢‘å®šä½ï¼ˆSTVGï¼‰æ–¹æ³•ã€‚æ–‡ç« æ­ç¤ºäº†MLLMsåœ¨STVGä¸­çš„ä¸¤ä¸ªå…³é”®è§è§£ï¼Œå¹¶æå‡ºäº†åŸºäºMLLMçš„é›¶æ ·æœ¬æ¡†æ¶ï¼ŒåŒ…æ‹¬åˆ†è§£æ—¶ç©ºçªå‡ºæ˜¾ç¤ºï¼ˆDSTHï¼‰å’Œæ—¶é—´å¢å¼ºè£…é…ï¼ˆTASï¼‰ç­–ç•¥ï¼Œä»¥é‡Šæ”¾MLLMçš„æ¨ç†èƒ½åŠ›ã€‚DSTHç­–ç•¥å°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºå±æ€§å­æŸ¥è¯¢å’ŒåŠ¨ä½œå­æŸ¥è¯¢ï¼Œä»¥å­¦ä¹ ç©ºé—´å’Œæ—¶é—´æç¤ºã€‚TASç­–ç•¥é€šè¿‡ç»“åˆåŸå§‹è§†é¢‘å¸§å’Œæ—¶é—´å¢å¼ºå¸§æ¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚åœ¨å¤šä¸ªMLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå¸¸è§çš„STVGåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†æ—¶ç©ºè§†é¢‘å®šä½ï¼ˆSTVGï¼‰çš„ç›®æ ‡ï¼Œæ—¨åœ¨é€šè¿‡æ–‡æœ¬æŸ¥è¯¢å®šä½è§†é¢‘ä¸­çš„æ—¶ç©ºç®¡é“ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¢ç´¢STVGçš„é›¶æ ·æœ¬è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ­ç¤ºäº†MLLMsåœ¨STVGä¸­çš„ä¸¤ä¸ªå…³é”®è§è§£ï¼šåŠ¨æ€åˆ†é…ç‰¹æ®Šæ ‡è®°ç”¨äºæ–‡æœ¬æŸ¥è¯¢å®šä½ä»¥åŠç”±äºæ— æ³•å®Œå…¨æ•´åˆæ–‡æœ¬æŸ¥è¯¢ä¸­çš„çº¿ç´¢è€Œå¯¼è‡´çš„æ¬¡ä¼˜å®šä½é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºMLLMçš„é›¶æ ·æœ¬æ¡†æ¶ï¼ŒåŒ…æ‹¬åˆ†è§£æ—¶ç©ºçªå‡ºæ˜¾ç¤ºï¼ˆDSTHï¼‰å’Œæ—¶é—´å¢å¼ºè£…é…ï¼ˆTASï¼‰ç­–ç•¥ã€‚</li>
<li>DSTHç­–ç•¥é€šè¿‡å°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºå±æ€§å­æŸ¥è¯¢å’ŒåŠ¨ä½œå­æŸ¥è¯¢æ¥å­¦ä¹ ç©ºé—´å’Œæ—¶é—´æç¤ºï¼Œæé«˜æ¨¡å‹å®šä½ç²¾åº¦ã€‚</li>
<li>TASç­–ç•¥ç»“åˆåŸå§‹è§†é¢‘å¸§å’Œæ—¶é—´å¢å¼ºå¸§æ¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªMLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå¸¸è§çš„STVGåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15178v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15178v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15178v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15178v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Data-Rewriting-for-Stable-Off-Policy-Supervised-Fine-Tuning"><a href="#Mind-the-Gap-Data-Rewriting-for-Stable-Off-Policy-Supervised-Fine-Tuning" class="headerlink" title="Mind the Gap: Data Rewriting for Stable Off-Policy Supervised   Fine-Tuning"></a>Mind the Gap: Data Rewriting for Stable Off-Policy Supervised   Fine-Tuning</h2><p><strong>Authors:Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong Qin</strong></p>
<p>Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Off-Policy-SFT">https://github.com/NKU-HLT/Off-Policy-SFT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥è¢«è§†ä¸ºä¸€ç§ç¦»çº¿ç­–ç•¥å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­ä¸“å®¶æ¼”ç¤ºæ¥è‡ªå›ºå®šçš„è¡Œä¸ºç­–ç•¥ï¼Œè€Œè®­ç»ƒçš„ç›®æ ‡æ˜¯ä¼˜åŒ–ç›®æ ‡ç­–ç•¥ã€‚é‡è¦æ€§é‡‡æ ·æ˜¯çº æ­£è¿™ç§åˆ†å¸ƒä¸åŒ¹é…çš„æ ‡å‡†å·¥å…·ï¼Œä½†ç­–ç•¥å·®è·è¾ƒå¤§ä¼šå¯¼è‡´æ–¹å·®é«˜å’Œè®­ç»ƒä¸ç¨³å®šã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨KLæƒ©ç½šæˆ–è£å‰ªæ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œè¿™äº›æ–¹æ³•è¢«åŠ¨çº¦æŸæ›´æ–°è€Œä¸æ˜¯ä¸»åŠ¨ç¼©å°å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®é‡å†™æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¿ç•™æ­£ç¡®è§£å†³æ–¹æ¡ˆä½œä¸ºåœ¨çº¿ç­–ç•¥æ•°æ®ï¼Œå¹¶å¼•å¯¼è§£å†³æ–¹å¼é‡å†™é”™è¯¯æ•°æ®ï¼Œä»…åœ¨å¿…è¦æ—¶æ‰å›é€€åˆ°ä¸“å®¶æ¼”ç¤ºã€‚è¿™åœ¨å¯¹ä¼˜åŒ–ä¹‹å‰å¯¹é½è®­ç»ƒåˆ†å¸ƒä¸ç›®æ ‡ç­–ç•¥ï¼Œå‡å°‘é‡è¦æ€§é‡‡æ ·çš„æ–¹å·®å¹¶ç¨³å®šç¦»çº¿å¾®è°ƒã€‚åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æ™®é€šçš„SFTå’Œæœ€æ–°çš„åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¸€è‡´ä¸”æ˜¾è‘—çš„æ”¶ç›Šã€‚æ•°æ®å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Off-Policy-SFT%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NKU-HLT/Off-Policy-SFTä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªç¦»çº¿ç­–ç•¥å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­ä¸“å®¶æ¼”ç¤ºæ¥è‡ªå›ºå®šçš„è¡Œä¸ºç­–ç•¥ï¼Œè€Œè®­ç»ƒæ—¨åœ¨ä¼˜åŒ–ç›®æ ‡ç­–ç•¥ã€‚é‡è¦æ€§é‡‡æ ·æ˜¯çº æ­£è¿™ç§åˆ†å¸ƒä¸åŒ¹é…çš„æ ‡å‡†å·¥å…·ï¼Œä½†å¤§çš„ç­–ç•¥é—´éš”ä¼šå¯¼è‡´é«˜æ–¹å·®å’Œè®­ç»ƒä¸ç¨³å®šã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨KLæƒ©ç½šæˆ–è£å‰ªæ¥è¢«åŠ¨çº¦æŸæ›´æ–°ï¼Œè€Œä¸æ˜¯ä¸»åŠ¨å‡å°‘é—´éš”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®é‡å†™æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¿æŒæ­£ç¡®çš„è§£å†³æ–¹æ¡ˆä½œä¸ºåœ¨çº¿ç­–ç•¥æ•°æ®ï¼Œå¹¶å¼•å¯¼è§£å†³ä¸æ­£ç¡®çš„é—®é¢˜ï¼Œåªåœ¨å¿…è¦æ—¶å›é€€åˆ°ä¸“å®¶æ¼”ç¤ºï¼Œä»è€Œä¸»åŠ¨ç¼©å°ç­–ç•¥é—´éš”ã€‚è¿™å°†å¯¹é½è®­ç»ƒåˆ†å¸ƒä¸ç›®æ ‡ç­–ç•¥ï¼Œå‡å°‘é‡è¦æ€§é‡‡æ ·çš„æ–¹å·®å¹¶ç¨³å®šç¦»çº¿ç­–ç•¥çš„å¾®è°ƒã€‚åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æ ‡å‡†çš„SFTå’Œå½“å‰æœ€å…ˆè¿›çš„åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å…·æœ‰ä¸€è‡´ä¸”æ˜¾è‘—çš„ä¼˜ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯è§†ä¸ºç¦»çº¿ç­–ç•¥å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­ä¸“å®¶æ¼”ç¤ºä¸è®­ç»ƒç›®æ ‡ç­–ç•¥ä¹‹é—´å­˜åœ¨åˆ†å¸ƒä¸åŒ¹é…ã€‚</li>
<li>é‡è¦æ€§é‡‡æ ·æ˜¯çº æ­£è¿™ç§åˆ†å¸ƒä¸åŒ¹é…çš„æ ‡å‡†å·¥å…·ï¼Œä½†å­˜åœ¨é«˜æ–¹å·®å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨KLæƒ©ç½šæˆ–è£å‰ªè¢«åŠ¨çº¦æŸæ›´æ–°ï¼Œæ•ˆæœæœ‰é™ã€‚</li>
<li>æå‡ºçš„æ•°æ®é‡å†™æ¡†æ¶é€šè¿‡ä¸»åŠ¨ç¼©å°ç­–ç•¥é—´éš”ï¼Œä¿æŒæ­£ç¡®è§£å†³æ–¹æ¡ˆå¹¶å¼•å¯¼è§£å†³é”™è¯¯é—®é¢˜ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ä¸“å®¶æ¼”ç¤ºç›¸ç»“åˆï¼Œå¯¹é½è®­ç»ƒåˆ†å¸ƒä¸ç›®æ ‡ç­–ç•¥ï¼Œå‡å°‘é‡è¦æ€§é‡‡æ ·çš„æ–¹å·®ã€‚</li>
<li>åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ¡†æ¶ç›¸æ¯”æ ‡å‡†SFTå’ŒDFTå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MedFact-R1-Towards-Factual-Medical-Reasoning-via-Pseudo-Label-Augmentation"><a href="#MedFact-R1-Towards-Factual-Medical-Reasoning-via-Pseudo-Label-Augmentation" class="headerlink" title="MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label   Augmentation"></a>MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label   Augmentation</h2><p><strong>Authors:Gengliang Li, Rongyu Chen, Bin Li, Linlin Yang, Guodong Ding</strong></p>
<p>Ensuring factual consistency and reliable reasoning remains a critical challenge for medical vision-language models. We introduce MEDFACT-R1, a two-stage framework that integrates external knowledge grounding with reinforcement learning to improve the factual medical reasoning. The first stage uses pseudo-label supervised fine-tuning (SFT) to incorporate external factual expertise; while the second stage applies Group Relative Policy Optimization (GRPO) with four tailored factual reward signals to encourage self-consistent reasoning. Across three public medical QA benchmarks, MEDFACT-R1 delivers up to 22.5% absolute improvement in factual accuracy over previous state-of-the-art methods. Ablation studies highlight the necessity of pseudo-label SFT cold start and validate the contribution of each GRPO reward, underscoring the synergy between knowledge grounding and RL-driven reasoning for trustworthy medical AI. Codes are released at <a target="_blank" rel="noopener" href="https://github.com/Garfieldgengliang/MEDFACT-R1">https://github.com/Garfieldgengliang/MEDFACT-R1</a>. </p>
<blockquote>
<p>ç¡®ä¿äº‹å®ä¸€è‡´æ€§å’Œå¯é çš„æ¨ç†ä»ç„¶æ˜¯åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹çš„å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä»‹ç»äº†MEDFACT-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¤–éƒ¨çŸ¥è¯†æ¥åœ°å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜åŒ»å­¦äº‹å®çš„æ¨ç†èƒ½åŠ›ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨ä¼ªæ ‡ç­¾ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥èå…¥å¤–éƒ¨äº‹å®ä¸“ä¸šçŸ¥è¯†ï¼›è€Œç¬¬äºŒé˜¶æ®µé‡‡ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå››ç§å®šåˆ¶çš„å®æ—¶å¥–åŠ±ä¿¡å·æ¥é¼“åŠ±è‡ªæˆ‘ä¸€è‡´çš„æ¨ç†ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€çš„åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMEDFACT-R1åœ¨äº‹å®å‡†ç¡®æ€§æ–¹é¢è¾ƒä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†é«˜è¾¾22.5%çš„ç»å¯¹å¹…åº¦ã€‚æ¶ˆèç ”ç©¶çªå‡ºäº†ä¼ªæ ‡ç­¾SFTå†·å¯åŠ¨çš„å¿…è¦æ€§ï¼ŒéªŒè¯äº†æ¯ä¸ªGRPOå¥–åŠ±çš„è´¡çŒ®ï¼Œå¼ºè°ƒäº†çŸ¥è¯†æ¥åœ°å’ŒRLé©±åŠ¨æ¨ç†ä¹‹é—´çš„ååŒä½œç”¨ï¼Œä»¥å®ç°å¯ä¿¡èµ–çš„åŒ»å­¦äººå·¥æ™ºèƒ½ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Garfieldgengliang/MEDFACT-R1%E3%80%82">https://github.com/Garfieldgengliang/MEDFACT-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15154v1">PDF</a> Tech report</p>
<p><strong>Summary</strong><br>åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¿éšœäº‹å®ä¸€è‡´æ€§å’Œå¯é æ¨ç†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºMEDFACT-R1ï¼Œä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ•´åˆå¤–éƒ¨çŸ¥è¯†æ¥åœ°ä¸å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æ”¹å–„åŒ»å­¦äº‹å®æ¨ç†ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨ä¼ªæ ‡ç­¾ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰èå…¥å¤–éƒ¨äº‹å®ä¸“ä¸šçŸ¥è¯†ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸å››ä¸ªå®šåˆ¶çš„äº‹å®å¥–åŠ±ä¿¡å·ï¼Œä»¥é¼“åŠ±è‡ªæˆ‘ä¸€è‡´æ¨ç†ã€‚åœ¨ä¸‰ä¸ªå…¬å…±åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMEDFACT-R1åœ¨äº‹å®å‡†ç¡®æ€§æ–¹é¢è¾ƒä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†é«˜è¾¾22.5%çš„ç»å¯¹å‡†ç¡®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MEDFACT-R1æ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶æ•´åˆäº†å¤–éƒ¨çŸ¥è¯†æ¥åœ°ä¸å¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨æé«˜äº‹å®æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡ä¼ªæ ‡ç­¾ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰èå…¥å¤–éƒ¨äº‹å®ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸å®šåˆ¶çš„äº‹å®å¥–åŠ±ä¿¡å·ï¼Œé¼“åŠ±è‡ªæˆ‘ä¸€è‡´æ¨ç†ã€‚</li>
<li>MEDFACT-R1åœ¨ä¸‰ä¸ªåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„äº‹å®å‡†ç¡®æ€§æå‡ã€‚</li>
<li>æ¶ˆèç ”ç©¶å¼ºè°ƒäº†ä¼ªæ ‡ç­¾SFTå†·å¯åŠ¨çš„å¿…è¦æ€§ï¼Œå¹¶éªŒè¯äº†æ¯ä¸ªGRPOå¥–åŠ±çš„è´¡çŒ®ã€‚</li>
<li>çŸ¥è¯†æ¥åœ°å’ŒRLé©±åŠ¨æ¨ç†ä¹‹é—´çš„ååŒä½œç”¨å¯¹äºæ„å»ºå¯ä¿¡èµ–çš„åŒ»å­¦AIè‡³å…³é‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15154v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15154v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15154v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15154v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TDRM-Smooth-Reward-Models-with-Temporal-Difference-for-LLM-RL-and-Inference"><a href="#TDRM-Smooth-Reward-Models-with-Temporal-Difference-for-LLM-RL-and-Inference" class="headerlink" title="TDRM: Smooth Reward Models with Temporal Difference for LLM RL and   Inference"></a>TDRM: Smooth Reward Models with Temporal Difference for LLM RL and   Inference</h2><p><strong>Authors:Dan Zhang, Min Cai, Jonathan Li, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang</strong></p>
<p>Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences during training. This temporal-difference (TD) regularization produces smooth rewards and improves alignment with long-term objectives. Incorporating TDRM into the actor-critic style online RL loop yields consistent empirical gains. It is worth noting that TDRM is a supplement to verifiable reward methods, and both can be used in series. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL â€“ achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain â€“ and yield higher-quality language model policies on 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at <a target="_blank" rel="noopener" href="https://github.com/THUDM/TDRM">https://github.com/THUDM/TDRM</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹åœ¨ç»“åˆè¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†æ—¶é—´éªŒè¯ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¥–åŠ±æ¨¡å‹é€šå¸¸ç¼ºä¹æ—¶é—´ä¸€è‡´æ€§ï¼Œå¯¼è‡´ç­–ç•¥æ›´æ–°æ— æ•ˆå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸ç¨³å®šã€‚æˆ‘ä»¬å¼•å…¥äº†TDRMæ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¶é—´å·®å¼‚æ¥å­¦ä¹ æ›´å¹³æ»‘ã€æ›´å¯é çš„å¥–åŠ±æ¨¡å‹ã€‚è¿™ç§æ—¶é—´å·®å¼‚ï¼ˆTDï¼‰æ­£åˆ™åŒ–äº§ç”Ÿäº†å¹³æ»‘çš„å¥–åŠ±ï¼Œå¹¶æ”¹å–„äº†ä¸é•¿æœŸç›®æ ‡çš„å¯¹é½ã€‚å°†TDRMçº³å…¥æ¼”å‘˜è¯„è®ºå®¶é£æ ¼çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¾ªç¯ä¸­ï¼Œå¯ä»¥è·å¾—æŒç»­çš„ç»éªŒæ”¶ç›Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTDRMæ˜¯éªŒè¯å¥–åŠ±æ–¹æ³•çš„ä¸€ç§è¡¥å……ï¼Œä¸¤è€…å¯ä»¥ä¸²è”ä½¿ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡TDè®­ç»ƒçš„è¿›ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨æœ€ä½³Nï¼ˆæœ€é«˜6.6%ï¼‰å’Œæ ‘æœç´¢ï¼ˆæœ€é«˜23.7%ï¼‰ç¯å¢ƒä¸­æé«˜äº†æ€§èƒ½ã€‚å½“ä¸å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ç›¸ç»“åˆæ—¶ï¼ŒTDè®­ç»ƒçš„PRMèƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆçš„æ•°æ®å‹å¼ºåŒ–å­¦ä¹ â€”â€”ä»…ç”¨2.5kæ•°æ®å°±èƒ½è¾¾åˆ°ä¸åŸºçº¿æ–¹æ³•éœ€è¦50.1kæ•°æ®ç›¸å½“çš„æ€§èƒ½â€”â€”å¹¶åœ¨8ç§æ¨¡å‹å˜ä½“ï¼ˆ5ä¸ªç³»åˆ—ï¼‰ä¸Šäº§ç”Ÿäº†æ›´é«˜è´¨é‡çš„è¯­è¨€æ¨¡å‹ç­–ç•¥ï¼Œä¾‹å¦‚Qwen2.5-ï¼ˆ0.5Bï¼Œ1.5Bï¼‰ï¼ŒGLM4-9B-0414ï¼ŒGLM-Z1-9B-0414ï¼ŒQwen2.5-Math-ï¼ˆ1.5Bï¼Œ7Bï¼‰å’ŒDeepSeek-R1-Distill-Qwen-ï¼ˆ1.5Bï¼Œ7Bï¼‰ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/TDRM%E5%8F%91%E5%B8%83%E6%89%80%E6%9C%89%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/THUDM/TDRMå‘å¸ƒæ‰€æœ‰ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15110v1">PDF</a> 9 figures, 7 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TDRMæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æœ€å°åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¶é—´å·®å¼‚æ¥å­¦ä¹ æ›´å¹³æ»‘ã€æ›´å¯é çš„å¥–åŠ±æ¨¡å‹ã€‚TDRMçš„ä¸´æ—¶å·®å¼‚æ­£åˆ™åŒ–äº§ç”Ÿå¹³æ»‘çš„å¥–åŠ±ï¼Œå¹¶æ”¹å–„ä¸é•¿æœŸç›®æ ‡çš„å¯¹é½ã€‚å°†TDRMçº³å…¥åœ¨çº¿RLå¾ªç¯ä¸­çš„actor-criticé£æ ¼ï¼Œå¯å¸¦æ¥æŒç»­çš„å®è¯æ”¶ç›Šã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡TDè®­ç»ƒçš„è¿›ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨Best-of-Nå’Œæ ‘æœç´¢è®¾ç½®ä¸­æé«˜äº†æ€§èƒ½ã€‚å½“ä¸å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ç»“åˆæ—¶ï¼ŒTDè®­ç»ƒçš„PRMå¯å®ç°æ›´é«˜æ•ˆçš„æ•°æ®é©±åŠ¨RLï¼Œå¹¶åœ¨å¤šä¸ªè¯­è¨€æ¨¡å‹å˜ä½“ä¸Šäº§ç”Ÿæ›´é«˜è´¨é‡çš„è¯­è¨€æ¨¡å‹ç­–ç•¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>TDRMæ–¹æ³•é€šè¿‡å­¦ä¹ æ›´å¹³æ»‘ã€æ›´å¯é çš„å¥–åŠ±æ¨¡å‹æ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ•ˆæœã€‚</li>
<li>é€šè¿‡æœ€å°åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¶é—´å·®å¼‚æ¥å®ç°å¥–åŠ±æ¨¡å‹çš„ä¸´æ—¶å·®å¼‚æ­£åˆ™åŒ–ã€‚</li>
<li>TDRMèƒ½æé«˜å¥–åŠ±æ¨¡å‹ä¸é•¿æœŸç›®æ ‡çš„å¯¹é½ç¨‹åº¦ã€‚</li>
<li>åœ¨actor-criticé£æ ¼çš„åœ¨çº¿RLå¾ªç¯ä¸­å¼•å…¥TDRMï¼Œå¯è·å¾—æŒç»­çš„å®è¯æ•ˆç›Šã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒTDè®­ç»ƒçš„è¿›ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨å¤šç§è®¾ç½®ä¸‹æé«˜äº†æ€§èƒ½ï¼ŒåŒ…æ‹¬Best-of-Nå’Œæ ‘æœç´¢ã€‚</li>
<li>å½“ä¸RLVRç»“åˆæ—¶ï¼ŒTDè®­ç»ƒçš„PRMå¯å®ç°æ›´é«˜æ•ˆçš„æ•°æ®é©±åŠ¨RLï¼Œå¹¶åœ¨å¤šä¸ªè¯­è¨€æ¨¡å‹å˜ä½“ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>æ‰€æœ‰ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/TDRM%E3%80%82">https://github.com/THUDM/TDRMã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15110v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15110v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15110v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15110v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing"><a href="#AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing" class="headerlink" title="AutoEdit: Automatic Hyperparameter Tuning for Image Editing"></a>AutoEdit: Automatic Hyperparameter Tuning for Image Editing</h2><p><strong>Authors:Chau Pham, Quan Dao, Mahesh Bhosale, Yunjie Tian, Dimitris Metaxas, David Doermann</strong></p>
<p>Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editingâ€™s hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world. </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•å½»åº•æ”¹å˜äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ï¼Œä½†ç°æœ‰çš„ç¼–è¾‘æ–¹æ³•åœ¨è¶…å‚æ•°è¯†åˆ«æ–¹é¢é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ã€‚ä¸ºäº†è·å¾—åˆç†çš„ç¼–è¾‘æ€§èƒ½ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸è¦æ±‚ç”¨æˆ·å¼ºè¡Œè°ƒæ•´å¤šä¸ªç›¸äº’ä¾èµ–çš„è¶…å‚æ•°ï¼Œå¦‚åè½¬æ—¶é—´æ­¥é•¿å’Œæ³¨æ„åŠ›ä¿®æ”¹ç­‰ã€‚è¿™ä¸€è¿‡ç¨‹ç”±äºè¶…å‚æ•°æœç´¢ç©ºé—´å·¨å¤§è€Œäº§ç”Ÿäº†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬è®¤ä¸ºåœ¨æ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­æœç´¢æœ€ä½³ç¼–è¾‘è¶…å‚æ•°æ˜¯ä¸€é¡¹åºè´¯å†³ç­–ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå»ºç«‹äº†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œåœ¨å»å™ªæ­¥éª¤ä¸­åŠ¨æ€è°ƒæ•´è¶…å‚æ•°ï¼Œå¹¶å°†ç¼–è¾‘ç›®æ ‡æ•´åˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å®ç°äº†æ—¶é—´æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†æœ€ä½³è¶…å‚æ•°é…ç½®ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æš´åŠ›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æœç´¢æ—¶é—´å’Œè®¡ç®—å¼€é”€æ–¹é¢å¤§å¤§å‡å°‘äº†ï¼Œæ¨åŠ¨äº†åŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘æ¡†æ¶åœ¨å®é™…ä¸–ç•Œä¸­çš„å®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15031v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong>ï¼š<br>æœ€æ–°æ‰©æ•£æ¨¡å‹æŠ€æœ¯çš„è¿›å±•åœ¨æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§å˜åŒ–ï¼Œä½†ç°æœ‰ç¼–è¾‘æ–¹æ³•åœ¨è¶…å‚æ•°è¯†åˆ«æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è·å¾—åˆç†çš„ç¼–è¾‘æ€§èƒ½ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦ç”¨æˆ·æš´åŠ›è°ƒæ•´å¤šä¸ªç›¸äº’ä¾èµ–çš„è¶…å‚æ•°ï¼Œå¦‚åè½¬æ—¶é—´æ­¥é•¿å’Œæ³¨æ„åŠ›ä¿®æ”¹ç­‰ã€‚è¿™ä¸€è¿‡ç¨‹ç”±äºè¶…å‚æ•°æœç´¢ç©ºé—´çš„å·¨å¤§è€Œäº§ç”Ÿäº†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æœ¬ç ”ç©¶å°†ä¼˜åŒ–ç¼–è¾‘è¶…å‚æ•°è§†ä¸ºæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªå†³ç­–åˆ¶å®šä»»åŠ¡ï¼Œå¹¶æå‡ºä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶å»ºç«‹äº†ä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¯ä»¥åœ¨å»å™ªæ­¥éª¤ä¸­åŠ¨æ€è°ƒæ•´è¶…å‚æ•°ï¼Œå¹¶å°†ç¼–è¾‘ç›®æ ‡é›†æˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å®ç°äº†æ—¶é—´æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†æœ€ä¼˜è¶…å‚æ•°é…ç½®ã€‚å®éªŒè¯æ˜ï¼Œä¸ç°æœ‰çš„æš´åŠ›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æœç´¢æ—¶é—´å’Œè®¡ç®—å¼€é”€ï¼Œæ¨åŠ¨äº†æ‰©æ•£å›¾åƒç¼–è¾‘æ¡†æ¶åœ¨ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è¿›æ­¥æ¨åŠ¨äº†æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘çš„é©æ–°ã€‚</li>
<li>ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨è¶…å‚æ•°è¯†åˆ«æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç”¨æˆ·éœ€è¦æš´åŠ›è°ƒæ•´å¤šä¸ªè¶…å‚æ•°ä»¥è·å¾—åˆç†ç¼–è¾‘æ€§èƒ½ï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥å¤„ç†è¶…å‚æ•°ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>å»ºç«‹äº†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¯åœ¨å»å™ªæ­¥éª¤ä¸­åŠ¨æ€è°ƒæ•´è¶…å‚æ•°ã€‚</li>
<li>å°†ç¼–è¾‘ç›®æ ‡é›†æˆåˆ°å¥–åŠ±å‡½æ•°ä¸­ï¼Œå®ç°æ—¶é—´æ•ˆç‡å¹¶ç»´æŒè¶…å‚æ•°ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15031v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15031v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15031v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Indoor-Fluid-Antenna-Systems-Enabled-by-Layout-Specific-Modeling-and-Group-Relative-Policy-Optimization"><a href="#Indoor-Fluid-Antenna-Systems-Enabled-by-Layout-Specific-Modeling-and-Group-Relative-Policy-Optimization" class="headerlink" title="Indoor Fluid Antenna Systems Enabled by Layout-Specific Modeling and   Group Relative Policy Optimization"></a>Indoor Fluid Antenna Systems Enabled by Layout-Specific Modeling and   Group Relative Policy Optimization</h2><p><strong>Authors:Tong Zhang, Qianren Li, Shuai Wang, Wanli Ni, Jiliang Zhang, Rui Wang, Kai-Kit Wong, Chan-Byoung Chae</strong></p>
<p>The fluid antenna system (FAS) revolutionizes wireless communications by employing position-flexible antennas that dynamically optimize channel conditions and mitigate multipath fading. This innovation is particularly valuable in indoor environments, where signal propagation is severely degraded due to structural obstructions and complex multipath reflections. In this paper, we study the channel modeling and joint optimization of antenna positioning, beamforming, and power allocation for indoor FAS. In particular, we propose, for the first time, a layout-specific channel model and a novel group relative policy optimization (GRPO) algorithm for indoor FAS. Compared to the state-of-the-art Sionna model, our approach achieves an $83.3%$ reduction in computation time with an approximately $3$ dB increase in root-mean-square error (RMSE). When simplified to a two-ray model, our channel model enables a closed-form solution for the optimal antenna position, achieving near-optimal performance. {For the joint optimization problem, the proposed GRPO algorithm outperforms proximal policy optimization (PPO) and other baselines in sum-rate, while requiring only 49.2% computational resources of PPO, due to its group-based advantage estimation.} Simulation results reveal that increasing either the group size or trajectory length in GRPO does not yield significant improvements in sum-rate, suggesting that these parameters can be selected conservatively without sacrificing performance. </p>
<blockquote>
<p>æµä½“å¤©çº¿ç³»ç»Ÿï¼ˆFASï¼‰é€šè¿‡é‡‡ç”¨åŠ¨æ€ä¼˜åŒ–ä¿¡é“æ¡ä»¶å¹¶å‡è½»å¤šè·¯å¾„è¡°è½çš„å®šä½çµæ´»å¤©çº¿ï¼Œå®ç°äº†æ— çº¿é€šä¿¡çš„é©å‘½æ€§å˜é©ã€‚è¿™ç§åˆ›æ–°åœ¨å®¤å†…éƒ¨ç½²ä¸­å°¤å…¶å…·æœ‰ä»·å€¼ï¼Œå› ä¸ºç»“æ„é˜»ç¢å’Œå¤æ‚çš„å¤šè·¯å¾„åå°„ä¼šä¸¥é‡å‰Šå¼±ä¿¡å·ä¼ æ’­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å®¤å†…FASçš„ä¿¡é“å»ºæ¨¡ä»¥åŠå¤©çº¿å®šä½ã€æ³¢æŸå½¢æˆå’ŒåŠŸç‡åˆ†é…çš„è”åˆä¼˜åŒ–ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬é¦–æ¬¡æå‡ºäº†é’ˆå¯¹å¸ƒå±€çš„ç‰¹å®šä¿¡é“æ¨¡å‹å’Œç”¨äºå®¤å†…FASçš„æ–°å‹åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ã€‚ä¸æœ€æ–°çš„Sionnaæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æ—¶é—´ä¸Šå®ç°äº†83.3%çš„å‡å°‘ï¼ŒåŒæ—¶å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å¤§çº¦å¢åŠ äº†3 dBã€‚ç®€åŒ–ä¸ºä¸¤å°„çº¿æ¨¡å‹åï¼Œæˆ‘ä»¬çš„ä¿¡é“æ¨¡å‹èƒ½å¤Ÿå¯¹æœ€ä½³å¤©çº¿ä½ç½®æä¾›å°é—­å¼è§£å†³æ–¹æ¡ˆï¼Œå®ç°æ¥è¿‘æœ€ä½³æ€§èƒ½ã€‚å¯¹äºè”åˆä¼˜åŒ–é—®é¢˜ï¼Œæ‰€æå‡ºçš„GRPOç®—æ³•åœ¨æ€»å’Œé€Ÿç‡ä¸Šä¼˜äºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œå…¶ä»–åŸºå‡†çº¿ï¼Œå¹¶ä¸”åªéœ€è¦PPOçš„49.2%çš„è®¡ç®—èµ„æºï¼Œè¿™å¾—ç›Šäºå…¶åŸºäºåˆ†ç»„çš„ä¼˜åŠ¿è¯„ä¼°ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œåœ¨GRPOä¸­å¢åŠ ç¾¤ä½“è§„æ¨¡æˆ–è½¨è¿¹é•¿åº¦å¹¶ä¸ä¼šåœ¨æ€»å’Œé€Ÿç‡ä¸Šäº§ç”Ÿé‡å¤§æ”¹è¿›ï¼Œè¿™è¡¨æ˜è¿™äº›å‚æ•°å¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹ä¿å®ˆé€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15006v1">PDF</a> 15 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æµä½“å¤©çº¿ç³»ç»Ÿï¼ˆFASï¼‰åœ¨å®¤å†…ç¯å¢ƒä¸­çš„æ— çº¿é€šä¿¡æŠ€æœ¯ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä½ç½®çµæ´»çš„å¤©çº¿ï¼Œå¯åŠ¨æ€ä¼˜åŒ–ä¿¡é“æ¡ä»¶å¹¶å‡è½»å¤šè·¯å¾„è¡°å‡ã€‚ç ”ç©¶äº†å¯¹å®¤å†…FASçš„ä¿¡é“å»ºæ¨¡å’Œå¤©çº¿å®šä½ã€æ³¢æŸå½¢æˆä»¥åŠåŠŸç‡åˆ†é…çš„è”åˆä¼˜åŒ–é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§å¸ƒå±€ç‰¹å®šçš„ä¿¡é“æ¨¡å‹å’Œä¸€ç§æ–°çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ã€‚ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—æ—¶é—´ä¸Šå‡å°‘äº†83.3%ï¼ŒåŒæ—¶å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å¢åŠ äº†çº¦3åˆ†è´ã€‚ç®€åŒ–ä¸ºä¸¤å°„çº¿æ¨¡å‹åï¼Œä¸ºæœ€ä¼˜å¤©çº¿ä½ç½®æä¾›äº†é—­å¼è§£å†³æ–¹æ¡ˆï¼Œå®ç°è¿‘æœ€ä¼˜æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒGRPOç®—æ³•åœ¨è”åˆä¼˜åŒ–é—®é¢˜ä¸­è¡¨ç°å‡ºè‰²ï¼Œè®¡ç®—èµ„æºä»…ä¸ºè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„49.2%ï¼Œä¸”å¢åŠ ç¾¤ç»„å¤§å°æˆ–è½¨è¿¹é•¿åº¦å¹¶ä¸ä¼šæ˜¾è‘—æé«˜æ€»å’Œé€Ÿç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµä½“å¤©çº¿ç³»ç»Ÿï¼ˆFASï¼‰åˆ©ç”¨ä½ç½®çµæ´»çš„å¤©çº¿ï¼Œå¯åœ¨å®¤å†…ç¯å¢ƒä¸­åŠ¨æ€ä¼˜åŒ–æ— çº¿ä¿¡é“æ¡ä»¶å¹¶ç¼“è§£å¤šè·¯å¾„è¡°å‡é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†å®¤å†…FASçš„ä¿¡é“å»ºæ¨¡å’Œå¤©çº¿å®šä½ã€æ³¢æŸå½¢æˆä»¥åŠåŠŸç‡åˆ†é…çš„è”åˆä¼˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¸ƒå±€ç‰¹å®šçš„ä¿¡é“æ¨¡å‹å’Œæ–°çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œæ–°æ–¹æ³•åœ¨è®¡ç®—æ—¶é—´ä¸Šæ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶å®ç°äº†è¾ƒé«˜çš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ã€‚</li>
<li>åœ¨ä¸¤å°„çº¿æ¨¡å‹ä¸‹ï¼Œä¸ºæœ€ä¼˜å¤©çº¿ä½ç½®æä¾›äº†é—­å¼è§£å†³æ–¹æ¡ˆã€‚</li>
<li>GRPOç®—æ³•åœ¨è”åˆä¼˜åŒ–é—®é¢˜ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸”è®¡ç®—èµ„æºéœ€æ±‚è¾ƒä½ã€‚</li>
<li>å¢åŠ ç¾¤ç»„å¤§å°æˆ–è½¨è¿¹é•¿åº¦å¹¶ä¸ä¼šæ˜¾è‘—æé«˜ç³»ç»Ÿæ€»å’Œé€Ÿç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Novel-Task-Driven-Diffusion-Based-Policy-with-Affordance-Learning-for-Generalizable-Manipulation-of-Articulated-Objects"><a href="#A-Novel-Task-Driven-Diffusion-Based-Policy-with-Affordance-Learning-for-Generalizable-Manipulation-of-Articulated-Objects" class="headerlink" title="A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for   Generalizable Manipulation of Articulated Objects"></a>A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for   Generalizable Manipulation of Articulated Objects</h2><p><strong>Authors:Hao Zhang, Zhen Kan, Weiwei Shang, Yongduan Song</strong></p>
<p>Despite recent advances in dexterous manipulations, the manipulation of articulated objects and generalization across different categories remain significant challenges. To address these issues, we introduce DART, a novel framework that enhances a diffusion-based policy with affordance learning and linear temporal logic (LTL) representations to improve the learning efficiency and generalizability of articulated dexterous manipulation. Specifically, DART leverages LTL to understand task semantics and affordance learning to identify optimal interaction points. The {diffusion-based policy} then generalizes these interactions across various categories. Additionally, we exploit an optimization method based on interaction data to refine actions, overcoming the limitations of traditional diffusion policies that typically rely on offline reinforcement learning or learning from demonstrations. Experimental results demonstrate that DART outperforms most existing methods in manipulation ability, generalization performance, transfer reasoning, and robustness. For more information, visit our project website at: <a target="_blank" rel="noopener" href="https://sites.google.com/view/dart0257/">https://sites.google.com/view/dart0257/</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çµå·§æ“ä½œæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å¤„ç†å…³èŠ‚å¼ç‰©ä½“å’Œä¸åŒç±»åˆ«ä¹‹é—´çš„æ³›åŒ–ä»æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DARTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆåŸºäºæ‰©æ•£çš„ç­–ç•¥ã€å¯è¾¾æ€§å­¦ä¹ ä»¥åŠçº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰è¡¨ç¤ºï¼Œæé«˜äº†å…³èŠ‚çµå·§æ“ä½œçš„å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒDARTåˆ©ç”¨LTLç†è§£ä»»åŠ¡è¯­ä¹‰ï¼Œå¹¶åˆ©ç”¨å¯è¾¾æ€§å­¦ä¹ ç¡®å®šæœ€ä½³äº¤äº’ç‚¹ã€‚ç„¶åï¼Œ{åŸºäºæ‰©æ•£çš„ç­–ç•¥}å°†è¿™äº›äº¤äº’åœ¨ä¸åŒç±»åˆ«ä¸­è¿›è¡Œæ¨å¹¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºäº¤äº’æ•°æ®çš„ä¼˜åŒ–æ–¹æ³•æ¥å®Œå–„åŠ¨ä½œï¼Œå…‹æœäº†ä¼ ç»Ÿä¾èµ–ç¦»çº¿å¼ºåŒ–å­¦ä¹ æˆ–æ¨¡ä»¿å­¦ä¹ çš„æ‰©æ•£ç­–ç•¥çš„é™åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDARTåœ¨æ“ä½œèƒ½åŠ›ã€æ³›åŒ–æ€§èƒ½ã€è¿ç§»æ¨ç†å’Œç¨³å¥æ€§æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰å¤§å¤šæ•°æ–¹æ³•ã€‚å¦‚éœ€æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/dart0">https://sites.google.com/view/dart0</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14939v1">PDF</a> Accepted by IEEE&#x2F;ASME Transactions on Mechatronics</p>
<p><strong>Summary</strong><br>ï¼šé’ˆå¯¹çµå·§æ“ä½œä¸­çš„å…³èŠ‚ç‰©ä½“æ“ä½œå’Œè·¨ä¸åŒç±»åˆ«çš„æ³›åŒ–ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDARTçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ‰©æ•£ç­–ç•¥ã€ä»¿å°„å­¦ä¹ å’Œçº¿æ€§æ—¶åºé€»è¾‘è¡¨ç¤ºï¼Œä»¥æé«˜å…³èŠ‚çµå·§æ“ä½œçš„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚DARTåˆ©ç”¨çº¿æ€§æ—¶åºé€»è¾‘ç†è§£ä»»åŠ¡è¯­ä¹‰ï¼Œå¹¶é€šè¿‡ä»¿å°„å­¦ä¹ ç¡®å®šæœ€ä½³äº¤äº’ç‚¹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åˆ©ç”¨åŸºäºäº¤äº’æ•°æ®çš„ä¼˜åŒ–æ–¹æ³•æ¥å®Œå–„åŠ¨ä½œï¼Œå…‹æœäº†ä¼ ç»Ÿæ‰©æ•£ç­–ç•¥ä¾èµ–äºç¦»çº¿å¼ºåŒ–å­¦ä¹ æˆ–ç¤ºèŒƒå­¦ä¹ çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDARTåœ¨æ“ä½œèƒ½åŠ›ã€æ³›åŒ–æ€§èƒ½ã€è¿ç§»æ¨ç†å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DARTæ¡†æ¶ç»“åˆäº†æ‰©æ•£ç­–ç•¥ã€ä»¿å°„å­¦ä¹ å’Œçº¿æ€§æ—¶åºé€»è¾‘ï¼ˆLTLï¼‰ä»¥æé«˜å…³èŠ‚çµå·§æ“ä½œçš„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DARTåˆ©ç”¨LTLç†è§£ä»»åŠ¡è¯­ä¹‰ï¼Œè¿™æœ‰åŠ©äºæå‡æ“ä½œçš„ç²¾å‡†åº¦å’Œæ•ˆç‡ã€‚</li>
<li>ä»¿å°„å­¦ä¹ åœ¨DARTä¸­ç”¨äºç¡®å®šæœ€ä½³äº¤äº’ç‚¹ï¼Œå¢å¼ºäº†æ“ä½œè¿‡ç¨‹ä¸­çš„ç»†èŠ‚æŠŠæ¡ã€‚</li>
<li>DARTé€šè¿‡åŸºäºäº¤äº’æ•°æ®çš„ä¼˜åŒ–æ–¹æ³•å®Œå–„äº†åŠ¨ä½œï¼Œè¿™ä¸€æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿæ‰©æ•£ç­–ç•¥çš„å±€é™æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDARTåœ¨æ“ä½œèƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å’Œå¤šæ ·åŒ–çš„ç‰©ä½“ä¸Šã€‚</li>
<li>DARTåœ¨æ³›åŒ–æ€§èƒ½ã€è¿ç§»æ¨ç†æ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„ä»»åŠ¡å’Œåœºæ™¯ã€‚</li>
<li>DARTçš„ç¨³å¥æ€§å¾—åˆ°äº†éªŒè¯ï¼Œèƒ½å¤Ÿåœ¨å„ç§æ¡ä»¶ä¸‹ç¨³å®šåœ°è¿›è¡Œæ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14939v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14939v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14939v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MARIC-Multi-Agent-Reasoning-for-Image-Classification"><a href="#MARIC-Multi-Agent-Reasoning-for-Image-Classification" class="headerlink" title="MARIC: Multi-Agent Reasoning for Image Classification"></a>MARIC: Multi-Agent Reasoning for Image Classification</h2><p><strong>Authors:Wonduk Seo, Minhyeong Yu, Hyunjin An, Seunghyun Lee</strong></p>
<p>Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification. </p>
<blockquote>
<p>å›¾åƒåˆ†ç±»ä¼ ç»Ÿä¸Šä¾èµ–äºå‚æ•°å¯†é›†å‹çš„æ¨¡å‹è®­ç»ƒï¼Œéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†å’Œå¤§é‡çš„å¾®è°ƒæ‰èƒ½è¾¾åˆ°ç«äº‰æ€§èƒ½ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç¼“è§£äº†ä¸€äº›è¿™äº›çº¦æŸï¼Œä½†å®ƒä»¬ä»ç„¶å—é™äºå¯¹å•é€šé“è¡¨ç¤ºçš„ä¾èµ–ï¼Œå¾€å¾€æ— æ³•æ•è·è§†è§‰å†…å®¹çš„äº’è¡¥æ–¹é¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºå¤šæ™ºèƒ½ä½“çš„å›¾åƒåˆ†ç±»æ¨ç†ï¼ˆMARICï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå®ƒé‡æ–°å®šä¹‰äº†å›¾åƒåˆ†ç±»ä¸ºåä½œæ¨ç†è¿‡ç¨‹ã€‚MARICé¦–å…ˆåˆ©ç”¨æ¦‚è¿°æ™ºèƒ½ä½“åˆ†æå›¾åƒçš„æ•´ä½“ä¸»é¢˜å¹¶ç”Ÿæˆæœ‰é’ˆå¯¹æ€§çš„æç¤ºã€‚åŸºäºè¿™äº›æç¤ºï¼Œä¸‰ä¸ªæ–¹é¢æ™ºèƒ½ä½“æ²¿ç€ä¸åŒçš„è§†è§‰ç»´åº¦æå–ç²¾ç»†çš„æè¿°ã€‚æœ€åï¼Œä¸€ä¸ªæ¨ç†æ™ºèƒ½ä½“é€šè¿‡é›†æˆåå°„æ­¥éª¤åˆæˆè¿™äº›äº’è¡¥è¾“å‡ºï¼Œäº§ç”Ÿç”¨äºåˆ†ç±»çš„ç»Ÿä¸€è¡¨ç¤ºã€‚é€šè¿‡æ˜¾å¼åœ°å°†ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªè§’åº¦å¹¶é¼“åŠ±åæ€åˆæˆï¼ŒMARICç¼“è§£äº†å‚æ•°ç¹é‡çš„è®­ç»ƒå’Œå•ä¸€VLMæ¨ç†çš„ç¼ºç‚¹ã€‚åœ¨å››ä¸ªä¸åŒçš„å›¾åƒåˆ†ç±»åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMARICæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œçªæ˜¾äº†å¤šæ™ºèƒ½ä½“è§†è§‰æ¨ç†åœ¨ç¨³å¥å’Œå¯è§£é‡Šçš„å›¾åƒåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14860v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>å›¾åƒåˆ†ç±»è¿‡å»ä¾èµ–äºå‚æ•°å¯†é›†å‹çš„æ¨¡å‹è®­ç»ƒï¼Œéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†å’Œç²¾ç»†è°ƒæ•´æ¥è·å¾—ç«äº‰æ€§èƒ½ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç¼“è§£äº†ä¸€äº›è¿™äº›çº¦æŸï¼Œä½†å®ƒä»¬ä»ç„¶å—é™äºå•ä¸€é€šé“è¡¨ç¤ºï¼Œå¾€å¾€æ— æ³•æ•æ‰è§†è§‰å†…å®¹çš„äº’è¡¥æ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤šæ™ºèƒ½ä½“çš„å›¾åƒåˆ†ç±»æ¨ç†æ–¹æ³•ï¼ˆMARICï¼‰ï¼Œå®ƒå°†å›¾åƒåˆ†ç±»é‡æ–°å®šä¹‰ä¸ºä¸€ç§åä½œæ¨ç†è¿‡ç¨‹ã€‚é¦–å…ˆï¼ŒMARICä½¿ç”¨Outline Agentåˆ†æå›¾åƒçš„å…¨å±€ä¸»é¢˜å¹¶ç”Ÿæˆç›®æ ‡æç¤ºã€‚åŸºäºè¿™äº›æç¤ºï¼Œä¸‰ä¸ªAspect Agentsæ²¿ä¸åŒè§†è§‰ç»´åº¦æå–ç²¾ç»†çš„è¯¦ç»†æè¿°ã€‚æœ€åï¼ŒReasoning Agenté€šè¿‡ç»¼åˆåæ˜ æ­¥éª¤åˆæˆè¿™äº›äº’è¡¥è¾“å‡ºï¼Œä¸ºåˆ†ç±»æä¾›ç»Ÿä¸€è¡¨ç¤ºã€‚é€šè¿‡æ˜ç¡®åœ°å°†ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªè§’åº¦å¹¶é¼“åŠ±åæ€ç»¼åˆï¼ŒMARICç¼“è§£äº†å‚æ•°å¯†é›†è®­ç»ƒå’Œå•ä¸€VLMæ¨ç†çš„ç¼ºç‚¹ã€‚åœ¨å››ä¸ªä¸åŒçš„å›¾åƒåˆ†ç±»åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMARICæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œçªæ˜¾äº†å¤šæ™ºèƒ½ä½“è§†è§‰æ¨ç†åœ¨é²æ£’å’Œå¯è§£é‡Šæ€§å›¾åƒåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå›¾åƒåˆ†ç±»ä¾èµ–äºå‚æ•°å¯†é›†å‹çš„æ¨¡å‹è®­ç»ƒï¼Œéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†å’Œç²¾ç»†è°ƒæ•´ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½ç„¶ç¼“è§£äº†ä¸€äº›çº¦æŸï¼Œä½†ä»å—é™äºå•ä¸€é€šé“è¡¨ç¤ºã€‚</li>
<li>MARICé‡‡ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†å›¾åƒåˆ†ç±»å®šä¹‰ä¸ºåä½œæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>MARICä½¿ç”¨Outliner Agentåˆ†æå›¾åƒå…¨å±€ä¸»é¢˜å¹¶ç”Ÿæˆç›®æ ‡æç¤ºã€‚</li>
<li>Aspect Agentsæå–ä¸åŒè§†è§‰ç»´åº¦çš„ç²¾ç»†æè¿°ã€‚</li>
<li>Reasoning Agentåˆæˆäº’è¡¥è¾“å‡ºï¼Œä¸ºåˆ†ç±»æä¾›ç»Ÿä¸€è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects"><a href="#CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects" class="headerlink" title="CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End   Code Review Evaluation in Python Projects"></a>CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End   Code Review Evaluation in Python Projects</h2><p><strong>Authors:Hanyang Guo, Xunjin Zheng, Zihan Liao, Hang Yu, Peng DI, Ziyin Zhang, Hong-Ning Dai</strong></p>
<p>Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a â€œreality gapâ€: existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose a novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥ï¼ˆCRï¼‰æ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®åº”ç”¨ï¼Œä½†è¿›å±•å—åˆ°â€œç°å®å·®è·â€çš„é˜»ç¢ï¼šç°æœ‰åŸºå‡†æµ‹è¯•ä½¿ç”¨ç®€åŒ–ã€ç¼ºä¹ä¸Šä¸‹æ–‡çš„æ•°æ®åœ¨å­¤ç«‹çš„å­ä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹ã€‚è¿™æœªèƒ½åæ˜ ç°å®ä¸–ç•ŒCRçš„æ•´ä½“ä¸°å¯Œä¸Šä¸‹æ–‡ç‰¹æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CodeFuse-CR-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºä»“åº“çº§åˆ«CRè¯„ä¼°çš„å…¨é¢è®¤çŸ¥åŸºå‡†æµ‹è¯•ã€‚CodeFuse-CR-BenchåŒ…å«æ¥è‡ªæ¶µç›–ä¹ä¸ªPullè¯·æ±‚ï¼ˆPRï¼‰é—®é¢˜åŸŸçš„70ä¸ªPythoné¡¹ç›®çš„601ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹éƒ½æä¾›äº†ä¸°å¯Œçš„å¤šæ–¹é¢ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬ç›¸å…³é—®é¢˜ã€PRè¯¦ç»†ä¿¡æ¯å’Œä»“åº“çŠ¶æ€ï¼Œä»è€Œå®ç°ç«¯åˆ°ç«¯çš„è¯„ä¼°ã€‚é™¤äº†è¡¨é¢æŒ‡æ ‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºäºä½ç½®çš„å’ŒåŸºäºè¯­æ³•çš„è§„åˆ™æ£€æŸ¥ä»¥åŠåŸºäºæ¨¡å‹çš„å®¡æŸ¥è´¨é‡åˆ¤æ–­ã€‚æˆ‘ä»¬å¯¹è¿™ä¸€å…¨é¢çš„CRä»»åŠ¡è¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡è¯„ä¼°ï¼Œå¯¹æœ€æ–°LLMè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœå»ºç«‹äº†å…³é”®çš„åŸºå‡†çº¿å¹¶æ­ç¤ºï¼šï¼ˆ1ï¼‰æ²¡æœ‰å•ä¸€çš„LLMåœ¨æ‰€æœ‰æ–¹é¢çš„CRä¸­å ä¸»å¯¼åœ°ä½ï¼›ï¼ˆ2ï¼‰Gemini 2.5 Proçš„ç»¼åˆæ€§èƒ½æœ€é«˜ï¼›ï¼ˆ3ï¼‰ä¸åŒçš„LLMå¯¹å†—ä½™ä¸Šä¸‹æ–‡çš„ç¨³å¥æ€§è¡¨ç°å‡ºå·®å¼‚ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å…¨é¢å¤šç»´è¯„ä¼°çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘çœŸæ­£æ™ºèƒ½ä¸”å®ç”¨çš„CRåŠ©æ‰‹æä¾›äº†å¯æ“ä½œæ€§çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14856v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥ï¼ˆCRï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®åº”ç”¨ä¹‹ä¸€ï¼Œä½†ç°å®å·®è·é˜»ç¢äº†å…¶è¿›å±•ï¼šç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä½¿ç”¨ç®€åŒ–ä¸”ç¼ºä¹ä¸Šä¸‹æ–‡çš„æ•°æ®å¯¹å­¤ç«‹çš„å­ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œæ— æ³•åæ˜ ç°å®ä¸–ç•ŒCRçš„æ•´ä½“ä¸°å¯Œä¸Šä¸‹æ–‡ç‰¹æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CodeFuse-CR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘å­˜å‚¨åº“çº§åˆ«çš„CRè¯„ä¼°çš„å…¨é¢æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ã€‚CodeFuse-CR-BenchåŒ…å«æ¥è‡ªæ¶µç›–ä¹ç§æ‹‰å–è¯·æ±‚ï¼ˆPRï¼‰é—®é¢˜åŸŸçš„Pythoné¡¹ç›®çš„æ•°æ®ï¼Œæä¾›äº†ä¸°å¯Œçš„å¤šè§’åº¦ä¸Šä¸‹æ–‡ç¯å¢ƒã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºäºè§„åˆ™çš„ä½ç½®å’Œè¯­æ³•æ£€æŸ¥ä¸åŸºäºæ¨¡å‹çš„å®¡æŸ¥è´¨é‡åˆ¤æ–­ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡çš„CRä»»åŠ¡è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºå„æ¨¡å‹å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œè¿™è¡¨æ˜äº†å…¨é¢çš„å¤šç»´åº¦è¯„ä¼°çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘æ›´æ™ºèƒ½å®ç”¨çš„CRåŠ©æ‰‹æä¾›äº†å¯æ“ä½œè§è§£ã€‚å› æ­¤ç ”ç©¶å±•ç¤ºäº†å¼ºå¤§çš„å·¥å…·ä¸æœªæ¥å‘å±•æ½œåŠ›ç›¸ç»“åˆçš„å…·ä½“è¡¨ç°ï¼Œæä¾›äº†å¼ºå¤§çš„æ”¯æ’‘ä½œç”¨ã€‚è¿™ä¸ºè®¾è®¡æ›´å…¨é¢å¯é çš„ä»£ç å®¡æŸ¥è‡ªåŠ¨åŒ–ç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ä»å‚å·®ä¸é½ï¼Œæœªæ¥è¿˜æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹ç ”ç©¶å¯¹äºæ¨åŠ¨ä»£ç å®¡æŸ¥è‡ªåŠ¨åŒ–æŠ€æœ¯çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚åŒæ—¶å¼ºè°ƒäº†è¯„ä¼°æ ‡å‡†ä½œä¸ºå¼•å¯¼æ–¹å‘çš„é‡è¦ä½œç”¨å’Œå±•ç¤ºè·¨å¤šä¸ªLLMå’Œå„ç±»ç°å®æƒ…å¢ƒçš„å‡†ç¡®å’Œè¯¦å°½ç ”ç©¶çš„å¿…è¦æ€§ã€‚ä¸ºä»Šåç ”ç©¶æä¾›äº†å®è´µçš„å‚è€ƒä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Empathy-R1-A-Chain-of-Empathy-and-Reinforcement-Learning-Framework-for-Long-Form-Mental-Health-Support"><a href="#Empathy-R1-A-Chain-of-Empathy-and-Reinforcement-Learning-Framework-for-Long-Form-Mental-Health-Support" class="headerlink" title="Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for   Long-Form Mental Health Support"></a>Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for   Long-Form Mental Health Support</h2><p><strong>Authors:Xianrong Yao, Dong She, Chenxu Zhang, Yimeng Zhang, Yueru Sun, Noman Ahmed, Yang Gao, Zhanpeng Jin</strong></p>
<p>Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seekerâ€™s emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoEâ€™s reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support. </p>
<blockquote>
<p>å…±æƒ…å¯¹äºæœ‰æ•ˆçš„å¿ƒç†å¥åº·æ”¯æŒè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿å’¨è¯¢æ–‡æœ¬ï¼ˆLCTsï¼‰æ—¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ç”Ÿæˆçš„å›å¤åœ¨è¯­ä¹‰ä¸Šå¾ˆæµç•…ï¼Œä½†ç¼ºä¹çœŸæ­£çš„å¿ƒç†æ”¯æŒæ‰€éœ€çš„ç»“æ„åŒ–æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­æ–‡ç¯å¢ƒä¸­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Empathy-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå…±æƒ…é“¾ï¼ˆCoEï¼‰æ¨ç†è¿‡ç¨‹ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œä»¥æé«˜å¯¹LCTsçš„å›å¤è´¨é‡ã€‚æˆ‘ä»¬çš„CoEèŒƒå¼å—è®¤çŸ¥è¡Œä¸ºç–—æ³•çš„å¯å‘ï¼Œå¼•å¯¼æ¨¡å‹å¯¹æ±‚åŠ©è€…çš„æƒ…ç»ªã€åŸå› å’Œæ„å›¾è¿›è¡Œé¡ºåºæ¨ç†ï¼Œä½¿å…¶æ€è€ƒè¿‡ç¨‹æ—¢é€æ˜åˆæ˜“äºè§£é‡Šã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡æ–°çš„å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®é›†Empathy-QAå’Œä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹æ¥å®ç°ã€‚é¦–å…ˆï¼Œç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰çŒè¾“CoEçš„æ¨ç†ç»“æ„ã€‚éšåï¼Œç”±ä¸“ç”¨å¥–åŠ±æ¨¡å‹å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥å®Œå–„æœ€ç»ˆå›å¤çš„æ²»ç–—ç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡æ°å½“æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒEmpathy-R1åœ¨å…³é”®è‡ªåŠ¨æŒ‡æ ‡ä¸Šå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œäººç±»è¯„ä¼°è¯å®äº†å…¶ä¼˜è¶Šæ€§ï¼Œæ˜¾ç¤ºå‡ºå¯¹å¼ºå¤§åŸºå‡†æµ‹è¯•æœ‰æ˜æ˜¾çš„åå¥½ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„æ–°åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†44.30%çš„Win@1ç‡ã€‚é€šè¿‡ç”Ÿæˆå¯è§£é‡Šä¸”ä¸Šä¸‹æ–‡ä¸°å¯Œçš„å›åº”ï¼ŒEmpathy-R1åœ¨å¼€å‘ç”¨äºå¿ƒç†å¥åº·æ”¯æŒçš„è´Ÿè´£ä»»å’ŒçœŸæ­£æœ‰ç›Šçš„AIæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14851v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¿ƒç†å’¨è¯¢æ–‡æœ¬ï¼ˆLCTsï¼‰çš„AIæ”¯æŒæ¡†æ¶Empathy-R1ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æƒ…æ„Ÿé“¾ï¼ˆCoEï¼‰æ¨ç†è¿‡ç¨‹ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ—¨åœ¨æé«˜AIå¯¹å¿ƒç†å’¨è¯¢æ–‡æœ¬å›åº”çš„è´¨é‡ã€‚Empathy-R1å—åˆ°è®¤çŸ¥è¡Œä¸ºç–—æ³•çš„å¯å‘ï¼Œé€šè¿‡æ¨¡æ‹Ÿæ²»ç–—å¸ˆçš„æ€è€ƒè¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œå›åº”æ±‚åŠ©è€…çš„æƒ…æ„Ÿã€åŸå› å’Œæ„å›¾ã€‚è¯¥æ¡†æ¶ä½¿ç”¨å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®é›†Empathy-QAè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¸¤ä¸ªé˜¶æ®µä¼˜åŒ–å›åº”è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEmpathy-R1åœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨äººç±»è¯„ä¼°ä¸­ä¹Ÿè¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼Œå–å¾—äº†æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚è¿™ä¸€æŠ€æœ¯å¯¹äºå¼€å‘è´Ÿè´£ä¸”çœŸæ­£æœ‰ç›Šäºå¿ƒç†å¥åº·æ”¯æŒçš„AIç³»ç»Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Empathyåœ¨ç²¾ç¥å¥åº·æ”¯æŒä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿å’¨è¯¢æ–‡æœ¬ï¼ˆLCTsï¼‰æ—¶ã€‚</li>
<li>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå›åº”æ—¶è™½ç„¶è¯­ä¹‰æµç•…ï¼Œä½†ç¼ºä¹ç»“æ„åŒ–çš„æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•æä¾›çœŸæ­£çš„å¿ƒç†æ”¯æŒã€‚</li>
<li>Empathy-R1æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œç»“åˆäº†æƒ…æ„Ÿé“¾ï¼ˆCoEï¼‰æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ—¨åœ¨æé«˜é’ˆå¯¹LCTsçš„å›åº”è´¨é‡ã€‚</li>
<li>Empathy-R1å—åˆ°è®¤çŸ¥è¡Œä¸ºç–—æ³•çš„å¯å‘ï¼Œæ¨¡æ‹Ÿæ²»ç–—å¸ˆçš„æ€è€ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç†è§£æ±‚åŠ©è€…çš„æƒ…æ„Ÿã€åŸå› å’Œæ„å›¾ã€‚</li>
<li>Empathy-R1ä½¿ç”¨å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®é›†Empathy-QAè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªé˜¶æ®µâ€”â€”ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–å›åº”è´¨é‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºEmpathy-R1åœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå¹¶ä¸”åœ¨äººç±»è¯„ä¼°ä¸­ä¹Ÿè¶…è¶Šäº†åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14851v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14851v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14851v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLM-Agents-at-the-Roundtable-A-Multi-Perspective-and-Dialectical-Reasoning-Framework-for-Essay-Scoring"><a href="#LLM-Agents-at-the-Roundtable-A-Multi-Perspective-and-Dialectical-Reasoning-Framework-for-Essay-Scoring" class="headerlink" title="LLM Agents at the Roundtable: A Multi-Perspective and Dialectical   Reasoning Framework for Essay Scoring"></a>LLM Agents at the Roundtable: A Multi-Perspective and Dialectical   Reasoning Framework for Essay Scoring</h2><p><strong>Authors:Jinhee Jang, Ayoung Moon, Minkyoung Jung, YoungBin Kim. Seung Jin Lee</strong></p>
<p>The emergence of large language models (LLMs) has brought a new paradigm to automated essay scoring (AES), a long-standing and practical application of natural language processing in education. However, achieving human-level multi-perspective understanding and judgment remains a challenge. In this work, we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework designed to perform precise and human-aligned scoring under a zero-shot setting. RES constructs evaluator agents based on LLMs, each tailored to a specific prompt and topic context. Each agent independently generates a trait-based rubric and conducts a multi-perspective evaluation. Then, by simulating a roundtable-style discussion, RES consolidates individual evaluations through a dialectical reasoning process to produce a final holistic score that more closely aligns with human evaluation. By enabling collaboration and consensus among agents with diverse evaluation perspectives, RES outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in average QWK over straightforward prompting (Vanilla) methods. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ä¸ºè‡ªåŠ¨ä½œæ–‡è¯„åˆ†ï¼ˆAESï¼‰å¸¦æ¥äº†æ–°çš„èŒƒå¼ï¼ŒAESæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†åœ¨æ•™è‚²é¢†åŸŸçš„ä¸€é¡¹é•¿æœŸä¸”å®ç”¨çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå®ç°äººç±»çš„å¤šè§’åº¦ç†è§£å’Œåˆ¤æ–­ä»æ˜¯æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åœ†æ¡Œä½œæ–‡è¯„åˆ†ï¼ˆRESï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»£ç†è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨æ— æ ·æœ¬è®¾ç½®ä¸‹æ‰§è¡Œç²¾ç¡®ä¸”ç¬¦åˆäººç±»è¯„åˆ†æ ‡å‡†çš„è¯„åˆ†ã€‚RESåŸºäºLLMæ„å»ºè¯„ä¼°ä»£ç†ï¼Œæ¯ä¸ªä»£ç†éƒ½é’ˆå¯¹ç‰¹å®šçš„æç¤ºå’Œä¸»é¢˜ä¸Šä¸‹æ–‡è¿›è¡Œå®šåˆ¶ã€‚æ¯ä¸ªä»£ç†ç‹¬ç«‹ç”ŸæˆåŸºäºç‰¹å¾çš„è¯„åˆ†è¡¨å¹¶è¿›è¡Œå¤šè§’åº¦è¯„ä¼°ã€‚ç„¶åï¼Œé€šè¿‡æ¨¡æ‹Ÿåœ†æ¡Œå¼è®¨è®ºï¼ŒRESé€šè¿‡è¾©è¯æ¨ç†è¿‡ç¨‹æ•´åˆä¸ªäººè¯„ä¼°ï¼Œäº§ç”Ÿæœ€ç»ˆçš„æ•´ä½“è¯„åˆ†ï¼Œæ›´è´´è¿‘äººç±»è¯„ä¼°ã€‚é€šè¿‡ä¿ƒè¿›æ‹¥æœ‰ä¸åŒè¯„ä¼°è§†è§’çš„ä»£ç†ä¹‹é—´çš„åä½œå’Œå…±è¯†ï¼ŒRESä¼˜äºä¹‹å‰çš„é›¶æ ·æœ¬AESæ–¹æ³•ã€‚åœ¨ASAPæ•°æ®é›†ä¸Šä½¿ç”¨ChatGPTå’ŒClaudeè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒRESç›¸å¯¹äºç®€å•çš„æç¤ºï¼ˆVanillaï¼‰æ–¹æ³•ï¼Œå¹³å‡QWKæé«˜äº†é«˜è¾¾34.86%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14834v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ä¸ºè‡ªåŠ¨ä½œæ–‡è¯„åˆ†ï¼ˆAESï¼‰å¸¦æ¥äº†æ–°çš„æ¨¡å¼ï¼Œä½†åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å®ç°äººç±»çº§åˆ«å¤šè§’åº¦ç†è§£å’Œåˆ¤æ–­ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºRoundtable Essay Scoringï¼ˆRESï¼‰å¤šæ™ºèƒ½ä½“è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¿›è¡Œç²¾ç¡®ä¸”ç¬¦åˆäººç±»è¯„ä¼°çš„è¯„åˆ†ã€‚RESåŸºäºLLMæ„å»ºè¯„ä¼°å™¨æ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“é’ˆå¯¹ç‰¹å®šæç¤ºå’Œä¸»é¢˜ä¸Šä¸‹æ–‡è¿›è¡Œå®šåˆ¶ï¼Œç‹¬ç«‹ç”ŸæˆåŸºäºç‰¹å¾çš„è¯„åˆ†è¡¨å¹¶è¿›è¡Œå¤šè§’åº¦è¯„ä¼°ã€‚é€šè¿‡æ¨¡æ‹Ÿåœ†æ¡Œè®¨è®ºï¼ŒRESé€šè¿‡è¾©è¯æ¨ç†è¿‡ç¨‹æ•´åˆä¸ªäººè¯„ä¼°ç»“æœï¼Œäº§ç”Ÿæ›´æ¥è¿‘äººç±»è¯„ä»·çš„æ€»ä½“è¯„åˆ†ã€‚RESé€šè¿‡ä¿ƒè¿›æ‹¥æœ‰ä¸åŒè¯„ä¼°è§’åº¦çš„æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œå’Œå…±è¯†ï¼Œè¡¨ç°å‡ºä¼˜äºå…ˆå‰é›¶æ ·æœ¬AESæ–¹æ³•çš„æ•ˆæœã€‚åœ¨ASAPæ•°æ®é›†ä¸Šè¿›è¡Œçš„ChatGPTå’ŒClaudeå®éªŒè¡¨æ˜ï¼ŒRESç›¸è¾ƒäºç®€å•çš„æç¤ºæ–¹æ³•å¹³å‡QWKæé«˜äº†é«˜è¾¾34.86%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè‡ªåŠ¨ä½œæ–‡è¯„åˆ†ï¼ˆAESï¼‰å¸¦æ¥æ–°èŒƒå¼ã€‚</li>
<li>å®ç°äººç±»çº§åˆ«å¤šè§’åº¦ç†è§£å’Œåˆ¤æ–­åœ¨AESä¸­æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>Roundtable Essay Scoringï¼ˆRESï¼‰æ¡†æ¶æ¨¡æ‹Ÿäººç±»è¯„ä¼°è¿‡ç¨‹ï¼Œæ—¨åœ¨ç²¾ç¡®ä¸”ç¬¦åˆäººç±»è¯„ä¼°è¿›è¡Œè¯„åˆ†ã€‚</li>
<li>RESåŸºäºLLMæ„å»ºè¯„ä¼°å™¨æ™ºèƒ½ä½“ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“é’ˆå¯¹ç‰¹å®šæƒ…å¢ƒå®šåˆ¶ã€‚</li>
<li>æ™ºèƒ½ä½“ç‹¬ç«‹ç”Ÿæˆè¯„åˆ†è¡¨å¹¶è¿›è¡Œå¤šè§’åº¦è¯„ä¼°ã€‚</li>
<li>RESé€šè¿‡æ¨¡æ‹Ÿåœ†æ¡Œè®¨è®ºæ•´åˆä¸ªäººè¯„ä¼°ç»“æœï¼Œæ›´æ¥è¿‘äººç±»è¯„ä»·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Re-ranking-for-Image-Retrieval-Tasks"><a href="#Chain-of-Thought-Re-ranking-for-Image-Retrieval-Tasks" class="headerlink" title="Chain-of-Thought Re-ranking for Image Retrieval Tasks"></a>Chain-of-Thought Re-ranking for Image Retrieval Tasks</h2><p><strong>Authors:Shangrong Wu, Yanghong Zhou, Yang Chen, Feng Zhang, P. Y. Mok</strong></p>
<p>Image retrieval remains a fundamental yet challenging problem in computer vision. While recent advances in Multimodal Large Language Models (MLLMs) have demonstrated strong reasoning capabilities, existing methods typically employ them only for evaluation, without involving them directly in the ranking process. As a result, their rich multimodal reasoning abilities remain underutilized, leading to suboptimal performance. In this paper, we propose a novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue. Specifically, we design a listwise ranking prompt that enables MLLM to directly participate in re-ranking candidate images. This ranking process is grounded in an image evaluation prompt, which assesses how well each candidate aligns with users query. By allowing MLLM to perform listwise reasoning, our method supports global comparison, consistent reasoning, and interpretable decision-making - all of which are essential for accurate image retrieval. To enable structured and fine-grained analysis, we further introduce a query deconstruction prompt, which breaks down the original query into multiple semantic components. Extensive experiments on five datasets demonstrate the effectiveness of our CoTRR method, which achieves state-of-the-art performance across three image retrieval tasks, including text-to-image retrieval (TIR), composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our code is available at <a target="_blank" rel="noopener" href="https://github.com/freshfish15/CoTRR">https://github.com/freshfish15/CoTRR</a> . </p>
<blockquote>
<p>å›¾åƒæ£€ç´¢ä»ç„¶æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªåŸºæœ¬ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚å°½ç®¡æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä»…å°†å®ƒä»¬ç”¨äºè¯„ä¼°ï¼Œè€Œæ²¡æœ‰ç›´æ¥å‚ä¸åˆ°æ’åºè¿‡ç¨‹ä¸­ã€‚å› æ­¤ï¼Œå®ƒä»¬çš„ä¸°å¯Œå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ€è€ƒé“¾é‡æ–°æ’åºï¼ˆCoTRRï¼‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ—è¡¨æ’åºæç¤ºï¼Œä½¿MLLMèƒ½å¤Ÿç›´æ¥å‚ä¸é‡æ–°æ’åºå€™é€‰å›¾åƒã€‚æ­¤æ’åºè¿‡ç¨‹åŸºäºå›¾åƒè¯„ä¼°æç¤ºï¼Œè¯„ä¼°æ¯ä¸ªå€™é€‰å›¾åƒä¸ç”¨æˆ·æŸ¥è¯¢çš„åŒ¹é…ç¨‹åº¦ã€‚é€šè¿‡å…è®¸MLLMè¿›è¡Œåˆ—è¡¨æ¨ç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå…¨å±€æ¯”è¾ƒã€ä¸€è‡´æ¨ç†å’Œå¯è§£é‡Šå†³ç­–åˆ¶å®šï¼Œè¿™äº›éƒ½æ˜¯å‡†ç¡®å›¾åƒæ£€ç´¢æ‰€å¿…éœ€çš„ã€‚ä¸ºäº†è¿›è¡Œç»“æ„å’Œç²¾ç»†çš„åˆ†æï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æŸ¥è¯¢è§£æ„æç¤ºï¼Œå°†åŸå§‹æŸ¥è¯¢æ‹†åˆ†ä¸ºå¤šä¸ªè¯­ä¹‰ç»„ä»¶ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„CoTRRæ–¹æ³•å®ç°äº†ä¸‰é¡¹å›¾åƒæ£€ç´¢ä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ï¼ˆTIRï¼‰ã€ç»„åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰å’ŒåŸºäºèŠå¤©çš„å›¾åƒæ£€ç´¢ï¼ˆChat-IRï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/freshfish15/CoTRR">https://github.com/freshfish15/CoTRR</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14746v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºChain-of-Thought Re-Rankingï¼ˆCoTRRï¼‰çš„æ–°æ–¹æ³•ï¼Œä»¥è§£å†³å›¾åƒæ£€ç´¢ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ç§åˆ—è¡¨æ’åæç¤ºï¼Œä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç›´æ¥å‚ä¸å›¾åƒå€™é€‰çš„é‡æ–°æ’åè¿‡ç¨‹ã€‚é€šè¿‡å›¾åƒè¯„ä¼°æç¤ºï¼Œè¯„ä¼°æ¯ä¸ªå€™é€‰å›¾åƒä¸ç”¨æˆ·æŸ¥è¯¢çš„åŒ¹é…ç¨‹åº¦ã€‚è¯¥æ–¹æ³•æ”¯æŒå…¨å±€æ¯”è¾ƒã€ä¸€è‡´æ¨ç†å’Œå¯è§£é‡Šå†³ç­–åˆ¶å®šï¼Œå¯¹äºå‡†ç¡®å›¾åƒæ£€ç´¢è‡³å…³é‡è¦ã€‚å¼•å…¥æŸ¥è¯¢è§£æ„æç¤ºï¼Œå®ç°æ›´ç²¾ç»†çš„åˆ†æã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCoTRRæ–¹æ³•åœ¨ä¸‰ç§å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ£€ç´¢ä»ç„¶æ˜¯ä¸€ä¸ªåŸºç¡€ä¸”å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å›¾åƒæ£€ç´¢ä¸­çš„ä½¿ç”¨æœ‰é™ã€‚</li>
<li>æå‡ºçš„Chain-of-Thought Re-Rankingï¼ˆCoTRRï¼‰æ–¹æ³•ä½¿MLLMç›´æ¥å‚ä¸å›¾åƒå€™é€‰çš„é‡æ–°æ’åã€‚</li>
<li>CoTRRä½¿ç”¨å›¾åƒè¯„ä¼°æç¤ºï¼Œè¯„ä¼°æ¯ä¸ªå€™é€‰å›¾åƒä¸ç”¨æˆ·æŸ¥è¯¢çš„åŒ¹é…ç¨‹åº¦ã€‚</li>
<li>æ–¹æ³•æ”¯æŒå…¨å±€æ¯”è¾ƒã€ä¸€è‡´æ¨ç†å’Œå¯è§£é‡Šå†³ç­–åˆ¶å®šï¼Œå¯¹å‡†ç¡®å›¾åƒæ£€ç´¢è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥æŸ¥è¯¢è§£æ„æç¤ºï¼Œå®ç°æ›´ç²¾ç»†çš„åˆ†æå’Œç»“æ„åŒ–æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14746v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14746v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14746v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14746v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RationAnomaly-Log-Anomaly-Detection-with-Rationality-via-Chain-of-Thought-and-Reinforcement-Learning"><a href="#RationAnomaly-Log-Anomaly-Detection-with-Rationality-via-Chain-of-Thought-and-Reinforcement-Learning" class="headerlink" title="RationAnomaly: Log Anomaly Detection with Rationality via   Chain-of-Thought and Reinforcement Learning"></a>RationAnomaly: Log Anomaly Detection with Rationality via   Chain-of-Thought and Reinforcement Learning</h2><p><strong>Authors:Song Xu, Yilun Liu, Minggui He, Mingchen Dai, Ziang Chen, Chunguang Zhao, Jingzhou Du, Shimin Tao, Weibin Meng, Shenglin Zhang, Yongqian Sun, Boxing Chen, Daimeng Wei</strong></p>
<p>Logs constitute a form of evidence signaling the operational status of software systems. Automated log anomaly detection is crucial for ensuring the reliability of modern software systems. However, existing approaches face significant limitations: traditional deep learning models lack interpretability and generalization, while methods leveraging Large Language Models are often hindered by unreliability and factual inaccuracies. To address these issues, we propose RationAnomaly, a novel framework that enhances log anomaly detection by synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our approach first instills expert-like reasoning patterns using CoT-guided supervised fine-tuning, grounded in a high-quality dataset corrected through a rigorous expert-driven process. Subsequently, a reinforcement learning phase with a multi-faceted reward function optimizes for accuracy and logical consistency, effectively mitigating hallucinations. Experimentally, RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks while providing transparent, step-by-step analytical outputs. We have released the corresponding resources, including code and datasets. </p>
<blockquote>
<p>æ—¥å¿—æ˜¯æ˜¾ç¤ºè½¯ä»¶ç³»ç»Ÿè¿è¡ŒçŠ¶æ€çš„ä¸€ç§è¯æ®å½¢å¼ã€‚è‡ªåŠ¨æ—¥å¿—å¼‚å¸¸æ£€æµ‹å¯¹äºç¡®ä¿ç°ä»£è½¯ä»¶ç³»ç»Ÿçš„å¯é æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨é‡å¤§å±€é™æ€§ï¼šä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè€Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•é€šå¸¸å—åˆ°å¯é æ€§å’Œäº‹å®å‡†ç¡®æ€§çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RationAnomalyï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ååŒæ€è€ƒé“¾ï¼ˆCoTï¼‰å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæ—¥å¿—å¼‚å¸¸æ£€æµ‹çš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨CoTå¼•å¯¼çš„ç›‘ç£å¾®è°ƒæ¥çŒè¾“ä¸“å®¶çº§çš„æ¨ç†æ¨¡å¼ï¼Œè¿™åŸºäºä¸€ä¸ªé€šè¿‡ä¸¥æ ¼çš„ä¸“å®¶é©±åŠ¨è¿‡ç¨‹è¿›è¡Œæ ¡æ­£çš„é«˜è´¨é‡æ•°æ®é›†ã€‚éšåï¼Œä¸€ä¸ªå…·æœ‰å¤šé¢å¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µä¼˜åŒ–äº†å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†å¹»è§‰ã€‚å®éªŒè¡¨æ˜ï¼ŒRationAnomalyä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æŠ€æœ¯ï¼Œåœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†è¾ƒé«˜çš„F1åˆ†æ•°ï¼ŒåŒæ—¶æä¾›é€æ˜ã€é€æ­¥çš„åˆ†æè¾“å‡ºã€‚æˆ‘ä»¬å·²ç»å‘å¸ƒäº†ç›¸åº”çš„èµ„æºï¼ŒåŒ…æ‹¬ä»£ç å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14693v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong><br>     æ—¥å¿—æ˜¯åæ˜ è½¯ä»¶ç³»ç»Ÿè¿è¡ŒçŠ¶æ€çš„é‡è¦è¯æ®ï¼Œè‡ªåŠ¨æ—¥å¿—å¼‚å¸¸æ£€æµ‹å¯¹ç¡®ä¿ç°ä»£è½¯ä»¶ç³»ç»Ÿçš„å¯é æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§å’Œé€šç”¨æ€§ï¼Œè€Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åˆ™ç»å¸¸å—åˆ°ä¸å¯é å’Œäº‹å®ä¸å‡†ç¡®çš„å½±å“ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RationAnomalyæ¡†æ¶ï¼Œå®ƒé€šè¿‡ååŒé“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç²¾ç»†è°ƒæ•´ä¸å¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºäº†æ—¥å¿—å¼‚å¸¸æ£€æµ‹ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡CoTå¼•å¯¼çš„ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼Œä»¥é«˜è´¨é‡æ•°æ®é›†ä¸ºåŸºç¡€ï¼Œé€šè¿‡ä¸¥æ ¼çš„ä¸“å®¶é©±åŠ¨è¿‡ç¨‹è¿›è¡Œæ ¡æ­£ï¼Œæ¤å…¥ä¸“å®¶å¼çš„æ¨ç†æ¨¡å¼ã€‚éšåï¼Œé‡‡ç”¨å…·æœ‰å¤šæ–¹é¢å¥–åŠ±åŠŸèƒ½çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›è¡Œä¼˜åŒ–ï¼Œæ—¢æé«˜å‡†ç¡®æ€§åˆä¿æŒé€»è¾‘è¿è´¯æ€§ï¼Œæœ‰æ•ˆæŠ‘åˆ¶è™šæ„ã€‚å®éªŒè¡¨æ˜ï¼ŒRationAnomalyåœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†è¾ƒé«˜çš„F1åˆ†æ•°ï¼Œä¼˜äºæœ€æ–°åŸºçº¿ï¼ŒåŒæ—¶æä¾›é€æ˜ã€é€æ­¥çš„åˆ†æè¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¥å¿—æ˜¯è½¯ä»¶ç³»ç»Ÿè¿è¡ŒçŠ¶æ€çš„è¯æ®ï¼Œè‡ªåŠ¨æ—¥å¿—å¼‚å¸¸æ£€æµ‹å¯¹ç¡®ä¿è½¯ä»¶å¯é æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ—¥å¿—å¼‚å¸¸æ£€æµ‹æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚æ·±åº¦å­¦ä¹ æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§å’Œé€šç”¨æ€§ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹æ–¹æ³•å­˜åœ¨ä¸å¯é å’Œäº‹å®ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>RationAnomalyæ¡†æ¶é€šè¿‡ååŒé“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç²¾ç»†è°ƒæ•´ä¸å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæ—¥å¿—å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>RationAnomalyé¦–å…ˆé€šè¿‡CoTå¼•å¯¼çš„ç›‘ç£ç²¾ç»†è°ƒæ•´æ¤å…¥ä¸“å®¶å¼çš„æ¨ç†æ¨¡å¼ï¼Œä½¿ç”¨é«˜è´¨é‡æ•°æ®é›†å¹¶ç»è¿‡ä¸“å®¶é©±åŠ¨è¿‡ç¨‹çš„æ ¡æ­£ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é˜¶æ®µé‡‡ç”¨å¤šæ–¹é¢å¥–åŠ±å‡½æ•°ä¼˜åŒ–ï¼Œæé«˜æ£€æµ‹å‡†ç¡®æ€§å’Œé€»è¾‘è¿è´¯æ€§ï¼Œå‡å°‘è™šæ„ã€‚</li>
<li>RationAnomalyåœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸Šå®ç°è¾ƒé«˜F1åˆ†æ•°ï¼Œä¼˜äºæœ€æ–°åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14693v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14693v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14693v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LEED-A-Highly-Efficient-and-Scalable-LLM-Empowered-Expert-Demonstrations-Framework-for-Multi-Agent-Reinforcement-Learning"><a href="#LEED-A-Highly-Efficient-and-Scalable-LLM-Empowered-Expert-Demonstrations-Framework-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="LEED: A Highly Efficient and Scalable LLM-Empowered Expert   Demonstrations Framework for Multi-Agent Reinforcement Learning"></a>LEED: A Highly Efficient and Scalable LLM-Empowered Expert   Demonstrations Framework for Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Tianyang Duan, Zongyuan Zhang, Songxiao Guo, Dong Huang, Yuanye Zhao, Zheng Lin, Zihan Fang, Dianxin Luan, Heming Cui, Yong Cui</strong></p>
<p>Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ™ºèƒ½å†³ç­–æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œéšç€æ™ºèƒ½ä½“æ•°é‡çš„å¢åŠ ï¼Œå®ƒé¢ä¸´ç€åè°ƒå’Œå¯æ‰©å±•æ€§çš„ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹èµ‹èƒ½çš„ä¸“å®¶æ¼”ç¤ºæ¡†æ¶çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆLEEDï¼‰ã€‚LEEDåŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼šæ¼”ç¤ºç”Ÿæˆï¼ˆDGï¼‰æ¨¡å—å’Œæ”¿ç­–ä¼˜åŒ–ï¼ˆPOï¼‰æ¨¡å—ã€‚å…·ä½“æ¥è¯´ï¼ŒDGæ¨¡å—åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸ç¯å¢ƒäº¤äº’çš„æŒ‡ä»¤ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„æ¼”ç¤ºã€‚POæ¨¡å—é‡‡ç”¨åˆ†æ•£å¼è®­ç»ƒèŒƒå¼ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“åˆ©ç”¨ç”Ÿæˆçš„æ¼”ç¤ºæ¥æ„å»ºä¸“å®¶æ”¿ç­–æŸå¤±ï¼Œç„¶åå°†å…¶ä¸è‡ªå·±çš„æ”¿ç­–æŸå¤±ç›¸ç»“åˆã€‚è¿™ä½¿å¾—æ¯ä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåœ°æ ¹æ®ä¸ªäººçŸ¥è¯†å’Œä¸ªäººç»éªŒæœ¬åœ°åŒ–å¹¶ä¼˜åŒ–å…¶æœ¬åœ°æ”¿ç­–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒLEEDåœ¨æ ·æœ¬æ•ˆç‡ã€æ—¶é—´æ•ˆç‡å’Œç¨³å¥çš„å¯æ‰©å±•æ€§æ–¹é¢å–å¾—äº†ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14680v1">PDF</a> 5 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨å¤æ‚ç¯å¢ƒä¸­å±•ç°å‡ºæ™ºèƒ½å†³ç­–çš„å·¨å¤§æ½œåŠ›ï¼Œä½†éšç€æ™ºèƒ½ä½“æ•°é‡çš„å¢åŠ ï¼Œåè°ƒæ€§å’Œå¯æ‰©å±•æ€§æˆä¸ºå…¶ç“¶é¢ˆã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸“å®¶æ¼”ç¤ºæ¡†æ¶ï¼ˆLEEDï¼‰ï¼ŒåŒ…æ‹¬æ¼”ç¤ºç”Ÿæˆï¼ˆDGï¼‰æ¨¡å—å’Œæ”¿ç­–ä¼˜åŒ–ï¼ˆPOï¼‰æ¨¡å—ã€‚DGæ¨¡å—åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸äº¤äº’ç¯å¢ƒç›¸å…³çš„æŒ‡ä»¤ï¼Œä»è€Œåˆ¶ä½œé«˜è´¨é‡çš„æ¼”ç¤ºã€‚POæ¨¡å—é‡‡ç”¨åˆ†æ•£å¼è®­ç»ƒèŒƒå¼ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“åˆ©ç”¨ç”Ÿæˆçš„æ¼”ç¤ºæ„å»ºä¸“å®¶æ”¿ç­–æŸå¤±ï¼Œå¹¶å°†å…¶ä¸è‡ªèº«æ”¿ç­–æŸå¤±ç»“åˆã€‚è¿™ä½¿å¾—æ¯ä¸ªæ™ºèƒ½ä½“èƒ½å¤ŸåŸºäºä¸“å®¶çŸ¥è¯†å’Œä¸ªäººç»éªŒæœ‰æ•ˆåœ°ä¸ªæ€§åŒ–å¹¶ä¼˜åŒ–å…¶æœ¬åœ°ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼ŒLEEDåœ¨æ ·æœ¬æ•ˆç‡ã€æ—¶é—´æ•ˆç‡å’Œç¨³å¥å¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨å¤„ç†å¤æ‚ç¯å¢ƒå†³ç­–æ—¶é¢ä¸´åè°ƒæ€§å’Œæ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>LEEDæ¡†æ¶é€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æ¼”ç¤ºã€‚</li>
<li>LEEDåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šæ¼”ç¤ºç”Ÿæˆï¼ˆDGï¼‰å’Œæ”¿ç­–ä¼˜åŒ–ï¼ˆPOï¼‰ã€‚</li>
<li>DGæ¨¡å—åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç¯å¢ƒäº¤äº’æŒ‡ä»¤ã€‚</li>
<li>POæ¨¡å—é‡‡ç”¨åˆ†æ•£å¼è®­ç»ƒï¼Œæ™ºèƒ½ä½“ç»“åˆä¸“å®¶æ”¿ç­–å’Œè‡ªèº«æ”¿ç­–æŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>LEEDåœ¨æ ·æœ¬æ•ˆç‡ã€æ—¶é—´æ•ˆç‡å’Œå¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AgentCompass-Towards-Reliable-Evaluation-of-Agentic-Workflows-in-Production"><a href="#AgentCompass-Towards-Reliable-Evaluation-of-Agentic-Workflows-in-Production" class="headerlink" title="AgentCompass: Towards Reliable Evaluation of Agentic Workflows in   Production"></a>AgentCompass: Towards Reliable Evaluation of Agentic Workflows in   Production</h2><p><strong>Authors:NVJK Kartik, Garvit Sapra, Rishav Hada, Nikhil Pareek</strong></p>
<p>With the growing adoption of Large Language Models (LLMs) in automating complex, multi-agent workflows, organizations face mounting risks from errors, emergent behaviors, and systemic failures that current evaluation methods fail to capture. We present AgentCompass, the first evaluation framework designed specifically for post-deployment monitoring and debugging of agentic workflows. AgentCompass models the reasoning process of expert debuggers through a structured, multi-stage analytical pipeline: error identification and categorization, thematic clustering, quantitative scoring, and strategic summarization. The framework is further enhanced with a dual memory system-episodic and semantic-that enables continual learning across executions. Through collaborations with design partners, we demonstrate the frameworkâ€™s practical utility on real-world deployments, before establishing its efficacy against the publicly available TRAIL benchmark. AgentCompass achieves state-of-the-art results on key metrics, while uncovering critical issues missed in human annotations, underscoring its role as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å¤æ‚å¤šä»£ç†å·¥ä½œæµç¨‹ä¸­çš„æ—¥ç›Šæ™®åŠï¼Œç»„ç»‡é¢ä¸´ç€æ—¥ç›Šå¢é•¿çš„é”™è¯¯ã€çªå‘è¡Œä¸ºå’Œç³»ç»Ÿå¤±è´¥é£é™©ï¼Œè€Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å´æ— æ³•æ•æ‰åˆ°è¿™äº›é£é™©ã€‚æˆ‘ä»¬æ¨å‡ºäº†AgentCompassï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºä»£ç†å·¥ä½œæµç¨‹çš„éƒ¨ç½²åç›‘æ§å’Œè°ƒè¯•è®¾è®¡çš„ç¬¬ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ã€‚AgentCompassé€šè¿‡ç»“æ„åŒ–ã€å¤šé˜¶æ®µçš„åˆ†æç®¡é“æ¥æ¨¡æ‹Ÿä¸“å®¶è°ƒè¯•å™¨çš„æ¨ç†è¿‡ç¨‹ï¼šé”™è¯¯è¯†åˆ«å’Œåˆ†ç±»ã€ä¸»é¢˜èšç±»ã€å®šé‡è¯„åˆ†å’Œæˆ˜ç•¥æ€»ç»“ã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨åŒå†…å­˜ç³»ç»Ÿï¼ˆå³æƒ…èŠ‚è®°å¿†å’Œè¯­ä¹‰è®°å¿†ï¼‰ï¼Œä½¿å„æ‰§è¡Œè¿‡ç¨‹ä¹‹é—´çš„æŒç»­å­¦ä¹ æˆä¸ºå¯èƒ½ã€‚é€šè¿‡ä¸è®¾è®¡åˆä½œä¼™ä¼´çš„åˆä½œï¼Œæˆ‘ä»¬åœ¨å®é™…éƒ¨ç½²ä¸­å±•ç¤ºäº†è¯¥æ¡†æ¶çš„å®é™…æ•ˆç”¨ï¼Œç„¶åå†åœ¨å…¬å¼€å¯ç”¨çš„TRAILåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚AgentCompassåœ¨å…³é”®æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°ç»“æœï¼ŒåŒæ—¶å‘ç°äº†äººç±»æ³¨é‡Šä¸­é—æ¼çš„å…³é”®é—®é¢˜ï¼Œè¿™è¿›ä¸€æ­¥å‡¸æ˜¾äº†å…¶åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯¹ä»£ç†ç³»ç»Ÿå¯é ç›‘æ§å’Œæ”¹è¿›çš„ç¨³å¥ã€é¢å‘å¼€å‘è€…çš„å·¥å…·çš„é‡è¦ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14647v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å¤æ‚ã€å¤šä»£ç†å·¥ä½œæµç¨‹ä¸­çš„æ™®åŠï¼Œä½¿å¾—ç»„ç»‡é¢ä¸´è¶Šæ¥è¶Šå¤šçš„é£é™©ï¼Œå¦‚é”™è¯¯ã€æ–°å…´è¡Œä¸ºå’Œç³»ç»Ÿå¤±æ•ˆç­‰ï¼Œè€Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•æ— æ³•æ•æ‰åˆ°è¿™äº›é£é™©ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AgentCompassï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºä»£ç†å·¥ä½œæµç¨‹çš„åéƒ¨ç½²ç›‘æ§å’Œè°ƒè¯•è®¾è®¡çš„è¯„ä¼°æ¡†æ¶ã€‚AgentCompassé€šè¿‡ç»“æ„åŒ–ã€å¤šé˜¶æ®µçš„åˆ†æç®¡é“æ¥æ¨¡æ‹Ÿä¸“å®¶è°ƒè¯•å™¨çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é”™è¯¯è¯†åˆ«å’Œåˆ†ç±»ã€ä¸»é¢˜èšç±»ã€å®šé‡è¯„åˆ†å’Œæˆ˜ç•¥æ€»ç»“ã€‚è¯¥æ¡†æ¶è¿˜é‡‡ç”¨åŒè®°å¿†ç³»ç»Ÿâ€”â€”æƒ…æ™¯è®°å¿†å’Œè¯­ä¹‰è®°å¿†ï¼Œä»¥å®ç°è·¨æ‰§è¡Œçš„æŒç»­å­¦ä¹ ã€‚é€šè¿‡ä¸è®¾è®¡åˆä½œä¼™ä¼´çš„åˆä½œï¼Œæˆ‘ä»¬åœ¨å®é™…éƒ¨ç½²ä¸­å±•ç¤ºäº†è¯¥æ¡†æ¶çš„å®é™…æ•ˆç”¨ï¼Œå¹¶åœ¨å…¬å¼€çš„TRAILåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚AgentCompassåœ¨å…³é”®æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼ŒåŒæ—¶å‘ç°äº†äººç±»æ³¨é‡Šä¸­é—æ¼çš„å…³é”®é—®é¢˜ï¼Œå‡¸æ˜¾äº†å…¶åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯é ç›‘æ§å’Œæ”¹è¿›ä»£ç†ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œé¢å‘å¼€å‘è€…çš„ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ä¸­çš„æ™®åŠå¸¦æ¥äº†é”™è¯¯ã€æ–°å…´è¡Œä¸ºå’Œç³»ç»Ÿå¤±æ•ˆç­‰é£é™©ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•å……åˆ†æ•æ‰è¿™äº›é£é™©ã€‚</li>
<li>AgentCompassæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºä»£ç†å·¥ä½œæµç¨‹çš„åéƒ¨ç½²ç›‘æ§å’Œè°ƒè¯•çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>AgentCompassæ¨¡æ‹Ÿä¸“å®¶è°ƒè¯•å™¨çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é”™è¯¯è¯†åˆ«ã€ä¸»é¢˜èšç±»ã€å®šé‡è¯„åˆ†å’Œæˆ˜ç•¥æ€»ç»“ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŒè®°å¿†ç³»ç»Ÿå®ç°è·¨æ‰§è¡Œçš„æŒç»­å­¦ä¹ ã€‚</li>
<li>AgentCompassåœ¨å®é™…éƒ¨ç½²ä¸­è¡¨ç°å‡ºå®é™…æ•ˆç”¨ï¼Œå¹¶åœ¨å…¬å¼€åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14647v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14647v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14647v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Position-Thematic-Analysis-of-Unstructured-Clinical-Transcripts-with-Large-Language-Models"><a href="#Position-Thematic-Analysis-of-Unstructured-Clinical-Transcripts-with-Large-Language-Models" class="headerlink" title="Position: Thematic Analysis of Unstructured Clinical Transcripts with   Large Language Models"></a>Position: Thematic Analysis of Unstructured Clinical Transcripts with   Large Language Models</h2><p><strong>Authors:Seungjun Yi, Joakim Nguyen, Terence Lim, Andrew Well, Joseph Skrovan, Mehak Beri, YongGeon Lee, Kavita Radhakrishnan, Liu Leqi, Mia Markey, Ying Ding</strong></p>
<p>This position paper examines how large language models (LLMs) can support thematic analysis of unstructured clinical transcripts, a widely used but resource-intensive method for uncovering patterns in patient and provider narratives. We conducted a systematic review of recent studies applying LLMs to thematic analysis, complemented by an interview with a practicing clinician. Our findings reveal that current approaches remain fragmented across multiple dimensions including types of thematic analysis, datasets, prompting strategies and models used, most notably in evaluation. Existing evaluation methods vary widely (from qualitative expert review to automatic similarity metrics), hindering progress and preventing meaningful benchmarking across studies. We argue that establishing standardized evaluation practices is critical for advancing the field. To this end, we propose an evaluation framework centered on three dimensions: validity, reliability, and interpretability. </p>
<blockquote>
<p>è¿™ç¯‡ç«‹åœºè®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•æ”¯æŒå¯¹éæ­£å¼ä¸´åºŠè®°å½•çš„ä¸»é¢˜åˆ†æã€‚ä¸»é¢˜åˆ†ææ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨ä½†èµ„æºå¯†é›†çš„æ–¹æ³•ï¼Œç”¨äºå‘ç°æ‚£è€…å’Œæä¾›è€…å™è¿°ä¸­çš„æ¨¡å¼ã€‚æˆ‘ä»¬å¯¹è¿‘æœŸåº”ç”¨LLMè¿›è¡Œä¸»é¢˜åˆ†æçš„ç ”ç©¶è¿›è¡Œäº†ç³»ç»Ÿå›é¡¾ï¼Œå¹¶è¾…ä»¥ä¸å®è·µåŒ»ç”Ÿçš„è®¿è°ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„æ–¹æ³•åœ¨å¤šä¸ªç»´åº¦ä¸Šä»ç„¶å‘ˆç°ç¢ç‰‡åŒ–ï¼ŒåŒ…æ‹¬ä¸»é¢˜åˆ†æçš„ç±»å‹ã€æ•°æ®é›†ã€æç¤ºç­–ç•¥å’Œä½¿ç”¨çš„æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨è¯„ä¼°æ–¹é¢ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å·®å¼‚å¾ˆå¤§ï¼ˆä»å®šæ€§ä¸“å®¶è¯„å®¡åˆ°è‡ªåŠ¨ç›¸ä¼¼æ€§åº¦é‡ï¼‰ï¼Œé˜»ç¢äº†è¿›å±•ï¼Œå¹¶é˜»ç¢äº†ç ”ç©¶ä¹‹é—´çš„æœ‰æ„ä¹‰åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è®¤ä¸ºå»ºç«‹æ ‡å‡†åŒ–çš„è¯„ä¼°å®è·µå¯¹äºæ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥ä¸‰ä¸ªç»´åº¦ä¸ºä¸­å¿ƒçš„è¯„ä¼°æ¡†æ¶ï¼šæœ‰æ•ˆæ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14597v1">PDF</a> Submitted to GenAI4Health@NeurIPS 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•æ”¯æŒå¯¹ä¸´åºŠè½¬å½•è¿›è¡Œä¸»é¢˜åˆ†æï¼Œè¿™æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨ä½†èµ„æºå¯†é›†çš„æ–¹æ³•ï¼Œç”¨äºæŒ–æ˜æ‚£è€…å’Œæä¾›è€…å™è¿°ä¸­çš„æ¨¡å¼ã€‚é€šè¿‡ç³»ç»Ÿå›é¡¾è¿‘æœŸåº”ç”¨LLMsè¿›è¡Œä¸»é¢˜åˆ†æçš„ç ”ç©¶ï¼Œå¹¶ç»“åˆå¯¹æ‰§ä¸šåŒ»ç”Ÿçš„è®¿è°ˆï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„æ–¹æ³•åœ¨å¤šä¸ªç»´åº¦ä¸Šä»ç„¶åˆ†æ•£ï¼Œæœ€æ˜¾è‘—çš„æ˜¯åœ¨è¯„ä¼°æ–¹é¢ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å·®å¼‚å¾ˆå¤§ï¼ˆä»å®šæ€§ä¸“å®¶è¯„å®¡åˆ°è‡ªåŠ¨ç›¸ä¼¼æ€§åº¦é‡ï¼‰ï¼Œé˜»ç¢äº†è¿›å±•ï¼Œå¹¶é˜»ç¢äº†è·¨ç ”ç©¶çš„åŸºå‡†æµ‹è¯•ã€‚æ–‡ç« å¼ºè°ƒå»ºç«‹æ ‡å‡†åŒ–çš„è¯„ä¼°å®è·µå¯¹äºæ¨åŠ¨è¯¥é¢†åŸŸçš„é‡è¦æ€§ï¼Œå¹¶æå‡ºä¸€ä¸ªä»¥æœ‰æ•ˆæ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§ä¸ºä¸­å¿ƒçš„è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ç”¨äºæ”¯æŒå¯¹ä¸´åºŠè½¬å½•çš„ä¸»é¢˜åˆ†æï¼Œè¯¥æ–¹æ³•åœ¨å‘ç°æ‚£è€…å’Œæä¾›è€…å™è¿°ä¸­çš„æ¨¡å¼æ–¹é¢éå¸¸æœ‰æ•ˆä½†èµ„æºæ¶ˆè€—å¤§ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•åœ¨å¤šä¸ªç»´åº¦ï¼ˆå¦‚ä¸»é¢˜åˆ†æç±»å‹ã€æ•°æ®é›†ã€æç¤ºç­–ç•¥å’Œä½¿ç”¨çš„æ¨¡å‹ï¼‰ä¸Šä»ç„¶åˆ†æ•£ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°æ–¹é¢ã€‚</li>
<li>ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å·®å¼‚å¾ˆå¤§ï¼Œä»å®šæ€§ä¸“å®¶è¯„å®¡åˆ°è‡ªåŠ¨ç›¸ä¼¼æ€§åº¦é‡ä¸ç­‰ï¼Œè¿™é˜»ç¢äº†ç ”ç©¶çš„è¿›å±•å’Œè·¨ç ”ç©¶çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å»ºç«‹æ ‡å‡†åŒ–çš„è¯„ä¼°å®è·µå¯¹äºæ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•è‡³å…³é‡è¦ã€‚</li>
<li>æ–‡ç« æå‡ºä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œé‡ç‚¹è€ƒè™‘ä¸‰ä¸ªç»´åº¦ï¼šæœ‰æ•ˆæ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿå›é¡¾è¿‘æœŸç ”ç©¶å¹¶ç»“åˆæ‰§ä¸šåŒ»ç”Ÿçš„è®¿è°ˆï¼Œæ–‡ç« æä¾›äº†æ·±å…¥çš„è§è§£å’Œå»ºè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14597v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14597v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14597v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SynBench-A-Benchmark-for-Differentially-Private-Text-Generation"><a href="#SynBench-A-Benchmark-for-Differentially-Private-Text-Generation" class="headerlink" title="SynBench: A Benchmark for Differentially Private Text Generation"></a>SynBench: A Benchmark for Differentially Private Text Generation</h2><p><strong>Authors:Yidan Sun, Viktor Schlegel, Srinivasan Nandakumar, Iqra Zahid, Yuping Wu, Yulong Wu, Hao Li, Jie Zhang, Warren Del-Pinto, Goran Nenadic, Siew Kei Lam, Anil Anthony Bharath</strong></p>
<p>Data-driven decision support in high-stakes domains like healthcare and finance faces significant barriers to data sharing due to regulatory, institutional, and privacy concerns. While recent generative AI models, such as large language models, have shown impressive performance in open-domain tasks, their adoption in sensitive environments remains limited by unpredictable behaviors and insufficient privacy-preserving datasets for benchmarking. Existing anonymization methods are often inadequate, especially for unstructured text, as redaction and masking can still allow re-identification. Differential Privacy (DP) offers a principled alternative, enabling the generation of synthetic data with formal privacy assurances. In this work, we address these challenges through three key contributions. First, we introduce a comprehensive evaluation framework with standardized utility and fidelity metrics, encompassing nine curated datasets that capture domain-specific complexities such as technical jargon, long-context dependencies, and specialized document structures. Second, we conduct a large-scale empirical study benchmarking state-of-the-art DP text generation methods and LLMs of varying sizes and different fine-tuning strategies, revealing that high-quality domain-specific synthetic data generation under DP constraints remains an unsolved challenge, with performance degrading as domain complexity increases. Third, we develop a membership inference attack (MIA) methodology tailored for synthetic text, providing first empirical evidence that the use of public datasets - potentially present in pre-training corpora - can invalidate claimed privacy guarantees. Our findings underscore the urgent need for rigorous privacy auditing and highlight persistent gaps between open-domain and specialist evaluations, informing responsible deployment of generative AI in privacy-sensitive, high-stakes settings. </p>
<blockquote>
<p>åœ¨åŒ»ç–—ä¿å¥å’Œè´¢åŠ¡ç­‰é«˜é£é™©é¢†åŸŸä¸­ï¼Œæ•°æ®é©±åŠ¨å†³ç­–æ”¯æŒç”±äºç›‘ç®¡ã€åˆ¶åº¦å’Œéšç§æ‹…å¿§è€Œé¢ä¸´æ•°æ®å…±äº«çš„é‡å¤§éšœç¢ã€‚è™½ç„¶æœ€è¿‘çš„ç”Ÿæˆå¼AIæ¨¡å‹ï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨å¼€æ”¾åŸŸä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä½†åœ¨æ•æ„Ÿç¯å¢ƒä¸­çš„é‡‡ç”¨ä»ç„¶å—åˆ°ä¸å¯é¢„æµ‹çš„è¡Œä¸ºå’Œä¸è¶³çš„éšç§ä¿æŠ¤æ•°æ®é›†çš„é™åˆ¶ï¼Œç”¨äºåŸºå‡†æµ‹è¯•ã€‚ç°æœ‰çš„åŒ¿ååŒ–æ–¹æ³•é€šå¸¸ä¸è¶³å¤Ÿï¼Œç‰¹åˆ«æ˜¯å¯¹äºéç»“æ„åŒ–æ–‡æœ¬ï¼Œå› ä¸ºåˆ é™¤å’Œé®è”½ä»ç„¶å¯èƒ½å¯¼è‡´é‡æ–°è¯†åˆ«ã€‚å·®åˆ†éšç§ï¼ˆDPï¼‰æä¾›äº†ä¸€ç§æœ‰åŸåˆ™çš„æ›¿ä»£æ–¹æ¡ˆï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ­£å¼éšç§ä¿è¯çš„åˆæˆæ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªä¸»è¦è´¡çŒ®æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«æ ‡å‡†åŒ–çš„å®ç”¨æ€§å’Œä¿çœŸåº¦æŒ‡æ ‡ï¼Œæ¶µç›–äº†ä¹ä¸ªç²¾é€‰æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ•æ‰åˆ°äº†ç‰¹å®šé¢†åŸŸçš„å¤æ‚æ€§ï¼Œå¦‚æŠ€æœ¯æœ¯è¯­ã€é•¿æœŸä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»å’Œç‰¹æ®Šæ–‡æ¡£ç»“æ„ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯¹æœ€æ–°çš„DPæ–‡æœ¬ç”Ÿæˆæ–¹æ³•å’Œä¸åŒè§„æ¨¡å’Œå¾®è°ƒç­–ç•¥çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œå‘ç°é«˜è´¨é‡é¢†åŸŸç‰¹å®šçš„åˆæˆæ•°æ®ç”Ÿæˆåœ¨DPçº¦æŸä¸‹ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„æŒ‘æˆ˜ï¼Œéšç€é¢†åŸŸå¤æ‚æ€§çš„å¢åŠ ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é’ˆå¯¹åˆæˆæ–‡æœ¬çš„ä¼šå‘˜æ¨ç†æ”»å‡»æ–¹æ³•ï¼Œæä¾›äº†ç¬¬ä¸€æ‰¹ç»éªŒè¯æ®è¡¨æ˜ä½¿ç”¨å…¬å…±æ•°æ®é›† - å¯èƒ½å­˜åœ¨äºé¢„è®­ç»ƒè¯­æ–™åº“ä¸­ - å¯èƒ½ä½¿æ‰€è°“çš„éšç§ä¿è¯å¤±æ•ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä¸¥æ ¼éšç§å®¡è®¡çš„è¿«åˆ‡éœ€æ±‚ï¼Œå¹¶çªå‡ºäº†å¼€æ”¾åŸŸå’Œä¸“ä¸šè¯„ä¼°ä¹‹é—´æŒä¹…çš„å·®è·ï¼Œä¸ºåœ¨éšç§æ•æ„Ÿã€é«˜é£é™©ç¯å¢ƒä¸­è´Ÿè´£ä»»åœ°éƒ¨ç½²ç”Ÿæˆå¼AIæä¾›äº†ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14594v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong><br>æ•°æ®é©±åŠ¨å†³ç­–æ”¯æŒåœ¨åŒ»ç–—å’Œé‡‘èç­‰é«˜é£é™©é¢†åŸŸé¢ä¸´æ•°æ®å…±äº«çš„é‡å¤§éšœç¢ï¼Œä¸»è¦ç”±äºç›‘ç®¡ã€åˆ¶åº¦å’Œéšç§æ‹…å¿§ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ç­‰ç”Ÿæˆå¼AIæ¨¡å‹åœ¨å¼€æ”¾åŸŸä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ•æ„Ÿç¯å¢ƒä¸­çš„é‡‡ç”¨å´å—é™äºä¸å¯é¢„æµ‹çš„è¡Œä¸ºå’Œä¸è¶³çš„éšç§ä¿æŠ¤æ•°æ®é›†ã€‚ç°æœ‰åŒ¿ååŒ–æ–¹æ³•å¯¹äºéç»“æ„åŒ–æ–‡æœ¬å¸¸å¸¸ä¸è¶³ï¼Œå·®å¼‚éšç§ï¼ˆDPï¼‰æä¾›æœ‰åŸåˆ™çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸‰é¡¹å…³é”®è´¡çŒ®è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šå¼•å…¥ç»¼åˆè¯„ä¼°æ¡†æ¶ã€æ ‡å‡†åŒ–å®ç”¨æ€§å’Œä¿çœŸåº¦æŒ‡æ ‡ï¼Œæ¶µç›–ä¹ä¸ªä¸“ä¸šæ•°æ®é›†ï¼›å¯¹æœ€å…ˆè¿›çš„DPæ–‡æœ¬ç”Ÿæˆæ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼›å¼€å‘é’ˆå¯¹åˆæˆæ–‡æœ¬çš„ä¼šå‘˜æ¨ç†æ”»å‡»æ–¹æ³•ã€‚ç ”ç©¶æ­ç¤ºäº†åœ¨å·®å¼‚éšç§çº¦æŸä¸‹ç”Ÿæˆé«˜è´¨é‡çš„ä¸“ä¸šç‰¹å®šåˆæˆæ•°æ®çš„æŒ‘æˆ˜ï¼Œä»¥åŠéšç€é¢†åŸŸå¤æ‚æ€§å¢åŠ æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œä½¿ç”¨é¢„è®­ç»ƒè¯­æ–™åº“ä¸­çš„å…¬å…±æ•°æ®é›†å¯èƒ½ä¼šä½¿å£°ç§°çš„éšç§ä¿è¯å¤±æ•ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å‡¸æ˜¾äº†ä¸¥æ ¼éšç§å®¡è®¡çš„ç´§è¿«éœ€æ±‚ï¼Œå¹¶å¼ºè°ƒäº†å…¬å¼€é¢†åŸŸå’Œä¸“ä¸šè¯„ä¼°ä¹‹é—´çš„æŒç»­å·®è·ï¼Œä¸ºåœ¨éšç§æ•æ„Ÿçš„é«˜é£é™©ç¯å¢ƒä¸­è´Ÿè´£ä»»åœ°éƒ¨ç½²ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æä¾›äº†ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é©±åŠ¨å†³ç­–æ”¯æŒåœ¨é«˜é£é™©é¢†åŸŸé¢ä¸´æ•°æ®å…±äº«éšœç¢ï¼Œä¸»è¦æºäºç›‘ç®¡ã€åˆ¶åº¦å’Œéšç§æ‹…å¿§ã€‚</li>
<li>ç”Ÿæˆå¼AIæ¨¡å‹åœ¨å¼€æ”¾åŸŸä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ•æ„Ÿç¯å¢ƒä¸­çš„é‡‡ç”¨å—é™ã€‚</li>
<li>ç°æœ‰åŒ¿ååŒ–æ–¹æ³•å¯¹äºéç»“æ„åŒ–æ–‡æœ¬å¸¸å¸¸ä¸è¶³ï¼Œå·®å¼‚éšç§ï¼ˆDPï¼‰ä¸ºæ­¤æä¾›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–ä¹ä¸ªä¸“ä¸šæ•°æ®é›†ï¼Œä»¥åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>å¤§è§„æ¨¡å®è¯ç ”ç©¶æ­ç¤ºäº†ç”Ÿæˆé«˜è´¨é‡ä¸“ä¸šç‰¹å®šåˆæˆæ•°æ®çš„æŒ‘æˆ˜ï¼Œæ€§èƒ½éšé¢†åŸŸå¤æ‚æ€§å¢åŠ ä¸‹é™ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒè¯­æ–™åº“ä¸­çš„å…¬å…±æ•°æ®é›†å¯èƒ½ä½¿å£°ç§°çš„éšç§ä¿è¯å¤±æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14594v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14594v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14594v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14594v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DSPC-Dual-Stage-Progressive-Compression-Framework-for-Efficient-Long-Context-Reasoning"><a href="#DSPC-Dual-Stage-Progressive-Compression-Framework-for-Efficient-Long-Context-Reasoning" class="headerlink" title="DSPC: Dual-Stage Progressive Compression Framework for Efficient   Long-Context Reasoning"></a>DSPC: Dual-Stage Progressive Compression Framework for Efficient   Long-Context Reasoning</h2><p><strong>Authors:Yaxin Gao, Yao Lu, Zongfei Zhang, Jiaqi Nie, Shanqing Yu, Qi Xuan</strong></p>
<p>Large language models (LLMs) have achieved remarkable success in many natural language processing (NLP) tasks. To achieve more accurate output, the prompts used to drive LLMs have become increasingly longer, which incurs higher computational costs. To address this prompt inflation problem, prompt compression has been proposed. However, most existing methods require training a small auxiliary model for compression, incurring a significant amount of additional computation. To avoid this, we propose a two-stage, training-free approach, called Dual-Stage Progressive Compression (DSPC). In the coarse-grained stage, semantic-related sentence filtering removes sentences with low semantic value based on TF-IDF. In the fine-grained stage, token importance is assessed using attention contribution, cross-model loss difference, and positional importance, enabling the pruning of low-utility tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo under a constrained token budget and observe consistent improvements. For instance, in the FewShot task of the Longbench dataset, DSPC achieves a performance of 49.17 by using only 3x fewer tokens, outperforming the best state-of-the-art baseline LongLLMLingua by 7.76. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ä¸ºäº†è·å¾—æ›´å‡†ç¡®çš„è¾“å‡ºï¼Œç”¨äºé©±åŠ¨LLMçš„æç¤ºå˜å¾—è¶Šæ¥è¶Šé•¿ï¼Œè¿™å¯¼è‡´äº†æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³æç¤ºè†¨èƒ€é—®é¢˜ï¼Œæå‡ºäº†æç¤ºå‹ç¼©ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éœ€è¦è®­ç»ƒä¸€ä¸ªç”¨äºå‹ç¼©çš„è¾…åŠ©æ¨¡å‹ï¼Œè¿™äº§ç”Ÿäº†å¤§é‡çš„é¢å¤–è®¡ç®—ã€‚ä¸ºäº†é¿å…è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŒé˜¶æ®µæ¸è¿›å‹ç¼©ï¼ˆDSPCï¼‰çš„ä¸¤é˜¶æ®µæ— è®­ç»ƒæ–¹æ³•ã€‚åœ¨ç²—ç²’åº¦é˜¶æ®µï¼Œé€šè¿‡è¯­ä¹‰ç›¸å…³å¥å­è¿‡æ»¤ï¼ŒåŸºäºTF-IDFå»é™¤ä½è¯­ä¹‰ä»·å€¼çš„å¥å­ã€‚åœ¨ç»†ç²’åº¦é˜¶æ®µï¼Œé€šè¿‡æ³¨æ„åŠ›è´¡çŒ®ã€è·¨æ¨¡å‹æŸå¤±å·®å¼‚å’Œä½ç½®é‡è¦æ€§æ¥è¯„ä¼°ä»¤ç‰Œçš„é‡è¦æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™è¯­ä¹‰çš„åŒæ—¶åˆ é™¤ä½æ•ˆç”¨ä»¤ç‰Œã€‚æˆ‘ä»¬åœ¨æœ‰é™çš„ä»¤ç‰Œé¢„ç®—ä¸‹å¯¹LLaMA-3.1-8B-Instructå’ŒGPT-3.5-Turboè¿›è¡Œäº†DSPCéªŒè¯ï¼Œå¹¶è§‚å¯Ÿåˆ°äº†ä¸€è‡´æ€§çš„æ”¹è¿›ã€‚ä¾‹å¦‚ï¼Œåœ¨Longbenchæ•°æ®é›†çš„FewShotä»»åŠ¡ä¸­ï¼ŒDSPCä»…ä½¿ç”¨3å€è¾ƒå°‘çš„ä»¤ç‰Œå°±å®ç°äº†49.17çš„æ€§èƒ½ï¼Œä¼˜äºç›®å‰æœ€ä½³åŸºçº¿LongLLMLingua 7.76ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13723v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨NLPä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œä½†é©±åŠ¨LLMsçš„æç¤ºé€æ¸å¢é•¿ï¼Œå¯¼è‡´æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºè§£å†³æç¤ºè†¨èƒ€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDual-Stage Progressive Compressionï¼ˆDSPCï¼‰çš„ä¸¤é˜¶æ®µæ— è®­ç»ƒå‹ç¼©æ–¹æ³•ã€‚é¦–å…ˆé€šè¿‡è¯­ä¹‰ç›¸å…³å¥å­è¿‡æ»¤å»é™¤ä½è¯­ä¹‰ä»·å€¼å¥å­ï¼Œç„¶åè¯„ä¼°æ ‡è®°çš„é‡è¦æ€§ï¼Œé€šè¿‡æ³¨æ„åŠ›è´¡çŒ®ã€è·¨æ¨¡å‹æŸå¤±å·®å¼‚å’Œä½ç½®é‡è¦æ€§æ¥ä¿®å‰ªä½æ•ˆç”¨æ ‡è®°åŒæ—¶ä¿ç•™è¯­ä¹‰ã€‚åœ¨LLaMAå’ŒGPTæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDSPCåœ¨æœ‰é™çš„æ ‡è®°é¢„ç®—ä¸‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æç¤ºè†¨èƒ€å¯¼è‡´é«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>ä¸ºè§£å†³æç¤ºè†¨èƒ€é—®é¢˜ï¼Œæå‡ºäº†Dual-Stage Progressive Compressionï¼ˆDSPCï¼‰æ–¹æ³•ã€‚</li>
<li>DSPCæ˜¯ä¸€ç§ä¸¤é˜¶æ®µã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç²—ç²’åº¦çš„è¯­ä¹‰ç›¸å…³å¥å­è¿‡æ»¤å’Œç»†ç²’åº¦çš„æ ‡è®°é‡è¦æ€§è¯„ä¼°ã€‚</li>
<li>é€šè¿‡æ³¨æ„åŠ›è´¡çŒ®ã€è·¨æ¨¡å‹æŸå¤±å·®å¼‚å’Œä½ç½®é‡è¦æ€§æ¥è¯„ä¼°æ ‡è®°çš„é‡è¦æ€§ã€‚</li>
<li>DSPCèƒ½åœ¨æœ‰é™çš„æ ‡è®°é¢„ç®—ä¸‹å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>DSPCåœ¨LLaMAå’ŒGPTæ¨¡å‹ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æœ€ä½³åŸºçº¿LongLLMLinguaã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.13723v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.13723v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.13723v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.13723v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_LLM/2509.15217v1/page_5_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  LNE-Blocking An Efficient Framework for Contamination Mitigation   Evaluation on Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/Interactive/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Interactive/2509.13737v1/page_0_0.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-19  Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force   Control and Gait Control
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
