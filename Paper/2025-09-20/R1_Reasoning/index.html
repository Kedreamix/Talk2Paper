<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-20  Generalizable Geometric Image Caption Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15194v1/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-20-更新"><a href="#2025-09-20-更新" class="headerlink" title="2025-09-20 更新"></a>2025-09-20 更新</h1><h2 id="Generalizable-Geometric-Image-Caption-Synthesis"><a href="#Generalizable-Geometric-Image-Caption-Synthesis" class="headerlink" title="Generalizable Geometric Image Caption Synthesis"></a>Generalizable Geometric Image Caption Synthesis</h2><p><strong>Authors:Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Renjie Pi, Shizhe Diao, Tong Zhang</strong></p>
<p>Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8%\text{-}4.8%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4%\text{-}3.9%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU. </p>
<blockquote>
<p>多模态大型语言模型具有各种实际应用，这些应用需要强大的推理能力。尽管最近有进展，但这些模型在解决复杂的几何问题时仍然感到困难。主要挑战源于缺乏用于理解几何图像的高质量图像文本对数据集。此外，大多数基于模板的数据合成管道通常无法推广到其预定义模板之外的问题。在本文中，我们通过将可验证奖励强化学习（RLVR）的补充过程引入数据生成管道来弥补这一差距。通过采用RLVR来精炼由50种基本几何关系合成的几何图像的标题，并使用来源于数学问题解决任务的奖励信号，我们的管道成功地捕捉到了解决几何问题的关键特征。这实现了更好的任务泛化，并产生了不小的改进。此外，即使在超出分布的场景下，生成的数据集也能提高多模态大型语言模型的一般推理能力，在数学视野（MathVista）和数学宇宙（MathVerse）的非几何输入图像统计、算术、代数和数值任务中准确率提高了2.8%~4.8%，在MMU的艺术、设计、技术和工程任务中准确率提高了2.4%~3.9%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15217v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多模态大型语言模型在需要强大推理能力的实际应用中存在挑战，特别是在解决复杂几何问题时。本文引入了一种基于强化学习与可验证奖励（RLVR）的数据生成管道，通过精炼几何图像合成中的字幕并使用来自数学问题解决任务的奖励信号，成功解决了这一问题。这不仅提高了模型的几何问题解决能力，还增强了模型在统计、算术、代数和数值任务中的一般推理能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型在解决复杂几何问题时存在挑战。</li>
<li>缺乏高质量的图像文本对数据集是理解几何图像的主要障碍之一。</li>
<li>大多数基于模板的数据合成管道无法推广到超出其预定义模板的问题。</li>
<li>引入强化学习与可验证奖励（RLVR）的数据生成管道，解决了上述问题。</li>
<li>通过精炼几何图像合成的字幕，该管道成功捕捉了解决几何问题的关键特征。</li>
<li>在数学和非几何图像的任务中，生成的数据集增强了模型的一般推理能力，并带来显著的准确性提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15217">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15217v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15217v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15217v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15217v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation"><a href="#Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation" class="headerlink" title="Evolving Language Models without Labels: Majority Drives Selection,   Novelty Promotes Variation"></a>Evolving Language Models without Labels: Majority Drives Selection,   Novelty Promotes Variation</h2><p><strong>Authors:Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu</strong></p>
<p>Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model’s inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL’s 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability. </p>
<blockquote>
<p>大型语言模型（LLMs）越来越多地采用基于可验证奖励的强化学习（RLVR）进行训练，但在现实世界的部署中，需要模型能够在没有标签或外部评判的情况下自我改进。现有的无标签方法，如置信度最小化、自我一致性或多数投票目标，虽然可以稳定学习，但会逐步减少探索，导致熵崩溃：生成的内容变得更短、更少样化和脆弱。不同于现有的测试时强化学习（TTRL）方法，后者主要适应手头的即时无标签数据集，我们的目标更为广泛：即在不影响模型固有的探索能力和泛化能力的情况下实现一般性的改进，即进化。我们形式化了这个问题，并提出了面向进化的无标签强化学习（EVOL-RL），这是一个简单的规则，在标签设置下实现稳定性和变异的结合。EVOL-RL将多数投票的答案作为稳定的锚点（选择），同时添加一个意识到新颖性的奖励，该奖励有利于那些推理与已产生的结果不同的回答（变异），并在语义空间中进行测量。结合GPRO实现的EVOL-RL还使用不对称裁剪来保留强信号和熵正则化来维持搜索。这种“多数投票选择+新奇变异”的设计可以防止崩溃，保持更长的、更具信息量的思考链，并提高了pass@1和pass@n。EVOL-RL始终优于仅使用投票选择的TTRL基线；例如，在无标签AIME24上进行训练使Qwen3-4B-Base AIME25的pass@1从TTRL的4.6%提升到16.4%，pass@16从18.5%提升到37.9%。EVOL-RL不仅防止了多样性崩溃，还提高了跨领域的泛化能力（例如GPQA）。此外，我们还证明了EVOL-RL在RLVR设置中也提升了性能，凸显了其广泛的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型采用强化学习从可验证奖励（RLVR）进行训练越来越普遍，但在现实世界的部署中需要模型能够自我改进而无需标签或外部评判。现有的无标签方法如信心最小化、自我一致性或多数投票目标等，虽然稳定了学习，但会逐步减少探索，导致熵崩溃，生成的文本变得简短、缺乏多样性且脆弱。本文提出一种名为EVOL-RL的新方法，旨在实现无标签下的通用改进而不牺牲模型的探索能力和泛化能力。EVOL-RL将多数投票的答案作为稳定的锚点（选择），同时增加一个重视响应中不同于已生成内容的新颖性奖励（变异），并在语义空间中进行测量。该方法防止了崩溃，保持了更长的思考链，并提高了pass@1和pass@n。EVOL-RL在多个实验上均表现出超越仅依赖多数投票的基线方法的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在强化学习从可验证奖励训练过程中面临现实世界的部署挑战。</li>
<li>现有无标签方法稳定了学习但减少了探索，导致生成文本的多样性和创新性下降。</li>
<li>EVOL-RL方法旨在在无标签下实现更广泛的改进，旨在平衡稳定性和探索能力。</li>
<li>EVOL-RL结合了多数投票稳定和基于新颖性的奖励（变异），以防止模型生成单一答案并保持语义多样性。</li>
<li>通过使用GRPO和不对称裁剪等技术实现EVOL-RL，有效保留强信号并维持搜索。</li>
<li>EVOL-RL在多种实验设置中表现优于基线方法，包括无标签AIME24训练和跨域泛化能力测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15194">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15194v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15194v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15194v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding"><a href="#Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding" class="headerlink" title="Unleashing the Potential of Multimodal LLMs for Zero-Shot   Spatio-Temporal Video Grounding"></a>Unleashing the Potential of Multimodal LLMs for Zero-Shot   Spatio-Temporal Video Grounding</h2><p><strong>Authors:Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lau</strong></p>
<p>Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model’s attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at <a target="_blank" rel="noopener" href="https://github.com/zaiquanyang/LLaVA_Next_STVG">https://github.com/zaiquanyang/LLaVA_Next_STVG</a>. </p>
<blockquote>
<p>时空视频定位（STVG）旨在根据输入的文本查询定位视频中的时空管道。在本文中，我们利用多模态大型语言模型（MLLMs）探索STVG中的零样本解决方案。我们揭示了关于MLLMs的两个关键见解：（1）MLLMs倾向于动态分配特殊令牌，称为“定位令牌”，用于定位文本查询；（2）由于无法完全整合文本查询中的线索（例如属性、动作）进行推理，MLLMs经常遭受次优定位。基于这些见解，我们提出了基于MLLM的零样本STVG框架，包括新型分解时空高亮（DSTH）和时间增强装配（TAS）策略，以释放MLLM的推理能力。DSTH策略首先是将原始查询分解为属性和动作子查询，以询问目标在空间和时间上的存在。然后，它使用新型的对数引导再注意（LRA）模块来学习作为空间和时间提示的潜在变量，通过规范化每个子查询的令牌预测。这些提示分别突出属性和动作线索，引导模型关注与空间和时间相关的可靠视觉区域。此外，由于属性子查询的空间定位在时间上应该是一致的，我们引入了TAS策略，使用原始视频帧和时间增强帧作为输入来组装预测，以帮助提高时间一致性。我们在各种MLLMs上评估了我们的方法，并在三个常见的STVG基准测试上展示了其超越现有技术方法的效果。相关代码将发布在<a target="_blank" rel="noopener" href="https://github.com/zaiquanyang/LLaVA_Next_STVG%E3%80%82">https://github.com/zaiquanyang/LLaVA_Next_STVG。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15178v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于多模态大型语言模型（MLLMs）的零样本时空视频定位（STVG）方法。文章揭示了MLLMs在STVG中的两个关键见解，并提出了基于MLLM的零样本框架，包括分解时空突出显示（DSTH）和时间增强装配（TAS）策略，以释放MLLM的推理能力。DSTH策略将原始查询分解为属性子查询和动作子查询，以学习空间和时间提示。TAS策略通过结合原始视频帧和时间增强帧来提高时间一致性。在多个MLLM上的实验表明，该方法在三个常见的STVG基准测试上优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文介绍了时空视频定位（STVG）的目标，旨在通过文本查询定位视频中的时空管道。</li>
<li>利用多模态大型语言模型（MLLMs）探索STVG的零样本解决方案。</li>
<li>揭示了MLLMs在STVG中的两个关键见解：动态分配特殊标记用于文本查询定位以及由于无法完全整合文本查询中的线索而导致的次优定位问题。</li>
<li>提出了一种基于MLLM的零样本框架，包括分解时空突出显示（DSTH）和时间增强装配（TAS）策略。</li>
<li>DSTH策略通过将原始查询分解为属性子查询和动作子查询来学习空间和时间提示，提高模型定位精度。</li>
<li>TAS策略结合原始视频帧和时间增强帧来提高时间一致性。</li>
<li>在多个MLLM上的实验表明，该方法在三个常见的STVG基准测试上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15178v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15178v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15178v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15178v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Data-Rewriting-for-Stable-Off-Policy-Supervised-Fine-Tuning"><a href="#Mind-the-Gap-Data-Rewriting-for-Stable-Off-Policy-Supervised-Fine-Tuning" class="headerlink" title="Mind the Gap: Data Rewriting for Stable Off-Policy Supervised   Fine-Tuning"></a>Mind the Gap: Data Rewriting for Stable Off-Policy Supervised   Fine-Tuning</h2><p><strong>Authors:Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong Qin</strong></p>
<p>Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Off-Policy-SFT">https://github.com/NKU-HLT/Off-Policy-SFT</a>. </p>
<blockquote>
<p>大型语言模型的监督微调（SFT）可以被视为一种离线策略学习问题，其中专家演示来自固定的行为策略，而训练的目标是优化目标策略。重要性采样是纠正这种分布不匹配的标准工具，但策略差距较大会导致方差高和训练不稳定。现有方法使用KL惩罚或裁剪来缓解这个问题，这些方法被动约束更新而不是主动缩小差距。我们提出了一种简单有效的数据重写框架，它通过保留正确解决方案作为在线策略数据，并引导解决方式重写错误数据，仅在必要时才回退到专家演示。这在对优化之前对齐训练分布与目标策略，减少重要性采样的方差并稳定离线微调。在五个数学推理基准测试上的实验表明，与普通的SFT和最新的动态微调（DFT）方法相比，该方法具有一致且显著的收益。数据和代码将在<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Off-Policy-SFT%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NKU-HLT/Off-Policy-SFT上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的监督微调（SFT）可以看作是一个离线策略学习问题，其中专家演示来自固定的行为策略，而训练旨在优化目标策略。重要性采样是纠正这种分布不匹配的标准工具，但大的策略间隔会导致高方差和训练不稳定。现有方法使用KL惩罚或裁剪来被动约束更新，而不是主动减少间隔。我们提出了一种简单有效的数据重写框架，它通过保持正确的解决方案作为在线策略数据，并引导解决不正确的问题，只在必要时回退到专家演示，从而主动缩小策略间隔。这将对齐训练分布与目标策略，减少重要性采样的方差并稳定离线策略的微调。在五个数学推理基准测试上的实验表明，与标准的SFT和当前最先进的动态微调（DFT）方法相比，该框架具有一致且显著的优点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>监督微调（SFT）可视为离线策略学习问题，其中专家演示与训练目标策略之间存在分布不匹配。</li>
<li>重要性采样是纠正这种分布不匹配的标准工具，但存在高方差和训练不稳定的问题。</li>
<li>现有方法使用KL惩罚或裁剪被动约束更新，效果有限。</li>
<li>提出的数据重写框架通过主动缩小策略间隔，保持正确解决方案并引导解决错误问题，提高训练稳定性。</li>
<li>该框架与专家演示相结合，对齐训练分布与目标策略，减少重要性采样的方差。</li>
<li>在五个数学推理基准测试上，该框架相比标准SFT和DFT具有显著优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15157v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MedFact-R1-Towards-Factual-Medical-Reasoning-via-Pseudo-Label-Augmentation"><a href="#MedFact-R1-Towards-Factual-Medical-Reasoning-via-Pseudo-Label-Augmentation" class="headerlink" title="MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label   Augmentation"></a>MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label   Augmentation</h2><p><strong>Authors:Gengliang Li, Rongyu Chen, Bin Li, Linlin Yang, Guodong Ding</strong></p>
<p>Ensuring factual consistency and reliable reasoning remains a critical challenge for medical vision-language models. We introduce MEDFACT-R1, a two-stage framework that integrates external knowledge grounding with reinforcement learning to improve the factual medical reasoning. The first stage uses pseudo-label supervised fine-tuning (SFT) to incorporate external factual expertise; while the second stage applies Group Relative Policy Optimization (GRPO) with four tailored factual reward signals to encourage self-consistent reasoning. Across three public medical QA benchmarks, MEDFACT-R1 delivers up to 22.5% absolute improvement in factual accuracy over previous state-of-the-art methods. Ablation studies highlight the necessity of pseudo-label SFT cold start and validate the contribution of each GRPO reward, underscoring the synergy between knowledge grounding and RL-driven reasoning for trustworthy medical AI. Codes are released at <a target="_blank" rel="noopener" href="https://github.com/Garfieldgengliang/MEDFACT-R1">https://github.com/Garfieldgengliang/MEDFACT-R1</a>. </p>
<blockquote>
<p>确保事实一致性和可靠的推理仍然是医学视觉语言模型的关键挑战。我们介绍了MEDFACT-R1，这是一个两阶段的框架，它结合了外部知识接地和强化学习，以提高医学事实的推理能力。第一阶段使用伪标签监督微调（SFT）来融入外部事实专业知识；而第二阶段采用集团相对策略优化（GRPO）和四种定制的实时奖励信号来鼓励自我一致的推理。在三个公开的医学问答基准测试中，MEDFACT-R1在事实准确性方面较之前的最先进方法提高了高达22.5%的绝对幅度。消融研究突出了伪标签SFT冷启动的必要性，验证了每个GRPO奖励的贡献，强调了知识接地和RL驱动推理之间的协同作用，以实现可信赖的医学人工智能。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/Garfieldgengliang/MEDFACT-R1%E3%80%82">https://github.com/Garfieldgengliang/MEDFACT-R1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15154v1">PDF</a> Tech report</p>
<p><strong>Summary</strong><br>医学视觉语言模型在保障事实一致性和可靠推理方面面临挑战。我们推出MEDFACT-R1，一个两阶段框架，整合外部知识接地与强化学习，以改善医学事实推理。第一阶段使用伪标签监督微调（SFT）融入外部事实专业知识；第二阶段采用群体相对策略优化（GRPO）与四个定制的事实奖励信号，以鼓励自我一致推理。在三个公共医学问答基准测试中，MEDFACT-R1在事实准确性方面较之前的最先进方法提高了高达22.5%的绝对准确度。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MEDFACT-R1是一个针对医学视觉语言模型的两阶段框架。</li>
<li>该框架整合了外部知识接地与强化学习，旨在提高事实推理能力。</li>
<li>第一阶段通过伪标签监督微调（SFT）融入外部事实专业知识。</li>
<li>第二阶段采用群体相对策略优化（GRPO）与定制的事实奖励信号，鼓励自我一致推理。</li>
<li>MEDFACT-R1在三个医学问答基准测试中实现了显著的事实准确性提升。</li>
<li>消融研究强调了伪标签SFT冷启动的必要性，并验证了每个GRPO奖励的贡献。</li>
<li>知识接地和RL驱动推理之间的协同作用对于构建可信赖的医学AI至关重要。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15154">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15154v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15154v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15154v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15154v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TDRM-Smooth-Reward-Models-with-Temporal-Difference-for-LLM-RL-and-Inference"><a href="#TDRM-Smooth-Reward-Models-with-Temporal-Difference-for-LLM-RL-and-Inference" class="headerlink" title="TDRM: Smooth Reward Models with Temporal Difference for LLM RL and   Inference"></a>TDRM: Smooth Reward Models with Temporal Difference for LLM RL and   Inference</h2><p><strong>Authors:Dan Zhang, Min Cai, Jonathan Li, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang</strong></p>
<p>Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences during training. This temporal-difference (TD) regularization produces smooth rewards and improves alignment with long-term objectives. Incorporating TDRM into the actor-critic style online RL loop yields consistent empirical gains. It is worth noting that TDRM is a supplement to verifiable reward methods, and both can be used in series. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL – achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain – and yield higher-quality language model policies on 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at <a target="_blank" rel="noopener" href="https://github.com/THUDM/TDRM">https://github.com/THUDM/TDRM</a>. </p>
<blockquote>
<p>奖励模型在结合语言模型的强化学习（RL）和推理时间验证中起着核心作用。然而，现有的奖励模型通常缺乏时间一致性，导致策略更新无效和强化学习训练不稳定。我们引入了TDRM方法，通过最小化训练过程中的时间差异来学习更平滑、更可靠的奖励模型。这种时间差异（TD）正则化产生了平滑的奖励，并改善了与长期目标的对齐。将TDRM纳入演员评论家风格的在线强化学习循环中，可以获得持续的经验收益。值得注意的是，TDRM是验证奖励方法的一种补充，两者可以串联使用。实验表明，经过TD训练的进程奖励模型（PRM）在最佳N（最高6.6%）和树搜索（最高23.7%）环境中提高了性能。当与可验证奖励的强化学习（RLVR）相结合时，TD训练的PRM能够实现更高效的数据型强化学习——仅用2.5k数据就能达到与基线方法需要50.1k数据相当的性能——并在8种模型变体（5个系列）上产生了更高质量的语言模型策略，例如Qwen2.5-（0.5B，1.5B），GLM4-9B-0414，GLM-Z1-9B-0414，Qwen2.5-Math-（1.5B，7B）和DeepSeek-R1-Distill-Qwen-（1.5B，7B）。我们已在<a target="_blank" rel="noopener" href="https://github.com/THUDM/TDRM%E5%8F%91%E5%B8%83%E6%89%80%E6%9C%89%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/THUDM/TDRM发布所有代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15110v1">PDF</a> 9 figures, 7 tables</p>
<p><strong>摘要</strong></p>
<p>本文介绍了TDRM方法，该方法通过最小化训练过程中的时间差异来学习更平滑、更可靠的奖励模型。TDRM的临时差异正则化产生平滑的奖励，并改善与长期目标的对齐。将TDRM纳入在线RL循环中的actor-critic风格，可带来持续的实证收益。实验表明，经过TD训练的进程奖励模型（PRM）在Best-of-N和树搜索设置中提高了性能。当与可验证奖励的强化学习（RLVR）结合时，TD训练的PRM可实现更高效的数据驱动RL，并在多个语言模型变体上产生更高质量的语言模型策略。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>TDRM方法通过学习更平滑、更可靠的奖励模型来增强强化学习（RL）的效果。</li>
<li>通过最小化训练过程中的时间差异来实现奖励模型的临时差异正则化。</li>
<li>TDRM能提高奖励模型与长期目标的对齐程度。</li>
<li>在actor-critic风格的在线RL循环中引入TDRM，可获得持续的实证效益。</li>
<li>实验显示，TD训练的进程奖励模型（PRM）在多种设置下提高了性能，包括Best-of-N和树搜索。</li>
<li>当与RLVR结合时，TD训练的PRM可实现更高效的数据驱动RL，并在多个语言模型变体上表现出更高的性能。</li>
<li>所有代码已发布在<a target="_blank" rel="noopener" href="https://github.com/THUDM/TDRM%E3%80%82">https://github.com/THUDM/TDRM。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15110">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15110v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15110v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15110v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15110v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing"><a href="#AutoEdit-Automatic-Hyperparameter-Tuning-for-Image-Editing" class="headerlink" title="AutoEdit: Automatic Hyperparameter Tuning for Image Editing"></a>AutoEdit: Automatic Hyperparameter Tuning for Image Editing</h2><p><strong>Authors:Chau Pham, Quan Dao, Mahesh Bhosale, Yunjie Tian, Dimitris Metaxas, David Doermann</strong></p>
<p>Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing’s hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world. </p>
<blockquote>
<p>最近扩散模型的进展彻底改变了文本引导的图像编辑，但现有的编辑方法在超参数识别方面面临严峻挑战。为了获得合理的编辑性能，这些方法通常要求用户强行调整多个相互依赖的超参数，如反转时间步长和注意力修改等。这一过程由于超参数搜索空间巨大而产生了高昂的计算成本。我们认为在扩散去噪过程中搜索最佳编辑超参数是一项序贯决策任务。具体来说，我们提出了一种强化学习框架，它建立了马尔可夫决策过程，在去噪步骤中动态调整超参数，并将编辑目标整合到奖励函数中。该方法通过近端策略优化实现了时间效率，同时保持了最佳超参数配置。实验表明，与现有的暴力方法相比，该方法在搜索时间和计算开销方面大大减少了，推动了基于扩散的图像编辑框架在实际世界中的实际应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15031v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong>：<br>最新扩散模型技术的进展在文本引导的图像编辑领域带来了革命性变化，但现有编辑方法在超参数识别方面面临重大挑战。为了获得合理的编辑性能，这些方法通常需要用户暴力调整多个相互依赖的超参数，如反转时间步长和注意力修改等。这一过程由于超参数搜索空间的巨大而产生了高昂的计算成本。本研究将优化编辑超参数视为扩散去噪过程中的一个决策制定任务，并提出一种强化学习框架。该框架建立了一个马尔可夫决策过程，可以在去噪步骤中动态调整超参数，并将编辑目标集成到奖励函数中。该方法通过近端策略优化实现了时间效率，同时保持了最优超参数配置。实验证明，与现有的暴力方法相比，该方法显著减少了搜索时间和计算开销，推动了扩散图像编辑框架在现实世界的实际应用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>扩散模型进步推动了文本引导图像编辑的革新。</li>
<li>现有图像编辑方法在超参数识别方面存在挑战。</li>
<li>用户需要暴力调整多个超参数以获得合理编辑性能，导致高计算成本。</li>
<li>研究采用强化学习框架来处理超参数优化问题。</li>
<li>建立了马尔可夫决策过程，可在去噪步骤中动态调整超参数。</li>
<li>将编辑目标集成到奖励函数中，实现时间效率并维持超参数优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15031">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15031v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15031v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15031v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Indoor-Fluid-Antenna-Systems-Enabled-by-Layout-Specific-Modeling-and-Group-Relative-Policy-Optimization"><a href="#Indoor-Fluid-Antenna-Systems-Enabled-by-Layout-Specific-Modeling-and-Group-Relative-Policy-Optimization" class="headerlink" title="Indoor Fluid Antenna Systems Enabled by Layout-Specific Modeling and   Group Relative Policy Optimization"></a>Indoor Fluid Antenna Systems Enabled by Layout-Specific Modeling and   Group Relative Policy Optimization</h2><p><strong>Authors:Tong Zhang, Qianren Li, Shuai Wang, Wanli Ni, Jiliang Zhang, Rui Wang, Kai-Kit Wong, Chan-Byoung Chae</strong></p>
<p>The fluid antenna system (FAS) revolutionizes wireless communications by employing position-flexible antennas that dynamically optimize channel conditions and mitigate multipath fading. This innovation is particularly valuable in indoor environments, where signal propagation is severely degraded due to structural obstructions and complex multipath reflections. In this paper, we study the channel modeling and joint optimization of antenna positioning, beamforming, and power allocation for indoor FAS. In particular, we propose, for the first time, a layout-specific channel model and a novel group relative policy optimization (GRPO) algorithm for indoor FAS. Compared to the state-of-the-art Sionna model, our approach achieves an $83.3%$ reduction in computation time with an approximately $3$ dB increase in root-mean-square error (RMSE). When simplified to a two-ray model, our channel model enables a closed-form solution for the optimal antenna position, achieving near-optimal performance. {For the joint optimization problem, the proposed GRPO algorithm outperforms proximal policy optimization (PPO) and other baselines in sum-rate, while requiring only 49.2% computational resources of PPO, due to its group-based advantage estimation.} Simulation results reveal that increasing either the group size or trajectory length in GRPO does not yield significant improvements in sum-rate, suggesting that these parameters can be selected conservatively without sacrificing performance. </p>
<blockquote>
<p>流体天线系统（FAS）通过采用动态优化信道条件并减轻多路径衰落的定位灵活天线，实现了无线通信的革命性变革。这种创新在室内部署中尤其具有价值，因为结构阻碍和复杂的多路径反射会严重削弱信号传播。在本文中，我们研究了室内FAS的信道建模以及天线定位、波束形成和功率分配的联合优化。特别是我们首次提出了针对布局的特定信道模型和用于室内FAS的新型分组相对策略优化（GRPO）算法。与最新的Sionna模型相比，我们的方法在计算时间上实现了83.3%的减少，同时均方根误差（RMSE）大约增加了3 dB。简化为两射线模型后，我们的信道模型能够对最佳天线位置提供封闭式解决方案，实现接近最佳性能。对于联合优化问题，所提出的GRPO算法在总和速率上优于近端策略优化（PPO）和其他基准线，并且只需要PPO的49.2%的计算资源，这得益于其基于分组的优势评估。仿真结果表明，在GRPO中增加群体规模或轨迹长度并不会在总和速率上产生重大改进，这表明这些参数可以在不牺牲性能的情况下保守选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15006v1">PDF</a> 15 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了流体天线系统（FAS）在室内环境中的无线通信技术。该系统采用位置灵活的天线，可动态优化信道条件并减轻多路径衰减。研究了对室内FAS的信道建模和天线定位、波束形成以及功率分配的联合优化问题。提出了一种布局特定的信道模型和一种新的群组相对策略优化（GRPO）算法。与现有技术相比，该方法在计算时间上减少了83.3%，同时均方根误差（RMSE）增加了约3分贝。简化为两射线模型后，为最优天线位置提供了闭式解决方案，实现近最优性能。此外，GRPO算法在联合优化问题中表现出色，计算资源仅为近端策略优化（PPO）的49.2%，且增加群组大小或轨迹长度并不会显著提高总和速率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>流体天线系统（FAS）利用位置灵活的天线，可在室内环境中动态优化无线信道条件并缓解多路径衰减问题。</li>
<li>本文研究了室内FAS的信道建模和天线定位、波束形成以及功率分配的联合优化。</li>
<li>提出了一种布局特定的信道模型和新的群组相对策略优化（GRPO）算法。</li>
<li>与现有模型相比，新方法在计算时间上显著减少，同时实现了较高的均方根误差（RMSE）。</li>
<li>在两射线模型下，为最优天线位置提供了闭式解决方案。</li>
<li>GRPO算法在联合优化问题中表现出优异的性能，且计算资源需求较低。</li>
<li>增加群组大小或轨迹长度并不会显著提高系统总和速率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15006">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.15006v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Novel-Task-Driven-Diffusion-Based-Policy-with-Affordance-Learning-for-Generalizable-Manipulation-of-Articulated-Objects"><a href="#A-Novel-Task-Driven-Diffusion-Based-Policy-with-Affordance-Learning-for-Generalizable-Manipulation-of-Articulated-Objects" class="headerlink" title="A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for   Generalizable Manipulation of Articulated Objects"></a>A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for   Generalizable Manipulation of Articulated Objects</h2><p><strong>Authors:Hao Zhang, Zhen Kan, Weiwei Shang, Yongduan Song</strong></p>
<p>Despite recent advances in dexterous manipulations, the manipulation of articulated objects and generalization across different categories remain significant challenges. To address these issues, we introduce DART, a novel framework that enhances a diffusion-based policy with affordance learning and linear temporal logic (LTL) representations to improve the learning efficiency and generalizability of articulated dexterous manipulation. Specifically, DART leverages LTL to understand task semantics and affordance learning to identify optimal interaction points. The {diffusion-based policy} then generalizes these interactions across various categories. Additionally, we exploit an optimization method based on interaction data to refine actions, overcoming the limitations of traditional diffusion policies that typically rely on offline reinforcement learning or learning from demonstrations. Experimental results demonstrate that DART outperforms most existing methods in manipulation ability, generalization performance, transfer reasoning, and robustness. For more information, visit our project website at: <a target="_blank" rel="noopener" href="https://sites.google.com/view/dart0257/">https://sites.google.com/view/dart0257/</a>. </p>
<blockquote>
<p>尽管最近灵巧操作方面取得了进展，但处理关节式物体和不同类别之间的泛化仍是重大挑战。为了解决这些问题，我们引入了DART，这是一种新型框架，它通过结合基于扩散的策略、可达性学习以及线性时序逻辑（LTL）表示，提高了关节灵巧操作的学习效率和泛化能力。具体来说，DART利用LTL理解任务语义，并利用可达性学习确定最佳交互点。然后，{基于扩散的策略}将这些交互在不同类别中进行推广。此外，我们采用基于交互数据的优化方法来完善动作，克服了传统依赖离线强化学习或模仿学习的扩散策略的限制。实验结果表明，DART在操作能力、泛化性能、迁移推理和稳健性方面大大优于现有大多数方法。如需更多信息，请访问我们的项目网站：<a target="_blank" rel="noopener" href="https://sites.google.com/view/dart0">https://sites.google.com/view/dart0</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14939v1">PDF</a> Accepted by IEEE&#x2F;ASME Transactions on Mechatronics</p>
<p><strong>Summary</strong><br>：针对灵巧操作中的关节物体操作和跨不同类别的泛化等挑战，提出了一种名为DART的新型框架。该框架结合了扩散策略、仿射学习和线性时序逻辑表示，以提高关节灵巧操作的效率和泛化能力。DART利用线性时序逻辑理解任务语义，并通过仿射学习确定最佳交互点。此外，它还利用基于交互数据的优化方法来完善动作，克服了传统扩散策略依赖于离线强化学习或示范学习的局限性。实验结果表明，DART在操作能力、泛化性能、迁移推理和稳健性方面优于大多数现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DART框架结合了扩散策略、仿射学习和线性时序逻辑（LTL）以提高关节灵巧操作的效率和泛化能力。</li>
<li>DART利用LTL理解任务语义，这有助于提升操作的精准度和效率。</li>
<li>仿射学习在DART中用于确定最佳交互点，增强了操作过程中的细节把握。</li>
<li>DART通过基于交互数据的优化方法完善了动作，这一方法克服了传统扩散策略的局限性。</li>
<li>实验结果显示，DART在操作能力上表现出色，特别是在复杂和多样化的物体上。</li>
<li>DART在泛化性能、迁移推理方面也有显著优势，能够适应不同的任务和场景。</li>
<li>DART的稳健性得到了验证，能够在各种条件下稳定地进行操作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14939">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14939v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14939v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14939v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MARIC-Multi-Agent-Reasoning-for-Image-Classification"><a href="#MARIC-Multi-Agent-Reasoning-for-Image-Classification" class="headerlink" title="MARIC: Multi-Agent Reasoning for Image Classification"></a>MARIC: Multi-Agent Reasoning for Image Classification</h2><p><strong>Authors:Wonduk Seo, Minhyeong Yu, Hyunjin An, Seunghyun Lee</strong></p>
<p>Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification. </p>
<blockquote>
<p>图像分类传统上依赖于参数密集型的模型训练，需要大规模标注数据集和大量的微调才能达到竞争性能。虽然最近的视觉语言模型（VLMs）缓解了一些这些约束，但它们仍然受限于对单通道表示的依赖，往往无法捕获视觉内容的互补方面。在本文中，我们介绍了基于多智能体的图像分类推理（MARIC），这是一个多智能体框架，它重新定义了图像分类为协作推理过程。MARIC首先利用概述智能体分析图像的整体主题并生成有针对性的提示。基于这些提示，三个方面智能体沿着不同的视觉维度提取精细的描述。最后，一个推理智能体通过集成反射步骤合成这些互补输出，产生用于分类的统一表示。通过显式地将任务分解为多个角度并鼓励反思合成，MARIC缓解了参数繁重的训练和单一VLM推理的缺点。在四个不同的图像分类基准数据集上的实验表明，MARIC显著优于基线，突显了多智能体视觉推理在稳健和可解释的图像分类中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14860v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>图像分类过去依赖于参数密集型的模型训练，需要大规模标注数据集和精细调整来获得竞争性能。虽然最近的视觉语言模型（VLMs）缓解了一些这些约束，但它们仍然受限于单一通道表示，往往无法捕捉视觉内容的互补方面。本文介绍了一种基于多智能体的图像分类推理方法（MARIC），它将图像分类重新定义为一种协作推理过程。首先，MARIC使用Outline Agent分析图像的全局主题并生成目标提示。基于这些提示，三个Aspect Agents沿不同视觉维度提取精细的详细描述。最后，Reasoning Agent通过综合反映步骤合成这些互补输出，为分类提供统一表示。通过明确地将任务分解为多个角度并鼓励反思综合，MARIC缓解了参数密集训练和单一VLM推理的缺点。在四个不同的图像分类基准数据集上的实验表明，MARIC显著优于基线方法，突显了多智能体视觉推理在鲁棒和可解释性图像分类中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统图像分类依赖于参数密集型的模型训练，需要大规模标注数据集和精细调整。</li>
<li>视觉语言模型（VLMs）虽然缓解了一些约束，但仍受限于单一通道表示。</li>
<li>MARIC采用多智能体框架，将图像分类定义为协作推理过程。</li>
<li>MARIC使用Outliner Agent分析图像全局主题并生成目标提示。</li>
<li>Aspect Agents提取不同视觉维度的精细描述。</li>
<li>Reasoning Agent合成互补输出，为分类提供统一表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14860">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14860v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects"><a href="#CodeFuse-CR-Bench-A-Comprehensiveness-aware-Benchmark-for-End-to-End-Code-Review-Evaluation-in-Python-Projects" class="headerlink" title="CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End   Code Review Evaluation in Python Projects"></a>CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End   Code Review Evaluation in Python Projects</h2><p><strong>Authors:Hanyang Guo, Xunjin Zheng, Zihan Liao, Hang Yu, Peng DI, Ziyin Zhang, Hong-Ning Dai</strong></p>
<p>Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a “reality gap”: existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose a novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants. </p>
<blockquote>
<p>自动化代码审查（CR）是大规模语言模型（LLM）的关键应用，但进展受到“现实差距”的阻碍：现有基准测试使用简化、缺乏上下文的数据在孤立的子任务上评估模型。这未能反映现实世界CR的整体丰富上下文特性。为了弥补这一差距，我们引入了CodeFuse-CR-Bench，这是第一个用于仓库级别CR评估的全面认知基准测试。CodeFuse-CR-Bench包含来自涵盖九个Pull请求（PR）问题域的70个Python项目的601个高质量实例，每个实例都提供了丰富的多方面上下文，包括相关问题、PR详细信息和仓库状态，从而实现端到端的评估。除了表面指标之外，我们还提出了一个新的评估框架，该框架结合了基于位置的和基于语法的规则检查以及基于模型的审查质量判断。我们对这一全面的CR任务进行了首次大规模评估，对最新LLM进行了评估。我们的结果建立了关键的基准线并揭示：（1）没有单一的LLM在所有方面的CR中占主导地位；（2）Gemini 2.5 Pro的综合性能最高；（3）不同的LLM对冗余上下文的稳健性表现出差异。这些发现强调了全面多维评估的必要性，并为开发真正智能且实用的CR助手提供了可操作性的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14856v1">PDF</a> </p>
<p><strong>Summary</strong><br>自动化代码审查（CR）是大型语言模型（LLM）的关键应用之一，但现实差距阻碍了其进展：现有的基准测试通常使用简化且缺乏上下文的数据对孤立的子任务进行评估，无法反映现实世界CR的整体丰富上下文特性。为了弥补这一差距，我们推出了CodeFuse-CR-Bench，这是一个面向存储库级别的CR评估的全面感知基准测试。CodeFuse-CR-Bench包含来自涵盖九种拉取请求（PR）问题域的Python项目的数据，提供了丰富的多角度上下文环境。除此之外，我们还提出了一种新的评估框架，该框架结合了基于规则的位置和语法检查与基于模型的审查质量判断。我们对最先进的大型语言模型进行了大规模的CR任务评估。结果显示各模型具有独特的优势和劣势，这表明了全面的多维度评估的必要性，并为开发更智能实用的CR助手提供了可操作见解。因此研究展示了强大的工具与未来发展潜力相结合的具体表现，提供了强大的支撑作用。这为设计更全面可靠的代码审查自动化系统指明了方向。然而，现有模型的性能表现仍参差不齐，未来还有很大的提升空间。总的来说，这项研究对于推动代码审查自动化技术的发展具有重要意义。同时强调了评估标准作为引导方向的重要作用和展示跨多个LLM和各类现实情境的准确和详尽研究的必要性。为今后研究提供了宝贵的参考依据。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是基于文本的关键见解：</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14856">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14856v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Empathy-R1-A-Chain-of-Empathy-and-Reinforcement-Learning-Framework-for-Long-Form-Mental-Health-Support"><a href="#Empathy-R1-A-Chain-of-Empathy-and-Reinforcement-Learning-Framework-for-Long-Form-Mental-Health-Support" class="headerlink" title="Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for   Long-Form Mental Health Support"></a>Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for   Long-Form Mental Health Support</h2><p><strong>Authors:Xianrong Yao, Dong She, Chenxu Zhang, Yimeng Zhang, Yueru Sun, Noman Ahmed, Yang Gao, Zhanpeng Jin</strong></p>
<p>Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seeker’s emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoE’s reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support. </p>
<blockquote>
<p>共情对于有效的心理健康支持至关重要，特别是在处理长咨询文本（LCTs）时。然而，现有的大型语言模型（LLM）通常生成的回复在语义上很流畅，但缺乏真正的心理支持所需的结构化推理，特别是在中文环境中。为了弥补这一差距，我们引入了Empathy-R1，这是一个结合共情链（CoE）推理过程与强化学习（RL）的新型框架，以提高对LCTs的回复质量。我们的CoE范式受认知行为疗法的启发，引导模型对求助者的情绪、原因和意图进行顺序推理，使其思考过程既透明又易于解释。我们的框架通过新的大规模中文数据集Empathy-QA和两阶段训练过程来实现。首先，监督微调（Supervised Fine-Tuning）灌输CoE的推理结构。随后，由专用奖励模型引导的强化学习（RL）进一步完善最终回复的治疗相关性和上下文恰当性。实验表明，Empathy-R1在关键自动指标上取得了强大的性能。更重要的是，人类评估证实了其优越性，显示出对强大基准测试有明显的偏好，并在我们的新基准测试中实现了44.30%的Win@1率。通过生成可解释且上下文丰富的回应，Empathy-R1在开发用于心理健康支持的负责任和真正有益的AI方面取得了重大进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14851v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对心理咨询文本（LCTs）的AI支持框架Empathy-R1。该框架结合了情感链（CoE）推理过程与强化学习（RL），旨在提高AI对心理咨询文本回应的质量。Empathy-R1受到认知行为疗法的启发，通过模拟治疗师的思考过程，使模型能够理解和回应求助者的情感、原因和意图。该框架使用大规模中文数据集Empathy-QA进行训练，并通过监督微调与强化学习两个阶段优化回应质量。实验结果显示，Empathy-R1在自动评估指标上表现优异，并且在人类评估中也超越了基线方法，取得了明显的优势。这一技术对于开发负责且真正有益于心理健康支持的AI系统具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Empathy在精神健康支持中至关重要，特别是在处理长咨询文本（LCTs）时。</li>
<li>当前的大型语言模型（LLMs）在生成回应时虽然语义流畅，但缺乏结构化的推理能力，无法提供真正的心理支持。</li>
<li>Empathy-R1是一个新的框架，结合了情感链（CoE）推理和强化学习（RL），旨在提高针对LCTs的回应质量。</li>
<li>Empathy-R1受到认知行为疗法的启发，模拟治疗师的思考过程，包括理解求助者的情感、原因和意图。</li>
<li>Empathy-R1使用大规模中文数据集Empathy-QA进行训练，并通过两个阶段——监督微调与强化学习来优化回应质量。</li>
<li>实验结果显示Empathy-R1在自动评估指标上表现优秀，并且在人类评估中也超越了基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14851">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14851v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14851v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14851v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLM-Agents-at-the-Roundtable-A-Multi-Perspective-and-Dialectical-Reasoning-Framework-for-Essay-Scoring"><a href="#LLM-Agents-at-the-Roundtable-A-Multi-Perspective-and-Dialectical-Reasoning-Framework-for-Essay-Scoring" class="headerlink" title="LLM Agents at the Roundtable: A Multi-Perspective and Dialectical   Reasoning Framework for Essay Scoring"></a>LLM Agents at the Roundtable: A Multi-Perspective and Dialectical   Reasoning Framework for Essay Scoring</h2><p><strong>Authors:Jinhee Jang, Ayoung Moon, Minkyoung Jung, YoungBin Kim. Seung Jin Lee</strong></p>
<p>The emergence of large language models (LLMs) has brought a new paradigm to automated essay scoring (AES), a long-standing and practical application of natural language processing in education. However, achieving human-level multi-perspective understanding and judgment remains a challenge. In this work, we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework designed to perform precise and human-aligned scoring under a zero-shot setting. RES constructs evaluator agents based on LLMs, each tailored to a specific prompt and topic context. Each agent independently generates a trait-based rubric and conducts a multi-perspective evaluation. Then, by simulating a roundtable-style discussion, RES consolidates individual evaluations through a dialectical reasoning process to produce a final holistic score that more closely aligns with human evaluation. By enabling collaboration and consensus among agents with diverse evaluation perspectives, RES outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in average QWK over straightforward prompting (Vanilla) methods. </p>
<blockquote>
<p>大型语言模型（LLM）的出现为自动作文评分（AES）带来了新的范式，AES是自然语言处理在教育领域的一项长期且实用的应用。然而，实现人类的多角度理解和判断仍是挑战。在这项工作中，我们提出了圆桌作文评分（RES），这是一个多代理评估框架，旨在在无样本设置下执行精确且符合人类评分标准的评分。RES基于LLM构建评估代理，每个代理都针对特定的提示和主题上下文进行定制。每个代理独立生成基于特征的评分表并进行多角度评估。然后，通过模拟圆桌式讨论，RES通过辩证推理过程整合个人评估，产生最终的整体评分，更贴近人类评估。通过促进拥有不同评估视角的代理之间的协作和共识，RES优于之前的零样本AES方法。在ASAP数据集上使用ChatGPT和Claude进行的实验表明，RES相对于简单的提示（Vanilla）方法，平均QWK提高了高达34.86%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14834v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的出现为自动作文评分（AES）带来了新的模式，但在零样本设置下实现人类级别多角度理解和判断仍存在挑战。本研究提出Roundtable Essay Scoring（RES）多智能体评估框架，旨在进行精确且符合人类评估的评分。RES基于LLM构建评估器智能体，每个智能体针对特定提示和主题上下文进行定制，独立生成基于特征的评分表并进行多角度评估。通过模拟圆桌讨论，RES通过辩证推理过程整合个人评估结果，产生更接近人类评价的总体评分。RES通过促进拥有不同评估角度的智能体之间的协作和共识，表现出优于先前零样本AES方法的效果。在ASAP数据集上进行的ChatGPT和Claude实验表明，RES相较于简单的提示方法平均QWK提高了高达34.86%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）为自动作文评分（AES）带来新范式。</li>
<li>实现人类级别多角度理解和判断在AES中是挑战。</li>
<li>Roundtable Essay Scoring（RES）框架模拟人类评估过程，旨在精确且符合人类评估进行评分。</li>
<li>RES基于LLM构建评估器智能体，每个智能体针对特定情境定制。</li>
<li>智能体独立生成评分表并进行多角度评估。</li>
<li>RES通过模拟圆桌讨论整合个人评估结果，更接近人类评价。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14834">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14834v1/page_3_2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Re-ranking-for-Image-Retrieval-Tasks"><a href="#Chain-of-Thought-Re-ranking-for-Image-Retrieval-Tasks" class="headerlink" title="Chain-of-Thought Re-ranking for Image Retrieval Tasks"></a>Chain-of-Thought Re-ranking for Image Retrieval Tasks</h2><p><strong>Authors:Shangrong Wu, Yanghong Zhou, Yang Chen, Feng Zhang, P. Y. Mok</strong></p>
<p>Image retrieval remains a fundamental yet challenging problem in computer vision. While recent advances in Multimodal Large Language Models (MLLMs) have demonstrated strong reasoning capabilities, existing methods typically employ them only for evaluation, without involving them directly in the ranking process. As a result, their rich multimodal reasoning abilities remain underutilized, leading to suboptimal performance. In this paper, we propose a novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue. Specifically, we design a listwise ranking prompt that enables MLLM to directly participate in re-ranking candidate images. This ranking process is grounded in an image evaluation prompt, which assesses how well each candidate aligns with users query. By allowing MLLM to perform listwise reasoning, our method supports global comparison, consistent reasoning, and interpretable decision-making - all of which are essential for accurate image retrieval. To enable structured and fine-grained analysis, we further introduce a query deconstruction prompt, which breaks down the original query into multiple semantic components. Extensive experiments on five datasets demonstrate the effectiveness of our CoTRR method, which achieves state-of-the-art performance across three image retrieval tasks, including text-to-image retrieval (TIR), composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our code is available at <a target="_blank" rel="noopener" href="https://github.com/freshfish15/CoTRR">https://github.com/freshfish15/CoTRR</a> . </p>
<blockquote>
<p>图像检索仍然是计算机视觉中的一个基本且具有挑战性的问题。尽管最近的多模态大型语言模型（MLLM）在推理能力方面取得了显著的进展，但现有方法通常仅将它们用于评估，而没有直接参与到排序过程中。因此，它们的丰富多模态推理能力未得到充分利用，导致性能不佳。针对这一问题，我们在本文中提出了一种新颖的思考链重新排序（CoTRR）方法。具体来说，我们设计了一种列表排序提示，使MLLM能够直接参与重新排序候选图像。此排序过程基于图像评估提示，评估每个候选图像与用户查询的匹配程度。通过允许MLLM进行列表推理，我们的方法支持全局比较、一致推理和可解释决策制定，这些都是准确图像检索所必需的。为了进行结构和精细的分析，我们还引入了查询解构提示，将原始查询拆分为多个语义组件。在五个数据集上的大量实验表明，我们的CoTRR方法实现了三项图像检索任务的最佳性能，包括文本到图像检索（TIR）、组合图像检索（CIR）和基于聊天的图像检索（Chat-IR）。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/freshfish15/CoTRR">https://github.com/freshfish15/CoTRR</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14746v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Chain-of-Thought Re-Ranking（CoTRR）的新方法，以解决图像检索中的核心挑战。该方法设计了一种列表排名提示，使多模态大型语言模型（MLLM）直接参与图像候选的重新排名过程。通过图像评估提示，评估每个候选图像与用户查询的匹配程度。该方法支持全局比较、一致推理和可解释决策制定，对于准确图像检索至关重要。引入查询解构提示，实现更精细的分析。在五个数据集上的实验表明，CoTRR方法在三种图像检索任务上达到最新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像检索仍然是一个基础且具挑战性的问题。</li>
<li>多模态大型语言模型（MLLM）具有强大的推理能力，但在图像检索中的使用有限。</li>
<li>提出的Chain-of-Thought Re-Ranking（CoTRR）方法使MLLM直接参与图像候选的重新排名。</li>
<li>CoTRR使用图像评估提示，评估每个候选图像与用户查询的匹配程度。</li>
<li>方法支持全局比较、一致推理和可解释决策制定，对准确图像检索至关重要。</li>
<li>引入查询解构提示，实现更精细的分析和结构化比较。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14746v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14746v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14746v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14746v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RationAnomaly-Log-Anomaly-Detection-with-Rationality-via-Chain-of-Thought-and-Reinforcement-Learning"><a href="#RationAnomaly-Log-Anomaly-Detection-with-Rationality-via-Chain-of-Thought-and-Reinforcement-Learning" class="headerlink" title="RationAnomaly: Log Anomaly Detection with Rationality via   Chain-of-Thought and Reinforcement Learning"></a>RationAnomaly: Log Anomaly Detection with Rationality via   Chain-of-Thought and Reinforcement Learning</h2><p><strong>Authors:Song Xu, Yilun Liu, Minggui He, Mingchen Dai, Ziang Chen, Chunguang Zhao, Jingzhou Du, Shimin Tao, Weibin Meng, Shenglin Zhang, Yongqian Sun, Boxing Chen, Daimeng Wei</strong></p>
<p>Logs constitute a form of evidence signaling the operational status of software systems. Automated log anomaly detection is crucial for ensuring the reliability of modern software systems. However, existing approaches face significant limitations: traditional deep learning models lack interpretability and generalization, while methods leveraging Large Language Models are often hindered by unreliability and factual inaccuracies. To address these issues, we propose RationAnomaly, a novel framework that enhances log anomaly detection by synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our approach first instills expert-like reasoning patterns using CoT-guided supervised fine-tuning, grounded in a high-quality dataset corrected through a rigorous expert-driven process. Subsequently, a reinforcement learning phase with a multi-faceted reward function optimizes for accuracy and logical consistency, effectively mitigating hallucinations. Experimentally, RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks while providing transparent, step-by-step analytical outputs. We have released the corresponding resources, including code and datasets. </p>
<blockquote>
<p>日志是显示软件系统运行状态的一种证据形式。自动日志异常检测对于确保现代软件系统的可靠性至关重要。然而，现有方法存在重大局限性：传统深度学习模型缺乏可解释性和泛化能力，而利用大型语言模型的方法通常受到可靠性和事实准确性的限制。为了解决这些问题，我们提出了RationAnomaly，这是一个通过协同思考链（CoT）微调与强化学习来增强日志异常检测的新框架。我们的方法首先使用CoT引导的监督微调来灌输专家级的推理模式，这基于一个通过严格的专家驱动过程进行校正的高质量数据集。随后，一个具有多面奖励函数的强化学习阶段优化了准确性和逻辑一致性，有效地减轻了幻觉。实验表明，RationAnomaly优于最先进的基线技术，在关键基准测试上实现了较高的F1分数，同时提供透明、逐步的分析输出。我们已经发布了相应的资源，包括代码和数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14693v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong><br>     日志是反映软件系统运行状态的重要证据，自动日志异常检测对确保现代软件系统的可靠性至关重要。然而，现有方法存在局限性，传统深度学习模型缺乏可解释性和通用性，而利用大型语言模型的方法则经常受到不可靠和事实不准确的影响。为解决这些问题，我们提出了RationAnomaly框架，它通过协同链式思维（CoT）精细调整与强化学习，增强了日志异常检测。该方法首先通过CoT引导的监督精细调整，以高质量数据集为基础，通过严格的专家驱动过程进行校正，植入专家式的推理模式。随后，采用具有多方面奖励功能的强化学习阶段进行优化，既提高准确性又保持逻辑连贯性，有效抑制虚构。实验表明，RationAnomaly在关键基准测试上实现了较高的F1分数，优于最新基线，同时提供透明、逐步的分析输出。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>日志是软件系统运行状态的证据，自动日志异常检测对确保软件可靠性至关重要。</li>
<li>现有日志异常检测方法存在局限性，如深度学习模型缺乏可解释性和通用性，大型语言模型方法存在不可靠和事实不准确的问题。</li>
<li>RationAnomaly框架通过协同链式思维（CoT）精细调整与强化学习来增强日志异常检测。</li>
<li>RationAnomaly首先通过CoT引导的监督精细调整植入专家式的推理模式，使用高质量数据集并经过专家驱动过程的校正。</li>
<li>强化学习阶段采用多方面奖励函数优化，提高检测准确性和逻辑连贯性，减少虚构。</li>
<li>RationAnomaly在关键基准测试上实现较高F1分数，优于最新基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14693v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14693v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14693v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LEED-A-Highly-Efficient-and-Scalable-LLM-Empowered-Expert-Demonstrations-Framework-for-Multi-Agent-Reinforcement-Learning"><a href="#LEED-A-Highly-Efficient-and-Scalable-LLM-Empowered-Expert-Demonstrations-Framework-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="LEED: A Highly Efficient and Scalable LLM-Empowered Expert   Demonstrations Framework for Multi-Agent Reinforcement Learning"></a>LEED: A Highly Efficient and Scalable LLM-Empowered Expert   Demonstrations Framework for Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Tianyang Duan, Zongyuan Zhang, Songxiao Guo, Dong Huang, Yuanye Zhao, Zheng Lin, Zihan Fang, Dianxin Luan, Heming Cui, Yong Cui</strong></p>
<p>Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines. </p>
<blockquote>
<p>多智能体强化学习（MARL）在复杂环境中的智能决策方面具有巨大潜力。然而，随着智能体数量的增加，它面临着协调和可扩展性的瓶颈。为了解决这些问题，我们提出了基于大型语言模型赋能的专家演示框架的多智能体强化学习（LEED）。LEED包括两个组件：演示生成（DG）模块和政策优化（PO）模块。具体来说，DG模块利用大型语言模型生成与环境交互的指令，从而生成高质量的演示。PO模块采用分散式训练范式，每个智能体利用生成的演示来构建专家政策损失，然后将其与自己的政策损失相结合。这使得每个智能体能够有效地根据个人知识和个人经验本地化并优化其本地政策。实验结果表明，与最新技术相比，LEED在样本效率、时间效率和稳健的可扩展性方面取得了优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14680v1">PDF</a> 5 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>多智能体强化学习（MARL）在复杂环境中展现出智能决策的巨大潜力，但随着智能体数量的增加，协调性和可扩展性成为其瓶颈。为应对这些问题，我们提出了基于大型语言模型的专家演示框架（LEED），包括演示生成（DG）模块和政策优化（PO）模块。DG模块利用大型语言模型生成与交互环境相关的指令，从而制作高质量的演示。PO模块采用分散式训练范式，每个智能体利用生成的演示构建专家政策损失，并将其与自身政策损失结合。这使得每个智能体能够基于专家知识和个人经验有效地个性化并优化其本地策略。实验结果表明，与现有先进技术相比，LEED在样本效率、时间效率和稳健可扩展性方面表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体强化学习（MARL）在处理复杂环境决策时面临协调性和扩展性问题。</li>
<li>LEED框架通过引入大型语言模型来解决这些问题，生成高质量的演示。</li>
<li>LEED包含两个模块：演示生成（DG）和政策优化（PO）。</li>
<li>DG模块利用大型语言模型生成环境交互指令。</li>
<li>PO模块采用分散式训练，智能体结合专家政策和自身政策损失进行优化。</li>
<li>LEED在样本效率、时间效率和可扩展性方面表现优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14680">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14680v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AgentCompass-Towards-Reliable-Evaluation-of-Agentic-Workflows-in-Production"><a href="#AgentCompass-Towards-Reliable-Evaluation-of-Agentic-Workflows-in-Production" class="headerlink" title="AgentCompass: Towards Reliable Evaluation of Agentic Workflows in   Production"></a>AgentCompass: Towards Reliable Evaluation of Agentic Workflows in   Production</h2><p><strong>Authors:NVJK Kartik, Garvit Sapra, Rishav Hada, Nikhil Pareek</strong></p>
<p>With the growing adoption of Large Language Models (LLMs) in automating complex, multi-agent workflows, organizations face mounting risks from errors, emergent behaviors, and systemic failures that current evaluation methods fail to capture. We present AgentCompass, the first evaluation framework designed specifically for post-deployment monitoring and debugging of agentic workflows. AgentCompass models the reasoning process of expert debuggers through a structured, multi-stage analytical pipeline: error identification and categorization, thematic clustering, quantitative scoring, and strategic summarization. The framework is further enhanced with a dual memory system-episodic and semantic-that enables continual learning across executions. Through collaborations with design partners, we demonstrate the framework’s practical utility on real-world deployments, before establishing its efficacy against the publicly available TRAIL benchmark. AgentCompass achieves state-of-the-art results on key metrics, while uncovering critical issues missed in human annotations, underscoring its role as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production. </p>
<blockquote>
<p>随着大型语言模型（LLMs）在自动化复杂多代理工作流程中的日益普及，组织面临着日益增长的错误、突发行为和系统失败风险，而现有的评估方法却无法捕捉到这些风险。我们推出了AgentCompass，这是专门为代理工作流程的部署后监控和调试设计的第一个评估框架。AgentCompass通过结构化、多阶段的分析管道来模拟专家调试器的推理过程：错误识别和分类、主题聚类、定量评分和战略总结。该框架还采用双内存系统（即情节记忆和语义记忆），使各执行过程之间的持续学习成为可能。通过与设计合作伙伴的合作，我们在实际部署中展示了该框架的实际效用，然后再在公开可用的TRAIL基准测试上验证其有效性。AgentCompass在关键指标上达到了最新结果，同时发现了人类注释中遗漏的关键问题，这进一步凸显了其在生产环境中对代理系统可靠监控和改进的稳健、面向开发者的工具的重要作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14647v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在自动化复杂、多代理工作流程中的普及，使得组织面临越来越多的风险，如错误、新兴行为和系统失效等，而现有的评估方法无法捕捉到这些风险。为此，我们推出了AgentCompass，这是专门为代理工作流程的后部署监控和调试设计的评估框架。AgentCompass通过结构化、多阶段的分析管道来模拟专家调试器的推理过程，包括错误识别和分类、主题聚类、定量评分和战略总结。该框架还采用双记忆系统——情景记忆和语义记忆，以实现跨执行的持续学习。通过与设计合作伙伴的合作，我们在实际部署中展示了该框架的实际效用，并在公开的TRAIL基准测试上验证了其有效性。AgentCompass在关键指标上达到了最新水平的结果，同时发现了人类注释中遗漏的关键问题，凸显了其在生产环境中可靠监控和改进代理系统的稳健性和面向开发者的作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在自动化工作流程中的普及带来了错误、新兴行为和系统失效等风险。</li>
<li>现有评估方法无法充分捕捉这些风险。</li>
<li>AgentCompass是一个专门用于代理工作流程的后部署监控和调试的评估框架。</li>
<li>AgentCompass模拟专家调试器的推理过程，包括错误识别、主题聚类、定量评分和战略总结。</li>
<li>该框架采用双记忆系统实现跨执行的持续学习。</li>
<li>AgentCompass在实际部署中表现出实际效用，并在公开基准测试上验证了其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14647">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14647v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14647v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14647v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Position-Thematic-Analysis-of-Unstructured-Clinical-Transcripts-with-Large-Language-Models"><a href="#Position-Thematic-Analysis-of-Unstructured-Clinical-Transcripts-with-Large-Language-Models" class="headerlink" title="Position: Thematic Analysis of Unstructured Clinical Transcripts with   Large Language Models"></a>Position: Thematic Analysis of Unstructured Clinical Transcripts with   Large Language Models</h2><p><strong>Authors:Seungjun Yi, Joakim Nguyen, Terence Lim, Andrew Well, Joseph Skrovan, Mehak Beri, YongGeon Lee, Kavita Radhakrishnan, Liu Leqi, Mia Markey, Ying Ding</strong></p>
<p>This position paper examines how large language models (LLMs) can support thematic analysis of unstructured clinical transcripts, a widely used but resource-intensive method for uncovering patterns in patient and provider narratives. We conducted a systematic review of recent studies applying LLMs to thematic analysis, complemented by an interview with a practicing clinician. Our findings reveal that current approaches remain fragmented across multiple dimensions including types of thematic analysis, datasets, prompting strategies and models used, most notably in evaluation. Existing evaluation methods vary widely (from qualitative expert review to automatic similarity metrics), hindering progress and preventing meaningful benchmarking across studies. We argue that establishing standardized evaluation practices is critical for advancing the field. To this end, we propose an evaluation framework centered on three dimensions: validity, reliability, and interpretability. </p>
<blockquote>
<p>这篇立场论文探讨了大型语言模型（LLM）如何支持对非正式临床记录的主题分析。主题分析是一种广泛使用但资源密集的方法，用于发现患者和提供者叙述中的模式。我们对近期应用LLM进行主题分析的研究进行了系统回顾，并辅以与实践医生的访谈。我们的研究发现，当前的方法在多个维度上仍然呈现碎片化，包括主题分析的类型、数据集、提示策略和使用的模型，尤其是在评估方面。现有的评估方法差异很大（从定性专家评审到自动相似性度量），阻碍了进展，并阻碍了研究之间的有意义基准测试。我们认为建立标准化的评估实践对于推动该领域的发展至关重要。为此，我们提出了一个以三个维度为中心的评估框架：有效性、可靠性和可解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14597v1">PDF</a> Submitted to GenAI4Health@NeurIPS 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文探讨了大型语言模型（LLMs）如何支持对临床转录进行主题分析，这是一种广泛使用但资源密集的方法，用于挖掘患者和提供者叙述中的模式。通过系统回顾近期应用LLMs进行主题分析的研究，并结合对执业医生的访谈，我们发现当前的方法在多个维度上仍然分散，最显著的是在评估方面。现有的评估方法差异很大（从定性专家评审到自动相似性度量），阻碍了进展，并阻碍了跨研究的基准测试。文章强调建立标准化的评估实践对于推动该领域的重要性，并提出一个以有效性、可靠性和可解释性为中心的评估框架。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLMs）可用于支持对临床转录的主题分析，该方法在发现患者和提供者叙述中的模式方面非常有效但资源消耗大。</li>
<li>当前的方法在多个维度（如主题分析类型、数据集、提示策略和使用的模型）上仍然分散，特别是在评估方面。</li>
<li>现有的评估方法差异很大，从定性专家评审到自动相似性度量不等，这阻碍了研究的进展和跨研究的基准测试。</li>
<li>建立标准化的评估实践对于推动该领域的发展至关重要。</li>
<li>文章提出一个评估框架，重点考虑三个维度：有效性、可靠性和可解释性。</li>
<li>通过系统回顾近期研究并结合执业医生的访谈，文章提供了深入的见解和建议。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14597v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14597v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14597v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SynBench-A-Benchmark-for-Differentially-Private-Text-Generation"><a href="#SynBench-A-Benchmark-for-Differentially-Private-Text-Generation" class="headerlink" title="SynBench: A Benchmark for Differentially Private Text Generation"></a>SynBench: A Benchmark for Differentially Private Text Generation</h2><p><strong>Authors:Yidan Sun, Viktor Schlegel, Srinivasan Nandakumar, Iqra Zahid, Yuping Wu, Yulong Wu, Hao Li, Jie Zhang, Warren Del-Pinto, Goran Nenadic, Siew Kei Lam, Anil Anthony Bharath</strong></p>
<p>Data-driven decision support in high-stakes domains like healthcare and finance faces significant barriers to data sharing due to regulatory, institutional, and privacy concerns. While recent generative AI models, such as large language models, have shown impressive performance in open-domain tasks, their adoption in sensitive environments remains limited by unpredictable behaviors and insufficient privacy-preserving datasets for benchmarking. Existing anonymization methods are often inadequate, especially for unstructured text, as redaction and masking can still allow re-identification. Differential Privacy (DP) offers a principled alternative, enabling the generation of synthetic data with formal privacy assurances. In this work, we address these challenges through three key contributions. First, we introduce a comprehensive evaluation framework with standardized utility and fidelity metrics, encompassing nine curated datasets that capture domain-specific complexities such as technical jargon, long-context dependencies, and specialized document structures. Second, we conduct a large-scale empirical study benchmarking state-of-the-art DP text generation methods and LLMs of varying sizes and different fine-tuning strategies, revealing that high-quality domain-specific synthetic data generation under DP constraints remains an unsolved challenge, with performance degrading as domain complexity increases. Third, we develop a membership inference attack (MIA) methodology tailored for synthetic text, providing first empirical evidence that the use of public datasets - potentially present in pre-training corpora - can invalidate claimed privacy guarantees. Our findings underscore the urgent need for rigorous privacy auditing and highlight persistent gaps between open-domain and specialist evaluations, informing responsible deployment of generative AI in privacy-sensitive, high-stakes settings. </p>
<blockquote>
<p>在医疗保健和财务等高风险领域中，数据驱动决策支持由于监管、制度和隐私担忧而面临数据共享的重大障碍。虽然最近的生成式AI模型，如大型语言模型，在开放域任务中表现出令人印象深刻的表现，但在敏感环境中的采用仍然受到不可预测的行为和不足的隐私保护数据集的限制，用于基准测试。现有的匿名化方法通常不足够，特别是对于非结构化文本，因为删除和遮蔽仍然可能导致重新识别。差分隐私（DP）提供了一种有原则的替代方案，能够生成具有正式隐私保证的合成数据。在这项工作中，我们通过三个主要贡献来解决这些挑战。首先，我们引入了一个综合评估框架，其中包含标准化的实用性和保真度指标，涵盖了九个精选数据集，这些数据集捕捉到了特定领域的复杂性，如技术术语、长期上下文依赖关系和特殊文档结构。其次，我们对最新的DP文本生成方法和不同规模和微调策略的大型语言模型进行了大规模实证研究，发现高质量领域特定的合成数据生成在DP约束下仍然是一个未解决的挑战，随着领域复杂性的增加，性能会下降。第三，我们开发了一种针对合成文本的会员推理攻击方法，提供了第一批经验证据表明使用公共数据集 - 可能存在于预训练语料库中 - 可能使所谓的隐私保证失效。我们的研究结果强调了严格隐私审计的迫切需求，并突出了开放域和专业评估之间持久的差距，为在隐私敏感、高风险环境中负责任地部署生成式AI提供了信息。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14594v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong><br>数据驱动决策支持在医疗和金融等高风险领域面临数据共享的重大障碍，主要由于监管、制度和隐私担忧。尽管大型语言模型等生成式AI模型在开放域任务中表现出色，但在敏感环境中的采用却受限于不可预测的行为和不足的隐私保护数据集。现有匿名化方法对于非结构化文本常常不足，差异隐私（DP）提供有原则的解决方案。本研究通过三项关键贡献解决这些挑战：引入综合评估框架、标准化实用性和保真度指标，涵盖九个专业数据集；对最先进的DP文本生成方法和大型语言模型进行大规模实证研究；开发针对合成文本的会员推理攻击方法。研究揭示了在差异隐私约束下生成高质量的专业特定合成数据的挑战，以及随着领域复杂性增加性能下降的问题。同时，使用预训练语料库中的公共数据集可能会使声称的隐私保证失效。我们的研究凸显了严格隐私审计的紧迫需求，并强调了公开领域和专业评估之间的持续差距，为在隐私敏感的高风险环境中负责任地部署生成式人工智能提供了信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据驱动决策支持在高风险领域面临数据共享障碍，主要源于监管、制度和隐私担忧。</li>
<li>生成式AI模型在开放域任务中表现出色，但在敏感环境中的采用受限。</li>
<li>现有匿名化方法对于非结构化文本常常不足，差异隐私（DP）为此提供解决方案。</li>
<li>本研究引入综合评估框架，涵盖九个专业数据集，以应对挑战。</li>
<li>大规模实证研究揭示了生成高质量专业特定合成数据的挑战，性能随领域复杂性增加下降。</li>
<li>使用预训练语料库中的公共数据集可能使声称的隐私保证失效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14594">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14594v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14594v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14594v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.14594v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DSPC-Dual-Stage-Progressive-Compression-Framework-for-Efficient-Long-Context-Reasoning"><a href="#DSPC-Dual-Stage-Progressive-Compression-Framework-for-Efficient-Long-Context-Reasoning" class="headerlink" title="DSPC: Dual-Stage Progressive Compression Framework for Efficient   Long-Context Reasoning"></a>DSPC: Dual-Stage Progressive Compression Framework for Efficient   Long-Context Reasoning</h2><p><strong>Authors:Yaxin Gao, Yao Lu, Zongfei Zhang, Jiaqi Nie, Shanqing Yu, Qi Xuan</strong></p>
<p>Large language models (LLMs) have achieved remarkable success in many natural language processing (NLP) tasks. To achieve more accurate output, the prompts used to drive LLMs have become increasingly longer, which incurs higher computational costs. To address this prompt inflation problem, prompt compression has been proposed. However, most existing methods require training a small auxiliary model for compression, incurring a significant amount of additional computation. To avoid this, we propose a two-stage, training-free approach, called Dual-Stage Progressive Compression (DSPC). In the coarse-grained stage, semantic-related sentence filtering removes sentences with low semantic value based on TF-IDF. In the fine-grained stage, token importance is assessed using attention contribution, cross-model loss difference, and positional importance, enabling the pruning of low-utility tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo under a constrained token budget and observe consistent improvements. For instance, in the FewShot task of the Longbench dataset, DSPC achieves a performance of 49.17 by using only 3x fewer tokens, outperforming the best state-of-the-art baseline LongLLMLingua by 7.76. </p>
<blockquote>
<p>大型语言模型（LLM）在许多自然语言处理（NLP）任务中取得了显著的成功。为了获得更准确的输出，用于驱动LLM的提示变得越来越长，这导致了更高的计算成本。为了解决提示膨胀问题，提出了提示压缩。然而，大多数现有方法需要训练一个用于压缩的辅助模型，这产生了大量的额外计算。为了避免这一点，我们提出了一种名为双阶段渐进压缩（DSPC）的两阶段无训练方法。在粗粒度阶段，通过语义相关句子过滤，基于TF-IDF去除低语义价值的句子。在细粒度阶段，通过注意力贡献、跨模型损失差异和位置重要性来评估令牌的重要性，能够在保留语义的同时删除低效用令牌。我们在有限的令牌预算下对LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo进行了DSPC验证，并观察到了一致性的改进。例如，在Longbench数据集的FewShot任务中，DSPC仅使用3倍较少的令牌就实现了49.17的性能，优于目前最佳基线LongLLMLingua 7.76。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13723v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型语言模型（LLMs）在NLP任务中的出色表现，但驱动LLMs的提示逐渐增长，导致更高的计算成本。为解决提示膨胀问题，本文提出了一种名为Dual-Stage Progressive Compression（DSPC）的两阶段无训练压缩方法。首先通过语义相关句子过滤去除低语义价值句子，然后评估标记的重要性，通过注意力贡献、跨模型损失差异和位置重要性来修剪低效用标记同时保留语义。在LLaMA和GPT模型上的实验表明，DSPC在有限的标记预算下实现了显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在NLP任务中表现出色，但提示膨胀导致高计算成本。</li>
<li>为解决提示膨胀问题，提出了Dual-Stage Progressive Compression（DSPC）方法。</li>
<li>DSPC是一种两阶段、无需训练的方法，包括粗粒度的语义相关句子过滤和细粒度的标记重要性评估。</li>
<li>通过注意力贡献、跨模型损失差异和位置重要性来评估标记的重要性。</li>
<li>DSPC能在有限的标记预算下实现性能提升。</li>
<li>DSPC在LLaMA和GPT模型上的实验表现优于现有最佳基线LongLLMLingua。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13723">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.13723v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.13723v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.13723v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_R1_Reasoning/2509.13723v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_LLM/2509.15217v1/page_5_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-09-20  LNE-Blocking An Efficient Framework for Contamination Mitigation   Evaluation on Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-19/Interactive/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-19\./crop_Interactive/2509.13737v1/page_0_0.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-09-19  Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force   Control and Gait Control
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
