<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  LNE-Blocking An Efficient Framework for Contamination Mitigation   Evaluation on Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-4ad8a618ac62fe7fddf9aaaa5fc2dfba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028595&auth_key=1760028595-0-0-aa68238dd34d503503533c8e629c9689&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    69 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-20-æ›´æ–°"><a href="#2025-09-20-æ›´æ–°" class="headerlink" title="2025-09-20 æ›´æ–°"></a>2025-09-20 æ›´æ–°</h1><h2 id="LNE-Blocking-An-Efficient-Framework-for-Contamination-Mitigation-Evaluation-on-Large-Language-Models"><a href="#LNE-Blocking-An-Efficient-Framework-for-Contamination-Mitigation-Evaluation-on-Large-Language-Models" class="headerlink" title="LNE-Blocking: An Efficient Framework for Contamination Mitigation   Evaluation on Large Language Models"></a>LNE-Blocking: An Efficient Framework for Contamination Mitigation   Evaluation on Large Language Models</h2><p><strong>Authors:Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu</strong></p>
<p>The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the modelâ€™s greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at <a target="_blank" rel="noopener" href="https://github.com/RuijieH/LNE-Blocking">https://github.com/RuijieH/LNE-Blocking</a> to facilitate research. </p>
<blockquote>
<p>æ•°æ®æ±¡æŸ“é—®é¢˜åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•è¿‡ç¨‹ä¸­å‡ ä¹ä¸å¯é¿å…ï¼Œè®­ç»ƒæ•°æ®é€šå¸¸ä¼šæ— æ„ä¸­èå…¥è¯„ä¼°åŸºå‡†ï¼Œä»è€Œä½¿å¾—å…¬å¹³è¯„ä¼°LLMå˜å¾—å›°éš¾ã€‚æˆ‘ä»¬å¹¶ä¸ä¸»å¼ æ„å»ºæ— æ±¡æŸ“çš„æ•°æ®é›†ï¼ˆè¿™ç›¸å½“å›°éš¾ï¼‰ï¼Œè€Œæ˜¯æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåä¸ºâ€œLNE-Blockingâ€ï¼Œä»¥åœ¨å¯èƒ½æ³„éœ²çš„æ•°æ®é›†ä¸Šæ¢å¤æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šæ±¡æŸ“æ£€æµ‹ä¸å¹²æ‰°æ“ä½œã€‚å¯¹äºæç¤ºï¼Œæ¡†æ¶é¦–å…ˆä½¿ç”¨æ±¡æŸ“æ£€æµ‹æ–¹æ³•â€œLNEâ€æ¥è¯„ä¼°æ¨¡å‹ä¸­çš„æ±¡æŸ“ç¨‹åº¦ã€‚åŸºäºæ­¤ï¼Œå®ƒè°ƒæ•´å¹²æ‰°æ“ä½œâ€œBlockingâ€çš„å¼ºåº¦ï¼Œä»¥æ¿€å‘æ¨¡å‹ç»™å‡ºéè®°å¿†åŒ–çš„å›åº”ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ˜¯é¦–ä¸ªèƒ½å¤Ÿé«˜æ•ˆæ¢å¤æ¨¡å‹è´ªå©ªè§£ç æ€§èƒ½çš„æŠ€æœ¯ã€‚å®ƒåœ¨å¤šä¸ªæ½œåœ¨æ³„éœ²é£é™©çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ¨¡å‹å’Œä¸åŒç¨‹åº¦çš„æ•°æ®æ±¡æŸ“æƒ…å†µä¸‹ï¼Œéƒ½èƒ½å®ç°ç¨³å®šçš„æ¢å¤ç»“æœã€‚ä¸ºäº†æ–¹ä¾¿ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RuijieH/LNE-Blocking%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/RuijieH/LNE-Blockingä¸Šå‘å¸ƒäº†ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15218v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®æ±¡æŸ“é—®é¢˜å‡ ä¹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•è¿‡ç¨‹ä¸­ä¸å¯é¿å…ï¼Œè®­ç»ƒæ•°æ®é€šå¸¸æ— æ„ä¸­æ•´åˆäº†è¯„ä¼°åŸºå‡†ï¼Œè¿™ä½¿å¾—å…¬å¹³è¯„ä¼°LLMå˜å¾—å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLNE-Blockingçš„æ–°é¢–æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å¯èƒ½æ³„éœ²çš„æ•°æ®é›†ä¸Šæ¢å¤æ¨¡å‹æ€§èƒ½æ¥åº”å¯¹è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼šæ±¡æŸ“æ£€æµ‹ä¸å¹²æ‰°æ“ä½œã€‚å¯¹äºæç¤ºï¼Œè¯¥æ¡†æ¶é¦–å…ˆä½¿ç”¨æ±¡æŸ“æ£€æµ‹æ–¹æ³•LNEæ¥è¯„ä¼°æ¨¡å‹çš„æ±¡æŸ“ç¨‹åº¦ã€‚åŸºäºæ­¤ï¼Œå®ƒè°ƒæ•´å¹²æ‰°æ“ä½œBlockingçš„å¼ºåº¦ï¼Œä»¥æ¿€å‘æ¨¡å‹çš„éè®°å¿†å“åº”ã€‚è¯¥æ¡†æ¶æ˜¯é¦–ä¸ªèƒ½å¤Ÿé«˜æ•ˆæ¢å¤æ¨¡å‹è´ªå©ªè§£ç æ€§èƒ½çš„æ–¹æ³•ï¼Œåœ¨å¤šä¸ªæ½œåœ¨æ³„éœ²é£é™©çš„æ•°æ®é›†ä¸Šå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ¨¡å‹å’Œä¸åŒçº§åˆ«çš„æ•°æ®æ±¡æŸ“æƒ…å†µä¸‹éƒ½èƒ½å®ç°ç¨³å®šçš„æ¢å¤ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®æ±¡æŸ“åœ¨LLMçš„å‘å±•ä¸­å‡ ä¹ä¸å¯é¿å…ï¼Œè®­ç»ƒæ•°æ®ç»å¸¸æ— æ„ä¸­åŒ…å«è¯„ä¼°åŸºå‡†ã€‚</li>
<li>LNE-Blockingæ¡†æ¶æ—¨åœ¨è§£å†³æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œé€šè¿‡æ¢å¤æ¨¡å‹æ€§èƒ½æ¥åº”å¯¹æ½œåœ¨çš„æ•°æ®æ³„éœ²ã€‚</li>
<li>LNE-Blockingæ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šæ±¡æŸ“æ£€æµ‹å’Œå¹²æ‰°æ“ä½œã€‚</li>
<li>æ±¡æŸ“æ£€æµ‹æ–¹æ³•LNEç”¨äºè¯„ä¼°æ¨¡å‹çš„æ±¡æŸ“ç¨‹åº¦ã€‚</li>
<li>åŸºäºæ±¡æŸ“ç¨‹åº¦çš„è¯„ä¼°ï¼Œè°ƒæ•´å¹²æ‰°æ“ä½œçš„å¼ºåº¦ï¼Œæ¿€å‘æ¨¡å‹éè®°å¿†å“åº”ã€‚</li>
<li>LNE-Blockingæ¡†æ¶èƒ½å¤Ÿé«˜æ•ˆæ¢å¤æ¨¡å‹çš„è´ªå©ªè§£ç æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-96be50bcac62f69f44bd82a1c21aa055~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028602&auth_key=1760028602-0-0-72d790a36e822e33a5873ee2341c524c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-31e22974fae5fd45956a6d65aca87a65~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028610&auth_key=1760028610-0-0-c0b0be9194a4bb580a7c0d26b513c54c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Generalizable-Geometric-Image-Caption-Synthesis"><a href="#Generalizable-Geometric-Image-Caption-Synthesis" class="headerlink" title="Generalizable Geometric Image Caption Synthesis"></a>Generalizable Geometric Image Caption Synthesis</h2><p><strong>Authors:Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Renjie Pi, Shizhe Diao, Tong Zhang</strong></p>
<p>Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8%\text{-}4.8%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4%\text{-}3.9%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å„ç§éœ€è¦å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å®é™…åº”ç”¨ã€‚å°½ç®¡æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨è§£å†³å¤æ‚çš„å‡ ä½•é—®é¢˜æ—¶ä»ç„¶æ„Ÿåˆ°å›°éš¾ã€‚ä¸»è¦æŒ‘æˆ˜æºäºç¼ºä¹ç”¨äºç†è§£å‡ ä½•å›¾åƒçš„é«˜è´¨é‡å›¾åƒæ–‡æœ¬å¯¹æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°åŸºäºæ¨¡æ¿çš„æ•°æ®åˆæˆç®¡é“é€šå¸¸æ— æ³•æ¨å¹¿åˆ°å…¶é¢„å®šä¹‰æ¨¡æ¿ä¹‹å¤–çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„äº’è¡¥è¿‡ç¨‹æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå°†å…¶çº³å…¥æ•°æ®ç”Ÿæˆç®¡é“ã€‚é€šè¿‡é‡‡ç”¨RLVRæ¥å®Œå–„ç”±50ç§åŸºæœ¬å‡ ä½•å…³ç³»åˆæˆçš„å‡ ä½•å›¾åƒçš„æ ‡é¢˜ï¼Œå¹¶ä½¿ç”¨æ¥è‡ªæ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡çš„å¥–åŠ±ä¿¡å·ï¼Œæˆ‘ä»¬çš„ç®¡é“æˆåŠŸæ•æ‰äº†è§£å†³å‡ ä½•é—®é¢˜çš„å…³é”®ç‰¹å¾ã€‚è¿™èƒ½å¤Ÿå®ç°æ›´å¥½çš„ä»»åŠ¡æ³›åŒ–ï¼Œå¹¶äº§ç”Ÿäº†ä¸å°çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨è¶…å‡ºåˆ†å¸ƒçš„åœºæ™¯ä¸‹ï¼Œç”Ÿæˆçš„æ•°æ®é›†ä¹Ÿå¢å¼ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œåœ¨MathVistaå’ŒMathVerseçš„éå‡ ä½•è¾“å…¥å›¾åƒä¸­ï¼Œç»Ÿè®¡ã€ç®—æœ¯ã€ä»£æ•°å’Œæ•°å€¼ä»»åŠ¡çš„å‡†ç¡®ç‡æé«˜äº†2.8%~4.8%ï¼Œåœ¨MMMUçš„è‰ºæœ¯ã€è®¾è®¡ã€æŠ€æœ¯å’Œå·¥ç¨‹ä»»åŠ¡ä¸­æé«˜äº†2.4%~3.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15217v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éœ€è¦å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å®é™…åº”ç”¨ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å¤æ‚å‡ ä½•é—®é¢˜æ—¶ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé€šè¿‡ç²¾ç‚¼åˆæˆå‡ ä½•å›¾åƒçš„æ ‡é¢˜ï¼Œå¹¶ä½¿ç”¨æ¥è‡ªæ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡çš„å¥–åŠ±ä¿¡å·ï¼ŒæˆåŠŸæ•æ‰å‡ ä½•é—®é¢˜è§£å†³çš„å…³é”®ç‰¹å¾ã€‚è¿™æé«˜äº†ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºéå‡ ä½•è¾“å…¥å›¾åƒçš„æ•°å­¦è§†è§‰å’Œæ•°å­¦ä¸–ç•Œä»»åŠ¡å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚å‡ ä½•é—®é¢˜æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹é«˜è´¨é‡å›¾åƒæ–‡æœ¬å¯¹æ•°æ®é›†æ˜¯ç†è§£å‡ ä½•å›¾åƒçš„ä¸€ä¸ªå…³é”®éš¾é¢˜ã€‚</li>
<li>å¤§å¤šæ•°åŸºäºæ¨¡æ¿çš„æ•°æ®åˆæˆç®¡é“æ— æ³•æ¨å¹¿åˆ°è¶…å‡ºå…¶é¢„å®šä¹‰æ¨¡æ¿çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åˆ°æ•°æ®ç”Ÿæˆç®¡é“ä¸­ï¼ŒæˆåŠŸè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç²¾ç‚¼å‡ ä½•å›¾åƒçš„æ ‡é¢˜å¹¶ä½¿ç”¨æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡çš„å¥–åŠ±ä¿¡å·ï¼Œæ•æ‰åˆ°å‡ ä½•é—®é¢˜è§£å†³çš„å…³é”®ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œå¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e7e2b0aaf14b2e5a35223700015e1fb8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028618&auth_key=1760028618-0-0-6e11a5e4757dff74b33a396bd3620026&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cc45795351e9fef7516a37c65c1931bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028626&auth_key=1760028626-0-0-b84d0e2a92a17a195c64f9370fe3ad53&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bd02a78f1f73b9e9cb04e18ac5efda7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028634&auth_key=1760028634-0-0-7a26f3978af1b22d16572c03deb80f23&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4ad8a618ac62fe7fddf9aaaa5fc2dfba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028641&auth_key=1760028641-0-0-f7b7d68e774272a5dcdb2cecfc3d83e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Assessing-Historical-Structural-Oppression-Worldwide-via-Rule-Guided-Prompting-of-Large-Language-Models"><a href="#Assessing-Historical-Structural-Oppression-Worldwide-via-Rule-Guided-Prompting-of-Large-Language-Models" class="headerlink" title="Assessing Historical Structural Oppression Worldwide via Rule-Guided   Prompting of Large Language Models"></a>Assessing Historical Structural Oppression Worldwide via Rule-Guided   Prompting of Large Language Models</h2><p><strong>Authors:Sreejato Chatterjee, Linh Tran, Quoc Duy Nguyen, Roni Kirson, Drue Hamlin, Harvest Aquino, Hanjia Lyu, Jiebo Luo, Timothy Dye</strong></p>
<p>Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (<a target="_blank" rel="noopener" href="https://github.com/chattergpt/llm-oppression-benchmark">https://github.com/chattergpt/llm-oppression-benchmark</a>). </p>
<blockquote>
<p>ä¼ ç»Ÿçš„å†å²ç»“æ„å‹è¿«æµ‹é‡åŠªåŠ›ç”±äºæ¯ä¸ªå›½å®¶çš„ç‹¬ç‰¹ã€å±€éƒ¨ç‰¹å®šçš„æ’æ–¥ã€æ®–æ°‘å’Œç¤¾ä¼šåœ°ä½å†å²ï¼Œåœ¨è·¨å›½æœ‰æ•ˆæ€§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”é€šå¸¸ä¾èµ–äºä»¥ç‰©è´¨èµ„æºä¸ºä¼˜åŠ¿çš„ç»“æ„åŒ–æŒ‡æ•°ï¼ŒåŒæ—¶å¿½è§†äº†åŸºäºç”Ÿæ´»çš„èº«ä»½æ’æ–¥ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å‹è¿«æµ‹é‡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§åœ°ç†æ”¿æ²»èƒŒæ™¯ä¸‹ç”Ÿæˆä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„ç”Ÿæ´»å†å²ä¸åˆ©å¢ƒé‡å¾—åˆ†ã€‚æˆ‘ä»¬ä»å…¨çƒæ–°å† è‚ºç‚ç–«æƒ…çš„å¤šè¯­ç§ç ”ç©¶ä¸­æå–è‡ªæˆ‘è®¤å®šçš„ç§æ—ä¸»ä¹‰è¨€è®ºï¼Œè®¾è®¡å—è§„åˆ™å¼•å¯¼çš„å›è¯ç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆå¯è§£é‡Šä¸”åŸºäºç†è®ºåŸºç¡€çš„å‹è¿«ä¼°è®¡å€¼ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†è¿™äº›ç­–ç•¥åœ¨å¤šä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“å—åˆ°æ˜ç¡®è§„åˆ™æŒ‡å¯¼æ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°å›½å®¶å†…éƒ¨çš„åŸºäºèº«ä»½çš„å¾®å¦™å†å²å‹è¿«å½¢å¼ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ä¸ªè¡¥å……æ€§çš„æµ‹é‡å·¥å…·ï¼Œçªå‡ºä½“ç°äº†ç³»ç»Ÿæ’æ–¥çš„å„ä¸ªæ–¹é¢ï¼Œæä¾›äº†ä¸€ä¸ªè·¨æ–‡åŒ–è§†è§’ï¼Œç”¨äºäº†è§£å‹è¿«å¦‚ä½•åœ¨æ•°æ®é©±åŠ¨çš„ç ”ç©¶å’Œå…¬å…±å«ç”Ÿç¯å¢ƒä¸­å‡ºç°ã€‚ä¸ºäº†æ”¯æŒå¯é‡å¤è¯„ä¼°ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªå¼€æºåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‹è¿«æµ‹é‡æ–¹é¢çš„è¡¨ç°ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/chattergpt/llm-oppression-benchmark%EF%BC%89%E3%80%82">https://github.com/chattergpt/llm-oppression-benchmarkï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15216v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‹è¿«æ„Ÿæµ‹é‡æ–°æ¡†æ¶ï¼Œåˆ©ç”¨å¤šå…ƒæ–°å† ç–«æƒ…å…¨çƒç ”ç©¶ä¸­çš„è‡ªæˆ‘è®¤å®šçš„æ°‘æ—è¨€è®ºï¼Œè®¾è®¡è§„åˆ™å¼•å¯¼æç¤ºç­–ç•¥ï¼Œäº§ç”Ÿå¯è§£é‡Šã€ç†è®ºæ”¯æ’‘çš„å†å²å‹è¿«æ„Ÿè¯„ä¼°ã€‚è¯¥æ¡†æ¶è€ƒè™‘äº†åœ°åŸŸæ€§å†å²å› ç´ ï¼Œå¦‚æ’æ–¥ã€æ®–æ°‘å’Œç¤¾ä¼šåœ°ä½ç­‰ï¼Œå¹¶å¼ºè°ƒèº«ä»½è®¤åŒçš„é‡è¦æ€§ã€‚é€šè¿‡è·¨å¤šä¸ªå‰æ²¿LLMçš„ç³»ç»Ÿè¯„ä¼°ï¼Œè¯æ˜è¯¥æ¡†æ¶å¯æ•æ‰åˆ°å¤æ‚çš„èº«ä»½å†å²æ€§å‹è¿«ç°è±¡ã€‚è¿™ä¸€å·¥å…·å¯¹äºå…¬å…±å¥åº·ç ”ç©¶ã€ç†è§£å‹è¿«çš„æ•°æ®é©±åŠ¨èƒŒæ™¯æœ‰é‡è¦ä½œç”¨ï¼ŒåŒæ—¶ä¸ºç ”ç©¶è€…æä¾›ä¸€ä¸ªå¼€æºè¯„ä¼°åŸºå‡†æ•°æ®é›†ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/chattergpt/llm-oppression-benchmark%EF%BC%89%E3%80%82">https://github.com/chattergpt/llm-oppression-benchmarkï¼‰ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå†å²ç»“æ„å‹è¿«æµ‹é‡æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥è·¨å›½å®¶æœ‰æ•ˆåº”ç”¨ã€‚</li>
<li>æ–°æ¡†æ¶åˆ©ç”¨LLMæµ‹é‡å‹è¿«æ„Ÿï¼Œè€ƒè™‘åœ°åŸŸæ€§å†å²å› ç´ ï¼ŒåŒ…æ‹¬æ’æ–¥ã€æ®–æ°‘å’Œç¤¾ä¼šåœ°ä½ç­‰ã€‚</li>
<li>æ¡†æ¶ç»“åˆè‡ªæˆ‘è®¤å®šçš„æ°‘æ—è¨€è®ºï¼Œè®¾è®¡è§„åˆ™å¼•å¯¼æç¤ºç­–ç•¥ï¼Œäº§ç”Ÿå¯è§£é‡Šçš„å†å²å‹è¿«æ„Ÿè¯„ä¼°ã€‚</li>
<li>è¯„ä¼°ç­–ç•¥è·¨å¤šä¸ªLLMè¿›è¡ŒéªŒè¯ï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºæ•æ‰å¤æ‚çš„èº«ä»½å†å²æ€§å‹è¿«ç°è±¡ï¼Œä¸ºå…¬å…±å¥åº·ç ”ç©¶å’Œæ•°æ®é©±åŠ¨çš„èƒŒæ™¯ç†è§£æä¾›é‡è¦å·¥å…·ã€‚</li>
<li>å¼€æºè¯„ä¼°åŸºå‡†æ•°æ®é›†æ”¯æŒå¯å¤åˆ¶æ€§è¯„ä»·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-858c89b9a59728a0f696a62eb538ed5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028648&auth_key=1760028648-0-0-99c961fe4cdec64d8d21e591e35b3c40&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b1b4ced7a27b84960584a173f562861~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028656&auth_key=1760028656-0-0-24c393204ac8dd0be1a0055be5cded38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9d36bb0a5f3a45de7982e4c5c5e8aaa2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028662&auth_key=1760028662-0-0-1ddd2972f01dd474dffadc939054d366&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evil-Vizier-Vulnerabilities-of-LLM-Integrated-XR-Systems"><a href="#Evil-Vizier-Vulnerabilities-of-LLM-Integrated-XR-Systems" class="headerlink" title="Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems"></a>Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems</h2><p><strong>Authors:Yicheng Zhang, Zijian Huang, Sophie Chen, Erfan Shayegani, Jiasi Chen, Nael Abu-Ghazaleh</strong></p>
<p>Extended reality (XR) applications increasingly integrate Large Language Models (LLMs) to enhance user experience, scene understanding, and even generate executable XR content, and are often called â€œAI glassesâ€. Despite these potential benefits, the integrated XR-LLM pipeline makes XR applications vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR systems in the literature and in practice and categorize them along different dimensions from a systems perspective. Building on this categorization, we identify a common threat model and demonstrate a series of proof-of-concept attacks on multiple XR platforms that employ various LLM models (Meta Quest 3, Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models). Although these platforms each implement LLM integration differently, they share vulnerabilities where an attacker can modify the public context surrounding a legitimate LLM query, resulting in erroneous visual or auditory feedback to users, thus compromising their safety or privacy, sowing confusion, or other harmful effects. To defend against these threats, we discuss mitigation strategies and best practices for developers, including an initial defense prototype, and call on the community to develop new protection mechanisms to mitigate these risks. </p>
<blockquote>
<p>æ‰©å±•ç°å®ï¼ˆXRï¼‰åº”ç”¨ç¨‹åºè¶Šæ¥è¶Šå¤šåœ°é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒã€åœºæ™¯ç†è§£ï¼Œç”šè‡³ç”Ÿæˆå¯æ‰§è¡ŒXRå†…å®¹ï¼Œé€šå¸¸è¢«ç§°ä¸ºâ€œäººå·¥æ™ºèƒ½çœ¼é•œâ€ã€‚å°½ç®¡æœ‰è¿™äº›æ½œåœ¨ä¼˜åŠ¿ï¼Œä½†é›†æˆçš„XR-LLMç®¡é“ä½¿XRåº”ç”¨ç¨‹åºå®¹æ˜“å—åˆ°æ–°å½¢å¼æ”»å‡»çš„å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»æ–‡çŒ®å’Œå®è·µä¸­åˆ†æäº†é›†æˆLLMçš„XRç³»ç»Ÿï¼Œå¹¶ä»ç³»ç»Ÿè§’åº¦æ²¿ä¸åŒç»´åº¦å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç±»ã€‚åŸºäºè¿™ç§åˆ†ç±»ï¼Œæˆ‘ä»¬ç¡®å®šäº†å¸¸è§çš„å¨èƒæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªXRå¹³å°ä¸Šå¯¹é‡‡ç”¨å„ç§LLMæ¨¡å‹ï¼ˆåœ¨Meta Quest 3ã€Meta Ray-Banã€Androidå’ŒMicrosoft HoloLens 2ä¸Šè¿è¡ŒLlamaå’ŒGPTæ¨¡å‹ï¼‰è¿›è¡Œäº†æ¦‚å¿µéªŒè¯æ”»å‡»ã€‚å°½ç®¡è¿™äº›å¹³å°å„è‡ªå®ç°LLMé›†æˆçš„æ–¹å¼ä¸åŒï¼Œä½†å®ƒä»¬éƒ½å­˜åœ¨å…±åŒçš„æ¼æ´ï¼Œå³æ”»å‡»è€…å¯ä»¥ä¿®æ”¹åˆæ³•LLMæŸ¥è¯¢å‘¨å›´çš„å…¬å¼€ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´å‘ç”¨æˆ·æä¾›é”™è¯¯çš„è§†è§‰æˆ–å¬è§‰åé¦ˆï¼Œä»è€Œå±åŠä»–ä»¬çš„å®‰å…¨æˆ–éšç§ï¼Œé€ æˆæ··æ·†æˆ–å…¶ä»–æœ‰å®³å½±å“ã€‚ä¸ºäº†åº”å¯¹è¿™äº›å¨èƒï¼Œæˆ‘ä»¬è®¨è®ºäº†å¼€å‘è€…çš„ç¼“è§£ç­–ç•¥å’Œæœ€ä½³å®è·µï¼ŒåŒ…æ‹¬åˆæ­¥é˜²å¾¡åŸå‹ï¼Œå¹¶å‘¼åç¤¾åŒºå¼€å‘æ–°çš„ä¿æŠ¤æœºåˆ¶æ¥å‡è½»è¿™äº›é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15213v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>XRåº”ç”¨é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å¢å¼ºç”¨æˆ·ä½“éªŒã€åœºæ™¯ç†è§£ï¼Œç”šè‡³ç”Ÿæˆå¯æ‰§è¡ŒXRå†…å®¹ï¼Œè¢«ç§°ä¸ºâ€œAIçœ¼é•œâ€ã€‚ç„¶è€Œï¼Œè¿™ç§é›†æˆä½¿å¾—XRåº”ç”¨é¢ä¸´æ–°å‹æ”»å‡»é£é™©ã€‚æœ¬æ–‡å¯¹æ–‡çŒ®å’Œå®é™…ä¸­çš„LLMé›†æˆXRç³»ç»Ÿè¿›è¡Œäº†åˆ†æï¼Œå¹¶ä»ç³»ç»Ÿè§’åº¦å¯¹å…¶è¿›è¡Œäº†åˆ†ç±»ã€‚åœ¨æ­¤åˆ†ç±»çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç¡®å®šäº†é€šç”¨çš„å¨èƒæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªXRå¹³å°ä¸Šå¯¹é‡‡ç”¨ä¸åŒLLMæ¨¡å‹çš„æ”»å‡»è¿›è¡Œäº†æ¦‚å¿µéªŒè¯ã€‚è¿™äº›å¹³å°è™½ç„¶LLMé›†æˆæ–¹å¼å„å¼‚ï¼Œä½†éƒ½å­˜åœ¨ä¸€ç§æ”»å‡»æ–¹å¼ï¼šæ”»å‡»è€…å¯ä»¥ä¿®æ”¹åˆæ³•LLMæŸ¥è¯¢å‘¨å›´çš„å…¬å¼€ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´ç”¨æˆ·æ”¶åˆ°é”™è¯¯çš„è§†è§‰æˆ–å¬è§‰åé¦ˆï¼Œä»è€Œå±åŠä»–ä»¬çš„å®‰å…¨æˆ–éšç§ï¼Œå¼•å‘æ··ä¹±æˆ–å…¶ä»–æœ‰å®³å½±å“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¼€å‘è€…åº”å¯¹çš„ç¼“è§£ç­–ç•¥å’Œæœ€ä½³å®è·µï¼Œå¹¶å‘¼åç¤¾åŒºå¼€å‘æ–°çš„ä¿æŠ¤æœºåˆ¶æ¥å‡è½»è¿™äº›é£é™©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>XRåº”ç”¨é€šè¿‡é›†æˆLLMæ¥æå‡ç”¨æˆ·ä½“éªŒå’Œå†…å®¹ç”Ÿæˆï¼Œä½†è¿™ä¹Ÿå¢åŠ äº†ç³»ç»Ÿçš„æ–°å®‰å…¨å¨èƒã€‚</li>
<li>LLMé›†æˆçš„XRç³»ç»Ÿé¢ä¸´å…¬å…±ä¸Šä¸‹æ–‡è¢«ä¿®æ”¹çš„é£é™©ï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯åé¦ˆã€‚</li>
<li>ä¸åŒXRå¹³å°ï¼ˆå¦‚Meta Quest 3ã€Meta Ray-Banã€Androidå’ŒMicrosoft HoloLens 2ï¼‰é‡‡ç”¨ä¸åŒLLMæ¨¡å‹é›†æˆæ–¹å¼ï¼Œä½†éƒ½å­˜åœ¨ç±»ä¼¼çš„å®‰å…¨éšæ‚£ã€‚</li>
<li>æ”»å‡»è€…èƒ½å¤Ÿé€šè¿‡ä¿®æ”¹å…¬å¼€ä¸Šä¸‹æ–‡å½±å“ç”¨æˆ·æ¥æ”¶çš„è§†è§‰æˆ–å¬è§‰ä¿¡æ¯ï¼Œé€ æˆå®‰å…¨ã€éšç§å’Œæ··æ·†ç­‰é£é™©ã€‚</li>
<li>ä¸ºåº”å¯¹è¿™äº›å¨èƒï¼Œå¼€å‘è€…éœ€è¦é‡‡å–ç¼“è§£ç­–ç•¥å’Œæœ€ä½³å®è·µã€‚</li>
<li>éœ€è¦ç¤¾åŒºå…±åŒåŠªåŠ›å¼€å‘æ–°çš„ä¿æŠ¤æœºåˆ¶æ¥å‡è½»è¿™äº›é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-abbdcc48738f0ddb4a242ca79d05fb68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028670&auth_key=1760028670-0-0-4f367963d016026087d54cdfb2be5746&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-95bfd026d211e6618bdc04e5a6544427~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028677&auth_key=1760028677-0-0-2ebb5399bda0a6e0f213de99872034bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d35677a23c35e7767a53a1d8221def2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028684&auth_key=1760028684-0-0-be583abe1be4943ed2e621bb0fa65c00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3899c99dc99fc5a0550ce6ff1c2adf39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028691&auth_key=1760028691-0-0-e69ed37207bccfb5f66369163ef73bdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-24dae1447bdbf32f18a42482f01d4896~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028698&auth_key=1760028698-0-0-bf60baf54818ec3486192cb3818902e2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6e81a58af9f17e8a220a2dda9141fb63~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028705&auth_key=1760028705-0-0-9d81693ed55ec1e7368b438d9712f9a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation"><a href="#Evolving-Language-Models-without-Labels-Majority-Drives-Selection-Novelty-Promotes-Variation" class="headerlink" title="Evolving Language Models without Labels: Majority Drives Selection,   Novelty Promotes Variation"></a>Evolving Language Models without Labels: Majority Drives Selection,   Novelty Promotes Variation</h2><p><strong>Authors:Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu</strong></p>
<p>Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the modelâ€™s inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRLâ€™s 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­ï¼Œéœ€è¦æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰æ ‡ç­¾æˆ–å¤–éƒ¨è¯„åˆ¤çš„æƒ…å†µä¸‹è‡ªæˆ‘æ”¹è¿›ã€‚ç°æœ‰çš„æ— æ ‡ç­¾æ–¹æ³•ï¼Œå¦‚æœ€å°åŒ–ç½®ä¿¡åº¦ã€è‡ªæˆ‘ä¸€è‡´æ€§æˆ–å¤šæ•°æŠ•ç¥¨ç›®æ ‡ï¼Œè™½ç„¶å¯ä»¥ç¨³å®šå­¦ä¹ ï¼Œä½†ä¼šä¸æ–­ç¼©å°æ¢ç´¢èŒƒå›´ï¼Œå¯¼è‡´ç†µå´©æºƒï¼šç”Ÿæˆçš„å†…å®¹å˜å¾—æ›´çŸ­ã€æ›´å°‘æ ·åŒ–å’Œè„†å¼±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸­è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­éœ€è¦æ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘æ”¹è¿›ï¼Œæ— éœ€æ ‡ç­¾æˆ–å¤–éƒ¨è¯„åˆ¤ã€‚ç°æœ‰çš„æ— æ ‡ç­¾æ–¹æ³•ï¼Œå¦‚ä¿¡å¿ƒæœ€å°åŒ–ã€è‡ªæˆ‘ä¸€è‡´æ€§æˆ–å¤šæ•°æŠ•ç¥¨ç›®æ ‡ï¼Œè™½ç„¶å¯ä»¥ç¨³å®šå­¦ä¹ ï¼Œä½†ä¼šç¼©å‡æ¢ç´¢ï¼Œå¯¼è‡´ç†µå´©æºƒï¼Œç”Ÿæˆçš„æ–‡æœ¬å˜çŸ­ã€ç¼ºä¹å¤šæ ·æ€§ä¸”è„†å¼±ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºEVOL-RLçš„è¿›åŒ–å¯¼å‘æ— æ ‡ç­¾å¼ºåŒ–å­¦ä¹ è§„åˆ™ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚EVOL-RLå°†å¤šæ•°æŠ•ç¥¨ç­”æ¡ˆä½œä¸ºç¨³å®šé”šç‚¹ï¼ˆé€‰æ‹©ï¼‰ï¼ŒåŒæ—¶å¢åŠ ä¸€ä¸ªé‡è§†å“åº”æ¨ç†çš„æ–°é¢–æ€§å¥–åŠ±ï¼ˆå˜å¼‚ï¼‰ï¼Œåœ¨è¯­ä¹‰ç©ºé—´ä¸­è¡¡é‡ã€‚è¯¥æ–¹æ³•é˜²æ­¢äº†å´©æºƒï¼Œä¿æŒäº†æ›´é•¿çš„ã€æ›´æœ‰ä¿¡æ¯é‡çš„æ€è€ƒé“¾ï¼Œå¹¶æé«˜äº†pass@1å’Œpass@nã€‚EVOL-RLåœ¨æ— éœ€æ ‡ç­¾çš„AIME24ä¸Šçš„è¡¨ç°å§‹ç»ˆä¼˜äºä»…ä½¿ç”¨å¤šæ•°æŠ•ç¥¨çš„TTRLåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç°æœ‰æ— æ ‡ç­¾æ–¹æ³•è™½ç¨³å®šå­¦ä¹ ä½†ä¼šç¼©å‡æ¢ç´¢ï¼Œå¯¼è‡´ç†µå´©æºƒã€‚</li>
<li>EVOL-RLæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç»“åˆç¨³å®šä¸å˜å¼‚åœ¨æ— æ ‡ç­¾è®¾ç½®ä¸‹ã€‚</li>
<li>EVOL-RLé‡‡ç”¨å¤šæ•°æŠ•ç¥¨ç­”æ¡ˆä½œä¸ºç¨³å®šé”šç‚¹ï¼ŒåŒæ—¶å¢åŠ é‡è§†å“åº”æ¨ç†çš„æ–°é¢–æ€§å¥–åŠ±ã€‚</li>
<li>EVOL-RLåœ¨æ— éœ€æ ‡ç­¾çš„AIME24ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†pass@1å’Œpass@nã€‚</li>
<li>EVOL-RLä¸ä»…é˜²æ­¢å¤šæ ·æ€§å´©æºƒï¼Œè¿˜æé«˜äº†è·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c15cc7b5a98bd027cf16c0bd4f66d097~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028713&auth_key=1760028713-0-0-3804f3420c5ddbc9c26dc27d333932d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-763a11858fa6cc9bb7b7e3d471e35967~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028720&auth_key=1760028720-0-0-cc8b9278929667de50d9bb371714bed2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c9af23c4384d183d9b789db7408e57c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028727&auth_key=1760028727-0-0-ab935d89f089f941863ee0294692f011&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding"><a href="#Unleashing-the-Potential-of-Multimodal-LLMs-for-Zero-Shot-Spatio-Temporal-Video-Grounding" class="headerlink" title="Unleashing the Potential of Multimodal LLMs for Zero-Shot   Spatio-Temporal Video Grounding"></a>Unleashing the Potential of Multimodal LLMs for Zero-Shot   Spatio-Temporal Video Grounding</h2><p><strong>Authors:Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lau</strong></p>
<p>Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the modelâ€™s attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at <a target="_blank" rel="noopener" href="https://github.com/zaiquanyang/LLaVA_Next_STVG">https://github.com/zaiquanyang/LLaVA_Next_STVG</a>. </p>
<blockquote>
<p>æ—¶ç©ºè§†é¢‘å®šä½ï¼ˆSTVGï¼‰æ—¨åœ¨æ ¹æ®è¾“å…¥çš„æ–‡æœ¬æŸ¥è¯¢å®šä½è§†é¢‘ä¸­çš„æ—¶ç©ºç®¡é“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥æ¢ç´¢STVGä¸­çš„é›¶æ ·æœ¬è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æ­ç¤ºäº†å…³äºMLLMsçš„ä¸¤ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰MLLMså€¾å‘äºåŠ¨æ€åˆ†é…ç‰¹æ®Šä»¤ç‰Œï¼Œç§°ä¸ºâ€œå®šä½ä»¤ç‰Œâ€ï¼Œç”¨äºå®šä½æ–‡æœ¬æŸ¥è¯¢ï¼›ï¼ˆ2ï¼‰ç”±äºæ— æ³•å®Œå…¨æ•´åˆæ–‡æœ¬æŸ¥è¯¢ä¸­çš„çº¿ç´¢ï¼ˆä¾‹å¦‚å±æ€§ã€åŠ¨ä½œï¼‰è¿›è¡Œæ¨ç†ï¼ŒMLLMsç»å¸¸é­å—æ¬¡ä¼˜å®šä½ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºMLLMçš„STVGé›¶æ ·æœ¬æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬æ–°é¢–çš„åˆ†è§£æ—¶ç©ºçªå‡ºæ˜¾ç¤ºï¼ˆDSTHï¼‰å’Œæ—¶é—´å¢å¼ºç»„è£…ï¼ˆTASï¼‰ç­–ç•¥ï¼Œä»¥é‡Šæ”¾MLLMçš„æ¨ç†èƒ½åŠ›ã€‚DSTHç­–ç•¥é¦–å…ˆå°†åŸå§‹æŸ¥è¯¢åˆ†è§£ä¸ºå±æ€§å’ŒåŠ¨ä½œå­æŸ¥è¯¢ï¼Œä»¥è¯¢é—®ç›®æ ‡åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šçš„å­˜åœ¨ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨æ–°å‹çš„é€»è¾‘å¼•å¯¼å†æ³¨æ„ï¼ˆLRAï¼‰æ¨¡å—æ¥å­¦ä¹ ä½œä¸ºç©ºé—´å’Œæ—¶é—´æç¤ºçš„æ½œåœ¨å˜é‡ï¼Œé€šè¿‡è§„èŒƒåŒ–æ¯ä¸ªå­æŸ¥è¯¢çš„ä»¤ç‰Œé¢„æµ‹ã€‚è¿™äº›æç¤ºåˆ†åˆ«çªå‡ºå±æ€§å’ŒåŠ¨ä½œçº¿ç´¢ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨å¯é çš„ç©ºé—´å’Œæ—¶é—´ç›¸å…³çš„è§†è§‰åŒºåŸŸã€‚æ­¤å¤–ï¼Œç”±äºå±æ€§å­æŸ¥è¯¢çš„ç©ºé—´å®šä½åœ¨æ—¶é—´ä¸Šåº”è¯¥æ˜¯ä¸€è‡´çš„ï¼Œæˆ‘ä»¬å¼•å…¥äº†TASç­–ç•¥ï¼Œä½¿ç”¨åŸå§‹è§†é¢‘å¸§å’Œæ—¶é—´å¢å¼ºå¸§ä½œä¸ºè¾“å…¥æ¥ç»„åˆé¢„æµ‹ï¼Œä»¥å¸®åŠ©æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨å„ç§MLLMä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‰ä¸ªå¸¸è§çš„STVGåŸºå‡†æµ‹è¯•ä¸Šè¯æ˜äº†å…¶ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ç›¸å…³ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zaiquanyang/LLaVA_Next_STVG">https://github.com/zaiquanyang/LLaVA_Next_STVG</a>ä¸­æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15178v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¢ç´¢æ—¶ç©ºè§†é¢‘å®šä½ï¼ˆSTVGï¼‰çš„é›¶æ ·æœ¬è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å‘ç°MLLMsåœ¨å®šä½æ—¶å€¾å‘äºä½¿ç”¨ç‰¹å®šçš„æ ‡è®°ï¼Œä½†å—é™äºæ— æ³•å®Œå…¨æ•´åˆæ–‡æœ¬æŸ¥è¯¢ä¸­çš„çº¿ç´¢ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åŸºäºMLLMçš„é›¶æ ·æœ¬STVGæ¡†æ¶ï¼ŒåŒ…æ‹¬åˆ†è§£æ—¶ç©ºçªå‡ºæ˜¾ç¤ºï¼ˆDSTHï¼‰å’Œæ—¶é—´å¢å¼ºè£…é…ï¼ˆTASï¼‰ç­–ç•¥ï¼Œä»¥é‡Šæ”¾MLLMçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡DSTHç­–ç•¥å°†æŸ¥è¯¢åˆ†è§£ä¸ºå±æ€§åŠ¨ä½œå­æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨LRæ¨¡å—å­¦ä¹ æ½œåœ¨å˜é‡ä½œä¸ºæ—¶ç©ºæç¤ºã€‚æ­¤å¤–ï¼Œé‡‡ç”¨TASç­–ç•¥ç»“åˆåŸå§‹è§†é¢‘å¸§å’Œæ—¶é—´å¢å¼ºå¸§è¿›è¡Œé¢„æµ‹ï¼Œä»¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚åœ¨å¤šä¸ªMLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå¸¸è§çš„STVGåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¢ç´¢æ—¶ç©ºè§†é¢‘å®šä½ï¼ˆSTVGï¼‰çš„é›¶æ ·æœ¬è§£å†³æ–¹æ¡ˆã€‚</li>
<li>MLLMsåœ¨å®šä½æ—¶å€¾å‘äºä½¿ç”¨ç‰¹å®šçš„æ ‡è®°ï¼Œå³â€œgrounding tokensâ€ï¼Œä½†ä»å­˜åœ¨éš¾ä»¥æ•´åˆæ–‡æœ¬æŸ¥è¯¢ä¸­çš„æ‰€æœ‰çº¿ç´¢çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºMLLMçš„é›¶æ ·æœ¬STVGæ¡†æ¶ï¼ŒåŒ…æ‹¬åˆ†è§£æ—¶ç©ºçªå‡ºæ˜¾ç¤ºï¼ˆDSTHï¼‰å’Œæ—¶é—´å¢å¼ºè£…é…ï¼ˆTASï¼‰ç­–ç•¥ã€‚</li>
<li>DSTHç­–ç•¥é€šè¿‡å°†æŸ¥è¯¢åˆ†è§£ä¸ºå±æ€§åŠ¨ä½œå­æŸ¥è¯¢å¹¶åˆ©ç”¨LRæ¨¡å—å­¦ä¹ æ½œåœ¨å˜é‡ï¼Œä»¥çªå‡ºå±æ€§åŠ¨ä½œçº¿ç´¢å¹¶å¼•å¯¼æ¨¡å‹å…³æ³¨å¯é çš„æ—¶ç©ºç›¸å…³è§†è§‰åŒºåŸŸã€‚</li>
<li>TASç­–ç•¥ç»“åˆåŸå§‹è§†é¢‘å¸§å’Œæ—¶é—´å¢å¼ºå¸§è¿›è¡Œé¢„æµ‹ï¼Œæ—¨åœ¨æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªMLLMä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨ä¸‰ä¸ªå¸¸è§çš„STVGåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a9db2cc7411dd66b2f797dba0e57dd62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028734&auth_key=1760028734-0-0-644fe563de8a19ad2b99e0b8937a6bcb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-56132dbbc3601750b7dd7d646ff5ecb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028741&auth_key=1760028741-0-0-7724df08a98e0df2145f1ecb0f90d6f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b1220158f7cfd48939b78fb21122e7ff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028748&auth_key=1760028748-0-0-bf3f250c6fa476ef0cdd2cbce9a1fdb0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d35bfb2beefb7cae25d1b4ea4443f0e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028755&auth_key=1760028755-0-0-8b4fd274de4a5f610cd34e15ee5ef1dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Data-Rewriting-for-Stable-Off-Policy-Supervised-Fine-Tuning"><a href="#Mind-the-Gap-Data-Rewriting-for-Stable-Off-Policy-Supervised-Fine-Tuning" class="headerlink" title="Mind the Gap: Data Rewriting for Stable Off-Policy Supervised   Fine-Tuning"></a>Mind the Gap: Data Rewriting for Stable Off-Policy Supervised   Fine-Tuning</h2><p><strong>Authors:Shiwan Zhao, Xuyang Zhao, Jiaming Zhou, Aobo Kong, Qicheng Li, Yong Qin</strong></p>
<p>Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Off-Policy-SFT">https://github.com/NKU-HLT/Off-Policy-SFT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥è¢«è§†ä¸ºä¸€ç§ç¦»çº¿å­¦ä¹ ç­–ç•¥é—®é¢˜ï¼Œå…¶ä¸­ä¸“å®¶æ¼”ç¤ºæ¥è‡ªå›ºå®šçš„è¡Œä¸ºç­–ç•¥ï¼Œè€Œè®­ç»ƒçš„ç›®æ ‡æ˜¯ä¼˜åŒ–ç›®æ ‡ç­–ç•¥ã€‚é‡è¦æ€§é‡‡æ ·æ˜¯çº æ­£è¿™ç§åˆ†å¸ƒä¸åŒ¹é…çš„æ ‡å‡†å·¥å…·ï¼Œä½†ç­–ç•¥å·®è·è¾ƒå¤§å¯¼è‡´æ–¹å·®è¾ƒå¤§å’Œè®­ç»ƒä¸ç¨³å®šã€‚ç°æœ‰æ–¹æ³•é€šè¿‡ä½¿ç”¨KLæƒ©ç½šæˆ–è£å‰ªæ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œè¿™äº›æ–¹æ³•è¢«åŠ¨åœ°çº¦æŸæ›´æ–°ï¼Œè€Œä¸æ˜¯ä¸»åŠ¨åœ°ç¼©å°å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®é‡å†™æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¿æŒæ­£ç¡®çš„è§£å†³æ–¹æ¡ˆä½œä¸ºåœ¨çº¿ç­–ç•¥æ•°æ®ï¼Œå¹¶ç”¨å¼•å¯¼è§£å†³æ–¹å¼é‡å†™é”™è¯¯çš„è§£å†³æ–¹æ¡ˆï¼Œä»…åœ¨å¿…è¦æ—¶å›æº¯åˆ°ä¸“å®¶æ¼”ç¤ºã€‚è¿™åœ¨å¯¹ä¼˜åŒ–ä¹‹å‰å¯¹é½è®­ç»ƒåˆ†å¸ƒä¸ç›®æ ‡ç­–ç•¥ï¼Œå‡å°‘é‡è¦æ€§é‡‡æ ·æ–¹å·®å¹¶ç¨³å®šç¦»çº¿å¾®è°ƒã€‚åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æ™®é€šçš„SFTå’Œæœ€æ–°çš„åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å…·æœ‰ä¸€è‡´ä¸”æ˜¾è‘—çš„æ”¶ç›Šã€‚æ•°æ®å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/Off-Policy-SFT%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/NKU-HLT/Off-Policy-SFTå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15157v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªç¦»çº¿ç­–ç•¥å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­ä¸“å®¶æ¼”ç¤ºæ¥è‡ªå›ºå®šçš„è¡Œä¸ºç­–ç•¥ï¼Œè€Œè®­ç»ƒçš„ç›®æ ‡æ˜¯ä¼˜åŒ–ç›®æ ‡ç­–ç•¥ã€‚é‡è¦æ€§é‡‡æ ·æ˜¯è§£å†³è¿™ç§åˆ†å¸ƒä¸åŒ¹é…çš„æ ‡å‡†å·¥å…·ï¼Œä½†å½“ç­–ç•¥å·®è·è¾ƒå¤§æ—¶ï¼Œä¼šå¯¼è‡´é«˜æ–¹å·®å’Œè®­ç»ƒä¸ç¨³å®šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®é‡å†™æ¡†æ¶ï¼Œé€šè¿‡ä¿æŒæ­£ç¡®çš„è§£å†³æ–¹æ¡ˆä¸ºåœ¨çº¿ç­–ç•¥æ•°æ®ï¼Œå¹¶ç”¨æŒ‡å¯¼è§£å†³æ–¹å¼é‡å†™é”™è¯¯çš„è§£å†³æ–¹æ¡ˆï¼Œä»…åœ¨å¿…è¦æ—¶å›é€€åˆ°ä¸“å®¶æ¼”ç¤ºï¼Œè¿™å¯ä»¥åœ¨ä¼˜åŒ–ä¹‹å‰å¯¹é½è®­ç»ƒåˆ†å¸ƒä¸ç›®æ ‡ç­–ç•¥ï¼Œå‡å°‘é‡è¦æ€§é‡‡æ ·çš„æ–¹å·®å¹¶ç¨³å®šç¦»çº¿ç­–ç•¥çš„å¾®è°ƒã€‚åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æ™®é€šçš„SFTå’Œæœ€æ–°çš„åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å…·æœ‰ä¸€è‡´ä¸”æ˜¾è‘—çš„å¢ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒå¯ä»¥è¢«è§†ä¸ºç¦»çº¿ç­–ç•¥å­¦ä¹ é—®é¢˜ã€‚</li>
<li>é‡è¦æ€§é‡‡æ ·æ˜¯è§£å†³åˆ†å¸ƒä¸åŒ¹é…çš„æ ‡å‡†å·¥å…·ï¼Œä½†åœ¨ç­–ç•¥å·®è·è¾ƒå¤§æ—¶å­˜åœ¨é«˜æ–¹å·®å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨KLæƒ©ç½šæˆ–è£å‰ªæ¥è¢«åŠ¨çº¦æŸæ›´æ–°ï¼Œè€Œéä¸»åŠ¨ç¼©å°ç­–ç•¥å·®è·ã€‚</li>
<li>æå‡ºçš„ç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®é‡å†™æ¡†æ¶é€šè¿‡ä¿æŒæ­£ç¡®çš„è§£å†³æ–¹æ¡ˆå¹¶ä¸»åŠ¨ç¼©å°ç­–ç•¥å·®è·æ¥æ”¹å–„è®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºæ™®é€šSFTå’Œæœ€æ–°çš„DFTæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å’Œå®éªŒç»†èŠ‚å°†åœ¨NKU-HLTçš„Off-Policy SFTé¡¹ç›®ä¸­å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5dd54f5070fd0af2e9334d93deb69955~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028763&auth_key=1760028763-0-0-fc1fd6f08a46be9182095e32cb05ef71&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-90b7d3450ccbfa98906cdb6089afe035~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028770&auth_key=1760028770-0-0-e841b1fc5687c03151c91ec1518b8229&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66f756cf898f839c6b42a6997aaa925e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028776&auth_key=1760028776-0-0-4598827738192c351ac94e2978fbfb6f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d9d30d5df0f0b37b9bcd952fed87657~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028783&auth_key=1760028783-0-0-9049a8d058fcf568fb19574552d2e950&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf0c970f0395244379a92d57a43a0627~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028789&auth_key=1760028789-0-0-f9fe7cc4847af8b0f81397d8352e6c90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1052ce9c3ccf1b77093d22cfbd2521da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028797&auth_key=1760028797-0-0-db4851d166a480b9e501d427dc64ea56&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Self-Improving-Embodied-Foundation-Models"><a href="#Self-Improving-Embodied-Foundation-Models" class="headerlink" title="Self-Improving Embodied Foundation Models"></a>Self-Improving Embodied Foundation Models</h2><p><strong>Authors:Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, Igor Mordatch</strong></p>
<p>Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, we propose a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, our novel post-training recipe unveils significant results on Embodied Foundation Models. First, we demonstrate that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, we demonstrate that our proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. Our project website can be found at <a target="_blank" rel="noopener" href="https://self-improving-efms.github.io/">https://self-improving-efms.github.io</a> . </p>
<blockquote>
<p>åŸºäºç½‘ç»œè§„æ¨¡æ•°æ®è®­ç»ƒçš„åŸºçŸ³æ¨¡å‹å·²ç»ä¸ºæœºå™¨äººæŠ€æœ¯å¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ï¼Œä½†å…¶åœ¨ä½çº§æ§åˆ¶æ–¹é¢çš„åº”ç”¨ä»ç„¶ä¸»è¦å±€é™äºè¡Œä¸ºå…‹éš†ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­å¼ºåŒ–å­¦ä¹ é˜¶æ®µæˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬ä¸ºæœºå™¨äººæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„åè®­ç»ƒæ³•ã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½¿ç”¨aï¼‰è¡Œä¸ºå…‹éš†å’Œbï¼‰å¾…è¿›æ­¥é¢„æµ‹ç›®æ ‡å¯¹é¢„è®­ç»ƒçš„åŸºçŸ³æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå³è‡ªæˆ‘æå‡é˜¶æ®µï¼Œå¾…è¿›æ­¥é¢„æµ‹æœ‰åŠ©äºæå–å½¢çŠ¶è‰¯å¥½çš„å¥–åŠ±å‡½æ•°å’Œç¨³å¥çš„æˆåŠŸæ£€æµ‹å™¨ï¼Œä»è€Œå…è®¸ä¸€ç³»åˆ—æœºå™¨äººåœ¨å‡ ä¹æ— éœ€äººå·¥ç›‘ç£çš„æƒ…å†µä¸‹è‡ªä¸»ç»ƒä¹ ä¸‹æ¸¸ä»»åŠ¡ã€‚é€šè¿‡åœ¨å®é™…æœºå™¨äººå’Œæ¨¡æ‹Ÿæœºå™¨äººä¸Šçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬çš„æ–°å‹åè®­ç»ƒé…æ–¹åœ¨åµŒå…¥å¼åŸºçŸ³æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜äº†SFTä¸è‡ªæˆ‘æå‡çš„ç»“åˆåœ¨æ ·æœ¬æ•ˆç‡ä¸Šæ˜æ˜¾ä¼˜äºæ‰©å¤§æ¨¡ä»¿æ•°æ®çš„æ”¶é›†ç”¨äºç›‘ç£å­¦ä¹ ï¼Œå¹¶ä¸”å®ƒå¯¼è‡´çš„æ”¿ç­–æˆåŠŸç‡æ›´é«˜ã€‚è¿›ä¸€æ­¥çš„æ¶ˆèç ”ç©¶å¼ºè°ƒï¼Œç»“åˆç½‘ç»œè§„æ¨¡é¢„è®­ç»ƒå’Œè‡ªæˆ‘æå‡æ˜¯è¿™ç§æ ·æœ¬æ•ˆç‡çš„å…³é”®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ç»„åˆå…·æœ‰ç›®å‰æ–¹æ³•æ— æ³•å®ç°çš„ç‹¬ç‰¹èƒ½åŠ›ï¼šè‡ªä¸»ç»ƒä¹ å¹¶è·å–è¶…å‡ºåœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„æ¨¡ä»¿å­¦ä¹ æ•°æ®é›†ä¸­è§‚å¯Ÿåˆ°çš„è¡Œä¸ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†å°†é¢„è®­ç»ƒçš„åŸºçŸ³æ¨¡å‹ä¸åœ¨çº¿è‡ªæˆ‘æå‡ç›¸ç»“åˆï¼Œä»¥å®ç°æœºå™¨äººåœ¨æŠ€èƒ½è·å–æ–¹é¢çš„è‡ªä¸»æ€§çš„å˜é©æ½œåŠ›ã€‚æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://self-improving-efms.github.ioæ‰¾åˆ°./">https://self-improving-efms.github.ioæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15155v1">PDF</a> Appearing in the Conference on Neural Information Processing Systems   (NeurIPS 2025)</p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºäº’è”ç½‘è§„æ¨¡æ•°æ®çš„é¢„è®­ç»ƒæ¨¡å‹å·²åœ¨æœºå™¨äººé¢†åŸŸå¼•å‘é©å‘½ï¼Œä½†å…¶åº”ç”¨äºä½å±‚æ¬¡æ§åˆ¶ä¸»è¦å±€é™äºè¡Œä¸ºå…‹éš†ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­å¼ºåŒ–å­¦ä¹ é˜¶æ®µæˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæœºå™¨äººçš„ä¸¤é˜¶æ®µåè®­ç»ƒæ³•ã€‚ç¬¬ä¸€é˜¶æ®µä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½¿ç”¨è¡Œä¸ºå…‹éš†å’Œæ­¥è·é¢„æµ‹ç›®æ ‡å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç¬¬äºŒé˜¶æ®µä¸ºè‡ªæˆ‘æå‡ï¼Œæ­¥è·é¢„æµ‹å¯æå–å‡ºå½¢çŠ¶è‰¯å¥½çš„å¥–åŠ±å‡½æ•°å’Œç¨³å¥çš„æˆåŠŸæ£€æµ‹å™¨ï¼Œä½¿æœºå™¨äººç¾¤ä½“èƒ½å¤Ÿåœ¨æå°‘äººç±»ç›‘ç£çš„æƒ…å†µä¸‹è‡ªä¸»ç»ƒä¹ ä¸‹æ¸¸ä»»åŠ¡ã€‚é€šè¿‡åœ¨å®é™…å’Œæ¨¡æ‹Ÿæœºå™¨äººå®ä½“ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ–°å‹åè®­ç»ƒé…æ–¹åœ¨åµŒå…¥å¼åŸºç¡€æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜äº†SFTä¸è‡ªæˆ‘æå‡çš„ç»“åˆæ¯”æ‰©å¤§æ¨¡ä»¿æ•°æ®æ”¶é›†è¿›è¡Œæœ‰ç›‘ç£å­¦ä¹ æ›´åŠ æ ·æœ¬é«˜æ•ˆï¼Œä¸”äº§ç”Ÿçš„ç­–ç•¥æˆåŠŸç‡æ›´é«˜ã€‚è¿›ä¸€æ­¥çš„å‰–æè¡¨æ˜ï¼Œç»“åˆäº’è”ç½‘è§„æ¨¡é¢„è®­ç»ƒå’Œè‡ªæˆ‘æå‡æ˜¯æ ·æœ¬æ•ˆç‡çš„å…³é”®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ç»„åˆç‹¬ç‰¹åœ°è§£é”äº†ä¸€ç§å½“å‰æ–¹æ³•æ— æ³•å®ç°çš„èƒ½åŠ›ï¼šè‡ªä¸»ç»ƒä¹ å¹¶è·å–è¶…è¶Šè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ¨¡ä»¿å­¦ä¹ æ•°æ®é›†çš„æ–°æŠ€èƒ½ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å°†é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ä¸åœ¨çº¿è‡ªæˆ‘æå‡ç›¸ç»“åˆï¼Œåœ¨æœºå™¨äººé¢†åŸŸå®ç°è‡ªä¸»æŠ€èƒ½è·å–çš„å˜é©æ€§æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µåè®­ç»ƒæ³•ç”¨äºæœºå™¨äººé¢†åŸŸï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œè‡ªæˆ‘æå‡é˜¶æ®µã€‚</li>
<li>SFTç»“åˆäº†è¡Œä¸ºå…‹éš†å’Œæ­¥è·é¢„æµ‹ç›®æ ‡ï¼Œä»¥ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è‡ªæˆ‘æå‡é˜¶æ®µé€šè¿‡æ­¥è·é¢„æµ‹æå–å¥–åŠ±å‡½æ•°å’ŒæˆåŠŸæ£€æµ‹å™¨ï¼Œä½¿æœºå™¨äººèƒ½è‡ªä¸»å®è·µä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ä¸ä»…é€šè¿‡æ‰©å¤§æ¨¡ä»¿æ•°æ®æ”¶é›†è¿›è¡Œæœ‰ç›‘ç£å­¦ä¹ ç›¸æ¯”ï¼ŒSFTä¸è‡ªæˆ‘æå‡çš„ç»“åˆæ›´åŠ æ ·æœ¬é«˜æ•ˆï¼Œä¸”ç­–ç•¥æˆåŠŸç‡æ›´é«˜ã€‚</li>
<li>ç»“åˆäº’è”ç½‘è§„æ¨¡é¢„è®­ç»ƒå’Œè‡ªæˆ‘æå‡æ˜¯å–å¾—æ ·æœ¬æ•ˆç‡çš„å…³é”®ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿè§£é”è‡ªä¸»è·å–å¹¶ç»ƒä¹ æ–°æŠ€èƒ½çš„èƒ½åŠ›ï¼Œè¿™äº›æŠ€èƒ½èƒ½å¤Ÿæ³›åŒ–åˆ°è¶…è¶Šè®­ç»ƒæœŸé—´è§‚å¯Ÿåˆ°çš„è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-60f366716a77ec7e6d4f66b0badf7398~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028804&auth_key=1760028804-0-0-9ff4c71f3ed51985e3e9353f47117459&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-78f4ffbe566463e0be3188f00ef66680~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028811&auth_key=1760028811-0-0-c717deae84f2bd4e3d8befddb95296bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b2d5e0b08b7af4e6761a2a8cc124c9e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028818&auth_key=1760028818-0-0-f6cac6b2b906c0276f7b71e252126868&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-11d76a9f3ecba16f194f43a5c34cf6d7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028825&auth_key=1760028825-0-0-d48484219a063f695606aac9872cc9db&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A1-Asynchronous-Test-Time-Scaling-via-Conformal-Prediction"><a href="#A1-Asynchronous-Test-Time-Scaling-via-Conformal-Prediction" class="headerlink" title="A1: Asynchronous Test-Time Scaling via Conformal Prediction"></a>A1: Asynchronous Test-Time Scaling via Conformal Prediction</h2><p><strong>Authors:Jing Xiong, Qiujiang Chen, Fanghua Ye, Zhongwei Wan, Chuanyang Zheng, Chenyang Zhao, Hui Shen, Alexander Hanbo Li, Chaofan Tao, Haochen Tan, Haoli Bai, Lifeng Shang, Lingpeng Kong, Ngai Wong</strong></p>
<p>Large language models (LLMs) benefit from test-time scaling, but existing methods face significant challenges, including severe synchronization overhead, memory bottlenecks, and latency, especially during speculative decoding with long reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a statistically guaranteed adaptive inference framework that addresses these challenges. A1 refines arithmetic intensity to identify synchronization as the dominant bottleneck, proposes an online calibration strategy to enable asynchronous inference, and designs a three-stage rejection sampling pipeline that supports both sequential and parallel scaling. Through experiments on the MATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model families, we demonstrate that A1 achieves a remarkable 56.7x speedup in test-time scaling and a 4.14x improvement in throughput, all while maintaining accurate rejection-rate control, reducing latency and memory overhead, and no accuracy loss compared to using target model scaling alone. These results position A1 as an efficient and principled solution for scalable LLM inference. We have released the code at <a target="_blank" rel="noopener" href="https://github.com/menik1126/asynchronous-test-time-scaling">https://github.com/menik1126/asynchronous-test-time-scaling</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å—ç›Šäºæµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸¥é‡çš„åŒæ­¥å¼€é”€ã€å†…å­˜ç“¶é¢ˆå’Œå»¶è¿Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰é•¿æ¨ç†é“¾çš„æŠ•æœºè§£ç è¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬å¼•å…¥äº†A1ï¼ˆå¼‚æ­¥æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿè®¡ä¸Šä¿è¯çš„è‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œè§£å†³äº†è¿™äº›æŒ‘æˆ˜ã€‚A1é€šè¿‡ä¼˜åŒ–ç®—æœ¯å¼ºåº¦æ¥ç¡®å®šåŒæ­¥æ˜¯ä¸»è¦çš„ç“¶é¢ˆï¼Œæå‡ºäº†åœ¨çº¿æ ¡å‡†ç­–ç•¥ä»¥å®ç°å¼‚æ­¥æ¨ç†ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªä¸‰é˜¶æ®µæ‹’ç»é‡‡æ ·ç®¡é“ï¼Œæ”¯æŒé¡ºåºå’Œå¹¶è¡Œç¼©æ”¾ã€‚æˆ‘ä»¬åœ¨MATHã€AMC23ã€AIME24å’ŒAIME25æ•°æ®é›†ä¸Šï¼Œå¯¹å„ç§è‰ç¨¿ç›®æ ‡æ¨¡å‹å®¶æ—è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒA1åœ¨æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹é¢å®ç°äº†æƒŠäººçš„56.7å€åŠ é€Ÿï¼Œååé‡æé«˜äº†4.14å€ï¼ŒåŒæ—¶ä¿æŒäº†ç²¾ç¡®çš„æ‹’ç»ç‡æ§åˆ¶ï¼Œé™ä½äº†å»¶è¿Ÿå’Œå†…å­˜å¼€é”€ï¼Œä¸”ç›¸è¾ƒäºä»…ä½¿ç”¨ç›®æ ‡æ¨¡å‹ç¼©æ”¾ï¼Œæ²¡æœ‰ç²¾åº¦æŸå¤±ã€‚è¿™äº›ç»“æœä½¿A1æˆä¸ºé«˜æ•ˆä¸”æœ‰åŸåˆ™çš„å¯æ‰©å±•LLMæ¨ç†è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å·²å°†ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/menik1126/asynchronous-test-time-scaling%E4%B8%8A%E3%80%82">https://github.com/menik1126/asynchronous-test-time-scalingä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15148v1">PDF</a> Tech Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æµ‹è¯•æ—¶é—´ç¼©æ”¾é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºA1çš„å¼‚æ­¥æµ‹è¯•æ—¶é—´ç¼©æ”¾æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–ç®—æœ¯å¼ºåº¦è¯†åˆ«åŒæ­¥ç“¶é¢ˆï¼Œé‡‡ç”¨åœ¨çº¿æ ¡å‡†ç­–ç•¥å®ç°å¼‚æ­¥æ¨ç†ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ”¯æŒä¸²è¡Œå’Œå¹¶è¡Œç¼©æ”¾çš„ä¸‰é˜¶æ®µæ‹’ç»é‡‡æ ·ç®¡é“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒA1åœ¨æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹é¢å®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆçš„æ‹’ç»ç‡æ§åˆ¶ã€é™ä½å»¶è¿Ÿå’Œå†…å­˜å¼€é”€ï¼Œä¸”æ²¡æœ‰æŸå¤±ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯ä»æµ‹è¯•æ—¶é—´ç¼©æ”¾ä¸­è·ç›Šï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨åŒæ­¥å¼€é”€ã€å†…å­˜ç“¶é¢ˆå’Œå»¶è¿Ÿç­‰æŒ‘æˆ˜ã€‚</li>
<li>A1æ¡†æ¶æ˜¯ä¸€ç§é’ˆå¯¹è¿™äº›é—®é¢˜çš„å¼‚æ­¥æµ‹è¯•æ—¶é—´ç¼©æ”¾è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰ç»Ÿè®¡ä¿è¯çš„é€‚åº”æ€§æ¨ç†ã€‚</li>
<li>A1é€šè¿‡ä¼˜åŒ–ç®—æœ¯å¼ºåº¦è¯†åˆ«åŒæ­¥ç“¶é¢ˆï¼Œå¹¶æå‡ºåœ¨çº¿æ ¡å‡†ç­–ç•¥å®ç°å¼‚æ­¥æ¨ç†ã€‚</li>
<li>A1è®¾è®¡äº†ä¸€ä¸ªä¸‰é˜¶æ®µæ‹’ç»é‡‡æ ·ç®¡é“ï¼Œæ”¯æŒä¸²è¡Œå’Œå¹¶è¡Œç¼©æ”¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒA1åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹å®¶æ—ä¸Šå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼Œæœ€é«˜è¾¾åˆ°56.7å€ã€‚</li>
<li>A1åœ¨ä¿æŒæ‹’ç»ç‡æ§åˆ¶ã€é™ä½å»¶è¿Ÿå’Œå†…å­˜å¼€é”€çš„åŒæ—¶ï¼Œæ²¡æœ‰æŸå¤±ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4b28e3327e98b10d031a0ee607430fc2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028833&auth_key=1760028833-0-0-a1b5391cec15d26ae6135b2f4eaec316&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ccd47169dcdfb6f20b5f74f9a1433f4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028840&auth_key=1760028840-0-0-1fca82c0b3d29a93e9787bb5394476d0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b38aa650c9859d9529b52232e390b80~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028846&auth_key=1760028846-0-0-825f4d125c22c46d68abc83e56e92be6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e0c38a936769fc640e8317281a99099~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028852&auth_key=1760028852-0-0-06e3694a680c1bc47f06068784214a26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b224f776a9830fa9214fc10bbf05197~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028859&auth_key=1760028859-0-0-10aa5030f1f1705b1fcfd859b0fd933a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-Pixels-to-Urban-Policy-Intelligence-Recovering-Legacy-Effects-of-Redlining-with-a-Multimodal-LLM"><a href="#From-Pixels-to-Urban-Policy-Intelligence-Recovering-Legacy-Effects-of-Redlining-with-a-Multimodal-LLM" class="headerlink" title="From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of   Redlining with a Multimodal LLM"></a>From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of   Redlining with a Multimodal LLM</h2><p><strong>Authors:Anthony Howell, Nancy Wu, Sharmistha Bagchi, Yushim Kim, Chayn Sun</strong></p>
<p>This paper shows how a multimodal large language model (MLLM) can expand urban measurement capacity and support tracking of place-based policy interventions. Using a structured, reason-then-estimate pipeline on street-view imagery, GPT-4o infers neighborhood poverty and tree canopy, which we embed in a quasi-experimental design evaluating the legacy of 1930s redlining. GPT-4o recovers the expected adverse socio-environmental legacy effects of redlining, with estimates statistically indistinguishable from authoritative sources, and it outperforms a conventional pixel-based segmentation baseline-consistent with the idea that holistic scene reasoning extracts higher-order information beyond object counts alone. These results position MLLMs as policy-grade instruments for neighborhood measurement and motivate broader validation across policy-evaluation settings. </p>
<blockquote>
<p>è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¦‚ä½•æ‰©å¤§åŸå¸‚æµ‹é‡èƒ½åŠ›ï¼Œå¹¶æ”¯æŒåŸºäºåœ°ç‚¹çš„æ”¿ç­–å¹²é¢„çš„è·Ÿè¸ªã€‚é€šè¿‡ä½¿ç”¨è¡—é“æ™¯è§‚å›¾åƒçš„ç»“æ„åŒ–ã€æ¨ç†ä¼°ç®—æµç¨‹ï¼ŒGPT-4oæ¨æ–­å‡ºé‚»é‡Œè´«å›°å’Œæ ‘å† è¦†ç›–æƒ…å†µï¼Œæˆ‘ä»¬å°†è¿™äº›æƒ…å†µåµŒå…¥åˆ°å¯¹20ä¸–çºª30å¹´ä»£çº¢çº¿æ”¿ç­–çš„é—äº§è¿›è¡Œå‡†å®éªŒè®¾è®¡è¯„ä¼°ä¸­ã€‚GPT-4oæ¢å¤äº†çº¢çº¿æ”¿ç­–é¢„æœŸçš„è´Ÿé¢ç¤¾ä¼šç¯å¢ƒå½±å“ï¼Œå…¶ä¼°è®¡å€¼ä¸æƒå¨æ¥æºæ— æ³•åŒºåˆ†ï¼Œä¸”è¡¨ç°ä¼˜äºåŸºäºåƒç´ çš„å¸¸è§„åˆ†å‰²åŸºçº¿ï¼Œè¿™ç¬¦åˆæ•´ä½“åœºæ™¯æ¨ç†èƒ½å¤Ÿæå–å‡ºå•çº¯è®¡æ•°å¯¹è±¡ä¹‹å¤–çš„é«˜é˜¶ä¿¡æ¯çš„ç†å¿µã€‚è¿™äº›ç»“æœç¡®ç«‹äº†MLLMä½œä¸ºæ”¿ç­–çº§é‚»é‡Œæµ‹é‡å·¥å…·çš„åœ°ä½ï¼Œå¹¶æ¿€åŠ±æˆ‘ä»¬åœ¨æ›´å¹¿æ³›çš„æ”¿ç­–è¯„ä¼°ç¯å¢ƒä¸­è¿›è¡ŒéªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15132v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¦‚ä½•æå‡åŸå¸‚æµ‹é‡èƒ½åŠ›å¹¶æ”¯æŒåŸºäºåœ°ç‚¹çš„æ”¿ç­–å¹²é¢„è·Ÿè¸ªã€‚ç ”ç©¶é€šè¿‡è¿ç”¨è¡—é“æ™¯è§‚å›¾åƒçš„ç»“æ„åŒ–æ¨ç†ä¼°è®¡æµç¨‹ï¼Œä½¿ç”¨GPT-4oæ¨æ–­å‡ºé‚»é‡Œè´«å›°å’Œæ ‘æœ¨è¦†ç›–æƒ…å†µï¼Œå¹¶åµŒå…¥å‡†å®éªŒè®¾è®¡è¯„ä¼°äº†ä¸Šä¸–çºª30å¹´ä»£çº¢çº¿æ”¿ç­–çš„é—ç•™å½±å“ã€‚GPT-4oæ¢å¤äº†é¢„æœŸçš„çº¢çº¿æ”¿ç­–å¸¦æ¥çš„ä¸è‰¯ç¤¾ä¼šç¯å¢ƒå½±å“ï¼Œå…¶ä¼°è®¡ç»“æœä¸æƒå¨æ¥æºæ— æ³•åŒºåˆ†ï¼Œä¸”ä¼˜äºä¼ ç»Ÿçš„åŸºäºåƒç´ çš„åˆ†å‰²åŸºçº¿ã€‚è¿™è¡¨æ˜æ•´ä½“åœºæ™¯æ¨ç†èƒ½å¤Ÿæå–è¶…å‡ºå•çº¯ç‰©ä½“è®¡æ•°çš„é«˜é˜¶ä¿¡æ¯ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†MLLMsåœ¨é‚»é‡Œæµ‹é‡æ–¹é¢çš„æ”¿ç­–çº§åœ°ä½ï¼Œå¹¶æ¿€åŠ±æˆ‘ä»¬åœ¨æ›´å¹¿æ³›çš„æ”¿ç­–è¯„ä¼°ç¯å¢ƒä¸­è¿›è¡ŒéªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯æå‡åŸå¸‚æµ‹é‡èƒ½åŠ›ï¼Œæ”¯æŒè·Ÿè¸ªåŸºäºåœ°ç‚¹çš„æ”¿ç­–å¹²é¢„ã€‚</li>
<li>GPT-4oèƒ½é€šè¿‡è¡—é“æ™¯è§‚å›¾åƒæ¨æ–­é‚»é‡Œè´«å›°å’Œæ ‘æœ¨è¦†ç›–æƒ…å†µã€‚</li>
<li>GPT-4oåœ¨è¯„ä¼°çº¢çº¿æ”¿ç­–çš„é—ç•™å½±å“æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¼°è®¡ç»“æœä¸æƒå¨æ¥æºç›¸è¿‘ã€‚</li>
<li>GPT-4oçš„è¡¨ç°åœ¨è¯„ä¼°æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„åŸºäºåƒç´ çš„åˆ†å‰²åŸºçº¿ã€‚</li>
<li>æ•´ä½“åœºæ™¯æ¨ç†èƒ½å¤Ÿæå–è¶…å‡ºå•çº¯ç‰©ä½“è®¡æ•°çš„é«˜é˜¶ä¿¡æ¯ã€‚</li>
<li>MLLMså¯æˆä¸ºæ”¿ç­–è¯„ä¼°çš„å¯é å·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨é‚»é‡Œæµ‹é‡æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7528007537f4cd1773446c74c1c86349~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028866&auth_key=1760028866-0-0-5c5a445651cc500d1b8d2a149dd9bc60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Evaluating-Large-Language-Models-for-Cross-Lingual-Retrieval"><a href="#Evaluating-Large-Language-Models-for-Cross-Lingual-Retrieval" class="headerlink" title="Evaluating Large Language Models for Cross-Lingual Retrieval"></a>Evaluating Large Language Models for Cross-Lingual Retrieval</h2><p><strong>Authors:Longfei Zuo, Pingjun Hong, Oliver Kraus, Barbara Plank, Robert Litschko</strong></p>
<p>Multi-stage information retrieval (IR) has become a widely-adopted paradigm in search. While Large Language Models (LLMs) have been extensively evaluated as second-stage reranking models for monolingual IR, a systematic large-scale comparison is still lacking for cross-lingual IR (CLIR). Moreover, while prior work shows that LLM-based rerankers improve CLIR performance, their evaluation setup relies on lexical retrieval with machine translation (MT) for the first stage. This is not only prohibitively expensive but also prone to error propagation across stages. Our evaluation on passage-level and document-level CLIR reveals that further gains can be achieved with multilingual bi-encoders as first-stage retrievers and that the benefits of translation diminishes with stronger reranking models. We further show that pairwise rerankers based on instruction-tuned LLMs perform competitively with listwise rerankers. To the best of our knowledge, we are the first to study the interaction between retrievers and rerankers in two-stage CLIR with LLMs. Our findings reveal that, without MT, current state-of-the-art rerankers fall severely short when directly applied in CLIR. </p>
<blockquote>
<p>å¤šé˜¶æ®µä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰å·²æˆä¸ºæœç´¢ä¸­å¹¿æ³›é‡‡ç”¨çš„èŒƒå¼ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå•è¯­IRçš„ç¬¬äºŒé˜¶æ®µé‡æ’åºæ¨¡å‹å·²ç»å¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ï¼Œä½†åœ¨è·¨è¯­è¨€IRï¼ˆCLIRï¼‰æ–¹é¢ï¼Œä»ç¼ºä¹ç³»ç»Ÿçš„å¤§è§„æ¨¡æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œè™½ç„¶å…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼ŒåŸºäºLLMçš„é‡æ’åºå™¨å¯ä»¥æ”¹å–„CLIRçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„è¯„ä¼°è®¾ç½®ä¾èµ–äºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰çš„ç¬¬ä¸€é˜¶æ®µè¯æ±‡æ£€ç´¢ã€‚è¿™ä¸ä»…æˆæœ¬é«˜æ˜‚ï¼Œè€Œä¸”åœ¨å„é˜¶æ®µä¹‹é—´å®¹æ˜“å‡ºç°é”™è¯¯ä¼ æ’­ã€‚æˆ‘ä»¬åœ¨æ®µè½çº§å’Œæ–‡æ¡£çº§çš„CLIRè¯„ä¼°ä¸­å‘ç°ï¼Œä½¿ç”¨å¤šè¯­è¨€åŒç¼–ç å™¨ä½œä¸ºç¬¬ä¸€é˜¶æ®µçš„æ£€ç´¢å™¨å¯ä»¥å–å¾—è¿›ä¸€æ­¥çš„æ”¶ç›Šï¼Œå¹¶ä¸”éšç€é‡æ’åºæ¨¡å‹çš„å¢å¼ºï¼Œç¿»è¯‘çš„ä¼˜åŠ¿åœ¨å‡å°ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒåŸºäºæŒ‡ä»¤è®­ç»ƒLLMçš„é…å¯¹é‡æ’åºå™¨ä¸åˆ—è¡¨é‡æ’åºå™¨çš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ‰¹åœ¨ä¸¤é˜¶æ®µCLIRä¸­ç ”ç©¶æ£€ç´¢å™¨å’Œé‡æ’åºå™¨äº¤äº’çš„LLMçš„ç ”ç©¶äººå‘˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸ä½¿ç”¨æœºå™¨ç¿»è¯‘çš„æƒ…å†µä¸‹ï¼Œå½“å‰æœ€å…ˆè¿›çš„é‡æ’åºå™¨åœ¨ç›´æ¥åº”ç”¨äºCLIRæ—¶å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14749v1">PDF</a> Accepted at EMNLP 2025 (Findings)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šé˜¶æ®µä¿¡æ¯æ£€ç´¢åœ¨è‡ªç„¶æœç´¢ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨ç¬¬äºŒé˜¶æ®µçš„æ’åºæ¨¡å‹æ–¹é¢å·²åœ¨å•è¯­ä¿¡æ¯æ£€ç´¢ä¸­å¾—åˆ°å¹¿æ³›è¯„ä¼°ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶æ˜¾ç¤ºï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ’åºå™¨èƒ½æå‡è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ï¼ˆCLIRï¼‰çš„æ€§èƒ½ï¼Œä½†å…¶è¯„ä¼°è®¾ç½®ä¾èµ–äºæœºå™¨ç¿»è¯‘è¿›è¡Œç¬¬ä¸€é˜¶æ®µçš„è¯æ±‡æ£€ç´¢ï¼Œè¿™æ—¢æ˜‚è´µåˆå®¹æ˜“å‡ºç°é˜¶æ®µé—´çš„é”™è¯¯ä¼ æ’­ã€‚æœ¬æ–‡é‡‡ç”¨è·¨è¯­è¨€æƒ…å¢ƒè¯„ä¼°ä¸åŒé˜¶æ®µçš„CLIRæ–¹æ³•ï¼Œæ­ç¤ºé‡‡ç”¨å¤šè¯­è¨€biç¼–ç å™¨ä½œä¸ºç¬¬ä¸€é˜¶æ®µçš„æ£€ç´¢å™¨å¯å–å¾—è¿›ä¸€æ­¥çš„æé«˜ï¼Œè€Œä¸”ç¿»è¯‘çš„å¥½å¤„ä¼šéšç€æ›´å¼ºå¤§çš„æ’åºæ¨¡å‹çš„å¢åŠ è€Œå‡å°‘ã€‚å¦å¤–è¿˜å‘ç°ï¼ŒåŸºäºæŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é…å¯¹æ’åºå™¨ä¸åˆ—è¡¨æ’åºå™¨çš„æ€§èƒ½ç«äº‰ç›¸å½“ã€‚æ®äº†è§£ï¼Œæˆ‘ä»¬é¦–æ¬¡ç ”ç©¶äº†ä¸¤é˜¶æ®µCLIRçš„å¤§å‹è¯­è¨€æ¨¡å‹æ£€ç´¢å™¨å’Œæ’åºå™¨ä¹‹é—´çš„äº¤äº’ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œå¦‚æœä¸ä½¿ç”¨æœºå™¨ç¿»è¯‘ï¼Œå½“å‰å…ˆè¿›çš„æ’åºå™¨åœ¨ç›´æ¥åº”ç”¨äºCLIRæ—¶è¡¨ç°è¾ƒå·®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨è¯­è¨€ä¿¡æ¯æ£€ç´¢ï¼ˆCLIRï¼‰çš„ç¬¬äºŒé˜¶æ®µæ’åºæ¨¡å‹ä¸­çš„è¯„ä¼°ä»ç„¶ç¼ºä¹ç³»ç»Ÿæ€§å¤§è§„æ¨¡å¯¹æ¯”ã€‚</li>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ’åºå™¨èƒ½æé«˜CLIRæ€§èƒ½ï¼Œä½†ä¾èµ–äºæœºå™¨ç¿»è¯‘çš„ç¬¬ä¸€é˜¶æ®µæ£€ç´¢æˆæœ¬é«˜æ˜‚ä¸”æ˜“å‡ºç°é”™è¯¯ä¼ æ’­ã€‚</li>
<li>é‡‡ç”¨å¤šè¯­è¨€biç¼–ç å™¨ä½œä¸ºç¬¬ä¸€é˜¶æ®µçš„æ£€ç´¢å™¨å¯ä»¥è¿›ä¸€æ­¥æé«˜CLIRæ€§èƒ½ã€‚</li>
<li>éšç€æ’åºæ¨¡å‹çš„å¢å¼ºï¼Œç¿»è¯‘çš„é‡è¦æ€§é€æ¸å‡å°‘ã€‚</li>
<li>åŸºäºæŒ‡ä»¤è°ƒä¼˜çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é…å¯¹æ’åºå™¨è¡¨ç°è‰¯å¥½ã€‚</li>
<li>åœ¨ä¸¤é˜¶æ®µCLIRä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å™¨å’Œæ’åºå™¨ä¹‹é—´çš„äº¤äº’ä½œç”¨å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-aa6efe0597ca483601b807808f0145b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028874&auth_key=1760028874-0-0-e9d86b06cefd14aa8e5958ca605b126e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f5f252af5e4f4fe5a5ce97d309522cc3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028881&auth_key=1760028881-0-0-64b3db4b4f20a0977c32af5fc91ebd1e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-920fda5f24e4171dade9ebae21d54fb9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028888&auth_key=1760028888-0-0-b5c2b9c5a00f7601aa03aec7b4fd3a0a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d207eb173a6283544ba07ef052a01c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028895&auth_key=1760028895-0-0-37ccce9fd62c572a2c3bbe5ce9fb1719&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CodeLSI-Leveraging-Foundation-Models-for-Automated-Code-Generation-with-Low-Rank-Optimization-and-Domain-Specific-Instruction-Tuning"><a href="#CodeLSI-Leveraging-Foundation-Models-for-Automated-Code-Generation-with-Low-Rank-Optimization-and-Domain-Specific-Instruction-Tuning" class="headerlink" title="CodeLSI: Leveraging Foundation Models for Automated Code Generation with   Low-Rank Optimization and Domain-Specific Instruction Tuning"></a>CodeLSI: Leveraging Foundation Models for Automated Code Generation with   Low-Rank Optimization and Domain-Specific Instruction Tuning</h2><p><strong>Authors:Huy Le, Phong Nguyen, Hao Do, Tuan Nguyen, Thien Pham, Anh Nguyen-Duc, Tho Quan</strong></p>
<p>Context: Automated code generation using Foundation Models (FMs) offers promising solutions for enhancing software development efficiency. However, challenges remain in ensuring domain specificity, cost-effectiveness, and security - especially when relying on third-party APIs. This paper introduces CodeLSI, a framework that combines low-rank optimization and domain-specific instruction tuning to address these challenges.   Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel approach for generating high-quality code tailored to specific domains, using FMs fine-tuned on company infrastructure without dependence on external APIs.   Methods: CodeLSI applies low-rank adaptation techniques to reduce the computational cost of model pre-training and fine-tuning. Domain-specific instruction tuning is employed to align code generation with organizational needs. We implemented and tested the framework on real-world JavaScript coding tasks using datasets drawn from internal software projects.   Results: Experimental evaluations show that CodeLSI produces high-quality, context aware code. It outperforms baseline models in terms of relevance, accuracy, and domain fit. The use of low-rank optimization significantly reduced resource requirements, enabling scalable training on company-owned infrastructure.   Conclusion: CodeLSI demonstrates that combining low-rank optimization with domain specific tuning can enhance the practicality and performance of FMs for automated code generation. This approach provides a secure, cost-efficient alternative to commercial API based solutions and supports faster, more targeted innovation in software development. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šåˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰è¿›è¡Œè‡ªåŠ¨ä»£ç ç”Ÿæˆä¸ºæé«˜è½¯ä»¶å¼€å‘æ•ˆç‡æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨ä¿éšœé¢†åŸŸç‰¹å¼‚æ€§ã€æˆæœ¬æ•ˆç›Šå’Œå®‰å…¨æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¾èµ–ç¬¬ä¸‰æ–¹APIæ—¶ã€‚æœ¬æ–‡ä»‹ç»äº†CodeLSIæ¡†æ¶ï¼Œå®ƒç»“åˆäº†ä½ç§©ä¼˜åŒ–å’Œé¢†åŸŸç‰¹å®šæŒ‡ä»¤è°ƒæ•´æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p>ç›®æ ‡ï¼šæœ¬ç ”ç©¶çš„ç›®æ ‡æ˜¯å¼€å‘å’Œè¯„ä¼°CodeLSIï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨FMsç”Ÿæˆé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„é«˜è´¨é‡ä»£ç çš„æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡å¯¹å…¬å¸å†…éƒ¨åŸºç¡€è®¾æ–½è¿›è¡Œå¾®è°ƒï¼Œä¸ä¾èµ–å¤–éƒ¨APIã€‚</p>
<p>æ–¹æ³•ï¼šCodeLSIåº”ç”¨ä½ç§©é€‚åº”æŠ€æœ¯æ¥é™ä½æ¨¡å‹é¢„è®­ç»ƒå’Œç²¾ç»†è°ƒæ•´çš„è®¡ç®—æˆæœ¬ã€‚é‡‡ç”¨é¢†åŸŸç‰¹å®šæŒ‡ä»¤è°ƒæ•´ä½¿ä»£ç ç”Ÿæˆä¸ç»„ç»‡éœ€æ±‚ä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„JavaScriptç¼–ç¨‹ä»»åŠ¡ä¸Šå®ç°äº†è¯¥æ¡†æ¶ï¼Œå¹¶ä½¿ç”¨ä»å†…éƒ¨è½¯ä»¶é¡¹ç›®ä¸­æŠ½å–çš„æ•°æ®é›†è¿›è¡Œäº†æµ‹è¯•ã€‚</p>
<p>ç»“æœï¼šå®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒCodeLSIäº§ç”Ÿäº†é«˜è´¨é‡ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä»£ç ã€‚å®ƒåœ¨ç›¸å…³æ€§ã€å‡†ç¡®æ€§å’Œé¢†åŸŸé€‚åº”æ€§æ–¹é¢ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚ä½ç§©ä¼˜åŒ–çš„ä½¿ç”¨æ˜¾è‘—é™ä½äº†èµ„æºè¦æ±‚ï¼Œèƒ½å¤Ÿåœ¨å…¬å¸æ‹¥æœ‰çš„åŸºç¡€è®¾æ–½ä¸Šè¿›è¡Œå¯æ‰©å±•çš„åŸ¹è®­ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14373v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCodeLSIçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰é’ˆå¯¹ç‰¹å®šé¢†åŸŸç”Ÿæˆé«˜è´¨é‡ä»£ç ï¼Œä»è€Œæé«˜è½¯ä»¶å¼€å‘æ•ˆç‡ã€‚CodeLSIç»“åˆäº†ä½é˜¶ä¼˜åŒ–å’Œé¢†åŸŸç‰¹å®šæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥è§£å†³åœ¨è®¡ç®—æˆæœ¬ã€å®‰å…¨æ€§å’Œé¢†åŸŸç‰¹å¼‚æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹çœŸå®ä¸–ç•ŒJavaScriptç¼–ç¨‹ä»»åŠ¡çš„æµ‹è¯•ï¼ŒCodeLSIè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä»£ç ï¼Œä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä½é˜¶ä¼˜åŒ–æ˜¾è‘—é™ä½äº†èµ„æºéœ€æ±‚ï¼Œä½¿åœ¨å…¬å¸è‡ªæœ‰åŸºç¡€è®¾æ–½ä¸Šè¿›è¡Œå¯æ‰©å±•æ€§è®­ç»ƒæˆä¸ºå¯èƒ½ã€‚CodeLSIå±•ç¤ºäº†å°†ä½é˜¶ä¼˜åŒ–ä¸é¢†åŸŸç‰¹å®šè°ƒæ•´ç›¸ç»“åˆå¯ä»¥å¢å¼ºè‡ªåŠ¨ä»£ç ç”Ÿæˆçš„å®ç”¨æ€§ï¼Œä¸ºåŸºäºå•†ä¸šAPIçš„è§£å†³æ–¹æ¡ˆæä¾›äº†å®‰å…¨ã€æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶æ”¯æŒæ›´å¿«ã€æ›´æœ‰é’ˆå¯¹æ€§çš„è½¯ä»¶å¼€å‘åˆ›æ–°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CodeLSIæ¡†æ¶ç»“åˆäº†ä½é˜¶ä¼˜åŒ–å’Œé¢†åŸŸç‰¹å®šæŒ‡ä»¤è°ƒæ•´ï¼Œæ—¨åœ¨æé«˜è½¯ä»¶å¼€å‘æ•ˆç‡å¹¶æ»¡è¶³ç‰¹å®šéœ€æ±‚ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³åœ¨è®¡ç®—æˆæœ¬ã€å®‰å…¨æ€§å’Œé¢†åŸŸç‰¹å¼‚æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>CodeLSIé€šè¿‡çœŸå®ä¸–ç•Œçš„JavaScriptç¼–ç¨‹ä»»åŠ¡æµ‹è¯•ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>CodeLSIç”Ÿæˆçš„é«˜è´¨é‡ä»£ç å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
<li>ä½é˜¶ä¼˜åŒ–æ˜¾è‘—é™ä½äº†èµ„æºéœ€æ±‚ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´å…·å¯æ‰©å±•æ€§ã€‚</li>
<li>CodeLSIæä¾›äº†ä¸€ä¸ªå®‰å…¨ã€æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºåŸºäºå•†ä¸šAPIçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-627cd4a3b45c0c7e6ac5ff54e0708c79~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028903&auth_key=1760028903-0-0-d870fd468957f810e8aa37b7ff9e34f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4974f076168d35381d7d0ae6e615310d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028911&auth_key=1760028911-0-0-cc95f85bd4954f8c52885e162e3fc2fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f298bddc5bcfccb2c20e64ae0eb3c355~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028918&auth_key=1760028918-0-0-409ed32e25384dfba8be032af9fd4cdf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-Forget-the-Nonlinearity-Unlocking-Activation-Functions-in-Efficient-Fine-Tuning"><a href="#Donâ€™t-Forget-the-Nonlinearity-Unlocking-Activation-Functions-in-Efficient-Fine-Tuning" class="headerlink" title="Donâ€™t Forget the Nonlinearity: Unlocking Activation Functions in   Efficient Fine-Tuning"></a>Donâ€™t Forget the Nonlinearity: Unlocking Activation Functions in   Efficient Fine-Tuning</h2><p><strong>Authors:Bo Yin, Xingyi Yang, Xinchao Wang</strong></p>
<p>Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt weight matrices while keeping activation functions fixed. We introduce \textbf{NoRA}, the first PEFT framework that directly adapts nonlinear activation functions in pretrained transformer-based models. NoRA replaces fixed activations with learnable rational functions and applies structured low-rank updates to numerator and denominator coefficients, with a group-wise design that localizes adaptation and improves stability at minimal cost. On vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds full fine-tuning while updating only 0.4% of parameters (0.02M), achieving accuracy gains of +0.17% and +0.27%. When combined with LoRA (\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++ consistently improves generation quality, yielding average MMLU gains of +0.3%â€“0.8%, including +1.6% on STEM (Alpaca) and +1.3% on OpenOrca. We further show that NoRA constrains adaptation to a low-dimensional functional subspace, implicitly regularizing update magnitude and direction. These results establish activation-space tuning as a complementary and highly parameter-efficient alternative to weight-based PEFT, positioning activation functions as first-class objects for model adaptation. </p>
<blockquote>
<p>ç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ä¸»è¦é€‚åº”æƒé‡çŸ©é˜µï¼ŒåŒæ—¶ä¿æŒæ¿€æ´»å‡½æ•°å›ºå®šã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>NoRA</strong>ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç›´æ¥é€‚åº”é¢„è®­ç»ƒåŸºäºtransformeræ¨¡å‹çš„éçº¿æ€§æ¿€æ´»å‡½æ•°çš„PEFTæ¡†æ¶ã€‚NoRAç”¨å¯å­¦ä¹ çš„æœ‰ç†å‡½æ•°æ›¿æ¢å›ºå®šæ¿€æ´»å‡½æ•°ï¼Œå¹¶å¯¹åˆ†å­å’Œåˆ†æ¯ç³»æ•°åº”ç”¨ç»“æ„åŒ–ä½ç§©æ›´æ–°ï¼Œé‡‡ç”¨åˆ†ç»„è®¾è®¡å®ç°å±€éƒ¨é€‚åº”ï¼Œå¹¶åœ¨å‡ ä¹ä¸å¢åŠ æˆæœ¬çš„æƒ…å†µä¸‹æé«˜ç¨³å®šæ€§ã€‚åœ¨CIFAR-10å’ŒCIFAR-100ä¸Šè®­ç»ƒçš„è§†è§‰å˜å‹å™¨ä¸­ï¼ŒNoRAåœ¨ä»…æ›´æ–°0.4%ï¼ˆå³0.02Mï¼‰çš„å‚æ•°æ—¶ï¼Œå°±èƒ½è¾¾åˆ°æˆ–è¶…è¿‡å®Œå…¨å¾®è°ƒçš„æ•ˆæœï¼Œå‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†+0.17%å’Œ+0.27%ã€‚ä¸LoRAç»“åˆä½¿ç”¨æ—¶ï¼ˆ**NoRA++**ï¼‰ï¼Œåœ¨åŒ¹é…çš„è®­ç»ƒé¢„ç®—ä¸‹ï¼Œé€šè¿‡æ·»åŠ æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ï¼Œå®ƒä¼˜äºLoRAå’ŒDoRAã€‚åœ¨LLaMA3-8BæŒ‡ä»¤è°ƒæ•´ä¸­ï¼ŒNoRA++æŒç»­æé«˜ç”Ÿæˆè´¨é‡ï¼Œå¹³å‡MMLUå¢ç›Šä¸º+0.3%~+0.8%ï¼Œå…¶ä¸­STEMï¼ˆAlpacaï¼‰ä¸Šæé«˜+1.6%ï¼ŒOpenOrcaä¸Šæé«˜+1.3%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒNoRAå°†é€‚åº”çº¦æŸåœ¨ä½ç»´å‡½æ•°å­ç©ºé—´å†…ï¼Œéšå«åœ°æ­£åˆ™åŒ–æ›´æ–°å¹…åº¦å’Œæ–¹å‘ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†æ¿€æ´»ç©ºé—´è°ƒæ•´ä½œä¸ºåŸºäºæƒé‡çš„PEFTçš„äº’è¡¥å’Œé«˜åº¦å‚æ•°é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå°†æ¿€æ´»å‡½æ•°å®šä½ä¸ºæ¨¡å‹é€‚åº”çš„ä¸€æµå¯¹è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13240v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•â€”â€”NoRAï¼Œå®ƒç›´æ¥é€‚åº”é¢„è®­ç»ƒtransformeræ¨¡å‹ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚NoRAç”¨å¯å­¦ä¹ çš„æœ‰ç†å‡½æ•°æ›¿æ¢å›ºå®šæ¿€æ´»ï¼Œå¹¶å¯¹åˆ†å­å’Œåˆ†æ¯ç³»æ•°åº”ç”¨ç»“æ„åŒ–ä½ç§©æ›´æ–°ï¼Œä»¥å±€éƒ¨åŒ–é€‚åº”å¹¶æé«˜ç¨³å®šæ€§ï¼ŒåŒæ—¶æˆæœ¬æä½ã€‚åœ¨CIFAR-10å’ŒCIFAR-100çš„è§†è§‰ä¸Šï¼ŒNoRAåŒ¹é…æˆ–è¶…è¿‡å…¨å¾®è°ƒï¼Œä»…æ›´æ–°0.4%çš„å‚æ•°ï¼ˆ0.02Mï¼‰ï¼Œå‡†ç¡®ç‡æé«˜+0.17%å’Œ+0.27%ã€‚ä¸LoRAç»“åˆä½¿ç”¨æ—¶ï¼ˆNoRA++ï¼‰ï¼Œåœ¨åŒ¹é…çš„è®­ç»ƒé¢„ç®—ä¸‹ï¼Œé€šè¿‡æ·»åŠ æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ï¼Œå®ƒä¼˜äºLoRAå’ŒDoRAã€‚åœ¨LLaMA3-8BæŒ‡ä»¤è°ƒæ•´ä¸­ï¼ŒNoRA++æŒç»­æé«˜ç”Ÿæˆè´¨é‡ï¼Œå¹³å‡MMLUå¢ç›Šä¸º+0.3%~+0.8%ï¼Œå…¶ä¸­STEMï¼ˆAlpacaï¼‰å’ŒOpenOrcaåˆ†åˆ«æé«˜+1.6%å’Œ+1.3%ã€‚æ­¤å¤–ï¼ŒNoRAå°†é€‚åº”é™åˆ¶åœ¨ä½ç»´åŠŸèƒ½å­ç©ºé—´ä¸­ï¼Œéšå¼åœ°è§„èŒƒæ›´æ–°å¹…åº¦å’Œæ–¹å‘ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†æ¿€æ´»ç©ºé—´è°ƒæ•´ä½œä¸ºæƒé‡åŸºç¡€PEFTçš„äº’è¡¥å’Œé«˜åº¦å‚æ•°é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œå°†æ¿€æ´»å‡½æ•°å®šä½ä¸ºæ¨¡å‹é€‚åº”çš„ä¸€æµå¯¹è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NoRAæ˜¯ä¸€ç§æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œä¸“æ³¨äºé€‚åº”é¢„è®­ç»ƒtransformeræ¨¡å‹ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚</li>
<li>NoRAé€šè¿‡ç”¨å¯å­¦ä¹ çš„æœ‰ç†å‡½æ•°æ›¿æ¢å›ºå®šæ¿€æ´»ï¼Œå¹¶åˆ©ç”¨ç»“æ„åŒ–ä½ç§©æ›´æ–°æ¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>åœ¨è§†è§‰ä»»åŠ¡ä¸Šï¼ŒNoRAåœ¨å‚æ•°æ›´æ–°æå°‘çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿè¾¾åˆ°æˆ–è¶…è¿‡å…¨è°ƒçš„æ•ˆæœï¼Œå¹¶æœ‰ä¸€å®šçš„å‡†ç¡®ç‡æå‡ã€‚</li>
<li>NoRA++æ˜¯NoRAä¸LoRAçš„ç»“åˆï¼Œå®ƒåœ¨åŒ¹é…çš„è®­ç»ƒé¢„ç®—ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>åœ¨LLaMA3-8BæŒ‡ä»¤è°ƒæ•´ä¸­ï¼ŒNoRA++æ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>NoRAå°†æ¨¡å‹é€‚åº”é™åˆ¶åœ¨ä½ç»´åŠŸèƒ½å­ç©ºé—´ä¸­ï¼Œéšå¼åœ°è§„èŒƒæ›´æ–°å¹…åº¦å’Œæ–¹å‘ã€‚</li>
<li>è¿™äº›ç»“æœç¡®ç«‹äº†æ¿€æ´»ç©ºé—´è°ƒæ•´ä½œä¸ºå‚æ•°é«˜æ•ˆå¾®è°ƒçš„ä¸€ç§é‡è¦ä¸”æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6192bea7b9719222fc80b32ddb0e8996~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028925&auth_key=1760028925-0-0-919ffec6ff509f9f70b3d6a4faa63de0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-720d21628ee898b02043bf82742bdc10~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028935&auth_key=1760028935-0-0-2474047da42aaa22cade2630a3e818a9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-60d72efb45af9de4bcd9d280a34f90e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028942&auth_key=1760028942-0-0-bdbe9127a3e36b360bfadae0bdfc1495&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models"><a href="#Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models" class="headerlink" title="Probing the Representational Power of Sparse Autoencoders in Vision   Models"></a>Probing the Representational Power of Sparse Autoencoders in Vision   Models</h2><p><strong>Authors:Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng</strong></p>
<p>Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting the hidden states of large language models (LLMs). By learning to reconstruct activations from a sparse bottleneck layer, SAEs discover interpretable features from the high-dimensional internal representations of LLMs. Despite their popularity with language models, SAEs remain understudied in the visual domain. In this work, we provide an extensive evaluation the representational power of SAEs for vision models using a broad range of image-based tasks. Our experimental results demonstrate that SAE features are semantically meaningful, improve out-of-distribution generalization, and enable controllable generation across three vision model architectures: vision embedding models, multi-modal LMMs and diffusion models. In vision embedding models, we find that learned SAE features can be used for OOD detection and provide evidence that they recover the ontological structure of the underlying model. For diffusion models, we demonstrate that SAEs enable semantic steering through text encoder manipulation and develop an automated pipeline for discovering human-interpretable attributes. Finally, we conduct exploratory experiments on multi-modal LLMs, finding evidence that SAE features reveal shared representations across vision and language modalities. Our study provides a foundation for SAE evaluation in vision models, highlighting their strong potential improving interpretability, generalization, and steerability in the visual domain. </p>
<blockquote>
<p>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencodersï¼Œç®€ç§°SAEï¼‰å·²æˆä¸ºè§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼Œç®€ç§°LLMï¼‰éšè—çŠ¶æ€çš„ä¸€ç§æµè¡Œå·¥å…·ã€‚é€šè¿‡å­¦ä¼šä»ç¨€ç–ç“¶é¢ˆå±‚é‡å»ºæ¿€æ´»ï¼ŒSAEä»LLMçš„é«˜ç»´å†…éƒ¨è¡¨ç¤ºä¸­å‘ç°å¯è§£é‡Šçš„ç‰¹å¾ã€‚å°½ç®¡SAEåœ¨è¯­è¨€æ¨¡å‹ä¸­å¾ˆå—æ¬¢è¿ï¼Œä½†åœ¨è§†è§‰é¢†åŸŸå®ƒä»¬ä»è¢«ç ”ç©¶å¾—ä¸å¤Ÿæ·±å…¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ä¸€ç³»åˆ—å›¾åƒä»»åŠ¡çš„å¤§é‡è¯„ä¼°ï¼Œå…¨é¢è¯„ä¼°äº†SAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„è¡¨å¾èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAEç‰¹å¾æ˜¯è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ï¼Œèƒ½å¤Ÿæ”¹å–„åˆ†å¸ƒå¤–çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸‰ç§è§†è§‰æ¨¡å‹æ¶æ„ä¸­å®ç°äº†å¯æ§ç”Ÿæˆï¼šè§†è§‰åµŒå…¥æ¨¡å‹ã€å¤šæ¨¡æ€LLMå’Œæ‰©æ•£æ¨¡å‹ã€‚åœ¨è§†è§‰åµŒå…¥æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°å­¦åˆ°çš„SAEç‰¹å¾å¯ç”¨äºOODæ£€æµ‹ï¼Œå¹¶æä¾›è¯æ®è¡¨æ˜å®ƒä»¬æ¢å¤äº†åº•å±‚æ¨¡å‹çš„æœ¬ä½“ç»“æ„ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†SAEé€šè¿‡æ–‡æœ¬ç¼–ç å™¨æ“ä½œå®ç°è¯­ä¹‰è½¬å‘ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“æ¥å‘ç°äººç±»å¯è§£é‡Šçš„å±æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹å¤šæ¨¡æ€LLMè¿›è¡Œäº†æ¢ç´¢æ€§å®éªŒï¼Œå‘ç°è¯æ®è¡¨æ˜SAEç‰¹å¾æ­ç¤ºäº†è·¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å…±äº«è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºSAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„è¯„ä¼°å¥ å®šäº†åŸºç¡€ï¼Œçªæ˜¾äº†å®ƒä»¬åœ¨æé«˜è§†è§‰é¢†åŸŸçš„å¯è§£é‡Šæ€§ã€æ³›åŒ–å’Œå¯æ§æ€§æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11277v2">PDF</a> ICCV 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>åŸºäºSparse Autoencodersï¼ˆSAEï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è§£é‡Šéšè—çŠ¶æ€çš„åº”ç”¨ï¼Œæœ¬æ–‡å¯¹å…¶åœ¨è§†è§‰æ¨¡å‹ä¸­çš„ä»£è¡¨æ€§èƒ½åŠ›è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAEç‰¹å¾å…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼Œå¯æé«˜ç¦»ç¾¤åˆ†å¸ƒæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸‰ç§è§†è§‰æ¨¡å‹æ¶æ„ä¸­å®ç°å¯æ§ç”Ÿæˆã€‚æœ¬æ–‡çš„ç ”ç©¶ä¸ºSAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„åº”ç”¨æä¾›äº†è¯„ä¼°åŸºç¡€ï¼Œçªæ˜¾å…¶åœ¨æé«˜è§†è§‰é¢†åŸŸçš„è§£é‡Šæ€§ã€æ³›åŒ–å’Œå¯æ§æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAEsç”¨äºè§£æè§†è§‰æ¨¡å‹çš„éšè—çŠ¶æ€è¡¨ç°è‰¯å¥½ã€‚</li>
<li>SAEç‰¹å¾å…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼Œå¯æé«˜æ¨¡å‹çš„ç¦»ç¾¤åˆ†å¸ƒæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SAEsåœ¨ä¸‰ç§è§†è§‰æ¨¡å‹æ¶æ„ä¸­å®ç°äº†å¯æ§ç”Ÿæˆã€‚</li>
<li>åœ¨è§†è§‰åµŒå…¥æ¨¡å‹ä¸­ï¼ŒSAEç‰¹å¾å¯ç”¨äºç¦»ç¾¤å€¼æ£€æµ‹ï¼Œå¹¶æ­ç¤ºäº†åº•å±‚æ¨¡å‹çš„æœ¬ä½“ç»“æ„ã€‚</li>
<li>åœ¨æ‰©æ•£æ¨¡å‹ä¸­ï¼ŒSAEé€šè¿‡æ–‡æœ¬ç¼–ç å™¨æ“ä½œå®ç°äº†è¯­ä¹‰æ§åˆ¶ï¼Œå¹¶å¼€å‘äº†å‘ç°äººç±»å¯è§£é‡Šå±æ€§çš„è‡ªåŠ¨åŒ–ç®¡é“ã€‚</li>
<li>SAEç‰¹å¾åœ¨å¤šæ¨¡æ€LLMä¸­æ˜¾ç¤ºå‡ºè·¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å…±äº«è¡¨ç¤ºã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶ä¸ºSAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„åº”ç”¨æä¾›äº†è¯„ä¼°åŸºç¡€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-17a3bc4e3f3c5292a225aab902b3db42~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028949&auth_key=1760028949-0-0-bc0313b5163296499e2b02e21cf1927b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9dc7f6230aef793f5188cc1f68aa0339~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028957&auth_key=1760028957-0-0-6af9b9916abb5d7e0bc684a600694ac0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9a43df17e27a99dd99a7d20823f8fa45~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028964&auth_key=1760028964-0-0-359b8fef8e8614edf3f45914e3bd7267&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe3261aaac461393420903ec09a57edd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028971&auth_key=1760028971-0-0-a15aa835b1792f048a898b70667d0c14&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SMART-Simulated-Students-Aligned-with-Item-Response-Theory-for-Question-Difficulty-Prediction"><a href="#SMART-Simulated-Students-Aligned-with-Item-Response-Theory-for-Question-Difficulty-Prediction" class="headerlink" title="SMART: Simulated Students Aligned with Item Response Theory for Question   Difficulty Prediction"></a>SMART: Simulated Students Aligned with Item Response Theory for Question   Difficulty Prediction</h2><p><strong>Authors:Alexander Scarlatos, Nigel Fernandez, Christopher Ormerod, Susan Lottridge, Andrew Lan</strong></p>
<p>Item (question) difficulties play a crucial role in educational assessments, enabling accurate and efficient assessment of student abilities and personalization to maximize learning outcomes. Traditionally, estimating item difficulties can be costly, requiring real students to respond to items, followed by fitting an item response theory (IRT) model to get difficulty estimates. This approach cannot be applied to the cold-start setting for previously unseen items either. In this work, we present SMART (Simulated Students Aligned with IRT), a novel method for aligning simulated students with instructed ability, which can then be used in simulations to predict the difficulty of open-ended items. We achieve this alignment using direct preference optimization (DPO), where we form preference pairs based on how likely responses are under a ground-truth IRT model. We perform a simulation by generating thousands of responses, evaluating them with a large language model (LLM)-based scoring model, and fit the resulting data to an IRT model to obtain item difficulty estimates. Through extensive experiments on two real-world student response datasets, we show that SMART outperforms other item difficulty prediction methods by leveraging its improved ability alignment. </p>
<blockquote>
<p>é¢˜ç›®éš¾åº¦åœ¨æ•™è‚²è¯„ä¼°ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œèƒ½å¤Ÿå‡†ç¡®é«˜æ•ˆåœ°è¯„ä¼°å­¦ç”Ÿèƒ½åŠ›å¹¶å®ç°ä¸ªæ€§åŒ–ï¼Œä»¥æœ€å¤§åŒ–å­¦ä¹ æ•ˆæœã€‚ä¼ ç»Ÿä¸Šï¼Œä¼°ç®—é¢˜ç›®éš¾åº¦å¯èƒ½ä¼šæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦çœŸå®å­¦ç”Ÿå¯¹é¢˜ç›®åšå‡ºå›åº”ï¼Œç„¶åå¯¹é¢˜ç›®ååº”ç†è®ºï¼ˆIRTï¼‰æ¨¡å‹è¿›è¡Œæ‹Ÿåˆä»¥è·å–éš¾åº¦ä¼°ç®—ã€‚è¿™ç§æ–¹æ³•ä¹Ÿä¸èƒ½åº”ç”¨äºä¹‹å‰æœªè§è¿‡çš„é¢˜ç›®çš„å†·å¯åŠ¨è®¾ç½®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SMARTï¼ˆä¸IRTå¯¹é½çš„æ¨¡æ‹Ÿå­¦ç”Ÿï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†æ¨¡æ‹Ÿå­¦ç”Ÿä¸æŒ‡å®šèƒ½åŠ›å¯¹é½çš„æ–°æ–¹æ³•ï¼Œç„¶åå¯åœ¨æ¨¡æ‹Ÿä¸­ç”¨äºé¢„æµ‹å¼€æ”¾å¼é¢˜ç›®çš„éš¾åº¦ã€‚æˆ‘ä»¬é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å®ç°è¿™ç§å¯¹é½ï¼Œæ ¹æ®çœŸå®IRTæ¨¡å‹ä¸‹å“åº”çš„å¯èƒ½æ€§æ¥å½¢æˆåå¥½å¯¹ã€‚æˆ‘ä»¬é€šè¿‡ç”Ÿæˆæ•°åƒä¸ªå“åº”æ¥è¿›è¡Œæ¨¡æ‹Ÿï¼Œä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„åˆ†æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¹¶å°†å¾—åˆ°çš„æ•°æ®æ‹Ÿåˆåˆ°IRTæ¨¡å‹ä¸­ä»¥è·å¾—é¢˜ç›®éš¾åº¦ä¼°ç®—ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œå­¦ç”Ÿå“åº”æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨æ”¹è¿›çš„èƒ½åŠ›å¯¹é½åŠŸèƒ½ï¼ŒSMARTåœ¨é¢˜ç›®éš¾åº¦é¢„æµ‹æ–¹æ³•ä¸Šå…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05129v2">PDF</a> Published in EMNLP 2025: The 2025 Conference on Empirical Methods in   Natural Language Processing</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨æ•™è‚²è¯„ä¼°ä¸­ï¼Œé¢˜ç›®éš¾åº¦æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œèƒ½å‡†ç¡®é«˜æ•ˆåœ°è¯„ä¼°å­¦ç”Ÿèƒ½åŠ›ï¼Œå¹¶æ ¹æ®ä¸ªäººæƒ…å†µè¿›è¡Œè°ƒæ•´ï¼Œä»¥æœ€å¤§åŒ–å­¦ä¹ æ•ˆæœã€‚ä¼ ç»Ÿä¸Šï¼Œä¼°ç®—é¢˜ç›®éš¾åº¦æˆæœ¬è¾ƒé«˜ï¼Œéœ€çœŸå®å­¦ç”Ÿå¯¹é¢˜ç›®ä½œå‡ºååº”ï¼Œå†æ‹Ÿåˆé¡¹ç›®ååº”ç†è®ºï¼ˆIRTï¼‰æ¨¡å‹å¾—åˆ°éš¾åº¦ä¼°ç®—å€¼ã€‚æ­¤æ–¹æ³•æ— æ³•åº”ç”¨äºæ–°é¢˜ç›®çš„å†·å¯åŠ¨è®¾ç½®ã€‚æœ¬ç ”ç©¶æå‡ºSMARTï¼ˆä¸IRTå¯¹é½çš„æ¨¡æ‹Ÿå­¦ç”Ÿï¼‰æ–¹æ³•ï¼Œé€šè¿‡æŒ‡ä»¤èƒ½åŠ›å¯¹é½æ¨¡æ‹Ÿå­¦ç”Ÿï¼Œå¯åœ¨æ¨¡æ‹Ÿä¸­é¢„æµ‹å¼€æ”¾å¼é¢˜ç›®çš„éš¾åº¦ã€‚æˆ‘ä»¬é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æŠ€æœ¯å®ç°å¯¹é½ï¼Œæ ¹æ®çœŸå®IRTæ¨¡å‹ä¸‹çš„å›ç­”å¯èƒ½æ€§å½¢æˆåå¥½å¯¹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªçœŸå®å­¦ç”Ÿå›ç­”æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†SMARTåˆ©ç”¨æ”¹è¿›çš„èƒ½åŠ›å¯¹é½åœ¨é¢˜ç›®éš¾åº¦é¢„æµ‹æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢˜ç›®éš¾åº¦åœ¨æ•™è‚²è¯„ä¼°ä¸­è‡³å…³é‡è¦ï¼Œå½±å“å­¦ç”Ÿèƒ½åŠ›çš„å‡†ç¡®å’Œé«˜æ•ˆè¯„ä¼°ã€‚</li>
<li>ä¼ ç»Ÿä¼°ç®—é¢˜ç›®éš¾åº¦çš„æ–¹æ³•æˆæœ¬è¾ƒé«˜ï¼Œä¸”æ— æ³•åº”ç”¨äºæ–°é¢˜ç›®çš„å†·å¯åŠ¨ç¯å¢ƒã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†SMARTæ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿå­¦ç”Ÿä¸IRTæ¨¡å‹çš„å¯¹é½ï¼Œé¢„æµ‹å¼€æ”¾å¼é¢˜ç›®çš„éš¾åº¦ã€‚</li>
<li>SMARTé‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–æŠ€æœ¯å®ç°æ¨¡æ‹Ÿå­¦ç”Ÿä¸IRTæ¨¡å‹çš„å¯¹é½ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿä¸­ï¼Œé€šè¿‡ç”Ÿæˆæ•°åƒä¸ªå›ç­”å¹¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„åˆ†æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå†æ‹ŸåˆIRTæ¨¡å‹å¾—åˆ°é¢˜ç›®éš¾åº¦ä¼°ç®—å€¼ã€‚</li>
<li>åœ¨ä¸¤ä¸ªçœŸå®å­¦ç”Ÿå›ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSMARTåœ¨é¢˜ç›®éš¾åº¦é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-abff4266dfde0c7b73f9694fd68fa4fc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028979&auth_key=1760028979-0-0-e31e11352e11b54db660296a9abd543d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22dbc75cc01a1181510c35cc930827fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028986&auth_key=1760028986-0-0-d48ab847d6b9965ed91bd20fce30a8b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bc6b234455b8c1e3d1030a93d333fae9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028993&auth_key=1760028993-0-0-b176fd1d157c8451020ebaaf96d83fbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="QA-LIGN-Aligning-LLMs-through-Constitutionally-Decomposed-QA"><a href="#QA-LIGN-Aligning-LLMs-through-Constitutionally-Decomposed-QA" class="headerlink" title="QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA"></a>QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA</h2><p><strong>Authors:Jacob Dineen, Aswin RRV, Qin Liu, Zhikun Xu, Xiao Ye, Ming Shen, Zhaonan Li, Shijie Lu, Chitta Baral, Muhao Chen, Ben Zhou</strong></p>
<p>Alignment of large language models (LLMs) with principles like helpfulness, honesty, and harmlessness typically relies on scalar rewards that obscure which objectives drive the training signal. We introduce QA-LIGN, which decomposes monolithic rewards into interpretable principle-specific evaluations through structured natural language programs. Models learn through a draft, critique, and revise pipeline, where symbolic evaluation against the rubrics provides transparent feedback for both initial and revised responses during GRPO training. Applied to uncensored Llama-3.1-8B-Instruct, QA-LIGN reduces attack success rates by up to 68.7% while maintaining a 0.67% false refusal rate, achieving Pareto optimal safety-helpfulness performance and outperforming both DPO and GRPO with state-of-the-art reward models given equivalent training. These results demonstrate that making reward signals interpretable and modular improves alignment effectiveness, suggesting transparency enhances LLM safety. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æœ‰åŠ©ç›Šæ€§ã€è¯šå®æ€§å’Œæ— å®³æ€§ç­‰åŸåˆ™çš„å¯¹é½ï¼Œé€šå¸¸ä¾èµ–äºæ ‡é‡å¥–åŠ±ï¼Œè¿™äº›å¥–åŠ±ä¼šæ©ç›–é©±åŠ¨è®­ç»ƒä¿¡å·çš„å¤šä¸ªç›®æ ‡ã€‚æˆ‘ä»¬å¼•å…¥äº†QA-LIGNï¼Œå®ƒé€šè¿‡ç»“æ„åŒ–çš„è‡ªç„¶è¯­è¨€ç¨‹åºå°†å•ä¸€çš„å¥–åŠ±åˆ†è§£ä¸ºå¯è§£é‡Šçš„åŸåˆ™ç‰¹å®šè¯„ä¼°ã€‚æ¨¡å‹é€šè¿‡è‰æ¡ˆã€è¯„è®ºå’Œä¿®è®¢ç®¡é“è¿›è¡Œå­¦ä¹ ï¼Œå…¶ä¸­ç¬¦å·è¯„ä¼°ä¸è¯„åˆ†æ ‡å‡†å¯¹ç…§ä¸ºåœ¨GRPOè®­ç»ƒæœŸé—´çš„åˆå§‹å’Œä¿®è®¢å“åº”æä¾›é€æ˜åé¦ˆã€‚åœ¨æœªç»å®¡æŸ¥çš„Llama-3.1-8B-Instructä¸Šåº”ç”¨QA-LIGNï¼Œå¯å°†æ”»å‡»æˆåŠŸç‡é™ä½é«˜è¾¾68.7%ï¼ŒåŒæ—¶ä¿æŒ0.67%çš„è¯¯æ‹’ç»ç‡ï¼Œå®ç°äº†å¸•ç´¯æ‰˜æœ€ä¼˜çš„å®‰å…¨ä¸æœ‰ç›Šæ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç­‰æ•ˆè®­ç»ƒçš„æƒ…å†µä¸‹ä½¿ç”¨æœ€å…ˆè¿›çš„å¥–åŠ±æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¼˜äºDPOå’ŒGRPOã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä½¿å¥–åŠ±ä¿¡å·å¯è§£é‡Šå’Œæ¨¡å—åŒ–å¯æé«˜å¯¹é½æ•ˆç‡ï¼Œè¿™è¡¨æ˜é€æ˜åº¦æé«˜äº†LLMçš„å®‰å…¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08123v3">PDF</a> Accepted to Findings of EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è¯¸å¦‚æœ‰ç”¨æ€§ã€è¯šå®æ€§å’Œæ— å®³æ€§ç­‰åŸåˆ™çš„å¯¹é½é€šå¸¸ä¾èµ–äºæ ‡é‡å¥–åŠ±ï¼Œè¿™ä¼šæ©ç›–é©±åŠ¨è®­ç»ƒä¿¡å·çš„å„ä¸ªç›®æ ‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†QA-LIGNï¼Œå®ƒé€šè¿‡ç»“æ„åŒ–çš„è‡ªç„¶è¯­è¨€ç¨‹åºå°†å•ä¸€çš„å¥–åŠ±åˆ†è§£æˆå¯è§£é‡Šçš„åŸåˆ™ç‰¹å®šè¯„ä¼°ï¼Œä½¿æ¨¡å‹å­¦ä¹ ä¸€ä¸ªè‰æ¡ˆã€è¯„ä¼°å’Œä¿®è®¢çš„ç®¡é“ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œç¬¦å·è¯„ä»·æä¾›åˆå§‹å’Œä¿®è®¢å›åº”æ—¶çš„é€æ˜åé¦ˆï¼Œä»¥ä¿ƒè¿›GRPOè®­ç»ƒã€‚åœ¨å¯¹éç®¡æ§çš„Llama-3.1-8B-Instructçš„åº”ç”¨ä¸­ï¼ŒQA-LIGNå°†æ”»å‡»æˆåŠŸç‡é™ä½äº†é«˜è¾¾68.7%ï¼ŒåŒæ—¶ä¿æŒ0.67%çš„è¯¯æ‹’ç»ç‡ï¼Œå®ç°å¸•ç´¯æ‰˜æœ€ä¼˜çš„å®‰å…¨æ€§èƒ½å’Œæé«˜çš„æœ‰ç”¨æ€§è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå°†å¥–åŠ±ä¿¡å·è®¾è®¡ä¸ºå¯è§£é‡Šå’Œæ¨¡å—åŒ–èƒ½å¤Ÿæé«˜å¯¹é½æ•ˆç‡ï¼Œè¡¨æ˜é€æ˜åº¦å¢å¼ºäº†LLMçš„å®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸åŸåˆ™çš„å¯¹é½é€šå¸¸ä¾èµ–æ ‡é‡å¥–åŠ±ï¼Œè¿™å¯¼è‡´è®­ç»ƒä¿¡å·çš„å„ä¸ªç›®æ ‡å˜å¾—æ¨¡ç³Šã€‚</li>
<li>QA-LIGNé€šè¿‡ç»“æ„åŒ–çš„è‡ªç„¶è¯­è¨€ç¨‹åºå°†å¥–åŠ±åˆ†è§£æˆå¯è§£é‡Šçš„åŸåˆ™ç‰¹å®šè¯„ä¼°ã€‚</li>
<li>æ¨¡å‹å­¦ä¹ åŒ…æ‹¬è‰æ¡ˆã€è¯„ä¼°å’Œä¿®è®¢çš„ç®¡é“ï¼Œä»¥æé«˜å¯¹é½æ•ˆç‡ã€‚</li>
<li>åœ¨éç®¡æ§çš„Llamaæ¨¡å‹ä¸­ï¼ŒQA-LIGNæ˜¾è‘—æé«˜äº†å®‰å…¨æ€§å’Œæ€§èƒ½è¡¨ç°ã€‚</li>
<li>QA-LIGNé™ä½äº†æ”»å‡»æˆåŠŸç‡é«˜è¾¾68.7%ï¼ŒåŒæ—¶ç»´æŒè¾ƒä½çš„è¯¯æ‹’ç»ç‡ã€‚</li>
<li>å¥–åŠ±ä¿¡å·çš„é€æ˜åº¦å’Œæ¨¡å—åŒ–è®¾è®¡å¯¹äºæé«˜LLMçš„å¯¹é½æ•ˆæœè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0ce8ca50dbe41c0c27a3ebb52b669cd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029000&auth_key=1760029000-0-0-d95efc490b5c4ad4809622421000f7dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ecb9c50dc3735bc7002aa5d98381a9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029007&auth_key=1760029007-0-0-31258937057abfd694594197dacd4cae&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d2748d0387a636a19ac5944fa4a1ded3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029014&auth_key=1760029014-0-0-19255356b5c1d3166c62573c3617562b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c4933b7c0ad6297088b0cdef8d9455a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029020&auth_key=1760029020-0-0-9b944f7473da39c6c0461837b4c7f04c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-104aed0888561e679c714d8481a13283~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029027&auth_key=1760029027-0-0-f51b189949ffe1eabf06e488340056b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67af0a84e3bfeed3d1bc27379fc6f362~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029034&auth_key=1760029034-0-0-2ab2a9a6287ac2f892aa435e06ec7ec6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="PMPO-Probabilistic-Metric-Prompt-Optimization-for-Small-and-Large-Language-Models"><a href="#PMPO-Probabilistic-Metric-Prompt-Optimization-for-Small-and-Large-Language-Models" class="headerlink" title="PMPO: Probabilistic Metric Prompt Optimization for Small and Large   Language Models"></a>PMPO: Probabilistic Metric Prompt Optimization for Small and Large   Language Models</h2><p><strong>Authors:Chenzhuo Zhao, Ziqian Liu, Xinda Wang, Junting Lu, Chaoyi Ruan</strong></p>
<p>Prompt optimization is a practical and widely applicable alternative to fine tuning for improving large language model performance. Yet many existing methods evaluate candidate prompts by sampling full outputs, often coupled with self critique or human annotated preferences, which limits scalability, especially for smaller models or models that are not instruction tuned. We present PMPO (Probabilistic Metric Prompt Optimization), a unified framework that uses token level cross entropy as a direct, lightweight evaluation signal. PMPO locates low quality prompt segments via a masking based analysis and iteratively rewrites them to propose improved variants. Crucially, during evaluation, PMPO selects among variants by minimizing loss in a single forward pass, eliminating output sampling and human or judge based scoring for selection while still using standard generation only to propose rewrites. This unified, loss based strategy supports both supervised and preference based tasks. Across model sizes and datasets, PMPO outperforms prior prompt optimizers: it achieves the highest average accuracy on BBH, performs strongly on GSM8K and AQUA RAT, and raises AlpacaEval 2.0 win rates by over 19 points. These results demonstrate PMPOâ€™s effectiveness, efficiency, and broad applicability. </p>
<blockquote>
<p>æç¤ºä¼˜åŒ–æ˜¯ä¸€ç§å®ç”¨çš„ã€å¹¿æ³›åº”ç”¨äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å¾®è°ƒæ›¿ä»£æ–¹æ³•ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ–¹æ³•é€šè¿‡é‡‡æ ·å®Œæ•´è¾“å‡ºæ¥è¯„ä¼°å€™é€‰æç¤ºï¼Œé€šå¸¸ä¸è‡ªæˆ‘æ‰¹è¯„æˆ–äººå·¥æ³¨é‡Šåå¥½ç›¸ç»“åˆï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæœªè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´çš„å°å‹æ¨¡å‹æˆ–æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†PMPOï¼ˆåŸºäºæ¦‚ç‡åº¦é‡çš„æç¤ºä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œä½¿ç”¨ä»¤ç‰Œçº§åˆ«çš„äº¤å‰ç†µä½œä¸ºç›´æ¥ã€è½»é‡çº§çš„è¯„ä¼°ä¿¡å·ã€‚PMPOé€šè¿‡åŸºäºé®ç½©çš„åˆ†ææ‰¾åˆ°è´¨é‡è¾ƒä½çš„æç¤ºç‰‡æ®µï¼Œå¹¶å¯¹å…¶è¿›è¡Œè¿­ä»£é‡å†™ï¼Œä»¥æå‡ºæ”¹è¿›åçš„å˜ä½“ã€‚å…³é”®çš„æ˜¯ï¼Œåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼ŒPMPOé€šè¿‡æœ€å°åŒ–å•æ¬¡å‰å‘ä¼ é€’ä¸­çš„æŸå¤±æ¥é€‰æ‹©å˜ä½“ï¼Œæ¶ˆé™¤äº†è¾“å‡ºé‡‡æ ·å’ŒåŸºäºäººç±»æˆ–æ³•å®˜çš„è¯„åˆ†é€‰æ‹©ï¼ŒåŒæ—¶ä»ä»…ä½¿ç”¨æ ‡å‡†ç”Ÿæˆæ¥æå‡ºé‡å†™ã€‚è¿™ç§ç»Ÿä¸€çš„ã€åŸºäºæŸå¤±çš„ç­–ç•¥æ—¢æ”¯æŒæœ‰ç›‘ç£çš„ä»»åŠ¡ï¼Œä¹Ÿæ”¯æŒåŸºäºåå¥½çš„ä»»åŠ¡ã€‚åœ¨å„ç§æ¨¡å‹å¤§å°å’Œæ•°æ®é›†ä¸Šï¼ŒPMPOçš„è¡¨ç°éƒ½ä¼˜äºå…ˆå‰çš„æç¤ºä¼˜åŒ–å™¨ï¼šå®ƒåœ¨BBHä¸Šè¾¾åˆ°äº†æœ€é«˜çš„å¹³å‡å‡†ç¡®ç‡ï¼Œåœ¨GSM8Kå’ŒAQUA RATä¸Šè¡¨ç°å¼ºåŠ²ï¼Œå¹¶å°†AlpacaEval 2.0çš„èƒœç‡æé«˜äº†è¶…è¿‡19ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœè¯æ˜äº†PMPOçš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œå¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16307v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§åä¸ºPMPOï¼ˆæ¦‚ç‡åº¦é‡æç¤ºä¼˜åŒ–ï¼‰çš„å®ç”¨æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç›´æ¥ä½¿ç”¨æ ‡è®°çº§åˆ«çš„äº¤å‰ç†µä½œä¸ºè¯„ä¼°ä¿¡å·æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºé‡‡æ ·è¾“å‡ºã€ç»“åˆè‡ªæˆ‘è¯„ä¼°æˆ–äººå·¥æ ‡æ³¨åå¥½çš„è¯„ä¼°æ–¹æ³•ä¸åŒï¼ŒPMPOèƒ½å¤Ÿåœ¨æ— éœ€é‡‡æ ·è¾“å‡ºçš„æƒ…å†µä¸‹æ‰¾åˆ°è´¨é‡ä¸ä½³çš„æç¤ºæ®µå¹¶è¿›è¡Œè¿­ä»£é‡å†™ã€‚å®ƒé€šè¿‡ä¸€æ¬¡å‰å‘ä¼ é€’æœ€å°åŒ–æŸå¤±æ¥è‡ªåŠ¨é€‰æ‹©æœ€ä½³æç¤ºå˜ä½“ï¼Œæ— éœ€ä½¿ç”¨è¾“å‡ºé‡‡æ ·æˆ–åŸºäºäººç±»æˆ–è¯„å§”çš„è¯„åˆ†æœºåˆ¶è¿›è¡Œç­›é€‰ã€‚è¿™ä¸€åŸºäºæŸå¤±çš„æ–¹æ³•æ—¢æ”¯æŒç›‘ç£ä»»åŠ¡ä¹Ÿæ”¯æŒåå¥½ä»»åŠ¡ï¼Œå¹¶åœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œæ•°æ®é›†ä¸Šå‡ä¼˜äºå…ˆå‰çš„æç¤ºä¼˜åŒ–å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PMPOæ˜¯ä¸€ç§ç”¨äºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å®ç”¨æ¡†æ¶ï¼Œç‰¹åˆ«é€‚ç”¨äºæœªç»æŒ‡ä»¤è°ƒæ•´ä¼˜åŒ–çš„æ¨¡å‹å’Œå°è§„æ¨¡æ¨¡å‹ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡é‡‡æ ·è¾“å‡ºç»“åˆè‡ªæˆ‘è¯„ä¼°æˆ–äººå·¥æ ‡æ³¨åå¥½æ¥è¯„ä¼°å€™é€‰æç¤ºï¼Œè€ŒPMPOåˆ™ä½¿ç”¨æ ‡è®°çº§åˆ«çš„äº¤å‰ç†µä½œä¸ºç›´æ¥ã€è½»é‡çº§çš„è¯„ä¼°ä¿¡å·ã€‚</li>
<li>PMPOé€šè¿‡åŸºäºæ©ç çš„è¯†åˆ«æ¥å®šä½ä½è´¨é‡æç¤ºæ®µå¹¶è¿›è¡Œè¿­ä»£é‡å†™ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æ— éœ€é‡‡æ ·è¾“å‡ºçš„æƒ…å†µä¸‹é€‰æ‹©æœ€ä½³æç¤ºå˜ä½“ï¼Œä»è€Œæé«˜äº†æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>PMPOä½¿ç”¨ä¸€æ¬¡å‰å‘ä¼ é€’æ¥æœ€å°åŒ–æŸå¤±ï¼Œä»è€Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³æç¤ºå˜ä½“ï¼Œæ— éœ€åŸºäºäººç±»æˆ–è¯„å§”çš„è¯„åˆ†æœºåˆ¶è¿›è¡Œç­›é€‰ã€‚</li>
<li>PMPOæ”¯æŒç›‘ç£ä»»åŠ¡å’Œåå¥½ä»»åŠ¡ï¼Œé€‚ç”¨äºå¤šç§åº”ç”¨åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8aa6f28d47f3feff9342ef35e426bc49~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029041&auth_key=1760029041-0-0-32363c9dd94a7ff65aad578fbd14ee5e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3eda912a6446e111180a1a36a00cc3dc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029049&auth_key=1760029049-0-0-8e2221e6fbfa8d68b96da51016181afc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2516ed455b6214eb9235a57c1fd5a731~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029055&auth_key=1760029055-0-0-f99dc99c959c34cfe6c4691a816a98bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c985e277ee02c1bacae02b3260efc7da~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029062&auth_key=1760029062-0-0-6c187648f5b9af2036135e3415aea7a7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-19f877595158f8e9bd206c4ee7e85cb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029069&auth_key=1760029069-0-0-898edd11119f365ab5fe9bfb7594c753&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  ScaleCUA Scaling Open-Source Computer Use Agents with Cross-Platform   Data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-3c9af23c4384d183d9b789db7408e57c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760027946&auth_key=1760027946-0-0-e4436081aa1f2a7ab1223fe254d34bef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  Generalizable Geometric Image Caption Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
