<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-09-20  ScaleCUA Scaling Open-Source Computer Use Agents with Cross-Platform   Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-19f877595158f8e9bd206c4ee7e85cb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029069&auth_key=1760029069-0-0-898edd11119f365ab5fe9bfb7594c753&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-20-更新"><a href="#2025-09-20-更新" class="headerlink" title="2025-09-20 更新"></a>2025-09-20 更新</h1><h2 id="ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data"><a href="#ScaleCUA-Scaling-Open-Source-Computer-Use-Agents-with-Cross-Platform-Data" class="headerlink" title="ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform   Data"></a>ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform   Data</h2><p><strong>Authors:Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang</strong></p>
<p>Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: <a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/ScaleCUA">https://github.com/OpenGVLab/ScaleCUA</a>. </p>
<blockquote>
<p>视觉语言模型（VLMs）已经能够让计算机使用代理（CUAs）自主地操作图形用户界面（GUI），显示出巨大的潜力。然而，由于缺乏大规模、开源的计算机使用数据和基础模型，进展受到限制。在这项工作中，我们引入了ScaleCUA，这是朝着扩大开源计算机使用代理规模迈出的一步。它提供了一个跨越6个操作系统和3个任务领域的大规模数据集，通过联合自动化代理和人类专家建立了一个闭环管道。在扩大规模的数据集上进行训练后，ScaleCUA可以无缝地跨平台运行。具体来说，它在基准测试上取得了显著的提升（+26.6在WebArena-Lite-v2，+10.7在ScreenSpot-Pro），并创下了最新的最佳结果（MMBench-GUI L1-Hard达到94.4%，OSWorld-G达到60.6%，WebArena-Lite-v2达到47.4%）。这些发现强调了数据驱动规模化对于通用计算机使用代理的威力。我们将公开数据、模型和代码以促进未来研究：<a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/ScaleCUA%E3%80%82">https://github.com/OpenGVLab/ScaleCUA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了ScaleCUA项目，该项目旨在推动开源计算机使用代理（CUAs）的发展。通过构建大规模数据集，跨越6个操作系统和3个任务域，并借助自动化代理与专家之间的闭环管道进行训练，ScaleCUA实现了跨平台无缝操作。该项目在多个基准测试上取得了显著成果，包括WebArena-Lite-v2、ScreenSpot-Pro等，展示了数据驱动计算机使用代理的潜力。该项目数据、模型和代码已公开发布，以便推进未来研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ScaleCUA项目旨在推动开源计算机使用代理（CUAs）的发展，解决其面临的挑战。</li>
<li>项目构建了一个大规模数据集，跨越六个操作系统和三个任务域。</li>
<li>通过闭环管道结合自动化代理和专家进行数据收集和处理。</li>
<li>ScaleCUA可以在多个平台上无缝操作，实现了计算机使用代理的跨平台应用。</li>
<li>该项目在多个基准测试上取得了显著成果，包括WebArena-Lite-v2、ScreenSpot-Pro等测试。</li>
<li>数据驱动的方法对于计算机使用代理的发展至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-19f877595158f8e9bd206c4ee7e85cb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029077&auth_key=1760029077-0-0-3e7cd63f3a9d608317768b4b1e0feb0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a7191a21fd9717492ac1ac72126d2a87~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029084&auth_key=1760029084-0-0-825f865f8478a67c3e24a01b876d03de&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-376c5c640deb70995911d9657871a4bf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029091&auth_key=1760029091-0-0-e88388ffcf7c5a9fd7f0837dc6247a53&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Sentinel-Agents-for-Secure-and-Trustworthy-Agentic-AI-in-Multi-Agent-Systems"><a href="#Sentinel-Agents-for-Secure-and-Trustworthy-Agentic-AI-in-Multi-Agent-Systems" class="headerlink" title="Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent   Systems"></a>Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent   Systems</h2><p><strong>Authors:Diego Gosmar, Deborah A. Dahl</strong></p>
<p>This paper proposes a novel architectural framework aimed at enhancing security and reliability in multi-agent systems (MAS). A central component of this framework is a network of Sentinel Agents, functioning as a distributed security layer that integrates techniques such as semantic analysis via large language models (LLMs), behavioral analytics, retrieval-augmented verification, and cross-agent anomaly detection. Such agents can potentially oversee inter-agent communications, identify potential threats, enforce privacy and access controls, and maintain comprehensive audit records. Complementary to the idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator Agent supervises policy implementation, and manages agent participation. In addition, the Coordinator also ingests alerts from Sentinel Agents. Based on these alerts, it can adapt policies, isolate or quarantine misbehaving agents, and contain threats to maintain the integrity of the MAS ecosystem. This dual-layered security approach, combining the continuous monitoring of Sentinel Agents with the governance functions of Coordinator Agents, supports dynamic and adaptive defense mechanisms against a range of threats, including prompt injection, collusive agent behavior, hallucinations generated by LLMs, privacy breaches, and coordinated multi-agent attacks. In addition to the architectural design, we present a simulation study where 162 synthetic attacks of different families (prompt injection, hallucination, and data exfiltration) were injected into a multi-agent conversational environment. The Sentinel Agents successfully detected the attack attempts, confirming the practical feasibility of the proposed monitoring approach. The framework also offers enhanced system observability, supports regulatory compliance, and enables policy evolution over time. </p>
<blockquote>
<p>本文提出了一种旨在提高多智能体系统（MAS）安全性和可靠性的新型架构框架。该框架的核心组成部分是一个哨兵智能体网络，充当分布式安全层，集成了通过大型语言模型（LLM）进行的语义分析、行为分析、检索增强验证和跨智能体异常检测等技术。这些智能体可以监督智能体之间的通信，识别潜在威胁，实施隐私和访问控制，并保持全面的审计记录。与哨兵智能体的理念相补充的是协调员智能体的使用。协调员智能体负责监督策略实施和管理智能体参与。此外，协调员还接收来自哨兵智能体的警报。基于这些警报，它可以适应策略，隔离或检疫表现不当的智能体，并遏制威胁以保持MAS生态系统的完整性。这种双层安全方法结合了哨兵智能体的持续监控和协调员智能体的治理功能，为对抗各种威胁提供了动态和自适应的防御机制，包括即时注入、勾结的智能体行为、LLM产生的幻觉、隐私泄露和协同多智能体攻击。除了架构设计外，我们还进行了一项模拟研究，向多智能体会话环境中注入了162个不同家族的合成攻击（提示注入、幻觉和数据泄露）。哨兵智能体成功检测到了攻击尝试，证实了所提议的监控方法的实际可行性。该框架还提供了增强的系统可观察性，支持法规合规性，并随时间支持策略演变。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14956v1">PDF</a> 25 pages, 12 figures</p>
<p><strong>Summary</strong><br>     本文提出一种新型架构框架，旨在增强多智能体系统（MAS）的安全性和可靠性。该框架的核心是哨兵智能体网络，作为分布式安全层，集成了大型语言模型（LLM）的语义分析、行为分析、检索增强验证和跨智能体异常检测等技术。哨兵智能体能监督智能体间的通信，识别潜在威胁，执行隐私和访问控制，并保持全面的审计记录。协调智能体负责监督策略实施和管理智能体参与，它能根据哨兵智能体的警报调整策略、隔离行为不当的智能体并遏制威胁，以维护MAS生态系统的完整性。仿真研究表明，哨兵智能体能成功检测攻击尝试，证明了监测方法实用性。该框架还提高了系统可观性、支持法规合规性并实现了策略的随时间演变。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种新型的多智能体系统架构框架，旨在增强安全性和可靠性。</li>
<li>架构中包含哨兵智能体和协调智能体，分别负责分布式安全监控和策略管理。</li>
<li>哨兵智能体集成多种技术如语义分析、行为分析、检索增强验证和异常检测等。</li>
<li>哨兵智能体可以监督通信、识别威胁、执行隐私控制和维持审计记录。</li>
<li>协调智能体负责策略实施和管理参与，能基于哨兵智能体的警报作出响应。</li>
<li>架构通过仿真研究得到了验证，展示了哨兵智能体检测攻击尝试的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14956">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-849c5b6a178449af35638c5a03ac5b3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029099&auth_key=1760029099-0-0-1c9dbee9ae71ab5b14a13446fe9deeea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f28dac5333226542364df38cd1e8de7a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029107&auth_key=1760029107-0-0-2caa7f5432981fbbe9610b8583c2e464&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f2df1b1167a98ad4189acc6528683fa0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029114&auth_key=1760029114-0-0-6ea548288effebeaebb3db9c082d8796&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7fdf6fef8f26e11aa43d7aa68d56ca56~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029120&auth_key=1760029120-0-0-b7392f5fdce73204c71e02d28f1afd6c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MARIC-Multi-Agent-Reasoning-for-Image-Classification"><a href="#MARIC-Multi-Agent-Reasoning-for-Image-Classification" class="headerlink" title="MARIC: Multi-Agent Reasoning for Image Classification"></a>MARIC: Multi-Agent Reasoning for Image Classification</h2><p><strong>Authors:Wonduk Seo, Minhyeong Yu, Hyunjin An, Seunghyun Lee</strong></p>
<p>Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification. </p>
<blockquote>
<p>图像分类传统上依赖于参数密集型的模型训练，需要大规模标注数据集和大量的微调才能达到竞争性能。虽然最近的视觉语言模型（VLM）缓解了一些这些约束，但它们仍然受限于对单通道表示的依赖，往往无法捕获视觉内容的互补方面。在本文中，我们介绍了用于图像分类的多代理推理（MARIC），这是一个多代理框架，它重新定义了图像分类作为一个协作推理过程。MARIC首先利用Outline Agent分析图像的整体主题并生成有针对性的提示。基于这些提示，三个Aspect Agents沿着不同的视觉维度提取精细的描述。最后，一个Reasoning Agent通过集成反射步骤合成这些互补输出，产生一个用于分类的统一表示。通过明确地将任务分解为多个角度并鼓励反思合成，MARIC缓解了参数繁重的训练和单一VLM推理的缺点。在四个不同的图像分类基准数据集上的实验表明，MARIC显著优于基线，突显了多代理视觉推理在稳健和可解释的图像分类中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14860v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>图像分类传统上依赖于参数密集型的模型训练，需要大规模标注数据集和大量的微调才能获得竞争力。最近出现的视觉语言模型（VLMs）虽然减轻了这些约束，但仍受限于单一传递表示，往往无法捕捉视觉内容的互补方面。本文提出基于多智能体的图像分类推理（MARIC），这是一个多智能体框架，将图像分类重新定义为协作推理过程。MARIC首先利用概述智能体分析图像的整体主题并生成有针对性的提示。基于这些提示，三个方面智能体会提取沿不同视觉维度的精细描述。最后，一个推理智能体通过综合反思步骤合成这些互补输出，产生用于分类的统一表示。通过显式地从多个角度分解任务并鼓励反思合成，MARIC缓解了参数密集型训练和单一VLM推理的缺点。在四个多样化的图像分类基准数据集上的实验表明，MARIC显著优于基准测试，突显了多智能体视觉推理在稳健和可解释图像分类中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像分类传统上依赖于大规模标注数据集和参数密集型的模型训练。</li>
<li>视觉语言模型（VLMs）虽然能减轻约束，但仍受限于单一传递表示。</li>
<li>MARIC是一个多智能体框架，将图像分类定义为协作推理过程。</li>
<li>MARIC包含多个智能体阶段，从概述到具体的细节提取，再到最终的推理合成。</li>
<li>MARIC通过分解任务并鼓励多视角的反思合成来克服传统方法和单一VLM的缺点。</li>
<li>在四个基准数据集上的实验表明，MARIC显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14860">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-70a3906cedcea38606b2bdaac5eea889~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029128&auth_key=1760029128-0-0-60501cd020b0a98d0940ea7c991e5c4a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d60cd00b7c0cf3a49fd0a4c04d1a6b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029136&auth_key=1760029136-0-0-64bafc721967608a3fc741f55386c2ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb2cec4ecec897061ca975f88146e493~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029143&auth_key=1760029143-0-0-e8d32fe8502a21928f339784f93a2e90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-208b4409e3f0d30e14e922b497c94b50~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029149&auth_key=1760029149-0-0-e3f254be3c27533a84d3efa32c815ac8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-96ae79a2a109032037d877b95daf4702~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029156&auth_key=1760029156-0-0-3b4128049452e90c3ee05d0a6b1203a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee4c502a4000985bfeeb2d4ee3ff73ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029162&auth_key=1760029162-0-0-8dc16b3de3ff9b3425e2e7f1529be78d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-Agents-at-the-Roundtable-A-Multi-Perspective-and-Dialectical-Reasoning-Framework-for-Essay-Scoring"><a href="#LLM-Agents-at-the-Roundtable-A-Multi-Perspective-and-Dialectical-Reasoning-Framework-for-Essay-Scoring" class="headerlink" title="LLM Agents at the Roundtable: A Multi-Perspective and Dialectical   Reasoning Framework for Essay Scoring"></a>LLM Agents at the Roundtable: A Multi-Perspective and Dialectical   Reasoning Framework for Essay Scoring</h2><p><strong>Authors:Jinhee Jang, Ayoung Moon, Minkyoung Jung, YoungBin Kim. Seung Jin Lee</strong></p>
<p>The emergence of large language models (LLMs) has brought a new paradigm to automated essay scoring (AES), a long-standing and practical application of natural language processing in education. However, achieving human-level multi-perspective understanding and judgment remains a challenge. In this work, we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework designed to perform precise and human-aligned scoring under a zero-shot setting. RES constructs evaluator agents based on LLMs, each tailored to a specific prompt and topic context. Each agent independently generates a trait-based rubric and conducts a multi-perspective evaluation. Then, by simulating a roundtable-style discussion, RES consolidates individual evaluations through a dialectical reasoning process to produce a final holistic score that more closely aligns with human evaluation. By enabling collaboration and consensus among agents with diverse evaluation perspectives, RES outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in average QWK over straightforward prompting (Vanilla) methods. </p>
<blockquote>
<p>大型语言模型（LLM）的出现为自动作文评分（AES）带来了新的范式，这是自然语言处理在教育领域的一项长期且实用的应用。然而，实现人类水平的多角度理解和判断仍然是一个挑战。在这项工作中，我们提出了圆桌作文评分（RES），这是一个多代理评估框架，旨在在无样本设置下执行精确且符合人类评分标准的评分。RES基于LLM构建评估代理，每个代理都针对特定的提示和主题上下文进行定制。每个代理独立生成基于特征的评分标准，并进行多角度评估。然后，通过模拟圆桌式讨论，RES通过辩证推理过程整合个人评估，以产生最终的整体评分，该评分更接近于人类评估。通过促进拥有不同评估角度的代理之间的协作和共识，RES超越了之前的零样本AES方法。在ASAP数据集上进行的ChatGPT和Claude实验表明，RES相对于简单的提示（Vanilla）方法平均QWK提高了高达34.86%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14834v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的出现为自动评分（AES）带来了新的模式，但仍存在实现人类多视角理解和判断的挑战。本研究提出圆桌作文评分（RES）系统，这是一个多代理评估框架，旨在在零样本设置下进行精确和人类对齐评分。通过模拟圆桌讨论，RES 整合各个代理的评估结果，生成更贴近人类评估的最终整体评分。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在自动作文评分（AES）中引发新变革。</li>
<li>圆桌作文评分（RES）系统是一个多代理评估框架，模拟人类评价的多视角。</li>
<li>RES 在零样本设置下实现精确和人类对齐的评分。</li>
<li>RES 通过构建针对特定提示和主题上下文的评价代理，进行独立评分和多视角评估。</li>
<li>通过模拟圆桌讨论，RES 整合个体评价，通过辩证推理过程生成最终整体评分。</li>
<li>RES 允许不同评价视角的代理之间的协作和共识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14834">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-054dc30238437642e4e8cb1d07462411~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029169&auth_key=1760029169-0-0-0082a518fe4e52435234db6ff71b069a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2651d5be7297f422c3862bd3a55689d5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029176&auth_key=1760029176-0-0-c68067f9f0d52a2061d196a958e117c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cd1050bc31677d0f709cef62421a9db7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029182&auth_key=1760029182-0-0-c1c625fc8096b14157a3969372cdab47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b76da96530addb88fd2fdedfb8736c7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029189&auth_key=1760029189-0-0-848c3da1126f212ac5342a3d52ea5395&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2901a752f853e2fd8cee4e905f96cf3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029195&auth_key=1760029195-0-0-51f18a73ec36721c6bbdc3bad7809528&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d05340560f9a8db4ae98fb02a0344104~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029201&auth_key=1760029201-0-0-0180f71dea54431e6a611dfa4e2801c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LEED-A-Highly-Efficient-and-Scalable-LLM-Empowered-Expert-Demonstrations-Framework-for-Multi-Agent-Reinforcement-Learning"><a href="#LEED-A-Highly-Efficient-and-Scalable-LLM-Empowered-Expert-Demonstrations-Framework-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="LEED: A Highly Efficient and Scalable LLM-Empowered Expert   Demonstrations Framework for Multi-Agent Reinforcement Learning"></a>LEED: A Highly Efficient and Scalable LLM-Empowered Expert   Demonstrations Framework for Multi-Agent Reinforcement Learning</h2><p><strong>Authors:Tianyang Duan, Zongyuan Zhang, Songxiao Guo, Dong Huang, Yuanye Zhao, Zheng Lin, Zihan Fang, Dianxin Luan, Heming Cui, Yong Cui</strong></p>
<p>Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines. </p>
<blockquote>
<p>多智能体强化学习（MARL）在复杂环境中的智能决策方面具有巨大潜力。然而，随着智能体的数量增加，它面临着协调和可扩展性的瓶颈。为了解决这些问题，我们提出了基于大模型的专家演示框架的多智能体强化学习（LEED）。LEED由两个组件构成：演示生成模块和政策优化模块。具体来说，演示生成模块利用大型语言模型生成与环境交互的指令，从而生成高质量的演示。政策优化模块采用分散训练范式，每个智能体利用生成的演示来构建专家政策损失，然后将其与自己的政策损失结合。这可以使每个智能体基于专家知识和个人经验有效地个性化并优化其本地策略。实验结果表明，与最先进的基线相比，LEED在样本效率、时间效率和稳健的扩展性方面达到了优越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14680v1">PDF</a> 5 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>多智能体强化学习（MARL）在复杂环境中进行智能决策具有巨大潜力，但随着智能体数量的增加，协调性和可扩展性成为瓶颈。为解决这些问题，我们提出了基于大型语言模型的专家演示框架（LEED），包括演示生成（DG）模块和政策优化（PO）模块。DG模块利用大型语言模型生成与环境交互的指令，从而产生高质量的演示。PO模块采用分散式训练范式，每个智能体利用生成的演示构建专家政策损失，然后将其与自己的政策损失相结合。这使得每个智能体都能够根据专家知识和个人经验有效地定制和优化本地策略。实验结果表明，与现有先进技术相比，LEED在样本效率、时间效率和稳健的扩展性方面表现更优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体强化学习（MARL）在复杂环境中展现智能决策潜力，但需解决协调与可扩展性问题。</li>
<li>提出的LEED框架包括演示生成（DG）模块和政策优化（PO）模块，旨在改进MARL的性能。</li>
<li>DG模块利用大型语言模型生成与环境交互的指令，产生高质量演示。</li>
<li>PO模块采用分散式训练，使每个智能体能结合专家知识和个人经验优化本地策略。</li>
<li>LEED框架在样本效率、时间效率和扩展性方面表现优越。</li>
<li>该框架有助于实现个性化策略优化，提高智能决策的质量和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14680">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e9ba2fd61cb2bdcb394c7fb999285aa0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029209&auth_key=1760029209-0-0-2627145fa288eee40a2b9fe45621b93e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d23d03530be0cde46989be948e1f5bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029217&auth_key=1760029217-0-0-77595aff73a7c7895df6bedf0f13484a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-01755d27d39af66b05ab1660586cc322~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029223&auth_key=1760029223-0-0-aede8832ad58b14f24801b9e0da9da2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8ab20a58e512cbc19d8947ac94a8bf84~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029234&auth_key=1760029234-0-0-b7858cdc13623a45943dcb70b3ae3b5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c9bb14bc5ba176d6fa98a3c3234a9606~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029240&auth_key=1760029240-0-0-110ded69c036cf2f10ab977a65da25e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AgentCompass-Towards-Reliable-Evaluation-of-Agentic-Workflows-in-Production"><a href="#AgentCompass-Towards-Reliable-Evaluation-of-Agentic-Workflows-in-Production" class="headerlink" title="AgentCompass: Towards Reliable Evaluation of Agentic Workflows in   Production"></a>AgentCompass: Towards Reliable Evaluation of Agentic Workflows in   Production</h2><p><strong>Authors:NVJK Kartik, Garvit Sapra, Rishav Hada, Nikhil Pareek</strong></p>
<p>With the growing adoption of Large Language Models (LLMs) in automating complex, multi-agent workflows, organizations face mounting risks from errors, emergent behaviors, and systemic failures that current evaluation methods fail to capture. We present AgentCompass, the first evaluation framework designed specifically for post-deployment monitoring and debugging of agentic workflows. AgentCompass models the reasoning process of expert debuggers through a structured, multi-stage analytical pipeline: error identification and categorization, thematic clustering, quantitative scoring, and strategic summarization. The framework is further enhanced with a dual memory system-episodic and semantic-that enables continual learning across executions. Through collaborations with design partners, we demonstrate the framework’s practical utility on real-world deployments, before establishing its efficacy against the publicly available TRAIL benchmark. AgentCompass achieves state-of-the-art results on key metrics, while uncovering critical issues missed in human annotations, underscoring its role as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production. </p>
<blockquote>
<p>随着大型语言模型（LLMs）在自动化复杂多代理工作流程中的日益普及，组织面临着越来越多的风险，包括错误、新兴行为以及当前评估方法无法捕获的系统故障。我们推出了AgentCompass，这是专门为代理工作流程的后期监控和调试设计的第一个评估框架。AgentCompass通过结构化、多阶段的分析管道模拟专家调试器的推理过程：错误识别和分类、主题聚类、定量评分和战略总结。该框架还采用双内存系统（即情节记忆和语义记忆），以实现跨执行的持续学习。通过与设计合作伙伴的合作，我们在实际部署中展示了该框架的实际效用，然后在公开的TRAIL基准测试上证明了其有效性。AgentCompass在关键指标上达到了最新的最佳水平结果，同时发现了被人为注释所忽略的关键问题，凸显了其在生产环境中可靠监控和改进代理系统的稳健性和以开发者为中心的工具的重要作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14647v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型（LLMs）在自动化复杂多智能体工作流中的广泛应用，组织面临着日益增长的错误、新兴行为和系统性失败的风险，而现有的评估方法无法捕捉到这些风险。为此，我们推出了AgentCompass，这是专门为智能体工作流的后期监控和调试设计的评估框架。AgentCompass通过结构化、多阶段的管道模拟专家调试过程，包括错误识别和分类、主题聚类、定量评分和战略总结。该框架还采用双内存系统——情景记忆和语义记忆，以实现跨执行的持续学习。通过与设计合作伙伴的合作，我们在实际部署中展示了该框架的实际效用，并通过与公开可用的TRAIL基准测试对比，证明了其有效性。AgentCompass在关键指标上达到了最新水平，并发现了人类注释中遗漏的关键问题，突显了其在生产环境中可靠监控和改进智能体系统方面的强大作用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在多智能体工作流中的广泛应用带来了错误、新兴行为和系统性失败的风险。</li>
<li>当前评估方法无法有效捕捉这些风险。</li>
<li>AgentCompass是专门为智能体工作流的后期监控和调试设计的评估框架。</li>
<li>AgentCompass包括一个结构化、多阶段的管道模拟专家调试过程，用于错误识别、分类、主题聚类等。</li>
<li>该框架采用了双内存系统——情景记忆和语义记忆，支持跨执行的持续学习。</li>
<li>在实际部署中展示了AgentCompass的实际效用和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14647">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-2058d689729660e37ee22138b982b223~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029249&auth_key=1760029249-0-0-9126b7167b6c5f98df317f220d2b5ac7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1684bb75f8a14c51ad14ca59dc00880c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029256&auth_key=1760029256-0-0-7e39a47080a682252a8378480f63291b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-beda181668c7d0c0aeaa40a6c8816347~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029264&auth_key=1760029264-0-0-aad5a7b6f0af1c5f596edb7359064622&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Human-like-Multimodal-Conversational-Agent-by-Generating-Engaging-Speech"><a href="#Towards-Human-like-Multimodal-Conversational-Agent-by-Generating-Engaging-Speech" class="headerlink" title="Towards Human-like Multimodal Conversational Agent by Generating   Engaging Speech"></a>Towards Human-like Multimodal Conversational Agent by Generating   Engaging Speech</h2><p><strong>Authors:Taesoo Kim, Yongsik Jo, Hyunmin Song, Taehwan Kim</strong></p>
<p>Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in <a target="_blank" rel="noopener" href="https://github.com/kimtaesu24/MSenC">https://github.com/kimtaesu24/MSenC</a> </p>
<blockquote>
<p>人类对话涉及语言、语音和视觉线索，每种媒介都提供互补信息。例如，语音传达了一种氛围或语气，这是仅凭文本无法完全捕捉到的。虽然多模态大型语言模型专注于从各种输入生成文本响应，但对于生成自然且引人入胜的语音的关注度较低。我们提出了一种基于对话情绪和响应风格信息生成语音响应的人形代理。为了实现这一目标，我们构建了一个全新的MultiSensory Conversation数据集，专注于语音，以让代理能够生成自然语音。然后，我们提出了一个基于多模态大型语言模型的文本响应和语音描述生成模型，用于生成覆盖副语言信息的语音。实验结果表明，在对话中利用视觉和音频模式生成引人入胜的语音是有效的。[源代码位于<a target="_blank" rel="noopener" href="https://github.com/kimtaesu24/MSenC]%EF%BC%88%E6%B3%A8%EF%BC%9A%E7%94%B1%E4%BA%8E%E5%AD%98%E5%9C%A8%E7%89%B9%E5%AE%9A%E7%9A%84%E6%8A%80%E6%9C%AF%E6%9C%AF%E8%AF%AD%E5%92%8C%E7%BB%86%E8%8A%82%EF%BC%8C%E8%AF%91%E6%96%87%E5%8F%AF%E8%83%BD%E9%9C%80%E8%A6%81%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E8%B0%83%E6%95%B4%E5%92%8C%E5%AE%8C%E5%96%84%E3%80%82%EF%BC%89">https://github.com/kimtaesu24/MSenC]（注：由于存在特定的技术术语和细节，译文可能需要进一步的调整和完善。）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14627v1">PDF</a> Published in Interspeech 2025</p>
<p><strong>Summary</strong>：<br>人类对话涉及语言、语音和视觉线索，每种媒介提供互补信息。现有的多模态大型语言模型主要关注根据多种输入生成文本响应，但在生成自然流畅对话的语音方面关注较少。本文提出了一种基于对话情绪和响应风格信息生成语音响应的人形代理。为此，建立了一个名为MultiSensory Conversation的新数据集，以语音为核心，帮助代理生成自然流畅的语音。并提出了基于多模态大型语言模型的响应模型，该模型用于生成涵盖非语言信息的语音描述。实验结果显示利用视觉和音频模态在对话中生成吸引人语音的有效性。相关源代码已上传至GitHub上供公开使用。代码地址为：<a target="_blank" rel="noopener" href="https://github.com/kimtaesu24/MSenC">https://github.com/kimtaesu24/MSenC</a>。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>人对话涉及语言、语音和视觉线索，三者提供互补信息，共同构建完整的交流体验。</li>
<li>当前的多模态大型语言模型更多地关注于根据多种输入生成文本响应，而非自然流畅的语音生成。</li>
<li>研究提出了一种新型的人形代理技术，该技术能够根据对话情绪和响应风格信息生成语音响应。</li>
<li>为实现该技术，建立了名为MultiSensory Conversation的新数据集，专注于语音方面，以模拟真实对话场景。</li>
<li>该模型通过生成涵盖非语言信息的语音描述来提高语音的自然度和流畅性。</li>
<li>实验结果证明了在对话系统中结合视觉和音频模态的有效性，能够提高生成的语音的吸引力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-769e56e37f44e40ba84ee8c23aff5bf5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029272&auth_key=1760029272-0-0-e53fd94c2eecb0f7c45ce8bc66aead72&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bee3ba27063d267ebd8405c3b255b19c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029279&auth_key=1760029279-0-0-1c46cb50f1a5a1f2ac96a93db2354d62&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb3c8376704d6cadc4e8f4e4453ffba7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029286&auth_key=1760029286-0-0-dd27b3f3899b697d76b4c88dd37386b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-da5e9a3a741847dda9c3bfb0d0a5eb3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029328&auth_key=1760029328-0-0-971086ff975518f87c71109b71d16175&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-28c5a050108ae047c42f6cc08fb313b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029334&auth_key=1760029334-0-0-68945a9b6cda917eac39d40ffb55cc00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-425cb9eb0a0b82238033dd4e5aa2d47f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029341&auth_key=1760029341-0-0-95c1cfa5aa7e9f120de65669a8f11acb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c547b4e12b5fbbd1a9ec6119a171f2c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029347&auth_key=1760029347-0-0-4f299567e0032837f3ec4a71a565ad5b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-30b49e84161490e21b7dc5da3df8e71c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029353&auth_key=1760029353-0-0-ca6011bfae0b0c4c1c92158ce831ee31&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="P-rior-D-yna-F-low-A-Priori-Dynamic-Workflow-Construction-via-Multi-Agent-Collaboration"><a href="#P-rior-D-yna-F-low-A-Priori-Dynamic-Workflow-Construction-via-Multi-Agent-Collaboration" class="headerlink" title="(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via   Multi-Agent Collaboration"></a>(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via   Multi-Agent Collaboration</h2><p><strong>Authors:Yi Lin, Lujin Zhao, Yijie Shi</strong></p>
<p>Recent studies have shown that carefully designed workflows coordinating large language models(LLMs) significantly enhance task-solving capabilities compared to using a single model. While an increasing number of works focus on autonomous workflow construction, most existing approaches rely solely on historical experience, leading to limitations in efficiency and adaptability. We argue that while historical experience is valuable, workflow construction should also flexibly respond to the unique characteristics of each task. To this end, we propose an a priori dynamic framework for automated workflow construction. Our framework first leverages Q-table learning to optimize the decision space, guiding agent decisions and enabling effective use of historical experience. At the same time, agents evaluate the current task progress and make a priori decisions regarding the next executing agent, allowing the system to proactively select the more suitable workflow structure for each given task. Additionally, we incorporate mechanisms such as cold-start initialization, early stopping, and pruning to further improve system efficiency. Experimental evaluations on four benchmark datasets demonstrate the feasibility and effectiveness of our approach. Compared to state-of-the-art baselines, our method achieves an average improvement of 4.05%, while reducing workflow construction and inference costs to only 30.68%-48.31% of those required by existing methods. </p>
<blockquote>
<p>最近的研究表明，精心设计的工作流，通过协调大型语言模型（LLMs），相比使用单一模型，能显著提升任务解决能力。尽管越来越多的工作聚焦于自主工作流构建，但大多数现有方法仅依赖于历史经验，导致效率和适应性的局限。我们认为，虽然历史经验很有价值，但工作流构建还应灵活应对每项任务的独特特性。为此，我们提出了一种先验动态框架，用于自动化工作流构建。我们的框架首先利用Q表学习来优化决策空间，指导代理决策，并实现对历史经验的有效利用。同时，代理会评估当前任务进度，并根据先验做出关于下一个执行代理的决策，使系统能够主动为每项给定任务选择更合适的工作流结构。此外，我们还引入了冷启动初始化、早期停止和修剪等机制，以进一步提高系统效率。在四个基准数据集上的实验评估证明了我们方法的可行性和有效性。与最新基线相比，我们的方法在平均改进了4.05%的同时，将工作流构建和推理成本降低至现有方法的仅30.68%-48.31%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14547v1">PDF</a> </p>
<p><strong>Summary</strong>：近期研究表明，通过精心设计的工作流协调大型语言模型（LLMs），比单一模型更能提升任务解决能力。尽管自主工作流构建的研究日益增多，但大多数现有方法仅依赖历史经验，导致效率和适应性受限。为此，本文提出了一种先验动态框架用于自动化工作流构建。该框架利用Q-table学习优化决策空间，指导代理决策并有效利用历史经验。同时，代理评估当前任务进度并预先决定下一个执行代理，使系统能够为每个给定任务主动选择更合适的工作流结构。实验评估表明，该方法在四个基准数据集上表现出可行性和有效性，与最新基线相比，平均提高了4.05%，同时将工作流构建和推理成本降低了30.68%-48.31%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型的协调工作流可显著提升任务解决能力。</li>
<li>现有自主工作流构建方法主要依赖历史经验，存在效率和适应性局限。</li>
<li>提出了先验动态框架用于自动化工作流构建，结合Q-table学习与任务进度评估。</li>
<li>框架能指导代理决策、有效利用历史经验，并预先决定下一个执行代理。</li>
<li>引入冷启动初始化、早期停止和修剪机制进一步提高系统效率。</li>
<li>在四个基准数据集上的实验评估证明了该方法的可行性和有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14547">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-507e609524827be4d715d5e851be6e27~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029361&auth_key=1760029361-0-0-58abccf613f2e35c09db62b6cdc38fd2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-268cefbc7926591f988402b386e9ca22~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029368&auth_key=1760029368-0-0-bb9b561bdd9dc30ef190cbec5ae171e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4f49e088125dc47a5c87174917dd0970~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029375&auth_key=1760029375-0-0-15d487dd36352b52e13a0bd11bd4c35e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b88585f80c1bb136bf1e89b69d19f8c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029382&auth_key=1760029382-0-0-37001e9e1aad7c0d4eba6168cb6add85&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-698bb403a6cd87767835f729df31380c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029389&auth_key=1760029389-0-0-28307ab81dda8c72af8521fd1e18f6ab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MACO-A-Multi-Agent-LLM-Based-Hardware-Software-Co-Design-Framework-for-CGRAs"><a href="#MACO-A-Multi-Agent-LLM-Based-Hardware-Software-Co-Design-Framework-for-CGRAs" class="headerlink" title="MACO: A Multi-Agent LLM-Based Hardware&#x2F;Software Co-Design Framework for   CGRAs"></a>MACO: A Multi-Agent LLM-Based Hardware&#x2F;Software Co-Design Framework for   CGRAs</h2><p><strong>Authors:Zesong Jiang, Yuqi Sun, Qing Zhong, Mahathi Krishna, Deepak Patil, Cheng Tan, Sriram Krishnamoorthy, Jeff Zhang</strong></p>
<p>Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing architecture that can deliver high-performance, energy-efficient acceleration across diverse domains. By supporting reconfiguration at the functional unit level, CGRAs efficiently adapt to varying computational patterns and optimize resource utilization. However, designing CGRAs is highly challenging due to the vast design space, independent architectural parameters, and the time-consuming nature of manual design. Fortunately, the rapid advancement of large language models (LLMs) presents new opportunities to automate this process.   In this work, we propose MACO – an open-source multi-agent LLM-based framework for Hardware&#x2F;Software (HW&#x2F;SW) co-design of CGRAs. The framework employs LLM reasoning to generate CGRAs across four stages: HW&#x2F;SW co-design, Design error correction, Best design selection, and Evaluation &amp; Feedback. Furthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent reasoning and feedback to achieve higher PPA (that is, power, performance, and area) design points for a given domain. In addition, we introduce an LLM self-learning mechanism that employs LLM-driven decision making to select the optimal CGRA to accelerate the design process.   We evaluate the framework with state-of-the-art LLM-based methods and manual CGRA design, in terms of performance, power consumption, and area. Experimental results show that MACO efficiently generates high-quality CGRA architectures, significantly reducing manual design effort and demonstrating the potential of our framework for real-world CGRA design. </p>
<blockquote>
<p>粗粒度可重构阵列（CGRAs）是一种有前景的计算架构，可以在不同的领域提供高性能、能源效率的加速。通过支持功能单元级的重构，CGRAs能够高效地适应不同的计算模式，并优化资源利用。然而，设计CGRAs是一项极具挑战性的任务，因为设计空间大、独立的架构参数以及手动设计的耗时性质。幸运的是，大型语言模型（LLM）的快速发展为自动化这一过程提供了新的机会。</p>
</blockquote>
<p>在这项工作中，我们提出了MACO——一个开源的、基于多代理的大型语言模型的硬件&#x2F;软件（HW&#x2F;SW）协同设计CGRAs的框架。该框架采用大型语言模型的推理功能，在四个阶段生成CGRAs：HW&#x2F;SW协同设计、设计错误校正、最佳设计选择、评估与反馈。此外，MACO通过利用代理推理和反馈来迭代优化生成的CGRAs，以实现给定领域更高的PPA（即功率、性能和面积）设计点。此外，我们还引入了一种大型语言模型自我学习机制，采用大型语言模型驱动的决策制定来选择最佳CGRA来加速设计过程。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13557v2">PDF</a> Due to certain confidentiality requirements, this article needs to be   withdrawn</p>
<p><strong>Summary</strong></p>
<p>基于粗粒度可重构阵列（CGRAs）的计算机架构是一种有前途的高性能、节能加速方案，适用于不同领域。CGRAs通过功能单元级别的重构来适应不同的计算模式并优化资源利用。然而，设计CGRAs面临巨大挑战，包括庞大的设计空间、独立的架构参数以及耗时的人工设计。本文提出一种利用大型语言模型（LLM）的开源多智能体框架MACO，用于CGRAs的软硬件协同设计。MACO通过LLM推理生成CGRAs，并在四个阶段进行优化：软硬件协同设计、设计错误校正、最佳设计选择和评估与反馈。此外，MACO引入LLM自我学习机制，采用LLM驱动决策选择最佳CGRA以加速设计过程。实验结果表明，MACO能够高效生成高质量的CGRA架构，显著减少人工设计工作量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CGRAs是一种具有高性能和能源效率的计算机架构，可通过功能单位级别的重构来适应不同的计算模式。</li>
<li>设计CGRAs具有挑战性，因为需要处理庞大的设计空间、独立的架构参数以及耗时的人工设计过程。</li>
<li>MACO是一个基于LLM的开源多智能体框架，用于CGRAs的软硬件协同设计。</li>
<li>MACO通过LLM推理生成CGRAs并在四个阶段进行优化：软硬件协同设计、设计错误校正、最佳设计选择以及评估与反馈。</li>
<li>MACO能够利用LLM自我学习机制来加速设计过程，并通过LLM驱动的决策选择最佳CGRA。</li>
<li>实验结果表明，MACO能够高效生成高质量的CGRA架构，显著减少人工设计的工作量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13557">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-352e1529d101736191a85752b7ab56c7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029396&auth_key=1760029396-0-0-217136a1e89a921f23eda79326c152b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-29188bb93d37be6a781190ad3c86546d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029404&auth_key=1760029404-0-0-0efed0f4b8d6c36bfd2f5e87aa338a80&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-24f378fcacd16a20cb53e24e302fcb4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029411&auth_key=1760029411-0-0-168dc31461f0d4c36c6a086fd16d71fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-93aab1e7b4f3e4811c6485890e50f539~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029418&auth_key=1760029418-0-0-f05a77a563fb9b58c8edca53d265491d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-398ad472d42ce34435392c29b3bd8bd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029425&auth_key=1760029425-0-0-661f47572d7654ba7aeffc02a0131903&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-17516edf5f77e4dd9885f81becb0da8f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029432&auth_key=1760029432-0-0-a43e782794e7566a7d014b0bf8b5e1a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa8040995280b2e3026034017fe0cd8c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029439&auth_key=1760029439-0-0-c4f9e290d1e39c21a709844d0e1617e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5fe80a4224d2d0dad27614c0a5c1b230~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029446&auth_key=1760029446-0-0-4bf4797bfa627c902bc38ea559a909d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e0ed247e25c8aa7c6c00460192a3042e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029452&auth_key=1760029452-0-0-5f453d6b77898e7132e63998529f8ce7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="T-araVLN-Translator-for-Agricultural-Robotic-Agents-on-Vision-and-Language-Navigation"><a href="#T-araVLN-Translator-for-Agricultural-Robotic-Agents-on-Vision-and-Language-Navigation" class="headerlink" title="T-araVLN: Translator for Agricultural Robotic Agents on   Vision-and-Language Navigation"></a>T-araVLN: Translator for Agricultural Robotic Agents on   Vision-and-Language Navigation</h2><p><strong>Authors:Xiaobei Zhao, Xingqi Lyu, Xiang Li</strong></p>
<p>Agricultural robotic agents have been becoming powerful helpers in a wide range of agricultural tasks, however, still heavily rely on manual operation or fixed railways for movement. To address this limitation, the AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling agents to navigate to the target positions following the natural language instructions. AgriVLN effectively understands the simple instructions, but often misunderstands the complex ones. To bridge this gap, we propose the method of Translator for Agricultural Robotic Agents on Vision-and-Language Navigation (T-araVLN), in which the Instruction Translator module translates the original instruction to be more refined and precise. When evaluated on the A2A benchmark, our T-araVLN effectively improves Success Rate from 0.47 to 0.63 and reduces Navigation Error from 2.91m to 2.28m, demonstrating the state-of-the-art performance in the agricultural domain. Code: <a target="_blank" rel="noopener" href="https://github.com/AlexTraveling/T-araVLN">https://github.com/AlexTraveling/T-araVLN</a>. </p>
<blockquote>
<p>农业机器人代理已经在广泛的农业任务中成为强大的助手，然而，仍然严重依赖于手动操作或固定铁路进行移动。为了解决这一局限性，AgriVLN方法和A2A基准率先将视觉和语言导航（VLN）扩展到农业领域，使代理能够按照自然语言指令导航到目标位置。AgriVLN能够有效地理解简单指令，但经常误解复杂指令。为了弥补这一差距，我们提出了农业机器人代理视觉和语言导航翻译方法（T-araVLN），其中的指令翻译模块将原始指令翻译为更精细和精确的语言。在A2A基准测试中评估时，我们的T-araVLN成功率从0.47提高到0.63，导航误差从2.91米减少到2.28米，显示出农业领域的最先进的性能。代码地址：<a target="_blank" rel="noopener" href="https://github.com/AlexTraveling/T-araVLN%E3%80%82">https://github.com/AlexTraveling/T-araVLN。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06644v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>农业机器人代理在农业领域的广泛应用中已成为强大的助手，然而仍严重依赖于手动操作或固定轨道移动。为解决此局限性，AgriVLN方法和A2A基准率先将视觉和语言导航（VLN）扩展到农业领域，使代理能够根据自然语言指令导航到目标位置。AgriVLN虽然能有效理解简单指令，但常常误解复杂指令。为弥补这一差距，我们提出了针对农业机器人代理的视觉和语言导航翻译方法（T-araVLN），其中的指令翻译模块将原始指令翻译为更精细和精确的语言。在A2A基准测试中，我们的T-araVLN成功率为0.63，提高了成功率和降低了导航误差至2.28米，展现了农业领域的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>农业机器人代理在多项农业任务中表现出强大的辅助能力，但仍存在对复杂指令的误解问题。</li>
<li>AgriVLN方法和A2A基准将视觉和语言导航（VLN）技术引入农业领域，使代理能够根据自然语言导航到目标位置。</li>
<li>T-araVLN方法通过引入指令翻译模块，提高了农业机器人代理对复杂指令的理解能力。</li>
<li>在A2A基准测试中，T-araVLN提高了成功率和降低了导航误差，展现出卓越性能。</li>
<li>T-araVLN方法适用于解决农业机器人代理在执行复杂指令时遇到的挑战。</li>
<li>此研究为农业机器人领域提供了一种新的、更精确的自然语言导航解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-540973a0aaee4cb1294fb523869c9dfa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029459&auth_key=1760029459-0-0-a86c6bdd5b60057fd3f96faa64c45ed1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5890358fba57179089ba11433d2d5818~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029467&auth_key=1760029467-0-0-99a60b147d9670d6659004fd7f43de7e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7719a85366b99542288c1207d4ecec74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029474&auth_key=1760029474-0-0-6a42977549c7a88399f5610d47879a94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c775969ad90eea11c479fb4e46f9377~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029481&auth_key=1760029481-0-0-2aea792de4a7e551a0450170ed78fde8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c8bb2fb38caca8891c5a2aa7a1e35b26~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029488&auth_key=1760029488-0-0-1524f589f5bdf665dcce3afad1edaf24&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="WebCoT-Enhancing-Web-Agent-Reasoning-by-Reconstructing-Chain-of-Thought-in-Reflection-Branching-and-Rollback"><a href="#WebCoT-Enhancing-Web-Agent-Reasoning-by-Reconstructing-Chain-of-Thought-in-Reflection-Branching-and-Rollback" class="headerlink" title="WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought   in Reflection, Branching, and Rollback"></a>WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought   in Reflection, Branching, and Rollback</h2><p><strong>Authors:Minda Hu, Tianqing Fang, Jianshu Zhang, Junyu Ma, Zhisong Zhang, Jingyan Zhou, Hongming Zhang, Haitao Mi, Dong Yu, Irwin King</strong></p>
<p>Web agents powered by Large Language Models (LLMs) show promise for next-generation AI, but their limited reasoning in uncertain, dynamic web environments hinders robust deployment. In this paper, we identify key reasoning skills essential for effective web agents, i.e., reflection &amp; lookahead, branching, and rollback, and curate trajectory data that exemplifies these abilities by reconstructing the agent’s (inference-time) reasoning algorithms into chain-of-thought rationales. We conduct experiments in the agent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling salient reasoning patterns into the backbone LLM via simple fine-tuning can substantially enhance its performance. Our approach yields significant improvements across multiple benchmarks, including WebVoyager, Mind2web-live, and SimpleQA (web search), highlighting the potential of targeted reasoning skill enhancement for web agents. </p>
<blockquote>
<p>基于大型语言模型（LLM）的Web代理对下一代人工智能显示出巨大的潜力，但它们在不确定、动态的Web环境中有限的推理能力阻碍了其稳健部署。在本文中，我们确定了有效Web代理所必需的关键推理技能，即反思与前瞻性、分支和回滚，并通过重建代理的（推理时间）推理算法来形成思维链的理由，从而例证这些能力。我们在代理自我改进基准测试OpenWebVoyager中进行了实验，并证明通过简单微调将突出的推理模式蒸馏到主干LLM中，可以极大地提高其性能。我们的方法在多基准测试中产生了显著的改进，包括WebVoyager、Mind2web-live和SimpleQA（网络搜索），突出了针对Web代理进行有针对性的推理技能增强的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20013v2">PDF</a> 18 pages</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型的网络代理展现出下一代人工智能的潜力，但在不确定、动态的网络环境中，其有限的推理能力限制了其稳健部署。本文确定了网络代理所需的关键推理技能，包括反思与前瞻性、分支和回滚，并通过重建代理（推理时间）的推理算法来例证这些能力，以思维链作为论据。通过OpenWebVoyager等基准测试实验表明，通过简单微调将显著推理模式蒸馏到基础大型语言模型中，可大幅提高其在多个基准测试中的性能表现，包括WebVoyager、Mind2web-live和SimpleQA（网络搜索）。这突显出针对网络代理进行有针对性的推理技能增强的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型驱动的网络代理展现出下一代AI的潜力。</li>
<li>在不确定、动态的网络环境中，网络代理的推理能力受到限制。</li>
<li>关键推理技能包括反思与前瞻性、分支和回滚。</li>
<li>通过重建代理的推理算法，可以例证这些关键推理技能。</li>
<li>在基准测试（如OpenWebVoyager）中验证了通过简单微调将显著推理模式蒸馏到基础大型语言模型中的方法。</li>
<li>该方法显著提高了网络代理在多个基准测试中的性能表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20013">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3ac0b82c5ad4d809250530ef7527c6eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029495&auth_key=1760029495-0-0-0f930c0dc7bf0a253236a02156e1cc20&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d0633a52e03483d0baa75456775bc9a9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029503&auth_key=1760029503-0-0-13013446696b324f1fbf2c049b7ba47d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8858dc9def424cf10a320ce5fc2b863b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029510&auth_key=1760029510-0-0-bbb33b4e6b9f5bf8800658ad65015c6e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VLM-Agents-Generate-Their-Own-Memories-Distilling-Experience-into-Embodied-Programs-of-Thought"><a href="#VLM-Agents-Generate-Their-Own-Memories-Distilling-Experience-into-Embodied-Programs-of-Thought" class="headerlink" title="VLM Agents Generate Their Own Memories: Distilling Experience into   Embodied Programs of Thought"></a>VLM Agents Generate Their Own Memories: Distilling Experience into   Embodied Programs of Thought</h2><p><strong>Authors:Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, Katerina Fragkiadaki</strong></p>
<p>Large-scale generative language and vision-language models (LLMs and VLMs) excel in few-shot learning but require high-quality demonstrations. We propose In-Context Abstraction Learning (ICAL), enabling VLM agents to transform suboptimal trajectories into high-quality training data through self-reflection and human feedback. Given imperfect task demonstrations, a VLM abstracts trajectories into generalized strategies and action annotations by correcting inefficiencies and annotating cognitive abstractions: causal relationships, object state changes, temporal subgoals, and task-relevant visual elements. These annotations are iteratively refined through human feedback during execution in similar environments. The resulting examples significantly improve decision-making when used for retrieval-augmented generation or fine-tuning. As the agent’s example library grows, it becomes more efficient at abstracting new examples, requiring less human feedback and fewer environment interactions. ICAL achieves state-of-the-art results across multiple benchmarks. In TEACh dialogue-based instruction following, combining fine-tuning and retrieval on ICAL examples outperforms raw human demonstrations and expert examples by 17.5% in goal-condition success. In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success 1.6x, while fine-tuned Qwen2-VL achieves 2.8x improvement over the base model. In Ego4D action forecasting, we surpass few-shot GPT-4V and remain competitive with supervised models. Our approach scales 2x better than raw demonstrations and significantly reduces manual prompt engineering requirements. </p>
<blockquote>
<p>大规模生成式语言和视觉语言模型（LLM和VLM）在少量学习样本上表现出色，但需要高质量演示。我们提出上下文抽象学习（ICAL）方法，使VLM代理能够通过自我反思和人类反馈将次优轨迹转化为高质量训练数据。面对不完美的任务演示，VLM通过纠正无效操作和标注认知抽象（如因果关系、对象状态变化、时间子目标和任务相关视觉元素），将轨迹转化为通用策略和动作注解。在执行类似环境的过程中，这些标注会通过人类反馈进行迭代优化。使用这些生成的示例进行检索增强生成或微调时，可以显著改善决策效果。随着代理示例库的增长，它会在抽象新示例时变得更加高效，需要更少的人类反馈和环境交互。ICAL在多基准测试中达到最新水平。在基于对话的指令遵循TEACh任务中，结合ICAL示例进行微调与检索的表现优于原始人类演示和专业示例，目标条件下的成功率提高了17.5%。在VisualWebArena任务中，结合ICAL的检索增强GPT-4V的任务成功率提高了1.6倍，而经过精细调整的Qwen2-VL相较于基础模型提高了2.8倍。在Ego4D动作预测任务中，我们的方法超越了少量样本的GPT-4V，并与监督模型保持竞争力。我们的方法比原始演示的扩展性提高了两倍，并大大降低了手动提示工程要求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14596v6">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://ical-learning.github.io/">https://ical-learning.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>大规模生成式语言和视觉语言模型（LLMs和VLMs）在少样本学习上表现出色，但需要高质量示范。本研究提出了上下文抽象学习（ICAL）方法，使VLM智能体能够通过自我反思和人类反馈将次优轨迹转化为高质量训练数据。借助非完美的任务示范，VLM可以抽象出通用的策略和行动注解，通过纠正无效率并注解因果关联、对象状态变化、临时子目标和任务相关视觉元素等认知抽象。这些注解在执行类似环境的过程中通过人类反馈进行迭代优化。利用这些实例，显著提高决策能力，用于检索增强生成或微调。随着智能体示例库的扩大，它更有效地抽象出新的示例，需要更少的人类反馈和环境交互。本研究在多个基准测试中实现了最先进的成果。在TEACh对话式指令遵循任务中，结合ICAL实例的微调与检索优于原始人类示范和专业示例，目标条件下的成功率提高了17.5%。在VisualWebArena中，结合ICAL的GPT-4V检索增强任务成功率提高了1.6倍，而Qwen2-VL微调版本较基础模型任务成功率提高了2.8倍。在Ego4D动作预测任务中，我们的方法超越少样本GPT-4V并维持与监督模型的竞争力。总体来说，该研究开发的ICAL方法可有效将非完美示范转化为高价值训练资源，大幅度提高模型的效能并扩大规模效益。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模生成式语言和视觉语言模型（LLMs和VLMs）在少样本学习上表现优秀，但需高质量示范。</li>
<li>提出上下文抽象学习（ICAL）方法，使VLM智能体能够将次优轨迹转化为高质量训练数据。</li>
<li>ICAL通过人类反馈和自我反思进行迭代优化，包括纠正无效率并注解认知抽象如因果关联等。</li>
<li>ICAL方法显著提高了决策能力，尤其在任务示范质量不高的情况下。</li>
<li>在多个基准测试中，ICAL实现了最先进的成果，尤其是在TEACh对话式指令遵循任务中。</li>
<li>结合ICAL实例的GPT-4V检索增强和Qwen2-VL微调均显著提高了任务成功率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14596">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-a309fc6b3817dab3ec02c2fc7473c601~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029517&auth_key=1760029517-0-0-877e161c21aa2e22fce95464d5d4ddc4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f50a399f8145c12177ffd0d2239e90bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029525&auth_key=1760029525-0-0-65e4b0c9bdc57d8d3315e4500773199d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bf3e5fc2d4993a836fd351ecc79180b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029532&auth_key=1760029532-0-0-d8af0a808212bdb6bb329d58170d3437&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-7e3f8f182fc20e823326aea988214435~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029538&auth_key=1760029538-0-0-764be6934c9a29e0b3cfe82da6e00a9a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-09-20  Seeing 3D Through 2D Lenses 3D Few-Shot Class-Incremental Learning via   Cross-Modal Geometric Rectification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-4ad8a618ac62fe7fddf9aaaa5fc2dfba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760028595&auth_key=1760028595-0-0-aa68238dd34d503503533c8e629c9689&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-09-20  LNE-Blocking An Efficient Framework for Contamination Mitigation   Evaluation on Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
