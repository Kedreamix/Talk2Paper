<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-09-20  SynParaSpeech Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-1c71bd23752a4433c968d48c2060d1ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029960&auth_key=1760029960-0-0-d44d9f7284488c19a8a0682b093694d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    49 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-20-更新"><a href="#2025-09-20-更新" class="headerlink" title="2025-09-20 更新"></a>2025-09-20 更新</h1><h2 id="SynParaSpeech-Automated-Synthesis-of-Paralinguistic-Datasets-for-Speech-Generation-and-Understanding"><a href="#SynParaSpeech-Automated-Synthesis-of-Paralinguistic-Datasets-for-Speech-Generation-and-Understanding" class="headerlink" title="SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding"></a>SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding</h2><p><strong>Authors:Bingsong Bai, Qihang Lu, Wenbing Yang, Zihan Sun, YueRan Hou, Peilei Jia, Songbai Pu, Ruibo Fu, Yingming Gao, Ya Li, Jun Gao</strong></p>
<p>Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing more realistic and engaging speech. However, existing methods typically depend on proprietary datasets, while publicly available resources often suffer from incomplete speech, inaccurate or missing timestamps, and limited real-world relevance. To address these problems, we propose an automated framework for generating large-scale paralinguistic data and apply it to construct the SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with 118.75 hours of data and precise timestamps, all derived from natural conversational speech. Our contributions lie in introducing the first automated method for constructing large-scale paralinguistic datasets and releasing the SynParaSpeech corpus, which advances speech generation through more natural paralinguistic synthesis and enhances speech understanding by improving paralinguistic event detection. The dataset and audio samples are available at <a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech">https://github.com/ShawnPi233/SynParaSpeech</a>. </p>
<blockquote>
<p>副语言声音，如笑声和叹息声，对于合成更真实、更吸引人的语音至关重要。然而，现有方法通常依赖于专有数据集，而公开可用的资源往往存在语音不完整、时间戳不准确或缺失、与真实世界的关联有限等问题。为了解决这些问题，我们提出了一个自动生成大规模副语言数据集的框架，并应用该框架构建了SynParaSpeech数据集。该数据集包含6个副语言类别，数据时长为118.75小时，带有精确的时间戳，均来自自然对话语音。我们的贡献在于引入了构建大规模副语言数据集的首个自动方法，并发布了SynParaSpeech语料库，这通过更自然的副语言合成推动了语音生成的发展，并通过提高副语言事件检测改善了语音理解。数据集和音频样本可在[<a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/ShawnPi233/SynParaSpeech获取。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14946v1">PDF</a> submitted to ICASSP 2026</p>
<p><strong>摘要</strong></p>
<p>文章强调了辅语言声音（如笑声和叹息声）在合成更真实、更吸引人的语音方面的重要性。然而，现有方法通常依赖于专有数据集，而公开可用的资源往往存在语音不完整、时间戳不准确或缺失、现实相关性有限等问题。为解决这些问题，本文提出了一个用于生成大规模辅语言数据集的自动化框架，并据此构建了SynParaSpeech数据集。该数据集包含6个辅语言类别，数据时长为118.75小时，具有精确的时间戳，均来自自然对话语音。本文的贡献在于引入了构建大规模辅语言数据集的自动化方法，并发布了SynParaSpeech语料库，这通过更自然的辅语言合成推动了语音生成的发展，并通过改进辅语言事件检测提高了语音理解。数据集和音频样本可在<a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech">链接</a>中找到。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>辅语言声音对于合成更真实、更吸引人的语音至关重要。</li>
<li>现有数据集存在依赖专有数据、公开资源语音不完整、时间戳不准确或缺失等问题。</li>
<li>提出了一个自动化框架用于生成大规模辅语言数据集。</li>
<li>构建了包含6个辅语言类别、118.75小时数据的SynParaSpeech数据集，具有精确时间戳。</li>
<li>数据集来自自然对话语音，增强了现实相关性。</li>
<li>该数据集的发布推动了更自然的辅语言合成和语音生成的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14946">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-d6fbc8980f0bb4584c0ef571ebf2a79d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029967&auth_key=1760029967-0-0-af68273b2bccbf2b47138766024dd7f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-857a55cf3ee4c49f05b60ccf35358df7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029975&auth_key=1760029975-0-0-c657d5b9f67675b618a78534b8b97f95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9cba3ec5dd80fddb5fffeb7c0416c3db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029982&auth_key=1760029982-0-0-aecb4abaf1d5660ef585ad729524279d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ebf4bbf6c0de3a8caac8eabac37384e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029988&auth_key=1760029988-0-0-62b62755eb43d15dd83ec4d4e8ed5887&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="From-Hype-to-Insight-Rethinking-Large-Language-Model-Integration-in-Visual-Speech-Recognition"><a href="#From-Hype-to-Insight-Rethinking-Large-Language-Model-Integration-in-Visual-Speech-Recognition" class="headerlink" title="From Hype to Insight: Rethinking Large Language Model Integration in   Visual Speech Recognition"></a>From Hype to Insight: Rethinking Large Language Model Integration in   Visual Speech Recognition</h2><p><strong>Authors:Rishabh Jain, Naomi Harte</strong></p>
<p>Advances in self-supervised encoders have improved Visual Speech Recognition (VSR). Recent approaches integrating these encoders with LLM decoders improves transcription accuracy; however, it remains unclear whether these gains stem from visual understanding or stronger language modeling. In this work, we systematically evaluate LLM decoders by freezing or selectively updating the visual encoder, scaling decoder size, comparing adaptation strategies and architectures, and varying training data across LRS2, LRS3, and their combination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and adaptation yield limited improvements, while combining datasets enhances generalization. Semantic analysis reveals that gains arise primarily from lexical rather than semantic processing. Our Llama-2-13B model trained on the combined set achieves 24.7% WER on LRS3 and 47.0% on WildVSR, establishing SOTA among models trained without additional supervision. Our findings indicate LLM decoders refine contextual reasoning rather than visual features, emphasizing the need for stronger visual encoders to drive meaningful progress. </p>
<blockquote>
<p>随着自监督编码器的进步，视觉语音识别（VSR）得到了提升。最近将这些编码器与大型语言模型（LLM）解码器结合的方法提高了转录准确性；然而，尚不清楚这些收益是来源于视觉理解还是更强大的语言建模。在这项工作中，我们通过冻结或选择性更新视觉编码器、扩展解码器规模、比较适应策略和架构、以及在不同数据集（LRS2、LRS3及其组合）之间改变训练数据来系统地评估LLM解码器。在LRS2、LRS3和WildVSR上的评估显示，扩展和适应带来有限的改进，而组合数据集增强了泛化能力。语义分析表明，收益主要来自于词汇而非语义处理。我们的在组合数据集上训练的Llama-2-13B模型在LRS3上实现了24.7%的字词错误率（WER），在WildVSR上实现了47.0%的字词错误率，成为在没有额外监督的情况下训练的模型中最新顶尖技术。我们的研究结果表明，LLM解码器提高了上下文推理能力，而非视觉特征，这强调了需要更强大的视觉编码器来推动有意义的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14880v1">PDF</a> submitted to ICASSP 2026. This work has been submitted to the IEEE   for possible publication</p>
<p><strong>Summary</strong></p>
<p>本文研究了在自监督编码器进展的基础上，如何改善视觉语音识别（VSR）的技术。文章深入评估了LLM解码器的效能，并通过实验验证了在固定或选择性更新视觉编码器、调整解码器规模、对比不同适应策略和架构以及使用LRS2、LRS3及其组合数据集进行训练的情况下，LLM解码器的表现。评估结果显示，结合数据集能提升泛化能力，语义分析表明效果主要来自于词汇而非语义处理。最佳模型在LRS3上达到24.7%的WER，在WildVSR上达到47.0%的WER，且在无需额外监督的情况下达到了先进的表现。研究指出LLM解码器强化了上下文推理而非视觉特征，强调了需要更强大的视觉编码器来推动技术进步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督编码器的进展改善了视觉语音识别（VSR）。</li>
<li>LLM解码器与视觉编码器的结合提高了转录准确性。</li>
<li>评估表明，解码器规模的扩大和适应策略仅带来有限改进，而结合数据集有助于增强泛化能力。</li>
<li>语义分析显示，效果主要来自于词汇处理而非语义处理。</li>
<li>最佳模型在LRS3和WildVSR上的表现达到先进水平。</li>
<li>LLM解码器强化了上下文推理，而非视觉特征。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14880">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ec2442939bc6d8673133c9a100650e1c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029996&auth_key=1760029996-0-0-721dd4a63b9a0857e02284b7168ca1d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-844772b1555e72c264c6866ce9102c3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030005&auth_key=1760030005-0-0-663a19ca0c3f98d909d5838ecdfe5359&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-adc808f3204b2f84eeb43b82c723a3e4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030012&auth_key=1760030012-0-0-83c5cad2cfaefdca9f477551185f9aa6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b87bee07a2508705df31045cd7290b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030020&auth_key=1760030020-0-0-2f6468842be54bd7ca37860db7301c4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AmbiDrop-Array-Agnostic-Speech-Enhancement-Using-Ambisonics-Encoding-and-Dropout-Based-Learning"><a href="#AmbiDrop-Array-Agnostic-Speech-Enhancement-Using-Ambisonics-Encoding-and-Dropout-Based-Learning" class="headerlink" title="AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding   and Dropout-Based Learning"></a>AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding   and Dropout-Based Learning</h2><p><strong>Authors:Michael Tatarjitzky, Boaz Rafaely</strong></p>
<p>Multichannel speech enhancement leverages spatial cues to improve intelligibility and quality, but most learning-based methods rely on specific microphone array geometry, unable to account for geometry changes. To mitigate this limitation, current array-agnostic approaches employ large multi-geometry datasets but may still fail to generalize to unseen layouts. We propose AmbiDrop (Ambisonics with Dropouts), an Ambisonics-based framework that encodes arbitrary array recordings into the spherical harmonics domain using Ambisonics Signal Matching (ASM). A deep neural network is trained on simulated Ambisonics data, combined with channel dropout for robustness against array-dependent encoding errors, therefore omitting the need for a diverse microphone array database. Experiments show that while the baseline and proposed models perform similarly on the training arrays, the baseline degrades on unseen arrays. In contrast, AmbiDrop consistently improves SI-SDR, PESQ, and STOI, demonstrating strong generalization and practical potential for array-agnostic speech enhancement. </p>
<blockquote>
<p>多通道语音增强利用空间线索提高语音的清晰度和质量，但大多数基于学习的方法依赖于特定的麦克风阵列几何结构，无法适应几何变化。为了缓解这一局限性，当前的阵列无关方法采用大型多几何数据集，但仍可能无法推广到未见过的布局。我们提出了AmbiDrop（带有缺失值的四面体声技术），这是一种基于四面体声技术的框架，它使用四面体声信号匹配（ASM）将任意阵列录音编码到球面谐波域。深度神经网络在模拟的四面体声数据上进行训练，结合通道丢失以增强对阵列相关编码错误的鲁棒性，从而无需使用多样化的麦克风阵列数据库。实验表明，虽然在训练阵列上，基线模型和所提出的模型表现相似，但基线模型在未见过的阵列上性能下降。相比之下，AmbiDrop持续提高了SI-SDR、PESQ和STOI指标，显示出强大的通用性和阵列无关的语音增强的实际应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14855v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为AmbiDrop的基于Ambisonics的框架，用于对任意阵列录音进行编码。该框架采用Ambisonics信号匹配（ASM）技术，将录音转化为球面谐波域。通过训练在模拟的Ambisonics数据上的深度神经网络，并结合通道丢弃策略，以增强对阵列依赖的编码错误的稳健性，从而无需使用多样化的麦克风阵列数据库。实验表明，在未见过的阵列上，AmbiDrop相较于基线模型在SI-SDR、PESQ和STOI等指标上表现更优，显示出强大的泛化能力和实际应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多通道语音增强利用空间线索提高语音的清晰度和质量。</li>
<li>大多数基于学习的方法依赖于特定的麦克风阵列几何结构，无法适应几何变化。</li>
<li>当前的阵列无关方法使用多几何数据集，但仍可能无法泛化到未见过的布局。</li>
<li>提出的AmbiDrop框架采用Ambisonics技术，将任意阵列录音编码到球面谐波域。</li>
<li>通过训练在模拟的Ambisonics数据上的深度神经网络，结合通道丢弃策略，提高模型对阵列依赖的编码错误的稳健性。</li>
<li>实验表明，在未见过的阵列上，AmbiDrop相较于基线模型表现更优。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14855">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-3803e64d5ee4196632817e5386434d83~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030027&auth_key=1760030027-0-0-2bd15446b717c32d1b3476f1d6ad1b5a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-05d081e2ef8a49d19c3e88bdac709075~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030069&auth_key=1760030069-0-0-28bdf3af8249022b15be0cdb52bf772f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e5a5abd113033aae32fb923a80138bfd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030075&auth_key=1760030075-0-0-4d1d44a8ad5e9b69a44e48e7131a4dc9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cfbb0f8702ba771782719240ce711e79~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030082&auth_key=1760030082-0-0-8f969c53077c4356013f7c06cfa0fe65&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98415f2e948217d45a9ac41ef1ef03f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030089&auth_key=1760030089-0-0-ac1e85f5461120a9fb6cc3f135a42f67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Acoustic-Simulation-Framework-for-Multi-channel-Replay-Speech-Detection"><a href="#Acoustic-Simulation-Framework-for-Multi-channel-Replay-Speech-Detection" class="headerlink" title="Acoustic Simulation Framework for Multi-channel Replay Speech Detection"></a>Acoustic Simulation Framework for Multi-channel Replay Speech Detection</h2><p><strong>Authors:Michael Neri, Tuomas Virtanen</strong></p>
<p>Replay speech attacks pose a significant threat to voice-controlled systems, especially in smart environments where voice assistants are widely deployed. While multi-channel audio offers spatial cues that can enhance replay detection robustness, existing datasets and methods predominantly rely on single-channel recordings. In this work, we introduce an acoustic simulation framework designed to simulate multi-channel replay speech configurations using publicly available resources. Our setup models both genuine and spoofed speech across varied environments, including realistic microphone and loudspeaker impulse responses, room acoustics, and noise conditions. The framework employs measured loudspeaker directionalities during the replay attack to improve the realism of the simulation. We define two spoofing settings, which simulate whether a reverberant or an anechoic speech is used in the replay scenario, and evaluate the impact of omnidirectional and diffuse noise on detection performance. Using the state-of-the-art M-ALRAD model for replay speech detection, we demonstrate that synthetic data can support the generalization capabilities of the detector across unseen enclosures. </p>
<blockquote>
<p>回放语音攻击对声控系统构成重大威胁，特别是在广泛部署语音助手的智能环境中。虽然多通道音频提供了可以增强回放检测稳健性的空间线索，但现有数据集和方法主要依赖于单通道录音。在这项工作中，我们引入了一个声学模拟框架，利用公开资源设计模拟多通道回放语音配置。我们的设置模型涵盖了各种环境下的真实和假冒语音，包括真实的麦克风和扬声器脉冲响应、房间声学特性和噪声条件。该框架在回放攻击期间采用测量的扬声器方向性，以提高模拟的真实性。我们定义了两种欺骗设置，模拟回放场景中是否使用混响或无声语音，并评估了全向和扩散噪声对检测性能的影响。我们使用先进的M-ALRAD模型进行回放语音检测，证明合成数据可以支持检测器在未见过的封闭空间中的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14789v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>多通道回放语音对语音控制系统构成重大威胁，特别是在智能环境中广泛部署的语音助手。尽管多通道音频可以提供增强回放检测稳健性的空间线索，但现有数据集和方法主要依赖于单通道录音。本研究引入了一个声学模拟框架，利用公开资源模拟多通道回放语音配置。该框架模拟各种环境，包括真实的麦克风和扬声器冲击响应、房间声学特性和噪声条件。该框架采用回放攻击期间的实测扬声器方向性来提高模拟的真实性。定义了两个欺骗设置，模拟回放场景中是否使用混响或无声语音，并评估了全向和扩散噪声对检测性能的影响。利用先进的M-ALRAD模型进行回放语音检测，证明合成数据可以增强检测器在未见过的封闭空间中的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多通道回放语音对语音控制系统构成威胁，特别是在智能环境中。</li>
<li>现有数据集和方法主要依赖单通道录音，缺乏多通道音频的空间线索。</li>
<li>引入了一个声学模拟框架，用以模拟多通道回放语音配置和多种环境。</li>
<li>框架考虑真实的麦克风和扬声器冲击响应、房间声学特性和噪声条件。</li>
<li>该框架采用实测的扬声器方向性在回放攻击时提高模拟真实性。</li>
<li>定义了两个欺骗设置，以模拟回放场景中的混响或无声语音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14789">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-8ef82dd3c35791fccf640500b17ee8d9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030098&auth_key=1760030098-0-0-c3da3c069214a2278a01528e5feb57c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0b887e1eb24748a1dfc3e35aa377b05c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030105&auth_key=1760030105-0-0-c1a0aad3fe1f72fb4ce12632429ca62d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f17aae2dc52936d9f0db59f352e36289~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030112&auth_key=1760030112-0-0-930c94e990111496cb9e099b08436c4b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-038c46c4069ad52ea4ee2eec774b5064~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030118&auth_key=1760030118-0-0-245dc1efa4a915701cc60e332f9932b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-967098d396e6399cdd60b4bc3ccd684e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030124&auth_key=1760030124-0-0-c00bfeb8292f0868d48d5f8c96e06fbf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-daf4a882de38bd9d6eb4671bae2b2695~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030131&auth_key=1760030131-0-0-a96eba2d8c7fba6c2be47c8e6532fa22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MELA-TTS-Joint-transformer-diffusion-model-with-representation-alignment-for-speech-synthesis"><a href="#MELA-TTS-Joint-transformer-diffusion-model-with-representation-alignment-for-speech-synthesis" class="headerlink" title="MELA-TTS: Joint transformer-diffusion model with representation   alignment for speech synthesis"></a>MELA-TTS: Joint transformer-diffusion model with representation   alignment for speech synthesis</h2><p><strong>Authors:Keyu An, Zhiyu Zhang, Changfeng Gao, Yabin Li, Zhendong Peng, Haoxu Wang, Zhihao Du, Han Zhao, Zhifu Gao, Xiangang Li</strong></p>
<p>This work introduces MELA-TTS, a novel joint transformer-diffusion framework for end-to-end text-to-speech synthesis. By autoregressively generating continuous mel-spectrogram frames from linguistic and speaker conditions, our architecture eliminates the need for speech tokenization and multi-stage processing pipelines. To address the inherent difficulties of modeling continuous features, we propose a representation alignment module that aligns output representations of the transformer decoder with semantic embeddings from a pretrained ASR encoder during training. This mechanism not only speeds up training convergence, but also enhances cross-modal coherence between the textual and acoustic domains. Comprehensive experiments demonstrate that MELA-TTS achieves state-of-the-art performance across multiple evaluation metrics while maintaining robust zero-shot voice cloning capabilities, in both offline and streaming synthesis modes. Our results establish a new benchmark for continuous feature generation approaches in TTS, offering a compelling alternative to discrete-token-based paradigms. </p>
<blockquote>
<p>本文介绍了MELA-TTS，这是一种新型端到端的文本到语音合成联合Transformer-Diffusion框架。通过自回归生成语言条件和说话者条件下的连续梅尔频谱帧，我们的架构消除了对语音标记化和多阶段处理管道的需求。为了解决建模连续特征固有的困难，我们提出了一个表示对齐模块，该模块在训练期间将对齐transformer解码器的输出表示与预训练的ASR编码器的语义嵌入。这种机制不仅加快了训练收敛速度，而且增强了文本和声学域之间的跨模态一致性。综合实验表明，MELA-TTS在多个评估指标上达到了最先进的性能，同时保持了离线合成模式和流式合成模式下的零样本语音克隆能力。我们的结果为TTS中的连续特征生成方法建立了新的基准，为基于离散标记的方法提供了引人注目的替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14784v1">PDF</a> submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了MELA-TTS，这是一种新颖的联合Transformer-Diffusion框架，用于端到端的文本到语音合成。通过自回归生成连续的梅尔频谱图帧，该架构消除了对语音标记化和多阶段处理管道的需求。为了解决连续特征建模的固有困难，本文提出了一种表示对齐模块，该模块在训练期间将Transformer解码器的输出表示与预训练的ASR编码器的语义嵌入进行对齐。这种机制不仅加快了训练收敛速度，而且增强了文本和声音域之间的跨模态一致性。实验表明，MELA-TTS在多个评估指标上达到了最先进的性能，同时保持了离线合成模式和流式合成模式下的零样本语音克隆能力。这为TTS中的连续特征生成方法建立了新的基准，为基于离散符号的方法提供了引人注目的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MELA-TTS是一种新颖的联合Transformer-Diffusion框架，用于端到端的文本到语音合成。</li>
<li>该架构通过自回归生成连续的梅尔频谱图帧，无需语音标记化和多阶段处理。</li>
<li>提出了一个表示对齐模块，以在训练期间提高文本和语音域之间的跨模态一致性。</li>
<li>该机制不仅加快了训练收敛速度，而且增强了模型的性能。</li>
<li>MELA-TTS在多个评估指标上达到了最先进的性能。</li>
<li>MELA-TTS支持离线合成模式和流式合成模式下的零样本语音克隆能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14784">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-931235bc9ff842cef1d3dd4333a974c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030139&auth_key=1760030139-0-0-06969aa8ed3e8526a29c1ae1e83eecfc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-83ad6ba3f409d90492dae797dd3f803b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030146&auth_key=1760030146-0-0-45b92985f9daeb2a96d4b6f38505e8ad&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c71bd23752a4433c968d48c2060d1ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030152&auth_key=1760030152-0-0-1cb9de2ae8a4b581046120cff3644c32&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-732b51e234adec9a9e43707b280872e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030159&auth_key=1760030159-0-0-79ac271d8dc4e58340e30bc6445676e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c31bbd5def7d35a0fb0da4866a11b7e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030165&auth_key=1760030165-0-0-4a0b87bdbc1d31546c034c21df5a6632&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HARNESS-Lightweight-Distilled-Arabic-Speech-Foundation-Models"><a href="#HARNESS-Lightweight-Distilled-Arabic-Speech-Foundation-Models" class="headerlink" title="HARNESS: Lightweight Distilled Arabic Speech Foundation Models"></a>HARNESS: Lightweight Distilled Arabic Speech Foundation Models</h2><p><strong>Authors:Vrunda N. sukhadia, Shammur Absar Chowdhury</strong></p>
<p>Large pre-trained speech models excel in downstream tasks but their deployment is impractical for resource-limited environments. In this paper, we introduce HArnESS, the first Arabic-centric self-supervised speech model family, designed to capture Arabic speech nuances. Using iterative self-distillation, we train large bilingual HArnESS (HL) SSL models and then distill knowledge into compressed student models (HS, HST), preserving Arabic-specific representations. We use low-rank approximation to further compact the teacher’s discrete supervision into shallow, thin models. We evaluate HArnESS on Arabic ASR, Speaker Emotion Recognition (SER), and Dialect Identification (DID), demonstrating effectiveness against HuBERT and XLS-R. With minimal fine-tuning, HArnESS achieves SOTA or comparable performance, making it a lightweight yet powerful alternative for real-world use. We release our distilled models and findings to support responsible research and deployment in low-resource settings. </p>
<blockquote>
<p>大型预训练语音模型在下游任务中表现出色，但在资源有限的环境中其部署并不实用。在本文中，我们介绍了HArnESS，这是一个以阿拉伯语为中心的自我监督语音模型家族，旨在捕捉阿拉伯语语音的细微差别。我们使用迭代自蒸馏技术训练大型双语HArnESS（HL）SSL模型，然后将知识蒸馏到压缩的学生模型（HS，HST）中，同时保留阿拉伯语的特定表示。我们使用低秩近似来进一步将教师的离散监督压缩成浅薄的模型。我们在阿拉伯语ASR、说话人情绪识别（SER）和方言识别（DID）上对HArnESS进行了评估，与HuBERT和XLS-R相比表现出其有效性。HArnESS在最小微调的情况下达到了SOTA或相当的性能，成为现实世界使用的轻便而强大的替代品。我们发布我们的蒸馏模型和研究成果，以支持低资源环境中的负责任研究和部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14689v1">PDF</a> 5 pages, 4 figures</p>
<p><strong>摘要</strong></p>
<p>阿拉伯语音处理模型的提出：引入首个阿拉伯语为中心的自监督语音模型家族HArnESS，采用迭代自蒸馏方法训练大型双语模型，再将其知识蒸馏为压缩的学生模型。采用低秩逼近将教师的离散监督压缩为浅瘦模型，以适应资源受限的环境。评估结果表明白其在阿拉伯语ASR、说话人情绪识别和方言识别方面的有效性，达到了最前沿或可比性能。</p>
<p><strong>关键见解</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14689">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-b48fa92a7b286e1fe3c435e549fc19e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030173&auth_key=1760030173-0-0-618111a087da9f92afc5a337ef296426&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-20e3665d5f7aa1bcd0ac7203053768a7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030180&auth_key=1760030180-0-0-608595646db8b6d916e70d2329e14e8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8cb6308b423f2c9ce4e6516de867ea6f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030187&auth_key=1760030187-0-0-8b71ff0b1f1533e76e8eff006fa2c604&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-32cc45f89096e521b4abb1205fedf75a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030194&auth_key=1760030194-0-0-3a15c3ae35ae86250e135c8a776efb74&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3d06eedb7a10af8f253f7665dedb9e42~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030201&auth_key=1760030201-0-0-4a1f3c7707e1e6eed45e08b911692bea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1e9884c161d15e0d7b8e7ebc8c50aa0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030208&auth_key=1760030208-0-0-30176c8f4f1999fb9786afed46b04d34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DAIEN-TTS-Disentangled-Audio-Infilling-for-Environment-Aware-Text-to-Speech-Synthesis"><a href="#DAIEN-TTS-Disentangled-Audio-Infilling-for-Environment-Aware-Text-to-Speech-Synthesis" class="headerlink" title="DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware   Text-to-Speech Synthesis"></a>DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware   Text-to-Speech Synthesis</h2><p><strong>Authors:Ye-Xin Lu, Yu Gu, Kun Wei, Hui-Peng Du, Yang Ai, Zhen-Hua Ling</strong></p>
<p>This paper presents DAIEN-TTS, a zero-shot text-to-speech (TTS) framework that enables ENvironment-aware synthesis through Disentangled Audio Infilling. By leveraging separate speaker and environment prompts, DAIEN-TTS allows independent control over the timbre and the background environment of the synthesized speech. Built upon F5-TTS, the proposed DAIEN-TTS first incorporates a pretrained speech-environment separation (SES) module to disentangle the environmental speech into mel-spectrograms of clean speech and environment audio. Two random span masks of varying lengths are then applied to both mel-spectrograms, which, together with the text embedding, serve as conditions for infilling the masked environmental mel-spectrogram, enabling the simultaneous continuation of personalized speech and time-varying environmental audio. To further enhance controllability during inference, we adopt dual class-free guidance (DCFG) for the speech and environment components and introduce a signal-to-noise ratio (SNR) adaptation strategy to align the synthesized speech with the environment prompt. Experimental results demonstrate that DAIEN-TTS generates environmental personalized speech with high naturalness, strong speaker similarity, and high environmental fidelity. </p>
<blockquote>
<p>本文介绍了DAIEN-TTS，这是一个零样本文本到语音（TTS）框架，它通过解耦音频填充（Disentangled Audio Infilling）实现了环境感知合成。通过利用单独的说话人和环境提示，DAIEN-TTS可以独立控制合成语音的音调和背景环境。基于F5-TTS构建的DAIEN-TTS首先集成了一个预训练的语音环境分离（SES）模块，将环境语音分解成干净语音的梅尔频谱图和环境音频。然后，在梅尔频谱图上应用两个随机长度不同的跨度掩码，与文本嵌入一起作为填充掩码环境梅尔频谱图的条件，从而实现个性化语音和随时间变化的环境音频的同时延续。为了进一步提高推理过程中的可控性，我们采用了针对语音和环境组件的双重无类别引导（DCFG），并引入了一种信噪比（SNR）自适应策略，以使合成语音与环境提示保持一致。实验结果表明，DAIEN-TTS生成的环境个性化语音具有高度的自然性、强烈的说话人相似性和高环境保真度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14684v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本文介绍了DAIEN-TTS，一个零样本文本转语音（TTS）框架，它通过解耦音频填充实现了环境感知合成。该框架利用独立的说话人和环境提示，实现对合成语音的音调和背景环境的独立控制。实验结果表明，DAIEN-TTS生成的环境个性化语音具有高度的自然性、强烈的说话人相似性和高环境保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DAIEN-TTS是一个零样本文本转语音（TTS）框架，能实现环境感知合成。</li>
<li>通过解耦音频填充，DAIEN-TTS实现对合成语音的音调和背景环境的独立控制。</li>
<li>DAIEN-TTS建立在F5-TTS之上，融入了一个预训练的语音环境分离（SES）模块，将环境语音分解为纯净语音和环境音频的mel-spectrogram。</li>
<li>通过应用两种随机跨度掩码到mel-spectrogram，结合文本嵌入，实现对掩码环境mel-spectrogram的填充，实现个性化语音和时间变化环境音频的同时延续。</li>
<li>采用无类别引导（DCFG）增强语音和环境组件的控制性，并引入信噪比（SNR）自适应策略，使合成语音与环境提示对齐。</li>
<li>实验结果表明，DAIEN-TTS生成的环境个性化语音具有高度的自然性、强烈的说话人相似性和高环境保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14684">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-e71364446f42d253ba0ec5ccc056e879~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030215&auth_key=1760030215-0-0-4e9d5e5be9ee2e3e4ca2f2fa2ceed6ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-46a6baae3822f4b83edb67fb84ccf455~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030223&auth_key=1760030223-0-0-27d1bcd3e1a168930a3c7330c81a85dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5000b6ebf49b9642553988642b16704f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030230&auth_key=1760030230-0-0-275c6af80adbff3c2b56a0022beaa64d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6292b4c1fc3c09f4d596bfa41a1fdd0b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030236&auth_key=1760030236-0-0-e2404a4891eff24635028560a10f2625&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UMA-Split-unimodal-aggregation-for-both-English-and-Mandarin-non-autoregressive-speech-recognition"><a href="#UMA-Split-unimodal-aggregation-for-both-English-and-Mandarin-non-autoregressive-speech-recognition" class="headerlink" title="UMA-Split: unimodal aggregation for both English and Mandarin   non-autoregressive speech recognition"></a>UMA-Split: unimodal aggregation for both English and Mandarin   non-autoregressive speech recognition</h2><p><strong>Authors:Ying Fang, Xiaofei Li</strong></p>
<p>This paper proposes a unimodal aggregation (UMA) based nonautoregressive model for both English and Mandarin speech recognition. The original UMA explicitly segments and aggregates acoustic frames (with unimodal weights that first monotonically increase and then decrease) of the same text token to learn better representations than regular connectionist temporal classification (CTC). However, it only works well in Mandarin. It struggles with other languages, such as English, for which a single syllable may be tokenized into multiple fine-grained tokens, or a token spans fewer than 3 acoustic frames and fails to form unimodal weights. To address this problem, we propose allowing each UMA-aggregated frame map to multiple tokens, via a simple split module that generates two tokens from each aggregated frame before computing the CTC loss. </p>
<blockquote>
<p>本文提出了一种基于单模态聚合（UMA）的非自回归模型，用于英语和普通话的语音识别。原始的UMA会明确地分割并聚合同一文本标记的声学帧（使用先单调增加后减少的单模态权重），以学习比常规连接时序分类（CTC）更好的表示。然而，它只在普通话中表现良好。对于英语等其他语言，它很难处理，因为英语中的单个音节可能被细分为多个标记，或者一个标记跨越少于3个声学帧而无法形成单模态权重。为了解决这一问题，我们提出了一种允许每个UMA聚合帧映射到多个标记的方法，通过简单的分割模块在计算CTC损失之前从每个聚合帧生成两个标记。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14653v1">PDF</a> Submit to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种基于单模态聚合（UMA）的非自回归模型，用于英语和普通话的语音识别。原始的UMA通过对同一文本标记的声帧进行分段和聚合（采用先递增后递减的单模态权重），以学习比连接时序分类（CTC）更好的表示。然而，它在普通话以外的语言，如英语中的表现并不理想。因为英语中的单个音节可能被细分为多个标记，或者一个标记跨越的声帧少于3个，导致无法形成单模态权重。为解决这一问题，论文提议允许每个UMA聚合的声帧映射到多个标记上，通过简单的分割模块为每个聚合的声帧生成两个标记，然后再计算CTC损失。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种基于单模态聚合（UMA）的非自回归模型，用于语音识别。</li>
<li>原始的UMA在普通话中表现良好，但在英语等其他语言中表现不佳。</li>
<li>英语中的语音标记比普通话更复杂，一个音节可能分为多个精细标记。</li>
<li>在英语中，由于声帧数量少于3个的标记较多，导致UMA无法形成稳定的单模态权重。</li>
<li>为解决这一问题，论文提出了允许每个UMA聚合的声帧映射到多个标记的方法。</li>
<li>通过简单的分割模块为每个聚合的声帧生成两个标记，提高模型的灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14653">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-bb66a5569d3657a37388176a7d3efdea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030244&auth_key=1760030244-0-0-623a18a63c423b44e5d1d8cbad255e7a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-62bd5ba2d9f169c98038240ea85d3ba1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030251&auth_key=1760030251-0-0-37a1eff1f08e90eece946a412cfd6364&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e6e138bde9f41b6ce5de91918a07c927~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030258&auth_key=1760030258-0-0-267611ef751e68a54d1d8cf413178c7d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3fe864be582e62a67ea428a06755d372~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030265&auth_key=1760030265-0-0-c0e2c843de807f520b80d84a9a2a5b3c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mitigating-Intra-Speaker-Variability-in-Diarization-with-Style-Controllable-Speech-Augmentation"><a href="#Mitigating-Intra-Speaker-Variability-in-Diarization-with-Style-Controllable-Speech-Augmentation" class="headerlink" title="Mitigating Intra-Speaker Variability in Diarization with   Style-Controllable Speech Augmentation"></a>Mitigating Intra-Speaker Variability in Diarization with   Style-Controllable Speech Augmentation</h2><p><strong>Authors:Miseul Kim, Soo Jin Park, Kyungguen Byun, Hyeon-Kyeong Shin, Sunkuk Moon, Shuhua Zhang, Erik Visser</strong></p>
<p>Speaker diarization systems often struggle with high intrinsic intra-speaker variability, such as shifts in emotion, health, or content. This can cause segments from the same speaker to be misclassified as different individuals, for example, when one raises their voice or speaks faster during conversation. To address this, we propose a style-controllable speech generation model that augments speech across diverse styles while preserving the target speaker’s identity. The proposed system starts with diarized segments from a conventional diarizer. For each diarized segment, it generates augmented speech samples enriched with phonetic and stylistic diversity. And then, speaker embeddings from both the original and generated audio are blended to enhance the system’s robustness in grouping segments with high intrinsic intra-speaker variability. We validate our approach on a simulated emotional speech dataset and the truncated AMI dataset, demonstrating significant improvements, with error rate reductions of 49% and 35% on each dataset, respectively. </p>
<blockquote>
<p>说话人分类系统经常面临高内在说话人内部变化的问题，如情绪、健康状况或内容的改变。这可能导致来自同一说话人的片段被错误地分类为不同的个体，例如当某人提高声音或加快语速进行对话时。为了解决这一问题，我们提出了一种风格可控的语音生成模型，该模型可以在保持目标说话人身份的同时，增强不同风格的语音。该系统的起点是常规分类器生成的分类语音片段。对于每个分类语音片段，它生成了丰富语音学和风格多样性的增强语音样本。然后，将原始音频和生成音频的说话人嵌入融合在一起，以提高系统对具有较高内在说话人内部变化片段的分组稳健性。我们在模拟情感语音数据集和截断AMI数据集上验证了我们的方法，显示出显著改进，每个数据集上的错误率分别降低了49%和35%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14632v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong>：针对说话人识别系统中存在的高内在说话人内部变异问题，如情感、健康状况或内容的改变可能导致同一说话人的片段被错误地分类为不同个体。为此，我们提出了一种风格可控的语音生成模型，该模型能够在保持目标说话人身份的同时，增加语音风格的多样性。通过融合原始和生成音频的说话人嵌入，提高了系统对高内在说话人内部变异性的分段分组能力。在模拟的情感语音数据集和AMI截断数据集上的实验验证，该方法分别降低了错误率49%和35%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>说话人识别系统面临高内在说话人内部变异问题。</li>
<li>同一说话人的片段可能因情感、健康状况或内容的变化而被误分类。</li>
<li>提出了一种风格可控的语音生成模型来增强语音的多样性和保持说话人身份。</li>
<li>该模型通过融合原始和生成音频的说话人嵌入来提高系统的鲁棒性。</li>
<li>验证实验在模拟的情感语音数据集和AMI截断数据集上进行。</li>
<li>该方法显著提高了说话人识别性能，分别降低了错误率49%和35%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14632">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-144489a7a01e9d668d9af54a29154217~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030273&auth_key=1760030273-0-0-ecb83770dc365f5a940de042e62fe895&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5abd5354dd0e11f50bcf54ccad06a274~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030280&auth_key=1760030280-0-0-a6639fdcf4973ef6087bbd251a3e2e2f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f87a5367b67b8dcb0e1970e3bc064171~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030287&auth_key=1760030287-0-0-a18710c8838b96ef51837cc46257bda3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-49bcb1204abd1df2088c4240bf73935d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030294&auth_key=1760030294-0-0-e3ae4f98d4972852541fd7f3d2725bd1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multi-Channel-Differential-ASR-for-Robust-Wearer-Speech-Recognition-on-Smart-Glasses"><a href="#Multi-Channel-Differential-ASR-for-Robust-Wearer-Speech-Recognition-on-Smart-Glasses" class="headerlink" title="Multi-Channel Differential ASR for Robust Wearer Speech Recognition on   Smart Glasses"></a>Multi-Channel Differential ASR for Robust Wearer Speech Recognition on   Smart Glasses</h2><p><strong>Authors:Yufeng Yang, Yiteng Huang, Yong Xu, Li Wan, Suwon Shon, Yang Liu, Yifeng Fan, Zhaojun Yang, Olivier Siohan, Yue Liu, Ming Sun, Florian Metze</strong></p>
<p>With the growing adoption of wearable devices such as smart glasses for AI assistants, wearer speech recognition (WSR) is becoming increasingly critical to next-generation human-computer interfaces. However, in real environments, interference from side-talk speech remains a significant challenge to WSR and may cause accumulated errors for downstream tasks such as natural language processing. In this work, we introduce a novel multi-channel differential automatic speech recognition (ASR) method for robust WSR on smart glasses. The proposed system takes differential inputs from different frontends that complement each other to improve the robustness of WSR, including a beamformer, microphone selection, and a lightweight side-talk detection model. Evaluations on both simulated and real datasets demonstrate that the proposed system outperforms the traditional approach, achieving up to an 18.0% relative reduction in word error rate. </p>
<blockquote>
<p>随着智能眼镜等可穿戴设备在人工智能助手方面的普及度不断增长，穿戴者语音识别（WSR）对于下一代人机交互界面变得日益关键。然而，在实际环境中，来自旁边说话的干扰仍然是WSR面临的重大挑战，并可能导致下游任务（如自然语言处理）出现累积错误。在这项工作中，我们针对智能眼镜上的稳健WSR引入了一种新型的多通道差分自动语音识别（ASR）方法。所提出系统从不同前端获取差分输入，这些输入相互补充，以提高WSR的稳健性，包括波束成形器、麦克风选择和轻量级旁边说话检测模型。对模拟和实际数据集的评估表明，所提出系统的性能优于传统方法，实现了高达18.0%的单词错误率相对降低。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14430v1">PDF</a> </p>
<p><strong>摘要</strong><br>随着智能眼镜等可穿戴设备的普及，佩戴者语音识别（WSR）对于下一代人机交互界面变得愈发关键。然而，在实际环境中，来自侧谈话语的干扰仍是WSR面临的一大挑战，并可能为自然语言处理等下游任务带来累积误差。本研究提出了一种新型的多通道差分自动语音识别（ASR）方法，旨在提升智能眼镜的稳健WSR。所提系统从不同前端采集差分输入，相互补充以提升WSR的稳健性，包括波束成形器、麦克风选择和轻量级侧谈检测模型。在模拟和真实数据集上的评估表明，所提系统优于传统方法，词错误率相对降低18.0%。</p>
<p><strong>要点解析</strong></p>
<ol>
<li>智能眼镜等可穿戴设备的普及使得佩戴者语音识别（WSR）变得日益关键。</li>
<li>侧谈话语干扰是WSR面临的实际环境挑战之一，可能导致下游任务如自然语言处理的累积误差。</li>
<li>研究提出了一种多通道差分自动语音识别（ASR）新方法，旨在增强智能眼镜WSR的稳健性。</li>
<li>所提系统集成了波束成形器、麦克风选择和轻量级侧谈检测模型等不同的前端差分输入。</li>
<li>该系统相互补充这些输入以提升WSR性能。</li>
<li>在模拟和真实数据集上的评估表明，新系统较传统方法在词错误率方面有明显的改进，相对降低了18.0%。</li>
<li>此研究为智能眼镜的语音识别技术带来了新的突破和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14430">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-4587301724eb9ee71fc0160b2faaa36d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030305&auth_key=1760030305-0-0-e9b9fe17564c9a793e932405c828a1bb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a0cdd0b80a8ed07f849a9ff8b4fcddf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030312&auth_key=1760030312-0-0-ffed08a6b1f1d4d1d4e134abb003e780&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b09293a34542b025dce4328e1bee08b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030318&auth_key=1760030318-0-0-4469a21252f209d50864fff9a0ace04d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ceaddfd17b1a6c9f55aa8b4ef5d9efd4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030324&auth_key=1760030324-0-0-a0ffabf997a92963c3fc9d796366355f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-065295ccadce3e098b9eb1ff2a148bbc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030331&auth_key=1760030331-0-0-35e9c329436a47cb99ebf7fae7294c95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fc31e9194a857e3cf2773afeac5620d8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030337&auth_key=1760030337-0-0-37fbf2bc419443df1624efd53a2b4d25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SpeechOp-Inference-Time-Task-Composition-for-Generative-Speech-Processing"><a href="#SpeechOp-Inference-Time-Task-Composition-for-Generative-Speech-Processing" class="headerlink" title="SpeechOp: Inference-Time Task Composition for Generative Speech   Processing"></a>SpeechOp: Inference-Time Task Composition for Generative Speech   Processing</h2><p><strong>Authors:Justin Lovelace, Rithesh Kumar, Jiaqi Su, Ke Chen, Kilian Q Weinberger, Zeyu Jin</strong></p>
<p>While generative Text-to-Speech (TTS) systems leverage vast &#96;&#96;in-the-wild” data to achieve remarkable success, speech-to-speech processing tasks like enhancement face data limitations, which lead data-hungry generative approaches to distort speech content and speaker identity. To bridge this gap, we present SpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS models into a universal speech processor capable of performing a wide range of speech tasks and composing them in novel ways at inference time. By adapting a pre-trained TTS model, SpeechOp inherits a rich understanding of natural speech, accelerating training and improving S2S task quality, while simultaneously enhancing core TTS performance. Finally, we introduce Implicit Task Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g., from Whisper) guide SpeechOp’s enhancement via our principled inference-time task composition. ITC achieves state-of-the-art content preservation by robustly combining web-scale speech understanding with SpeechOp’s generative capabilities. Audio samples are available at <a target="_blank" rel="noopener" href="https://justinlovelace.github.io/projects/speechop">https://justinlovelace.github.io/projects/speechop</a> </p>
<blockquote>
<p>虽然生成式文本到语音（TTS）系统利用大量的“野生”数据取得了显著的成功，但语音到语音的处理任务如增强面部数据仍存在局限性，这导致数据饥饿的生成方法会扭曲语音内容和说话人身份。为了弥补这一差距，我们提出了SpeechOp，这是一个多任务潜在扩散模型，它将预训练的TTS模型转变为通用语音处理器，能够在推理时间执行各种语音任务并以新颖的方式组合它们。通过适应预训练的TTS模型，SpeechOp继承了丰富的自然语音理解，加速了训练，提高了S2S任务质量，同时提高了核心TTS性能。最后，我们引入了隐式任务组合（ITC），这是一种新型管道，其中ASR衍生的转录本（例如，来自whisper）通过我们的有原则的推理时间任务组合来指导SpeechOp的增强。ITC通过稳健地将网页规模的语音理解与SpeechOp的生成能力相结合，实现了最新的内容保留。音频样本可在<a target="_blank" rel="noopener" href="https://justinlovelace.github.io/projects/speechop%E6%89%BE%E5%88%B0%E3%80%82">https://justinlovelace.github.io/projects/speechop找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14298v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本介绍了SpeechOp模型，这是一个多任务潜在扩散模型，能将预训练的文本转语音（TTS）模型转化为通用的语音处理器。SpeechOp可以执行多种语音任务并在推理时间以新颖的方式组合它们。通过适应预训练的TTS模型，SpeechOp继承了丰富的自然语音理解，加速训练，提高语音转语音（S2S）任务质量，同时提高TTS的核心性能。此外，还介绍了隐式任务组合（ITC）这一新流程，利用自动语音识别（ASR）产生的文本（如Whisper）引导SpeechOp的增强功能。ITC通过结合大规模的网页语音理解与SpeechOp的生成能力，实现了内容的最佳保留。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SpeechOp是一个多任务潜在扩散模型，能够将预训练的TTS模型转化为能够执行多种语音任务的通用语音处理器。</li>
<li>SpeechOp通过适应预训练的TTS模型，继承了丰富的自然语音理解，从而加速训练并提高语音处理任务的质量。</li>
<li>SpeechOp在推理时间可以以新颖的方式组合不同的语音任务。</li>
<li>ITC是一种隐式任务组合方法，利用ASR产生的文本引导SpeechOp进行语音增强。</li>
<li>ITC结合大规模的网页语音理解与SpeechOp的生成能力，实现了内容的最佳保留。</li>
<li>SpeechOp模型提高了TTS的核心性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14298">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-781827a641539cdcc40c2cf3dd3a541a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030345&auth_key=1760030345-0-0-fc6b328ba0e8155926f3a91042932e8f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8193b3664fd6144d6ce9b4677b468b3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030352&auth_key=1760030352-0-0-34f40fd8b4e25f0298c53a6d01deda37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d8eb3ace2cdbe0c5075c5d57930d9a3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030359&auth_key=1760030359-0-0-91325290904853d1f0540f7e7d9ea789&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e1433d077daad956dbedfcc751dbe4e0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030366&auth_key=1760030366-0-0-a174c96d2861970f6227e94f073f30b8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d2784922a57de09ecdc2c60aad3a180~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030372&auth_key=1760030372-0-0-9b3deb2ca177a8e8cca4772bca07a52f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GLAD-Global-Local-Aware-Dynamic-Mixture-of-Experts-for-Multi-Talker-ASR"><a href="#GLAD-Global-Local-Aware-Dynamic-Mixture-of-Experts-for-Multi-Talker-ASR" class="headerlink" title="GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR"></a>GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR</h2><p><strong>Authors:Yujie Guo, Jiaming Zhou, Yuhang Jia, Shiwan Zhao, Yong Qin</strong></p>
<p>End-to-end multi-talker automatic speech recognition (MTASR) faces significant challenges in accurately transcribing overlapping speech, especially under high-overlap conditions. To address these challenges, we proposed Global-Local Aware Dynamic (GLAD) Mixture-of-Experts, which dynamically fuse speaker-aware global information and fine-grained local features to guide expert selection. This mechanism enables speaker-specific routing by leveraging both global context and local acoustic cues. Experiments on LibriSpeechMix show that GLAD outperforms existing MTASR approaches, particularly in challenging multi-talker scenarios. To our best knowledge, this is the first work to apply Mixture-of-Experts (MoE) to end-to-end MTASR with a global-local fusion strategy. Our code and train dataset can be found at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/GLAD">https://github.com/NKU-HLT/GLAD</a>. </p>
<blockquote>
<p>端到端多说话人自动语音识别（MTASR）在准确转录重叠语音方面面临重大挑战，特别是在高重叠条件下。为了解决这些挑战，我们提出了Global-Local Aware Dynamic（GLAD）专家混合模型，该模型能够动态融合说话者感知全局信息和精细局部特征，以指导专家选择。这种机制通过利用全局上下文和局部声学线索，实现了针对说话者的特定路由。在LibriSpeechMix上的实验表明，GLAD优于现有的MTASR方法，特别是在具有挑战性的多说话人场景中。据我们所知，这是首次将专家混合（MoE）应用于端到端MTASR的全局-局部融合策略。我们的代码和训练数据集可在<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/GLAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/GLAD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13093v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对多说话人自动语音识别（MTASR）面临的挑战，提出了一种基于全局-局部感知动态（GLAD）的专家混合模型。该模型通过融合说话者感知的全局信息和精细的局部特征，实现了专家选择的动态融合，利用全局上下文和局部声学线索进行说话者特定路由。在LibriSpeechMix上的实验表明，GLAD在具有挑战性的多说话人场景下优于现有的MTASR方法。这是首次将专家混合（MoE）应用于端到端MTASR的全局-局部融合策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>端到端的多说话人自动语音识别（MTASR）在转录重叠语音时面临挑战。</li>
<li>提出的GLAD Mixture-of-Experts模型通过融合全局和局部信息实现动态专家选择。</li>
<li>GLAD模型利用全局上下文和局部声学线索进行说话者特定路由。</li>
<li>GLAD在LibriSpeechMix上的实验表现优于现有的MTASR方法，特别是在多说话人场景下。</li>
<li>这是首次将Mixture-of-Experts（MoE）应用于端到端MTASR的全局-局部融合策略。</li>
<li>模型的代码和训练数据集可在<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/GLAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/GLAD找到。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-ff3542de9a7a60faa71ff83a4a4ddc57~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030380&auth_key=1760030380-0-0-3d367c5b8476740a9e1578c5fa2351ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a18800e96fea9d321f982a9f8986e59d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030387&auth_key=1760030387-0-0-f93f3838dc5d20bd9a5490217e3ede94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f3e06600817fa6a5cff35f0b1b472174~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030393&auth_key=1760030393-0-0-050ec6c79eba9d3abc40b111a4e136f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1adf84ea58dba46ed91b8cad571ef46a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030400&auth_key=1760030400-0-0-bc058b72e01279b2b912db0bd0013264&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-12e0557ef391deb2000289e40fcfb25a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030407&auth_key=1760030407-0-0-e38c2beaef6185cf3594bd9e11631f50&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FunAudio-ASR-Technical-Report"><a href="#FunAudio-ASR-Technical-Report" class="headerlink" title="FunAudio-ASR Technical Report"></a>FunAudio-ASR Technical Report</h2><p><strong>Authors:Keyu An, Yanni Chen, Chong Deng, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Wen Wang, Wupeng Wang, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou</strong></p>
<p>In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, FunAudio-ASR achieves SOTA performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. </p>
<blockquote>
<p>近年来，自动语音识别（ASR）见证了由三个互补范式驱动的变革性进展：数据规模扩大、模型规模扩大，以及与大型语言模型（LLM）的深度集成。然而，LLM容易出现幻觉，这在真实世界的ASR应用中可能会显著地降低用户体验。在本文中，我们提出了FunAudio-ASR，这是一个基于LLM的大规模ASR系统，它协同结合了大规模数据、大型模型容量、LLM集成和强化学习，以在多样且复杂的语音识别场景中实现最先进的性能。此外，FunAudio-ASR针对实际部署进行了专门优化，增强了流式处理能力、噪声鲁棒性、代码切换、热词定制以及其他现实世界应用需求。实验结果表明，虽然大多数基于LLM的ASR系统在开源基准测试上表现强劲，但它们在真实的行业评估集上往往表现不佳。由于面向生产的优化，FunAudio-ASR在真实应用数据集上实现了SOTA性能，证明了其在现实环境中的有效性和稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12508v2">PDF</a> Authors are listed in alphabetical order</p>
<p><strong>Summary</strong><br>     近年来，语音识别技术受益于数据扩充、模型规模扩大以及与大型语言模型的深度融合，取得了突破性进展。然而，大型语言模型容易引发幻觉现象，显著降低了在现实世界语音识别应用中的用户体验。本研究提出FunAudio-ASR系统，该系统结合大规模数据、强大模型能力、大型语言模型集成及强化学习，优化为实际应用部署设计。它在多样的复杂语音识别场景中实现领先性能。特别在流能力、噪声稳健性、代码切换等方面做了改进和提升。实验证明，相较于大多数大型语言模型主导的语音识别系统，在真实行业评估集上表现欠佳的状况有所改善。得益于面向生产的优化，FunAudio-ASR在真实应用数据集上取得了最佳性能表现，证明了其在现实环境中的有效性和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别技术近年来取得突破性进展，得益于数据规模扩大、模型规模增长和大型语言模型的深度融合。</li>
<li>大型语言模型容易引发幻觉现象，影响用户体验。</li>
<li>FunAudio-ASR系统结合了大规模数据、大型模型、大型语言模型集成和强化学习技术，在多种复杂语音识别场景中实现先进性能。</li>
<li>FunAudio-ASR针对实际应用场景进行了优化，如流处理能力、噪声环境下的稳健性、代码切换等。</li>
<li>实验结果显示FunAudio-ASR在真实应用数据集上表现优异，证明其在现实环境中的有效性和稳健性。</li>
<li>尽管大型语言模型主导的语音识别系统在开源基准测试上表现良好，但在真实行业评估集上表现往往欠佳。通过面向生产的优化措施，可有效改善其性能表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-f5f5c11836ca4f4c98960459ac6dd8ed~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030414&auth_key=1760030414-0-0-f1cdd5e5d0931af83e1ca82b11bde1f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-edbaba40a9d3c83f4ddca95d4e211847~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030421&auth_key=1760030421-0-0-d82ded010877025a24d6c11bb193fb4b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a11573e614d61299ecfb4a2590bee7a8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030428&auth_key=1760030428-0-0-7f71875b3d3079f6bf78e4594d33aae1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb33f1721a23e49dd1c852765445b997~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030434&auth_key=1760030434-0-0-300dfc8525f96357a5a1e2d18d9b0f28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Large-Scale-Probing-Analysis-of-Speaker-Specific-Attributes-in-Self-Supervised-Speech-Representations"><a href="#A-Large-Scale-Probing-Analysis-of-Speaker-Specific-Attributes-in-Self-Supervised-Speech-Representations" class="headerlink" title="A Large-Scale Probing Analysis of Speaker-Specific Attributes in   Self-Supervised Speech Representations"></a>A Large-Scale Probing Analysis of Speaker-Specific Attributes in   Self-Supervised Speech Representations</h2><p><strong>Authors:Aemon Yat Fei Chiu, Kei Ching Fung, Roger Tsz Yeung Li, Jingyu Li, Tan Lee</strong></p>
<p>Speech self-supervised learning (SSL) models are known to learn hierarchical representations, yet how they encode different speaker-specific attributes remains under-explored. This study investigates the layer-wise disentanglement of speaker information across multiple speech SSL model families and their variants. Drawing from phonetic frameworks, we conduct a large-scale probing analysis of attributes categorised into functional groups: Acoustic (Gender), Prosodic (Pitch, Tempo, Energy), and Paralinguistic (Emotion), which we use to deconstruct the model’s representation of Speaker Identity. Our findings validate a consistent three-stage hierarchy: initial layers encode fundamental timbre and prosody; middle layers synthesise abstract traits; and final layers suppress speaker identity to abstract linguistic content. An ablation study shows that while specialised speaker embeddings excel at identifying speaker identity, the intermediate layers of speech SSL models better represent dynamic prosody. This work is the first large-scale study covering a wide range of speech SSL model families and variants with fine-grained speaker-specific attributes on how they hierarchically separate the dynamic style of speech from its intrinsic characteristics, offering practical implications for downstream tasks. </p>
<blockquote>
<p>语音自监督学习（SSL）模型已经学会学习分层表示，但它们如何编码不同的说话人特定属性仍然知之甚少。本研究探讨了多个语音SSL模型家族及其变体中说话人信息的逐层分离。借鉴语音学框架，我们对属性进行了大规模探测分析，按功能组分类：声学（性别）、韵律学（音调、语速、能量）和副语言（情感），用于解构模型对说话人身份的表示。我们的研究结果验证了三个阶段的层次结构的一致性：初始层编码基本音色和韵律；中间层合成抽象特征；最终层抑制说话者身份以抽象语言内容。消融研究表明，虽然专业的说话者嵌入在识别说话者身份方面表现出色，但语音SSL模型的中间层更能代表动态的韵律。这项工作是第一项大规模研究，涵盖了多种语音SSL模型家族及其变体，并详细研究了说话人特定的属性，它们如何分层地将语音的动态风格与其内在特征区分开来，为下游任务提供了实际影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05310v2">PDF</a> Submitted to the 2026 IEEE International Conference on Acoustics,   Speech, and Signal Processing (ICASSP 2026). Under review</p>
<p><strong>Summary</strong></p>
<p>本文探讨了不同语音自监督学习（SSL）模型在层级化表示中对说话人特定属性的编码方式。研究通过对多个SSL模型家族及其变种进行大规模探测分析，将属性分为功能组，包括声学（性别）、韵律（音调、语速、能量）和副语言（情感），以解构说话人身份的模型表示。研究发现了一个三层级的结构：初始层编码基本音色和韵律；中间层合成抽象特征；最终层抑制说话人身份以抽象语言内容。消融研究表明，虽然专业说话人嵌入在识别说话人身份方面表现出色，但语音SSL模型的中间层更能代表动态的韵律。本文首次大规模地研究了多种语音SSL模型家族及其变种如何层次分离语音的动态风格和其内在特征，为下游任务提供了实际启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音自监督学习（SSL）模型学习层次化的表示，但它们在编码说话人特定属性方面的机制尚未得到充分探索。</li>
<li>研究通过对多个SSL模型的大规模探测分析，发现说话人信息在模型中的层级化编码方式。</li>
<li>将属性分为声学、韵律和副语言功能组，以解构说话人身份的模型表示。</li>
<li>研究揭示了一个三层级的结构：初始层编码基本音色和韵律特征；中间层合成抽象特征；最终层则侧重于语言内容的抽象表达。</li>
<li>专业说话人嵌入在识别说话人身份方面表现出色，但语音SSL模型的中间层更能捕捉动态的韵律信息。</li>
<li>这是首个涵盖多种语音SSL模型的大规模研究，揭示了如何层次分离语音的动态风格和内在特征。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05310">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic-private.zhihu.com/v2-7214fbdc85b5ecdca0582938e7a83281~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030442&auth_key=1760030442-0-0-5119e2c9caf2ce181ba9e87dab89fc29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c57eeafcd8db399f54b277e5a14d106~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030450&auth_key=1760030450-0-0-109600a7aa4ece59d177ee5d2e62519d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-31f59ed2e2c83cbdcd1698369683a872~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030457&auth_key=1760030457-0-0-30ea00c7594afb6aedb92e5318bf8d48&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b5c34c303be0e6708b2745006f7b30a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030463&auth_key=1760030463-0-0-ca7798600433dcf0b01a86afb16dba1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ecc5c9b66d68c034dced30ff5c1373c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030470&auth_key=1760030470-0-0-1a202a85c1d97fffe451ae0b0c7b822b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b7d3876462396322261d3c0e91888454~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030477&auth_key=1760030477-0-0-b28a78202b6ec4634598954980c04fab&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-c6137d2050dabf879722f20e08ae7c07~resize:0:q75.jpg?source=1f5c5e47&expiration=1760030484&auth_key=1760030484-0-0-1231f51c80e9bc150c813eca5db23caa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-09-20  A Race Bias Free Face Aging Model for Reliable Kinship Verification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-324a4295b240cf74fe2dfe6d195e44fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760029917&auth_key=1760029917-0-0-d72b5219b9c3d93a181d9364e653aca7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-09-20  PVLM Parsing-Aware Vision Language Model with Dynamic Contrastive   Learning for Zero-Shot Deepfake Attribution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
