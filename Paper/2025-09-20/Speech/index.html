<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  SynParaSpeech Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1c71bd23752a4433c968d48c2060d1ec')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-20-æ›´æ–°"><a href="#2025-09-20-æ›´æ–°" class="headerlink" title="2025-09-20 æ›´æ–°"></a>2025-09-20 æ›´æ–°</h1><h2 id="SynParaSpeech-Automated-Synthesis-of-Paralinguistic-Datasets-for-Speech-Generation-and-Understanding"><a href="#SynParaSpeech-Automated-Synthesis-of-Paralinguistic-Datasets-for-Speech-Generation-and-Understanding" class="headerlink" title="SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding"></a>SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech   Generation and Understanding</h2><p><strong>Authors:Bingsong Bai, Qihang Lu, Wenbing Yang, Zihan Sun, YueRan Hou, Peilei Jia, Songbai Pu, Ruibo Fu, Yingming Gao, Ya Li, Jun Gao</strong></p>
<p>Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing more realistic and engaging speech. However, existing methods typically depend on proprietary datasets, while publicly available resources often suffer from incomplete speech, inaccurate or missing timestamps, and limited real-world relevance. To address these problems, we propose an automated framework for generating large-scale paralinguistic data and apply it to construct the SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with 118.75 hours of data and precise timestamps, all derived from natural conversational speech. Our contributions lie in introducing the first automated method for constructing large-scale paralinguistic datasets and releasing the SynParaSpeech corpus, which advances speech generation through more natural paralinguistic synthesis and enhances speech understanding by improving paralinguistic event detection. The dataset and audio samples are available at <a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech">https://github.com/ShawnPi233/SynParaSpeech</a>. </p>
<blockquote>
<p>å‰¯è¯­è¨€å£°éŸ³ï¼Œå¦‚ç¬‘å£°å’Œå¹æ¯å£°ï¼Œå¯¹äºåˆæˆæ›´çœŸå®ã€æ›´å¸å¼•äººçš„è¯­éŸ³è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸“æœ‰æ•°æ®é›†ï¼Œè€Œå…¬å¼€å¯ç”¨çš„èµ„æºå¾€å¾€å­˜åœ¨è¯­éŸ³ä¸å®Œæ•´ã€æ—¶é—´æˆ³ä¸å‡†ç¡®æˆ–ç¼ºå¤±ã€ä¸çœŸå®ä¸–ç•Œçš„å…³è”æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆå¤§è§„æ¨¡å‰¯è¯­è¨€æ•°æ®é›†çš„æ¡†æ¶ï¼Œå¹¶åº”ç”¨è¯¥æ¡†æ¶æ„å»ºäº†SynParaSpeechæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«6ä¸ªå‰¯è¯­è¨€ç±»åˆ«ï¼Œæ•°æ®æ—¶é•¿ä¸º118.75å°æ—¶ï¼Œå¸¦æœ‰ç²¾ç¡®çš„æ—¶é—´æˆ³ï¼Œå‡æ¥è‡ªè‡ªç„¶å¯¹è¯è¯­éŸ³ã€‚æˆ‘ä»¬çš„è´¡çŒ®åœ¨äºå¼•å…¥äº†æ„å»ºå¤§è§„æ¨¡å‰¯è¯­è¨€æ•°æ®é›†çš„é¦–ä¸ªè‡ªåŠ¨æ–¹æ³•ï¼Œå¹¶å‘å¸ƒäº†SynParaSpeechè¯­æ–™åº“ï¼Œè¿™é€šè¿‡æ›´è‡ªç„¶çš„å‰¯è¯­è¨€åˆæˆæ¨åŠ¨äº†è¯­éŸ³ç”Ÿæˆçš„å‘å±•ï¼Œå¹¶é€šè¿‡æé«˜å‰¯è¯­è¨€äº‹ä»¶æ£€æµ‹æ”¹å–„äº†è¯­éŸ³ç†è§£ã€‚æ•°æ®é›†å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/ShawnPi233/SynParaSpeechè·å–ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14946v1">PDF</a> submitted to ICASSP 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡ç« å¼ºè°ƒäº†è¾…è¯­è¨€å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå¹æ¯å£°ï¼‰åœ¨åˆæˆæ›´çœŸå®ã€æ›´å¸å¼•äººçš„è¯­éŸ³æ–¹é¢çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸“æœ‰æ•°æ®é›†ï¼Œè€Œå…¬å¼€å¯ç”¨çš„èµ„æºå¾€å¾€å­˜åœ¨è¯­éŸ³ä¸å®Œæ•´ã€æ—¶é—´æˆ³ä¸å‡†ç¡®æˆ–ç¼ºå¤±ã€ç°å®ç›¸å…³æ€§æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºç”Ÿæˆå¤§è§„æ¨¡è¾…è¯­è¨€æ•°æ®é›†çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¹¶æ®æ­¤æ„å»ºäº†SynParaSpeechæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«6ä¸ªè¾…è¯­è¨€ç±»åˆ«ï¼Œæ•°æ®æ—¶é•¿ä¸º118.75å°æ—¶ï¼Œå…·æœ‰ç²¾ç¡®çš„æ—¶é—´æˆ³ï¼Œå‡æ¥è‡ªè‡ªç„¶å¯¹è¯è¯­éŸ³ã€‚æœ¬æ–‡çš„è´¡çŒ®åœ¨äºå¼•å…¥äº†æ„å»ºå¤§è§„æ¨¡è¾…è¯­è¨€æ•°æ®é›†çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå¹¶å‘å¸ƒäº†SynParaSpeechè¯­æ–™åº“ï¼Œè¿™é€šè¿‡æ›´è‡ªç„¶çš„è¾…è¯­è¨€åˆæˆæ¨åŠ¨äº†è¯­éŸ³ç”Ÿæˆçš„å‘å±•ï¼Œå¹¶é€šè¿‡æ”¹è¿›è¾…è¯­è¨€äº‹ä»¶æ£€æµ‹æé«˜äº†è¯­éŸ³ç†è§£ã€‚æ•°æ®é›†å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShawnPi233/SynParaSpeech">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¾…è¯­è¨€å£°éŸ³å¯¹äºåˆæˆæ›´çœŸå®ã€æ›´å¸å¼•äººçš„è¯­éŸ³è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å­˜åœ¨ä¾èµ–ä¸“æœ‰æ•°æ®ã€å…¬å¼€èµ„æºè¯­éŸ³ä¸å®Œæ•´ã€æ—¶é—´æˆ³ä¸å‡†ç¡®æˆ–ç¼ºå¤±ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ç”¨äºç”Ÿæˆå¤§è§„æ¨¡è¾…è¯­è¨€æ•°æ®é›†ã€‚</li>
<li>æ„å»ºäº†åŒ…å«6ä¸ªè¾…è¯­è¨€ç±»åˆ«ã€118.75å°æ—¶æ•°æ®çš„SynParaSpeechæ•°æ®é›†ï¼Œå…·æœ‰ç²¾ç¡®æ—¶é—´æˆ³ã€‚</li>
<li>æ•°æ®é›†æ¥è‡ªè‡ªç„¶å¯¹è¯è¯­éŸ³ï¼Œå¢å¼ºäº†ç°å®ç›¸å…³æ€§ã€‚</li>
<li>è¯¥æ•°æ®é›†çš„å‘å¸ƒæ¨åŠ¨äº†æ›´è‡ªç„¶çš„è¾…è¯­è¨€åˆæˆå’Œè¯­éŸ³ç”Ÿæˆçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6fbc8980f0bb4584c0ef571ebf2a79d" align="middle">
<img src="https://picx.zhimg.com/v2-857a55cf3ee4c49f05b60ccf35358df7" align="middle">
<img src="https://picx.zhimg.com/v2-9cba3ec5dd80fddb5fffeb7c0416c3db" align="middle">
<img src="https://picx.zhimg.com/v2-ebf4bbf6c0de3a8caac8eabac37384e0" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="From-Hype-to-Insight-Rethinking-Large-Language-Model-Integration-in-Visual-Speech-Recognition"><a href="#From-Hype-to-Insight-Rethinking-Large-Language-Model-Integration-in-Visual-Speech-Recognition" class="headerlink" title="From Hype to Insight: Rethinking Large Language Model Integration in   Visual Speech Recognition"></a>From Hype to Insight: Rethinking Large Language Model Integration in   Visual Speech Recognition</h2><p><strong>Authors:Rishabh Jain, Naomi Harte</strong></p>
<p>Advances in self-supervised encoders have improved Visual Speech Recognition (VSR). Recent approaches integrating these encoders with LLM decoders improves transcription accuracy; however, it remains unclear whether these gains stem from visual understanding or stronger language modeling. In this work, we systematically evaluate LLM decoders by freezing or selectively updating the visual encoder, scaling decoder size, comparing adaptation strategies and architectures, and varying training data across LRS2, LRS3, and their combination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and adaptation yield limited improvements, while combining datasets enhances generalization. Semantic analysis reveals that gains arise primarily from lexical rather than semantic processing. Our Llama-2-13B model trained on the combined set achieves 24.7% WER on LRS3 and 47.0% on WildVSR, establishing SOTA among models trained without additional supervision. Our findings indicate LLM decoders refine contextual reasoning rather than visual features, emphasizing the need for stronger visual encoders to drive meaningful progress. </p>
<blockquote>
<p>éšç€è‡ªç›‘ç£ç¼–ç å™¨çš„è¿›æ­¥ï¼Œè§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰å¾—åˆ°äº†æå‡ã€‚æœ€è¿‘å°†è¿™äº›ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£ç å™¨ç»“åˆçš„æ–¹æ³•æé«˜äº†è½¬å½•å‡†ç¡®æ€§ï¼›ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šè¿™äº›æ”¶ç›Šæ˜¯æ¥æºäºè§†è§‰ç†è§£è¿˜æ˜¯æ›´å¼ºå¤§çš„è¯­è¨€å»ºæ¨¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å†»ç»“æˆ–é€‰æ‹©æ€§æ›´æ–°è§†è§‰ç¼–ç å™¨ã€æ‰©å±•è§£ç å™¨è§„æ¨¡ã€æ¯”è¾ƒé€‚åº”ç­–ç•¥å’Œæ¶æ„ã€ä»¥åŠåœ¨ä¸åŒæ•°æ®é›†ï¼ˆLRS2ã€LRS3åŠå…¶ç»„åˆï¼‰ä¹‹é—´æ”¹å˜è®­ç»ƒæ•°æ®æ¥ç³»ç»Ÿåœ°è¯„ä¼°LLMè§£ç å™¨ã€‚åœ¨LRS2ã€LRS3å’ŒWildVSRä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæ‰©å±•å’Œé€‚åº”å¸¦æ¥æœ‰é™çš„æ”¹è¿›ï¼Œè€Œç»„åˆæ•°æ®é›†å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ã€‚è¯­ä¹‰åˆ†æè¡¨æ˜ï¼Œæ”¶ç›Šä¸»è¦æ¥è‡ªäºè¯æ±‡è€Œéè¯­ä¹‰å¤„ç†ã€‚æˆ‘ä»¬çš„åœ¨ç»„åˆæ•°æ®é›†ä¸Šè®­ç»ƒçš„Llama-2-13Bæ¨¡å‹åœ¨LRS3ä¸Šå®ç°äº†24.7%çš„å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œåœ¨WildVSRä¸Šå®ç°äº†47.0%çš„å­—è¯é”™è¯¯ç‡ï¼Œæˆä¸ºåœ¨æ²¡æœ‰é¢å¤–ç›‘ç£çš„æƒ…å†µä¸‹è®­ç»ƒçš„æ¨¡å‹ä¸­æœ€æ–°é¡¶å°–æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMè§£ç å™¨æé«˜äº†ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œè€Œéè§†è§‰ç‰¹å¾ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦æ›´å¼ºå¤§çš„è§†è§‰ç¼–ç å™¨æ¥æ¨åŠ¨æœ‰æ„ä¹‰çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14880v1">PDF</a> submitted to ICASSP 2026. This work has been submitted to the IEEE   for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨è‡ªç›‘ç£ç¼–ç å™¨è¿›å±•çš„åŸºç¡€ä¸Šï¼Œå¦‚ä½•æ”¹å–„è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰çš„æŠ€æœ¯ã€‚æ–‡ç« æ·±å…¥è¯„ä¼°äº†LLMè§£ç å™¨çš„æ•ˆèƒ½ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†åœ¨å›ºå®šæˆ–é€‰æ‹©æ€§æ›´æ–°è§†è§‰ç¼–ç å™¨ã€è°ƒæ•´è§£ç å™¨è§„æ¨¡ã€å¯¹æ¯”ä¸åŒé€‚åº”ç­–ç•¥å’Œæ¶æ„ä»¥åŠä½¿ç”¨LRS2ã€LRS3åŠå…¶ç»„åˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒLLMè§£ç å™¨çš„è¡¨ç°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç»“åˆæ•°æ®é›†èƒ½æå‡æ³›åŒ–èƒ½åŠ›ï¼Œè¯­ä¹‰åˆ†æè¡¨æ˜æ•ˆæœä¸»è¦æ¥è‡ªäºè¯æ±‡è€Œéè¯­ä¹‰å¤„ç†ã€‚æœ€ä½³æ¨¡å‹åœ¨LRS3ä¸Šè¾¾åˆ°24.7%çš„WERï¼Œåœ¨WildVSRä¸Šè¾¾åˆ°47.0%çš„WERï¼Œä¸”åœ¨æ— éœ€é¢å¤–ç›‘ç£çš„æƒ…å†µä¸‹è¾¾åˆ°äº†å…ˆè¿›çš„è¡¨ç°ã€‚ç ”ç©¶æŒ‡å‡ºLLMè§£ç å™¨å¼ºåŒ–äº†ä¸Šä¸‹æ–‡æ¨ç†è€Œéè§†è§‰ç‰¹å¾ï¼Œå¼ºè°ƒäº†éœ€è¦æ›´å¼ºå¤§çš„è§†è§‰ç¼–ç å™¨æ¥æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£ç¼–ç å™¨çš„è¿›å±•æ”¹å–„äº†è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰ã€‚</li>
<li>LLMè§£ç å™¨ä¸è§†è§‰ç¼–ç å™¨çš„ç»“åˆæé«˜äº†è½¬å½•å‡†ç¡®æ€§ã€‚</li>
<li>è¯„ä¼°è¡¨æ˜ï¼Œè§£ç å™¨è§„æ¨¡çš„æ‰©å¤§å’Œé€‚åº”ç­–ç•¥ä»…å¸¦æ¥æœ‰é™æ”¹è¿›ï¼Œè€Œç»“åˆæ•°æ®é›†æœ‰åŠ©äºå¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯­ä¹‰åˆ†ææ˜¾ç¤ºï¼Œæ•ˆæœä¸»è¦æ¥è‡ªäºè¯æ±‡å¤„ç†è€Œéè¯­ä¹‰å¤„ç†ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨LRS3å’ŒWildVSRä¸Šçš„è¡¨ç°è¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</li>
<li>LLMè§£ç å™¨å¼ºåŒ–äº†ä¸Šä¸‹æ–‡æ¨ç†ï¼Œè€Œéè§†è§‰ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec2442939bc6d8673133c9a100650e1c" align="middle">
<img src="https://picx.zhimg.com/v2-844772b1555e72c264c6866ce9102c3e" align="middle">
<img src="https://picx.zhimg.com/v2-adc808f3204b2f84eeb43b82c723a3e4" align="middle">
<img src="https://picx.zhimg.com/v2-9b87bee07a2508705df31045cd7290b7" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AmbiDrop-Array-Agnostic-Speech-Enhancement-Using-Ambisonics-Encoding-and-Dropout-Based-Learning"><a href="#AmbiDrop-Array-Agnostic-Speech-Enhancement-Using-Ambisonics-Encoding-and-Dropout-Based-Learning" class="headerlink" title="AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding   and Dropout-Based Learning"></a>AmbiDrop: Array-Agnostic Speech Enhancement Using Ambisonics Encoding   and Dropout-Based Learning</h2><p><strong>Authors:Michael Tatarjitzky, Boaz Rafaely</strong></p>
<p>Multichannel speech enhancement leverages spatial cues to improve intelligibility and quality, but most learning-based methods rely on specific microphone array geometry, unable to account for geometry changes. To mitigate this limitation, current array-agnostic approaches employ large multi-geometry datasets but may still fail to generalize to unseen layouts. We propose AmbiDrop (Ambisonics with Dropouts), an Ambisonics-based framework that encodes arbitrary array recordings into the spherical harmonics domain using Ambisonics Signal Matching (ASM). A deep neural network is trained on simulated Ambisonics data, combined with channel dropout for robustness against array-dependent encoding errors, therefore omitting the need for a diverse microphone array database. Experiments show that while the baseline and proposed models perform similarly on the training arrays, the baseline degrades on unseen arrays. In contrast, AmbiDrop consistently improves SI-SDR, PESQ, and STOI, demonstrating strong generalization and practical potential for array-agnostic speech enhancement. </p>
<blockquote>
<p>å¤šé€šé“è¯­éŸ³å¢å¼ºåˆ©ç”¨ç©ºé—´çº¿ç´¢æé«˜è¯­éŸ³çš„æ¸…æ™°åº¦å’Œè´¨é‡ï¼Œä½†å¤§å¤šæ•°åŸºäºå­¦ä¹ çš„æ–¹æ³•ä¾èµ–äºç‰¹å®šçš„éº¦å…‹é£é˜µåˆ—å‡ ä½•ç»“æ„ï¼Œæ— æ³•é€‚åº”å‡ ä½•å˜åŒ–ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€å±€é™æ€§ï¼Œå½“å‰çš„é˜µåˆ—æ— å…³æ–¹æ³•é‡‡ç”¨å¤§å‹å¤šå‡ ä½•æ•°æ®é›†ï¼Œä½†ä»å¯èƒ½æ— æ³•æ¨å¹¿åˆ°æœªè§è¿‡çš„å¸ƒå±€ã€‚æˆ‘ä»¬æå‡ºäº†AmbiDropï¼ˆå¸¦æœ‰ç¼ºå¤±å€¼çš„å››é¢ä½“å£°æŠ€æœ¯ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå››é¢ä½“å£°æŠ€æœ¯çš„æ¡†æ¶ï¼Œå®ƒä½¿ç”¨å››é¢ä½“å£°ä¿¡å·åŒ¹é…ï¼ˆASMï¼‰å°†ä»»æ„é˜µåˆ—å½•éŸ³ç¼–ç åˆ°çƒé¢è°æ³¢åŸŸã€‚æ·±åº¦ç¥ç»ç½‘ç»œåœ¨æ¨¡æ‹Ÿçš„å››é¢ä½“å£°æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç»“åˆé€šé“ä¸¢å¤±ä»¥å¢å¼ºå¯¹é˜µåˆ—ç›¸å…³ç¼–ç é”™è¯¯çš„é²æ£’æ€§ï¼Œä»è€Œæ— éœ€ä½¿ç”¨å¤šæ ·åŒ–çš„éº¦å…‹é£é˜µåˆ—æ•°æ®åº“ã€‚å®éªŒè¡¨æ˜ï¼Œè™½ç„¶åœ¨è®­ç»ƒé˜µåˆ—ä¸Šï¼ŒåŸºçº¿æ¨¡å‹å’Œæ‰€æå‡ºçš„æ¨¡å‹è¡¨ç°ç›¸ä¼¼ï¼Œä½†åŸºçº¿æ¨¡å‹åœ¨æœªè§è¿‡çš„é˜µåˆ—ä¸Šæ€§èƒ½ä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒAmbiDropæŒç»­æé«˜äº†SI-SDRã€PESQå’ŒSTOIæŒ‡æ ‡ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œé˜µåˆ—æ— å…³çš„è¯­éŸ³å¢å¼ºçš„å®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14855v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºAmbiDropçš„åŸºäºAmbisonicsçš„æ¡†æ¶ï¼Œç”¨äºå¯¹ä»»æ„é˜µåˆ—å½•éŸ³è¿›è¡Œç¼–ç ã€‚è¯¥æ¡†æ¶é‡‡ç”¨Ambisonicsä¿¡å·åŒ¹é…ï¼ˆASMï¼‰æŠ€æœ¯ï¼Œå°†å½•éŸ³è½¬åŒ–ä¸ºçƒé¢è°æ³¢åŸŸã€‚é€šè¿‡è®­ç»ƒåœ¨æ¨¡æ‹Ÿçš„Ambisonicsæ•°æ®ä¸Šçš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¹¶ç»“åˆé€šé“ä¸¢å¼ƒç­–ç•¥ï¼Œä»¥å¢å¼ºå¯¹é˜µåˆ—ä¾èµ–çš„ç¼–ç é”™è¯¯çš„ç¨³å¥æ€§ï¼Œä»è€Œæ— éœ€ä½¿ç”¨å¤šæ ·åŒ–çš„éº¦å…‹é£é˜µåˆ—æ•°æ®åº“ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æœªè§è¿‡çš„é˜µåˆ—ä¸Šï¼ŒAmbiDropç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨SI-SDRã€PESQå’ŒSTOIç­‰æŒ‡æ ‡ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šé€šé“è¯­éŸ³å¢å¼ºåˆ©ç”¨ç©ºé—´çº¿ç´¢æé«˜è¯­éŸ³çš„æ¸…æ™°åº¦å’Œè´¨é‡ã€‚</li>
<li>å¤§å¤šæ•°åŸºäºå­¦ä¹ çš„æ–¹æ³•ä¾èµ–äºç‰¹å®šçš„éº¦å…‹é£é˜µåˆ—å‡ ä½•ç»“æ„ï¼Œæ— æ³•é€‚åº”å‡ ä½•å˜åŒ–ã€‚</li>
<li>å½“å‰çš„é˜µåˆ—æ— å…³æ–¹æ³•ä½¿ç”¨å¤šå‡ ä½•æ•°æ®é›†ï¼Œä½†ä»å¯èƒ½æ— æ³•æ³›åŒ–åˆ°æœªè§è¿‡çš„å¸ƒå±€ã€‚</li>
<li>æå‡ºçš„AmbiDropæ¡†æ¶é‡‡ç”¨AmbisonicsæŠ€æœ¯ï¼Œå°†ä»»æ„é˜µåˆ—å½•éŸ³ç¼–ç åˆ°çƒé¢è°æ³¢åŸŸã€‚</li>
<li>é€šè¿‡è®­ç»ƒåœ¨æ¨¡æ‹Ÿçš„Ambisonicsæ•°æ®ä¸Šçš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç»“åˆé€šé“ä¸¢å¼ƒç­–ç•¥ï¼Œæé«˜æ¨¡å‹å¯¹é˜µåˆ—ä¾èµ–çš„ç¼–ç é”™è¯¯çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œåœ¨æœªè§è¿‡çš„é˜µåˆ—ä¸Šï¼ŒAmbiDropç›¸è¾ƒäºåŸºçº¿æ¨¡å‹è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3803e64d5ee4196632817e5386434d83" align="middle">
<img src="https://picx.zhimg.com/v2-05d081e2ef8a49d19c3e88bdac709075" align="middle">
<img src="https://picx.zhimg.com/v2-e5a5abd113033aae32fb923a80138bfd" align="middle">
<img src="https://picx.zhimg.com/v2-cfbb0f8702ba771782719240ce711e79" align="middle">
<img src="https://picx.zhimg.com/v2-98415f2e948217d45a9ac41ef1ef03f9" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Acoustic-Simulation-Framework-for-Multi-channel-Replay-Speech-Detection"><a href="#Acoustic-Simulation-Framework-for-Multi-channel-Replay-Speech-Detection" class="headerlink" title="Acoustic Simulation Framework for Multi-channel Replay Speech Detection"></a>Acoustic Simulation Framework for Multi-channel Replay Speech Detection</h2><p><strong>Authors:Michael Neri, Tuomas Virtanen</strong></p>
<p>Replay speech attacks pose a significant threat to voice-controlled systems, especially in smart environments where voice assistants are widely deployed. While multi-channel audio offers spatial cues that can enhance replay detection robustness, existing datasets and methods predominantly rely on single-channel recordings. In this work, we introduce an acoustic simulation framework designed to simulate multi-channel replay speech configurations using publicly available resources. Our setup models both genuine and spoofed speech across varied environments, including realistic microphone and loudspeaker impulse responses, room acoustics, and noise conditions. The framework employs measured loudspeaker directionalities during the replay attack to improve the realism of the simulation. We define two spoofing settings, which simulate whether a reverberant or an anechoic speech is used in the replay scenario, and evaluate the impact of omnidirectional and diffuse noise on detection performance. Using the state-of-the-art M-ALRAD model for replay speech detection, we demonstrate that synthetic data can support the generalization capabilities of the detector across unseen enclosures. </p>
<blockquote>
<p>å›æ”¾è¯­éŸ³æ”»å‡»å¯¹å£°æ§ç³»ç»Ÿæ„æˆé‡å¤§å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¿æ³›éƒ¨ç½²è¯­éŸ³åŠ©æ‰‹çš„æ™ºèƒ½ç¯å¢ƒä¸­ã€‚è™½ç„¶å¤šé€šé“éŸ³é¢‘æä¾›äº†å¯ä»¥å¢å¼ºå›æ”¾æ£€æµ‹ç¨³å¥æ€§çš„ç©ºé—´çº¿ç´¢ï¼Œä½†ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦ä¾èµ–äºå•é€šé“å½•éŸ³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå£°å­¦æ¨¡æ‹Ÿæ¡†æ¶ï¼Œåˆ©ç”¨å…¬å¼€èµ„æºè®¾è®¡æ¨¡æ‹Ÿå¤šé€šé“å›æ”¾è¯­éŸ³é…ç½®ã€‚æˆ‘ä»¬çš„è®¾ç½®æ¨¡å‹æ¶µç›–äº†å„ç§ç¯å¢ƒä¸‹çš„çœŸå®å’Œå‡å†’è¯­éŸ³ï¼ŒåŒ…æ‹¬çœŸå®çš„éº¦å…‹é£å’Œæ‰¬å£°å™¨è„‰å†²å“åº”ã€æˆ¿é—´å£°å­¦ç‰¹æ€§å’Œå™ªå£°æ¡ä»¶ã€‚è¯¥æ¡†æ¶åœ¨å›æ”¾æ”»å‡»æœŸé—´é‡‡ç”¨æµ‹é‡çš„æ‰¬å£°å™¨æ–¹å‘æ€§ï¼Œä»¥æé«˜æ¨¡æ‹Ÿçš„çœŸå®æ€§ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸¤ç§æ¬ºéª—è®¾ç½®ï¼Œæ¨¡æ‹Ÿå›æ”¾åœºæ™¯ä¸­æ˜¯å¦ä½¿ç”¨æ··å“æˆ–æ— å£°è¯­éŸ³ï¼Œå¹¶è¯„ä¼°äº†å…¨å‘å’Œæ‰©æ•£å™ªå£°å¯¹æ£€æµ‹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬ä½¿ç”¨å…ˆè¿›çš„M-ALRADæ¨¡å‹è¿›è¡Œå›æ”¾è¯­éŸ³æ£€æµ‹ï¼Œè¯æ˜åˆæˆæ•°æ®å¯ä»¥æ”¯æŒæ£€æµ‹å™¨åœ¨æœªè§è¿‡çš„å°é—­ç©ºé—´ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14789v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>å¤šé€šé“å›æ”¾è¯­éŸ³å¯¹è¯­éŸ³æ§åˆ¶ç³»ç»Ÿæ„æˆé‡å¤§å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ™ºèƒ½ç¯å¢ƒä¸­å¹¿æ³›éƒ¨ç½²çš„è¯­éŸ³åŠ©æ‰‹ã€‚å°½ç®¡å¤šé€šé“éŸ³é¢‘å¯ä»¥æä¾›å¢å¼ºå›æ”¾æ£€æµ‹ç¨³å¥æ€§çš„ç©ºé—´çº¿ç´¢ï¼Œä½†ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦ä¾èµ–äºå•é€šé“å½•éŸ³ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªå£°å­¦æ¨¡æ‹Ÿæ¡†æ¶ï¼Œåˆ©ç”¨å…¬å¼€èµ„æºæ¨¡æ‹Ÿå¤šé€šé“å›æ”¾è¯­éŸ³é…ç½®ã€‚è¯¥æ¡†æ¶æ¨¡æ‹Ÿå„ç§ç¯å¢ƒï¼ŒåŒ…æ‹¬çœŸå®çš„éº¦å…‹é£å’Œæ‰¬å£°å™¨å†²å‡»å“åº”ã€æˆ¿é—´å£°å­¦ç‰¹æ€§å’Œå™ªå£°æ¡ä»¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å›æ”¾æ”»å‡»æœŸé—´çš„å®æµ‹æ‰¬å£°å™¨æ–¹å‘æ€§æ¥æé«˜æ¨¡æ‹Ÿçš„çœŸå®æ€§ã€‚å®šä¹‰äº†ä¸¤ä¸ªæ¬ºéª—è®¾ç½®ï¼Œæ¨¡æ‹Ÿå›æ”¾åœºæ™¯ä¸­æ˜¯å¦ä½¿ç”¨æ··å“æˆ–æ— å£°è¯­éŸ³ï¼Œå¹¶è¯„ä¼°äº†å…¨å‘å’Œæ‰©æ•£å™ªå£°å¯¹æ£€æµ‹æ€§èƒ½çš„å½±å“ã€‚åˆ©ç”¨å…ˆè¿›çš„M-ALRADæ¨¡å‹è¿›è¡Œå›æ”¾è¯­éŸ³æ£€æµ‹ï¼Œè¯æ˜åˆæˆæ•°æ®å¯ä»¥å¢å¼ºæ£€æµ‹å™¨åœ¨æœªè§è¿‡çš„å°é—­ç©ºé—´ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šé€šé“å›æ”¾è¯­éŸ³å¯¹è¯­éŸ³æ§åˆ¶ç³»ç»Ÿæ„æˆå¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨æ™ºèƒ½ç¯å¢ƒä¸­ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦ä¾èµ–å•é€šé“å½•éŸ³ï¼Œç¼ºä¹å¤šé€šé“éŸ³é¢‘çš„ç©ºé—´çº¿ç´¢ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå£°å­¦æ¨¡æ‹Ÿæ¡†æ¶ï¼Œç”¨ä»¥æ¨¡æ‹Ÿå¤šé€šé“å›æ”¾è¯­éŸ³é…ç½®å’Œå¤šç§ç¯å¢ƒã€‚</li>
<li>æ¡†æ¶è€ƒè™‘çœŸå®çš„éº¦å…‹é£å’Œæ‰¬å£°å™¨å†²å‡»å“åº”ã€æˆ¿é—´å£°å­¦ç‰¹æ€§å’Œå™ªå£°æ¡ä»¶ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨å®æµ‹çš„æ‰¬å£°å™¨æ–¹å‘æ€§åœ¨å›æ”¾æ”»å‡»æ—¶æé«˜æ¨¡æ‹ŸçœŸå®æ€§ã€‚</li>
<li>å®šä¹‰äº†ä¸¤ä¸ªæ¬ºéª—è®¾ç½®ï¼Œä»¥æ¨¡æ‹Ÿå›æ”¾åœºæ™¯ä¸­çš„æ··å“æˆ–æ— å£°è¯­éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ef82dd3c35791fccf640500b17ee8d9" align="middle">
<img src="https://picx.zhimg.com/v2-0b887e1eb24748a1dfc3e35aa377b05c" align="middle">
<img src="https://picx.zhimg.com/v2-f17aae2dc52936d9f0db59f352e36289" align="middle">
<img src="https://picx.zhimg.com/v2-038c46c4069ad52ea4ee2eec774b5064" align="middle">
<img src="https://picx.zhimg.com/v2-967098d396e6399cdd60b4bc3ccd684e" align="middle">
<img src="https://picx.zhimg.com/v2-daf4a882de38bd9d6eb4671bae2b2695" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MELA-TTS-Joint-transformer-diffusion-model-with-representation-alignment-for-speech-synthesis"><a href="#MELA-TTS-Joint-transformer-diffusion-model-with-representation-alignment-for-speech-synthesis" class="headerlink" title="MELA-TTS: Joint transformer-diffusion model with representation   alignment for speech synthesis"></a>MELA-TTS: Joint transformer-diffusion model with representation   alignment for speech synthesis</h2><p><strong>Authors:Keyu An, Zhiyu Zhang, Changfeng Gao, Yabin Li, Zhendong Peng, Haoxu Wang, Zhihao Du, Han Zhao, Zhifu Gao, Xiangang Li</strong></p>
<p>This work introduces MELA-TTS, a novel joint transformer-diffusion framework for end-to-end text-to-speech synthesis. By autoregressively generating continuous mel-spectrogram frames from linguistic and speaker conditions, our architecture eliminates the need for speech tokenization and multi-stage processing pipelines. To address the inherent difficulties of modeling continuous features, we propose a representation alignment module that aligns output representations of the transformer decoder with semantic embeddings from a pretrained ASR encoder during training. This mechanism not only speeds up training convergence, but also enhances cross-modal coherence between the textual and acoustic domains. Comprehensive experiments demonstrate that MELA-TTS achieves state-of-the-art performance across multiple evaluation metrics while maintaining robust zero-shot voice cloning capabilities, in both offline and streaming synthesis modes. Our results establish a new benchmark for continuous feature generation approaches in TTS, offering a compelling alternative to discrete-token-based paradigms. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MELA-TTSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆè”åˆTransformer-Diffusionæ¡†æ¶ã€‚é€šè¿‡è‡ªå›å½’ç”Ÿæˆè¯­è¨€æ¡ä»¶å’Œè¯´è¯è€…æ¡ä»¶ä¸‹çš„è¿ç»­æ¢…å°”é¢‘è°±å¸§ï¼Œæˆ‘ä»¬çš„æ¶æ„æ¶ˆé™¤äº†å¯¹è¯­éŸ³æ ‡è®°åŒ–å’Œå¤šé˜¶æ®µå¤„ç†ç®¡é“çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³å»ºæ¨¡è¿ç»­ç‰¹å¾å›ºæœ‰çš„å›°éš¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¡¨ç¤ºå¯¹é½æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è®­ç»ƒæœŸé—´å°†å¯¹é½transformerè§£ç å™¨çš„è¾“å‡ºè¡¨ç¤ºä¸é¢„è®­ç»ƒçš„ASRç¼–ç å™¨çš„è¯­ä¹‰åµŒå…¥ã€‚è¿™ç§æœºåˆ¶ä¸ä»…åŠ å¿«äº†è®­ç»ƒæ”¶æ•›é€Ÿåº¦ï¼Œè€Œä¸”å¢å¼ºäº†æ–‡æœ¬å’Œå£°å­¦åŸŸä¹‹é—´çš„è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMELA-TTSåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ç¦»çº¿åˆæˆæ¨¡å¼å’Œæµå¼åˆæˆæ¨¡å¼ä¸‹çš„é›¶æ ·æœ¬è¯­éŸ³å…‹éš†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœä¸ºTTSä¸­çš„è¿ç»­ç‰¹å¾ç”Ÿæˆæ–¹æ³•å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œä¸ºåŸºäºç¦»æ•£æ ‡è®°çš„æ–¹æ³•æä¾›äº†å¼•äººæ³¨ç›®çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14784v1">PDF</a> submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MELA-TTSï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è”åˆTransformer-Diffusionæ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆã€‚é€šè¿‡è‡ªå›å½’ç”Ÿæˆè¿ç»­çš„æ¢…å°”é¢‘è°±å›¾å¸§ï¼Œè¯¥æ¶æ„æ¶ˆé™¤äº†å¯¹è¯­éŸ³æ ‡è®°åŒ–å’Œå¤šé˜¶æ®µå¤„ç†ç®¡é“çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿ç»­ç‰¹å¾å»ºæ¨¡çš„å›ºæœ‰å›°éš¾ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è¡¨ç¤ºå¯¹é½æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è®­ç»ƒæœŸé—´å°†Transformerè§£ç å™¨çš„è¾“å‡ºè¡¨ç¤ºä¸é¢„è®­ç»ƒçš„ASRç¼–ç å™¨çš„è¯­ä¹‰åµŒå…¥è¿›è¡Œå¯¹é½ã€‚è¿™ç§æœºåˆ¶ä¸ä»…åŠ å¿«äº†è®­ç»ƒæ”¶æ•›é€Ÿåº¦ï¼Œè€Œä¸”å¢å¼ºäº†æ–‡æœ¬å’Œå£°éŸ³åŸŸä¹‹é—´çš„è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMELA-TTSåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ç¦»çº¿åˆæˆæ¨¡å¼å’Œæµå¼åˆæˆæ¨¡å¼ä¸‹çš„é›¶æ ·æœ¬è¯­éŸ³å…‹éš†èƒ½åŠ›ã€‚è¿™ä¸ºTTSä¸­çš„è¿ç»­ç‰¹å¾ç”Ÿæˆæ–¹æ³•å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œä¸ºåŸºäºç¦»æ•£ç¬¦å·çš„æ–¹æ³•æä¾›äº†å¼•äººæ³¨ç›®çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MELA-TTSæ˜¯ä¸€ç§æ–°é¢–çš„è”åˆTransformer-Diffusionæ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆã€‚</li>
<li>è¯¥æ¶æ„é€šè¿‡è‡ªå›å½’ç”Ÿæˆè¿ç»­çš„æ¢…å°”é¢‘è°±å›¾å¸§ï¼Œæ— éœ€è¯­éŸ³æ ‡è®°åŒ–å’Œå¤šé˜¶æ®µå¤„ç†ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè¡¨ç¤ºå¯¹é½æ¨¡å—ï¼Œä»¥åœ¨è®­ç»ƒæœŸé—´æé«˜æ–‡æœ¬å’Œè¯­éŸ³åŸŸä¹‹é—´çš„è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æœºåˆ¶ä¸ä»…åŠ å¿«äº†è®­ç»ƒæ”¶æ•›é€Ÿåº¦ï¼Œè€Œä¸”å¢å¼ºäº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>MELA-TTSåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>MELA-TTSæ”¯æŒç¦»çº¿åˆæˆæ¨¡å¼å’Œæµå¼åˆæˆæ¨¡å¼ä¸‹çš„é›¶æ ·æœ¬è¯­éŸ³å…‹éš†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-931235bc9ff842cef1d3dd4333a974c3" align="middle">
<img src="https://picx.zhimg.com/v2-83ad6ba3f409d90492dae797dd3f803b" align="middle">
<img src="https://picx.zhimg.com/v2-1c71bd23752a4433c968d48c2060d1ec" align="middle">
<img src="https://picx.zhimg.com/v2-732b51e234adec9a9e43707b280872e6" align="middle">
<img src="https://picx.zhimg.com/v2-1c31bbd5def7d35a0fb0da4866a11b7e" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HARNESS-Lightweight-Distilled-Arabic-Speech-Foundation-Models"><a href="#HARNESS-Lightweight-Distilled-Arabic-Speech-Foundation-Models" class="headerlink" title="HARNESS: Lightweight Distilled Arabic Speech Foundation Models"></a>HARNESS: Lightweight Distilled Arabic Speech Foundation Models</h2><p><strong>Authors:Vrunda N. sukhadia, Shammur Absar Chowdhury</strong></p>
<p>Large pre-trained speech models excel in downstream tasks but their deployment is impractical for resource-limited environments. In this paper, we introduce HArnESS, the first Arabic-centric self-supervised speech model family, designed to capture Arabic speech nuances. Using iterative self-distillation, we train large bilingual HArnESS (HL) SSL models and then distill knowledge into compressed student models (HS, HST), preserving Arabic-specific representations. We use low-rank approximation to further compact the teacherâ€™s discrete supervision into shallow, thin models. We evaluate HArnESS on Arabic ASR, Speaker Emotion Recognition (SER), and Dialect Identification (DID), demonstrating effectiveness against HuBERT and XLS-R. With minimal fine-tuning, HArnESS achieves SOTA or comparable performance, making it a lightweight yet powerful alternative for real-world use. We release our distilled models and findings to support responsible research and deployment in low-resource settings. </p>
<blockquote>
<p>å¤§å‹é¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å…¶éƒ¨ç½²å¹¶ä¸å®ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HArnESSï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥é˜¿æ‹‰ä¼¯è¯­ä¸ºä¸­å¿ƒçš„è‡ªæˆ‘ç›‘ç£è¯­éŸ³æ¨¡å‹å®¶æ—ï¼Œæ—¨åœ¨æ•æ‰é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³çš„ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬ä½¿ç”¨è¿­ä»£è‡ªè’¸é¦æŠ€æœ¯è®­ç»ƒå¤§å‹åŒè¯­HArnESSï¼ˆHLï¼‰SSLæ¨¡å‹ï¼Œç„¶åå°†çŸ¥è¯†è’¸é¦åˆ°å‹ç¼©çš„å­¦ç”Ÿæ¨¡å‹ï¼ˆHSï¼ŒHSTï¼‰ä¸­ï¼ŒåŒæ—¶ä¿ç•™é˜¿æ‹‰ä¼¯è¯­çš„ç‰¹å®šè¡¨ç¤ºã€‚æˆ‘ä»¬ä½¿ç”¨ä½ç§©è¿‘ä¼¼æ¥è¿›ä¸€æ­¥å°†æ•™å¸ˆçš„ç¦»æ•£ç›‘ç£å‹ç¼©æˆæµ…è–„çš„æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨é˜¿æ‹‰ä¼¯è¯­ASRã€è¯´è¯äººæƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰å’Œæ–¹è¨€è¯†åˆ«ï¼ˆDIDï¼‰ä¸Šå¯¹HArnESSè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸HuBERTå’ŒXLS-Rç›¸æ¯”è¡¨ç°å‡ºå…¶æœ‰æ•ˆæ€§ã€‚HArnESSåœ¨æœ€å°å¾®è°ƒçš„æƒ…å†µä¸‹è¾¾åˆ°äº†SOTAæˆ–ç›¸å½“çš„æ€§èƒ½ï¼Œæˆä¸ºç°å®ä¸–ç•Œä½¿ç”¨çš„è½»ä¾¿è€Œå¼ºå¤§çš„æ›¿ä»£å“ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„è’¸é¦æ¨¡å‹å’Œç ”ç©¶æˆæœï¼Œä»¥æ”¯æŒä½èµ„æºç¯å¢ƒä¸­çš„è´Ÿè´£ä»»ç ”ç©¶å’Œéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14689v1">PDF</a> 5 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é˜¿æ‹‰ä¼¯è¯­éŸ³å¤„ç†æ¨¡å‹çš„æå‡ºï¼šå¼•å…¥é¦–ä¸ªé˜¿æ‹‰ä¼¯è¯­ä¸ºä¸­å¿ƒçš„è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹å®¶æ—HArnESSï¼Œé‡‡ç”¨è¿­ä»£è‡ªè’¸é¦æ–¹æ³•è®­ç»ƒå¤§å‹åŒè¯­æ¨¡å‹ï¼Œå†å°†å…¶çŸ¥è¯†è’¸é¦ä¸ºå‹ç¼©çš„å­¦ç”Ÿæ¨¡å‹ã€‚é‡‡ç”¨ä½ç§©é€¼è¿‘å°†æ•™å¸ˆçš„ç¦»æ•£ç›‘ç£å‹ç¼©ä¸ºæµ…ç˜¦æ¨¡å‹ï¼Œä»¥é€‚åº”èµ„æºå—é™çš„ç¯å¢ƒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ç™½å…¶åœ¨é˜¿æ‹‰ä¼¯è¯­ASRã€è¯´è¯äººæƒ…ç»ªè¯†åˆ«å’Œæ–¹è¨€è¯†åˆ«æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè¾¾åˆ°äº†æœ€å‰æ²¿æˆ–å¯æ¯”æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b48fa92a7b286e1fe3c435e549fc19e3" align="middle">
<img src="https://picx.zhimg.com/v2-20e3665d5f7aa1bcd0ac7203053768a7" align="middle">
<img src="https://picx.zhimg.com/v2-8cb6308b423f2c9ce4e6516de867ea6f" align="middle">
<img src="https://picx.zhimg.com/v2-32cc45f89096e521b4abb1205fedf75a" align="middle">
<img src="https://picx.zhimg.com/v2-3d06eedb7a10af8f253f7665dedb9e42" align="middle">
<img src="https://picx.zhimg.com/v2-d1e9884c161d15e0d7b8e7ebc8c50aa0" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DAIEN-TTS-Disentangled-Audio-Infilling-for-Environment-Aware-Text-to-Speech-Synthesis"><a href="#DAIEN-TTS-Disentangled-Audio-Infilling-for-Environment-Aware-Text-to-Speech-Synthesis" class="headerlink" title="DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware   Text-to-Speech Synthesis"></a>DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware   Text-to-Speech Synthesis</h2><p><strong>Authors:Ye-Xin Lu, Yu Gu, Kun Wei, Hui-Peng Du, Yang Ai, Zhen-Hua Ling</strong></p>
<p>This paper presents DAIEN-TTS, a zero-shot text-to-speech (TTS) framework that enables ENvironment-aware synthesis through Disentangled Audio Infilling. By leveraging separate speaker and environment prompts, DAIEN-TTS allows independent control over the timbre and the background environment of the synthesized speech. Built upon F5-TTS, the proposed DAIEN-TTS first incorporates a pretrained speech-environment separation (SES) module to disentangle the environmental speech into mel-spectrograms of clean speech and environment audio. Two random span masks of varying lengths are then applied to both mel-spectrograms, which, together with the text embedding, serve as conditions for infilling the masked environmental mel-spectrogram, enabling the simultaneous continuation of personalized speech and time-varying environmental audio. To further enhance controllability during inference, we adopt dual class-free guidance (DCFG) for the speech and environment components and introduce a signal-to-noise ratio (SNR) adaptation strategy to align the synthesized speech with the environment prompt. Experimental results demonstrate that DAIEN-TTS generates environmental personalized speech with high naturalness, strong speaker similarity, and high environmental fidelity. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DAIEN-TTSï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡è§£è€¦éŸ³é¢‘å¡«å……ï¼ˆDisentangled Audio Infillingï¼‰å®ç°äº†ç¯å¢ƒæ„ŸçŸ¥åˆæˆã€‚é€šè¿‡åˆ©ç”¨å•ç‹¬çš„è¯´è¯äººå’Œç¯å¢ƒæç¤ºï¼ŒDAIEN-TTSå¯ä»¥ç‹¬ç«‹æ§åˆ¶åˆæˆè¯­éŸ³çš„éŸ³è°ƒå’ŒèƒŒæ™¯ç¯å¢ƒã€‚åŸºäºF5-TTSæ„å»ºçš„DAIEN-TTSé¦–å…ˆé›†æˆäº†ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­éŸ³ç¯å¢ƒåˆ†ç¦»ï¼ˆSESï¼‰æ¨¡å—ï¼Œå°†ç¯å¢ƒè¯­éŸ³åˆ†è§£æˆå¹²å‡€è¯­éŸ³çš„æ¢…å°”é¢‘è°±å›¾å’Œç¯å¢ƒéŸ³é¢‘ã€‚ç„¶åï¼Œåœ¨æ¢…å°”é¢‘è°±å›¾ä¸Šåº”ç”¨ä¸¤ä¸ªéšæœºé•¿åº¦ä¸åŒçš„è·¨åº¦æ©ç ï¼Œä¸æ–‡æœ¬åµŒå…¥ä¸€èµ·ä½œä¸ºå¡«å……æ©ç ç¯å¢ƒæ¢…å°”é¢‘è°±å›¾çš„æ¡ä»¶ï¼Œä»è€Œå®ç°ä¸ªæ€§åŒ–è¯­éŸ³å’Œéšæ—¶é—´å˜åŒ–çš„ç¯å¢ƒéŸ³é¢‘çš„åŒæ—¶å»¶ç»­ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„å¯æ§æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é’ˆå¯¹è¯­éŸ³å’Œç¯å¢ƒç»„ä»¶çš„åŒé‡æ— ç±»åˆ«å¼•å¯¼ï¼ˆDCFGï¼‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰è‡ªé€‚åº”ç­–ç•¥ï¼Œä»¥ä½¿åˆæˆè¯­éŸ³ä¸ç¯å¢ƒæç¤ºä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDAIEN-TTSç”Ÿæˆçš„ç¯å¢ƒä¸ªæ€§åŒ–è¯­éŸ³å…·æœ‰é«˜åº¦çš„è‡ªç„¶æ€§ã€å¼ºçƒˆçš„è¯´è¯äººç›¸ä¼¼æ€§å’Œé«˜ç¯å¢ƒä¿çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14684v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DAIEN-TTSï¼Œä¸€ä¸ªé›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡è§£è€¦éŸ³é¢‘å¡«å……å®ç°äº†ç¯å¢ƒæ„ŸçŸ¥åˆæˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç‹¬ç«‹çš„è¯´è¯äººå’Œç¯å¢ƒæç¤ºï¼Œå®ç°å¯¹åˆæˆè¯­éŸ³çš„éŸ³è°ƒå’ŒèƒŒæ™¯ç¯å¢ƒçš„ç‹¬ç«‹æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDAIEN-TTSç”Ÿæˆçš„ç¯å¢ƒä¸ªæ€§åŒ–è¯­éŸ³å…·æœ‰é«˜åº¦çš„è‡ªç„¶æ€§ã€å¼ºçƒˆçš„è¯´è¯äººç›¸ä¼¼æ€§å’Œé«˜ç¯å¢ƒä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DAIEN-TTSæ˜¯ä¸€ä¸ªé›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¡†æ¶ï¼Œèƒ½å®ç°ç¯å¢ƒæ„ŸçŸ¥åˆæˆã€‚</li>
<li>é€šè¿‡è§£è€¦éŸ³é¢‘å¡«å……ï¼ŒDAIEN-TTSå®ç°å¯¹åˆæˆè¯­éŸ³çš„éŸ³è°ƒå’ŒèƒŒæ™¯ç¯å¢ƒçš„ç‹¬ç«‹æ§åˆ¶ã€‚</li>
<li>DAIEN-TTSå»ºç«‹åœ¨F5-TTSä¹‹ä¸Šï¼Œèå…¥äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­éŸ³ç¯å¢ƒåˆ†ç¦»ï¼ˆSESï¼‰æ¨¡å—ï¼Œå°†ç¯å¢ƒè¯­éŸ³åˆ†è§£ä¸ºçº¯å‡€è¯­éŸ³å’Œç¯å¢ƒéŸ³é¢‘çš„mel-spectrogramã€‚</li>
<li>é€šè¿‡åº”ç”¨ä¸¤ç§éšæœºè·¨åº¦æ©ç åˆ°mel-spectrogramï¼Œç»“åˆæ–‡æœ¬åµŒå…¥ï¼Œå®ç°å¯¹æ©ç ç¯å¢ƒmel-spectrogramçš„å¡«å……ï¼Œå®ç°ä¸ªæ€§åŒ–è¯­éŸ³å’Œæ—¶é—´å˜åŒ–ç¯å¢ƒéŸ³é¢‘çš„åŒæ—¶å»¶ç»­ã€‚</li>
<li>é‡‡ç”¨æ— ç±»åˆ«å¼•å¯¼ï¼ˆDCFGï¼‰å¢å¼ºè¯­éŸ³å’Œç¯å¢ƒç»„ä»¶çš„æ§åˆ¶æ€§ï¼Œå¹¶å¼•å…¥ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰è‡ªé€‚åº”ç­–ç•¥ï¼Œä½¿åˆæˆè¯­éŸ³ä¸ç¯å¢ƒæç¤ºå¯¹é½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDAIEN-TTSç”Ÿæˆçš„ç¯å¢ƒä¸ªæ€§åŒ–è¯­éŸ³å…·æœ‰é«˜åº¦çš„è‡ªç„¶æ€§ã€å¼ºçƒˆçš„è¯´è¯äººç›¸ä¼¼æ€§å’Œé«˜ç¯å¢ƒä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e71364446f42d253ba0ec5ccc056e879" align="middle">
<img src="https://picx.zhimg.com/v2-46a6baae3822f4b83edb67fb84ccf455" align="middle">
<img src="https://picx.zhimg.com/v2-5000b6ebf49b9642553988642b16704f" align="middle">
<img src="https://picx.zhimg.com/v2-6292b4c1fc3c09f4d596bfa41a1fdd0b" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UMA-Split-unimodal-aggregation-for-both-English-and-Mandarin-non-autoregressive-speech-recognition"><a href="#UMA-Split-unimodal-aggregation-for-both-English-and-Mandarin-non-autoregressive-speech-recognition" class="headerlink" title="UMA-Split: unimodal aggregation for both English and Mandarin   non-autoregressive speech recognition"></a>UMA-Split: unimodal aggregation for both English and Mandarin   non-autoregressive speech recognition</h2><p><strong>Authors:Ying Fang, Xiaofei Li</strong></p>
<p>This paper proposes a unimodal aggregation (UMA) based nonautoregressive model for both English and Mandarin speech recognition. The original UMA explicitly segments and aggregates acoustic frames (with unimodal weights that first monotonically increase and then decrease) of the same text token to learn better representations than regular connectionist temporal classification (CTC). However, it only works well in Mandarin. It struggles with other languages, such as English, for which a single syllable may be tokenized into multiple fine-grained tokens, or a token spans fewer than 3 acoustic frames and fails to form unimodal weights. To address this problem, we propose allowing each UMA-aggregated frame map to multiple tokens, via a simple split module that generates two tokens from each aggregated frame before computing the CTC loss. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå•æ¨¡æ€èšåˆï¼ˆUMAï¼‰çš„éè‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºè‹±è¯­å’Œæ™®é€šè¯çš„è¯­éŸ³è¯†åˆ«ã€‚åŸå§‹çš„UMAä¼šæ˜ç¡®åœ°åˆ†å‰²å¹¶èšåˆåŒä¸€æ–‡æœ¬æ ‡è®°çš„å£°å­¦å¸§ï¼ˆä½¿ç”¨å…ˆå•è°ƒå¢åŠ åå‡å°‘çš„å•æ¨¡æ€æƒé‡ï¼‰ï¼Œä»¥å­¦ä¹ æ¯”å¸¸è§„è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰æ›´å¥½çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå®ƒåªåœ¨æ™®é€šè¯ä¸­è¡¨ç°è‰¯å¥½ã€‚å¯¹äºè‹±è¯­ç­‰å…¶ä»–è¯­è¨€ï¼Œå®ƒå¾ˆéš¾å¤„ç†ï¼Œå› ä¸ºè‹±è¯­ä¸­çš„å•ä¸ªéŸ³èŠ‚å¯èƒ½è¢«ç»†åˆ†ä¸ºå¤šä¸ªæ ‡è®°ï¼Œæˆ–è€…ä¸€ä¸ªæ ‡è®°è·¨è¶Šå°‘äº3ä¸ªå£°å­¦å¸§è€Œæ— æ³•å½¢æˆå•æ¨¡æ€æƒé‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…è®¸æ¯ä¸ªUMAèšåˆå¸§æ˜ å°„åˆ°å¤šä¸ªæ ‡è®°çš„æ–¹æ³•ï¼Œé€šè¿‡ç®€å•çš„åˆ†å‰²æ¨¡å—åœ¨è®¡ç®—CTCæŸå¤±ä¹‹å‰ä»æ¯ä¸ªèšåˆå¸§ç”Ÿæˆä¸¤ä¸ªæ ‡è®°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14653v1">PDF</a> Submit to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå•æ¨¡æ€èšåˆï¼ˆUMAï¼‰çš„éè‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºè‹±è¯­å’Œæ™®é€šè¯çš„è¯­éŸ³è¯†åˆ«ã€‚åŸå§‹çš„UMAé€šè¿‡å¯¹åŒä¸€æ–‡æœ¬æ ‡è®°çš„å£°å¸§è¿›è¡Œåˆ†æ®µå’Œèšåˆï¼ˆé‡‡ç”¨å…ˆé€’å¢åé€’å‡çš„å•æ¨¡æ€æƒé‡ï¼‰ï¼Œä»¥å­¦ä¹ æ¯”è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰æ›´å¥½çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå®ƒåœ¨æ™®é€šè¯ä»¥å¤–çš„è¯­è¨€ï¼Œå¦‚è‹±è¯­ä¸­çš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚å› ä¸ºè‹±è¯­ä¸­çš„å•ä¸ªéŸ³èŠ‚å¯èƒ½è¢«ç»†åˆ†ä¸ºå¤šä¸ªæ ‡è®°ï¼Œæˆ–è€…ä¸€ä¸ªæ ‡è®°è·¨è¶Šçš„å£°å¸§å°‘äº3ä¸ªï¼Œå¯¼è‡´æ— æ³•å½¢æˆå•æ¨¡æ€æƒé‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æè®®å…è®¸æ¯ä¸ªUMAèšåˆçš„å£°å¸§æ˜ å°„åˆ°å¤šä¸ªæ ‡è®°ä¸Šï¼Œé€šè¿‡ç®€å•çš„åˆ†å‰²æ¨¡å—ä¸ºæ¯ä¸ªèšåˆçš„å£°å¸§ç”Ÿæˆä¸¤ä¸ªæ ‡è®°ï¼Œç„¶åå†è®¡ç®—CTCæŸå¤±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå•æ¨¡æ€èšåˆï¼ˆUMAï¼‰çš„éè‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºè¯­éŸ³è¯†åˆ«ã€‚</li>
<li>åŸå§‹çš„UMAåœ¨æ™®é€šè¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è‹±è¯­ç­‰å…¶ä»–è¯­è¨€ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>è‹±è¯­ä¸­çš„è¯­éŸ³æ ‡è®°æ¯”æ™®é€šè¯æ›´å¤æ‚ï¼Œä¸€ä¸ªéŸ³èŠ‚å¯èƒ½åˆ†ä¸ºå¤šä¸ªç²¾ç»†æ ‡è®°ã€‚</li>
<li>åœ¨è‹±è¯­ä¸­ï¼Œç”±äºå£°å¸§æ•°é‡å°‘äº3ä¸ªçš„æ ‡è®°è¾ƒå¤šï¼Œå¯¼è‡´UMAæ— æ³•å½¢æˆç¨³å®šçš„å•æ¨¡æ€æƒé‡ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†å…è®¸æ¯ä¸ªUMAèšåˆçš„å£°å¸§æ˜ å°„åˆ°å¤šä¸ªæ ‡è®°çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç®€å•çš„åˆ†å‰²æ¨¡å—ä¸ºæ¯ä¸ªèšåˆçš„å£°å¸§ç”Ÿæˆä¸¤ä¸ªæ ‡è®°ï¼Œæé«˜æ¨¡å‹çš„çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb66a5569d3657a37388176a7d3efdea" align="middle">
<img src="https://picx.zhimg.com/v2-62bd5ba2d9f169c98038240ea85d3ba1" align="middle">
<img src="https://picx.zhimg.com/v2-e6e138bde9f41b6ce5de91918a07c927" align="middle">
<img src="https://picx.zhimg.com/v2-3fe864be582e62a67ea428a06755d372" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mitigating-Intra-Speaker-Variability-in-Diarization-with-Style-Controllable-Speech-Augmentation"><a href="#Mitigating-Intra-Speaker-Variability-in-Diarization-with-Style-Controllable-Speech-Augmentation" class="headerlink" title="Mitigating Intra-Speaker Variability in Diarization with   Style-Controllable Speech Augmentation"></a>Mitigating Intra-Speaker Variability in Diarization with   Style-Controllable Speech Augmentation</h2><p><strong>Authors:Miseul Kim, Soo Jin Park, Kyungguen Byun, Hyeon-Kyeong Shin, Sunkuk Moon, Shuhua Zhang, Erik Visser</strong></p>
<p>Speaker diarization systems often struggle with high intrinsic intra-speaker variability, such as shifts in emotion, health, or content. This can cause segments from the same speaker to be misclassified as different individuals, for example, when one raises their voice or speaks faster during conversation. To address this, we propose a style-controllable speech generation model that augments speech across diverse styles while preserving the target speakerâ€™s identity. The proposed system starts with diarized segments from a conventional diarizer. For each diarized segment, it generates augmented speech samples enriched with phonetic and stylistic diversity. And then, speaker embeddings from both the original and generated audio are blended to enhance the systemâ€™s robustness in grouping segments with high intrinsic intra-speaker variability. We validate our approach on a simulated emotional speech dataset and the truncated AMI dataset, demonstrating significant improvements, with error rate reductions of 49% and 35% on each dataset, respectively. </p>
<blockquote>
<p>è¯´è¯äººåˆ†ç±»ç³»ç»Ÿç»å¸¸é¢ä¸´é«˜å†…åœ¨è¯´è¯äººå†…éƒ¨å˜åŒ–çš„é—®é¢˜ï¼Œå¦‚æƒ…ç»ªã€å¥åº·çŠ¶å†µæˆ–å†…å®¹çš„æ”¹å˜ã€‚è¿™å¯èƒ½å¯¼è‡´æ¥è‡ªåŒä¸€è¯´è¯äººçš„ç‰‡æ®µè¢«é”™è¯¯åœ°åˆ†ç±»ä¸ºä¸åŒçš„ä¸ªä½“ï¼Œä¾‹å¦‚å½“æŸäººæé«˜å£°éŸ³æˆ–åŠ å¿«è¯­é€Ÿè¿›è¡Œå¯¹è¯æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é£æ ¼å¯æ§çš„è¯­éŸ³ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨ä¿æŒç›®æ ‡è¯´è¯äººèº«ä»½çš„åŒæ—¶ï¼Œå¢å¼ºä¸åŒé£æ ¼çš„è¯­éŸ³ã€‚è¯¥ç³»ç»Ÿçš„èµ·ç‚¹æ˜¯å¸¸è§„åˆ†ç±»å™¨ç”Ÿæˆçš„åˆ†ç±»è¯­éŸ³ç‰‡æ®µã€‚å¯¹äºæ¯ä¸ªåˆ†ç±»è¯­éŸ³ç‰‡æ®µï¼Œå®ƒç”Ÿæˆäº†ä¸°å¯Œè¯­éŸ³å­¦å’Œé£æ ¼å¤šæ ·æ€§çš„å¢å¼ºè¯­éŸ³æ ·æœ¬ã€‚ç„¶åï¼Œå°†åŸå§‹éŸ³é¢‘å’Œç”ŸæˆéŸ³é¢‘çš„è¯´è¯äººåµŒå…¥èåˆåœ¨ä¸€èµ·ï¼Œä»¥æé«˜ç³»ç»Ÿå¯¹å…·æœ‰è¾ƒé«˜å†…åœ¨è¯´è¯äººå†…éƒ¨å˜åŒ–ç‰‡æ®µçš„åˆ†ç»„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿæƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†å’Œæˆªæ–­AMIæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼Œæ¯ä¸ªæ•°æ®é›†ä¸Šçš„é”™è¯¯ç‡åˆ†åˆ«é™ä½äº†49%å’Œ35%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14632v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹è¯´è¯äººè¯†åˆ«ç³»ç»Ÿä¸­å­˜åœ¨çš„é«˜å†…åœ¨è¯´è¯äººå†…éƒ¨å˜å¼‚é—®é¢˜ï¼Œå¦‚æƒ…æ„Ÿã€å¥åº·çŠ¶å†µæˆ–å†…å®¹çš„æ”¹å˜å¯èƒ½å¯¼è‡´åŒä¸€è¯´è¯äººçš„ç‰‡æ®µè¢«é”™è¯¯åœ°åˆ†ç±»ä¸ºä¸åŒä¸ªä½“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é£æ ¼å¯æ§çš„è¯­éŸ³ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒç›®æ ‡è¯´è¯äººèº«ä»½çš„åŒæ—¶ï¼Œå¢åŠ è¯­éŸ³é£æ ¼çš„å¤šæ ·æ€§ã€‚é€šè¿‡èåˆåŸå§‹å’Œç”ŸæˆéŸ³é¢‘çš„è¯´è¯äººåµŒå…¥ï¼Œæé«˜äº†ç³»ç»Ÿå¯¹é«˜å†…åœ¨è¯´è¯äººå†…éƒ¨å˜å¼‚æ€§çš„åˆ†æ®µåˆ†ç»„èƒ½åŠ›ã€‚åœ¨æ¨¡æ‹Ÿçš„æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†å’ŒAMIæˆªæ–­æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åˆ†åˆ«é™ä½äº†é”™è¯¯ç‡49%å’Œ35%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯´è¯äººè¯†åˆ«ç³»ç»Ÿé¢ä¸´é«˜å†…åœ¨è¯´è¯äººå†…éƒ¨å˜å¼‚é—®é¢˜ã€‚</li>
<li>åŒä¸€è¯´è¯äººçš„ç‰‡æ®µå¯èƒ½å› æƒ…æ„Ÿã€å¥åº·çŠ¶å†µæˆ–å†…å®¹çš„å˜åŒ–è€Œè¢«è¯¯åˆ†ç±»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é£æ ¼å¯æ§çš„è¯­éŸ³ç”Ÿæˆæ¨¡å‹æ¥å¢å¼ºè¯­éŸ³çš„å¤šæ ·æ€§å’Œä¿æŒè¯´è¯äººèº«ä»½ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡èåˆåŸå§‹å’Œç”ŸæˆéŸ³é¢‘çš„è¯´è¯äººåµŒå…¥æ¥æé«˜ç³»ç»Ÿçš„é²æ£’æ€§ã€‚</li>
<li>éªŒè¯å®éªŒåœ¨æ¨¡æ‹Ÿçš„æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†å’ŒAMIæˆªæ–­æ•°æ®é›†ä¸Šè¿›è¡Œã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è¯´è¯äººè¯†åˆ«æ€§èƒ½ï¼Œåˆ†åˆ«é™ä½äº†é”™è¯¯ç‡49%å’Œ35%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-144489a7a01e9d668d9af54a29154217" align="middle">
<img src="https://picx.zhimg.com/v2-5abd5354dd0e11f50bcf54ccad06a274" align="middle">
<img src="https://picx.zhimg.com/v2-f87a5367b67b8dcb0e1970e3bc064171" align="middle">
<img src="https://picx.zhimg.com/v2-49bcb1204abd1df2088c4240bf73935d" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multi-Channel-Differential-ASR-for-Robust-Wearer-Speech-Recognition-on-Smart-Glasses"><a href="#Multi-Channel-Differential-ASR-for-Robust-Wearer-Speech-Recognition-on-Smart-Glasses" class="headerlink" title="Multi-Channel Differential ASR for Robust Wearer Speech Recognition on   Smart Glasses"></a>Multi-Channel Differential ASR for Robust Wearer Speech Recognition on   Smart Glasses</h2><p><strong>Authors:Yufeng Yang, Yiteng Huang, Yong Xu, Li Wan, Suwon Shon, Yang Liu, Yifeng Fan, Zhaojun Yang, Olivier Siohan, Yue Liu, Ming Sun, Florian Metze</strong></p>
<p>With the growing adoption of wearable devices such as smart glasses for AI assistants, wearer speech recognition (WSR) is becoming increasingly critical to next-generation human-computer interfaces. However, in real environments, interference from side-talk speech remains a significant challenge to WSR and may cause accumulated errors for downstream tasks such as natural language processing. In this work, we introduce a novel multi-channel differential automatic speech recognition (ASR) method for robust WSR on smart glasses. The proposed system takes differential inputs from different frontends that complement each other to improve the robustness of WSR, including a beamformer, microphone selection, and a lightweight side-talk detection model. Evaluations on both simulated and real datasets demonstrate that the proposed system outperforms the traditional approach, achieving up to an 18.0% relative reduction in word error rate. </p>
<blockquote>
<p>éšç€æ™ºèƒ½çœ¼é•œç­‰å¯ç©¿æˆ´è®¾å¤‡åœ¨äººå·¥æ™ºèƒ½åŠ©æ‰‹æ–¹é¢çš„æ™®åŠåº¦ä¸æ–­å¢é•¿ï¼Œç©¿æˆ´è€…è¯­éŸ³è¯†åˆ«ï¼ˆWSRï¼‰å¯¹äºä¸‹ä¸€ä»£äººæœºäº¤äº’ç•Œé¢å˜å¾—æ—¥ç›Šå…³é”®ã€‚ç„¶è€Œï¼Œåœ¨å®é™…ç¯å¢ƒä¸­ï¼Œæ¥è‡ªæ—è¾¹è¯´è¯çš„å¹²æ‰°ä»ç„¶æ˜¯WSRé¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ï¼Œå¹¶å¯èƒ½å¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼‰å‡ºç°ç´¯ç§¯é”™è¯¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æ™ºèƒ½çœ¼é•œä¸Šçš„ç¨³å¥WSRå¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤šé€šé“å·®åˆ†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹æ³•ã€‚æ‰€æå‡ºç³»ç»Ÿä»ä¸åŒå‰ç«¯è·å–å·®åˆ†è¾“å…¥ï¼Œè¿™äº›è¾“å…¥ç›¸äº’è¡¥å……ï¼Œä»¥æé«˜WSRçš„ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬æ³¢æŸæˆå½¢å™¨ã€éº¦å…‹é£é€‰æ‹©å’Œè½»é‡çº§æ—è¾¹è¯´è¯æ£€æµ‹æ¨¡å‹ã€‚å¯¹æ¨¡æ‹Ÿå’Œå®é™…æ•°æ®é›†çš„è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºç³»ç»Ÿçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾18.0%çš„å•è¯é”™è¯¯ç‡ç›¸å¯¹é™ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14430v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>éšç€æ™ºèƒ½çœ¼é•œç­‰å¯ç©¿æˆ´è®¾å¤‡çš„æ™®åŠï¼Œä½©æˆ´è€…è¯­éŸ³è¯†åˆ«ï¼ˆWSRï¼‰å¯¹äºä¸‹ä¸€ä»£äººæœºäº¤äº’ç•Œé¢å˜å¾—æ„ˆå‘å…³é”®ã€‚ç„¶è€Œï¼Œåœ¨å®é™…ç¯å¢ƒä¸­ï¼Œæ¥è‡ªä¾§è°ˆè¯è¯­çš„å¹²æ‰°ä»æ˜¯WSRé¢ä¸´çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå¹¶å¯èƒ½ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä¸‹æ¸¸ä»»åŠ¡å¸¦æ¥ç´¯ç§¯è¯¯å·®ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šé€šé“å·®åˆ†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ™ºèƒ½çœ¼é•œçš„ç¨³å¥WSRã€‚æ‰€æç³»ç»Ÿä»ä¸åŒå‰ç«¯é‡‡é›†å·®åˆ†è¾“å…¥ï¼Œç›¸äº’è¡¥å……ä»¥æå‡WSRçš„ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬æ³¢æŸæˆå½¢å™¨ã€éº¦å…‹é£é€‰æ‹©å’Œè½»é‡çº§ä¾§è°ˆæ£€æµ‹æ¨¡å‹ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æç³»ç»Ÿä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¯é”™è¯¯ç‡ç›¸å¯¹é™ä½18.0%ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>æ™ºèƒ½çœ¼é•œç­‰å¯ç©¿æˆ´è®¾å¤‡çš„æ™®åŠä½¿å¾—ä½©æˆ´è€…è¯­éŸ³è¯†åˆ«ï¼ˆWSRï¼‰å˜å¾—æ—¥ç›Šå…³é”®ã€‚</li>
<li>ä¾§è°ˆè¯è¯­å¹²æ‰°æ˜¯WSRé¢ä¸´çš„å®é™…ç¯å¢ƒæŒ‘æˆ˜ä¹‹ä¸€ï¼Œå¯èƒ½å¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡å¦‚è‡ªç„¶è¯­è¨€å¤„ç†çš„ç´¯ç§¯è¯¯å·®ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šé€šé“å·®åˆ†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºæ™ºèƒ½çœ¼é•œWSRçš„ç¨³å¥æ€§ã€‚</li>
<li>æ‰€æç³»ç»Ÿé›†æˆäº†æ³¢æŸæˆå½¢å™¨ã€éº¦å…‹é£é€‰æ‹©å’Œè½»é‡çº§ä¾§è°ˆæ£€æµ‹æ¨¡å‹ç­‰ä¸åŒçš„å‰ç«¯å·®åˆ†è¾“å…¥ã€‚</li>
<li>è¯¥ç³»ç»Ÿç›¸äº’è¡¥å……è¿™äº›è¾“å…¥ä»¥æå‡WSRæ€§èƒ½ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæ–°ç³»ç»Ÿè¾ƒä¼ ç»Ÿæ–¹æ³•åœ¨è¯é”™è¯¯ç‡æ–¹é¢æœ‰æ˜æ˜¾çš„æ”¹è¿›ï¼Œç›¸å¯¹é™ä½äº†18.0%ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºæ™ºèƒ½çœ¼é•œçš„è¯­éŸ³è¯†åˆ«æŠ€æœ¯å¸¦æ¥äº†æ–°çš„çªç ´å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4587301724eb9ee71fc0160b2faaa36d" align="middle">
<img src="https://picx.zhimg.com/v2-a0cdd0b80a8ed07f849a9ff8b4fcddf8" align="middle">
<img src="https://picx.zhimg.com/v2-2b09293a34542b025dce4328e1bee08b" align="middle">
<img src="https://picx.zhimg.com/v2-ceaddfd17b1a6c9f55aa8b4ef5d9efd4" align="middle">
<img src="https://picx.zhimg.com/v2-065295ccadce3e098b9eb1ff2a148bbc" align="middle">
<img src="https://picx.zhimg.com/v2-fc31e9194a857e3cf2773afeac5620d8" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SpeechOp-Inference-Time-Task-Composition-for-Generative-Speech-Processing"><a href="#SpeechOp-Inference-Time-Task-Composition-for-Generative-Speech-Processing" class="headerlink" title="SpeechOp: Inference-Time Task Composition for Generative Speech   Processing"></a>SpeechOp: Inference-Time Task Composition for Generative Speech   Processing</h2><p><strong>Authors:Justin Lovelace, Rithesh Kumar, Jiaqi Su, Ke Chen, Kilian Q Weinberger, Zeyu Jin</strong></p>
<p>While generative Text-to-Speech (TTS) systems leverage vast &#96;&#96;in-the-wildâ€ data to achieve remarkable success, speech-to-speech processing tasks like enhancement face data limitations, which lead data-hungry generative approaches to distort speech content and speaker identity. To bridge this gap, we present SpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS models into a universal speech processor capable of performing a wide range of speech tasks and composing them in novel ways at inference time. By adapting a pre-trained TTS model, SpeechOp inherits a rich understanding of natural speech, accelerating training and improving S2S task quality, while simultaneously enhancing core TTS performance. Finally, we introduce Implicit Task Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g., from Whisper) guide SpeechOpâ€™s enhancement via our principled inference-time task composition. ITC achieves state-of-the-art content preservation by robustly combining web-scale speech understanding with SpeechOpâ€™s generative capabilities. Audio samples are available at <a target="_blank" rel="noopener" href="https://justinlovelace.github.io/projects/speechop">https://justinlovelace.github.io/projects/speechop</a> </p>
<blockquote>
<p>è™½ç„¶ç”Ÿæˆå¼æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿåˆ©ç”¨å¤§é‡çš„â€œé‡ç”Ÿâ€æ•°æ®å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†è¯­éŸ³åˆ°è¯­éŸ³çš„å¤„ç†ä»»åŠ¡å¦‚å¢å¼ºé¢éƒ¨æ•°æ®ä»å­˜åœ¨å±€é™æ€§ï¼Œè¿™å¯¼è‡´æ•°æ®é¥¥é¥¿çš„ç”Ÿæˆæ–¹æ³•ä¼šæ‰­æ›²è¯­éŸ³å†…å®¹å’Œè¯´è¯äººèº«ä»½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†SpeechOpï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå®ƒå°†é¢„è®­ç»ƒçš„TTSæ¨¡å‹è½¬å˜ä¸ºé€šç”¨è¯­éŸ³å¤„ç†å™¨ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶é—´æ‰§è¡Œå„ç§è¯­éŸ³ä»»åŠ¡å¹¶ä»¥æ–°é¢–çš„æ–¹å¼ç»„åˆå®ƒä»¬ã€‚é€šè¿‡é€‚åº”é¢„è®­ç»ƒçš„TTSæ¨¡å‹ï¼ŒSpeechOpç»§æ‰¿äº†ä¸°å¯Œçš„è‡ªç„¶è¯­éŸ³ç†è§£ï¼ŒåŠ é€Ÿäº†è®­ç»ƒï¼Œæé«˜äº†S2Sä»»åŠ¡è´¨é‡ï¼ŒåŒæ—¶æé«˜äº†æ ¸å¿ƒTTSæ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†éšå¼ä»»åŠ¡ç»„åˆï¼ˆITCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç®¡é“ï¼Œå…¶ä¸­ASRè¡ç”Ÿçš„è½¬å½•æœ¬ï¼ˆä¾‹å¦‚ï¼Œæ¥è‡ªwhisperï¼‰é€šè¿‡æˆ‘ä»¬çš„æœ‰åŸåˆ™çš„æ¨ç†æ—¶é—´ä»»åŠ¡ç»„åˆæ¥æŒ‡å¯¼SpeechOpçš„å¢å¼ºã€‚ITCé€šè¿‡ç¨³å¥åœ°å°†ç½‘é¡µè§„æ¨¡çš„è¯­éŸ³ç†è§£ä¸SpeechOpçš„ç”Ÿæˆèƒ½åŠ›ç›¸ç»“åˆï¼Œå®ç°äº†æœ€æ–°çš„å†…å®¹ä¿ç•™ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://justinlovelace.github.io/projects/speechop%E6%89%BE%E5%88%B0%E3%80%82">https://justinlovelace.github.io/projects/speechopæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14298v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†SpeechOpæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å°†é¢„è®­ç»ƒçš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹è½¬åŒ–ä¸ºé€šç”¨çš„è¯­éŸ³å¤„ç†å™¨ã€‚SpeechOpå¯ä»¥æ‰§è¡Œå¤šç§è¯­éŸ³ä»»åŠ¡å¹¶åœ¨æ¨ç†æ—¶é—´ä»¥æ–°é¢–çš„æ–¹å¼ç»„åˆå®ƒä»¬ã€‚é€šè¿‡é€‚åº”é¢„è®­ç»ƒçš„TTSæ¨¡å‹ï¼ŒSpeechOpç»§æ‰¿äº†ä¸°å¯Œçš„è‡ªç„¶è¯­éŸ³ç†è§£ï¼ŒåŠ é€Ÿè®­ç»ƒï¼Œæé«˜è¯­éŸ³è½¬è¯­éŸ³ï¼ˆS2Sï¼‰ä»»åŠ¡è´¨é‡ï¼ŒåŒæ—¶æé«˜TTSçš„æ ¸å¿ƒæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†éšå¼ä»»åŠ¡ç»„åˆï¼ˆITCï¼‰è¿™ä¸€æ–°æµç¨‹ï¼Œåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰äº§ç”Ÿçš„æ–‡æœ¬ï¼ˆå¦‚Whisperï¼‰å¼•å¯¼SpeechOpçš„å¢å¼ºåŠŸèƒ½ã€‚ITCé€šè¿‡ç»“åˆå¤§è§„æ¨¡çš„ç½‘é¡µè¯­éŸ³ç†è§£ä¸SpeechOpçš„ç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°äº†å†…å®¹çš„æœ€ä½³ä¿ç•™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SpeechOpæ˜¯ä¸€ä¸ªå¤šä»»åŠ¡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†é¢„è®­ç»ƒçš„TTSæ¨¡å‹è½¬åŒ–ä¸ºèƒ½å¤Ÿæ‰§è¡Œå¤šç§è¯­éŸ³ä»»åŠ¡çš„é€šç”¨è¯­éŸ³å¤„ç†å™¨ã€‚</li>
<li>SpeechOpé€šè¿‡é€‚åº”é¢„è®­ç»ƒçš„TTSæ¨¡å‹ï¼Œç»§æ‰¿äº†ä¸°å¯Œçš„è‡ªç„¶è¯­éŸ³ç†è§£ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒå¹¶æé«˜è¯­éŸ³å¤„ç†ä»»åŠ¡çš„è´¨é‡ã€‚</li>
<li>SpeechOpåœ¨æ¨ç†æ—¶é—´å¯ä»¥ä»¥æ–°é¢–çš„æ–¹å¼ç»„åˆä¸åŒçš„è¯­éŸ³ä»»åŠ¡ã€‚</li>
<li>ITCæ˜¯ä¸€ç§éšå¼ä»»åŠ¡ç»„åˆæ–¹æ³•ï¼Œåˆ©ç”¨ASRäº§ç”Ÿçš„æ–‡æœ¬å¼•å¯¼SpeechOpè¿›è¡Œè¯­éŸ³å¢å¼ºã€‚</li>
<li>ITCç»“åˆå¤§è§„æ¨¡çš„ç½‘é¡µè¯­éŸ³ç†è§£ä¸SpeechOpçš„ç”Ÿæˆèƒ½åŠ›ï¼Œå®ç°äº†å†…å®¹çš„æœ€ä½³ä¿ç•™ã€‚</li>
<li>SpeechOpæ¨¡å‹æé«˜äº†TTSçš„æ ¸å¿ƒæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-781827a641539cdcc40c2cf3dd3a541a" align="middle">
<img src="https://picx.zhimg.com/v2-b8193b3664fd6144d6ce9b4677b468b3" align="middle">
<img src="https://picx.zhimg.com/v2-d8eb3ace2cdbe0c5075c5d57930d9a3e" align="middle">
<img src="https://picx.zhimg.com/v2-e1433d077daad956dbedfcc751dbe4e0" align="middle">
<img src="https://picx.zhimg.com/v2-5d2784922a57de09ecdc2c60aad3a180" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GLAD-Global-Local-Aware-Dynamic-Mixture-of-Experts-for-Multi-Talker-ASR"><a href="#GLAD-Global-Local-Aware-Dynamic-Mixture-of-Experts-for-Multi-Talker-ASR" class="headerlink" title="GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR"></a>GLAD: Global-Local Aware Dynamic Mixture-of-Experts for Multi-Talker ASR</h2><p><strong>Authors:Yujie Guo, Jiaming Zhou, Yuhang Jia, Shiwan Zhao, Yong Qin</strong></p>
<p>End-to-end multi-talker automatic speech recognition (MTASR) faces significant challenges in accurately transcribing overlapping speech, especially under high-overlap conditions. To address these challenges, we proposed Global-Local Aware Dynamic (GLAD) Mixture-of-Experts, which dynamically fuse speaker-aware global information and fine-grained local features to guide expert selection. This mechanism enables speaker-specific routing by leveraging both global context and local acoustic cues. Experiments on LibriSpeechMix show that GLAD outperforms existing MTASR approaches, particularly in challenging multi-talker scenarios. To our best knowledge, this is the first work to apply Mixture-of-Experts (MoE) to end-to-end MTASR with a global-local fusion strategy. Our code and train dataset can be found at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/GLAD">https://github.com/NKU-HLT/GLAD</a>. </p>
<blockquote>
<p>ç«¯åˆ°ç«¯å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆMTASRï¼‰åœ¨å‡†ç¡®è½¬å½•é‡å è¯­éŸ³æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é‡å æ¡ä»¶ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Global-Local Aware Dynamicï¼ˆGLADï¼‰ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤ŸåŠ¨æ€èåˆè¯´è¯è€…æ„ŸçŸ¥å…¨å±€ä¿¡æ¯å’Œç²¾ç»†å±€éƒ¨ç‰¹å¾ï¼Œä»¥æŒ‡å¯¼ä¸“å®¶é€‰æ‹©ã€‚è¿™ç§æœºåˆ¶é€šè¿‡åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨å£°å­¦çº¿ç´¢ï¼Œå®ç°äº†é’ˆå¯¹è¯´è¯è€…çš„ç‰¹å®šè·¯ç”±ã€‚åœ¨LibriSpeechMixä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGLADä¼˜äºç°æœ‰çš„MTASRæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè¯´è¯äººåœºæ™¯ä¸­ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†ä¸“å®¶æ··åˆï¼ˆMoEï¼‰åº”ç”¨äºç«¯åˆ°ç«¯MTASRçš„å…¨å±€-å±€éƒ¨èåˆç­–ç•¥ã€‚æˆ‘ä»¬çš„ä»£ç å’Œè®­ç»ƒæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/GLAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/GLADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13093v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆMTASRï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå…¨å±€-å±€éƒ¨æ„ŸçŸ¥åŠ¨æ€ï¼ˆGLADï¼‰çš„ä¸“å®¶æ··åˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡èåˆè¯´è¯è€…æ„ŸçŸ¥çš„å…¨å±€ä¿¡æ¯å’Œç²¾ç»†çš„å±€éƒ¨ç‰¹å¾ï¼Œå®ç°äº†ä¸“å®¶é€‰æ‹©çš„åŠ¨æ€èåˆï¼Œåˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨å£°å­¦çº¿ç´¢è¿›è¡Œè¯´è¯è€…ç‰¹å®šè·¯ç”±ã€‚åœ¨LibriSpeechMixä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGLADåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè¯´è¯äººåœºæ™¯ä¸‹ä¼˜äºç°æœ‰çš„MTASRæ–¹æ³•ã€‚è¿™æ˜¯é¦–æ¬¡å°†ä¸“å®¶æ··åˆï¼ˆMoEï¼‰åº”ç”¨äºç«¯åˆ°ç«¯MTASRçš„å…¨å±€-å±€éƒ¨èåˆç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯åˆ°ç«¯çš„å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆMTASRï¼‰åœ¨è½¬å½•é‡å è¯­éŸ³æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„GLAD Mixture-of-Expertsæ¨¡å‹é€šè¿‡èåˆå…¨å±€å’Œå±€éƒ¨ä¿¡æ¯å®ç°åŠ¨æ€ä¸“å®¶é€‰æ‹©ã€‚</li>
<li>GLADæ¨¡å‹åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨å£°å­¦çº¿ç´¢è¿›è¡Œè¯´è¯è€…ç‰¹å®šè·¯ç”±ã€‚</li>
<li>GLADåœ¨LibriSpeechMixä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰çš„MTASRæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯´è¯äººåœºæ™¯ä¸‹ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å°†Mixture-of-Expertsï¼ˆMoEï¼‰åº”ç”¨äºç«¯åˆ°ç«¯MTASRçš„å…¨å±€-å±€éƒ¨èåˆç­–ç•¥ã€‚</li>
<li>æ¨¡å‹çš„ä»£ç å’Œè®­ç»ƒæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/GLAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/GLADæ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff3542de9a7a60faa71ff83a4a4ddc57" align="middle">
<img src="https://picx.zhimg.com/v2-a18800e96fea9d321f982a9f8986e59d" align="middle">
<img src="https://picx.zhimg.com/v2-f3e06600817fa6a5cff35f0b1b472174" align="middle">
<img src="https://picx.zhimg.com/v2-1adf84ea58dba46ed91b8cad571ef46a" align="middle">
<img src="https://picx.zhimg.com/v2-12e0557ef391deb2000289e40fcfb25a" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FunAudio-ASR-Technical-Report"><a href="#FunAudio-ASR-Technical-Report" class="headerlink" title="FunAudio-ASR Technical Report"></a>FunAudio-ASR Technical Report</h2><p><strong>Authors:Keyu An, Yanni Chen, Chong Deng, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Wen Wang, Wupeng Wang, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou</strong></p>
<p>In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, FunAudio-ASR achieves SOTA performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è§è¯äº†ç”±ä¸‰ä¸ªäº’è¡¥èŒƒå¼é©±åŠ¨çš„å˜é©æ€§è¿›å±•ï¼šæ•°æ®è§„æ¨¡æ‰©å¤§ã€æ¨¡å‹è§„æ¨¡æ‰©å¤§ï¼Œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ·±åº¦é›†æˆã€‚ç„¶è€Œï¼ŒLLMå®¹æ˜“å‡ºç°å¹»è§‰ï¼Œè¿™åœ¨çœŸå®ä¸–ç•Œçš„ASRåº”ç”¨ä¸­å¯èƒ½ä¼šæ˜¾è‘—åœ°é™ä½ç”¨æˆ·ä½“éªŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FunAudio-ASRï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¤§è§„æ¨¡ASRç³»ç»Ÿï¼Œå®ƒååŒç»“åˆäº†å¤§è§„æ¨¡æ•°æ®ã€å¤§å‹æ¨¡å‹å®¹é‡ã€LLMé›†æˆå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥åœ¨å¤šæ ·ä¸”å¤æ‚çš„è¯­éŸ³è¯†åˆ«åœºæ™¯ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFunAudio-ASRé’ˆå¯¹å®é™…éƒ¨ç½²è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–ï¼Œå¢å¼ºäº†æµå¼å¤„ç†èƒ½åŠ›ã€å™ªå£°é²æ£’æ€§ã€ä»£ç åˆ‡æ¢ã€çƒ­è¯å®šåˆ¶ä»¥åŠå…¶ä»–ç°å®ä¸–ç•Œåº”ç”¨éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¤§å¤šæ•°åŸºäºLLMçš„ASRç³»ç»Ÿåœ¨å¼€æºåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å¼ºåŠ²ï¼Œä½†å®ƒä»¬åœ¨çœŸå®çš„è¡Œä¸šè¯„ä¼°é›†ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ã€‚ç”±äºé¢å‘ç”Ÿäº§çš„ä¼˜åŒ–ï¼ŒFunAudio-ASRåœ¨çœŸå®åº”ç”¨æ•°æ®é›†ä¸Šå®ç°äº†SOTAæ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.12508v2">PDF</a> Authors are listed in alphabetical order</p>
<p><strong>Summary</strong><br>     è¿‘å¹´æ¥ï¼Œè¯­éŸ³è¯†åˆ«æŠ€æœ¯å—ç›Šäºæ•°æ®æ‰©å……ã€æ¨¡å‹è§„æ¨¡æ‰©å¤§ä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±åº¦èåˆï¼Œå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å®¹æ˜“å¼•å‘å¹»è§‰ç°è±¡ï¼Œæ˜¾è‘—é™ä½äº†åœ¨ç°å®ä¸–ç•Œè¯­éŸ³è¯†åˆ«åº”ç”¨ä¸­çš„ç”¨æˆ·ä½“éªŒã€‚æœ¬ç ”ç©¶æå‡ºFunAudio-ASRç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆå¤§è§„æ¨¡æ•°æ®ã€å¼ºå¤§æ¨¡å‹èƒ½åŠ›ã€å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåŠå¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–ä¸ºå®é™…åº”ç”¨éƒ¨ç½²è®¾è®¡ã€‚å®ƒåœ¨å¤šæ ·çš„å¤æ‚è¯­éŸ³è¯†åˆ«åœºæ™¯ä¸­å®ç°é¢†å…ˆæ€§èƒ½ã€‚ç‰¹åˆ«åœ¨æµèƒ½åŠ›ã€å™ªå£°ç¨³å¥æ€§ã€ä»£ç åˆ‡æ¢ç­‰æ–¹é¢åšäº†æ”¹è¿›å’Œæå‡ã€‚å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºå¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹ä¸»å¯¼çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œåœ¨çœŸå®è¡Œä¸šè¯„ä¼°é›†ä¸Šè¡¨ç°æ¬ ä½³çš„çŠ¶å†µæœ‰æ‰€æ”¹å–„ã€‚å¾—ç›Šäºé¢å‘ç”Ÿäº§çš„ä¼˜åŒ–ï¼ŒFunAudio-ASRåœ¨çœŸå®åº”ç”¨æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯è¿‘å¹´æ¥å–å¾—çªç ´æ€§è¿›å±•ï¼Œå¾—ç›Šäºæ•°æ®è§„æ¨¡æ‰©å¤§ã€æ¨¡å‹è§„æ¨¡å¢é•¿å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ·±åº¦èåˆã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å®¹æ˜“å¼•å‘å¹»è§‰ç°è±¡ï¼Œå½±å“ç”¨æˆ·ä½“éªŒã€‚</li>
<li>FunAudio-ASRç³»ç»Ÿç»“åˆäº†å¤§è§„æ¨¡æ•°æ®ã€å¤§å‹æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆå’Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œåœ¨å¤šç§å¤æ‚è¯­éŸ³è¯†åˆ«åœºæ™¯ä¸­å®ç°å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>FunAudio-ASRé’ˆå¯¹å®é™…åº”ç”¨åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¦‚æµå¤„ç†èƒ½åŠ›ã€å™ªå£°ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€ä»£ç åˆ‡æ¢ç­‰ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºFunAudio-ASRåœ¨çœŸå®åº”ç”¨æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜å…¶åœ¨ç°å®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ä¸»å¯¼çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨å¼€æºåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨çœŸå®è¡Œä¸šè¯„ä¼°é›†ä¸Šè¡¨ç°å¾€å¾€æ¬ ä½³ã€‚é€šè¿‡é¢å‘ç”Ÿäº§çš„ä¼˜åŒ–æªæ–½ï¼Œå¯æœ‰æ•ˆæ”¹å–„å…¶æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5f5c11836ca4f4c98960459ac6dd8ed" align="middle">
<img src="https://picx.zhimg.com/v2-edbaba40a9d3c83f4ddca95d4e211847" align="middle">
<img src="https://picx.zhimg.com/v2-a11573e614d61299ecfb4a2590bee7a8" align="middle">
<img src="https://picx.zhimg.com/v2-eb33f1721a23e49dd1c852765445b997" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Large-Scale-Probing-Analysis-of-Speaker-Specific-Attributes-in-Self-Supervised-Speech-Representations"><a href="#A-Large-Scale-Probing-Analysis-of-Speaker-Specific-Attributes-in-Self-Supervised-Speech-Representations" class="headerlink" title="A Large-Scale Probing Analysis of Speaker-Specific Attributes in   Self-Supervised Speech Representations"></a>A Large-Scale Probing Analysis of Speaker-Specific Attributes in   Self-Supervised Speech Representations</h2><p><strong>Authors:Aemon Yat Fei Chiu, Kei Ching Fung, Roger Tsz Yeung Li, Jingyu Li, Tan Lee</strong></p>
<p>Speech self-supervised learning (SSL) models are known to learn hierarchical representations, yet how they encode different speaker-specific attributes remains under-explored. This study investigates the layer-wise disentanglement of speaker information across multiple speech SSL model families and their variants. Drawing from phonetic frameworks, we conduct a large-scale probing analysis of attributes categorised into functional groups: Acoustic (Gender), Prosodic (Pitch, Tempo, Energy), and Paralinguistic (Emotion), which we use to deconstruct the modelâ€™s representation of Speaker Identity. Our findings validate a consistent three-stage hierarchy: initial layers encode fundamental timbre and prosody; middle layers synthesise abstract traits; and final layers suppress speaker identity to abstract linguistic content. An ablation study shows that while specialised speaker embeddings excel at identifying speaker identity, the intermediate layers of speech SSL models better represent dynamic prosody. This work is the first large-scale study covering a wide range of speech SSL model families and variants with fine-grained speaker-specific attributes on how they hierarchically separate the dynamic style of speech from its intrinsic characteristics, offering practical implications for downstream tasks. </p>
<blockquote>
<p>è¯­éŸ³è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹å·²ç»å­¦ä¼šå­¦ä¹ åˆ†å±‚è¡¨ç¤ºï¼Œä½†å®ƒä»¬å¦‚ä½•ç¼–ç ä¸åŒçš„è¯´è¯äººç‰¹å®šå±æ€§ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤šä¸ªè¯­éŸ³SSLæ¨¡å‹å®¶æ—åŠå…¶å˜ä½“ä¸­è¯´è¯äººä¿¡æ¯çš„é€å±‚åˆ†ç¦»ã€‚å€Ÿé‰´è¯­éŸ³å­¦æ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹å±æ€§è¿›è¡Œäº†å¤§è§„æ¨¡æ¢æµ‹åˆ†æï¼ŒæŒ‰åŠŸèƒ½ç»„åˆ†ç±»ï¼šå£°å­¦ï¼ˆæ€§åˆ«ï¼‰ã€éŸµå¾‹å­¦ï¼ˆéŸ³è°ƒã€è¯­é€Ÿã€èƒ½é‡ï¼‰å’Œå‰¯è¯­è¨€ï¼ˆæƒ…æ„Ÿï¼‰ï¼Œç”¨äºè§£æ„æ¨¡å‹å¯¹è¯´è¯äººèº«ä»½çš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœéªŒè¯äº†ä¸‰ä¸ªé˜¶æ®µçš„å±‚æ¬¡ç»“æ„çš„ä¸€è‡´æ€§ï¼šåˆå§‹å±‚ç¼–ç åŸºæœ¬éŸ³è‰²å’ŒéŸµå¾‹ï¼›ä¸­é—´å±‚åˆæˆæŠ½è±¡ç‰¹å¾ï¼›æœ€ç»ˆå±‚æŠ‘åˆ¶è¯´è¯è€…èº«ä»½ä»¥æŠ½è±¡è¯­è¨€å†…å®¹ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶ä¸“ä¸šçš„è¯´è¯è€…åµŒå…¥åœ¨è¯†åˆ«è¯´è¯è€…èº«ä»½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†è¯­éŸ³SSLæ¨¡å‹çš„ä¸­é—´å±‚æ›´èƒ½ä»£è¡¨åŠ¨æ€çš„éŸµå¾‹ã€‚è¿™é¡¹å·¥ä½œæ˜¯ç¬¬ä¸€é¡¹å¤§è§„æ¨¡ç ”ç©¶ï¼Œæ¶µç›–äº†å¤šç§è¯­éŸ³SSLæ¨¡å‹å®¶æ—åŠå…¶å˜ä½“ï¼Œå¹¶è¯¦ç»†ç ”ç©¶äº†è¯´è¯äººç‰¹å®šçš„å±æ€§ï¼Œå®ƒä»¬å¦‚ä½•åˆ†å±‚åœ°å°†è¯­éŸ³çš„åŠ¨æ€é£æ ¼ä¸å…¶å†…åœ¨ç‰¹å¾åŒºåˆ†å¼€æ¥ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›äº†å®é™…å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05310v2">PDF</a> Submitted to the 2026 IEEE International Conference on Acoustics,   Speech, and Signal Processing (ICASSP 2026). Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸åŒè¯­éŸ³è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹åœ¨å±‚çº§åŒ–è¡¨ç¤ºä¸­å¯¹è¯´è¯äººç‰¹å®šå±æ€§çš„ç¼–ç æ–¹å¼ã€‚ç ”ç©¶é€šè¿‡å¯¹å¤šä¸ªSSLæ¨¡å‹å®¶æ—åŠå…¶å˜ç§è¿›è¡Œå¤§è§„æ¨¡æ¢æµ‹åˆ†æï¼Œå°†å±æ€§åˆ†ä¸ºåŠŸèƒ½ç»„ï¼ŒåŒ…æ‹¬å£°å­¦ï¼ˆæ€§åˆ«ï¼‰ã€éŸµå¾‹ï¼ˆéŸ³è°ƒã€è¯­é€Ÿã€èƒ½é‡ï¼‰å’Œå‰¯è¯­è¨€ï¼ˆæƒ…æ„Ÿï¼‰ï¼Œä»¥è§£æ„è¯´è¯äººèº«ä»½çš„æ¨¡å‹è¡¨ç¤ºã€‚ç ”ç©¶å‘ç°äº†ä¸€ä¸ªä¸‰å±‚çº§çš„ç»“æ„ï¼šåˆå§‹å±‚ç¼–ç åŸºæœ¬éŸ³è‰²å’ŒéŸµå¾‹ï¼›ä¸­é—´å±‚åˆæˆæŠ½è±¡ç‰¹å¾ï¼›æœ€ç»ˆå±‚æŠ‘åˆ¶è¯´è¯äººèº«ä»½ä»¥æŠ½è±¡è¯­è¨€å†…å®¹ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶ä¸“ä¸šè¯´è¯äººåµŒå…¥åœ¨è¯†åˆ«è¯´è¯äººèº«ä»½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†è¯­éŸ³SSLæ¨¡å‹çš„ä¸­é—´å±‚æ›´èƒ½ä»£è¡¨åŠ¨æ€çš„éŸµå¾‹ã€‚æœ¬æ–‡é¦–æ¬¡å¤§è§„æ¨¡åœ°ç ”ç©¶äº†å¤šç§è¯­éŸ³SSLæ¨¡å‹å®¶æ—åŠå…¶å˜ç§å¦‚ä½•å±‚æ¬¡åˆ†ç¦»è¯­éŸ³çš„åŠ¨æ€é£æ ¼å’Œå…¶å†…åœ¨ç‰¹å¾ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›äº†å®é™…å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹å­¦ä¹ å±‚æ¬¡åŒ–çš„è¡¨ç¤ºï¼Œä½†å®ƒä»¬åœ¨ç¼–ç è¯´è¯äººç‰¹å®šå±æ€§æ–¹é¢çš„æœºåˆ¶å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¯¹å¤šä¸ªSSLæ¨¡å‹çš„å¤§è§„æ¨¡æ¢æµ‹åˆ†æï¼Œå‘ç°è¯´è¯äººä¿¡æ¯åœ¨æ¨¡å‹ä¸­çš„å±‚çº§åŒ–ç¼–ç æ–¹å¼ã€‚</li>
<li>å°†å±æ€§åˆ†ä¸ºå£°å­¦ã€éŸµå¾‹å’Œå‰¯è¯­è¨€åŠŸèƒ½ç»„ï¼Œä»¥è§£æ„è¯´è¯äººèº«ä»½çš„æ¨¡å‹è¡¨ç¤ºã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªä¸‰å±‚çº§çš„ç»“æ„ï¼šåˆå§‹å±‚ç¼–ç åŸºæœ¬éŸ³è‰²å’ŒéŸµå¾‹ç‰¹å¾ï¼›ä¸­é—´å±‚åˆæˆæŠ½è±¡ç‰¹å¾ï¼›æœ€ç»ˆå±‚åˆ™ä¾§é‡äºè¯­è¨€å†…å®¹çš„æŠ½è±¡è¡¨è¾¾ã€‚</li>
<li>ä¸“ä¸šè¯´è¯äººåµŒå…¥åœ¨è¯†åˆ«è¯´è¯äººèº«ä»½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†è¯­éŸ³SSLæ¨¡å‹çš„ä¸­é—´å±‚æ›´èƒ½æ•æ‰åŠ¨æ€çš„éŸµå¾‹ä¿¡æ¯ã€‚</li>
<li>è¿™æ˜¯é¦–ä¸ªæ¶µç›–å¤šç§è¯­éŸ³SSLæ¨¡å‹çš„å¤§è§„æ¨¡ç ”ç©¶ï¼Œæ­ç¤ºäº†å¦‚ä½•å±‚æ¬¡åˆ†ç¦»è¯­éŸ³çš„åŠ¨æ€é£æ ¼å’Œå†…åœ¨ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7214fbdc85b5ecdca0582938e7a83281" align="middle">
<img src="https://picx.zhimg.com/v2-0c57eeafcd8db399f54b277e5a14d106" align="middle">
<img src="https://picx.zhimg.com/v2-31f59ed2e2c83cbdcd1698369683a872" align="middle">
<img src="https://picx.zhimg.com/v2-7b5c34c303be0e6708b2745006f7b30a" align="middle">
<img src="https://picx.zhimg.com/v2-2ecc5c9b66d68c034dced30ff5c1373c" align="middle">
<img src="https://picx.zhimg.com/v2-b7d3876462396322261d3c0e91888454" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-20/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c6137d2050dabf879722f20e08ae7c07" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  A Race Bias Free Face Aging Model for Reliable Kinship Verification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-324a4295b240cf74fe2dfe6d195e44fb" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  PVLM Parsing-Aware Vision Language Model with Dynamic Contrastive   Learning for Zero-Shot Deepfake Attribution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
