<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  ARGenSeg Image Segmentation with Autoregressive Image Generation Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-48d4c4b95795b831ed77a0ced85c355f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348526&auth_key=1761348526-0-0-a5f97054cbe1a49676f6914ec25c5c4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-25-æ›´æ–°"><a href="#2025-10-25-æ›´æ–°" class="headerlink" title="2025-10-25 æ›´æ–°"></a>2025-10-25 æ›´æ–°</h1><h2 id="ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model"><a href="#ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model" class="headerlink" title="ARGenSeg: Image Segmentation with Autoregressive Image Generation Model"></a>ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</h2><p><strong>Authors:Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</strong></p>
<p>We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºAutoRegressive Generationçš„å›¾åƒåˆ†å‰²èŒƒå¼ï¼ˆARGenSegï¼‰ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…å®ç°äº†å¤šæ¨¡æ€ç†è§£å’Œåƒç´ çº§æ„ŸçŸ¥ã€‚å…ˆå‰å°†å›¾åƒåˆ†å‰²é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­çš„å·¥ä½œé€šå¸¸é‡‡ç”¨è¾¹ç•Œç‚¹è¡¨ç¤ºæˆ–ä¸“ç”¨åˆ†å‰²å¤´ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºç¦»æ•£è¡¨ç¤ºæˆ–è¯­ä¹‰æç¤ºï¼Œè¿™äº›æç¤ºè¢«è¾“å…¥åˆ°ç‰¹å®šä»»åŠ¡çš„è§£ç å™¨ä¸­ï¼Œè¿™é™åˆ¶äº†MLLMæ•è·ç²¾ç»†ç²’åº¦è§†è§‰ç»†èŠ‚çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå›¾åƒç”Ÿæˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è‡ªç„¶åœ°äº§ç”Ÿç›®æ ‡å¯¹è±¡çš„å¯†é›†æ©ç ã€‚æˆ‘ä»¬åˆ©ç”¨MLLMè¾“å‡ºè§†è§‰ä»¤ç‰Œï¼Œå¹¶ä½¿ç”¨é€šç”¨VQ-VAEå°†å®ƒä»¬è§£ç æˆå›¾åƒï¼Œä½¿åˆ†å‰²å®Œå…¨ä¾èµ–äºMLLMçš„åƒç´ çº§ç†è§£ã€‚ä¸ºäº†å‡å°‘æ¨ç†å»¶è¿Ÿï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥æ¥å¹¶è¡Œç”Ÿæˆæ‰€éœ€çš„è§†è§‰ä»¤ç‰Œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€æ–°å‰æ²¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ¨ç†é€Ÿåº¦ä¸Šæœ‰æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶ä¿æŒç€å¼ºå¤§çš„ç†è§£èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20803v1">PDF</a> Accepted to NeurIPS 2025, 18 pages</p>
<p><strong>Summary</strong><br>     æå‡ºåŸºäºAutoRegressive Generationçš„æ–°å‹å›¾åƒåˆ†å‰²æ–¹æ³•ARGenSegï¼Œå®ç°å¤šæ¨¡æ€ç†è§£ä¸åƒç´ çº§æ„ŸçŸ¥çš„ç»Ÿä¸€æ¡†æ¶ã€‚ä¸ç°æœ‰æ•´åˆå›¾åƒåˆ†å‰²çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å¼•å…¥åŸºäºå›¾åƒç”Ÿæˆçš„åˆ†å‰²æ¡†æ¶ï¼Œå¯è‡ªç„¶ç”Ÿæˆç›®æ ‡å¯¹è±¡çš„å¯†é›†æ©æ¨¡ï¼Œå¹¶å®Œå…¨ä¾èµ–äºè¯­è¨€æ¨¡å‹çš„åƒç´ çº§ç†è§£è¿›è¡Œåˆ†å‰²ã€‚ä½¿ç”¨ä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹ç­–ç•¥æ¥å‡å°‘æ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶æé«˜äº†æ¨ç†é€Ÿåº¦å’Œåˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºåŸºäºAutoRegressive Generationçš„ARGenSegæ–¹æ³•ç”¨äºå›¾åƒåˆ†å‰²ã€‚</li>
<li>å®ç°å¤šæ¨¡æ€ç†è§£ä¸åƒç´ çº§æ„ŸçŸ¥çš„ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>å¼•å…¥åŸºäºå›¾åƒç”Ÿæˆçš„åˆ†å‰²æ¡†æ¶ï¼Œè‡ªç„¶ç”Ÿæˆç›®æ ‡å¯¹è±¡çš„å¯†é›†æ©æ¨¡ã€‚</li>
<li>ä¾èµ–äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åƒç´ çº§ç†è§£è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>ä½¿ç”¨ä¸‹ä¸€ä¸ªå°ºåº¦é¢„æµ‹ç­–ç•¥å‡å°‘æ¨ç†å»¶è¿Ÿã€‚</li>
<li>åœ¨å¤šä¸ªåˆ†å‰²æ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c3fe9a38dfcf86b94fff39cb7edde217~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348405&auth_key=1761348405-0-0-baac1c3ee2a3fd144f21075247be778c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6d4305f05a21f4f90693f322b923dc3f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348412&auth_key=1761348412-0-0-59ee326587cb20d2f3e8676cac8bb814&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff13e147e62b54ac56e41c64643256cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348419&auth_key=1761348419-0-0-feb2cc0dfa1f85ca00c73da120a24b36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Better-Tokens-for-Better-3D-Advancing-Vision-Language-Modeling-in-3D-Medical-Imaging"><a href="#Better-Tokens-for-Better-3D-Advancing-Vision-Language-Modeling-in-3D-Medical-Imaging" class="headerlink" title="Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D   Medical Imaging"></a>Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D   Medical Imaging</h2><p><strong>Authors:Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Dong Yang, Pengfei Guo, Marc Edgar, Daguang Xu, Bernhard Kainz, Bjoern Menze</strong></p>
<p>Recent progress in vision-language modeling for 3D medical imaging has been fueled by large-scale computed tomography (CT) corpora with paired free-text reports, stronger architectures, and powerful pretrained models. This has enabled applications such as automated report generation and text-conditioned 3D image synthesis. Yet, current approaches struggle with high-resolution, long-sequence volumes: contrastive pretraining often yields vision encoders that are misaligned with clinical language, and slice-wise tokenization blurs fine anatomy, reducing diagnostic performance on downstream tasks. We introduce BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder that unifies 2D and 3D training and inference while producing compact, frequency-aware volumetric tokens. A three-stage training curriculum enables (i) local reconstruction, (ii) overlapping-window tiling, and (iii) long-context decoder refinement, during which the model learns from short slice excerpts yet generalizes to scans exceeding 300 slices without additional memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it improves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and Merlin for report generation; and it reduces FID by 75% and halves FVD compared to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically consistent 512<em>512</em>241 volumes. These results confirm that precise three-dimensional tokenization, rather than larger language backbones alone, is essential for scalable vision-language modeling in 3D medical imaging. The codebase is available at: <a target="_blank" rel="noopener" href="https://github.com/ibrahimethemhamamci/BTB3D">https://github.com/ibrahimethemhamamci/BTB3D</a> </p>
<blockquote>
<p>è¿‘æœŸï¼Œ3DåŒ»å­¦å½±åƒçš„è§†è¯­è¨€æ¨¡å‹è¿›æ­¥æ˜¾è‘—ï¼Œè¿™å¾—ç›Šäºå¤§è§„æ¨¡é…æœ‰è‡ªç”±æ–‡æœ¬æŠ¥å‘Šçš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ•°æ®é›†ã€æ›´å¼ºå¤§çš„æ¶æ„å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™æ¨åŠ¨äº†è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶3Då›¾åƒåˆæˆç­‰åº”ç”¨çš„å‘å±•ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡ã€é•¿åºåˆ—ä½“ç§¯æ•°æ®æ—¶é‡åˆ°å›°éš¾ï¼šå¯¹æ¯”é¢„è®­ç»ƒå¾€å¾€ä¼šå¾—åˆ°ä¸ä¸´åºŠè¯­è¨€ä¸åŒ¹é…çš„è§†è§‰ç¼–ç å™¨ï¼Œè€Œé€ç‰‡åˆ‡åˆ†ä¼šæ¨¡ç³Šç²¾ç»†ç»“æ„ï¼Œé™ä½ä¸‹æ¸¸ä»»åŠ¡çš„è¯Šæ–­æ€§èƒ½ã€‚æˆ‘ä»¬æ¨å‡ºäº†BTB3Dï¼ˆæ›´å¥½çš„ä»¤ç‰Œç”¨äºæ›´å¥½çš„ä¸‰ç»´ï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾å› æœå·ç§¯ç¼–ç å™¨-è§£ç å™¨ï¼Œå¯ä»¥ç»Ÿä¸€2Då’Œ3Dçš„è®­ç»ƒå’Œæ¨ç†ï¼ŒåŒæ—¶äº§ç”Ÿç´§å‡‘ã€é¢‘ç‡æ„ŸçŸ¥çš„ä¸‰ç»´ä»¤ç‰Œã€‚ä¸‰é˜¶æ®µçš„è®­ç»ƒè¯¾ç¨‹ä½¿æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œï¼ˆiï¼‰å±€éƒ¨é‡å»ºï¼Œï¼ˆiiï¼‰é‡å çª—å£å¹³é“ºï¼Œï¼ˆiiiï¼‰é•¿ä¸Šä¸‹æ–‡è§£ç å™¨ç»†åŒ–ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä»çŸ­åˆ‡ç‰‡æ‘˜å½•ä¸­å­¦ä¹ ï¼Œå¹¶å¯ä»¥æ¨å¹¿åˆ°è¶…è¿‡300åˆ‡ç‰‡çš„æ‰«æï¼Œæ— éœ€é¢å¤–çš„å†…å­˜å¼€é”€ã€‚åœ¨ä¸¤é¡¹å…³é”®ä»»åŠ¡ä¸Šï¼ŒBTB3Dåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ï¼šåœ¨æŠ¥å‘Šç”Ÿæˆæ–¹é¢ï¼Œç›¸è¾ƒäºCT2Repã€CT-CHATå’ŒMerlinï¼ŒBLEUåˆ†æ•°å¾—åˆ°äº†æå‡ï¼Œä¸´åºŠF1å¢åŠ äº†40%ï¼›åœ¨æ–‡æœ¬åˆ°CTåˆæˆæ–¹é¢ï¼Œç›¸è¾ƒäºGenerateCTå’ŒMedSynï¼ŒFIDé™ä½äº†75%ï¼ŒFVDå‡åŠï¼Œç”Ÿæˆäº†ç»“æ„ä¸€è‡´çš„512<em>512</em>241ä½“ç§¯å›¾åƒã€‚è¿™äº›ç»“æœè¯å®ï¼Œç²¾ç¡®çš„ä¸‰ç»´ä»¤ç‰ŒåŒ–å¯¹äºå¯æ‰©å±•çš„ä¸‰ç»´åŒ»å­¦å½±åƒè§†è¯­è¨€å»ºæ¨¡è‡³å…³é‡è¦ï¼Œè€Œéå•é æ›´å¤§çš„è¯­è¨€éª¨å¹²ç½‘ç»œã€‚ä»£ç åº“å¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ibrahimethemhamamci/BTB3D">https://github.com/ibrahimethemhamamci/BTB3D</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20639v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åœ¨ä¸‰ç»´åŒ»å­¦æˆåƒä¸­ï¼Œç²¾å‡†çš„ä¸‰ç»´æ ‡è®°åŒ–å¯¹äºå¯ä¼¸ç¼©çš„è§†è§‰è¯­è¨€æ¨¡å‹çš„é‡è¦æ€§ã€‚ç ”ç©¶å¼•å…¥äº†BTB3Dæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å› æœå·ç§¯ç¼–ç å™¨è§£ç å™¨ï¼Œç»Ÿä¸€äº†äºŒç»´å’Œä¸‰ç»´çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œæé«˜äº†æŠ¥å‘Šç”Ÿæˆå’Œæ–‡æœ¬åˆ°CTå›¾åƒåˆæˆçš„æ€§èƒ½ã€‚é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„è®­ç»ƒè¯¾ç¨‹ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ çŸ­åˆ‡ç‰‡æ‘˜å½•å¹¶åœ¨æ²¡æœ‰é¢å¤–å†…å­˜å¼€é”€çš„æƒ…å†µä¸‹æ¨å¹¿åˆ°è¶…è¿‡300åˆ‡ç‰‡çš„æ‰«æã€‚è¯¥é¡¹ç›®ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°è¿›å±•ï¼šä»‹ç»äº†åŸºäºå¤§è§„æ¨¡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰è¯­æ–™åº“ã€å¼ºå¤§çš„æ¶æ„å’Œé¢„è®­ç»ƒæ¨¡å‹çš„è§†è§‰è¯­è¨€å»ºæ¨¡åœ¨ä¸‰ç»´åŒ»å­¦æˆåƒä¸­çš„æœ€æ–°è¿›å±•ã€‚</li>
<li>åº”ç”¨å®ä¾‹ï¼šå®ç°äº†è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶çš„ä¸‰ç»´å›¾åƒåˆæˆç­‰åº”ç”¨ã€‚</li>
<li>å½“å‰æŒ‘æˆ˜ï¼šå¯¹æ¯”é¢„è®­ç»ƒå¸¸å¸¸å¯¼è‡´ä¸ä¸´åºŠè¯­è¨€ä¸ä¸€è‡´çš„è§†è§‰ç¼–ç å™¨ï¼Œåˆ‡ç‰‡çº§åˆ«çš„æ ‡è®°åŒ–ä¼šæ¨¡ç³Šç²¾ç»†çš„è§£å‰–å­¦ç»“æ„ï¼Œé™ä½ä¸‹æ¸¸ä»»åŠ¡çš„è¯Šæ–­æ€§èƒ½ã€‚</li>
<li>BTB3Dæ¨¡å‹ä»‹ç»ï¼šå¼•å…¥BTB3Dæ¨¡å‹ï¼Œä¸€ä¸ªå› æœå·ç§¯ç¼–ç å™¨è§£ç å™¨ï¼Œèƒ½ç»Ÿä¸€äºŒç»´å’Œä¸‰ç»´çš„è®­ç»ƒå’Œæ¨ç†ï¼Œç”Ÿæˆç´§å‡‘çš„é¢‘ç‡æ„ŸçŸ¥ä½“ç§¯æ ‡è®°ã€‚</li>
<li>ä¸‰é˜¶æ®µè®­ç»ƒè¯¾ç¨‹ï¼šé€šè¿‡å±€éƒ¨é‡å»ºã€é‡å çª—å£æ‹¼è´´å’Œé•¿æœŸä¸Šä¸‹æ–‡è§£ç å™¨ç»†åŒ–ä¸‰ä¸ªé˜¶æ®µçš„è®­ç»ƒï¼Œæ¨¡å‹èƒ½åœ¨æ²¡æœ‰é¢å¤–å†…å­˜å¼€é”€çš„æƒ…å†µä¸‹å¤„ç†é•¿åºåˆ—ä½“ç§¯ã€‚</li>
<li>æ€§èƒ½è¡¨ç°ï¼šBTB3Dæ¨¡å‹åœ¨æŠ¥å‘Šç”Ÿæˆå’Œæ–‡æœ¬åˆ°CTå›¾åƒåˆæˆç­‰å…³é”®ä»»åŠ¡ä¸Šè®¾å®šäº†æ–°çš„æœ€ä½³è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-36511ff535b8fea089a2bd66675e6b66~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348427&auth_key=1761348427-0-0-7c9088e4c2cc1c915eba9b3aacac7156&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8d54942fe9bf6b6c993513d8ddcbd6bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348434&auth_key=1761348434-0-0-6fccd7f43559804e1274d8aa40442532&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d986755975e21211f4be91802d3c13db~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348441&auth_key=1761348441-0-0-bae9d8c2898f6663ebdbe1ffb5f2ae02&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Unlock-Anionic-Behavior-of-Calcium-Through-Pressure-Engineering"><a href="#Unlock-Anionic-Behavior-of-Calcium-Through-Pressure-Engineering" class="headerlink" title="Unlock Anionic Behavior of Calcium Through Pressure Engineering"></a>Unlock Anionic Behavior of Calcium Through Pressure Engineering</h2><p><strong>Authors:Yang Lv, Junwei Li, Jianfu Li, Yong Liu, Jianan Yuan, 1 Jiani Lin, Saori Kawaguchi-Imada, Qingyang Hu, Xiaoli Wang</strong></p>
<p>An isolated calcium (Ca) atom has empty d-orbitals under ambient conditions. However, s-d band hybridization has been observed in both elemental Ca and compounds by manipulating thermodynamic conditions. Here, we reveal that the Ca 3d-band can even capture electrons from halogen atoms under pressure, exhibiting anionic behaviors in iodides. We predict a CsCl-type monovalent CaI at above 50 GPa by employing first-principles structural searching and successfully identified the phase at 84 GPa using in situ X-ray diffraction. We further reveal that, due to the effect of orbital broadening, unusual charge transfer from the 5p orbitals of I to the 3d orbitals of Ca in CaI, gradually reverses the ionicity of Ca and becomes the anionic ICa at 485 GPa. Multivalent Ca stabilizes a set of metallic iodides with eight- to ten-fold iodine hyper-coordination. Our findings demonstrate that the valence states of Ca can vary from negative to +2, suggesting much greater complexity of Ca chemistry under ultrahigh pressures. </p>
<blockquote>
<p>ä¸€ä¸ªå­¤ç«‹çš„é’™ï¼ˆCaï¼‰åŸå­åœ¨ç¯å¢ƒæ¡ä»¶ä¸‹å…·æœ‰ç©ºçš„dè½¨é“ã€‚ç„¶è€Œï¼Œé€šè¿‡è°ƒèŠ‚çƒ­åŠ›å­¦æ¡ä»¶ï¼Œå…ƒç´ é’™å’ŒåŒ–åˆç‰©ä¸­å‡è§‚å¯Ÿåˆ°s-då¸¦æ‚äº¤ç°è±¡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‘ç°Caçš„3dè½¨é“ç”šè‡³å¯ä»¥åœ¨å‹åŠ›ä¸‹ä»å¤ç´ åŸå­ä¸­æ•è·ç”µå­ï¼Œåœ¨ç¢˜åŒ–ç‰©ä¸­è¡¨ç°å‡ºé˜´ç¦»å­è¡Œä¸ºã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨ç¬¬ä¸€æ€§åŸç†ç»“æ„æœç´¢æ–¹æ³•é¢„æµ‹äº†åœ¨50 GPaä»¥ä¸Šå­˜åœ¨çš„CsClå‹ä¸€ä»·CaIï¼Œå¹¶åœ¨84 GPaä¸‹é€šè¿‡åŸä½Xå°„çº¿è¡å°„æˆåŠŸé‰´å®šäº†è¯¥ç›¸ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ­ç¤ºï¼Œç”±äºè½¨é“æ‰©å±•çš„å½±å“ï¼Œç¢˜çš„5pè½¨é“åˆ°é’™çš„3dè½¨é“çš„å¼‚å¸¸ç”µè·è½¬ç§»åœ¨CaIä¸­é€æ¸é€†è½¬äº†é’™çš„ç¦»å­æ€§ï¼Œå¹¶åœ¨485 GPaä¸‹å½¢æˆé˜´ç¦»å­ICaã€‚å¤šä»·é’™ç¨³å®šäº†ä¸€ç³»åˆ—å…·æœ‰å…«åˆ°åå€ç¢˜è¶…é…ä½çš„é‡‘å±ç¢˜åŒ–ç‰©ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé’™çš„ä»·æ€å¯ä»¥ä»è´Ÿåˆ°+2å˜åŒ–ï¼Œè¡¨æ˜åœ¨è¶…é«˜å‹åŠ›ä¸‹é’™åŒ–å­¦çš„å¤æ‚æ€§æ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20395v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é’™åŸå­åœ¨é«˜å‹ä¸‹çš„ç‰¹æ®Šè¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨åŠ å‹æ¡ä»¶ä¸‹ï¼Œé’™çš„3dè½¨é“èƒ½æ•è·å¤ç´ åŸå­çš„ç”µå­ï¼Œè¡¨ç°å‡ºé˜´ç¦»å­æ€§è´¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¢˜åŒ–ç‰©ä¸­ã€‚é€šè¿‡ç¬¬ä¸€æ€§åŸç†ç»“æ„æœç´¢é¢„æµ‹äº†CsClå‹ä¸€ä»·CaIåœ¨50 GPaä»¥ä¸Šçš„å­˜åœ¨ï¼Œå¹¶é€šè¿‡åŸä½Xå°„çº¿è¡å°„åœ¨84 GPaä¸‹æˆåŠŸè¯†åˆ«å‡ºè¯¥ç›¸ã€‚æ­¤å¤–ï¼Œç”±äºè½¨é“å±•å®½æ•ˆåº”ï¼Œç¢˜çš„5pè½¨é“å‘é’™çš„3dè½¨é“å‘ç”Ÿç”µè·è½¬ç§»ï¼Œåœ¨485 GPaä¸‹é’™çš„ç¦»å­æ€§é€æ¸é€†è½¬ï¼Œå½¢æˆé˜´ç¦»å­ICaã€‚åŒæ—¶ï¼Œå¤šä»·é’™ç¨³å®šäº†ä¸€ç³»åˆ—å…·æœ‰å…«åˆ°åå€ç¢˜è¶…é…ä½çš„é‡‘å±ç¢˜åŒ–ç‰©ï¼Œè¡¨æ˜é’™åœ¨è¶…é«˜å‹åŠ›ä¸‹çš„åŒ–å­¦æ€§è´¨æ›´åŠ å¤æ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’™åŸå­åœ¨é«˜å‹ä¸‹è¡¨ç°å‡ºç‰¹æ®Šè¡Œä¸ºï¼Œå…¶3dè½¨é“èƒ½æ•è·å¤ç´ åŸå­çš„ç”µå­ã€‚</li>
<li>åœ¨ç¢˜åŒ–ç‰©ä¸­ï¼Œé’™è¡¨ç°å‡ºé˜´ç¦»å­æ€§è´¨ã€‚</li>
<li>é€šè¿‡ç¬¬ä¸€æ€§åŸç†ç»“æ„æœç´¢é¢„æµ‹äº†CsClå‹ä¸€ä»·CaIåœ¨é«˜å‹ä¸‹çš„å­˜åœ¨ã€‚</li>
<li>åœ¨84 GPaçš„å®éªŒæ¡ä»¶ä¸‹æˆåŠŸè¯†åˆ«å‡ºCsClå‹CaIç›¸ã€‚</li>
<li>è½¨é“å±•å®½æ•ˆåº”å¯¼è‡´ç¢˜çš„5pè½¨é“å‘é’™çš„3dè½¨é“å‘ç”Ÿç”µè·è½¬ç§»ã€‚</li>
<li>åœ¨æé«˜å‹åŠ›ï¼ˆ485 GPaï¼‰ä¸‹ï¼Œé’™çš„ç¦»å­æ€§é€æ¸é€†è½¬ï¼Œå½¢æˆé˜´ç¦»å­ICaã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-911c4df4159f46f21249e8133fa83d2c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348448&auth_key=1761348448-0-0-11c65205e02613475fc13ef05bb6f7ed&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FlowCycle-Pursuing-Cycle-Consistent-Flows-for-Text-based-Editing"><a href="#FlowCycle-Pursuing-Cycle-Consistent-Flows-for-Text-based-Editing" class="headerlink" title="FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing"></a>FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing</h2><p><strong>Authors:Yanghao Wang, Zhen Wang, Long Chen</strong></p>
<p>Recent advances in pre-trained text-to-image flow models have enabled remarkable progress in text-based image editing. Mainstream approaches always adopt a corruption-then-restoration paradigm, where the source image is first corrupted into an &#96;&#96;intermediate stateâ€™â€™ and then restored to the target image under the prompt guidance. However, current methods construct this intermediate state in a target-agnostic manner, i.e., they primarily focus on realizing source image reconstruction while neglecting the semantic gaps towards the specific editing target. This design inherently results in limited editability or inconsistency when the desired modifications substantially deviate from the source. In this paper, we argue that the intermediate state should be target-aware, i.e., selectively corrupting editing-relevant contents while preserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel inversion-free and flow-based editing framework that parameterizes corruption with learnable noises and optimizes them through a cycle-consistent process. By iteratively editing the source to the target and recovering back to the source with dual consistency constraints, FlowCycle learns to produce a target-aware intermediate state, enabling faithful modifications while preserving source consistency. Extensive ablations have demonstrated that FlowCycle achieves superior editing quality and consistency over state-of-the-art methods. </p>
<blockquote>
<p>è¿‘æœŸé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæµåŠ¨æ¨¡å‹çš„è¿›æ­¥ï¼Œä¸ºåŸºäºæ–‡æœ¬çš„å›¾åƒç¼–è¾‘å¸¦æ¥äº†æ˜¾è‘—çš„è¿›å±•ã€‚ä¸»æµæ–¹æ³•é€šå¸¸é‡‡ç”¨â€œç ´å-ç„¶å-æ¢å¤â€çš„æ¨¡å¼ï¼Œé¦–å…ˆå°†æºå›¾åƒç ´åä¸ºâ€œä¸­é—´çŠ¶æ€â€ï¼Œç„¶ååœ¨æç¤ºçš„æŒ‡å¯¼ä¸‹å°†å…¶æ¢å¤ä¸ºç›®æ ‡å›¾åƒã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä»¥ç›®æ ‡æ— å…³çš„æ–¹å¼æ„å»ºè¿™ä¸ªä¸­é—´çŠ¶æ€ï¼Œå³ä»–ä»¬ä¸»è¦å…³æ³¨æºå›¾åƒçš„é‡å»ºï¼Œè€Œå¿½ç•¥äº†å‘ç‰¹å®šç¼–è¾‘ç›®æ ‡çš„è¯­ä¹‰å·®è·ã€‚è¿™ç§è®¾è®¡å›ºæœ‰åœ°å¯¼è‡´ç¼–è¾‘èƒ½åŠ›æœ‰é™æˆ–ä¸ä¸€è‡´ï¼Œå½“æ‰€éœ€çš„ä¿®æ”¹ä¸æºå›¾åƒæœ‰è¾ƒå¤§åå·®æ—¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºä¸­é—´çŠ¶æ€åº”è¯¥æ˜¯ç›®æ ‡æ„ŸçŸ¥çš„ï¼Œå³é€‰æ‹©æ€§åœ°ç ´åä¸ç¼–è¾‘ç›¸å…³çš„å†…å®¹ï¼ŒåŒæ—¶ä¿ç•™ä¸ç¼–è¾‘æ— å…³çš„å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FlowCycleï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„æ— åæ¼”å’ŒåŸºäºæµåŠ¨çš„ç¼–è¾‘æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¯å­¦ä¹ çš„å™ªå£°å¯¹ç ´åè¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶é€šè¿‡å¾ªç¯ä¸€è‡´çš„è¿‡ç¨‹å¯¹å…¶è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡è¿­ä»£åœ°ä»æºç¼–è¾‘åˆ°ç›®æ ‡å¹¶æ¢å¤åˆ°æºï¼Œä½¿ç”¨åŒé‡ä¸€è‡´æ€§çº¦æŸï¼ŒFlowCycleå­¦ä¹ äº§ç”Ÿç›®æ ‡æ„ŸçŸ¥çš„ä¸­é—´çŠ¶æ€ï¼Œå®ç°åœ¨ä¿ç•™æºä¸€è‡´æ€§çš„åŒæ—¶ï¼Œè¿›è¡Œå¿ å®çš„ä¿®æ”¹ã€‚å¤§é‡çš„æ¶ˆèå®éªŒè¡¨æ˜ï¼ŒFlowCycleçš„ç¼–è¾‘è´¨é‡å’Œä¸€è‡´æ€§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20212v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒçš„é¢„è®­ç»ƒæµæ¨¡å‹åœ¨æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç°æœ‰ä¸»æµæ–¹æ³•éµå¾ªâ€œå…ˆç ´åå†æ¢å¤â€çš„æ¨¡å¼ï¼Œå°†åŸå§‹å›¾åƒè½¬åŒ–ä¸ºä¸­é—´çŠ¶æ€ï¼Œç„¶åæ ¹æ®æ–‡æœ¬æç¤ºæ¢å¤ä¸ºç›®æ ‡å›¾åƒã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•æ„å»ºä¸­é—´çŠ¶æ€æ—¶å¿½è§†äº†ç›®æ ‡å¯¼å‘æ€§ï¼Œä¸»è¦å…³æ³¨å›¾åƒé‡å»ºè€Œéå¯¹ç‰¹å®šç¼–è¾‘ç›®æ ‡çš„è¯­ä¹‰å¡«å……ã€‚è¿™å¯¼è‡´å½“æ‰€éœ€ä¿®æ”¹ä¸åŸå§‹å›¾åƒæœ‰è¾ƒå¤§å·®å¼‚æ—¶ï¼Œç¼–è¾‘èƒ½åŠ›å—é™æˆ–ç»“æœä¸ä¸€è‡´ã€‚æœ¬æ–‡ä¸»å¼ ä¸­é—´çŠ¶æ€åº”å…·å¤‡ç›®æ ‡å¯¼å‘æ€§ï¼Œå³é€‰æ‹©æ€§ç ´åä¸ç¼–è¾‘ç›¸å…³çš„å†…å®¹åŒæ—¶ä¿ç•™ä¸ç¼–è¾‘æ— å…³çš„å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FlowCycleï¼Œä¸€ç§æ— åæ¼”å’ŒåŸºäºæµçš„ç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡å‚æ•°åŒ–å™ªå£°å¹¶å¯¹å…¶è¿›è¡Œä¼˜åŒ–å®ç°å‘¨æœŸä¸€è‡´æ€§è¿‡ç¨‹ã€‚FlowCycleé€šè¿‡è¿­ä»£å¼åœ°ä»æºå›¾åƒç¼–è¾‘åˆ°ç›®æ ‡å›¾åƒå†æ¢å¤å›æºå›¾åƒï¼Œå­¦ä¹ ç”Ÿæˆç›®æ ‡å¯¼å‘çš„ä¸­é—´çŠ¶æ€ï¼Œå®ç°äº†å¿ å®ä¿®æ”¹çš„åŒæ—¶ä¿æŒæºä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒçš„æµæ¨¡å‹åœ¨æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘ä¸­å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å½“å‰ä¸»æµæ–¹æ³•é‡‡ç”¨â€œå…ˆç ´åå†æ¢å¤â€çš„æ¨¡å¼ï¼Œä½†æ„å»ºä¸­é—´çŠ¶æ€æ—¶ç¼ºä¹ç›®æ ‡å¯¼å‘æ€§ã€‚</li>
<li>æœ¬æ–‡ä¸»å¼ ä¸­é—´çŠ¶æ€åº”å…·å¤‡ç›®æ ‡å¯¼å‘æ€§ï¼Œå³é€‰æ‹©æ€§ç ´åä¸ç¼–è¾‘ç›¸å…³çš„å†…å®¹ã€‚</li>
<li>FlowCycleæ¡†æ¶è¢«æå‡ºï¼Œé€šè¿‡å‚æ•°åŒ–å™ªå£°å’Œä¼˜åŒ–å®ç°å‘¨æœŸä¸€è‡´æ€§è¿‡ç¨‹ã€‚</li>
<li>FlowCycleé€šè¿‡è¿­ä»£å¼ç¼–è¾‘å’Œæ¢å¤è¿‡ç¨‹ï¼Œå­¦ä¹ ç”Ÿæˆç›®æ ‡å¯¼å‘çš„ä¸­é—´çŠ¶æ€ã€‚</li>
<li>FlowCycleåœ¨ç¼–è¾‘è´¨é‡å’Œä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20212">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-11f2c017f7126b5e4ce5722badb74ced~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348456&auth_key=1761348456-0-0-6ca4eff9f2960e4ce056bc2c9bd229f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c66fc44928696cd4eb1d67be4455dab9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348463&auth_key=1761348463-0-0-67a878810d6c4c63c0833ac9ead94695&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-259ae5d648e875618c12a59352b603d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348470&auth_key=1761348470-0-0-757df31f429928af104005a2ce12fb91&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Filter-Based-Reconstruction-of-Images-from-Events"><a href="#Filter-Based-Reconstruction-of-Images-from-Events" class="headerlink" title="Filter-Based Reconstruction of Images from Events"></a>Filter-Based Reconstruction of Images from Events</h2><p><strong>Authors:Bernd Pfrommer</strong></p>
<p>Reconstructing an intensity image from the events of a moving event camera is a challenging task that is typically approached with neural networks deployed on graphics processing units. This paper presents a much simpler, FIlter Based Asynchronous Reconstruction method (FIBAR). First, intensity changes signaled by events are integrated with a temporal digital IIR filter. To reduce reconstruction noise, stale pixels are detected by a novel algorithm that regulates a window of recently updated pixels. Arguing that for a moving camera, the absence of events at a pixel location likely implies a low image gradient, stale pixels are then blurred with a Gaussian filter. In contrast to most existing methods, FIBAR is asynchronous and permits image read-out at an arbitrary time. It runs on a modern laptop CPU at about 42(140) million events&#x2F;s with (without) spatial filtering enabled. A few simple qualitative experiments are presented that show the difference in image reconstruction between FIBAR and a neural network-based approach (FireNet). FIBARâ€™s reconstruction is noisier than neural network-based methods and suffers from ghost images. However, it is sufficient for certain tasks such as the detection of fiducial markers. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ros-event-camera/event_image_reconstruction_fibar">https://github.com/ros-event-camera/event_image_reconstruction_fibar</a> </p>
<blockquote>
<p>ä»åŠ¨æ€äº‹ä»¶ç›¸æœºçš„äº‹ä»¶é‡å»ºå¼ºåº¦å›¾åƒæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œé€šå¸¸é€šè¿‡éƒ¨ç½²åœ¨å›¾å½¢å¤„ç†å•å…ƒä¸Šçš„ç¥ç»ç½‘ç»œæ¥è§£å†³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ›´ä¸ºç®€å•çš„åŸºäºæ»¤æ³¢å™¨çš„å¼‚æ­¥é‡å»ºæ–¹æ³•ï¼ˆFIBARï¼‰ã€‚é¦–å…ˆï¼Œé€šè¿‡æ—¶é—´æ•°å­—IIRæ»¤æ³¢å™¨æ•´åˆç”±äº‹ä»¶å¼•èµ·çš„å¼ºåº¦å˜åŒ–ã€‚ä¸ºäº†å‡å°‘é‡å»ºå™ªå£°ï¼Œé€šè¿‡ä¸€ç§æ–°å‹ç®—æ³•æ£€æµ‹è¿‡æ—¶çš„åƒç´ ï¼Œè¯¥ç®—æ³•å¯è°ƒæ§æœ€è¿‘æ›´æ–°çš„åƒç´ çª—å£ã€‚å¯¹äºç§»åŠ¨ç›¸æœºè€Œè¨€ï¼Œè®ºæ–‡ä¸»å¼ æŸä¸ªåƒç´ ä½ç½®æ²¡æœ‰äº‹ä»¶å¾ˆå¯èƒ½æ„å‘³ç€å›¾åƒæ¢¯åº¦è¾ƒä½ï¼Œéšåç”¨è¿‡æ—¶çš„åƒç´ è¿›è¡Œé«˜æ–¯æ»¤æ³¢ã€‚ä¸å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒFIBARæ˜¯å¼‚æ­¥çš„ï¼Œå¯ä»¥åœ¨ä»»æ„æ—¶é—´è¿›è¡Œå›¾åƒè¯»å–ã€‚å®ƒåœ¨å¯ç”¨ï¼ˆç¦ç”¨ï¼‰ç©ºé—´æ»¤æ³¢çš„æƒ…å†µä¸‹ï¼Œåœ¨ç°ä»£ç¬”è®°æœ¬ç”µè„‘CPUä¸Šçš„è¿è¡Œé€Ÿåº¦çº¦ä¸ºæ¯ç§’42ï¼ˆ140ï¼‰ç™¾ä¸‡äº‹ä»¶ã€‚æœ¬æ–‡è¿›è¡Œäº†ä¸€äº›ç®€å•çš„å®šæ€§å®éªŒï¼Œå±•ç¤ºäº†FIBARä¸åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼ˆå¦‚FireNetï¼‰åœ¨å›¾åƒé‡å»ºä¸Šçš„å·®å¼‚ã€‚FIBARçš„é‡å»ºç»“æœè¾ƒåŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•æ›´ä¸ºå˜ˆæ‚ï¼Œå¹¶å¯èƒ½å‡ºç°å¹½çµå›¾åƒã€‚ç„¶è€Œï¼Œå¯¹äºæŸäº›ä»»åŠ¡ï¼ˆå¦‚æ ‡è®°æ£€æµ‹ï¼‰è€Œè¨€ï¼Œå®ƒæ˜¯è¶³å¤Ÿçš„ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ros-event-camera/event_image_reconstruction_fibar%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ros-event-camera/event_image_reconstruction_fibaræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20071v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ»¤æ³¢çš„å¼‚æ­¥é‡å»ºæ–¹æ³•ï¼ˆFIBARï¼‰ï¼Œç”¨äºä»åŠ¨æ€äº‹ä»¶ç›¸æœºçš„äº‹ä»¶é‡å»ºå¼ºåº¦å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆäº‹ä»¶ä¿¡å·å¼ºåº¦å˜åŒ–ï¼Œé‡‡ç”¨æ•°å­—IIRæ»¤æ³¢å™¨è¿›è¡Œæ—¶é—´å¤„ç†ï¼Œå¹¶å¼•å…¥æ–°å‹ç®—æ³•æ£€æµ‹æ—§åƒç´ ä»¥å‡å°‘é‡å»ºå™ªå£°ã€‚æ­¤å¤–ï¼Œå¯¹äºç§»åŠ¨ç›¸æœºè€Œè¨€ï¼Œæ— åƒç´ ä½ç½®çš„äº‹ä»¶å¾ˆå¯èƒ½æš—ç¤ºä½å›¾åƒæ¢¯åº¦ï¼Œå› æ­¤ä½¿ç”¨é«˜æ–¯æ»¤æ³¢å™¨æ¨¡ç³Šæ—§åƒç´ ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒFIBARæ˜¯å¼‚æ­¥çš„ï¼Œå¯åœ¨ä»»æ„æ—¶é—´è¿›è¡Œå›¾åƒè¯»å–ã€‚åœ¨å…·æœ‰ï¼ˆä¸å…·æœ‰ï¼‰ç©ºé—´è¿‡æ»¤åŠŸèƒ½çš„ç°ä»£ç¬”è®°æœ¬ç”µè„‘CPUä¸Šï¼Œå…¶è¿è¡Œé€Ÿåº¦å¯è¾¾æ¯ç§’çº¦42ï¼ˆ140ï¼‰ç™¾ä¸‡äº‹ä»¶ã€‚ç®€å•å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼ˆå¦‚FireNetï¼‰ç›¸æ¯”ï¼ŒFIBARçš„å›¾åƒé‡å»ºå­˜åœ¨å™ªå£°å’Œé¬¼å½±ç°è±¡ï¼Œä½†å¯¹äºæŸäº›ä»»åŠ¡å¦‚æ£€æµ‹å®šä½æ ‡è®°ä»æ˜¯è¶³å¤Ÿçš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FIBARæ˜¯ä¸€ç§ç”¨äºä»åŠ¨æ€äº‹ä»¶ç›¸æœºçš„äº‹ä»¶é‡å»ºå¼ºåº¦å›¾åƒçš„ç®€åŒ–æ–¹æ³•ã€‚</li>
<li>å®ƒé‡‡ç”¨æ•°å­—IIRæ»¤æ³¢å™¨æ•´åˆäº‹ä»¶ä¿¡å·å¼ºåº¦å˜åŒ–ï¼Œå¹¶æ£€æµ‹æ—§åƒç´ ä»¥å‡å°‘é‡å»ºå™ªå£°ã€‚</li>
<li>å¯¹äºç§»åŠ¨ç›¸æœºï¼Œæ— åƒç´ ä½ç½®çš„äº‹ä»¶å¯èƒ½è¡¨ç¤ºä½å›¾åƒæ¢¯åº¦ï¼Œå› æ­¤ä½¿ç”¨é«˜æ–¯æ»¤æ³¢å™¨å¤„ç†æ—§åƒç´ ã€‚</li>
<li>FIBARæ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œå…è®¸åœ¨ä»»æ„æ—¶é—´è¿›è¡Œå›¾åƒè¯»å–ã€‚</li>
<li>FIBARåœ¨ç°ä»£ç¬”è®°æœ¬ç”µè„‘CPUä¸Šçš„è¿è¡Œé€Ÿåº¦è¾ƒå¿«ã€‚</li>
<li>ä¸åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ç›¸æ¯”ï¼ŒFIBARçš„å›¾åƒé‡å»ºå­˜åœ¨å™ªå£°å’Œé¬¼å½±ç°è±¡ã€‚</li>
<li>å°½ç®¡å­˜åœ¨è¿™äº›ç¼ºç‚¹ï¼Œä½†FIBARå¯¹äºæŸäº›ä»»åŠ¡å¦‚æ£€æµ‹å®šä½æ ‡è®°ä»æ˜¯æœ‰æ•ˆçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5c9ef597c906e9822e38bfab5be43e43~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348477&auth_key=1761348477-0-0-7678e7b8ac9069e3140b7723ce1f786b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-840f6a2ea05cb48b36f7bfdd9eda4932~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348485&auth_key=1761348485-0-0-5863bbf75de283d586a0fa81dd12e838&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-723677d67adc90a30afc84b22fbb3887~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348491&auth_key=1761348491-0-0-bbe238c3e968d71837106a90b5b05af5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-294df646e3f99bdafac93ed1cd41379e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348498&auth_key=1761348498-0-0-8d4e07164ef5c59a06baa00099f18380&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bc2bd2b0b4cfa649444a6b473e0506b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348505&auth_key=1761348505-0-0-610495757dcfd884cfdc59262720e907&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Machine-Learning-Based-Localization-Accuracy-of-RFID-Sensor-Networks-via-RSSI-Decision-Trees-and-CAD-Modeling-for-Defense-Applications"><a href="#Machine-Learning-Based-Localization-Accuracy-of-RFID-Sensor-Networks-via-RSSI-Decision-Trees-and-CAD-Modeling-for-Defense-Applications" class="headerlink" title="Machine Learning-Based Localization Accuracy of RFID Sensor Networks via   RSSI Decision Trees and CAD Modeling for Defense Applications"></a>Machine Learning-Based Localization Accuracy of RFID Sensor Networks via   RSSI Decision Trees and CAD Modeling for Defense Applications</h2><p><strong>Authors:Curtis Lee Shull, Merrick Green</strong></p>
<p>Radio Frequency Identification (RFID) tracking may be a viable solution for defense assets that must be stored in accordance with security guidelines. However, poor sensor specificity (vulnerabilities include long range detection, spoofing, and counterfeiting) can lead to erroneous detection and operational security events. We present a supervised learning simulation with realistic Received Signal Strength Indicator (RSSI) data and Decision Tree classification in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some of the challenges encountered in defense storage. In this work, we focused on classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw dataset had approximately 980,000 reads. Class frequencies were imbalanced, and class weights were calculated to account for class imbalance in this multi-class setting. The model, trained on stratified subsamples to 5,000 balanced observations, yielded an overall accuracy of 34.2% and F1-scores greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare classes (most notably LabZoneC) were often misclassified, even with the use of class weights. An adjacency-aware confusion matrix was calculated to allow better interpretation of physically adjacent zones. These results suggest that RSSI-based decision trees can be applied in realistic simulations to enable zone-level anomaly detection or misplacement monitoring for defense supply logistics. Reliable classification performance in low-coverage and low-signal zones could be improved with better antenna placement or additional sensors and sensor fusion with other modalities. </p>
<blockquote>
<p>å°„é¢‘è¯†åˆ«ï¼ˆRFIDï¼‰è·Ÿè¸ªå¯¹äºå¿…é¡»æŒ‰ç…§å®‰å…¨æŒ‡å—å­˜å‚¨çš„å›½é˜²èµ„äº§å¯èƒ½æ˜¯ä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¼ æ„Ÿå™¨ç‰¹å¼‚æ€§è¾ƒå·®ï¼ˆæ¼æ´åŒ…æ‹¬è¿œç¨‹æ£€æµ‹ã€æ¬ºéª—å’Œä¼ªé€ ï¼‰å¯èƒ½å¯¼è‡´é”™è¯¯æ£€æµ‹å’Œæ“ä½œå®‰å…¨äº‹ä»¶ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç›‘ç£å­¦ä¹ æ¨¡æ‹Ÿæ–¹æ³•ï¼Œä½¿ç”¨çœŸå®çš„æ¥æ”¶ä¿¡å·å¼ºåº¦æŒ‡ç¤ºï¼ˆRSSIï¼‰æ•°æ®å’Œå†³ç­–æ ‘åˆ†ç±»ï¼Œåœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰å»ºæ¨¡çš„å¹³é¢å›¾ä¸­ï¼Œä½“ç°äº†å›½é˜²å­˜å‚¨ä¸­é‡åˆ°çš„ä¸€äº›æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¯¹12ä¸ªå®éªŒå®¤åŒºåŸŸï¼ˆLabZoneA-Lï¼‰è¿›è¡Œåˆ†ç±»ï¼Œä»¥è¿›è¡Œä½ç½®æ¨æ–­ã€‚åŸå§‹æ•°æ®é›†å¤§çº¦æœ‰98ä¸‡ä¸ªè¯»å–æ•°æ®ã€‚ç±»åˆ«é¢‘ç‡åˆ†å¸ƒä¸å‡è¡¡ï¼Œæˆ‘ä»¬è®¡ç®—äº†ç±»åˆ«æƒé‡ï¼Œä»¥å¼¥è¡¥å¤šç±»åˆ«è®¾ç½®ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚è¯¥æ¨¡å‹ç»è¿‡åˆ†å±‚å­æ ·æœ¬è®­ç»ƒï¼Œè¾¾åˆ°5000ä¸ªå¹³è¡¡è§‚æµ‹å€¼ï¼Œæ€»ä½“å‡†ç¡®åº¦ä¸º34.2%ï¼Œå¤šä¸ªåŒºåŸŸçš„F1å¾—åˆ†å¤§äº0.40ï¼ˆåŒºåŸŸFã€Gã€Hç­‰ï¼‰ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨ä½¿ç”¨ç±»åˆ«æƒé‡çš„æƒ…å†µä¸‹ï¼Œç¨€æœ‰ç±»åˆ«ï¼ˆå°¤å…¶æ˜¯LabZoneCï¼‰é€šå¸¸ä¼šè¢«è¯¯åˆ†ç±»ã€‚è®¡ç®—äº†é‚»æ¥æ„ŸçŸ¥æ··æ·†çŸ©é˜µï¼Œä»¥æ›´å¥½åœ°è§£é‡Šç‰©ç†ä¸Šç›¸é‚»çš„åŒºåŸŸã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŸºäºRSSIçš„å†³ç­–æ ‘å¯ä»¥åº”ç”¨äºç°å®æ¨¡æ‹Ÿï¼Œä»¥å®ç°åŒºåŸŸçº§åˆ«çš„å¼‚å¸¸æ£€æµ‹æˆ–å›½é˜²ä¾›åº”ç‰©æµçš„é”™ä½ç›‘æµ‹ã€‚åœ¨ä½è¦†ç›–å’Œä½ä¿¡å·åŒºåŸŸçš„å¯é åˆ†ç±»æ€§èƒ½å¯ä»¥é€šè¿‡æ”¹å–„å¤©çº¿æ”¾ç½®ã€å¢åŠ ä¼ æ„Ÿå™¨æˆ–å…¶ä»–æ¨¡æ€çš„ä¼ æ„Ÿå™¨èåˆæ¥æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20019v1">PDF</a> 10 pages, 5 figures. Submitted to the Journal of Defense Modeling and   Simulation (JDMS) for the Special Issue Integrating AI&#x2F;ML Into Modeling and   Simulation (J22-4). This work evaluates machine learning-based RFID   localization for defense logistics environments using CAD-modeled simulations   and RSSI-driven decision tree classification</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å°„é¢‘è¯†åˆ«ï¼ˆRFIDï¼‰æŠ€æœ¯è·Ÿè¸ªå›½é˜²èµ„äº§çš„é—®é¢˜ã€‚è™½ç„¶RFIDå…·æœ‰å¯è¡Œæ€§ï¼Œä½†å…¶ä¼ æ„Ÿå™¨ç‰¹å¼‚æ€§è¾ƒå·®ï¼Œå¯èƒ½å¯¼è‡´è¯¯æ£€æµ‹å’Œæ“ä½œå®‰å…¨äº‹ä»¶ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰å»ºæ¨¡çš„å†³ç­–æ ‘åˆ†ç±»æ–¹æ³•ï¼Œç”¨äºæ¨¡æ‹ŸRSSIæ•°æ®çš„ç›‘ç£å­¦ä¹ ï¼Œå¹¶è§£å†³äº†åˆ†ç±»ä¸å¹³è¡¡é—®é¢˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRSSIå†³ç­–æ ‘å¯ç”¨äºå®ç°åŒºåŸŸçº§åˆ«çš„å¼‚å¸¸æ£€æµ‹æˆ–è¯¯æ”¾ç½®ç›‘æ§ï¼Œä½†åœ¨ä½è¦†ç›–ç‡å’Œä½ä¿¡å·åŒºåŸŸï¼Œå¯é€šè¿‡æ”¹è¿›å¤©çº¿æ”¾ç½®æˆ–ä½¿ç”¨å…¶ä»–ä¼ æ„Ÿå™¨å’Œä¼ æ„Ÿå™¨èåˆæ¥æé«˜å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RFIDæŠ€æœ¯åœ¨å›½é˜²èµ„äº§ç®¡ç†ä¸­æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œä½†éœ€è§£å†³ä¼ æ„Ÿå™¨ç‰¹å¼‚æ€§å·®çš„é—®é¢˜ã€‚</li>
<li>è¯¯æ£€æµ‹å’Œæ“ä½œå®‰å…¨äº‹ä»¶æ˜¯RFIDæŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>ä½¿ç”¨è®¡ç®—æœºå»ºæ¨¡çš„å†³ç­–æ ‘åˆ†ç±»æ–¹æ³•å¯ä»¥å¤„ç†RFIDæ•°æ®ä¸­çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>æ¨¡æ‹Ÿä¸­é‡‡ç”¨äº†RSSIæ•°æ®å¹¶è€ƒè™‘äº†ä¸å¹³è¡¡çš„åˆ†ç±»é—®é¢˜ã€‚</li>
<li>æ¨¡æ‹Ÿæ€»ä½“å‡†ç¡®åº¦ä¸º34.2%ï¼Œç‰¹å®šåŒºåŸŸçš„F1å¾—åˆ†è¾ƒé«˜ã€‚</li>
<li>å¯¹æŸäº›ç¨€æœ‰ç±»åˆ«çš„åˆ†ç±»æ•ˆæœä¸ç†æƒ³ï¼Œéœ€è¿›ä¸€æ­¥ä¼˜åŒ–ç®—æ³•å’Œç¡¬ä»¶é…ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b6b347c4b299085d73298a3dfce37071~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348512&auth_key=1761348512-0-0-71b99287365f2356d553fa363b25843c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97c5f41e4d14c198a718f06f6b8cdd9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348519&auth_key=1761348519-0-0-514b62878c1835edb0afdb98d7bd873a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-48d4c4b95795b831ed77a0ced85c355f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348526&auth_key=1761348526-0-0-a5f97054cbe1a49676f6914ec25c5c4d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FairGRPO-Fair-Reinforcement-Learning-for-Equitable-Clinical-Reasoning"><a href="#FairGRPO-Fair-Reinforcement-Learning-for-Equitable-Clinical-Reasoning" class="headerlink" title="FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning"></a>FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning</h2><p><strong>Authors:Shiqi Dai, Wei Dai, Jiaee Cheong, Paul Pu Liang</strong></p>
<p>Medical artificial intelligence systems have achieved remarkable diagnostic capabilities, yet they consistently exhibit performance disparities across demographic groups, causing real-world harm to underrepresented populations. While recent multimodal reasoning foundation models have advanced clinical diagnosis through integrated analysis of diverse medical data, reasoning trainings via reinforcement learning inherit and often amplify biases present in training datasets dominated by majority populations. We introduce Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical reinforcement learning approach that promotes equitable learning across heterogeneous clinical populations. FairGRPO employs adaptive importance weighting of advantages based on representation, task difficulty, and data source. To address the common issue of missing demographic labels in the clinical domain, we further employ unsupervised clustering, which automatically discovers latent demographic groups when labels are unavailable. Through comprehensive experiments across 7 clinical diagnostic datasets spanning 5 clinical modalities across X-ray, CT scan, dermoscropy, mammography and ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2% against all vanilla and bias mitigated RL baselines, while improving F1 score by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO progressively improves fairness throughout optimization, while baseline RL methods exhibit deteriorating fairness as training progresses. Based on FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that achieves state-of-the-art performance while demonstrating significantly reduced disparities across demographic groups. </p>
<blockquote>
<p>åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿå·²ç»å…·å¤‡äº†å“è¶Šçš„è¯Šç–—èƒ½åŠ›ï¼Œç„¶è€Œå®ƒä»¬åœ¨ä¸åŒäººç¾¤ä¸­çš„è¡¨ç°å§‹ç»ˆå­˜åœ¨å·®å¼‚ï¼Œç»™ä»£è¡¨æ€§ä¸è¶³çš„äººç¾¤å¸¦æ¥äº†ç°å®ä¸–ç•Œçš„ä¼¤å®³ã€‚è™½ç„¶æœ€è¿‘çš„è·¨æ¨¡æ€æ¨ç†åŸºç¡€æ¨¡å‹é€šè¿‡ç»¼åˆåˆ†æå¤šæ ·çš„åŒ»ç–—æ•°æ®æ¨åŠ¨äº†ä¸´åºŠè¯Šæ–­çš„è¿›æ­¥ï¼Œä½†å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¨ç†è®­ç»ƒä¼šç»§æ‰¿å¹¶ç»å¸¸æ”¾å¤§ç”±å¤šæ•°ç¾¤ä½“ä¸»å¯¼çš„è®­ç»ƒæ•°æ®é›†å­˜åœ¨çš„åè§ã€‚æˆ‘ä»¬å¼•å…¥äº†å…¬å¹³æ„ŸçŸ¥ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆFairGRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä¿ƒè¿›åœ¨å¼‚è´¨ä¸´åºŠäººç¾¤ä¸­çš„å…¬å¹³å­¦ä¹ ã€‚FairGRPOé‡‡ç”¨åŸºäºè¡¨ç¤ºã€ä»»åŠ¡éš¾åº¦å’Œæ•°æ®æºçš„é€‚åº”æ€§æƒé‡ä¼˜åŠ¿åŠ æƒã€‚ä¸ºäº†è§£å†³ä¸´åºŠé¢†åŸŸå¸¸è§çš„ç¼ºå¤±äººå£ç»Ÿè®¡æ ‡ç­¾çš„é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†æ— ç›‘ç£èšç±»ï¼Œå½“æ ‡ç­¾ä¸å¯ç”¨æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å‘ç°æ½œåœ¨çš„äººç¾¤åˆ†ç»„ã€‚é€šè¿‡å¯¹æ¶µç›–Xå…‰ã€CTæ‰«æã€çš®è‚¤é•œæ£€æŸ¥ã€ä¹³è…ºXçº¿å’Œè¶…å£°æ³¢ç­‰5ç§ä¸´åºŠæ¨¡æ€çš„7ä¸ªä¸´åºŠè¯Šæ–­æ•°æ®é›†è¿›è¡Œå…¨é¢å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†FairGRPOç›¸å¯¹äºæ‰€æœ‰åŸºç¡€å’Œæ— åè§å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œé¢„æµ‹å…¬å¹³æ€§æé«˜äº†27.2%ï¼ŒåŒæ—¶F1åˆ†æ•°æé«˜äº†12.49%ã€‚æ­¤å¤–ï¼Œè®­ç»ƒåŠ¨æ€åˆ†æè¡¨æ˜ï¼ŒFairGRPOåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€æ¸æ”¹å–„å…¬å¹³æ€§ï¼Œè€ŒåŸºçº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å…¬å¹³æ€§é€æ¸æ¶åŒ–ã€‚åŸºäºFairGRPOï¼Œæˆ‘ä»¬å‘å¸ƒäº†FairMedGemma-4Bï¼Œè¿™æ˜¯ä¸€æ¬¾å…·å¤‡å…¬å¹³æ„è¯†çš„ä¸´åºŠVLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ï¼Œåœ¨è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†ä¸åŒäººç¾¤ä¹‹é—´çš„å·®å¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19893v1">PDF</a> Accepted as Oral on NeurIPS 2025 GenAI4Health Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ä¸åŒäººç¾¤ä¸­çš„æ€§èƒ½å·®å¼‚é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¬å¹³æ„ŸçŸ¥çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼ˆFairGRPOï¼‰ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼Œä¿ƒè¿›ä¸åŒä¸´åºŠäººç¾¤ä¹‹é—´çš„å…¬å¹³å­¦ä¹ ã€‚é€šè¿‡è‡ªé€‚åº”æƒé‡è°ƒæ•´ä¼˜åŠ¿ï¼Œè€ƒè™‘ä»£è¡¨æ€§ã€ä»»åŠ¡éš¾åº¦å’Œæ¥æºç­‰æ•°æ®å› ç´ ã€‚åŒæ—¶ï¼Œè§£å†³ä¸´åºŠé¢†åŸŸå¸¸è§çš„ç¼ºå°‘äººå£ç»Ÿè®¡æ ‡ç­¾é—®é¢˜ï¼Œé‡‡ç”¨æ— ç›‘ç£èšç±»è‡ªåŠ¨å‘ç°æ½œåœ¨äººç¾¤ã€‚å®éªŒè¯æ˜ï¼ŒFairGRPOèƒ½å‡å°‘é¢„æµ‹åå·®ï¼Œæé«˜F1åˆ†æ•°ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸æ”¹å–„å…¬å¹³æ€§ã€‚åŸºäºFairGRPOï¼Œå‘å¸ƒäº†å…¬å¹³åŒ»ç–—å®çŸ³-4Bæ¨¡å‹ï¼Œå®ç°äº†è·¨äººç¾¤å…¬å¹³æ€§çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ä¸åŒäººç¾¤ä¸­çš„è¯Šæ–­æ€§èƒ½å­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“å—åˆ°å®é™…ä¼¤å®³ã€‚</li>
<li>å¤šæ¨¡æ€æ¨ç†åŸºç¡€æ¨¡å‹é€šè¿‡ç»¼åˆåˆ†æå¤šæ ·åŒ»ç–—æ•°æ®æå‡äº†ä¸´åºŠè¯Šæ–­ã€‚</li>
<li>åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¨ç†è®­ç»ƒå¯èƒ½ç»§æ‰¿æˆ–æ”¾å¤§åè§ï¼Œå°¤å…¶åœ¨ä»¥ä¸»æµäººç¾¤ä¸ºä¸»çš„è®­ç»ƒæ•°æ®é›†ä¸Šã€‚</li>
<li>FairGRPOæ–¹æ³•é‡‡ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ ä¿ƒè¿›å…¬å¹³å­¦ä¹ ï¼Œå¹¶è€ƒè™‘ä»£è¡¨æ€§ã€ä»»åŠ¡éš¾åº¦å’Œæ¥æºç­‰æ•°æ®å› ç´ è¿›è¡Œè‡ªé€‚åº”æƒé‡è°ƒæ•´ã€‚</li>
<li>FairGRPOè§£å†³äº†ä¸´åºŠé¢†åŸŸç¼ºå°‘äººå£ç»Ÿè®¡æ ‡ç­¾çš„é—®é¢˜ï¼Œé€šè¿‡æ— ç›‘ç£èšç±»è‡ªåŠ¨å‘ç°æ½œåœ¨äººç¾¤ã€‚</li>
<li>å®éªŒè¯æ˜FairGRPOèƒ½å‡å°‘é¢„æµ‹åå·®å’Œæé«˜F1åˆ†æ•°ï¼Œä¸å…¶ä»–åŸºå‡†å¼ºåŒ–å­¦ä¹ æ¨¡å‹ç›¸æ¯”è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fc25ff3825c1fb11afc3ae86d7c95705~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348533&auth_key=1761348533-0-0-2689fc3806c543fd2e19adce4adc3317&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-35ced155e6de6cf01581740892e6353f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348540&auth_key=1761348540-0-0-4fc1e629d51a73a15f1bf78a98c39c90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54165f6a4ad2820ac3510de7dcf49825~resize:0:q75.jpg?source=1f5c5e47&expiration=1761348547&auth_key=1761348547-0-0-70788c386130b883761e7527f274c9aa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-22d5d34b39dbbaab9df851f86d387f8c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309623&auth_key=1762309623-0-0-400c931c39f90a50bcc5a3bb0a8fb867&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Curvilinear-Structure-preserving-Unpaired-Cross-domain-Medical-Image-Translation"><a href="#Curvilinear-Structure-preserving-Unpaired-Cross-domain-Medical-Image-Translation" class="headerlink" title="Curvilinear Structure-preserving Unpaired Cross-domain Medical Image   Translation"></a>Curvilinear Structure-preserving Unpaired Cross-domain Medical Image   Translation</h2><p><strong>Authors:Zihao Chen, Yi Zhou, Xudong Jiang, Li Chen, Leopold Schmetterer, Bingyao Tan, Jun Cheng</strong></p>
<p>Unpaired image-to-image translation has emerged as a crucial technique in medical imaging, enabling cross-modality synthesis, domain adaptation, and data augmentation without costly paired datasets. Yet, existing approaches often distort fine curvilinear structures, such as microvasculature, undermining both diagnostic reliability and quantitative analysis. This limitation is consequential in ophthalmic and vascular imaging, where subtle morphological changes carry significant clinical meaning. We propose Curvilinear Structure-preserving Translation (CST), a general framework that explicitly preserves fine curvilinear structures during unpaired translation by integrating structure consistency into the training. Specifically, CST augments baseline models with a curvilinear extraction module for topological supervision. It can be seamlessly incorporated into existing methods. We integrate it into CycleGAN and UNSB as two representative backbones. Comprehensive evaluation across three imaging modalities: optical coherence tomography angiography, color fundus and X-ray coronary angiography demonstrates that CST improves translation fidelity and achieves state-of-the-art performance. By reinforcing geometric integrity in learned mappings, CST establishes a principled pathway toward curvilinear structure-aware cross-domain translation in medical imaging. </p>
<blockquote>
<p>éé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘åœ¨åŒ»å­¦æˆåƒä¸­å·²ç»æˆä¸ºä¸€é¡¹å…³é”®æŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜‚è´µçš„é…å¯¹æ•°æ®é›†çš„æƒ…å†µä¸‹å®ç°è·¨æ¨¡æ€åˆæˆã€åŸŸé€‚åº”å’Œæ•°æ®å¢å¼ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¼šæ‰­æ›²ç»†å¾®çš„æ›²çº¿ç»“æ„ï¼Œå¦‚å¾®è¡€ç®¡ï¼Œè¿™æ—¢å½±å“äº†è¯Šæ–­çš„å¯é æ€§ï¼Œä¹Ÿå½±å“äº†å®šé‡åˆ†æã€‚è¿™ä¸€å±€é™æ€§åœ¨çœ¼ç§‘å’Œè¡€ç®¡æˆåƒä¸­å°¤ä¸ºé‡è¦ï¼Œé‚£é‡Œç»†å¾®çš„å½¢æ€å˜åŒ–å…·æœ‰é‡è¦çš„ä¸´åºŠæ„ä¹‰ã€‚æˆ‘ä»¬æå‡ºäº†æ›²çº¿ç»“æ„ä¿ç•™ç¿»è¯‘ï¼ˆCSTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆç»“æ„ä¸€è‡´æ€§åˆ°è®­ç»ƒä¸­ï¼Œåœ¨ä¸æˆå¯¹çš„ç¿»è¯‘ä¸­æ˜ç¡®ä¿ç•™ç»†å¾®çš„æ›²çº¿ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼ŒCSTé€šè¿‡æ‹“æ‰‘ç›‘ç£å¢å¼ºåŸºçº¿æ¨¡å‹ï¼Œå¼•å…¥äº†æ›²çº¿æå–æ¨¡å—ã€‚å®ƒå¯ä»¥æ— ç¼åœ°èå…¥ç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬å°†å…¶èå…¥CycleGANå’ŒUNSBä½œä¸ºä¸¤ä¸ªä»£è¡¨æ€§çš„ä¸»å¹²ã€‚åœ¨ä¸‰ç§æˆåƒæ¨¡æ€çš„å…¨é¢è¯„ä¼°ï¼šå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡é€ å½±ã€å½©è‰²çœ¼åº•å’ŒXå°„çº¿å† çŠ¶åŠ¨è„‰é€ å½±è¡¨æ˜ï¼ŒCSTæé«˜äº†ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡åŠ å¼ºå­¦ä¹ æ˜ å°„ä¸­çš„å‡ ä½•å®Œæ•´æ€§ï¼ŒCSTä¸ºåŒ»å­¦æˆåƒä¸­é¢å‘æ›²çº¿ç»“æ„çš„è·¨åŸŸç¿»è¯‘å»ºç«‹äº†æœ‰åŸåˆ™çš„è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19679v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œæ— é…å¯¹å›¾åƒè½¬æ¢æŠ€æœ¯å·²å‘å±•ä¸ºä¸€é¡¹å…³é”®æŠ€æœ¯ï¼Œå¯åœ¨æ— éœ€æˆæœ¬é«˜æ˜‚çš„é…å¯¹æ•°æ®é›†çš„æƒ…å†µä¸‹å®ç°è·¨æ¨¡æ€åˆæˆã€åŸŸé€‚åº”å’Œæ•°æ®å¢å¼ºã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸ä¼šåœ¨ç»†å¾®æ›²çº¿ç»“æ„ï¼ˆå¦‚å¾®è¡€ç®¡ï¼‰ä¸Šäº§ç”Ÿæ‰­æ›²ï¼Œè¿™ä¼šå½±å“è¯Šæ–­çš„å¯é æ€§å’Œå®šé‡åˆ†æã€‚æœ¬æ–‡æå‡ºäº†æ›²çº¿ç»“æ„ä¿ç•™è½¬æ¢ï¼ˆCSTï¼‰è¿™ä¸€é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡é›†æˆç»“æ„ä¸€è‡´æ€§æ¥æ˜¾å¼ä¿ç•™ç»†å¾®æ›²çº¿ç»“æ„è¿›è¡Œæ— é…å¯¹è½¬æ¢ã€‚å…·ä½“æ¥è¯´ï¼ŒCSTé€šè¿‡æ‹“æ‰‘ç›‘ç£ä¸ºåŸºçº¿æ¨¡å‹å¢åŠ ä¸€ä¸ªæ›²çº¿æå–æ¨¡å—è¿›è¡Œå¢å¼ºã€‚å®ƒå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰æ–¹æ³•ä¸­ã€‚æˆ‘ä»¬å°†å…¶é›†æˆåˆ°CycleGANå’ŒUNSBä½œä¸ºä¸¤ä¸ªä»£è¡¨æ€§çš„éª¨å¹²ç½‘ã€‚åœ¨å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡é€ å½±ã€å½©è‰²çœ¼åº•å’ŒXå°„çº¿å† çŠ¶åŠ¨è„‰é€ å½±ä¸‰ç§æˆåƒæ¨¡å¼è¿›è¡Œçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒCSTæé«˜äº†è½¬æ¢ä¿çœŸåº¦ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡å¼ºåŒ–å‡ ä½•å®Œæ•´æ€§åœ¨å­¦ä¹ çš„æ˜ å°„ä¸­ï¼ŒCSTä¸ºåŒ»å­¦æˆåƒä¸­çš„æ›²çº¿ç»“æ„æ„ŸçŸ¥è·¨åŸŸè½¬æ¢å»ºç«‹äº†åŸåˆ™æ€§çš„é€”å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ— é…å¯¹å›¾åƒè½¬æ¢æŠ€æœ¯å·²æˆä¸ºåŒ»å­¦æˆåƒä¸­çš„å…³é”®æ–¹æ³•ï¼Œå¯åº”ç”¨äºè·¨æ¨¡æ€åˆæˆã€åŸŸé€‚åº”å’Œæ•°æ®å¢å¼ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç»†å¾®æ›²çº¿ç»“æ„ï¼ˆå¦‚å¾®è¡€ç®¡ï¼‰æ—¶ä¼šäº§ç”Ÿæ‰­æ›²ï¼Œå½±å“è¯Šæ–­å¯é æ€§å’Œå®šé‡åˆ†æã€‚</li>
<li>æå‡ºçš„CSTæ¡†æ¶é€šè¿‡é›†æˆç»“æ„ä¸€è‡´æ€§æ¥æ˜¾å¼ä¿ç•™ç»†å¾®æ›²çº¿ç»“æ„è¿›è¡Œæ— é…å¯¹è½¬æ¢ã€‚</li>
<li>CSTé€šè¿‡æ‹“æ‰‘ç›‘ç£å¢å¼ºåŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ›²çº¿æå–æ¨¡å—ã€‚</li>
<li>CSTå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰æ–¹æ³•ä¸­ï¼Œå¦‚CycleGANå’ŒUNSBã€‚</li>
<li>åœ¨å¤šç§æˆåƒæ¨¡å¼ä¸‹è¯„ä¼°ï¼ŒCSTæé«˜äº†è½¬æ¢çš„ä¿çœŸåº¦å¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4152cd3d5f2a89331fc01c9e15fddbc6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309630&auth_key=1762309630-0-0-4e1b7b168a487f5ce4a866ab4ccb5683&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6721f4888c07916ccf8de85599a5ba2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309638&auth_key=1762309638-0-0-2cc018c973d19ea005f96250a9a84605&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ace988afe912cc755bda1e93d324e52f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309644&auth_key=1762309644-0-0-3b8d2538398d667eb41eddf7ada48868&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f239c6e44ab78e8bf382e5e02f43bff~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309651&auth_key=1762309651-0-0-be90d04ad0fac97bbab224ce6f1c04be&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f57d4f98c6db8e7ac650f81fc5d204b~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309658&auth_key=1762309658-0-0-f71e11946a4685a3e4a36d01f505571c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94544c86e6e5e702d9d9e6f63d505a49~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309665&auth_key=1762309665-0-0-f88ae47d23a108c6e67b9299b3d2bbf0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-214dd98159f30f73849e098f1e71feaf~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309672&auth_key=1762309672-0-0-f183a157662fd8096b6462b5487f5cec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MedReason-R1-Learning-to-Reason-for-CT-Diagnosis-with-Reinforcement-Learning-and-Local-Zoom"><a href="#MedReason-R1-Learning-to-Reason-for-CT-Diagnosis-with-Reinforcement-Learning-and-Local-Zoom" class="headerlink" title="MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement   Learning and Local Zoom"></a>MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement   Learning and Local Zoom</h2><p><strong>Authors:Yifan Li, Fenghe Tang, Yingtai Li, Shaohua Kevin Zhou</strong></p>
<p>General-purpose large Vision-Language Models (VLMs) demonstrate strong capabilities in generating detailed descriptions for natural images. However, their performance in the medical domain remains suboptimal, even for relatively straightforward tasks, primarily due to the lack of large-scale, high-quality, specialized medical imaging datasets and the neglect of the diagnostic process that progresses from coarse to fine-grained. To address the first issue, we construct the CT-RATE-VQA dataset, which has 84K QA pairs. For the second issue, we propose MedReason-R1, a medical VLM with explicit reasoning process for disease diagnosis. MedReason-R1 incorporates a novel strategy that embeds zoom-in disease region-of-interest areas into the image, highlighting the crucial role of both global localization and disease-specific details in enhancing the modelâ€™s diagnostic performance. Furthermore, we introduce the GRPO reinforcement learning framework to MedReason-R1, which enables effective reasoning without relying on costly manual annotations. Compared to recent general-purpose and medical VLMs, MedReason-R1 achieves state-of-the-art performance in CT disease diagnosis while retaining generalization. The code, checkpoints, and dataset are available at: <a target="_blank" rel="noopener" href="https://github.com/Leevan001/MedReason-R1">https://github.com/Leevan001/MedReason-R1</a> </p>
<blockquote>
<p>é€šç”¨çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç”Ÿæˆè‡ªç„¶å›¾åƒè¯¦ç»†æè¿°æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŒ»å­¦é¢†åŸŸçš„è¡¨ç°ä»ç„¶ä¸å°½äººæ„ï¼Œå³ä½¿åœ¨ç›¸å¯¹ç®€å•çš„ä»»åŠ¡ä¸Šä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€ä¸“ä¸šçš„åŒ»å­¦æˆåƒæ•°æ®é›†ï¼Œä»¥åŠå¿½ç•¥äº†ä»ç²—ç•¥åˆ°ç²¾ç»†çš„è¯Šæ–­è¿‡ç¨‹çš„æ¨è¿›ã€‚ä¸ºäº†è§£å†³ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†CT-RATE-VQAæ•°æ®é›†ï¼ŒåŒ…å«84Kä¸ªé—®ç­”å¯¹ã€‚å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MedReason-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç–¾ç—…è¯Šæ–­çš„åŒ»å­¦VLMï¼Œå…·æœ‰æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚MedReason-R1é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ç­–ç•¥ï¼Œå°†æ”¾å¤§ç–¾ç—…æ„Ÿå…´è¶£åŒºåŸŸåµŒå…¥å›¾åƒä¸­ï¼Œå¼ºè°ƒäº†å…¨å±€å®šä½å’Œç–¾ç—…ç‰¹å®šç»†èŠ‚åœ¨æå‡æ¨¡å‹è¯Šæ–­æ€§èƒ½ä¸­çš„å…³é”®ä½œç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†GRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶å¼•å…¥åˆ°MedReason-R1ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¸ä¾èµ–æ˜‚è´µçš„äººå·¥æ³¨é‡Šçš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆæ¨ç†ã€‚ä¸æœ€æ–°çš„é€šç”¨å’ŒåŒ»å­¦VLMsç›¸æ¯”ï¼ŒMedReason-R1åœ¨CTç–¾ç—…è¯Šæ–­æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç ã€æ£€æŸ¥ç‚¹å’Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/Leevan001/MedReason-R1">https://github.com/Leevan001/MedReason-R1</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19626v1">PDF</a> The code, checkpoints, and dataset are available at:   <a target="_blank" rel="noopener" href="https://github.com/Leevan001/MedReason-R1">https://github.com/Leevan001/MedReason-R1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒé¢†åŸŸå­˜åœ¨çš„é—®é¢˜ï¼Œæ„å»ºäº†CT-RATE-VQAæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§å…·æœ‰æ˜ç¡®æ¨ç†è¿‡ç¨‹çš„åŒ»ç–—VLMâ€”â€”MedReason-R1ï¼Œç”¨äºç–¾ç—…è¯Šæ–­ã€‚MedReason-R1é‡‡ç”¨äº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œå°†ç–¾ç—…åŒºåŸŸçš„ç»†èŠ‚åµŒå…¥å›¾åƒä¸­ï¼Œå¼ºè°ƒå…¨å±€å®šä½å’Œç–¾ç—…ç‰¹å¼‚æ€§ç»†èŠ‚çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†GRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿MedReason-R1åœ¨ä¾èµ–æˆæœ¬é«˜æ˜‚çš„æ‰‹åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¿›è¡Œæœ‰æ•ˆæ¨ç†ã€‚åœ¨CTç–¾ç—…è¯Šæ–­æ–¹é¢ï¼ŒMedReason-R1è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¿æŒäº†æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMåœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„æ€§èƒ½ä»ç„¶ä¸å¤Ÿç†æƒ³ï¼Œä¸»è¦ç”±äºç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡çš„ä¸“ä¸šåŒ»å­¦æˆåƒæ•°æ®é›†å’Œè¯Šæ–­è¿‡ç¨‹ä»ç²—ç•¥åˆ°ç²¾ç»†çš„å¿½è§†ã€‚</li>
<li>æ„å»ºäº†CT-RATE-VQAæ•°æ®é›†ï¼ŒåŒ…å«8.4ä¸‡å¯¹é—®ç­”å¯¹ï¼Œä»¥æ”¹å–„ç¬¬ä¸€ä¸ªé—®é¢˜ã€‚</li>
<li>MedReason-R1æ˜¯ä¸€ç§å…·æœ‰æ˜ç¡®æ¨ç†è¿‡ç¨‹çš„åŒ»ç–—VLMï¼Œç”¨äºç–¾ç—…è¯Šæ–­ï¼Œç»“åˆäº†åŒ»å­¦å›¾åƒä¸­çš„ç»†èŠ‚å’Œå…¨å±€å®šä½ä¿¡æ¯ã€‚</li>
<li>MedReason-R1é‡‡ç”¨äº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œå°†ç–¾ç—…åŒºåŸŸçš„ç»†èŠ‚åµŒå…¥å›¾åƒä¸­ï¼Œä»¥æé«˜æ¨¡å‹çš„è¯Šæ–­æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†GRPOå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿MedReason-R1åœ¨ä¸éœ€è¦æ˜‚è´µçš„æ‰‹åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆæ¨ç†ã€‚</li>
<li>MedReason-R1åœ¨CTç–¾ç—…è¯Šæ–­æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¿æŒäº†æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dd9a18a289296bb69c568b25c3c0f0ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309681&auth_key=1762309681-0-0-fa7ff2dc9a5b974639eb8d6e0ecc3fe9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9539c4024142d860be4ac0363a203750~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309710&auth_key=1762309710-0-0-47a2c3c7af63c1c4e81afb4d483598a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16b6cf953f8aa1ce159d81a95990fa3c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309717&auth_key=1762309717-0-0-639a901a648ee7435bdf58374a54a102&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-315b9a0b89467e81fef9b1ec48892880~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309723&auth_key=1762309723-0-0-325eab99e1a2bf2f9e814f519858e1fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-29975293a67d374aaf054d0987e0ce77~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309730&auth_key=1762309730-0-0-1f8bb7c8cfb858cedd2f544c0072a768&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2f354fd719dd91bb6678ea89cf7904dc~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309736&auth_key=1762309736-0-0-ca448f898279d1968e42da1b7078cfb3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Addressing-the-Depth-of-Field-Constraint-A-New-Paradigm-for-High-Resolution-Multi-Focus-Image-Fusion"><a href="#Addressing-the-Depth-of-Field-Constraint-A-New-Paradigm-for-High-Resolution-Multi-Focus-Image-Fusion" class="headerlink" title="Addressing the Depth-of-Field Constraint: A New Paradigm for High   Resolution Multi-Focus Image Fusion"></a>Addressing the Depth-of-Field Constraint: A New Paradigm for High   Resolution Multi-Focus Image Fusion</h2><p><strong>Authors:Luca Piano, Peng Huanwen, Radu Ciprian Bilcu</strong></p>
<p>Multi-focus image fusion (MFIF) addresses the depth-of-field (DOF) limitations of optical lenses, where only objects within a specific range appear sharp. Although traditional and deep learning methods have advanced the field, challenges persist, including limited training data, domain gaps from synthetic datasets, and difficulties with regions lacking information. We propose VAEEDOF, a novel MFIF method that uses a distilled variational autoencoder for high-fidelity, efficient image reconstruction. Our fusion module processes up to seven images simultaneously, enabling robust fusion across diverse focus points. To address data scarcity, we introduce MattingMFIF, a new syntetic 4K dataset, simulating realistic DOF effects from real photographs. Our method achieves state-of-the-art results, generating seamless artifact-free fused images and bridging the gap between synthetic and real-world scenarios, offering a significant step forward in addressing complex MFIF challenges. The code, and weights are available here: </p>
<blockquote>
<p>å¤šç„¦ç‚¹å›¾åƒèåˆï¼ˆMFIFï¼‰è§£å†³äº†å…‰å­¦é•œå¤´çš„æ™¯æ·±ï¼ˆDOFï¼‰é™åˆ¶é—®é¢˜ï¼Œæ™¯æ·±é™åˆ¶å¯¼è‡´åªæœ‰ç‰¹å®šèŒƒå›´å†…çš„ç‰©ä½“æ‰æ˜¾å¾—æ¸…æ™°ã€‚å°½ç®¡ä¼ ç»Ÿå’Œæ·±åº¦å­¦ä¹ æ–¹æ³•å·²ç»æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®æœ‰é™ã€åˆæˆæ•°æ®é›†ä¸å®é™…åº”ç”¨åœºæ™¯ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä»¥åŠç¼ºä¹ä¿¡æ¯çš„åŒºåŸŸå¤„ç†å›°éš¾ç­‰é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†VAEEDOFï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹MFIFæ–¹æ³•ï¼Œå®ƒé‡‡ç”¨æç‚¼å‡ºçš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œé«˜ä¿çœŸã€é«˜æ•ˆçš„å›¾åƒé‡å»ºã€‚æˆ‘ä»¬çš„èåˆæ¨¡å—å¯ä»¥åŒæ—¶å¤„ç†å¤šè¾¾ä¸ƒå¼ å›¾åƒï¼Œå®ç°ä¸åŒç„¦ç‚¹åŒºåŸŸçš„ç¨³å¥èåˆã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MattingMFIFè¿™ä¸€æ–°çš„åˆæˆ4Kæ•°æ®é›†ï¼Œæ¨¡æ‹ŸçœŸå®ç…§ç‰‡ä¸­çš„ç°å®æ™¯æ·±æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œç”Ÿæˆæ— ç¼ã€æ— ç‘•ç–µçš„èåˆå›¾åƒï¼Œå¹¶ç¼©å°äº†åˆæˆåœºæ™¯å’ŒçœŸå®åœºæ™¯ä¹‹é—´çš„å·®è·ï¼Œä¸ºè§£å†³å¤æ‚çš„MFIFæŒ‘æˆ˜è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚ä»£ç å’Œæƒé‡å¯åœ¨æ­¤å¤„è·å–ï¼š</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19581v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šç„¦ç‚¹å›¾åƒèåˆï¼ˆMFIFï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯è§£å†³äº†å…‰å­¦é€é•œæ™¯æ·±ï¼ˆDOFï¼‰é™åˆ¶çš„é—®é¢˜ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•å’Œæ·±åº¦å­¦ä¹ åœ¨è¯¥é¢†åŸŸçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä½¿ç”¨è’¸é¦å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEEDOFï¼‰çš„æ–°MFIFæ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸã€é«˜æ•ˆçš„å›¾åƒé‡å»ºã€‚åŒæ—¶ï¼Œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¼•å…¥äº†æ¨¡æ‹ŸçœŸå®æ™¯æ·±æ•ˆæœçš„æ–°åˆæˆ4Kæ•°æ®é›†MattingMFIFã€‚è¯¥æ–¹æ³•å®ç°äº†ä¸šç•Œé¢†å…ˆçš„ç»“æœï¼Œç”Ÿæˆæ— ç¼ã€æ— ç‘•ç–µçš„èåˆå›¾åƒï¼Œå¹¶ç¼©å°äº†åˆæˆä¸çœŸå®åœºæ™¯ä¹‹é—´çš„å·®è·ï¼Œä¸ºè§£å†³å¤æ‚çš„MFIFæŒ‘æˆ˜è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç„¦ç‚¹å›¾åƒèåˆï¼ˆMFIFï¼‰æŠ€æœ¯è§£å†³äº†å…‰å­¦é€é•œæ™¯æ·±ï¼ˆDOFï¼‰é™åˆ¶çš„é—®é¢˜ï¼Œä½¿ä¸åŒç„¦ç‚¹ç‚¹çš„å›¾åƒèƒ½å¤Ÿèåˆã€‚</li>
<li>æå‡ºäº†ä½¿ç”¨è’¸é¦å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEEDOFï¼‰çš„æ–°MFIFæ–¹æ³•ï¼Œæé«˜äº†å›¾åƒé‡å»ºçš„æ•ˆç‡å’Œä¿çœŸåº¦ã€‚</li>
<li>VAEEDOFèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šè¾¾7å¼ å›¾åƒï¼Œå®ç°è·¨ä¸åŒç„¦ç‚¹çš„ç¨³å¥èåˆã€‚</li>
<li>é’ˆå¯¹æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå¼•å…¥äº†æ–°çš„åˆæˆ4Kæ•°æ®é›†MattingMFIFï¼Œæ¨¡æ‹ŸçœŸå®æ™¯æ·±æ•ˆæœã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†ä¸šç•Œé¢†å…ˆçš„ç»“æœï¼Œç”Ÿæˆäº†æ— ç¼ã€æ— ç‘•ç–µçš„èåˆå›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•ç¼©å°äº†åˆæˆå›¾åƒå’ŒçœŸå®åœºæ™¯ä¹‹é—´çš„å·®è·ï¼Œæé«˜äº†MFIFæŠ€æœ¯çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4d4636631cf17e1c8eb6496d2829bf1c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309743&auth_key=1762309743-0-0-4b59ebb149713c15914b7d61e0bf5676&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8c4c46ae3889e126d38ae889f6ec9b5~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309752&auth_key=1762309752-0-0-8f0094c475c917d71cc6c9e603c7c71c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c17cf9a45d3685b30c901c0b070846ab~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309758&auth_key=1762309758-0-0-e88d22ca912166332fb5b1e7fa0ae7fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b2bbcce9fb21bb272917d8e5da4f847~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309765&auth_key=1762309765-0-0-19ffa7989621b8f9afb09cb1c2c9f6b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b6924d385d7a3cbf2fc6788a056524ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309772&auth_key=1762309772-0-0-af6b2c58cb114b60e3cddf2fe457657d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Predicting-before-Reconstruction-A-generative-prior-framework-for-MRI-acceleration"><a href="#Predicting-before-Reconstruction-A-generative-prior-framework-for-MRI-acceleration" class="headerlink" title="Predicting before Reconstruction: A generative prior framework for MRI   acceleration"></a>Predicting before Reconstruction: A generative prior framework for MRI   acceleration</h2><p><strong>Authors:Juhyung Park, Rokgi Hong, Roh-Eul Yoo, Jaehyeon Koo, Se Young Chun, Seung Hong Choi, Jongho Lee</strong></p>
<p>Recent advancements in artificial intelligence have created transformative capabilities in image synthesis and generation, enabling diverse research fields to innovate at revolutionary speed and spectrum. In this study, we leverage this generative power to introduce a new paradigm for accelerating Magnetic Resonance Imaging (MRI), introducing a shift from image reconstruction to proactive predictive imaging. Despite being a cornerstone of modern patient care, MRIâ€™s lengthy acquisition times limit clinical throughput. Our novel framework addresses this challenge by first predicting a target contrast image, which then serves as a data-driven prior for reconstructing highly under-sampled data. This informative prior is predicted by a generative model conditioned on diverse data sources, such as other contrast images, previously scanned images, acquisition parameters, patient information. We demonstrate this approach with two key applications: (1) reconstructing FLAIR images using predictions from T1w and&#x2F;or T2w scans, and (2) reconstructing T1w images using predictions from previously acquired T1w scans. The framework was evaluated on internal and multiple public datasets (total 14,921 scans; 1,051,904 slices), including multi-channel k-space data, for a range of high acceleration factors (x4, x8 and x12). The results demonstrate that our prediction-prior reconstruction method significantly outperforms other approaches, including those with alternative or no prior information. Through this framework we introduce a fundamental shift from image reconstruction towards a new paradigm of predictive imaging. </p>
<blockquote>
<p>è¿‘æœŸäººå·¥æ™ºèƒ½çš„è¿›æ­¥ä¸ºå›¾åƒåˆæˆå’Œç”Ÿæˆé¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„èƒ½åŠ›ï¼Œä¿ƒä½¿ä¸åŒç ”ç©¶é¢†åŸŸä»¥é©å‘½æ€§çš„é€Ÿåº¦å’Œå¹¿åº¦è¿›è¡Œåˆ›æ–°ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™ç§ç”Ÿæˆèƒ½åŠ›ï¼Œå¼•å…¥äº†ä¸€ç§åŠ é€Ÿç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„æ–°èŒƒå¼ï¼Œå®ç°ä»å›¾åƒé‡å»ºåˆ°ç§¯æé¢„æµ‹æˆåƒçš„è½¬å˜ã€‚å°½ç®¡MRIæ˜¯ç°ä»£æ‚£è€…æŠ¤ç†çš„åŸºçŸ³ï¼Œä½†å…¶æ¼«é•¿çš„é‡‡é›†æ—¶é—´é™åˆ¶äº†ä¸´åºŠååé‡ã€‚æˆ‘ä»¬çš„æ–°æ¡†æ¶é€šè¿‡é¦–å…ˆé¢„æµ‹ç›®æ ‡å¯¹æ¯”å›¾åƒæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥å›¾åƒç„¶åä½œä¸ºæ•°æ®é©±åŠ¨çš„å…ˆéªŒæ¥é‡å»ºé«˜åº¦æ¬ é‡‡æ ·çš„æ•°æ®ã€‚è¿™ä¸€ä¿¡æ¯å…ˆéªŒæ˜¯ç”±ç”Ÿæˆæ¨¡å‹æ ¹æ®å¤šç§æ•°æ®æºé¢„æµ‹çš„ï¼Œå¦‚å…¶ä»–å¯¹æ¯”å›¾åƒã€å…ˆå‰æ‰«æçš„å›¾åƒã€é‡‡é›†å‚æ•°ã€æ‚£è€…ä¿¡æ¯ç­‰ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªå…³é”®åº”ç”¨å±•ç¤ºäº†è¿™ç§æ–¹æ³•ï¼šï¼ˆ1ï¼‰ä½¿ç”¨T1wå’Œ&#x2F;æˆ–T2wæ‰«æçš„é¢„æµ‹ç»“æœé‡å»ºFLAIRå›¾åƒï¼›ï¼ˆ2ï¼‰ä½¿ç”¨å…ˆå‰è·å–çš„T1wæ‰«æçš„é¢„æµ‹ç»“æœé‡å»ºT1wå›¾åƒã€‚è¯¥æ¡†æ¶åœ¨å†…éƒ¨å’Œå¤šä¸ªå…¬å…±æ•°æ®é›†ï¼ˆå…±14921æ¬¡æ‰«æï¼Œ105ä¸‡1åƒ9ç™¾é›¶å››æ¬¡åˆ‡ç‰‡ï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å¤šé€šé“kç©ºé—´æ•°æ®ï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—é«˜åŠ é€Ÿå› å­ï¼ˆx4ã€x8å’Œx12ï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é¢„æµ‹å…ˆéªŒé‡å»ºæ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ›¿ä»£æˆ–æ²¡æœ‰å…ˆéªŒä¿¡æ¯çš„æ–¹æ³•ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬ä»å›¾åƒé‡å»ºè½¬å‘é¢„æµ‹æˆåƒçš„æ–°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19472v1">PDF</a> 33 pages, 8figures</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨äººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•ï¼Œå¼•å…¥ä¸€ç§æ–°å‹èŒƒå¼åŠ é€Ÿç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œå®ç°ä»å›¾åƒé‡å»ºåˆ°é¢„æµ‹æ€§æˆåƒçš„è½¬å˜ã€‚é€šè¿‡é¢„æµ‹ç›®æ ‡å¯¹æ¯”å›¾åƒä½œä¸ºæ•°æ®é©±åŠ¨å…ˆéªŒï¼Œé‡å»ºé«˜åº¦æ¬ é‡‡æ ·çš„æ•°æ®ï¼Œè§£å†³MRIé‡‡é›†æ—¶é—´é•¿çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨å›¾åƒåˆæˆå’Œç”Ÿæˆæ–¹é¢çš„æœ€æ–°è¿›å±•ä¸ºå¤šä¸ªç ”ç©¶é¢†åŸŸå¸¦æ¥äº†åˆ›æ–°ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨ç”Ÿæˆèƒ½åŠ›åŠ é€Ÿç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œä»å›¾åƒé‡å»ºè½¬å‘é¢„æµ‹æ€§æˆåƒã€‚</li>
<li>MRIé‡‡é›†æ—¶é—´é•¿æ˜¯ä¸´åºŠé€šè¿‡é‡çš„ç“¶é¢ˆã€‚</li>
<li>é€šè¿‡é¢„æµ‹ç›®æ ‡å¯¹æ¯”å›¾åƒä½œä¸ºæ•°æ®é©±åŠ¨å…ˆéªŒï¼Œé‡å»ºé«˜åº¦æ¬ é‡‡æ ·çš„æ•°æ®ã€‚</li>
<li>é¢„æµ‹æ€§å…ˆéªŒæ˜¯ç”±ä»¥å¤šç§æ•°æ®æºä¸ºæ¡ä»¶çš„ç”Ÿæˆæ¨¡å‹é¢„æµ‹çš„ï¼Œå¦‚å…¶ä»–å¯¹æ¯”å›¾åƒã€å…ˆå‰æ‰«æçš„å›¾åƒã€é‡‡é›†å‚æ•°ã€ç—…äººä¿¡æ¯ç­‰ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†ä¸¤ä¸ªå…³é”®åº”ç”¨ï¼šåˆ©ç”¨T1wå’Œ&#x2F;æˆ–T2wæ‰«æçš„é¢„æµ‹é‡å»ºFLAIRå›¾åƒå’Œåˆ©ç”¨å…ˆå‰è·å–çš„T1wæ‰«æçš„é¢„æµ‹é‡å»ºT1wå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-03f9da3f3306b68f92ae68b2af2da7d1~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309779&auth_key=1762309779-0-0-3ba0242716b36549e0441b21c961981c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cda0d51b5063a1b44a26edc5544763d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309787&auth_key=1762309787-0-0-2e31b7df7673e8c0d551ff57b5c16a5f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Training-Free-Framework-for-Open-Vocabulary-Image-Segmentation-and-Recognition-with-EfficientNet-and-CLIP"><a href="#A-Training-Free-Framework-for-Open-Vocabulary-Image-Segmentation-and-Recognition-with-EfficientNet-and-CLIP" class="headerlink" title="A Training-Free Framework for Open-Vocabulary Image Segmentation and   Recognition with EfficientNet and CLIP"></a>A Training-Free Framework for Open-Vocabulary Image Segmentation and   Recognition with EfficientNet and CLIP</h2><p><strong>Authors:Ying Dai, Wei Yu Chen</strong></p>
<p>This paper presents a novel training-free framework for open-vocabulary image segmentation and object recognition (OVSR), which leverages EfficientNetB0, a convolutional neural network, for unsupervised segmentation and CLIP, a vision-language model, for open-vocabulary object recognition. The proposed framework adopts a two stage pipeline: unsupervised image segmentation followed by segment-level recognition via vision-language alignment. In the first stage, pixel-wise features extracted from EfficientNetB0 are decomposed using singular value decomposition to obtain latent representations, which are then clustered using hierarchical clustering to segment semantically meaningful regions. The number of clusters is adaptively determined by the distribution of singular values. In the second stage, the segmented regions are localized and encoded into image embeddings using the Vision Transformer backbone of CLIP. Text embeddings are precomputed using CLIPâ€™s text encoder from category-specific prompts, including a generic something else prompt to support open set recognition. The image and text embeddings are concatenated and projected into a shared latent feature space via SVD to enhance cross-modal alignment. Recognition is performed by computing the softmax over the similarities between the projected image and text embeddings. The proposed method is evaluated on standard benchmarks, including COCO, ADE20K, and PASCAL VOC, achieving state-of-the-art performance in terms of Hungarian mIoU, precision, recall, and F1-score. These results demonstrate the effectiveness, flexibility, and generalizability of the proposed framework. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²å’Œå¯¹è±¡è¯†åˆ«ï¼ˆOVSRï¼‰çš„æ–°å‹æ— è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨EfficientNetB0å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œæ— ç›‘ç£åˆ†å‰²ï¼Œå¹¶åˆ©ç”¨CLIPè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¼€æ”¾è¯æ±‡å¯¹è±¡è¯†åˆ«ã€‚æ‰€æå‡ºæ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šæ— ç›‘ç£å›¾åƒåˆ†å‰²ï¼Œç„¶åé€šè¿‡è§†è§‰è¯­è¨€å¯¹é½è¿›è¡Œåˆ†æ®µçº§è¯†åˆ«ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä»EfficientNetB0æå–çš„åƒç´ çº§ç‰¹å¾ä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£è¿›è¡Œåˆ†è§£ï¼Œä»¥è·å¾—æ½œåœ¨è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨å±‚æ¬¡èšç±»å¯¹è¿™äº›æ½œåœ¨è¡¨ç¤ºè¿›è¡Œèšç±»ï¼Œä»è€Œåˆ†å‰²å‡ºè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åŒºåŸŸã€‚èšç±»çš„æ•°é‡æ˜¯æ ¹æ®å¥‡å¼‚å€¼çš„åˆ†å¸ƒè‡ªé€‚åº”ç¡®å®šçš„ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨CLIPçš„è§†è§‰è½¬æ¢å™¨ä¸»å¹²å¯¹åˆ†å‰²åŒºåŸŸè¿›è¡Œå®šä½å’Œç¼–ç ï¼Œç”Ÿæˆå›¾åƒåµŒå…¥ã€‚ä½¿ç”¨CLIPçš„æ–‡æœ¬ç¼–ç å™¨æ ¹æ®ç±»åˆ«ç‰¹å®šçš„æç¤ºé¢„å…ˆè®¡ç®—æ–‡æœ¬åµŒå…¥ï¼ŒåŒ…æ‹¬ä¸€ä¸ªé€šç”¨çš„å…¶ä»–æç¤ºä»¥æ”¯æŒå¼€æ”¾é›†è¯†åˆ«ã€‚ç„¶åå°†å›¾åƒå’Œæ–‡æœ¬åµŒå…¥é€šè¿‡SVDè¿æ¥å¹¶æŠ•å½±åˆ°å…±äº«æ½œåœ¨ç‰¹å¾ç©ºé—´ï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚é€šè¿‡è®¡ç®—æŠ•å½±å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼æ€§çš„softmaxæ¥æ‰§è¡Œè¯†åˆ«ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨COCOã€ADE20Kå’ŒPASCAL VOCç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨åŒˆç‰™åˆ©mIoUã€ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢å‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è¿™äº›ç»“æœè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19333v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²ä¸å¯¹è±¡è¯†åˆ«ï¼ˆOVSRï¼‰æ¡†æ¶ï¼Œç»“åˆäº†EfficientNetB0å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œæ— ç›‘ç£åˆ†å‰²å’ŒCLIPè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¼€æ”¾è¯æ±‡å¯¹è±¡è¯†åˆ«ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼šæ— ç›‘ç£å›¾åƒåˆ†å‰²å’Œé€šè¿‡è§†è§‰è¯­è¨€å¯¹é½è¿›è¡Œåˆ†æ®µçº§åˆ«è¯†åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç”¨äºå¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²ä¸å¯¹è±¡è¯†åˆ«ã€‚</li>
<li>åˆ©ç”¨EfficientNetB0è¿›è¡Œæ— ç›‘ç£å›¾åƒåˆ†å‰²ã€‚</li>
<li>ä½¿ç”¨CLIPçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¼€æ”¾è¯æ±‡å¯¹è±¡è¯†åˆ«ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šæ— ç›‘ç£å›¾åƒåˆ†å‰²å’Œåˆ†æ®µçº§åˆ«è¯†åˆ«ã€‚</li>
<li>é€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰è·å¾—æ½œåœ¨è¡¨ç¤ºå¹¶è¿›è¡Œèšç±»ä»¥è¿›è¡Œè¯­ä¹‰æœ‰æ„ä¹‰çš„åŒºåŸŸåˆ†å‰²ã€‚</li>
<li>åˆ©ç”¨CLIPçš„Vision Transformeråœ¨ç¬¬äºŒé˜¶æ®µå¯¹åˆ†å‰²åŒºåŸŸè¿›è¡Œå®šä½å’Œç¼–ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-63acf6a94e7f177a9f5c075c70f0208a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309795&auth_key=1762309795-0-0-1604334b689f4a6a45e8c65a6332ee18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4f841554b8ad2bde1146743dc5f8648c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309802&auth_key=1762309802-0-0-9f4bb336372b5dda274b33181ebb2508&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e774c50c7b23822c2893080471ed9ab8~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309812&auth_key=1762309812-0-0-cc299773800cb8060ef23b928667bdd1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-de476c39616bc55719d6fde38d1bdb03~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309818&auth_key=1762309818-0-0-38dedad6eef33380a780ef7ec275fb55&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-372c03558a7f64b912f4ccddaaa24d49~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309859&auth_key=1762309859-0-0-9e2937ef0bcc0b5d72817cce618521cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5bf550b9ee9a65d07ef1742d55b08756~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309865&auth_key=1762309865-0-0-e6754c012b6983f7151a982268ffb42d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7dfc5c1de470bf7a65330e282d98f42a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309872&auth_key=1762309872-0-0-ae43ee3f74ae2be9e51d7ccd76cb9aa3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Early-Alzheimer-Disease-Detection-through-Big-Data-and-Ensemble-Few-Shot-Learning"><a href="#Enhancing-Early-Alzheimer-Disease-Detection-through-Big-Data-and-Ensemble-Few-Shot-Learning" class="headerlink" title="Enhancing Early Alzheimer Disease Detection through Big Data and   Ensemble Few-Shot Learning"></a>Enhancing Early Alzheimer Disease Detection through Big Data and   Ensemble Few-Shot Learning</h2><p><strong>Authors:Safa Ben Atitallah, Maha Driss, Wadii Boulila, Anis Koubaa</strong></p>
<p>Alzheimer disease is a severe brain disorder that causes harm in various brain areas and leads to memory damage. The limited availability of labeled medical data poses a significant challenge for accurate Alzheimer disease detection. There is a critical need for effective methods to improve the accuracy of Alzheimer disease detection, considering the scarcity of labeled data, the complexity of the disease, and the constraints related to data privacy. To address this challenge, our study leverages the power of big data in the form of pre-trained Convolutional Neural Networks (CNNs) within the framework of Few-Shot Learning (FSL) and ensemble learning. We propose an ensemble approach based on a Prototypical Network (ProtoNet), a powerful method in FSL, integrating various pre-trained CNNs as encoders. This integration enhances the richness of features extracted from medical images. Our approach also includes a combination of class-aware loss and entropy loss to ensure a more precise classification of Alzheimer disease progression levels. The effectiveness of our method was evaluated using two datasets, the Kaggle Alzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and 99.86%, respectively. The comparison of our results with relevant state-of-the-art studies demonstrated that our approach achieved superior accuracy and highlighted its validity and potential for real-world applications in early Alzheimer disease detection. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…æ˜¯ä¸€ç§ä¸¥é‡çš„è„‘ç–¾ç—…ï¼Œä¼šæŸå®³å¤§è„‘çš„å¤šä¸ªåŒºåŸŸå¹¶å¯¼è‡´è®°å¿†åŠ›ä¸‹é™ã€‚æ ‡è®°åŒ»ç–—æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ç»™å‡†ç¡®çš„é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚è€ƒè™‘åˆ°æ ‡è®°æ•°æ®çš„ç¨€ç¼ºæ€§ã€ç–¾ç—…çš„å¤æ‚æ€§ä»¥åŠä¸æ•°æ®éšç§ç›¸å…³çš„é™åˆ¶ï¼Œè¿«åˆ‡éœ€è¦æœ‰æœ‰æ•ˆçš„æ–¹æ³•æ¥æé«˜é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶åˆ©ç”¨äº†åœ¨å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰å’Œé›†æˆå­¦ä¹ æ¡†æ¶ä¸‹ä»¥é¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å½¢å¼çš„å¤§æ•°æ®çš„åŠ›é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŸå‹ç½‘ç»œï¼ˆProtoNetï¼‰çš„é›†æˆæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„FSLæ–¹æ³•ï¼Œå°†å„ç§é¢„è®­ç»ƒçš„CNNä½œä¸ºç¼–ç å™¨è¿›è¡Œé›†æˆã€‚è¿™ç§é›†æˆæé«˜äº†ä»åŒ»å­¦å›¾åƒä¸­æå–çš„ç‰¹å¾çš„ä¸°å¯Œæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜åŒ…æ‹¬ç»“åˆç±»åˆ«æ„ŸçŸ¥æŸå¤±å’Œç†µæŸå¤±ï¼Œä»¥ç¡®ä¿æ›´ç²¾ç¡®åœ°åˆ†ç±»é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¿›å±•æ°´å¹³ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨Kaggleé˜¿å°”èŒ¨æµ·é»˜ç—‡æ•°æ®é›†å’ŒADNIæ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œå‡†ç¡®ç‡åˆ†åˆ«ä¸º99.72%å’Œ99.86%ã€‚å°†æˆ‘ä»¬çš„ç»“æœä¸æœ€æ–°çš„ç›¸å…³ç ”ç©¶è¿›è¡Œæ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨æ—©æœŸé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€å®ç”¨æ€§å’Œæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19282v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨å¤§æ•°æ®å’Œé¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥è§£å†³é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹çš„æŒ‘æˆ˜ã€‚ç ”ç©¶é‡‡ç”¨å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰å’Œé›†æˆå­¦ä¹ çš„æ–¹æ³•ï¼ŒåŸºäºåŸå‹ç½‘ç»œï¼ˆProtoNetï¼‰æå‡ºä¸€ç§é›†æˆç­–ç•¥ï¼Œæ•´åˆå¤šç§é¢„è®­ç»ƒCNNä½œä¸ºç¼–ç å™¨ï¼Œæé«˜åŒ»å­¦å›¾åƒç‰¹å¾çš„ä¸°å¯Œæ€§ã€‚è¯¥æ–¹æ³•ç»“åˆç±»åˆ«æ„ŸçŸ¥æŸå¤±å’Œç†µæŸå¤±ï¼Œç¡®ä¿æ›´ç²¾ç¡®åœ°åˆ†ç±»é˜¿å°”èŒ¨æµ·é»˜ç—…è¿›å±•æ°´å¹³ã€‚åœ¨Kaggleé˜¿å°”èŒ¨æµ·é»˜æ•°æ®é›†å’ŒADNIæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å‡†ç¡®ç‡åˆ†åˆ«é«˜è¾¾99.72%å’Œ99.86%ï¼Œä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ½œåŠ›ï¼Œå¯åº”ç”¨äºæ—©æœŸé˜¿å°”èŒ¨æµ·é»˜ç—…çš„å®é™…æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿å°”èŒ¨æµ·é»˜ç—…æ˜¯ä¸€ç§ä¸¥é‡çš„è„‘ç–¾ç—…ï¼Œå¯¹å¤§è„‘å„åŒºåŸŸé€ æˆæŸå®³ï¼Œå¯¼è‡´è®°å¿†åŠ›ä¸‹é™ã€‚</li>
<li>åŒ»å­¦æ•°æ®æ ‡æ³¨çš„æœ‰é™æ€§å¯¹å‡†ç¡®çš„é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨å¤§æ•°æ®å’Œé¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰å’Œé›†æˆå­¦ä¹ çš„æ–¹æ³•ï¼ŒåŸºäºåŸå‹ç½‘ç»œï¼ˆProtoNetï¼‰æå‡ºä¸€ç§é›†æˆç­–ç•¥ã€‚</li>
<li>æ•´åˆå¤šç§é¢„è®­ç»ƒCNNä»¥æé«˜åŒ»å­¦å›¾åƒç‰¹å¾çš„ä¸°å¯Œæ€§ã€‚</li>
<li>ç»“åˆç±»åˆ«æ„ŸçŸ¥æŸå¤±å’Œç†µæŸå¤±ï¼Œä»¥æé«˜é˜¿å°”èŒ¨æµ·é»˜ç—…è¿›å±•æ°´å¹³çš„åˆ†ç±»ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a3d7356b6449c6626e86e062eced2573~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309880&auth_key=1762309880-0-0-9d54df1b13f3d2032d53125db553d6fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-88705e4cf089f8f241d8be351f44ccd2~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309887&auth_key=1762309887-0-0-3f36618a21b1c9ce1deac29c82e79072&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0952e3309f7bd80b1e8070309f3be933~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309894&auth_key=1762309894-0-0-c363a579c0e91431aabcf7b24a9baba9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Multi-Evidence-Framework-Rescues-Low-Power-Prognostic-Signals-and-Rejects-Statistical-Artifacts-in-Cancer-Genomics"><a href="#A-Multi-Evidence-Framework-Rescues-Low-Power-Prognostic-Signals-and-Rejects-Statistical-Artifacts-in-Cancer-Genomics" class="headerlink" title="A Multi-Evidence Framework Rescues Low-Power Prognostic Signals and   Rejects Statistical Artifacts in Cancer Genomics"></a>A Multi-Evidence Framework Rescues Low-Power Prognostic Signals and   Rejects Statistical Artifacts in Cancer Genomics</h2><p><strong>Authors:Gokturk Aytug Akarlar</strong></p>
<p>Motivation: Standard genome-wide association studies in cancer genomics rely on statistical significance with multiple testing correction, but systematically fail in underpowered cohorts. In TCGA breast cancer (n&#x3D;967, 133 deaths), low event rates (13.8%) create severe power limitations, producing false negatives for known drivers and false positives for large passenger genes. Results: We developed a five-criteria computational framework integrating causal inference (inverse probability weighting, doubly robust estimation) with orthogonal biological validation (expression, mutation patterns, literature evidence). Applied to TCGA-BRCA mortality analysis, standard Cox+FDR detected zero genes at FDR&lt;0.05, confirming complete failure in underpowered settings. Our framework correctly identified RYR2 â€“ a cardiac gene with no cancer function â€“ as a false positive despite nominal significance (p&#x3D;0.024), while identifying KMT2C as a complex candidate requiring validation despite marginal significance (p&#x3D;0.047, q&#x3D;0.954). Power analysis revealed median power of 15.1% across genes, with KMT2C achieving only 29.8% power (HR&#x3D;1.55), explaining borderline statistical significance despite strong biological evidence. The framework distinguished true signals from artifacts through mutation pattern analysis: RYR2 showed 29.8% silent mutations (passenger signature) with no hotspots, while KMT2C showed 6.7% silent mutations with 31.4% truncating variants (driver signature). This multi-evidence approach provides a template for analyzing underpowered cohorts, prioritizing biological interpretability over purely statistical significance.   Availability: All code and analysis pipelines available at github.com&#x2F;akarlaraytu&#x2F;causal-inference-for-cancer-genomics </p>
<blockquote>
<p>åŠ¨æœºï¼šæ ‡å‡†çš„å…¨åŸºå› ç»„å…³è”ç ”ç©¶åœ¨ç™Œç—‡åŸºå› ç»„å­¦ä¸­ä¾èµ–äºå¤šé‡æ£€éªŒæ ¡æ­£çš„ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œä½†åœ¨åŠŸæ•ˆä¸è¶³çš„é˜Ÿåˆ—ä¸­ç³»ç»Ÿæ€§åœ°å¤±è´¥ã€‚åœ¨TCGAä¹³è…ºç™Œï¼ˆn&#x3D;967ï¼Œ133ä¾‹æ­»äº¡ï¼‰ä¸­ï¼Œä½äº‹ä»¶ç‡ï¼ˆ13.8%ï¼‰é€ æˆäº†ä¸¥é‡çš„åŠŸæ•ˆé™åˆ¶ï¼Œå¯¹äºå·²çŸ¥é©±åŠ¨å› ç´ å’Œå¤§å‹ä¹˜å®¢åŸºå› äº§ç”Ÿäº†å‡é˜´æ€§å’Œå‡é˜³æ€§ç»“æœã€‚ç»“æœï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªäº”æ ‡å‡†è®¡ç®—æ¡†æ¶ï¼Œèåˆäº†å› æœæ¨ç†ï¼ˆé€†æ¦‚ç‡åŠ æƒï¼ŒåŒé‡ç¨³å¥ä¼°è®¡ï¼‰ä¸æ­£äº¤ç”Ÿç‰©éªŒè¯ï¼ˆè¡¨è¾¾ï¼Œçªå˜æ¨¡å¼ï¼Œæ–‡çŒ®è¯æ®ï¼‰ã€‚åº”ç”¨äºTCGA-BRCAæ­»äº¡åˆ†æï¼Œæ ‡å‡†çš„Cox+FDRæ–¹æ³•åœ¨FDR&lt;0.05æ—¶æœªæ£€æµ‹åˆ°åŸºå› ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ­£ç¡®åœ°è¯†åˆ«äº†RYR2â€”â€”ä¸€ä¸ªå…·æœ‰æ— ç™Œç—‡åŠŸèƒ½çš„å¿ƒè„åŸºå› æ˜¯ä¸€ä¸ªå‡é˜³æ€§ç»“æœï¼Œå°½ç®¡å…¶åä¹‰ä¸Šå…·æœ‰é‡è¦æ„ä¹‰ï¼ˆp&#x3D;0.024ï¼‰ï¼ŒåŒæ—¶è¯†åˆ«äº†KMT2Cä½œä¸ºä¸€ä¸ªå¤æ‚çš„å€™é€‰åŸºå› éœ€è¦éªŒè¯ï¼Œå°½ç®¡å…¶è¾¹ç¼˜æ˜¾è‘—ï¼ˆp&#x3D;0.047ï¼Œq&#x3D;0.954ï¼‰ã€‚åŠŸæ•ˆåˆ†ææ˜¾ç¤ºåŸºå› çš„ä¸­ä½åŠŸæ•ˆä¸º15.1%ï¼Œå…¶ä¸­KMT2Cä»…è¾¾åˆ°29.8%çš„åŠŸæ•ˆï¼ˆé£é™©æ¯”&#x3D;1.55ï¼‰ï¼Œè§£é‡Šäº†å°½ç®¡æœ‰å¼ºæœ‰åŠ›çš„ç”Ÿç‰©å­¦è¯æ®ä½†ç»Ÿè®¡ç»“æœå´å¤„äºä¸´ç•ŒçŠ¶æ€çš„åŸå› ã€‚è¯¥æ¡†æ¶é€šè¿‡çªå˜æ¨¡å¼åˆ†æåŒºåˆ†äº†çœŸä¿¡å·å’Œäººå·¥åˆ¶å“ï¼šRYR2æ˜¾ç¤ºå‡º29.8%çš„æ— ä¹‰çªå˜ï¼ˆä¹˜å®¢ç­¾åï¼‰ä¸”æ— çƒ­ç‚¹ï¼Œè€ŒKMT2Cæ˜¾ç¤ºå‡º6.7%çš„æ— ä¹‰çªå˜å’Œ31.4%çš„æˆªæ–­å˜å¼‚ï¼ˆé©±åŠ¨ç­¾åï¼‰ã€‚è¿™ç§å¤šè¯æ®æ–¹æ³•æä¾›äº†åˆ†æåŠŸæ•ˆä¸è¶³é˜Ÿåˆ—çš„æ¨¡æ¿ï¼Œä¼˜å…ˆè€ƒè™‘ç”Ÿç‰©å¯è§£é‡Šæ€§è€Œéçº¯ç²¹çš„ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚å¯ç”¨æ€§ï¼šæ‰€æœ‰ä»£ç å’Œåˆ†æç®¡é“å¯åœ¨github.com&#x2F;akarlaraytu&#x2F;causal-inference-for-cancer-genomicsæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18571v1">PDF</a> 17 pages (main text), 4 figures (main text), 7 supplementary figures,   4 supplementary tables. Focuses on a computational framework using causal   inference and biological validation for underpowered cancer genomic studies</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç™Œç—‡åŸºå› ç»„ä¸­çš„æ ‡å‡†å…¨åŸºå› ç»„å…³è”ç ”ç©¶ï¼Œå› ä½äº‹ä»¶ç‡å¯¼è‡´çš„æ•ˆèƒ½ä¸è¶³é—®é¢˜ï¼Œæå‡ºä¸€ç§ç»“åˆå› æœæ¨æ–­ä¸æ­£äº¤ç”Ÿç‰©éªŒè¯çš„è®¡ç®—æ¡†æ¶ã€‚åº”ç”¨äºTCGAä¹³è…ºç™Œæ­»äº¡ç‡åˆ†æï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å‡ºæ ‡å‡†Cox+FDRæ–¹æ³•æ— æ³•æ£€æµ‹åˆ°çš„åŸºå› ï¼Œå¹¶é€šè¿‡å¤šé‡è¯æ®åŒºåˆ†çœŸå®ä¿¡å·ä¸ä¼ªå½±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ ‡å‡†å…¨åŸºå› ç»„å…³è”ç ”ç©¶åœ¨ç™Œç—‡åŸºå› ç»„ä¸­ä¾èµ–ç»Ÿè®¡æ˜¾è‘—æ€§è¿›è¡Œå¤šé‡æµ‹è¯•æ ¡æ­£ï¼Œä½†åœ¨æ•ˆèƒ½ä¸è¶³çš„ç¾¤ä½“ä¸­ç³»ç»Ÿæ€§åœ°å¤±è´¥ã€‚</li>
<li>åœ¨TCGAä¹³è…ºç™Œç ”ç©¶ä¸­ï¼Œä½äº‹ä»¶ç‡å¯¼è‡´ä¸¥é‡çš„æ•ˆèƒ½é™åˆ¶ï¼Œäº§ç”Ÿå·²çŸ¥é©±åŠ¨å› ç´ çš„å‡é˜´æ€§å’Œå¤§å‹ä¹˜å®¢åŸºå› çš„å‡é˜³æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§äº”æ ‡å‡†è®¡ç®—æ¡†æ¶ï¼Œç»“åˆå› æœæ¨æ–­ï¼ˆé€†å‘æ¦‚ç‡åŠ æƒï¼ŒåŒé‡ç¨³å¥ä¼°è®¡ï¼‰å’Œæ­£äº¤ç”Ÿç‰©éªŒè¯ï¼ˆè¡¨è¾¾ï¼Œçªå˜æ¨¡å¼ï¼Œæ–‡çŒ®è¯æ®ï¼‰ã€‚</li>
<li>åº”ç”¨äºTCGA-BRCAæ­»äº¡ç‡åˆ†æï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å‡ºæ ‡å‡†æ–¹æ³•æœªèƒ½æ£€æµ‹åˆ°çš„åŸºå› ï¼Œå¹¶æ­£ç¡®åŒºåˆ†çœŸå®ä¿¡å·å’Œä¼ªå½±ã€‚</li>
<li>é€šè¿‡çªå˜æ¨¡å¼åˆ†æï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè¿›ä¸€æ­¥éªŒè¯åŸºå› çš„çœŸå®æ€§å’ŒåŠŸèƒ½ç±»å‹ã€‚ä¾‹å¦‚ï¼ŒRYR2æ˜¾ç¤ºå‡ºä¹˜å®¢åŸºå› ç‰¹å¾ï¼Œè€ŒKMT2Cåˆ™æ˜¾ç¤ºå‡ºé©±åŠ¨åŸºå› ç‰¹å¾ã€‚</li>
<li>è¯¥æ¡†æ¶æä¾›äº†ä¸€ç§åˆ†ææ•ˆèƒ½ä¸è¶³ç¾¤ä½“çš„æ¨¡æ¿ï¼Œä¼˜å…ˆè€ƒè™‘ç”Ÿç‰©å¯è§£é‡Šæ€§è€Œéçº¯ç²¹çš„ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d54ffdb93472d4e22eff29459742fe7f~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309902&auth_key=1762309902-0-0-197fa2dcc091def8832ee5de2612bff6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-025f388a8052bb6b4fd0f12d6675fe2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309909&auth_key=1762309909-0-0-42a28a42808d4eb2e4c689e285c70004&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Rethinking-Hebbian-Principle-Low-Dimensional-Structural-Projection-for-Unsupervised-Learning"><a href="#Rethinking-Hebbian-Principle-Low-Dimensional-Structural-Projection-for-Unsupervised-Learning" class="headerlink" title="Rethinking Hebbian Principle: Low-Dimensional Structural Projection for   Unsupervised Learning"></a>Rethinking Hebbian Principle: Low-Dimensional Structural Projection for   Unsupervised Learning</h2><p><strong>Authors:Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu</strong></p>
<p>Hebbian learning is a biological principle that intuitively describes how neurons adapt their connections through repeated stimuli. However, when applied to machine learning, it suffers serious issues due to the unconstrained updates of the connections and the lack of accounting for feedback mediation. Such shortcomings limit its effective scaling to complex network architectures and tasks. To this end, here we introduce the Structural Projection Hebbian Representation (SPHeRe), a novel unsupervised learning method that integrates orthogonality and structural information preservation through a local auxiliary nonlinear block. The loss for structural information preservation backpropagates to the input through an auxiliary lightweight projection that conceptually serves as feedback mediation while the orthogonality constraints account for the boundedness of updating magnitude. Extensive experimental results show that SPHeRe achieves SOTA performance among unsupervised synaptic plasticity approaches on standard image classification benchmarks, including CIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong effectiveness in continual learning and transfer learning scenarios, and image reconstruction tasks show the robustness and generalizability of the extracted features. This work demonstrates the competitiveness and potential of Hebbian unsupervised learning rules within modern deep learning frameworks, demonstrating the possibility of efficient and biologically inspired learning algorithms without the strong dependence on strict backpropagation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/SPHeRe">https://github.com/brain-intelligence-lab/SPHeRe</a>. </p>
<blockquote>
<p>èµ«å¸ƒå­¦ä¹ æ˜¯ä¸€ç§ç”Ÿç‰©åŸç†ï¼Œç›´è§‚åœ°æè¿°äº†ç¥ç»å…ƒå¦‚ä½•é€šè¿‡é‡å¤åˆºæ¿€è°ƒæ•´å…¶è¿æ¥ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºæœºå™¨å­¦ä¹ æ—¶ï¼Œå®ƒç”±äºè¿æ¥çš„æ— é™æ›´æ–°å’Œç¼ºä¹åé¦ˆè°ƒèŠ‚è€Œé¢ä¸´ä¸¥é‡é—®é¢˜ã€‚è¿™äº›ç¼ºç‚¹é™åˆ¶äº†å…¶åœ¨å¤æ‚ç½‘ç»œç»“æ„å’Œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ‰©å±•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¼•å…¥äº†ç»“æ„æŠ•å½±èµ«å¸ƒè¡¨ç¤ºï¼ˆSPHeReï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å±€éƒ¨è¾…åŠ©éçº¿æ€§å—æ•´åˆæ­£äº¤æ€§å’Œç»“æ„ä¿¡æ¯ä¿ç•™ã€‚ç»“æ„ä¿¡æ¯ä¿ç•™çš„æŸå¤±é€šè¿‡è¾…åŠ©çš„è½»é‡çº§æŠ•å½±åå‘ä¼ æ’­åˆ°è¾“å…¥ç«¯ï¼Œè¿™åœ¨æ¦‚å¿µä¸Šèµ·åˆ°äº†åé¦ˆè°ƒèŠ‚çš„ä½œç”¨ï¼Œè€Œæ­£äº¤æ€§çº¦æŸåˆ™è€ƒè™‘äº†æ›´æ–°å¹…åº¦çš„æœ‰ç•Œæ€§ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSPHeReåœ¨åŒ…æ‹¬CIFAR-10ã€CIFAR-100å’ŒTiny-ImageNetåœ¨å†…çš„æ ‡å‡†å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨æ— ç›‘ç£çªè§¦å¯å¡‘æ€§æ–¹æ³•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æŒç»­å­¦ä¹ å’Œè¿ç§»å­¦ä¹ åœºæ™¯ä¸­å…·æœ‰å¾ˆå¼ºçš„æœ‰æ•ˆæ€§ï¼Œå›¾åƒé‡å»ºä»»åŠ¡æ˜¾ç¤ºäº†æ‰€æå–ç‰¹å¾çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†èµ«å¸ƒæ— ç›‘ç£å­¦ä¹ è§„åˆ™åœ¨ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„ç«äº‰åŠ›å’Œæ½œåŠ›ï¼Œå±•ç¤ºäº†åœ¨æ²¡æœ‰ä¸¥æ ¼ä¾èµ–åå‘ä¼ æ’­çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆä¸”å—ç”Ÿç‰©å¯å‘çš„å­¦ä¹ ç®—æ³•çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/SPHeRe%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/brain-intelligence-lab/SPHeReæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14810v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§æ–°å‹æ— ç›‘ç£å­¦ä¹ æ–¹æ³•â€”â€”ç»“æ„æŠ•å½±èµ«å¸ƒè¡¨ç¤ºï¼ˆSPHeReï¼‰ï¼Œè¯¥æ–¹æ³•ç»“åˆæ­£äº¤æ€§å’Œç»“æ„ä¿¡æ¯ä¿ç•™ï¼Œé€šè¿‡å±€éƒ¨è¾…åŠ©éçº¿æ€§å—å®ç°ã€‚è¯¥æ–¹æ³•è§£å†³äº†èµ«å¸ƒå­¦ä¹ åœ¨æœºå™¨å­¦ä¹ åº”ç”¨ä¸­çš„ä¸è¶³ï¼Œå…·æœ‰åé¦ˆä¸­ä»‹å’Œæ›´æ–°å¹…åº¦æœ‰ç•Œæ€§ã€‚åœ¨å›¾åƒåˆ†ç±»ã€æŒç»­å­¦ä¹ å’Œè¿ç§»å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èµ«å¸ƒå­¦ä¹ æ˜¯æè¿°ç¥ç»å…ƒå¦‚ä½•é€šè¿‡é‡å¤åˆºæ¿€é€‚åº”è¿æ¥çš„ç”Ÿç‰©åŸç†ï¼Œä½†åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>SPHeReæ˜¯ä¸€ç§æ–°å‹æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè§£å†³äº†èµ«å¸ƒå­¦ä¹ åœ¨å¤æ‚ç½‘ç»œæ¶æ„å’Œä»»åŠ¡ä¸­çš„ç¼©æ”¾é—®é¢˜ã€‚</li>
<li>SPHeReé€šè¿‡ç»“åˆæ­£äº¤æ€§å’Œç»“æ„ä¿¡æ¯ä¿ç•™æ¥å®ç°å­¦ä¹ ï¼Œåˆ©ç”¨å±€éƒ¨è¾…åŠ©éçº¿æ€§å—è¿›è¡Œåé¦ˆä¸­ä»‹å’Œæ›´æ–°å¹…åº¦æœ‰ç•Œæ€§ç®¡ç†ã€‚</li>
<li>SPHeReåœ¨æ ‡å‡†å›¾åƒåˆ†ç±»åŸºå‡†æµ‹è¯•ï¼ˆå¦‚CIFAR-10ã€CIFAR-100å’ŒTiny-ImageNetï¼‰ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>SPHeReåœ¨æŒç»­å­¦ä¹ å’Œè¿ç§»å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ•ˆæœã€‚</li>
<li>å›¾åƒé‡å»ºä»»åŠ¡è¯æ˜äº†SPHeReæå–ç‰¹å¾çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9cb73652affc99b63f316d9129369e24~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309917&auth_key=1762309917-0-0-d17e71bb41c678400afc284226e8fb99&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Epistemic-aware-Vision-Language-Foundation-Model-for-Fetal-Ultrasound-Interpretation"><a href="#Epistemic-aware-Vision-Language-Foundation-Model-for-Fetal-Ultrasound-Interpretation" class="headerlink" title="Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound   Interpretation"></a>Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound   Interpretation</h2><p><strong>Authors:Xiao He, Huangxuan Zhao, Guojia Wan, Wei Zhou, Yanxing Liu, Juhua Liu, Yongchao Xu, Yong Luo, Dacheng Tao, Bo Du</strong></p>
<p>Recent medical vision-language models have shown promise on tasks such as VQA, report generation, and anomaly detection. However, most are adapted to structured adult imaging and underperform in fetal ultrasound, which poses challenges of multi-view image reasoning, numerous diseases, and image diversity. To bridge this gap, we introduce FetalMind, a medical AI system tailored to fetal ultrasound for both report generation and diagnosis. Guided by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which injects an expert-curated bipartite graph into the model to decouple view-disease associations and to steer preference selection along clinically faithful steps via reinforcement learning. This design mitigates variability across diseases and heterogeneity across views, reducing learning bottlenecks while aligning the modelâ€™s inference with obstetric practice. To train FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale fetal ultrasound report corpus, comprising 20K reports from twelve medical centers, addressing the scarcity of domain data. Extensive experiments show that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving +14% average gains and +61.2% higher accuracy on critical conditions while remaining efficient, stable, and scalable. Project Page: <a target="_blank" rel="noopener" href="https://hexiao0275.github.io/FetalMind">https://hexiao0275.github.io/FetalMind</a>. </p>
<blockquote>
<p>è¿‘æœŸåŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯¸å¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€æŠ¥å‘Šç”Ÿæˆå’Œå¼‚å¸¸æ£€æµ‹ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹éƒ½é€‚åº”äºç»“æ„åŒ–æˆäººå½±åƒï¼Œè€Œåœ¨èƒå„¿è¶…å£°æ£€æŸ¥ä¸­è¡¨ç°ä¸ä½³ï¼Œè¿™å¸¦æ¥äº†å¤šè§†è§’å›¾åƒæ¨ç†ã€å¤šç§ç–¾ç—…å’Œå›¾åƒå¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸“ä¸ºèƒå„¿è¶…å£°æ£€æŸ¥è®¾è®¡çš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»ŸFetalMindï¼Œç”¨äºæŠ¥å‘Šç”Ÿæˆå’Œè¯Šæ–­ã€‚åœ¨ä¸´åºŠå·¥ä½œæµç¨‹çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†æ˜¾è‘—è®¤çŸ¥åˆ†è§£ï¼ˆSEDï¼‰ï¼Œå®ƒå°†ä¸“å®¶å®šåˆ¶çš„äºŒéƒ¨å›¾æ³¨å…¥æ¨¡å‹ï¼Œä»¥è§£è€¦è§†å›¾-ç–¾ç—…å…³è”ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼åå¥½é€‰æ‹©æ²¿ç€ä¸´åºŠå¿ å®æ­¥éª¤è¿›è¡Œã€‚è¿™ç§è®¾è®¡å‡è½»äº†ç–¾ç—…é—´çš„å¯å˜æ€§å’Œè§†å›¾é—´çš„å¼‚è´¨æ€§ï¼Œå‡å°‘äº†å­¦ä¹ ç“¶é¢ˆï¼Œä½¿æ¨¡å‹çš„æ¨æ–­ä¸äº§ç§‘å®è·µç›¸ç¬¦ã€‚ä¸ºäº†å¤§è§„æ¨¡è®­ç»ƒFetalMindï¼Œæˆ‘ä»¬æ•´ç†äº†FetalSigma-1Mæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡èƒå„¿è¶…å£°æŠ¥å‘Šè¯­æ–™åº“ï¼ŒåŒ…å«æ¥è‡ªåäºŒä¸ªåŒ»ç–—ä¸­å¿ƒçš„2ä¸‡ä»½æŠ¥å‘Šï¼Œè§£å†³äº†é¢†åŸŸæ•°æ®çš„ç¨€ç¼ºæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFetalMindåœ¨æ‰€æœ‰å¦Šå¨ é˜¶æ®µçš„è¡¨ç°å‡ä¼˜äºå¼€æºå’Œé—­æºçš„åŸºçº¿ï¼Œåœ¨å…³é”®æ¡ä»¶ä¸‹å¹³å‡æé«˜äº†+14%çš„å¢ç›Šå’Œ+61.2%çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆã€ç¨³å®šå’Œå¯æ‰©å±•æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://hexiao0275.github.io/FetalMind%E3%80%82">https://hexiao0275.github.io/FetalMindã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12953v2">PDF</a> This paper contains fundamental errors and will not be replaced</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹èƒå„¿è¶…å£°åŒ»å­¦å½±åƒçš„äººå·¥æ™ºèƒ½ç³»ç»ŸFetalMindçš„è®¾è®¡å’Œåº”ç”¨ã€‚ç³»ç»Ÿé€šè¿‡å¼•å…¥ä¸´åºŠå·¥ä½œæµç¨‹æŒ‡å¯¼çš„æ˜¾è‘—è®¤çŸ¥åˆ†è§£ï¼ˆSEDï¼‰æŠ€æœ¯å’Œå¤§è§„æ¨¡èƒå„¿è¶…å£°æŠ¥å‘Šæ•°æ®é›†FetalSigma-1Mï¼Œå®ç°äº†æŠ¥å‘Šç”Ÿæˆå’Œè¯Šæ–­åŠŸèƒ½ï¼Œå¹¶æé«˜äº†å¯¹ä¸åŒç–¾ç—…å’Œè§†å›¾å˜åŒ–çš„é€‚åº”æ€§ï¼Œé™ä½äº†å­¦ä¹ ç“¶é¢ˆï¼Œæ¨¡å‹æ¨ç†ç¬¦åˆäº§ç§‘å®è·µã€‚å®éªŒè¡¨æ˜ï¼ŒFetalMindåœ¨æ‰€æœ‰å¦Šå¨ é˜¶æ®µçš„è¡¨ç°å‡ä¼˜äºå¼€æ”¾å’Œå°é—­åŸºçº¿ï¼Œå¯¹å…³é”®ç–¾ç—…çš„è¯Šæ–­å‡†ç¡®ç‡æé«˜61.2%ï¼Œå…·æœ‰é«˜æ•ˆã€ç¨³å®šå’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FetalMindæ˜¯ä¸€ä¸ªé’ˆå¯¹èƒå„¿è¶…å£°åŒ»å­¦å½±åƒçš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œç”¨äºæŠ¥å‘Šç”Ÿæˆå’Œè¯Šæ–­ã€‚</li>
<li>ç³»ç»Ÿé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯èƒå„¿è¶…å£°å›¾åƒçš„å¤šè§†è§’æ€§ã€ç–¾ç—…å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚</li>
<li>æå‡ºäº†æ˜¾è‘—è®¤çŸ¥åˆ†è§£ï¼ˆSEDï¼‰æŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥ä¸“å®¶æ„å»ºçš„åŒå‘å›¾å’Œå¼ºåŒ–å­¦ä¹ æ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ ï¼Œè§£å†³äº†è§†å›¾ä¸ç–¾ç—…çš„å…³è”é—®é¢˜ã€‚</li>
<li>FetalMindé‡‡ç”¨ä¸´åºŠå·¥ä½œæµç¨‹æŒ‡å¯¼çš„è®¾è®¡ï¼Œä»¥å‡å°‘ç–¾ç—…é—´çš„å·®å¼‚å’Œè§†å›¾å¤šæ ·æ€§å¯¹æ¨¡å‹æ¨ç†çš„å½±å“ã€‚</li>
<li>è®­ç»ƒFetalMindçš„æ•°æ®é›†æ˜¯é¦–ä¸ªå¤§è§„æ¨¡èƒå„¿è¶…å£°æŠ¥å‘Šè¯­æ–™åº“FetalSigma-1Mï¼ŒåŒ…å«æ¥è‡ªåäºŒå®¶åŒ»ç–—ä¸­å¿ƒçš„2ä¸‡ä»½æŠ¥å‘Šã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFetalMindåœ¨æ‰€æœ‰å¦Šå¨ é˜¶æ®µçš„è¡¨ç°å‡ä¼˜äºç°æœ‰ç³»ç»Ÿï¼Œå¯¹å…³é”®ç–¾ç—…çš„è¯Šæ–­å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ce3d06968bd51fd1ba40811087d92fc0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309924&auth_key=1762309924-0-0-9400542ab3ef77fc177e746285463508&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-339fa56abbc72b7fb52d5dcef25dfca6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309931&auth_key=1762309931-0-0-42736302f0738b7f56f4884d0c7dbf80&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-365264306c7d58cda1a8d64d3c452c1c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309938&auth_key=1762309938-0-0-3af91b720fdd082baf2843a8e5eaf1fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89ab62ea3ecc9d7e6616cbd49975dd00~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309945&auth_key=1762309945-0-0-5a4ae6ea3c687cb986c9872c813c22bf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Structured-Spectral-Graph-Representation-Learning-for-Multi-label-Abnormality-Analysis-from-3D-CT-Scans"><a href="#Structured-Spectral-Graph-Representation-Learning-for-Multi-label-Abnormality-Analysis-from-3D-CT-Scans" class="headerlink" title="Structured Spectral Graph Representation Learning for Multi-label   Abnormality Analysis from 3D CT Scans"></a>Structured Spectral Graph Representation Learning for Multi-label   Abnormality Analysis from 3D CT Scans</h2><p><strong>Authors:Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel</strong></p>
<p>With the growing volume of CT examinations, there is an increasing demand for automated tools such as organ segmentation, abnormality detection, and report generation to support radiologists in managing their clinical workload. Multi-label classification of 3D Chest CT scans remains a critical yet challenging problem due to the complex spatial relationships inherent in volumetric data and the wide variability of abnormalities. Existing methods based on 3D convolutional neural networks struggle to capture long-range dependencies, while Vision Transformers often require extensive pre-training on large-scale, domain-specific datasets to perform competitively. In this work of academic research, we propose a 2.5D alternative by introducing a new graph-based framework that represents 3D CT volumes as structured graphs, where axial slice triplets serve as nodes processed through spectral graph convolution, enabling the model to reason over inter-slice dependencies while maintaining complexity compatible with clinical deployment. Our method, trained and evaluated on 3 datasets from independent institutions, achieves strong cross-dataset generalization, and shows competitive performance compared to state-of-the-art visual encoders. We further conduct comprehensive ablation studies to evaluate the impact of various aggregation strategies, edge-weighting schemes, and graph connectivity patterns. Additionally, we demonstrate the broader applicability of our approach through transfer experiments on automated radiology report generation and abdominal CT data. </p>
<blockquote>
<p>éšç€CTæ£€æŸ¥é‡çš„å¢é•¿ï¼Œå¯¹è‡ªåŠ¨åŒ–å·¥å…·çš„éœ€æ±‚ä¹Ÿåœ¨å¢åŠ ï¼Œå¦‚å™¨å®˜åˆ†å‰²ã€å¼‚å¸¸æ£€æµ‹ã€æŠ¥å‘Šç”Ÿæˆç­‰ï¼Œä»¥æ”¯æŒæ”¾å°„ç§‘åŒ»ç”Ÿç®¡ç†ä»–ä»¬çš„å·¥ä½œè´Ÿæ‹…ã€‚å¯¹3Dèƒ¸éƒ¨CTæ‰«æçš„å¤šæ ‡ç­¾åˆ†ç±»ä»ç„¶æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦ä¸”å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºä½“ç§¯æ•°æ®ä¸­çš„å¤æ‚ç©ºé—´å…³ç³»å’Œå¼‚å¸¸æƒ…å†µçš„å¹¿æ³›å˜åŒ–ã€‚åŸºäºç°æœ‰æ–¹æ³•çš„ä¾èµ–å…³ç³»å¾ˆéš¾æ•è·åˆ°ä¸‰ç»´å·ç§¯ç¥ç»ç½‘ç»œçš„é•¿è·ç¦»å…³ç³»ï¼Œè€Œè§†è§‰Transformerå¾€å¾€éœ€è¦åœ¨å¤§è§„æ¨¡ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›çš„é¢„è®­ç»ƒæ‰èƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å­¦æœ¯ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡å¼•å…¥æ–°çš„åŸºäºå›¾çš„æ¡†æ¶æ¥è¡¨ç¤ºä¸‰ç»´CTä½“ç§¯ä½œä¸ºç»“æ„åŒ–å›¾ï¼Œå…¶ä¸­è½´å‘åˆ‡ç‰‡ä¸‰å…ƒç»„ä½œä¸ºèŠ‚ç‚¹é€šè¿‡è°±å›¾å·ç§¯è¿›è¡Œå¤„ç†ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ¨ç†åˆ‡ç‰‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼ŒåŒæ—¶ä¿æŒä¸ä¸´åºŠéƒ¨ç½²å…¼å®¹çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»è¿‡ç‹¬ç«‹æœºæ„çš„ä¸‰ä¸ªæ•°æ®é›†çš„åŸ¹è®­å’Œè¯„ä¼°ï¼Œå®ç°äº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œä¸æœ€å…ˆè¿›çš„è§†è§‰ç¼–ç å™¨ç›¸æ¯”æ˜¾ç¤ºå‡ºæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œä»¥è¯„ä¼°å„ç§èšåˆç­–ç•¥ã€è¾¹ç¼˜åŠ æƒæ–¹æ¡ˆå’Œå›¾å½¢è¿æ¥æ¨¡å¼çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šè½¬ç§»å®éªŒå’Œè…¹éƒ¨CTæ•°æ®æ¥å±•ç¤ºæˆ‘ä»¬æ–¹æ³•çš„æ›´å¹¿æ³›åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10779v2">PDF</a> 24 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›¾çš„æ–°æ¡†æ¶ï¼Œç”¨äºå¤„ç†ä¸‰ç»´CTä½“ç§¯æ•°æ®ï¼Œå°†å…¶è¡¨ç¤ºä¸ºç»“æ„åŒ–å›¾ï¼Œé€šè¿‡è°±å›¾å·ç§¯å¤„ç†è½´å‘åˆ‡ç‰‡ä¸‰é‡ä½œä¸ºèŠ‚ç‚¹ï¼Œèƒ½å¤Ÿåœ¨è·¨åˆ‡ç‰‡ä¹‹é—´å»ºç«‹ä¾èµ–å…³ç³»çš„åŒæ—¶ä¿æŒä¸ä¸´åºŠéƒ¨ç½²çš„å…¼å®¹æ€§ã€‚è¯¥ç ”ç©¶å®ç°äº†å¯¹å¤šæ ‡ç­¾ä¸‰ç»´èƒ¸éƒ¨CTæ‰«æçš„åˆ†ç±»ï¼Œå¹¶åœ¨ç‹¬ç«‹æœºæ„çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ï¼Œå…·æœ‰è‰¯å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›å’Œç«äº‰åŠ›è¡¨ç°ã€‚åŒæ—¶ï¼Œè¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œæ¢è®¨äº†å„ç§èšåˆç­–ç•¥ã€è¾¹ç¼˜åŠ æƒæ–¹æ¡ˆå’Œå›¾è¿æ¥æ¨¡å¼çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå’Œè…¹éƒ¨CTæ•°æ®ä¸Šçš„æ›´å¹¿æ³›åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€CTæ£€æŸ¥é‡çš„å¢é•¿ï¼Œå¯¹è‡ªåŠ¨åŒ–å·¥å…·ï¼ˆå¦‚å™¨å®˜åˆ†å‰²ã€å¼‚å¸¸æ£€æµ‹ã€æŠ¥å‘Šç”Ÿæˆç­‰ï¼‰çš„éœ€æ±‚å¢åŠ ï¼Œä»¥æ”¯æŒæ”¾å°„ç§‘åŒ»ç”Ÿç®¡ç†ä¸´åºŠå·¥ä½œé‡ã€‚</li>
<li>å¤šæ ‡ç­¾ä¸‰ç»´èƒ¸éƒ¨CTæ‰«æåˆ†ç±»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºä½“ç§¯æ•°æ®ä¸­çš„å¤æ‚ç©ºé—´å…³ç³»å’Œå¼‚å¸¸çš„å¹¿æ³›å˜åŒ–ã€‚</li>
<li>åŸºäºä¸‰ç»´å·ç§¯ç¥ç»ç½‘ç»œçš„æ–¹æ³•éš¾ä»¥æ•æ‰é•¿æœŸä¾èµ–å…³ç³»ï¼Œè€Œè§†è§‰è½¬æ¢å™¨åˆ™éœ€è¦å¤§è§„æ¨¡ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®æ‰èƒ½è¡¨ç°è‰¯å¥½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºå›¾çš„æ¡†æ¶ï¼Œå°†ä¸‰ç»´CTä½“ç§¯è¡¨ç¤ºä¸ºç»“æ„åŒ–å›¾ï¼Œé€šè¿‡è°±å›¾å·ç§¯å¤„ç†è½´å‘åˆ‡ç‰‡ä¸‰é‡ä½œä¸ºèŠ‚ç‚¹ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†è‰¯å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›å’Œç«äº‰åŠ›è¡¨ç°ï¼Œå¹¶åœ¨ç‹¬ç«‹æœºæ„çš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</li>
<li>æ¶ˆèç ”ç©¶æ¢è®¨äº†å„ç§èšåˆç­–ç•¥ã€è¾¹ç¼˜åŠ æƒæ–¹æ¡ˆå’Œå›¾è¿æ¥æ¨¡å¼å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6a9d0fd641b9ace0d103dd23edf91fce~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309953&auth_key=1762309953-0-0-6723d6e065b96bb8e2543bb364150cb2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-816c0e1ec9080fa4eba89823bbee59c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309960&auth_key=1762309960-0-0-45d366635582da59398e1c94fb65d99d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5af759bfd57a721e91d6dfb49a10ef9c~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309968&auth_key=1762309968-0-0-3ef2b64aac769b97afa98292ab6e94b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e57265371480eb30be9a4c6e8395a47d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309975&auth_key=1762309975-0-0-b904ccb43dc9ddb5d38f363d7d6e4f4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d755729c09725253de24e30990338d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309982&auth_key=1762309982-0-0-2ab5bbb79d10614f0a5a85f8892cf480&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Toward-a-Vision-Language-Foundation-Model-for-Medical-Data-Multimodal-Dataset-and-Benchmarks-for-Vietnamese-PET-CT-Report-Generation"><a href="#Toward-a-Vision-Language-Foundation-Model-for-Medical-Data-Multimodal-Dataset-and-Benchmarks-for-Vietnamese-PET-CT-Report-Generation" class="headerlink" title="Toward a Vision-Language Foundation Model for Medical Data: Multimodal   Dataset and Benchmarks for Vietnamese PET&#x2F;CT Report Generation"></a>Toward a Vision-Language Foundation Model for Medical Data: Multimodal   Dataset and Benchmarks for Vietnamese PET&#x2F;CT Report Generation</h2><p><strong>Authors:Huu Tien Nguyen, Dac Thai Nguyen, The Minh Duc Nguyen, Trung Thanh Nguyen, Thao Nguyen Truong, Huy Hieu Pham, Johan Barthelemy, Minh Quan Tran, Thanh Tam Nguyen, Quoc Viet Hung Nguyen, Quynh Anh Chau, Hong Son Mai, Thanh Trung Nguyen, Phi Le Nguyen</strong></p>
<p>Vision-Language Foundation Models (VLMs), trained on large-scale multimodal datasets, have driven significant advances in Artificial Intelligence (AI) by enabling rich cross-modal reasoning. Despite their success in general domains, applying these models to medical imaging remains challenging due to the limited availability of diverse imaging modalities and multilingual clinical data. Most existing medical VLMs are trained on a subset of imaging modalities and focus primarily on high-resource languages, thus limiting their generalizability and clinical utility. To address these limitations, we introduce a novel Vietnamese-language multimodal medical dataset consisting of 2,757 whole-body PET&#x2F;CT volumes from independent patients and their corresponding full-length clinical reports. This dataset is designed to fill two pressing gaps in medical AI development: (1) the lack of PET&#x2F;CT imaging data in existing VLMs training corpora, which hinders the development of models capable of handling functional imaging tasks; and (2) the underrepresentation of low-resource languages, particularly the Vietnamese language, in medical vision-language research. To the best of our knowledge, this is the first dataset to provide comprehensive PET&#x2F;CT-report pairs in Vietnamese. We further introduce a training framework to enhance VLMsâ€™ learning, including data augmentation and expert-validated test sets. We conduct comprehensive experiments benchmarking state-of-the-art VLMs on downstream tasks. The experimental results show that incorporating our dataset significantly improves the performance of existing VLMs. We believe this dataset and benchmark will serve as a pivotal step in advancing the development of more robust VLMs for medical imaging, especially for low-resource languages and clinical use in Vietnamese healthcare. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/AIoT-Lab-BKAI/ViPET-ReportGen">https://github.com/AIoT-Lab-BKAI/ViPET-ReportGen</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLMsï¼‰ç»è¿‡å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†çš„è®­ç»ƒï¼Œé€šè¿‡ä¸°å¯Œçš„è·¨æ¨¡æ€æ¨ç†æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„é‡å¤§è¿›æ­¥ã€‚å°½ç®¡å®ƒä»¬åœ¨é€šç”¨é¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†è¿™äº›æ¨¡å‹åœ¨åŒ»å­¦æˆåƒæ–¹é¢çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå­˜åœ¨å„ç§æˆåƒæ¨¡å¼å’Œå¤šè¯­è¨€ä¸´åºŠæ•°æ®çš„æœ‰é™å¯ç”¨æ€§ã€‚ç°æœ‰çš„å¤§å¤šæ•°åŒ»å­¦VLMsåªåœ¨éƒ¨åˆ†æˆåƒæ¨¡å¼ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸»è¦å…³æ³¨èµ„æºä¸°å¯Œçš„è¯­è¨€ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„é€šç”¨æ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¶Šå—è¯­å¤šæ¨¡æ€åŒ»å­¦æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªç‹¬ç«‹æ‚£è€…çš„2757ä¸ªå…¨èº«PET&#x2F;CTä½“ç§¯åŠå…¶ç›¸åº”çš„å®Œæ•´ä¸´åºŠæŠ¥å‘Šã€‚è¯¥æ•°æ®é›†æ—¨åœ¨å¡«è¡¥åŒ»å­¦äººå·¥æ™ºèƒ½å‘å±•ä¸­çš„ä¸¤ä¸ªç´§è¿«ç©ºç™½ï¼šï¼ˆ1ï¼‰ç°æœ‰VLMsè®­ç»ƒè¯­æ–™åº“ä¸­ç¼ºä¹PET&#x2F;CTæˆåƒæ•°æ®ï¼Œè¿™é˜»ç¢äº†èƒ½å¤Ÿå¤„ç†åŠŸèƒ½æ€§æˆåƒä»»åŠ¡çš„æ¨¡å‹çš„å¼€å‘ï¼›ï¼ˆ2ï¼‰åœ¨åŒ»å­¦è§†è§‰è¯­è¨€ç ”ç©¶ä¸­ï¼Œç‰¹åˆ«æ˜¯è¶Šå—è¯­çš„ä½èµ„æºè¯­è¨€ä»£è¡¨æ€§ä¸è¶³ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæä¾›è¶Šå—è¯­å…¨é¢çš„PET&#x2F;CTæŠ¥å‘Šé…å¯¹çš„æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ä¸ªå¢å¼ºVLMså­¦ä¹ çš„è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºå’Œä¸“å®¶éªŒè¯çš„æµ‹è¯•é›†ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œè¯„ä¼°äº†æœ€å…ˆè¿›VLMsåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠ å…¥æˆ‘ä»¬çš„æ•°æ®é›†å¯ä»¥æ˜¾ç€æé«˜ç°æœ‰VLMsçš„æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¯¥æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å°†æ˜¯æ¨åŠ¨æ›´ç¨³å¥çš„VLMsåœ¨åŒ»å­¦æˆåƒæ–¹é¢çš„å‘å±•çš„å…³é”®ä¸€æ­¥ï¼Œç‰¹åˆ«æ˜¯å¯¹ä½èµ„æºè¯­è¨€å’Œè¶Šå—åŒ»ç–—ä¿å¥çš„ä¸´åºŠåº”ç”¨ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIoT-Lab-BKAI/ViPET-ReportGen%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AIoT-Lab-BKAI/ViPET-ReportGenæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.24739v2">PDF</a> 39th Conference on Neural Information Processing Systems (NeurIPS   2025)</p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶é’ˆå¯¹åŒ»å­¦å›¾åƒé¢†åŸŸï¼Œå¼•å…¥äº†ä¸€æ¬¾è¶Šå—è¯­çš„å¤šæ¨¡æ€åŒ»ç–—æ•°æ®é›†ï¼ŒåŒ…å«PET&#x2F;CTå½±åƒä¸ä¸´åºŠæŠ¥å‘Šã€‚æ­¤æ•°æ®é›†è§£å†³äº†ç°æœ‰VLMæ¨¡å‹ä¸­ç¼ºä¹PET&#x2F;CTå½±åƒæ•°æ®å’Œä½èµ„æºè¯­è¨€ä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ–°çš„è®­ç»ƒæ¡†æ¶å’Œå®éªŒéªŒè¯ï¼Œè¯¥æ•°æ®é›†æ˜¾è‘—æå‡äº†VLMæ¨¡å‹æ€§èƒ½ã€‚è¿™å°†ä¸ºä½èµ„æºè¯­è¨€å°¤å…¶æ˜¯è¶Šå—è¯­çš„åŒ»å­¦å½±åƒå‘å±•è¿ˆå‡ºé‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼•å…¥äº†è¶Šå—è¯­çš„å¤šæ¨¡æ€åŒ»ç–—æ•°æ®é›†ï¼Œæ¶µç›–PET&#x2F;CTå½±åƒåŠå…¶å¯¹åº”çš„ä¸´åºŠæŠ¥å‘Šã€‚</li>
<li>æ•°æ®é›†è§£å†³äº†ç°æœ‰VLMæ¨¡å‹ç¼ºä¹PET&#x2F;CTå½±åƒæ•°æ®å’Œä½èµ„æºè¯­è¨€ä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºå’Œä¸“å®¶éªŒè¯æµ‹è¯•é›†ï¼Œå¢å¼ºäº†VLMæ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†æ˜¾è‘—æå‡äº†VLMæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>è¯¥æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¯¹äºæ¨åŠ¨åŒ»å­¦å½±åƒçš„ç¨³å¥VLMæ¨¡å‹å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€å’Œè¶Šå—è¯­ä¸´åºŠåº”ç”¨æ–¹é¢ï¼Œå…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-399cc46d3469c18d0bc93536c7c6ce70~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309989&auth_key=1762309989-0-0-6ea4d395ad88f6dcb8fbd4697716834a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8bf816974e670d05a695baf1b4ad6eec~resize:0:q75.jpg?source=1f5c5e47&expiration=1762309997&auth_key=1762309997-0-0-096a013bb8a83c52299b65e1f93d8cb5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9717ce2be181ed57ada9b9d7f827cf13~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310004&auth_key=1762310004-0-0-551ac93fdc2888eb7e0ff913bdbd3b3b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2ac85df38c33c8cfe59aff2bca2efbd7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310011&auth_key=1762310011-0-0-25c12bbd5c5df20449f20db8e3d9b7bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-417d91fefb6a83779f4743c466aee5e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310018&auth_key=1762310018-0-0-40205ad8f4516ad24ca5b73224a4011f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Untangling-Vascular-Trees-for-Surgery-and-Interventional-Radiology"><a href="#Untangling-Vascular-Trees-for-Surgery-and-Interventional-Radiology" class="headerlink" title="Untangling Vascular Trees for Surgery and Interventional Radiology"></a>Untangling Vascular Trees for Surgery and Interventional Radiology</h2><p><strong>Authors:Guillaume Houry, Tom Boeken, StÃ©phanie AllassonniÃ¨re, Jean Feydy</strong></p>
<p>The diffusion of minimally invasive, endovascular interventions motivates the development of visualization methods for complex vascular networks. We propose a planar representation of blood vessel trees which preserves the properties that are most relevant to catheter navigation: topology, length and curvature. Taking as input a three-dimensional digital angiography, our algorithm produces a faithful two-dimensional map of the patientâ€™s vessels within a few seconds. To this end, we propose optimized implementations of standard morphological filters and a new recursive embedding algorithm that preserves the global orientation of the vascular network. We showcase our method on peroperative images of the brain, pelvic and knee artery networks. On the clinical side, our method simplifies the choice of devices prior to and during the intervention. This lowers the risk of failure during navigation or device deployment and may help to reduce the gap between expert and common intervention centers. From a research perspective, our method simulates the cadaveric display of artery trees from anatomical dissections. This opens the door to large population studies on the branching patterns and tortuosity of fine human blood vessels. Our code is released under the permissive MIT license as part of the scikit-shapes Python library (<a target="_blank" rel="noopener" href="https://scikit-shapes.github.io/">https://scikit-shapes.github.io</a> ). </p>
<blockquote>
<p>å¾®åˆ›è¡€ç®¡å†…å¹²é¢„çš„æ™®åŠä¿ƒä½¿äº†é’ˆå¯¹å¤æ‚è¡€ç®¡ç½‘ç»œçš„å¯è§†åŒ–æ–¹æ³•çš„å‘å±•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è¡€ç®¡æ ‘çš„å¹³é¢è¡¨ç¤ºæ–¹æ³•ï¼Œä¿ç•™äº†ä¸å¯¼ç®¡å¯¼èˆªæœ€ç›¸å…³çš„å±æ€§ï¼šæ‹“æ‰‘ç»“æ„ã€é•¿åº¦å’Œæ›²ç‡ã€‚ä»¥ä¸‰ç»´æ•°å­—è¡€ç®¡é€ å½±ä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬çš„ç®—æ³•å¯åœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆæ‚£è€…è¡€ç®¡çš„å¿ å®äºŒç»´åœ°å›¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹æ ‡å‡†å½¢æ€å­¦æ»¤æ³¢å™¨è¿›è¡Œäº†ä¼˜åŒ–å®ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„é€’å½’åµŒå…¥ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¿ç•™äº†è¡€ç®¡ç½‘ç»œçš„æ•´ä½“æ–¹å‘ã€‚æˆ‘ä»¬åœ¨è„‘ã€éª¨ç›†å’Œè†å…³èŠ‚åŠ¨è„‰ç½‘ç»œçš„æœ¯ä¸­å›¾åƒä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚åœ¨ä¸´åºŠæ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç®€åŒ–äº†å¹²é¢„å‰åè®¾å¤‡çš„é€‰æ‹©ã€‚è¿™é™ä½äº†å¯¼èˆªæˆ–è®¾å¤‡éƒ¨ç½²è¿‡ç¨‹ä¸­çš„å¤±è´¥é£é™©ï¼Œå¹¶æœ‰åŠ©äºç¼©å°ä¸“å®¶ä¸æ™®é€šå¹²é¢„ä¸­å¿ƒä¹‹é—´çš„å·®è·ã€‚ä»ç ”ç©¶çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¨¡æ‹Ÿäº†è§£å‰–è§£å‰–ä¸­çš„åŠ¨è„‰æ ‘å°¸æ£€æ˜¾ç¤ºã€‚è¿™ä¸ºå¯¹äººç±»ç²¾ç»†è¡€ç®¡çš„åˆ†æ”¯æ¨¡å¼å’Œæ‰­æ›²æ€§è¿›è¡Œå¤§è§„æ¨¡äººç¾¤ç ”ç©¶æ‰“å¼€äº†å¤§é—¨ã€‚æˆ‘ä»¬çš„ä»£ç ä½œä¸ºscikit-shapes Pythonåº“çš„ä¸€éƒ¨åˆ†ï¼Œåœ¨è®¸å¯çš„MITè®¸å¯è¯ä¸‹å‘å¸ƒï¼ˆ<a target="_blank" rel="noopener" href="https://scikit-shapes.github.io)./">https://scikit-shapes.github.ioï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23165v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å¹³é¢è¡¨ç¤ºæ³•å±•ç¤ºè¡€ç®¡æ ‘ï¼Œä»¥å‘ˆç°å¯¼ç®¡å¯¼èˆªæœ€ç›¸å…³çš„å±æ€§ï¼šæ‹“æ‰‘ç»“æ„ã€é•¿åº¦å’Œæ›²ç‡ã€‚é€šè¿‡ä¸‰ç»´æ•°å­—è¡€ç®¡é€ å½±æœ¯è¾“å…¥ï¼Œç®—æ³•å¯åœ¨å‡ ç§’å†…ç”Ÿæˆæ‚£è€…è¡€ç®¡çš„å¿ å®äºŒç»´åœ°å›¾ã€‚è¯¥æ–¹æ³•ç®€åŒ–äº†ä»‹å…¥æ‰‹æœ¯å‰çš„è®¾å¤‡é€‰æ‹©ï¼Œé™ä½äº†å¯¼èˆªæˆ–è®¾å¤‡éƒ¨ç½²è¿‡ç¨‹ä¸­çš„å¤±è´¥é£é™©ï¼Œæœ‰åŠ©äºç¼©å°ä¸“å®¶ä¸æ™®é€šä»‹å…¥ä¸­å¿ƒä¹‹é—´çš„å·®è·ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æ¨¡æ‹Ÿäº†åŠ¨è„‰æ ‘çš„å°¸ä½“è§£å‰–æ˜¾ç¤ºï¼Œä¸ºç ”ç©¶äººç±»è¡€ç®¡åˆ†æ”¯æ¨¡å¼å’Œå¼¯æ›²åº¦æä¾›äº†æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§ç”¨äºå‘ˆç°å¤æ‚è¡€ç®¡ç½‘ç»œçš„å¹³é¢è¡¨ç¤ºæ³•ï¼Œæ—¨åœ¨è¾…åŠ©å¾®åˆ›æ€§è¡€ç®¡å†…å¹²é¢„çš„å¯è§†åŒ–æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤ŸåŸºäºä¸‰ç»´æ•°å­—è¡€ç®¡é€ å½±æœ¯å¿«é€Ÿç”Ÿæˆæ‚£è€…è¡€ç®¡çš„äºŒç»´åœ°å›¾ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿä¿ç•™å¯¹å¯¼ç®¡å¯¼èˆªè‡³å…³é‡è¦çš„æ‹“æ‰‘ç»“æ„ã€é•¿åº¦å’Œæ›²ç‡å±æ€§ã€‚</li>
<li>è¿™ç§æ–¹æ³•ç®€åŒ–äº†æ‰‹æœ¯å‰çš„è®¾å¤‡é€‰æ‹©ï¼Œå¹¶é™ä½äº†æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„å¤±è´¥é£é™©ã€‚</li>
<li>æ­¤æ–¹æ³•æœ‰åŠ©äºç¼©å°ä¸“å®¶ä¸éä¸“å®¶ä»‹å…¥ä¸­å¿ƒä¹‹é—´çš„å·®è·ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­å±•ç¤ºäº†å…¶åœ¨æ‰‹æœ¯ä¸­å¯¹å¤§è„‘ã€éª¨ç›†å’Œè†ç›–åŠ¨è„‰ç½‘ç»œçš„è‰¯å¥½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-805524fce28b9a929eacb005c8a4587d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310026&auth_key=1762310026-0-0-ad8481e0f0ce74e3463a08f9115e4b87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-27c169e34f32f151692bf90b0075aa37~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310033&auth_key=1762310033-0-0-3a50545308cdab1aa70a930d356eadd6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-072f6c74979d9600a70dbf3d6a04b70d~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310040&auth_key=1762310040-0-0-92574c01a5bd3cdba9839f429e785d9c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e5090785f9afbc0bdd9e82d9ce36bd60~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310046&auth_key=1762310046-0-0-ee472173c0a007aa27d0d81f25ef57c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Cryo-RL-automating-prostate-cancer-cryoablation-planning-with-reinforcement-learning"><a href="#Cryo-RL-automating-prostate-cancer-cryoablation-planning-with-reinforcement-learning" class="headerlink" title="Cryo-RL: automating prostate cancer cryoablation planning with   reinforcement learning"></a>Cryo-RL: automating prostate cancer cryoablation planning with   reinforcement learning</h2><p><strong>Authors:Trixia Simangan, Ahmed Nadeem Abbasi, Yipeng Hu, Shaheer U. Saeed</strong></p>
<p>Cryoablation is a minimally invasive localised treatment for prostate cancer that destroys malignant tissue during de-freezing, while sparing surrounding healthy structures. Its success depends on accurate preoperative planning of cryoprobe placements to fully cover the tumour and avoid critical anatomy. This planning is currently manual, expertise-dependent, and time-consuming, leading to variability in treatment quality and limited scalability. In this work, we introduce Cryo-RL, a reinforcement learning framework that models cryoablation planning as a Markov decision process and learns an optimal policy for cryoprobe placement. Within a simulated environment that models clinical constraints and stochastic intraoperative variability, an agent sequentially selects cryoprobe positions and ice sphere diameters. Guided by a reward function based on tumour coverage, this agent learns a cryoablation strategy that leads to optimal cryoprobe placements without the need for any manually-designed plans. Evaluated on 583 retrospective prostate cancer cases, Cryo-RL achieved over 8 percentage-point Dice improvements compared with the best automated baselines, based on geometric optimisation, and matched human expert performance while requiring substantially less planning time. These results highlight the potential of reinforcement learning to deliver clinically viable, reproducible, and efficient cryoablation plans. </p>
<blockquote>
<p>å†·å†»æ¶ˆèæ˜¯ä¸€ç§å¯¹å‰åˆ—è…ºç™Œçš„å¾®åˆ›å±€éƒ¨æ²»ç–—æ–¹æ³•ï¼Œå®ƒèƒ½åœ¨è§£å†»è¿‡ç¨‹ä¸­ç ´åæ¶æ€§ç»„ç»‡ï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´çš„å¥åº·ç»“æ„ã€‚å…¶æˆåŠŸå–å†³äºå†·å†»æ¢é’ˆæ”¾ç½®çš„æœ¯å‰è®¡åˆ’å‡†ç¡®ï¼Œä»¥å…¨é¢è¦†ç›–è‚¿ç˜¤å¹¶é¿å…å…³é”®è§£å‰–ç»“æ„ã€‚å½“å‰çš„è§„åˆ’æ˜¯æ‰‹åŠ¨çš„ï¼Œä¾èµ–äºä¸“å®¶ï¼Œå¹¶ä¸”è€—æ—¶ï¼Œå¯¼è‡´æ²»ç–—è´¨é‡å‚å·®ä¸é½ï¼Œå¯æ‰©å±•æ€§æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å†·å†»å¼ºåŒ–å­¦ä¹ ï¼ˆCryo-RLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†å†·å†»æ¶ˆèè®¡åˆ’å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶å­¦ä¹ å†·å†»æ¢é’ˆæ”¾ç½®çš„æœ€ä¼˜ç­–ç•¥ã€‚åœ¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„ç¯å¢ƒä¸­ï¼Œè¯¥ç¯å¢ƒæ¨¡æ‹Ÿäº†ä¸´åºŠçº¦æŸå’Œæœ¯ä¸­éšæœºå˜åŒ–ï¼Œæ™ºèƒ½ä½“æŒ‰é¡ºåºé€‰æ‹©å†·å†»æ¢é’ˆçš„ä½ç½®å’Œå†°çƒç›´å¾„ã€‚åœ¨è‚¿ç˜¤è¦†ç›–çš„å¥–åŠ±å‡½æ•°æŒ‡å¯¼ä¸‹ï¼Œæ™ºèƒ½ä½“å­¦ä¹ ä¸€ç§å†·å†»æ¶ˆèç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¯¼è‡´æœ€ä¼˜çš„å†·å†»æ¢é’ˆæ”¾ç½®ï¼Œæ— éœ€ä»»ä½•æ‰‹åŠ¨è®¾è®¡è®¡åˆ’ã€‚åœ¨583ä¾‹å›é¡¾æ€§å‰åˆ—è…ºç™Œç—…ä¾‹ä¸­è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŸºäºå‡ ä½•ä¼˜åŒ–çš„æœ€ä½³è‡ªåŠ¨åŒ–åŸºçº¿ç›¸æ¯”ï¼ŒCryo-RLçš„DiceæŒ‡æ•°æé«˜äº†è¶…è¿‡8ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶åŒ¹é…äº†äººç±»ä¸“å®¶çš„æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§å‡å°‘äº†è§„åˆ’æ—¶é—´ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨æä¾›ä¸´åºŠå¯è¡Œã€å¯é‡å¤å’Œé«˜æ•ˆçš„å†·å†»æ¶ˆèè®¡åˆ’æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04886v3">PDF</a> Accepted at MICAD (Medical Imaging and Computer-Aided Diagnosis) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Cryo-RLè¿™ä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨å‰åˆ—è…ºç™Œå†·å†»æ¶ˆèæ²»ç–—è®¡åˆ’ä¸­çš„åº”ç”¨ã€‚è¯¥æ¡†æ¶å°†å†·å†»æ¶ˆèè®¡åˆ’è§†ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå­¦ä¹ å†·å†»æ¢é’ˆæ”¾ç½®çš„æœ€ä¼˜ç­–ç•¥ã€‚åœ¨æ¨¡æ‹Ÿçš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œé€šè¿‡å¥–åŠ±å‡½æ•°å¼•å¯¼ï¼Œè‡ªä¸»å­¦ä¼šæ— éœ€æ‰‹åŠ¨è®¾è®¡çš„å†·å†»æ¶ˆèç­–ç•¥ï¼Œå®ç°å¯¹è‚¿ç˜¤çš„æœ€ä½³è¦†ç›–ã€‚ä¸å‡ ä½•ä¼˜åŒ–ç­‰è‡ªåŠ¨åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œå…¶åœ¨å›é¡¾æ€§å‰åˆ—è…ºç™Œç—…ä¾‹ä¸Šå–å¾—äº†è¶…è¿‡8ä¸ªç™¾åˆ†ç‚¹çš„Diceæ”¹å–„å€¼ï¼Œä¸”åŒ¹é…äº†ä¸“å®¶çš„äººç±»è¡¨ç°ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘äº†è§„åˆ’æ—¶é—´ã€‚æ­¤ç ”ç©¶å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨å†·å†»æ¶ˆèè®¡åˆ’ä¸­çš„ä¸´åºŠåº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cryoablationæ˜¯ä¸€ç§å¾®åˆ›çš„å±€éƒ¨å‰åˆ—è…ºç™Œæ²»ç–—æ–¹æ³•ï¼Œé€šè¿‡å†·å†»æ¶ˆèæ¶æ€§ç»„ç»‡ï¼ŒåŒæ—¶ä¿æŠ¤å‘¨å›´å¥åº·ç»“æ„ã€‚</li>
<li>å½“å‰Cryoablationçš„æœ¯å‰è§„åˆ’ä¾èµ–äºä¸“å®¶ç»éªŒå’Œæ—¶é—´æ¶ˆè€—ï¼Œå¯¼è‡´æ²»ç–—è´¨é‡ä¸ä¸€ä¸”éš¾ä»¥è§„æ¨¡åŒ–ã€‚</li>
<li>å¼•å…¥Cryo-RLå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå°†å†·å†»æ¶ˆèè§„åˆ’è§†ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œé€šè¿‡å¥–åŠ±å‡½æ•°å¼•å¯¼å­¦ä¹ æœ€ä¼˜å†·å†»æ¢é’ˆæ”¾ç½®ç­–ç•¥ï¼Œå®ç°å¯¹è‚¿ç˜¤çš„æœ€ä½³è¦†ç›–ã€‚</li>
<li>ä¸å‡ ä½•ä¼˜åŒ–ç­‰è‡ªåŠ¨åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒCryo-RLåœ¨å›é¡¾æ€§å‰åˆ—è…ºç™Œç—…ä¾‹ä¸Šå–å¾—äº†æ˜¾è‘—æ•ˆæœï¼Œæé«˜äº†Diceç³»æ•°å€¼ã€‚</li>
<li>Cryo-RLåŒ¹é…äº†ä¸“å®¶çš„äººç±»è¡¨ç°ï¼Œå¹¶å¤§å¹…å‡å°‘äº†æœ¯å‰è§„åˆ’æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d4a59fe6e9641fe50f0cf3eef57bf4c7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310054&auth_key=1762310054-0-0-badc68b7f57c641a21b73565814a35f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8711b3b95e78473a90ad9a2f64e6e4fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310061&auth_key=1762310061-0-0-5cd974c33aca157af54ce686a052ff58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-c5e6db09a073f6c66aee05d9747bc3f7~resize:0:q75.jpg?source=1f5c5e47&expiration=1762310067&auth_key=1762310067-0-0-af3c3e8f07677230233e4b77f368a117&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  AgentTTS Large Language Model Agent for Test-time Compute-optimal   Scaling Strategy in Complex Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-a1532839f8b84003f880d4f986b9f33c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761346704&auth_key=1761346704-0-0-e5dd7c2d0bc9345a86749346a40d1639&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian   Responses
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
