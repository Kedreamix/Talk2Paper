<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Small Drafts, Big Verdict Information-Intensive Visual Reasoning via   Speculation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-9fdef8b5dc5e569fb7e3c506069c9f05~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328671&auth_key=1761328671-0-0-3a695c63368636c04361ff8d7c972c90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-25-æ›´æ–°"><a href="#2025-10-25-æ›´æ–°" class="headerlink" title="2025-10-25 æ›´æ–°"></a>2025-10-25 æ›´æ–°</h1><h2 id="Small-Drafts-Big-Verdict-Information-Intensive-Visual-Reasoning-via-Speculation"><a href="#Small-Drafts-Big-Verdict-Information-Intensive-Visual-Reasoning-via-Speculation" class="headerlink" title="Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via   Speculation"></a>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via   Speculation</h2><p><strong>Authors:Yuhan Liu, Lianhui Qin, Shengjie Wang</strong></p>
<p>Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Tinaliu0123/speculative-verdict">https://github.com/Tinaliu0123/speculative-verdict</a> </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å¤„ç†ä¿¡æ¯å¯†é›†çš„å›¾åƒæ—¶é¢ä¸´å›°éš¾ï¼Œè¿™äº›å›¾åƒå°†æ–‡æœ¬æ³¨é‡Šä¸ç²¾ç»†çš„å›¾å½¢å…ƒç´ ç´§å¯†äº¤ç»‡åœ¨ä¸€èµ·ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨å¯†é›†å¸ƒå±€ä¸­ç²¾ç¡®å®šä½å…³é”®çº¿ç´¢ï¼Œä»¥åŠè¿›è¡Œå¤šè·³æ¨ç†ä»¥æ•´åˆåˆ†æ•£çš„è¯æ®ã€‚æˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„â€œæ¨æµ‹åˆ¤å†³â€ï¼ˆSVï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å—åˆ°æ¨æµ‹è§£ç çš„å¯å‘ï¼Œç»“åˆäº†å¤šä¸ªè½»å‹è‰ç¨¿ä¸“å®¶å’Œä¸€ä¸ªå¤§å‹åˆ¤å†³æ¨¡å‹ã€‚åœ¨è‰ç¨¿é˜¶æ®µï¼Œå°å‹VLMsä½œä¸ºè‰ç¨¿ä¸“å®¶ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œæä¾›å¤šæ ·åŒ–çš„å®šä½å€™é€‰ï¼›åœ¨åˆ¤å†³é˜¶æ®µï¼Œå¼ºå¤§çš„VLMåˆæˆè¿™äº›è·¯å¾„ä»¥äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆï¼Œè¿™å¯ä»¥åœ¨å‡å°‘è®¡ç®—æˆæœ¬çš„åŒæ—¶æ¢å¤æ­£ç¡®ç­”æ¡ˆã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒSVå¼•å…¥äº†ä¸€ç§å…±è¯†ä¸“å®¶é€‰æ‹©æœºåˆ¶ï¼Œåªå°†é«˜åº¦ä¸€è‡´çš„æ¨ç†è·¯å¾„è½¬å‘åˆ°åˆ¤å†³é˜¶æ®µã€‚åœ¨å®è·µä¸­ï¼ŒSVåœ¨å…·æœ‰æŒ‘æˆ˜æ€§å’Œé«˜åˆ†è¾¨ç‡çš„è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æŒç»­çš„è¿›æ­¥ï¼ŒåŒ…æ‹¬ä¿¡æ¯å›¾è¡¨VQAã€å›¾è¡¨åšç‰©é¦†ã€ChartQAProå’ŒHR-Bench 4Kç­‰ã€‚é€šè¿‡ä»å¤šä¸ªéƒ¨åˆ†å‡†ç¡®çš„æ¨ç†è·¯å¾„ä¸­ç»¼åˆæ­£ç¡®çš„è§è§£ï¼Œä¸å¤§å‹ä¸“æœ‰æ¨¡å‹æˆ–è®­ç»ƒç®¡é“ç›¸æ¯”ï¼ŒSVå®ç°äº†é”™è¯¯çº æ­£å’Œæˆæœ¬æ•ˆç›Šã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Tinaliu0123/speculative-verdict">https://github.com/Tinaliu0123/speculative-verdict</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20812v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯å¯†é›†å‹å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›å›¾åƒå°†æ–‡æœ¬æ³¨é‡Šä¸ç²¾ç»†å›¾å½¢å…ƒç´ ç´§å¯†äº¤ç»‡ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨å¯†é›†å¸ƒå±€ä¸­ç²¾ç¡®å®šä½å…³é”®çº¿ç´¢ï¼Œä»¥åŠè¿›è¡Œå¤šè·³æ¨ç†ä»¥æ•´åˆåˆ†æ•£çš„è¯æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„â€œæŠ•æœºè£å†³â€ï¼ˆSpeculative Verdictï¼Œç®€ç§°SVï¼‰æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¤šä¸ªè½»é‡çº§è‰æ¡ˆä¸“å®¶å’Œä¸€ä¸ªå¤§å‹è£å†³æ¨¡å‹ã€‚åœ¨è‰æ¡ˆé˜¶æ®µï¼Œå°å‹è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºè‰æ¡ˆä¸“å®¶ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œæä¾›å¤šæ ·åŒ–çš„å®šä½å€™é€‰ï¼›åœ¨è£å†³é˜¶æ®µï¼Œå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ç»¼åˆè¿™äº›è·¯å¾„äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆã€‚æ­¤å¤–ï¼ŒSVè¿˜å¼•å…¥äº†ä¸€ç§å…±è¯†ä¸“å®¶é€‰æ‹©æœºåˆ¶ï¼Œåªå°†é«˜åº¦ä¸€è‡´çš„æ¨ç†è·¯å¾„è½¬å‘åˆ°è£å†³é˜¶æ®µï¼Œä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒSVåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¿¡æ¯å¯†é›†å‹å’Œé«˜åˆ†è¾¨ç‡è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æŒç»­çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬InfographicVQAã€Chartmuseumã€ChartQAProå’ŒHR-Bench 4Kã€‚SVèƒ½å¤Ÿä»å¤šä¸ªéƒ¨åˆ†å‡†ç¡®çš„æ¨ç†è·¯å¾„ä¸­ç»¼åˆæ­£ç¡®çš„è§è§£ï¼Œä¸å¤§å‹ä¸“æœ‰æ¨¡å‹æˆ–è®­ç»ƒç®¡é“ç›¸æ¯”ï¼Œå®ç°äº†è¯¯å·®æ ¡æ­£å’Œæˆæœ¬æ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯å¯†é›†å‹å›¾åƒæ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦ç²¾ç¡®çš„å®šä½å’Œå¤æ‚çš„å¤šè·³æ¨ç†ã€‚</li>
<li>æå‡ºçš„â€œæŠ•æœºè£å†³â€ï¼ˆSVï¼‰æ¡†æ¶ç»“åˆäº†è½»é‡çº§è‰æ¡ˆä¸“å®¶ä¸å¤§å‹è£å†³æ¨¡å‹ï¼Œä»¥å¤„ç†è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>åœ¨è‰æ¡ˆé˜¶æ®µï¼Œå°å‹è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>åœ¨è£å†³é˜¶æ®µï¼Œå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ç»¼åˆè¿™äº›è·¯å¾„ä»¥äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆã€‚</li>
<li>SVå¼•å…¥å…±è¯†ä¸“å®¶é€‰æ‹©æœºåˆ¶ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>SVåœ¨å¤šä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å¤„ç†ä¿¡æ¯å¯†é›†å‹å’Œé«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-354cb9ce79b26c485c803c7c95211c26~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328572&auth_key=1761328572-0-0-0e092120b27ac0dbdb517e39c0f4123a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-84636bf522589bf07d781c09d2dba0ac~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328580&auth_key=1761328580-0-0-f8d9172fa67e47ec01c09ca2f349e251&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33cccc1c93a2a4acf0b5f7ef7da03e94~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328587&auth_key=1761328587-0-0-e55ce5faebc600b5c37d8f5bf9fc5654&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-60a2c34abef450d10eeac6425caac958~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328593&auth_key=1761328593-0-0-a9df37166682651af4b98a3ff1c52505&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost"><a href="#Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost" class="headerlink" title="Are Large Reasoning Models Good Translation Evaluators? Analysis and   Performance Boost"></a>Are Large Reasoning Models Good Translation Evaluators? Analysis and   Performance Boost</h2><p><strong>Authors:Runzhe Zhan, Zhihong Huang, Xinyi Yang, Lidia S. Chao, Min Yang, Derek F. Wong</strong></p>
<p>Recent advancements in large reasoning models (LRMs) have introduced an intermediate â€œthinkingâ€ process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to â€œoverthinkâ€ simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„è¿›å±•åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆä¹‹å‰å¼•å…¥äº†ä¸€ä¸ªä¸­é—´çš„â€œæ€è€ƒâ€è¿‡ç¨‹ï¼Œæé«˜äº†å®ƒä»¬åœ¨å¤æ‚çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLRMä½œä¸ºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰è´¨é‡çš„è¯„ä¼°å™¨çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬å¯¹LRMä½œä¸ºè¯„ä¼°è€…åœ¨MTè¯„ä¼°ä¸­è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿåˆ†æã€‚æˆ‘ä»¬ç¡®å®šäº†å…³é”®æŒ‘æˆ˜ï¼Œå‘ç°LRMéœ€è¦å®šåˆ¶è¯„ä¼°ææ–™ï¼Œå¾€å¾€ä¼šå¯¹è¾ƒç®€å•çš„å®ä¾‹â€œè¿‡åº¦æ€è€ƒâ€ï¼Œä»¥åŠè¯„åˆ†æœºåˆ¶é—®é¢˜å¯¼è‡´è¿‡åº¦ä¼°è®¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åˆæˆäººç±»æ€ç»´è½¨è¿¹æ¥è®­ç»ƒLRMï¼Œä»¥æ ¡æ­£å…¶æ€è€ƒæ–¹å¼ã€‚æˆ‘ä»¬åœ¨WMT24 MetricsåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•å°†æ€è€ƒé¢„ç®—å‡å°‘äº†çº¦35å€ï¼ŒåŒæ—¶åœ¨ä¸åŒè§„æ¨¡çš„LRMï¼ˆä»7Båˆ°32Bï¼‰ä¸Šæé«˜äº†è¯„ä¼°æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒR1-Distill-Qwen-7Bå®ç°äº†8.7ä¸ªç›¸å…³ç³»æ•°ç‚¹çš„æ”¹è¿›ï¼‰ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æœ‰æ•ˆæ ¡å‡†çš„LRMåœ¨æ¨åŠ¨ç²¾ç»†ç²’åº¦çš„è‡ªåŠ¨MTè¯„ä¼°æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20780v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰è¯„ä¼°é¢†åŸŸçš„åº”ç”¨é€æ¸å—åˆ°å…³æ³¨ã€‚ç ”ç©¶å›¢é˜Ÿé¦–æ¬¡ç³»ç»Ÿåœ°åˆ†æäº†LRMä½œä¸ºMTè¯„ä»·è€…çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†å…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚éœ€è¦å®šåˆ¶è¯„ä¼°ææ–™ã€è¿‡åº¦æ€è€ƒç®€å•æ¡ˆä¾‹ä»¥åŠè¯„åˆ†æœºåˆ¶å¯¼è‡´è¿‡åº¦ä¼°è®¡ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†é€šè¿‡åˆæˆäººç±»æ€ç»´è½¨è¿¹æ¥æ ¡å‡†LRMæ€ç»´çš„æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘æ€è€ƒé¢„ç®—çš„åŒæ—¶æé«˜äº†è¯„ä¼°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›å¾—åˆ°æ”¹è¿›ï¼Œå¼€å§‹åœ¨æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰è¯„ä¼°ä¸­å±•ç°æ½œåŠ›ã€‚</li>
<li>LRMä½œä¸ºMTè¯„ä»·è€…é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éœ€è¦å®šåˆ¶è¯„ä¼°ææ–™ã€è¿‡åº¦æ€è€ƒç®€å•æ¡ˆä¾‹ä»¥åŠè¯„åˆ†æœºåˆ¶é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†é€šè¿‡åˆæˆäººç±»æ€ç»´è½¨è¿¹æ¥æ ¡å‡†LRMæ€ç»´çš„æ–¹æ³•ï¼Œä»¥é™ä½æ€è€ƒé¢„ç®—å¹¶æé«˜è¯„ä¼°æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è§„æ¨¡çš„LRMä¸Šå‡æœ‰æ•ˆï¼Œå¦‚åœ¨WMT24 MetricsåŸºå‡†æµ‹è¯•ä¸­ï¼ŒR1-Distill-Qwen-7Bæ¨¡å‹å®ç°äº†+8.7çš„å…³è”ç‚¹æ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d95db1e02b69de497620afdf5aef58e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328601&auth_key=1761328601-0-0-94f79f76543c8de715017335ca582b0e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fa89547c174a837cdaf5ca93e9643fef~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328608&auth_key=1761328608-0-0-04073ac9084784c6a1ed410cc53462f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1742528b3429e196ac7cb1eb5ddc3675~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328616&auth_key=1761328616-0-0-49c84643061e91a73f6e9a0ed3523be7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b8cde3087adeaa35245d6b7b98b6f451~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328623&auth_key=1761328623-0-0-1f78c82d8b21d40d774148e0863bc170&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b56100e96fb409abef7e59110d11302~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328629&auth_key=1761328629-0-0-10b4b159ccb212cf44025fa4862ad155&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8dd83926e33f621176aba30ae8e5173e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328636&auth_key=1761328636-0-0-59c90c10f2c66ab63512f48bb4424f06&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Diagnosing-Visual-Reasoning-Challenges-Insights-and-a-Path-Forward"><a href="#Diagnosing-Visual-Reasoning-Challenges-Insights-and-a-Path-Forward" class="headerlink" title="Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward"></a>Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward</h2><p><strong>Authors:Jing Bi, Guangyu Sun, Ali Vosoughi, Chen Chen, Chenliang Xu</strong></p>
<p>Multimodal large language models (MLLMs) that integrate visual and textual reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual tasks, yet continue to exhibit visual hallucinations and an over-reliance on textual priors. We present a systematic diagnosis of state-of-the-art vision-language models using a three-stage evaluation framework, uncovering key failure modes. To address these, we propose an agent-based architecture that combines LLM reasoning with lightweight visual modules, enabling fine-grained analysis and iterative refinement of reasoning chains. Our results highlight future visual reasoning models should focus on integrating a broader set of specialized tools for analyzing visual content. Our system achieves significant gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or surpassing much larger models. We will release our framework and evaluation suite to facilitate future research. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èåˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨ç†ï¼Œåˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¥è§£å†³å¤æ‚çš„è§†è§‰ä»»åŠ¡ï¼Œä½†ä»ä¼šå‡ºç°è§†è§‰å¹»è§‰ï¼Œè¿‡åº¦ä¾èµ–æ–‡æœ¬å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶å¯¹æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯Šæ–­ï¼Œæ­ç¤ºäº†å…³é”®å¤±è´¥æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„æ¶æ„ï¼Œå°†LLMæ¨ç†ä¸è½»é‡çº§è§†è§‰æ¨¡å—ç›¸ç»“åˆï¼Œå®ç°å¯¹æ¨ç†é“¾çš„ç²¾ç»†åˆ†æè¿­ä»£ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œæœªæ¥çš„è§†è§‰æ¨ç†æ¨¡å‹åº”ä¾§é‡äºé›†æˆæ›´å¤šç”¨äºåˆ†æè§†è§‰å†…å®¹çš„ä¸“ç”¨å·¥å…·é›†ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨MMMUä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ï¼ˆæé«˜äº†10.3ï¼‰ï¼Œåœ¨MathVistaä¸Šæ¯”7BåŸºçº¿æé«˜äº†6.0ï¼Œä¸æˆ–è¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ¡†æ¶å’Œè¯„ä¼°å¥—ä»¶ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20696v1">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†å¤æ‚è§†è§‰ä»»åŠ¡æ—¶çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è§†è§‰å¹»è§‰å’Œå¯¹æ–‡æœ¬å…ˆéªŒçš„è¿‡åº¦ä¾èµ–ã€‚æ–‡ç« é€šè¿‡ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶å¯¹æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯Šæ–­ï¼Œæ­ç¤ºäº†å…³é”®å¤±è´¥æ¨¡å¼ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„æ¶æ„ï¼Œç»“åˆLLMæ¨ç†å’Œè½»é‡çº§è§†è§‰æ¨¡å—ï¼Œå®ç°å¯¹æ¨ç†é“¾çš„ç²¾ç»†åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œæœªæ¥è§†è§‰æ¨ç†æ¨¡å‹åº”ä¸“æ³¨äºæ•´åˆæ›´å¤šä¸“é—¨ç”¨äºåˆ†æè§†è§‰å†…å®¹çš„å·¥å…·ã€‚è¯¥ç³»ç»Ÿçš„æ”¹è¿›æ˜¾è‘—ï¼ˆ+10.3äºMMMUï¼Œ+6.0äºMathVistaä¼˜äº7BåŸºçº¿ï¼‰ï¼Œä¸æ›´å¤§çš„æ¨¡å‹ç›¸åŒ¹é…ç”šè‡³æ›´èƒœä¸€ç­¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨ç†ï¼Œé‡‡ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¥è§£å†³å¤æ‚çš„è§†è§‰ä»»åŠ¡ã€‚</li>
<li>MLLMså­˜åœ¨è§†è§‰å¹»è§‰å’Œå¯¹æ–‡æœ¬å…ˆéªŒçš„è¿‡åº¦ä¾èµ–çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼Œæ­ç¤ºäº†MLLMsçš„å…³é”®å¤±è´¥æ¨¡å¼ã€‚</li>
<li>æå‡ºçš„åŸºäºä»£ç†çš„æ¶æ„ç»“åˆäº†LLMæ¨ç†å’Œè½»é‡çº§è§†è§‰æ¨¡å—ï¼Œå®ç°å¯¹æ¨ç†é“¾çš„ç²¾ç»†åˆ†æå¹¶è¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>è¯¥æ¶æ„åœ¨MMMUå’ŒMathVistaä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
<li>æœªæ¥è§†è§‰æ¨ç†æ¨¡å‹åº”é‡è§†é›†æˆæ›´å¤šä¸“é—¨ç”¨äºåˆ†æè§†è§‰å†…å®¹çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20696">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0d186c1ec2215abb85fbd8027b68519c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328644&auth_key=1761328644-0-0-44ddacc2370eeec51917ba18198cf715&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-66e29526aaa1fa8f6e94e085ea62de82~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328651&auth_key=1761328651-0-0-7dd203a8f399d864f1c6cf71508d186f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d1fecdcb0217ef0cdfd9bc4cfc37e8f1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328657&auth_key=1761328657-0-0-81d77e41a8fd169720025a29cf088b8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-36b56607411d8384038b1c2984c53922~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328664&auth_key=1761328664-0-0-115688af699b73bb290cd530aa009931&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9fdef8b5dc5e569fb7e3c506069c9f05~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328671&auth_key=1761328671-0-0-3a695c63368636c04361ff8d7c972c90&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c867b38dcc6a9f200b3730124271d1c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328678&auth_key=1761328678-0-0-ad994c3b925932958b9717cb559a51c5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generalizable-Reasoning-through-Compositional-Energy-Minimization"><a href="#Generalizable-Reasoning-through-Compositional-Energy-Minimization" class="headerlink" title="Generalizable Reasoning through Compositional Energy Minimization"></a>Generalizable Reasoning through Compositional Energy Minimization</h2><p><strong>Authors:Alexandru Oarga, Yilun Du</strong></p>
<p>Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: <a target="_blank" rel="noopener" href="https://alexoarga.github.io/compositional_reasoning/">https://alexoarga.github.io/compositional_reasoning/</a> </p>
<blockquote>
<p>æ³›åŒ–æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ã€‚åœ¨æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹è¢«æœŸæœ›è§£å†³æ¯”è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°çš„æ›´å¤æ‚çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹å¼å¯¹æ¨ç†æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œç›´æ¥å°†è¾“å…¥å®ä¾‹æ˜ å°„åˆ°è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶è¿™å…è®¸æ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ æœ‰ç”¨çš„å¯å‘å¼çŸ¥è¯†ï¼Œä½†å®ƒé€šå¸¸ä¼šå¯¼è‡´åœ¨è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡åœ¨å­¦ä¹ æ›´å°ã€æ›´æ˜“å¤„ç†çš„å­é—®é¢˜çš„è§£å†³æ–¹æ¡ˆç©ºé—´ä¸Šçš„èƒ½é‡æ™¯è§‚æ¥å®ç°æ¨ç†æ³›åŒ–çš„æ–°æ–¹æ³•ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡ç»„åˆå¤šä¸ªå­é—®é¢˜çš„èƒ½é‡å‡½æ•°ï¼Œä¸ºç»™å®šé—®é¢˜æ„å»ºå…¨å±€èƒ½é‡æ™¯è§‚ã€‚è¿™ç§ç»„åˆæ–¹æ³•å…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ å…¥é¢å¤–çš„çº¦æŸï¼Œä»è€Œèƒ½å¤Ÿæ„å»ºéš¾åº¦é€’å¢çš„é—®é¢˜çš„èƒ½é‡æ™¯è§‚ã€‚ä¸ºäº†æé«˜ä»æ–°æ„å»ºçš„èƒ½æºæ™¯è§‚ä¸­è·å¾—çš„æ ·æœ¬è´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¹¶è¡Œèƒ½é‡æœ€å°åŒ–ï¼ˆPEMï¼‰ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›çš„æ¨ç†é—®é¢˜ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è§£å†³æ›´å¤§å’Œæ›´å¤æ‚é—®é¢˜ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚é¡¹ç›®ç½‘ç«™åœ°å€æ˜¯ï¼š[<a target="_blank" rel="noopener" href="https://alexoarga.github.io/compositional_reasoning/]">https://alexoarga.github.io/compositional_reasoning/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20607v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å­¦ä¹ å’Œåˆ©ç”¨é—®é¢˜å­ç©ºé—´èƒ½é‡æ™¯è§‚æ¥å¢å¼ºæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­æ³›åŒ–èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚é€šè¿‡æ„å»ºå…¨å±€èƒ½é‡æ™¯è§‚å¹¶ç»“åˆå¤šä¸ªå­é—®é¢˜çš„èƒ½é‡å‡½æ•°ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥é¢å¤–çš„çº¦æŸï¼Œä»è€Œå¤„ç†æ›´å¤æ‚çš„æ¨ç†é—®é¢˜ã€‚åŒæ—¶ï¼Œä¸ºæé«˜ä»èƒ½é‡æ™¯è§‚ä¸­è·å¾—çš„æ ·æœ¬è´¨é‡ï¼Œå¼•å…¥äº†å¹¶è¡Œèƒ½é‡æœ€å°åŒ–ï¼ˆPEMï¼‰æŠ€æœ¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´æ³›åŒ–æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡ç«¯åˆ°ç«¯çš„æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´åœ¨è®­ç»ƒåˆ†å¸ƒå¤–çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ³›åŒ–æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ å’Œåˆ©ç”¨é—®é¢˜å­ç©ºé—´çš„èƒ½é‡æ™¯è§‚æ¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤šä¸ªå­é—®é¢˜çš„èƒ½é‡å‡½æ•°æ¥æ„å»ºå…¨å±€èƒ½é‡æ™¯è§‚ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥é¢å¤–çº¦æŸï¼Œå¤„ç†æ›´å¤æ‚çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥å¹¶è¡Œèƒ½é‡æœ€å°åŒ–ï¼ˆPEMï¼‰æŠ€æœ¯ï¼Œæé«˜äº†ä»èƒ½é‡æ™¯è§‚ä¸­è·å¾—çš„æ ·æœ¬è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†æ¨¡å‹åœ¨è§£å†³è¶Šæ¥è¶Šå¤æ‚çš„é—®é¢˜æ—¶çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20607">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-89942f7b42291177c36cdd0a9398e132~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328685&auth_key=1761328685-0-0-ebcf7173e944dfbeaa57001bf4e7e7e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8eaffc88deb16c3ca6ae22f9f058d52~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328693&auth_key=1761328693-0-0-4cca214dee10d47bcf0c63e346bf7077&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c200ef200bc9b247a0065c85a98af40e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328700&auth_key=1761328700-0-0-8fa8caec74c3d2a09ac04bd91d6065fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-83f3ab352f41f7ec747bf8308bef7707~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328706&auth_key=1761328706-0-0-6516dfee426519ee80e9e1b6752ea66f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence"><a href="#Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence" class="headerlink" title="Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal   Evidence"></a>Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal   Evidence</h2><p><strong>Authors:Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang</strong></p>
<p>Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability. </p>
<blockquote>
<p>å¤§éƒ¨åˆ†è§†é¢‘æ¨ç†æ¨¡å‹åªç”Ÿæˆæ–‡æœ¬æ¨ç†ç—•è¿¹ï¼Œæ²¡æœ‰æ ‡æ˜å…³é”®è¯æ®å‡ºç°çš„æ—¶é—´å’Œåœ°ç‚¹ã€‚è™½ç„¶æœ€è¿‘çš„æ¨¡å‹å¦‚OpenAI-o3å¼•å‘äº†äººä»¬å¯¹å›¾åƒä¸­å¿ƒåŒ–æ¨ç†çš„å¹¿æ³›å…´è¶£ï¼Œä½†å°†è¿™ç§èƒ½åŠ›æ‰©å±•åˆ°è§†é¢‘æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦åœ¨åŠ¨æ€åœºæ™¯ä¸­è”åˆè¿›è¡Œæ—¶é—´è·Ÿè¸ªå’Œç©ºé—´å®šä½ã€‚æˆ‘ä»¬æ¨å‡ºäº†Open-o3 Videoï¼Œè¿™æ˜¯ä¸€ä¸ªéä»£ç†æ¡†æ¶ï¼Œå®ƒå°†æ˜ç¡®çš„æ—¶ç©ºè¯æ®èå…¥è§†é¢‘æ¨ç†ä¸­ï¼Œå¹¶è°¨æ…åœ°æ”¶é›†è®­ç»ƒæ•°æ®å¹¶è®¾è®¡è®­ç»ƒç­–ç•¥æ¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åœ¨ç­”æ¡ˆæ—è¾¹çªå‡ºäº†å…³é”®çš„æ—¶é—´æˆ³ã€å¯¹è±¡å’Œè¾¹ç•Œæ¡†ï¼Œä½¿æ¨ç†èƒ½å¤ŸåŸºäºå…·ä½“çš„è§†è§‰è§‚å¯Ÿã€‚ä¸ºäº†å®ç°æ­¤åŠŸèƒ½ï¼Œæˆ‘ä»¬é¦–å…ˆç²¾å¿ƒåˆ›å»ºäº†ä¸¤ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒSTGR-CoT-30kç”¨äºSFTï¼ŒSTGR-RL-36kç”¨äºRLï¼Œå®ƒä»¬å…·æœ‰ç²¾å¿ƒæ„å»ºçš„æ—¶é—´å’Œç©ºé—´æ³¨é‡Šï¼Œå› ä¸ºå¤§å¤šæ•°ç°æœ‰æ•°æ®é›†åªä¸ºè§†é¢‘æä¾›æ—¶é—´è·¨åº¦æˆ–ä¸ºå›¾åƒæä¾›ç©ºé—´æ¡†ï¼Œç¼ºä¹ç»Ÿä¸€çš„æ—¶ç©ºç›‘ç£å’Œæ¨ç†ç—•è¿¹ã€‚æ¥ç€ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å†·å¯åŠ¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å…·æœ‰å¤šç§ä¸“é—¨è®¾è®¡çš„å¥–åŠ±ï¼Œå¯ä»¥å…±åŒé¼“åŠ±ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€æ—¶é—´å¯¹é½æ€§å’Œç©ºé—´ç²¾åº¦ã€‚åœ¨V-STARåŸºå‡†æµ‹è¯•ä¸­ï¼ŒOpen-o3 Videoè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œå°†Qwen2.5-VLåŸºå‡†çš„mAMæé«˜äº†14.4%ï¼ŒmLGMæé«˜äº†24.2%ã€‚åœ¨å¹¿æ³›çš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŒ…æ‹¬VideoMMEã€Worldsenseã€VideoMMMUå’ŒTVGBenchç­‰ï¼Œä¹Ÿè§‚å¯Ÿåˆ°äº†ä¸€è‡´çš„æ”¹è¿›ã€‚é™¤äº†å‡†ç¡®æ€§ä¹‹å¤–ï¼ŒOpen-o3 Videoäº§ç”Ÿçš„æ¨ç†ç—•è¿¹è¿˜ä¸ºæµ‹è¯•æ—¶çš„ç¼©æ”¾æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡å·ï¼Œèƒ½å¤Ÿå®ç°ä¿¡å¿ƒæ„ŸçŸ¥éªŒè¯å¹¶æé«˜å¯¹ç­”æ¡ˆçš„å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20579v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Open-o3 Videoæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ˜ç¡®çš„æ—¶ç©ºè¯æ®èå…¥è§†é¢‘æ¨ç†ä¸­ã€‚ä¸ºè§£å†³åœ¨åŠ¨æ€åœºæ™¯ä¸­è”åˆæ—¶é—´è¿½è¸ªå’Œç©ºé—´å®šä½çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†éä»£ç†æ–¹å¼ï¼Œå¹¶æ”¶é›†äº†è®­ç»ƒæ•°æ®ã€è®¾è®¡äº†è®­ç»ƒç­–ç•¥ã€‚æ¨¡å‹åœ¨å›ç­”é—®é¢˜æ—¶ï¼Œèƒ½é«˜äº®å…³é”®çš„æ—¶é—´æˆ³ã€å¯¹è±¡å’Œè¾¹ç•Œæ¡†ï¼Œä½¿æ¨ç†åŸºäºå…·ä½“çš„è§†è§‰è§‚å¯Ÿã€‚ä¸ºæ”¯æŒæ­¤åŠŸèƒ½ï¼Œæ„å»ºäº†STGR-CoT-30kå’ŒSTGR-RL-36kä¸¤ä¸ªé«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«ç²¾å¿ƒæ„å»ºçš„æ—¶ç©ºæ ‡æ³¨ã€‚é‡‡ç”¨å†·å¯åŠ¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å¤šé¡¹ä¸“é—¨è®¾è®¡çš„å¥–åŠ±æ¥é¼“åŠ±ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€æ—¶é—´å¯¹é½æ€§å’Œç©ºé—´ç²¾åº¦ã€‚åœ¨V-STARåŸºå‡†æµ‹è¯•ä¸­ï¼ŒOpen-o3 Videoå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæé«˜äº†mAMå’ŒmLGMçš„å¾—åˆ†ã€‚åŒæ—¶ï¼Œåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè§‚å¯Ÿåˆ°äº†ä¸€è‡´çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒOpen-o3 Videoäº§ç”Ÿçš„æ¨ç†è½¨è¿¹è¿˜ä¸ºæµ‹è¯•æ—¶çš„ç¼©æ”¾æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡å·ï¼Œæé«˜äº†ç­”æ¡ˆçš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Open-o3 Videoæ˜¯ä¸€ä¸ªéä»£ç†æ¡†æ¶ï¼Œå°†æ˜ç¡®çš„æ—¶ç©ºè¯æ®èå…¥è§†é¢‘æ¨ç†ä¸­ã€‚</li>
<li>æ¨¡å‹è§£å†³äº†åœ¨åŠ¨æ€åœºæ™¯ä¸­è¿›è¡Œè”åˆæ—¶é—´è¿½è¸ªå’Œç©ºé—´å®šä½çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡æ„å»ºSTGR-CoT-30kå’ŒSTGR-RL-36kä¸¤ä¸ªæ•°æ®é›†æ¥å®ç°æ—¶ç©ºæ ‡æ³¨çš„ç»Ÿä¸€ç›‘ç£ã€‚</li>
<li>é‡‡ç”¨å†·å¯åŠ¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡å¤šé¡¹å¥–åŠ±é¼“åŠ±ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€æ—¶é—´å¯¹é½æ€§å’Œç©ºé—´ç²¾åº¦ã€‚</li>
<li>åœ¨V-STARåŸºå‡†æµ‹è¯•ä¸­ï¼ŒOpen-o3 Videoå–å¾—äº†æœ€ä½³æ€§èƒ½è¡¨ç°ã€‚</li>
<li>Open-o3 Videoåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæ”¹å–„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7af2ece0f765a8f3ea63b3a84318b435~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328714&auth_key=1761328714-0-0-cbc08af9aef59d6477eee5facd9fc6f3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5dd0ed1e1dee0d393c00afc77cc1be0~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328722&auth_key=1761328722-0-0-d05bdcf6cde0c64197aa7c1ca4a9d4b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a730a8196a472b65ab0edf21c238536b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328729&auth_key=1761328729-0-0-22639415448a5ff31e10fdf6ff3e6b67&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-096bc002092bb9f1d7c3e5b151a6e973~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328736&auth_key=1761328736-0-0-3f89132f363ccb1b664c8d4fe5b8861b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EmbodiedBrain-Expanding-Performance-Boundaries-of-Task-Planning-for-Embodied-Intelligence"><a href="#EmbodiedBrain-Expanding-Performance-Boundaries-of-Task-Planning-for-Embodied-Intelligence" class="headerlink" title="EmbodiedBrain: Expanding Performance Boundaries of Task Planning for   Embodied Intelligence"></a>EmbodiedBrain: Expanding Performance Boundaries of Task Planning for   Embodied Intelligence</h2><p><strong>Authors:Ding Zou, Feifan Wang, Mengyu Ge, Siyuan Fan, Zongbing Zhang, Wei Chen, Lingfeng Wang, Zhongyou Hu, Wenrui Yan, Zhengwei Gao, Hao Wang, Weizhao Jin, Yu Zhang, Hainan Zhao, Mingliang Zhang, Xianxian Xi, Yaru Zhang, Wenyuan Li, Zhengguang Gao, Yurui Zhu</strong></p>
<p>The realization of Artificial General Intelligence (AGI) necessitates Embodied AI agents capable of robust spatial perception, effective task planning, and adaptive execution in physical environments. However, current large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks suffer from key limitations, including a significant gap between model design and agent requirements, an unavoidable trade-off between real-time latency and performance, and the use of unauthentic, offline evaluation metrics. To address these challenges, we propose EmbodiedBrain, a novel vision-language foundation model available in both 7B and 32B parameter sizes. Our framework features an agent-aligned data structure and employs a powerful training methodology that integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group Relative Policy Optimization (Step-GRPO), which boosts long-horizon task success by integrating preceding steps as Guided Precursors. Furthermore, we incorporate a comprehensive reward system, including a Generative Reward Model (GRM) accelerated at the infrastructure level, to improve training efficiency. For enable thorough validation, we establish a three-part evaluation system encompassing General, Planning, and End-to-End Simulation Benchmarks, highlighted by the proposal and open-sourcing of a novel, challenging simulation environment. Experimental results demonstrate that EmbodiedBrain achieves superior performance across all metrics, establishing a new state-of-the-art for embodied foundation models. Towards paving the way for the next generation of generalist embodied agents, we open-source all of our data, model weight, and evaluating methods, which are available at <a target="_blank" rel="noopener" href="https://zterobot.github.io/EmbodiedBrain.github.io">https://zterobot.github.io/EmbodiedBrain.github.io</a>. </p>
<blockquote>
<p>å®ç°äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰éœ€è¦å…·æœ‰å¼ºå¥çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€æœ‰æ•ˆçš„ä»»åŠ¡è§„åˆ’èƒ½åŠ›ä»¥åŠåœ¨ç‰©ç†ç¯å¢ƒä¸­è¿›è¡Œè‡ªé€‚åº”æ‰§è¡Œèƒ½åŠ›çš„å®ä½“äººå·¥æ™ºèƒ½ä»£ç†ã€‚ç„¶è€Œï¼Œå½“å‰ç”¨äºå®ä½“ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å­˜åœ¨å…³é”®å±€é™ï¼ŒåŒ…æ‹¬æ¨¡å‹è®¾è®¡ä¸ä»£ç†è¦æ±‚ä¹‹é—´çš„é‡å¤§å·®è·ã€å®æ—¶å»¶è¿Ÿä¸æ€§èƒ½ä¹‹é—´ä¸å¯é¿å…çš„æƒè¡¡ï¼Œä»¥åŠä½¿ç”¨ä¸çœŸå®ã€ç¦»çº¿è¯„ä¼°æŒ‡æ ‡çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†EmbodiedBrainï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œæœ‰7Bå’Œ32Bä¸¤ç§å‚æ•°è§„æ¨¡å¯ä¾›é€‰æ‹©ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨ä¸ä»£ç†å¯¹é½çš„æ•°æ®ç»“æ„ï¼Œå¹¶é‡‡ç”¨å¼ºå¤§çš„è®­ç»ƒæ–¹æ³•ï¼Œå°†å¤§è§„æ¨¡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åˆ†æ­¥å¢å¼ºç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆStep-GRPOï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡å¼•å¯¼å‰é©±å°†é•¿æœŸä»»åŠ¡æˆåŠŸæ•´åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬èå…¥äº†ä¸€ä¸ªå…¨é¢çš„å¥–åŠ±ç³»ç»Ÿï¼ŒåŒ…æ‹¬åœ¨åŸºç¡€è®¾æ–½å±‚é¢åŠ é€Ÿçš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚ä¸ºäº†è¿›è¡Œå…¨é¢çš„éªŒè¯ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸‰é¡¹è¯„ä¼°ç³»ç»Ÿï¼ŒåŒ…æ‹¬é€šç”¨åŸºå‡†æµ‹è¯•ã€è§„åˆ’åŸºå‡†æµ‹è¯•å’Œç«¯åˆ°ç«¯ä»¿çœŸåŸºå‡†æµ‹è¯•ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»¿çœŸç¯å¢ƒå¹¶è¿›è¡Œå¼€æºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEmbodiedBrainåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºå®ä½“åŸºç¡€æ¨¡å‹å»ºç«‹äº†æ–°çš„æœ€æ–°æ°´å¹³ã€‚æˆ‘ä»¬æ—¨åœ¨ä¸ºæ­¤ä½œä¸ºä¸‹ä¸€ä»£ä¸“å®¶å®ä½“ä»£ç†çš„é“ºå¹³é“è·¯ï¼Œä¸ºæ­¤æˆ‘ä»¬å¼€æºæ‰€æœ‰çš„æ•°æ®ã€æ¨¡å‹æƒé‡å’Œè¯„ä¼°æ–¹æ³•ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://zterobot.github.io/EmbodiedBrain%20%E3%80%82github%E3%80%82io%E8%8E%B7%E5%8F%96%E3%80%82">https://zterobot.github.io/EmbodiedBrain.github.ioè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20578v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½çš„å®ç°éœ€è¦å…·æœ‰å¼ºå¤§çš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ã€æœ‰æ•ˆçš„ä»»åŠ¡è§„åˆ’èƒ½åŠ›å’Œé€‚åº”ç‰©ç†ç¯å¢ƒæ‰§è¡Œèƒ½åŠ›çš„åµŒå…¥å¼AIä»£ç†ã€‚é’ˆå¯¹å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åµŒå…¥å¼ä»»åŠ¡æ–¹é¢çš„å±€é™æ€§ï¼Œå¦‚æ¨¡å‹è®¾è®¡ä¸ä»£ç†éœ€æ±‚ä¹‹é—´çš„é¸¿æ²Ÿã€å®æ—¶å»¶è¿Ÿä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ä»¥åŠä½¿ç”¨ä¸çœŸå®çš„ç¦»çº¿è¯„ä¼°æŒ‡æ ‡ç­‰é—®é¢˜ï¼Œæå‡ºäº†EmbodiedBrainè¿™ä¸€æ–°é¢–çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸ä»£ç†å¯¹é½çš„æ•°æ®ç»“æ„ï¼Œç»“åˆå¤§è§„æ¨¡ç›‘ç£å¾®è°ƒä¸æ­¥éª¤å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶èå…¥å…¨é¢çš„å¥–åŠ±ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç”Ÿæˆå¥–åŠ±æ¨¡å‹ã€‚ä¸ºè¿›è¡Œå…¨é¢éªŒè¯ï¼Œå»ºç«‹äº†åŒ…æ‹¬é€šç”¨ã€è§„åˆ’å’Œç«¯åˆ°ç«¯ä»¿çœŸåŸºå‡†åœ¨å†…çš„ä¸‰éƒ¨åˆ†è¯„ä¼°ç³»ç»Ÿï¼Œå¹¶å…¬å¼€æå‡ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°ä»¿çœŸç¯å¢ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒEmbodiedBrainåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡å–å¾—äº†ä¼˜è¶Šæ€§èƒ½ï¼Œä¸ºåµŒå…¥å¼åŸºç¡€æ¨¡å‹å»ºç«‹äº†æ–°çš„å…ˆè¿›æŠ€æœ¯æ ‡å‡†ã€‚æˆ‘ä»¬å…¬å¼€äº†æ‰€æœ‰æ•°æ®ã€æ¨¡å‹æƒé‡å’Œè¯„ä¼°æ–¹æ³•ï¼Œä»¥æ¨åŠ¨ä¸‹ä¸€ä»£é€šç”¨åµŒå…¥å¼ä»£ç†çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åµŒå…¥å¼AIä»£ç†æ˜¯å®ç°äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½çš„å…³é”®ï¼Œéœ€è¦å¼ºå¤§çš„ç©ºé—´æ„ŸçŸ¥ã€ä»»åŠ¡è§„åˆ’å’Œç‰©ç†ç¯å¢ƒæ‰§è¡Œèƒ½åŠ›ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åµŒå…¥å¼ä»»åŠ¡æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼ŒåŒ…æ‹¬æ¨¡å‹è®¾è®¡ä¸ä»£ç†éœ€æ±‚çš„å·®è·ã€å®æ—¶æ€§èƒ½ä¸å»¶è¿Ÿçš„æƒè¡¡ä»¥åŠä½¿ç”¨ç¦»çº¿è¯„ä¼°æŒ‡æ ‡çš„å±€é™æ€§ã€‚</li>
<li>EmbodiedBrainæ˜¯ä¸€ä¸ªæ–°é¢–çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæä¾›ä¸¤ç§å‚æ•°è§„æ¨¡é€‰æ‹©ï¼ˆ7Bå’Œ32Bï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨ä¸ä»£ç†å¯¹é½çš„æ•°æ®ç»“æ„ï¼Œç»“åˆå¤§è§„æ¨¡ç›‘ç£å¾®è°ƒä¸æ­¥éª¤å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>ç»¼åˆå¥–åŠ±ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ä¸ºå…¨é¢éªŒè¯æ¨¡å‹æ€§èƒ½ï¼Œå»ºç«‹äº†åŒ…æ‹¬é€šç”¨ã€è§„åˆ’å’Œç«¯åˆ°ç«¯ä»¿çœŸåŸºå‡†çš„ä¸‰éƒ¨åˆ†è¯„ä¼°ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d440250f5d60e11c706283814132a922~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328744&auth_key=1761328744-0-0-458a03c7f29ab53115498008f57bf484&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b1980083668ea253d766f2804a266f6e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328752&auth_key=1761328752-0-0-afaaf8e63238ae512427e8605725cedd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c940d1dffeddc3b4f3bb2f490856fc10~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328759&auth_key=1761328759-0-0-cc461d8d2e87f53b23e402f0b79070c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e4b15d7827a0f28139046fcf83426f87~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328766&auth_key=1761328766-0-0-bd9a330cc4908c26b6e37163c05491b5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GlobalRAG-Enhancing-Global-Reasoning-in-Multi-hop-Question-Answering-via-Reinforcement-Learning"><a href="#GlobalRAG-Enhancing-Global-Reasoning-in-Multi-hop-Question-Answering-via-Reinforcement-Learning" class="headerlink" title="GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering   via Reinforcement Learning"></a>GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering   via Reinforcement Learning</h2><p><strong>Authors:Jinchang Luo, Mingquan Cheng, Fan Wan, Ni Li, Xiaoling Xia, Shuangshuang Tian, Tingcheng Bian, Haiwei Wang, Haohuan Fu, Yan Tao</strong></p>
<p>Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ åœ¨æ”¹è¿›æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹é¢æœ€è¿‘æ˜¾ç¤ºå‡ºå‰æ™¯ã€‚å°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼Œå…¶åœ¨å¤šè·³é—®ç­”ï¼ˆQAï¼‰ä¸­çš„æœ‰æ•ˆæ€§ä»å—åˆ°ä¸¤ä¸ªåŸºæœ¬é™åˆ¶çš„å½±å“ï¼šï¼ˆiï¼‰ç¼ºä¹å…¨å±€è§„åˆ’æ¥æ„å»ºå¤šæ­¥éª¤æ¨ç†ï¼Œä»¥åŠï¼ˆiiï¼‰æ‰§è¡Œä¸å¿ å®ï¼Œè¿™é˜»ç¢äº†æœ‰æ•ˆæŸ¥è¯¢çš„å½¢æˆå’Œæ£€ç´¢è¯æ®çš„è¿ç»­ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†GlobalRAGï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šè·³QAä¸­çš„å…¨å±€æ¨ç†ã€‚GlobalRAGå°†é—®é¢˜åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œåè°ƒæ£€ç´¢ä¸æ¨ç†ï¼Œå¹¶è¿­ä»£åœ°å®Œå–„è¯æ®ã€‚ä¸ºäº†å¼•å¯¼è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§„åˆ’è´¨é‡å¥–åŠ±å’Œå­ç›®æ ‡å®Œæˆå¥–åŠ±ï¼Œä»¥é¼“åŠ±è¿è´¯çš„è§„åˆ’å’Œå¯é çš„å­ç›®æ ‡æ‰§è¡Œã€‚æ­¤å¤–ï¼Œæ¸è¿›çš„æƒé‡é€€ç«ç­–ç•¥å¹³è¡¡äº†é¢å‘è¿‡ç¨‹å’ŒåŸºäºç»“æœçš„ç›®æ ‡ã€‚åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGlobalRAGåœ¨ä»…ä½¿ç”¨8kè®­ç»ƒæ•°æ®ï¼ˆä»…å å¼ºåŸºå‡†æµ‹è¯•æ‰€ç”¨è®­ç»ƒæ•°æ®çš„42%ï¼‰çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºå¼ºåŸºå‡†æµ‹è¯•ï¼Œåœ¨EMå’ŒF1æ–¹é¢éƒ½å¹³å‡æé«˜äº†14.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20548v1">PDF</a> 8 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æ”¹è¿›æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤šè·³é—®ç­”ï¼ˆQAï¼‰ä¸­çš„æ•ˆæœæœ‰é™ï¼Œä¸»è¦ä½“ç°åœ¨ç¼ºä¹å…¨å±€è§„åˆ’å’Œä¸å¿ å®çš„æ‰§è¡Œä¸¤ä¸ªæ–¹é¢ã€‚æˆ‘ä»¬æå‡ºäº†GlobalRAGï¼Œä¸€ä¸ªæ—¨åœ¨å¢å¼ºå¤šè·³é—®ç­”ä¸­å…¨å±€æ¨ç†çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚GlobalRAGå°†é—®é¢˜åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œåè°ƒæ£€ç´¢ä¸æ¨ç†ï¼Œå¹¶è¿­ä»£ä¼˜åŒ–è¯æ®ã€‚é€šè¿‡å¼•å…¥è§„åˆ’è´¨é‡å¥–åŠ±å’Œå­ç›®æ ‡å®Œæˆå¥–åŠ±ï¼Œé¼“åŠ±è¿è´¯çš„è§„åˆ’å’Œå¯é çš„å­ç›®æ ‡æ‰§è¡Œã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ¸è¿›æƒé‡é€€ç«ç­–ç•¥å¹³è¡¡è¿‡ç¨‹å¯¼å‘å’Œç»“æœå¯¼å‘çš„ç›®æ ‡ã€‚å®éªŒè¡¨æ˜ï¼ŒGlobalRAGåœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šéƒ½æ˜¾è‘—ä¼˜äºå¼ºåŸºçº¿ï¼Œä¸”ä»…ä½¿ç”¨8kè®­ç»ƒæ•°æ®ï¼ˆä¸ºå¼ºåŸºçº¿æ‰€ç”¨æ•°æ®çš„42%ï¼‰ï¼Œåœ¨EMå’ŒF1ä¸Šå¹³å‡æé«˜14.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æ”¹è¿›æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸­æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>å¤šè·³é—®ç­”ï¼ˆQAï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­é¢ä¸´å…¨å±€è§„åˆ’ç¼ºå¤±å’Œä¸å¿ å®æ‰§è¡Œçš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºGlobalRAGæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šè·³é—®ç­”ä¸­çš„å…¨å±€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GlobalRAGé€šè¿‡åˆ†è§£é—®é¢˜ä¸ºå­ç›®æ ‡ï¼Œåè°ƒæ£€ç´¢ä¸æ¨ç†ï¼Œå¹¶è¿­ä»£ä¼˜åŒ–è¯æ®ã€‚</li>
<li>å¼•å…¥è§„åˆ’è´¨é‡å¥–åŠ±å’Œå­ç›®æ ‡å®Œæˆå¥–åŠ±ï¼Œæé«˜è§„åˆ’å’Œæ‰§è¡Œçš„è¿è´¯æ€§å’Œå¯é æ€§ã€‚</li>
<li>é‡‡ç”¨æ¸è¿›æƒé‡é€€ç«ç­–ç•¥å¹³è¡¡è¿‡ç¨‹å¯¼å‘å’Œç»“æœå¯¼å‘çš„ç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f079538cd459c49ecd623b37d6fcfb0c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328773&auth_key=1761328773-0-0-2b34cc8351a3b9e0addd156dcea54af9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-90391d4afd493862c201d6a30a729977~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328780&auth_key=1761328780-0-0-26ffdebd0bb91b985399380afc60d4ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0fbb2bd98fe9d203a5182a3ff90833d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328787&auth_key=1761328787-0-0-4229139e21d0bdfd8dd4391570ab5cd7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence"><a href="#Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence" class="headerlink" title="Conan: Progressive Learning to Reason Like a Detective over Multi-Scale   Visual Evidence"></a>Conan: Progressive Learning to Reason Like a Detective over Multi-Scale   Visual Evidence</h2><p><strong>Authors:Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun</strong></p>
<p>Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding but still struggle with inaccurate evidence localization. To address these challenges, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies contextual and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we (1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that includes frame identification, evidence reasoning, and action decision, and (2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to jointly enhance multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long-video understanding tasks, validating its strong scalability and robustness. </p>
<blockquote>
<p>è§†é¢‘æ¨ç†åœ¨å¤šå¸§ä¹‹é—´è¿›è¡Œå¤šæ­¥éª¤æ¨æ–­ï¼Œå¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºåªåŒ…å«æ–‡æœ¬çš„é“¾æ¡ï¼Œä»è€Œäº§ç”Ÿæ— æ ¹æ®æˆ–è™šæ„çš„ç»“è®ºã€‚ç›¸åï¼ŒåŸºäºå¸§æ£€ç´¢çš„æ–¹æ³•å¼•å…¥äº†è§†è§‰å®šä½ï¼Œä½†ä»ç„¶é¢ä¸´ä¸å‡†ç¡®è¯æ®å®šä½çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Conanï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè¯æ®çš„å¤šæ­¥éª¤è§†é¢‘æ¨ç†æ¡†æ¶ã€‚Conanèƒ½å¤Ÿè¯†åˆ«ä¸Šä¸‹æ–‡å’Œè¯æ®å¸§ï¼Œå¯¹è·¨å¸§çº¿ç´¢è¿›è¡Œæ¨ç†ï¼Œå¹¶è‡ªé€‚åº”åœ°å†³å®šä½•æ—¶å¾—å‡ºç»“è®ºæˆ–è¿›ä¸€æ­¥æ¢ç´¢ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ï¼ˆ1ï¼‰æ„å»ºäº†Conan-91Kï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„æ¨ç†è½¨è¿¹å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…æ‹¬å¸§è¯†åˆ«ã€è¯æ®æ¨ç†å’ŒåŠ¨ä½œå†³ç­–ï¼Œï¼ˆ2ï¼‰è®¾è®¡äº†ä¸€ä¸ªå¤šé˜¶æ®µæ¸è¿›çš„å†·å¯åŠ¨ç­–ç•¥ï¼Œç»“åˆè¯†åˆ«-æ¨ç†-åŠ¨ä½œï¼ˆAIRï¼‰RLVRè®­ç»ƒæ¡†æ¶ï¼Œä»¥å…±åŒæé«˜å¤šæ­¥éª¤è§†è§‰æ¨ç†èƒ½åŠ›ã€‚åœ¨å…­ä¸ªå¤šæ­¥éª¤æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒConanåœ¨å‡†ç¡®æ€§ä¸Šå¹³å‡è¶…å‡ºåŸºçº¿Qwen2.5-VL-7B-Instructè¶…è¿‡10%ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒConanåœ¨é•¿æ—¶é—´è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ä¹Ÿèƒ½æœ‰æ•ˆæ¨å¹¿ï¼ŒéªŒè¯äº†å…¶å¼ºå¤§çš„å¯æ‰©å±•æ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20470v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘æ¨ç†å¯¹äºè·¨å¸§å¤šæ­¥æ¨ç†çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è™½ç„¶å¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†å¾€å¾€ä¾èµ–äºæ–‡æœ¬é“¾è€Œäº§ç”Ÿæ— æ ¹æ®æˆ–è™šæ„çš„ç»“è®ºã€‚è€Œå¸§æ£€ç´¢æ–¹æ³•å¼•å…¥äº†è§†è§‰å®šä½ï¼Œä½†ä»å­˜åœ¨å®šä½ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Conanæ¡†æ¶ï¼Œç”¨äºè¯æ®æ”¯æ’‘çš„å¤šæ­¥è§†é¢‘æ¨ç†ã€‚Conanèƒ½å¤Ÿè¯†åˆ«ä¸Šä¸‹æ–‡å’Œè¯æ®å¸§ï¼Œæ¨ç†è·¨å¸§çº¿ç´¢ï¼Œå¹¶è‡ªé€‚åº”åœ°å†³å®šä½•æ—¶å¾—å‡ºç»“è®ºæˆ–è¿›ä¸€æ­¥æ¢ç´¢ã€‚ä¸ºè¾¾åˆ°æ­¤ç›®çš„ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Conan-91Kï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå¤šé˜¶æ®µçš„æ¸è¿›å¼å†·å¯åŠ¨ç­–ç•¥ï¼Œç»“åˆIdentification-Reasoning-Actionï¼ˆAIRï¼‰RLVRè®­ç»ƒæ¡†æ¶ï¼Œå…±åŒæå‡å¤šæ­¥è§†è§‰æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒConanåœ¨å¤šä¸ªå¤šæ­¥æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡äº†åŸºå‡†æ¨¡å‹Qwen2.5-VL-7B-Instructï¼Œè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œå¹¶æœ‰æ•ˆæ³›åŒ–åˆ°é•¿è§†é¢‘ç†è§£ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ¨ç†å¯¹è·¨å¸§å¤šæ­¥æ¨ç†æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯èƒ½äº§ç”Ÿæ— æ ¹æ®æˆ–è™šæ„çš„ç»“è®ºã€‚</li>
<li>Conanæ¡†æ¶ç”¨äºè¯æ®æ”¯æ’‘çš„å¤šæ­¥è§†é¢‘æ¨ç†ï¼Œèƒ½è¯†åˆ«ä¸Šä¸‹æ–‡å’Œè¯æ®å¸§ï¼Œæ¨ç†è·¨å¸§çº¿ç´¢ã€‚</li>
<li>Conanæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Conan-91Kç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>Conané‡‡ç”¨å¤šé˜¶æ®µçš„æ¸è¿›å¼å†·å¯åŠ¨ç­–ç•¥å’ŒAIR RLVRè®­ç»ƒæ¡†æ¶æå‡å¤šæ­¥è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Conanåœ¨å¤šä¸ªå¤šæ­¥æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡ç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d115c0dec6dfa66458cbe19ec24f0f59~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328794&auth_key=1761328794-0-0-cea2c02a74f4c145dfe0a25e4db7ff13&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9b6dcae9204feafb394f22210650408f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328802&auth_key=1761328802-0-0-15e709e34f89373c29e606581e99bf22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LM-mixup-Text-Data-Augmentation-via-Language-Model-based-Mixup"><a href="#LM-mixup-Text-Data-Augmentation-via-Language-Model-based-Mixup" class="headerlink" title="LM-mixup: Text Data Augmentation via Language Model based Mixup"></a>LM-mixup: Text Data Augmentation via Language Model based Mixup</h2><p><strong>Authors:Zhijie Deng, Zhouan Shen, Ling Li, Yao Zhou, Zhaowei Zhu, Yanji He, Wei Wang, Jiaheng Wei</strong></p>
<p>Instruction tuning is crucial for aligning Large Language Models (LLMs), yet the quality of instruction-following data varies significantly. While high-quality data is paramount, it is often scarce; conversely, abundant low-quality data is frequently discarded, leading to substantial information loss. Existing data augmentation methods struggle to augment this low-quality data effectively, and the evaluation of such techniques remains poorly defined. To address this, we formally define the task of Instruction Distillation: distilling multiple low-quality and redundant inputs into high-quality and coherent instruction-output pairs. Specifically, we introduce a comprehensive data construction pipeline to create MIXTURE, a 144K-sample dataset pairing low-quality or semantically redundant imperfect instruction clusters with their high-quality distillations. We then introduce LM-Mixup, by first performing supervised fine-tuning on MIXTURE and then optimizing it with reinforcement learning. This process uses three complementary reward signals: quality, semantic alignment, and format compliance, via Group Relative Policy Optimization (GRPO). We demonstrate that LM-Mixup effectively augments imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for only about 3% of the entire dataset, not only surpasses full-dataset training but also competes with state-of-the-art high-quality data selection methods across multiple benchmarks. Our work establishes that low-quality data is a valuable resource when properly distilled and augmented with LM-Mixup, significantly enhancing the efficiency and performance of instruction-tuned LLMs. </p>
<blockquote>
<p>æŒ‡ä»¤è°ƒæ•´å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œç„¶è€ŒæŒ‡ä»¤éµå¾ªæ•°æ®çš„è´¨é‡å·®å¼‚å¾ˆå¤§ã€‚è™½ç„¶é«˜è´¨é‡æ•°æ®è‡³å…³é‡è¦ï¼Œä½†å®ƒå¾€å¾€å¾ˆç¨€ç¼ºï¼›ç›¸åï¼Œå¤§é‡ä½è´¨é‡æ•°æ®ç»å¸¸è¢«ä¸¢å¼ƒï¼Œå¯¼è‡´å¤§é‡ä¿¡æ¯ä¸¢å¤±ã€‚ç°æœ‰çš„æ•°æ®å¢å¼ºæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°å¢å¼ºè¿™ç§ä½è´¨é‡æ•°æ®ï¼Œå¹¶ä¸”å¯¹æ­¤ç±»æŠ€æœ¯çš„è¯„ä¼°å®šä¹‰ä»ç„¶ä¸æ˜ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ­£å¼å®šä¹‰äº†æŒ‡ä»¤è’¸é¦çš„ä»»åŠ¡ï¼šå°†å¤šä¸ªä½è´¨é‡å’Œå†—ä½™çš„è¾“å…¥è’¸é¦æˆé«˜è´¨é‡å’Œè¿è´¯çš„æŒ‡ä»¤-è¾“å‡ºå¯¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ„å»ºæµç¨‹æ¥åˆ›å»ºMIXTUREï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰144Kæ ·æœ¬çš„æ•°æ®é›†ï¼Œå°†ä½è´¨é‡æˆ–è¯­ä¹‰ä¸Šå†—ä½™çš„ä¸å®Œç¾æŒ‡ä»¤é›†ç¾¤ä¸å…¶é«˜è´¨é‡è’¸é¦é…å¯¹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†LM-Mixupï¼Œé¦–å…ˆä½¿ç”¨MIXTUREè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä¸€è¿‡ç¨‹ä½¿ç”¨ä¸‰ç§äº’è¡¥çš„å¥–åŠ±ä¿¡å·ï¼šè´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œæ ¼å¼åˆè§„æ€§ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚æˆ‘ä»¬è¯æ˜LM-Mixupå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºä¸å®Œç¾çš„æ•°æ®é›†ï¼šåœ¨è’¸é¦æ•°æ®ä¸Šå¾®è°ƒLLMï¼Œè¿™éƒ¨åˆ†æ•°æ®åªå æ•´ä¸ªæ•°æ®é›†çš„çº¦3%ï¼Œä¸ä»…è¶…è¶Šäº†å…¨æ•°æ®é›†çš„è®­ç»ƒï¼Œè€Œä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸æœ€æ–°çš„é«˜è´¨é‡æ•°æ®é€‰æ‹©æ–¹æ³•ç›¸ç«äº‰ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œå½“é€‚å½“è’¸é¦å’Œç”¨LM-Mixupå¢å¼ºæ—¶ï¼Œä½è´¨é‡æ•°æ®æ˜¯ä¸€ç§æœ‰ä»·å€¼çš„èµ„æºï¼Œèƒ½æ˜¾è‘—æé«˜æŒ‡ä»¤è°ƒæ•´å‹LLMçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20449v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºæŒ‡ä»¤è’¸é¦ä»»åŠ¡ï¼Œæ—¨åœ¨å°†å¤§é‡ä½è´¨é‡å’Œå†—ä½™è¾“å…¥è½¬åŒ–ä¸ºé«˜è´¨é‡ã€è¿è´¯çš„æŒ‡ä»¤è¾“å‡ºå¯¹ã€‚ä¸ºæ­¤ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ„å»ºç®¡é“ï¼Œæ„å»ºäº†åŒ…å«144Kæ ·æœ¬çš„MIXTUREæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å°†ä½è´¨é‡æˆ–è¯­ä¹‰å†—ä½™çš„ä¸å®Œç¾æŒ‡ä»¤é›†ç¾¤ä¸å…¶é«˜è´¨é‡è’¸é¦æ•°æ®é…å¯¹ã€‚æ¥ç€ï¼Œæ–‡ç« æå‡ºäº†LM-Mixupæ–¹æ³•ï¼Œé¦–å…ˆä½¿ç”¨MIXTUREæ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸‰ç§äº’è¡¥å¥–åŠ±ä¿¡å·ï¼šè´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œæ ¼å¼åˆè§„æ€§ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLM-Mixupèƒ½æœ‰æ•ˆå¢å¼ºä¸å®Œç¾çš„æ•°æ®é›†ï¼šå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè’¸é¦æ•°æ®å¾®è°ƒï¼Œä»…ä½¿ç”¨çº¦3%çš„æ•°æ®ä¾¿è¶…è¿‡äº†å…¨æ•°æ®é›†è®­ç»ƒçš„æ•ˆæœï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸é«˜è´¨é‡æ•°æ®é€‰æ‹©æ–¹æ³•ç«äº‰ã€‚æ–‡ç« è¡¨æ˜ï¼Œä½è´¨é‡æ•°æ®åœ¨é€‚å½“è’¸é¦å’Œä¸LM-Mixupç»“åˆå¢å¼ºåï¼Œå¯æˆä¸ºæœ‰ä»·å€¼çš„èµ„æºï¼Œæ˜¾è‘—æé«˜æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤è’¸é¦æ˜¯å°†ä½è´¨é‡å’Œå†—ä½™æ•°æ®è½¬åŒ–ä¸ºé«˜è´¨é‡æ•°æ®çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>åˆ›å»ºäº†MIXTUREæ•°æ®é›†ï¼Œç”¨äºé…å¯¹ä½è´¨é‡æŒ‡ä»¤ä¸é«˜è´¨é‡è’¸é¦æ•°æ®ã€‚</li>
<li>æå‡ºäº†LM-Mixupæ–¹æ³•ï¼Œç»“åˆäº†ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡ŒæŒ‡ä»¤è’¸é¦ä»»åŠ¡ã€‚</li>
<li>LM-Mixupåˆ©ç”¨ä¸‰ç§å¥–åŠ±ä¿¡å·è¿›è¡Œä¼˜åŒ–ï¼šè´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œæ ¼å¼åˆè§„æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨çº¦3%çš„è’¸é¦æ•°æ®å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒLM-Mixupè¶…è¶Šäº†å…¨æ•°æ®é›†è®­ç»ƒçš„æ•ˆæœã€‚</li>
<li>LM-Mixupåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¸é«˜è´¨é‡æ•°æ®é€‰æ‹©æ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4e7a1a871dd837140f02425045c6bbd1~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328809&auth_key=1761328809-0-0-37150ae87ff543a9f0760226a6b325d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a8f8a9d03f8725e63149b4658cad7118~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328817&auth_key=1761328817-0-0-3fce72dcc151002f8b0fbc87ff361c37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a4b237459a8e31c7976f1e82a75fd574~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328823&auth_key=1761328823-0-0-b34db252223871e3b387d1e9b28d15ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fe86cea0f90840ae720bc1b859841ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328830&auth_key=1761328830-0-0-138ecd69f9a2e2981b0d7d54a91a596d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Ask-a-Strong-LLM-Judge-when-Your-Reward-Model-is-Uncertain"><a href="#Ask-a-Strong-LLM-Judge-when-Your-Reward-Model-is-Uncertain" class="headerlink" title="Ask a Strong LLM Judge when Your Reward Model is Uncertain"></a>Ask a Strong LLM Judge when Your Reward Model is Uncertain</h2><p><strong>Authors:Zhenghao Xu, Qin Lu, Qingru Zhang, Liang Qiu, Ilgee Hong, Changlong Yu, Wenlin Yao, Yao Liu, Haoming Jiang, Lihong Li, Hyokun Yun, Tuo Zhao</strong></p>
<p>Reward model (RM) plays a pivotal role in reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs). However, classical RMs trained on human preferences are vulnerable to reward hacking and generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM judges equipped with reasoning capabilities demonstrate superior generalization, even without additional training, but incur significantly higher inference costs, limiting their applicability in online RLHF. In this work, we propose an uncertainty-based routing framework that efficiently complements a fast RM with a strong but costly LLM judge. Our approach formulates advantage estimation in policy gradient (PG) methods as pairwise preference classification, enabling principled uncertainty quantification to guide routing. Uncertain pairs are forwarded to the LLM judge, while confident ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our uncertainty-based routing strategy significantly outperforms random judge calling at the same cost, and downstream alignment results showcase its effectiveness in improving online RLHF. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨ç»“åˆäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¡å‡†èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼ŒåŸºäºäººç±»åå¥½è®­ç»ƒçš„ç»å…¸RMå®¹æ˜“å—åˆ°å¥–åŠ±æ”»å‡»ï¼Œå¯¹ç¦»åˆ†å¸ƒï¼ˆOODï¼‰è¾“å…¥çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé…å¤‡æ¨ç†èƒ½åŠ›çš„é«˜çº§LLMæ³•å®˜å³ä½¿æ²¡æœ‰é¢å¤–çš„è®­ç»ƒä¹Ÿèƒ½è¡¨ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†æ¨ç†æˆæœ¬æ˜¾è‘—è¾ƒé«˜ï¼Œé™åˆ¶äº†å…¶åœ¨åœ¨çº¿RLHFä¸­çš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„è·¯ç”±æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°ç”¨å¿«é€ŸRMé…åˆå¼ºå¤§ä½†æ˜‚è´µLLMæ³•å®˜è¿›è¡Œè¡¥å……ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æ–¹æ³•ä¸­çš„ä¼˜åŠ¿ä¼°è®¡åˆ¶å®šä¸ºé…å¯¹åå¥½åˆ†ç±»ï¼Œå®ç°æœ‰åŸåˆ™çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¥æŒ‡å¯¼è·¯ç”±ã€‚ä¸ç¡®å®šçš„å¯¹è¢«è½¬å‘ç»™LLMæ³•å®˜ï¼Œè€Œè‡ªä¿¡çš„è¢«RMè¯„ä¼°ã€‚åœ¨RMåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºä¸ç¡®å®šæ€§çš„è·¯ç”±ç­–ç•¥åœ¨ç›¸åŒæˆæœ¬ä¸‹æ˜¾è‘—ä¼˜äºéšæœºæ³•å®˜å‘¼å«ï¼Œä¸‹æ¸¸å¯¹é½ç»“æœå±•ç¤ºäº†å…¶åœ¨æ”¹è¿›åœ¨çº¿RLHFä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20369v1">PDF</a> NeurIPS 2025, 18 pages</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨ç»“åˆäººç±»åé¦ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼ŒåŸºäºäººç±»åå¥½çš„ç»å…¸RMå®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»ï¼Œå¯¹ç¦»ç¾¤è¾“å…¥ï¼ˆOODï¼‰çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé…å¤‡æ¨ç†èƒ½åŠ›çš„å¼ºå¤§LLMæ³•å®˜è¡¨ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå³ä½¿åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œä½†å®ƒä»¬ä¼šå¸¦æ¥æ›´é«˜çš„æ¨ç†æˆæœ¬ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„è·¯ç”±æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°ç”¨ä¸€ä¸ªå¿«é€Ÿçš„RMæ¥è¾…åŠ©å¼ºå¤§ä½†æˆæœ¬é«˜æ˜‚çš„LLMæ³•å®˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æ–¹æ³•ä¸­çš„ä¼˜åŠ¿ä¼°è®¡å…¬å¼åŒ–ä¸ºæˆå¯¹åå¥½åˆ†ç±»ï¼Œä»¥å®ç°æœ‰åŸåˆ™çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¥æŒ‡å¯¼è·¯ç”±ã€‚ä¸ç¡®å®šçš„å¯¹è¢«è½¬å‘åˆ°LLMæ³•å®˜è¿›è¡Œè£å†³ï¼Œè€Œç¡®å®šçš„åˆ™ç”±RMè¿›è¡Œè¯„ä¼°ã€‚åœ¨RMåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºä¸ç¡®å®šæ€§çš„è·¯ç”±ç­–ç•¥åœ¨ç›¸åŒæˆæœ¬ä¸‹æ˜¾è‘—ä¼˜äºéšæœºæ³•å®˜è°ƒç”¨ï¼Œä¸‹æ¸¸å¯¹é½ç»“æœå±•ç¤ºäº†å…¶åœ¨æ”¹è¿›åœ¨çº¿RLHFä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„å…³é”®ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨ç»“åˆäººç±»åé¦ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚</li>
<li>ç»å…¸RMçš„å±€é™æ€§ï¼šå®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»å’Œå¯¹ç¦»ç¾¤è¾“å…¥ï¼ˆOODï¼‰çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚</li>
<li>å¼ºå¤§LLMçš„ä¼˜è¶Šæ³›åŒ–èƒ½åŠ›ï¼Œé…å¤‡æ¨ç†èƒ½åŠ›ï¼Œä½†é«˜æ¨ç†æˆæœ¬é™åˆ¶äº†å…¶åœ¨åœ¨çº¿RLHFä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„è·¯ç”±æ¡†æ¶ï¼Œç»“åˆäº†å¿«é€ŸRMå’Œå¼ºå¤§LLMçš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å…¬å¼åŒ–ä¼˜åŠ¿ä¼°è®¡ä¸ºæˆå¯¹åå¥½åˆ†ç±»ï¼Œå®ç°æœ‰åŸåˆ™çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚</li>
<li>ä¸ç¡®å®šçš„å¯¹ç”±LLMæ³•å®˜è£å†³ï¼Œç¡®å®šçš„åˆ™ç”±RMè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-176f57a11a53d8fec821337f015ac418~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328837&auth_key=1761328837-0-0-4c55bd3385f7a1a6879985091f971a34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1a52d74c5bce688a83fb10190a465d40~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328844&auth_key=1761328844-0-0-dbe6ceba574284ddb516b4a41799a20e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8214fa04e85c9ac98305c0aad270b6ae~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328851&auth_key=1761328851-0-0-51c09ff8a761dd91b198507134784376&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Teaching-Language-Models-to-Reason-with-Tools"><a href="#Teaching-Language-Models-to-Reason-with-Tools" class="headerlink" title="Teaching Language Models to Reason with Tools"></a>Teaching Language Models to Reason with Tools</h2><p><strong>Authors:Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu</strong></p>
<p>Large reasoning models (LRMs) like OpenAI-o1 have shown impressive capabilities in natural language reasoning. However, these models frequently demonstrate inefficiencies or inaccuracies when tackling complex mathematical operations. While integrating computational tools such as Code Interpreters (CIs) offers a promising solution, it introduces a critical challenge: a conflict between the modelâ€™s internal, probabilistic reasoning and the external, deterministic knowledge provided by the CI, which often leads models to unproductive deliberation. To overcome this, we introduce CoRT (Code-Optimized Reasoning Training), a post-training framework designed to teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a new data synthesis strategy that strategically injects diverse hints at optimal points within reasoning paths. This approach generates high-quality, code-integrated reasoning data specifically tailored to optimize LRM-CI interaction. Using this method, we have synthesized 30 high-quality samples to post-train models ranging from 1.5B to 32B parameters through supervised fine-tuning. CoRT further refines the multi-round interleaving of external CI usage and internal thinking by employing rejection sampling and reinforcement learning. Our experimental evaluations demonstrate CoRTâ€™s effectiveness, yielding absolute improvements of 4% and 8% on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging mathematical reasoning datasets. Moreover, CoRT significantly enhances efficiency, reducing token usage by approximately 30% for the 32B model and 50% for the 1.5B model compared to pure natural language reasoning baselines. The models and code are available at: <a target="_blank" rel="noopener" href="https://github.com/ChengpengLi1003/CoRT">https://github.com/ChengpengLi1003/CoRT</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚OpenAI-o1ï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨è¿›è¡Œå¤æ‚çš„æ•°å­¦è¿ç®—æ—¶ï¼Œç»å¸¸è¡¨ç°å‡ºæ•ˆç‡ä½ä¸‹æˆ–ä¸å‡†ç¡®çš„é—®é¢˜ã€‚è™½ç„¶æ•´åˆè®¡ç®—å·¥å…·ï¼ˆå¦‚ä»£ç è§£é‡Šå™¨ï¼‰æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒå¸¦æ¥äº†ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¨¡å‹å†…éƒ¨çš„æ¦‚ç‡æ¨ç†ä¸å¤–éƒ¨ç”±ä»£ç è§£é‡Šå™¨æä¾›çš„ç¡®å®šæ€§çŸ¥è¯†ä¹‹é—´çš„å†²çªï¼Œè¿™å¸¸å¸¸å¯¼è‡´æ¨¡å‹é™·å…¥æ— æ•ˆçš„äº‰è®ºã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoRTï¼ˆä»£ç ä¼˜åŒ–æ¨ç†è®­ç»ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ•™æˆå¤§å‹æ¨ç†æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨ä»£ç è§£é‡Šå™¨çš„åè®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®åˆæˆç­–ç•¥â€”â€”æç¤ºå·¥ç¨‹ï¼ˆHint-Engineeringï¼‰ï¼Œè¯¥ç­–ç•¥åœ¨æ¨ç†è·¯å¾„çš„åˆé€‚ä½ç½®æ³¨å…¥å¤šæ ·åŒ–çš„æç¤ºã€‚è¿™ç§æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡ã€ä¸ä»£ç æ•´åˆçš„æ¨ç†æ•°æ®ï¼Œä¸“ä¸ºä¼˜åŒ–å¤§å‹æ¨ç†æ¨¡å‹ä¸ä»£ç è§£é‡Šå™¨çš„äº¤äº’è€Œè®¾è®¡ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬åˆæˆäº†30ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå¯¹å‚æ•°èŒƒå›´ä»1.5Båˆ°32Bçš„æ¨¡å‹è¿›è¡Œåè®­ç»ƒã€‚CoRTé€šè¿‡é‡‡ç”¨æ‹’ç»é‡‡æ ·å’Œå¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥æ”¹è¿›äº†å¤–éƒ¨ä»£ç è§£é‡Šå™¨çš„ä½¿ç”¨ä¸å†…éƒ¨æ€è€ƒçš„å¤šè½®äº¤æ›¿è¿›è¡Œã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¯æ˜äº†CoRTçš„æœ‰æ•ˆæ€§ï¼Œåœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼ŒDeepSeek-R1-Distill-Qwen-32Bå’ŒDeepSeek-R1-Distill-Qwen-1.5Båˆ†åˆ«å®ç°äº†4%å’Œ8%çš„ç»å¯¹æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒCoRTæ˜¾è‘—æé«˜äº†æ•ˆç‡ï¼Œç›¸è¾ƒäºçº¯è‡ªç„¶è¯­è¨€æ¨ç†åŸºçº¿ï¼Œä»¤ç‰Œä½¿ç”¨é‡å¤§çº¦å‡å°‘äº†30%ï¼ˆé’ˆå¯¹32Bæ¨¡å‹ï¼‰å’Œ50%ï¼ˆé’ˆå¯¹1.5Bæ¨¡å‹ï¼‰ã€‚æ¨¡å‹å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChengpengLi1003/CoRT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ChengpengLi1003/CoRTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20342v1">PDF</a> NIPS2025 Accepted</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚OpenAI-o1ï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ•°å­¦è¿ç®—æ—¶å¯èƒ½æ•ˆç‡ä½ä¸‹æˆ–ä¸å‡†ç¡®ã€‚ç»“åˆä»£ç è§£é‡Šå™¨ï¼ˆCIï¼‰ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†å¸Œæœ›ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼šæ¨¡å‹å†…éƒ¨çš„æ¦‚ç‡æ¨ç†ä¸å¤–éƒ¨CIæä¾›çš„ç¡®å®šæ€§çŸ¥è¯†ä¹‹é—´çš„å†²çªã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºCoRTï¼ˆä»£ç ä¼˜åŒ–æ¨ç†è®­ç»ƒï¼‰ï¼Œä¸€ä¸ªé’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹è®¾è®¡çš„åè®­ç»ƒæ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–å…¶ä¸ä»£ç è§£é‡Šå™¨çš„äº¤äº’ã€‚ä½¿ç”¨ä¸€ç§åä¸ºæç¤ºå·¥ç¨‹çš„æ–°æ•°æ®åˆæˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨æ¨ç†è·¯å¾„çš„å…³é”®ç‚¹æ³¨å…¥å¤šæ ·çš„æç¤ºï¼Œç”Ÿæˆä¸“é—¨é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹ä¼˜åŒ–çš„é«˜è´¨é‡æ•°æ®ã€‚é€šè¿‡å®éªŒè¯„ä¼°ï¼ŒCoRTåœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼Œå¹¶åœ¨æ•ˆç‡ä¸Šæœ‰æ‰€æå‡ã€‚æ¨¡å‹ä¸ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨æ•°å­¦è¿ç®—ä¸Šå¯èƒ½å­˜åœ¨æ•ˆç‡æˆ–å‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>ç»“åˆä»£ç è§£é‡Šå™¨ä¸ºè§£å†³å¤§å‹æ¨ç†æ¨¡å‹çš„æ•°å­¦è¿ç®—é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œä½†å¸¦æ¥äº†æ¨¡å‹å†…éƒ¨ä¸å¤–éƒ¨çŸ¥è¯†ä¹‹é—´çš„å†²çªæŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥CoRTæ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹æ¨ç†æ¨¡å‹ä¸ä»£ç è§£é‡Šå™¨çš„äº¤äº’ã€‚</li>
<li>æå‡ºæç¤ºå·¥ç¨‹æ•°æ®åˆæˆç­–ç•¥ï¼Œé€šè¿‡æ³¨å…¥å¤šæ ·æç¤ºç”Ÿæˆé’ˆå¯¹æ¨¡å‹ä¼˜åŒ–çš„æ•°æ®ã€‚</li>
<li>CoRTæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„æ•ˆæœï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æ€§èƒ½æå‡ã€‚</li>
<li>CoRTæé«˜äº†æ¨¡å‹çš„æ•ˆç‡ï¼Œå‡å°‘äº†åœ¨å¤„ç†æ•°å­¦è¿ç®—æ—¶çš„ä»¤ç‰Œä½¿ç”¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-fa9aa43aeccea9036bf200d40b6a1b82~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328858&auth_key=1761328858-0-0-8a0583d354f08c8cb6c7e4f0b83216fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c8431fb79b39c89c1fcf031e05e524f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328865&auth_key=1761328865-0-0-be424ec1460cee2955f626f70da04097&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2c5df83b90c2f6a64b46fe9131cb4901~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328872&auth_key=1761328872-0-0-ef74bbdf2ffe47e4baa78d6425c07df5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb200a8bfbe6659dd06c8fff104afca6~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328879&auth_key=1761328879-0-0-dce9d870ea7105b96cb4bdf00099bdd3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aa991afbc4c27d37b38aed299844a258~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328886&auth_key=1761328886-0-0-7a911aa3bdb8ee5b9a33379933a5a7f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning"><a href="#UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning" class="headerlink" title="UI-Ins: Enhancing GUI Grounding with Multi-Perspective   Instruction-as-Reasoning"></a>UI-Ins: Enhancing GUI Grounding with Multi-Perspective   Instruction-as-Reasoning</h2><p><strong>Authors:Liangyu Chen, Hanzhang Zhou, Chenglin Cai, Jianan Zhang, Panrong Tong, Quyu Kong, Xu Zhang, Chen Liu, Yuqi Liu, Wenxuan Wang, Yue Wang, Qin Jin, Steven Hoi</strong></p>
<p>GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in <a target="_blank" rel="noopener" href="https://github.com/alibaba/UI-Ins">https://github.com/alibaba/UI-Ins</a>. </p>
<blockquote>
<p>GUIæ¥åœ°ï¼ˆå³å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°å¯æ“ä½œçš„UIå…ƒç´ ï¼‰æ˜¯GUIä»£ç†çš„æ ¸å¿ƒåŠŸèƒ½ã€‚å…ˆå‰çš„ç ”ç©¶å¤§å¤šå°†æŒ‡ä»¤è§†ä¸ºç”¨æˆ·æ„å›¾çš„é™æ€ä»£ç†ï¼Œå¿½ç•¥äº†æŒ‡ä»¤å¤šæ ·æ€§å’Œè´¨é‡å¯¹æ¥åœ°æ€§èƒ½çš„å½±å“ã€‚é€šè¿‡å¯¹ç°æœ‰æ¥åœ°æ•°æ®é›†çš„ä»”ç»†ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°æŒ‡ä»¤ä¸­å­˜åœ¨23.3%çš„é”™è¯¯ç‡ï¼Œå¹¶è¯æ˜åœ¨æ¨ç†æ—¶é—´åˆ©ç”¨æŒ‡ä»¤å¤šæ ·æ€§å¯ä»¥å¸¦æ¥é«˜è¾¾76%çš„ç›¸å¯¹æ€§èƒ½æ”¹è¿›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æŒ‡ä»¤ä½œä¸ºæ¨ç†èŒƒå¼ï¼Œå°†æŒ‡ä»¤è§†ä¸ºæä¾›ä¸åŒè§†è§’çš„åŠ¨æ€åˆ†æè·¯å¾„ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€‰æ‹©æœ€æœ‰æ•ˆçš„è·¯å¾„ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆåœ¨åˆæˆã€å¤šæ ·çš„æŒ‡ä»¤ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä»¥çŒè¾“å¤šè§’åº¦æ¨ç†èƒ½åŠ›ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–è·¯å¾„é€‰æ‹©å’Œç»„åˆã€‚æˆ‘ä»¬å¾—åˆ°çš„æ¨¡å‹UI-Ins-7Bå’ŒUI-Ins-31Båœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¥åœ°åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œå¹¶å±•ç°å‡ºæ–°å…´æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­æœ‰é€‰æ‹©åœ°ç»„åˆå’Œåˆæˆæ–°çš„æŒ‡ä»¤è·¯å¾„ã€‚ç‰¹åˆ«æ˜¯UI-Ins-32Båœ¨UI-I2E-Benchä¸Šå–å¾—äº†æœ€é«˜çš„æ¥åœ°ç²¾åº¦ï¼Œè¾¾åˆ°87.3%ï¼Œåœ¨ScreenSpot-Proä¸Šè¾¾åˆ°57.0%ï¼Œåœ¨MMBench-GUI L2ä¸Šè¾¾åˆ°84.9%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„ä»£ç†æ½œåŠ›ï¼Œåœ¨ä½¿ç”¨UI-Ins-7Bä½œä¸ºæ‰§è¡Œè€…çš„æƒ…å†µä¸‹ï¼Œåœ¨AndroidWorldä¸Šçš„æˆåŠŸç‡è¾¾åˆ°74.1%ã€‚æˆ‘ä»¬çš„æ·±å…¥åˆ†ææ­ç¤ºäº†é¢å¤–çš„è§è§£ï¼Œä¾‹å¦‚å¦‚ä½•åˆ¶å®šæ¨ç†æ¥å¢å¼ºè€Œä¸æ˜¯é˜»ç¢æ¥åœ°æ€§èƒ½ï¼Œä»¥åŠæˆ‘ä»¬çš„æ–¹æ³•å¦‚ä½•ç¼“è§£SFT+RLæ¡†æ¶ä¸­çš„ç­–ç•¥å´©æºƒé—®é¢˜ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/alibaba/UI-Ins%E3%80%82">https://github.com/alibaba/UI-Insã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20286v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†GUIæ¥åœ°ä¸­çš„æŒ‡ä»¤å¤šæ ·æ€§å¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶å¼•å…¥äº†â€œæŒ‡ä»¤ä½œä¸ºæ¨ç†â€çš„èŒƒå¼ã€‚è¯¥ç ”ç©¶é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼ˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼‰å®ç°ï¼Œä»¥å®ç°å¤šè§†è§’æ¨ç†å’Œä¼˜åŒ–è·¯å¾„é€‰æ‹©ã€‚æå‡ºçš„æ¨¡å‹UI-Ins-7Bå’ŒUI-Ins-32Båœ¨äº”ä¸ªæŒ‘æˆ˜æ€§çš„æ¥åœ°åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€ä½³ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨UI-I2E-Benchä¸Šçš„æ¥åœ°ç²¾åº¦è¾¾åˆ°äº†87.3%ã€‚è¯¥ç ”ç©¶è¿˜æ·±å…¥æ¢è®¨äº†æ¨ç†å¦‚ä½•å¢å¼ºæ¥åœ°æ€§èƒ½ï¼Œä»¥åŠå¦‚ä½•ç¼“è§£SFT+RLæ¡†æ¶ä¸­çš„ç­–ç•¥å´©æºƒé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUIæ¥åœ°ä¸­æŒ‡ä»¤å¤šæ ·æ€§çš„é‡è¦æ€§åŠå…¶å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>ç°æœ‰æ¥åœ°æ•°æ®é›†ä¸­çš„æŒ‡ä»¤å­˜åœ¨ç‘•ç–µï¼Œå¹¶å¯¼è‡´æ¨ç†æ—¶çš„é”™è¯¯é€‰æ‹©ã€‚</li>
<li>æå‡ºâ€œæŒ‡ä»¤ä½œä¸ºæ¨ç†â€çš„èŒƒå¼ï¼Œå°†æŒ‡ä»¤è§†ä¸ºåŠ¨æ€åˆ†æè·¯å¾„ï¼Œä¸ºæ¨¡å‹æä¾›ä¸åŒçš„è§†è§’ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥å¼ºåŒ–å¤šè§†è§’æ¨ç†å’Œè·¯å¾„é€‰æ‹©ã€‚</li>
<li>UI-Ins-7Bå’ŒUI-Ins-32Bæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨UI-I2E-Benchä¸Šçš„æ¥åœ°ç²¾åº¦è¾¾åˆ°87.3%ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ä»£ç†æ½œåŠ›ï¼Œåœ¨AndroidWorldä¸Šè¾¾åˆ°äº†74.1%çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8d568e61caf5764dafde2477cd19ba48~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328894&auth_key=1761328894-0-0-9005b9daf9c1d2b445b206155cf32875&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0da5211e3e84031895e691ef177e7a15~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328901&auth_key=1761328901-0-0-4a37b988768bf361c6d290a727b1382c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f298e8cfdaa4421898133026fcfde78c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328907&auth_key=1761328907-0-0-52362237caf7063e40493707671460f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0224b8f854323ae6c3eb7945ff66fe2c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328914&auth_key=1761328914-0-0-c4c9d98e98446d4b1fb88c1402210a28&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8245ff9ffa82a1cf5aefc0fa3ee7ed7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328920&auth_key=1761328920-0-0-d5c5fa069d973fb1d478dddc6c7f4315&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c18908c20f474ce6101902f84e9b859~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328927&auth_key=1761328927-0-0-36397c4210ea2833106865569e73ad60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-53d31b4c9551f407102783cb378474bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328934&auth_key=1761328934-0-0-c8cb67d30eedea65ececcdc18eb4bc9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SynTSBench-Rethinking-Temporal-Pattern-Learning-in-Deep-Learning-Models-for-Time-Series"><a href="#SynTSBench-Rethinking-Temporal-Pattern-Learning-in-Deep-Learning-Models-for-Time-Series" class="headerlink" title="SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models   for Time Series"></a>SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models   for Time Series</h2><p><strong>Authors:Qitai Tan, Yiyun Chen, Mo Li, Ruiwen Gu, Yilin Su, Xiao-Ping Zhang</strong></p>
<p>Recent advances in deep learning have driven rapid progress in time series forecasting, yet many state-of-the-art models continue to struggle with robust performance in real-world applications, even when they achieve strong results on standard benchmark datasets. This persistent gap can be attributed to the black-box nature of deep learning architectures and the inherent limitations of current evaluation frameworks, which frequently lack the capacity to provide clear, quantitative insights into the specific strengths and weaknesses of different models, thereby complicating the selection of appropriate models for particular forecasting scenarios. To address these issues, we propose a synthetic data-driven evaluation paradigm, SynTSBench, that systematically assesses fundamental modeling capabilities of time series forecasting models through programmable feature configuration. Our framework isolates confounding factors and establishes an interpretable evaluation system with three core analytical dimensions: (1) temporal feature decomposition and capability mapping, which enables systematic evaluation of model capacities to learn specific pattern types; (2) robustness analysis under data irregularities, which quantifies noise tolerance thresholds and anomaly recovery capabilities; and (3) theoretical optimum benchmarking, which establishes performance boundaries for each pattern type-enabling direct comparison between model predictions and mathematical optima. Our experiments show that current deep learning models do not universally approach optimal baselines across all types of temporal features.The code is available at <a target="_blank" rel="noopener" href="https://github.com/TanQitai/SynTSBench">https://github.com/TanQitai/SynTSBench</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥æ·±åº¦å­¦ä¹ çš„å‘å±•æ¨åŠ¨äº†æ—¶é—´åºåˆ—é¢„æµ‹çš„è¿…é€Ÿè¿›æ­¥ï¼Œç„¶è€Œè®¸å¤šæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨ä¸­ä»ç„¶éš¾ä»¥ä¿æŒç¨³å¥çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨æ ‡å‡†åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†è‰¯å¥½çš„æ•ˆæœã€‚è¿™ç§æŒä¹…çš„å·®è·å¯å½’å› äºæ·±åº¦å­¦ä¹ æ¶æ„çš„é»‘ç®±æ€§è´¨å’Œå½“å‰è¯„ä¼°æ¡†æ¶çš„å†…åœ¨å±€é™æ€§ï¼Œè¿™äº›æ¡†æ¶é€šå¸¸æ— æ³•æä¾›å…³äºä¸åŒæ¨¡å‹ç‰¹å®šä¼˜åŠ¿å’ŒåŠ£åŠ¿çš„æ˜ç¡®ã€å®šé‡è§è§£ï¼Œä»è€Œå¢åŠ äº†ä¸ºç‰¹å®šé¢„æµ‹åœºæ™¯é€‰æ‹©é€‚å½“æ¨¡å‹çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆæˆæ•°æ®é©±åŠ¨è¯„ä¼°èŒƒå¼SynTSBenchï¼Œå®ƒé€šè¿‡å¯ç¼–ç¨‹ç‰¹å¾é…ç½®ç³»ç»Ÿåœ°è¯„ä¼°æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹çš„åŸºæœ¬å»ºæ¨¡èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡†æ¶éš”ç¦»äº†æ··æ·†å› ç´ ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªå¯è§£é‡Šçš„è¯„ä¼°ç³»ç»Ÿï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒåˆ†æç»´åº¦ï¼šï¼ˆ1ï¼‰æ—¶é—´ç‰¹å¾åˆ†è§£å’Œèƒ½åŠ›æ˜ å°„ï¼Œè¿™å¯ä»¥ç³»ç»Ÿåœ°è¯„ä¼°æ¨¡å‹å­¦ä¹ ç‰¹å®šæ¨¡å¼ç±»å‹çš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰æ•°æ®ä¸è§„åˆ™ä¸‹çš„ç¨³å¥æ€§åˆ†æï¼Œé‡åŒ–å™ªå£°å®¹å¿é˜ˆå€¼å’Œå¼‚å¸¸æ¢å¤èƒ½åŠ›ï¼›ï¼ˆ3ï¼‰ç†è®ºæœ€ä¼˜åŸºå‡†æµ‹è¯•ï¼Œä¸ºæ¯ç§æ¨¡å¼ç±»å‹è®¾å®šæ€§èƒ½è¾¹ç•Œï¼Œå®ç°æ¨¡å‹é¢„æµ‹ä¸æ•°å­¦æœ€ä¼˜å€¼ä¹‹é—´çš„ç›´æ¥æ¯”è¾ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹å¹¶éåœ¨æ‰€æœ‰ç±»å‹çš„æ—¶åºç‰¹å¾ä¸Šéƒ½æ™®éæ¥è¿‘æœ€ä¼˜åŸºå‡†ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/TanQitai/SynTSBench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/TanQitai/SynTSBenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20273v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>æ—¶é—´åºåˆ—é¢„æµ‹é¢†åŸŸè™½ç„¶å–å¾—äº†æ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•ï¼Œä½†è®¸å¤šæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ç¨³å¥æ€§èƒ½ä»ç„¶å­˜åœ¨é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåˆæˆæ•°æ®é©±åŠ¨çš„è¯„ä»·èŒƒå¼SynTSBenchï¼Œé€šè¿‡å¯ç¼–ç¨‹ç‰¹å¾é…ç½®ç³»ç»Ÿåœ°è¯„ä¼°æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹çš„åŸºæœ¬å»ºæ¨¡èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¨¡å¼åˆ†è§£ä¸èƒ½åŠ›æ˜ å°„ã€æ•°æ®ä¸è§„åˆ™ä¸‹çš„ç¨³å¥æ€§åˆ†æå’Œç†è®ºæœ€ä¼˜åŸºå‡†å¯¹æ¯”ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹å¹¶éå¯¹æ‰€æœ‰ç±»å‹çš„æ—¶åºç‰¹å¾éƒ½èƒ½è¾¾åˆ°æœ€ä¼˜åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å–å¾—è¿›å±•ï¼Œä½†ç°å®åº”ç”¨ä¸­çš„ç¨³å¥æ€§èƒ½ä»å­˜åœ¨å·®è·ã€‚</li>
<li>ç°æœ‰è¯„ä»·æ¡†æ¶éš¾ä»¥æä¾›å…³äºæ¨¡å‹ä¼˜ç¼ºç‚¹çš„æ¸…æ™°ã€å®šé‡è§è§£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åˆæˆæ•°æ®é©±åŠ¨è¯„ä»·èŒƒå¼SynTSBenchï¼Œä»¥ç³»ç»Ÿåœ°è¯„ä¼°æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ã€‚</li>
<li>SynTSBenchåŒ…æ‹¬ä¸‰å¤§æ ¸å¿ƒåˆ†æç»´åº¦ï¼šæ¨¡å¼åˆ†è§£ä¸èƒ½åŠ›æ˜ å°„ã€æ•°æ®ä¸è§„åˆ™ä¸‹çš„ç¨³å¥æ€§åˆ†æã€ç†è®ºæœ€ä¼˜åŸºå‡†å¯¹æ¯”ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹å¹¶éåœ¨æ‰€æœ‰ç±»å‹çš„æ—¶åºç‰¹å¾ä¸Šéƒ½è¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</li>
<li>SynTSBenchå¯é€šè¿‡éš”ç¦»æ··æ·†å› ç´ å¹¶å»ºç«‹å¯è§£é‡Šçš„è¯„ä»·ç³»ç»Ÿæ¥ç®€åŒ–æ¨¡å‹é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d9254458dd653e30acf4701e0b49dae7~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328942&auth_key=1761328942-0-0-292ed666f8301989e4ee9e184050c0c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1b3b9e725353a8e751a8e71ab738b9d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328949&auth_key=1761328949-0-0-d3fc9d0ffc561f2b730e6672ed6b7674&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e680983f49329c86e37ae8698a8cf2c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328957&auth_key=1761328957-0-0-90850d4bd4b0be1e7099ac5345db7896&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a03ada912ee0571636bfe6b1e073f372~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328964&auth_key=1761328964-0-0-35ca1c22ec049863d595e146e327c911&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f8847322377968d1cfeee7b09b3c605b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328971&auth_key=1761328971-0-0-db2f88418569eaa8d6faeca055fc13d4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Limits-of-PRM-Guided-Tree-Search-for-Mathematical-Reasoning-with-LLMs"><a href="#Limits-of-PRM-Guided-Tree-Search-for-Mathematical-Reasoning-with-LLMs" class="headerlink" title="Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs"></a>Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs</h2><p><strong>Authors:Tristan Cinquin, Geoff Pleiss, Agustinus Kristiadi</strong></p>
<p>While chain-of-thought prompting with Best-of-N (BoN) selection has become popular for mathematical reasoning in large language models (LLMs), its linear structure fails to capture the branching and exploratory nature of complex problem-solving. In this work, we propose an adaptive algorithm to maximize process reward model (PRM) scores over the intractable action space, and investigate whether PRM-guided tree search can improve mathematical reasoning by exploring multiple partial solution paths. Across $23$ diverse mathematical problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case study, we find that: (1) PRM-guided tree search shows no statistically significant improvements over BoN despite higher costs, (2) Monte Carlo tree search and beam search outperform other PRM-guided tree search methods, (3) PRMs poorly approximate state values and their reliability degrades with reasoning depth, and (4) PRMs generalize poorly out of distribution. This underperformance stems from tree searchâ€™s greater reliance on unreliable PRM scores, suggesting different reward modeling is necessary before tree search can effectively enhance mathematical reasoning in LLMs. </p>
<blockquote>
<p>è™½ç„¶ä½¿ç”¨Best-of-Nï¼ˆBoNï¼‰é€‰æ‹©çš„æ€ç»´é“¾æç¤ºåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†ä¸­å¾ˆå—æ¬¢è¿ï¼Œä½†å…¶çº¿æ€§ç»“æ„æ— æ³•æ•æ‰åˆ°å¤æ‚é—®é¢˜è§£å†³çš„åˆ†æ”¯å’Œæ¢ç´¢æ€§ç‰¹ç‚¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç®—æ³•ï¼Œä»¥åœ¨éš¾ä»¥å¤„ç†çš„è¡Œä¸ºç©ºé—´å†…æœ€å¤§åŒ–è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å¾—åˆ†ï¼Œå¹¶ç ”ç©¶PRMå¼•å¯¼çš„æ ‘æœç´¢æ˜¯å¦å¯ä»¥é€šè¿‡æ¢ç´¢å¤šæ¡éƒ¨åˆ†è§£å†³æ–¹æ¡ˆè·¯å¾„æ¥æé«˜æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ä»¥ä½¿ç”¨Qwen2.5-Math-7B-InstructåŠå…¶ç›¸å…³PRMçš„23ä¸ªä¸åŒæ•°å­¦é—®é¢˜ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ï¼šï¼ˆ1ï¼‰å°½ç®¡æˆæœ¬è¾ƒé«˜ï¼Œä½†PRMå¼•å¯¼çš„æ ‘æœç´¢å¹¶æœªæ˜¾ç¤ºå‡ºåœ¨ç»Ÿè®¡ä¸Šæ˜¾è‘—ä¼˜äºBoNï¼Œï¼ˆ2ï¼‰è’™ç‰¹å¡æ´›æ ‘æœç´¢å’Œå…‰æŸæœç´¢åœ¨è¡¨ç°ä¸Šä¼˜äºå…¶ä»–PRMå¼•å¯¼çš„æ ‘æœç´¢æ–¹æ³•ï¼Œï¼ˆ3ï¼‰PRMåœ¨è¿‘ä¼¼çŠ¶æ€å€¼æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œéšç€æ¨ç†æ·±åº¦çš„å¢åŠ ï¼Œå…¶å¯é æ€§ä¼šä¸‹é™ï¼Œï¼ˆ4ï¼‰PRMçš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚è¿™ç§æ€§èƒ½ä¸ä½³æºäºæ ‘æœç´¢å¯¹ä¸å¯é çš„PRMåˆ†æ•°çš„è¿‡åº¦ä¾èµ–ï¼Œè¿™è¡¨æ˜åœ¨æ ‘æœç´¢èƒ½æœ‰æ•ˆæé«˜LLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ä¹‹å‰ï¼Œéœ€è¦è¿›è¡Œä¸åŒçš„å¥–åŠ±å»ºæ¨¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20272v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é“¾å¼æ€ç»´æç¤ºç»“åˆBest-of-Né€‰æ‹©çš„ç­–ç•¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ•°å­¦æ¨ç†çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†åŸºäºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„é€‚åº”æ€§ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ¢ç´¢å¤šç§éƒ¨åˆ†è§£å†³æ–¹æ¡ˆè·¯å¾„æ¥æ”¹å–„æ•°å­¦æ¨ç†ã€‚é€šè¿‡å¯¹å¤šç§æ•°å­¦é—®é¢˜çš„ç ”ç©¶ï¼Œå‘ç°PRMå¯¼å‘çš„æ ‘æœç´¢ç­–ç•¥å¹¶æ²¡æœ‰æ˜¾è‘—æé«˜æ•ˆæœï¼Œå¹¶ä¸”å­˜åœ¨å¯é æ€§éšæ¨ç†æ·±åº¦ä¸‹é™ä»¥åŠæ³›åŒ–æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæ ‘æœç´¢ä¾èµ–çš„PRMå¾—åˆ†å¯é æ€§ä¸é«˜ï¼Œå»ºè®®åœ¨è¿›è¡Œæ ‘æœç´¢ä¹‹å‰æ”¹è¿›å¥–åŠ±å»ºæ¨¡ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´æç¤ºç»“åˆBest-of-Né€‰æ‹©ç­–ç•¥åœ¨æ•°å­¦æ¨ç†ä¸­å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æ•æ‰å¤æ‚é—®é¢˜è§£å†³çš„åˆ†æ”¯å’Œæ¢ç´¢æ€§ç‰¹ç‚¹ã€‚</li>
<li>æå‡ºåŸºäºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„é€‚åº”æ€§ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ¢ç´¢å¤šç§éƒ¨åˆ†è§£å†³æ–¹æ¡ˆè·¯å¾„æ”¹å–„æ•°å­¦æ¨ç†ã€‚</li>
<li>PRMå¯¼å‘çš„æ ‘æœç´¢ç­–ç•¥åœ¨æ•°å­¦æ¨ç†ä¸­æœªæ˜¾è‘—æ”¹å–„æ•ˆæœï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å¦‚Monte Carloæ ‘æœç´¢å’Œbeamæœç´¢è¡¨ç°è¾ƒå·®ã€‚</li>
<li>PRMåœ¨è¿‘ä¼¼çŠ¶æ€å€¼æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå…¶å¯é æ€§éšæ¨ç†æ·±åº¦è€Œä¸‹é™ã€‚</li>
<li>PRMçš„æ³›åŒ–æ€§èƒ½ä¸ä½³ï¼Œè¡¨ç°åœ¨å¤„ç†åˆ†å¸ƒå¤–çš„æ•°æ®ä¸Šè¡¨ç°è¾ƒå·®ã€‚</li>
<li>æ ‘æœç´¢æ–¹æ³•æ›´ä¾èµ–äºä¸å¯é çš„PRMå¾—åˆ†ï¼Œè¿™å¯èƒ½æ˜¯å…¶æ€§èƒ½ä¸ä½³çš„åŸå› ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7adc1608ffe9da6935bee8e8776361c9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328979&auth_key=1761328979-0-0-54ae6d2975dca9ce5b422745fba2dbbc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9f57a10a964ea078464f660241e7b175~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328986&auth_key=1761328986-0-0-14e41c9a159374bf4feefb82d409450e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5f2aaa0ceab7fd0adffef5ccfe38160b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328992&auth_key=1761328992-0-0-f7ff1f111f62927832bd8ec8d974460d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values"><a href="#Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values" class="headerlink" title="Every Question Has Its Own Value: Reinforcement Learning with Explicit   Human Values"></a>Every Question Has Its Own Value: Reinforcement Learning with Explicit   Human Values</h2><p><strong>Authors:Dian Yu, Yulai Zhao, Kishan Panaganti, Linfeng Song, Haitao Mi, Dong Yu</strong></p>
<p>We propose Reinforcement Learning with Explicit Human Values (RLEV), a method that aligns Large Language Model (LLM) optimization directly with quantifiable human value signals. While Reinforcement Learning with Verifiable Rewards (RLVR) effectively trains models in objective domains using binary correctness rewards, it overlooks that not all tasks are equally significant. RLEV extends this framework by incorporating human-defined value signals directly into the reward function. Using exam-style data with explicit ground-truth value labels, RLEV consistently outperforms correctness-only baselines across multiple RL algorithms and model scales. Crucially, RLEV policies not only improve value-weighted accuracy but also learn a value-sensitive termination policy: concise for low-value prompts, thorough for high-value ones. We demonstrate this behavior stems from value-weighted gradient amplification on end-of-sequence tokens. Ablation studies confirm the gain is causally linked to value alignment. RLEV remains robust under noisy value signals, such as difficulty-based labels, demonstrating that optimizing for an explicit utility function offers a practical path to aligning LLMs with human priorities. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºä¸€ç§ä¸æ˜ç¡®äººç±»ä»·å€¼å¯¹é½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning with Explicit Human Valuesï¼Œç®€ç§°RLEVï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŒ–ä¸å¯é‡åŒ–çš„äººç±»ä»·å€¼ä¿¡å·å¯¹é½ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ é€šè¿‡å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä½¿ç”¨äºŒå…ƒæ­£ç¡®æ€§å¥–åŠ±æœ‰æ•ˆåœ°åœ¨ç›®æ ‡é¢†åŸŸè®­ç»ƒæ¨¡å‹ï¼Œä½†å®ƒå¿½ç•¥äº†å¹¶éæ‰€æœ‰ä»»åŠ¡éƒ½å…·æœ‰åŒç­‰é‡è¦æ€§ã€‚RLEVé€šè¿‡ç›´æ¥å°†äººç±»å®šä¹‰çš„ä»·å€¼ä¿¡å·çº³å…¥å¥–åŠ±å‡½æ•°æ¥æ‰©å±•è¿™ä¸€æ¡†æ¶ã€‚ä½¿ç”¨å¸¦æœ‰æ˜ç¡®åœ°é¢çœŸå®ä»·å€¼æ ‡ç­¾çš„è€ƒè¯•å¼æ•°æ®ï¼ŒRLEVåœ¨å¤šä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¨¡å‹è§„æ¨¡ä¸Šå‡ä¼˜äºä»…åŸºäºæ­£ç¡®æ€§çš„åŸºçº¿ã€‚å…³é”®çš„æ˜¯ï¼ŒRLEVç­–ç•¥ä¸ä»…æé«˜äº†ä»·å€¼åŠ æƒå‡†ç¡®æ€§ï¼Œè¿˜å­¦ä¹ äº†ä»·å€¼æ•æ„Ÿç»ˆæ­¢ç­–ç•¥ï¼šå¯¹ä½ä»·å€¼æç¤ºç®€æ´æ˜äº†ï¼Œå¯¹é«˜ä»·å€¼æç¤ºåˆ™å…¨é¢æ·±å…¥ã€‚æˆ‘ä»¬è¯æ˜è¿™ç§è¡Œä¸ºæºäºåºåˆ—ç»“æŸæ ‡è®°çš„ä»·å€¼åŠ æƒæ¢¯åº¦æ”¾å¤§ã€‚æ¶ˆé™¤ç ”ç©¶è¡¨æ˜å¢ç›Šä¸ä»·å€¼å¯¹é½ä¹‹é—´å­˜åœ¨å› æœå…³ç³»ã€‚åœ¨å™ªå£°ä»·å€¼ä¿¡å·ä¸‹ï¼ŒRLEVä»ç„¶ä¿æŒç¨³å¥ï¼Œä¾‹å¦‚åŸºäºéš¾åº¦çš„æ ‡ç­¾ï¼Œè¡¨æ˜ä¼˜åŒ–æ˜ç¡®æ•ˆç”¨å‡½æ•°ä¸ºå°†LLMä¸äººç±»ä¼˜å…ˆäº‹é¡¹å¯¹é½æä¾›äº†å®ç”¨é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20187v1">PDF</a> 15 pages, 4 figures</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸æ˜ç¡®äººç±»ä»·å€¼è§‚ï¼ˆRLEVï¼‰æ–¹æ³•èƒ½å¤Ÿå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŒ–ä¸äººç±»å¯é‡åŒ–çš„ä»·å€¼ä¿¡å·ç›´æ¥å¯¹é½ã€‚ç›¸è¾ƒäºå¼ºåŒ–å­¦ä¹ ä½¿ç”¨å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä»…åœ¨ç›®æ ‡é¢†åŸŸä½¿ç”¨äºŒå…ƒæ­£ç¡®æ€§å¥–åŠ±è®­ç»ƒæ¨¡å‹ï¼ŒRLEVé€šè¿‡èå…¥äººç±»å®šä¹‰çš„ä»·å€¼ä¿¡å·è¿›ä¸€æ­¥æ‰©å±•äº†è¿™ä¸€æ¡†æ¶ã€‚ä½¿ç”¨å¸¦æœ‰æ˜ç¡®åœ°é¢çœŸå®ä»·å€¼æ ‡ç­¾çš„è€ƒè¯•å¼æ•°æ®ï¼ŒRLEVåœ¨å¤šä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¨¡å‹è§„æ¨¡ä¸Šå‡è¡¨ç°è¶…è¶Šä»…åŸºäºæ­£ç¡®æ€§çš„åŸºçº¿ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒRLEVç­–ç•¥ä¸ä»…æé«˜äº†ä»·å€¼åŠ æƒç²¾åº¦ï¼Œè¿˜å­¦ä¼šäº†ä»·å€¼æ•æ„Ÿå‹ç»ˆæ­¢ç­–ç•¥ï¼šå¯¹äºä½ä»·å€¼æç¤ºè¿›è¡Œç®€æ´å›åº”ï¼Œå¯¹äºé«˜ä»·å€¼æç¤ºåˆ™è¿›è¡Œå½»åº•å¤„ç†ã€‚è¿™ç§è¡Œä¸ºçš„æ ¹æºåœ¨äºåºåˆ—ç»“æŸä»¤ç‰Œçš„ä»·å€¼åŠ æƒæ¢¯åº¦æ”¾å¤§ã€‚å› æœå…³ç³»ç ”ç©¶è¡¨æ˜ä»·å€¼çš„å¢ç›Šæ¥è‡ªäºä»·å€¼å¯¹é½ã€‚å³ä½¿åœ¨æœ‰å™ªéŸ³çš„ä»·å€¼ä¿¡å·ä¸‹ï¼ŒRLEVä»ç„¶ç¨³å¥ï¼Œè¯æ˜ä¼˜åŒ–æ˜ç¡®çš„æ•ˆç”¨å‡½æ•°æ˜¯å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»ä¼˜å…ˆäº‹é¡¹å¯¹é½çš„å®é™…é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RLEVæ–¹æ³•å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŒ–ä¸äººç±»å¯é‡åŒ–çš„ä»·å€¼ä¿¡å·å¯¹é½ã€‚</li>
<li>RLEVæ‰©å±•äº†RLVRæ¡†æ¶ï¼Œé€šè¿‡èå…¥äººç±»å®šä¹‰çš„ä»·å€¼ä¿¡å·æ¥æ”¹è¿›æ¨¡å‹è®­ç»ƒã€‚</li>
<li>RLEVåœ¨å¤šä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¨¡å‹è§„æ¨¡ä¸Šè¡¨ç°è¶…è¶Šä»…åŸºäºæ­£ç¡®æ€§çš„åŸºçº¿ã€‚</li>
<li>RLEVç­–ç•¥èƒ½å­¦ä¼šä»·å€¼æ•æ„Ÿå‹ç»ˆæ­¢ç­–ç•¥ï¼Œæ ¹æ®ä»·å€¼é«˜ä½è°ƒæ•´å›åº”æ–¹å¼ã€‚</li>
<li>ä»·å€¼åŠ æƒæ¢¯åº¦æ”¾å¤§åœ¨åºåˆ—ç»“æŸä»¤ç‰Œä¸Šè¡¨ç°æ˜æ˜¾ã€‚</li>
<li>RLEVç­–ç•¥çš„å¢ç›Šæºäºä»·å€¼å¯¹é½ï¼Œä¸”é€šè¿‡å› æœå…³ç³»ç ”ç©¶å¾—åˆ°è¯å®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20187">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a270826610ebfdb56a0f3ab5e4e39382~resize:0:q75.jpg?source=1f5c5e47&expiration=1761328999&auth_key=1761328999-0-0-53ffe3c9b585c27ec9dbc65c18aa902b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning"><a href="#Rank-GRPO-Training-LLM-based-Conversational-Recommender-Systems-with-Reinforcement-Learning" class="headerlink" title="Rank-GRPO: Training LLM-based Conversational Recommender Systems with   Reinforcement Learning"></a>Rank-GRPO: Training LLM-based Conversational Recommender Systems with   Reinforcement Learning</h2><p><strong>Authors:Yaochen Zhu, Harald Steck, Dawen Liang, Yinhan He, Jundong Li, Nathan Kallus</strong></p>
<p>Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at <a target="_blank" rel="noopener" href="https://github.com/yaochenzhu/Rank-GRPO">https://github.com/yaochenzhu/Rank-GRPO</a>. </p>
<blockquote>
<p>åŸºäºå¯¹è¯çš„æ¨èç³»ç»Ÿé‡å¡‘äº†æ¨èç³»ç»Ÿçš„èŒƒå¼ï¼Œä½¿å¾—ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡å¯¹è¯æ¥è¡¨è¾¾è‡ªå·±çš„åå¥½å¹¶æ¥å—æ¨èã€‚ç„¶è€Œï¼Œå¦‚ä½•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºæ¨èä»»åŠ¡ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼šé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å¾€å¾€ä¼šç”Ÿæˆè¶…å‡ºç›®å½•èŒƒå›´çš„é¡¹ç›®ã€è¿åå¿…è¦çš„è¾“å‡ºæ ¼å¼ï¼Œå¹¶ä¸”å…¶æ’åè´¨é‡åœ¨ç”Ÿæˆçš„åˆ—è¡¨æœ«å°¾æ€¥å‰§ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ConvRec-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯¹è¯æ¨èç³»ç»Ÿçš„ç«¯åˆ°ç«¯è®­ç»ƒçš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰Remap-Reflect-Adjustç®¡é“çš„è¡Œä¸ºå…‹éš†æ•°æ®é›†ï¼Œä»å¼ºå¤§çš„é»‘ç®±å¤§å‹è¯­è¨€æ¨¡å‹ä¸­äº§ç”Ÿé«˜è´¨é‡ã€åŸºäºç›®å½•çš„æ¼”ç¤ºå†…å®¹ï¼Œä»¥å¯åŠ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†Rank-GRPOï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ’åè¾“å‡ºä»»åŠ¡å®šåˆ¶çš„åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æœ‰åŸåˆ™æ‰©å±•ã€‚Rank-GRPOå°†æ¨èåˆ—è¡¨ä¸­çš„æ¯ä¸ªæ’åè§†ä¸ºä¸€ä¸ªå•å…ƒï¼Œè€Œä¸æ˜¯ä»¤ç‰Œï¼ˆè¿‡äºç²¾ç»†ï¼‰æˆ–åºåˆ—ï¼ˆè¿‡äºç²—ç•¥ï¼‰ï¼Œé€šè¿‡é‡æ–°å®šä¹‰å¥–åŠ±æ¥æ¶ˆé™¤éå› æœä¿¡ç”¨åˆ†é…ï¼Œå¹¶å¼•å…¥åŸºäºæ’åä»¤ç‰Œæ¦‚ç‡å‡ ä½•å¹³å‡å€¼çš„æ’åçº§é‡è¦æ€§æ¯”ç‡æ¥ç¨³å®šç­–ç•¥æ›´æ–°ã€‚åœ¨å…¬å¼€Reddit-v2æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒConvRec-R1çš„æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œåœ¨å¬å›ç‡å’Œå½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Šæ–¹é¢è¾¾åˆ°äº†é«˜äºGRPOé£æ ¼çš„åŸºå‡†æµ‹è¯•æˆç»©ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/yaochenzhu/Rank-GRPO%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/yaochenzhu/Rank-GRPOå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20150v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨é‡å¡‘æ¨èç³»ç»ŸèŒƒå¼ï¼Œé€šè¿‡å¯¹è¯è¡¨è¾¾ç”¨æˆ·åå¥½å’Œæ¥æ”¶æ¨èã€‚ç„¶è€Œï¼Œå°†LLMsä¸æ¨èä»»åŠ¡å¯¹é½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼šé¢„è®­ç»ƒçš„LLMså¾€å¾€ä¼šç”Ÿæˆè¶…å‡ºç›®å½•èŒƒå›´çš„é¡¹ç›®ï¼Œè¿åå¿…è¦çš„è¾“å‡ºæ ¼å¼ï¼Œå¹¶ä¸”å…¶æ’åè´¨é‡åœ¨ç”Ÿæˆåˆ—è¡¨çš„æœ«å°¾æ€¥å‰§ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ConvRec-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯è®­ç»ƒåŸºäºLLMçš„å¯¹è¯æ¨èç³»ç»Ÿã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡è¡Œä¸ºå…‹éš†æ•°æ®é›†å’ŒRemap-Reflect-Adjustç®¡é“äº§ç”Ÿé«˜è´¨é‡ã€åŸºäºç›®å½•çš„æ¼”ç¤ºï¼Œä»¥é¢„çƒ­RLè®­ç»ƒã€‚ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†Rank-GRPOï¼Œå®ƒæ˜¯é’ˆå¯¹å…·æœ‰æ’åè¾“å‡ºä»»åŠ¡é‡èº«å®šåˆ¶çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æœ‰åŸåˆ™æ‰©å±•ã€‚Rank-GRPOå°†æ¨èåˆ—è¡¨ä¸­çš„æ¯ä¸ªæ’åè§†ä¸ºå•å…ƒï¼Œè€Œä¸æ˜¯ä»¤ç‰Œï¼ˆå¤ªç²¾ç»†ï¼‰æˆ–åºåˆ—ï¼ˆå¤ªç²—ç³™ï¼‰ï¼Œé‡æ–°å®šä¹‰å¥–åŠ±ä»¥æ¶ˆé™¤éå› æœä¿¡ç”¨åˆ†é…ï¼Œå¹¶å¼•å…¥åŸºäºæ’åçº§åˆ«é‡è¦æ€§æ¯”ç‡ï¼ˆå³æ’åèŒƒå›´å†…ä»¤ç‰Œæ¦‚ç‡çš„å‡ ä½•å¹³å‡å€¼ï¼‰æ¥ç¨³å®šç­–ç•¥æ›´æ–°ã€‚åœ¨å…¬å…±Reddit-v2æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒConvRec-R1æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå¬å›ç‡å’ŒNDCGé«˜äºGRPOé£æ ¼çš„åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨æ¨åŠ¨å¯¹è¯æ¨èç³»ç»Ÿçš„å˜é©ã€‚</li>
<li>å¯¹è¯æ¨èç³»ç»Ÿé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•å°†LLMsä¸æ¨èä»»åŠ¡å¯¹é½ã€‚</li>
<li>ConvRec-R1æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³LLMåœ¨æ¨èä»»åŠ¡ä¸­çš„å¯¹é½é—®é¢˜ã€‚</li>
<li>Stage 1é€šè¿‡è¡Œä¸ºå…‹éš†æ•°æ®é›†å’ŒRemap-Reflect-Adjustç®¡é“äº§ç”Ÿé«˜è´¨é‡ã€åŸºäºç›®å½•çš„æ¼”ç¤ºæ¥é¢„çƒ­RLè®­ç»ƒã€‚</li>
<li>Stage 2æå‡ºäº†Rank-GRPOï¼Œä¸€ä¸ªé’ˆå¯¹æ’åè¾“å‡ºä»»åŠ¡çš„ä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>Rank-GRPOå°†æ¨èåˆ—è¡¨ä¸­çš„æ¯ä¸ªæ’åè§†ä¸ºç‹¬ç«‹å•å…ƒï¼Œå¹¶å¼•å…¥é‡è¦æ€§æ¯”ç‡æ¥ç¨³å®šç­–ç•¥æ›´æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2a9273c37974561027a913daa59db3eb~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329007&auth_key=1761329007-0-0-3c55294b835a8cc4ef02bce58689b3ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-the-Power-of-Large-Language-Models-in-Entity-Linking-via-Adaptive-Routing-and-Targeted-Reasoning"><a href="#Leveraging-the-Power-of-Large-Language-Models-in-Entity-Linking-via-Adaptive-Routing-and-Targeted-Reasoning" class="headerlink" title="Leveraging the Power of Large Language Models in Entity Linking via   Adaptive Routing and Targeted Reasoning"></a>Leveraging the Power of Large Language Models in Entity Linking via   Adaptive Routing and Targeted Reasoning</h2><p><strong>Authors:Yajie Li, Albert Galimov, Mitra Datta Ganapaneni, Pujitha Thejaswi, De Meng, Priyanshu Kumar, Saloni Potdar</strong></p>
<p>Entity Linking (EL) has traditionally relied on large annotated datasets and extensive model fine-tuning. While recent few-shot methods leverage large language models (LLMs) through prompting to reduce training requirements, they often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER (Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline that achieves high performance without deep fine-tuning by strategically combining candidate generation, context-based scoring, adaptive routing, and selective reasoning. ARTER computes a small set of complementary signals(both embedding and LLM-based) over the retrieved candidates to categorize contextual mentions into easy and hard cases. The cases are then handled by a low-computational entity linker (e.g. ReFinED) and more expensive targeted LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets, and performs comparably to pipelines using LLM-based reasoning for all mentions, while being as twice as efficient in terms of the number of LLM tokens. </p>
<blockquote>
<p>å®ä½“é“¾æ¥ï¼ˆELï¼‰ä¼ ç»Ÿä¸Šä¾èµ–äºå¤§å‹æ ‡æ³¨æ•°æ®é›†å’Œå¹¿æ³›çš„æ¨¡å‹å¾®è°ƒã€‚è™½ç„¶æœ€è¿‘çš„å°‘æ ·æœ¬æ–¹æ³•é€šè¿‡æç¤ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å‡å°‘è®­ç»ƒè¦æ±‚ï¼Œä½†å®ƒä»¬é€šå¸¸å› ä¸ºæ˜‚è´µçš„LLMåŸºäºçš„æ¨ç†è€Œå‡ºç°æ•ˆç‡ä¸é«˜çš„æƒ…å†µã€‚ARTERï¼ˆè‡ªé€‚åº”è·¯ç”±å’Œé¶å‘å®ä½“æ¨ç†ï¼‰æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–ç®¡é“ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°ç»“åˆå€™é€‰ç”Ÿæˆã€åŸºäºä¸Šä¸‹æ–‡è¯„åˆ†ã€è‡ªé€‚åº”è·¯ç”±å’Œé€‰æ‹©æ€§æ¨ç†ï¼Œæ— éœ€æ·±åº¦å¾®è°ƒå³å¯å®ç°é«˜æ€§èƒ½ã€‚ARTERè®¡ç®—ä¸€å°éƒ¨åˆ†è¡¥å……ä¿¡å·ï¼ˆåŸºäºåµŒå…¥å’ŒLLMï¼‰å¯¹æ£€ç´¢åˆ°çš„å€™é€‰å¯¹è±¡è¿›è¡Œåˆ†ç±»ï¼Œå°†ä¸Šä¸‹æ–‡æåŠåˆ†ä¸ºç®€å•å’Œå›°éš¾çš„æƒ…å†µã€‚ç„¶åè¿™äº›æƒ…å†µåˆ†åˆ«ç”±ä½è®¡ç®—å®ä½“é“¾æ¥å™¨ï¼ˆä¾‹å¦‚ReFinEDï¼‰å’Œæ›´æ˜‚è´µçš„é¶å‘LLMåŸºç¡€æ¨ç†è¿›è¡Œå¤„ç†ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒARTERæ¯”ReFinEDé«˜å‡º+4.47%ï¼Œåœ¨6ä¸ªæ•°æ®é›†ä¸­çš„5ä¸ªæ•°æ®é›†ä¸Šå¹³å‡æå‡+2.53%ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰æåŠçš„ç®¡é“ä¸­ï¼Œä¸åŸºäºLLMçš„æ¨ç†ç®¡é“è¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶åœ¨LLMä»¤ç‰Œæ•°é‡æ–¹é¢æ•ˆç‡æé«˜äº†ä¸€å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20098v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡æ•°æ®é›†å’Œæ·±åº¦æ¨¡å‹çš„ç²¾ç»†è°ƒæ•´ä¼ ç»Ÿå®ä½“é“¾æ¥ï¼ˆELï¼‰çš„æ–¹æ³•é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œèµ„æºéœ€æ±‚çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œæœ€æ–°çš„å°‘æ•°æ¨¡å¼æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æç¤ºæ¥å‡å°‘è®­ç»ƒéœ€æ±‚ï¼Œä½†ä»å­˜åœ¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚è€ŒARTERæ–¹æ³•åˆ™æå‡ºäº†ä¸€ç§ç»“æ„åŒ–ç®¡é“ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°ç»“åˆå€™é€‰ç”Ÿæˆã€åŸºäºä¸Šä¸‹æ–‡çš„è¯„åˆ†ã€è‡ªé€‚åº”è·¯ç”±å’Œé€‰æ‹©æ€§æ¨ç†ç­‰æŠ€æœ¯ï¼Œåœ¨ä¸è¿›è¡Œæ·±åº¦ç²¾ç»†è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°é«˜æ€§èƒ½ã€‚å®ƒåˆ©ç”¨ä¸€å°éƒ¨åˆ†äº’è¡¥ä¿¡å·ï¼ˆåŒ…æ‹¬åµŒå…¥å’ŒåŸºäºLLMçš„ä¿¡å·ï¼‰å¯¹æ£€ç´¢åˆ°çš„å€™é€‰å®ä½“è¿›è¡Œåˆ†ç±»ï¼Œå°†å…¶åˆ†ä¸ºç®€å•å’Œå›°éš¾çš„æƒ…å†µã€‚ç„¶ååˆ†åˆ«ç”±ä½è®¡ç®—å®ä½“é“¾æ¥å™¨ï¼ˆå¦‚ReFinEDï¼‰å’Œé’ˆå¯¹å¤æ‚æƒ…å†µçš„é«˜æ€§èƒ½LLMæ¨ç†å™¨å¤„ç†è¿™ä¸¤ç§æƒ…å†µã€‚åœ¨æ ‡å‡†æµ‹è¯•ä¸­ï¼ŒARTERçš„å‡†ç¡®ç‡ç›¸æ¯”ReFinEDæé«˜é«˜è¾¾+4.47%ï¼Œå¹¶åœ¨äº”ä¸ªæ•°æ®é›†ä¸­å¹³å‡æå‡+2.53%ã€‚ä¸æ­¤åŒæ—¶ï¼Œç›¸è¾ƒäºå…¶ä»–å…¨é¢ä½¿ç”¨LLMçš„æ¨ç†æ–¹æ³•ï¼Œå…¶å¤„ç†æ‰€æœ‰æåŠå†…å®¹çš„æ•ˆæœæ›´åŠ å‡ºè‰²ï¼Œä¸”æ•ˆç‡æ›´é«˜ã€‚å°½ç®¡æ€§èƒ½ä¼˜å¼‚ï¼Œä½†å…¶èƒ½ä»¥æ›´ä½çš„è®¡ç®—æˆæœ¬å’Œèµ„æºæ¶ˆè€—è¾¾åˆ°è¿™ç§æ•ˆæœã€‚æ€»ä¹‹ï¼ŒARTERä¸ºè§£å†³å®ä½“é“¾æ¥é—®é¢˜æä¾›äº†æ–°çš„è§†è§’å’Œè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ARTERæ˜¯ä¸€ç§æ–°çš„å®ä½“é“¾æ¥æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•å’Œç°æœ‰å°‘æ•°æ¨¡å¼æ–¹æ³•çš„é—®é¢˜ã€‚</li>
<li>ARTERé€šè¿‡ç»“åˆå¤šç§æŠ€æœ¯å®ç°é«˜æ•ˆå®ä½“é“¾æ¥ï¼ŒåŒ…æ‹¬å€™é€‰ç”Ÿæˆã€ä¸Šä¸‹æ–‡è¯„åˆ†ã€è‡ªé€‚åº”è·¯ç”±å’Œé€‰æ‹©æ€§æ¨ç†ç­‰ã€‚</li>
<li>ARTERèƒ½åˆ©ç”¨å°‘æ•°èµ„æºç”Ÿæˆè¾…åŠ©ä¿¡å·æ¥å¯¹å®ä½“è¿›è¡Œç­›é€‰ï¼Œæœ‰æ•ˆåœ°å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç±»ç®€åŒ–å¹¶æ”¹å–„è®¡ç®—æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0f999af592fb49daa7c93508e9818968~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329016&auth_key=1761329016-0-0-70d8648e706c93138a686eb244ab542d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-857bc19cfc3f54550b9ae3cf35cfbc1c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329023&auth_key=1761329023-0-0-5b6e69bf5449995d89c689c632f64c8b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-263f18abe5cae4cb9aaabb2353cdf736~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329029&auth_key=1761329029-0-0-98a0afcb62c4d7182d79fb038a4f68e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e72185f3951c0bf3c3e54cc7085122da~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329037&auth_key=1761329037-0-0-f6b8e3950224cf68d671af5d9f683f3a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1c4952097425bb5a6250b9100d3cec92~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329044&auth_key=1761329044-0-0-193e5c5c182f414e239caa0b893cc71a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c271e74f1103cd08b0018cc9de6c6242~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329051&auth_key=1761329051-0-0-d282f0a143062176a3a04f4cfc14dc88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CreativityPrism-A-Holistic-Benchmark-for-Large-Language-Model-Creativity"><a href="#CreativityPrism-A-Holistic-Benchmark-for-Large-Language-Model-Creativity" class="headerlink" title="CreativityPrism: A Holistic Benchmark for Large Language Model   Creativity"></a>CreativityPrism: A Holistic Benchmark for Large Language Model   Creativity</h2><p><strong>Authors:Zhaoyi Joey Hou, Bowei Alvin Zhang, Yining Lu, Bhiman Kumar Baghel, Anneliese Brei, Ximing Lu, Meng Jiang, Faeze Brahman, Snigdha Chaturvedi, Haw-Shiuan Chang, Daniel Khashabi, Xiang Lorraine Li</strong></p>
<p>Creativity is often seen as a hallmark of human intelligence. While large language models (LLMs) are increasingly perceived as producing creative text, there is still no holistic framework to evaluate their creativity across diverse scenarios. Existing evaluation methods remain fragmented, with dramatic variation across domains and tasks, largely due to differing definitions and measurements of creativity. Inspired by the hypothesis that creativity is not one fixed idea, we propose CreativityPrism, an evaluation analysis framework that decomposes creativity into three dimensions: quality, novelty, and diversity. CreativityPrism incorporates nine tasks, three domains, i.e., divergent thinking, creative writing, and logical reasoning, and twenty evaluation metrics, which measure each dimension in task-specific, unique ways. We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on CreativityPrism and analyze the performance correlations among different metrics and task domains. Our results reveal a notable gap between proprietary and open-source models. Overall, model performance tends to be highly correlated across tasks within the same domain and less so across different domains. Among evaluation dimensions, diversity and quality metrics show strong correlations - models that perform well on one often excel on the other - whereas novelty exhibits much weaker correlation with either. These findings support our hypothesis that strong performance in one creativity task or dimension does not necessarily generalize to others, underscoring the need for a holistic evaluation of LLM creativity. </p>
<blockquote>
<p>åˆ›é€ åŠ›é€šå¸¸è¢«è§†ä¸ºäººç±»æ™ºåŠ›çš„æ ‡å¿—ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«è®¤ä¸ºèƒ½å¤Ÿäº§ç”Ÿåˆ›é€ æ€§çš„æ–‡æœ¬ï¼Œä½†è¿˜æ²¡æœ‰ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥è¯„ä¼°å®ƒä»¬åœ¨ä¸åŒåœºæ™¯ä¸­çš„åˆ›é€ åŠ›ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä»ç„¶ç¢ç‰‡åŒ–ï¼Œä¸åŒé¢†åŸŸå’Œä»»åŠ¡ä¹‹é—´å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå¯¹åˆ›é€ åŠ›çš„ä¸åŒå®šä¹‰å’Œæµ‹é‡ã€‚å—åˆ›é€ åŠ›ä¸æ˜¯ä¸€ç§å›ºå®šæƒ³æ³•çš„å‡è®¾å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†CreativityPrismï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°åˆ†ææ¡†æ¶ï¼Œå°†åˆ›é€ åŠ›åˆ†è§£ä¸ºä¸‰ä¸ªç»´åº¦ï¼šè´¨é‡ã€æ–°é¢–æ€§å’Œå¤šæ ·æ€§ã€‚CreativityPrismåŒ…å«äº†ä¹ä¸ªä»»åŠ¡ã€ä¸‰ä¸ªé¢†åŸŸï¼ˆå‘æ•£æ€ç»´ã€åˆ›é€ æ€§å†™ä½œå’Œé€»è¾‘æ¨ç†ï¼‰ï¼Œä»¥åŠäºŒåä¸ªè¯„ä»·æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡ä»¥ä»»åŠ¡ç‰¹å®šå’Œç‹¬ç‰¹çš„æ–¹å¼æµ‹é‡æ¯ä¸ªç»´åº¦ã€‚æˆ‘ä»¬åœ¨CreativityPrismä¸Šè¯„ä¼°äº†17ä¸ªæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ†æäº†ä¸åŒæŒ‡æ ‡å’Œä»»åŠ¡é¢†åŸŸä¹‹é—´çš„æ€§èƒ½ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†ä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„æ˜æ˜¾å·®è·ã€‚æ€»ä½“è€Œè¨€ï¼ŒåŒä¸€é¢†åŸŸå†…çš„ä»»åŠ¡ä¹‹é—´æ€§èƒ½é«˜åº¦ç›¸å…³ï¼Œè€Œä¸åŒé¢†åŸŸé—´çš„ä»»åŠ¡åˆ™ç›¸å…³æ€§è¾ƒä½ã€‚åœ¨è¯„ä¼°ç»´åº¦ä¸­ï¼Œå¤šæ ·æ€§å’Œè´¨é‡æŒ‡æ ‡è¡¨ç°å‡ºå¼ºçƒˆçš„ç›¸å…³æ€§â€”â€”åœ¨ä¸€ä¸ªæ–¹é¢è¡¨ç°è‰¯å¥½çš„æ¨¡å‹é€šå¸¸åœ¨å¦ä¸€æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²â€”â€”è€Œæ–°é¢–æ€§ä¸ä¸¤è€…ä¹‹é—´çš„ç›¸å…³æ€§åˆ™è¾ƒå¼±å¾—å¤šã€‚è¿™äº›å‘ç°æ”¯æŒæˆ‘ä»¬çš„å‡è®¾ï¼Œå³åœ¨ä¸€ä¸ªåˆ›é€ åŠ›ä»»åŠ¡æˆ–ç»´åº¦ä¸Šçš„å‡ºè‰²è¡¨ç°å¹¶ä¸ä¸€å®šé€‚ç”¨äºå…¶ä»–ä»»åŠ¡æˆ–ç»´åº¦ï¼Œè¿™å¼ºè°ƒäº†å…¨é¢è¯„ä¼°LLMåˆ›é€ åŠ›çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ä¸ªè¯„ä»·åˆ†ææ¡†æ¶CreativityPrismï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ›é€ åŠ›æ–¹é¢çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å°†åˆ›é€ åŠ›åˆ†è§£ä¸ºè´¨é‡ã€æ–°é¢–æ€§å’Œå¤šæ ·æ€§ä¸‰ä¸ªç»´åº¦ï¼Œå¹¶è®¾è®¡äº†ä¹ä¸ªä»»åŠ¡ï¼Œæ¶‰åŠä¸‰ä¸ªé¢†åŸŸï¼šå‘æ•£æ€§æ€ç»´ã€åˆ›é€ æ€§å†™ä½œå’Œé€»è¾‘æ¨ç†ã€‚é€šè¿‡å¯¹17ä¸ªæœ€å…ˆè¿›çš„LLMåœ¨CreativityPrismä¸Šçš„è¯„ä¼°ï¼Œå‘ç°ä¸åŒæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œå¼ºè°ƒäº†å¯¹LLMåˆ›é€ åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åˆ›é€ åŠ›çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>åˆ›é€ åŠ›å¹¶éå•ä¸€æ¦‚å¿µï¼Œéœ€è¦å¤šç»´åº¦è¯„ä¼°ã€‚</li>
<li>CreativityPrismæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªç»´åº¦ï¼šè´¨é‡ã€æ–°é¢–æ€§ã€å¤šæ ·æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«ä¹ä¸ªä»»åŠ¡ï¼Œæ¶‰åŠå‘æ•£æ€§æ€ç»´ã€åˆ›é€ æ€§å†™ä½œå’Œé€»è¾‘æ¨ç†ä¸‰ä¸ªé¢†åŸŸã€‚</li>
<li>è¯„ä¼°äº†17ä¸ªæœ€å…ˆè¿›çš„LLMï¼Œå‘ç°ä¸åŒæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¹‹é—´çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>ä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-19f2c60c6a1909964b854a30db94a535~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329058&auth_key=1761329058-0-0-1efb61d1a50d43e873a147ee82b15696&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c0ecbdc9ac28e2caee520a0c776c4637~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329066&auth_key=1761329066-0-0-e97a3109eefc34744a5ea8c45b5cf886&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-97670af68bd4142c832456d0ea52be27~resize:0:q75.jpg?source=1f5c5e47&expiration=1761329074&auth_key=1761329074-0-0-fc8d0580fd4a1c0122ff20d19ebc0674&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-6ce5253c371246d6bc19de916eafe302~resize:0:q75.jpg?source=1f5c5e47&expiration=1761330976&auth_key=1761330976-0-0-6d29fa448f2e08600276207533d5485d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  ARGenSeg Image Segmentation with Autoregressive Image Generation Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-23/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-d045b1f06db546ee7abb23b6fcdcea83~resize:0:q75.jpg?source=1f5c5e47&expiration=1761181326&auth_key=1761181326-0-0-6d1bbdf10b90a5b1412859bad3f2e5f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-23  Sherlock Your Queries Learning to Ask the Right Questions for   Dialogue-Based Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
