<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  ARGenSeg Image Segmentation with Autoregressive Image Generation Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6ce5253c371246d6bc19de916eafe302')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-25-æ›´æ–°"><a href="#2025-10-25-æ›´æ–°" class="headerlink" title="2025-10-25 æ›´æ–°"></a>2025-10-25 æ›´æ–°</h1><h2 id="ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model"><a href="#ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model" class="headerlink" title="ARGenSeg: Image Segmentation with Autoregressive Image Generation Model"></a>ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</h2><p><strong>Authors:Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng, Jingdong Chen, Jun Zhou</strong></p>
<p>We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºAutoRegressiveç”Ÿæˆçš„æ–°é¢–å›¾åƒåˆ†å‰²ï¼ˆARGenSegï¼‰èŒƒå¼ï¼Œå®ç°äº†åœ¨ç»Ÿä¸€æ¡†æ¶å†…çš„å¤šæ¨¡æ€ç†è§£å’Œåƒç´ çº§æ„ŸçŸ¥ã€‚å…ˆå‰å°†å›¾åƒåˆ†å‰²é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­çš„å·¥ä½œé€šå¸¸é‡‡ç”¨è¾¹ç•Œç‚¹è¡¨ç¤ºæˆ–ä¸“ç”¨åˆ†å‰²å¤´ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºç¦»æ•£è¡¨ç¤ºæˆ–è¯­ä¹‰æç¤ºï¼Œè¿™äº›æç¤ºè¢«è¾“å…¥åˆ°ç‰¹å®šä»»åŠ¡çš„è§£ç å™¨ä¸­ï¼Œè¿™é™åˆ¶äº†MLLMæ•è·ç²¾ç»†ç²’åº¦è§†è§‰ç»†èŠ‚çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå›¾åƒç”Ÿæˆçš„MLLMåˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è‡ªç„¶ä¼šäº§ç”Ÿé’ˆå¯¹ç›®æ ‡å¯¹è±¡çš„å¯†é›†æ©ç ã€‚æˆ‘ä»¬åˆ©ç”¨MLLMè¾“å‡ºè§†è§‰ä»¤ç‰Œï¼Œå¹¶ä½¿ç”¨é€šç”¨VQ-VAEå°†å®ƒä»¬è§£ç ä¸ºå›¾åƒï¼Œä½¿åˆ†å‰²å®Œå…¨ä¾èµ–äºMLLMçš„åƒç´ çº§ç†è§£ã€‚ä¸ºäº†å‡å°‘æ¨ç†å»¶è¿Ÿï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥æ¥å¹¶è¡Œç”Ÿæˆæ‰€éœ€çš„è§†è§‰ä»¤ç‰Œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€æ–°çš„å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨æ¨ç†é€Ÿåº¦ä¸Šæœ‰äº†æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„ç†è§£èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20803v1">PDF</a> Accepted to NeurIPS 2025, 18 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºAutoRegressive Generationçš„æ–°å‹å›¾åƒåˆ†å‰²èŒƒå¼ï¼ˆARGenSegï¼‰ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…å®ç°äº†å¤šæ¨¡æ€ç†è§£å’Œåƒç´ çº§æ„ŸçŸ¥ã€‚ä¸ä¼ ç»Ÿçš„å°†å›¾åƒåˆ†å‰²é›†æˆåˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒARGenSegé‡‡ç”¨åŸºäºå›¾åƒç”Ÿæˆçš„åˆ†å‰²æ¡†æ¶ï¼Œè‡ªç„¶åœ°äº§ç”Ÿç›®æ ‡å¯¹è±¡çš„å¯†é›†æ©è†œã€‚åˆ©ç”¨MLLMè¾“å‡ºè§†è§‰ç¬¦å·ï¼Œå†é€šè¿‡é€šç”¨VQ-VAEè§£ç æˆå›¾åƒï¼Œä½¿åˆ†å‰²å®Œå…¨ä¾èµ–äºMLLMçš„åƒç´ çº§ç†è§£ã€‚ä¸ºæé«˜æ¨ç†é€Ÿåº¦ï¼Œé‡‡ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥å¹¶è¡Œç”Ÿæˆæ‰€éœ€è§†è§‰ç¬¦å·ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶æ˜¾è‘—æé«˜æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºAutoRegressive Generationçš„æ–°å‹å›¾åƒåˆ†å‰²èŒƒå¼ï¼ˆARGenSegï¼‰ã€‚</li>
<li>åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…å®ç°äº†å¤šæ¨¡æ€ç†è§£å’Œåƒç´ çº§æ„ŸçŸ¥ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒARGenSegé‡‡ç”¨åŸºäºå›¾åƒç”Ÿæˆçš„åˆ†å‰²æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨MLLMè¾“å‡ºè§†è§‰ç¬¦å·å¹¶é€šè¿‡é€šç”¨VQ-VAEè§£ç æˆå›¾åƒã€‚</li>
<li>åˆ†å‰²è¿‡ç¨‹å®Œå…¨ä¾èµ–äºMLLMçš„åƒç´ çº§ç†è§£ã€‚</li>
<li>é‡‡ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹ç­–ç•¥æé«˜æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3fe9a38dfcf86b94fff39cb7edde217" align="middle">
<img src="https://picx.zhimg.com/v2-6d4305f05a21f4f90693f322b923dc3f" align="middle">
<img src="https://picx.zhimg.com/v2-ff13e147e62b54ac56e41c64643256cd" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Simple-Context-Compression-Mean-Pooling-and-Multi-Ratio-Training"><a href="#Simple-Context-Compression-Mean-Pooling-and-Multi-Ratio-Training" class="headerlink" title="Simple Context Compression: Mean-Pooling and Multi-Ratio Training"></a>Simple Context Compression: Mean-Pooling and Multi-Ratio Training</h2><p><strong>Authors:Yair Feldman, Yoav Artzi</strong></p>
<p>A common strategy to reduce the computational costs of using long contexts in retrieval-augmented generation (RAG) with large language models (LLMs) is soft context compression, where the input sequence is transformed into a shorter continuous representation. We develop a lightweight and simple mean-pooling approach that consistently outperforms the widely used compression-tokens architecture, and study training the same compressor to output multiple compression ratios. We conduct extensive experiments across in-domain and out-of-domain QA datasets, as well as across model families, scales, and compression ratios. Overall, our simple mean-pooling approach achieves the strongest performance, with a relatively small drop when training for multiple compression ratios. More broadly though, across architectures and training regimes the trade-offs are more nuanced, illustrating the complex landscape of compression methods. </p>
<blockquote>
<p>åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æ—¶ï¼Œä¸ºäº†å‡å°‘ä½¿ç”¨é•¿ä¸Šä¸‹æ–‡å¸¦æ¥çš„è®¡ç®—æˆæœ¬ï¼Œä¸€ç§å¸¸è§ç­–ç•¥æ˜¯è¿›è¡Œè½¯ä¸Šä¸‹æ–‡å‹ç¼©ï¼Œå³å°†è¾“å…¥åºåˆ—è½¬æ¢ä¸ºæ›´çŸ­çš„è¿ç»­è¡¨ç¤ºã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è½»ä¾¿ç®€å•çš„å¹³å‡æ± æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å¹¿æ³›ä½¿ç”¨çš„å‹ç¼©ä»¤ç‰Œæ¶æ„ä¸Šè¡¨ç°æŒç»­ä¼˜å¼‚ã€‚åŒæ—¶ç ”ç©¶å¦‚ä½•è®­ç»ƒåŒä¸€å‹ç¼©å™¨è¾“å‡ºå¤šç§å‹ç¼©æ¯”ç‡ã€‚æˆ‘ä»¬åœ¨åŒåŸŸå’Œè·¨åŸŸé—®ç­”æ•°æ®é›†ä»¥åŠè·¨æ¨¡å‹å®¶æ—ã€è§„æ¨¡å’Œå‹ç¼©æ¯”ç‡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç®€å•å¹³å‡æ± æ–¹æ³•å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œåœ¨è®­ç»ƒå¤šä¸ªå‹ç¼©æ¯”ç‡æ—¶æ€§èƒ½ä¸‹é™ç›¸å¯¹è¾ƒå°ã€‚ç„¶è€Œï¼Œåœ¨æ¶æ„å’Œè®­ç»ƒåˆ¶åº¦æ–¹é¢ï¼Œæƒè¡¡æ›´ä¸ºå¾®å¦™ï¼Œè¿™è¯´æ˜äº†å‹ç¼©æ–¹æ³•çš„å¤æ‚æ ¼å±€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20797v1">PDF</a> Code available at   <a target="_blank" rel="noopener" href="https://github.com/lil-lab/simple-context-compression">https://github.com/lil-lab/simple-context-compression</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå‡å°‘ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ—¶çš„è®¡ç®—æˆæœ¬çš„å¸¸è§ç­–ç•¥ï¼Œå³è½¯ä¸Šä¸‹æ–‡å‹ç¼©ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§è½»ä¾¿ä¸”ç®€å•çš„å¹³å‡æ± åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å¹¿æ³›çš„å‹ç¼©ä»¤ç‰Œæ¶æ„ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†è®­ç»ƒåŒä¸€å‹ç¼©æœºä»¥è¾“å‡ºå¤šç§å‹ç¼©æ¯”ç‡çš„å¯èƒ½æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨é¢†åŸŸé—®ç­”æ•°æ®é›†ã€æ¨¡å‹å®¶æ—ã€è§„æ¨¡å’Œå‹ç¼©æ¯”ç‡ä¸Šå‡è¡¨ç°æœ€ä½³ï¼Œåªåœ¨è®­ç»ƒå¤šç§å‹ç¼©æ¯”ç‡æ—¶å‡ºç°è¾ƒå°çš„æ€§èƒ½ä¸‹é™ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ–¹æ³•çš„ä¼˜ç‚¹åœ¨äºå¹³è¡¡äº†æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”å…¶åœ¨å‹ç¼©æ–¹é¢çš„æƒè¡¡æ›´åŠ å¾®å¦™å¤æ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¯ä¸Šä¸‹æ–‡å‹ç¼©æ˜¯å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„è®¡ç®—æˆæœ¬çš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°çš„å¹³å‡æ± åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å‹ç¼©ä¸Šä¸‹æ–‡æ–¹é¢è¡¨ç°ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„å‹ç¼©ä»¤ç‰Œæ¶æ„ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥è®­ç»ƒä»¥è¾“å‡ºå¤šç§å‹ç¼©æ¯”ç‡ï¼Œå±•ç¤ºäº†å…¶çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨é¢†åŸŸé—®ç­”æ•°æ®é›†å’Œå¤šç§æ¨¡å‹è§„æ¨¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨é¢å¯¹ä¸åŒå‹ç¼©æ¯”ç‡æ—¶çš„æ€§èƒ½ä¸‹é™ç›¸å¯¹è¾ƒå°ï¼Œè¡¨æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>å°½ç®¡åœ¨ä¸åŒæ¶æ„å’Œè®­ç»ƒæ–¹æ¡ˆä¹‹é—´å­˜åœ¨å¾®å¦™çš„æƒè¡¡ï¼Œä½†è¯¥æ–¹æ³•çš„ä¼˜ç‚¹åœ¨äºèƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡è®¡ç®—æˆæœ¬å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc738dbe849396e06910b53dbf7628da" align="middle">
<img src="https://picx.zhimg.com/v2-931c0352b6b3f984497a4929b17e368c" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Learning-to-Triage-Taint-Flows-Reported-by-Dynamic-Program-Analysis-in-Node-js-Packages"><a href="#Learning-to-Triage-Taint-Flows-Reported-by-Dynamic-Program-Analysis-in-Node-js-Packages" class="headerlink" title="Learning to Triage Taint Flows Reported by Dynamic Program Analysis in   Node.js Packages"></a>Learning to Triage Taint Flows Reported by Dynamic Program Analysis in   Node.js Packages</h2><p><strong>Authors:Ronghao Ni, Aidan Z. H. Yang, Min-Chien Hsu, Nuno Sabino, Limin Jia, Ruben Martins, Darion Cassel, Kevin Cheang</strong></p>
<p>Program analysis tools often produce large volumes of candidate vulnerability reports that require costly manual review, creating a practical challenge: how can security analysts prioritize the reports most likely to be true vulnerabilities?   This paper investigates whether machine learning can be applied to prioritizing vulnerabilities reported by program analysis tools. We focus on Node.js packages and collect a benchmark of 1,883 Node.js packages, each containing one reported ACE or ACI vulnerability. We evaluate a variety of machine learning approaches, including classical models, graph neural networks (GNNs), large language models (LLMs), and hybrid models that combine GNN and LLMs, trained on data based on a dynamic program analysis toolâ€™s output. The top LLM achieves $F_{1} {&#x3D;} 0.915$, while the best GNN and classical ML models reaching $F_{1} {&#x3D;} 0.904$. At a less than 7% false-negative rate, the leading model eliminates 66.9% of benign packages from manual review, taking around 60 ms per package. If the best model is tuned to operate at a precision level of 0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can detect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating strong potential for real-world vulnerability triage. </p>
<blockquote>
<p>ç¨‹åºåˆ†æå·¥å…·ç»å¸¸äº§ç”Ÿå¤§é‡å€™é€‰æ¼æ´æŠ¥å‘Šï¼Œè¿™äº›æŠ¥å‘Šéœ€è¦æ˜‚è´µçš„äººå·¥å®¡æŸ¥ï¼Œä»è€Œå¸¦æ¥ä¸€ä¸ªå®é™…æŒ‘æˆ˜ï¼šå®‰å…¨åˆ†æå¸ˆå¦‚ä½•ä¼˜å…ˆå¤„ç†æœ€å¯èƒ½æ˜¯çœŸå®æ¼æ´çš„æŠ¥å‘Šï¼Ÿæœ¬æ–‡æ—¨åœ¨ç ”ç©¶æ˜¯å¦å¯ä»¥å°†æœºå™¨å­¦ä¹ åº”ç”¨äºä¼˜å…ˆå¤„ç†ç¨‹åºåˆ†æå·¥å…·æŠ¥å‘Šçš„æ¼æ´ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨Node.jsåŒ…ï¼Œå¹¶æ”¶é›†äº†åŒ…å«æŠ¥å‘Šçš„ACEæˆ–ACIæ¼æ´çš„1883ä¸ªNode.jsåŒ…ä½œä¸ºåŸºå‡†æ•°æ®é›†ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§æœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç»å…¸æ¨¡å‹ã€å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥åŠç»“åˆGNNå’ŒLLMçš„æ··åˆæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åŸºäºåŠ¨æ€ç¨‹åºåˆ†æå·¥å…·çš„è¾“å‡ºæ¥è®­ç»ƒæ•°æ®ã€‚è¡¨ç°æœ€ä½³çš„LLMçš„F1åˆ†æ•°è¾¾åˆ°0.915ï¼Œè€Œæœ€ä½³çš„GNNå’Œç»å…¸MLæ¨¡å‹çš„F1åˆ†æ•°è¾¾åˆ°0.904ã€‚åœ¨å‡é˜´æ€§ç‡ä½äº7%çš„æƒ…å†µä¸‹ï¼Œé¢†å…ˆçš„æ¨¡å‹å¯ä»¥ä»äººå·¥å®¡æŸ¥ä¸­å‰”é™¤66.9%çš„è‰¯æ€§åŒ…ï¼Œæ¯ä¸ªåŒ…çš„å¤„ç†æ—¶é—´å¤§çº¦ä¸º60æ¯«ç§’ã€‚å¦‚æœæœ€ä½³æ¨¡å‹è°ƒæ•´ä¸ºåœ¨ç²¾åº¦æ°´å¹³ä¸º0.8ï¼ˆå³å…è®¸æ‰€æœ‰è­¦å‘Šä¸­æœ‰20%çš„è¯¯æŠ¥ï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ£€æµ‹åˆ°99.2%çš„å¯åˆ©ç”¨æ±¡æŸ“æµï¼ŒåŒæ—¶åªé”™è¿‡0.8%ï¼Œæ˜¾ç¤ºå‡ºåœ¨å®é™…æ¼æ´å¤„ç†ä¸­å…·æœ‰å¼ºå¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20739v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯å¤„ç†ç¨‹åºåˆ†æå·¥å…·äº§ç”Ÿçš„å¤§é‡å€™é€‰æ¼æ´æŠ¥å‘Šï¼Œä»¥å¸®åŠ©å®‰å…¨åˆ†æå¸ˆä¼˜å…ˆå¤„ç†æœ€æœ‰å¯èƒ½ä¸ºçœŸå®æ¼æ´çš„æŠ¥å‘Šã€‚é€šè¿‡å¯¹Node.jsåŒ…çš„åŸºå‡†æµ‹è¯•ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ç­‰æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå®ç°äº†ä¸€ç§æœ‰æ•ˆçš„æ¼æ´æŠ¥å‘Šä¼˜å…ˆçº§æ’åºæ–¹æ³•ã€‚é¡¶çº§LLMæ¨¡å‹çš„F1å¾—åˆ†è¾¾åˆ°0.915ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘è¯¯æŠ¥çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜åˆ†ææ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨å¤„ç†ç¨‹åºåˆ†æå·¥å…·äº§ç”Ÿçš„æ¼æ´æŠ¥å‘Šæ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>LLMæ¨¡å‹åœ¨ä¼˜å…ˆå¤„ç†æ¼æ´æŠ¥å‘Šæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒF1å¾—åˆ†è¾¾åˆ°0.915ã€‚</li>
<li>GNNæ¨¡å‹ä»¥åŠç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼ŒF1å¾—åˆ†è¾¾åˆ°0.904ã€‚</li>
<li>é¡¶çº§æ¨¡å‹èƒ½å¤Ÿåœ¨è¾ƒä½çš„è¯¯æŠ¥ç‡ä¸‹ï¼Œæ˜¾è‘—å‡å°‘éœ€è¦äººå·¥å®¡æŸ¥çš„è‰¯æ€§åŒ…æ•°é‡ã€‚</li>
<li>æ¨¡å‹å¤„ç†æ¯ä¸ªåŒ…çš„æ—¶é—´çº¦ä¸º60æ¯«ç§’ï¼Œå…·æœ‰è¾ƒé«˜çš„å¤„ç†æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹å¯ä»¥åœ¨ç²¾åº¦è¾¾åˆ°0.8çš„æƒ…å†µä¸‹ï¼Œæ£€æµ‹å‡ºå¤§éƒ¨åˆ†å¯åˆ©ç”¨çš„æ±¡æŸ“æµï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨çœŸå®ä¸–ç•Œä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†å®‰å…¨åˆ†æå¸ˆåœ¨å¤„ç†å¤§é‡æ¼æ´æŠ¥å‘Šæ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db44813fa4d1a3619653c0865e6b3c24" align="middle">
<img src="https://picx.zhimg.com/v2-1c7caea2f8715f21938d719ae57595a9" align="middle">
<img src="https://picx.zhimg.com/v2-8e71b4b57686a09dff2d640d82d723cd" align="middle">
<img src="https://picx.zhimg.com/v2-2129ff861b9f536e60e145ddd592b740" align="middle">
<img src="https://picx.zhimg.com/v2-db5db91d902b329cfe863f09c53bb2cf" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diagnosing-Visual-Reasoning-Challenges-Insights-and-a-Path-Forward"><a href="#Diagnosing-Visual-Reasoning-Challenges-Insights-and-a-Path-Forward" class="headerlink" title="Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward"></a>Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward</h2><p><strong>Authors:Jing Bi, Guangyu Sun, Ali Vosoughi, Chen Chen, Chenliang Xu</strong></p>
<p>Multimodal large language models (MLLMs) that integrate visual and textual reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual tasks, yet continue to exhibit visual hallucinations and an over-reliance on textual priors. We present a systematic diagnosis of state-of-the-art vision-language models using a three-stage evaluation framework, uncovering key failure modes. To address these, we propose an agent-based architecture that combines LLM reasoning with lightweight visual modules, enabling fine-grained analysis and iterative refinement of reasoning chains. Our results highlight future visual reasoning models should focus on integrating a broader set of specialized tools for analyzing visual content. Our system achieves significant gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or surpassing much larger models. We will release our framework and evaluation suite to facilitate future research. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èåˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨ç†ï¼Œåˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¥è§£å†³å¤æ‚çš„è§†è§‰ä»»åŠ¡ï¼Œä½†ä»ç„¶ä¼šå‡ºç°è§†è§‰å¹»è§‰ï¼Œè¿‡åº¦ä¾èµ–æ–‡æœ¬å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶å¯¹æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯Šæ–­ï¼Œæ­ç¤ºäº†å…³é”®çš„å¤±è´¥æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„æ¶æ„ï¼Œå®ƒå°†LLMæ¨ç†ä¸è½»é‡çº§è§†è§‰æ¨¡å—ç›¸ç»“åˆï¼Œå®ç°å¯¹æ¨ç†é“¾çš„ç²¾ç»†åˆ†æä»¥åŠè¿­ä»£ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œæœªæ¥çš„è§†è§‰æ¨ç†æ¨¡å‹åº”ä¾§é‡äºé›†æˆæ›´å¤šç”¨äºåˆ†æè§†è§‰å†…å®¹çš„ä¸“ç”¨å·¥å…·é›†ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨MMMUä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ˆæé«˜10.3ä¸ªç‚¹ï¼‰ï¼Œåœ¨MathVistaä¸Šè¶…è¿‡åŸºçº¿æ¨¡å‹7B 6ä¸ªç‚¹ï¼Œæ€§èƒ½ä¸æ›´å¤§çš„æ¨¡å‹ç›¸åŒ¹é…ç”šè‡³è¶…è¿‡å®ƒä»¬ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ¡†æ¶å’Œè¯„ä¼°å¥—ä»¶ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20696v1">PDF</a> 5 pages</p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡ç»“åˆè§†è§‰å’Œæ–‡æœ¬æ¨ç†ï¼Œè¿ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ¥è§£å†³å¤æ‚çš„è§†è§‰ä»»åŠ¡ï¼Œä½†ä»å­˜åœ¨è§†è§‰é”™è§‰å’Œå¯¹æ–‡æœ¬å…ˆéªŒçŸ¥è¯†çš„è¿‡åº¦ä¾èµ–ã€‚æœ¬ç ”ç©¶é‡‡ç”¨ä¸€ä¸ªä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶å¯¹æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿçš„è¯Šæ–­ï¼Œæ­ç¤ºäº†å…¶å…³é”®å¤±è´¥æ¨¡å¼ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„æ¶æ„ï¼Œå°†LLMæ¨ç†ä¸è½»é‡çº§è§†è§‰æ¨¡å—ç›¸ç»“åˆï¼Œå®ç°å¯¹æ¨ç†é“¾çš„ç²¾ç»†åˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœªæ¥çš„è§†è§‰æ¨ç†æ¨¡å‹åº”ä¸“æ³¨äºæ•´åˆæ›´å¤šç”¨äºåˆ†æè§†è§‰å†…å®¹çš„ä¸“ç”¨å·¥å…·ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨MMMUå’ŒMathVistaä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ï¼ˆåˆ†åˆ«è¶…è¿‡åŸºçº¿æ¨¡å‹7Bçš„+10.3å’Œ+6.0ï¼‰ï¼Œä¸æ›´å¤§çš„æ¨¡å‹ç›¸æ¯”è¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ¡†æ¶å’Œè¯„ä¼°å¥—ä»¶ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡ç»“åˆè§†è§‰å’Œæ–‡æœ¬æ¨ç†æ¥è§£å†³å¤æ‚çš„è§†è§‰ä»»åŠ¡ã€‚</li>
<li>MLLMsä»å­˜åœ¨è§†è§‰é”™è§‰å’Œå¯¹æ–‡æœ¬å…ˆéªŒçŸ¥è¯†çš„è¿‡åº¦ä¾èµ–çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶å¯¹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿçš„è¯Šæ–­ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„æ¶æ„ï¼Œç»“åˆLLMæ¨ç†å’Œè½»é‡çº§è§†è§‰æ¨¡å—ï¼Œä»¥æ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æœªæ¥çš„è§†è§‰æ¨ç†æ¨¡å‹åº”ä¸“æ³¨äºæ•´åˆæ›´å¤šç”¨äºåˆ†æè§†è§‰å†…å®¹çš„ä¸“ç”¨å·¥å…·ã€‚</li>
<li>ç³»ç»Ÿåœ¨MMMUå’ŒMathVistaä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œè¶…è¿‡äº†åŸºçº¿æ¨¡å‹ã€‚</li>
<li>å°†å‘å¸ƒè¯„ä¼°æ¡†æ¶å’Œå¥—ä»¶ä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20696">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d186c1ec2215abb85fbd8027b68519c" align="middle">
<img src="https://picx.zhimg.com/v2-66e29526aaa1fa8f6e94e085ea62de82" align="middle">
<img src="https://picx.zhimg.com/v2-d1fecdcb0217ef0cdfd9bc4cfc37e8f1" align="middle">
<img src="https://picx.zhimg.com/v2-36b56607411d8384038b1c2984c53922" align="middle">
<img src="https://picx.zhimg.com/v2-9fdef8b5dc5e569fb7e3c506069c9f05" align="middle">
<img src="https://picx.zhimg.com/v2-4c867b38dcc6a9f200b3730124271d1c" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Finding-the-Sweet-Spot-Trading-Quality-Cost-and-Speed-During-Inference-Time-LLM-Reflection"><a href="#Finding-the-Sweet-Spot-Trading-Quality-Cost-and-Speed-During-Inference-Time-LLM-Reflection" class="headerlink" title="Finding the Sweet Spot: Trading Quality, Cost, and Speed During   Inference-Time LLM Reflection"></a>Finding the Sweet Spot: Trading Quality, Cost, and Speed During   Inference-Time LLM Reflection</h2><p><strong>Authors:Jack Butler, Nikita Kozodoi, Zainab Afolabi, Brian Tyacke, Gaiar Baimuratov</strong></p>
<p>As Large Language Models (LLMs) continue to evolve, practitioners face increasing options for enhancing inference-time performance without model retraining, including budget tuning and multi-step techniques like self-reflection. While these methods improve output quality, they create complex trade-offs among accuracy, cost, and latency that remain poorly understood across different domains. This paper systematically compares self-reflection and budget tuning across mathematical reasoning and translation tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and Mistral families, along with other models under varying reflection depths and compute budgets to derive Pareto optimal performance frontiers. Our analysis reveals substantial domain dependent variation in self-reflection effectiveness, with performance gains up to 220% in mathematical reasoning. We further investigate how reflection round depth and feedback mechanism quality influence performance across model families. To validate our findings in a real-world setting, we deploy a self-reflection enhanced marketing content localisation system at Lounge by Zalando, where it shows market-dependent effectiveness, reinforcing the importance of domain specific evaluation when deploying these techniques. Our results provide actionable guidance for selecting optimal inference strategies given specific domains and resource constraints. We open source our self-reflection implementation for reproducibility at <a target="_blank" rel="noopener" href="https://github.com/aws-samples/sample-genai-reflection-for-bedrock">https://github.com/aws-samples/sample-genai-reflection-for-bedrock</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸æ–­å‘å±•ï¼Œå®è·µè€…åœ¨ä¸éœ€è¦æ¨¡å‹é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé¢ä¸´è¶Šæ¥è¶Šå¤šçš„æé«˜æ¨ç†æ—¶é—´æ€§èƒ½çš„é€‰æ‹©ï¼ŒåŒ…æ‹¬é¢„ç®—è°ƒæ•´å’Œå¤šé‡æ­¥éª¤æŠ€æœ¯ï¼Œå¦‚è‡ªæˆ‘åæ€ã€‚è™½ç„¶è¿™äº›æ–¹æ³•æé«˜äº†è¾“å‡ºè´¨é‡ï¼Œä½†å®ƒä»¬ä¹Ÿåœ¨å‡†ç¡®æ€§ã€æˆæœ¬å’Œå»¶è¿Ÿä¹‹é—´åˆ›é€ äº†å¤æ‚çš„æƒè¡¡ï¼Œè¿™äº›æƒè¡¡åœ¨ä¸åŒé¢†åŸŸä¸­çš„ç†è§£ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¯”è¾ƒäº†æ•°å­¦æ¨ç†å’Œç¿»è¯‘ä»»åŠ¡ä¸­çš„è‡ªæˆ‘åæ€å’Œé¢„ç®—è°ƒæ•´ã€‚æˆ‘ä»¬è¯„ä¼°äº†çªå‡ºçš„LLMï¼ŒåŒ…æ‹¬Anthropic Claudeã€Amazon Novaå’ŒMistralå®¶æ—ï¼Œä»¥åŠå…¶ä»–æ¨¡å‹åœ¨ä¸åŒåå°„æ·±åº¦å’Œè®¡ç®—é¢„ç®—ä¸‹ï¼Œå¾—å‡ºå¸•ç´¯æ‰˜æœ€ä¼˜æ€§èƒ½è¾¹ç•Œã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†è‡ªæˆ‘åæ€æ•ˆæœåœ¨é¢†åŸŸä¾èµ–æ–¹é¢çš„å·¨å¤§å·®å¼‚ï¼Œæ•°å­¦æ¨ç†çš„æ€§èƒ½æå‡é«˜è¾¾220%ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ç ”ç©¶äº†åå°„å›åˆæ·±åº¦å’Œåé¦ˆæœºåˆ¶è´¨é‡å¯¹è·¨æ¨¡å‹å®¶æ—æ€§èƒ½çš„å½±å“ã€‚ä¸ºäº†åœ¨ç°å®ä¸–ç•Œä¸­éªŒè¯æˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬åœ¨Zalandoçš„ä¼‘æ¯å®¤éƒ¨ç½²äº†å¢å¼ºè‡ªæˆ‘åæ€çš„è¥é”€å†…å®¹æœ¬åœ°åŒ–ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæ˜¾ç¤ºäº†å¸‚åœºä¾èµ–çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†éƒ¨ç½²è¿™äº›æŠ€æœ¯æ—¶é¢†åŸŸç‰¹å®šè¯„ä¼°çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæä¾›äº†åœ¨ç‰¹å®šé¢†åŸŸå’Œèµ„æºçº¦æŸä¸‹é€‰æ‹©æœ€ä½³æ¨ç†ç­–ç•¥çš„å¯æ“ä½œæŒ‡å¯¼ã€‚æˆ‘ä»¬å·²å°†è‡ªæˆ‘åæ€å®ç°åœ¨<a target="_blank" rel="noopener" href="https://github.com/aws-samples/sample-genai-reflection-for-bedrock%E4%B8%8A%E5%BC%80%E6%BA%90%EF%BC%8C%E4%BB%A5%E7%A1%AE%E4%BF%9D%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E3%80%82">https://github.com/aws-samples/sample-genai-reflection-for-bedrockä¸Šå¼€æºï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20653v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŒ–ç­–ç•¥æ¯”è¾ƒï¼šæœ¬æ–‡æ¢è®¨äº†ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Anthropic Claudeã€Amazon Novaå’ŒMistralç³»åˆ—æ¨¡å‹ï¼‰åœ¨æ•°å­¦æ¨ç†å’Œç¿»è¯‘ä»»åŠ¡ä¸­çš„è‡ªæˆ‘åæ€å’Œé¢„ç®—è°ƒæ•´ä¼˜åŒ–ç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼Œè‡ªæˆ‘åæ€åœ¨ä¸åŒé¢†åŸŸä¸­çš„æ•ˆæœå·®å¼‚æ˜¾è‘—ï¼Œæ•°å­¦æ¨ç†æ–¹é¢çš„æ€§èƒ½æå‡æœ€é«˜å¯è¾¾220%ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†åæ€è½®æ¬¡æ·±åº¦å’Œåé¦ˆæœºåˆ¶è´¨é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æœ€ç»ˆï¼Œé€šè¿‡åœ¨Zalandoçš„Loungeéƒ¨ç½²è‡ªæˆ‘åæ€å¢å¼ºçš„è¥é”€å†…å®¹æœ¬åœ°åŒ–ç³»ç»Ÿï¼ŒéªŒè¯äº†å…¶åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„å¸‚åœºä¾èµ–æ€§æ•ˆæœã€‚æœ¬ç ”ç©¶ä¸ºç‰¹å®šé¢†åŸŸå’Œèµ„æºçº¦æŸä¸‹é€‰æ‹©æœ€ä½³æ¨ç†ç­–ç•¥æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŒ–ç­–ç•¥åŒ…æ‹¬é¢„ç®—è°ƒæ•´å’Œè‡ªæˆ‘åæ€æŠ€æœ¯ã€‚</li>
<li>è¿™äº›ä¼˜åŒ–ç­–ç•¥å¯æ”¹å–„æ¨ç†æ—¶é—´æ€§èƒ½è€Œä¸éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>è‡ªæˆ‘åæ€åœ¨æ•°å­¦æ¨ç†é¢†åŸŸçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œæœ€é«˜å¯è¾¾220%ã€‚</li>
<li>ä¸åŒæ¨¡å‹å®¶æ—åœ¨è‡ªæˆ‘åæ€æ•ˆæœä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>åæ€è½®æ¬¡æ·±åº¦å’Œåé¦ˆæœºåˆ¶è´¨é‡å½±å“æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>éƒ¨ç½²è‡ªæˆ‘åæ€å¢å¼ºçš„æœ¬åœ°åŒ–ç³»ç»ŸéªŒè¯äº†å…¶åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98c8ddbc969bb2a7b5be414b4e8ce4f0" align="middle">
<img src="https://picx.zhimg.com/v2-0b31cbe8511d485edb5cc5cabdaf5449" align="middle">
<img src="https://picx.zhimg.com/v2-29db2d1210fba6c261db26aba97c7443" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LM-mixup-Text-Data-Augmentation-via-Language-Model-based-Mixup"><a href="#LM-mixup-Text-Data-Augmentation-via-Language-Model-based-Mixup" class="headerlink" title="LM-mixup: Text Data Augmentation via Language Model based Mixup"></a>LM-mixup: Text Data Augmentation via Language Model based Mixup</h2><p><strong>Authors:Zhijie Deng, Zhouan Shen, Ling Li, Yao Zhou, Zhaowei Zhu, Yanji He, Wei Wang, Jiaheng Wei</strong></p>
<p>Instruction tuning is crucial for aligning Large Language Models (LLMs), yet the quality of instruction-following data varies significantly. While high-quality data is paramount, it is often scarce; conversely, abundant low-quality data is frequently discarded, leading to substantial information loss. Existing data augmentation methods struggle to augment this low-quality data effectively, and the evaluation of such techniques remains poorly defined. To address this, we formally define the task of Instruction Distillation: distilling multiple low-quality and redundant inputs into high-quality and coherent instruction-output pairs. Specifically, we introduce a comprehensive data construction pipeline to create MIXTURE, a 144K-sample dataset pairing low-quality or semantically redundant imperfect instruction clusters with their high-quality distillations. We then introduce LM-Mixup, by first performing supervised fine-tuning on MIXTURE and then optimizing it with reinforcement learning. This process uses three complementary reward signals: quality, semantic alignment, and format compliance, via Group Relative Policy Optimization (GRPO). We demonstrate that LM-Mixup effectively augments imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for only about 3% of the entire dataset, not only surpasses full-dataset training but also competes with state-of-the-art high-quality data selection methods across multiple benchmarks. Our work establishes that low-quality data is a valuable resource when properly distilled and augmented with LM-Mixup, significantly enhancing the efficiency and performance of instruction-tuned LLMs. </p>
<blockquote>
<p>æŒ‡ä»¤è°ƒæ•´å¯¹äºå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œä½†æŒ‡ä»¤éµå¾ªæ•°æ®çš„è´¨é‡å·®å¼‚å¾ˆå¤§ã€‚è™½ç„¶é«˜è´¨é‡æ•°æ®è‡³å…³é‡è¦ï¼Œä½†å®ƒå¾€å¾€å¾ˆç¨€ç¼ºï¼›ç›¸åï¼Œå¤§é‡ä½è´¨é‡æ•°æ®ç»å¸¸è¢«ä¸¢å¼ƒï¼Œå¯¼è‡´å¤§é‡ä¿¡æ¯ä¸¢å¤±ã€‚ç°æœ‰çš„æ•°æ®å¢å¼ºæ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°å¢å¼ºè¿™ç§ä½è´¨é‡æ•°æ®ï¼Œè€Œä¸”å¯¹è¿™äº›æŠ€æœ¯çš„è¯„ä¼°å®šä¹‰å°šä¸æ¸…æ¥šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ­£å¼å®šä¹‰äº†æŒ‡ä»¤è’¸é¦çš„ä»»åŠ¡ï¼šå°†å¤šä¸ªä½è´¨é‡å’Œå†—ä½™çš„è¾“å…¥è’¸é¦æˆé«˜è´¨é‡å’Œè¿è´¯çš„æŒ‡ä»¤-è¾“å‡ºå¯¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ„å»ºæµç¨‹æ¥åˆ›å»ºMIXTUREï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰14ä¸‡æ ·æœ¬çš„æ•°æ®é›†ï¼Œå°†ä½è´¨é‡æˆ–è¯­ä¹‰ä¸Šå†—ä½™çš„ä¸å®Œç¾æŒ‡ä»¤é›†ç¾¤ä¸å®ƒä»¬çš„é«˜è´¨é‡è’¸é¦é…å¯¹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†LM-Mixupï¼Œé¦–å…ˆä½¿ç”¨MIXTUREè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä¸€è¿‡ç¨‹ä½¿ç”¨ä¸‰ç§äº’è¡¥çš„å¥–åŠ±ä¿¡å·ï¼šè´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œæ ¼å¼åˆè§„æ€§ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚æˆ‘ä»¬è¯æ˜LM-Mixupå¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºä¸å®Œç¾çš„æ•°æ®é›†ï¼šåœ¨è’¸é¦æ•°æ®ä¸Šå¾®è°ƒLLMï¼ˆä»…å æ•´ä¸ªæ•°æ®é›†çš„3%ï¼‰ï¼Œä¸ä»…è¶…è¿‡äº†å…¨æ•°æ®é›†çš„è®­ç»ƒï¼Œè€Œä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¸å›½å®¶æœ€å…ˆè¿›çš„ä¼˜è´¨æ•°æ®é€‰æ‹©æ–¹æ³•ç›¸ç«äº‰ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œå½“é€‚å½“è’¸é¦å’Œç”¨LM-Mixupå¢å¼ºæ—¶ï¼Œä½è´¨é‡æ•°æ®æ˜¯ä¸€ç§æœ‰ä»·å€¼çš„èµ„æºï¼Œå¯ä»¥æ˜¾è‘—æé«˜æŒ‡ä»¤è°ƒæ•´å‹LLMçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20449v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æŒ‡ä»¤è’¸é¦ä»»åŠ¡çš„é‡è¦æ€§ï¼Œé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°æ®è´¨é‡é—®é¢˜ï¼Œæå‡ºäº†æŒ‡ä»¤è’¸é¦çš„æ¦‚å¿µã€‚æ–‡ç« é€šè¿‡æ„å»ºMIXTUREæ•°æ®é›†ï¼Œå°†ä½è´¨é‡å’Œå†—ä½™çš„æŒ‡ä»¤è¾“å…¥è½¬åŒ–ä¸ºé«˜è´¨é‡å’Œè¿è´¯çš„æŒ‡ä»¤è¾“å‡ºå¯¹ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†LM-Mixupæ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œä½¿ç”¨è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œæ ¼å¼åˆè§„æ€§ä¸‰ä¸ªäº’è¡¥å¥–åŠ±ä¿¡å·è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒLM-Mixupèƒ½æœ‰æ•ˆå¢å¼ºä¸å®Œç¾çš„æ•°æ®é›†ï¼Œä»…ä½¿ç”¨çº¦3%çš„è’¸é¦æ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä¸ä»…è¶…è¶Šäº†å…¨æ•°æ®é›†è®­ç»ƒçš„æ•ˆæœï¼Œè€Œä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸é«˜è´¨é‡æ•°æ®é€‰æ‹©æ–¹æ³•ç›¸ç«äº‰ã€‚è¿™è¡¨æ˜ï¼Œä½è´¨é‡æ•°æ®åœ¨é€‚å½“è’¸é¦å’ŒLM-Mixupå¢å¼ºåï¼Œèƒ½æ˜¾è‘—æé«˜æŒ‡ä»¤è°ƒæ•´å‹LLMçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤è’¸é¦æ˜¯å°†ä½è´¨é‡å’Œå†—ä½™çš„æŒ‡ä»¤è¾“å…¥è½¬åŒ–ä¸ºé«˜è´¨é‡å’Œè¿è´¯çš„æŒ‡ä»¤è¾“å‡ºçš„è¿‡ç¨‹ã€‚</li>
<li>MIXTUREæ•°æ®é›†æ˜¯é€šè¿‡ä¸€ä¸ªç»¼åˆæ•°æ®æ„å»ºç®¡é“åˆ›å»ºçš„ï¼ŒåŒ…å«ä½è´¨é‡æˆ–è¯­ä¹‰å†—ä½™çš„æŒ‡ä»¤é›†ç¾¤ä¸å…¶é«˜è´¨é‡è’¸é¦é…å¯¹ã€‚</li>
<li>LM-Mixupæ–¹æ³•ç»“åˆäº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼Œä½¿ç”¨ä¸‰ä¸ªäº’è¡¥å¥–åŠ±ä¿¡å·æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä½è´¨é‡æ•°æ®åœ¨ç»è¿‡LM-Mixupå¤„ç†åè½¬åŒ–ä¸ºæœ‰ä»·å€¼çš„èµ„æºï¼Œèƒ½æœ‰æ•ˆå¢å¼ºæŒ‡ä»¤è°ƒæ•´å‹LLMçš„æ€§èƒ½ã€‚</li>
<li>ä»…ä½¿ç”¨çº¦3%çš„è’¸é¦æ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œå…¶æ•ˆæœè¶…è¶Šäº†å…¨æ•°æ®é›†è®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†ä½è´¨é‡æ•°æ®åœ¨é€‚å½“å¤„ç†åçš„æ½œåœ¨ä»·å€¼ï¼Œå¯¹äºæé«˜LLMæ€§èƒ½å’Œæ•ˆç‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e7a1a871dd837140f02425045c6bbd1" align="middle">
<img src="https://picx.zhimg.com/v2-a8f8a9d03f8725e63149b4658cad7118" align="middle">
<img src="https://picx.zhimg.com/v2-a4b237459a8e31c7976f1e82a75fd574" align="middle">
<img src="https://picx.zhimg.com/v2-2fe86cea0f90840ae720bc1b859841ad" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Beyond-Text-Multimodal-Jailbreaking-of-Vision-Language-and-Audio-Models-through-Perceptually-Simple-Transformations"><a href="#Beyond-Text-Multimodal-Jailbreaking-of-Vision-Language-and-Audio-Models-through-Perceptually-Simple-Transformations" class="headerlink" title="Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models   through Perceptually Simple Transformations"></a>Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models   through Perceptually Simple Transformations</h2><p><strong>Authors:Divyanshu Kumar, Shreyas Jena, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi</strong></p>
<p>Multimodal large language models (MLLMs) have achieved remarkable progress, yet remain critically vulnerable to adversarial attacks that exploit weaknesses in cross-modal processing. We present a systematic study of multimodal jailbreaks targeting both vision-language and audio-language models, showing that even simple perceptual transformations can reliably bypass state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts across three high-risk safety categories harmful content, CBRN (Chemical, Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation Material) tested against seven frontier models. We explore the effectiveness of attack techniques on MLLMs, including FigStep-Pro (visual keyword decomposition), Intelligent Masking (semantic obfuscation), and audio perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe vulnerabilities: models with almost perfect text-only safety (0% ASR) suffer &gt;75% attack success under perceptually modified inputs, with FigStep-Pro achieving up to 89% ASR in Llama-4 variants. Audio-based attacks further uncover provider-specific weaknesses, with even basic modality transfer yielding 25% ASR for technical queries. These findings expose a critical gap between text-centric alignment and multimodal threats, demonstrating that current safeguards fail to generalize across cross-modal attacks. The accessibility of these attacks, which require minimal technical expertise, suggests that robust multimodal AI safety will require a paradigm shift toward broader semantic-level reasoning to mitigate possible risks. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä»ç„¶é¢ä¸´ç€åˆ©ç”¨è·¨æ¨¡æ€å¤„ç†å¼±ç‚¹å‘èµ·å¯¹æŠ—æ€§æ”»å‡»çš„ä¸¥é‡å¨èƒã€‚æˆ‘ä»¬å¯¹é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹å’ŒéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€è¶Šç‹±è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œå‘ç°å³ä½¿ç®€å•çš„æ„ŸçŸ¥è½¬æ¢ä¹Ÿèƒ½å¯é ç»•è¿‡æœ€æ–°å®‰å…¨è¿‡æ»¤å™¨ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†é’ˆå¯¹ä¸‰å¤§é«˜é£é™©å®‰å…¨ç±»åˆ«ï¼ˆæœ‰å®³å†…å®¹ã€CBRNï¼ˆåŒ–å­¦ã€ç”Ÿç‰©ã€è¾å°„ã€æ ¸ï¼‰å’ŒCSEMï¼ˆå„¿ç«¥æ€§å‰¥å‰Šææ–™ï¼‰ï¼‰çš„1900ä¸ªå¯¹æŠ—æ€§æç¤ºï¼Œå¹¶å¯¹ä¸ƒæ¬¾å‰æ²¿æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ã€‚æˆ‘ä»¬æ¢è®¨äº†æ”»å‡»æŠ€æœ¯åœ¨MLLMsä¸Šçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬FigStep-Proï¼ˆè§†è§‰å…³é”®è¯åˆ†è§£ï¼‰ã€æ™ºèƒ½æ©ç ï¼ˆè¯­ä¹‰æ¨¡ç³Šï¼‰å’ŒéŸ³é¢‘æ‰°åŠ¨ï¼ˆæ³¢å›å£°ã€æ³¢éŸ³é«˜ã€æ³¢é€Ÿåº¦ï¼‰ã€‚ç»“æœæ­ç¤ºäº†ä¸¥é‡çš„æ¼æ´ï¼šå‡ ä¹å®Œç¾çš„çº¯æ–‡æœ¬å®‰å…¨æ¨¡å‹ï¼ˆæ”»å‡»æˆåŠŸç‡ä¸º0%ï¼‰åœ¨æ„ŸçŸ¥ä¿®æ”¹åçš„è¾“å…¥ä¸‹é­å—è¶…è¿‡75%çš„æ”»å‡»æˆåŠŸï¼Œå…¶ä¸­FigStep-Proåœ¨Llama-4å˜ç§ä¸­è¾¾åˆ°é«˜è¾¾89%çš„æ”»å‡»æˆåŠŸç‡ã€‚åŸºäºéŸ³é¢‘çš„æ”»å‡»è¿›ä¸€æ­¥æ­ç¤ºäº†ç‰¹å®šä¾›åº”å•†çš„å¼±ç‚¹ï¼Œç”šè‡³åŸºæœ¬çš„æ¨¡æ€è½¬æ¢ä¹Ÿä¼šäº§ç”Ÿ25%çš„æ”»å‡»æˆåŠŸç‡ï¼ˆé’ˆå¯¹æŠ€æœ¯æŸ¥è¯¢ï¼‰ã€‚è¿™äº›å‘ç°æš´éœ²äº†ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å¯¹ç­–å’Œå¤šæ¨¡æ€å¨èƒä¹‹é—´çš„å…³é”®å·®è·ï¼Œè¯æ˜äº†å½“å‰çš„å®‰å…¨æªæ–½æ— æ³•åœ¨è·¨æ¨¡æ€æ”»å‡»ä¸­æ¨å¹¿ã€‚è¿™äº›æ”»å‡»æ˜“äºå®æ–½ï¼Œå‡ ä¹ä¸éœ€è¦ä¸“ä¸šæŠ€æœ¯çŸ¥è¯†ï¼Œå› æ­¤ç¨³å¥çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½å®‰å…¨å°†éœ€è¦è½¬å‘æ›´å¹¿æ³›è¯­ä¹‰å±‚é¢æ¨ç†çš„æ–°èŒƒå¼ï¼Œä»¥å‡è½»æ½œåœ¨é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20223v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è·¨æ¨¡æ€å¤„ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—æ¼æ´ï¼Œå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹å’ŒéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€è¶Šç‹±å°è¯•è¡¨æ˜ï¼Œå³ä½¿ç®€å•çš„æ„ŸçŸ¥è½¬æ¢ä¹Ÿèƒ½å¯é ç»•è¿‡æœ€å…ˆè¿›çš„å®‰å…¨è¿‡æ»¤å™¨ã€‚é’ˆå¯¹æœ‰å®³å†…å®¹ã€åŒ–å­¦æ­¦å™¨ã€ç”Ÿç‰©æ­¦å™¨ç­‰é«˜é£é™©å®‰å…¨ç±»åˆ«çš„æµ‹è¯•æ˜¾ç¤ºï¼Œæ”»å‡»æŠ€æœ¯æ•ˆæœæ˜¾è‘—ï¼Œç°æœ‰æ¨¡å‹å­˜åœ¨ä¸¥é‡æ¼æ´ã€‚è¿™äº›æ”»å‡»åŒ…æ‹¬è§†è§‰å…³é”®è¯åˆ†è§£ã€æ™ºèƒ½æ©ç å’ŒéŸ³é¢‘æ‰°åŠ¨ç­‰æŠ€æœ¯ã€‚ç»“æœæ­ç¤ºäº†è·¨æ¨¡æ€æ”»å‡»çš„å¨èƒå’Œç°æœ‰å®‰å…¨æªæ–½ä¹‹é—´çš„å·¨å¤§å·®è·ï¼Œæ˜¾ç¤ºå‡ºç°æœ‰å®‰å…¨æªæ–½æ— æ³•æœ‰æ•ˆåº”å¯¹è·¨æ¨¡æ€æ”»å‡»çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œè¿™äº›æ”»å‡»åˆ©ç”¨è·¨æ¨¡æ€å¤„ç†çš„å¼±ç‚¹è¿›è¡Œæ”»å‡»ã€‚</li>
<li>ç®€å•çš„æ„ŸçŸ¥è½¬æ¢å¯ä»¥ç»•è¿‡æœ€å…ˆè¿›çš„å®‰å…¨è¿‡æ»¤å™¨ã€‚</li>
<li>åœ¨é«˜é£é™©å®‰å…¨ç±»åˆ«æµ‹è¯•ä¸­ï¼Œæ”»å‡»æŠ€æœ¯æ•ˆæœæ˜¾è‘—ï¼Œç°æœ‰æ¨¡å‹å­˜åœ¨ä¸¥é‡æ¼æ´ã€‚</li>
<li>æµ‹è¯•æ¶µç›–äº†å¤šç§æ”»å‡»æŠ€æœ¯ï¼ŒåŒ…æ‹¬è§†è§‰å…³é”®è¯åˆ†è§£ã€æ™ºèƒ½æ©ç å’ŒéŸ³é¢‘æ‰°åŠ¨ç­‰ã€‚</li>
<li>è·¨æ¨¡æ€æ”»å‡»çš„å¨èƒå’Œç°æœ‰å®‰å…¨æªæ–½ä¹‹é—´å­˜åœ¨å·¨å¤§å·®è·ï¼Œæ˜¾ç¤ºå‡ºç°æœ‰å®‰å…¨æªæ–½æ— æ³•æœ‰æ•ˆåº”å¯¹è·¨æ¨¡æ€æ”»å‡»çš„é—®é¢˜ã€‚</li>
<li>éŸ³é¢‘æ”»å‡»æ­ç¤ºäº†ç‰¹å®šä¾›åº”å•†çš„å¼±ç‚¹ï¼Œå³ä½¿æ˜¯åŸºæœ¬çš„æ¨¡æ€è½¬æ¢ä¹Ÿä¼šäº§ç”Ÿå¨èƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eae7128380c2c12c4d4c3846a4bcc409" align="middle">
<img src="https://picx.zhimg.com/v2-6ce5253c371246d6bc19de916eafe302" align="middle">
<img src="https://picx.zhimg.com/v2-af47d19dffe0fd176f1f073db9c31712" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Teaming-LLMs-to-Detect-and-Mitigate-Hallucinations"><a href="#Teaming-LLMs-to-Detect-and-Mitigate-Hallucinations" class="headerlink" title="Teaming LLMs to Detect and Mitigate Hallucinations"></a>Teaming LLMs to Detect and Mitigate Hallucinations</h2><p><strong>Authors:Demian Till, John Smeaton, Peter Haubrick, Gouse Saheb, Florian Graef, David Berman</strong></p>
<p>Recent work has demonstrated state-of-the-art results in large language model (LLM) hallucination detection and mitigation through consistency-based approaches which involve aggregating multiple responses sampled from a single LLM for a given prompt. These approaches help offset limitations stemming from the imperfect data on which LLMs are trained, which includes biases and under-representation of information required at deployment time among other limitations which can lead to hallucinations. We show that extending these single-model consistency methods to combine responses from multiple LLMs with different training data, training schemes and model architectures can result in substantial further improvements in hallucination detection and mitigation capabilities beyond their single-model consistency counterparts. We evaluate this â€œconsortium consistencyâ€ approach across many model teams from a pool of 15 LLMs and explore under what conditions it is beneficial to team together different LLMs in this manner. Further, we show that these performance improvements often come with reduced inference costs, offsetting a significant drawback with single-model consistency methods. </p>
<blockquote>
<p>æœ€è¿‘çš„å·¥ä½œé€šè¿‡åŸºäºä¸€è‡´æ€§çš„æ–¹æ³•ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»è§‰æ£€æµ‹å’Œç¼“è§£æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚è¿™äº›æ–¹æ³•æ¶‰åŠå¯¹ç»™å®šæç¤ºå•ä¸ªLLMç”Ÿæˆçš„å¤šä¸ªå“åº”è¿›è¡Œèšåˆã€‚å®ƒä»¬æœ‰åŠ©äºæŠµæ¶ˆLLMè®­ç»ƒæ—¶å› ä¸å®Œç¾æ•°æ®æ‰€å¸¦æ¥çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬åè§ã€éƒ¨ç½²æ‰€éœ€ä¿¡æ¯çš„ä¸è¶³ç­‰é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¯èƒ½å¯¼è‡´å¹»è§‰ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå°†è¿™äº›å•æ¨¡å‹ä¸€è‡´æ€§æ–¹æ³•æ‰©å±•åˆ°ç»“åˆæ¥è‡ªå…·æœ‰ä¸åŒè®­ç»ƒæ•°æ®ã€è®­ç»ƒæ–¹æ¡ˆå’Œæ¨¡å‹æ¶æ„çš„å¤šä¸ªLLMçš„å“åº”ï¼Œå¯ä»¥åœ¨å¹»è§‰æ£€æµ‹å’Œç¼“è§£èƒ½åŠ›æ–¹é¢å®ç°é‡å¤§æ”¹è¿›ï¼Œè¶…è¶Šå…¶å•æ¨¡å‹ä¸€è‡´æ€§æ–¹æ³•ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¿™ç§è·¨å¤šä¸ªæ¨¡å‹å›¢é˜Ÿçš„â€œè”ç›Ÿä¸€è‡´æ€§â€æ–¹æ³•ï¼Œä»ä¸€ç»„åŒ…å«15ä¸ªLLMçš„æ¨¡å‹ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ¢è®¨äº†å“ªäº›æƒ…å†µä¸‹é€šè¿‡è¿™ç§åˆä½œå¯ä»¥æé«˜ä¸åŒLLMçš„å¥½å¤„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°æ€§èƒ½æå‡é€šå¸¸ä¼´éšç€æ¨ç†æˆæœ¬çš„é™ä½ï¼ŒæŠµæ¶ˆäº†å•ä¸€æ¨¡å‹ä¸€è‡´æ€§æ–¹æ³•çš„é‡å¤§ç¼ºé™·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19507v2">PDF</a> Accepted to NeurIPS 2025 workshop on Reliable ML from Unreliable Data</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»è§‰æ£€æµ‹ä¸ç¼“è§£é¢†åŸŸï¼Œæœ€æ–°å·¥ä½œå±•ç¤ºäº†åŸºäºä¸€è‡´æ€§æ–¹æ³•çš„å…ˆè¿›ç»“æœã€‚æ­¤æ–¹æ³•é€šè¿‡å¯¹å•ä¸€LLMçš„å¤šä¸ªå“åº”è¿›è¡Œèšåˆï¼Œæ¥åº”å¯¹æ¨¡å‹è®­ç»ƒæ•°æ®çš„ä¸å®Œç¾é—®é¢˜ï¼ŒåŒ…æ‹¬åè§å’Œéƒ¨ç½²æ—¶æ‰€éœ€ä¿¡æ¯çš„ä¸è¶³ç­‰ã€‚é€šè¿‡å°†å•ä¸ªæ¨¡å‹çš„ä¸€è‡´æ€§æ–¹æ³•æ‰©å±•åˆ°ç»“åˆå¤šä¸ªå…·æœ‰ä¸åŒè®­ç»ƒæ•°æ®ã€è®­ç»ƒæ–¹æ¡ˆå’Œæ¨¡å‹æ¶æ„çš„LLMå“åº”ï¼Œèƒ½å¤Ÿåœ¨å¹»è§‰æ£€æµ‹ä¸ç¼“è§£èƒ½åŠ›ä¸Šå®ç°è¿›ä¸€æ­¥çš„æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†â€œè”ç›Ÿä¸€è‡´æ€§â€æ–¹æ³•ï¼Œå¹¶åœ¨åŒ…å«15ä¸ªLLMçš„å¤šä¸ªæ¨¡å‹å›¢é˜Ÿä¸­è¿›è¡Œäº†æ¢ç´¢ï¼Œäº†è§£åœ¨ä½•ç§æƒ…å†µä¸‹ä»¥è¿™ç§æ–¹å¼è”åˆä¸åŒçš„LLMä¼šæ›´æœ‰ç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä¸å•ä¸€æ¨¡å‹ä¸€è‡´æ€§æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›æ€§èƒ½æ”¹è¿›å¾€å¾€ä¼´éšç€æ¨ç†æˆæœ¬çš„é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºä¸€è‡´æ€§æ–¹æ³•çš„LLMå¹»è§‰æ£€æµ‹ä¸ç¼“è§£å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>é€šè¿‡èšåˆå•ä¸€LLMçš„å¤šä¸ªå“åº”æ¥åº”å¯¹è®­ç»ƒæ•°æ®çš„ä¸å®Œç¾é—®é¢˜ã€‚</li>
<li>ç»“åˆå¤šä¸ªå…·æœ‰ä¸åŒè®­ç»ƒæ•°æ®ã€æ–¹æ¡ˆå’Œæ¶æ„çš„LLMå“åº”ï¼Œå®ç°äº†å¹»è§‰æ£€æµ‹ä¸ç¼“è§£èƒ½åŠ›çš„è¿›ä¸€æ­¥æé«˜ã€‚</li>
<li>â€œè”ç›Ÿä¸€è‡´æ€§â€æ–¹æ³•èƒ½å¤Ÿåœ¨å¤šä¸ªæ¨¡å‹å›¢é˜Ÿä¸­è¡¨ç°è‰¯å¥½ï¼Œæ¶µç›–å¤šç§ä¸åŒçš„LLMã€‚</li>
<li>åœ¨ç‰¹å®šæ¡ä»¶ä¸‹è”åˆä¸åŒçš„LLMä¼šæ›´æœ‰ç›Šã€‚</li>
<li>ä¸å•ä¸€æ¨¡å‹ä¸€è‡´æ€§æ–¹æ³•ç›¸æ¯”ï¼Œè”ç›Ÿä¸€è‡´æ€§æ–¹æ³•çš„æ€§èƒ½æ”¹è¿›ä¼´éšç€æ¨ç†æˆæœ¬çš„é™ä½ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æœ›ä¸ºLLMçš„åº”ç”¨æä¾›æ›´å‡†ç¡®ã€æ›´å¯é çš„æ¨¡å‹å“åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8cfb8003655718e688a5d652769b50f8" align="middle">
<img src="https://picx.zhimg.com/v2-4a38178e157aa0c62167f0da18d18bd3" align="middle">
<img src="https://picx.zhimg.com/v2-c949c0b5e78d32da794a6996bb873aad" align="middle">
<img src="https://picx.zhimg.com/v2-d942214dcbf96828d8d123d878fd6dfa" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Transformer-Based-Low-Resource-Language-Translation-A-Study-on-Standard-Bengali-to-Sylheti"><a href="#Transformer-Based-Low-Resource-Language-Translation-A-Study-on-Standard-Bengali-to-Sylheti" class="headerlink" title="Transformer-Based Low-Resource Language Translation: A Study on Standard   Bengali to Sylheti"></a>Transformer-Based Low-Resource Language Translation: A Study on Standard   Bengali to Sylheti</h2><p><strong>Authors:Mangsura Kabir Oni, Tabia Tanzin Prama</strong></p>
<p>Machine Translation (MT) has advanced from rule-based and statistical methods to neural approaches based on the Transformer architecture. While these methods have achieved impressive results for high-resource languages, low-resource varieties such as Sylheti remain underexplored. In this work, we investigate Bengali-to-Sylheti translation by fine-tuning multilingual Transformer models and comparing them with zero-shot large language models (LLMs). Experimental results demonstrate that fine-tuned models significantly outperform LLMs, with mBART-50 achieving the highest translation adequacy and MarianMT showing the strongest character-level fidelity. These findings highlight the importance of task-specific adaptation for underrepresented languages and contribute to ongoing efforts toward inclusive language technologies. </p>
<blockquote>
<p>æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰å·²ç»ä»åŸºäºè§„åˆ™å’Œç»Ÿè®¡çš„æ–¹æ³•å‘å±•åˆ°åŸºäºTransformeræ¶æ„çš„ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚è™½ç„¶è¿™äº›æ–¹æ³•ä¸ºé«˜èµ„æºè¯­è¨€å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†åƒé”¡å°”èµ«æè¿™æ ·èµ„æºåŒ®ä¹çš„è¯­è¨€ä»ç„¶è¢«å¿½è§†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¾®è°ƒå¤šè¯­è¨€Transformeræ¨¡å‹æ¥ç ”ç©¶å­ŸåŠ æ‹‰è¯­åˆ°é”¡å°”èµ«æè¯­çš„ç¿»è¯‘ï¼Œå¹¶å°†å…¶ä¸é›¶æ ·æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒæ¨¡å‹æ˜¾è‘—ä¼˜äºLLMï¼Œå…¶ä¸­mBART-50çš„ç¿»è¯‘å‡†ç¡®åº¦æœ€é«˜ï¼Œè€ŒMarianMTåœ¨å­—ç¬¦çº§åˆ«çš„å¿ å®åº¦æœ€å¼ºã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ä»»åŠ¡ç‰¹å®šé€‚åº”å¯¹ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå®ç°åŒ…å®¹æ€§è¯­è¨€æŠ€æœ¯çš„æŒç»­åŠªåŠ›åšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18898v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºTransformeræ¶æ„çš„ç¥ç»æ–¹æ³•ï¼Œä½¿å¾—æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰å·²ç»ä»åŸºäºè§„åˆ™å’Œç»Ÿè®¡çš„æ–¹æ³•å‘å±•åˆ°æ›´é«˜çº§é˜¶æ®µã€‚å°½ç®¡è¿™äº›æ–¹æ³•ä¸ºé«˜èµ„æºè¯­è¨€å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å¯¹èµ„æºè¾ƒå°‘çš„è¯­è¨€å¦‚Sylhetiçš„æ¢ç´¢ä»ç„¶ä¸è¶³ã€‚æœ¬ç ”ç©¶é€šè¿‡å¾®è°ƒå¤šè¯­è¨€Transformeræ¨¡å‹å¹¶å°†å…¶ä¸é›¶æ ·æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œç ”ç©¶å­ŸåŠ æ‹‰è¯­åˆ°Sylhetiçš„ç¿»è¯‘é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒæ¨¡å‹æ˜¾è‘—ä¼˜äºLLMï¼Œå…¶ä¸­mBART-50åœ¨ç¿»è¯‘å‡†ç¡®æ€§å’Œæµç•…æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€ŒMarianMTåœ¨å­—ç¬¦çº§åˆ«çš„ä¿çœŸåº¦ä¸Šè¡¨ç°æœ€å¼ºã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ä»»åŠ¡ç‰¹å®šé€‚åº”å¯¹ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºåŒ…å®¹æ€§è¯­è¨€æŠ€æœ¯çš„å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨ç¿»è¯‘å·²ä»åŸºäºè§„åˆ™å’Œç»Ÿè®¡çš„æ–¹æ³•å‘å±•åˆ°åŸºäºTransformeræ¶æ„çš„ç¥ç»æ–¹æ³•ã€‚</li>
<li>é«˜èµ„æºè¯­è¨€çš„æœºå™¨ç¿»è¯‘å·²ç»å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä½èµ„æºè¯­è¨€å¦‚Sylhetiä»å¾…æ¢ç´¢ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¾®è°ƒå¤šè¯­è¨€Transformeræ¨¡å‹è¿›è¡Œå­ŸåŠ æ‹‰è¯­åˆ°Sylhetiçš„ç¿»è¯‘ã€‚</li>
<li>ç›¸æ¯”é›¶æ ·æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¾®è°ƒæ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>mBART-50åœ¨ç¿»è¯‘å‡†ç¡®æ€§å’Œæµç•…æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>MarianMTåœ¨å­—ç¬¦çº§åˆ«çš„ä¿çœŸåº¦ä¸Šè¡¨ç°æœ€å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be66d0dd3ea94b6a47261c7ad049f55b" align="middle">
<img src="https://picx.zhimg.com/v2-d579aa9811458dc5d11bbdd17e0d0d2f" align="middle">
<img src="https://picx.zhimg.com/v2-58323f5eb368f148c02f08298bcb9212" align="middle">
<img src="https://picx.zhimg.com/v2-ffb179c07da85398946a7f0d982cea28" align="middle">
<img src="https://picx.zhimg.com/v2-9e97bdf74e8d7aef60d0bba4ef05ed85" align="middle">
<img src="https://picx.zhimg.com/v2-67205b89d5228b392a8b2154b755d590" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GPTFace-Generative-Pre-training-of-Facial-Linguistic-Transformer-by-Span-Masking-and-Weakly-Correlated-Text-image-Data"><a href="#GPTFace-Generative-Pre-training-of-Facial-Linguistic-Transformer-by-Span-Masking-and-Weakly-Correlated-Text-image-Data" class="headerlink" title="GPTFace: Generative Pre-training of Facial-Linguistic Transformer by   Span Masking and Weakly Correlated Text-image Data"></a>GPTFace: Generative Pre-training of Facial-Linguistic Transformer by   Span Masking and Weakly Correlated Text-image Data</h2><p><strong>Authors:Yudong Li, Hao Li, Xianxu Hou, Linlin Shen</strong></p>
<p>Compared to the prosperity of pre-training models in natural image understanding, the research on large-scale pre-training models for facial knowledge learning is still limited. Current approaches mainly rely on manually assembled and annotated face datasets for training, but labeling such datasets is labor-intensive and the trained models have limited scalability beyond the training data. To address these limitations, we present a generative pre-training model for facial knowledge learning that leverages large-scale web-built data for training. We use texts and images containing human faces crawled from the internet and conduct pre-training on self-supervised tasks, including masked image&#x2F;language modeling (MILM) and image-text matching (ITM). During the generation stage, we further utilize the image-text matching loss to pull the generation distribution towards the control signal for controllable image&#x2F;text generation. Experimental results demonstrate that our model achieves comparable performance to state-of-the-art pre-training models for various facial downstream tasks, such as attribution classification and expression recognition. Furthermore, our approach is also applicable to a wide range of face editing tasks, including face attribute editing, expression manipulation, mask removal, and photo inpainting. </p>
<blockquote>
<p>ç›¸è¾ƒäºè‡ªç„¶è¯­è¨€å›¾åƒç†è§£ä¸­é¢„è®­ç»ƒæ¨¡å‹çš„ç¹è£ï¼Œå¯¹é¢éƒ¨çŸ¥è¯†å­¦ä¹ çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ‰‹åŠ¨ç»„è£…å’Œæ³¨é‡Šçš„é¢éƒ¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä½†æ˜¯æ ‡æ³¨è¿™æ ·çš„æ•°æ®é›†æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ï¼Œå¹¶ä¸”è®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¹‹å¤–çš„æ‰©å±•èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé¢éƒ¨çŸ¥è¯†å­¦ä¹ çš„ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å¤§è§„æ¨¡ç½‘ç»œæ„å»ºçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ä»äº’è”ç½‘ä¸Šçˆ¬å–åŒ…å«äººè„¸çš„æ–‡æœ¬å’Œå›¾åƒï¼Œå¹¶åœ¨è‡ªæˆ‘ç›‘ç£çš„ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒ…æ‹¬æ©ç å›¾åƒ&#x2F;è¯­è¨€å»ºæ¨¡ï¼ˆMILMï¼‰å’Œå›¾åƒæ–‡æœ¬åŒ¹é…ï¼ˆITMï¼‰ã€‚åœ¨ç”Ÿæˆé˜¶æ®µï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åˆ©ç”¨å›¾åƒæ–‡æœ¬åŒ¹é…æŸå¤±å°†ç”Ÿæˆåˆ†å¸ƒæ‹‰å‘æ§åˆ¶ä¿¡å·ï¼Œä»¥å®ç°å¯æ§çš„å›¾åƒ&#x2F;æ–‡æœ¬ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§é¢éƒ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°äº†ä¸æœ€æ–°é¢„è®­ç»ƒæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå¦‚å±æ€§åˆ†ç±»å’Œæƒ…æ„Ÿè¯†åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜é€‚ç”¨äºå„ç§é¢éƒ¨ç¼–è¾‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬é¢éƒ¨å±æ€§ç¼–è¾‘ã€è¡¨æƒ…æ“æ§ã€å£ç½©å»é™¤å’Œç…§ç‰‡ä¿®å¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18345v1">PDF</a> This work was initially drafted in November 2022</p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡é¢éƒ¨çŸ¥è¯†å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œå½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹åŠ¨ç»„è£…å’Œæ ‡æ³¨çš„é¢æ•°æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä½†æ ‡æ³¨å·¥ä½œé‡å¤§ä¸”æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®å¤–çš„å¯æ‰©å±•æ€§æœ‰é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨äº’è”ç½‘å¤§è§„æ¨¡ç½‘ç»œæ•°æ®æ„å»ºé¢éƒ¨çŸ¥è¯†å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿ç”¨åŒ…å«äººè„¸çš„æ–‡æœ¬å’Œå›¾åƒè¿›è¡Œè‡ªç›‘ç£ä»»åŠ¡çš„é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬é®ç½©å›¾åƒ&#x2F;è¯­è¨€å»ºæ¨¡ï¼ˆMILMï¼‰å’Œå›¾åƒæ–‡æœ¬åŒ¹é…ï¼ˆITMï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§é¢éƒ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°ä¸æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶é€‚ç”¨äºå„ç§é¢éƒ¨ç¼–è¾‘ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨çŸ¥è¯†å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹çš„ç ”ç©¶ç›¸è¾ƒäºè‡ªç„¶å›¾åƒç†è§£é¢„è®­ç»ƒæ¨¡å‹ä»ç„¶æœ‰é™ã€‚</li>
<li>å½“å‰é¢éƒ¨çŸ¥è¯†å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹ä¸»è¦ä¾èµ–æ‰‹åŠ¨æ ‡æ³¨çš„é¢æ•°æ•°æ®é›†ï¼Œå­˜åœ¨æ ‡æ³¨å·¥ä½œé‡å¤§å’Œæ¨¡å‹å¯æ‰©å±•æ€§æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨äº’è”ç½‘å¤§è§„æ¨¡ç½‘ç»œæ•°æ®æ„å»ºé¢éƒ¨çŸ¥è¯†å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨åŒ…å«äººè„¸çš„æ–‡æœ¬å’Œå›¾åƒè¿›è¡Œè‡ªç›‘ç£ä»»åŠ¡çš„é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬é®ç½©å›¾åƒ&#x2F;è¯­è¨€å»ºæ¨¡ï¼ˆMILMï¼‰å’Œå›¾åƒæ–‡æœ¬åŒ¹é…ï¼ˆITMï¼‰ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨å¤šç§é¢éƒ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>è¯¥æ¨¡å‹é€‚ç”¨äºå¤šç§é¢éƒ¨ç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚é¢éƒ¨å±æ€§ç¼–è¾‘ã€è¡¨æƒ…æ“æ§ã€å£ç½©å»é™¤å’Œç…§ç‰‡ä¿®å¤ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79088e6e6b90fa6fef8f08c567b08874" align="middle">
<img src="https://picx.zhimg.com/v2-d328ee7d03753a43a3fc7187479cdea3" align="middle">
<img src="https://picx.zhimg.com/v2-a0f614a2c28144aa05cf39d91d763499" align="middle">
<img src="https://picx.zhimg.com/v2-f1722881935be875ac9cec55f6d61b85" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TabR1-Taming-GRPO-for-tabular-reasoning-LLMs"><a href="#TabR1-Taming-GRPO-for-tabular-reasoning-LLMs" class="headerlink" title="TabR1: Taming GRPO for tabular reasoning LLMs"></a>TabR1: Taming GRPO for tabular reasoning LLMs</h2><p><strong>Authors:Pengxiang Cai, Zihao Gao, Jintai Chen</strong></p>
<p>Tabular prediction has traditionally relied on gradient-boosted decision trees and specialized deep learning models, which excel within tasks but provide limited interpretability and weak transfer across tables. Reasoning large language models (LLMs) promise cross-task adaptability with trans- parent reasoning traces, yet their potential has not been fully realized for tabular data. This paper presents TabR1, the first reasoning LLM for tabular prediction with multi-step reasoning. At its core is Permutation Relative Policy Optimization (PRPO), a simple yet efficient reinforcement learning method that encodes column-permutation invariance as a structural prior. By construct- ing multiple label-preserving permutations per sample and estimating advantages both within and across permutations, PRPO transforms sparse rewards into dense learning signals and improves generalization. With limited supervision, PRPO activates the reasoning ability of LLMs for tabular prediction, enhancing few-shot and zero-shot performance as well as interpretability. Comprehensive experiments demonstrate that TabR1 achieves performance comparable to strong baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1 approaches the performance of strong baselines under the 32-shot setting. Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B). </p>
<blockquote>
<p>è¡¨æ ¼é¢„æµ‹ä¼ ç»Ÿä¸Šä¾èµ–äºæ¢¯åº¦æå‡å†³ç­–æ ‘å’Œä¸“ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ä»»åŠ¡å†…éƒ¨è¡¨ç°å‡ºè‰²ï¼Œä½†è§£é‡Šæ€§æœ‰é™ï¼Œå¹¶ä¸”åœ¨è·¨è¡¨æ—¶çš„è¿ç§»èƒ½åŠ›è¾ƒå¼±ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†åŠŸèƒ½å¸¦æ¥äº†è·¨ä»»åŠ¡é€‚åº”æ€§çš„æ‰¿è¯ºï¼Œå…·æœ‰é€æ˜çš„æ¨ç†è½¨è¿¹ï¼Œä½†å…¶åœ¨è¡¨æ ¼æ•°æ®ä¸Šçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†å®ç°ã€‚æœ¬æ–‡æå‡ºäº†TabR1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¡¨æ ¼é¢„æµ‹çš„å¤šæ­¥æ¨ç†æ¨ç†LLMã€‚å…¶æ ¸å¿ƒæ˜¯ç½®æ¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå°†åˆ—ç½®æ¢ä¸å˜æ€§ç¼–ç ä¸ºç»“æ„å…ˆéªŒã€‚é€šè¿‡å¯¹æ¯ä¸ªæ ·æœ¬æ„å»ºå¤šä¸ªæ ‡ç­¾ä¿ç•™ç½®æ¢ï¼Œå¹¶åœ¨ç½®æ¢å†…éƒ¨å’Œè·¨ç½®æ¢è¿›è¡Œä¼˜åŠ¿ä¼°è®¡ï¼ŒPRPOå°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†å­¦ä¹ ä¿¡å·å¹¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ‰é™çš„ç›‘ç£ä¸‹ï¼ŒPRPOæ¿€æ´»äº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œç”¨äºè¡¨æ ¼é¢„æµ‹ï¼Œæé«˜äº†å°‘æ ·æœ¬å’Œæ— æ ·æœ¬çš„æ€§èƒ½ä»¥åŠå¯è§£é‡Šæ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒTabR1åœ¨å…¨ç›‘ç£å¾®è°ƒä¸‹è¾¾åˆ°äº†ä¸å¼ºåŸºçº¿ç›¸å½“çš„æ€§èƒ½ã€‚åœ¨æ— æ ·æœ¬è®¾ç½®ä¸­ï¼ŒTabR1è¾¾åˆ°äº†åœ¨32æ ·æœ¬è®¾ç½®ä¸‹çš„å¼ºåŸºçº¿æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼ŒTabR1ï¼ˆ8Bï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæ›´å¤§çš„LLMsï¼Œç›¸å¯¹äºDeepSeek-R1ï¼ˆ685Bï¼‰å®ç°äº†é«˜è¾¾53.17%çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17385v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹è¡¨æ ¼é¢„æµ‹çš„æ¨ç†LLMæ¨¡å‹TabR1ï¼Œå…¶æ ¸å¿ƒæ˜¯Permutation Relative Policy Optimizationï¼ˆPRPOï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºå¤šä¸ªæ ‡ç­¾ä¿ç•™çš„æ’åˆ—æ ·æœ¬å¹¶ä¼°è®¡æ’åˆ—å†…çš„ä¼˜åŠ¿ï¼Œå°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†å­¦ä¹ ä¿¡å·ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚TabR1åœ¨æœ‰é™ç›‘ç£ä¸‹æ¿€æ´»äº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œæé«˜äº†å°‘æ ·æœ¬å’Œæ— æ ·æœ¬çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒTabR1åœ¨å®Œå…¨ç›‘ç£å¾®è°ƒä¸‹çš„æ€§èƒ½ä¸å¼ºåŸºçº¿ç›¸å½“ï¼Œåœ¨æ— æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½æ¥è¿‘32æ ·æœ¬è®¾ç½®ä¸‹çš„å¼ºåŸºçº¿ã€‚æ­¤å¤–ï¼ŒTabR1ï¼ˆ8Bï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæ›´å¤§çš„LLMï¼Œæœ€é«˜å¯æé«˜53.17%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TabR1æ˜¯é¦–ä¸ªé’ˆå¯¹è¡¨æ ¼é¢„æµ‹çš„æ¨ç†LLMæ¨¡å‹ã€‚</li>
<li>TabR1çš„æ ¸å¿ƒæ˜¯Permutation Relative Policy Optimizationï¼ˆPRPOï¼‰æ–¹æ³•ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>PRPOé€šè¿‡æ„å»ºå¤šä¸ªæ ‡ç­¾ä¿ç•™çš„æ’åˆ—æ ·æœ¬ï¼Œå°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†å­¦ä¹ ä¿¡å·ã€‚</li>
<li>TabR1åœ¨æœ‰é™ç›‘ç£ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œæé«˜äº†å°‘æ ·æœ¬å’Œæ— æ ·æœ¬å­¦ä¹ çš„æ€§èƒ½ã€‚</li>
<li>TabR1åœ¨å®Œå…¨ç›‘ç£å¾®è°ƒä¸‹çš„æ€§èƒ½ä¸ç°æœ‰å¼ºåŸºçº¿ç›¸å½“ï¼Œæ— æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½æ¥è¿‘32æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚</li>
<li>TabR1åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæ›´å¤§çš„LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fd7813e7d742acb1ff4a2e900cc5a42" align="middle">
<img src="https://picx.zhimg.com/v2-8033dabbae94edd65d15c0b313054964" align="middle">
<img src="https://picx.zhimg.com/v2-4f210f25ffe34e304b5f43fb6ba01d64" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts"><a href="#FlyLoRA-Boosting-Task-Decoupling-and-Parameter-Efficiency-via-Implicit-Rank-Wise-Mixture-of-Experts" class="headerlink" title="FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts"></a>FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit   Rank-Wise Mixture-of-Experts</h2><p><strong>Authors:Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</strong></p>
<p>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains â€“ general knowledge understanding, scientific question answering, mathematical reasoning, and code generation â€“ demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at <a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA">https://github.com/gfyddha/FlyLoRA</a>. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºåŸºç¡€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œä½†å­˜åœ¨å‚æ•°å¹²æ‰°çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚è™½ç„¶åŸºäºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„LoRAå˜ä½“åœ¨å•ä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ä¸­æ˜¾ç¤ºå‡ºç¼“è§£ä»»åŠ¡å†…å…³è”çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¼•å…¥äº†é¢å¤–çš„è·¯ç”±å™¨å‚æ•°ï¼Œå¹¶ä¸”åœ¨å¤šä»»åŠ¡æ¨¡å‹åˆå¹¶ä¸­ä»ç„¶æ— æ•ˆï¼Œå› ä¸ºä¼šå‡ºç°ä»»åŠ¡é—´å¹²æ‰°ã€‚å—è‡ç±»å—…è§‰ç”µè·¯çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†FlyLoRAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºéšå¼MoEçš„LoRAå˜ä½“ï¼Œå®ƒå¼•å…¥äº†ï¼šï¼ˆ1ï¼‰ä¸ŠæŠ•å½±çŸ©é˜µä¸­çš„ç­‰çº§ä¸“å®¶æ¿€æ´»ï¼›ï¼ˆ2ï¼‰ä¸€ç§éšå¼è·¯ç”±å™¨ï¼Œç»Ÿä¸€äº†ä¸“å®¶è·¯ç”±å’Œä¸‹æŠ•å½±ï¼Œå…¶ä¸­å†»ç»“çš„ç¨€ç–éšæœºæŠ•å½±çŸ©é˜µå–ä»£äº†ä¼ ç»Ÿçš„å¯†é›†å¯è®­ç»ƒç‰ˆæœ¬ã€‚è¿™ç§è®¾è®¡é€šè¿‡ä¸éœ€è¦æ˜¾å¼è·¯ç”±å™¨è§£å†³äº†ä»»åŠ¡å†…å»ç›¸å…³å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼ŒåŒæ—¶å› ä¸ºéšæœºçŸ©é˜µçš„æ­£äº¤æ€§å›ºæœ‰çš„å‡è½»äº†ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚åœ¨å››ä¸ªé¢†åŸŸâ€”â€”é€šç”¨çŸ¥è¯†ç†è§£ã€ç§‘å­¦é—®é¢˜å›ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆâ€”â€”çš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFlyLoRAåœ¨æ€§èƒ½ä¸Šå®ç°äº†æŒç»­çš„æå‡ã€‚é™¤äº†ç»éªŒæ”¶ç›Šä¹‹å¤–ï¼ŒFlyLoRAè¿˜çªå‡ºäº†ç”Ÿç‰©ç»“æ„å¦‚ä½•æ¿€å‘AIæŠ€æœ¯çš„åˆ›æ–°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gfyddha/FlyLoRA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gfyddha/FlyLoRAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08396v2">PDF</a> NeurIPS 2025 accepted paper</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä»¿ç”Ÿå­¦åŸç†çš„FlyLoRAæ–¹æ³•é€šè¿‡å¼•å…¥rank-wiseä¸“å®¶æ¿€æ´»å’Œéšå¼è·¯ç”±å™¨è®¾è®¡ï¼Œè§£å†³äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•ä¸­å‚æ•°å¹²æ‰°çš„é—®é¢˜ï¼Œä¼˜åŒ–äº†æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ¶ˆé™¤ä¼ ç»Ÿè·¯ç”±å™¨çš„éœ€æ±‚ï¼Œå®ç°äº†é«˜æ•ˆçš„è®¡ç®—ä¸ä»»åŠ¡é—´å¹²æ‰°çš„å¤©ç„¶æŠ‘åˆ¶ã€‚åœ¨å››ä¸ªé¢†åŸŸå†…çš„å®éªŒè¯æ˜ï¼ŒFlyLoRAåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FlyLoRAæ˜¯ä¸€ç§ä»¥ä»¿ç”Ÿå­¦ä¸ºçµæ„Ÿçš„æ–¹æ³•ï¼Œè§£å†³ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•ä¸­çš„å‚æ•°å¹²æ‰°é—®é¢˜ã€‚</li>
<li>å¼•å…¥rank-wiseä¸“å®¶æ¿€æ´»å’Œéšå¼è·¯ç”±å™¨è®¾è®¡ï¼Œä¼˜åŒ–äº†æ€§èƒ½å¹¶æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>éšå¼è·¯ç”±å™¨è®¾è®¡é€šè¿‡å†»ç»“ç¨€ç–éšæœºæŠ•å½±çŸ©é˜µä»£æ›¿ä¼ ç»Ÿçš„å¯†é›†å¯è®­ç»ƒç‰ˆæœ¬ï¼Œå‡å°‘äº†è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>FlyLoRAå®éªŒè¯æ˜åœ¨å¤šä»»åŠ¡æ¨¡å‹åˆå¹¶ä¸­å¯ä»¥ç¼“è§£ä»»åŠ¡é—´çš„å¹²æ‰°é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šé¢†åŸŸå®éªŒä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬é€šç”¨çŸ¥è¯†ç†è§£ã€ç§‘å­¦é—®ç­”ã€æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰ã€‚</li>
<li>FlyLoRAä¸ä»…å®ç°äº†æ€§èƒ½æå‡ï¼Œè¿˜å±•ç¤ºäº†ç”Ÿç‰©å­¦ç»“æ„å¦‚ä½•å¯å‘äººå·¥æ™ºèƒ½æŠ€æœ¯çš„åˆ›æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5b0dcec8915d196c4df323e036ee6e5" align="middle">
<img src="https://picx.zhimg.com/v2-2275cc2dd6f71f3d9bd7b386e5f869ff" align="middle">
<img src="https://picx.zhimg.com/v2-35b0df053417bdac845dd0485704f2ed" align="middle">
<img src="https://picx.zhimg.com/v2-67d3d13e77349b9b9aea02d983321c3b" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DragFlow-Unleashing-DiT-Priors-with-Region-Based-Supervision-for-Drag-Editing"><a href="#DragFlow-Unleashing-DiT-Priors-with-Region-Based-Supervision-for-Drag-Editing" class="headerlink" title="DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag   Editing"></a>DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag   Editing</h2><p><strong>Authors:Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</strong></p>
<p>Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUXâ€™s rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication. </p>
<blockquote>
<p>åŸºäºæ‹–åŠ¨çš„å›¾åƒç¼–è¾‘é•¿æœŸå­˜åœ¨ç›®æ ‡åŒºåŸŸå¤±çœŸé—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ—©æœŸåŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼ˆå¦‚Stable Diffusionï¼‰ä¸è¶³ä»¥å°†ä¼˜åŒ–åçš„æ½œåœ¨å˜é‡æŠ•å½±å›è‡ªç„¶å›¾åƒæµå½¢ã€‚éšç€ä»åŸºäºUNetçš„DDPMsè½¬å‘æ›´å…·å¯æ‰©å±•æ€§çš„DiTæµåŒ¹é…ï¼ˆä¾‹å¦‚SD3.5ã€FLUXï¼‰ï¼Œç”Ÿæˆå…ˆéªŒçŸ¥è¯†å˜å¾—æ›´ä¸ºå¼ºå¤§ï¼Œèƒ½å¤Ÿåœ¨å„ç§ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—è¿›å±•ã€‚ç„¶è€Œï¼ŒåŸºäºæ‹–åŠ¨çš„ç¼–è¾‘å°šæœªä»è¿™äº›æ›´å¼ºçš„å…ˆéªŒçŸ¥è¯†ä¸­å—ç›Šã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ç¬¬ä¸€ä¸ªæœ‰æ•ˆåˆ©ç”¨FLUXä¸°å¯Œå…ˆéªŒçŸ¥è¯†è¿›è¡ŒåŸºäºæ‹–åŠ¨ç¼–è¾‘çš„æ¡†æ¶ï¼Œåä¸ºDragFlowï¼Œç›¸è¾ƒäºåŸºçº¿å®ç°æ˜¾è‘—çš„æå‡ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬é¦–å…ˆå±•ç¤ºç›´æ¥å°†åŸºäºç‚¹çš„æ‹–åŠ¨ç¼–è¾‘åº”ç”¨äºDiTsè¡¨ç°ä¸ä½³ï¼šä¸UNetçš„é«˜åº¦å‹ç¼©ç‰¹å¾ä¸åŒï¼ŒDiTçš„ç‰¹å¾ç»“æ„ä¸å¤Ÿå……è¶³ï¼Œæ— æ³•ä¸ºç‚¹å¼è¿åŠ¨ç›‘ç£æä¾›å¯é çš„æŒ‡å¯¼ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼ŒDragFlowå¼•å…¥äº†ä¸€ç§åŸºäºåŒºåŸŸçš„ç¼–è¾‘æ¨¡å¼ï¼Œå…¶ä¸­ä»¿å°„å˜æ¢ä½¿ç‰¹å¾ç›‘ç£æ›´åŠ ä¸°å¯Œå’Œä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†é¢„è®­ç»ƒçš„å¼€æ”¾åŸŸä¸ªæ€§åŒ–é€‚é…å™¨ï¼ˆä¾‹å¦‚IP-Adapterï¼‰ä»¥å¢å¼ºä¸»é¢˜ä¸€è‡´æ€§ï¼ŒåŒæ—¶é€šè¿‡åŸºäºæ¢¯åº¦æ©ç çš„ç¡¬çº¦æŸä¿ç•™èƒŒæ™¯ä¿çœŸåº¦ã€‚è¿˜è¿›ä¸€æ­¥é‡‡ç”¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è§£å†³ä»»åŠ¡æ­§ä¹‰ã€‚</p>
<p>ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„åŸºäºåŒºåŸŸçš„æ‹–åŠ¨åŸºå‡†æµ‹è¯•ï¼ˆReD Benchï¼‰ï¼Œä»¥åŒºåŸŸçº§çš„æ‹–åŠ¨æŒ‡ä»¤ä¸ºç‰¹å¾ã€‚åœ¨DragBench-DRå’ŒReD Benchä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDragFlowè¶…è¶Šäº†åŸºäºç‚¹å’ŒåŸºäºåŒºåŸŸçš„åŸºçº¿ï¼Œåœ¨åŸºäºæ‹–åŠ¨çš„å›¾åƒç¼–è¾‘ä¸­è¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨å‘è¡¨æ—¶å…¬å¼€ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.02253v2">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨Fluxä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†çš„æ¡†æ¶ï¼Œç”¨äºåŸºäºæ‹–æ”¾çš„å›¾åƒç¼–è¾‘ï¼Œç§°ä¸ºDragFlowã€‚è¯¥æ¡†æ¶å®ç°äº†å¯¹åŸºçº¿æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚ç›´æ¥åº”ç”¨ç‚¹åŸºæ‹–æ”¾ç¼–è¾‘åˆ°DiTsæ•ˆæœè¾ƒå·®ï¼Œå› æ­¤DragFlowå¼•å…¥åŸºäºåŒºåŸŸçš„ç¼–è¾‘æ¨¡å¼ï¼Œé€šè¿‡ä»¿å°„å˜æ¢å®ç°æ›´ä¸°å¯Œå’Œä¸€è‡´çš„ç‰¹å¾ç›‘ç£ã€‚ç»“åˆé¢„è®­ç»ƒçš„å¼€æ”¾åŸŸä¸ªæ€§åŒ–é€‚é…å™¨ï¼ˆå¦‚IP-Adapterï¼‰è§£å†³ä»»åŠ¡æ¨¡ç³Šæ€§ã€‚åœ¨åŒºåŸŸçº§æ‹–æ”¾æŒ‡ä»¤çš„åŸºå‡†æµ‹è¯•ï¼ˆReD Benchï¼‰ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜DragFlowè¶…è¶Šäº†ç‚¹åŸºå’ŒåŒºåŸŸåŸºçš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨åŸºäºæ‹–æ”¾çš„å›¾åƒç¼–è¾‘é¢†åŸŸè¾¾åˆ°æœ€æ–°çŠ¶æ€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Drag-basedå›¾åƒç¼–è¾‘åœ¨ç›®æ ‡åŒºåŸŸå­˜åœ¨å¤±çœŸé—®é¢˜ï¼Œä¸»è¦åŸå› æ˜¯æ—©æœŸåŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä¸è¶³ä»¥å°†ä¼˜åŒ–åçš„æ½œåœ¨å˜é‡æŠ•å½±åˆ°è‡ªç„¶å›¾åƒæµå½¢ä¸Šã€‚</li>
<li>ä»UNet-based DDPMsåˆ°æ›´å¯æ‰©å±•çš„DiT with flow matchingï¼ˆå¦‚SD3.5ï¼ŒFLUXï¼‰çš„è½¬å˜ï¼Œä½¿å¾—ç”Ÿæˆå…ˆéªŒçŸ¥è¯†å˜å¾—æ›´å¼ºï¼Œè¿™æ¨åŠ¨äº†å„ç§ç¼–è¾‘ä»»åŠ¡çš„è¿›æ­¥ã€‚</li>
<li>ç›´æ¥å°†ç‚¹åŸºæ‹–æ”¾ç¼–è¾‘åº”ç”¨åˆ°DiTsä¸Šè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºDiTç‰¹å¾ä¸å¦‚UNetç‰¹å¾ç»“æ„ä¸°å¯Œï¼Œæ— æ³•ä¸ºç‚¹çº§è¿åŠ¨ç›‘ç£æä¾›å¯é æŒ‡å¯¼ã€‚</li>
<li>DragFlowå¼•å…¥åŸºäºåŒºåŸŸçš„ç¼–è¾‘æ¨¡å¼ï¼Œé€šè¿‡ä»¿å°„å˜æ¢å®ç°æ›´ä¸°å¯Œå’Œä¸€è‡´çš„ç‰¹å¾ç›‘ç£ï¼Œå…‹æœè¿™ä¸€å±€é™æ€§ã€‚</li>
<li>ç»“åˆé¢„è®­ç»ƒçš„å¼€æ”¾åŸŸä¸ªæ€§åŒ–é€‚é…å™¨ï¼Œå¢å¼ºä¸»é¢˜ä¸€è‡´æ€§ï¼ŒåŒæ—¶é€šè¿‡æ¢¯åº¦æ©è†œç¡¬çº¦æŸä¿ç•™èƒŒæ™¯ä¿çœŸåº¦ã€‚</li>
<li>ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è§£å†³ä»»åŠ¡æ¨¡ç³Šæ€§ã€‚</li>
<li>åœ¨åŒºåŸŸçº§æ‹–æ”¾åŸºå‡†æµ‹è¯•ï¼ˆReD Benchï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDragFlowè¶…è¶Šäº†å…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°åŸºäºæ‹–æ”¾çš„å›¾åƒç¼–è¾‘çš„æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.02253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1087f0c941aa81e6ed233eca75228efb" align="middle">
<img src="https://picx.zhimg.com/v2-399435e06be435df4aa8cc0e8e7834a7" align="middle">
<img src="https://picx.zhimg.com/v2-2746e4d7f48858005fa047bc2cd90fd7" align="middle">
<img src="https://picx.zhimg.com/v2-4773c416d61fa06687f1795a12e7cc72" align="middle">
<img src="https://picx.zhimg.com/v2-4d5b2363baf930fbfb6ad7dde4a73902" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Whoâ€™s-Asking-Investigating-Bias-Through-the-Lens-of-Disability-Framed-Queries-in-LLMs"><a href="#Whoâ€™s-Asking-Investigating-Bias-Through-the-Lens-of-Disability-Framed-Queries-in-LLMs" class="headerlink" title="Whoâ€™s Asking? Investigating Bias Through the Lens of Disability Framed   Queries in LLMs"></a>Whoâ€™s Asking? Investigating Bias Through the Lens of Disability Framed   Queries in LLMs</h2><p><strong>Authors:Vishnu Hari, Kalpana Panda, Srikant Panda, Amit Agarwal, Hitesh Laxmichand Patel</strong></p>
<p>Large Language Models (LLMs) routinely infer users demographic traits from phrasing alone, which can result in biased responses, even when no explicit demographic information is provided. The role of disability cues in shaping these inferences remains largely uncharted. Thus, we present the first systematic audit of disability-conditioned demographic bias across eight state-of-the-art instruction-tuned LLMs ranging from 3B to 72B parameters. Using a balanced template corpus that pairs nine disability categories with six real-world business domains, we prompt each model to predict five demographic attributes - gender, socioeconomic status, education, cultural background, and locality - under both neutral and disability-aware conditions.   Across a varied set of prompts, models deliver a definitive demographic guess in up to 97% of cases, exposing a strong tendency to make arbitrary inferences with no clear justification. Disability context heavily shifts predicted attribute distributions, and domain context can further amplify these deviations. We observe that larger models are simultaneously more sensitive to disability cues and more prone to biased reasoning, indicating that scale alone does not mitigate stereotype amplification.   Our findings reveal persistent intersections between ableism and other demographic stereotypes, pinpointing critical blind spots in current alignment strategies. We release our evaluation framework and results to encourage disability-inclusive benchmarking and recommend integrating abstention calibration and counterfactual fine-tuning to curb unwarranted demographic inference. Code and data will be released on acceptance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ä»…é€šè¿‡æªè¾å°±èƒ½æ¨æ–­å‡ºç”¨æˆ·çš„äººå£ç»Ÿè®¡ç‰¹å¾ï¼Œè¿™å¯èƒ½å¯¼è‡´å‡ºç°åè§å“åº”ï¼Œå³ä½¿æœªæä¾›æ˜ç¡®çš„äººå£ç»Ÿè®¡ä¿¡æ¯ã€‚æ®‹ç–¾çº¿ç´¢åœ¨å¡‘é€ è¿™äº›æ¨æ–­ä¸­çš„ä½œç”¨ä»ç„¶å¤§éƒ¨åˆ†æœªè¢«æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹å…«ç§æœ€å…ˆè¿›ã€ç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„LLMè¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§å®¡è®¡ï¼Œè¿™äº›æ¨¡å‹çš„å‚æ•°èŒƒå›´ä»3Båˆ°72Bã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¹³è¡¡çš„æ¨¡æ¿è¯­æ–™åº“ï¼Œå°†ä¹ä¸ªæ®‹ç–¾ç±»åˆ«ä¸å…­ä¸ªç°å®ä¸–ç•Œå•†ä¸šé¢†åŸŸç›¸åŒ¹é…ï¼Œæç¤ºæ¯ä¸ªæ¨¡å‹åœ¨ä¸­æ€§å’Œæ®‹ç–¾æ„è¯†ä¸¤ç§æƒ…å†µä¸‹é¢„æµ‹äº”ä¸ªäººå£ç»Ÿè®¡å±æ€§ï¼šæ€§åˆ«ã€ç¤¾ä¼šç»æµåœ°ä½ã€æ•™è‚²ã€æ–‡åŒ–èƒŒæ™¯å’Œåœ°åŸŸã€‚åœ¨å„ç§æç¤ºä¸‹ï¼Œæ¨¡å‹åœ¨97%çš„æƒ…å†µä¸‹åšå‡ºäº†æ˜ç¡®çš„äººå£ç»Ÿè®¡çŒœæµ‹ï¼Œè¡¨ç°å‡ºå¼ºçƒˆçš„å€¾å‘ï¼Œåšå‡ºä»»æ„æ¨æ–­è€Œæ²¡æœ‰æ˜ç¡®çš„ä¾æ®ã€‚æ®‹ç–¾èƒŒæ™¯ä¼šæå¤§åœ°æ”¹å˜é¢„æµ‹å±æ€§åˆ†å¸ƒï¼Œè€Œé¢†åŸŸèƒŒæ™¯å¯èƒ½ä¼šè¿›ä¸€æ­¥æ”¾å¤§è¿™äº›åå·®ã€‚æˆ‘ä»¬å‘ç°æ›´å¤§çš„æ¨¡å‹å¯¹æ®‹ç–¾çº¿ç´¢æ›´ä¸ºæ•æ„Ÿï¼ŒåŒæ—¶æ›´å®¹æ˜“å‡ºç°åè§æ¨ç†ï¼Œè¿™è¡¨æ˜è§„æ¨¡æœ¬èº«å¹¶ä¸èƒ½å‡è½»åˆ»æ¿å°è±¡çš„æ”¾å¤§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†æŒç»­å­˜åœ¨çš„å¯¹æ®‹ç–¾å’Œå…¶ä»–äººå£ç»Ÿè®¡åˆ»æ¿å°è±¡çš„äº¤é›†ï¼ŒæŒ‡å‡ºäº†å½“å‰å¯¹é½ç­–ç•¥ä¸­çš„å…³é”®ç›²ç‚¹ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å’Œç»“æœï¼Œä»¥é¼“åŠ±è¿›è¡ŒåŒ…å®¹æ®‹ç–¾äººçš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å»ºè®®ä½¿ç”¨æ”¾å¼ƒæ ¡å‡†å’Œå‡è®¾å¾®è°ƒçš„æ–¹æ³•æ¥éåˆ¶ä¸å½“çš„äººå£ç»Ÿè®¡æ¨æ–­ã€‚è®ºæ–‡æ¥å—åå°†å…¬å¼€ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15831v2">PDF</a> Accepted at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”¨æˆ·è¡¨è¾¾ä¸­æ¨æ–­äººå£ç‰¹å¾ç‰¹è´¨çš„ç°è±¡ã€‚é€šè¿‡å¯¹å…«ç§å…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´LLMsè¿›è¡Œç³»ç»Ÿçš„å®¡è®¡ï¼Œå‘ç°æ¨¡å‹èƒ½å¤Ÿåœ¨é«˜è¾¾97%çš„æƒ…å†µä¸‹æ˜ç¡®é¢„æµ‹å‡ºæ€§åˆ«ã€ç¤¾ä¼šç»æµåœ°ä½ç­‰äº”ä¸ªå±æ€§ï¼Œå¹¶ä¸”å­˜åœ¨æ˜æ˜¾çš„åè§å€¾å‘ã€‚åœ¨è€ƒè™‘åˆ°æ®‹ç–¾çš„æƒ…å¢ƒä¸­ï¼Œé¢„æµ‹çš„ç‰¹è´¨åˆ†å¸ƒå‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œä¸åŒé¢†åŸŸçš„è¯­å¢ƒå¯èƒ½è¿›ä¸€æ­¥æ”¾å¤§è¿™ç§åå·®ã€‚è¿˜å‘ç°è§„æ¨¡æ›´å¤§çš„æ¨¡å‹æ›´æ˜“äºå—åˆ°æ®‹ç–¾ä¿¡å·çš„æš—ç¤ºå½±å“ï¼Œæ›´æ˜“äº§ç”Ÿåè§æ¨ç†ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†èƒ½åŠ›ä¸»ä¹‰å’Œå…¶ä»–äººå£ç‰¹å¾åè§ä¹‹é—´çš„æŒç»­äº¤ç»‡é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰å¯¹é½ç­–ç•¥çš„å…³é”®ç›²ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsèƒ½ä»ç”¨æˆ·è¡¨è¾¾ä¸­æ¨æ–­äººå£ç‰¹å¾ç‰¹è´¨ï¼Œå­˜åœ¨åè§å€¾å‘ã€‚</li>
<li>åœ¨è€ƒè™‘æ®‹ç–¾æƒ…å¢ƒä¸‹ï¼ŒLLMsçš„é¢„æµ‹ç‰¹è´¨åˆ†å¸ƒå‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚</li>
<li>ä¸åŒé¢†åŸŸçš„è¯­å¢ƒå¯èƒ½è¿›ä¸€æ­¥æ”¾å¤§LLMsçš„é¢„æµ‹åå·®ã€‚</li>
<li>è§„æ¨¡æ›´å¤§çš„LLMsæ›´æ˜“å—åˆ°æ®‹ç–¾ä¿¡å·çš„æš—ç¤ºå½±å“ï¼Œäº§ç”Ÿåè§æ¨ç†ã€‚</li>
<li>ç°æœ‰LLMså­˜åœ¨ä¸æ®‹ç–¾æœ‰å…³çš„åè§å’Œåˆ»æ¿å°è±¡ã€‚</li>
<li>ç›®å‰å¯¹é½ç­–ç•¥å­˜åœ¨å…³é”®ç›²ç‚¹ï¼Œéœ€è¦æ›´åŠ å…¨é¢å’ŒåŒ…å®¹æ€§çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7358cc3996ec4d8a3b9998f0b076c0a2" align="middle">
<img src="https://picx.zhimg.com/v2-345a80f4a4398d793bf5b4bdf7ad5fe6" align="middle">
<img src="https://picx.zhimg.com/v2-a377716c95dec81b0273c29e1bbe396b" align="middle">
<img src="https://picx.zhimg.com/v2-5d5f8f8cb9a04740fb6b07e89c30143d" align="middle">
<img src="https://picx.zhimg.com/v2-09732edf7ab4d5a4699560a51c0ce1fe" align="middle">
<img src="https://picx.zhimg.com/v2-d19e70a21b863cbac7719b22a62146e3" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Prover-Agent-An-Agent-Based-Framework-for-Formal-Mathematical-Proofs"><a href="#Prover-Agent-An-Agent-Based-Framework-for-Formal-Mathematical-Proofs" class="headerlink" title="Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs"></a>Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs</h2><p><strong>Authors:Kaito Baba, Chaoran Liu, Shuhei Kurita, Akiyoshi Sannai</strong></p>
<p>We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas. These auxiliary lemmas are not limited to subgoals in the formal proof but can also include special cases or potentially useful facts derived from the assumptions, which help in discovering a viable proof strategy. It achieves an 88.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present theoretical analyses and case studies that illustrate how these generated lemmas contribute to solving challenging problems. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/kAIto47802/Prover-Agent">https://github.com/kAIto47802/Prover-Agent</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Prover Agentï¼Œè¿™æ˜¯ä¸€æ¬¾ç”¨äºè‡ªåŠ¨åŒ–å®šç†è¯æ˜çš„æ–°å‹äººå·¥æ™ºèƒ½ä»£ç†ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å½¢å¼åŒ–è¯æ˜åŠ©æ‰‹Leané›†æˆåœ¨ä¸€èµ·ã€‚Prover Agentåè°ƒéæ­£å¼æ¨ç†LLMã€å½¢å¼åŒ–è¯æ˜æ¨¡å‹ä»¥åŠæ¥è‡ªLeançš„åé¦ˆï¼ŒåŒæ—¶ç”Ÿæˆè¾…åŠ©å¼•ç†ã€‚è¿™äº›è¾…åŠ©å¼•ç†ä¸ä»…é™äºå½¢å¼è¯æ˜ä¸­çš„å­ç›®æ ‡ï¼Œè¿˜å¯ä»¥åŒ…æ‹¬ç”±å‡è®¾å¾—å‡ºçš„ç‰¹æ®Šæƒ…å†µæˆ–å¯èƒ½æœ‰ç”¨çš„äº‹å®ï¼Œè¿™æœ‰åŠ©äºå‘ç°å¯è¡Œçš„è¯æ˜ç­–ç•¥ã€‚å®ƒåœ¨MiniF2FåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†88.1%çš„æˆåŠŸç‡ï¼Œåœ¨é‡‡ç”¨å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ–¹æ³•ä¸­å»ºç«‹äº†æœ€æ–° state of the artï¼Œå¹¶ä¸”æ ·æœ¬é¢„ç®—æ¯”ä»¥å‰çš„æ–¹æ³•å¤§å¤§é™ä½ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ç†è®ºåˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œè¯´æ˜äº†è¿™äº›ç”Ÿæˆçš„å¼•ç†å¦‚ä½•è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/kAIto47802/Prover-Agent%E3%80%82">https://github.com/kAIto47802/Prover-Agentã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19923v4">PDF</a> 36 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Prover Agentï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹AIå®šç†è¯æ˜ä»£ç†ï¼Œé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå½¢å¼åŒ–è¯æ˜åŠ©æ‰‹Leanã€‚Prover Agentåè°ƒéæ­£å¼æ¨ç†LLMã€å½¢å¼åŒ–è¯æ˜æ¨¡å‹å’Œæ¥è‡ªLeançš„åé¦ˆï¼ŒåŒæ—¶ç”Ÿæˆè¾…åŠ©å¼•ç†ã€‚è¿™äº›è¾…åŠ©å¼•ç†ä¸ä»…é™äºå½¢å¼è¯æ˜ä¸­çš„å­ç›®æ ‡ï¼Œè¿˜åŒ…æ‹¬å‡è®¾è¡ç”Ÿçš„ç‰¹æ®Šæƒ…å†µæˆ–å¯èƒ½æœ‰ç”¨çš„äº‹å®ï¼Œæœ‰åŠ©äºå‘ç°å¯è¡Œçš„è¯æ˜ç­–ç•¥ã€‚åœ¨MiniF2FåŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æˆåŠŸç‡è¾¾åˆ°äº†88.1%ï¼Œåœ¨å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ–¹æ³•ä¸­å»ºç«‹äº†æ–°çš„æŠ€æœ¯é¢†å…ˆçŠ¶æ€ï¼Œå¹¶ä¸”æ ·æœ¬é¢„ç®—è¿œä½äºä»¥å‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ç†è®ºåˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œä»¥è¯´æ˜è¿™äº›ç”Ÿæˆçš„å¼•ç†å¦‚ä½•è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨å…¬å¼€è®¿é—®å¹³å°ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/kAIto47802/Prover-Agent%E3%80%82">https://github.com/kAIto47802/Prover-Agentã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Prover Agentæ˜¯ä¸€ä¸ªé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå½¢å¼åŒ–è¯æ˜åŠ©æ‰‹Leançš„æ–°å‹AIå®šç†è¯æ˜ä»£ç†ã€‚</li>
<li>Prover Agentèƒ½åè°ƒéæ­£å¼æ¨ç†LLMã€å½¢å¼åŒ–è¯æ˜æ¨¡å‹å’Œæ¥è‡ªLeançš„åé¦ˆã€‚</li>
<li>Prover Agentç”Ÿæˆè¾…åŠ©å¼•ç†ï¼Œè¿™äº›å¼•ç†ä¸ä»…é™äºå½¢å¼è¯æ˜ä¸­çš„å­ç›®æ ‡ï¼Œè¿˜åŒ…æ‹¬ç‰¹æ®Šæƒ…å†µæˆ–åŸºäºå‡è®¾çš„æœ‰ç”¨äº‹å®ã€‚</li>
<li>Prover Agentåœ¨MiniF2FåŸºå‡†æµ‹è¯•ä¸­çš„æˆåŠŸç‡è¾¾åˆ°äº†88.1%ï¼Œè¡¨æ˜å…¶åœ¨å°å‹è¯­è¨€æ¨¡å‹æ–¹æ³•ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>Prover Agentçš„æ ·æœ¬é¢„ç®—è¿œä½äºä¹‹å‰çš„æ–¹æ³•ã€‚</li>
<li>ç†è®ºåˆ†æå’Œæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†ç”Ÿæˆçš„å¼•ç†å¦‚ä½•å¸®åŠ©è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16924d72ddd5452c4b6ed73e92f6f7d6" align="middle">
<img src="https://picx.zhimg.com/v2-758362019113818027b4a946656af50a" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ReDit-Reward-Dithering-for-Improved-LLM-Policy-Optimization"><a href="#ReDit-Reward-Dithering-for-Improved-LLM-Policy-Optimization" class="headerlink" title="ReDit: Reward Dithering for Improved LLM Policy Optimization"></a>ReDit: Reward Dithering for Improved LLM Policy Optimization</h2><p><strong>Authors:Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu</strong></p>
<p>DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While itâ€™s a â€˜â€™perfectâ€™â€™ reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages. </p>
<blockquote>
<p>DeepSeek-R1å·²æˆåŠŸé€šè¿‡å…¶åŸºäºè§„åˆ™çš„å¥–åŠ±ç³»ç»Ÿå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶å®ƒæ˜¯ä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆéåˆ¶å¥–åŠ±æ“çºµçš„â€œå®Œç¾â€å¥–åŠ±ç³»ç»Ÿï¼Œä½†è¿™ç§å¥–åŠ±åŠŸèƒ½é€šå¸¸æ˜¯ç¦»æ•£çš„ã€‚æˆ‘ä»¬çš„å®éªŒè§‚å¯Ÿè¡¨æ˜ï¼Œç¦»æ•£å¥–åŠ±ä¼šå¯¼è‡´æ¢¯åº¦å¼‚å¸¸ã€ä¼˜åŒ–ä¸ç¨³å®šå’Œæ”¶æ•›ç¼“æ…¢ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReDitï¼ˆå¥–åŠ±æŠ–åŠ¨ï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ ç®€å•çš„éšæœºå™ªå£°æ¥æ‰°ä¹±ç¦»æ•£å¥–åŠ±ä¿¡å·ã€‚è¿™ç§å—æ‰°å¥–åŠ±åœ¨æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸­æŒç»­æä¾›æ¢ç´¢æ€§æ¢¯åº¦ï¼Œå®ç°æ›´å¹³æ»‘çš„æ¢¯åº¦æ›´æ–°å¹¶åŠ é€Ÿæ”¶æ•›ã€‚æ³¨å…¥çš„å™ªå£°è¿˜ä¸ºå¹³å¦å¥–åŠ±åŒºåŸŸå¼•å…¥äº†éšæœºæ€§ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ–°ç­–ç•¥å¹¶é€ƒç¦»å±€éƒ¨æœ€ä¼˜ã€‚åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†ReDitçš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚å¹³å‡è€Œè¨€ï¼ŒReDitåœ¨ä»…ä½¿ç”¨å¤§çº¦10%çš„è®­ç»ƒæ­¥éª¤çš„æƒ…å†µä¸‹å®ç°äº†ä¸æ ‡å‡†GRPOç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç»è¿‡ç±»ä¼¼æ—¶é—´çš„è®­ç»ƒåï¼Œå…¶æ€§èƒ½ä»æ¯”æ ‡å‡†GRPOæé«˜äº†4%ã€‚å¯è§†åŒ–ç»“æœè¯å®äº†ReDitåœ¨è§£å†³æ¢¯åº¦é—®é¢˜æ–¹é¢çš„æ˜¾è‘—æˆæ•ˆã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ç†è®ºåˆ†æä»¥è¿›ä¸€æ­¥éªŒè¯è¿™äº›ä¼˜åŠ¿ã€‚ </p>
</blockquote>
<hr>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18631v3">PDF</a> 34 pages, 19 figures</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å™¨ä¿¡å·åŠ å…¥éšæœºå™ªå£°å¯¹ç¦»æ•£å¥–åŠ±è¿›è¡Œå¤„ç†åæˆåŠŸæé«˜äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½ã€‚ReDiTï¼ˆå¥–åŠ±æŠ–åŠ¨ï¼‰æ–¹æ³•é€šè¿‡åœ¨å¥–åŠ±ä¿¡å·ä¸­æ·»åŠ ç®€å•éšæœºå™ªå£°æ¥æŠ–åŠ¨ç¦»æ•£å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæä¾›æŒç»­æ¢ç´¢æ¢¯åº¦ï¼Œå¹³æ»‘æ¢¯åº¦æ›´æ–°å¹¶åŠ é€Ÿæ”¶æ•›è¿‡ç¨‹ã€‚åœ¨å¹¿æ³›çš„ä»»åŠ¡æµ‹è¯•ä¸­è¯æ˜äº†ReDiTçš„æœ‰æ•ˆæ€§ï¼Œä¸ä¼ ç»ŸGRPOç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚é€šè¿‡å¯¹å¥–åŠ±ç³»ç»Ÿçš„æ‰°åŠ¨æ“ä½œä½¿å¾—æ¨¡å‹æ¢ç´¢æ–°å‹ç­–ç•¥ï¼Œä»è€Œæ‘†è„±å±€éƒ¨æœ€ä¼˜è§£é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºä¼ ç»ŸGRPOç®—æ³•ï¼ŒReDiTç®—æ³•ä¸ä»…å®ç°äº†æ›´é«˜æ•ˆçš„è®­ç»ƒè¿‡ç¨‹ï¼Œè¿˜åœ¨ç›¸ä¼¼çš„è®­ç»ƒæ—¶é•¿å†…å®ç°äº†å¹³å‡æ€§èƒ½æå‡ã€‚å¯è§†åŒ–ç»“æœè¡¨æ˜ReDiTæˆåŠŸç¼“è§£äº†æ¢¯åº¦é—®é¢˜ï¼ŒåŒæ—¶ä¹Ÿè·å¾—äº†ç†è®ºåˆ†æéªŒè¯çš„æ”¯æŒã€‚å®ƒä¸ä»…é€‚ç”¨äºè®¸å¤šå…·ä½“åœºæ™¯çš„å®é™…é—®é¢˜è§£å†³æ–¹æ¡ˆçš„å®ç°ä¸å®Œå–„å‘å±•æ¨¡å‹çš„ä¸æ–­è°ƒæ•´æ›´æ–°ä¸­çš„ä¸€éƒ¨åˆ†ä¸éœ€è¦ç²¾ç»†åŒ–è¿è¥çš„å¸¸è§å®é™…é—®é¢˜åœºæ™¯ä¸­å¤§æœ‰è£¨ç›Šè€Œä¸”ä¹Ÿå¾—åˆ°äº†ç”¨æˆ·å¯¹å…¶ä½“éªŒçš„æ™®éè®¤å¯å’Œèµæ‰¬è¤’å¥–ã€‚â€åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ‹¥æœ‰å¾ˆå¤§æ½œèƒ½ç­‰å¾…å‘æ˜è§£å†³å¤§è§„æ¨¡çš„ä¸šåŠ¡æ•°æ®æ·±åº¦åº”ç”¨å¤„ç†ç­‰ç›¸å…³çš„ç°å®é—®é¢˜å‘æŒ¥æ˜¾è‘—çš„ä½œç”¨å¯ä½œä¸ºä¸€ç§å‰æ²¿é«˜æ•ˆçš„ç³»ç»Ÿå®ç°å¼ºæœ‰åŠ›çš„è§£å†³æ–¹æ¡ˆä»¥æ»¡è¶³ä¸æ–­æå‡çš„éœ€æ±‚é—®é¢˜å¹¶å…·æœ‰å®é™…ä»·å€¼çš„æé«˜å®è·µæ½œåŠ›è¯¥æŠ€æœ¯çš„å‘å±•å¯ä»¥è§†ä¸ºä¿ƒè¿›æœªæ¥çš„ç›¸å…³é¢†åŸŸçš„å…³é”®åŠ›é‡æ ¸å¿ƒæŠ€æœ¯å’Œäº§ä¸šä¼˜åŒ–å‘å±•çš„é‡è¦æ¨æ‰‹è¡Œä¸šä¹‹ä¸­çš„ç„¦ç‚¹èšç‚¹å°¤å…¶å‰æ™¯æ½œåŠ›è¾ƒä¸ºçœ‹å¥½è§£å†³æ•°æ®è´¨é‡é—®é¢˜äº¦æœ‰ä¸€å®šåº”ç”¨ä»·å€¼è´¡çŒ®æ¨åŠ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®ç”¨æ€§ä¸å¤§è§„æ¨¡æ™ºèƒ½æ—¶ä»£çš„åˆ°æ¥åŠ©åŠ›çªç ´å¼æŠ€æœ¯å˜é©å¯¹è¯¸å¤šè¡Œä¸šå…·æœ‰å·¨å¤§ä»·å€¼æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ä¸ç¤¾ä¼šå‘å±•äº§ç”Ÿç§¯æå½±å“åŠ©åŠ›å¤§æ•°æ®è¡Œä¸šé«˜é€Ÿå‘å±•ä»¥åŠåˆ›æ–°æŠ€æœ¯çš„è¿›ä¸€æ­¥åº”ç”¨å’Œå‘å±•ä¸ºç¤¾ä¼šå‘å±•æ³¨å…¥æ–°çš„æ´»åŠ›ã€‚ç„¶è€Œå®ƒä»ç„¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜å’Œé—®é¢˜éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œè§£å†³ã€‚æœªæ¥éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œåº”ç”¨çš„æ·±å…¥å°†ä¼šæœ‰æ›´å¤šçš„çªç ´å’Œå‘å±•æ½œåŠ›å¯å°†å…¶åº”ç”¨åœ¨ä¼—å¤šé‡è¦é¢†åŸŸä¸ºè§£å†³ä¸€äº›å®é™…ä¸šåŠ¡é—®é¢˜å’Œç—›ç‚¹å¸¦æ¥æ–°çš„å¯èƒ½æ€§å’Œåˆ›æ–°æ€è·¯ã€‚æ€»çš„æ¥è¯´ReDiTä½œä¸ºä¸€ç§åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆåœ¨å¤§æ•°æ®å¤„ç†é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œå·¨å¤§çš„å‘å±•æ½œåŠ›å€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨æ¨å¹¿ã€‚è¿™ç§å°†ç¥ç»ç½‘ç»œå­¦ä¹ æ¨¡å‹å’Œç‰©ç†ä¸–ç•Œçš„è®¤çŸ¥ç»“åˆçš„éå¸¸å·§å¦™æƒ³æ³•æˆ–åº”ç”¨å°†æˆä¸ºå¤§æ•°æ®æ—¶ä»£çš„æœ€æ–°å® å„¿æœ‰ç€å¹¿æ³›çš„æ¢ç´¢å’Œç ”ç©¶ä»·å€¼å¹¶åœ¨å®é™…ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œä»·å€¼æ„ä¹‰å°†ä¼šå¾—åˆ°å¹¿æ³›çš„å…³æ³¨å’Œä¸æ–­çš„å‘å±•åº”ç”¨æ¨å¹¿ä»¥åŠæŒç»­çš„æŠ€æœ¯åˆ›æ–°å’Œæ”¹è¿›å‡çº§æˆä¸ºæœªæ¥å¤§æ•°æ®é¢†åŸŸçš„é‡è¦å‘å±•æ–¹å‘ä¹‹ä¸€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20c7424f3c7994022d2ab13031934415" align="middle">
<img src="https://picx.zhimg.com/v2-a5e1f77da13f82c435dbb6aebdb8dc04" align="middle">
<img src="https://picx.zhimg.com/v2-abbcca31ae53da6ae6631e0913488108" align="middle">
<img src="https://picx.zhimg.com/v2-8a315cd68fd73cdca5614c464aa82769" align="middle">
<img src="https://picx.zhimg.com/v2-3357ba61f5e5ca9c59bc405dc899912a" align="middle">
<img src="https://picx.zhimg.com/v2-9a9287205ec3fbe1a3f7ef41c25423aa" align="middle">
<img src="https://picx.zhimg.com/v2-f66e1a119fdff7bd388ee57c7c053568" align="middle">
<img src="https://picx.zhimg.com/v2-dec8d92d61d732653bf72d1a8b2c90d0" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Polyline-Path-Masked-Attention-for-Vision-Transformer"><a href="#Polyline-Path-Masked-Attention-for-Vision-Transformer" class="headerlink" title="Polyline Path Masked Attention for Vision Transformer"></a>Polyline Path Masked Attention for Vision Transformer</h2><p><strong>Authors:Zhongchen Zhao, Chaodong Xiao, Hui Lin, Qi Xie, Lei Zhang, Deyu Meng</strong></p>
<p>Global dependency modeling and spatial position modeling are two core issues of the foundational architecture design in current deep learning frameworks. Recently, Vision Transformers (ViTs) have achieved remarkable success in computer vision, leveraging the powerful global dependency modeling capability of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its significant potential in natural language processing tasks by explicitly modeling the spatial adjacency prior through the structured mask. In this paper, we propose Polyline Path Masked Attention (PPMA) that integrates the self-attention mechanism of ViTs with an enhanced structured mask of Mamba2, harnessing the complementary strengths of both architectures. Specifically, we first ameliorate the traditional structured mask of Mamba2 by introducing a 2D polyline path scanning strategy and derive its corresponding structured mask, polyline path mask, which better preserves the adjacency relationships among image tokens. Notably, we conduct a thorough theoretical analysis on the structural characteristics of the proposed polyline path mask and design an efficient algorithm for the computation of the polyline path mask. Next, we embed the polyline path mask into the self-attention mechanism of ViTs, enabling explicit modeling of spatial adjacency prior. Extensive experiments on standard benchmarks, including image classification, object detection, and segmentation, demonstrate that our model outperforms previous state-of-the-art approaches based on both state-space models and Transformers. For example, our proposed PPMA-T&#x2F;S&#x2F;B models achieve 48.7%&#x2F;51.1%&#x2F;52.3% mIoU on the ADE20K semantic segmentation task, surpassing RMT-T&#x2F;S&#x2F;B by 0.7%&#x2F;1.3%&#x2F;0.3%, respectively. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zhongchenzhao/PPMA">https://github.com/zhongchenzhao/PPMA</a>. </p>
<blockquote>
<p>å…¨å±€ä¾èµ–å»ºæ¨¡å’Œç©ºé—´ä½ç½®å»ºæ¨¡æ˜¯å½“å‰æ·±åº¦å­¦ä¹ æ¡†æ¶åŸºç¡€æ¶æ„è®¾è®¡ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ã€‚æœ€è¿‘ï¼ŒVision Transformersï¼ˆViTsï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œåˆ©ç”¨äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå¤§å…¨å±€ä¾èµ–å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMamba2 é€šè¿‡æ˜¾å¼å»ºæ¨¡ç©ºé—´é‚»æ¥å…ˆéªŒï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œé‡‡ç”¨çš„æ˜¯ç»“æ„åŒ–æ©ç ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Polyline Path Masked Attentionï¼ˆPPMAï¼‰ï¼Œå®ƒç»“åˆäº†ViTsçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒMamba2çš„å¢å¼ºç»“æ„åŒ–æ©ç ï¼Œå……åˆ†åˆ©ç”¨äº†ä¸¤ç§æ¶æ„çš„äº’è¡¥ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡å¼•å…¥2DæŠ˜çº¿è·¯å¾„æ‰«æç­–ç•¥ï¼Œæ”¹è¿›äº†Mamba2çš„ä¼ ç»Ÿç»“æ„åŒ–æ©ç ï¼Œå¹¶æ´¾ç”Ÿå‡ºç›¸åº”çš„ç»“æ„åŒ–æ©ç ï¼Œå³æŠ˜çº¿è·¯å¾„æ©ç ï¼Œå®ƒèƒ½æ›´å¥½åœ°ä¿ç•™å›¾åƒæ ‡è®°ä¹‹é—´çš„é‚»æ¥å…³ç³»ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å¯¹æ‰€æå‡ºçš„æŠ˜çº¿è·¯å¾„æ©ç çš„ç»“æ„ç‰¹æ€§è¿›è¡Œäº†æ·±å…¥çš„ç†è®ºåˆ†æï¼Œå¹¶è®¾è®¡äº†è®¡ç®—æŠ˜çº¿è·¯å¾„æ©ç çš„é«˜æ•ˆç®—æ³•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŠ˜çº¿è·¯å¾„æ©ç åµŒå…¥ViTsçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œå®ç°äº†ç©ºé—´é‚»æ¥å…ˆéªŒçš„æ˜¾å¼å»ºæ¨¡ã€‚åœ¨åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²åœ¨å†…çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºåŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹å’ŒTransformerçš„å…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æå‡ºçš„PPMA-T&#x2F;S&#x2F;Bæ¨¡å‹åœ¨ADE20Kè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„mIoUè¾¾åˆ°48.7%&#x2F;51.1%&#x2F;52.3%ï¼Œåˆ†åˆ«è¶…è¶Šäº†RMT-T&#x2F;S&#x2F;Bæ¨¡å‹0.7%&#x2F;1.3%&#x2F;0.3%ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/zhongchenzhao/PPMA%E3%80%82">https://github.com/zhongchenzhao/PPMAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15940v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºå°†è®¡ç®—æœºè§†è§‰ä¸­çš„Vision Transformersï¼ˆViTsï¼‰ä¸è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„Mamba2ç»“åˆï¼Œé€šè¿‡æ•´åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œç»“æ„åŒ–æ©ç ï¼Œæ„å»ºPolyline Path Masked Attentionï¼ˆPPMAï¼‰ã€‚æ–°æ–¹æ³•é€šè¿‡å¼•å…¥äºŒç»´æŠ˜çº¿è·¯å¾„æ‰«æç­–ç•¥å’Œç›¸åº”çš„ç»“æ„åŒ–æ©ç ï¼Œæ›´æœ‰æ•ˆåœ°å»ºæ¨¡å›¾åƒæ ‡è®°é—´çš„é‚»æ¥å…³ç³»ã€‚åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ï¼Œè¯¥æ¨¡å‹è¶…è¶Šäº†å½“å‰å…ˆè¿›æ¨¡å‹çš„è¡¨ç°ã€‚æ›´å¤šä¿¡æ¯å¯é€šè¿‡è®¿é—®ç‰¹å®šGitHubé“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) ç»“åˆè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å…¨å±€ä¾èµ–å»ºæ¨¡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>Mamba2 é€šè¿‡æ˜¾å¼å»ºæ¨¡ç©ºé—´é‚»æ¥å…ˆéªŒåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>PPMAç»“åˆäº†ViTsçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒMamba2çš„ç»“æ„åŒ–æ©ç æŠ€æœ¯ã€‚</li>
<li>PPMAå¼•å…¥äºŒç»´æŠ˜çº¿è·¯å¾„æ‰«æç­–ç•¥åŠå¯¹åº”çš„ç»“æ„åŒ–æ©ç ï¼Œæœ‰æ•ˆä¿ç•™å›¾åƒæ ‡è®°é—´çš„é‚»æ¥å…³ç³»ã€‚</li>
<li>é€šè¿‡æ•´åˆè¿™äº›æŠ€æœ¯ï¼ŒPPMAæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†è¶…è¶Šç°æœ‰æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>PPMAæ¨¡å‹åœ¨ADE20Kè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œå®ç°äº†è¾ƒé«˜çš„mIoUå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef4893d8853b9a38b4573070de87f621" align="middle">
<img src="https://picx.zhimg.com/v2-c8a4fc6807fe2a326b92f13248a11b83" align="middle">
<img src="https://picx.zhimg.com/v2-25a246d40760c59c7265a59faaa7057d" align="middle">
<img src="https://picx.zhimg.com/v2-e7fa8061d3807573664c286fdf4bcdde" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="What-Happens-During-the-Loss-Plateau-Understanding-Abrupt-Learning-in-Transformers"><a href="#What-Happens-During-the-Loss-Plateau-Understanding-Abrupt-Learning-in-Transformers" class="headerlink" title="What Happens During the Loss Plateau? Understanding Abrupt Learning in   Transformers"></a>What Happens During the Loss Plateau? Understanding Abrupt Learning in   Transformers</h2><p><strong>Authors:Pulkit Gopalani, Wei Hu</strong></p>
<p>Training Transformers on algorithmic tasks frequently demonstrates an intriguing abrupt learning phenomenon: an extended performance plateau followed by a sudden, sharp improvement. This work investigates the underlying mechanisms for such dynamics, primarily in shallow Transformers. We reveal that during the plateau, the model often develops an interpretable partial solution while simultaneously exhibiting a strong repetition bias in their outputs. This output degeneracy is accompanied by internal representation collapse, where hidden states across different tokens become nearly parallel. We further identify the slow learning of optimal attention maps as a key bottleneck. Hidden progress in attention configuration during the plateau precedes the eventual rapid convergence, and directly intervening on attention significantly alters plateau duration and the severity of repetition bias and representational collapse. We validate that these identified phenomena-repetition bias and representation collapse-are not artifacts of toy setups but also manifest in the early pre-training stage of large language models like Pythia and OLMo. </p>
<blockquote>
<p>åœ¨å¯¹ç®—æ³•ä»»åŠ¡è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒTransformeræ¨¡å‹ç»å¸¸è¡¨ç°å‡ºä¸€ç§æœ‰è¶£çš„çªå‘å­¦ä¹ ç°è±¡ï¼šé¦–å…ˆæ˜¯é•¿æ—¶é—´çš„æ€§èƒ½ç¨³å®šé˜¶æ®µï¼Œéšåæ˜¯çªç„¶è€Œå¿«é€Ÿçš„æ”¹è¿›ã€‚æœ¬ç ”ç©¶ä¸»è¦æ¢è®¨äº†æµ…å±‚Transformerçš„å†…åœ¨æœºåˆ¶ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨æ€§èƒ½ç¨³å®šé˜¶æ®µï¼Œæ¨¡å‹é€šå¸¸ä¼šå‘å±•å‡ºä¸€ç§å¯è§£é‡Šçš„å±€éƒ¨è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶åœ¨è¾“å‡ºä¸­è¡¨ç°å‡ºå¼ºçƒˆçš„é‡å¤å€¾å‘ã€‚è¿™ç§è¾“å‡ºé€€åŒ–ä¼´éšç€å†…éƒ¨è¡¨ç¤ºå´©æºƒï¼Œå…¶ä¸­ä¸åŒç¬¦å·çš„éšè—çŠ¶æ€å˜å¾—å‡ ä¹å¹³è¡Œã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç¡®å®šäº†å­¦ä¹ æœ€ä¼˜æ³¨æ„åŠ›å›¾çš„ç¼“æ…¢è¿‡ç¨‹æ˜¯ä¸€ä¸ªå…³é”®ç“¶é¢ˆã€‚åœ¨ç¨³å®šæœŸé—´éšè—çš„æ³¨æ„åŠ›é…ç½®è¿›å±•å…ˆäºæœ€ç»ˆçš„å¿«é€Ÿæ”¶æ•›ï¼Œç›´æ¥å¹²é¢„æ³¨æ„åŠ›ä¼šæ˜¾è‘—æ”¹å˜ç¨³å®šæœŸçš„æŒç»­æ—¶é—´ä»¥åŠé‡å¤å€¾å‘å’Œä»£è¡¨æ€§å´©æºƒçš„ä¸¥é‡ç¨‹åº¦ã€‚æˆ‘ä»¬éªŒè¯äº†æ‰€è¯†åˆ«çš„ç°è±¡â€”â€”é‡å¤å€¾å‘å’Œä»£è¡¨æ€§å´©æºƒâ€”â€”å¹¶éç©å…·è®¾ç½®çš„äº§ç‰©ï¼Œè€Œæ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ—©æœŸé¢„è®­ç»ƒé˜¶æ®µï¼ˆå¦‚Pythiaå’ŒOLMoï¼‰ä¸­ä¹Ÿä¼šå‡ºç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13688v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒTransformerè¿›è¡Œç®—æ³•ä»»åŠ¡æ—¶ï¼Œä¼šå‡ºç°ä¸€ç§æœ‰è¶£çš„ç°è±¡ï¼šå…ˆæ˜¯ä¸€æ®µé•¿æ—¶é—´çš„æ€§èƒ½ç¨³å®šæœŸï¼Œç„¶åçªç„¶æœ‰ä¸€ä¸ªæ˜æ˜¾çš„æå‡é˜¶æ®µã€‚æœ¬æ–‡æ¢ç©¶äº†è¿™ç§ç°è±¡åœ¨æµ…å±‚Transformerä¸­çš„å†…åœ¨æœºåˆ¶ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ç¨³å®šæœŸï¼Œæ¨¡å‹é€šå¸¸ä¼šå‡ºç°ä¸€ç§å¯è§£é‡Šçš„å±€éƒ¨è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶è¾“å‡ºè¡¨ç°å‡ºå¼ºçƒˆçš„é‡å¤å€¾å‘ã€‚è¿™ç§è¾“å‡ºé€€åŒ–ä¼´éšç€å†…éƒ¨è¡¨ç¤ºçš„å´©æºƒï¼Œä¸åŒæ ‡è®°çš„éšè—çŠ¶æ€å˜å¾—å‡ ä¹å¹³è¡Œã€‚æˆ‘ä»¬è®¤ä¸ºå­¦ä¹ æœ€ä¼˜æ³¨æ„åŠ›åœ°å›¾çš„ç¼“æ…¢è¿›å±•æ˜¯ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆã€‚æ³¨æ„åŠ›é…ç½®è¿‡ç¨‹ä¸­çš„éšè”½è¿›æ­¥æœ€ç»ˆå¯¼è‡´äº†å¿«é€Ÿçš„æ”¶æ•›è¿‡ç¨‹ï¼Œå¹¶ä¸”ç›´æ¥å½±å“æ¨¡å‹çš„è¡¨ç°ã€‚ç›´æ¥å¹²é¢„æ³¨æ„åŠ›æœºåˆ¶ä¼šæ˜¾è‘—æ”¹å˜ç¨³å®šæœŸçš„æŒç»­æ—¶é—´ä»¥åŠé‡å¤å€¾å‘å’Œä»£è¡¨æ€§å´©æºƒçš„ä¸¥é‡æ€§ã€‚æˆ‘ä»¬éªŒè¯äº†è¿™äº›ç°è±¡ï¼ˆé‡å¤åè§å’Œä»£è¡¨æ€§å´©æºƒï¼‰ä¸ä»…å­˜åœ¨äºç©å…·åœºæ™¯ä¸­ï¼Œè¿˜å­˜åœ¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Pythiaå’ŒOLMoï¼‰çš„æ—©æœŸé¢„è®­ç»ƒé˜¶æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨ç®—æ³•ä»»åŠ¡è®­ç»ƒæ—¶ä¼šå‡ºç°æ€§èƒ½ç¨³å®šæœŸåçš„çªç„¶æå‡ç°è±¡ã€‚</li>
<li>åœ¨æ€§èƒ½ç¨³å®šæœŸï¼Œæ¨¡å‹å­˜åœ¨ä¸€ç§å¯è§£é‡Šçš„å±€éƒ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ¨¡å‹è¾“å‡ºå‡ºç°é‡å¤å€¾å‘çš„ç°è±¡ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹çš„éšè—çŠ¶æ€ï¼ˆå†…éƒ¨è¡¨ç¤ºï¼‰å´©æºƒæ‰€å¯¼è‡´çš„ï¼Œè¡¨ç°ä¸ºä¸åŒä»¤ç‰Œä¹‹é—´çš„éšè—çŠ¶æ€è¿‘ä¼¼å¹³è¡ŒåŒ–ã€‚è¿™è¡¨æ˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›åœ¨è¿™ä¸ªé˜¶æ®µæ˜¯æœ‰é™çš„å¹¶ä¸”æœ‰ä¸€å®šçš„å€¾å‘æ€§åå·®é—®é¢˜å­˜åœ¨ï¼Œå¯èƒ½åœ¨æŸä¸ªæ—¶æœŸæœªæœ‰æ•ˆè·å–ä¿¡æ¯çš„ä¸åŒå½¢å¼ä¹‹é—´çš„å…³ç³»è€Œå¯¼è‡´äº†ä¸€äº›ç¨‹åº¦çš„å†…éƒ¨è¡¨è¿°åç¼©é—®é¢˜ã€‚ã€‚è¿™å¯¹äºæ¨¡å‹æ€§èƒ½å’Œé²æ£’æ€§æœ‰å½±å“ä¼šå¼•å…¥æ½œåœ¨é—®é¢˜ç”šè‡³ä½¿ç³»ç»Ÿéš¾ä»¥ç»§ç»­å­¦ä¹ å’Œæ”¹è¿›è¡¨ç°æ°´å¹³åœ¨é¢ä¸´æ›´å¤æ‚æˆ–å¤šæ ·çš„é—®é¢˜æ—¶å¯èƒ½å‡ºç°å›°éš¾å½±å“ç³»ç»Ÿçš„è¡¨ç°ã€‚æˆ‘ä»¬åº”è¯¥æ³¨æ„ç›‘æ§æ¨¡å‹ä»¥é¿å…è¿™ä¸€ç°è±¡çš„äº§ç”Ÿå’Œå¼€å‘é¢„é˜²æªæ–½ä»¥æé«˜ç³»ç»Ÿçš„é€šç”¨èƒ½åŠ›å’Œå­¦ä¹ æ•ˆæœå°¤å…¶æ˜¯åœ¨æ¥è¿‘æ„å»ºå®Œå¤‡çš„ç®—æ³•è¯­è¨€æˆ–æ–‡æœ¬ç”Ÿæˆçš„æœºå™¨ç³»ç»Ÿä¸­æ ¼å¤–é‡è§†éœ€è¦ç»´æŒå¥½çš„è¡¨è¾¾æ–¹å¼çš„é€‰æ‹©æˆ–ç€æœ€å°åŒ–çŸ¥è¯†è¡¨ç¤ºä¸­çš„å†—ä½™å’Œæ··æ·†é—®é¢˜æ¥é¿å…æ­¤ç±»ç°è±¡çš„å‘ç”Ÿã€‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e07c438a125aa0bea24729ac14457702" align="middle">
<img src="https://picx.zhimg.com/v2-27f10e809ef5e9f4ec23eec7d1b946dd" align="middle">
<img src="https://picx.zhimg.com/v2-c48cb68f842d65bf2de3d98185938d72" align="middle">
<img src="https://picx.zhimg.com/v2-f3aa8f6fc43c4da9135e72a2d9f7d6b3" align="middle">
<img src="https://picx.zhimg.com/v2-c07167511b97896f7218fcf7e3f943c4" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Blending-Complementary-Memory-Systems-in-Hybrid-Quadratic-Linear-Transformers"><a href="#Blending-Complementary-Memory-Systems-in-Hybrid-Quadratic-Linear-Transformers" class="headerlink" title="Blending Complementary Memory Systems in Hybrid Quadratic-Linear   Transformers"></a>Blending Complementary Memory Systems in Hybrid Quadratic-Linear   Transformers</h2><p><strong>Authors:Kazuki Irie, Morris Yau, Samuel J. Gershman</strong></p>
<p>We develop hybrid memory architectures for general-purpose sequence processing neural networks, that combine key-value memory using softmax attention (KV-memory) with fast weight memory through dynamic synaptic modulation (FW-memory) â€“ the core principles of quadratic and linear transformers, respectively. These two memory systems have complementary but individually limited properties: KV-memory offers precise retrieval but is constrained by quadratic complexity in sequence length, while FW-memory supports arbitrarily long sequences and enables more expressive computation but sacrifices precise recall. We propose and compare three methods to blend these two systems into a single memory system, differing in how and when input information is delivered to each system, to leverage the strengths of both. We conduct experiments on general language modeling and retrieval tasks by training 340M- and 1.3B-parameter models from scratch, as well as on synthetic algorithmic tasks designed to precisely illustrate the benefits of certain hybrid methods over others. We also evaluate our hybrid memory systems on reinforcement learning in partially observable environments. Overall, we demonstrate how a well-designed hybrid can overcome the limitations of its individual components, offering new insights into the design principle of neural memory systems. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºé€šç”¨åºåˆ—å¤„ç†ç¥ç»ç½‘ç»œå¼€å‘æ··åˆå†…å­˜æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†ä½¿ç”¨softmaxæ³¨æ„åŠ›çš„é”®å€¼å†…å­˜ï¼ˆKV-å†…å­˜ï¼‰å’Œé€šè¿‡åŠ¨æ€çªè§¦è°ƒåˆ¶å®ç°çš„å¿«é€Ÿæƒé‡å†…å­˜ï¼ˆFW-å†…å­˜ï¼‰â€”â€”åˆ†åˆ«æ˜¯äºŒæ¬¡å’Œçº¿æ€§å˜å‹å™¨çš„æ ¸å¿ƒåŸç†ã€‚è¿™ä¸¤ç§è®°å¿†ç³»ç»Ÿå…·æœ‰äº’è¡¥ä½†å•ç‹¬å—é™çš„ç‰¹æ€§ï¼šKV-å†…å­˜æä¾›ç²¾ç¡®æ£€ç´¢ï¼Œä½†å—é™äºåºåˆ—é•¿åº¦çš„äºŒæ¬¡å¤æ‚æ€§ï¼›è€ŒFW-å†…å­˜æ”¯æŒä»»æ„é•¿åº¦çš„åºåˆ—ï¼Œèƒ½å¤Ÿå®ç°æ›´å¯Œæœ‰è¡¨ç°åŠ›çš„è®¡ç®—ï¼Œä½†ç‰ºç‰²äº†ç²¾ç¡®å›å¿†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§å°†è¿™ä¸¤ç§ç³»ç»Ÿèåˆæˆå•ä¸€å†…å­˜ç³»ç»Ÿçš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨å°†è¾“å…¥ä¿¡æ¯ä¼ é€’ç»™æ¯ä¸ªç³»ç»Ÿçš„æ–¹å¼å’Œæ—¶æœºä¸Šæœ‰æ‰€ä¸åŒï¼Œä»¥å……åˆ†åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬é€šè¿‡ä»å¤´å¼€å§‹è®­ç»ƒ3.4äº¿å‚æ•°å’Œ13äº¿å‚æ•°çš„æ¨¡å‹ï¼Œä»¥åŠé€šè¿‡è®¾è®¡ç”¨äºç²¾ç¡®è¯´æ˜æŸç§æ··åˆæ–¹æ³•ç›¸å¯¹äºå…¶ä»–æ–¹æ³•çš„ä¼˜åŠ¿çš„åˆæˆç®—æ³•ä»»åŠ¡æ¥è¿›è¡Œå®éªŒï¼Œæ¥éªŒè¯æˆ‘ä»¬çš„æ··åˆè®°å¿†ç³»ç»Ÿåœ¨ä¸€èˆ¬è¯­è¨€å»ºæ¨¡å’Œæ£€ç´¢ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†æˆ‘ä»¬çš„æ··åˆè®°å¿†ç³»ç»Ÿåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒä¸­çš„å¼ºåŒ–å­¦ä¹ è¡¨ç°ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç²¾å¿ƒè®¾è®¡çš„æ··åˆç³»ç»Ÿå¦‚ä½•å…‹æœå…¶å•ä¸ªç»„ä»¶çš„é™åˆ¶ï¼Œä¸ºç¥ç»ç½‘ç»œç³»ç»Ÿçš„è®¾è®¡åŸåˆ™æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00744v2">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ··åˆè®°å¿†æ¶æ„åœ¨é€šç”¨åºåˆ—å¤„ç†ç¥ç»ç½‘ç»œä¸­çš„åº”ç”¨ï¼Œç»“åˆäº†åŸºäºsoftmaxæ³¨æ„åŠ›çš„é”®å€¼è®°å¿†ï¼ˆKV-memoryï¼‰å’Œé€šè¿‡åŠ¨æ€çªè§¦è°ƒåˆ¶å®ç°çš„å¿«é€Ÿæƒé‡è®°å¿†ï¼ˆFW-memoryï¼‰ï¼Œåˆ†åˆ«å¯¹åº”äºŒæ¬¡å’Œçº¿æ€§å˜å‹å™¨çš„æ ¸å¿ƒåŸç†ã€‚è¿™ä¸¤ç§è®°å¿†ç³»ç»Ÿå…·æœ‰äº’è¡¥ä½†ä¸ªä½“å±€é™çš„æ€§è´¨ï¼šKV-è®°å¿†æä¾›ç²¾ç¡®æ£€ç´¢ï¼Œä½†å—é™äºåºåˆ—é•¿åº¦çš„äºŒæ¬¡å¤æ‚æ€§ï¼›FW-è®°å¿†æ”¯æŒä»»æ„é•¿åº¦çš„åºåˆ—å¹¶å¯å®ç°æ›´ä¸°å¯Œçš„è®¡ç®—ï¼Œä½†ç‰ºç‰²äº†ç²¾ç¡®å›å¿†ã€‚æœ¬æ–‡æå‡ºäº†ä¸‰ç§èåˆè¿™ä¸¤ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼Œåœ¨ä½•æ—¶ä»¥åŠå¦‚ä½•å‘å„ç³»ç»Ÿæä¾›è¾“å…¥ä¿¡æ¯æ–¹é¢å­˜åœ¨å·®å¼‚ï¼Œä»¥åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ã€‚é€šè¿‡å®éªŒéªŒè¯äº†æ··åˆè®°å¿†ç³»ç»Ÿåœ¨ä¸€èˆ¬è¯­è¨€å»ºæ¨¡å’Œæ£€ç´¢ä»»åŠ¡ä»¥åŠåˆæˆç®—æ³•ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶ä¸å¼ºåŒ–å­¦ä¹ åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿç¯å¢ƒä¸­çš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè‰¯å¥½çš„æ··åˆè®¾è®¡å¯ä»¥å…‹æœå•ä¸ªç»„ä»¶çš„é™åˆ¶ï¼Œä¸ºç¥ç»ç½‘ç»œè®°å¿†ç³»ç»Ÿçš„è®¾è®¡åŸåˆ™æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†æ··åˆè®°å¿†æ¶æ„åœ¨ç¥ç»ç½‘ç»œä¸­çš„åº”ç”¨ï¼Œç»“åˆäº†é”®å€¼è®°å¿†å’Œå¿«é€Ÿæƒé‡è®°å¿†ã€‚</li>
<li>ä¸¤ç§è®°å¿†ç³»ç»Ÿå…·æœ‰äº’è¡¥æ€§è´¨ï¼šé”®å€¼è®°å¿†æä¾›ç²¾ç¡®æ£€ç´¢ï¼Œä½†å—é™äºäºŒæ¬¡å¤æ‚æ€§ï¼›å¿«é€Ÿæƒé‡è®°å¿†æ”¯æŒä»»æ„é•¿åº¦åºåˆ—ï¼Œä½†å¯èƒ½ç‰ºç‰²ç²¾ç¡®å›å¿†ã€‚</li>
<li>æå‡ºäº†ä¸‰ç§èåˆè¿™ä¸¤ç§è®°å¿†ç³»ç»Ÿçš„æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ã€‚</li>
<li>å®éªŒéªŒè¯äº†æ··åˆè®°å¿†ç³»ç»Ÿåœ¨è¯­è¨€å»ºæ¨¡ã€æ£€ç´¢ä»»åŠ¡å’Œåˆæˆç®—æ³•ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>è¯„ä¼°äº†æ··åˆè®°å¿†ç³»ç»Ÿåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è¡¨ç°ï¼Œåœ¨éƒ¨åˆ†å¯è§‚å¯Ÿçš„ç¯å¢ƒä¸­çš„è¡¨ç°è‰¯å¥½ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œæ··åˆè®°å¿†æ¶æ„å¯ä»¥å…‹æœå•ä¸ªç»„ä»¶çš„é™åˆ¶ï¼Œä¸ºç¥ç»ç½‘ç»œè®°å¿†ç³»ç»Ÿçš„è®¾è®¡åŸåˆ™æä¾›äº†æ–°è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab5038c47c6e125b22d1a34bca7b2455" align="middle">
<img src="https://picx.zhimg.com/v2-d2dae0a9cdf53ae68746793df2d247d2" align="middle">
<img src="https://picx.zhimg.com/v2-13af6ff8313e8e3d7d11a65ab4e239bb" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="QoQ-Med-Building-Multimodal-Clinical-Foundation-Models-with-Domain-Aware-GRPO-Training"><a href="#QoQ-Med-Building-Multimodal-Clinical-Foundation-Models-with-Domain-Aware-GRPO-Training" class="headerlink" title="QoQ-Med: Building Multimodal Clinical Foundation Models with   Domain-Aware GRPO Training"></a>QoQ-Med: Building Multimodal Clinical Foundation Models with   Domain-Aware GRPO Training</h2><p><strong>Authors:Wei Dai, Peilin Chen, Chanakya Ekbote, Paul Pu Liang</strong></p>
<p>Clinical decision-making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision-centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B&#x2F;32B, the first open generalist clinical foundation model that jointly reasons across medical images, time-series signals, and text reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization (DRPO), a novel reinforcement-learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces at <a target="_blank" rel="noopener" href="https://github.com/DDVD233/QoQ_Med">https://github.com/DDVD233/QoQ_Med</a>. </p>
<blockquote>
<p>ä¸´åºŠå†³ç­–é€šå¸¸éœ€è¦å¤„ç†å„ç§å¼‚è´¨æ•°æ®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»ç„¶ä¸»è¦ä¾§é‡äºè§†è§‰é¢†åŸŸï¼Œæ— æ³•åœ¨ä¸´åºŠå„ç§‘ä¹‹é—´å®ç°é€šç”¨åŒ–ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†QoQ-Med-7B&#x2F;32Bï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æ”¾çš„ä¸´åºŠåŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥è”åˆå¤„ç†åŒ»å­¦å›¾åƒã€æ—¶é—´åºåˆ—ä¿¡å·å’Œæ–‡æœ¬æŠ¥å‘Šã€‚QoQ-Medé‡‡ç”¨é¢†åŸŸæ„ŸçŸ¥çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDRPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ ç›®æ ‡ï¼ŒæŒ‰é¢†åŸŸç¨€æœ‰æ€§å’Œæ¨¡æ€éš¾åº¦åˆ†å±‚ç¼©æ”¾æ ‡å‡†åŒ–å¥–åŠ±ï¼Œä»è€Œç¼“è§£ç”±å€¾æ–œçš„ä¸´åºŠæ•°æ®åˆ†å¸ƒå¯¼è‡´çš„æ€§èƒ½ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨æ¶µç›–9ä¸ªä¸´åºŠé¢†åŸŸçš„261ä¸‡ä¸ªæŒ‡ä»¤è°ƒæ•´å¯¹æ•°æ®å¯¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬è¯æ˜ï¼Œä¸å…¶ä»–æ— æ‰¹åˆ¤è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ç›¸æ¯”ï¼ŒDRPOè®­ç»ƒåœ¨å¹³å‡å®è§‚F1å¾—åˆ†ä¸Šæé«˜äº†43%çš„è¯Šæ–­æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨QoQ-Medåœ¨å¯†é›†åˆ†å‰²æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œå®ƒèƒ½å¤Ÿçªå‡ºæ˜¾ç¤ºä¸è¯Šæ–­ç›¸å…³çš„å…³é”®åŒºåŸŸï¼Œå…¶IoUå€¼é«˜å‡ºå¼€æ”¾æ¨¡å‹çš„åå€ä»¥ä¸Šï¼ŒåŒæ—¶è¾¾åˆ°OpenAI o4-miniçš„æ€§èƒ½æ°´å¹³ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§å’Œä¸‹æ¸¸ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/DDVD233/QoQ_Med">https://github.com/DDVD233/QoQ_Med</a>ä¸Šå‘å¸ƒäº†ï¼ˆiï¼‰å®Œæ•´çš„æ¨¡å‹æƒé‡ï¼Œï¼ˆiiï¼‰æ¨¡å—åŒ–è®­ç»ƒç®¡é“å’Œï¼ˆiiiï¼‰æ‰€æœ‰ä¸­é—´æ¨ç†è·Ÿè¸ªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00711v2">PDF</a> Accepted as Oral at NeurIPS 2025. Revision after camera ready</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å…¨æ–°çš„ä¸´åºŠé€šç”¨åŸºç¡€æ¨¡å‹â€”â€”QoQ-Med-7B&#x2F;32Bï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨åŒ»å­¦å›¾åƒã€æ—¶é—´åºåˆ—ä¿¡å·å’Œæ–‡æœ¬æŠ¥å‘Šä¹‹é—´è¿›è¡Œè”åˆæ¨ç†ã€‚è¯¥æ¨¡å‹é‡‡ç”¨é¢†åŸŸæ„ŸçŸ¥ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDRPOï¼‰è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿç¼“è§£ç”±äºä¸´åºŠæ•°æ®åˆ†å¸ƒä¸å‡å¯¼è‡´çš„æ€§èƒ½ä¸å¹³è¡¡é—®é¢˜ã€‚ä¸å…¶ä»–æ— æ‰¹è¯„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒDRPOè®­ç»ƒæé«˜äº†å¹³å‡å®è§‚F1åˆ†æ•°43%ã€‚æ­¤å¤–ï¼Œç»è¿‡å¯†é›†åˆ†å‰²æ•°æ®è®­ç»ƒçš„QoQ-Medèƒ½å¤Ÿçªå‡ºä¸è¯Šæ–­ç›¸å…³çš„å…³é”®åŒºåŸŸï¼Œå…¶IoUæ€§èƒ½è¾¾åˆ°å¼€æºæ¨¡å‹çš„åå€ä»¥ä¸Šã€‚ä¸ºäº†ä¿ƒè¿›å¯å¤åˆ¶æ€§å’Œä¸‹æ¸¸ç ”ç©¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æ¨¡å‹æƒé‡ã€æ¨¡å—åŒ–è®­ç»ƒç®¡é“å’Œæ‰€æœ‰ä¸­é—´æ¨ç†è½¨è¿¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>QoQ-Medæ˜¯é¦–ä¸ªå¼€æ”¾çš„ä¸´åºŠé€šç”¨åŸºç¡€æ¨¡å‹ï¼Œæ”¯æŒè·¨åŒ»å­¦å›¾åƒã€æ—¶é—´åºåˆ—ä¿¡å·å’Œæ–‡æœ¬æŠ¥å‘Šçš„è”åˆæ¨ç†ã€‚</li>
<li>DRPOè®­ç»ƒæ³•è¢«ç”¨äºè§£å†³ä¸´åºŠæ•°æ®åˆ†å¸ƒä¸å‡å¯¼è‡´çš„æ€§èƒ½ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>DRPOè®­ç»ƒæé«˜äº†è¯Šæ–­æ€§èƒ½çš„å®è§‚F1åˆ†æ•°43%ï¼Œç›¸è¾ƒäºå…¶ä»–æ— æ‰¹è¯„è®­ç»ƒæ–¹æ³•è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>QoQ-Medç»è¿‡å¯†é›†åˆ†å‰²æ•°æ®è®­ç»ƒåï¼Œèƒ½å¤Ÿé«˜äº®è¯Šæ–­ç›¸å…³çš„å…³é”®åŒºåŸŸï¼ŒIoUæ€§èƒ½ä¼˜å¼‚ã€‚</li>
<li>QoQ-Medçš„IoUæ€§èƒ½è¾¾åˆ°å¼€æºæ¨¡å‹çš„åå€ä»¥ä¸Šã€‚</li>
<li>æ¨¡å‹æƒé‡ã€æ¨¡å—åŒ–è®­ç»ƒç®¡é“å’Œä¸­é—´æ¨ç†è½¨è¿¹å·²å…¬å¼€ï¼Œä¾¿äºç ”ç©¶è€…å’Œå¼€å‘è€…è¿›è¡Œå¯å¤åˆ¶æ€§å’Œä¸‹æ¸¸ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47ac9ab54076583895952dc8fcf6ec49" align="middle">
<img src="https://picx.zhimg.com/v2-acaa80c8c38bb7bed4c7713b6547e84c" align="middle">
<img src="https://picx.zhimg.com/v2-4916383529228dc725c010bac8a90d27" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fa71f2937fc7f81fe63ca8dfe9263990" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Co-Designing Quantum Codes with Transversal Diagonal Gates via   Multi-Agent Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9fdef8b5dc5e569fb7e3c506069c9f05" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Small Drafts, Big Verdict Information-Intensive Visual Reasoning via   Speculation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32298.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
