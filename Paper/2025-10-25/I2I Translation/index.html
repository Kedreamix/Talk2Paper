<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-94544c86e6e5e702d9d9e6f63d505a49~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336303&auth_key=1761336303-0-0-003b7dc88952533bd4b3af89cbae1bbb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    33 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-25-æ›´æ–°"><a href="#2025-10-25-æ›´æ–°" class="headerlink" title="2025-10-25 æ›´æ–°"></a>2025-10-25 æ›´æ–°</h1><h2 id="Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge"><a href="#Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge" class="headerlink" title="Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge"></a>Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge</h2><p><strong>Authors:Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</strong></p>
<p>Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home">https://sites.google.com/view/lddbm/home</a>. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆå»ºæ¨¡çš„è¿›å±•ä½¿å¾—æ‰©æ•£æ¨¡å‹æˆä¸ºä»å¤æ‚æ•°æ®åˆ†å¸ƒä¸­é‡‡æ ·çš„æœ€å…ˆè¿›çš„å·¥å…·ã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬çš„èƒ½åŠ›åœ¨è·¨æ¨¡æ€ç¿»è¯‘ï¼ˆMTï¼‰ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå³åœ¨ä¸åŒæ„Ÿå®˜æ¨¡æ€ä¹‹é—´ç¿»è¯‘ä¿¡æ¯ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºä¸€äº›é™åˆ¶æ€§çš„å‡è®¾ï¼ŒåŒ…æ‹¬å…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œç‰¹å®šæ¨¡æ€æ¶æ„ç­‰ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„é€šç”¨æ€§å’Œç†è®ºåŸºç¡€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLDDBMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹çš„æ½œåœ¨å˜é‡æ‰©å±•çš„è·¨æ¨¡æ€ç¿»è¯‘é€šç”¨æ¡†æ¶ã€‚é€šè¿‡åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— éœ€å¯¹é½ç»´åº¦å°±èƒ½å­¦ä¹ ä»»æ„æ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯¹æ¯”å¯¹é½æŸå¤±æ¥å¼ºåˆ¶é…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é’ˆå¯¹æ½œåœ¨ç©ºé—´ä¸­å™ªå£°é¢„æµ‹çš„åŸŸæ— å…³ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢„æµ‹æŸå¤±æ¥å¼•å¯¼è®­ç»ƒå®ç°å‡†ç¡®çš„è·¨åŸŸç¿»è¯‘ï¼Œå¹¶æ¢ç´¢äº†å¤šç§è®­ç»ƒç­–ç•¥æ¥æé«˜ç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒä»»æ„æ¨¡æ€å¯¹ï¼Œå¹¶åœ¨å¤šç§MTä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼ŒåŒ…æ‹¬å¤šè§†å›¾åˆ°ä¸‰ç»´å½¢çŠ¶ç”Ÿæˆã€å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå¤šè§†å›¾åœºæ™¯åˆæˆç­‰ã€‚å…¨é¢çš„å®éªŒå’Œæ¶ˆèå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¸€èˆ¬æ¨¡æ€ç¿»è¯‘å»ºç«‹äº†æ–°çš„å¼ºå¤§åŸºå‡†ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home%E3%80%82">https://sites.google.com/view/lddbm/homeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20819v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ½œåœ¨å˜ä½“çš„å»å™ªæ‰©æ•£æ¡¥æ¨¡å‹ï¼ˆLatent Denoising Diffusion Bridge Modelï¼Œç®€ç§°LDDBMï¼‰çš„é€šç”¨æ¡†æ¶ï¼Œç”¨äºæ¨¡æ€ç¿»è¯‘ï¼ˆModality Translationï¼Œç®€ç§°MTï¼‰ã€‚è¯¥æ¡†æ¶åœ¨æ½œåœ¨ç©ºé—´æ“ä½œï¼Œæ— éœ€å¯¹é½ç»´åº¦å³å¯å»ºç«‹ä»»æ„æ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”å¯¹é½æŸå¤±å’Œé’ˆå¯¹æ½œåœ¨ç©ºé—´å™ªå£°é¢„æµ‹çš„åŸŸæ— å…³ç¼–ç å™¨è§£ç å™¨æ¶æ„ï¼Œè¯¥æ¡†æ¶å®ç°äº†è·¨åŸŸæ¨¡æ€ç¿»è¯‘çš„ç²¾ç¡®æ€§ã€‚åŒæ—¶æ”¯æŒä»»æ„æ¨¡æ€å¯¹ï¼Œå¹¶åœ¨å¤šç§æ¨¡æ€ç¿»è¯‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤ç ”ç©¶ä¸ºæ¨¡æ€ç¿»è¯‘é¢†åŸŸæä¾›äº†æ–°çš„å¼ºå¤§åŸºçº¿ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§é¡¹ç›®é¡µé¢ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¤æ‚æ•°æ®åˆ†å¸ƒé‡‡æ ·æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨å¤šæ¨¡æ€ç¿»è¯‘ï¼ˆMTï¼‰æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºé™åˆ¶æ€§å‡è®¾ï¼Œå¦‚å…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œç‰¹å®šäºæ¨¡æ€çš„æ¶æ„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„é€šç”¨æ€§å’Œç†è®ºæ”¯æŒã€‚</li>
<li>LDDBMæ¡†æ¶æ˜¯ä¸€ä¸ªé€šç”¨çš„æ¨¡æ€ç¿»è¯‘æ–¹æ³•ï¼ŒåŸºäºæ½œåœ¨å˜é‡çš„å»å™ªæ‰©æ•£æ¡¥æ¨¡å‹ï¼Œå¯åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œæ— éœ€å¯¹é½ç»´åº¦å³å¯å»ºç«‹ä»»æ„æ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ã€‚</li>
<li>LDDBMé€šè¿‡å¼•å…¥å¯¹æ¯”å¯¹é½æŸå¤±æ¥å¼ºåˆ¶æ‰§è¡Œé…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>LDDBMè®¾è®¡äº†ä¸€ç§é’ˆå¯¹æ½œåœ¨ç©ºé—´å™ªå£°é¢„æµ‹çš„åŸŸæ— å…³ç¼–ç å™¨è§£ç å™¨æ¶æ„ã€‚</li>
<li>LDDBMæå‡ºäº†é¢„æµ‹æŸå¤±æ¥å¼•å¯¼è®­ç»ƒä»¥å®ç°å‡†ç¡®çš„è·¨åŸŸç¿»è¯‘ï¼Œå¹¶æ¢ç´¢äº†å¤šç§è®­ç»ƒç­–ç•¥æ¥æé«˜ç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-71b7d87ab3147c25125212c3b2948c06~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336179&auth_key=1761336179-0-0-03dfd4943a6f3f22e6dbbd1a8423e8b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d66e2842144f846f899b138e2dd2d51~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336221&auth_key=1761336221-0-0-f47c27f384dbcbe82eea4c1dfe8e79b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-227bee8414c0a17e145209c970af48a3~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336227&auth_key=1761336227-0-0-ab61837ef4d3b72fce78a1859b3f56b7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Monocular-Visual-8D-Pose-Estimation-for-Articulated-Bicycles-and-Cyclists"><a href="#Monocular-Visual-8D-Pose-Estimation-for-Articulated-Bicycles-and-Cyclists" class="headerlink" title="Monocular Visual 8D Pose Estimation for Articulated Bicycles and   Cyclists"></a>Monocular Visual 8D Pose Estimation for Articulated Bicycles and   Cyclists</h2><p><strong>Authors:Eduardo R. Corral-Soto, Yang Liu, Yuan Ren, Bai Dongfeng, Liu Bingbing</strong></p>
<p>In Autonomous Driving, cyclists belong to the safety-critical class of Vulnerable Road Users (VRU), and accurate estimation of their pose is critical for cyclist crossing intention classification, behavior prediction, and collision avoidance. Unlike rigid objects, articulated bicycles are composed of movable rigid parts linked by joints and constrained by a kinematic structure. 6D pose methods can estimate the 3D rotation and translation of rigid bicycles, but 6D becomes insufficient when the steering&#x2F;pedals angles of the bicycle vary. That is because: 1) varying the articulated pose of the bicycle causes its 3D bounding box to vary as well, and 2) the 3D box orientation is not necessarily aligned to the orientation of the steering which determines the actual intended travel direction. In this work, we introduce a method for category-level 8D pose estimation for articulated bicycles and cyclists from a single RGB image. Besides being able to estimate the 3D translation and rotation of a bicycle from a single image, our method also estimates the rotations of its steering handles and pedals with respect to the bicycle body frame. These two new parameters enable the estimation of a more fine-grained bicycle pose state and travel direction. Our proposed model jointly estimates the 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mix of synthetic and real image data to generalize on real images. We include an evaluation section where we evaluate the accuracy of our estimated 8D pose parameters, and our method shows promising results by achieving competitive scores when compared against state-of-the-art category-level 6D pose estimators that use rigid canonical object templates for matching. </p>
<blockquote>
<p>åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œéª‘è‡ªè¡Œè½¦çš„äººå±äºå…³é”®å®‰å…¨çš„è„†å¼±é“è·¯ä½¿ç”¨ç±»ï¼ˆVRUï¼‰ï¼Œå¯¹å…¶å§¿æ€çš„å‡†ç¡®ä¼°è®¡æ˜¯è¿›è¡Œéª‘è‡ªè¡Œè½¦è€…ç©¿è¶Šæ„å›¾åˆ†ç±»ã€è¡Œä¸ºé¢„æµ‹å’Œé¿å…ç¢°æ’çš„å…³é”®ã€‚ä¸åˆšä½“ä¸åŒï¼Œå…³èŠ‚å¼è‡ªè¡Œè½¦æ˜¯ç”±é€šè¿‡å…³èŠ‚è¿æ¥çš„å¯ç§»åŠ¨åˆšæ€§éƒ¨ä»¶ç»„æˆï¼Œå¹¶å—è¿åŠ¨å­¦ç»“æ„çš„çº¦æŸã€‚6Då§¿æ€æ–¹æ³•èƒ½å¤Ÿä¼°è®¡åˆšæ€§è‡ªè¡Œè½¦çš„3Dæ—‹è½¬å’Œè½¬æ¢ï¼Œä½†å½“è‡ªè¡Œè½¦çš„è½¬å‘&#x2F;è¸æ¿è§’åº¦å‘ç”Ÿå˜åŒ–æ—¶ï¼Œ6Då°±ä¸è¶³ä»¥åº”å¯¹äº†ã€‚è¿™æ˜¯å› ä¸ºï¼š1ï¼‰æ”¹å˜è‡ªè¡Œè½¦çš„å…³èŠ‚å§¿æ€ä¼šå¯¼è‡´å…¶3Dè¾¹ç•Œæ¡†éšä¹‹å˜åŒ–ï¼›2ï¼‰3Dæ¡†çš„æ–¹å‘å¹¶ä¸ä¸€å®šä¸å†³å®šå®é™…è¡Œé©¶æ–¹å‘çš„è½¬å‘æ–¹å‘ä¸€è‡´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é’ˆå¯¹å…³èŠ‚å¼è‡ªè¡Œè½¦å’Œéª‘è‡ªè¡Œè½¦è€…çš„ç±»åˆ«çº§åˆ«8Då§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œä»å•å¼ RGBå›¾åƒè¿›è¡Œä¼°è®¡ã€‚é™¤äº†èƒ½å¤Ÿä»å•ä¸ªå›¾åƒä¸­ä¼°è®¡è‡ªè¡Œè½¦çš„3Dè½¬æ¢å’Œæ—‹è½¬ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥ä¼°è®¡è½¬å‘æ‰‹æŸ„å’Œè¸æ¿ç›¸å¯¹äºè‡ªè¡Œè½¦è½¦èº«æ¡†æ¶çš„æ—‹è½¬ã€‚è¿™ä¸¤ä¸ªæ–°å‚æ•°èƒ½å¤Ÿä¼°è®¡æ›´ç²¾ç»†çš„è‡ªè¡Œè½¦å§¿æ€çŠ¶æ€å’Œè¡Œé©¶æ–¹å‘ã€‚æˆ‘ä»¬æå‡ºçš„æ¨¡å‹è”åˆä¼°è®¡8Då§¿æ€å’Œå…³èŠ‚å¼è‡ªè¡Œè½¦çš„3Då…³é”®ç‚¹ï¼Œå¹¶ç”¨åˆæˆå›¾åƒå’ŒçœŸå®å›¾åƒæ•°æ®çš„æ··åˆè¿›è¡Œè®­ç»ƒï¼Œä»¥åœ¨çœŸå®å›¾åƒä¸Šè¿›è¡Œæ¨å¹¿ã€‚æˆ‘ä»¬åŒ…å«ä¸€ä¸ªè¯„ä¼°éƒ¨åˆ†ï¼Œè¯„ä¼°æˆ‘ä»¬ä¼°è®¡çš„8Då§¿æ€å‚æ•°çš„å‡†ç¡®æ€§ï¼Œå½“ä¸ä½¿ç”¨åˆšæ€§è§„èŒƒå¯¹è±¡æ¨¡æ¿è¿›è¡ŒåŒ¹é…çš„æœ€æ–°6Då§¿æ€ä¼°è®¡å™¨ç›¸æ¯”æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œå®ç°äº†æœ‰ç«äº‰åŠ›çš„åˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20158v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å•è½¦éª‘è¡Œè€…çš„å®‰å…¨å…³é”®ç±»åˆ«â€”â€”è„†å¼±é“è·¯ä½¿ç”¨è€…ï¼ˆVRUï¼‰çš„8Då§¿æ€ä¼°è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½ä»å•å¼ RGBå›¾åƒä¸­ä¼°è®¡è‡ªè¡Œè½¦çš„3Då¹³ç§»å’Œæ—‹è½¬ï¼Œè¿˜èƒ½ä¼°è®¡è½¬å‘æ‰‹æŸ„å’Œè¸æ¿ç›¸å¯¹äºè½¦èº«æ¡†æ¶çš„æ—‹è½¬ï¼Œä»è€Œæ›´ç²¾ç»†åœ°ä¼°è®¡è‡ªè¡Œè½¦çš„å§¿æ€çŠ¶æ€å’Œè¡Œé©¶æ–¹å‘ã€‚é€šè¿‡ç»“åˆåˆæˆå’ŒçœŸå®å›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®å›¾åƒä¸Šå…·æœ‰æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>éª‘è¡Œè€…åŠè‡ªè¡Œè½¦å§¿æ€ä¼°è®¡åœ¨è‡ªåŠ¨é©¾é©¶ä¸­éå¸¸é‡è¦ï¼Œå¯¹äºéª‘è¡Œè€…ç©¿è¶Šæ„å›¾åˆ†ç±»ã€è¡Œä¸ºé¢„æµ‹å’Œç¢°æ’é¿å…å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>ä¼ ç»Ÿçš„6Då§¿æ€ä¼°è®¡æ–¹æ³•æ— æ³•å‡†ç¡®å¤„ç†è‡ªè¡Œè½¦è½¬å‘å’Œè¸æ¿è§’åº¦å˜åŒ–ï¼Œå› æ­¤éœ€è¦æ›´ç²¾ç»†çš„8Då§¿æ€ä¼°è®¡ã€‚</li>
<li>æå‡ºçš„8Då§¿æ€ä¼°è®¡æ–¹æ³•èƒ½åŒæ—¶ä¼°è®¡è‡ªè¡Œè½¦çš„3Då¹³ç§»å’Œæ—‹è½¬ã€è½¬å‘æ‰‹æŸ„å’Œè¸æ¿çš„æ—‹è½¬ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨åˆæˆå’ŒçœŸå®å›¾åƒæ•°æ®çš„æ··åˆè®­ç»ƒï¼Œå®ç°äº†åœ¨çœŸå®å›¾åƒä¸Šçš„æ³›åŒ–ã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸å½“å‰æœ€å…ˆè¿›çš„åŸºäºåˆšæ€§è§„èŒƒå¯¹è±¡æ¨¡æ¿åŒ¹é…çš„6Då§¿æ€ä¼°è®¡å™¨ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7b2338d4df52ec0fcb3fb5d75b5a2224~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336234&auth_key=1761336234-0-0-f239f02bd7745f046d4455abf626f3e1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-efb1ac00fbc7d9ea9810ec59887d92cb~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336242&auth_key=1761336242-0-0-fe70bdba5f8362e1589844bec8361d4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c81a26e0c99076a9beccf7c7bc51a121~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336249&auth_key=1761336249-0-0-3a23e71c2e1684e0ddf674d6a6977c87&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fb3084812ed1da34481b96a6f5a0965c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336255&auth_key=1761336255-0-0-4b0f3002f034cd87cdf36ed6e3010ca2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4d30f092aa428041cc0b1cb418b0a9ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336262&auth_key=1761336262-0-0-d40c5b463a87a4e97ef8399adc9eab54&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Curvilinear-Structure-preserving-Unpaired-Cross-domain-Medical-Image-Translation"><a href="#Curvilinear-Structure-preserving-Unpaired-Cross-domain-Medical-Image-Translation" class="headerlink" title="Curvilinear Structure-preserving Unpaired Cross-domain Medical Image   Translation"></a>Curvilinear Structure-preserving Unpaired Cross-domain Medical Image   Translation</h2><p><strong>Authors:Zihao Chen, Yi Zhou, Xudong Jiang, Li Chen, Leopold Schmetterer, Bingyao Tan, Jun Cheng</strong></p>
<p>Unpaired image-to-image translation has emerged as a crucial technique in medical imaging, enabling cross-modality synthesis, domain adaptation, and data augmentation without costly paired datasets. Yet, existing approaches often distort fine curvilinear structures, such as microvasculature, undermining both diagnostic reliability and quantitative analysis. This limitation is consequential in ophthalmic and vascular imaging, where subtle morphological changes carry significant clinical meaning. We propose Curvilinear Structure-preserving Translation (CST), a general framework that explicitly preserves fine curvilinear structures during unpaired translation by integrating structure consistency into the training. Specifically, CST augments baseline models with a curvilinear extraction module for topological supervision. It can be seamlessly incorporated into existing methods. We integrate it into CycleGAN and UNSB as two representative backbones. Comprehensive evaluation across three imaging modalities: optical coherence tomography angiography, color fundus and X-ray coronary angiography demonstrates that CST improves translation fidelity and achieves state-of-the-art performance. By reinforcing geometric integrity in learned mappings, CST establishes a principled pathway toward curvilinear structure-aware cross-domain translation in medical imaging. </p>
<blockquote>
<p>éé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä½œä¸ºåŒ»å­¦æˆåƒä¸­çš„ä¸€é¡¹å…³é”®æŠ€æœ¯å·²ç»å‡ºç°ï¼Œå®ç°äº†è·¨æ¨¡æ€åˆæˆã€åŸŸé€‚åº”å’Œæ•°æ®å¢å¼ºï¼Œè€Œæ— éœ€æ˜‚è´µçš„é…å¯¹æ•°æ®é›†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¼šæ‰­æ›²ç²¾ç»†çš„æ›²çº¿ç»“æ„ï¼Œå¦‚å¾®è¡€ç®¡ï¼Œä»è€Œç ´åè¯Šæ–­çš„å¯é æ€§å’Œå®šé‡åˆ†æã€‚è¿™ä¸€å±€é™æ€§åœ¨çœ¼ç§‘å’Œè¡€ç®¡æˆåƒä¸­å°¤å…¶é‡è¦ï¼Œç»†å¾®çš„å½¢æ€å˜åŒ–å…·æœ‰é‡è¦çš„ä¸´åºŠæ„ä¹‰ã€‚æˆ‘ä»¬æå‡ºäº†æ›²çº¿ç»“æ„ä¿ç•™ç¿»è¯‘ï¼ˆCSTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆç»“æ„ä¸€è‡´æ€§è¿›è¡Œéé…å¯¹ç¿»è¯‘æ—¶æ˜¾å¼ä¿ç•™ç²¾ç»†çš„æ›²çº¿ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼ŒCSTé€šè¿‡æ‹“æ‰‘ç›‘ç£å¢å¼ºåŸºçº¿æ¨¡å‹ï¼Œå¹¶å¼•å…¥æ›²çº¿æå–æ¨¡å—ã€‚å®ƒå¯ä»¥æ— ç¼åœ°çº³å…¥ç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬å°†å…¶æ•´åˆåˆ°CycleGANå’ŒUNSBä½œä¸ºä¸¤ä¸ªä»£è¡¨æ€§çš„ä¸»å¹²ã€‚åœ¨ä¸‰ç§æˆåƒæ¨¡æ€çš„ç»¼åˆè¯„ä¼°ä¸­ï¼šå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡é€ å½±ã€å½©è‰²çœ¼åº•å’ŒXå°„çº¿å† çŠ¶åŠ¨è„‰é€ å½±ï¼Œè¡¨æ˜CSTæé«˜äº†ç¿»è¯‘ç²¾åº¦å¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨å­¦ä¹ çš„æ˜ å°„ä¸­å¼ºåŒ–å‡ ä½•å®Œæ•´æ€§ï¼ŒCSTä¸ºåŒ»å­¦æˆåƒä¸­çš„é¢å‘æ›²çº¿ç»“æ„çš„è·¨åŸŸç¿»è¯‘å»ºç«‹äº†ç†è®ºé€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19679v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒçš„æ— é…å¯¹è½¬æ¢æŠ€æœ¯å·²å´­éœ²å¤´è§’ï¼Œä¸ºè·¨æ¨¡æ€åˆæˆã€åŸŸé€‚åº”å’Œæ•°æ®å¢å¼ºæä¾›äº†å…³é”®æ‰‹æ®µã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸æ‰­æ›²å¾®ç»†æ›²çº¿ç»“æ„ï¼Œå¦‚å¾®è¡€ç®¡ç­‰ï¼Œå½±å“è¯Šæ–­å¯é æ€§å’Œå®šé‡åˆ†æã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºCSTï¼ˆæ›²çº¿ç»“æ„ä¿ç•™è½¬æ¢ï¼‰çš„é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡é›†æˆç»“æ„ä¸€è‡´æ€§è¿›è¡Œæ— é…å¯¹è½¬æ¢æ—¶æ˜ç¡®ä¿ç•™å¾®ç»†æ›²çº¿ç»“æ„ã€‚CSTé€šè¿‡æ‹“æ‰‘ç›‘ç£å¢å¼ºåŸºçº¿æ¨¡å‹ï¼Œå¯æ— ç¼èå…¥ç°æœ‰æ–¹æ³•ï¼Œå¦‚CycleGANå’ŒUNSBã€‚è¯„ä¼°è¡¨æ˜ï¼ŒCSTæå‡è½¬æ¢ä¿çœŸåº¦å¹¶å®ç°ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚å®ƒå¼ºåŒ–å­¦ä¹ æ˜ å°„ä¸­çš„å‡ ä½•å®Œæ•´æ€§ï¼Œä¸ºåŒ»å­¦å½±åƒçš„æ›²çº¿ç»“æ„æ„ŸçŸ¥è·¨åŸŸè½¬æ¢å»ºç«‹ç†è®ºè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— é…å¯¹å›¾åƒè½¬æ¢æŠ€æœ¯å·²åœ¨åŒ»å­¦æˆåƒä¸­æ˜¾ç¤ºå‡ºé‡è¦ä½œç”¨ï¼Œèƒ½å¤Ÿå®ç°è·¨æ¨¡æ€åˆæˆã€åŸŸé€‚åº”å’Œæ•°æ®å¢å¼ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†åŒ»å­¦å›¾åƒæ—¶å®¹æ˜“æ‰­æ›²å¾®ç»†æ›²çº¿ç»“æ„ï¼Œå¦‚å¾®è¡€ç®¡ï¼Œå½±å“è¯Šæ–­å’Œå®šé‡åˆ†æã€‚</li>
<li>CSTæ¡†æ¶æ—¨åœ¨é€šè¿‡é›†æˆç»“æ„ä¸€è‡´æ€§æ¥ä¿ç•™å¾®ç»†æ›²çº¿ç»“æ„ï¼Œä»è€Œæé«˜è½¬æ¢çš„ä¿çœŸåº¦ã€‚</li>
<li>CSTé€šè¿‡å¼•å…¥æ‹“æ‰‘ç›‘ç£å¢å¼ºåŸºçº¿æ¨¡å‹ï¼Œå¹¶å¯ä»¥æ— ç¼èå…¥ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚CycleGANå’ŒUNSBã€‚</li>
<li>åœ¨ä¸‰ç§æˆåƒæ¨¡æ€ï¼ˆå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡é€ å½±ã€å½©è‰²çœ¼åº•å’ŒXå°„çº¿å† çŠ¶åŠ¨è„‰é€ å½±ï¼‰çš„ç»¼åˆè¯„ä¼°ä¸­ï¼ŒCSTè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>CSTé€šè¿‡å¼ºåŒ–å‡ ä½•å®Œæ•´æ€§åœ¨å­¦ä¹ çš„æ˜ å°„ä¸­å»ºç«‹äº†ä¸€ä¸ªç†è®ºè·¯å¾„ï¼Œä¸ºåŒ»å­¦å½±åƒçš„è·¨åŸŸè½¬æ¢æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4152cd3d5f2a89331fc01c9e15fddbc6~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336270&auth_key=1761336270-0-0-445fbe2a73f0ea312e1140ac1b46f543&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6721f4888c07916ccf8de85599a5ba2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336277&auth_key=1761336277-0-0-52c01c92724b95e4bb0eebe91c1d148a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ace988afe912cc755bda1e93d324e52f~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336283&auth_key=1761336283-0-0-1a52c8bc9d1d86797fa8f1ffc8202677&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6f239c6e44ab78e8bf382e5e02f43bff~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336290&auth_key=1761336290-0-0-477d414735db18ea8a757fe3bb013a43&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3f57d4f98c6db8e7ac650f81fc5d204b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336297&auth_key=1761336297-0-0-4717723ae1a9d239b396dc04a8db46b6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94544c86e6e5e702d9d9e6f63d505a49~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336303&auth_key=1761336303-0-0-003b7dc88952533bd4b3af89cbae1bbb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-214dd98159f30f73849e098f1e71feaf~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336310&auth_key=1761336310-0-0-6aab1b0f6edf3ac5fa68ff8988292d58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Re-evaluating-Minimum-Bayes-Risk-Decoding-for-Automatic-Speech-Recognition"><a href="#Re-evaluating-Minimum-Bayes-Risk-Decoding-for-Automatic-Speech-Recognition" class="headerlink" title="Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech   Recognition"></a>Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech   Recognition</h2><p><strong>Authors:Yuu Jinnai</strong></p>
<p>Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding outperforms beam search in text-to-text generation tasks, such as machine translation, text summarization, and image captioning. On the other hand, beam search is the current practice for speech-to-text tasks such as automatic speech recognition (ASR) and Speech Translation (ST). Given that MBR decoding is effective in text-to-text generation tasks, it is reasonable to expect it to also be effective for speech-to-text tasks. In this paper, we evaluate MBR decoding for ASR and ST tasks on English and Japanese using Whisper and its derivative models. We observe that the accuracy of MBR decoding outperforms that of beam search in most of the experimental settings we have evaluated. The results show that MBR decoding is a promising method for offline ASR and ST tasks that require high accuracy. The code is available at <a target="_blank" rel="noopener" href="https://github.com/CyberAgentAILab/mbr-for-asr">https://github.com/CyberAgentAILab/mbr-for-asr</a> </p>
<blockquote>
<p>è¿‘æœŸçš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºæ ·æœ¬çš„æœ€å°è´å¶æ–¯é£é™©ï¼ˆMBRï¼‰è§£ç åœ¨æ–‡æœ¬åˆ°æ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦å’Œå›¾åƒæè¿°ï¼‰ä¸­è¡¨ç°ä¼˜äºé›†æŸæœç´¢ã€‚å¦ä¸€æ–¹é¢ï¼Œé›†æŸæœç´¢æ˜¯å½“å‰è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡ï¼ˆå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰ï¼‰çš„å¸¸ç”¨æ–¹æ³•ã€‚é‰´äºMBRè§£ç åœ¨æ–‡æœ¬åˆ°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æœ‰ç†ç”±æœŸå¾…å®ƒåœ¨è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡ä¸­ä¹ŸåŒæ ·æœ‰æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä½¿ç”¨WhisperåŠå…¶è¡ç”Ÿæ¨¡å‹å¯¹ASRå’ŒSTä»»åŠ¡çš„MBRè§£ç çš„è‹±è¯­å’Œæ—¥è¯­è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨æ‰€è¯„ä¼°çš„å¤§éƒ¨åˆ†å®éªŒç¯å¢ƒä¸­ï¼ŒMBRè§£ç çš„å‡†ç¡®æ€§éƒ½ä¼˜äºé›†æŸæœç´¢ã€‚ç»“æœè¡¨æ˜ï¼ŒMBRè§£ç æ˜¯åœ¨çº¿ASRå’ŒSTä»»åŠ¡ä¸­ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨é«˜å‡†ç¡®æ€§è¦æ±‚çš„ä»»åŠ¡ä¸­ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/CyberAgentAILab/mbr-for-asr%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CyberAgentAILab/mbr-for-asrè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19471v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºæ ·æœ¬çš„æœ€å°è´å¶æ–¯é£é™©ï¼ˆMBRï¼‰è§£ç åœ¨è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡ï¼ˆå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ï¼‰ä¸­çš„è¡¨ç°ã€‚æ–‡ç« è¯„ä¼°äº†MBRè§£ç åœ¨è‹±è¯­å’Œæ—¥è¯­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå‘ç°å…¶åœ¨å¤šæ•°å®éªŒè®¾ç½®ä¸‹çš„è¡¨ç°ä¼˜äºç›®å‰å¸¸ç”¨çš„beam searchç®—æ³•ã€‚ç»“æœè¯æ˜MBRè§£ç å¯¹äºéœ€è¦é«˜ç²¾åº¦çš„ç¦»çº¿è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MBRè§£ç åœ¨æ–‡æœ¬åˆ°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å·²è¡¨ç°å‡ºä¼˜äºbeam searchçš„æ€§èƒ½ã€‚</li>
<li>æ–‡ç« é¦–æ¬¡æ¢ç´¢äº†MBRè§£ç åœ¨è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡ï¼ˆå¦‚ASRå’ŒSTï¼‰ä¸­çš„åº”ç”¨ã€‚</li>
<li>åœ¨è‹±è¯­å’Œæ—¥è¯­çš„ASRå’ŒSTä»»åŠ¡ä¸Šï¼ŒMBRè§£ç çš„å®éªŒè¡¨ç°ä¼˜äºbeam searchã€‚</li>
<li>MBRè§£ç ç‰¹åˆ«é€‚ç”¨äºéœ€è¦é«˜å‡†ç¡®ç‡çš„ç¦»çº¿ASRå’ŒSTä»»åŠ¡ã€‚</li>
<li>æ–‡ç« æä¾›çš„ä»£ç å¯ç”¨äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨MBRè§£ç äºASRä»»åŠ¡ã€‚</li>
<li>MBRè§£ç æ–¹æ³•å…·æœ‰æ½œåŠ›ï¼Œå¯èƒ½æˆä¸ºæœªæ¥è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡çš„æ–°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a94913ccdea8706da36e8e4463932141~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336319&auth_key=1761336319-0-0-3e6508b81f84e8aa97f3db13e7ed2838&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bfd0e38d8fe2eff8a9fb0df767cb2ffd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336326&auth_key=1761336326-0-0-7b79139c7c0944dc9828554f382953b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2f422cde4291d9d07aee4fa01e4210d0~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336332&auth_key=1761336332-0-0-f4a837273cb885fb2663b0ec1e11b5c5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d91a6f964bda89357bba3dc4beec3052~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336339&auth_key=1761336339-0-0-f89cf2033c56f5a9e21b3b4362105329&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0069302f91397efb88f113e0a2bc9e18~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336346&auth_key=1761336346-0-0-b41323866605f3d1036c45347be58f5d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Real-time-inline-quantitative-MRI-enabled-by-scanner-integrated-machine-learning-a-proof-of-principle-with-NODDI"><a href="#Real-time-inline-quantitative-MRI-enabled-by-scanner-integrated-machine-learning-a-proof-of-principle-with-NODDI" class="headerlink" title="Real-time, inline quantitative MRI enabled by scanner-integrated machine   learning: a proof of principle with NODDI"></a>Real-time, inline quantitative MRI enabled by scanner-integrated machine   learning: a proof of principle with NODDI</h2><p><strong>Authors:Samuel Rot, Iulius Dragonu, Christina Triantafyllou, Matthew Grech-Sollars, Anastasia Papadaki, Laura Mancini, Stephen Wastling, Jennifer Steeden, John S. Thornton, Tarek Yousry, Claudia A. M. Gandini Wheeler-Kingshott, David L. Thomas, Daniel C. Alexander, Hui Zhang</strong></p>
<p>Purpose: The clinical feasibility and translation of many advanced quantitative MRI (qMRI) techniques are inhibited by their restriction to â€˜research modeâ€™, due to resource-intensive, offline parameter estimation. This work aimed to achieve â€˜clinical modeâ€™ qMRI, by real-time, inline parameter estimation with a trained neural network (NN) fully integrated into a vendorâ€™s image reconstruction environment, therefore facilitating and encouraging clinical adoption of advanced qMRI techniques. Methods: The Siemens Image Calculation Environment (ICE) pipeline was customised to deploy trained NNs for advanced diffusion MRI parameter estimation with Open Neural Network Exchange (ONNX) Runtime. Two fully-connected NNs were trained offline with data synthesised with the neurite orientation dispersion and density imaging (NODDI) model, using either conventionally estimated (NNMLE) or ground truth (NNGT) parameters as training labels. The strategy was demonstrated online with an in vivo acquisition and evaluated offline with synthetic test data. Results: NNs were successfully integrated and deployed natively in ICE, performing inline, whole-brain, in vivo NODDI parameter estimation in &lt;10 seconds. DICOM parametric maps were exported from the scanner for further analysis, generally finding that NNMLE estimates were more consistent than NNGT with conventional estimates. Offline evaluation confirms that NNMLE has comparable accuracy (or bias) and precision (or robustness to noise), whereas NNGT exhibits compromised accuracy at the benefit of higher precision. Conclusion: Real-time, inline parameter estimation with the proposed generalisable framework resolves a key practical barrier to clinical uptake of advanced qMRI methods and enables their efficient integration into clinical workflows. </p>
<blockquote>
<p>ç›®çš„ï¼šè®¸å¤šå…ˆè¿›çš„å®šé‡ç£å…±æŒ¯æˆåƒï¼ˆqMRIï¼‰æŠ€æœ¯ç”±äºèµ„æºå¯†é›†å‹çš„ç¦»çº¿å‚æ•°ä¼°è®¡è€Œä»…é™äºâ€œç ”ç©¶æ¨¡å¼â€ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸Šçš„å¯è¡Œæ€§å’Œç¿»è¯‘ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨é€šè¿‡å®æ—¶ã€å†…è”å‚æ•°ä¼°è®¡ï¼Œåˆ©ç”¨ç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼ˆNNï¼‰å®Œå…¨èå…¥ä¾›åº”å•†çš„å›¾åƒé‡å»ºç¯å¢ƒï¼Œä»è€Œå®ç°â€œä¸´åºŠæ¨¡å¼â€çš„qMRIï¼Œä»è€Œä¿ƒè¿›å’Œé¼“åŠ±å…ˆè¿›çš„qMRIæŠ€æœ¯åœ¨ä¸´åºŠä¸Šçš„é‡‡ç”¨ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šå®šåˆ¶äº†Siemenså›¾åƒè®¡ç®—ç¯å¢ƒï¼ˆICEï¼‰ç®¡é“ï¼Œä»¥éƒ¨ç½²ç»è¿‡è®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼Œç”¨äºé«˜çº§æ‰©æ•£MRIå‚æ•°ä¼°è®¡ï¼Œå¹¶ä½¿ç”¨Openç¥ç»ç½‘ç»œäº¤æ¢ï¼ˆONNXï¼‰è¿è¡Œæ—¶ã€‚ä¸¤ä¸ªå…¨è¿æ¥çš„ç¥ç»ç½‘ç»œä½¿ç”¨ç¥ç»ä¸æ–¹å‘æ‰©æ•£å’Œå¯†åº¦æˆåƒï¼ˆNODDIï¼‰æ¨¡å‹åˆæˆçš„æ•°æ®è¿›è¡Œç¦»çº¿è®­ç»ƒï¼Œä½¿ç”¨ä¼ ç»Ÿä¼°è®¡ï¼ˆNNMLEï¼‰æˆ–çœŸå®å€¼ï¼ˆNNGTï¼‰å‚æ•°ä½œä¸ºè®­ç»ƒæ ‡ç­¾ã€‚è¯¥ç­–ç•¥é€šè¿‡åœ¨çº¿çš„æ´»ä½“é‡‡é›†è¿›è¡Œæ¼”ç¤ºï¼Œå¹¶ä½¿ç”¨åˆæˆæµ‹è¯•æ•°æ®è¿›è¡Œç¦»çº¿è¯„ä¼°ã€‚</p>
<p>ç»“æœï¼šç¥ç»ç½‘ç»œæˆåŠŸé›†æˆå¹¶åŸç”Ÿéƒ¨ç½²åœ¨ICEä¸­ï¼Œåœ¨&lt;10ç§’å†…æ‰§è¡Œå†…è”ã€å…¨è„‘ã€æ´»ä½“çš„NODDIå‚æ•°ä¼°è®¡ã€‚DICOMå‚æ•°å›¾ä»æ‰«æä»ªå¯¼å‡ºï¼Œç”¨äºè¿›ä¸€æ­¥åˆ†æï¼Œé€šå¸¸å‘ç°NNMLEä¼°è®¡ä¸ä¼ ç»Ÿä¼°è®¡ç›¸æ¯”æ›´ä¸ºä¸€è‡´ã€‚ç¦»çº¿è¯„ä¼°è¯å®ï¼ŒNNMLEå…·æœ‰ç›¸å½“çš„å‡†ç¡®æ€§ï¼ˆæˆ–åå·®ï¼‰å’Œç²¾ç¡®åº¦ï¼ˆæˆ–ç¨³å¥æ€§ï¼‰ï¼Œè€ŒNNGTåˆ™åœ¨ç‰ºç‰²å‡†ç¡®æ€§çš„æƒ…å†µä¸‹è¡¨ç°å‡ºæ›´é«˜çš„ç²¾ç¡®åº¦ã€‚</p>
<p>ç»“è®ºï¼šåˆ©ç”¨æ‰€æå‡ºçš„é€šç”¨æ¡†æ¶è¿›è¡Œå®æ—¶å†…è”å‚æ•°ä¼°è®¡ï¼Œè§£å†³äº†å…ˆè¿›qMRIæ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å…³é”®å®é™…éšœç¢ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°å°†å…¶æ•´åˆåˆ°ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12632v2">PDF</a> 19 pages, 5 figures, 2 supporting materials</p>
<p><strong>Summary</strong></p>
<p>æ­¤æ‘˜è¦åœ¨æ·±åº¦æŒ–æ˜æ–‡æœ¬æ ¸å¿ƒå†…å®¹çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡ç®€ç»ƒçš„ä¸­æ–‡è¡¨è¾¾å‡ºäº†ç ”ç©¶çš„å…³é”®æ„ä¹‰ï¼šâ€œé‡‡ç”¨è®­ç»ƒåçš„ç¥ç»ç½‘ç»œè¿›è¡Œå®æ—¶ã€åœ¨çº¿çš„å‚æ•°ä¼°è®¡ï¼Œçªç ´å…ˆè¿›å®šé‡æ ¸ç£å…±æŒ¯æˆåƒæŠ€æœ¯ä»â€˜ç ”ç©¶æ¨¡å¼â€™å‘â€˜ä¸´åºŠæ¨¡å¼â€™è½¬å˜çš„å£å’ï¼Œæ¨åŠ¨å…¶åœ¨ä¸´åºŠæ²»ç–—ä¸­çš„åº”ç”¨ã€‚â€æ­¤æ‘˜è¦ä¸ä»…å‡†ç¡®åœ°æ•æ‰äº†æ–‡æœ¬çš„ä¸»æ—¨ï¼Œè¿˜åšåˆ°äº†ç®€æ˜æ‰¼è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„ä¸ƒä¸ªå…³é”®æ´å¯Ÿè¦ç‚¹ï¼š</p>
<ul>
<li>æ­¤ç ”ç©¶å·¥ä½œçš„ç›®çš„æ˜¯å®ç°â€˜ä¸´åºŠæ¨¡å¼â€™çš„å®šé‡ç£å…±æŒ¯æˆåƒï¼ˆqMRIï¼‰ï¼Œé€šè¿‡å®æ—¶ã€åœ¨çº¿çš„å‚æ•°ä¼°è®¡ï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œï¼ˆNNï¼‰å®Œå…¨é›†æˆåˆ°ä¾›åº”å•†çš„å›¾åƒé‡å»ºç¯å¢ƒä¸­ã€‚</li>
<li>é‡‡ç”¨Siemens Image Calculation Environment (ICE)ç®¡é“éƒ¨ç½²è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œï¼Œç”¨äºé«˜çº§æ‰©æ•£MRIå‚æ•°ä¼°è®¡ã€‚</li>
<li>ä½¿ç”¨Open Neural Network Exchange (ONNX) Runtimeè¿›è¡Œç¥ç»ç½‘ç»œè®­ç»ƒä¸éƒ¨ç½²ã€‚</li>
<li>è®­ç»ƒäº†ä¸¤ä¸ªå…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œæ•°æ®é€šè¿‡ç¥ç»ä¸æ–¹å‘æ‰©æ•£å’Œå¯†åº¦æˆåƒï¼ˆNODDIï¼‰æ¨¡å‹åˆæˆã€‚</li>
<li>ç¥ç»ç½‘ç»œæˆåŠŸé›†æˆå¹¶éƒ¨ç½²åœ¨ICEä¸­ï¼Œå®ç°äº†åœ¨çº¿ã€å…¨è„‘ã€ä½“å†…NODDIå‚æ•°ä¼°è®¡ï¼Œè€—æ—¶ä¸åˆ°10ç§’ã€‚</li>
<li>DICOMå‚æ•°å›¾ä»æ‰«æä»ªå¯¼å‡ºï¼Œè¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒåŸºäºä¼ ç»Ÿä¼°è®¡çš„NNMLEé¢„æµ‹ç»“æœæ›´ä¸ºä¸€è‡´ã€‚</li>
<li>ç¦»çº¿è¯„ä¼°æ˜¾ç¤ºï¼ŒNNMLEå…·æœ‰ç›¸å½“çš„å‡†ç¡®æ€§ï¼ˆæˆ–åå·®ï¼‰å’Œç²¾ç¡®åº¦ï¼ˆæˆ–æŠ—å™ªå£°èƒ½åŠ›ï¼‰ï¼Œè€ŒåŸºäºçœŸå®æ ‡ç­¾NNGTçš„å‡†ç¡®æ€§æœ‰æ‰€é™ä½ä½†ç²¾åº¦æ›´é«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-bc1501f9864ba709a364ee3f30359c8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336353&auth_key=1761336353-0-0-f8fc2289aede90651fad736a1fbd503d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Doctor-Approved-Generating-Medically-Accurate-Skin-Disease-Images-through-AI-Expert-Feedback"><a href="#Doctor-Approved-Generating-Medically-Accurate-Skin-Disease-Images-through-AI-Expert-Feedback" class="headerlink" title="Doctor Approved: Generating Medically Accurate Skin Disease Images   through AI-Expert Feedback"></a>Doctor Approved: Generating Medically Accurate Skin Disease Images   through AI-Expert Feedback</h2><p><strong>Authors:Janet Wang, Yunbei Zhang, Zhengming Ding, Jihun Hamm</strong></p>
<p>Paucity of medical data severely limits the generalizability of diagnostic ML models, as the full spectrum of disease variability can not be represented by a small clinical dataset. To address this, diffusion models (DMs) have been considered as a promising avenue for synthetic image generation and augmentation. However, they frequently produce medically inaccurate images, deteriorating the model performance. Expert domain knowledge is critical for synthesizing images that correctly encode clinical information, especially when data is scarce and quality outweighs quantity. Existing approaches for incorporating human feedback, such as reinforcement learning (RL) and Direct Preference Optimization (DPO), rely on robust reward functions or demand labor-intensive expert evaluations. Recent progress in Multimodal Large Language Models (MLLMs) reveals their strong visual reasoning capabilities, making them adept candidates as evaluators. In this work, we propose a novel framework, coined MAGIC (Medically Accurate Generation of Images through AI-Expert Collaboration), that synthesizes clinically accurate skin disease images for data augmentation. Our method creatively translates expert-defined criteria into actionable feedback for image synthesis of DMs, significantly improving clinical accuracy while reducing the direct human workload. Experiments demonstrate that our method greatly improves the clinical quality of synthesized skin disease images, with outputs aligning with dermatologist assessments. Additionally, augmenting training data with these synthesized images improves diagnostic accuracy by +9.02% on a challenging 20-condition skin disease classification task, and by +13.89% in the few-shot setting. </p>
<blockquote>
<p>åŒ»å­¦æ•°æ®çš„åŒ®ä¹ä¸¥é‡é™åˆ¶äº†è¯Šæ–­æœºå™¨å­¦ä¹ æ¨¡å‹çš„é€šç”¨æ€§ï¼Œå› ä¸ºç–¾ç—…çš„å…¨éƒ¨å˜å¼‚è°±æ— æ³•ç”±å°å‹ä¸´åºŠæ•°æ®é›†æ¥è¡¨ç¤ºã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²è¢«è§†ä¸ºåˆæˆå›¾åƒç”Ÿæˆå’Œå¢å¼ºçš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸äº§ç”ŸåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒï¼Œä»è€Œé™ä½äº†æ¨¡å‹æ€§èƒ½ã€‚åœ¨æ•°æ®ç¨€ç¼ºä¸”è´¨é‡èƒœè¿‡æ•°é‡çš„æƒ…å†µä¸‹ï¼Œé¢†åŸŸä¸“å®¶çš„çŸ¥è¯†å¯¹äºåˆæˆæ­£ç¡®ç¼–ç ä¸´åºŠä¿¡æ¯çš„å›¾åƒè‡³å…³é‡è¦ã€‚ç°æœ‰çš„èå…¥äººç±»åé¦ˆçš„æ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œä¾èµ–äºç¨³å¥çš„å¥–åŠ±åŠŸèƒ½æˆ–éœ€è¦å¤§é‡çš„ä¸“å®¶è¯„ä¼°ã€‚æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºè¯„ä¼°è€…çš„åˆé€‚å€™é€‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåä¸ºMAGICï¼ˆé€šè¿‡AI-Expertåä½œè¿›è¡ŒåŒ»å­¦å‡†ç¡®çš„å›¾åƒç”Ÿæˆï¼‰ï¼Œè¯¥æ¡†æ¶åˆæˆä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç—…å›¾åƒä»¥è¿›è¡Œæ•°æ®å¢å¼ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ›é€ æ€§åœ°å°†ä¸“å®¶å®šä¹‰çš„æ ‡å‡†è½¬åŒ–ä¸ºå¯¹æ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆæˆçš„å¯æ“ä½œåé¦ˆï¼Œè¿™æ˜¾è‘—æé«˜äº†ä¸´åºŠå‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†ç›´æ¥äººåŠ›æŠ•å…¥ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§æé«˜äº†åˆæˆçš®è‚¤ç—…å›¾åƒçš„ä¸´åºŠè´¨é‡ï¼Œè¾“å‡ºç»“æœä¸çš®è‚¤ç§‘åŒ»ç”Ÿçš„è¯„ä¼°ç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è¿™äº›åˆæˆå›¾åƒå¢å¼ºè®­ç»ƒæ•°æ®ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„20ç§çš®è‚¤ç—…åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯Šæ–­å‡†ç¡®ç‡æé«˜äº†+9.02%ï¼Œåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹æé«˜äº†+13.89%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12323v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨åŒ»ç–—æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨äººå·¥æ™ºèƒ½ä¸ä¸“å®¶åˆä½œç”Ÿæˆä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç–¾ç—…å›¾åƒè¿›è¡Œæ•°æ®å¢å¼ºçš„é—®é¢˜ã€‚æå‡ºä¸€ç§åä¸ºMAGICçš„æ–°æ¡†æ¶ï¼Œç»“åˆæ‰©æ•£æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæ ¹æ®ä¸“å®¶å®šä¹‰çš„å‡†åˆ™ç”ŸæˆåŒ»å­¦å‡†ç¡®çš„å›¾åƒï¼Œæé«˜äº†ä¸´åºŠå‡†ç¡®æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—æ•°æ®çš„ç¼ºä¹é™åˆ¶äº†è¯Šæ–­MLæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œå¢å¼ºæ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å¸¸å¸¸äº§ç”ŸåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒã€‚</li>
<li>ä¸“å®¶é¢†åŸŸçŸ¥è¯†å¯¹äºåœ¨æ•°æ®ç¨€ç¼ºæ—¶æ­£ç¡®ç¼–ç ä¸´åºŠä¿¡æ¯è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰èå…¥äººç±»åé¦ˆçš„æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œä¾èµ–äºç¨³å¥çš„å¥–åŠ±å‡½æ•°æˆ–éœ€è¦è€—æ—¶çš„ä¸“å®¶è¯„ä¼°ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºçš„MAGICæ¡†æ¶ç»“åˆäº†äººå·¥æ™ºèƒ½ä¸ä¸“å®¶åˆä½œï¼Œèƒ½ç”Ÿæˆä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç–¾ç—…å›¾åƒç”¨äºæ•°æ®å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9c50aa8ed00c0f2d6f2b631ab5ced920~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336361&auth_key=1761336361-0-0-818d40bb78fb0c0c9f2b6dd53cf21cc4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a3fb120011fcfafaa2851c5fb95f845~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336368&auth_key=1761336368-0-0-1b8c786c1ccdb9c6c831cdb7c9c81ecf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fe52b39bc4308923cd78ed128ec6d42~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336375&auth_key=1761336375-0-0-283d288cfb32e85047cb116d1004b87a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89706c45d139275fd6d1e54199726085~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336382&auth_key=1761336382-0-0-d3d2eff1b09b0bbb0396984ccbe610a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OpenMIBOOD-Open-Medical-Imaging-Benchmarks-for-Out-Of-Distribution-Detection"><a href="#OpenMIBOOD-Open-Medical-Imaging-Benchmarks-for-Out-Of-Distribution-Detection" class="headerlink" title="OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution   Detection"></a>OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution   Detection</h2><p><strong>Authors:Max Gutbrod, David Rauber, Danilo Weber Nunes, Christoph Palm</strong></p>
<p>The growing reliance on Artificial Intelligence (AI) in critical domains such as healthcare demands robust mechanisms to ensure the trustworthiness of these systems, especially when faced with unexpected or anomalous inputs. This paper introduces the Open Medical Imaging Benchmarks for Out-Of-Distribution Detection (OpenMIBOOD), a comprehensive framework for evaluating out-of-distribution (OOD) detection methods specifically in medical imaging contexts. OpenMIBOOD includes three benchmarks from diverse medical domains, encompassing 14 datasets divided into covariate-shifted in-distribution, near-OOD, and far-OOD categories. We evaluate 24 post-hoc methods across these benchmarks, providing a standardized reference to advance the development and fair comparison of OOD detection methods. Results reveal that findings from broad-scale OOD benchmarks in natural image domains do not translate to medical applications, underscoring the critical need for such benchmarks in the medical field. By mitigating the risk of exposing AI models to inputs outside their training distribution, OpenMIBOOD aims to support the advancement of reliable and trustworthy AI systems in healthcare. The repository is available at <a target="_blank" rel="noopener" href="https://github.com/remic-othr/OpenMIBOOD">https://github.com/remic-othr/OpenMIBOOD</a>. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŒ»ç–—ç­‰å…³é”®é¢†åŸŸçš„ä¾èµ–ç¨‹åº¦ä¸æ–­å¢é•¿ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹æ„å¤–æˆ–å¼‚å¸¸è¾“å…¥æ—¶ï¼Œéœ€è¦å»ºç«‹ç¨³å¥çš„æœºåˆ¶æ¥ç¡®ä¿è¿™äº›ç³»ç»Ÿçš„å¯ä¿¡åº¦ã€‚æœ¬æ–‡ä»‹ç»äº†ç”¨äºæ£€æµ‹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰çš„å¼€æ”¾åŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ï¼ˆOpenMIBOODï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°åŒ»å­¦æˆåƒç¯å¢ƒä¸­OODæ£€æµ‹æ–¹æ³•çš„å…¨é¢æ¡†æ¶ã€‚OpenMIBOODåŒ…å«æ¥è‡ªä¸åŒåŒ»å­¦é¢†åŸŸçš„ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–14ä¸ªæ•°æ®é›†ï¼Œåˆ†ä¸ºåå˜é‡åç§»å†…åˆ†å¸ƒã€è¿‘OODå’Œè¿œOODç±»åˆ«ã€‚æˆ‘ä»¬åœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†2 4ç§äº‹åæ–¹æ³•ï¼Œä¸ºæ¨è¿›OODæ£€æµ‹æ–¹æ³•çš„å‘å±•å’Œå…¬å¹³æ¯”è¾ƒæä¾›äº†æ ‡å‡†åŒ–å‚è€ƒã€‚ç»“æœè¡¨æ˜ï¼Œè‡ªç„¶å›¾åƒåŸŸçš„å¤§è§„æ¨¡OODåŸºå‡†æµ‹è¯•çš„ç»“æœå¹¶ä¸èƒ½è½¬åŒ–ä¸ºåŒ»å­¦åº”ç”¨ï¼Œè¿™å¼ºè°ƒäº†åŒ»å­¦é¢†åŸŸå¯¹è¿™ç§åŸºå‡†æµ‹è¯•çš„å…³é”®éœ€æ±‚ã€‚é€šè¿‡é™ä½AIæ¨¡å‹æš´éœ²äºè®­ç»ƒåˆ†å¸ƒä¹‹å¤–è¾“å…¥çš„é£é™©ï¼ŒOpenMIBOODæ—¨åœ¨æ”¯æŒåŒ»ç–—é¢†åŸŸå¯é å’Œå¯ä¿¡çš„AIç³»ç»Ÿçš„è¿›æ­¥ã€‚è¯¥å­˜å‚¨åº“å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/remic-othr/OpenMIBOOD">https://github.com/remic-othr/OpenMIBOOD</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16247v2">PDF</a> Updated results for NNGuide and ViM</p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ç­‰é‡è¦é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œéœ€è¦å»ºç«‹å¯é çš„æœºåˆ¶ç¡®ä¿ç³»ç»Ÿçš„å¯ä¿¡åº¦ã€‚é¢å¯¹æ„å¤–æˆ–å¼‚å¸¸çš„è¾“å…¥ï¼Œæœ¬è®ºæ–‡æå‡ºå¼€æ”¾åŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ï¼ˆOpenMIBOODï¼‰ï¼Œä¸“é—¨è¯„ä¼°åŒ»å­¦æˆåƒä¸­è¶…å‡ºåˆ†å¸ƒèŒƒå›´ï¼ˆOODï¼‰çš„æ£€æµ‹æ–¹æ³•çš„ç»¼åˆæ€§æ¡†æ¶ã€‚OpenMIBOODåŒ…å«æ¥è‡ªä¸åŒåŒ»å­¦é¢†åŸŸçš„ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–14ä¸ªæ•°æ®é›†ï¼Œåˆ†ä¸ºåå˜é‡ç§»åŠ¨å†…éƒ¨åˆ†å¸ƒã€æ¥è¿‘OODå’Œè¿œç¦»OODç±»åˆ«ã€‚æˆ‘ä»¬åœ¨è¿™ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†24ç§äº‹åæ–¹æ³•ï¼Œä¸ºOODæ£€æµ‹æ–¹æ³•çš„å¼€å‘å’Œå…¬å¹³æ¯”è¾ƒæä¾›äº†æ ‡å‡†åŒ–å‚è€ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªç„¶å›¾åƒåŸŸçš„å¹¿æ³›OODåŸºå‡†æµ‹è¯•ä¸é€‚ç”¨äºåŒ»å­¦åº”ç”¨ï¼Œå¼ºè°ƒåŒ»å­¦é¢†åŸŸéœ€è¦æ­¤ç±»åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡é™ä½AIæ¨¡å‹æš´éœ²äºè®­ç»ƒåˆ†å¸ƒä¹‹å¤–è¾“å…¥çš„é£é™©ï¼ŒOpenMIBOODæ—¨åœ¨æ”¯æŒåŒ»ç–—é¢†åŸŸå¯é å’Œå¯ä¿¡çš„AIç³»ç»Ÿçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenMIBOODæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°åŒ»å­¦æˆåƒä¸­OODæ£€æµ‹æ–¹æ³•çš„ç»¼åˆæ€§æ¡†æ¶ã€‚</li>
<li>å®ƒåŒ…å«ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä¸åŒåŒ»å­¦é¢†åŸŸçš„14ä¸ªæ•°æ®é›†ã€‚</li>
<li>ç ”ç©¶äº†24ç§äº‹åæ–¹æ³•ï¼Œä¸ºOODæ£€æµ‹æ–¹æ³•çš„å¼€å‘æä¾›äº†æ ‡å‡†åŒ–å‚è€ƒã€‚</li>
<li>è‡ªç„¶å›¾åƒåŸŸçš„OODåŸºå‡†æµ‹è¯•ä¸é€‚ç”¨äºåŒ»å­¦åº”ç”¨ã€‚</li>
<li>åŒ»å­¦é¢†åŸŸéœ€è¦ä¸“é—¨çš„åŸºå‡†æµ‹è¯•æ¥ç¡®ä¿AIç³»ç»Ÿçš„å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚</li>
<li>OpenMIBOODæœ‰åŠ©äºé™ä½AIæ¨¡å‹æš´éœ²äºè®­ç»ƒåˆ†å¸ƒå¤–çš„é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16247">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cc6d9aa4c166d53c181c8dfa32633dea~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336389&auth_key=1761336389-0-0-0dbb8bf1d4509cb893ffa132e0328140&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2168f2227440762715b42055dc59a4fd~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336396&auth_key=1761336396-0-0-30f0f8a37c37b3a3c7be101220d6cee1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1006ee30509d8096598a6f3d2644ea2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336403&auth_key=1761336403-0-0-86e33f44375f86047be1791894bad8fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MsEdF-A-Multi-stream-Encoder-decoder-Framework-for-Remote-Sensing-Image-Captioning"><a href="#MsEdF-A-Multi-stream-Encoder-decoder-Framework-for-Remote-Sensing-Image-Captioning" class="headerlink" title="MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image   Captioning"></a>MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image   Captioning</h2><p><strong>Authors:Swadhin Das, Raksha Sharma</strong></p>
<p>Remote sensing images contain complex spatial patterns and semantic structures, which makes the captioning model difficult to accurately describe. Encoder-decoder architectures have become the widely used approach for RSIC by translating visual content into descriptive text. However, many existing methods rely on a single-stream architecture, which weakens the model to accurately describe the image. Such single-stream architectures typically struggle to extract diverse spatial features or capture complex semantic relationships, limiting their effectiveness in scenes with high intraclass similarity or contextual ambiguity. In this work, we propose a novel Multi-stream Encoder-decoder Framework (MsEdF) which improves the performance of RSIC by optimizing both the spatial representation and language generation of encoder-decoder architecture. The encoder fuses information from two complementary image encoders, thereby promoting feature diversity through the integration of multiscale and structurally distinct cues. To improve the capture of context-aware descriptions, we refine the input sequenceâ€™s semantic modeling on the decoder side using a stacked GRU architecture with an element-wise aggregation scheme. Experiments on three benchmark RSIC datasets show that MsEdF outperforms several baseline models. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒåŒ…å«å¤æ‚çš„ç©ºé—´æ¨¡å¼å’Œè¯­ä¹‰ç»“æ„ï¼Œè¿™ä½¿å¾—æ ‡æ³¨æ¨¡å‹éš¾ä»¥å‡†ç¡®æè¿°ã€‚ç¼–ç å™¨-è§£ç å™¨æ¶æ„å·²æˆä¸ºé¥æ„Ÿå›¾åƒæ ‡æ³¨ï¼ˆRSICï¼‰çš„å¹¿æ³›åº”ç”¨æ–¹æ³•ï¼Œå°†è§†è§‰å†…å®¹ç¿»è¯‘ä¸ºæè¿°æ€§æ–‡æœ¬ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ–¹æ³•ä¾èµ–äºå•æµæ¶æ„ï¼Œè¿™å‰Šå¼±äº†æ¨¡å‹å‡†ç¡®æè¿°å›¾åƒçš„èƒ½åŠ›ã€‚è¿™ç§å•æµæ¶æ„é€šå¸¸éš¾ä»¥æå–å¤šæ ·çš„ç©ºé—´ç‰¹å¾æˆ–æ•æ‰å¤æ‚çš„è¯­ä¹‰å…³ç³»ï¼Œåœ¨å…·æœ‰é«˜çš„ç±»å†…ç›¸ä¼¼æ€§æˆ–ä¸Šä¸‹æ–‡æ¨¡ç³Šçš„åœºæ™¯ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæµç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼ˆMsEdFï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„ç©ºé—´è¡¨ç¤ºå’Œè¯­è¨€ç”Ÿæˆï¼Œæé«˜äº†é¥æ„Ÿå›¾åƒæ ‡æ³¨çš„æ€§èƒ½ã€‚ç¼–ç å™¨èåˆäº†æ¥è‡ªä¸¤ä¸ªäº’è¡¥å›¾åƒç¼–ç å™¨çš„ä¿¡æ¯ï¼Œé€šè¿‡å¤šå°ºåº¦å’Œç»“æ„ä¸åŒç‰¹å¾çš„èåˆï¼Œä¿ƒè¿›äº†ç‰¹å¾å¤šæ ·æ€§ã€‚ä¸ºäº†æé«˜ä¸Šä¸‹æ–‡æ„ŸçŸ¥æè¿°çš„æ•æ‰èƒ½åŠ›ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨è§£ç å™¨ä¾§ä½¿ç”¨å¸¦æœ‰å…ƒç´ çº§èšåˆæ–¹æ¡ˆçš„å †å GRUæ¶æ„ï¼Œå¯¹è¾“å…¥åºåˆ—çš„è¯­ä¹‰å»ºæ¨¡è¿›è¡Œäº†æ”¹è¿›ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†é¥æ„Ÿå›¾åƒæ ‡æ³¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMsEdFä¼˜äºå‡ ç§åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09282v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¥æ„Ÿå›¾åƒçš„ç©ºé—´æ¨¡å¼å’Œè¯­ä¹‰ç»“æ„å¤æ‚ï¼Œä½¿å¾—å›¾åƒæè¿°å›°éš¾ã€‚ç›®å‰å¹¿æ³›ä½¿ç”¨çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„åœ¨å°†è§†è§‰å†…å®¹è½¬æ¢ä¸ºæè¿°æ–‡æœ¬æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–å•æµæ¶æ„ï¼Œéš¾ä»¥å‡†ç¡®æè¿°å›¾åƒã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹çš„å¤šæµç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼ˆMsEdFï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–ç©ºé—´è¡¨ç¤ºå’Œç¼–ç è§£ç å™¨çš„è¯­è¨€ç”Ÿæˆæ¥æå‡é¥æ„Ÿå›¾åƒæè¿°çš„æ€§èƒ½ã€‚ç¼–ç å™¨èåˆæ¥è‡ªä¸¤ä¸ªäº’è¡¥å›¾åƒç¼–ç å™¨çš„ä¿¡æ¯ï¼Œé€šè¿‡å¤šå°ºåº¦å’Œç»“æ„ä¸åŒçº¿ç´¢çš„æ•´åˆä¿ƒè¿›ç‰¹å¾å¤šæ ·æ€§ã€‚ä¸ºæé«˜ä¸Šä¸‹æ–‡æ„ŸçŸ¥æè¿°çš„æ•æ‰èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨è§£ç å™¨ä¾§ä½¿ç”¨å †å çš„GRUæ¶æ„å’Œå…ƒç´ çº§èšåˆæ–¹æ¡ˆè¿›è¡Œè¾“å…¥åºåˆ—çš„è¯­ä¹‰å»ºæ¨¡ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†é¥æ„Ÿå›¾åƒæè¿°æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMsEdFä¼˜äºå¤šç§åŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒåŒ…å«å¤æ‚çš„ç©ºé—´æ¨¡å¼å’Œè¯­ä¹‰ç»“æ„ï¼Œä½¿å¾—å›¾åƒæè¿°å›°éš¾ã€‚</li>
<li>ç°æœ‰ç¼–ç å™¨-è§£ç å™¨æ¶æ„åœ¨é¥æ„Ÿå›¾åƒæè¿°ï¼ˆRSICï¼‰æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–å•æµæ¶æ„ï¼Œéš¾ä»¥å‡†ç¡®æè¿°å›¾åƒï¼Œå°¤å…¶åœ¨å…·æœ‰é«˜åº¦çš„ç±»å†…ç›¸ä¼¼æ€§æˆ–ä¸Šä¸‹æ–‡æ¨¡ç³Šçš„åœºæ™¯ä¸­ã€‚</li>
<li>æå‡ºçš„MsEdFæ¡†æ¶é€šè¿‡ä¼˜åŒ–ç©ºé—´è¡¨ç¤ºå’Œç¼–ç è§£ç å™¨çš„è¯­è¨€ç”Ÿæˆæ¥æå‡é¥æ„Ÿå›¾åƒæè¿°çš„æ€§èƒ½ã€‚</li>
<li>ç¼–ç å™¨èåˆæ¥è‡ªä¸¤ä¸ªäº’è¡¥å›¾åƒç¼–ç å™¨çš„ä¿¡æ¯ï¼Œä¿ƒè¿›ç‰¹å¾å¤šæ ·æ€§ã€‚</li>
<li>é€šè¿‡åœ¨è§£ç å™¨ä¾§ä½¿ç”¨å †å çš„GRUæ¶æ„å’Œå…ƒç´ çº§èšåˆæ–¹æ¡ˆï¼Œæé«˜ä¸Šä¸‹æ–‡æ„ŸçŸ¥æè¿°çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMsEdFæ€§èƒ½ä¼˜äºå¤šç§ç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8c86046b4ee0618d5d9588ff2cdfd1e9~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336411&auth_key=1761336411-0-0-7e5dc41ccf618a5295062078d200748e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-419b6499a216b04a6357c3f0193bf99c~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336417&auth_key=1761336417-0-0-7fe37db53eaf2ead5bd1fe4ee1a5f200&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ca04538ae8ed26a7bac1ecb0b5655b64~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336424&auth_key=1761336424-0-0-5dcad73022cba3de2ca75dda7747375d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6fd9dfb571833f04cc58cb4d0bbe6042~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336431&auth_key=1761336431-0-0-303bb434735e0f447b8fa00b174ebfb8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d0b4c28a8aa92afcd54bac99f4eac84e~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336437&auth_key=1761336437-0-0-e364d0ec1c547f1c70df9621dad1eef4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-489526bf89c5303bbf2fde5471a129fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336444&auth_key=1761336444-0-0-dd595ac1729775c15f5a81884236ad8f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-f92a631ccd013400e00730c02cafc652~resize:0:q75.jpg?source=1f5c5e47&expiration=1761336722&auth_key=1761336722-0-0-aec3d487638d62b1f36fdec97845e5c6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  SeViCES Unifying Semantic-Visual Evidence Consensus for Long Video   Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-263f18abe5cae4cb9aaabb2353cdf736~resize:0:q75.jpg?source=1f5c5e47&expiration=1761335002&auth_key=1761335002-0-0-a67490130fdafc5c03fbe9f9c947baf3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  A Scalable, Causal, and Energy Efficient Framework for Neural Decoding   with Spiking Neural Networks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
