<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Decoding the Ear A Framework for Objectifying Expressiveness from Human   Preference Through Efficient Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ce1b3eae29aa6a9b555b2797acbfc630')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    43 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-25-æ›´æ–°"><a href="#2025-10-25-æ›´æ–°" class="headerlink" title="2025-10-25 æ›´æ–°"></a>2025-10-25 æ›´æ–°</h1><h2 id="Decoding-the-Ear-A-Framework-for-Objectifying-Expressiveness-from-Human-Preference-Through-Efficient-Alignment"><a href="#Decoding-the-Ear-A-Framework-for-Objectifying-Expressiveness-from-Human-Preference-Through-Efficient-Alignment" class="headerlink" title="Decoding the Ear: A Framework for Objectifying Expressiveness from Human   Preference Through Efficient Alignment"></a>Decoding the Ear: A Framework for Objectifying Expressiveness from Human   Preference Through Efficient Alignment</h2><p><strong>Authors:Zhiyu Lin, Jingwen Yang, Jiale Zhao, Meng Liu, Sunzhu Li, Benyou Wang</strong></p>
<p>Recent speech-to-speech (S2S) models generate intelligible speech but still lack natural expressiveness, largely due to the absence of a reliable evaluation metric. Existing approaches, such as subjective MOS ratings, low-level acoustic features, and emotion recognition are costly, limited, or incomplete. To address this, we present DeEAR (Decoding the Expressive Preference of eAR), a framework that converts human preference for speech expressiveness into an objective score. Grounded in phonetics and psychology, DeEAR evaluates speech across three dimensions: Emotion, Prosody, and Spontaneity, achieving strong alignment with human perception (Spearmanâ€™s Rank Correlation Coefficient, SRCC &#x3D; 0.86) using fewer than 500 annotated samples. Beyond reliable scoring, DeEAR enables fair benchmarking and targeted data curation. It not only distinguishes expressiveness gaps across S2S models but also selects 14K expressive utterances to form ExpressiveSpeech, which improves the expressive score (from 2.0 to 23.4 on a 100-point scale) of S2S models. Demos and codes are available at <a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/ExpressiveSpeech">https://github.com/FreedomIntelligence/ExpressiveSpeech</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰çš„æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆå¯ç†è§£çš„è¯­éŸ³ï¼Œä½†ä»ç„¶ç¼ºä¹è‡ªç„¶çš„è¡¨è¾¾æ€§ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹å¯é çš„è¯„ä¼°æŒ‡æ ‡ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚ä¸»è§‚MOSè¯„åˆ†ã€ä½çº§åˆ«çš„å£°éŸ³ç‰¹å¾ä»¥åŠæƒ…æ„Ÿè¯†åˆ«ç­‰ï¼Œéƒ½å­˜åœ¨æˆæœ¬é«˜æ˜‚ã€å±€é™æ€§å¤§æˆ–ä¸å®Œå…¨ç­‰ç¼ºç‚¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeEARï¼ˆè§£ç è¯­éŸ³è¡¨è¾¾åå¥½ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†äººç±»å¯¹è¯­éŸ³è¡¨è¾¾æ€§çš„åå¥½è½¬åŒ–ä¸ºå®¢è§‚åˆ†æ•°çš„æ¡†æ¶ã€‚DeEARåŸºäºè¯­éŸ³å­¦å’Œå¿ƒç†å­¦ï¼Œä»æƒ…æ„Ÿã€è¯­è°ƒå’Œè¯­é€Ÿä¸‰ä¸ªç»´åº¦å¯¹è¯­éŸ³è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡ä½¿ç”¨ä¸åˆ°500ä¸ªæ ‡æ³¨æ ·æœ¬ï¼ŒDeEARä¸äººç±»æ„ŸçŸ¥å®ç°äº†å¼ºå¯¹é½ï¼ˆæ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°SRCC&#x3D;0.86ï¼‰ã€‚é™¤äº†å¯é çš„è¯„åˆ†å¤–ï¼ŒDeEARè¿˜èƒ½å®ç°å…¬æ­£çš„åŸºå‡†æµ‹è¯•å’Œæœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ•´ç†ã€‚å®ƒä¸ä»…èƒ½å¤ŸåŒºåˆ†ä¸åŒS2Sæ¨¡å‹ä¹‹é—´çš„è¡¨è¾¾å·®è·ï¼Œè¿˜èƒ½æŒ‘é€‰å‡º14,000æ¡è¡¨è¾¾æ€§è¯è¯­æ¥æ„å»ºè¡¨è¾¾æ€§è¯­éŸ³ï¼ˆExpressiveSpeechï¼‰ï¼Œè¿™æé«˜äº†S2Sæ¨¡å‹çš„è¡¨è¾¾å¾—åˆ†ï¼ˆåœ¨100åˆ†åˆ¶çš„å°ºåº¦ä¸Šä»2.0æé«˜åˆ°23.4ï¼‰ã€‚æ¼”ç¤ºå’Œä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/ExpressiveSpeech%E3%80%82">https://github.com/FreedomIntelligence/ExpressiveSpeechã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20513v1">PDF</a> Submitted to ICASSP 2026. Demos and codes are available at   <a target="_blank" rel="noopener" href="https://github.com/FreedomIntelligence/ExpressiveSpeech">https://github.com/FreedomIntelligence/ExpressiveSpeech</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDeEARçš„æ¡†æ¶ï¼Œç”¨äºå°†äººç±»å¯¹è¯è¯­è¡¨è¾¾åŠ›çš„åå¥½è½¬åŒ–ä¸ºå®¢è§‚è¯„åˆ†ã€‚è¯¥æ¡†æ¶åŸºäºè¯­éŸ³å­¦å’Œå¿ƒç†å­¦ï¼Œä»æƒ…æ„Ÿã€è¯­è°ƒå’Œè‡ªç„¶åº¦ä¸‰ä¸ªæ–¹é¢è¯„ä»·è¯­éŸ³ï¼Œä¸äººç±»æ„ŸçŸ¥é«˜åº¦ä¸€è‡´ï¼ˆæ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°SRCC&#x3D;0.86ï¼‰ï¼Œå¹¶ä¸”ä»…éœ€è¦å°‘é‡çš„æ ‡æ³¨æ ·æœ¬ã€‚é™¤äº†å¯é çš„è¯„åˆ†åŠŸèƒ½å¤–ï¼ŒDeEARè¿˜èƒ½å®ç°å…¬å¹³çš„æ€§èƒ½è¯„ä¼°å’Œæœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ”¶é›†ï¼Œä¸ä»…èƒ½å¤Ÿè¯†åˆ«ä¸åŒè¯­éŸ³è½¬è¯­éŸ³æ¨¡å‹ä¸­çš„è¡¨è¾¾å·®è·ï¼Œè¿˜èƒ½ç­›é€‰å‡ºå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­å¥ä»¥æ„å»ºExpressiveSpeechæ•°æ®é›†ï¼Œè¿›è€Œæå‡è¯­éŸ³è½¬è¯­éŸ³æ¨¡å‹çš„è¡¨è¾¾å¾—åˆ†ã€‚æœ‰å…³æ¼”ç¤ºå’Œä»£ç å¯åœ¨ç›¸å…³é“¾æ¥æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯­éŸ³è½¬è¯­éŸ³ï¼ˆS2Sï¼‰æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆå¯ç†è§£çš„è¯­éŸ³ï¼Œä½†ç¼ºä¹è‡ªç„¶è¡¨ç°åŠ›ï¼Œä¸»è¦å› ä¸ºç¼ºä¹å¯é çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>æå‡ºçš„DeEARæ¡†æ¶èƒ½å¤Ÿå°†äººç±»å¯¹è¯è¯­è¡¨è¾¾åŠ›çš„åå¥½è½¬åŒ–ä¸ºå®¢è§‚è¯„åˆ†ï¼ŒåŸºäºè¯­éŸ³å­¦å’Œå¿ƒç†å­¦è¿›è¡Œè¯„ä»·ã€‚</li>
<li>DeEARä»æƒ…æ„Ÿã€è¯­è°ƒå’Œè‡ªç„¶åº¦ä¸‰ä¸ªæ–¹é¢è¯„ä»·è¯­éŸ³ï¼Œä¸äººç±»æ„ŸçŸ¥é«˜åº¦ä¸€è‡´ï¼ˆæ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°SRCC&#x3D;0.86ï¼‰ã€‚</li>
<li>DeEARæ¡†æ¶ä»…éœ€å°‘é‡çš„æ ‡æ³¨æ ·æœ¬å°±èƒ½å®ç°å¯é çš„è¯„åˆ†ï¼Œæœ‰åŠ©äºå…¬å¹³çš„æ€§èƒ½è¯„ä¼°å’Œé’ˆå¯¹æ€§çš„æ•°æ®æ”¶é›†ã€‚</li>
<li>DeEARèƒ½è¯†åˆ«ä¸åŒS2Sæ¨¡å‹ä¸­çš„è¡¨è¾¾å·®è·ï¼Œå¹¶èƒ½å¤Ÿç­›é€‰å‡ºå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­å¥ä»¥æ„å»ºExpressiveSpeechæ•°æ®é›†ã€‚</li>
<li>ExpressiveSpeechæ•°æ®é›†èƒ½å¤Ÿæ˜¾è‘—æé«˜è¯­éŸ³è½¬è¯­éŸ³æ¨¡å‹çš„è¡¨è¾¾å¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-816ae09ca46036b088a985c2af07a23d" align="middle">
<img src="https://picx.zhimg.com/v2-3a9839227f20bc1f74e611ea92dd77e3" align="middle">
<img src="https://picx.zhimg.com/v2-b6c8d94f8347e7a7b94f19380139b7d1" align="middle">
<img src="https://picx.zhimg.com/v2-71b36e1e0c9ef2eeb732e9aa8932ffb5" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Speaking-Clearly-A-Simplified-Whisper-Based-Codec-for-Low-Bitrate-Speech-Coding"><a href="#Speaking-Clearly-A-Simplified-Whisper-Based-Codec-for-Low-Bitrate-Speech-Coding" class="headerlink" title="Speaking Clearly: A Simplified Whisper-Based Codec for Low-Bitrate   Speech Coding"></a>Speaking Clearly: A Simplified Whisper-Based Codec for Low-Bitrate   Speech Coding</h2><p><strong>Authors:Xin Zhang, Lin Li, Xiangni Lu, Jianquan Liu, Kong Aik Lee</strong></p>
<p>Speech codecs serve as bridges between continuous speech signals and large language models, yet face an inherent conflict between acoustic fidelity and semantic preservation. To mitigate this conflict, prevailing methods augment acoustic codecs with complex semantic supervision. We explore the opposite direction: a semantic-first approach that starts from a semantically-capable model and adapts it for high-fidelity acoustic reconstruction. Through empirical analysis, we discover that targeted architectural simplification can unlock the acoustic modeling potential of Whisper, a text-aligned Automatic Speech Recognition (ASR) model. Based on this finding, we propose SimWhisper-Codec, a novel codec that balances the semantic and acoustic preservation by leveraging a frozen, simplified Whisper encoder without requiring external supervision. Experimental results demonstrate that SimWhisper-Codec achieves superior performance in both semantic preservation and acoustic quality compared to semantically-supervised codecs such as Mimi Codec and SpeechTokenizer at similar bitrates, validating the effectiveness of our semantic-first approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ZhangXinWhut/SimWhisper-Codec">https://github.com/ZhangXinWhut/SimWhisper-Codec</a>. </p>
<blockquote>
<p>è¯­éŸ³ç¼–è§£ç å™¨åœ¨è¿ç»­çš„è¯­éŸ³ä¿¡å·å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´å……å½“æ¡¥æ¢ï¼Œä½†é¢ä¸´ç€å£°éŸ³ä¿çœŸåº¦å’Œè¯­ä¹‰ä¿ç•™ä¹‹é—´çš„å†…åœ¨å†²çªã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€å†²çªï¼Œæµè¡Œçš„æ–¹æ³•æ˜¯é€šè¿‡å¤æ‚çš„è¯­ä¹‰ç›‘ç£æ¥å¢å¼ºå£°éŸ³ç¼–è§£ç å™¨ã€‚æˆ‘ä»¬æ¢ç´¢äº†ç›¸åçš„æ–¹å‘ï¼šä¸€ç§ä»¥è¯­ä¹‰ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œä»ä¸€ä¸ªå…·å¤‡è¯­ä¹‰èƒ½åŠ›çš„æ¨¡å‹å¼€å§‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œé€‚åº”ä»¥å®ç°é«˜ä¿çœŸå£°éŸ³é‡å»ºã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°æœ‰é’ˆå¯¹æ€§çš„æ¶æ„ç®€åŒ–å¯ä»¥è§£é”whisperï¼ˆä¸€ç§æ–‡æœ¬å¯¹é½çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼‰çš„å£°éŸ³å»ºæ¨¡æ½œåŠ›ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†SimWhisper-Codecï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¼–è§£ç å™¨ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä¸€ä¸ªå†»ç»“çš„ç®€åŒ–ç‰ˆwhisperç¼–ç å™¨ï¼Œåœ¨ä¸éœ€è¦å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹å¹³è¡¡è¯­ä¹‰å’Œå£°éŸ³çš„ä¿ç•™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸Mimi Codecå’ŒSpeechTokenizerç­‰è¯­ä¹‰ç›‘ç£ç¼–è§£ç å™¨ç›¸æ¯”ï¼ŒSimWhisper-Codecåœ¨ç›¸ä¼¼æ¯”ç‰¹ç‡ä¸‹åœ¨è¯­ä¹‰ä¿ç•™å’Œå£°éŸ³è´¨é‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒéªŒè¯äº†æˆ‘ä»¬ä»¥è¯­ä¹‰ä¸ºä¸­å¿ƒæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhangXinWhut/SimWhisper-Codec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZhangXinWhut/SimWhisper-Codecæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20504v1">PDF</a> 5 pages, 3 figures, 2 tables</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¯­éŸ³ç¼–è§£ç å™¨åœ¨è¿ç»­è¯­éŸ³ä¿¡å·å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´æ‰®æ¼”æ¡¥æ¢è§’è‰²ï¼Œä½†é¢ä¸´å£°éŸ³ä¿çœŸä¸è¯­ä¹‰ä¿ç•™ä¹‹é—´çš„å†…åœ¨å†²çªã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å¤æ‚çš„è¯­ä¹‰ç›‘ç£æ¥å¢å¼ºå£°éŸ³ç¼–è§£ç å™¨ï¼Œæœ¬æ–‡åˆ™æ¢ç´¢ç›¸åæ–¹å‘ï¼šä»è¯­ä¹‰èƒ½åŠ›æ¨¡å‹å‡ºå‘ï¼Œé€šè¿‡é’ˆå¯¹æ€§æ¶æ„ç®€åŒ–ï¼Œä»¥é€‚åº”é«˜ä¿çœŸå£°éŸ³é‡å»ºã€‚åŸºäºå¯¹Whisperæ¨¡å‹ï¼ˆä¸€ç§æ–‡æœ¬å¯¹é½çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼‰çš„ç ”ç©¶ï¼Œæå‡ºSimWhisper-Codecæ–°å‹ç¼–è§£ç å™¨ï¼Œé€šè¿‡å†»ç»“ç®€åŒ–çš„Whisperç¼–ç å™¨ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ï¼Œå®ç°è¯­ä¹‰å’Œå£°éŸ³ä¿çœŸçš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSimWhisper-Codecåœ¨ç›¸ä¼¼æ¯”ç‰¹ç‡ä¸‹ï¼Œç›¸è¾ƒäºéœ€è¦è¯­ä¹‰ç›‘ç£çš„ç¼–è§£ç å™¨å¦‚Mimi Codecå’ŒSpeechTokenizerï¼Œåœ¨è¯­ä¹‰ä¿ç•™å’Œå£°éŸ³è´¨é‡æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚ä»£ç å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/ZhangXinWhut/SimWhisper-Codec%E3%80%82">https://github.com/ZhangXinWhut/SimWhisper-Codecã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³ç¼–è§£ç å™¨é¢ä¸´å£°éŸ³ä¿çœŸä¸è¯­ä¹‰ä¿ç•™çš„å†…åœ¨å†²çªã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å¤æ‚çš„è¯­ä¹‰ç›‘ç£å¢å¼ºå£°éŸ³ç¼–è§£ç å™¨æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡é‡‡å–è¯­ä¹‰ä¼˜å…ˆç­–ç•¥ï¼Œä»è¯­ä¹‰èƒ½åŠ›æ¨¡å‹å‡ºå‘ï¼Œé€šè¿‡é’ˆå¯¹æ€§æ¶æ„ç®€åŒ–æ¥é€‚åº”é«˜ä¿çœŸå£°éŸ³é‡å»ºã€‚</li>
<li>åŸºäºWhisperæ¨¡å‹çš„ç ”ç©¶ï¼Œæå‡ºSimWhisper-Codecç¼–è§£ç å™¨ï¼Œå®ç°è¯­ä¹‰å’Œå£°éŸ³ä¿çœŸçš„å¹³è¡¡ã€‚</li>
<li>SimWhisper-Codecåœ¨ç›¸ä¼¼æ¯”ç‰¹ç‡ä¸‹è¡¨ç°ä¼˜äºå…¶ä»–éœ€è¦è¯­ä¹‰ç›‘ç£çš„ç¼–è§£ç å™¨ã€‚</li>
<li>SimWhisper-Codecå…¬å¼€å¯ç”¨ï¼Œä¸ºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨æä¾›ä¾¿åˆ©ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè¯­éŸ³ç¼–è§£ç å™¨è®¾è®¡æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd6f223287d882c60844d12abf2ad396" align="middle">
<img src="https://picx.zhimg.com/v2-b2b593d67c9dc32365b3cf98eced308d" align="middle">
<img src="https://picx.zhimg.com/v2-ce1b3eae29aa6a9b555b2797acbfc630" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UniSE-A-Unified-Framework-for-Decoder-only-Autoregressive-LM-based-Speech-Enhancement"><a href="#UniSE-A-Unified-Framework-for-Decoder-only-Autoregressive-LM-based-Speech-Enhancement" class="headerlink" title="UniSE: A Unified Framework for Decoder-only Autoregressive LM-based   Speech Enhancement"></a>UniSE: A Unified Framework for Decoder-only Autoregressive LM-based   Speech Enhancement</h2><p><strong>Authors:Haoyin Yan, Chengwei Liu, Shaofei Xue, Xiaotao Liang, Zheng Xue</strong></p>
<p>The development of neural audio codecs (NACs) has largely promoted applications of language models (LMs) to speech processing and understanding. However, there lacks the verification on the effectiveness of autoregressive (AR) LMbased models in unifying different sub-tasks of speech enhancement (SE). In this work, we propose UniSE, a unified decoder-only LM-based framework to handle different SE tasks including speech restoration, target speaker extraction and speech separation. It takes input speech features as conditions and generates discrete tokens of the target speech using AR modeling, which facilitates a compatibility between distinct learning patterns of multiple tasks. Experiments on several benchmarks indicate the proposed UniSE can achieve competitive performance compared to discriminative and generative baselines, showing the capacity of LMs in unifying SE tasks. The demo page is available here: <a target="_blank" rel="noopener" href="https://github.com/hyyan2k/UniSE">https://github.com/hyyan2k/UniSE</a>. </p>
<blockquote>
<p>ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨ï¼ˆNACï¼‰çš„å‘å±•æå¤§åœ°ä¿ƒè¿›äº†è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨è¯­éŸ³å¤„ç†å’Œç†è§£ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç¼ºä¹åŸºäºè‡ªå›å½’ï¼ˆARï¼‰çš„LMæ¨¡å‹åœ¨ç»Ÿä¸€è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ä¸åŒå­ä»»åŠ¡çš„æœ‰æ•ˆæ€§éªŒè¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UniSEï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ã€ä»…è§£ç çš„ã€åŸºäºLMçš„æ¡†æ¶ï¼Œç”¨äºå¤„ç†ä¸åŒçš„SEä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯­éŸ³ä¿®å¤ã€ç›®æ ‡è¯´è¯äººæå–å’Œè¯­éŸ³åˆ†ç¦»ã€‚å®ƒä»¥è¯­éŸ³ç‰¹å¾ä¸ºè¾“å…¥æ¡ä»¶ï¼Œåˆ©ç”¨ARæ¨¡å‹ç”Ÿæˆç›®æ ‡è¯­éŸ³çš„ç¦»æ•£æ ‡è®°ï¼Œè¿™ä¿ƒè¿›äº†å¤šä¸ªä»»åŠ¡ä¸åŒå­¦ä¹ æ¨¡å¼ä¹‹é—´çš„å…¼å®¹æ€§ã€‚åœ¨å‡ ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸åˆ¤åˆ«å¼å’Œç”Ÿæˆå¼åŸºçº¿ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„UniSEå¯ä»¥å–å¾—å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œå±•ç¤ºäº†LMåœ¨ç»Ÿä¸€SEä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚ç¤ºèŒƒé¡µé¢å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/hyyan2k/UniSE">https://github.com/hyyan2k/UniSE</a> æŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20441v1">PDF</a> 5 pages, submitted to ICASSP 2026</p>
<p><strong>Summary</strong><br>ç¥ç»éŸ³é¢‘ç¼–ç å™¨çš„å¼€å‘ä¿ƒè¿›äº†è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³å¤„ç†å’Œç†è§£ä¸­çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€è§£ç å™¨ä»…åŸºäºè¯­è¨€æ¨¡å‹çš„æ¡†æ¶UniSEï¼Œç”¨äºå¤„ç†ä¸åŒçš„è¯­éŸ³å¢å¼ºä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯­éŸ³ä¿®å¤ã€ç›®æ ‡è¯´è¯äººæå–å’Œè¯­éŸ³åˆ†ç¦»ã€‚å®ƒé‡‡ç”¨è¾“å…¥è¯­éŸ³ç‰¹å¾ä½œä¸ºæ¡ä»¶ï¼Œåˆ©ç”¨è‡ªå›å½’å»ºæ¨¡ç”Ÿæˆç›®æ ‡è¯­éŸ³çš„ç¦»æ•£æ ‡è®°ï¼Œå®ç°äº†å¤šä»»åŠ¡å­¦ä¹ çš„å…¼å®¹æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒUniSEå…·æœ‰ä¸åˆ¤åˆ«å’Œç”ŸæˆåŸºçº¿æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ï¼Œå±•ç¤ºäº†è¯­è¨€æ¨¡å‹åœ¨ç»Ÿä¸€è¯­éŸ³å¢å¼ºä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»éŸ³é¢‘ç¼–ç å™¨(NAC)çš„å‘å±•ä¿ƒè¿›äº†è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³å¤„ç†å’Œç†è§£ä¸­çš„åº”ç”¨ã€‚</li>
<li>è‡ªå›å½’(AR)è¯­è¨€æ¨¡å‹åœ¨ç»Ÿä¸€ä¸åŒçš„è¯­éŸ³å¢å¼ºä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>UniSEæ˜¯ä¸€ä¸ªç»Ÿä¸€è§£ç å™¨ä»…åŸºäºè¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œèƒ½å¤„ç†å¤šç§è¯­éŸ³å¢å¼ºä»»åŠ¡ã€‚</li>
<li>UniSEåˆ©ç”¨è¾“å…¥è¯­éŸ³ç‰¹å¾ä½œä¸ºæ¡ä»¶ï¼Œé€šè¿‡è‡ªå›å½’å»ºæ¨¡ç”Ÿæˆç›®æ ‡è¯­éŸ³çš„ç¦»æ•£æ ‡è®°ã€‚</li>
<li>UniSEå®ç°äº†å¤šä»»åŠ¡å­¦ä¹ çš„å…¼å®¹æ€§ï¼Œæœ‰åŠ©äºä¸åŒä»»åŠ¡ä¹‹é—´å­¦ä¹ æ¨¡å¼çš„èåˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUniSEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¸åˆ¤åˆ«å’Œç”Ÿæˆæ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef95528d0b18b053ffeaed7953ab909d" align="middle">
<img src="https://picx.zhimg.com/v2-0d2e805cc0bde2bc46da34fce4225477" align="middle">
<img src="https://picx.zhimg.com/v2-c8d7754efa1c298083063aafa044e715" align="middle">
<img src="https://picx.zhimg.com/v2-591af8aca6f25da95d4ea887020e5fd6" align="middle">
<img src="https://picx.zhimg.com/v2-49c2f6284f3e7fbbde32c22b88785204" align="middle">
<img src="https://picx.zhimg.com/v2-a3babc1273e0210b07ea63d9a2090067" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OmniMotion-X-Versatile-Multimodal-Whole-Body-Motion-Generation"><a href="#OmniMotion-X-Versatile-Multimodal-Whole-Body-Motion-Generation" class="headerlink" title="OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation"></a>OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</h2><p><strong>Authors:Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu</strong></p>
<p>This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint&#x2F;trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†OmniMotion-Xï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨å¤šæ¨¡å¼æ¡†æ¶ï¼Œä»¥ç»Ÿä¸€åºåˆ—åˆ°åºåˆ—çš„æ–¹å¼ï¼Œé‡‡ç”¨è‡ªå›å½’æ‰©æ•£å˜å‹å™¨ï¼Œç”¨äºç”Ÿæˆå…¨èº«äººä½“è¿åŠ¨ã€‚OmniMotion-Xæœ‰æ•ˆåœ°æ”¯æŒäº†å¤šæ ·åŒ–çš„å¤šæ¨¡å¼ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¿åŠ¨ã€éŸ³ä¹åˆ°èˆè¹ˆã€è¯­éŸ³åˆ°æ‰‹åŠ¿ã€å…¨å±€æ—¶ç©ºæ§åˆ¶åœºæ™¯ï¼ˆå¦‚è¿åŠ¨é¢„æµ‹ã€ä¸­é—´å¸§ç”Ÿæˆã€å®Œæˆå’Œå…³èŠ‚&#x2F;è½¨è¿¹å¼•å¯¼åˆæˆç­‰ï¼‰ï¼Œä»¥åŠè¿™äº›ä»»åŠ¡çš„çµæ´»ç»„åˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å‚è€ƒè¿åŠ¨ä½œä¸ºæ–°å‹æ¡ä»¶ä¿¡å·ï¼Œè¿™æå¤§åœ°æé«˜äº†ç”Ÿæˆå†…å®¹çš„ä¸€è‡´æ€§ã€é£æ ¼å’Œæ—¶ç©ºåŠ¨æ€ï¼Œå¯¹äºç°å®åŠ¨ç”»è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³å¤šæ¨¡å¼å†²çªï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä»å¼±åˆ°å¼ºçš„æ¸è¿›æ··åˆæ¡ä»¶è®­ç»ƒç­–ç•¥ã€‚ä¸ºäº†è¿›è¡Œé«˜è´¨é‡çš„å¤šæ¨¡å¼è®­ç»ƒï¼Œæˆ‘ä»¬æ„å»ºäº†OmniMoCap-Xï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç»Ÿä¸€å¤šæ¨¡å¼è¿åŠ¨æ•°æ®é›†ï¼Œæ•´åˆäº†10ä¸ªä¸åŒä»»åŠ¡çš„28ä¸ªå…¬å¼€å¯ç”¨çš„MoCapæºï¼Œä»¥æ ‡å‡†åŒ–çš„SMPL-Xæ ¼å¼å’Œ30fpsè¿›è¡Œã€‚ä¸ºäº†ç¡®ä¿è¯¦ç»†å’Œä¸€è‡´çš„æ³¨é‡Šï¼Œæˆ‘ä»¬å°†åºåˆ—å‘ˆç°ä¸ºè§†é¢‘ï¼Œå¹¶ä½¿ç”¨GPT-4oè‡ªåŠ¨ç”Ÿæˆç»“æ„å’Œå±‚æ¬¡åŒ–çš„å­—å¹•ï¼Œæ•æ‰ä½çº§åˆ«çš„åŠ¨ä½œå’Œé«˜çº§åˆ«çš„è¯­ä¹‰ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¯å®ï¼ŒOmniMotion-Xæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨å¤šä¸ªå¤šæ¨¡å¼ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿå®ç°ç°å®ã€è¿è´¯å’Œå¯æ§çš„é•¿æœŸè¿åŠ¨ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19789v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OmniMotion-Xï¼Œä¸€ä¸ªå¤šåŠŸèƒ½æ¨¡æ€æ¡†æ¶ï¼Œèƒ½ä»¥ç»Ÿä¸€åºåˆ—åˆ°åºåˆ—çš„æ–¹å¼ç”Ÿæˆå…¨èº«äººä½“è¿åŠ¨ã€‚OmniMotion-Xé€šè¿‡åˆ©ç”¨è‡ªå›å½’æ‰©æ•£å˜å‹å™¨æœ‰æ•ˆåœ°æ”¯æŒå¤šæ ·åŒ–çš„å¤šä»»åŠ¡æ¨¡æ€ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¿åŠ¨ã€éŸ³ä¹åˆ°èˆè¹ˆã€è¯­éŸ³åˆ°æ‰‹åŠ¿ä»¥åŠå…¨çƒæ—¶ç©ºæ§åˆ¶åœºæ™¯ç­‰ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å‚è€ƒè¿åŠ¨ä½œä¸ºæ–°çš„æ¡ä»¶ä¿¡å·ï¼Œå¤§å¤§æé«˜äº†ç”Ÿæˆå†…å®¹çš„è¿è´¯æ€§ã€é£æ ¼å’Œé‡è¦çš„æ—¶é—´åŠ¨æ€æ€§ï¼Œä»¥å‘ˆç°é€¼çœŸçš„åŠ¨ç”»æ•ˆæœã€‚å¤„ç†å¤šæ¨¡æ€å†²çªæ–¹é¢ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»å¼±åˆ°å¼ºçš„æ¸è¿›å¼æ··åˆè®­ç»ƒç­–ç•¥ã€‚ä¸ºäº†è¿›è¡Œé«˜è´¨é‡çš„å¤šæ¨¡æ€è®­ç»ƒï¼Œæˆ‘ä»¬æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç»Ÿä¸€å¤šæ¨¡æ€è¿åŠ¨æ•°æ®é›†OmniMoCap-Xï¼Œæ•´åˆäº†æ¶µç›–åå¤§ä¸åŒä»»åŠ¡çš„28ä¸ªå…¬å¼€MoCapæºæ•°æ®ï¼Œå¹¶ç»Ÿä¸€é‡‡ç”¨SMPL-Xæ ¼å¼å’Œæ¯ç§’30å¸§çš„é€Ÿåº¦ã€‚ä¸ºç¡®ä¿è¯¦ç»†ä¸”ä¸€è‡´çš„æ³¨é‡Šï¼Œæˆ‘ä»¬å°†åºåˆ—æ¸²æŸ“æˆè§†é¢‘å¹¶ä½¿ç”¨GPT-4oè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–å’Œå±‚æ¬¡åŒ–çš„å­—å¹•ï¼Œä»¥æ•æ‰ä½å±‚æ¬¡åŠ¨ä½œå’Œé«˜å±‚æ¬¡è¯­ä¹‰ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒOmniMotion-Xåœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿå®ç°é€¼çœŸçš„ã€è¿è´¯çš„å’Œå¯æ§åˆ¶çš„é•¿å‘¨æœŸè¿åŠ¨ç”Ÿæˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>OmniMotion-Xæ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½æ¨¡æ€æ¡†æ¶ï¼Œæ”¯æŒå¤šç§è¿åŠ¨ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¿åŠ¨ã€éŸ³ä¹åˆ°èˆè¹ˆç­‰ã€‚</li>
<li>å¼•å…¥å‚è€ƒè¿åŠ¨ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œæé«˜ç”Ÿæˆå†…å®¹çš„è¿è´¯æ€§ã€é£æ ¼å’ŒåŠ¨æ€æ€§ã€‚</li>
<li>é‡‡ç”¨æ¸è¿›å¼å¼±åˆ°å¼ºçš„æ··åˆè®­ç»ƒç­–ç•¥æ¥å¤„ç†å¤šæ¨¡æ€å†²çªã€‚</li>
<li>æ„å»ºäº†OmniMoCap-Xæ•°æ®é›†ï¼Œé›†æˆäº†å¤šä¸ªMoCapæºæ•°æ®ï¼Œå¹¶ç»Ÿä¸€æ ¼å¼ä»¥æé«˜è®­ç»ƒè´¨é‡ã€‚</li>
<li>ä½¿ç”¨GPT-4oè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–å’Œå±‚æ¬¡åŒ–çš„å­—å¹•ï¼Œç¡®ä¿è¯¦ç»†ä¸”ä¸€è‡´çš„æ³¨é‡Šã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºOmniMotion-Xåœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>OmniMotion-Xèƒ½å¤Ÿå®ç°é€¼çœŸçš„ã€è¿è´¯çš„å’Œå¯æ§åˆ¶çš„é•¿å‘¨æœŸè¿åŠ¨ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8620922f467f05bcb80397391e2305da" align="middle">
<img src="https://picx.zhimg.com/v2-57532a697cc4f46df80ef75a411a2df3" align="middle">
<img src="https://picx.zhimg.com/v2-8b0d91ef5ff10cd1d2d878dad9bd4218" align="middle">
<img src="https://picx.zhimg.com/v2-fb0a823b10c5bd3402fd3b3c4484f13b" align="middle">
<img src="https://picx.zhimg.com/v2-51703277325e66fd250c8c10191b6214" align="middle">
<img src="https://picx.zhimg.com/v2-53a2c15f6aa1999027940424e088e41e" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Re-evaluating-Minimum-Bayes-Risk-Decoding-for-Automatic-Speech-Recognition"><a href="#Re-evaluating-Minimum-Bayes-Risk-Decoding-for-Automatic-Speech-Recognition" class="headerlink" title="Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech   Recognition"></a>Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech   Recognition</h2><p><strong>Authors:Yuu Jinnai</strong></p>
<p>Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding outperforms beam search in text-to-text generation tasks, such as machine translation, text summarization, and image captioning. On the other hand, beam search is the current practice for speech-to-text tasks such as automatic speech recognition (ASR) and Speech Translation (ST). Given that MBR decoding is effective in text-to-text generation tasks, it is reasonable to expect it to also be effective for speech-to-text tasks. In this paper, we evaluate MBR decoding for ASR and ST tasks on English and Japanese using Whisper and its derivative models. We observe that the accuracy of MBR decoding outperforms that of beam search in most of the experimental settings we have evaluated. The results show that MBR decoding is a promising method for offline ASR and ST tasks that require high accuracy. The code is available at <a target="_blank" rel="noopener" href="https://github.com/CyberAgentAILab/mbr-for-asr">https://github.com/CyberAgentAILab/mbr-for-asr</a> </p>
<blockquote>
<p>æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºæ ·æœ¬çš„æœ€å°è´å¶æ–¯é£é™©ï¼ˆMBRï¼‰è§£ç åœ¨æ–‡æœ¬åˆ°æ–‡æœ¬çš„ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦å’Œå›¾åƒæè¿°ï¼‰ä¸­è¡¨ç°ä¼˜äºæŸæœç´¢ã€‚å¦ä¸€æ–¹é¢ï¼ŒæŸæœç´¢æ˜¯ç›®å‰è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡ï¼ˆå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰ï¼‰çš„å¸¸ç”¨æ–¹æ³•ã€‚é‰´äºMBRè§£ç åœ¨æ–‡æœ¬åˆ°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé¢„æœŸå…¶åœ¨è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡ä¸­åŒæ ·æœ‰æ•ˆæ˜¯åˆç†çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä½¿ç”¨WhisperåŠå…¶è¡ç”Ÿæ¨¡å‹å¯¹ASRå’ŒSTä»»åŠ¡çš„MBRè§£ç çš„è‹±è¯­å’Œæ—¥è¯­è¡¨ç°ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨æ‰€è¯„ä¼°çš„å¤§éƒ¨åˆ†å®éªŒè®¾ç½®ä¸­ï¼ŒMBRè§£ç çš„å‡†ç¡®æ€§éƒ½ä¼˜äºæŸæœç´¢ã€‚ç»“æœè¡¨æ˜ï¼ŒMBRè§£ç å¯¹äºéœ€è¦é«˜å‡†ç¡®ç‡çš„ç¦»çº¿ASRå’ŒSTä»»åŠ¡æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/CyberAgentAILab/mbr-for-asr%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/CyberAgentAILab/mbr-for-asræ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19471v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ ·æœ¬åŸºç¡€ä¸Šçš„æœ€å°è´å¶æ–¯é£é™©è§£ç åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒæŒ‡å‡ºå…¶åœ¨æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦å’Œå›¾åƒæ ‡æ³¨ç­‰ä»»åŠ¡ä¸­ä¼˜äºå…‰æŸæœç´¢ç®—æ³•ã€‚å°½ç®¡å…‰æŸæœç´¢åœ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ç­‰è¯­éŸ³åˆ°æ–‡æœ¬çš„ä»»åŠ¡ä¸­æ˜¯å¸¸ç”¨æ–¹æ³•ï¼Œä½†è€ƒè™‘åˆ°æœ€å°è´å¶æ–¯é£é™©è§£ç åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡å¯¹å…¶åœ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­çš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šæ•°æµ‹è¯•ç¯å¢ƒä¸‹ï¼Œæœ€å°è´å¶æ–¯é£é™©è§£ç çš„å‡†ç¡®æ€§é«˜äºå…‰æŸæœç´¢ã€‚è¿™è¡¨æ˜æœ€å°è´å¶æ–¯é£é™©è§£ç å¯¹äºç¦»çº¿è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ç­‰è¦æ±‚é«˜å‡†ç¡®æ€§çš„ä»»åŠ¡å…·æœ‰åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€å°è´å¶æ–¯é£é™©è§£ç åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå…‰æŸæœç´¢ã€‚</li>
<li>è¯­éŸ³åˆ°æ–‡æœ¬çš„ä»»åŠ¡ä¸­å¸¸ä½¿ç”¨å…‰æŸæœç´¢ç®—æ³•ã€‚</li>
<li>å®éªŒè¯„ä¼°äº†æœ€å°è´å¶æ–¯é£é™©è§£ç åœ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>åœ¨å¤šæ•°æµ‹è¯•ç¯å¢ƒä¸‹ï¼Œæœ€å°è´å¶æ–¯é£é™©è§£ç çš„å‡†ç¡®æ€§é«˜äºå…‰æŸæœç´¢ã€‚</li>
<li>æœ€å°è´å¶æ–¯é£é™©è§£ç å¯¹ç¦»çº¿è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘çš„é«˜å‡†ç¡®æ€§ä»»åŠ¡å…·æœ‰åº”ç”¨å‰æ™¯ã€‚</li>
<li>ä»£ç å…¬å¼€ï¼Œä¾¿äºç ”ç©¶ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a94913ccdea8706da36e8e4463932141" align="middle">
<img src="https://picx.zhimg.com/v2-bfd0e38d8fe2eff8a9fb0df767cb2ffd" align="middle">
<img src="https://picx.zhimg.com/v2-2f422cde4291d9d07aee4fa01e4210d0" align="middle">
<img src="https://picx.zhimg.com/v2-d91a6f964bda89357bba3dc4beec3052" align="middle">
<img src="https://picx.zhimg.com/v2-0069302f91397efb88f113e0a2bc9e18" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FLASH-Viterbi-Fast-and-Adaptive-Viterbi-Decoding-for-Modern-Data-Systems"><a href="#FLASH-Viterbi-Fast-and-Adaptive-Viterbi-Decoding-for-Modern-Data-Systems" class="headerlink" title="FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data   Systems"></a>FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data   Systems</h2><p><strong>Authors:Ziheng Deng, Xue Liu, Jiantong Jiang, Yankai Li, Qingxu Deng, Xiaochun Yang</strong></p>
<p>The Viterbi algorithm is a key operator for structured sequence inference in modern data systems, with applications in trajectory analysis, online recommendation, and speech recognition. As these workloads increasingly migrate to resource-constrained edge platforms, standard Viterbi decoding remains memory-intensive and computationally inflexible. Existing methods typically trade decoding time for space efficiency, but often incur significant runtime overhead and lack adaptability to various system constraints. This paper presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly Viterbi decoding operator that enhances adaptability and resource efficiency. FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning and parallelization techniques to enhance both time and memory efficiency, making it well-suited for resource-constrained data systems. To further decouple space complexity from the hidden state space size, we present FLASH-BS Viterbi, a dynamic beam search variant built on a memory-efficient data structure. Both proposed algorithms exhibit strong adaptivity to diverse deployment scenarios by dynamically tuning internal parameters. To ensure practical deployment on edge devices, we also develop FPGA-based hardware accelerators for both algorithms, demonstrating high throughput and low resource usage. Extensive experiments show that our algorithms consistently outperform existing baselines in both decoding time and memory efficiency, while preserving adaptability and hardware-friendly characteristics essential for modern data systems. All codes are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Dzh-16/FLASH-Viterbi">https://github.com/Dzh-16/FLASH-Viterbi</a>. </p>
<blockquote>
<p>Viterbiç®—æ³•æ˜¯ç°ä»£æ•°æ®ç³»ç»Ÿä¸­ç»“æ„åŒ–åºåˆ—æ¨æ–­çš„å…³é”®ç®—å­ï¼Œåœ¨è½¨è¿¹åˆ†æã€åœ¨çº¿æ¨èå’Œè¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚éšç€è¿™äº›å·¥ä½œè´Ÿè½½é€æ¸è¿ç§»åˆ°èµ„æºå—é™çš„è¾¹ç¼˜å¹³å°ï¼Œæ ‡å‡†Viterbiè§£ç ä»ç„¶éœ€è¦å¤§é‡å†…å­˜å¹¶ä¸”åœ¨è®¡ç®—ä¸Šä¸å¤Ÿçµæ´»ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»¥é™ä½è§£ç æ—¶é—´ä¸ºä»£ä»·æ¥æé«˜ç©ºé—´æ•ˆç‡ï¼Œä½†å¾€å¾€äº§ç”Ÿæ˜¾è‘—çš„è¿è¡Œæ—¶å¼€é”€ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹å„ç§ç³»ç»Ÿçº¦æŸçš„é€‚åº”æ€§ã€‚æœ¬æ–‡æå‡ºäº†FLASH Viterbiï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿã€è½»é‡çº§ã€è‡ªé€‚åº”å’Œç¡¬ä»¶å‹å¥½çš„Viterbiè§£ç ç®—å­ï¼Œæé«˜äº†é€‚åº”æ€§å’Œèµ„æºæ•ˆç‡ã€‚FLASH Viterbiç»“åˆéé€’å½’çš„åˆ†æ²»ç­–ç•¥ä¸å‰ªæå’Œå¹¶è¡ŒåŒ–æŠ€æœ¯ï¼Œæé«˜äº†æ—¶é—´å’Œå†…å­˜æ•ˆç‡ï¼Œéå¸¸é€‚åˆèµ„æºå—é™çš„æ•°æ®ç³»ç»Ÿã€‚ä¸ºäº†è¿›ä¸€æ­¥å°†ç©ºé—´å¤æ‚åº¦ä¸éšè—çŠ¶æ€ç©ºé—´å¤§å°è§£è€¦ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå†…å­˜é«˜æ•ˆæ•°æ®ç»“æ„çš„åŠ¨æ€æŸæœç´¢å˜ä½“FLASH-BS Viterbiã€‚è¿™ä¸¤ç§ç®—æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´å†…éƒ¨å‚æ•°ï¼Œåœ¨å¤šç§éƒ¨ç½²åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§ã€‚ä¸ºç¡®ä¿åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®é™…éƒ¨ç½²ï¼Œæˆ‘ä»¬è¿˜ä¸ºè¿™ä¸¤ç§ç®—æ³•å¼€å‘äº†åŸºäºFPGAçš„ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œæ˜¾ç¤ºå‡ºé«˜ååé‡å’Œä½èµ„æºä½¿ç”¨ç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨è§£ç æ—¶é—´å’Œå†…å­˜æ•ˆç‡æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒäº†é€‚åº”æ€§å’Œå¯¹ç°ä»£æ•°æ®ç³»ç»Ÿè‡³å…³é‡è¦çš„ç¡¬ä»¶å‹å¥½ç‰¹æ€§ã€‚æ‰€æœ‰ä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Dzh-16/FLASH-Viterbi%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Dzh-16/FLASH-Viterbiä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19301v2">PDF</a> Accepted for ICDE 2026</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†FLASH Viterbiç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿã€è½»é‡çº§ã€è‡ªé€‚åº”ä¸”ç¡¬ä»¶å‹å¥½çš„Viterbiè§£ç æ“ä½œç¬¦ã€‚å®ƒé€šè¿‡ç»“åˆéé€’å½’çš„åˆ†æ²»ç­–ç•¥ã€ä¿®å‰ªå’Œå¹¶è¡ŒåŒ–æŠ€æœ¯ï¼Œæé«˜äº†æ—¶é—´å’Œå†…å­˜æ•ˆç‡ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„æ•°æ®ç³»ç»Ÿã€‚å¦å¤–ï¼Œè¿˜æ¨å‡ºäº†FLASH-BS Viterbiï¼Œä¸€ä¸ªåŸºäºå†…å­˜é«˜æ•ˆæ•°æ®ç»“æ„çš„åŠ¨æ€å…‰æŸæœç´¢å˜ä½“ã€‚ä¸¤ç§ç®—æ³•éƒ½é€šè¿‡åŠ¨æ€è°ƒæ•´å†…éƒ¨å‚æ•°ï¼Œé€‚åº”ä¸åŒçš„éƒ¨ç½²åœºæ™¯ã€‚åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œè¿˜å¼€å‘äº†åŸºäºFPGAçš„ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œå®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼Œè¿™äº›ç®—æ³•åœ¨è§£ç æ—¶é—´å’Œå†…å­˜æ•ˆç‡æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ï¼ŒåŒæ—¶ä¿æŒäº†ç°ä»£æ•°æ®ç³»ç»Ÿæ‰€éœ€çš„è‡ªé€‚åº”æ€§å’Œç¡¬ä»¶å‹å¥½æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Viterbiç®—æ³•æ˜¯ç°ä»£æ•°æ®ç³»ç»Ÿä¸­ç»“æ„åºåˆ—æ¨æ–­çš„å…³é”®ç®—å­ï¼Œå¹¿æ³›åº”ç”¨äºè½¨è¿¹åˆ†æã€åœ¨çº¿æ¨èå’Œè¯­éŸ³è¯†åˆ«ã€‚</li>
<li>éšç€å·¥ä½œè´Ÿè½½è¿ç§»åˆ°èµ„æºå—é™çš„è¾¹ç¼˜å¹³å°ï¼Œæ ‡å‡†çš„Viterbiè§£ç åœ¨å†…å­˜ä½¿ç”¨å’Œè®¡ç®—çµæ´»æ€§æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€åœ¨è§£ç æ—¶é—´å’Œç©ºé—´æ•ˆç‡ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œä½†è¿è¡Œæ—¶å¼€é”€å¤§ä¸”ç¼ºä¹ç³»ç»Ÿé€‚åº”æ€§ã€‚</li>
<li>FLASH Viterbiç®—æ³•é€šè¿‡ç»“åˆåˆ†æ²»ç­–ç•¥ã€ä¿®å‰ªå’Œå¹¶è¡ŒåŒ–æŠ€æœ¯ï¼Œæé«˜äº†æ—¶é—´å’Œå†…å­˜æ•ˆç‡ã€‚</li>
<li>FLASH-BS Viterbiæ˜¯ä¸€ç§åŠ¨æ€å…‰æŸæœç´¢å˜ä½“ï¼ŒåŸºäºå†…å­˜é«˜æ•ˆæ•°æ®ç»“æ„è®¾è®¡ï¼Œè¿›ä¸€æ­¥è§£è€¦äº†ç©ºé—´å¤æ‚åº¦ä¸éšè—çŠ¶æ€ç©ºé—´å¤§å°ã€‚</li>
<li>ä¸¤ç§ç®—æ³•éƒ½èƒ½é€šè¿‡åŠ¨æ€è°ƒæ•´å†…éƒ¨å‚æ•°ï¼Œé€‚åº”ä¸åŒçš„éƒ¨ç½²åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d45b4d0f5b33d863c41c68e63a8e0800" align="middle">
<img src="https://picx.zhimg.com/v2-e4beaa1e2e0d592088df5b4a248908d1" align="middle">
<img src="https://picx.zhimg.com/v2-02f9e8dccdbc2b5e397c18ed0fa1ef33" align="middle">
<img src="https://picx.zhimg.com/v2-88ad1ec450ba28d105aa2fb539c68405" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diffusion-Buffer-for-Online-Generative-Speech-Enhancement"><a href="#Diffusion-Buffer-for-Online-Generative-Speech-Enhancement" class="headerlink" title="Diffusion Buffer for Online Generative Speech Enhancement"></a>Diffusion Buffer for Online Generative Speech Enhancement</h2><p><strong>Authors:Bunlong Lay, Rostislav Makarov, Simon Welker, Maris Hillemann, Timo Gerkmann</strong></p>
<p>Online Speech Enhancement was mainly reserved for predictive models. A key advantage of these models is that for an incoming signal frame from a stream of data, the model is called only once for enhancement. In contrast, generative Speech Enhancement models often require multiple calls, resulting in a computational complexity that is too high for many online speech enhancement applications. This work presents the Diffusion Buffer, a generative diffusion-based Speech Enhancement model which only requires one neural network call per incoming signal frame from a stream of data and performs enhancement in an online fashion on a consumer-grade GPU. The key idea of the Diffusion Buffer is to align physical time with Diffusion time-steps. The approach progressively denoises frames through physical time, where past frames have more noise removed. Consequently, an enhanced frame is output to the listener with a delay defined by the Diffusion Buffer, and the output frame has a corresponding look-ahead. In this work, we extend upon our previous work by carefully designing a 2D convolutional UNet architecture that specifically aligns with the Diffusion Bufferâ€™s look-ahead. We observe that the proposed UNet improves performance, particularly when the algorithmic latency is low. Moreover, we show that using a Data Prediction loss instead of Denoising Score Matching loss enables flexible control over the trade-off between algorithmic latency and quality during inference. The extended Diffusion Buffer equipped with a novel NN and loss function drastically reduces the algorithmic latency from 320 - 960 ms to 32 - 176 ms with an even increased performance. While it has been shown before that offline generative diffusion models outperform predictive approaches in unseen noisy speech data, we confirm that the online Diffusion Buffer also outperforms its predictive counterpart on unseen noisy speech data. </p>
<blockquote>
<p>åœ¨çº¿è¯­éŸ³å¢å¼ºæœ€åˆä¸»è¦ç”¨äºé¢„æµ‹æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹çš„ä¸€ä¸ªä¸»è¦ä¼˜åŠ¿åœ¨äºï¼Œå¯¹äºæ¥è‡ªæ•°æ®æµçš„ä¸€å¸§è¾“å…¥ä¿¡å·ï¼Œåªéœ€å¯¹æ¨¡å‹è¿›è¡Œä¸€æ¬¡è°ƒç”¨å³å¯è¿›è¡Œå¢å¼ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¨¡å‹é€šå¸¸éœ€è¦å¤šæ¬¡è°ƒç”¨ï¼Œå¯¼è‡´è®¡ç®—å¤æ‚åº¦è¿‡é«˜ï¼Œä¸é€‚ç”¨äºè®¸å¤šåœ¨çº¿è¯­éŸ³å¢å¼ºåº”ç”¨ã€‚æœ¬æ–‡æå‡ºäº†æ‰©æ•£ç¼“å†²åŒºï¼ˆDiffusion Bufferï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç”Ÿæˆæ‰©æ•£çš„è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œå¯¹äºæ¥è‡ªæ•°æ®æµçš„ä¸€å¸§è¾“å…¥ä¿¡å·ï¼Œåªéœ€è¿›è¡Œä¸€æ¬¡ç¥ç»ç½‘ç»œè°ƒç”¨å³å¯è¿›è¡Œåœ¨çº¿å¢å¼ºã€‚æ‰©æ•£ç¼“å†²åŒºçš„å…³é”®æ€æƒ³æ˜¯å°†ç‰©ç†æ—¶é—´ä¸æ‰©æ•£æ—¶é—´æ­¥é•¿å¯¹é½ã€‚è¯¥æ–¹æ³•é€šè¿‡ç‰©ç†æ—¶é—´é€æ­¥å»å™ªå¸§ï¼Œå…¶ä¸­è¿‡å»å¸§çš„å™ªå£°å»é™¤æ›´å¤šã€‚å› æ­¤ï¼Œå¢å¼ºåçš„å¸§ä»¥æ‰©æ•£ç¼“å†²åŒºå®šä¹‰çš„å»¶è¿Ÿè¾“å‡ºç»™å¬ä¼—ï¼Œè¾“å‡ºå¸§å…·æœ‰ç›¸åº”çš„å‰ç»æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç²¾å¿ƒè®¾è®¡ä¸€ä¸ªä¸æ‰©æ•£ç¼“å†²åŒºçš„å‰ç»æ€§ç›¸åŒ¹é…çš„äºŒç»´å·ç§¯UNetæ¶æ„ï¼Œè¿›ä¸€æ­¥æ‰©å±•äº†æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œã€‚æˆ‘ä»¬å‘ç°ï¼Œæ‰€æå‡ºçš„UNetåœ¨ç®—æ³•å»¶è¿Ÿè¾ƒä½æ—¶æé«˜äº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨æ•°æ®é¢„æµ‹æŸå¤±è€Œä¸æ˜¯å»å™ªåˆ†æ•°åŒ¹é…æŸå¤±ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­çµæ´»åœ°æ§åˆ¶ç®—æ³•å»¶è¿Ÿå’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ã€‚é…å¤‡æ–°å‹ç¥ç»ç½‘ç»œå’ŒæŸå¤±å‡½æ•°çš„æ‰©å±•æ‰©æ•£ç¼“å†²åŒºå°†ç®—æ³•å»¶è¿Ÿä»320-960æ¯«ç§’å¤§å¹…é™ä½åˆ°32-176æ¯«ç§’ï¼ŒåŒæ—¶æé«˜äº†æ€§èƒ½ã€‚è™½ç„¶ä¹‹å‰å·²ç»è¡¨æ˜ï¼Œç¦»çº¿ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨æœªçŸ¥çš„å™ªå£°è¯­éŸ³æ•°æ®ä¸Šä¼˜äºé¢„æµ‹æ–¹æ³•ï¼Œä½†æˆ‘ä»¬è¯å®ï¼Œåœ¨çº¿æ‰©æ•£ç¼“å†²åŒºä¹Ÿåœ¨æœªçŸ¥çš„å™ªå£°è¯­éŸ³æ•°æ®ä¸Šä¼˜äºå…¶é¢„æµ‹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18744v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£ç¼“å†²å™¨çš„åœ¨çº¿è¯­éŸ³å¢å¼ºæ¨¡å‹ã€‚è¯¥æ¨¡å‹åªéœ€å¯¹æ¥è‡ªæ•°æ®æµä¸­çš„æ¯ä¸ªè¾“å…¥ä¿¡å·å¸§è¿›è¡Œä¸€æ¬¡ç¥ç»ç½‘ç»œè°ƒç”¨ï¼Œå³å¯åœ¨çº¿è¿›è¡Œå¢å¼ºå¤„ç†ï¼Œé€‚ç”¨äºæ¶ˆè´¹è€…çº§GPUã€‚æ‰©æ•£ç¼“å†²å™¨çš„å…³é”®æ€æƒ³æ˜¯å°†ç‰©ç†æ—¶é—´ä¸æ‰©æ•£æ—¶é—´æ­¥é•¿å¯¹é½ï¼Œé€æ­¥æ¶ˆé™¤å¸§ä¸­çš„å™ªå£°ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„2Då·ç§¯UNetæ¶æ„ï¼Œä¸æ‰©æ•£ç¼“å†²å™¨çš„å‰ç»æ€§ç›¸åŒ¹é…ï¼Œæé«˜äº†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç®—æ³•å»¶è¿Ÿè¾ƒä½çš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ•°æ®é¢„æµ‹æŸå¤±è€Œä¸æ˜¯å»å™ªåˆ†æ•°åŒ¹é…æŸå¤±ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­çµæ´»åœ°æ§åˆ¶ç®—æ³•å»¶è¿Ÿå’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ã€‚æ‰©å±•çš„æ‰©æ•£ç¼“å†²å™¨é…åˆæ–°å‹ç¥ç»ç½‘ç»œå’ŒæŸå¤±å‡½æ•°ï¼Œå°†ç®—æ³•å»¶è¿Ÿä»320-960æ¯«ç§’å¤§å¹…å‡å°‘åˆ°32-176æ¯«ç§’ï¼ŒåŒæ—¶æé«˜äº†æ€§èƒ½ã€‚å®éªŒè¯å®ï¼Œåœ¨çº¿æ‰©æ•£ç¼“å†²å™¨åœ¨æœªè§è¿‡çš„å™ªå£°è¯­éŸ³æ•°æ®ä¸Šï¼Œå…¶è¡¨ç°ä¼˜äºé¢„æµ‹å‹æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºæ‰©æ•£ç¼“å†²å™¨çš„åœ¨çº¿è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åªéœ€ä¸€æ¬¡ç¥ç»ç½‘ç»œè°ƒç”¨å³å¯å®Œæˆå¢å¼ºå¤„ç†ã€‚</li>
<li>æ‰©æ•£ç¼“å†²å™¨çš„è®¾è®¡æ€æƒ³æ˜¯å°†ç‰©ç†æ—¶é—´ä¸æ‰©æ•£æ—¶é—´æ­¥é•¿å¯¹é½ï¼Œé€æ­¥æ¶ˆé™¤å¸§å™ªå£°ã€‚</li>
<li>ä½¿ç”¨äº†ç²¾å¿ƒè®¾è®¡çš„2Då·ç§¯UNetæ¶æ„ï¼Œä¸æ‰©æ•£ç¼“å†²å™¨çš„å‰ç»æ€§ç›¸åŒ¹é…ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†æ•°æ®é¢„æµ‹æŸå¤±ï¼Œå®ç°äº†ç®—æ³•å»¶è¿Ÿå’Œè´¨é‡ä¹‹é—´çš„çµæ´»æ§åˆ¶ã€‚</li>
<li>æ‰©å±•çš„æ‰©æ•£ç¼“å†²å™¨å¤§å¹…é™ä½äº†ç®—æ³•å»¶è¿Ÿï¼ŒåŒæ—¶æé«˜äº†è¯­éŸ³å¢å¼ºçš„æ€§èƒ½ã€‚</li>
<li>é¦–æ¬¡è¯å®åœ¨çº¿æ‰©æ•£ç¼“å†²å™¨åœ¨æœªè§è¿‡çš„å™ªå£°è¯­éŸ³æ•°æ®ä¸Šçš„è¡¨ç°ä¼˜äºé¢„æµ‹å‹æ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹é€‚ç”¨äºæ¶ˆè´¹è€…çº§GPUè¿›è¡Œåœ¨çº¿è¯­éŸ³å¢å¼ºå¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-695a0556685410f8415f4ad2d7626500" align="middle">
<img src="https://picx.zhimg.com/v2-cb5931de1b452c5ded466b8af2c84891" align="middle">
<img src="https://picx.zhimg.com/v2-3d3dcc1e76c33c3ef1b368b3d1131cf0" align="middle">
<img src="https://picx.zhimg.com/v2-f4ec076e701f6a4c14d2120c7284d907" align="middle">
<img src="https://picx.zhimg.com/v2-c4e42cc1b529074f1af63df7113fe4ea" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MLMA-Towards-Multilingual-ASR-With-Mamba-based-Architectures"><a href="#MLMA-Towards-Multilingual-ASR-With-Mamba-based-Architectures" class="headerlink" title="MLMA: Towards Multilingual ASR With Mamba-based Architectures"></a>MLMA: Towards Multilingual ASR With Mamba-based Architectures</h2><p><strong>Authors:Mohamed Nabih Ali, Daniele Falavigna, Alessio Brutti</strong></p>
<p>Multilingual automatic speech recognition (ASR) remains a challenging task, especially when balancing performance across high- and low-resource languages. Recent advances in sequence modeling suggest that architectures beyond Transformers may offer better scalability and efficiency. In this work, we introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new approach that leverages the Mamba architecture â€“ an efficient state-space model optimized for long-context sequence processing â€“ for multilingual ASR. Using Mamba, MLMA implicitly incorporates language-aware conditioning and shared representations to support robust recognition across diverse languages. Experiments on standard multilingual benchmarks show that MLMA achieves competitive performance compared to Transformer-based architectures. These results highlight Mambaâ€™s potential as a strong backbone for scalable, efficient, and accurate multilingual speech recognition. </p>
<blockquote>
<p>å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨å¹³è¡¡é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€çš„æ€§èƒ½æ—¶ã€‚åºåˆ—å»ºæ¨¡çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œè¶…è¶ŠTransformerçš„æ¶æ„å¯èƒ½ä¼šæä¾›æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MLMAï¼ˆç”¨äºASRçš„å¸¦æœ‰Mambaçš„å¤šè¯­è¨€è¯­è¨€å»ºæ¨¡ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨Mambaæ¶æ„çš„æ–°æ–¹æ³•â€”â€”ä¸€ç§é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡åºåˆ—å¤„ç†ä¼˜åŒ–çš„é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹â€”â€”ç”¨äºå¤šè¯­è¨€ASRã€‚é€šè¿‡ä½¿ç”¨Mambaï¼ŒMLMAå¯ä»¥éšå«åœ°èå…¥è¯­è¨€æ„ŸçŸ¥æ¡ä»¶å’Œå…±äº«è¡¨ç¤ºï¼Œä»¥æ”¯æŒè·¨ä¸åŒè¯­è¨€çš„ç¨³å¥è¯†åˆ«ã€‚åœ¨æ ‡å‡†å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºTransformerçš„æ¶æ„ç›¸æ¯”ï¼ŒMLMAå…·æœ‰ç«äº‰åŠ›ã€‚è¿™äº›ç»“æœçªå‡ºäº†Mambaä½œä¸ºå¯æ‰©å±•ã€é«˜æ•ˆå’Œå‡†ç¡®çš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«å¼ºå¤§åç›¾çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18684v2">PDF</a> The paper is under review at ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåºåˆ—å»ºæ¨¡çš„æœ€æ–°è¿›å±•ï¼Œæå‡ºåˆ©ç”¨Mambaæ¶æ„è¿›è¡Œå¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚Mambaæ˜¯ä¸€ç§é€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡åºåˆ—å¤„ç†çš„é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥Mambaæ¶æ„ï¼ŒMLMAï¼ˆå¤šè¯­ç§è¯­è¨€å»ºæ¨¡ä¸Mambaç”¨äºASRï¼‰å®ç°äº†å¯¹å¤šç§è¯­è¨€çš„ç¨³å¥è¯†åˆ«ï¼Œå¹¶è¾¾åˆ°äº†ä¸åŸºäºTransformerçš„æ¶æ„ç›¸å½“çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹³è¡¡é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€æ€§èƒ½æ—¶ã€‚</li>
<li>æœ€è¿‘åºåˆ—å»ºæ¨¡çš„è¿›æ­¥è¡¨æ˜ï¼Œè¶…è¶ŠTransformerçš„æ¶æ„å¯èƒ½æä¾›æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚</li>
<li>å¼•å…¥Mambaæ¶æ„è¿›è¡Œå¤šè¯­ç§ASRï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡åºåˆ—å¤„ç†çš„é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚</li>
<li>MLMAé€šè¿‡åˆ©ç”¨Mambaæ¶æ„å®ç°äº†å¯¹å¤šç§è¯­è¨€çš„ç¨³å¥è¯†åˆ«ã€‚</li>
<li>MLMAåˆ©ç”¨è¯­è¨€æ„ŸçŸ¥æ¡ä»¶å’Œå…±äº«è¡¨ç¤ºæ”¯æŒå¤šè¯­ç§è¯†åˆ«ã€‚</li>
<li>åœ¨æ ‡å‡†çš„å¤šè¯­ç§åŸºå‡†æµ‹è¯•ä¸Šï¼ŒMLMAçš„è¡¨ç°ä¸åŸºäºTransformerçš„æ¶æ„ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54bff2b76dc486bbfc36d7c18ac4b939" align="middle">
<img src="https://picx.zhimg.com/v2-beb42e95985a302c2a882242b9440e28" align="middle">
<img src="https://picx.zhimg.com/v2-6bd430373cb364a83b27b705ab503638" align="middle">
<img src="https://picx.zhimg.com/v2-884f82e1b32bbad35c59f97c2e8afaf9" align="middle">
<img src="https://picx.zhimg.com/v2-89a5254876828f74ca046f8f6b09b2ad" align="middle">
<img src="https://picx.zhimg.com/v2-6b8645a57e2f928bf56321560e44b988" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Shallow-Flow-Matching-for-Coarse-to-Fine-Text-to-Speech-Synthesis"><a href="#Shallow-Flow-Matching-for-Coarse-to-Fine-Text-to-Speech-Synthesis" class="headerlink" title="Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis"></a>Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis</h2><p><strong>Authors:Dong Yang, Yiyi Cai, Yuki Saito, Lixu Wang, Hiroshi Saruwatari</strong></p>
<p>We propose Shallow Flow Matching (SFM), a novel mechanism that enhances flow matching (FM)-based text-to-speech (TTS) models within a coarse-to-fine generation paradigm. Unlike conventional FM modules, which use the coarse representations from the weak generator as conditions, SFM constructs intermediate states along the FM paths from these representations. During training, we introduce an orthogonal projection method to adaptively determine the temporal position of these states, and apply a principled construction strategy based on a single-segment piecewise flow. The SFM inference starts from the intermediate state rather than pure noise, thereby focusing computation on the latter stages of the FM paths. We integrate SFM into multiple TTS models with a lightweight SFM head. Experiments demonstrate that SFM yields consistent gains in speech naturalness across both objective and subjective evaluations, and significantly accelerates inference when using adaptive-step ODE solvers. Demo and codes are available at <a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/">https://ydqmkkx.github.io/SFMDemo/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†æµ…æµåŒ¹é…ï¼ˆSFMï¼‰è¿™ä¸€æ–°å‹æœºåˆ¶ï¼Œå®ƒåœ¨ç²—åˆ°ç»†ç”ŸæˆèŒƒå¼ä¸‹å¢å¼ºäº†åŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚ä¸åŒäºä¼ ç»Ÿä½¿ç”¨å¼±ç”Ÿæˆå™¨ç²—è¡¨ç¤ºçš„FMæ¨¡å—ä½œä¸ºæ¡ä»¶ï¼ŒSFMæ²¿ç€è¿™äº›è¡¨ç¤ºçš„FMè·¯å¾„æ„å»ºä¸­é—´çŠ¶æ€ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥æ­£äº¤æŠ•å½±æ–¹æ³•æ¥è‡ªé€‚åº”ç¡®å®šè¿™äº›çŠ¶æ€çš„æ—¶é—´ä½ç½®ï¼Œå¹¶åŸºäºå•æ®µåˆ†æ®µæµåº”ç”¨æœ‰åŸåˆ™çš„æ„å»ºç­–ç•¥ã€‚SFMæ¨ç†ä»ä¸­é—´çŠ¶æ€å¼€å§‹ï¼Œè€Œéçº¯å™ªå£°ï¼Œä»è€Œå°†è®¡ç®—é‡ç‚¹æ”¾åœ¨FMè·¯å¾„çš„åæœŸé˜¶æ®µã€‚æˆ‘ä»¬å°†SFMé›†æˆåˆ°å¤šä¸ªTTSæ¨¡å‹ä¸­ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§çš„SFMå¤´ã€‚å®éªŒè¡¨æ˜ï¼ŒSFMåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡è¡¨ç°å‡ºä¸€è‡´çš„è¯­éŸ³è‡ªç„¶åº¦æå‡ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨è‡ªé€‚åº”æ­¥é•¿ODEæ±‚è§£å™¨æ—¶æ˜¾è‘—åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚æ¼”ç¤ºå’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/%E8%AE%BF%E9%97%AE%E3%80%82">https://ydqmkkx.github.io/SFMDemo/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12226v2">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºShallow Flow Matchingï¼ˆSFMï¼‰çš„æ–°æœºåˆ¶ï¼Œå®ƒæ”¹è¿›äº†åŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„ç²—ç»†ç”ŸæˆèŒƒå¼ã€‚ä¸åŒäºä¼ ç»ŸFMæ¨¡å—ä½¿ç”¨å¼±ç”Ÿæˆå™¨çš„ç²—ç³™è¡¨ç¤ºä½œä¸ºæ¡ä»¶ï¼ŒSFMåœ¨FMè·¯å¾„ä¸Šæ„å»ºä¸­é—´çŠ¶æ€ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥æ­£äº¤æŠ•å½±æ–¹æ³•è‡ªé€‚åº”ç¡®å®šè¿™äº›çŠ¶æ€çš„æ—¶é—´ä½ç½®ï¼Œå¹¶é‡‡ç”¨åŸºäºå•æ®µåˆ†æ®µæµçš„æ„å»ºç­–ç•¥ã€‚SFMæ¨ç†ä»ä¸­é—´çŠ¶æ€å¼€å§‹ï¼Œè€Œéçº¯å™ªå£°ï¼Œä»è€Œå°†è®¡ç®—é‡ç‚¹æ”¾åœ¨FMè·¯å¾„çš„åæœŸé˜¶æ®µã€‚æˆ‘ä»¬å°†SFMé›†æˆåˆ°å¤šä¸ªTTSæ¨¡å‹ä¸­ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§çš„SFMå¤´ã€‚å®éªŒè¡¨æ˜ï¼ŒSFMåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡æé«˜äº†è¯­éŸ³çš„è‡ªç„¶åº¦ï¼Œå¹¶åœ¨ä½¿ç”¨è‡ªé€‚åº”æ­¥é•¿ODEæ±‚è§£å™¨æ—¶æ˜¾è‘—åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚ç›¸å…³æ¼”ç¤ºå’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/%E6%89%BE%E5%88%B0%E3%80%82">https://ydqmkkx.github.io/SFMDemo/æ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†Shallow Flow Matchingï¼ˆSFMï¼‰æœºåˆ¶ï¼Œæ”¹è¿›äº†åŸºäºæµåŒ¹é…çš„æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>SFMæ„å»ºä¸­é—´çŠ¶æ€ï¼Œæ²¿æµåŒ¹é…è·¯å¾„å½¢æˆä»ç²—ç³™è¡¨ç¤ºåˆ°ç²¾ç»†ç”Ÿæˆçš„è¿‡æ¸¡ã€‚</li>
<li>é‡‡ç”¨æ­£äº¤æŠ•å½±æ–¹æ³•è‡ªé€‚åº”ç¡®å®šä¸­é—´çŠ¶æ€çš„æ—¶é—´ä½ç½®ï¼Œå¹¶é‡‡ç”¨å•æ®µåˆ†æ®µæµç­–ç•¥æ„å»ºã€‚</li>
<li>SFMæ¨ç†ä¸“æ³¨äºæµåŒ¹é…è·¯å¾„çš„åæœŸé˜¶æ®µï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>SFMæœºåˆ¶å¯è½»æ¾é›†æˆåˆ°å¤šç§æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢æ¨¡å‹ä¸­ï¼Œå¹¶é€šè¿‡è½»é‡çº§SFMå¤´å®ç°ä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSFMåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡æé«˜äº†è¯­éŸ³çš„è‡ªç„¶åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c276ac84e5e511606dfcfa1350d272a" align="middle">
<img src="https://picx.zhimg.com/v2-53b3788cb1f4dafaef74173926d53003" align="middle">
<img src="https://picx.zhimg.com/v2-c5e6db09a073f6c66aee05d9747bc3f7" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Wireless-Hearables-With-Programmable-Speech-AI-Accelerators"><a href="#Wireless-Hearables-With-Programmable-Speech-AI-Accelerators" class="headerlink" title="Wireless Hearables With Programmable Speech AI Accelerators"></a>Wireless Hearables With Programmable Speech AI Accelerators</h2><p><strong>Authors:Malek Itani, Tuochao Chen, Arun Raghavan, Gavriel Kohlberg, Shyamnath Gollakota</strong></p>
<p>The conventional wisdom has been that designing ultra-compact, battery-constrained wireless hearables with on-device speech AI models is challenging due to the high computational demands of streaming deep learning models. Speech AI models require continuous, real-time audio processing, imposing strict computational and I&#x2F;O constraints. We present NeuralAids, a fully on-device speech AI system for wireless hearables, enabling real-time speech enhancement and denoising on compact, battery-constrained devices. Our system bridges the gap between state-of-the-art deep learning for speech enhancement and low-power AI hardware by making three key technical contributions: 1) a wireless hearable platform integrating a speech AI accelerator for efficient on-device streaming inference, 2) an optimized dual-path neural network designed for low-latency, high-quality speech enhancement, and 3) a hardware-software co-design that uses mixed-precision quantization and quantization-aware training to achieve real-time performance under strict power constraints. Our system processes 6 ms audio chunks in real-time, achieving an inference time of 5.54 ms while consuming 71.6 mW. In real-world evaluations, including a user study with 28 participants, our system outperforms prior on-device models in speech quality and noise suppression, paving the way for next-generation intelligent wireless hearables that can enhance hearing entirely on-device. </p>
<blockquote>
<p>ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºï¼Œè®¾è®¡è¶…ç´§å‡‘ã€å—ç”µæ± é™åˆ¶çš„æ— çº¿å¯ç©¿æˆ´å¬åŠ›è®¾å¤‡ï¼Œå¹¶æ­è½½è®¾å¤‡ç«¯çš„è¯­éŸ³äººå·¥æ™ºèƒ½æ¨¡å‹æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºæµå¼çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è®¡ç®—éœ€æ±‚è¾ƒé«˜ã€‚è¯­éŸ³äººå·¥æ™ºèƒ½æ¨¡å‹éœ€è¦è¿ç»­ã€å®æ—¶çš„éŸ³é¢‘å¤„ç†ï¼Œå¯¹è®¡ç®—å’Œè¾“å…¥&#x2F;è¾“å‡ºéƒ½æœ‰ä¸¥æ ¼çš„è¦æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†NeuralAidsï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨è®¾å¤‡ç«¯çš„è¯­éŸ³äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œç”¨äºæ— çº¿å¯ç©¿æˆ´å¬åŠ›è®¾å¤‡ï¼Œèƒ½å¤Ÿåœ¨ç´§å‡‘ã€ç”µæ± å—é™çš„è®¾å¤‡ä¸Šå®ç°å®æ—¶è¯­éŸ³å¢å¼ºå’Œé™å™ªã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ï¼Œå¡«è¡¥äº†å…ˆè¿›è¯­éŸ³å¢å¼ºæ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸ä½åŠŸè€—äººå·¥æ™ºèƒ½ç¡¬ä»¶ä¹‹é—´çš„ç©ºç™½ï¼š1ï¼‰ä¸€ä¸ªæ— çº¿å¯ç©¿æˆ´å¹³å°ï¼Œé›†æˆè¯­éŸ³äººå·¥æ™ºèƒ½åŠ é€Ÿå™¨ï¼Œç”¨äºé«˜æ•ˆçš„è®¾å¤‡ç«¯æµå¼æ¨ç†ï¼›2ï¼‰ä¸€ä¸ªé’ˆå¯¹ä½å»¶è¿Ÿã€é«˜è´¨é‡è¯­éŸ³å¢å¼ºçš„ä¼˜åŒ–åŒè·¯å¾„ç¥ç»ç½‘ç»œï¼›3ï¼‰ä¸€ä¸ªè½¯ç¡¬ä»¶ååŒè®¾è®¡ï¼Œä½¿ç”¨æ··åˆç²¾åº¦é‡åŒ–å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œåœ¨ä¸¥æ ¼çš„åŠŸç‡é™åˆ¶ä¸‹å®ç°å®æ—¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå®æ—¶å¤„ç†6æ¯«ç§’çš„éŸ³é¢‘å—ï¼Œæ¨ç†æ—¶é—´ä¸º5.54æ¯«ç§’ï¼ŒåŠŸè€—ä¸º71.6æ¯«ç“¦ã€‚åœ¨åŒ…æ‹¬28åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶åœ¨å†…çš„çœŸå®ä¸–ç•Œè¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨è¯­éŸ³è´¨é‡å’Œå™ªå£°æŠ‘åˆ¶æ–¹é¢è¶…è¶Šäº†ä¹‹å‰çš„è®¾å¤‡ç«¯æ¨¡å‹ï¼Œä¸ºä¸‹ä¸€ä»£æ™ºèƒ½æ— çº¿å¯ç©¿æˆ´å¬åŠ›è®¾å¤‡é“ºå¹³äº†é“è·¯ï¼Œè¿™äº›è®¾å¤‡å¯ä»¥åœ¨è®¾å¤‡ä¸Šå®Œå…¨å¢å¼ºå¬åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18698v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿè®¤çŸ¥ä¸­çš„éš¾é¢˜â€”â€”è®¾è®¡å…·æœ‰è®¾å¤‡ç«¯è¯­éŸ³äººå·¥æ™ºèƒ½æ¨¡å‹çš„è¶…ç´§å‡‘ç”µæ± é™åˆ¶æ— çº¿å¬åŠ›è®¾å¤‡ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºNeuralAidsç³»ç»Ÿï¼Œå¯åœ¨å®æ—¶è¯­éŸ³å¢å¼ºå’Œé™å™ªçš„åŸºç¡€ä¸Šå®ç°å…¨è®¾å¤‡ç«¯è¯­éŸ³äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚é€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®ç¼©å°äº†é¡¶å°–æ·±åº¦å­¦ä¹ è¯­éŸ³å¢å¼ºæŠ€æœ¯ä¸ä½åŠŸè€—äººå·¥æ™ºèƒ½ç¡¬ä»¶ä¹‹é—´çš„å·®è·ã€‚è¯¥ç³»ç»Ÿåœ¨ä¸¥æ ¼çš„åŠŸè€—é™åˆ¶ä¸‹å®ç°äº†å®æ—¶æ€§èƒ½ï¼Œå¹¶å¯¹ç°å®ä¸–ç•Œè¯„ä¼°å’Œç”¨æˆ·ç ”ç©¶è¿›è¡Œäº†å±•ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeuralAidsç³»ç»Ÿè§£å†³äº†è®¾è®¡è¶…ç´§å‡‘ç”µæ± é™åˆ¶æ— çº¿å¬åŠ›è®¾å¤‡çš„è¯­éŸ³AIæ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡å…¨è®¾å¤‡ç«¯è¯­éŸ³AIå®ç°å®æ—¶è¯­éŸ³å¢å¼ºå’Œé™å™ªã€‚</li>
<li>ä¸‰é¡¹å…³é”®æŠ€æœ¯è´¡çŒ®åŒ…æ‹¬ï¼šé›†æˆè¯­éŸ³AIåŠ é€Ÿå™¨çš„æ— çº¿å¬åŠ›å¹³å°ã€é’ˆå¯¹ä½å»¶è¿Ÿå’Œé«˜å“è´¨è¯­éŸ³å¢å¼ºçš„ä¼˜åŒ–åŒè·¯å¾„ç¥ç»ç½‘ç»œä»¥åŠè½¯ç¡¬ä»¶ååŒè®¾è®¡ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡æ··åˆç²¾åº¦é‡åŒ–å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒè¾¾åˆ°å®æ—¶æ€§èƒ½ã€‚</li>
<li>ç³»ç»Ÿçš„æ¨ç†æ—¶é—´è¾¾åˆ°äº†æ¯ç§’5.54æ¯«ç§’çš„å¤„ç†é€Ÿåº¦ï¼Œå¹¶æ¶ˆè€—71.6æ¯«ç“¦çš„åŠŸç‡ã€‚</li>
<li>åœ¨ç°å®ä¸–ç•Œè¯„ä¼°å’ŒåŒ…å«ç”¨æˆ·ç ”ç©¶çš„æµ‹è¯•ä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨è¯­éŸ³è´¨é‡å’Œå™ªå£°æŠ‘åˆ¶æ–¹é¢ä¼˜äºå…ˆå‰çš„è®¾å¤‡ç«¯æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd914cc8b46f5b1825a4edaac8504756" align="middle">
<img src="https://picx.zhimg.com/v2-fe341c5571bb136e8fbd5cc78f93102d" align="middle">
<img src="https://picx.zhimg.com/v2-b03a1975b2b64a83666949d71d4f1c98" align="middle">
<img src="https://picx.zhimg.com/v2-09102277550d8a3251fba0f03e287542" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LAMA-UT-Language-Agnostic-Multilingual-ASR-through-Orthography-Unification-and-Language-Specific-Transliteration"><a href="#LAMA-UT-Language-Agnostic-Multilingual-ASR-through-Orthography-Unification-and-Language-Specific-Transliteration" class="headerlink" title="LAMA-UT: Language Agnostic Multilingual ASR through Orthography   Unification and Language-Specific Transliteration"></a>LAMA-UT: Language Agnostic Multilingual ASR through Orthography   Unification and Language-Specific Transliteration</h2><p><strong>Authors:Sangmin Lee, Woo-Jin Chung, Hong-Goo Kang</strong></p>
<p>Building a universal multilingual automatic speech recognition (ASR) model that performs equitably across languages has long been a challenge due to its inherent difficulties. To address this task we introduce a Language-Agnostic Multilingual ASR pipeline through orthography Unification and language-specific Transliteration (LAMA-UT). LAMA-UT operates without any language-specific modules while matching the performance of state-of-the-art models trained on a minimal amount of data. Our pipeline consists of two key steps. First, we utilize a universal transcription generator to unify orthographic features into Romanized form and capture common phonetic characteristics across diverse languages. Second, we utilize a universal converter to transform these universal transcriptions into language-specific ones. In experiments, we demonstrate the effectiveness of our proposed method leveraging universal transcriptions for massively multilingual ASR. Our pipeline achieves a relative error reduction rate of 45% when compared to Whisper and performs comparably to MMS, despite being trained on only 0.1% of Whisperâ€™s training data. Furthermore, our pipeline does not rely on any language-specific modules. However, it performs on par with zero-shot ASR approaches which utilize additional language-specific lexicons and language models. We expect this framework to serve as a cornerstone for flexible multilingual ASR systems that are generalizable even to unseen languages. </p>
<blockquote>
<p>æ„å»ºä¸€ä¸ªé€šç”¨çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œä»¥åœ¨æ‰€æœ‰è¯­ç§ä¸­å®ç°å‡è¡¡è¡¨ç°ï¼Œé•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå…¶æœ¬èº«å°±å­˜åœ¨è¯¸å¤šå›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é€šè¿‡æ­£å­—æ³•ç»Ÿä¸€å’Œè¯­ç§ç‰¹å®šè½¬å†™ï¼ˆLAMA-UTï¼‰ï¼Œå¼•å…¥äº†è¯­ç§æ— å…³çš„å¤šè¯­ç§ASRç®¡é“ã€‚LAMA-UTåœ¨æ²¡æœ‰ä»»ä½•è¯­ç§ç‰¹å®šæ¨¡å—çš„æƒ…å†µä¸‹è¿è¡Œï¼ŒåŒæ—¶åŒ¹é…åœ¨å°‘é‡æ•°æ®ä¸Šè®­ç»ƒçš„æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç®¡é“åŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨é€šç”¨è½¬å½•ç”Ÿæˆå™¨ï¼Œå°†æ­£å­—ç‰¹å¾ç»Ÿä¸€ä¸ºç½—é©¬åŒ–å½¢å¼ï¼Œå¹¶æ•æ‰ä¸åŒè¯­ç§ä¹‹é—´å…±åŒçš„è¯­éŸ³ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨é€šç”¨è½¬æ¢å™¨å°†è¿™äº›é€šç”¨è½¬å½•è½¬åŒ–ä¸ºç‰¹å®šè¯­è¨€çš„è½¬å½•ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†åˆ©ç”¨é€šç”¨è½¬å½•è¿›è¡Œå¤§è§„æ¨¡å¤šè¯­ç§ASRçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸whisperç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨ç›¸å¯¹è¯¯å·®å‡å°‘ç‡æ–¹é¢è¾¾åˆ°äº†45%ï¼Œå°½ç®¡å®ƒåªåœ¨whisperçš„0.1%è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç®¡é“ä¸ä¾èµ–äºä»»ä½•è¯­ç§ç‰¹å®šæ¨¡å—ï¼Œä½†å®ƒçš„æ€§èƒ½ä¸é›¶å°„å‡»ASRæ–¹æ³•ç›¸å½“ï¼Œåè€…åˆ©ç”¨é¢å¤–çš„è¯­ç§ç‰¹å®šè¯æ±‡å’Œè¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬é¢„è®¡è¿™ä¸€æ¡†æ¶å°†æˆä¸ºçµæ´»çš„å¤šè¯­ç§ASRç³»ç»Ÿçš„åŸºçŸ³ï¼Œç”šè‡³å¯ä»¥å¯¹æœªè§è¿‡çš„è¯­ç§è¿›è¡Œæ¨å¹¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15299v4">PDF</a> Accepted to AAAI 2025 (Oral Presentation)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè¯­è¨€æ— å…³çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç®¡é“ï¼Œé€šè¿‡æ­£äº¤ç»Ÿä¸€å’Œè¯­è¨€ç‰¹å®šè½¬è¯‘ï¼ˆLAMA-UTï¼‰æ¥è§£å†³è·¨è¯­ç§å¹³ç­‰æ€§èƒ½çš„æŒ‘æˆ˜ã€‚è¯¥ç®¡é“åŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šé¦–å…ˆä½¿ç”¨é€šç”¨è½¬å½•ç”Ÿæˆå™¨å°†æ­£äº¤ç‰¹å¾ç»Ÿä¸€è½¬æ¢ä¸ºç½—é©¬åŒ–å½¢å¼ï¼Œæ•æ‰ä¸åŒè¯­è¨€ä¹‹é—´çš„å…±åŒè¯­éŸ³ç‰¹å¾ï¼›ç„¶åä½¿ç”¨é€šç”¨è½¬æ¢å™¨å°†è¿™äº›é€šç”¨è½¬å½•è½¬æ¢ä¸ºç‰¹å®šè¯­è¨€çš„è½¬å½•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§é‡å¤šè¯­ç§ASRä¸­åˆ©ç”¨é€šç”¨è½¬å½•éå¸¸æœ‰æ•ˆï¼Œä¸whisperç›¸æ¯”å®ç°äº†45%çš„ç›¸å¯¹è¯¯å·®é™ä½ç‡ï¼Œå¹¶ä¸”åœ¨ä»…ä½¿ç”¨whisper 0.1%çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œè¯¥ç®¡é“ä¸ä¾èµ–ä»»ä½•ç‰¹å®šè¯­è¨€çš„æ¨¡å—ï¼Œä½†è¡¨ç°ä¸é›¶å°„å‡»ASRæ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹LAMA-UTã€‚</li>
<li>LAMA-UTé‡‡ç”¨æ­£äº¤ç»Ÿä¸€å’Œè¯­è¨€ç‰¹å®šè½¬è¯‘ä¸¤å¤§æ­¥éª¤è¿›è¡Œå¤„ç†ã€‚</li>
<li>é€šç”¨è½¬å½•ç”Ÿæˆå™¨å°†ä¸åŒè¯­è¨€çš„æ­£äº¤ç‰¹å¾è½¬æ¢ä¸ºç½—é©¬åŒ–å½¢å¼ï¼Œå¹¶æ•æ‰å…±åŒè¯­éŸ³ç‰¹å¾ã€‚</li>
<li>ä¸whisperç›¸æ¯”ï¼ŒLAMA-UTå®ç°äº†ç›¸å¯¹è¯¯å·®é™ä½ç‡è¾¾åˆ°äº†45%ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚</li>
<li>è¯¥ç®¡é“ä¸ä¾èµ–ä»»ä½•ç‰¹å®šè¯­è¨€çš„æ¨¡å—ï¼Œè¡¨ç°ä¸é›¶å°„å‡»ASRæ–¹æ³•ç›¸å½“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-491a780e80b71a4fff70f376318db3a4" align="middle">
<img src="https://picx.zhimg.com/v2-13f6665919362a43b62f117bf6042fc0" align="middle">
<img src="https://picx.zhimg.com/v2-dfc81b8ef37d4829f839cb43f11615d4" align="middle">
<img src="https://picx.zhimg.com/v2-1a789242d166d387373e24a3ebfab27f" align="middle">
<img src="https://picx.zhimg.com/v2-7a07dd7997bda16270402fffc91cd0c3" align="middle">
<img src="https://picx.zhimg.com/v2-9ebf7f8fcaa96162aa84681fabe5e24f" align="middle">
<img src="https://picx.zhimg.com/v2-d8aaa412a79522c7557e862d10d32879" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2ba6dc26a168bdb8c802fda5197136e6" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  FerretNet Efficient Synthetic Image Detection via Local Pixel   Dependencies
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-841e9e5ed7afea95a0dcbc3e2ac6c366" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
