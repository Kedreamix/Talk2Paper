<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
    <meta name="description" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-841e9e5ed7afea95a0dcbc3e2ac6c366')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    6.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    27 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-25-æ›´æ–°"><a href="#2025-10-25-æ›´æ–°" class="headerlink" title="2025-10-25 æ›´æ–°"></a>2025-10-25 æ›´æ–°</h1><h2 id="Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge"><a href="#Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge" class="headerlink" title="Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge"></a>Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge</h2><p><strong>Authors:Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</strong></p>
<p>Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home">https://sites.google.com/view/lddbm/home</a>. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆå»ºæ¨¡çš„æœ€æ–°è¿›å±•ä½¿å¾—æ‰©æ•£æ¨¡å‹æˆä¸ºä»å¤æ‚æ•°æ®åˆ†å¸ƒä¸­é‡‡æ ·çš„æœ€å…ˆè¿›çš„å·¥å…·ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬çš„èƒ½åŠ›æ‹“å±•åˆ°è·¨ä¸åŒæ„Ÿå®˜æ¨¡æ€çš„æ¨¡æ€ç¿»è¯‘ï¼ˆMTï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé™åˆ¶æ€§å‡è®¾ï¼ŒåŒ…æ‹¬å…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œç‰¹å®šæ¨¡æ€æ¶æ„ï¼Œè¿™äº›å‡è®¾é™åˆ¶äº†å®ƒä»¬çš„é€šç”¨æ€§å’Œç†è®ºä¾æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLatent Denoising Diffusion Bridge Modelï¼ŒLDDBMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹çš„æ½œåœ¨å˜é‡æ‰©å±•çš„é€šç”¨æ¨¡æ€ç¿»è¯‘æ¡†æ¶ã€‚é€šè¿‡åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— éœ€å¯¹é½ç»´åº¦çš„æƒ…å†µä¸‹å­¦ä¹ äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯¹æ¯”å¯¹é½æŸå¤±æ¥å¼ºåˆ¶é…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é’ˆå¯¹æ½œåœ¨ç©ºé—´ä¸­å™ªå£°é¢„æµ‹çš„é¢†åŸŸé€šç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†é¢„æµ‹æŸå¤±æ¥å¼•å¯¼è®­ç»ƒä»¥å®ç°å‡†ç¡®çš„è·¨åŸŸç¿»è¯‘ï¼Œå¹¶æ¢ç´¢äº†å¤šç§è®­ç»ƒç­–ç•¥æ¥æé«˜ç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒä»»æ„æ¨¡æ€å¯¹ï¼Œå¹¶åœ¨å¤šç§MTä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ï¼ŒåŒ…æ‹¬å¤šè§†å›¾åˆ°3Då½¢çŠ¶ç”Ÿæˆã€å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå¤šè§†å›¾åœºæ™¯åˆæˆã€‚ç»¼åˆå®éªŒå’Œæ¶ˆèå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ä¸€èˆ¬æ¨¡æ€ç¿»è¯‘ä¸­å»ºç«‹äº†æ–°çš„å¼ºåŠ²åŸºå‡†ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home%E3%80%82">https://sites.google.com/view/lddbm/homeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20819v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€è¿‘å‘å±•çš„æ‰©æ•£æ¨¡å‹åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰ä¸­çš„æ˜¾è‘—æˆåŠŸï¼Œå¹¶é’ˆå¯¹è·¨ä¸åŒæ„Ÿå®˜æ¨¡æ€çš„ä¿¡æ¯ç¿»è¯‘ï¼ˆæ¨¡æ€ç¿»è¯‘ï¼‰çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLatent Denoising Diffusion Bridge Modelï¼ŒLDDBMï¼‰ã€‚è¯¥æ¨¡å‹åœ¨æ½œåœ¨å˜é‡æ‰©å±•çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å…±äº«æ½œåœ¨ç©ºé—´å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ï¼Œæ— éœ€å¯¹é½ç»´åº¦ã€‚é€šè¿‡å¼•å…¥å¯¹æ¯”å¯¹é½æŸå¤±å’Œé’ˆå¯¹æ½œåœ¨ç©ºé—´çš„å™ªå£°é¢„æµ‹è®¾è®¡çš„é¢†åŸŸé€šç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œä»¥åŠé¢„æµ‹æŸå¤±å’Œå¤šç§è®­ç»ƒç­–ç•¥ï¼Œè¯¥æ¨¡å‹æ”¯æŒä»»æ„æ¨¡æ€å¯¹ï¼Œå¹¶åœ¨å¤šç§æ¨¡æ€ç¿»è¯‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å·²å–å¾—æ˜¾è‘—æˆåŠŸã€‚</li>
<li>æ¨¡æ€ç¿»è¯‘ï¼ˆModality Translationï¼ŒMTï¼‰æ˜¯å°†ä¿¡æ¯ä»ä¸€ä¸ªæ„Ÿå®˜æ¨¡æ€ç¿»è¯‘åˆ°å¦ä¸€ä¸ªæ„Ÿå®˜æ¨¡æ€çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸å¸¸ä¾èµ–äºå…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œæ¨¡æ€ç‰¹å®šæ¶æ„ç­‰é™åˆ¶æ€§å‡è®¾ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œç†è®ºæ ¹æ®ã€‚</li>
<li>æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLDDBMï¼‰æ˜¯ä¸€ç§åŸºäºæ½œåœ¨å˜é‡æ‰©å±•çš„é€šç”¨æ¨¡æ€ç¿»è¯‘æ¡†æ¶ã€‚</li>
<li>LDDBMé€šè¿‡åœ¨å…±äº«æ½œåœ¨ç©ºé—´æ“ä½œï¼Œæ— éœ€å¯¹é½ç»´åº¦å³å¯å­¦ä¹ ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ã€‚</li>
<li>LDDBMå¼•å…¥å¯¹æ¯”å¯¹é½æŸå¤±ä»¥åŠ å¼ºé…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†é’ˆå¯¹æ½œåœ¨ç©ºé—´å™ªå£°é¢„æµ‹çš„é¢†åŸŸé€šç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71b7d87ab3147c25125212c3b2948c06" align="middle">
<img src="https://picx.zhimg.com/v2-5d66e2842144f846f899b138e2dd2d51" align="middle">
<img src="https://picx.zhimg.com/v2-227bee8414c0a17e145209c970af48a3" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Transformed-Multi-view-3D-Shape-Features-with-Contrastive-Learning"><a href="#Transformed-Multi-view-3D-Shape-Features-with-Contrastive-Learning" class="headerlink" title="Transformed Multi-view 3D Shape Features with Contrastive Learning"></a>Transformed Multi-view 3D Shape Features with Contrastive Learning</h2><p><strong>Authors:MÃ¡rcus VinÃ­cius Lobo Costa, Sherlon Almeida da Silva, BÃ¡rbara Caroline Benato, Leo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti</strong></p>
<p>This paper addresses the challenges in representation learning of 3D shape features by investigating state-of-the-art backbones paired with both contrastive supervised and self-supervised learning objectives. Computer vision methods struggle with recognizing 3D objects from 2D images, often requiring extensive labeled data and relying on Convolutional Neural Networks (CNNs) that may overlook crucial shape relationships. Our work demonstrates that Vision Transformers (ViTs) based architectures, when paired with modern contrastive objectives, achieve promising results in multi-view 3D analysis on our downstream tasks, unifying contrastive and 3D shape understanding pipelines. For example, supervised contrastive losses reached about 90.6% accuracy on ModelNet10. The use of ViTs and contrastive learning, leveraging ViTsâ€™ ability to understand overall shapes and contrastive learningâ€™s effectiveness, overcomes the need for extensive labeled data and the limitations of CNNs in capturing crucial shape relationships. The success stems from capturing global shape semantics via ViTs and refining local discriminative features through contrastive optimization. Importantly, our approach is empirical, as it is grounded on extensive experimental evaluation to validate the effectiveness of combining ViTs with contrastive objectives for 3D representation learning. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨æœ€å‰æ²¿çš„éª¨å¹²ç½‘ç»œé…å¯¹å¯¹æ¯”ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ç›®æ ‡çš„æŒ‘æˆ˜ï¼Œè¿›è¡Œä¸‰ç»´å½¢çŠ¶ç‰¹å¾è¡¨ç¤ºå­¦ä¹ çš„é—®é¢˜ã€‚è®¡ç®—æœºè§†è§‰æ–¹æ³•åœ¨è¯†åˆ«äºŒç»´å›¾åƒä¸­çš„ä¸‰ç»´ç‰©ä½“æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼Œå¹¶ä¾èµ–äºå¯èƒ½å¿½ç•¥å…³é”®å½¢çŠ¶å…³ç³»çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼ŒåŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„æ¶æ„ä¸ç°ä»£å¯¹æ¯”ç›®æ ‡ç›¸ç»“åˆæ—¶ï¼Œåœ¨æˆ‘ä»¬çš„ä¸‹æ¸¸ä»»åŠ¡å¤šè§†è§’ä¸‰ç»´åˆ†æä¸­å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œç»Ÿä¸€äº†å¯¹æ¯”å’Œä¸‰ç»´å½¢çŠ¶ç†è§£æµç¨‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ModelNet10ä¸Šï¼Œç›‘ç£å¯¹æ¯”æŸå¤±è¾¾åˆ°äº†çº¦90.6%çš„å‡†ç¡®ç‡ã€‚åˆ©ç”¨ViTå’Œå¯¹æ¯”å­¦ä¹ çš„ç»“åˆï¼Œå€ŸåŠ©ViTå¯¹æ•´ä½“å½¢çŠ¶çš„ç†è§£å’Œå¯¹æ¯”å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œå…‹æœäº†éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–å’Œå¯¹CNNæ•æ‰å…³é”®å½¢çŠ¶å…³ç³»çš„å±€é™æ€§ã€‚è¿™ç§æˆåŠŸçš„å…³é”®åœ¨äºé€šè¿‡ViTæ•è·å…¨å±€å½¢çŠ¶è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å¯¹æ¯”ä¼˜åŒ–æ¥æç‚¼å±€éƒ¨åˆ¤åˆ«ç‰¹å¾ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥å®è¯ç ”ç©¶ä¸ºåŸºç¡€ï¼Œé€šè¿‡å®éªŒéªŒè¯ViTä¸å¯¹æ¯”ç›®æ ‡ç»“åˆç”¨äºä¸‰ç»´è¡¨ç¤ºå­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19955v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å…ˆè¿›çš„éª¨å¹²ç½‘ç»œç»“åˆå¯¹æ¯”ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ç›®æ ‡è¿›è¡Œ3Då½¢çŠ¶ç‰¹å¾è¡¨ç¤ºå­¦ä¹ çš„æŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡ºè®¡ç®—æœºè§†è§‰æ–¹æ³•åœ¨è¯†åˆ«3Dç‰©ä½“æ—¶é¢ä¸´ä»äºŒç»´å›¾åƒä¸­è¯†åˆ«ä¸‰ç»´ç‰©ä½“çš„éš¾é¢˜ï¼Œéœ€è¦ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ä¸”å¯èƒ½å¿½ç•¥é‡è¦çš„å½¢çŠ¶å…³ç³»ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºVision Transformerï¼ˆViTï¼‰çš„æ¶æ„ç»“åˆç°ä»£å¯¹æ¯”ç›®æ ‡å¯å®ç°ä»¤äººé¼“èˆçš„å¤šè§†å›¾3Dåˆ†æç»“æœã€‚ä¾‹å¦‚ï¼Œåœ¨ModelNet10ä¸Šï¼Œä½¿ç”¨ç›‘ç£å¯¹æ¯”æŸå¤±è¾¾åˆ°äº†çº¦90.6%çš„å‡†ç¡®ç‡ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»“åˆViTså’Œå¯¹æ¯”å­¦ä¹ ï¼Œå…‹æœäº†éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„éœ€è¦ä»¥åŠå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨æ•æ‰å…³é”®å½¢çŠ¶å…³ç³»æ–¹é¢çš„å±€é™æ€§ã€‚å…¶æˆåŠŸæºäºé€šè¿‡ViTæ•è·å…¨å±€å½¢çŠ¶è¯­ä¹‰å¹¶é€šè¿‡å¯¹æ¯”ä¼˜åŒ–ç»†åŒ–å±€éƒ¨åˆ¤åˆ«ç‰¹å¾ã€‚æœ¬ç ”ç©¶é€šè¿‡å¹¿æ³›çš„å®éªŒè¯„ä¼°éªŒè¯äº†ViTä¸å¯¹æ¯”ç›®æ ‡ç»“åˆè¿›è¡Œ3Dè¡¨ç¤ºå­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æ¢è®¨äº†åˆ©ç”¨å…ˆè¿›çš„éª¨å¹²ç½‘ç»œï¼ˆå¦‚Vision Transformerï¼‰ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œç›‘ç£å­¦ä¹ è¿›è¡Œ3Då½¢çŠ¶ç‰¹å¾è¡¨ç¤ºçš„æŒ‘æˆ˜ã€‚</li>
<li>æŒ‡å‡ºè®¡ç®—æœºè§†è§‰æ–¹æ³•è¯†åˆ«ä¸‰ç»´ç‰©ä½“æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä»äºŒç»´å›¾åƒä¸­è¯†åˆ«ä¸‰ç»´ç‰©ä½“çš„éš¾åº¦ä»¥åŠä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å‘ç°Vision Transformerï¼ˆViTï¼‰ç»“åˆç°ä»£å¯¹æ¯”ç›®æ ‡åœ¨ä¸‰ç»´åˆ†æä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†çº¦90.6%çš„å‡†ç¡®ç‡ã€‚</li>
<li>ViTsèƒ½å¤Ÿæ•æ‰å…¨å±€å½¢çŠ¶è¯­ä¹‰ï¼Œè€Œå¯¹æ¯”å­¦ä¹ åˆ™æœ‰åŠ©äºç»†åŒ–å±€éƒ¨åˆ¤åˆ«ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•å…‹æœäº†éœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®çš„éš¾é¢˜ä»¥åŠå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨æ•æ‰å…³é”®å½¢çŠ¶å…³ç³»æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ç»“åˆViTå’Œå¯¹æ¯”å­¦ä¹ ç›®æ ‡çš„3Dè¡¨ç¤ºå­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3a2799cbd5c2749451f2efd5ad42631" align="middle">
<img src="https://picx.zhimg.com/v2-2971ab0648a129187ef162b81705f581" align="middle">
<img src="https://picx.zhimg.com/v2-d015ce38aae48fdea63d08706fa655dc" align="middle">
<img src="https://picx.zhimg.com/v2-6f3aa4f2ed37cd38668730757ce7852e" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Class-Aware-Prototype-Learning-with-Negative-Contrast-for-Test-Time-Adaptation-of-Vision-Language-Models"><a href="#Class-Aware-Prototype-Learning-with-Negative-Contrast-for-Test-Time-Adaptation-of-Vision-Language-Models" class="headerlink" title="Class-Aware Prototype Learning with Negative Contrast for Test-Time   Adaptation of Vision-Language Models"></a>Class-Aware Prototype Learning with Negative Contrast for Test-Time   Adaptation of Vision-Language Models</h2><p><strong>Authors:Xiaozhen Qiao, Jingkai Zhao, Yuqiu Jiang, Xianda Guo, Zhe Sun, Hongyuan Zhang, Xuelong Li</strong></p>
<p>Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization through large-scale image-text pretraining, yet their performance can drop once the deployment distribution diverges from the training distribution. To address this, Test-Time Adaptation (TTA) methods update models using unlabeled target data. However, existing approaches often ignore two key challenges: prototype degradation in long-tailed distributions and confusion between semantically similar classes. To tackle these issues, we propose \textbf{C}lass-Aware \textbf{P}rototype \textbf{L}earning with \textbf{N}egative \textbf{C}ontrast(\textbf{CPL-NC}), a lightweight TTA framework designed specifically for VLMs to enhance generalization under distribution shifts. CPL-NC introduces a \textit{Class-Aware Prototype Cache} Module that dynamically adjusts per-class capacity based on test-time frequency and activation history, with a rejuvenation mechanism for inactive classes to retain rare-category knowledge. Additionally, a \textit{Negative Contrastive Learning} Mechanism identifies and constrains hard visual-textual negatives to improve class separability. The framework employs asymmetric optimization, refining only textual prototypes while anchoring on stable visual features. Experiments on 15 benchmarks show that CPL-NC consistently outperforms prior TTA methods across both ResNet-50 and ViT-B&#x2F;16 backbones. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œç„¶è€Œä¸€æ—¦éƒ¨ç½²åˆ†å¸ƒä¸è®­ç»ƒåˆ†å¸ƒå‡ºç°åå·®ï¼Œå…¶æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ä½¿ç”¨æ— æ ‡ç­¾çš„ç›®æ ‡æ•°æ®æ¥æ›´æ–°æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šé•¿å°¾åˆ†å¸ƒä¸­çš„åŸå‹é€€åŒ–ä»¥åŠè¯­ä¹‰ç›¸ä¼¼ç±»åˆ«ä¹‹é—´çš„æ··æ·†ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘VLMsçš„ç±»æ„ŸçŸ¥åŸå‹å­¦ä¹ ä¸è´Ÿå¯¹æ¯”ï¼ˆCPL-NCï¼‰å¢å¼ºæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æµ‹è¯•æ—¶é—´é€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åˆ†å¸ƒåç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚CPL-NCå¼•å…¥äº†ç±»æ„ŸçŸ¥åŸå‹ç¼“å­˜æ¨¡å—ï¼Œæ ¹æ®æµ‹è¯•æ—¶çš„é¢‘ç‡å’Œæ¿€æ´»å†å²åŠ¨æ€è°ƒæ•´æ¯ç±»çš„å®¹é‡ï¼Œå¹¶ä¸ºä¸æ´»è·ƒç±»åˆ«æä¾›å¤è‹æœºåˆ¶ä»¥ä¿ç•™ç½•è§ç±»åˆ«çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè´Ÿå¯¹æ¯”å­¦ä¹ æœºåˆ¶èƒ½å¤Ÿè¯†åˆ«å’Œé™åˆ¶å›°éš¾çš„è§†è§‰æ–‡æœ¬è´Ÿæ ·æœ¬ï¼Œä»¥æé«˜ç±»é—´å¯åˆ†æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¯¹ç§°ä¼˜åŒ–æ–¹æ³•ï¼Œä»…ç²¾ç‚¼æ–‡æœ¬åŸå‹ï¼ŒåŒæ—¶ä»¥ç¨³å®šçš„è§†è§‰ç‰¹å¾ä¸ºé”šç‚¹ã€‚åœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCPL-NCåœ¨ResNet-50å’ŒViT-B&#x2F;16ä¸¤ç§ä¸»å¹²ç½‘ç»œä¸Šå‡ä¼˜äºå…ˆå‰çš„TTAæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19802v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ä¸“ä¸ºVision-Language Modelsï¼ˆVLMsï¼‰è®¾è®¡çš„æµ‹è¯•æ—¶é€‚åº”ï¼ˆTTAï¼‰æ¡†æ¶ï¼Œåä¸ºClass-Aware Prototype Learning with Negative Contrastï¼ˆCPL-NCï¼‰ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³VLMsåœ¨é¢å¯¹éƒ¨ç½²åˆ†å¸ƒä¸è®­ç»ƒåˆ†å¸ƒå·®å¼‚æ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ç±»æ„ŸçŸ¥åŸå‹ç¼“å­˜æ¨¡å—å’Œè´Ÿå¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼ŒCPL-NCæé«˜äº†æ¨¡å‹åœ¨åˆ†å¸ƒè½¬ç§»ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨é¢ä¸´éƒ¨ç½²åˆ†å¸ƒä¸è®­ç»ƒåˆ†å¸ƒå·®å¼‚æ—¶æ€§èƒ½å¯èƒ½ä¸‹é™ï¼Œéœ€è¦é‡‡ç”¨Test-Time Adaptationï¼ˆTTAï¼‰æ–¹æ³•è¿›è¡Œæ¨¡å‹æ›´æ–°ã€‚</li>
<li>ç°æœ‰TTAæ–¹æ³•å¸¸å¸¸å¿½ç•¥é•¿å°¾åˆ†å¸ƒä¸­çš„åŸå‹é€€åŒ–ä»¥åŠè¯­ä¹‰ç›¸ä¼¼ç±»åˆ«ä¹‹é—´çš„æ··æ·†é—®é¢˜ã€‚</li>
<li>CPL-NCæ¡†æ¶é€šè¿‡å¼•å…¥ç±»æ„ŸçŸ¥åŸå‹ç¼“å­˜æ¨¡å—ï¼Œæ ¹æ®æµ‹è¯•æ—¶çš„é¢‘ç‡å’Œæ¿€æ´»å†å²åŠ¨æ€è°ƒæ•´æ¯ç±»çš„å®¹é‡ï¼Œå¹¶è®¾æœ‰æœºåˆ¶ä¿ç•™ä¸æ´»è·ƒç±»åˆ«çš„çŸ¥è¯†ã€‚</li>
<li>CPL-NCé‡‡ç”¨è´Ÿå¯¹æ¯”å­¦ä¹ æœºåˆ¶ï¼Œè¯†åˆ«å¹¶çº¦æŸå›°éš¾çš„è§†è§‰æ–‡æœ¬è´Ÿæ ·æœ¬ï¼Œæé«˜ç±»é—´å¯åˆ†æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨å¯¹ç§°ä¼˜åŒ–ï¼Œåªç²¾ç‚¼æ–‡æœ¬åŸå‹ï¼ŒåŒæ—¶ä¾æ‰˜ç¨³å®šçš„è§†è§‰ç‰¹å¾è¿›è¡Œé”šå®šã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCPL-NCåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡ä¼˜äºå…ˆå‰çš„TTAæ–¹æ³•ï¼Œé€‚ç”¨äºResNet-50å’ŒViT-B&#x2F;16ä¸¤ç§éª¨å¹²ç½‘ç»œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35577b202d4e811cd7def374a4dffdaf" align="middle">
<img src="https://picx.zhimg.com/v2-5bc49ac95ec7cdaea53097f163f57da4" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-a-Unified-Vision-Centric-Contrastive-Alternatives-on-Multi-Modal-Web-Documents"><a href="#Exploring-a-Unified-Vision-Centric-Contrastive-Alternatives-on-Multi-Modal-Web-Documents" class="headerlink" title="Exploring a Unified Vision-Centric Contrastive Alternatives on   Multi-Modal Web Documents"></a>Exploring a Unified Vision-Centric Contrastive Alternatives on   Multi-Modal Web Documents</h2><p><strong>Authors:Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou</strong></p>
<p>Contrastive vision-language models such as CLIP have demonstrated strong performance across a wide range of multimodal tasks by learning from aligned image-text pairs. However, their ability to handle complex, real-world web documents remains limited, particularly in scenarios where text and images are interleaved, loosely aligned, or embedded in visual form. To address these challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified framework that models text, images, and their combinations using a single vision transformer. VC2L operates entirely in pixel space by rendering all inputs, whether textual, visual, or combined, as images, thus eliminating the need for OCR, text tokenization, or modality fusion strategy. To capture complex cross-modal relationships in multimodal web documents, VC2L employs a snippet-level contrastive learning objective that aligns consecutive multimodal segments, leveraging the inherent coherence of documents without requiring explicitly paired image-text data. To assess the effectiveness of this approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR, designed to evaluate cross-modal retrieval, fine-grained sequential understanding, and generalization to unseen data, respectively. Empirical results show that VC2L achieves competitive or superior performance compared to CLIP-style models on both the proposed benchmarks and established datasets such as M-BEIR and MTEB. These findings underscore the potential of multimodal web data as a valuable training resource for contrastive learning and illustrate the scalability of a unified, vision-centric approach for multimodal representation learning. Code and models are available at: <a target="_blank" rel="noopener" href="https://github.com/showlab/VC2L">https://github.com/showlab/VC2L</a>. </p>
<blockquote>
<p>å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¦‚CLIPï¼Œé€šè¿‡ä»å¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹ä¸­å­¦ä¹ ï¼Œå·²åœ¨å¤šç§è·¨æ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¤„ç†å¤æ‚ã€çœŸå®ä¸–ç•Œç½‘é¡µæ–‡æ¡£çš„èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬å’Œå›¾åƒäº¤é”™ã€æ¾æ•£å¯¹é½æˆ–ä»¥è§†è§‰å½¢å¼åµŒå…¥çš„åœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¯¹æ¯”å­¦ä¹ ï¼ˆVC2Lï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å•ä¸ªè§†è§‰è½¬æ¢å™¨å¯¹æ–‡æœ¬ã€å›¾åƒåŠå…¶ç»„åˆè¿›è¡Œå»ºæ¨¡ã€‚VC2Lå®Œå…¨åœ¨åƒç´ ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œå°†æ‰€æœ‰è¾“å…¥ï¼ˆæ— è®ºæ˜¯æ–‡æœ¬ã€è§†è§‰è¿˜æ˜¯ç»„åˆï¼‰å‘ˆç°ä¸ºå›¾åƒï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹OCRã€æ–‡æœ¬æ ‡è®°åŒ–æˆ–æ¨¡æ€èåˆç­–ç•¥çš„éœ€æ±‚ã€‚ä¸ºäº†æ•è·è·¨æ¨¡æ€ç½‘é¡µæ–‡æ¡£ä¸­çš„å¤æ‚å…³ç³»ï¼ŒVC2Lé‡‡ç”¨ç‰‡æ®µçº§çš„å¯¹æ¯”å­¦ä¹ ç›®æ ‡æ¥å¯¹é½è¿ç»­çš„è·¨æ¨¡æ€ç‰‡æ®µï¼Œåˆ©ç”¨æ–‡æ¡£çš„å†…åœ¨è¿è´¯æ€§ï¼Œæ— éœ€æ˜ç¡®é…å¯¹çš„å›¾åƒæ–‡æœ¬æ•°æ®ã€‚ä¸ºäº†è¯„ä¼°è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªæ£€ç´¢åŸºå‡†æµ‹è¯•ï¼Œå³AnyCIRã€SeqCIRå’ŒCSRï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°è·¨æ¨¡æ€æ£€ç´¢ã€ç²¾ç»†çš„åºåˆ—ç†è§£å’Œå¯¹æœªè§æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œä¸CLIPé£æ ¼çš„æ¨¡å‹ç›¸æ¯”ï¼ŒVC2Låœ¨æå‡ºçš„åŸºå‡†æµ‹è¯•å’ŒM-BEIRã€MTEBç­‰ç°æœ‰æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†è·¨æ¨¡æ€ç½‘ç»œæ•°æ®ä½œä¸ºå¯¹æ¯”å­¦ä¹ çš„å®è´µè®­ç»ƒèµ„æºçš„æ½œåŠ›ï¼Œå¹¶å±•ç¤ºäº†ç»Ÿä¸€ã€ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•çš„å¤§è§„æ¨¡æ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/showlab/VC2L%E3%80%82">https://github.com/showlab/VC2Lã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18703v1">PDF</a> Project page: this <a target="_blank" rel="noopener" href="https://linyq17.github.io/VC2L/">https://linyq17.github.io/VC2L/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤æ‚ã€çœŸå®ä¸–ç•Œç½‘é¡µæ–‡æ¡£å¤„ç†çš„è§†è§‰ä¸­å¿ƒå¯¹æ¯”å­¦ä¹ ï¼ˆVC2Lï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åƒç´ ç©ºé—´å»ºæ¨¡æ–‡æœ¬ã€å›¾åƒåŠå…¶ç»„åˆçš„å•ä¸€è§†è§‰è½¬æ¢å™¨ï¼Œè§£å†³å›¾åƒä¸æ–‡æœ¬äº¤å‰ã€æ¾æ•£å¯¹é½æˆ–åµŒå…¥è§†è§‰å½¢å¼çš„å¤šæ¨¡æ€åœºæ™¯æŒ‘æˆ˜ã€‚VC2Lé€šè¿‡æ¸²æŸ“æ‰€æœ‰è¾“å…¥ï¼ˆæ— è®ºæ˜¯æ–‡æœ¬ã€è§†è§‰è¿˜æ˜¯ç»„åˆï¼‰ä¸ºå›¾åƒæ¥å·¥ä½œï¼Œæ¶ˆé™¤äº†å¯¹OCRã€æ–‡æœ¬æ ‡è®°æˆ–æ¨¡æ€èåˆç­–ç•¥çš„éœ€æ±‚ã€‚VC2Lä½¿ç”¨ç‰‡æ®µçº§å¯¹æ¯”å­¦ä¹ ç›®æ ‡æ•æ‰å¤šæ¨¡æ€ç½‘é¡µæ–‡æ¡£ä¸­çš„å¤æ‚è·¨æ¨¡æ€å…³ç³»ï¼Œåˆ©ç”¨æ–‡æ¡£çš„å†…åœ¨è¿è´¯æ€§å¯¹é½è¿ç»­çš„å¤šæ¨¡æ€ç‰‡æ®µï¼Œæ— éœ€æ˜ç¡®é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVC2Låœ¨æå‡ºçš„åŸºå‡†æµ‹è¯•é›†å’Œç°æœ‰çš„æ•°æ®é›†ä¸Šå–å¾—äº†ç«äº‰æ€§æˆ–ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPå·²åœ¨å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤„ç†å¤æ‚ã€çœŸå®ä¸–ç•Œçš„ç½‘é¡µæ–‡æ¡£æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†Vision-Centric Contrastive Learningï¼ˆVC2Lï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ä¸€ä¸ªå•ä¸€çš„è§†è§‰è½¬æ¢å™¨æ¥å»ºæ¨¡æ–‡æœ¬ã€å›¾åƒåŠå…¶ç»„åˆã€‚</li>
<li>VC2Lå®Œå…¨åœ¨åƒç´ ç©ºé—´æ“ä½œï¼Œé€šè¿‡å°†æ‰€æœ‰è¾“å…¥æ¸²æŸ“ä¸ºå›¾åƒæ¥å·¥ä½œï¼Œç®€åŒ–äº†å¤šæ¨¡æ€æ•°æ®çš„å¤„ç†ã€‚</li>
<li>VC2Lä½¿ç”¨ç‰‡æ®µçº§å¯¹æ¯”å­¦ä¹ æ¥æ•æ‰å¤šæ¨¡æ€ç½‘é¡µæ–‡æ¡£ä¸­çš„å¤æ‚è·¨æ¨¡æ€å…³ç³»ï¼Œåˆ©ç”¨æ–‡æ¡£çš„å†…åœ¨è¿è´¯æ€§ã€‚</li>
<li>VC2Læ— éœ€æ˜ç¡®é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®ï¼Œå¢å¼ºäº†æ¨¡å‹çš„çµæ´»æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜VC2Låœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šå–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-841e9e5ed7afea95a0dcbc3e2ac6c366" align="middle">
<img src="https://picx.zhimg.com/v2-411e5147ea93358cbc2ca714659a0227" align="middle">
<img src="https://picx.zhimg.com/v2-20fd15e97ddd8948c86dc0643501ffa5" align="middle">
<img src="https://picx.zhimg.com/v2-205bbb2471ab5621ecacc4869e421b74" align="middle">
<img src="https://picx.zhimg.com/v2-de87865f883a4e632ef1c9e107d9276e" align="middle">
<img src="https://picx.zhimg.com/v2-c36ad276a0086f7d0e03c1145e25e997" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="BioCLIP-2-Emergent-Properties-from-Scaling-Hierarchical-Contrastive-Learning"><a href="#BioCLIP-2-Emergent-Properties-from-Scaling-Hierarchical-Contrastive-Learning" class="headerlink" title="BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive   Learning"></a>BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive   Learning</h2><p><strong>Authors:Jianyang Gu, Samuel Stevens, Elizabeth G Campolongo, Matthew J Thompson, Net Zhang, Jiaman Wu, Andrei Kopanev, Zheda Mai, Alexander E. White, James Balhoff, Wasila Dahdul, Daniel Rubenstein, Hilmar Lapp, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su</strong></p>
<p>Foundation models trained at scale exhibit remarkable emergent behaviors, learning new capabilities beyond their initial training objectives. We find such emergent behaviors in biological vision models via large-scale contrastive vision-language training. To achieve this, we first curate TreeOfLife-200M, comprising 214 million images of living organisms, the largest and most diverse biological organism image dataset to date. We then train BioCLIP 2 on TreeOfLife-200M to distinguish different species. Despite the narrow training objective, BioCLIP 2 yields extraordinary accuracy when applied to various biological visual tasks such as habitat classification and trait prediction. We identify emergent properties in the learned embedding space of BioCLIP 2. At the inter-species level, the embedding distribution of different species aligns closely with functional and ecological meanings (e.g., beak sizes and habitats). At the intra-species level, instead of being diminished, the intra-species variations (e.g., life stages and sexes) are preserved and better separated in subspaces orthogonal to inter-species distinctions. We provide formal proof and analyses to explain why hierarchical supervision and contrastive objectives encourage these emergent properties. Crucially, our results reveal that these properties become increasingly significant with larger-scale training data, leading to a biologically meaningful embedding space. </p>
<blockquote>
<p>å¤§è§„æ¨¡è®­ç»ƒçš„æ¨¡å‹å±•ç°å‡ºæ˜¾è‘—çš„æ¶Œç°è¡Œä¸ºï¼Œè¿™äº›è¡Œä¸ºè¶…è¶Šäº†å…¶åˆå§‹è®­ç»ƒç›®æ ‡ï¼Œå­¦ä¹ åˆ°äº†æ–°çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨é€šè¿‡å¤§è§„æ¨¡å¯¹æ¯”è§†è§‰è¯­è¨€è®­ç»ƒçš„ç”Ÿç‰©è§†è§‰æ¨¡å‹ä¸­å‘ç°äº†è¿™æ ·çš„æ¶Œç°è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆæ•´ç†äº†TreeOfLife-200Mæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«2.14äº¿ä¸ªç”Ÿç‰©ä½“å›¾åƒï¼Œæ˜¯ç›®å‰ä¸ºæ­¢æœ€å¤§ä¸”æœ€å¤šå…ƒçš„ç”Ÿç‰©ä½“å›¾åƒæ•°æ®é›†ã€‚ç„¶åæˆ‘ä»¬åœ¨TreeOfLife-200Mæ•°æ®é›†ä¸Šè®­ç»ƒBioCLIP 2ä»¥åŒºåˆ†ä¸åŒçš„ç‰©ç§ã€‚å°½ç®¡è®­ç»ƒç›®æ ‡è¾ƒä¸ºå•ä¸€ï¼Œä½†BioCLIP 2åœ¨åº”ç”¨äºå„ç§ç”Ÿç‰©è§†è§‰ä»»åŠ¡ï¼ˆå¦‚æ –æ¯åœ°åˆ†ç±»å’Œç‰¹å¾é¢„æµ‹ï¼‰æ—¶è¡¨ç°å‡ºæƒŠäººçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨BioCLIP 2çš„åµŒå…¥ç©ºé—´ä¸­ç¡®å®šäº†æ¶Œç°ç‰¹æ€§ã€‚åœ¨ç‰©ç§é—´å±‚é¢ï¼Œä¸åŒç‰©ç§çš„åµŒå…¥åˆ†å¸ƒä¸åŠŸèƒ½å’Œç”Ÿæ€æ„ä¹‰ç´§å¯†å¯¹é½ï¼ˆä¾‹å¦‚ï¼Œå–™çš„å¤§å°å’Œæ –æ¯åœ°ï¼‰ã€‚åœ¨ç‰©ç§å†…éƒ¨å±‚é¢ï¼Œç‰©ç§å†…çš„å˜åŒ–ï¼ˆå¦‚ç”Ÿå‘½é˜¶æ®µå’Œæ€§åˆ«ï¼‰å¹¶æœªè¢«å‰Šå¼±ï¼Œè€Œæ˜¯åœ¨ä¸ç‰©ç§é—´å·®å¼‚çš„æ­£äº¤å­ç©ºé—´ä¸­å¾—ä»¥ä¿ç•™å¹¶æ›´å¥½åœ°åˆ†ç¦»ã€‚æˆ‘ä»¬æä¾›æ­£å¼è¯æ˜å’Œåˆ†ææ¥è§£é‡Šä¸ºä»€ä¹ˆå±‚æ¬¡åŒ–ç›‘ç£å’Œå¯¹æ¯”ç›®æ ‡ä¼šä¿ƒè¿›è¿™äº›æ¶Œç°ç‰¹æ€§ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œéšç€è®­ç»ƒæ•°æ®è§„æ¨¡çš„å¢åŠ ï¼Œè¿™äº›ç‰¹æ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œä»è€Œå½¢æˆä¸€ä¸ªå…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„åµŒå…¥ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23883v2">PDF</a> NeurIPS 2025 Spotlight; Project page:   <a target="_blank" rel="noopener" href="https://imageomics.github.io/bioclip-2/">https://imageomics.github.io/bioclip-2/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è®­ç»ƒçš„æ¨¡å‹å±•ç°å‡ºä»¤äººç©ç›®çš„æ–°å…´è¡Œä¸ºï¼Œè¿™äº›è¡Œä¸ºè¶…è¶Šäº†å…¶åˆå§‹è®­ç»ƒç›®æ ‡ï¼Œå­¦ä¹ äº†æ–°çš„èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡å¤§è§„æ¨¡å¯¹æ¯”è§†è§‰è¯­è¨€è®­ç»ƒåœ¨ç”Ÿç‰©è§†è§‰æ¨¡å‹ä¸­å‘ç°æ­¤ç±»æ–°å…´è¡Œä¸ºã€‚ä¸ºè¾¾æˆè¿™ä¸€ç›®æ ‡ï¼Œé¦–å…ˆæ•´åˆäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§ä¸”æœ€å¤šå…ƒåŒ–çš„ç”Ÿç‰©æœ‰æœºä½“å›¾åƒæ•°æ®é›†TreeOfLife-200Mï¼ŒåŒ…å«2.14äº¿å¼ ç”Ÿç‰©å›¾åƒã€‚éšååœ¨TreeOfLife-200Mä¸Šè®­ç»ƒBioCLIP 2ä»¥åŒºåˆ†ä¸åŒç‰©ç§ã€‚å°½ç®¡å…¶è®­ç»ƒç›®æ ‡è¾ƒä¸ºå•ä¸€ï¼Œä½†åº”ç”¨äºå„ç§ç”Ÿç‰©è§†è§‰ä»»åŠ¡æ—¶è¡¨ç°å‡ºæƒŠäººçš„å‡†ç¡®æ€§ï¼Œå¦‚æ –æ¯åœ°åˆ†ç±»å’Œç‰¹å¾é¢„æµ‹ã€‚BioCLIP 2çš„åµŒå…¥ç©ºé—´ä¸­å­˜åœ¨æ–°å…´å±æ€§ã€‚åœ¨ç‰©ç§é—´å±‚é¢ï¼Œä¸åŒç‰©ç§çš„åµŒå…¥åˆ†å¸ƒä¸åŠŸèƒ½å’Œç”Ÿæ€æ„ä¹‰ç´§å¯†å¯¹é½ï¼›åœ¨ç‰©ç§å†…å±‚é¢ï¼Œç‰©ç§å†…å˜åŒ–ï¼ˆå¦‚ç”Ÿå‘½é˜¶æ®µå’Œæ€§åˆ«ï¼‰å¾—ä»¥ä¿ç•™ï¼Œå¹¶ä¸ç‰©ç§é—´å·®å¼‚æ­£äº¤å­ç©ºé—´æ›´å¥½åœ°åˆ†ç¦»ã€‚æœ¬æ–‡æä¾›äº†æ­£å¼è¯æ˜å’Œåˆ†æï¼Œè§£é‡Šä¸ºä½•å±‚æ¬¡ç›‘ç£å’Œå¯¹æ¯”ç›®æ ‡ä¼šä¿ƒè¿›è¿™äº›æ–°å…´å±æ€§çš„å‡ºç°ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œéšç€è®­ç»ƒæ•°æ®è§„æ¨¡çš„å¢å¤§ï¼Œè¿™äº›å±æ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œä»è€Œå½¢æˆä¸€ä¸ªå…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„åµŒå…¥ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡å¤§è§„æ¨¡å¯¹æ¯”è§†è§‰è¯­è¨€è®­ç»ƒï¼Œå‘ç°ç”Ÿç‰©è§†è§‰æ¨¡å‹ä¸­çš„æ–°å…´è¡Œä¸ºã€‚</li>
<li>æ•´åˆäº†TreeOfLife-200Mæ•°æ®é›†ï¼ŒåŒ…å«2.14äº¿å¼ ç”Ÿç‰©å›¾åƒï¼Œä¸ºè®­ç»ƒæä¾›äº†ä¸°å¯Œæ•°æ®ã€‚</li>
<li>BioCLIP 2åœ¨å¤šç§ç”Ÿç‰©è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ï¼Œå¦‚æ –æ¯åœ°åˆ†ç±»å’Œç‰¹å¾é¢„æµ‹ã€‚</li>
<li>BioCLIP 2çš„åµŒå…¥ç©ºé—´å…·æœ‰æ–°å…´å±æ€§ï¼Œç‰©ç§é—´å’Œç‰©ç§å†…çš„å·®å¼‚åœ¨å…¶ä¸­å¾—åˆ°ä½“ç°ã€‚</li>
<li>å±‚æ¬¡ç›‘ç£å’Œå¯¹æ¯”ç›®æ ‡æœ‰åŠ©äºä¿ƒè¿›æ–°å…´å±æ€§çš„å‡ºç°ã€‚</li>
<li>éšç€è®­ç»ƒæ•°æ®è§„æ¨¡çš„å¢åŠ ï¼Œè¿™äº›å±æ€§çš„é‡è¦æ€§é€æ¸å‡¸æ˜¾ï¼Œå½¢æˆå…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„åµŒå…¥ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b20388522c685da088a0816d1b9586d2" align="middle">
<img src="https://picx.zhimg.com/v2-8667cb9b90a73a91d65ec2f1a3abf986" align="middle">
<img src="https://picx.zhimg.com/v2-f83f980ae8f8e7b9e6392f1308567089" align="middle">
<img src="https://picx.zhimg.com/v2-6d8a9321406de4e6a2d6c29306ac2a83" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Multi-Task-Foundation-Model-for-Wireless-Channel-Representation-Using-Contrastive-and-Masked-Autoencoder-Learning"><a href="#A-Multi-Task-Foundation-Model-for-Wireless-Channel-Representation-Using-Contrastive-and-Masked-Autoencoder-Learning" class="headerlink" title="A Multi-Task Foundation Model for Wireless Channel Representation Using   Contrastive and Masked Autoencoder Learning"></a>A Multi-Task Foundation Model for Wireless Channel Representation Using   Contrastive and Masked Autoencoder Learning</h2><p><strong>Authors:Berkay Guler, Giovanni Geraci, Hamid Jafarkhani</strong></p>
<p>Current applications of self-supervised learning to wireless channel representation often borrow paradigms developed for text and image processing, without fully addressing the unique characteristics and constraints of wireless communications. To bridge this gap, we introduce ContraWiMAE, Wireless Contrastive Masked Autoencoder, a transformer-based foundation model that unifies masked reconstruction and masked contrastive learning for wireless channel representation. Our key innovation is a new wireless-inspired contrastive objective that exploits the inherent characteristics of wireless environment, including noise, fading, and partial observability, as natural augmentation. Through extensive evaluation on unseen scenarios and conditions, we demonstrate our methodâ€™s effectiveness in multiple downstream tasks, including cross-frequency beam selection, line-of-sight detection, and channel estimation. ContraWiMAE exhibits superior linear separability and adaptability in diverse wireless environments, demonstrating exceptional data efficiency and competitive performance compared with supervised baselines under challenging conditions. Comparative evaluations against a state-of-the-art wireless channel foundation model confirm the superior performance and data efficiency of our approach, highlighting its potential as a powerful baseline for future research in self-supervised wireless channel representation learning. To foster further work in this direction, we release the model weights and training pipeline for ContraWiMAE. </p>
<blockquote>
<p>å½“å‰è‡ªç›‘ç£å­¦ä¹ åœ¨æ— çº¿ä¿¡é“è¡¨ç¤ºä¸­çš„åº”ç”¨å¾€å¾€å€Ÿé‰´äº†æ–‡æœ¬å’Œå›¾åƒå¤„ç†çš„æ¨¡å¼ï¼Œå¹¶æœªå®Œå…¨è§£å†³æ— çº¿é€šä¿¡çš„ç‹¬ç‰¹ç‰¹æ€§å’Œçº¦æŸã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ContraWiMAEï¼Œå³æ— çº¿å¯¹æ¯”æ©ç è‡ªç¼–ç å™¨ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºtransformerçš„åŸºç¡€æ¨¡å‹ï¼Œç»Ÿä¸€äº†æ©ç é‡å»ºå’Œæ©ç å¯¹æ¯”å­¦ä¹ æ¥è¿›è¡Œæ— çº¿ä¿¡é“è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºä¸€ç§æ–°çš„å—æ— çº¿å¯å‘çš„å¯¹æ¯”ç›®æ ‡ï¼Œå®ƒåˆ©ç”¨æ— çº¿ç¯å¢ƒçš„å›ºæœ‰ç‰¹æ€§ï¼ŒåŒ…æ‹¬å™ªå£°ã€è¡°å‡å’Œéƒ¨åˆ†å¯è§‚æµ‹æ€§ï¼Œä½œä¸ºè‡ªç„¶å¢å¼ºã€‚é€šè¿‡å¯¹æœªè§åœºæ™¯å’Œæ¡ä»¶çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è·¨é¢‘é€‰æ³¢æŸã€è§†çº¿è·¯å¾„æ£€æµ‹å’Œä¿¡é“ä¼°è®¡ã€‚ContraWiMAEåœ¨ä¸åŒçš„æ— çº¿ç¯å¢ƒä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„çº¿æ€§å¯åˆ†æ€§å’Œé€‚åº”æ€§ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ï¼Œä¸æœ‰ç›‘ç£çš„åŸºçº¿ç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡å’Œç«äº‰æ€§èƒ½ã€‚ä¸æœ€å…ˆè¿›çš„æ— çº¿ä¿¡é“åŸºç¡€æ¨¡å‹çš„æ¯”è¾ƒè¯„ä¼°è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½å’Œæ•°æ®æ•ˆç‡ï¼Œçªæ˜¾äº†å…¶åœ¨è‡ªç›‘ç£æ— çº¿ä¿¡é“è¡¨ç¤ºå­¦ä¹ æ–¹é¢ä½œä¸ºæœªæ¥ç ”ç©¶çš„å¼ºå¤§åŸºå‡†çš„æ½œåŠ›ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€æ–¹å‘çš„ç ”ç©¶å·¥ä½œï¼Œæˆ‘ä»¬å‘å¸ƒäº†ContraWiMAEçš„æ¨¡å‹æƒé‡å’Œè®­ç»ƒæµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09160v2">PDF</a> - 17 pages, 7 figures, 5 tables - Submitted to IEEE JSAC Large AI   Models for Future Wireless Communication Systems - Some of the results will   appear in NeurIPS 2025, AI4NextG Workshop - This version is an extensive   improvement in all aspects over the previous version with the same title -   Dataset and implementation:   <a target="_blank" rel="noopener" href="https://github.com/BerkIGuler/WirelessContrastiveMaskedLearning">https://github.com/BerkIGuler/WirelessContrastiveMaskedLearning</a></p>
<p><strong>Summary</strong></p>
<p>æ— çº¿é€šé“è¡¨ç¤ºçš„è‡ªç›‘ç£å­¦ä¹ åº”ç”¨å¸¸å€Ÿé‰´æ–‡æœ¬å’Œå›¾åƒå¤„ç†çš„æ¨¡å¼ï¼Œæœªèƒ½å®Œå…¨è§£å†³æ— çº¿é€šä¿¡çš„ç‹¬ç‰¹ç‰¹æ€§å’Œçº¦æŸã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºContraWiMAEï¼ˆæ— çº¿å¯¹æ¯”æ©ç è‡ªç¼–ç å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºtransformerçš„åŸºç¡€æ¨¡å‹ï¼Œèåˆäº†æ©ç é‡å»ºå’Œæ©ç å¯¹æ¯”å­¦ä¹ æ¥è¿›è¡Œæ— çº¿é€šé“è¡¨ç¤ºã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºæ–°çš„æ— çº¿å¯å‘å¯¹æ¯”ç›®æ ‡ï¼Œåˆ©ç”¨æ— çº¿ç¯å¢ƒçš„å›ºæœ‰ç‰¹æ€§ï¼ˆå¦‚å™ªå£°ã€è¡°å‡å’Œéƒ¨åˆ†å¯è§‚æµ‹æ€§ï¼‰ä½œä¸ºè‡ªç„¶å¢å¼ºã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨é¢‘é€‰æ³¢æŸã€è§†è·æ£€æµ‹å’Œé€šé“ä¼°è®¡ç­‰å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¯æ˜äº†æœ‰æ•ˆæ€§ã€‚ContraWiMAEåœ¨ä¸åŒæ— çº¿ç¯å¢ƒä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„çº¿æ€§å¯åˆ†ç¦»æ€§å’Œé€‚åº”æ€§ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹å±•ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡å’Œç«äº‰åŠ›ã€‚ä¸æœ€æ–°çš„æ— çº¿é€šé“åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½å’Œæ•°æ®æ•ˆç‡ï¼Œçªå‡ºäº†å…¶åœ¨è‡ªç›‘ç£æ— çº¿é€šé“è¡¨ç¤ºå­¦ä¹ é¢†åŸŸä½œä¸ºæœ‰åŠ›åŸºå‡†çš„æ½œåŠ›ã€‚æˆ‘ä»¬å‘å¸ƒäº†ContraWiMAEçš„æ¨¡å‹æƒé‡å’Œè®­ç»ƒæµç¨‹ä»¥æ¨åŠ¨è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ— çº¿é€šé“è¡¨ç¤ºè‡ªç›‘ç£å­¦ä¹ éœ€è€ƒè™‘æ— çº¿é€šä¿¡çš„ç‹¬ç‰¹ç‰¹æ€§å’Œçº¦æŸã€‚</li>
<li>ContraWiMAEæ˜¯ä¸€æ¬¾åŸºäºtransformerçš„åŸºç¡€æ¨¡å‹ï¼Œèåˆæ©ç é‡å»ºå’Œæ©ç å¯¹æ¯”å­¦ä¹ è¿›è¡Œæ— çº¿é€šé“è¡¨ç¤ºã€‚</li>
<li>æ–°æå‡ºçš„æ— çº¿å¯å‘å¯¹æ¯”ç›®æ ‡åˆ©ç”¨æ— çº¿ç¯å¢ƒå›ºæœ‰ç‰¹æ€§ä½œä¸ºè‡ªç„¶å¢å¼ºã€‚</li>
<li>åœ¨è·¨é¢‘é€‰æ³¢æŸã€è§†è·æ£€æµ‹å’Œé€šé“ä¼°è®¡ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>ContraWiMAEåœ¨ä¸åŒæ— çº¿ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰¯å¥½çš„çº¿æ€§å¯åˆ†ç¦»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>ä¸å…¶ä»–æ— çº¿é€šé“åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼ŒContraWiMAEå…·æœ‰ä¼˜è¶Šæ€§èƒ½å’Œæ•°æ®æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afcd84f13a52e0c73e35984b6e817d31" align="middle">
<img src="https://picx.zhimg.com/v2-a38ad224ada3aa9082640ed2901bfc84" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ce1b3eae29aa6a9b555b2797acbfc630" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Decoding the Ear A Framework for Objectifying Expressiveness from Human   Preference Through Efficient Alignment
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3f92dffed2dd489671324f9248f5cd2c" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  A Unified Detection Pipeline for Robust Object Detection in   Fisheye-Based Traffic Surveillance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
