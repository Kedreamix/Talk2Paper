<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  ACS-SegNet An Attention-Based CNN-SegFormer Segmentation Network for   Tissue Segmentation in Histopathology">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f0ef77951381b6ebaacf4fe717e343c8')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    5.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    22 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-25-æ›´æ–°"><a href="#2025-10-25-æ›´æ–°" class="headerlink" title="2025-10-25 æ›´æ–°"></a>2025-10-25 æ›´æ–°</h1><h2 id="ACS-SegNet-An-Attention-Based-CNN-SegFormer-Segmentation-Network-for-Tissue-Segmentation-in-Histopathology"><a href="#ACS-SegNet-An-Attention-Based-CNN-SegFormer-Segmentation-Network-for-Tissue-Segmentation-in-Histopathology" class="headerlink" title="ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for   Tissue Segmentation in Histopathology"></a>ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for   Tissue Segmentation in Histopathology</h2><p><strong>Authors:Nima Torbati, Anastasia Meshcheryakova, Ramona Woitek, Diana Mechtcheriakova, Amirreza Mahbod</strong></p>
<p>Automated histopathological image analysis plays a vital role in computer-aided diagnosis of various diseases. Among developed algorithms, deep learning-based approaches have demonstrated excellent performance in multiple tasks, including semantic tissue segmentation in histological images. In this study, we propose a novel approach based on attention-driven feature fusion of convolutional neural networks (CNNs) and vision transformers (ViTs) within a unified dual-encoder model to improve semantic segmentation performance. Evaluation on two publicly available datasets showed that our model achieved {\mu}IoU&#x2F;{\mu}Dice scores of 76.79%&#x2F;86.87% on the GCPS dataset and 64.93%&#x2F;76.60% on the PUMA dataset, outperforming state-of-the-art and baseline benchmarks. The implementation of our method is publicly available in a GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/NimaTorbati/ACS-SegNet">https://github.com/NimaTorbati/ACS-SegNet</a> </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†æåœ¨è®¡ç®—æœºè¾…åŠ©å¤šç§ç–¾ç—…çš„è¯Šæ–­ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚åœ¨å·²å¼€å‘çš„ç®—æ³•ä¸­ï¼Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­çš„è¯­ä¹‰ç»„ç»‡åˆ†å‰²ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç»Ÿä¸€åŒç¼–ç å™¨æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›é©±åŠ¨ç‰¹å¾èåˆçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„æ–°å‹æ–¹æ³•ï¼Œä»¥æé«˜è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨GCPSæ•°æ®é›†ä¸Šè¾¾åˆ°äº†Î¼IoU&#x2F;Î¼Diceåˆ†æ•°ä¸º76.79%&#x2F;86.87%ï¼Œåœ¨PUMAæ•°æ®é›†ä¸Šä¸º64.93%&#x2F;76.60%ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›å’ŒåŸºå‡†æµ‹è¯•çš„åŸºå‡†çº¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å®ç°å¯ä»¥åœ¨GitHubä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/NimaTorbati/ACS-SegNet">https://github.com/NimaTorbati/ACS-SegNet</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20754v1">PDF</a> 5 pages</p>
<p><strong>Summary</strong><br>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›é©±åŠ¨ç‰¹å¾èåˆçš„ç»Ÿä¸€åŒç¼–ç å™¨æ¨¡å‹ï¼Œç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨GCPSæ•°æ®é›†ä¸Šè¾¾åˆ°äº†Î¼IoU&#x2F;Î¼Diceåˆ†æ•°ä¸º76.79%&#x2F;86.87%ï¼Œåœ¨PUMAæ•°æ®é›†ä¸Šä¸º64.93%&#x2F;76.60%ï¼Œè¶…è¿‡äº†å…ˆè¿›åŸºå‡†æµ‹è¯•å’ŒåŸºçº¿ã€‚è¯¥æ–¹æ³•çš„å…·ä½“å®ç°å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ­¤ç ”ç©¶å¼ºè°ƒäº†è‡ªåŠ¨åŒ–ç—…ç†å›¾åƒåˆ†æåœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹ç®—æ³•ï¼Œç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ï¼ŒåŸºäºæ³¨æ„åŠ›é©±åŠ¨ç‰¹å¾èåˆçš„ç»Ÿä¸€åŒç¼–ç å™¨æ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>åœ¨GCPSå’ŒPUMAä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜æ•ˆçš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚å…¶åœ¨GCPSæ•°æ®é›†ä¸Šçš„Î¼IoU&#x2F;Î¼Diceåˆ†æ•°é«˜äºç°æœ‰æŠ€æœ¯ï¼Œè¡¨æ˜å…¶ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>æ­¤æ–¹æ³•çš„å®ç°ç»†èŠ‚å·²è¢«å…¬å¼€åˆ†äº«åœ¨GitHubä¸Šï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…è¿›è¡Œå‚è€ƒå’Œä½¿ç”¨ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨ç—…ç†å›¾åƒåˆ†æä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯ç»“åˆäº†CNNå’ŒViTsçš„æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8df280725fa3cf3fc46045b12d7058ae" align="middle">
<img src="https://picx.zhimg.com/v2-82a6b01ff1ec9a33c80e5b221cd8688c" align="middle">
<img src="https://picx.zhimg.com/v2-5c85c32d5beeda3bdfc1d86884336cfa" align="middle">
<img src="https://picx.zhimg.com/v2-2840e0ecb0ecac9c4bed2047fee8a414" align="middle">
<img src="https://picx.zhimg.com/v2-f0ef77951381b6ebaacf4fe717e343c8" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Transformed-Multi-view-3D-Shape-Features-with-Contrastive-Learning"><a href="#Transformed-Multi-view-3D-Shape-Features-with-Contrastive-Learning" class="headerlink" title="Transformed Multi-view 3D Shape Features with Contrastive Learning"></a>Transformed Multi-view 3D Shape Features with Contrastive Learning</h2><p><strong>Authors:MÃ¡rcus VinÃ­cius Lobo Costa, Sherlon Almeida da Silva, BÃ¡rbara Caroline Benato, Leo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti</strong></p>
<p>This paper addresses the challenges in representation learning of 3D shape features by investigating state-of-the-art backbones paired with both contrastive supervised and self-supervised learning objectives. Computer vision methods struggle with recognizing 3D objects from 2D images, often requiring extensive labeled data and relying on Convolutional Neural Networks (CNNs) that may overlook crucial shape relationships. Our work demonstrates that Vision Transformers (ViTs) based architectures, when paired with modern contrastive objectives, achieve promising results in multi-view 3D analysis on our downstream tasks, unifying contrastive and 3D shape understanding pipelines. For example, supervised contrastive losses reached about 90.6% accuracy on ModelNet10. The use of ViTs and contrastive learning, leveraging ViTsâ€™ ability to understand overall shapes and contrastive learningâ€™s effectiveness, overcomes the need for extensive labeled data and the limitations of CNNs in capturing crucial shape relationships. The success stems from capturing global shape semantics via ViTs and refining local discriminative features through contrastive optimization. Importantly, our approach is empirical, as it is grounded on extensive experimental evaluation to validate the effectiveness of combining ViTs with contrastive objectives for 3D representation learning. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡æ¢ç©¶æœ€å‰æ²¿çš„éª¨å¹²ç½‘ç»œä¸å¯¹æ¯”ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ç›®æ ‡çš„é…å¯¹ï¼Œè§£å†³3Då½¢çŠ¶ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ä¸­çš„æŒ‘æˆ˜ã€‚è®¡ç®—æœºè§†è§‰æ–¹æ³•åœ¨è¯†åˆ«2Då›¾åƒä¸­çš„3Dç‰©ä½“æ—¶é¢ä¸´å›°éš¾ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼Œå¹¶ä¾èµ–äºå¯èƒ½å¿½ç•¥å…³é”®å½¢çŠ¶å…³ç³»çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œå½“åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ¶æ„ä¸ç°ä»£å¯¹æ¯”ç›®æ ‡ç›¸ç»“åˆæ—¶ï¼Œå®ƒä»¬åœ¨æˆ‘ä»¬çš„ä¸‹æ¸¸ä»»åŠ¡çš„å¤šè§†å›¾3Dåˆ†æä¸­å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œç»Ÿä¸€äº†å¯¹æ¯”å’Œ3Då½¢çŠ¶ç†è§£æµç¨‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ModelNet10ä¸Šï¼Œç›‘ç£å¯¹æ¯”æŸå¤±è¾¾åˆ°äº†çº¦90.6%çš„å‡†ç¡®ç‡ã€‚ä½¿ç”¨ViTå’Œå¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨ViTç†è§£æ•´ä½“å½¢çŠ¶çš„èƒ½åŠ›å’Œå¯¹æ¯”å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œå…‹æœäº†éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„éœ€è¦ä»¥åŠCNNåœ¨æ•æ‰å…³é”®å½¢çŠ¶å…³ç³»ä¸Šçš„å±€é™æ€§ã€‚æˆåŠŸçš„åŸå› åœ¨äºé€šè¿‡ViTæ•è·å…¨å±€å½¢çŠ¶è¯­ä¹‰ï¼Œå¹¶é€šè¿‡å¯¹æ¯”ä¼˜åŒ–ç²¾ç‚¼å±€éƒ¨åˆ¤åˆ«ç‰¹å¾ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯å®è¯çš„ï¼Œå®ƒåŸºäºå¹¿æ³›çš„å®éªŒè¯„ä¼°ï¼Œä»¥éªŒè¯å°†ViTä¸å¯¹æ¯”ç›®æ ‡ç›¸ç»“åˆè¿›è¡Œ3Dè¡¨ç¤ºå­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19955v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å…ˆè¿›çš„éª¨å¹²ç½‘ç»œæ­é…å¯¹æ¯”ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ç›®æ ‡è¿›è¡Œ3Då½¢çŠ¶ç‰¹å¾è¡¨ç¤ºå­¦ä¹ çš„æŒ‘æˆ˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè®¡ç®—æœºè§†è§‰æ–¹æ³•åœ¨è¯†åˆ«2Då›¾åƒä¸­çš„3Dç‰©ä½“æ—¶å­˜åœ¨å›°éš¾ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®å¹¶ä¾èµ–å¯èƒ½å¿½ç•¥å…³é”®å½¢çŠ¶å…³ç³»çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ¶æ„åœ¨æ­é…ç°ä»£å¯¹æ¯”ç›®æ ‡æ—¶ï¼Œåœ¨å¤šè§†å›¾3Dåˆ†æä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœï¼Œç»Ÿä¸€äº†å¯¹æ¯”å’Œ3Då½¢çŠ¶ç†è§£ç®¡é“ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡ç›‘ç£å¯¹æ¯”æŸå¤±ï¼ŒModelNet10çš„å‡†ç¡®ç‡è¾¾åˆ°äº†çº¦90.6%ã€‚æœ¬ç ”ç©¶åˆ©ç”¨ViTsçš„ç†è§£å’Œå¯¹æ¯”å­¦ä¹ çš„æœ‰æ•ˆæ€§ï¼Œå…‹æœäº†éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®å’ŒCNNåœ¨æ•æ‰å…³é”®å½¢çŠ¶å…³ç³»æ–¹é¢çš„å±€é™æ€§ã€‚å…¶æˆåŠŸæºäºViTæ•è·å…¨å±€å½¢çŠ¶è¯­ä¹‰å¹¶é€šè¿‡å¯¹æ¯”ä¼˜åŒ–ç»†åŒ–å±€éƒ¨åˆ¤åˆ«ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å…ˆè¿›çš„éª¨å¹²ç½‘ç»œä¸å¯¹æ¯”ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ç›®æ ‡åœ¨3Då½¢çŠ¶ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ä¸­çš„ç»“åˆåº”ç”¨ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰åœ¨è¯†åˆ«2Då›¾åƒä¸­çš„3Dç‰©ä½“æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰æ¶æ„åœ¨é…åˆç°ä»£å¯¹æ¯”ç›®æ ‡æ—¶ï¼Œå¯¹å¤šè§†å›¾3Dåˆ†æå±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¯¹æ¯”æŸå¤±ï¼ŒModelNet10çš„å‡†ç¡®ç‡è¾¾åˆ°äº†çº¦90.6%ã€‚</li>
<li>ViTsä¸å¯¹æ¯”å­¦ä¹ ç›¸ç»“åˆï¼Œå…‹æœäº†æ ‡æ³¨æ•°æ®çš„éœ€æ±‚å’ŒCNNæ•æ‰å…³é”®å½¢çŠ¶å…³ç³»çš„å±€é™æ€§ã€‚</li>
<li>ViTèƒ½å¤Ÿæ•è·å…¨å±€å½¢çŠ¶è¯­ä¹‰ï¼Œå¹¶é€šè¿‡å¯¹æ¯”ä¼˜åŒ–ç»†åŒ–å±€éƒ¨åˆ¤åˆ«ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3a2799cbd5c2749451f2efd5ad42631" align="middle">
<img src="https://picx.zhimg.com/v2-2971ab0648a129187ef162b81705f581" align="middle">
<img src="https://picx.zhimg.com/v2-d015ce38aae48fdea63d08706fa655dc" align="middle">
<img src="https://picx.zhimg.com/v2-6f3aa4f2ed37cd38668730757ce7852e" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OmniVIC-A-Self-Improving-Variable-Impedance-Controller-with-Vision-Language-In-Context-Learning-for-Safe-Robotic-Manipulation"><a href="#OmniVIC-A-Self-Improving-Variable-Impedance-Controller-with-Vision-Language-In-Context-Learning-for-Safe-Robotic-Manipulation" class="headerlink" title="OmniVIC: A Self-Improving Variable Impedance Controller with   Vision-Language In-Context Learning for Safe Robotic Manipulation"></a>OmniVIC: A Self-Improving Variable Impedance Controller with   Vision-Language In-Context Learning for Safe Robotic Manipulation</h2><p><strong>Authors:Heng Zhang, Wei-Hsing Huang, Gokhan Solak, Arash Ajoudani</strong></p>
<p>We present OmniVIC, a universal variable impedance controller (VIC) enhanced by a vision language model (VLM), which improves safety and adaptation in any contact-rich robotic manipulation task to enhance safe physical interaction. Traditional VIC have shown advantages when the robot physically interacts with the environment, but lack generalization in unseen, complex, and unstructured safe interactions in universal task scenarios involving contact or uncertainty. To this end, the proposed OmniVIC interprets task context derived reasoning from images and natural language and generates adaptive impedance parameters for a VIC controller. Specifically, the core of OmniVIC is a self-improving Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG retrieves relevant prior experiences from a structured memory bank to inform the controller about similar past tasks, and ICL leverages these retrieved examples and the prompt of current task to query the VLM for generating context-aware and adaptive impedance parameters for the current manipulation scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in universal task scenarios. The impedance parameter regulation is further informed by real-time force&#x2F;torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms baselines on a suite of complex contact-rich tasks, both in simulation and on real-world robotic tasks, with improved success rates and reduced force violations. OmniVIC takes a step towards bridging high-level semantic reasoning and low-level compliant control, enabling safer and more generalizable manipulation. Overall, the average success rate increases from 27% (baseline) to 61.4% (OmniVIC). </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†OmniVICï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¢å¼ºçš„é€šç”¨å¯å˜é˜»æŠ—æ§åˆ¶å™¨ï¼ˆVICï¼‰ï¼Œå®ƒæé«˜äº†ä»»ä½•æ¥è§¦ä¸°å¯Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å®‰å…¨æ€§å’Œé€‚åº”æ€§ï¼Œä»è€Œå¢å¼ºäº†å®‰å…¨ç‰©ç†äº¤äº’ã€‚ä¼ ç»ŸVICåœ¨æœºå™¨äººä¸ç¯å¢ƒè¿›è¡Œç‰©ç†äº¤äº’æ—¶æ˜¾ç¤ºå‡ºä¼˜åŠ¿ï¼Œä½†åœ¨æ¶‰åŠæ¥è§¦æˆ–ä¸ç¡®å®šæ€§çš„é€šç”¨ä»»åŠ¡åœºæ™¯ä¸­ï¼Œç¼ºä¹æœªè§ã€å¤æ‚å’Œéç»“æ„åŒ–å®‰å…¨äº¤äº’çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæ‰€æå‡ºçš„OmniVICè§£é‡Šä»å›¾åƒå’Œè‡ªç„¶è¯­è¨€ä¸­æ´¾ç”Ÿçš„ä»»åŠ¡ä¸Šä¸‹æ–‡æ¨ç†ï¼Œå¹¶ä¸ºVICæ§åˆ¶å™¨ç”Ÿæˆè‡ªé€‚åº”é˜»æŠ—å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼ŒOmniVICçš„æ ¸å¿ƒæ˜¯è‡ªæˆ‘å®Œå–„çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œå…¶ä¸­RAGä»ç»“æ„åŒ–è®°å¿†åº“ä¸­æ£€ç´¢ç›¸å…³å…ˆéªŒç»éªŒï¼Œä»¥å‘æ§åˆ¶å™¨æä¾›ç±»ä¼¼è¿‡å»ä»»åŠ¡çš„ä¿¡æ¯ï¼Œè€ŒICLåˆ™åˆ©ç”¨è¿™äº›æ£€ç´¢åˆ°çš„ç¤ºä¾‹å’Œå½“å‰ä»»åŠ¡çš„æç¤ºæ¥æŸ¥è¯¢VLMï¼Œä»¥ç”Ÿæˆå½“å‰æ“ä½œåœºæ™¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œè‡ªé€‚åº”é˜»æŠ—å‚æ•°ã€‚å› æ­¤ï¼Œè‡ªæˆ‘å®Œå–„çš„RAGå’ŒICLä¿è¯äº†OmniVICåœ¨é€šç”¨ä»»åŠ¡åœºæ™¯ä¸­çš„è¿è¡Œã€‚é˜»æŠ—å‚æ•°è°ƒèŠ‚è¿˜å—åˆ°å®æ—¶åŠ›&#x2F;æ‰­çŸ©åé¦ˆçš„å¯å‘ï¼Œä»¥ç¡®ä¿äº¤äº’åŠ›ä¿æŒåœ¨å®‰å…¨é˜ˆå€¼å†…ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€ç³»åˆ—å¤æ‚çš„æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ä¸­è¶…è¶Šäº†åŸºå‡†çº¿ï¼Œæ— è®ºæ˜¯åœ¨æ¨¡æ‹Ÿè¿˜æ˜¯åœ¨ç°å®ä¸–ç•Œçš„æœºå™¨äººä»»åŠ¡ä¸­ï¼Œéƒ½æé«˜äº†æˆåŠŸç‡å¹¶å‡å°‘äº†åŠ›è¿è§„ã€‚OmniVICæœç€å¼¥åˆé«˜çº§è¯­ä¹‰æ¨ç†å’Œä½çº§é¡ºåº”æ§åˆ¶çš„æ–¹å‘è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œä½¿æ“ä½œæ›´å®‰å…¨ã€æ›´å…·æ³›åŒ–èƒ½åŠ›ã€‚æ€»ä½“è€Œè¨€ï¼ŒæˆåŠŸç‡ä»åŸºå‡†çº¿çš„27%æé«˜åˆ°OmniVICçš„61.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17150v2">PDF</a> Code, video and RAG dataset are available at   \url{<a target="_blank" rel="noopener" href="https://sites.google.com/view/omni-vic%7D">https://sites.google.com/view/omni-vic}</a></p>
<p><strong>Summary</strong></p>
<p>OmniVICæ˜¯ä¸€ä¸ªç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é€šç”¨å¯å˜é˜»æŠ—æ§åˆ¶å™¨ï¼ˆVICï¼‰ï¼Œæ—¨åœ¨æé«˜æ¥è§¦ä¸°å¯Œçš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å®‰å…¨æ€§å’Œé€‚åº”æ€§ï¼Œå¢å¼ºå®‰å…¨ç‰©ç†äº¤äº’ã€‚OmniVICé€šè¿‡å›¾åƒå’Œè‡ªç„¶è¯­è¨€çš„ä¸Šä¸‹æ–‡ç†è§£æ¥ç”Ÿæˆé€‚åº”æ€§é˜»æŠ—å‚æ•°ï¼Œé€‚ç”¨äºé€šç”¨ä»»åŠ¡åœºæ™¯ä¸­çš„æœªçŸ¥ã€å¤æ‚å’Œéç»“æ„åŒ–å®‰å…¨äº¤äº’ã€‚å…¶æ ¸å¿ƒæ˜¯è‡ªæˆ‘æ”¹è¿›çš„å›å¿†å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œèƒ½å¤Ÿä»ç»“æ„åŒ–è®°å¿†åº“ä¸­æ£€ç´¢ç›¸å…³å…ˆéªŒç»éªŒï¼Œå¹¶åˆ©ç”¨å½“å‰ä»»åŠ¡çš„æç¤ºæ¥æŸ¥è¯¢VLMï¼Œç”Ÿæˆé€‚ç”¨äºå½“å‰æ“ä½œåœºæ™¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œè‡ªé€‚åº”é˜»æŠ—å‚æ•°ã€‚æ­¤å¤–ï¼ŒOmniVICè¿˜é€šè¿‡å®æ—¶åŠ›&#x2F;æ‰­çŸ©åé¦ˆæ¥è°ƒèŠ‚é˜»æŠ—å‚æ•°ï¼Œç¡®ä¿äº¤äº’åŠ›åœ¨å®‰å…¨é˜ˆå€¼å†…ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸Šçš„å¤æ‚æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡æ¼”ç¤ºè¡¨æ˜ï¼ŒOmniVICçš„æ–¹æ³•ä¼˜äºåŸºçº¿ï¼ŒæˆåŠŸç‡æé«˜ï¼ŒåŠ›è¿è§„å‡å°‘ã€‚OmniVICæœç€å¼¥åˆé«˜çº§è¯­ä¹‰æ¨ç†å’Œä½çº§é¡ºä»æ§åˆ¶çš„æ–¹å‘è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä½¿æ“ä½œæ›´å®‰å…¨ã€æ›´å…·é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniVICç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸å¯å˜é˜»æŠ—æ§åˆ¶å™¨ï¼ˆVICï¼‰ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å®‰å…¨æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>OmniVICé€šè¿‡è§£è¯»ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆå›¾åƒå’Œè‡ªç„¶è¯­è¨€ï¼‰æ¥ç”Ÿæˆè‡ªé€‚åº”é˜»æŠ—å‚æ•°ï¼Œé€‚ç”¨äºå¤æ‚ã€éç»“æ„åŒ–çš„å®‰å…¨äº¤äº’åœºæ™¯ã€‚</li>
<li>OmniVICçš„æ ¸å¿ƒæ˜¯è‡ªæˆ‘æ”¹è¿›çš„å›å¿†å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œèƒ½ä»ç»“æ„åŒ–è®°å¿†åº“ä¸­æ£€ç´¢ç›¸å…³å…ˆéªŒç»éªŒå¹¶æŸ¥è¯¢VLMä»¥ç”Ÿæˆé€‚åº”æ€§çš„é˜»æŠ—å‚æ•°ã€‚</li>
<li>OmniVICé€šè¿‡å®æ—¶åŠ›&#x2F;æ‰­çŸ©åé¦ˆè°ƒèŠ‚é˜»æŠ—å‚æ•°ï¼Œç¡®ä¿äº¤äº’åŠ›åœ¨å®‰å…¨èŒƒå›´å†…ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººä»»åŠ¡ä¸Šè¿›è¡Œçš„å¤æ‚æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡æ¼”ç¤ºè¡¨æ˜ï¼ŒOmniVICä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæé«˜äº†æˆåŠŸç‡å¹¶å‡å°‘äº†åŠ›è¿è§„ã€‚</li>
<li>OmniVICæé«˜äº†æœºå™¨äººæ“ä½œçš„å®‰å…¨æ€§å¹¶å¢å¼ºäº†å…¶é€šç”¨æ€§ï¼Œæœç€å¼¥åˆé«˜çº§è¯­ä¹‰æ¨ç†å’Œä½çº§æ§åˆ¶çš„æ–¹å‘è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8f21d6b828ca8b4fbcd176d4dea80cf" align="middle">
<img src="https://picx.zhimg.com/v2-35126a15775abf08a44bfb9966140954" align="middle">
<img src="https://picx.zhimg.com/v2-36efdb86b680f996657b5ecc0c517efa" align="middle">
<img src="https://picx.zhimg.com/v2-78af0e3ae5070d5e827ea6f9132f779c" align="middle">
<img src="https://picx.zhimg.com/v2-750967ccd19cd4eadc59705de6c72fa7" align="middle">
<img src="https://picx.zhimg.com/v2-983c67ca1e07e02e9f46e9114aafcd75" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning"><a href="#VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning" class="headerlink" title="VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"></a>VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</h2><p><strong>Authors:Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information or designing complex semantic fusion modules. However, they still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL">https://github.com/peacelwh/VT-FSL</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨ä»ä»…æœ‰çš„å°‘é‡æ ‡è®°æ ·æœ¬ä¸­è¯†åˆ«å‡ºæ–°æ¦‚å¿µã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡èå…¥é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚çš„è¯­ä¹‰èåˆæ¨¡å—æ¥æå‡æ”¯æ’‘ç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åœ¨å®é™…å®ä¾‹ä¸­çš„å®šä½ï¼Œå®ƒä»¬ä»ç„¶ä¼šå‡ºç°ä¸è§†è§‰è¯æ®ç›¸çŸ›ç›¾çš„å¹»è§‰è¯­ä¹‰ï¼Œå¯¼è‡´äº§ç”Ÿå˜ˆæ‚çš„æŒ‡å¯¼å’Œæ˜‚è´µçš„ä¿®æ­£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†è§‰ä¸æ–‡æœ¬èåˆå°‘é‡å­¦ä¹ ï¼ˆVT-FSLï¼‰ï¼Œå®ƒæ„å»ºç²¾ç¡®çš„è·¨æ¨¡æ€æç¤ºï¼Œè¿™äº›æç¤ºä»¥å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ”¯æ’‘å›¾åƒä¸ºæ¡ä»¶ï¼Œé€šè¿‡å‡ ä½•æ„ŸçŸ¥å¯¹é½æ— ç¼é›†æˆã€‚å®ƒä¸»è¦ç”±è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ç»„æˆã€‚å…·ä½“è€Œè¨€ï¼ŒCIPä»¥ç±»åå’Œæ”¯æ’‘å›¾åƒä¸ºæ¡ä»¶å¯¹LLMè¿›è¡Œè®­ç»ƒï¼Œä»¥ç”Ÿæˆç²¾ç¡®çš„ç±»æè¿°ï¼Œå¹¶é€šè¿‡å•ä¸€ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹è¿›è¡Œè¿­ä»£ã€‚è¿™äº›æè¿°ä¸ä»…ä¸°å¯Œäº†å¯¹æ–°é¢–ç±»çš„è¯­ä¹‰ç†è§£ï¼Œè¿˜å®ç°äº†è¯­ä¹‰ä¸€è‡´å›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚è¿™äº›æè¿°å’Œåˆæˆå›¾åƒåˆ†åˆ«ä½œä¸ºè¡¥å……çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§ç±»è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æ’‘æ•°æ®ã€‚æ­¤å¤–ï¼ŒCGAé€šè¿‡æœ€å°åŒ–å®ƒä»¬æ‰€è·¨è¶Šçš„3ç»´å¹³è¡Œå››è¾¹å½¢çš„æ ¸åŒ–ä½“ç§¯æ¥è”åˆå¯¹é½èåˆçš„æ–‡æœ¬ã€æ”¯æ’‘å’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚å®ƒæ•æ‰äº†æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°äº†ç»“æ„åŒ–å’Œä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚æ‰€æå‡ºçš„VT-FSLæ–¹æ³•åœ¨åŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç»†ç²’åº¦å°‘é‡å­¦ä¹ åœºæ™¯åœ¨å†…çš„åä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/peacelwh/VT-FSLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25033v3">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è·¨æ¨¡æ€å­¦ä¹ æ¡†æ¶â€”â€”VT-FSLï¼Œæ—¨åœ¨è§£å†³å°æ ·æœ¬å­¦ä¹ ä¸­çš„è¯­ä¹‰ç†è§£ä¸è§†è§‰è¯æ®ä¸ä¸€è‡´çš„é—®é¢˜ã€‚å®ƒé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ”¯æŒå›¾åƒï¼Œæ„å»ºç²¾ç¡®çš„è·¨æ¨¡æ€æç¤ºï¼Œå¹¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥å¯¹é½è¿›è¡Œæ— ç¼é›†æˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬è·¨æ¨¡æ€è¿­ä»£æç¤ºå’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ä¸¤éƒ¨åˆ†ï¼Œæ—¨åœ¨å¢å¼ºå¯¹æ–°å‹ç±»åˆ«çš„è¯­ä¹‰ç†è§£å¹¶åˆæˆè¯­ä¹‰ä¸€è‡´çš„å›¾åƒï¼Œä»¥å¼¥è¡¥æ”¯æŒæ•°æ®çš„ä¸è¶³ã€‚VT-FSLæ–¹æ³•åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VT-FSLæ¡†æ¶æ—¨åœ¨è§£å†³å°æ ·æœ¬å­¦ä¹ ä¸­ç”±äºç¼ºä¹å®é™…å®ä¾‹è€Œå¯¼è‡´çš„è¯­ä¹‰ä¸è§†è§‰è¯æ®ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>VT-FSLç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ”¯æŒå›¾åƒï¼Œæ„å»ºç²¾ç¡®çš„è·¨æ¨¡æ€æç¤ºã€‚</li>
<li>è·¨æ¨¡æ€è¿­ä»£æç¤ºèƒ½å¤Ÿç”Ÿæˆç²¾ç¡®ç±»åˆ«æè¿°ï¼Œä¸°å¯Œå¯¹æ–°å‹ç±»åˆ«çš„è¯­ä¹‰ç†è§£ï¼Œå¹¶åˆæˆè¯­ä¹‰ä¸€è‡´çš„å›¾åƒã€‚</li>
<li>åˆæˆå›¾åƒå’Œæ”¯æŒæ•°æ®ä¸ºå­¦ä¹ æä¾›äº’è¡¥çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æŒæ•°æ®ã€‚</li>
<li>è·¨æ¨¡æ€å‡ ä½•å¯¹é½é€šè¿‡æœ€å°åŒ–æ‰€æœ‰è¡¨ç¤ºæ‰€è·¨è¶Šçš„ä¸‰ç»´å¹³è¡Œå››è¾¹å½¢çš„å†…æ ¸ä½“ç§¯æ¥è”åˆå¯¹é½æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚</li>
<li>VT-FSLæ¡†æ¶èƒ½å¤Ÿæ•æ‰æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°ç»“æ„åŒ–å’Œä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚</li>
<li>VT-FSLæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç»†ç²’åº¦å°æ ·æœ¬å­¦ä¹ åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-816112b9892e4b477a73cc6c7f04e089" align="middle">
<img src="https://picx.zhimg.com/v2-b261842883675a1c84bef073d220ab93" align="middle">
<img src="https://picx.zhimg.com/v2-fda6a8e6593952cbc22784b0fba62ed6" align="middle">
<img src="https://picx.zhimg.com/v2-76bea1641940b6f14a37bc0429de3487" align="middle">
<img src="https://picx.zhimg.com/v2-1a09d3087398134cf568e19fcf367318" align="middle">
<img src="https://picx.zhimg.com/v2-4533d12fcf2349a58782783e5e4e8bed" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ViSpec-Accelerating-Vision-Language-Models-with-Vision-Aware-Speculative-Decoding"><a href="#ViSpec-Accelerating-Vision-Language-Models-with-Vision-Aware-Speculative-Decoding" class="headerlink" title="ViSpec: Accelerating Vision-Language Models with Vision-Aware   Speculative Decoding"></a>ViSpec: Accelerating Vision-Language Models with Vision-Aware   Speculative Decoding</h2><p><strong>Authors:Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen</strong></p>
<p>Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (&lt;1.5x). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft modelâ€™s attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target modelâ€™s hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KangJialiang/ViSpec">https://github.com/KangJialiang/ViSpec</a>. </p>
<blockquote>
<p>æ¨æµ‹è§£ç æ˜¯åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ï¼Œç„¶è€Œå®ƒåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œç°æœ‰æ–¹æ³•åªå®ç°äº†é€‚åº¦çš„åŠ é€Ÿï¼ˆ&lt;1.5å€ï¼‰ã€‚éšç€å¤šæ¨¡æ€èƒ½åŠ›æˆä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„æ ¸å¿ƒï¼Œè¿™ä¸€å·®è·å˜å¾—è¶Šæ¥è¶Šæ˜¾è‘—ã€‚æˆ‘ä»¬å‡è®¾å¤§å‹VLMå¯ä»¥é€å±‚æœ‰æ•ˆåœ°è¿‡æ»¤æ‰å†—ä½™çš„å›¾åƒä¿¡æ¯ï¼Œè€Œä¸æŸå®³æ–‡æœ¬ç†è§£ï¼Œè€Œè¾ƒå°çš„è‰ç¨¿æ¨¡å‹åˆ™å¾ˆéš¾åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹VLMé‡èº«å®šåˆ¶çš„Vision-Aware Speculative Decodingï¼ˆViSpecï¼‰æ–°å‹æ¡†æ¶ã€‚ViSpecé‡‡ç”¨è½»é‡çº§çš„è§†è§‰é€‚é…å™¨æ¨¡å—ï¼Œå°†å›¾åƒä»¤ç‰Œå‹ç¼©æˆç´§å‡‘çš„è¡¨ç¤ºå½¢å¼ï¼Œæ— ç¼åœ°é›†æˆåˆ°è‰ç¨¿æ¨¡å‹çš„æ³¨æ„æœºåˆ¶ä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºæ¯ä¸ªè¾“å…¥å›¾åƒæå–å…¨å±€ç‰¹å¾å‘é‡ï¼Œå¹¶å°†å…¶å¢å¼ºåˆ°æ‰€æœ‰éšåçš„æ–‡æœ¬ä»¤ç‰Œä¸­ï¼Œä»¥æé«˜å¤šæ¨¡æ€ä¸€è‡´æ€§ã€‚ä¸ºäº†å…‹æœç¼ºä¹å…·æœ‰é•¿è¾…åŠ©å“åº”çš„å¤šæ¨¡æ€æ•°æ®é›†çš„é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡é‡æ–°åˆ©ç”¨ç°æœ‰æ•°æ®é›†å¹¶ç”Ÿæˆç›®æ ‡æ¨¡å‹çš„æ‰©å±•è¾“å‡ºæ¥åˆ¶ä½œä¸“ç”¨çš„è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„è®­ç»ƒç­–ç•¥å‡è½»äº†è‰ç¨¿æ¨¡å‹ç›´æ¥è®¿é—®ç›®æ ‡æ¨¡å‹çš„éšè—çŠ¶æ€çš„é£é™©ï¼Œå¦‚æœä»…åœ¨ç›®æ ‡æ¨¡å‹è¾“å‡ºä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½å¯¼è‡´æ·å¾„å­¦ä¹ ã€‚å¤§é‡å®éªŒéªŒè¯äº†ViSpecçš„æœ‰æ•ˆæ€§ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯VLMæ¨æµ‹è§£ç ä¸­çš„é¦–æ¬¡å®è´¨æ€§åŠ é€Ÿã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KangJialiang/ViSpec">https://github.com/KangJialiang/ViSpec</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15235v5">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ–°çš„åŠ é€Ÿæ¨æ–­æ–¹æ³•â€”â€”Vision-Aware Speculative Decodingï¼ˆViSpecï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥è½»é‡çº§è§†è§‰é€‚é…å™¨æ¨¡å—æ¥å‹ç¼©å›¾åƒæ ‡è®°ä¸ºç´§å‡‘è¡¨ç¤ºï¼Œå¹¶å°†å…¶æ— ç¼é›†æˆåˆ°è‰æ¡ˆæ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å›¾åƒçš„ä½ç½®ä¿¡æ¯ã€‚åŒæ—¶ï¼Œä¸ºæ¯ä¸ªè¾“å…¥å›¾åƒæå–å…¨å±€ç‰¹å¾å‘é‡å¹¶å°†å…¶å¢å¼ºåˆ°æ‰€æœ‰åç»­æ–‡æœ¬æ ‡è®°ä¸­ï¼Œä»¥æé«˜å¤šæ¨¡æ€çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒViSpecåœ¨VLMæ¨æµ‹è§£ç ä¸­å®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°æœ‰è¯­è¨€æ¨¡å‹çš„æ¨æµ‹è§£ç æŠ€æœ¯åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„åº”ç”¨å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>å¤§å‹VLMsèƒ½æœ‰æ•ˆè¿‡æ»¤å†—ä½™å›¾åƒä¿¡æ¯ï¼Œè€Œå°å‹æ¨¡å‹åˆ™éš¾ä»¥åšåˆ°ã€‚</li>
<li>Vision-Aware Speculative Decoding (ViSpec) æ˜¯ä¸€ä¸ªé’ˆå¯¹VLMsçš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>ViSpecåˆ©ç”¨è½»é‡çº§è§†è§‰é€‚é…å™¨æ¨¡å—æ¥å‹ç¼©å›¾åƒæ ‡è®°ï¼Œå¹¶é›†æˆåˆ°æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚</li>
<li>ViSpecé€šè¿‡æå–å…¨å±€ç‰¹å¾å‘é‡å¢å¼ºæ–‡æœ¬ä¸å›¾åƒçš„è¿è´¯æ€§ã€‚</li>
<li>è®ºæ–‡é€šè¿‡ä½¿ç”¨ç‰¹å®šè®­ç»ƒæ•°æ®é›†è§£å†³äº†å¤šæ¨¡æ€æ•°æ®é›†çŸ­ç¼ºçš„é—®é¢˜ï¼Œé€šè¿‡é‡æ–°åˆ©ç”¨ç°æœ‰æ•°æ®é›†å¹¶ç”Ÿæˆæ‰©å±•è¾“å‡ºï¼Œä»¥ç›®æ ‡VLMå’Œä¿®æ”¹åçš„æç¤ºè¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd99d87e2c8a8e4c29db036cc15939b3" align="middle">
<img src="https://picx.zhimg.com/v2-8d62a5eca61a5112339bf9767926a06c" align="middle">
<img src="https://picx.zhimg.com/v2-ce7a09d69b42150f3387295e7d9678d5" align="middle">
<img src="https://picx.zhimg.com/v2-1e17e2f836736fc394f63c0c0d8038f4" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3f92dffed2dd489671324f9248f5cd2c" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  A Unified Detection Pipeline for Robust Object Detection in   Fisheye-Based Traffic Surveillance
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f92a631ccd013400e00730c02cafc652" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  SeViCES Unifying Semantic-Visual Evidence Consensus for Long Video   Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
