<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  A Scalable, Causal, and Energy Efficient Framework for Neural Decoding   with Spiking Neural Networks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-263f18abe5cae4cb9aaabb2353cdf736')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-25-æ›´æ–°"><a href="#2025-10-25-æ›´æ–°" class="headerlink" title="2025-10-25 æ›´æ–°"></a>2025-10-25 æ›´æ–°</h1><h2 id="A-Scalable-Causal-and-Energy-Efficient-Framework-for-Neural-Decoding-with-Spiking-Neural-Networks"><a href="#A-Scalable-Causal-and-Energy-Efficient-Framework-for-Neural-Decoding-with-Spiking-Neural-Networks" class="headerlink" title="A Scalable, Causal, and Energy Efficient Framework for Neural Decoding   with Spiking Neural Networks"></a>A Scalable, Causal, and Energy Efficient Framework for Neural Decoding   with Spiking Neural Networks</h2><p><strong>Authors:Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale</strong></p>
<p>Brain-computer interfaces (BCIs) promise to enable vital functions, such as speech and prosthetic control, for individuals with neuromotor impairments. Central to their success are neural decoders, models that map neural activity to intended behavior. Current learning-based decoding approaches fall into two classes: simple, causal models that lack generalization, or complex, non-causal models that generalize and scale offline but struggle in real-time settings. Both face a common challenge, their reliance on power-hungry artificial neural network backbones, which makes integration into real-world, resource-limited systems difficult. Spiking neural networks (SNNs) offer a promising alternative. Because they operate causally these models are suitable for real-time use, and their low energy demands make them ideal for battery-constrained environments. To this end, we introduce Spikachu: a scalable, causal, and energy-efficient neural decoding framework based on SNNs. Our approach processes binned spikes directly by projecting them into a shared latent space, where spiking modules, adapted to the timing of the input, extract relevant features; these latent representations are then integrated and decoded to generate behavioral predictions. We evaluate our approach on 113 recording sessions from 6 non-human primates, totaling 43 hours of recordings. Our method outperforms causal baselines when trained on single sessions using between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that scaling up training to multiple sessions and subjects improves performance and enables few-shot transfer to unseen sessions, subjects, and tasks. Overall, Spikachu introduces a scalable, online-compatible neural decoding framework based on SNNs, whose performance is competitive relative to state-of-the-art models while consuming orders of magnitude less energy. </p>
<blockquote>
<p>è„‘æœºæ¥å£ï¼ˆBCIsï¼‰ä¸ºå…·æœ‰ç¥ç»è¿åŠ¨éšœç¢çš„äººæä¾›äº†å®ç°é‡è¦åŠŸèƒ½ï¼ˆå¦‚è¯­è¨€å’Œå‡è‚¢æ§åˆ¶ï¼‰çš„æ‰¿è¯ºã€‚å…¶æˆåŠŸçš„å…³é”®æ˜¯ç¥ç»è§£ç å™¨ï¼Œå³èƒ½å°†ç¥ç»æ´»åŠ¨æ˜ å°„åˆ°é¢„æœŸè¡Œä¸ºçš„æ¨¡å‹ã€‚å½“å‰çš„åŸºäºå­¦ä¹ çš„è§£ç æ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼šç®€å•çš„å› æœæ¨¡å‹ç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼Œè€Œå¤æ‚çš„éå› æœæ¨¡å‹è™½ç„¶å¯ä»¥ç¦»çº¿æ³›åŒ–å’Œæ‰©å±•ï¼Œä½†åœ¨å®æ—¶ç¯å¢ƒä¸­å´è¡¨ç°ä¸ä½³ã€‚ä¸¤è€…éƒ½é¢ä¸´ä¸€ä¸ªå…±åŒçš„æŒ‘æˆ˜ï¼Œé‚£å°±æ˜¯å®ƒä»¬ä¾èµ–äºèƒ½è€—å·¨å¤§çš„äººå·¥ç¥ç»ç½‘ç»œä¸»å¹²ï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥é›†æˆåˆ°èµ„æºæœ‰é™çš„ç°å®ä¸–ç•Œä¸­ã€‚è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚å› ä¸ºè¿™äº›æ¨¡å‹ä»¥å› æœæ–¹å¼è¿è¡Œï¼Œæ‰€ä»¥é€‚åˆå®æ—¶ä½¿ç”¨ï¼Œè€Œä¸”å®ƒä»¬ä½åŠŸè€—çš„ç‰¹ç‚¹ä½¿å…¶æˆä¸ºç”µæ± å—é™ç¯å¢ƒçš„ç†æƒ³é€‰æ‹©ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Spikachuï¼šä¸€ä¸ªåŸºäºSNNçš„å¯æ‰©å±•ã€å› æœå’Œé«˜æ•ˆçš„ç¥ç»è§£ç æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç›´æ¥å°†åˆ†ç®±åçš„è„‰å†²æŠ•å½±åˆ°å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå¤„ç†ï¼Œåœ¨é‚£é‡Œï¼Œé€‚åº”è¾“å…¥æ—¶é—´çš„è„‰å†²æ¨¡å—æå–ç›¸å…³ç‰¹å¾ï¼›ç„¶åå°†è¿™äº›æ½œåœ¨è¡¨ç¤ºè¿›è¡Œé›†æˆå’Œè§£ç ï¼Œä»¥äº§ç”Ÿè¡Œä¸ºé¢„æµ‹ã€‚æˆ‘ä»¬åœ¨æ¥è‡ª6åªéäººç±»çµé•¿ç±»åŠ¨ç‰©çš„113ä¸ªè®°å½•æ—¶æ®µï¼ˆæ€»è®¡43å°æ—¶è®°å½•ï¼‰ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å½“åœ¨å•ä¸ªä¼šè¯ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°ä¼˜äºå› æœåŸºçº¿ï¼ŒåŒæ—¶æ¶ˆè€—2.26è‡³418.81å€æ›´å°‘çš„èƒ½é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯¹å¤šä¸ªä¼šè¯å’Œä¸»ä½“çš„è®­ç»ƒå¯æ‰©å±•æ€§å¯ä»¥æé«˜æ€§èƒ½ï¼Œå¹¶å¯å®ç°å°‘é‡ä¼šè¯è½¬ç§»åˆ°æœªè§è¿‡çš„ä¼šè¯ã€ä¸»ä½“å’Œä»»åŠ¡ã€‚æ€»ä½“è€Œè¨€ï¼ŒSpikachuå¼•å…¥äº†ä¸€ä¸ªåŸºäºSNNçš„å¯æ‰©å±•çš„ã€åœ¨çº¿å…¼å®¹çš„ç¥ç»è§£ç æ¡†æ¶ï¼Œå…¶æ€§èƒ½ä¸æœ€æ–°æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶æ¶ˆè€—çš„èƒ½é‡è¦å°‘å¾—å¤šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20683v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åŸºäºè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„ç¥ç»ç½‘ç»œè§£ç æ¡†æ¶Spikachuã€‚è¯¥æ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§ã€å› æœæ€§å’ŒèŠ‚èƒ½æ€§ï¼Œé€‚ç”¨äºå¤„ç†ç¥ç»æ´»åŠ¨å¹¶è½¬åŒ–ä¸ºè¡Œä¸ºé¢„æµ‹ã€‚åœ¨çŒ´å­å®éªŒæ•°æ®ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒSpikachuåœ¨èƒ½è€—è¾ƒä½çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ”¯æŒè·¨ä¼šè¯ã€è·¨ä¸»ä½“å’Œè·¨ä»»åŠ¡çš„å°‘é‡è¿ç§»å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spikachuæ˜¯ä¸€ä¸ªåŸºäºè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„ç¥ç»ç½‘ç»œè§£ç æ¡†æ¶ï¼Œé€‚ç”¨äºå¤„ç†ç¥ç»æ´»åŠ¨å¹¶è½¬åŒ–ä¸ºè¡Œä¸ºé¢„æµ‹ã€‚</li>
<li>SNNså…·æœ‰å› æœæ€§ï¼Œé€‚åˆå®æ—¶ä½¿ç”¨ï¼Œå¹¶ä¸”ä½åŠŸè€—ï¼Œé€‚åˆèµ„æºæœ‰é™çš„ç³»ç»Ÿã€‚</li>
<li>Spikachué€šè¿‡æŠ•å½±äºŒå€¼åŒ–è„‰å†²åˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼Œç„¶åè§£ç ç”Ÿæˆè¡Œä¸ºé¢„æµ‹ã€‚</li>
<li>åœ¨çŒ´å­å®éªŒæ•°æ®ä¸Šè¯„ä¼°æ˜¾ç¤ºï¼ŒSpikachuåœ¨å•ä¼šè¯è®­ç»ƒæ—¶æ¯”å› æœåŸºçº¿å…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ¶ˆè€—æ›´å°‘çš„èƒ½é‡ã€‚</li>
<li>Spikachuæ”¯æŒè·¨å¤šä¸ªä¼šè¯ã€ä¸»ä½“å’Œä»»åŠ¡çš„å°‘é‡è¿ç§»å­¦ä¹ ã€‚</li>
<li>Spikachuæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„åœ¨çº¿å…¼å®¹ç¥ç»ç½‘ç»œè§£ç æ¡†æ¶ï¼Œä¸æœ€æ–°æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93b81d730bfb6552f04fac949df753b6" align="middle">
<img src="https://picx.zhimg.com/v2-7e094e219752a528a72a8d6058116d1e" align="middle">
<img src="https://picx.zhimg.com/v2-a97933dcae8a1fb72b1051ffe6cafb13" align="middle">
<img src="https://picx.zhimg.com/v2-2cf50263b3f74317f5c8d9e07dc0e003" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BoundRL-Efficient-Structured-Text-Segmentation-through-Reinforced-Boundary-Generation"><a href="#BoundRL-Efficient-Structured-Text-Segmentation-through-Reinforced-Boundary-Generation" class="headerlink" title="BoundRL: Efficient Structured Text Segmentation through Reinforced   Boundary Generation"></a>BoundRL: Efficient Structured Text Segmentation through Reinforced   Boundary Generation</h2><p><strong>Authors:Haoyuan Li, Zhengyuan Shen, Sullam Jeoung, Yueyan Chen, Jiayu Li, Qi Zhu, Shuai Wang, Vassilis Ioannidis, Huzefa Rangwala</strong></p>
<p>As structured texts become increasingly complex across diverse domains â€“ from technical reports to generative AI prompts â€“ the need for text segmentation into semantically meaningful components becomes critical. Such texts often contain elements beyond plain language, including tables, code snippets, and placeholders, which conventional sentence- or paragraph-level segmentation methods cannot handle effectively. To address this challenge, we propose BoundRL, a novel and efficient approach that jointly performs token-level text segmentation and label prediction for long structured texts. Instead of generating complete contents for each segment, it generates only a sequence of starting tokens and reconstructs the complete contents by locating these tokens within the original texts, thereby reducing inference costs by orders of magnitude and minimizing hallucination. To adapt the model for the output format, BoundRL~performs reinforcement learning with verifiable rewards (RLVR) with a specifically designed reward that jointly optimizes document reconstruction fidelity and semantic alignment. To mitigate entropy collapse, it further constructs intermediate candidates by systematically perturbing a fraction of generated sequences of segments to create stepping stones toward higher-quality solutions. To demonstrate BoundRLâ€™s effectiveness on particularly challenging structured texts, we focus evaluation on complex prompts used for LLM applications. Experiments show that BoundRL enables small language models (1.7B parameters) to outperform few-shot prompting of much larger models. Moreover, RLVR with our designed reward yields significant improvements over supervised fine-tuning, and incorporating intermediate candidates further improves both performance and generalization. </p>
<blockquote>
<p>éšç€ç»“æ„åŒ–æ–‡æœ¬åœ¨å„ä¸ªé¢†åŸŸï¼ˆä»æŠ€æœ¯æŠ¥å‘Šåˆ°ç”Ÿæˆå¼AIæç¤ºï¼‰çš„æ—¥ç›Šå¤æ‚æ€§ï¼Œå°†æ–‡æœ¬åˆ†å‰²æˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç»„ä»¶çš„éœ€æ±‚å˜å¾—è‡³å…³é‡è¦ã€‚æ­¤ç±»æ–‡æœ¬é€šå¸¸åŒ…å«è¶…å‡ºçº¯è¯­è¨€çš„å…ƒç´ ï¼ŒåŒ…æ‹¬è¡¨æ ¼ã€ä»£ç ç‰‡æ®µå’Œå ä½ç¬¦ï¼Œä¼ ç»Ÿçš„å¥å­æˆ–æ®µè½çº§åˆ«çš„åˆ†å‰²æ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°å¤„ç†ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†BoundRLï¼Œè¿™æ˜¯ä¸€ç§è”åˆæ‰§è¡Œä»¤ç‰Œçº§åˆ«çš„æ–‡æœ¬åˆ†å‰²å’Œæ ‡ç­¾é¢„æµ‹çš„é•¿ç»“æ„åŒ–æ–‡æœ¬çš„æ–°å‹é«˜æ•ˆæ–¹æ³•ã€‚å®ƒä¸éœ€è¦ä¸ºæ¯ä¸ªæ®µç”Ÿæˆå®Œæ•´å†…å®¹ï¼Œè€Œæ˜¯åªç”Ÿæˆä¸€ç³»åˆ—èµ·å§‹ä»¤ç‰Œï¼Œå¹¶é€šè¿‡åœ¨åŸå§‹æ–‡æœ¬ä¸­å®šä½è¿™äº›ä»¤ç‰Œæ¥é‡å»ºå®Œæ•´å†…å®¹ï¼Œä»è€Œé€šè¿‡æ•°é‡çº§åœ°å‡å°‘æ¨ç†æˆæœ¬å¹¶æœ€å¤§é™åº¦åœ°å‡å°‘å¹»è§‰ã€‚ä¸ºäº†é€‚åº”è¾“å‡ºæ ¼å¼ï¼ŒBoundRLé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è¿›è¡Œå¥–åŠ±çš„ç‰¹å®šè®¾è®¡ï¼Œè”åˆä¼˜åŒ–æ–‡æ¡£é‡å»ºä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½ã€‚ä¸ºäº†å‡è½»ç†µå´©æºƒï¼Œå®ƒé€šè¿‡ç³»ç»Ÿåœ°æ‰°åŠ¨ä¸€éƒ¨åˆ†ç”Ÿæˆçš„æ®µåºåˆ—æ¥æ„å»ºä¸­é—´å€™é€‰è€…ï¼Œä»è€Œä¸ºå®ç°æ›´é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆåˆ›é€ é˜¶æ¢¯ã€‚ä¸ºäº†è¯æ˜BoundRLåœ¨å¤„ç†ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„ç»“æ„åŒ–æ–‡æœ¬ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å°†è¯„ä¼°é‡ç‚¹æ”¾åœ¨ç”¨äºLLMåº”ç”¨ç¨‹åºçš„å¤æ‚æç¤ºä¸Šã€‚å®éªŒè¡¨æ˜ï¼ŒBoundRLä½¿å°å‹è¯­è¨€æ¨¡å‹ï¼ˆ1.7Bå‚æ•°ï¼‰èƒ½å¤Ÿä¼˜äºæ›´å¤§æ¨¡å‹çš„å°‘æ•°æç¤ºã€‚æ­¤å¤–ï¼Œä½¿ç”¨æˆ‘ä»¬è®¾è®¡çš„å¥–åŠ±çš„RLVRåœ¨ç›‘ç£å¾®è°ƒæ–¹é¢å–å¾—äº†æ˜¾ç€æ”¹è¿›ï¼Œè€Œä¸­é—´å€™é€‰è€…çš„å¼•å…¥è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20151v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦æå‡ºäº†ä¸€ç§é’ˆå¯¹é•¿ç»“æ„æ–‡æœ¬è¿›è¡Œæ–‡æœ¬åˆ†å‰²å’Œæ ‡ç­¾é¢„æµ‹çš„æ–°æ–¹æ³•BoundRLã€‚å®ƒé€šè¿‡ç”Ÿæˆèµ·å§‹ä»¤ç‰Œåºåˆ—ï¼Œé‡æ„å®Œæ•´å†…å®¹ï¼Œå¤§å¹…é™ä½æ¨ç†æˆæœ¬å¹¶æœ€å°åŒ–è™šæ„å†…å®¹ã€‚é‡‡ç”¨å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è¿›è¡Œæ¨¡å‹é€‚åº”ï¼Œé€šè¿‡è®¾è®¡å¥–åŠ±æ¥ä¼˜åŒ–æ–‡æ¡£é‡æ„ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½ã€‚é€šè¿‡æ„å»ºä¸­é—´å€™é€‰æ¥å‡è½»ç†µå´©æºƒé—®é¢˜ï¼Œè¿›ä¸€æ­¥æé«˜è§£å†³æ–¹æ¡ˆè´¨é‡ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨ä¸­çš„å¤æ‚æç¤ºä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜BoundRLèƒ½æœ‰æ•ˆå¤„ç†å¤æ‚çš„ç»“æ„åŒ–æ–‡æœ¬ï¼Œå¹¶æå‡å°å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€ç»“æ„åŒ–æ–‡æœ¬çš„å¤æ‚æ€§å¢åŠ ï¼Œéœ€è¦è¿›è¡Œè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ–‡æœ¬åˆ†å‰²ã€‚</li>
<li>ä¼ ç»Ÿå¥å­æˆ–æ®µè½çº§åˆ«çš„æ–‡æœ¬åˆ†å‰²æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†åŒ…å«è¡¨æ ¼ã€ä»£ç ç‰‡æ®µå’Œå ä½ç¬¦ç­‰å…ƒç´ çš„ç»“æ„åŒ–æ–‡æœ¬ã€‚</li>
<li>BoundRLæ˜¯ä¸€ç§é’ˆå¯¹é•¿ç»“æ„æ–‡æœ¬è¿›è¡Œè”åˆæ–‡æœ¬åˆ†å‰²å’Œæ ‡ç­¾é¢„æµ‹çš„æ–°æ–¹æ³•ã€‚</li>
<li>BoundRLé€šè¿‡ç”Ÿæˆèµ·å§‹ä»¤ç‰Œåºåˆ—å’Œé‡æ„å®Œæ•´å†…å®¹æ¥é™ä½æ¨ç†æˆæœ¬å¹¶å‡å°‘è™šæ„å†…å®¹ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å’Œå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è¢«ç”¨äºæ¨¡å‹é€‚åº”ï¼Œé€šè¿‡è®¾è®¡å¥–åŠ±æ¥ä¼˜åŒ–æ–‡æ¡£é‡æ„çš„ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½ã€‚</li>
<li>é€šè¿‡æ„å»ºä¸­é—´å€™é€‰æ¥ç¼“è§£ç†µå´©æºƒé—®é¢˜ï¼Œè¿›ä¸€æ­¥æé«˜è§£å†³æ–¹æ¡ˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e473ebd422b8b249f8525dcfbd5b7830" align="middle">
<img src="https://picx.zhimg.com/v2-86837cdf62e7e82eda052f87440076db" align="middle">
<img src="https://picx.zhimg.com/v2-abd1d2352703230a621df261b9c8241c" align="middle">
<img src="https://picx.zhimg.com/v2-e7d9d8e4df8598d35c8c717c2e89643b" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Leveraging-the-Power-of-Large-Language-Models-in-Entity-Linking-via-Adaptive-Routing-and-Targeted-Reasoning"><a href="#Leveraging-the-Power-of-Large-Language-Models-in-Entity-Linking-via-Adaptive-Routing-and-Targeted-Reasoning" class="headerlink" title="Leveraging the Power of Large Language Models in Entity Linking via   Adaptive Routing and Targeted Reasoning"></a>Leveraging the Power of Large Language Models in Entity Linking via   Adaptive Routing and Targeted Reasoning</h2><p><strong>Authors:Yajie Li, Albert Galimov, Mitra Datta Ganapaneni, Pujitha Thejaswi, De Meng, Priyanshu Kumar, Saloni Potdar</strong></p>
<p>Entity Linking (EL) has traditionally relied on large annotated datasets and extensive model fine-tuning. While recent few-shot methods leverage large language models (LLMs) through prompting to reduce training requirements, they often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER (Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline that achieves high performance without deep fine-tuning by strategically combining candidate generation, context-based scoring, adaptive routing, and selective reasoning. ARTER computes a small set of complementary signals(both embedding and LLM-based) over the retrieved candidates to categorize contextual mentions into easy and hard cases. The cases are then handled by a low-computational entity linker (e.g. ReFinED) and more expensive targeted LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets, and performs comparably to pipelines using LLM-based reasoning for all mentions, while being as twice as efficient in terms of the number of LLM tokens. </p>
<blockquote>
<p>å®ä½“é“¾æ¥ï¼ˆELï¼‰ä¼ ç»Ÿä¸Šä¾èµ–äºå¤§å‹æ³¨é‡Šæ•°æ®é›†å’Œå¹¿æ³›çš„æ¨¡å‹å¾®è°ƒã€‚è™½ç„¶æœ€è¿‘çš„å°‘æ ·æœ¬æ–¹æ³•é€šè¿‡æç¤ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å‡å°‘è®­ç»ƒè¦æ±‚ï¼Œä½†å®ƒä»¬é€šå¸¸å› ä¸ºåŸºäºLLMçš„æ¨ç†è€Œé¢ä¸´æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ARTERï¼ˆè‡ªé€‚åº”è·¯ç”±å’Œé¶å‘å®ä½“æ¨ç†ï¼‰æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–ç®¡é“ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°ç»“åˆå€™é€‰ç”Ÿæˆã€åŸºäºä¸Šä¸‹æ–‡è¯„åˆ†ã€è‡ªé€‚åº”è·¯ç”±å’Œé€‰æ‹©æ€§æ¨ç†ï¼Œå®ç°äº†é«˜æ€§èƒ½è€Œæ— éœ€æ·±åº¦å¾®è°ƒã€‚ARTERè®¡ç®—ä¸€å°éƒ¨åˆ†äº’è¡¥ä¿¡å·ï¼ˆåŸºäºåµŒå…¥å’ŒLLMï¼‰ï¼Œå¯¹æ£€ç´¢åˆ°çš„å€™é€‰è¿›è¡Œæ’åºï¼Œå°†ä¸Šä¸‹æ–‡æåŠåˆ†ä¸ºç®€å•å’Œå›°éš¾ä¸¤ç§æƒ…å†µã€‚ç„¶åè¿™äº›æƒ…å†µåˆ†åˆ«ç”±ä½è®¡ç®—å®ä½“é“¾æ¥å™¨ï¼ˆä¾‹å¦‚ReFinEDï¼‰å’Œæ›´æ˜‚è´µçš„é¶å‘LLMåŸºç¡€æ¨ç†è¿›è¡Œå¤„ç†ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒARTERçš„æ€§èƒ½æ¯”ReFinEDé«˜å‡º+4.47%ï¼Œåœ¨6ä¸ªæ•°æ®é›†ä¸­çš„5ä¸ªæ•°æ®é›†ä¸Šå¹³å‡æé«˜+2.53%ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰æåŠçš„ç®¡é“ä¸­ä¸åŸºäºLLMçš„æ¨ç†ç®¡é“è¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶åœ¨LLMä»¤ç‰Œæ•°é‡æ–¹é¢æ•ˆç‡æé«˜ä¸€å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20098v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å®ä½“é“¾æ¥ï¼ˆELï¼‰ä¼ ç»Ÿä¸Šä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®é›†å’Œå¤æ‚çš„æ¨¡å‹å¾®è°ƒã€‚è€Œæœ€è¿‘çš„å°‘æ ·æœ¬æ–¹æ³•é€šè¿‡æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å‡å°‘è®­ç»ƒè¦æ±‚ï¼Œä½†å®ƒä»¬å¸¸å¸¸å› ä¸ºåŸºäºLLMçš„æ¨ç†è€Œæ•ˆç‡ä½ä¸‹ã€‚ARTERï¼ˆè‡ªé€‚åº”è·¯ç”±å’Œé’ˆå¯¹æ€§å®ä½“æ¨ç†ï¼‰æå‡ºä¸€ä¸ªç»“æ„åŒ–ç®¡é“ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°ç»“åˆå€™é€‰ç”Ÿæˆã€åŸºäºä¸Šä¸‹æ–‡æ‰“åˆ†ã€è‡ªé€‚åº”è·¯ç”±å’Œé€‰æ‹©æ€§æ¨ç†ï¼Œå®ç°é«˜æ€§èƒ½è€Œæ— éœ€æ·±åº¦å¾®è°ƒã€‚ARTERè®¡ç®—ä¸€å°éƒ¨åˆ†äº’è¡¥ä¿¡å·ï¼ˆåŒ…æ‹¬åµŒå…¥å’ŒåŸºäºLLMçš„ä¿¡å·ï¼‰å¯¹æ£€ç´¢åˆ°çš„å€™é€‰è¿›è¡Œæ’åºï¼Œå°†ä¸Šä¸‹æ–‡æåŠåˆ†ä¸ºç®€å•å’Œå›°éš¾ä¸¤ç§æƒ…å†µã€‚ç„¶åï¼Œè¿™äº›æƒ…å†µåˆ†åˆ«ç”±è®¡ç®—é‡è¾ƒä½çš„å®ä½“é“¾æ¥å™¨ï¼ˆä¾‹å¦‚ReFinEDï¼‰å’Œæ›´æ˜‚è´µçš„é’ˆå¯¹æ€§LLM-åŸºäºæ¨ç†è¿›è¡Œå¤„ç†ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒARTERæ¯”ReFinEDé«˜å‡º+4.47%ï¼Œåœ¨6ä¸ªæ•°æ®é›†ä¸­çš„5ä¸ªä¸Šå¹³å‡æå‡+2.53%ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰æåŠçš„ç®¡é“ä¸­è¡¨ç°ä¸åŸºäºLLMçš„æ¨ç†ç›¸å½“ï¼ŒåŒæ—¶åœ¨LLMä»¤ç‰Œçš„æ•ˆç‡ä¸Šæé«˜äº†ä¸¤å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARTERæå‡ºäº†ä¸€ç§æ–°çš„å®ä½“é“¾æ¥æ–¹æ³•ï¼Œç»“åˆäº†å€™é€‰ç”Ÿæˆã€ä¸Šä¸‹æ–‡æ‰“åˆ†ã€è‡ªé€‚åº”è·¯ç”±å’Œé€‰æ‹©æ€§æ¨ç†ã€‚</li>
<li>ARTERé€šè¿‡æˆ˜ç•¥æ€§åœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ç°äº†å°‘æ ·æœ¬å­¦ä¹ ï¼Œå‡å°‘äº†æ¨¡å‹å¾®è°ƒçš„éœ€æ±‚ã€‚</li>
<li>ARTERé€šè¿‡è®¡ç®—äº’è¡¥ä¿¡å·æ¥åŒºåˆ†ä¸Šä¸‹æ–‡æåŠçš„éš¾æ˜“ç¨‹åº¦ï¼Œå¹¶æ®æ­¤é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼ã€‚</li>
<li>ARTERåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºReFinEDï¼Œå¹³å‡æå‡+2.53%ï¼Œå¹¶åœ¨æŸäº›æ•°æ®é›†ä¸Šè¾¾åˆ°+4.47%çš„æå‡ã€‚</li>
<li>ARTERä¸åŸºäºLLMçš„æ¨ç†ç®¡é“ç›¸æ¯”å…·æœ‰ç›¸å½“çš„æ€§èƒ½ï¼Œä½†åœ¨æ•ˆç‡ä¸Šæœ‰æ‰€æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨LLMä»¤ç‰Œçš„åˆ©ç”¨ç‡ä¸Šã€‚</li>
<li>ARTERé€šè¿‡ç»“åˆä½è®¡ç®—å®ä½“é“¾æ¥å™¨å’Œé’ˆå¯¹æ€§LLM-åŸºäºæ¨ç†ï¼Œå®ç°äº†é«˜æ•ˆå’Œå‡†ç¡®çš„å®ä½“é“¾æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f999af592fb49daa7c93508e9818968" align="middle">
<img src="https://picx.zhimg.com/v2-857bc19cfc3f54550b9ae3cf35cfbc1c" align="middle">
<img src="https://picx.zhimg.com/v2-263f18abe5cae4cb9aaabb2353cdf736" align="middle">
<img src="https://picx.zhimg.com/v2-e72185f3951c0bf3c3e54cc7085122da" align="middle">
<img src="https://picx.zhimg.com/v2-1c4952097425bb5a6250b9100d3cec92" align="middle">
<img src="https://picx.zhimg.com/v2-c271e74f1103cd08b0018cc9de6c6242" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SEMPO-Lightweight-Foundation-Models-for-Time-Series-Forecasting"><a href="#SEMPO-Lightweight-Foundation-Models-for-Time-Series-Forecasting" class="headerlink" title="SEMPO: Lightweight Foundation Models for Time Series Forecasting"></a>SEMPO: Lightweight Foundation Models for Time Series Forecasting</h2><p><strong>Authors:Hui He, Kun Yi, Yuanchi Ma, Qi Zhang, Zhendong Niu, Guansong Pang</strong></p>
<p>The recent boom of large pre-trained models witnesses remarkable success in developing foundation models (FMs) for time series forecasting. Despite impressive performance across diverse downstream forecasting tasks, existing time series FMs possess massive network architectures and require substantial pre-training on large-scale datasets, which significantly hinders their deployment in resource-constrained environments. In response to this growing tension between versatility and affordability, we propose SEMPO, a novel lightweight foundation model that requires pretraining on relatively small-scale data, yet exhibits strong general time series forecasting. Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctral decomposition module, that substantially improves the utilization of pre-training data by modeling not only the high-energy frequency signals but also the low-energy yet informative frequency signals that are ignored in current methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns heterogeneous temporal patterns through small dataset-specific prompts and adaptively routes time series tokens to prompt-based experts for parameter-efficient model adaptation across different datasets and domains. Equipped with these modules, SEMPO significantly reduces both pre-training data scale and model size, while achieving strong generalization. Extensive experiments on two large-scale benchmarks covering 16 datasets demonstrate the superior performance of SEMPO in both zero-shot and few-shot forecasting scenarios compared with state-of-the-art methods. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/mala-lab/SEMPO">https://github.com/mala-lab/SEMPO</a>. </p>
<blockquote>
<p>è¿‘æœŸçš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çƒ­æ½®è§è¯äº†æ—¶é—´åºåˆ—é¢„æµ‹åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„æ˜¾è‘—æˆåŠŸã€‚å°½ç®¡åœ¨å„ç§ä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœï¼Œä½†ç°æœ‰çš„æ—¶é—´åºåˆ—FMså…·æœ‰å¤§è§„æ¨¡çš„ç½‘ç»œæ¶æ„ï¼Œéœ€è¦å¤§é‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ˜¾è‘—é˜»ç¢äº†å…¶éƒ¨ç½²ã€‚é’ˆå¯¹é€šç”¨æ€§å’Œå¯è´Ÿæ‹…æ€§ä¹‹é—´æ—¥ç›Šç´§å¼ çš„çŸ›ç›¾ï¼Œæˆ‘ä»¬æå‡ºäº†SEMPOï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è½»é‡çº§åŸºç¡€æ¨¡å‹ï¼Œå®ƒåªéœ€è¦åœ¨ç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½†å±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ—¶é—´åºåˆ—é¢„æµ‹èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒSEMPOåŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼š1ï¼‰èƒ½é‡æ„ŸçŸ¥çš„SpEctralåˆ†è§£æ¨¡å—ï¼Œé€šè¿‡å»ºæ¨¡ä¸ä»…é«˜èƒ½é‡é¢‘ç‡ä¿¡å·ï¼Œè¿˜åŒ…æ‹¬å½“å‰æ–¹æ³•ä¸­å¿½ç•¥çš„ä½èƒ½é‡ä½†ä¿¡æ¯ä¸°å¯Œçš„é¢‘ç‡ä¿¡å·ï¼Œä»è€Œå¤§å¤§æé«˜äº†é¢„è®­ç»ƒæ•°æ®çš„ä½¿ç”¨æ•ˆç‡ï¼›2ï¼‰æ··åˆæç¤ºå¯ç”¨çš„Transformerï¼Œå®ƒé€šè¿‡å°æ•°æ®é›†ç‰¹å®šçš„æç¤ºå­¦ä¹ ä¸åŒçš„æ—¶é—´æ¨¡å¼ï¼Œå¹¶è‡ªé€‚åº”åœ°å°†æ—¶é—´åºåˆ—ä»¤ç‰Œè·¯ç”±åˆ°åŸºäºæç¤ºçš„ä¸“å®¶ï¼Œä»¥å®ç°ä¸åŒæ•°æ®é›†å’Œé¢†åŸŸä¸­çš„å‚æ•°æœ‰æ•ˆæ¨¡å‹é€‚åº”ã€‚é…å¤‡äº†è¿™äº›æ¨¡å—ï¼ŒSEMPOåœ¨æ˜¾è‘—å‡å°‘é¢„è®­ç»ƒæ•°æ®è§„æ¨¡å’Œæ¨¡å‹å¤§å°çš„åŒæ—¶ï¼Œå®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¶µç›–16ä¸ªæ•°æ®é›†çš„ä¸¤å¤§åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒSEMPOåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é¢„æµ‹åœºæ™¯ä¸­å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mala-lab/SEMPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mala-lab/SEMPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19710v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹åœ¨æ—¶åºé¢„æµ‹é¢†åŸŸçš„åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰å‘å±•ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œç°æœ‰FMsç½‘ç»œç»“æ„åºå¤§ï¼Œéœ€è¦åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²æ—¶æ„æˆäº†æŒ‘æˆ˜ã€‚é’ˆå¯¹é€šç”¨æ€§å’Œå¯è´Ÿæ‹…æ€§ä¹‹é—´çš„æ—¥ç›Šç´§å¼ å…³ç³»ï¼Œæˆ‘ä»¬æå‡ºäº†SEMPOï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è½»é‡çº§åŸºç¡€æ¨¡å‹ï¼Œå®ƒåªéœ€è¦åœ¨ç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä¾¿èƒ½å¤Ÿå±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ—¶åºé¢„æµ‹èƒ½åŠ›ã€‚SEMPOåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šä¸€æ˜¯èƒ½é‡æ„ŸçŸ¥çš„è°±åˆ†è§£æ¨¡å—ï¼Œå®ƒé€šè¿‡å¯¹å½“å‰æ–¹æ³•æ‰€å¿½ç•¥çš„ä½èƒ½é‡ä½†åŒ…å«ä¿¡æ¯çš„é¢‘ç‡ä¿¡å·è¿›è¡Œå»ºæ¨¡ï¼Œå¤§å¤§æé«˜äº†é¢„è®­ç»ƒæ•°æ®çš„åˆ©ç”¨ç‡ï¼›äºŒæ˜¯æ··åˆæç¤ºèµ‹èƒ½Transformerï¼Œå®ƒé€šè¿‡å°æ•°æ®é›†ç‰¹å®šçš„æç¤ºæ¥å­¦ä¹ ä¸åŒçš„æ—¶é—´æ¨¡å¼ï¼Œå¹¶è‡ªé€‚åº”åœ°å°†æ—¶é—´åºåˆ—æ ‡è®°è·¯ç”±åˆ°åŸºäºæç¤ºçš„ä¸“å®¶ï¼Œä»¥å®ç°è·¨ä¸åŒæ•°æ®é›†å’Œé¢†åŸŸçš„å‚æ•°é«˜æ•ˆæ¨¡å‹é€‚åº”ã€‚é€šè¿‡è¿™äº›æ¨¡å—çš„åº”ç”¨ï¼ŒSEMPOæ˜¾è‘—å‡å°‘äº†é¢„è®­ç»ƒæ•°æ®è§„æ¨¡å’Œæ¨¡å‹å¤§å°ï¼ŒåŒæ—¶å®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¶µç›–16ä¸ªæ•°æ®é›†çš„ä¸¤ä¸ªå¤§å‹åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é¢„æµ‹åœºæ™¯ä¸­ï¼ŒSEMPOçš„æ€§èƒ½å‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨æˆ‘ä»¬çš„GitHubé¡µé¢æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/mala-lab/SEMPO">https://github.com/mala-lab/SEMPO</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SEMPOæ˜¯ä¸€ç§é’ˆå¯¹æ—¶åºé¢„æµ‹çš„æ–°å‹è½»é‡çº§åŸºç¡€æ¨¡å‹ï¼Œå¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²ã€‚</li>
<li>SEMPOé€šè¿‡èƒ½é‡æ„ŸçŸ¥çš„è°±åˆ†è§£æ¨¡å—æé«˜äº†é¢„è®­ç»ƒæ•°æ®çš„åˆ©ç”¨ç‡ã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ··åˆæç¤ºèµ‹èƒ½Transformerå®ç°äº†åœ¨ä¸åŒæ•°æ®é›†å’Œé¢†åŸŸçš„å‚æ•°é«˜æ•ˆæ¨¡å‹é€‚åº”ã€‚</li>
<li>SEMPOæ˜¾è‘—å‡å°‘äº†é¢„è®­ç»ƒæ•°æ®è§„æ¨¡å’Œæ¨¡å‹å¤§å°ã€‚</li>
<li>SEMPOåœ¨æ¶µç›–å¤šä¸ªæ•°æ®é›†çš„å¤§å‹åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>SEMPOçš„ä¼˜è¶Šæ€§èƒ½åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é¢„æµ‹åœºæ™¯ä¸­å°¤ä¸ºçªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70f302281acb12d43586ec2f1220cda4" align="middle">
<img src="https://picx.zhimg.com/v2-321b0e6964f90961b5d93bc8cdf74635" align="middle">
<img src="https://picx.zhimg.com/v2-fbc56736608f24a35f331700e14ac56d" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Neural-Variational-Dropout-Processes"><a href="#Neural-Variational-Dropout-Processes" class="headerlink" title="Neural Variational Dropout Processes"></a>Neural Variational Dropout Processes</h2><p><strong>Authors:Insu Jeon, Youngjin Park, Gunhee Kim</strong></p>
<p>Learning to infer the conditional posterior model is a key step for robust meta-learning. This paper presents a new Bayesian meta-learning approach called Neural Variational Dropout Processes (NVDPs). NVDPs model the conditional posterior distribution based on a task-specific dropout; a low-rank product of Bernoulli experts meta-model is utilized for a memory-efficient mapping of dropout rates from a few observed contexts. It allows for a quick reconfiguration of a globally learned and shared neural network for new tasks in multi-task few-shot learning. In addition, NVDPs utilize a novel prior conditioned on the whole task data to optimize the conditional \textit{dropout} posterior in the amortized variational inference. Surprisingly, this enables the robust approximation of task-specific dropout rates that can deal with a wide range of functional ambiguities and uncertainties. We compared the proposed method with other meta-learning approaches in the few-shot learning tasks such as 1D stochastic regression, image inpainting, and classification. The results show the excellent performance of NVDPs. </p>
<blockquote>
<p>å­¦ä¹ æ¨æ–­æ¡ä»¶åéªŒæ¨¡å‹æ˜¯å®ç°ç¨³å¥å…ƒå­¦ä¹ çš„é‡è¦æ­¥éª¤ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è´å¶æ–¯å…ƒå­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºç¥ç»å˜åˆ†ä¸¢å¼ƒè¿‡ç¨‹ï¼ˆNVDPsï¼‰ã€‚NVDPsåŸºäºä»»åŠ¡ç‰¹å®šçš„ä¸¢å¼ƒæ¥å»ºæ¨¡æ¡ä»¶åéªŒåˆ†å¸ƒï¼›å®ƒåˆ©ç”¨ä½ç§©çš„ä¼¯åŠªåˆ©ä¸“å®¶å…ƒæ¨¡å‹æ¥å®ç°ä»å°‘é‡è§‚æµ‹ä¸Šä¸‹æ–‡ä¸­çš„ä¸¢å¼ƒç‡çš„å†…å­˜æœ‰æ•ˆæ˜ å°„ã€‚è¿™å…è®¸å¿«é€Ÿé‡æ–°é…ç½®å…¨å±€å­¦ä¹ å’Œå…±äº«ç¥ç»ç½‘ç»œï¼Œä»¥è¿›è¡Œå¤šä»»åŠ¡å°æ ·æœ¬å­¦ä¹ ä¸­çš„æ–°ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒNVDPsä½¿ç”¨åŸºäºæ•´ä¸ªä»»åŠ¡æ•°æ®çš„æ–°å‹å…ˆéªŒï¼Œä»¥ä¼˜åŒ–æ‘Šé”€å˜åˆ†æ¨æ–­ä¸­çš„æ¡ä»¶â€œä¸¢å¼ƒâ€åéªŒã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™èƒ½å¤Ÿå®ç°ç¨³å¥çš„ä»»åŠ¡ç‰¹å®šä¸¢å¼ƒç‡çš„è¿‘ä¼¼ï¼Œè¯¥ä¸¢å¼ƒç‡å¯ä»¥å¤„ç†å¹¿æ³›çš„å‡½æ•°æ¨¡ç³Šæ€§å’Œä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ–¹æ³•ä¸å…¶ä»–å…ƒå­¦ä¹ æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼ŒåŒ…æ‹¬å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ï¼Œå¦‚1Déšæœºå›å½’ã€å›¾åƒä¿®å¤å’Œåˆ†ç±»ã€‚ç»“æœæ˜¾ç¤ºNVDPsè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19425v1">PDF</a> Accepted as a Poster at International Conference on Learning   Representations (ICLR) 2022 (Apr 25-29, 2022)</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œå˜åˆ†ä¸¢å¼ƒè¿‡ç¨‹ï¼ˆNVDPsï¼‰æ˜¯ä¸€ç§æ–°å‹çš„è´å¶æ–¯å…ƒå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹è¿›è¡Œæ¡ä»¶åéªŒæ¨¡å‹æ¨æ–­ã€‚å®ƒé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„ä¸¢å¼ƒæ³•æ¥æ¨¡æ‹Ÿæ¡ä»¶åéªŒåˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨ä½é˜¶çš„ä¼¯åŠªåˆ©ä¸“å®¶å…ƒæ¨¡å‹æ¥å®ç°ä»å°‘é‡è§‚æµ‹ä¸Šä¸‹æ–‡ä¸­çš„ä¸¢å¼ƒç‡çš„æ˜ å°„ã€‚NVDPså…è®¸å¿«é€Ÿé‡æ–°é…ç½®å…¨å±€å­¦ä¹ çš„å…±äº«ç¥ç»ç½‘ç»œï¼Œä»¥å¤„ç†å¤šä»»åŠ¡å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ–°ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒNVDPsä½¿ç”¨åŸºäºæ•´ä¸ªä»»åŠ¡æ•°æ®çš„æ–°å…ˆéªŒï¼Œä»¥ä¼˜åŒ–æ‘Šé”€å˜åˆ†æ¨æ–­ä¸­çš„æ¡ä»¶ä¸¢å¼ƒåéªŒã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç¨³å¥åœ°è¿‘ä¼¼ä»»åŠ¡ç‰¹å®šçš„ä¸¢å¼ƒç‡ï¼Œåº”å¯¹å„ç§åŠŸèƒ½æ¨¡ç³Šæ€§å’Œä¸ç¡®å®šæ€§ã€‚åœ¨å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ï¼ˆå¦‚1Déšæœºå›å½’ã€å›¾åƒä¿®å¤å’Œåˆ†ç±»ï¼‰ä¸­ï¼ŒNVDPsçš„è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NVDPsæ˜¯ä¸€ç§æ–°å‹çš„è´å¶æ–¯å…ƒå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ¡ä»¶åéªŒæ¨¡å‹æ¨æ–­ã€‚</li>
<li>NVDPsé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„ä¸¢å¼ƒæ³•æ¨¡æ‹Ÿæ¡ä»¶åéªŒåˆ†å¸ƒï¼Œå®ç°è®°å¿†é«˜æ•ˆçš„ä¸¢å¼ƒç‡æ˜ å°„ã€‚</li>
<li>NVDPså…è®¸å¿«é€Ÿé‡æ–°é…ç½®å…¨å±€å­¦ä¹ çš„å…±äº«ç¥ç»ç½‘ç»œï¼Œé€‚åº”å¤šä»»åŠ¡å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ–°ä»»åŠ¡ã€‚</li>
<li>NVDPsé‡‡ç”¨ä½é˜¶ä¼¯åŠªåˆ©ä¸“å®¶å…ƒæ¨¡å‹æ¥å¤„ç†åŠŸèƒ½æ¨¡ç³Šæ€§å’Œä¸ç¡®å®šæ€§ã€‚</li>
<li>NVDPsä½¿ç”¨åŸºäºæ•´ä¸ªä»»åŠ¡æ•°æ®çš„æ–°å…ˆéªŒæ¥ä¼˜åŒ–æ¡ä»¶ä¸¢å¼ƒåéªŒåœ¨æ‘Šé”€å˜åˆ†æ¨æ–­ä¸­ã€‚</li>
<li>NVDPsåœ¨å¤šç§å°‘æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬1Déšæœºå›å½’ã€å›¾åƒä¿®å¤å’Œåˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-882920ef6cb702cc404e09598e73bfb8" align="middle">
<img src="https://picx.zhimg.com/v2-3a0481cc42ce72a410a2dded51f87880" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learning-Noise-Resilient-and-Transferable-Graph-Text-Alignment-via-Dynamic-Quality-Assessment"><a href="#Learning-Noise-Resilient-and-Transferable-Graph-Text-Alignment-via-Dynamic-Quality-Assessment" class="headerlink" title="Learning Noise-Resilient and Transferable Graph-Text Alignment via   Dynamic Quality Assessment"></a>Learning Noise-Resilient and Transferable Graph-Text Alignment via   Dynamic Quality Assessment</h2><p><strong>Authors:Yuhang Liu, Minglai Shao, Zengyi Wo, Yunlong Chu, Bing Hao, Shengzhong Liu, Ruijie Wang, Jianxin Li</strong></p>
<p>Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs) is central to web-scale applications such as search, recommendation, and knowledge discovery. However, existing CLIP-style graph-text aligners face two key limitations: they assume strict one-to-one correspondences between nodes and texts, overlooking the inherent many-to-many relations in real-world graphs; and they rely on static alignment objectives that cannot adapt to varying data quality, making them brittle under noisy supervision. Together, these limitations expose a core dilemma: embracing expressive many-to-many alignment amplifies noise, while reverting to strict one-to-one strategies sacrifices semantic diversity and fails to handle inherently mismatched pairs. To address these challenges, we propose ADAligner, a dynamic, quality-aware graph-text alignment framework that dynamically adjusts between expressive many-to-many and conservative one-to-one objectives according to supervision quality. ADAligner estimates batch-level alignment reliability in real time and adapts its optimization accordingly, promoting soft, subgraph-level many-to-many alignment when supervision is clean, while emphasizing reliable one-to-one alignment by dynamically filtering low-confidence pairs under noise. Theoretically, we prove that this dynamic mechanism forms a stable negative feedback process, ensuring convergence and robustness. Comprehensive experiments on nine diverse TAG datasets demonstrate that ADAligner consistently outperforms prior graph-text aligners on zero-&#x2F;few-shot node classification, link prediction and cross-modal retrieval tasks. It maintains strong robustness under noisy supervision and accelerates pre-training by approximately 2 to 3 times compared to multimodal baselines, establishing a scalable and reliable foundation for graph-text representation learning in real-world web environments. </p>
<blockquote>
<p>é¢„è®­ç»ƒå›¾åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰åœ¨æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ä¸Šå¯¹äºæœç´¢ã€æ¨èå’ŒçŸ¥è¯†å‘ç°ç­‰å¤§è§„æ¨¡åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CLIPé£æ ¼çš„å›¾å½¢æ–‡æœ¬å¯¹é½å™¨é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šå®ƒä»¬å‡å®šèŠ‚ç‚¹å’Œæ–‡æœ¬ä¹‹é—´å­˜åœ¨ä¸¥æ ¼çš„ä¸€å¯¹ä¸€å¯¹åº”å…³ç³»ï¼Œå¿½ç•¥äº†ç°å®ä¸–ç•Œä¸­å›¾å½¢ä¸­å›ºæœ‰çš„å¤šå¯¹å¤šå…³ç³»ï¼›å®ƒä»¬ä¾èµ–äºé™æ€å¯¹é½ç›®æ ‡ï¼Œæ— æ³•é€‚åº”ä¸æ–­å˜åŒ–çš„æ•°æ®è´¨é‡ï¼Œä½¿å¾—å®ƒä»¬åœ¨å˜ˆæ‚çš„ç›‘ç£ä¸‹å˜å¾—è„†å¼±ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›å±€é™æ€§æš´éœ²äº†ä¸€ä¸ªæ ¸å¿ƒå›°å¢ƒï¼šæ¥å—è¡¨è¾¾æ€§çš„å¤šå¯¹å¤šå¯¹é½ä¼šæ”¾å¤§å™ªå£°ï¼Œè€Œæ¢å¤åˆ°ä¸€å¯¹ä¸€çš„ç­–ç•¥åˆ™ä¼šç‰ºç‰²è¯­ä¹‰å¤šæ ·æ€§ï¼Œå¹¶ä¸”æ— æ³•å¤„ç†å›ºæœ‰çš„ä¸åŒ¹é…å¯¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ADAlignerï¼Œä¸€ä¸ªåŠ¨æ€ã€è´¨é‡æ„ŸçŸ¥çš„å›¾å½¢æ–‡æœ¬å¯¹é½æ¡†æ¶ï¼Œå®ƒæ ¹æ®ç›‘ç£è´¨é‡åŠ¨æ€è°ƒæ•´è¡¨è¾¾æ€§çš„å¤šå¯¹å¤šå’Œä¿å®ˆçš„ä¸€å¯¹ä¸€ç›®æ ‡ä¹‹é—´çš„å¹³è¡¡ã€‚ADAlignerå®æ—¶ä¼°è®¡æ‰¹å¤„ç†çº§åˆ«çš„å¯¹é½å¯é æ€§å¹¶ç›¸åº”è°ƒæ•´å…¶ä¼˜åŒ–ï¼Œåœ¨ç›‘ç£å¹²å‡€æ—¶ä¿ƒè¿›æŸ”æ€§ã€å­å›¾çº§åˆ«çš„å¤šå¯¹å¤šå¯¹é½ï¼Œè€Œåœ¨å™ªå£°ä¸‹é€šè¿‡åŠ¨æ€è¿‡æ»¤ä½ä¿¡å¿ƒå¯¹æ¥å¼ºè°ƒå¯é çš„ä¸€å¯¹ä¸€å¯¹é½ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§åŠ¨æ€æœºåˆ¶å½¢æˆäº†ä¸€ä¸ªç¨³å®šçš„è´Ÿåé¦ˆè¿‡ç¨‹ï¼Œç¡®ä¿äº†æ”¶æ•›æ€§å’Œç¨³å¥æ€§ã€‚åœ¨ä¹ä¸ªä¸åŒçš„TAGæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒADAligneråœ¨é›¶&#x2F;å°‘æ¬¡èŠ‚ç‚¹åˆ†ç±»ã€é“¾æ¥é¢„æµ‹å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºå…ˆå‰çš„å›¾å½¢æ–‡æœ¬å¯¹é½å™¨ã€‚å®ƒåœ¨å˜ˆæ‚çš„ç›‘ç£ä¸‹ä¿æŒå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œä¸å¤šæ¨¡å¼åŸºçº¿ç›¸æ¯”ï¼Œé¢„è®­ç»ƒé€Ÿåº¦æé«˜äº†å¤§çº¦2åˆ°3å€ï¼Œä¸ºç°å®ä¸–ç•Œçš„ç½‘ç»œç¯å¢ƒä¸­çš„å›¾å½¢æ–‡æœ¬è¡¨ç¤ºå­¦ä¹ å»ºç«‹äº†å¯æ‰©å±•å’Œå¯é çš„åŸºçŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19384v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢„è®­ç»ƒå›¾åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰åœ¨æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ä¸Šçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰CLIPé£æ ¼çš„å›¾æ–‡æœ¬å¯¹é½å™¨é¢ä¸´çš„å…³é”®å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŠ¨æ€ã€è´¨é‡æ„ŸçŸ¥çš„å›¾æ–‡æœ¬å¯¹é½æ¡†æ¶ADAlignerã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®ç›‘ç£è´¨é‡åŠ¨æ€è°ƒæ•´è¡¨è¾¾ä¸°å¯Œå¤šæ ·çš„å¤šå¯¹å¤šå’Œå¯¹ä¿å®ˆçš„ä¸€å¯¹ä¸€ç›®æ ‡ã€‚åœ¨æ¸…æ´ç›‘ç£ä¸‹ä¿ƒè¿›è½¯å­å›¾çº§åˆ«çš„å¤šå¯¹å¤šå¯¹é½ï¼Œè€Œåœ¨å™ªå£°ç¯å¢ƒä¸‹åˆ™é€šè¿‡åŠ¨æ€è¿‡æ»¤ä½ä¿¡å¿ƒå¯¹æ¥å¼ºè°ƒå¯é çš„ä¸€å¯¹ä¸€å¯¹é½ã€‚å®éªŒè¯æ˜ï¼ŒADAligneråœ¨é›¶&#x2F;å°‘æ¬¡èŠ‚ç‚¹åˆ†ç±»ã€é“¾æ¥é¢„æµ‹å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå‡ä¼˜äºå…ˆå‰çš„å›¾æ–‡æœ¬å¯¹é½å™¨ï¼Œå¹¶åœ¨å™ªå£°ç›‘ç£ä¸‹ä¿æŒäº†å¼ºå¤§çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶é¢„è®­ç»ƒé€Ÿåº¦æé«˜äº†å¤§çº¦2åˆ°3å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒå›¾åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰åœ¨æ–‡æœ¬å±æ€§å›¾ï¼ˆTAGsï¼‰ä¸Šçš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰CLIPé£æ ¼çš„å›¾æ–‡æœ¬å¯¹é½å™¨é¢ä¸´çš„å…³é”®å±€é™æ€§ï¼šå¿½ç•¥çœŸå®ä¸–ç•Œå›¾ä¸­çš„è®¸å¤šå¯¹è®¸å¤šå…³ç³»ï¼Œä»¥åŠæ— æ³•é€‚åº”å˜åŒ–çš„æ•°æ®è´¨é‡ã€‚</li>
<li>ADAligneræ˜¯ä¸€ä¸ªåŠ¨æ€ã€è´¨é‡æ„ŸçŸ¥çš„å›¾æ–‡æœ¬å¯¹é½æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ç›‘ç£è´¨é‡åŠ¨æ€è°ƒæ•´å¯¹é½ç­–ç•¥ã€‚</li>
<li>ADAligneråœ¨æ¸…æ´ç›‘ç£ä¸‹é‡‡ç”¨è½¯å­å›¾çº§åˆ«çš„å¤šå¯¹å¤šå¯¹é½ï¼Œè€Œåœ¨å™ªå£°ç¯å¢ƒä¸‹åˆ™å¼ºè°ƒå¯é çš„ä¸€å¯¹ä¸€å¯¹é½ã€‚</li>
<li>ADAligneré€šè¿‡åŠ¨æ€è¿‡æ»¤ä½ä¿¡å¿ƒå¯¹æ¥æé«˜ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ADAligneråœ¨å¤šç§TAGæ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–å›¾æ–‡æœ¬å¯¹é½æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶&#x2F;å°‘æ¬¡èŠ‚ç‚¹åˆ†ç±»ã€é“¾æ¥é¢„æµ‹å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šã€‚</li>
<li>ADAligneråœ¨å™ªå£°ç›‘ç£ä¸‹ä¿æŒå¼ºå¤§ç¨³å¥æ€§ï¼Œå¹¶åŠ é€Ÿäº†é¢„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d21edb252cc0c456cc952fee4bfd2bdb" align="middle">
<img src="https://picx.zhimg.com/v2-004b109f15bf202d3d66a39ffd049141" align="middle">
<img src="https://picx.zhimg.com/v2-6bf74e79611fb52496900c5225a7b4f1" align="middle">
<img src="https://picx.zhimg.com/v2-bbcb6a6d227899c2473730427af0c5a9" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Early-Alzheimer-Disease-Detection-through-Big-Data-and-Ensemble-Few-Shot-Learning"><a href="#Enhancing-Early-Alzheimer-Disease-Detection-through-Big-Data-and-Ensemble-Few-Shot-Learning" class="headerlink" title="Enhancing Early Alzheimer Disease Detection through Big Data and   Ensemble Few-Shot Learning"></a>Enhancing Early Alzheimer Disease Detection through Big Data and   Ensemble Few-Shot Learning</h2><p><strong>Authors:Safa Ben Atitallah, Maha Driss, Wadii Boulila, Anis Koubaa</strong></p>
<p>Alzheimer disease is a severe brain disorder that causes harm in various brain areas and leads to memory damage. The limited availability of labeled medical data poses a significant challenge for accurate Alzheimer disease detection. There is a critical need for effective methods to improve the accuracy of Alzheimer disease detection, considering the scarcity of labeled data, the complexity of the disease, and the constraints related to data privacy. To address this challenge, our study leverages the power of big data in the form of pre-trained Convolutional Neural Networks (CNNs) within the framework of Few-Shot Learning (FSL) and ensemble learning. We propose an ensemble approach based on a Prototypical Network (ProtoNet), a powerful method in FSL, integrating various pre-trained CNNs as encoders. This integration enhances the richness of features extracted from medical images. Our approach also includes a combination of class-aware loss and entropy loss to ensure a more precise classification of Alzheimer disease progression levels. The effectiveness of our method was evaluated using two datasets, the Kaggle Alzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and 99.86%, respectively. The comparison of our results with relevant state-of-the-art studies demonstrated that our approach achieved superior accuracy and highlighted its validity and potential for real-world applications in early Alzheimer disease detection. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…æ˜¯ä¸€ç§ä¸¥é‡çš„è„‘éšœç¢ç–¾ç—…ï¼Œä¼šæŸå®³å¤§è„‘çš„å¤šä¸ªåŒºåŸŸå¹¶å¯¼è‡´è®°å¿†åŠ›ä¸‹é™ã€‚æ ‡æ³¨åŒ»ç–—æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ç»™é˜¿å°”èŒ¨æµ·é»˜ç—…çš„å‡†ç¡®æ£€æµ‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è€ƒè™‘åˆ°æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ã€ç–¾ç—…çš„å¤æ‚æ€§å’Œæ•°æ®éšç§çš„ç›¸å…³é™åˆ¶ï¼Œæ€¥éœ€æœ‰æ•ˆçš„æ–¹æ³•æ¥æé«˜é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶åˆ©ç”¨å¤§æ•°æ®çš„åŠ›é‡ï¼Œåœ¨å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰å’Œé›†æˆå­¦ä¹ çš„æ¡†æ¶ä¸‹ï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å½¢å¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŸå‹ç½‘ç»œï¼ˆProtoNetï¼‰çš„é›†æˆæ–¹æ³•ï¼Œè¿™æ˜¯FSLä¸­çš„ä¸€ç§å¼ºå¤§æ–¹æ³•ï¼Œå®ƒå°†å¤šä¸ªé¢„è®­ç»ƒçš„CNNä½œä¸ºç¼–ç å™¨è¿›è¡Œé›†æˆã€‚è¿™ç§é›†æˆæé«˜äº†ä»åŒ»ç–—å›¾åƒä¸­æå–ç‰¹å¾çš„ä¸°å¯Œæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜åŒ…æ‹¬ç»“åˆç±»æ„ŸçŸ¥æŸå¤±å’Œç†µæŸå¤±ï¼Œä»¥ç¡®ä¿æ›´ç²¾ç¡®åœ°åˆ†ç±»é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¿›å±•æ°´å¹³ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨Kaggleé˜¿å°”èŒ¨æµ·é»˜ç—…æ•°æ®é›†å’ŒADNIæ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œå‡†ç¡®ç‡åˆ†åˆ«ä¸º99.72%å’Œ99.86%ã€‚ä¸ç›¸å…³æœ€å…ˆè¿›çš„ç ”ç©¶çš„æ¯”è¾ƒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨æ—©æœŸé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€å®é™…åº”ç”¨æ½œåŠ›å’Œä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19282v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨å¤§æ•°æ®çš„åŠ›é‡ï¼Œå€ŸåŠ©é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒFew-Shotå­¦ä¹ ï¼ˆFSLï¼‰æ¡†æ¶ï¼Œæå‡ºä¸€ç§åŸºäºåŸå‹ç½‘ç»œï¼ˆProtoNetï¼‰çš„é›†æˆæ–¹æ³•æ¥è§£å†³é˜¿å°”èŒ¨æµ·é»˜ç—‡æ£€æµ‹çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚é€šè¿‡ç»“åˆå¤šç§é¢„è®­ç»ƒCNNä½œä¸ºç¼–ç å™¨ï¼Œæé«˜äº†ä»åŒ»å­¦å›¾åƒä¸­æå–çš„ç‰¹å¾çš„ä¸°å¯Œæ€§ã€‚è¯¥æ–¹æ³•è¿˜ç»“åˆäº†ç±»æ„ŸçŸ¥æŸå¤±å’Œç†µæŸå¤±ï¼Œä»¥ç¡®ä¿æ›´ç²¾ç¡®åœ°åˆ†ç±»é˜¿å°”èŒ¨æµ·é»˜ç—‡è¿›å±•æ°´å¹³ã€‚åœ¨Kaggleé˜¿å°”èŒ¨æµ·é»˜ç—‡æ•°æ®é›†å’ŒADNIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å‡†ç¡®æ€§é«˜è¾¾99.72%å’Œ99.86%ï¼Œä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿å°”å…¹æµ·é»˜ç—‡æ˜¯ä¸€ç§ä¸¥é‡çš„è„‘éƒ¨ç–¾ç—…ï¼Œå½±å“å¤§è„‘å¤šä¸ªåŒºåŸŸå¹¶å¯¼è‡´è®°å¿†æŸä¼¤ã€‚</li>
<li>åŒ»å­¦æ•°æ®çš„æœ‰é™å¯ç”¨æ€§å¯¹å‡†ç¡®æ£€æµ‹é˜¿å°”å…¹æµ·é»˜ç—‡æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨å¤§æ•°æ®çš„åŠ›é‡ï¼Œå€ŸåŠ©é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒFew-Shotå­¦ä¹ ï¼ˆFSLï¼‰æ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>é‡‡ç”¨åŸºäºåŸå‹ç½‘ç»œï¼ˆProtoNetï¼‰çš„é›†æˆæ–¹æ³•ï¼Œæ•´åˆå¤šç§é¢„è®­ç»ƒCNNä½œä¸ºç¼–ç å™¨ï¼Œæé«˜åŒ»å­¦å›¾åƒç‰¹å¾æå–çš„ä¸°å¯Œæ€§ã€‚</li>
<li>ç»“åˆç±»æ„ŸçŸ¥æŸå¤±å’Œç†µæŸå¤±ï¼Œç¡®ä¿æ›´ç²¾ç¡®åœ°åˆ†ç±»é˜¿å°”å…¹æµ·é»˜ç—‡è¿›å±•æ°´å¹³ã€‚</li>
<li>åœ¨Kaggleé˜¿å°”èŒ¨æµ·é»˜ç—‡æ•°æ®é›†å’ŒADNIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•å…·æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3d7356b6449c6626e86e062eced2573" align="middle">
<img src="https://picx.zhimg.com/v2-88705e4cf089f8f241d8be351f44ccd2" align="middle">
<img src="https://picx.zhimg.com/v2-0952e3309f7bd80b1e8070309f3be933" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Robust-Driving-QA-through-Metadata-Grounded-Context-and-Task-Specific-Prompts"><a href="#Robust-Driving-QA-through-Metadata-Grounded-Context-and-Task-Specific-Prompts" class="headerlink" title="Robust Driving QA through Metadata-Grounded Context and Task-Specific   Prompts"></a>Robust Driving QA through Metadata-Grounded Context and Task-Specific   Prompts</h2><p><strong>Authors:Seungjun Yu, Junsung Park, Youngsun Lim, Hyunjung Shim</strong></p>
<p>We present a two-phase vision-language QA system for autonomous driving that answers high-level perception, prediction, and planning questions. In Phase-1, a large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a short temporal window of history, and a chain-of-thought prompt with few-shot exemplars. A self-consistency ensemble (multiple sampled reasoning chains) further improves answer reliability. In Phase-2, we augment the prompt with nuScenes scene metadata (object annotations, ego-vehicle state, etc.) and category-specific question instructions (separate prompts for perception, prediction, planning tasks). In experiments on a driving QA benchmark, our approach significantly outperforms the baseline Qwen2.5 models. For example, using 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall accuracy (vs.62.61% with zero-shot); applying self-consistency raises this to 66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96% accuracy under severe visual corruption. These results demonstrate that carefully engineered prompts and contextual grounding can greatly enhance high-level driving QA with pretrained vision-language models. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨é©¾é©¶çš„ä¸¤é˜¶æ®µè§†è§‰è¯­è¨€é—®ç­”ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå›ç­”é«˜çº§æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’é—®é¢˜ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå¤§å‹å¤šæ¨¡æ€LLMï¼ˆQwen2.5-VL-32Bï¼‰åŸºäºå…­ä¸ªæ‘„åƒå¤´çš„è¾“å…¥ã€çŸ­æ—¶é—´çª—å£çš„å†å²ä¿¡æ¯ä»¥åŠå¸¦æœ‰å°‘é‡æ ·æœ¬çš„æ€ç»´é“¾æç¤ºè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§é›†åˆï¼ˆå¤šä¸ªé‡‡æ ·æ¨ç†é“¾ï¼‰è¿›ä¸€æ­¥æé«˜äº†ç­”æ¡ˆçš„å¯é æ€§ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨nuScenesåœºæ™¯å…ƒæ•°æ®ï¼ˆå¯¹è±¡æ³¨é‡Šã€è‡ªæˆ‘è½¦è¾†çŠ¶æ€ç­‰ï¼‰å’Œä»»åŠ¡ç‰¹å®šçš„é—®é¢˜æŒ‡ä»¤ï¼ˆæ„ŸçŸ¥ã€é¢„æµ‹ã€è§„åˆ’ä»»åŠ¡çš„å•ç‹¬æç¤ºï¼‰æ¥å¢å¼ºæç¤ºã€‚åœ¨é©¾é©¶é—®ç­”åŸºå‡†æµ‹è¯•çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿Qwen2.5æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µä½¿ç”¨5ä¸ªå†å²å¸§å’Œ10æ¬¡æ‹æ‘„æç¤ºè¾¾åˆ°65.1%çš„æ•´ä½“å‡†ç¡®ç‡ï¼ˆä¸é›¶æ¬¡æ‹æ‘„çš„62.61%ç›¸æ¯”ï¼‰ï¼›åº”ç”¨è‡ªæˆ‘ä¸€è‡´æ€§å°†å…¶æé«˜åˆ°66.85%ã€‚ç¬¬äºŒé˜¶æ®µè¾¾åˆ°67.37%çš„æ•´ä½“å‡†ç¡®ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç³»ç»Ÿåœ¨ä¸¥é‡çš„è§†è§‰å¤±çœŸæƒ…å†µä¸‹ä»èƒ½ä¿æŒ96%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºå’Œä¸Šä¸‹æ–‡å®šä½å¯ä»¥æå¤§åœ°å¢å¼ºä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹çš„é«˜çº§é©¾é©¶é—®ç­”èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19001v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è§†è§‰è¯­è¨€é—®ç­”ç³»ç»Ÿï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­çš„æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–å¤„ç†ï¼Œåˆ©ç”¨å…­ç›¸æœºè¾“å…¥ã€çŸ­æš‚å†å²æ—¶é—´çª—å£å’Œå°‘é‡æ ·æœ¬ç¤ºä¾‹è¿›è¡Œæ¨ç†ã€‚é€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§é›†åˆï¼ˆå¤šä¸ªé‡‡æ ·æ¨ç†é“¾ï¼‰æé«˜ç­”æ¡ˆçš„å¯é æ€§ã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡æ·»åŠ åœºæ™¯å…ƒæ•°æ®ï¼ˆå¯¹è±¡æ³¨é‡Šã€è½¦è¾†çŠ¶æ€ç­‰ï¼‰å’Œä»»åŠ¡ç‰¹å®šæŒ‡ä»¤æ¥å¢å¼ºæç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¹¶èƒ½åœ¨è§†è§‰å¤±çœŸæ¡ä»¶ä¸‹ä¿æŒé«˜å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æå–å‡ºçš„å…³é”®è¦ç‚¹ï¼Œä»¥ç²¾ç®€å’Œæ¸…æ™°çš„æ–¹å¼å‘ˆç°ï¼š</p>
<ul>
<li>æå‡ºä¸¤é˜¶æ®µè§†è§‰è¯­è¨€é—®ç­”ç³»ç»Ÿï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­çš„æ„ŸçŸ¥ã€é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–å¤„ç†ï¼Œé€šè¿‡å°‘é‡æ ·æœ¬ç¤ºä¾‹è¿›è¡Œæ¨ç†ã€‚</li>
<li>è‡ªæˆ‘ä¸€è‡´æ€§é›†åˆæ–¹æ³•æé«˜äº†ç­”æ¡ˆçš„å¯é æ€§ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡æ·»åŠ åœºæ™¯å…ƒæ•°æ®å’Œä»»åŠ¡ç‰¹å®šæŒ‡ä»¤å¢å¼ºæç¤ºã€‚</li>
<li>å®éªŒè¡¨æ˜è¯¥ç³»ç»Ÿåœ¨é©¾é©¶é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
<li>ç³»ç»Ÿèƒ½åœ¨è§†è§‰å¤±çœŸæ¡ä»¶ä¸‹ä¿æŒé«˜å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-379f4d15332533ccf4d2375a11ee0e60" align="middle">
<img src="https://picx.zhimg.com/v2-eb749d14a18a766523a32a23df2e838a" align="middle">
<img src="https://picx.zhimg.com/v2-a6aa2ec306e1670609425469bfdab3f4" align="middle">
<img src="https://picx.zhimg.com/v2-ece31b2d68d4719af3d40a69e1817752" align="middle">
<img src="https://picx.zhimg.com/v2-e6b96742164b34178cb4383dac81bfe7" align="middle">
<img src="https://picx.zhimg.com/v2-5f4f86c15df40d98322484ec9247b88d" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Prompting-the-Priorities-A-First-Look-at-Evaluating-LLMs-for-Vulnerability-Triage-and-Prioritization"><a href="#Prompting-the-Priorities-A-First-Look-at-Evaluating-LLMs-for-Vulnerability-Triage-and-Prioritization" class="headerlink" title="Prompting the Priorities: A First Look at Evaluating LLMs for   Vulnerability Triage and Prioritization"></a>Prompting the Priorities: A First Look at Evaluating LLMs for   Vulnerability Triage and Prioritization</h2><p><strong>Authors:Osama Al Haddad, Muhammad Ikram, Ejaz Ahmed, Young Lee</strong></p>
<p>Security analysts face increasing pressure to triage large and complex vulnerability backlogs. Large Language Models (LLMs) offer a potential aid by automating parts of the interpretation process. We evaluate four models (ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to interpret semi-structured and unstructured vulnerability information. As a concrete use case, we test each modelâ€™s ability to predict decision points in the Stakeholder-Specific Vulnerability Categorization (SSVC) framework: Exploitation, Automatable, Technical Impact, and Mission and Wellbeing.   Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more than 165,000 queries to assess performance under prompting styles including one-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC decision point and Cohenâ€™s kappa (weighted and unweighted) for the final SSVC decision outcomes. Gemini consistently ranked highest, leading on three of four decision points and yielding the most correct recommendations. Prompting with exemplars generally improved accuracy, although all models struggled on some decision points. Only DeepSeek achieved fair agreement under weighted metrics, and all models tended to over-predict risk.   Overall, current LLMs do not replace expert judgment. However, specific LLM and prompt combinations show moderate effectiveness for targeted SSVC decisions. When applied with care, LLMs can support vulnerability prioritization workflows and help security teams respond more efficiently to emerging threats. </p>
<blockquote>
<p>å®‰å…¨åˆ†æå¸ˆé¢ä¸´ç€å¤„ç†å¤§é‡å¤æ‚æ¼æ´è®°å½•çš„å·¨å¤§å‹åŠ›ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡è‡ªåŠ¨åŒ–éƒ¨åˆ†è§£é‡Šè¿‡ç¨‹æä¾›æ½œåœ¨å¸®åŠ©ã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ä¸ªæ¨¡å‹ï¼ˆChatGPTã€Claudeã€Geminiå’ŒDeepSeekï¼‰åœ¨è§£é‡ŠåŠç»“æ„åŒ–å’Œéç»“æ„åŒ–æ¼æ´ä¿¡æ¯æ—¶é‡‡ç”¨çš„åäºŒç§æç¤ºæŠ€æœ¯ã€‚ä½œä¸ºå…·ä½“çš„åº”ç”¨åœºæ™¯ï¼Œæˆ‘ä»¬æµ‹è¯•äº†æ¯ä¸ªæ¨¡å‹åœ¨åˆ©ç›Šç›¸å…³è€…ç‰¹å®šæ¼æ´åˆ†ç±»ï¼ˆSSVCï¼‰æ¡†æ¶ä¸­é¢„æµ‹å†³ç­–ç‚¹çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åˆ©ç”¨ã€å¯è‡ªåŠ¨åŒ–ã€æŠ€æœ¯å½±å“ä»¥åŠä»»åŠ¡å’Œç¦ç¥‰ç­‰æ–¹é¢ã€‚æˆ‘ä»¬ä½¿ç”¨VulZooæ•°æ®é›†ä¸­çš„384ä¸ªçœŸå®ä¸–ç•Œæ¼æ´ï¼Œå‘å‡ºäº†è¶…è¿‡16.5ä¸‡æ¬¡æŸ¥è¯¢ï¼Œä»¥è¯„ä¼°åœ¨ä¸€æ¬¡æ€§æç¤ºã€å°‘æ•°å‡ æ¬¡æç¤ºå’Œæ€ç»´é“¾æç¤ºç­‰æç¤ºé£æ ¼ä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†æ¯ä¸ªSSVCå†³ç­–ç‚¹çš„F1åˆ†æ•°ä»¥åŠæœ€ç»ˆSSVCå†³ç­–ç»“æœçš„åŠ æƒå’ŒéåŠ æƒCohençš„kappaå€¼ã€‚Geminiä¸€ç›´æ’åæœ€é«˜ï¼Œåœ¨å››ä¸ªå†³ç­–ç‚¹ä¸­çš„ä¸‰ä¸ªä¸Šé¢†å…ˆï¼Œå¹¶ç»™å‡ºäº†æœ€æ­£ç¡®çš„å»ºè®®ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä½¿ç”¨èŒƒä¾‹è¿›è¡Œæç¤ºæé«˜äº†å‡†ç¡®æ€§ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨æŸäº›å†³ç­–ç‚¹ä¸Šéƒ½å­˜åœ¨å›°éš¾ã€‚åªæœ‰åœ¨åŠ æƒæŒ‡æ ‡ä¸‹ï¼ŒDeepSeekæ‰å®ç°äº†å…¬å¹³åè®®ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½å€¾å‘äºè¿‡åº¦é¢„æµ‹é£é™©ã€‚æ€»ä½“è€Œè¨€ï¼Œå½“å‰LLMè¿˜ä¸èƒ½å–ä»£ä¸“å®¶åˆ¤æ–­ã€‚ç„¶è€Œï¼Œç‰¹å®šçš„LLMå’Œæç¤ºç»„åˆå¯¹äºç›®æ ‡SSVCå†³ç­–æ˜¾ç¤ºå‡ºä¸­ç­‰æœ‰æ•ˆæ€§ã€‚å¦‚æœåº”ç”¨å¾—å½“ï¼ŒLLMå¯ä»¥æ”¯æŒæ¼æ´ä¼˜å…ˆçº§æ’åºå·¥ä½œæµç¨‹ï¼Œå¸®åŠ©å®‰å…¨å›¢é˜Ÿæ›´æœ‰æ•ˆåœ°åº”å¯¹æ–°å…´å¨èƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18508v1">PDF</a> 19 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å®‰å…¨åˆ†æå¸ˆé¢ä¸´å¤„ç†å¤§é‡å¤æ‚æ¼æ´æ¸…å•çš„å‹åŠ›ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥é€šè¿‡è‡ªåŠ¨åŒ–éƒ¨åˆ†è§£é‡Šè¿‡ç¨‹æä¾›å¸®åŠ©ã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ç§æ¨¡å‹ï¼ˆChatGPTã€Claudeã€Geminiå’ŒDeepSeekï¼‰åœ¨è§£é‡ŠåŠç»“æ„åŒ–å’Œéç»“æ„åŒ–æ¼æ´ä¿¡æ¯æ–¹é¢çš„è¡¨ç°ï¼Œé‡‡ç”¨äº†åäºŒç§æç¤ºæŠ€æœ¯ã€‚ä½œä¸ºå…·ä½“çš„åº”ç”¨åœºæ™¯ï¼Œæˆ‘ä»¬æµ‹è¯•äº†æ¯ä¸ªæ¨¡å‹åœ¨åˆ©ç›Šç›¸å…³è€…ç‰¹å®šæ¼æ´åˆ†ç±»ï¼ˆSSVCï¼‰æ¡†æ¶ä¸­é¢„æµ‹å†³ç­–ç‚¹çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åˆ©ç”¨ã€è‡ªåŠ¨åŒ–ã€æŠ€æœ¯å½±å“å’Œä½¿å‘½åŠå¥åº·ã€‚æˆ‘ä»¬ä½¿ç”¨VulZooæ•°æ®é›†ä¸­çš„384ä¸ªçœŸå®ä¸–ç•Œæ¼æ´ï¼Œå‘å‡ºäº†è¶…è¿‡16.5ä¸‡æ¬¡æŸ¥è¯¢ï¼Œä»¥è¯„ä¼°åœ¨ä¸€æ¬¡æ€§æç¤ºã€å°‘æ•°æç¤ºå’Œæ€ç»´é“¾æç¤ºé£æ ¼ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†æ¯ä¸ªSSVCå†³ç­–ç‚¹çš„F1åˆ†æ•°ä»¥åŠæœ€ç»ˆSSVCå†³ç­–ç»“æœçš„åŠ æƒå’ŒéåŠ æƒCohen kappaå€¼ã€‚Geminiåœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€å¥½ï¼Œåœ¨å››ä¸ªå†³ç­–ç‚¹ä¸­çš„ä¸‰ä¸ªä¸Šé¢†å…ˆï¼Œå¹¶æä¾›äº†æœ€æ­£ç¡®çš„å»ºè®®ã€‚ç”¨èŒƒä¾‹æç¤ºé€šå¸¸å¯ä»¥æé«˜å‡†ç¡®æ€§ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨æŸäº›å†³ç­–ç‚¹ä¸Šä»å­˜åœ¨å›°éš¾ã€‚åªæœ‰DeepSeekåœ¨åŠ æƒæŒ‡æ ‡ä¸‹å®ç°äº†å…¬å¹³åè®®ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½æœ‰è¿‡åº¦é¢„æµ‹é£é™©çš„è¶‹åŠ¿ã€‚æ€»ä½“è€Œè¨€ï¼Œå½“å‰LLMsä¸èƒ½å–ä»£ä¸“å®¶åˆ¤æ–­ã€‚ç„¶è€Œï¼Œç‰¹å®šçš„LLMå’Œæç¤ºç»„åˆæ˜¾ç¤ºå¯¹ç›®æ ‡SSVCå†³ç­–å…·æœ‰ä¸­ç­‰æœ‰æ•ˆæ€§ã€‚é€‚å½“åº”ç”¨æ—¶ï¼ŒLLMså¯ä»¥æ”¯æŒæ¼æ´ä¼˜å…ˆçº§æ’åºå·¥ä½œæµç¨‹ï¼Œå¸®åŠ©å®‰å…¨å›¢é˜Ÿæ›´é«˜æ•ˆåœ°åº”å¯¹æ–°å…´å¨èƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–è§£é‡Šæ¼æ´ä¿¡æ¯æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§é‡å¤æ‚æ¼æ´æ¸…å•æ—¶ã€‚</li>
<li>åœ¨åˆ©ç›Šç›¸å…³è€…ç‰¹å®šæ¼æ´åˆ†ç±»ï¼ˆSSVCï¼‰æ¡†æ¶çš„å†³ç­–ç‚¹é¢„æµ‹ä¸­è¯„ä¼°äº†å››ç§LLMæ¨¡å‹ï¼Œå‘ç°Geminiåœ¨å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>ä½¿ç”¨èŒƒä¾‹æç¤ºå¯ä»¥æé«˜æ¨¡å‹å‡†ç¡®æ€§ï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨æŸäº›å†³ç­–ç‚¹ä¸Šä»æœ‰å›°éš¾ã€‚</li>
<li>å½“å‰LLMsæ— æ³•å®Œå…¨æ›¿ä»£ä¸“å®¶åˆ¤æ–­ï¼Œä½†åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹å¯ä½œä¸ºæœ‰æ•ˆæ”¯æŒå·¥å…·ã€‚</li>
<li>é€‚å½“åº”ç”¨LLMsæœ‰åŠ©äºä¼˜åŒ–æ¼æ´ä¼˜å…ˆçº§æ’åºå·¥ä½œæµç¨‹ï¼Œæé«˜å®‰å…¨å›¢é˜Ÿå“åº”æ–°å…´å¨èƒçš„æ•ˆç‡ã€‚</li>
<li>æ·±åº¦åˆ†æå‘ç°ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨é¢„æµ‹é£é™©æ–¹é¢å­˜åœ¨è¿‡åº¦é¢„æµ‹çš„è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-056c93d9a9f65333b9635888b58ad957" align="middle">
<img src="https://picx.zhimg.com/v2-8fc16a7e77497cdfd4c4adfc4fbb6bcc" align="middle">
<img src="https://picx.zhimg.com/v2-60b9faa87016eec00b2df1e99bcc97d8" align="middle">
<img src="https://picx.zhimg.com/v2-60a6db3e0103ab9c18de4ef48b1df7d3" align="middle">
<img src="https://picx.zhimg.com/v2-b50687f9880e5d84d578d8973f62f048" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TabR1-Taming-GRPO-for-tabular-reasoning-LLMs"><a href="#TabR1-Taming-GRPO-for-tabular-reasoning-LLMs" class="headerlink" title="TabR1: Taming GRPO for tabular reasoning LLMs"></a>TabR1: Taming GRPO for tabular reasoning LLMs</h2><p><strong>Authors:Pengxiang Cai, Zihao Gao, Jintai Chen</strong></p>
<p>Tabular prediction has traditionally relied on gradient-boosted decision trees and specialized deep learning models, which excel within tasks but provide limited interpretability and weak transfer across tables. Reasoning large language models (LLMs) promise cross-task adaptability with trans- parent reasoning traces, yet their potential has not been fully realized for tabular data. This paper presents TabR1, the first reasoning LLM for tabular prediction with multi-step reasoning. At its core is Permutation Relative Policy Optimization (PRPO), a simple yet efficient reinforcement learning method that encodes column-permutation invariance as a structural prior. By construct- ing multiple label-preserving permutations per sample and estimating advantages both within and across permutations, PRPO transforms sparse rewards into dense learning signals and improves generalization. With limited supervision, PRPO activates the reasoning ability of LLMs for tabular prediction, enhancing few-shot and zero-shot performance as well as interpretability. Comprehensive experiments demonstrate that TabR1 achieves performance comparable to strong baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1 approaches the performance of strong baselines under the 32-shot setting. Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B). </p>
<blockquote>
<p>è¡¨æ ¼é¢„æµ‹ä¼ ç»Ÿä¸Šä¾èµ–äºæ¢¯åº¦æå‡å†³ç­–æ ‘å’Œä¸“ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ä»»åŠ¡å†…è¡¨ç°ä¼˜å¼‚ï¼Œä½†æä¾›æœ‰é™çš„è§£é‡Šæ€§ï¼Œå¹¶ä¸”åœ¨è·¨è¡¨æ ¼æ—¶çš„è¿ç§»èƒ½åŠ›è¾ƒå¼±ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ‰¿è¯ºè·¨ä»»åŠ¡é€‚åº”æ€§ï¼Œå…·æœ‰é€æ˜çš„æ¨ç†è½¨è¿¹ï¼Œä½†å®ƒä»¬å¯¹è¡¨æ ¼æ•°æ®çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†å®ç°ã€‚æœ¬æ–‡æå‡ºäº†TabR1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¡¨æ ¼é¢„æµ‹çš„å¤šæ­¥æ¨ç†çš„æ¨ç†LLMã€‚å…¶æ ¸å¿ƒæ˜¯æ’åˆ—ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆPRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ƒå°†åˆ—æ’åˆ—ä¸å˜æ€§ç¼–ç ä¸ºç»“æ„å…ˆéªŒã€‚é€šè¿‡å¯¹æ¯ä¸ªæ ·æœ¬æ„å»ºå¤šä¸ªæ ‡ç­¾ä¿ç•™æ’åˆ—ï¼Œå¹¶åœ¨æ’åˆ—å†…éƒ¨å’Œä¹‹é—´ä¼°è®¡ä¼˜åŠ¿ï¼ŒPRPOå°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†çš„å­¦ä¹ ä¿¡å·å¹¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ‰é™çš„ç›‘ç£ä¸‹ï¼ŒPRPOæ¿€æ´»äº†LLMçš„æ¨ç†èƒ½åŠ›è¿›è¡Œè¡¨æ ¼é¢„æµ‹ï¼Œæé«˜äº†å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬çš„æ€§èƒ½ä»¥åŠå¯è§£é‡Šæ€§ã€‚å…¨é¢çš„å®éªŒè¡¨æ˜ï¼ŒTabR1åœ¨å…¨ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°äº†ä¸å¼ºå¤§åŸºå‡†çº¿ç›¸å½“çš„æ€§èƒ½ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼ŒTabR1çš„æ€§èƒ½æ¥è¿‘åœ¨32æ ·æœ¬è®¾ç½®ä¸‹çš„å¼ºå¤§åŸºçº¿ã€‚æ­¤å¤–ï¼ŒTabR1ï¼ˆ8Bï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šå¤§å¹…ä¼˜äºæ›´å¤§çš„LLMsï¼Œç›¸å¯¹äºDeepSeek-R1ï¼ˆ685Bï¼‰å®ç°äº†é«˜è¾¾53.17%çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17385v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTabR1çš„è¡¨æ ¼é¢„æµ‹æ¨ç†æ¨¡å‹ï¼Œå®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ­¥æ¨ç†æŠ€æœ¯ã€‚æ¨¡å‹çš„æ ¸å¿ƒæ˜¯Permutation Relative Policy Optimizationï¼ˆPRPOï¼‰æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿåœ¨æœ‰é™ç›‘ç£ä¸‹æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¢å¼ºè¡¨æ ¼é¢„æµ‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒTabR1åœ¨å…¨ç›‘ç£å¾®è°ƒä¸‹çš„æ€§èƒ½ä¸å¼ºåŸºçº¿ç›¸å½“ï¼Œé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½æ¥è¿‘32æ ·æœ¬è®¾ç½®ä¸‹çš„å¼ºåŸºçº¿ã€‚æ­¤å¤–ï¼ŒTabR1ï¼ˆ8Bï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæ›´å¤§çš„LLMsï¼Œæœ€é«˜æå‡äº†53.17%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TabR1æ˜¯é¦–ä¸ªé’ˆå¯¹è¡¨æ ¼é¢„æµ‹è®¾è®¡çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç»“åˆäº†å¤šæ­¥æ¨ç†æŠ€æœ¯ã€‚</li>
<li>PRPOæ˜¯è¯¥æ¨¡å‹çš„æ ¸å¿ƒï¼Œæ˜¯ä¸€ç§ç®€å•é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå°†åˆ—ç½®æ¢ä¸å˜æ€§ä½œä¸ºç»“æ„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>PRPOé€šè¿‡æ„å»ºæ¯ä¸ªæ ·æœ¬çš„å¤šä¸ªæ ‡ç­¾ä¿ç•™ç½®æ¢ï¼Œå¹¶åœ¨ç½®æ¢å†…éƒ¨å’Œä¹‹é—´ä¼°è®¡ä¼˜åŠ¿ï¼Œå°†ç¨€ç–å¥–åŠ±è½¬åŒ–ä¸ºå¯†é›†å­¦ä¹ ä¿¡å·ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨æœ‰é™ç›‘ç£ä¸‹ï¼ŒPRPOæ¿€æ´»äº†LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œç”¨äºè¡¨æ ¼é¢„æµ‹ï¼Œæé«˜äº†å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬æ€§èƒ½ä»¥åŠå¯è§£é‡Šæ€§ã€‚</li>
<li>ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒTabR1åœ¨å…¨ç›‘ç£å¾®è°ƒä¸‹çš„æ€§èƒ½ä¸å¼ºåŸºçº¿ç›¸å½“ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½æ¥è¿‘ç”šè‡³è¾¾åˆ°ä¸­ç­‰æ•°æ®é‡ä¸‹çš„åŸºçº¿æ€§èƒ½ã€‚</li>
<li>TabR1ï¼ˆ8Bï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºæ›´å¤§çš„LLMsï¼Œå¯¹æŸäº›ä»»åŠ¡æœ‰é«˜è¾¾53.17%çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fd7813e7d742acb1ff4a2e900cc5a42" align="middle">
<img src="https://picx.zhimg.com/v2-8033dabbae94edd65d15c0b313054964" align="middle">
<img src="https://picx.zhimg.com/v2-4f210f25ffe34e304b5f43fb6ba01d64" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Graph-Few-Shot-Learning-via-Adaptive-Spectrum-Experts-and-Cross-Set-Distribution-Calibration"><a href="#Graph-Few-Shot-Learning-via-Adaptive-Spectrum-Experts-and-Cross-Set-Distribution-Calibration" class="headerlink" title="Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set   Distribution Calibration"></a>Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set   Distribution Calibration</h2><p><strong>Authors:Yonghao Liu, Yajun Wang, Chunli Guo, Wei Pang, Ximing Li, Fausto Giunchiglia, Xiaoyue Feng, Renchu Guan</strong></p>
<p>Graph few-shot learning has attracted increasing attention due to its ability to rapidly adapt models to new tasks with only limited labeled nodes. Despite the remarkable progress made by existing graph few-shot learning methods, several key limitations remain. First, most current approaches rely on predefined and unified graph filters (e.g., low-pass or high-pass filters) to globally enhance or suppress node frequency signals. Such fixed spectral operations fail to account for the heterogeneity of local topological structures inherent in real-world graphs. Moreover, these methods often assume that the support and query sets are drawn from the same distribution. However, under few-shot conditions, the limited labeled data in the support set may not sufficiently capture the complex distribution of the query set, leading to suboptimal generalization. To address these challenges, we propose GRACE, a novel Graph few-shot leaRning framework that integrates Adaptive spectrum experts with Cross-sEt distribution calibration techniques. Theoretically, the proposed approach enhances model generalization by adapting to both local structural variations and cross-set distribution calibration. Empirically, GRACE consistently outperforms state-of-the-art baselines across a wide range of experimental settings. Our code can be found here. </p>
<blockquote>
<p>å›¾å°‘é‡å­¦ä¹ å·²ç»å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ï¼Œå› ä¸ºå®ƒåªéœ€è¦æœ‰é™çš„æ ‡è®°èŠ‚ç‚¹å°±èƒ½è¿…é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚å°½ç®¡ç°æœ‰çš„å›¾å°‘é‡å­¦ä¹ æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä»å­˜åœ¨ä¸€äº›å…³é”®å±€é™æ€§ã€‚é¦–å…ˆï¼Œå¤§å¤šæ•°å½“å‰æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰å’Œç»Ÿä¸€çš„å›¾æ»¤æ³¢å™¨ï¼ˆä¾‹å¦‚ä½é€šæˆ–é«˜é€šæ»¤æ³¢å™¨ï¼‰æ¥å…¨å±€å¢å¼ºæˆ–æŠ‘åˆ¶èŠ‚ç‚¹é¢‘ç‡ä¿¡å·ã€‚è¿™ç§å›ºå®šçš„è°±æ“ä½œå¿½ç•¥äº†çœŸå®ä¸–ç•Œå›¾ä¸­å›ºæœ‰çš„å±€éƒ¨æ‹“æ‰‘ç»“æ„çš„å¼‚è´¨æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å‡è®¾æ”¯æ’‘é›†å’ŒæŸ¥è¯¢é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒã€‚ç„¶è€Œï¼Œåœ¨å°‘é‡æ ·æœ¬æ¡ä»¶ä¸‹ï¼Œæ”¯æ’‘é›†ä¸­æœ‰é™çš„æ ‡è®°æ•°æ®å¯èƒ½ä¸è¶³ä»¥æ•æ‰æŸ¥è¯¢é›†çš„å¤æ‚åˆ†å¸ƒï¼Œå¯¼è‡´æ¬¡ä¼˜æ³›åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GRACEï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å›¾å°‘é‡å­¦ä¹ æ¡†æ¶ï¼Œèåˆäº†è‡ªé€‚åº”è°±ä¸“å®¶ä¸è·¨é›†åˆ†å¸ƒæ ¡å‡†æŠ€æœ¯ã€‚ç†è®ºä¸Šï¼Œè¯¥æ–¹æ³•é€šè¿‡é€‚åº”å±€éƒ¨ç»“æ„å˜åŒ–å’Œè·¨é›†åˆ†å¸ƒæ ¡å‡†æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»éªŒä¸Šï¼ŒGRACEåœ¨å¹¿æ³›çš„å®éªŒè®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12140v2">PDF</a> NeurIPS25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾å°‘æ ·æœ¬å­¦ä¹ é¢†åŸŸçš„ä¸€ä¸ªæ–°æ¡†æ¶GRACEã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨é¢å¯¹å¤æ‚ç°å®å›¾ç»“æ„æ—¶çš„å±€é™æ€§ï¼Œå¦‚å›ºå®šçš„è°±æ“ä½œæ— æ³•é€‚åº”å±€éƒ¨æ‹“æ‰‘ç»“æ„çš„å¼‚è´¨æ€§ï¼Œä»¥åŠå‡è®¾æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒçš„é—®é¢˜ã€‚GRACEé€šè¿‡é›†æˆè‡ªé€‚åº”è°±ä¸“å®¶å’Œè·¨é›†åˆ†å¸ƒæ ¡å‡†æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾å°‘æ ·æœ¬å­¦ä¹ èƒ½å¤Ÿåˆ©ç”¨æœ‰é™æ ‡è®°èŠ‚ç‚¹å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨é¢„å®šä¹‰çš„ç»Ÿä¸€å›¾æ»¤æ³¢å™¨æ¥å…¨å±€å¢å¼ºæˆ–æŠ‘åˆ¶èŠ‚ç‚¹é¢‘ç‡ä¿¡å·ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œä½†åœ¨å°‘æ ·æœ¬æ¡ä»¶ä¸‹ï¼Œè¿™å¯èƒ½ä¸æˆç«‹ã€‚</li>
<li>GRACEæ˜¯ä¸€ä¸ªæ–°çš„å›¾å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”è°±ä¸“å®¶å’Œè·¨é›†åˆ†å¸ƒæ ¡å‡†æŠ€æœ¯è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>GRACEèƒ½å¢å¼ºæ¨¡å‹å¯¹å±€éƒ¨ç»“æ„å˜åŒ–å’Œè·¨é›†åˆ†å¸ƒå·®å¼‚çš„é€‚åº”æ€§ã€‚</li>
<li>GRACEåœ¨å¹¿æ³›çš„å®éªŒè®¾ç½®ä¸‹ï¼Œæ€§èƒ½è¡¨ç°è¶…è¿‡ç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72574cf134568d634b2050e6a1ad0bb1" align="middle">
<img src="https://picx.zhimg.com/v2-34df968fc19ea0b9546f90c498122cee" align="middle">
<img src="https://picx.zhimg.com/v2-d9e62e180fdaebe9b0caf4c42733295a" align="middle">
<img src="https://picx.zhimg.com/v2-be8d2a9e6e39492b2873df1796e47c5a" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="mmWalk-Towards-Multi-modal-Multi-view-Walking-Assistance"><a href="#mmWalk-Towards-Multi-modal-Multi-view-Walking-Assistance" class="headerlink" title="mmWalk: Towards Multi-modal Multi-view Walking Assistance"></a>mmWalk: Towards Multi-modal Multi-view Walking Assistance</h2><p><strong>Authors:Kedi Ying, Ruiping Liu, Chongyan Chen, Mingzhe Tao, Hao Shi, Kailun Yang, Jiaming Zhang, Rainer Stiefelhagen</strong></p>
<p>Walking assistance in extreme or complex environments remains a significant challenge for people with blindness or low vision (BLV), largely due to the lack of a holistic scene understanding. Motivated by the real-world needs of the BLV community, we build mmWalk, a simulated multi-modal dataset that integrates multi-view sensor and accessibility-oriented features for outdoor safe navigation. Our dataset comprises 120 manually controlled, scenario-categorized walking trajectories with 62k synchronized frames. It contains over 559k panoramic images across RGB, depth, and semantic modalities. Furthermore, to emphasize real-world relevance, each trajectory involves outdoor corner cases and accessibility-specific landmarks for BLV users. Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual question-answer triplets across 9 categories tailored for safe and informed walking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs) using zero- and few-shot settings and found they struggle with our risk assessment and navigational tasks. We validate our mmWalk-finetuned model on real-world datasets and show the effectiveness of our dataset for advancing multi-modal walking assistance. </p>
<blockquote>
<p>å¯¹äºç›²äººæˆ–è§†åŠ›å—æŸï¼ˆBLVï¼‰äººç¾¤æ¥è¯´ï¼Œåœ¨æç«¯æˆ–å¤æ‚ç¯å¢ƒä¸­æä¾›æ­¥è¡Œè¾…åŠ©ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹å¯¹æ•´ä½“åœºæ™¯çš„ç†è§£ã€‚å—BLVç¤¾åŒºå®é™…éœ€æ±‚çš„é©±åŠ¨ï¼Œæˆ‘ä»¬æ„å»ºäº†mmWalkï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œé›†æˆäº†å¤šè§†å›¾ä¼ æ„Ÿå™¨å’Œé¢å‘å¯è®¿é—®æ€§çš„ç‰¹å¾ï¼Œç”¨äºå®¤å¤–å®‰å…¨å¯¼èˆªã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«120ä¸ªæ‰‹åŠ¨æ§åˆ¶ã€æŒ‰åœºæ™¯åˆ†ç±»çš„è¡Œèµ°è½¨è¿¹ï¼Œå…±62kåŒæ­¥å¸§ã€‚å®ƒåŒ…å«äº†è¶…è¿‡55.9ä¸‡å¼ å…¨æ™¯å›¾åƒï¼Œæ¶µç›–äº†RGBã€æ·±åº¦å’Œè¯­ä¹‰æ¨¡å¼ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¼ºè°ƒç°å®ä¸–ç•Œçš„å…³è”æ€§ï¼Œæ¯æ¡è½¨è¿¹éƒ½æ¶‰åŠå®¤å¤–è§’è½æƒ…å†µå’Œé’ˆå¯¹BLVç”¨æˆ·çš„ç‰¹å®šå¯è®¿é—®æ€§åœ°æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç”Ÿæˆäº†mmWalkVQAï¼Œè¿™æ˜¯ä¸€ä¸ªé‡èº«å®šåˆ¶äºå®‰å…¨å’Œæœ‰çŸ¥è¯†çš„æ­¥è¡Œè¾…åŠ©çš„VQAåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡6.9ä¸‡ä¸ªè§†è§‰é—®ç­”ä¸‰å…ƒç»„ï¼Œåˆ†ä¸º9ç±»ã€‚æˆ‘ä»¬è¯„ä¼°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„æœ€æ–°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå‘ç°å®ƒä»¬åœ¨æˆ‘ä»¬çš„é£é™©è¯„ä¼°å’Œå¯¼èˆªä»»åŠ¡æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸ŠéªŒè¯äº†ç»è¿‡mmWalkè®­ç»ƒçš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ•°æ®é›†åœ¨æ¨è¿›å¤šæ¨¡å¼æ­¥è¡Œè¾…åŠ©æ–¹é¢çš„ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11520v2">PDF</a> Accepted by NeurIPS 2025 Datasets and Benchmarks Track. Data and   Code: <a target="_blank" rel="noopener" href="https://github.com/KediYing/mmWalk">https://github.com/KediYing/mmWalk</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹ç›²äººæˆ–ä½è§†åŠ›äººç¾¤åœ¨æç«¯æˆ–å¤æ‚ç¯å¢ƒä¸­è¡Œèµ°çš„æ¨¡æ‹Ÿå¤šæ¨¡æ€æ•°æ®é›†mmWalkã€‚è¯¥æ•°æ®é›†èåˆäº†å¤šè§†è§’ä¼ æ„Ÿå™¨å’Œé¢å‘æ— éšœç¢åŠŸèƒ½çš„ç‰¹ç‚¹ï¼ŒåŒ…å«æˆ·å¤–å®‰å…¨å¯¼èˆªçš„å¤šç§åœºæ™¯è½¨è¿¹å’Œå›¾åƒæ•°æ®ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¼ºè°ƒå®é™…åº”ç”¨ï¼Œæ•°æ®é›†ä¸­è¿˜åŒ…æ‹¬å®¤å¤–ç‰¹æ®Šæƒ…å†µå’Œæ— éšœç¢ç‰¹å®šåœ°æ ‡ä¿¡æ¯ã€‚ç ”ç©¶è¯„ä¼°äº†å¤šç§å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶å‘ç°å®ƒä»¬åœ¨é£é™©è¯„ä¼°å’Œå¯¼èˆªä»»åŠ¡æ–¹é¢çš„è¡¨ç°ä¸è¶³ã€‚æœ€åéªŒè¯äº†ä½¿ç”¨mmWalkè¿›è¡Œå¾®è°ƒæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹ç›²äººæˆ–ä½è§†åŠ›äººç¾¤åœ¨æç«¯æˆ–å¤æ‚ç¯å¢ƒä¸­è¡Œèµ°çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†æ¨¡æ‹Ÿå¤šæ¨¡æ€æ•°æ®é›†mmWalkã€‚</li>
<li>mmWalké›†æˆäº†å¤šè§†è§’ä¼ æ„Ÿå™¨å’Œé¢å‘æ— éšœç¢åŠŸèƒ½çš„ç‰¹ç‚¹ï¼Œç”¨äºæˆ·å¤–å®‰å…¨å¯¼èˆªã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šç§åœºæ™¯è½¨è¿¹å’Œå›¾åƒæ•°æ®ï¼Œå¼ºè°ƒå®¤å¤–ç‰¹æ®Šæƒ…å†µå’Œæ— éšœç¢ç‰¹å®šåœ°æ ‡ä¿¡æ¯çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é£é™©è¯„ä¼°å’Œå¯¼èˆªä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°å®ƒä»¬é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„éªŒè¯ï¼Œè¯æ˜äº†ä½¿ç”¨mmWalkè¿›è¡Œå¾®è°ƒæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºæé«˜å¤šæ¨¡æ€è¡Œèµ°è¾…åŠ©æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a743c2f3a4f16d333a94d9de190a75d6" align="middle">
<img src="https://picx.zhimg.com/v2-9d81e0c53fcfa312ce3d574dcc1ed621" align="middle">
<img src="https://picx.zhimg.com/v2-f6483bee28876a0969774ff8a2b25905" align="middle">
<img src="https://picx.zhimg.com/v2-e4a11b1527ce33ac87169f296a2846ef" align="middle">
<img src="https://picx.zhimg.com/v2-94461524762b81e96d97d2259a626a8a" align="middle">
<img src="https://picx.zhimg.com/v2-bf19f3b1864cda3827301bb4925f7306" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning"><a href="#VT-FSL-Bridging-Vision-and-Text-with-LLMs-for-Few-Shot-Learning" class="headerlink" title="VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning"></a>VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning</h2><p><strong>Authors:Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin</strong></p>
<p>Few-shot learning (FSL) aims to recognize novel concepts from only a few labeled support samples. Recent studies enhance support features by incorporating additional semantic information or designing complex semantic fusion modules. However, they still suffer from hallucinating semantics that contradict the visual evidence due to the lack of grounding in actual instances, resulting in noisy guidance and costly corrections. To address these issues, we propose a novel framework, bridging Vision and Text with LLMs for Few-Shot Learning (VT-FSL), which constructs precise cross-modal prompts conditioned on Large Language Models (LLMs) and support images, seamlessly integrating them through a geometry-aware alignment. It mainly consists of Cross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment (CGA). Specifically, the CIP conditions an LLM on both class names and support images to generate precise class descriptions iteratively in a single structured reasoning pass. These descriptions not only enrich the semantic understanding of novel classes but also enable the zero-shot synthesis of semantically consistent images. The descriptions and synthetic images act respectively as complementary textual and visual prompts, providing high-level class semantics and low-level intra-class diversity to compensate for limited support data. Furthermore, the CGA jointly aligns the fused textual, support, and synthetic visual representations by minimizing the kernelized volume of the 3-dimensional parallelotope they span. It captures global and nonlinear relationships among all representations, enabling structured and consistent multimodal integration. The proposed VT-FSL method establishes new state-of-the-art performance across ten diverse benchmarks, including standard, cross-domain, and fine-grained few-shot learning scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL">https://github.com/peacelwh/VT-FSL</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨ä»ä»…æœ‰çš„å‡ ä¸ªæ ‡è®°æ”¯æŒæ ·æœ¬ä¸­è¯†åˆ«å‡ºæ–°æ¦‚å¿µã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡èå…¥é¢å¤–çš„è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚çš„è¯­ä¹‰èåˆæ¨¡å—æ¥å¢å¼ºæ”¯æŒç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åœ¨å®é™…å®ä¾‹ä¸­çš„åŸºç¡€ï¼Œå®ƒä»¬ä»ç„¶ä¼šé­å—ä¸è§†è§‰è¯æ®ç›¸çŸ›ç›¾çš„å¹»è§‰è¯­ä¹‰çš„å›°æ‰°ï¼Œå¯¼è‡´äº§ç”Ÿå˜ˆæ‚çš„æŒ‡å¯¼å’Œæ˜‚è´µçš„ä¿®æ­£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè§†è§‰ä¸æ–‡æœ¬æ¡¥æ¥çš„å°‘é‡å­¦ä¹ ï¼ˆVT-FSLï¼‰ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ç²¾ç¡®è·¨æ¨¡æ€æç¤ºï¼Œè¿™äº›æç¤ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ”¯æ’‘å›¾åƒçš„æ¡ä»¶ï¼Œå¹¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥å¯¹é½æ— ç¼é›†æˆã€‚å®ƒä¸»è¦ç”±è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ç»„æˆã€‚å…·ä½“è€Œè¨€ï¼ŒCIPåœ¨ç±»åå’Œæ”¯æ’‘å›¾åƒçš„åŸºç¡€ä¸Šå¯¹LLMè¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»¥åœ¨å•ä¸ªç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£ç”Ÿæˆç²¾ç¡®çš„ç±»æè¿°ã€‚è¿™äº›æè¿°ä¸ä»…ä¸°å¯Œäº†å¯¹æ–°é¢–ç±»çš„è¯­ä¹‰ç†è§£ï¼Œè¿˜å®ç°äº†è¯­ä¹‰ä¸€è‡´å›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚è¿™äº›æè¿°å’Œåˆæˆå›¾åƒåˆ†åˆ«ä½œä¸ºè¡¥å……çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§ç±»è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æ’‘æ•°æ®ã€‚æ­¤å¤–ï¼ŒCGAé€šè¿‡æœ€å°åŒ–å®ƒä»¬æ‰€è·¨è¶Šçš„3ç»´å¹³è¡Œå››è¾¹å½¢çš„æ ¸åŒ–ä½“ç§¯æ¥è”åˆå¯¹é½èåˆçš„æ–‡æœ¬ã€æ”¯æ’‘å’Œåˆæˆè§†è§‰è¡¨ç¤ºã€‚å®ƒæ•æ‰äº†æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œå®ç°äº†ç»“æ„åŒ–ä¸”ä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚æ‰€æå‡ºçš„VT-FSLæ–¹æ³•åœ¨åŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç»†ç²’åº¦å°‘é‡å­¦ä¹ åœºæ™¯åœ¨å†…çš„åä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/peacelwh/VT-FSL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/peacelwh/VT-FSLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25033v3">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å°‘æ•°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰æ—¨åœ¨ä»å°‘é‡æ ‡è®°æ ·æœ¬ä¸­è¯†åˆ«æ–°æ¦‚å¿µã€‚å°½ç®¡ç°æœ‰ç ”ç©¶é€šè¿‡èå…¥é¢å¤–è¯­ä¹‰ä¿¡æ¯æˆ–è®¾è®¡å¤æ‚è¯­ä¹‰èåˆæ¨¡å—æ¥å¢å¼ºæ”¯æŒç‰¹å¾ï¼Œä½†å®ƒä»¬ä»é¢ä¸´å› ç¼ºä¹å®é™…å®ä¾‹çš„æ”¯æ’‘è€Œå¯¼è‡´çš„è¯­ä¹‰çŸ›ç›¾é—®é¢˜ï¼Œäº§ç”Ÿè¯¯å¯¼æ€§çš„æŒ‡å¯¼å¹¶éœ€è¦æ˜‚è´µçš„ä¿®æ­£ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†è§‰ä¸æ–‡æœ¬æ¡¥æ¥å°‘æ•°æ ·æœ¬å­¦ä¹ ï¼ˆVT-FSLï¼‰ã€‚å®ƒæ„å»ºç²¾ç¡®çš„è·¨æ¨¡æ€æç¤ºï¼Œä»¥æ”¯æŒå›¾åƒå’ŒLLMä¸ºæ¡ä»¶ï¼Œæ— ç¼é›†æˆå®ƒä»¬é€šè¿‡ä¸€ä¸ªå‡ ä½•æ„ŸçŸ¥å¯¹é½ã€‚å®ƒä¸»è¦åŒ…æ‹¬è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰å’Œè·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰ã€‚å…·ä½“è€Œè¨€ï¼ŒCIPä»¥ç±»åå’Œå›¾åƒä¸ºæ¡ä»¶æ¥ç”Ÿæˆç²¾ç¡®ç±»æè¿°ï¼Œè¿™äº›æè¿°ä¸ä»…ä¸°å¯Œäº†å¯¹æ–°é¢–ç±»çš„è¯­ä¹‰ç†è§£ï¼Œè¿˜å®ç°äº†è¯­ä¹‰ä¸€è‡´åˆæˆå›¾åƒçš„é›¶æ ·æœ¬åˆæˆã€‚æè¿°å’Œåˆæˆå›¾åƒåˆ†åˆ«ä½œä¸ºäº’è¡¥çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§ç±»è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§ï¼Œä»¥å¼¥è¡¥æœ‰é™çš„æ”¯æŒæ•°æ®ã€‚æ­¤å¤–ï¼ŒCGAè”åˆå¯¹é½èåˆçš„æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡æœ€å°åŒ–å®ƒä»¬æ‰€è·¨è¶Šçš„å¹³è¡Œå››è¾¹å½¢çš„æ ¸åŒ–ä½“ç§¯æ¥æ•æ‰æ‰€æœ‰è¡¨ç¤ºä¹‹é—´çš„å…¨å±€å’Œéçº¿æ€§å…³ç³»ï¼Œä»è€Œå®ç°ç»“æ„åŒ–ä¸”ä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚VT-FSLæ–¹æ³•åœ¨æ–°çš„åä¸ªå¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç²¾ç»†å°‘æ•°æ ·æœ¬å­¦ä¹ åœºæ™¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VT-FSLæ¡†æ¶ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå°‘æ•°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ï¼Œè§£å†³ç°æœ‰æ¨¡å‹çš„è¯­ä¹‰çŸ›ç›¾é—®é¢˜ã€‚</li>
<li>è·¨æ¨¡æ€è¿­ä»£æç¤ºï¼ˆCIPï¼‰ç»“åˆäº†ç±»åå’Œå›¾åƒæ¥ç”Ÿæˆç²¾ç¡®çš„ç±»æè¿°ï¼Œå¢å¼ºäº†è¯­ä¹‰ç†è§£å¹¶å®ç°äº†é›¶æ ·æœ¬å›¾åƒåˆæˆã€‚</li>
<li>æè¿°å’Œåˆæˆå›¾åƒä½œä¸ºäº’è¡¥çš„æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œæä¾›é«˜çº§è¯­ä¹‰å’Œä½çº§ç±»å†…å¤šæ ·æ€§ï¼Œå¼¥è¡¥æœ‰é™çš„æ”¯æŒæ•°æ®ã€‚</li>
<li>è·¨æ¨¡æ€å‡ ä½•å¯¹é½ï¼ˆCGAï¼‰è”åˆå¯¹é½æ–‡æœ¬ã€æ”¯æŒå’Œåˆæˆè§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡æ•æ‰å…¨å±€å’Œéçº¿æ€§å…³ç³»å®ç°ä¸€è‡´çš„å¤šæ¨¡æ€é›†æˆã€‚</li>
<li>VT-FSLæ–¹æ³•å®ç°äº†åœ¨æ–°çš„åä¸ªå¤šæ ·åŒ–åŸºå‡†æµ‹è¯•ä¸­çš„æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬æ ‡å‡†ã€è·¨åŸŸå’Œç²¾ç»†å°‘æ•°æ ·æœ¬å­¦ä¹ åœºæ™¯ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç»“åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå°‘æ•°æ ·æœ¬å­¦ä¹ ä»»åŠ¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-816112b9892e4b477a73cc6c7f04e089" align="middle">
<img src="https://picx.zhimg.com/v2-b261842883675a1c84bef073d220ab93" align="middle">
<img src="https://picx.zhimg.com/v2-fda6a8e6593952cbc22784b0fba62ed6" align="middle">
<img src="https://picx.zhimg.com/v2-76bea1641940b6f14a37bc0429de3487" align="middle">
<img src="https://picx.zhimg.com/v2-1a09d3087398134cf568e19fcf367318" align="middle">
<img src="https://picx.zhimg.com/v2-4533d12fcf2349a58782783e5e4e8bed" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PlantSegNeRF-A-few-shot-cross-species-method-for-plant-3D-instance-point-cloud-reconstruction-via-joint-channel-NeRF-with-multi-view-image-instance-matching"><a href="#PlantSegNeRF-A-few-shot-cross-species-method-for-plant-3D-instance-point-cloud-reconstruction-via-joint-channel-NeRF-with-multi-view-image-instance-matching" class="headerlink" title="PlantSegNeRF: A few-shot, cross-species method for plant 3D instance   point cloud reconstruction via joint-channel NeRF with multi-view image   instance matching"></a>PlantSegNeRF: A few-shot, cross-species method for plant 3D instance   point cloud reconstruction via joint-channel NeRF with multi-view image   instance matching</h2><p><strong>Authors:Xin Yang, Ruiming Du, Hanyang Huang, Jiayang Xie, Pengyao Xie, Leisen Fang, Ziyue Guo, Nanjun Jiang, Yu Jiang, Haiyan Cen</strong></p>
<p>Organ segmentation of plant point clouds is a prerequisite for the high-resolution and accurate extraction of organ-level phenotypic traits. Although the fast development of deep learning has boosted much research on segmentation of plant point clouds, the existing techniques for organ segmentation still face limitations in resolution, segmentation accuracy, and generalizability across various plant species. In this study, we proposed a novel approach called plant segmentation neural radiance fields (PlantSegNeRF), aiming to directly generate high-precision instance point clouds from multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF performed 2D instance segmentation on the multi-view images to generate instance masks for each organ with a corresponding ID. The multi-view instance IDs corresponding to the same plant organ were then matched and refined using a specially designed instance matching module. The instance NeRF was developed to render an implicit scene, containing color, density, semantic and instance information. The implicit scene was ultimately converted into high-precision plant instance point clouds based on the volume density. The results proved that in semantic segmentation of point clouds, PlantSegNeRF outperformed the commonly used methods, demonstrating an average improvement of 16.1%, 18.3%, 17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the second-best results on structurally complex species. More importantly, PlantSegNeRF exhibited significant advantages in plant point cloud instance segmentation tasks. Across all plant species, it achieved average improvements of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively. This study extends the organ-level plant phenotyping and provides a high-throughput way to supply high-quality 3D data for the development of large-scale models in plant science. </p>
<blockquote>
<p>æ¤ç‰©ç‚¹äº‘å™¨å®˜åˆ†å‰²æ˜¯å®ç°é«˜åˆ†è¾¨ç‡å’Œç²¾ç¡®æå–å™¨å®˜æ°´å¹³è¡¨å‹ç‰¹å¾çš„å‰æã€‚å°½ç®¡æ·±åº¦å­¦ä¹ å¿«é€Ÿå‘å±•ï¼Œæ¨åŠ¨äº†æ¤ç‰©ç‚¹äº‘åˆ†å‰²é¢†åŸŸçš„å¤§é‡ç ”ç©¶ï¼Œä½†ç°æœ‰çš„å™¨å®˜åˆ†å‰²æŠ€æœ¯ä»é¢ä¸´åˆ†è¾¨ç‡ã€åˆ†å‰²ç²¾åº¦å’Œè·¨ç‰©ç§æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPlantSegNeRFçš„æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨ä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ç›´æ¥ç”Ÿæˆé«˜ç²¾åº¦çš„å®ä¾‹ç‚¹äº‘ï¼Œé€‚ç”¨äºå¹¿æ³›çš„æ¤ç‰©ç‰©ç§ã€‚PlantSegNeRFå¯¹å¤šè§†è§’å›¾åƒè¿›è¡ŒäºŒç»´å®ä¾‹åˆ†å‰²ï¼Œä¸ºæ¯ä¸ªå™¨å®˜ç”Ÿæˆå…·æœ‰ç›¸åº”IDçš„å®ä¾‹æ©è†œã€‚ç„¶åï¼Œä½¿ç”¨ä¸“é—¨è®¾è®¡çš„å®ä¾‹åŒ¹é…æ¨¡å—å¯¹å¯¹åº”äºåŒä¸€æ¤ç‰©å™¨å®˜çš„è·¨è§†è§’å®ä¾‹IDè¿›è¡ŒåŒ¹é…å’Œç»†åŒ–ã€‚å¼€å‘äº†å®ä¾‹NeRFæ¥å‘ˆç°åŒ…å«é¢œè‰²ã€å¯†åº¦ã€è¯­ä¹‰å’Œå®ä¾‹ä¿¡æ¯çš„éšå¼åœºæ™¯ã€‚æœ€ç»ˆï¼ŒåŸºäºä½“ç§¯å¯†åº¦å°†éšå¼åœºæ™¯è½¬æ¢ä¸ºé«˜ç²¾åº¦çš„æ¤ç‰©å®ä¾‹ç‚¹äº‘ã€‚ç»“æœè¯æ˜ï¼Œåœ¨ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ–¹é¢ï¼ŒPlantSegNeRFä¼˜äºå¸¸ç”¨æ–¹æ³•ï¼Œåœ¨ç»“æ„å¤æ‚çš„ç‰©ç§ä¸Šï¼Œç›¸è¾ƒäºç¬¬äºŒå¥½çš„ç»“æœï¼Œå…¶åœ¨ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’Œäº¤å¹¶æ¯”æ–¹é¢åˆ†åˆ«å¹³å‡æé«˜äº†16.1%ã€18.3%ã€17.8%å’Œ24.2%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒPlantSegNeRFåœ¨æ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨æ‰€æœ‰æ¤ç‰©ç‰©ç§ä¸­ï¼Œå…¶åœ¨mPrecã€mRecã€mCovå’ŒmWCovæ–¹é¢å¹³å‡åˆ†åˆ«æé«˜äº†11.7%ã€38.2%ã€32.2%å’Œ25.3%ã€‚æœ¬ç ”ç©¶æ‰©å±•äº†æ¤ç‰©å™¨å®˜æ°´å¹³çš„è¡¨å‹åˆ†æï¼Œå¹¶ä¸ºæ¤ç‰©ç§‘å­¦ä¸­å¤§è§„æ¨¡æ¨¡å‹çš„å¼€å‘æä¾›äº†ä¸€ç§æä¾›é«˜è´¨é‡3Dæ•°æ®çš„é«˜é€šé‡æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00371v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPlantSegNeRFçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ç›´æ¥ç”Ÿæˆé«˜ç²¾åº¦æ¤ç‰©å™¨å®˜ç‚¹äº‘ã€‚è¯¥æ–¹æ³•é€šè¿‡2Då®ä¾‹åˆ†å‰²ç”Ÿæˆæ¯ä¸ªå™¨å®˜çš„å®ä¾‹æ©æ¨¡å’Œå¯¹åº”IDï¼Œä½¿ç”¨ä¸“é—¨è®¾è®¡çš„å®ä¾‹åŒ¹é…æ¨¡å—è¿›è¡Œå¤šè§†è§’å®ä¾‹åŒ¹é…å’Œç»†åŒ–ï¼Œå¹¶å¼€å‘å®ä¾‹NeRFæ¸²æŸ“åŒ…å«é¢œè‰²ã€å¯†åº¦ã€è¯­ä¹‰å’Œå®ä¾‹ä¿¡æ¯çš„éšå¼åœºæ™¯ã€‚æœ€ç»ˆï¼Œæ ¹æ®ä½“ç§¯å¯†åº¦å°†éšå¼åœºæ™¯è½¬æ¢ä¸ºé«˜ç²¾åº¦æ¤ç‰©å®ä¾‹ç‚¹äº‘ã€‚åœ¨ç‚¹äº‘è¯­ä¹‰åˆ†å‰²å’Œæ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒPlantSegNeRFè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹³å‡æé«˜äº†ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒIoUç­‰æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PlantSegNeRFæ˜¯ä¸€ç§ç”¨äºæ¤ç‰©ç‚¹äº‘é«˜ç²¾åº¦å®ä¾‹åˆ†å‰²çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¤šè§†è§’RGBå›¾åƒåºåˆ—ç”Ÿæˆé«˜ç²¾åº¦æ¤ç‰©å™¨å®˜ç‚¹äº‘ã€‚</li>
<li>PlantSegNeRFé€šè¿‡2Då®ä¾‹åˆ†å‰²ç”Ÿæˆå®ä¾‹æ©æ¨¡å’Œå¯¹åº”IDï¼Œä½¿ç”¨å®ä¾‹åŒ¹é…æ¨¡å—è¿›è¡Œå¤šè§†è§’åŒ¹é…å’Œç»†åŒ–ã€‚</li>
<li>å¼€å‘äº†å®ä¾‹NeRFä»¥æ¸²æŸ“åŒ…å«é¢œè‰²ã€å¯†åº¦ã€è¯­ä¹‰å’Œå®ä¾‹ä¿¡æ¯çš„éšå¼åœºæ™¯ã€‚</li>
<li>PlantSegNeRFåœ¨è¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œæé«˜äº†å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§æ¤ç‰©ç‰©ç§ï¼Œä¸ºæ¤ç‰©ç§‘å­¦çš„å¤§è§„æ¨¡æ¨¡å‹å¼€å‘æä¾›é«˜è´¨é‡3Dæ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-711af185f90bd7649adac63ee8cfcfdf" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You"><a href="#With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You" class="headerlink" title="With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You"></a>With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You</h2><p><strong>Authors:Fabian GrÃ¶ger, Shuo Wen, Huyen Le, Maria BrbiÄ‡</strong></p>
<p>Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment, including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$\unicode{x2013}$less than $1%$ of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of $51.6%$ in classification and $91.8%$ in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ¨¡å‹åœ¨éœ€è¦å¤šæ¨¡æ€å¯¹é½çš„å¤æ‚ä»»åŠ¡ä¸­å±•ç°äº†å¼ºå¤§çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸ä¾èµ–äºæ•°ç™¾ä¸‡é…å¯¹çš„å¤šæ¨¡æ€æ ·æœ¬ï¼Œè¿™åœ¨è®¸å¤šé¢†åŸŸä¸­æ˜¯æ˜‚è´µä¸”éš¾ä»¥è·å¾—çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹é½é¢„è®­ç»ƒçš„å•æ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œæ¢ç´¢äº†ä½¿ç”¨æœ‰é™é…å¯¹æ•°æ®æ„å»ºå¤šæ¨¡æ€æ¨¡å‹çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œåªéœ€æ•°ä¸‡ä¸ªé…å¯¹æ ·æœ¬ï¼ˆä¸åˆ°è¯¥é¢†åŸŸé€šå¸¸ä½¿ç”¨æ•°æ®çš„1%ï¼‰ï¼Œå°±å¯ä»¥å®ç°é«˜è´¨é‡çš„å¯¹é½ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†STRUCTUREï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿä¿æŒå•æ¨¡æ€ç¼–ç å™¨æ½œåœ¨ç©ºé—´çš„é‚»è¿‘å‡ ä½•ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯¹é½æœ€åå‡ å±‚é€šå¸¸æ•ˆæœè¾ƒå·®ï¼Œå¹¶å±•ç¤ºäº†å¯¹é½å„æ¨¡æ€ä¸­ä»£è¡¨æ€§æœ€é«˜ç›¸ä¼¼æ€§çš„å±‚çš„å¥½å¤„ã€‚è¿™ä¸¤ä¸ªç»„ä»¶å¯ä»¥è½»æ¾åœ°èå…¥ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œåœ¨24ä¸ªé›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œåˆ†ç±»ä»»åŠ¡çš„å¹³å‡ç›¸å¯¹æ”¹è¿›ç‡ä¸º51.6%ï¼Œæ£€ç´¢ä»»åŠ¡çš„æ”¹è¿›ç‡ä¸º91.8%ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†æˆ‘ä»¬æ¡†æ¶åœ¨æœ‰é™æ ·æœ¬å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ï¼Œå¹¶ä¸ºèµ„æºå—é™é¢†åŸŸæä¾›äº†å‰æ™¯å…‰æ˜çš„æœªæ¥è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16895v2">PDF</a> NeurIPS 2025 camera-ready</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨æœ‰é™é…å¯¹æ•°æ®æ„å»ºå¤šæ¨¡æ€æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒçš„å•æ¨¡æ€åŸºç¡€æ¨¡å‹è¿›è¡Œå¯¹é½æ¥å®ç°ã€‚é€šè¿‡å¼•å…¥STRUCTURæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå®ç°äº†åœ¨ä»…ä½¿ç”¨æ•°åƒä¸ªé…å¯¹æ ·æœ¬ï¼ˆä¸åˆ°è¯¥é¢†åŸŸé€šå¸¸ä½¿ç”¨çš„æ•°æ®çš„1%ï¼‰çš„æƒ…å†µä¸‹ï¼Œå°±èƒ½è¾¾åˆ°é«˜è´¨é‡çš„å¯¹é½æ•ˆæœã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æŒ‡å‡ºå•çº¯å¯¹é½æœ€åä¸€å±‚å¹¶ä¸ç†æƒ³ï¼Œå¹¶å±•ç¤ºäº†å¯¹é½å…·æœ‰æœ€é«˜è·¨æ¨¡æ€ä»£è¡¨æ€§ç›¸ä¼¼æ€§çš„å±‚æ¬¡æ‰€å¸¦æ¥çš„ä¼˜åŠ¿ã€‚è¿™ä¸¤ä¸ªç»„æˆéƒ¨åˆ†å¯ä»¥å¾ˆå®¹æ˜“åœ°èå…¥ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œåœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œåˆ†ç±»å¹³å‡ç›¸å¯¹æå‡51.6%ï¼Œæ£€ç´¢ä»»åŠ¡æå‡91.8%ã€‚æœ¬æ–‡æ¡†æ¶å¯¹äºæœ‰é™æ ·æœ¬å¤šæ¨¡æ€å­¦ä¹ å…·æœ‰æœ‰æ•ˆæ€§å’Œå¹¿æ³›çš„åº”ç”¨æ€§ï¼Œä¸ºèµ„æºå—é™é¢†åŸŸæä¾›äº†æœ‰å‰é€”çš„å‘å±•è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„é…å¯¹æ ·æœ¬æ•°æ®ã€‚é’ˆå¯¹è¿™ä¸€ç‚¹ï¼Œæœ¬æ–‡æ¢ç´¢äº†åœ¨æœ‰é™çš„é…å¯¹æ•°æ®ä¸‹æ„å»ºå¤šæ¨¡æ€æ¨¡å‹çš„å¯è¡Œæ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥STRUCTURæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå®ç°äº†é«˜è´¨é‡çš„å¤šæ¨¡æ€å¯¹é½ï¼Œä»…ä½¿ç”¨æ•°åƒä¸ªé…å¯¹æ ·æœ¬ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºå•çº¯å¯¹é½æ¨¡å‹çš„æœ€åä¸€å±‚å¹¶ä¸ç†æƒ³ï¼Œå¼ºè°ƒäº†å¯¹é½å…·æœ‰æœ€é«˜è·¨æ¨¡æ€ä»£è¡¨æ€§ç›¸ä¼¼æ€§çš„å±‚æ¬¡çš„é‡è¦æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†æ–°æ–¹æ³•åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡èå…¥ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œæ–°æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ¡†æ¶å¯¹äºèµ„æºå—é™é¢†åŸŸå…·æœ‰ç‰¹åˆ«é‡è¦çš„æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51118d6c2182bae8254146caa9df3e57" align="middle">
<img src="https://picx.zhimg.com/v2-b308f941e570a0d9d7cae9055713f4be" align="middle">
<img src="https://picx.zhimg.com/v2-d5b5d7a3bf72e92282634cad17d024b1" align="middle">
<img src="https://picx.zhimg.com/v2-acba36ffcb61dc5d5e70131014d92cf1" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Doctor-Approved-Generating-Medically-Accurate-Skin-Disease-Images-through-AI-Expert-Feedback"><a href="#Doctor-Approved-Generating-Medically-Accurate-Skin-Disease-Images-through-AI-Expert-Feedback" class="headerlink" title="Doctor Approved: Generating Medically Accurate Skin Disease Images   through AI-Expert Feedback"></a>Doctor Approved: Generating Medically Accurate Skin Disease Images   through AI-Expert Feedback</h2><p><strong>Authors:Janet Wang, Yunbei Zhang, Zhengming Ding, Jihun Hamm</strong></p>
<p>Paucity of medical data severely limits the generalizability of diagnostic ML models, as the full spectrum of disease variability can not be represented by a small clinical dataset. To address this, diffusion models (DMs) have been considered as a promising avenue for synthetic image generation and augmentation. However, they frequently produce medically inaccurate images, deteriorating the model performance. Expert domain knowledge is critical for synthesizing images that correctly encode clinical information, especially when data is scarce and quality outweighs quantity. Existing approaches for incorporating human feedback, such as reinforcement learning (RL) and Direct Preference Optimization (DPO), rely on robust reward functions or demand labor-intensive expert evaluations. Recent progress in Multimodal Large Language Models (MLLMs) reveals their strong visual reasoning capabilities, making them adept candidates as evaluators. In this work, we propose a novel framework, coined MAGIC (Medically Accurate Generation of Images through AI-Expert Collaboration), that synthesizes clinically accurate skin disease images for data augmentation. Our method creatively translates expert-defined criteria into actionable feedback for image synthesis of DMs, significantly improving clinical accuracy while reducing the direct human workload. Experiments demonstrate that our method greatly improves the clinical quality of synthesized skin disease images, with outputs aligning with dermatologist assessments. Additionally, augmenting training data with these synthesized images improves diagnostic accuracy by +9.02% on a challenging 20-condition skin disease classification task, and by +13.89% in the few-shot setting. </p>
<blockquote>
<p>åŒ»å­¦æ•°æ®çš„åŒ®ä¹ä¸¥é‡é™åˆ¶äº†è¯Šæ–­æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºå°å‹çš„ä¸´åºŠæ•°æ®é›†æ— æ³•ä»£è¡¨ç–¾ç—…çš„å…¨éƒ¨å˜å¼‚è°±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²è¢«è§†ä¸ºåˆæˆå›¾åƒç”Ÿæˆå’Œå¢å¼ºçš„æœ‰å‰é€”çš„é€”å¾„ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸äº§ç”ŸåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒï¼Œä»è€Œé™ä½äº†æ¨¡å‹æ€§èƒ½ã€‚åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå°¤å…¶æ˜¯è´¨é‡èƒœè¿‡æ•°é‡æ—¶ï¼Œåˆ©ç”¨ä¸“ä¸šé¢†åŸŸçŸ¥è¯†åˆæˆæ­£ç¡®ç¼–ç ä¸´åºŠä¿¡æ¯çš„å›¾åƒè‡³å…³é‡è¦ã€‚ç°æœ‰çš„äººç±»åé¦ˆèå…¥æ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œéƒ½ä¾èµ–äºç¨³å¥çš„å¥–åŠ±å‡½æ•°æˆ–éœ€è¦å¤§é‡äººå·¥è¯„ä¼°ã€‚æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºå‡ºäº†å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºè¯„ä¼°è€…çš„åˆé€‚å€™é€‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºMAGICï¼ˆé€šè¿‡AI-ä¸“å®¶åä½œè¿›è¡ŒåŒ»å­¦å‡†ç¡®å›¾åƒç”Ÿæˆï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºåˆæˆç”¨äºæ•°æ®å¢å¼ºçš„ä¸´åºŠå‡†ç¡®çš®è‚¤ç—…å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ›é€ æ€§åœ°å°†ä¸“å®¶å®šä¹‰çš„æ ‡å‡†è½¬åŒ–ä¸ºå¯¹æ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆæˆçš„å¯æ“ä½œåé¦ˆï¼Œè¿™æ˜¾è‘—æé«˜äº†ä¸´åºŠå‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†ç›´æ¥äººå·¥å·¥ä½œé‡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§æé«˜äº†åˆæˆçš®è‚¤ç—…å›¾åƒçš„ä¸´åºŠè´¨é‡ï¼Œè¾“å‡ºç»“æœä¸çš®è‚¤ç§‘åŒ»ç”Ÿçš„è¯„ä¼°ç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è¿™äº›åˆæˆå›¾åƒå¢å¼ºè®­ç»ƒæ•°æ®ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„20ç§çš®è‚¤ç—…åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯Šæ–­å‡†ç¡®ç‡æé«˜äº†+9.02%ï¼Œåœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹æé«˜äº†+13.89%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12323v2">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMAGICçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡AIä¸ä¸“å®¶åˆä½œï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆæˆä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç–¾ç—…å›¾åƒç”¨äºæ•°æ®å¢å¼ºã€‚è¯¥æ–¹æ³•å°†ä¸“å®¶å®šä¹‰çš„è¯„ä¼°æ ‡å‡†è½¬åŒ–ä¸ºåˆæˆå›¾åƒçš„å®é™…åé¦ˆï¼Œä»è€Œæé«˜ä¸´åºŠå‡†ç¡®æ€§å¹¶å‡å°‘äººå·¥å·¥ä½œé‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜åˆæˆçš®è‚¤ç–¾ç—…å›¾åƒçš„ä¸´åºŠè´¨é‡ï¼Œå¹¶ä¸çš®è‚¤ç§‘åŒ»ç”Ÿçš„è¯„ä¼°ç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è¿™äº›åˆæˆå›¾åƒå¢å¼ºè®­ç»ƒæ•°æ®ï¼Œåœ¨20ç§çš®è‚¤ç–¾ç—…åˆ†ç±»ä»»åŠ¡ä¸­æé«˜äº†9.02%çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹æé«˜äº†13.89%çš„è¯Šæ–­å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æ•°æ®çš„ç¼ºä¹é™åˆ¶äº†è¯Šæ–­MLæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨åŒ»å­¦å›¾åƒç”Ÿæˆå’Œå¢å¼ºæ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å¸¸äº§ç”ŸåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒã€‚</li>
<li>ä¸“å®¶é¢†åŸŸçŸ¥è¯†å¯¹äºåˆæˆæ­£ç¡®ç¼–ç ä¸´åºŠä¿¡æ¯çš„å›¾åƒè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚</li>
<li>ç°æœ‰çš„äººç±»åé¦ˆæ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ å’Œç›´æ¥åå¥½ä¼˜åŒ–å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œé€‚åˆä½œä¸ºè¯„ä¼°è€…ã€‚</li>
<li>æå‡ºçš„MAGICæ¡†æ¶ç»“åˆäº†AIä¸ä¸“å®¶åˆä½œï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆæˆä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç–¾ç—…å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c50aa8ed00c0f2d6f2b631ab5ced920" align="middle">
<img src="https://picx.zhimg.com/v2-4a3fb120011fcfafaa2851c5fb95f845" align="middle">
<img src="https://picx.zhimg.com/v2-2fe52b39bc4308923cd78ed128ec6d42" align="middle">
<img src="https://picx.zhimg.com/v2-89706c45d139275fd6d1e54199726085" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models"><a href="#Roboflow100-VL-A-Multi-Domain-Object-Detection-Benchmark-for-Vision-Language-Models" class="headerlink" title="Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models"></a>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models</h2><p><strong>Authors:Peter Robicheaux, Matvei Popov, Anish Madan, Isaac Robinson, Joseph Nelson, Deva Ramanan, Neehar Peri</strong></p>
<p>Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Lastly, we discuss our recent CVPR 2025 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 17 mAP! Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl">https://github.com/roboflow/rf100-vl</a> and <a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a>. </p>
<blockquote>
<p>åŸºäºäº’è”ç½‘è§„æ¨¡æ•°æ®çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§å¯¹è±¡ï¼ˆå¦‚æ±½è½¦ã€å¡è½¦å’Œè¡Œäººï¼‰ä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½æ˜¾è‘—ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ¨å¹¿åˆ°å…¶é¢„è®­ç»ƒä¸­æ²¡æœ‰å‡ºç°çš„åˆ†å¸ƒå¤–çš„ç±»åˆ«ã€ä»»åŠ¡å’Œæˆåƒæ¨¡å¼æ—¶ï¼Œä»ç„¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬ä¸»å¼ ä¸åº”ä»…ä»…é€šè¿‡æ›´å¤šçš„è§†è§‰æ•°æ®é‡æ–°è®­ç»ƒVLMsï¼Œè€Œåº”è¯¥ä½¿ç”¨åŒ…å«å°‘é‡è§†è§‰ç¤ºä¾‹å’Œä¸°å¯Œæ–‡æœ¬æè¿°çš„æ–°æ¦‚å¿µå¯¹é½VLMsã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Roboflow100-VLï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªå¤šæ¨¡æ€å¯¹è±¡æ£€æµ‹æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œå…¶ä¸­åŒ…å«çš„æ¦‚å¿µå¹¶ä¸å¸¸è§äºVLMé¢„è®­ç»ƒã€‚æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†é›¶æ ·æœ¬ã€å°æ ·æœ¬æ¬¡æ•°ã€åŠç›‘ç£å’Œå…¨ç›‘ç£çš„è®¾ç½®è¿›è¡Œè¯„ä¼°ï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„æ•°æ®é¢†åŸŸè¿›è¡Œæ¯”è¾ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åƒGroundingDINOå’ŒQwen2.5-VLè¿™æ ·çš„VLMåœ¨Roboflow100-VLä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šé›¶æ ·æœ¬å‡†ç¡®ç‡ä½äº2%ï¼Œè¿™æ˜¾ç¤ºäº†å°‘æ ·æœ¬æ¦‚å¿µå¯¹é½çš„å¿…è¦æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†è¿‘æœŸCVPR 2025çš„åŸºç¡€FSODç«èµ›å¹¶åˆ†äº«äº†ç¤¾åŒºçš„è§è§£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå† å†›å›¢é˜Ÿè¶…è¿‡äº†æˆ‘ä»¬çš„åŸºçº¿æˆç»©17 mAPï¼æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf100-vl">https://github.com/roboflow/rf100-vl</a>å’Œ<a target="_blank" rel="noopener" href="https://universe.roboflow.com/rf100-vl/">https://universe.roboflow.com/rf100-vl/</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20612v4">PDF</a> The first two authors contributed equally. This work has been   accepted to the Neural Information Processing Systems (NeurIPS) 2025 Datasets   &amp; Benchmark Track. Project Page: <a target="_blank" rel="noopener" href="https://rf100-vl.org/">https://rf100-vl.org/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹äº’è”ç½‘è§„æ¨¡æ•°æ®çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¸¸è§ç‰©ä½“ä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨æ³›åŒ–åˆ°éåˆ†å¸ƒç±»åˆ«ã€ä»»åŠ¡å’Œæˆåƒæ¨¡å¼æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†Roboflow100-VLï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªå¤šæ¨¡å¼å¯¹è±¡æ£€æµ‹æ•°æ®é›†çš„å¤§è§„æ¨¡é›†åˆï¼Œå…¶ä¸­åŒ…å«å„ç§ä¸å¸¸è§äºVLMé¢„è®­ç»ƒçš„æ¦‚å¿µã€‚ä½œè€…åœ¨ä¸åŒçš„æ•°æ®ç¯å¢ƒä¸‹è¯„ä¼°äº†ç°æœ‰æ¨¡å‹ï¼Œå‘ç°ä¸€äº›æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—æˆåƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬ç²¾åº¦è¾ƒä½ï¼Œè¡¨æ˜éœ€è¦è¿›è¡Œå°‘é‡æ ·æœ¬æ¦‚å¿µå¯¹é½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†CVPR 2025çš„FSODç«èµ›å’Œç¤¾åŒºè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸Šçš„è®­ç»ƒå¯ä»¥åœ¨å¸¸è§ç‰©ä½“ä¸Šå®ç°å‡ºè‰²çš„é›¶æ ·æœ¬æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨æ³›åŒ–åˆ°éåˆ†å¸ƒç±»åˆ«ã€ä»»åŠ¡å’Œæˆåƒæ¨¡å¼æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥Roboflow100-VLï¼Œä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–æ¦‚å¿µçš„å¤§è§„æ¨¡å¤šæ¨¡æ€å¯¹è±¡æ£€æµ‹æ•°æ®é›†ã€‚</li>
<li>ä¸€äº›æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§åŒ»ç–—æˆåƒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬ç²¾åº¦è¾ƒä½ï¼Œéœ€è¦å°‘é‡æ ·æœ¬æ¦‚å¿µå¯¹é½ã€‚</li>
<li>CVPR 2025çš„FSODç«èµ›ä¸­ï¼Œå† å†›å›¢é˜Ÿæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿17 mAPã€‚</li>
<li>ä½œè€…åˆ†äº«äº†å…¶ä»£ç å’Œæ•°æ®é›†ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5fee11367992445af18c1372d8af1b00" align="middle">
<img src="https://picx.zhimg.com/v2-39f97d32ec01bf4be40727f5d59a3bbe" align="middle">
<img src="https://picx.zhimg.com/v2-94525ec6f3d591f43894fc2c1a976188" align="middle">
<img src="https://picx.zhimg.com/v2-0731706e142f12ec7985fe7ab5630eda" align="middle">
<img src="https://picx.zhimg.com/v2-7d3cea226c0073c3c69f1cb43dcf6bcd" align="middle">
<img src="https://picx.zhimg.com/v2-5efc4abbd8b1bfc8bfe9d36ce9cd2d36" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation"><a href="#CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation" class="headerlink" title="CLEVER: A Curated Benchmark for Formally Verified Code Generation"></a>CLEVER: A Curated Benchmark for Formally Verified Code Generation</h2><p><strong>Authors:Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzsche, Greg Durrett, Yisong Yue, Swarat Chaudhuri</strong></p>
<p>We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Leanâ€™s type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever">https://github.com/trishullab/clever</a>) as well as HuggingFace(<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/amitayusht/clever">https://huggingface.co/datasets/amitayusht/clever</a>). All our evaluation code is also available online(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever-prover">https://github.com/trishullab/clever-prover</a>). </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†${\rm C{\small LEVER}}$ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„ã€ç»è¿‡ç­›é€‰çš„åŒ…å«161ä¸ªé—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œé€‚ç”¨äºç«¯åˆ°ç«¯çš„éªŒè¯ä»£ç ç”Ÿæˆã€‚æ¯ä¸ªé—®é¢˜éƒ½åŒ…å«ï¼ˆ1ï¼‰ç”Ÿæˆä¸æœªå…¬å¼€çš„çœŸå®è§„èŒƒç›¸åŒ¹é…çš„è§„èŒƒçš„ä»»åŠ¡ï¼Œï¼ˆ2ï¼‰ç”Ÿæˆä¸€ä¸ªèƒ½å¤Ÿè¯æ˜æ»¡è¶³æ­¤è§„èŒƒçš„å¯ä¿¡Leanå®ç°çš„ä»»åŠ¡ã€‚ä¸åŒäºä»¥å‰çš„åŸºå‡†æµ‹è¯•ï¼Œ${\rm C{\small LEVER}}$é¿å…äº†æµ‹è¯•ç”¨ä¾‹çš„ç›‘ç£ã€ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ³¨é‡Šä»¥åŠæ³„éœ²å®ç°é€»è¾‘æˆ–å…è®¸ç©ºæ´è§£å†³æ–¹æ¡ˆçš„è§„èŒƒã€‚æ‰€æœ‰è¾“å‡ºéƒ½ä½¿ç”¨Leançš„ç±»å‹æ£€æŸ¥å™¨è¿›è¡Œäº‹åéªŒè¯ï¼Œä»¥ç¡®ä¿å¯æœºå™¨æ£€æŸ¥çš„æ­£ç¡®æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨${\rm C{\small LEVER}}$æ¥è¯„ä¼°åŸºäºæœ€æ–°è¯­è¨€æ¨¡å‹çš„å‡ ç§å°æ ·æœ¬å’Œæ™ºèƒ½æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•éƒ½å¾ˆéš¾å®ç°å®Œå…¨éªŒè¯ï¼Œè¿™ä½¿å…¶æˆä¸ºç¨‹åºåˆæˆå’Œå½¢å¼æ¨ç†çš„å‰æ²¿æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¯ä»¥åœ¨GitHubï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever%EF%BC%89%E4%BB%A5%E5%8F%8AHuggingFace%EF%BC%88https://huggingface.co/datasets/amitayusht/clever%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82%E6%89%80%E6%9C%AC%E6%BC%94-%E9%A2%84-%E4%BB%A3%E7%A0%B4%E5%BA%9X-GitHub-%E4%B9%9F%E5%AE%BD">https://github.com/trishullab/cleverï¼‰ä»¥åŠHuggingFaceï¼ˆhttps://huggingface.co/datasets/amitayusht/cleverï¼‰ä¸Šæ‰¾åˆ°ã€‚æ‰€æœ‰è¯„ä¼°ä»£ç ä¹Ÿåœ¨çº¿å¯ç”¨ï¼ˆhttps://github.com/trishullab/clever-proverï¼‰ã€‚</a>%E3%80%82)</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13938v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>${\rm C{\small LEVER}}$æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„ã€é’ˆå¯¹ç«¯å¯¹ç«¯éªŒè¯çš„ä»£ç ç”Ÿæˆä»»åŠ¡çš„åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«161ä¸ªé—®é¢˜ã€‚å®ƒé¿å…äº†æµ‹è¯•ç”¨ä¾‹ç›‘ç£ã€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ³¨è§£ä»¥åŠæ³„éœ²å®ç°é€»è¾‘æˆ–å…è®¸ç©ºæ´è§£å†³æ–¹æ¡ˆçš„è§„æ ¼ã€‚æ‰€æœ‰è¾“å‡ºéƒ½ä½¿ç”¨Leançš„ç±»å‹æ£€æŸ¥å™¨è¿›è¡Œäº‹åéªŒè¯ï¼Œä»¥ç¡®ä¿æœºå™¨å¯æ£€æŸ¥çš„æ­£ç¡®æ€§ã€‚è¯¥åŸºå‡†æµ‹è¯•é›†å¯¹äºè¯„ä¼°åŸºäºå½“å‰æœ€å‰æ²¿è¯­è¨€æ¨¡å‹çš„å°‘é‡æ–¹æ³•å’Œè‡ªä¸»æ–¹æ³•éƒ½æ˜¯ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå®ƒè¢«è®¤ä¸ºæ˜¯ä¸ªå¯Œæœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡å¯¹äºç¨‹åºåˆæˆå’Œå½¢å¼åŒ–æ¨ç†é¢†åŸŸçš„ç ”ç©¶äººå‘˜æ¥è¯´æå…·ä»·å€¼ã€‚è¿™ä¸ªåŸºå‡†æµ‹è¯•é›†å¯ä»¥åœ¨GitHubå’ŒHuggingFaceæ‰¾åˆ°ã€‚å…¶ç›¸å…³çš„è¯„ä¼°ä»£ç ä¹Ÿå·²ç»å…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>${\rm C{\small LEVER}}$æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„åŸºå‡†æµ‹è¯•é›†ï¼Œä¸“é—¨é’ˆå¯¹ç«¯å¯¹ç«¯éªŒè¯çš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚å®ƒåŒ…å«ç”¨äºè¯„ä¼°å’ŒéªŒè¯ä»£ç ç”Ÿæˆçš„å¤šä¸ªé—®é¢˜ã€‚è¿™ä¸ªåŸºå‡†æµ‹è¯•é›†åœ¨ç¨‹åºåˆæˆå’Œå½¢å¼åŒ–æ¨ç†é¢†åŸŸå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f17960786d3277521528f9b055a97100" align="middle">
<img src="https://picx.zhimg.com/v2-4d58a6dff74b2de1403d238e6720f197" align="middle">
<img src="https://picx.zhimg.com/v2-24cb5dcf874ee86ca39ed824a357eb8d" align="middle">
<img src="https://picx.zhimg.com/v2-63fc00f8e6beea5327487f9f386b4a6f" align="middle">
<img src="https://picx.zhimg.com/v2-8018e22d0d626106c7e47ae31beec130" align="middle">
<img src="https://picx.zhimg.com/v2-b511ea476ce070a8514c55f0d6e6443c" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MIR-Bench-Can-Your-LLM-Recognize-Complicated-Patterns-via-Many-Shot-In-Context-Reasoning"><a href="#MIR-Bench-Can-Your-LLM-Recognize-Complicated-Patterns-via-Many-Shot-In-Context-Reasoning" class="headerlink" title="MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot   In-Context Reasoning?"></a>MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot   In-Context Reasoning?</h2><p><strong>Authors:Kai Yan, Zhan Ling, Kang Liu, Yifan Yang, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen</strong></p>
<p>The ability to recognize patterns from examples and apply them to new ones is a primal ability for general intelligence, and is widely studied by psychology and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually &lt;10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations often focus on classification, and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context reasoning benchmark for pattern recognition that asks LLM to predict output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for many-shot in-context reasoning, and acquired many insightful findings including scaling effect, robustness, inductive vs. transductive reasoning, retrieval Augmented Generation (RAG), coding for inductive reasoning, cross-domain generalizability, etc. </p>
<blockquote>
<p>ä»ä¾‹å­ä¸­è¯†åˆ«æ¨¡å¼å¹¶å°†å…¶åº”ç”¨äºæ–°ä¾‹å­æ˜¯é€šç”¨æ™ºèƒ½çš„åŸºæœ¬èƒ½åŠ›ï¼Œä¹Ÿæ˜¯å¿ƒç†å­¦å’Œäººå·¥æ™ºèƒ½ç ”ç©¶è€…å¹¿æ³›ç ”ç©¶çš„è¯¾é¢˜ã€‚è®¸å¤šåŸºå‡†æµ‹è¯•å·²ç»è¢«æå‡ºæ¥è¡¡é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿™ç§èƒ½åŠ›ï¼›ç„¶è€Œï¼Œå®ƒä»¬ä¸»è¦é›†ä¸­åœ¨å°‘æ ·æœ¬ï¼ˆé€šå¸¸å°‘äº10ä¸ªï¼‰çš„è®¾ç½®ä¸Šï¼Œç¼ºä¹ä»é•¿æ–‡ä¸­èšåˆå¤§é‡ä¿¡æ¯çš„è¯„ä¼°ã€‚å¦ä¸€æ–¹é¢ï¼ŒLLMçš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ–­å¢é•¿ï¼Œå¸¦æ¥äº†å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„æ–°èŒƒå¼ï¼Œè¯¥èŒƒå¼å¯ä»¥ç”¨æ•°ç™¾åˆ°æ•°åƒä¸ªä¾‹å­è§£å†³æ–°ä»»åŠ¡ï¼Œè€Œæ— éœ€æ˜‚è´µä¸”ä½æ•ˆçš„å¾®è°ƒã€‚ç„¶è€Œï¼Œå¤šç¤ºä¾‹è¯„ä¼°é€šå¸¸ä¾§é‡äºåˆ†ç±»ï¼Œæµè¡Œçš„é•¿ä¸Šä¸‹æ–‡LLMä»»åŠ¡ï¼Œå¦‚â€œneedle in haystackâ€ï¼ˆæ‰¾é’ˆå·¥ä½œï¼‰ï¼Œå¾ˆå°‘éœ€è¦æ•´åˆå¤§é‡ä¿¡æ¯çš„å¤æ‚æ™ºèƒ½ã€‚ä¸ºäº†è§£å†³ä¸¤è€…çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MIR-Benchï¼Œè¿™æ˜¯ç”¨äºæ¨¡å¼è¯†åˆ«çš„é¦–ä¸ªå¤šç¤ºä¾‹ä¸Šä¸‹æ–‡æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚LLMé€šè¿‡è¾“å…¥å’Œè¾“å‡ºç¤ºä¾‹é¢„æµ‹è¾“å‡ºï¼Œè¿™äº›ç¤ºä¾‹æ¥è‡ªå…·æœ‰ä¸åŒæ•°æ®æ ¼å¼çš„åŸºæœ¬å‡½æ•°ã€‚åŸºäºMIR-Benchï¼Œæˆ‘ä»¬å¯¹å¤šç¤ºä¾‹ä¸Šä¸‹æ–‡æ¨ç†è¿›è¡Œäº†è®¸å¤šæ–°é—®é¢˜çš„ç ”ç©¶ï¼Œå¹¶è·å¾—äº†è®¸å¤šæ·±åˆ»çš„è§è§£ï¼ŒåŒ…æ‹¬è§„æ¨¡æ•ˆåº”ã€ç¨³å¥æ€§ã€å½’çº³ä¸æ¼”ç»æ¨ç†ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ç¼–ç å½’çº³æ¨ç†ã€è·¨åŸŸæ³›åŒ–ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09933v5">PDF</a> 39 pages, 11 figures. The paper is accepted at NeurIPS 2025 Datasets   &amp; Benchmarks Track, and the latest version adds modifications in camera-ready</p>
<p><strong>Summary</strong><br>     æ–‡ç« æ¢è®¨äº†ä»ä¾‹å­ä¸­è¯†åˆ«æ¨¡å¼å¹¶å°†å…¶åº”ç”¨äºæ–°æƒ…å¢ƒçš„æ™ºèƒ½èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå¿ƒç†å­¦å’Œäººå·¥æ™ºèƒ½ç ”ç©¶è€…å¯¹æ­¤è¿›è¡Œäº†å¹¿æ³›ç ”ç©¶ã€‚æ–‡ç« æå‡ºäº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°åŸºå‡†æµ‹è¯•MIR-Benchï¼Œè¯¥æµ‹è¯•ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨é€šè¿‡å¤§é‡å®ä¾‹ä¸Šä¸‹æ–‡è¿›è¡Œæ¨¡å¼è¯†åˆ«æ¨ç†æ—¶çš„èƒ½åŠ›ã€‚ MIR-Benchæ¶µç›–äº†å¤šæ ·æ•°æ®æ ¼å¼çš„è¾“å…¥å’Œè¾“å‡ºé¢„æµ‹é—®é¢˜ï¼Œèƒ½å¤Ÿè¯„ä¼°æ¨¡å‹åœ¨è®¸å¤šæƒ…å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯¹è®¸å¤šæ–°é¢–é—®é¢˜çš„æ¢è®¨å’Œå¤šç§æ´å¯ŸåŠ›çš„è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯†åˆ«æ¨¡å¼å¹¶å°†å…¶åº”ç”¨äºæ–°æƒ…å¢ƒæ˜¯æ™ºèƒ½èƒ½åŠ›çš„æ ¸å¿ƒï¼Œå—åˆ°å¿ƒç†å­¦å’Œäººå·¥æ™ºèƒ½ç ”ç©¶è€…çš„å¹¿æ³›å…³æ³¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°åŸºå‡†æµ‹è¯•é€šå¸¸å…³æ³¨äºå°è§„æ¨¡ç¤ºä¾‹ï¼ˆ&lt; 10ä¸ªï¼‰è¯„ä¼°æ–¹æ³•å°šæœªé€‚ç”¨äºå¤§è§„æ¨¡çš„ä¸Šä¸‹æ–‡èšåˆã€‚ç„¶è€Œæ–°çš„èŒƒå‹åœ¨è®¸å¤š-Shotæƒ…å¢ƒä¸‹çš„å‡ºç°ä¸ºæ¨¡å¼è¯†åˆ«æä¾›äº†æ–°çš„è§†è§’ã€‚ </li>
<li>æå‡ºMIR-BenchåŸºå‡†æµ‹è¯•ä½œä¸ºé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šä¸ªä¸Šä¸‹æ–‡æƒ…å¢ƒä¸‹çš„æ¨ç†æ¨¡å¼è¯†åˆ«æµ‹è¯•ï¼Œè¯¥æµ‹è¯•è¦æ±‚æ¨¡å‹é€šè¿‡ä¸åŒæ ¼å¼çš„è¾“å…¥å’Œè¾“å‡ºç¤ºä¾‹é¢„æµ‹è¾“å‡ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6de521f7639884009e38e9b4c23ee357" align="middle">
<img src="https://picx.zhimg.com/v2-b419c63dd06059fc6d3342bc5ba13042" align="middle">
<img src="https://picx.zhimg.com/v2-6af534db2fcd63bd82d1e51ff27e6f87" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Pre-training-Epidemic-Time-Series-Forecasters-with-Compartmental-Prototypes"><a href="#Pre-training-Epidemic-Time-Series-Forecasters-with-Compartmental-Prototypes" class="headerlink" title="Pre-training Epidemic Time Series Forecasters with Compartmental   Prototypes"></a>Pre-training Epidemic Time Series Forecasters with Compartmental   Prototypes</h2><p><strong>Authors:Zewen Liu, Juntong Ni, Max S. Y. Lau, Wei Jin</strong></p>
<p>Accurate epidemic forecasting is crucial for outbreak preparedness, but existing data-driven models are often brittle. Typically trained on a single pathogen, they struggle with data scarcity during new outbreaks and fail under distribution shifts caused by viral evolution or interventions. However, decades of surveillance data from diverse diseases offer an untapped source of transferable knowledge. To leverage the collective lessons from history, we propose CAPE, the first open-source pre-trained model for epidemic forecasting. Unlike existing time series foundation models that overlook epidemiological challenges, CAPE models epidemic dynamics as mixtures of latent population states, termed compartmental prototypes. It discovers a flexible dictionary of compartment prototypes directly from surveillance data, enabling each outbreak to be expressed as a time-varying mixture that links observed infections to latent population states. To promote robust generalization, CAPE combines self-supervised pre-training objectives with lightweight epidemic-aware regularizers that align the learned prototypes with epidemiological semantics. On a comprehensive benchmark spanning 17 diseases and 50+ regions, CAPE significantly outperforms strong baselines in zero-shot, few-shot, and full-shot forecasting. This work represents a principled step toward pre-trained epidemic models that are both transferable and epidemiologically grounded. </p>
<blockquote>
<p>å‡†ç¡®çš„ç–«æƒ…é¢„æµ‹å¯¹äºç–«æƒ…å‡†å¤‡è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„æ•°æ®é©±åŠ¨æ¨¡å‹å¾€å¾€å¾ˆè„†å¼±ã€‚è¿™äº›æ¨¡å‹é€šå¸¸é’ˆå¯¹å•ä¸€ç—…åŸä½“è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ–°ç–«æƒ…æœŸé—´æ•°æ®ç¨€ç¼ºæ—¶é¢ä¸´å›°å¢ƒï¼Œåœ¨ç—…æ¯’è¿›åŒ–æˆ–å¹²é¢„æªæ–½å¯¼è‡´çš„åˆ†å¸ƒå˜åŒ–ä¸‹ä¹Ÿä¼šå¤±æ•ˆã€‚ç„¶è€Œï¼Œæ¥è‡ªå¤šç§ç–¾ç—…çš„å‡ åå¹´ç›‘æ§æ•°æ®æä¾›äº†å°šæœªå¼€å‘çš„çŸ¥è¯†è½¬ç§»æ¥æºã€‚ä¸ºäº†åˆ©ç”¨å†å²çš„é›†ä½“æ•™è®­ï¼Œæˆ‘ä»¬æå‡ºäº†CAPEï¼ˆä¼ æŸ“ç—…é¢„æµ‹çš„é¦–ä¸ªå¼€æºé¢„è®­ç»ƒæ¨¡å‹ï¼‰ã€‚ä¸ç°æœ‰çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ä¸åŒï¼ŒCAPEå°†ç–«æƒ…åŠ¨æ€å»ºæ¨¡ä¸ºæ½œåœ¨äººç¾¤çŠ¶æ€çš„æ··åˆç‰©ï¼Œç§°ä¸ºâ€œéš”å®¤åŸå‹â€ã€‚å®ƒç›´æ¥ä»ç›‘æ§æ•°æ®ä¸­å‘ç°äº†çµæ´»çš„éš”å®¤åŸå‹è¯å…¸ï¼Œä½¿æ¯ä¸ªç–«æƒ…éƒ½èƒ½è¡¨è¾¾ä¸ºä¸€ç§éšæ—¶é—´å˜åŒ–æ··åˆä½“ï¼Œå°†è§‚å¯Ÿåˆ°çš„æ„ŸæŸ“ä¸æ½œåœ¨äººç¾¤çŠ¶æ€è”ç³»èµ·æ¥ã€‚ä¸ºäº†ä¿ƒè¿›ç¨³å¥çš„æ³›åŒ–ï¼ŒCAPEç»“åˆäº†è‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒç›®æ ‡ä¸è½»é‡çº§çš„ç–«æƒ…æ„è¯†è°ƒèŠ‚å™¨ï¼Œä½¿å­¦åˆ°çš„åŸå‹ä¸æµè¡Œç—…å­¦è¯­ä¹‰ç›¸ç¬¦ã€‚åœ¨æ¶µç›–17ç§ç–¾ç—…å’Œ50å¤šä¸ªåœ°åŒºçš„ç»¼åˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAPEåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå…¨æ ·æœ¬é¢„æµ‹ä¸­éƒ½æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºå‡†æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œæœç€æ—¢å¯è½¬ç§»åˆåŸºäºæµè¡Œç—…å­¦çš„é¢„è®­ç»ƒä¼ æŸ“ç—…æ¨¡å‹çš„æ–¹å‘è¿ˆå‡ºäº†æœ‰åŸåˆ™çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03393v5">PDF</a> version 2.0_fixed</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºCAPEï¼Œä¸€ä¸ªåˆ©ç”¨å†å²æ•°æ®è¿›è¡Œæµè¡Œç—…é¢„æµ‹çš„å¼€æºé¢„è®­ç»ƒæ¨¡å‹ã€‚é¢å¯¹ç°æœ‰çš„æ•°æ®é©±åŠ¨æ¨¡å‹åœ¨æ–°ç–«æƒ…çˆ†å‘æ—¶æ•°æ®ç¼ºä¹æƒ…å†µä¸‹çš„è„†å¼±æ€§ï¼ŒCAPEæ¨¡å‹ä»é•¿æœŸç–¾ç—…ç›‘æµ‹æ•°æ®ä¸­æŒ–æ˜å¯è¿ç§»çŸ¥è¯†ã€‚CAPEå°†æµè¡Œç—…åŠ¨æ€å»ºæ¨¡ä¸ºæ½œåœ¨äººç¾¤çŠ¶æ€çš„æ··åˆä½“ï¼Œç§°ä¸ºâ€œéš”å®¤åŸå‹â€ã€‚è¯¥æ¨¡å‹ç›´æ¥ä»ç›‘æµ‹æ•°æ®ä¸­è·å–çµæ´»çš„éš”å®¤åŸå‹è¯å…¸ï¼Œä½¿æ¯ä¸ªç–«æƒ…éƒ½èƒ½è¡¨è¾¾ä¸ºéšæ—¶é—´å˜åŒ–çš„æ··åˆä½“ï¼Œå°†è§‚å¯Ÿåˆ°çš„æ„ŸæŸ“ä¸æ½œåœ¨äººç¾¤çŠ¶æ€è”ç³»èµ·æ¥ã€‚ä¸ºæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼ŒCAPEç»“åˆäº†è‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒç›®æ ‡å’Œè½»é‡çº§çš„æµè¡Œç—…æ„ŸçŸ¥è°ƒèŠ‚å™¨ï¼Œä½¿å­¦ä¹ åˆ°çš„åŸå‹ä¸æµè¡Œç—…è¯­ä¹‰ç›¸ç¬¦åˆã€‚åœ¨è·¨è¶Š17ç§ç–¾ç—…å’Œ50å¤šä¸ªåœ°åŒºçš„ç»¼åˆåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAPEåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå…¨æ ·æœ¬é¢„æµ‹ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–å¼ºåŸºçº¿æ¨¡å‹ã€‚è¿™æ˜¯æœç€æ—¢å…·æœ‰å¯è¿ç§»æ€§åˆå…·å¤‡æµè¡Œç—…åŸºç¡€çš„å¯é¢„è®­ç»ƒæµè¡Œç—…æ¨¡å‹çš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAPEæ˜¯ä¸€ä¸ªç”¨äºæµè¡Œç—…é¢„æµ‹çš„å¼€æºé¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ç°æœ‰æ•°æ®é©±åŠ¨æ¨¡å‹åœ¨é¢å¯¹æ–°ç–«æƒ…çˆ†å‘æ—¶å­˜åœ¨æ•°æ®ç¼ºä¹çš„è„†å¼±æ€§ã€‚</li>
<li>CAPEä»é•¿æœŸç–¾ç—…ç›‘æµ‹æ•°æ®ä¸­æŒ–æ˜çŸ¥è¯†ï¼Œå°†æµè¡Œç—…åŠ¨æ€å»ºæ¨¡ä¸ºæ½œåœ¨äººç¾¤çŠ¶æ€çš„æ··åˆä½“ã€‚</li>
<li>CAPEé‡‡ç”¨è‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒç»“åˆæµè¡Œç—…æ„ŸçŸ¥è°ƒèŠ‚å™¨ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>CAPEé€šè¿‡å‘ç°éš”å®¤åŸå‹è¯å…¸ï¼Œä½¿æ¯ä¸ªç–«æƒ…éƒ½èƒ½è¡¨è¾¾ä¸ºæ—¶é—´å˜åŒ–çš„æ··åˆä½“ã€‚</li>
<li>åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAPEåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå…¨æ ·æœ¬é¢„æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e682887e843a562338aac17ae789ee4" align="middle">
<img src="https://picx.zhimg.com/v2-1571c5b622ea11035821e2108b4ef4b3" align="middle">
<img src="https://picx.zhimg.com/v2-a0d4b0d31861c694ab0b0444f63532b3" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-94544c86e6e5e702d9d9e6f63d505a49" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fa71f2937fc7f81fe63ca8dfe9263990" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Co-Designing Quantum Codes with Transversal Diagonal Gates via   Multi-Agent Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
