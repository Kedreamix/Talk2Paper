<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b4953ceb52e5fae9174405deb319a261')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-25-æ›´æ–°"><a href="#2025-10-25-æ›´æ–°" class="headerlink" title="2025-10-25 æ›´æ–°"></a>2025-10-25 æ›´æ–°</h1><h2 id="Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge"><a href="#Towards-General-Modality-Translation-with-Contrastive-and-Predictive-Latent-Diffusion-Bridge" class="headerlink" title="Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge"></a>Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge</h2><p><strong>Authors:Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</strong></p>
<p>Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home">https://sites.google.com/view/lddbm/home</a>. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆå»ºæ¨¡çš„æœ€æ–°è¿›å±•ä½¿å¾—æ‰©æ•£æ¨¡å‹æˆä¸ºä»å¤æ‚æ•°æ®åˆ†å¸ƒä¸­é‡‡æ ·çš„æœ€å…ˆè¿›çš„å·¥å…·ã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨å¤šæ¨¡æ€è½¬æ¢ï¼ˆä¸åŒæ„Ÿå®˜ä¿¡æ¯çš„è½¬æ¢ï¼‰æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºä¸€äº›é™åˆ¶æ€§çš„å‡è®¾ï¼ŒåŒ…æ‹¬å…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œç‰¹å®šæ¨¡æ€æ¶æ„ç­‰ï¼Œè¿™äº›å‡è®¾é™åˆ¶äº†å®ƒä»¬çš„é€šç”¨æ€§å’Œç†è®ºåŸºç¡€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼ˆLatent Denoising Diffusion Bridge Modelï¼ŒLDDBMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ½œåœ¨å˜é‡æ‰©å±•çš„å»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹çš„é€šç”¨å¤šæ¨¡æ€è½¬æ¢æ¡†æ¶ã€‚é€šè¿‡åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä»»æ„æ¨¡æ€ä¹‹é—´å»ºç«‹æ¡¥æ¢ï¼Œæ— éœ€å¯¹é½ç»´åº¦ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯¹æ¯”å¯¹é½æŸå¤±æ¥å¼ºåˆ¶é…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é’ˆå¯¹æ½œåœ¨ç©ºé—´ä¸­å™ªå£°é¢„æµ‹çš„é€šç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢„æµ‹æŸå¤±æ¥æŒ‡å¯¼è®­ç»ƒä»¥å®ç°å‡†ç¡®çš„è·¨åŸŸè½¬æ¢ï¼Œå¹¶æ¢ç´¢äº†å¤šç§è®­ç»ƒç­–ç•¥æ¥æé«˜ç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒä»»æ„æ¨¡æ€å¯¹ï¼Œå¹¶åœ¨å¤šç§å¤šæ¨¡æ€è½¬æ¢ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šè§†å›¾åˆ°ä¸‰ç»´å½¢çŠ¶çš„ç”Ÿæˆã€å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå¤šè§†å›¾åœºæ™¯åˆæˆç­‰ã€‚ç»¼åˆå®éªŒå’Œåˆ é™¤æ“ä½œéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¸€èˆ¬æ¨¡æ€è½¬æ¢é¢†åŸŸå»ºç«‹äº†æ–°çš„å¼ºå¤§åŸºçº¿ã€‚å¦‚éœ€æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home%E3%80%82">https://sites.google.com/view/lddbm/homeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20819v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹è¿‘æœŸåœ¨ç”Ÿæˆå»ºæ¨¡æ–¹é¢å–å¾—é‡å¤§è¿›å±•ï¼Œå·²æˆä¸ºå¤æ‚æ•°æ®åˆ†å¸ƒé‡‡æ ·çš„é¡¶å°–å·¥å…·ã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨å•æ¨¡æ€é¢†åŸŸï¼ˆå¦‚å›¾åƒå’ŒéŸ³é¢‘ï¼‰è¡¨ç°å‡ºæ˜¾è‘—çš„æˆåŠŸï¼Œä½†å°†å…¶èƒ½åŠ›æ‰©å±•åˆ°è·¨ä¸åŒæ„Ÿå®˜æ¨¡æ€çš„æ¨¡æ€ç¿»è¯‘ï¼ˆMTï¼‰ä»æ˜¯ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œæ¨¡æ€ç‰¹å®šæ¶æ„ç­‰é™åˆ¶æ€§å‡è®¾ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œç†è®ºæ ¹åŸºã€‚æœ¬ç ”ç©¶æå‡ºåŸºäºå»å™ªæ‰©æ•£æ¡¥æ¨¡å‹çš„æ½œåœ¨å»å™ªæ‰©æ•£æ¡¥æ¨¡å‹ï¼ˆLatent Denoising Diffusion Bridge Modelï¼ŒLDDBMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ¨¡æ€ç¿»è¯‘çš„é€šç”¨æ¡†æ¶ã€‚é€šè¿‡åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä»»æ„æ¨¡æ€ä¹‹é—´æ­å»ºæ¡¥æ¢ï¼Œæ— éœ€å¯¹é½ç»´åº¦ã€‚ç ”ç©¶å¼•å…¥äº†å¯¹æ¯”å¯¹é½æŸå¤±æ¥å¼ºåˆ¶é…å¯¹æ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§é’ˆå¯¹æ½œåœ¨ç©ºé—´ä¸­å™ªå£°é¢„æµ‹çš„åŸŸæ— å…³ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†é¢„æµ‹æŸå¤±æ¥æŒ‡å¯¼è®­ç»ƒä»¥å®ç°å‡†ç¡®çš„è·¨åŸŸç¿»è¯‘ï¼Œå¹¶æ¢ç´¢äº†è‹¥å¹²è®­ç»ƒç­–ç•¥ä»¥æé«˜ç¨³å®šæ€§ã€‚è¯¥æ–¹æ³•æ”¯æŒä»»æ„æ¨¡æ€å¯¹ï¼Œå¹¶åœ¨å¤šç§MTä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šè§†è§’åˆ°3Då½¢çŠ¶ç”Ÿæˆã€å›¾åƒè¶…åˆ†è¾¨ç‡å’Œå¤šè§†è§’åœºæ™¯åˆæˆã€‚å…¨é¢çš„å®éªŒå’Œæ¶ˆèéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåœ¨é€šç”¨æ¨¡æ€ç¿»è¯‘æ–¹é¢å»ºç«‹äº†æ–°çš„å¼ºåŠ²åŸºçº¿ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/lddbm/home%E3%80%82">https://sites.google.com/view/lddbm/homeã€‚</a></p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå¤æ‚æ•°æ®é‡‡æ ·é¢†åŸŸçš„é¡¶å°–å·¥å…·ã€‚</li>
<li>æ¨¡æ€ç¿»è¯‘ï¼ˆMTï¼‰æ˜¯å°†ä¿¡æ¯ä»ä¸€ç§æ„Ÿå®˜æ¨¡æ€è½¬æ¢åˆ°å¦ä¸€ç§æ¨¡æ€çš„ä»»åŠ¡ï¼Œæ˜¯ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºå…±äº«ç»´åº¦ã€é«˜æ–¯æºå…ˆéªŒå’Œç‰¹å®šæ¶æ„ï¼Œé™ä½äº†å…¶é€šç”¨æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†Latent Denoising Diffusion Bridge Modelï¼ˆLDDBMï¼‰ï¼Œä¸€ä¸ªç”¨äºæ¨¡æ€ç¿»è¯‘çš„é€šç”¨æ¡†æ¶ã€‚</li>
<li>LDDBMåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œæ— éœ€å¯¹é½ç»´åº¦ï¼Œæ”¯æŒä»»æ„æ¨¡æ€å¯¹ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†å¯¹æ¯”å¯¹é½æŸå¤±å’Œé¢„æµ‹æŸå¤±æ¥æé«˜ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71b7d87ab3147c25125212c3b2948c06" align="middle">
<img src="https://picx.zhimg.com/v2-5d66e2842144f846f899b138e2dd2d51" align="middle">
<img src="https://picx.zhimg.com/v2-227bee8414c0a17e145209c970af48a3" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AutoScape-Geometry-Consistent-Long-Horizon-Scene-Generation"><a href="#AutoScape-Geometry-Consistent-Long-Horizon-Scene-Generation" class="headerlink" title="AutoScape: Geometry-Consistent Long-Horizon Scene Generation"></a>AutoScape: Geometry-Consistent Long-Horizon Scene Generation</h2><p><strong>Authors:Jiacheng Chen, Ziyu Jiang, Mingfu Liang, Bingbing Zhuang, Jong-Chyi Su, Sparsh Garg, Ying Wu, Manmohan Chandraker</strong></p>
<p>This paper proposes AutoScape, a long-horizon driving scene generation framework. At its core is a novel RGB-D diffusion model that iteratively generates sparse, geometrically consistent keyframes, serving as reliable anchors for the sceneâ€™s appearance and geometry. To maintain long-range geometric consistency, the model 1) jointly handles image and depth in a shared latent space, 2) explicitly conditions on the existing scene geometry (i.e., rendered point clouds) from previously generated keyframes, and 3) steers the sampling process with a warp-consistent guidance. Given high-quality RGB-D keyframes, a video diffusion model then interpolates between them to produce dense and coherent video frames. AutoScape generates realistic and geometrically consistent driving videos of over 20 seconds, improving the long-horizon FID and FVD scores over the prior state-of-the-art by 48.6% and 43.0%, respectively. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†AutoScapeæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªé•¿è§†é‡é©¾é©¶åœºæ™¯ç”Ÿæˆæ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°å‹çš„RGB-Dæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè¿­ä»£ç”Ÿæˆç¨€ç–ä¸”å‡ ä½•ä¸€è‡´çš„å…³é”®å¸§ï¼Œä½œä¸ºåœºæ™¯å¤–è§‚å’Œå‡ ä½•çš„å¯é é”šç‚¹ã€‚ä¸ºäº†ä¿æŒé•¿è·ç¦»å‡ ä½•ä¸€è‡´æ€§ï¼Œè¯¥æ¨¡å‹1ï¼‰åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­åŒæ—¶å¤„ç†å›¾åƒå’Œæ·±åº¦ä¿¡æ¯ï¼›2ï¼‰æ˜¾å¼åœ°åŸºäºå…ˆå‰ç”Ÿæˆçš„å…³é”®å¸§çš„ç°æœ‰åœºæ™¯å‡ ä½•ï¼ˆå³æ¸²æŸ“ç‚¹äº‘ï¼‰è¿›è¡Œæ¡ä»¶å¤„ç†ï¼›3ï¼‰é€šè¿‡warpä¸€è‡´æ€§æŒ‡å¯¼æ¥æ§åˆ¶é‡‡æ ·è¿‡ç¨‹ã€‚ç»™å®šé«˜è´¨é‡RGB-Då…³é”®å¸§åï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹å°†åœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œä»¥äº§ç”Ÿå¯†é›†ä¸”è¿è´¯çš„è§†é¢‘å¸§ã€‚AutoScapeèƒ½å¤Ÿç”Ÿæˆè¶…è¿‡20ç§’çš„çœŸå®å’Œå‡ ä½•ä¸€è‡´çš„é©¾é©¶è§†é¢‘ï¼Œç›¸å¯¹äºå…ˆå‰æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œé•¿è§†é‡FIDå’ŒFVDå¾—åˆ†åˆ†åˆ«æé«˜äº†48.6%å’Œ43.0%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20726v1">PDF</a> ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://auto-scape.github.io/">https://auto-scape.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†AutoScapeæ¡†æ¶ï¼Œä¸€ä¸ªç”¨äºé•¿è¿œè§†è·é©¾é©¶åœºæ™¯ç”Ÿæˆçš„æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°é¢–çš„RGB-Dæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯è¿­ä»£ç”Ÿæˆç¨€ç–ä¸”å‡ ä½•ä¸€è‡´çš„å…³é”®å¸§ï¼Œä¸ºåœºæ™¯å¤–è§‚å’Œå‡ ä½•æä¾›å¯é é”šç‚¹ã€‚é€šè¿‡è”åˆå¤„ç†å›¾åƒå’Œæ·±åº¦ä¿¡æ¯ï¼Œæ˜¾å¼æ¡ä»¶åŒ–ç°æœ‰åœºæ™¯å‡ ä½•ï¼Œä»¥åŠç”¨warpä¸€è‡´æ€§æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹æ¥ä¿æŒé•¿è·ç¦»å‡ ä½•ä¸€è‡´æ€§ã€‚ç»™å®šé«˜è´¨é‡RGB-Då…³é”®å¸§ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹ä¼šåœ¨å®ƒä»¬ä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œç”Ÿæˆå¯†é›†ä¸”è¿è´¯çš„è§†é¢‘å¸§ã€‚AutoScapeå¯ç”Ÿæˆè¶…è¿‡20ç§’çš„é€¼çœŸä¸”å‡ ä½•ä¸€è‡´çš„é©¾é©¶è§†é¢‘ï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œå…¶é•¿è·ç¦»FIDå’ŒFVDå¾—åˆ†åˆ†åˆ«æé«˜äº†48.6%å’Œ43.0%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AutoScapeæ˜¯ä¸€ä¸ªç”¨äºé•¿è¿œè§†è·é©¾é©¶åœºæ™¯ç”Ÿæˆçš„æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªRGB-Dæ‰©æ•£æ¨¡å‹ï¼Œå¯ç”Ÿæˆç¨€ç–ä¸”å‡ ä½•ä¸€è‡´çš„å…³é”®å¸§ã€‚</li>
<li>æ¨¡å‹è”åˆå¤„ç†å›¾åƒå’Œæ·±åº¦ä¿¡æ¯ï¼Œå¹¶åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œã€‚</li>
<li>æ¨¡å‹æ˜¾å¼æ¡ä»¶åŒ–ç°æœ‰åœºæ™¯å‡ ä½•ï¼Œå¹¶ç”¨ä»¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>ç»™å®šé«˜è´¨é‡RGB-Då…³é”®å¸§ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆå¯†é›†ä¸”è¿è´¯çš„è§†é¢‘å¸§ã€‚</li>
<li>AutoScapeç”Ÿæˆçš„é©¾é©¶è§†é¢‘å…·æœ‰é€¼çœŸæ€§å’Œå‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶èƒ½ç»´æŒè¾ƒé•¿æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-030862e611e8ae1fd17061e928ed650b" align="middle">
<img src="https://picx.zhimg.com/v2-23d74eb32247a6e61117266ceccefcb2" align="middle">
<img src="https://picx.zhimg.com/v2-afe066ba22901539fdd5ea66a64c6085" align="middle">
<img src="https://picx.zhimg.com/v2-b584e931c934da00cbdec9118fa47dd4" align="middle">
<img src="https://picx.zhimg.com/v2-ed6e722d3fd6f4ba1ef52b896b232f66" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Downsizing-Diffusion-Models-for-Cardinality-Estimation"><a href="#Downsizing-Diffusion-Models-for-Cardinality-Estimation" class="headerlink" title="Downsizing Diffusion Models for Cardinality Estimation"></a>Downsizing Diffusion Models for Cardinality Estimation</h2><p><strong>Authors:Xinhe Mu, Zhaoqi Zhou, Zaijiu Shang, Chuan Zhou, Gang Fu, Guiying Yan, Guoliang Li, Zhiming Ma</strong></p>
<p>Inspired by the performance of score-based diffusion models in estimating complex text, video, and image distributions with thousands of dimensions, we introduce Accelerated Diffusion Cardest (ADC), the first joint distribution cardinality estimator based on a downsized diffusion model.   To calculate the pointwise density value of data distributions, ADCâ€™s density estimator uses a formula that evaluates log-likelihood by integrating the score function, a gradient mapping which ADC has learned to efficiently approximate using its lightweight score estimator. To answer ranged queries, ADCâ€™s selectivity estimator first predicts their selectivity using a Gaussian Mixture Model (GMM), then uses importance sampling Monte Carlo to correct its predictions with more accurate pointwise density values calculated by the density estimator. ADC+ further trains a decision tree to identify the high-volume, high-selectivity queries that the GMM alone can predict very accurately, in which case it skips the correction phase to prevent Monte Carlo from adding more variance. Doing so lowers median Q-error and cuts per-query latency by 25 percent, making ADC+ usually twice as fast as Naru, arguably the state-of-the-art joint distribution cardinality estimator.   Numerical experiments using well-established benchmarks show that on all real-world datasets tested, ADC+ is capable of rivaling Naru and outperforming MSCN, DeepDB, LW-Tree, and LW-NN using around 66 percent their storage space, being at least 3 times as accurate as MSCN on 95th and 99th percentile error. Furthermore, on a synthetic dataset where attributes exhibit complex, multilateral correlations, ADC and ADC+ are considerably robust while almost every other learned model suffered significant accuracy declines. In this case, ADC+ performs better than any other tested model, being 10 times as accurate as Naru on 95th and 99th percentile error. </p>
<blockquote>
<p>å—åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹åœ¨ä¼°è®¡å…·æœ‰æ•°åƒç»´çš„å¤æ‚æ–‡æœ¬ã€è§†é¢‘å’Œå›¾åƒåˆ†å¸ƒæ–¹é¢çš„æ€§èƒ½çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ é€Ÿæ‰©æ•£å¡å¾·æ–¯ç‰¹ï¼ˆADCï¼‰ï¼Œè¿™æ˜¯åŸºäºç®€åŒ–çš„æ‰©æ•£æ¨¡å‹çš„é¦–ä¸ªè”åˆåˆ†å¸ƒåŸºæ•°ä¼°è®¡å™¨ã€‚ä¸ºäº†è®¡ç®—æ•°æ®åˆ†å¸ƒçš„é€ç‚¹å¯†åº¦å€¼ï¼ŒADCçš„å¯†åº¦ä¼°è®¡å™¨ä½¿ç”¨ä¸€ä¸ªå…¬å¼æ¥è¯„ä¼°å¯¹æ•°ä¼¼ç„¶ï¼Œè¯¥å…¬å¼é€šè¿‡ç§¯åˆ†å¾—åˆ†å‡½æ•°ï¼ˆä¸€ä¸ªæ¢¯åº¦æ˜ å°„ï¼‰æ¥è¯„ä¼°ï¼ŒADCå·²ç»ä½¿ç”¨å…¶è½»é‡çº§çš„å¾—åˆ†ä¼°è®¡å™¨å­¦ä¼šäº†å¯¹å…¶è¿›è¡Œæœ‰æ•ˆçš„è¿‘ä¼¼ã€‚ä¸ºäº†å›ç­”èŒƒå›´æŸ¥è¯¢ï¼ŒADCçš„é€‰æ‹©æ€§ä¼°è®¡å™¨é¦–å…ˆä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰é¢„æµ‹å…¶é€‰æ‹©æ€§ï¼Œç„¶åä½¿ç”¨é‡è¦æ€§æŠ½æ ·è’™ç‰¹å¡æ´›ï¼ˆMonte Carloï¼‰é€šè¿‡å¯†åº¦ä¼°è®¡å™¨è®¡ç®—çš„æ›´å‡†ç¡®çš„é€ç‚¹å¯†åº¦å€¼æ¥æ ¡æ­£å…¶é¢„æµ‹ã€‚ADC+è¿›ä¸€æ­¥è®­ç»ƒå†³ç­–æ ‘ï¼Œä»¥è¯†åˆ«é«˜å®¹é‡ã€é«˜é€‰æ‹©æ€§çš„æŸ¥è¯¢ï¼Œè¿™äº›æŸ¥è¯¢å¯ä»¥ç”±GMMå•ç‹¬éå¸¸å‡†ç¡®åœ°é¢„æµ‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒè·³è¿‡æ ¡æ­£é˜¶æ®µä»¥é˜²æ­¢è’™ç‰¹å¡æ´›å¢åŠ æ›´å¤šçš„æ–¹å·®ã€‚è¿™æ ·åšé™ä½äº†ä¸­ä½æ•°Qè¯¯å·®ï¼Œå¹¶å°†æ¯ä¸ªæŸ¥è¯¢çš„å»¶è¿Ÿæ—¶é—´å‡å°‘äº†25%ï¼Œä½¿å¾—ADC+é€šå¸¸æ˜¯å…ˆè¿›çš„è”åˆåˆ†å¸ƒåŸºæ•°ä¼°è®¡å™¨Naruçš„ä¸¤å€é€Ÿåº¦ã€‚ä½¿ç”¨å…¬è®¤çš„åŸºå‡†æµ‹è¯•è¿›è¡Œçš„æ•°å€¼å®éªŒè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰çœŸå®ä¸–ç•Œæµ‹è¯•æ•°æ®é›†ä¸Šï¼ŒADC+èƒ½å¤Ÿä¸Naruç«äº‰å¹¶ä¸”ä¼˜äºMSCNã€DeepDBã€LW-Treeå’ŒLW-NNï¼Œå¤§çº¦ä½¿ç”¨å®ƒä»¬66%çš„å­˜å‚¨ç©ºé—´ï¼Œåœ¨95thå’Œ99thç™¾åˆ†ä½è¯¯å·®ä¸Šè‡³å°‘æ¯”MSCNå‡†ç¡®ä¸‰å€ã€‚æ­¤å¤–ï¼Œåœ¨å±æ€§è¡¨ç°å‡ºå¤æ‚ã€å¤šæ–¹é¢å…³è”çš„åˆæˆæ•°æ®é›†ä¸Šï¼ŒADCå’ŒADC+è¡¨ç°å‡ºç›¸å½“ç¨³å¥çš„æ€§èƒ½ï¼Œè€Œå‡ ä¹å…¶ä»–æ‰€æœ‰æ¨¡å‹éƒ½å‡ºç°äº†æ˜æ˜¾çš„å‡†ç¡®æ€§ä¸‹é™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒADC+çš„æ€§èƒ½ä¼˜äºå…¶ä»–æ‰€æœ‰æµ‹è¯•æ¨¡å‹ï¼Œåœ¨95thå’Œ99thç™¾åˆ†ä½è¯¯å·®ä¸Šæ¯”Naruå‡†ç¡®10å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20681v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºåˆ†æ•°æ‰©æ•£æ¨¡å‹åœ¨ä¼°è®¡æ•°åƒç»´åº¦çš„å¤æ‚æ–‡æœ¬ã€è§†é¢‘å’Œå›¾åƒåˆ†å¸ƒä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŠ é€Ÿæ‰©æ•£åŸºæ•°ä¼°è®¡å™¨ï¼ˆADCï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºç®€åŒ–æ‰©æ•£æ¨¡å‹çš„è”åˆåˆ†å¸ƒåŸºæ•°ä¼°è®¡å™¨ã€‚ADCçš„å¯†åº¦ä¼°è®¡å™¨ä½¿ç”¨å…¬å¼è®¡ç®—æ•°æ®åˆ†å¸ƒçš„é€ç‚¹å¯†åº¦å€¼ï¼Œè¯¥å…¬å¼é€šè¿‡ç§¯åˆ†å¾—åˆ†å‡½æ•°æ¥è¯„ä¼°å¯¹æ•°ä¼¼ç„¶æ€§ã€‚ADCå·²ç»å­¦ä¼šä½¿ç”¨å…¶è½»é‡çº§å¾—åˆ†ä¼°è®¡å™¨æœ‰æ•ˆåœ°è¿‘ä¼¼æ¢¯åº¦æ˜ å°„çš„å¾—åˆ†å‡½æ•°ã€‚ä¸ºäº†å›ç­”èŒƒå›´æŸ¥è¯¢ï¼ŒADCçš„é€‰æ‹©æ€§ä¼°è®¡å™¨é¦–å…ˆä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰é¢„æµ‹å…¶é€‰æ‹©æ€§ï¼Œç„¶åä½¿ç”¨é‡è¦æ€§æŠ½æ ·è’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMonte Carloï¼‰æ ¡æ­£ç”±å¯†åº¦ä¼°è®¡å™¨è®¡ç®—å¾—å‡ºçš„æ›´å‡†ç¡®çš„é€ç‚¹å¯†åº¦å€¼ã€‚ADC+è¿›ä¸€æ­¥è®­ç»ƒå†³ç­–æ ‘ï¼Œä»¥è¯†åˆ«GMMå¯ä»¥éå¸¸å‡†ç¡®é¢„æµ‹çš„é«˜å®¹é‡ã€é«˜é€‰æ‹©æ€§æŸ¥è¯¢ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒè·³è¿‡æ ¡æ­£é˜¶æ®µä»¥é˜²æ­¢è’™ç‰¹å¡æ´›å¢åŠ æ–¹å·®ã€‚è¿™æ ·åšé™ä½äº†ä¸­ä½æ•°Qè¯¯å·®å¹¶å‡å°‘äº†æ¯ç§’æŸ¥è¯¢å»¶è¿Ÿæ—¶é—´è¾¾ç™¾åˆ†ä¹‹äºŒåäº”ï¼Œä½¿å¾—ADC+é€šå¸¸æ¯”Naruå¿«ä¸€å€ï¼Œåè€…æ˜¯ç›®å‰è”åˆåˆ†å¸ƒåŸºæ•°ä¼°è®¡å™¨ä¸­çš„ä½¼ä½¼è€…ã€‚æ•°å€¼å®éªŒè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰çœŸå®æ•°æ®é›†ä¸Šæµ‹è¯•æ—¶ï¼ŒADC+èƒ½å¤Ÿä¸Naruç«äº‰å¹¶è¶…è¶ŠMSCNã€DeepDBã€LW-Treeå’ŒLW-NNç­‰æ¨¡å‹ï¼Œä»…ä½¿ç”¨å…¶çº¦ç™¾åˆ†ä¹‹å…­åå…­çš„å­˜å‚¨ç©ºé—´ã€‚åœ¨å±æ€§è¡¨ç°å‡ºå¤æ‚å¤šè¾¹å…³ç³»çš„åˆæˆæ•°æ®é›†ä¸Šï¼ŒADCå’ŒADC+è¡¨ç°å‡ºæ˜¾è‘—çš„ç¨³å¥æ€§ï¼Œè€Œå…¶ä»–å‡ ä¹æ‰€æœ‰å­¦ä¹ æ¨¡å‹éƒ½ç»å†äº†æ˜¾è‘—çš„å‡†ç¡®æ€§ä¸‹é™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒADC+çš„è¡¨ç°ä¼˜äºå…¶ä»–æ‰€æœ‰æµ‹è¯•æ¨¡å‹ï¼Œåœ¨95thå’Œ99thç™¾åˆ†ä½è¯¯å·®ä¸Šæ¯”Narué«˜å‡ºåå€å‡†ç¡®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºç®€åŒ–æ‰©æ•£æ¨¡å‹çš„è”åˆåˆ†å¸ƒåŸºæ•°ä¼°è®¡å™¨â€”â€”åŠ é€Ÿæ‰©æ•£åŸºæ•°ä¼°è®¡å™¨ï¼ˆADCï¼‰ã€‚</li>
<li>ADCä½¿ç”¨å¯†åº¦ä¼°è®¡å™¨è®¡ç®—é€ç‚¹å¯†åº¦å€¼ï¼Œé€šè¿‡ç§¯åˆ†å¾—åˆ†å‡½æ•°è¯„ä¼°å¯¹æ•°ä¼¼ç„¶æ€§ã€‚</li>
<li>ADCåˆ©ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰é¢„æµ‹é€‰æ‹©æ€§ï¼Œå¹¶ç»“åˆé‡è¦æ€§æŠ½æ ·è’™ç‰¹å¡æ´›æ–¹æ³•æ ¡æ­£é¢„æµ‹ç»“æœã€‚</li>
<li>ADC+é€šè¿‡è®­ç»ƒå†³ç­–æ ‘ä¼˜åŒ–æ€§èƒ½ï¼Œèƒ½è¯†åˆ«é«˜å®¹é‡ã€é«˜é€‰æ‹©æ€§çš„æŸ¥è¯¢ï¼Œä»è€Œè·³è¿‡æ ¡æ­£é˜¶æ®µæé«˜æ•ˆç‡ã€‚</li>
<li>ADC+ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹å¦‚Naruã€MSCNç­‰å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡å°‘æŸ¥è¯¢å»¶è¿Ÿå¹¶å‡å°‘å­˜å‚¨ç©ºé—´ä½¿ç”¨ã€‚</li>
<li>åœ¨å¤„ç†å…·æœ‰å¤æ‚å¤šè¾¹å…³ç³»çš„åˆæˆæ•°æ®é›†æ—¶ï¼ŒADCå’ŒADC+è¡¨ç°å‡ºæ˜¾è‘—ç¨³å¥æ€§ï¼Œè¿œè¶…å…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d4a6ada7f52fe257b9251b6b872302d" align="middle">
<img src="https://picx.zhimg.com/v2-8298756d9087eff1dce14525a4c3fa2b" align="middle">
<img src="https://picx.zhimg.com/v2-f1483202fa842232c47e9433305a037b" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset"><a href="#UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset" class="headerlink" title="UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale   High-Quality Dataset"></a>UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale   High-Quality Dataset</h2><p><strong>Authors:Chen Zhao, En Ci, Yunzhe Xu, Tiehan Fan, Shanyan Guan, Yanhao Ge, Jian Yang, Ying Tai</strong></p>
<p>Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce \textbf{UltraHR-100K}, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) \textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning on detail-critical denoising steps, and (ii) \textit{Soft-Weighting Frequency Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/NJU-PCALab/UltraHR-100k%7D%7Bhere%7D">https://github.com/NJU-PCALab/UltraHR-100k}{here}</a>. </p>
<blockquote>
<p>è¶…é«˜åˆ†è¾¨ç‡ï¼ˆUHRï¼‰æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿˜æœ‰ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š1ï¼‰ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡UHR T2Iæ•°æ®é›†ï¼›2ï¼‰å¿½è§†é’ˆå¯¹è¶…é«˜åˆ†è¾¨ç‡åœºæ™¯ä¸­ç²¾ç»†ç»†èŠ‚åˆæˆçš„å®šåˆ¶è®­ç»ƒç­–ç•¥ã€‚ä¸ºäº†è§£å†³ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>UltraHR-100K</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡ã€åŒ…å«ä¸°å¯Œæè¿°æ€§æ–‡æœ¬æ ‡æ³¨çš„è¶…é«˜åˆ†è¾¨ç‡å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«10ä¸‡å¼ å›¾åƒï¼Œå†…å®¹å¤šæ ·ä¸”è§†è§‰ä¿çœŸåº¦é«˜ã€‚æ¯å¼ å›¾åƒåˆ†è¾¨ç‡è¶…è¿‡3Kï¼Œå¹¶æ ¹æ®ç»†èŠ‚ä¸°å¯Œç¨‹åº¦ã€å†…å®¹å¤æ‚æ€§å’Œç¾å­¦è´¨é‡è¿›è¡Œä¸¥æ ¼ç­›é€‰ã€‚ä¸ºäº†è§£å†³ç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥çš„åè®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç²¾ç»†ç»†èŠ‚ç”Ÿæˆèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ï¼ˆiï¼‰é¢å‘ç»†èŠ‚çš„æ­¥é•¿é‡‡æ ·ï¼ˆDOTSï¼‰ï¼Œä½¿å­¦ä¹ ä¸“æ³¨äºå…³é”®ç»†èŠ‚å»å™ªæ­¥éª¤ï¼›ï¼ˆiiï¼‰è½¯åŠ æƒé¢‘ç‡æ­£åˆ™åŒ–ï¼ˆSWFRï¼‰ï¼Œåˆ©ç”¨ç¦»æ•£å‚…é‡Œå¶å˜æ¢ï¼ˆDFTï¼‰å¯¹é¢‘ç‡æˆåˆ†è¿›è¡Œè½¯çº¦æŸï¼Œé¼“åŠ±ä¿ç•™é«˜é¢‘ç»†èŠ‚ã€‚åœ¨æˆ‘ä»¬æå‡ºçš„UltraHR-eval4KåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è¶…é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„ç²¾ç»†ç»†èŠ‚è´¨é‡å’Œæ•´ä½“ä¿çœŸåº¦ã€‚ä»£ç å¯ä»[<a target="_blank" rel="noopener" href="https://github.com/NJU-PCALab/UltraHR-100k]%EF%BC%88%E6%AD%A4%E5%A4%84%EF%BC%89%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/NJU-PCALab/UltraHR-100k]ï¼ˆæ­¤å¤„ï¼‰è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20661v1">PDF</a> Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹è¶…é«˜åˆ†è¾¨ç‡ï¼ˆUHRï¼‰æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆé¢†åŸŸçš„ä¸¤ä¸ªæŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œä¸ºäº†åº”å¯¹ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡UHR T2Iæ•°æ®é›†çš„é—®é¢˜ï¼Œå¼•å…¥äº†UltraHR-100Kæ•°æ®é›†ï¼ŒåŒ…å«10ä¸‡å¼ è¶…è¿‡3Kåˆ†è¾¨ç‡çš„é«˜è´¨é‡å›¾åƒå’Œä¸°å¯Œæ ‡æ³¨ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è§£å†³ç²¾ç»†ç»†èŠ‚åˆæˆæ–¹é¢çš„ç‰¹å®šè®­ç»ƒç­–ç•¥ç¼ºä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥çš„åè®­ç»ƒæ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬é¢å‘ç»†èŠ‚çš„æ­¥éª¤é‡‡æ ·ï¼ˆDOTSï¼‰å’Œè½¯åŠ æƒé¢‘ç‡æ­£åˆ™åŒ–ï¼ˆSWFRï¼‰ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†UHRå›¾åƒç”Ÿæˆçš„ç»†èŠ‚è´¨é‡å’Œæ•´ä½“ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UltraHR-100Kæ•°æ®é›†è¢«å¼•å…¥ï¼Œä»¥è§£å†³è¶…é«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡3Kåˆ†è¾¨ç‡çš„å›¾åƒï¼Œå¹¶æ³¨é‡ç»†èŠ‚ä¸°å¯Œåº¦ã€å†…å®¹å¤æ‚åº¦å’Œç¾å­¦è´¨é‡ã€‚</li>
<li>æå‡ºäº†é¢‘ç‡æ„ŸçŸ¥çš„åè®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨è¶…é«˜åˆ†è¾¨ç‡ä¸‹çš„ç²¾ç»†ç»†èŠ‚ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>é¢å‘ç»†èŠ‚çš„æ­¥éª¤é‡‡æ ·ï¼ˆDOTSï¼‰è¢«è®¾è®¡ç”¨äºä¸“æ³¨äºç»†èŠ‚å…³é”®çš„é™å™ªæ­¥éª¤ã€‚</li>
<li>è½¯åŠ æƒé¢‘ç‡æ­£åˆ™åŒ–ï¼ˆSWFRï¼‰åˆ©ç”¨ç¦»æ•£å‚…é‡Œå¶å˜æ¢ï¼ˆDFTï¼‰æ¥æŸ”å’Œåœ°çº¦æŸé¢‘ç‡æˆåˆ†ï¼Œä¿ƒè¿›é«˜é¢‘ç»†èŠ‚çš„ä¿ç•™ã€‚</li>
<li>åœ¨UltraHR-eval4KåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è¶…é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„ç»†èŠ‚è´¨é‡å’Œæ•´ä½“ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15992add63c5b25c3fb72d1747348b83" align="middle">
<img src="https://picx.zhimg.com/v2-1480e0adde706fd4f32e0138a34ed388" align="middle">
<img src="https://picx.zhimg.com/v2-27954e9af1dc35eb58a784aaca273162" align="middle">
<img src="https://picx.zhimg.com/v2-fcccf0db568352e2e2d44d3aaf4da9cb" align="middle">
<img src="https://picx.zhimg.com/v2-aa45425536910c8e661fb17a4aea6fea" align="middle">
<img src="https://picx.zhimg.com/v2-e22abf51fb733ca56f6b5c0a234218ce" align="middle">
<img src="https://picx.zhimg.com/v2-0acc6d9fc36396157943acedec409998" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization"><a href="#EchoDistill-Bidirectional-Concept-Distillation-for-One-Step-Diffusion-Personalization" class="headerlink" title="EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion   Personalization"></a>EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion   Personalization</h2><p><strong>Authors:Yixiong Yang, Tao Wu, Senmao Li, Shiqi Yang, Yaxing Wang, Joost van de Weijer, Kai Wang</strong></p>
<p>Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the synthesis of high-fidelity images even in a single step. However, personalizing these models to incorporate novel concepts remains a challenge due to the limited capacity of one-step models to capture new concept distributions effectively. We propose a bidirectional concept distillation framework, EchoDistill, to enable one-step diffusion personalization (1-SDP). Our approach involves an end-to-end training process where a multi-step diffusion model (teacher) and a one-step diffusion model (student) are trained simultaneously. The concept is first distilled from the teacher model to the student, and then echoed back from the student to the teacher. During the EchoDistill, we share the text encoder between the two models to ensure consistent semantic understanding. Following this, the student model is optimized with adversarial losses to align with the real image distribution and with alignment losses to maintain consistency with the teacherâ€™s output. Furthermore, we introduce the bidirectional echoing refinement strategy, wherein the student model leverages its faster generation capability to feedback to the teacher model. This bidirectional concept distillation mechanism not only enhances the student ability to personalize novel concepts but also improves the generative quality of the teacher model. Our experiments demonstrate that this collaborative framework significantly outperforms existing personalization methods over the 1-SDP setup, establishing a novel paradigm for rapid and effective personalization in T2I diffusion models. </p>
<blockquote>
<p>è¿‘æœŸåŠ é€Ÿæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•ä½¿å¾—ä¸€æ­¥åˆæˆé«˜æ¸…å›¾åƒæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç”±äºä¸€æ­¥æ¨¡å‹åœ¨æœ‰æ•ˆæ•æ‰æ–°æ¦‚å¿µåˆ†å¸ƒæ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œå°†è¿™äº›æ¨¡å‹ä¸ªæ€§åŒ–ä»¥èå…¥æ–°è§‚å¿µä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒå‘æ¦‚å¿µè’¸é¦æ¡†æ¶EchoDistillï¼Œä»¥å®ç°ä¸€æ­¥æ‰©æ•£ä¸ªæ€§åŒ–ï¼ˆ1-SDPï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠç«¯åˆ°ç«¯çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…¶ä¸­å¤šæ­¥æ‰©æ•£æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰å’Œä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰åŒæ—¶è¿›è¡Œè®­ç»ƒã€‚æ¦‚å¿µé¦–å…ˆä»æ•™å¸ˆæ¨¡å‹è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ï¼Œç„¶åä»å­¦ç”Ÿæ¨¡å‹åé¦ˆå›æ•™å¸ˆæ¨¡å‹ã€‚åœ¨EchoDistillè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´å…±äº«æ–‡æœ¬ç¼–ç å™¨ï¼Œä»¥ç¡®ä¿ä¸€è‡´çš„è¯­ä¹‰ç†è§£ã€‚ä¹‹åï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡å¯¹æŠ—æŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ä¸çœŸå®å›¾åƒåˆ†å¸ƒå¯¹é½ï¼Œå¹¶é€šè¿‡å¯¹é½æŸå¤±æ¥ä¿æŒä¸æ•™å¸ˆæ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒå‘å›å£°ç»†åŒ–ç­–ç•¥ï¼Œå­¦ç”Ÿæ¨¡å‹åˆ©ç”¨å…¶æ›´å¿«çš„ç”Ÿæˆèƒ½åŠ›æ¥åé¦ˆç»™æ•™å¸ˆæ¨¡å‹ã€‚è¿™ç§åŒå‘æ¦‚å¿µè’¸é¦æœºåˆ¶ä¸ä»…æé«˜äº†å­¦ç”Ÿæ¨¡å‹å¯¹æ–°é¢–æ¦‚å¿µçš„ä¸ªæ€§åŒ–èƒ½åŠ›ï¼Œè€Œä¸”æé«˜äº†æ•™å¸ˆæ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ä¸€åä½œæ¡†æ¶åœ¨1-SDPè®¾ç½®ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„å¿«é€Ÿæœ‰æ•ˆä¸ªæ€§åŒ–å»ºç«‹äº†æ–°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20512v1">PDF</a> Project page available at   <a target="_blank" rel="noopener" href="https://liulisixin.github.io/EchoDistill-page/">https://liulisixin.github.io/EchoDistill-page/</a></p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºäº†ä¸€ç§åŒå‘æ¦‚å¿µè’¸é¦æ¡†æ¶EchoDistillï¼Œç”¨äºå®ç°ä¸€æ­¥æ‰©æ•£ä¸ªæ€§åŒ–ï¼ˆ1-SDPï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒè¿‡ç¨‹ï¼Œå°†å¤šæ­¥æ‰©æ•£æ¨¡å‹ï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰å’Œä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰åŒæ—¶è®­ç»ƒã€‚æ¦‚å¿µé¦–å…ˆç”±æ•™å¸ˆæ¨¡å‹è’¸é¦ç»™å­¦ç”Ÿæ¨¡å‹ï¼Œå†ä»å­¦ç”Ÿæ¨¡å‹åé¦ˆå›æ•™å¸ˆæ¨¡å‹ã€‚å…±äº«æ–‡æœ¬ç¼–ç å™¨ä»¥ç¡®ä¿ä¸€è‡´çš„è¯­ä¹‰ç†è§£ã€‚å­¦ç”Ÿæ¨¡å‹é€šè¿‡å¯¹æŠ—æ€§æŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ä¸ç°å®å›¾åƒåˆ†å¸ƒå¯¹é½ï¼Œå¹¶é€šè¿‡å¯¹é½æŸå¤±ä»¥ä¿æŒä¸æ•™å¸ˆæ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†åŒå‘å›å£°ä¼˜åŒ–ç­–ç•¥ï¼Œå­¦ç”Ÿæ¨¡å‹åˆ©ç”¨å…¶æ›´å¿«çš„ç”Ÿæˆèƒ½åŠ›åé¦ˆç»™æ•™å¸ˆæ¨¡å‹ã€‚è¿™ç§åŒå‘æ¦‚å¿µè’¸é¦æœºåˆ¶ä¸ä»…æé«˜äº†å­¦ç”Ÿå¯¹æ–°æ¦‚å¿µçš„ä¸ªæ€§åŒ–èƒ½åŠ›ï¼Œè¿˜æé«˜äº†æ•™å¸ˆæ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥åä½œæ¡†æ¶åœ¨1-SDPè®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œä¸ºT2Iæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿæœ‰æ•ˆä¸ªæ€§åŒ–å»ºç«‹äº†æ–°èŒƒå¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†EchoDistillåŒå‘æ¦‚å¿µè’¸é¦æ¡†æ¶ï¼Œç”¨äºåŠ é€Ÿæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–ã€‚</li>
<li>é€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶è®­ç»ƒæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ï¼Œå®ç°æ¦‚å¿µè’¸é¦ä¸åé¦ˆã€‚</li>
<li>å…±äº«æ–‡æœ¬ç¼–ç å™¨ä»¥ç¡®ä¿åœ¨è’¸é¦å’Œåé¦ˆè¿‡ç¨‹ä¸­è¯­ä¹‰ç†è§£çš„ä¸€è‡´æ€§ã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹é€šè¿‡ä¼˜åŒ–å¯¹æŠ—æ€§æŸå¤±å’Œå¯¹é½æŸå¤±ï¼Œä»¥ä¸ç°å®å›¾åƒåˆ†å¸ƒå¯¹é½å¹¶ç»´æŒä¸æ•™å¸ˆæ¨¡å‹çš„ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥åŒå‘å›å£°ä¼˜åŒ–ç­–ç•¥ï¼Œåˆ©ç”¨å­¦ç”Ÿæ¨¡å‹çš„å¿«é€Ÿç”Ÿæˆèƒ½åŠ›åé¦ˆç»™æ•™å¸ˆæ¨¡å‹ï¼Œè¿›ä¸€æ­¥æé«˜ä¸¤è€…æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸€æ­¥æ‰©æ•£ä¸ªæ€§åŒ–ï¼ˆ1-SDPï¼‰è®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffb0da0335cb7fd218397febabd20e5f" align="middle">
<img src="https://picx.zhimg.com/v2-9ed2755286b70e4847d3fffa7f0bc371" align="middle">
<img src="https://picx.zhimg.com/v2-0267ede65031c7b99b5e47f9d616b2a7" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EditInfinity-Image-Editing-with-Binary-Quantized-Generative-Models"><a href="#EditInfinity-Image-Editing-with-Binary-Quantized-Generative-Models" class="headerlink" title="EditInfinity: Image Editing with Binary-Quantized Generative Models"></a>EditInfinity: Image Editing with Binary-Quantized Generative Models</h2><p><strong>Authors:Jiahuan Wang, Yuxin Chen, Jun Yu, Guangming Lu, Wenjie Pei</strong></p>
<p>Adapting pretrained diffusion-based generative models for text-driven image editing with negligible tuning overhead has demonstrated remarkable potential. A classical adaptation paradigm, as followed by these methods, first infers the generative trajectory inversely for a given source image by image inversion, then performs image editing along the inferred trajectory guided by the target text prompts. However, the performance of image editing is heavily limited by the approximation errors introduced during image inversion by diffusion models, which arise from the absence of exact supervision in the intermediate generative steps. To circumvent this issue, we investigate the parameter-efficient adaptation of VQ-based generative models for image editing, and leverage their inherent characteristic that the exact intermediate quantized representations of a source image are attainable, enabling more effective supervision for precise image inversion. Specifically, we propose \emph{EditInfinity}, which adapts \emph{Infinity}, a binary-quantized generative model, for image editing. We propose an efficient yet effective image inversion mechanism that integrates text prompting rectification and image style preservation, enabling precise image inversion. Furthermore, we devise a holistic smoothing strategy which allows our \emph{EditInfinity} to perform image editing with high fidelity to source images and precise semantic alignment to the text prompts. Extensive experiments on the PIE-Bench benchmark across â€œaddâ€, â€œchangeâ€, and â€œdeleteâ€ editing operations, demonstrate the superior performance of our model compared to state-of-the-art diffusion-based baselines. Code available at: <a target="_blank" rel="noopener" href="https://github.com/yx-chen-ust/EditInfinity">https://github.com/yx-chen-ust/EditInfinity</a>. </p>
<blockquote>
<p>é€‚åº”é¢„è®­ç»ƒçš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬é©±åŠ¨çš„å›¾ç‰‡ç¼–è¾‘ï¼Œä¸”å‡ ä¹ä¸éœ€è¦è°ƒæ•´å¼€é”€ï¼Œå·²ç»æ˜¾ç¤ºå‡ºæ˜¾è‘—æ½œåŠ›ã€‚è¿™äº›æ–¹æ³•éµå¾ªçš„ç»å…¸é€‚åº”æ¨¡å¼é¦–å…ˆé€šè¿‡å¯¹ç»™å®šæºå›¾åƒè¿›è¡Œå›¾åƒåæ¼”æ¥é€†å‘æ¨æ–­ç”Ÿæˆè½¨è¿¹ï¼Œç„¶åæ²¿ç€æ¨æ–­å‡ºçš„è½¨è¿¹åœ¨ç›®æ ‡æ–‡æœ¬æç¤ºçš„æŒ‡å¯¼ä¸‹è¿›è¡Œå›¾åƒç¼–è¾‘ã€‚ç„¶è€Œï¼Œå›¾åƒç¼–è¾‘çš„æ€§èƒ½å—åˆ°æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåæ¼”è¿‡ç¨‹ä¸­å¼•å…¥çš„è¿‘ä¼¼è¯¯å·®çš„ä¸¥é‡é™åˆ¶ï¼Œè¿™äº›è¯¯å·®æºäºä¸­é—´ç”Ÿæˆæ­¥éª¤ä¸­ç¼ºä¹ç²¾ç¡®çš„ç›‘ç£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºVQçš„ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­çš„å‚æ•°é«˜æ•ˆé€‚åº”æ€§é—®é¢˜ï¼Œå¹¶åˆ©ç”¨äº†å®ƒä»¬çš„å›ºæœ‰ç‰¹æ€§ï¼Œå³å¯ä»¥è·å¾—æºå›¾åƒçš„ç²¾ç¡®ä¸­é—´é‡åŒ–è¡¨ç¤ºï¼Œä¸ºç²¾ç¡®å›¾åƒåæ¼”æä¾›äº†æ›´æœ‰æ•ˆçš„ç›‘ç£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†â€œEditInfinityâ€ï¼Œå®ƒé€‚åº”äº†â€œInfinityâ€è¿™ä¸€äºŒè¿›åˆ¶é‡åŒ–ç”Ÿæˆæ¨¡å‹ç”¨äºå›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„å›¾åƒåæ¼”æœºåˆ¶ï¼Œèåˆäº†æ–‡æœ¬æç¤ºæ ¡æ­£å’Œå›¾åƒé£æ ¼ä¿æŒï¼Œå®ç°äº†ç²¾ç¡®çš„å›¾åƒåæ¼”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æ•´ä½“å¹³æ»‘ç­–ç•¥ï¼Œä½¿æˆ‘ä»¬çš„â€œEditInfinityâ€èƒ½å¤Ÿåœ¨æºå›¾åƒä¸Šè¿›è¡Œé«˜ä¿çœŸåº¦çš„å›¾åƒç¼–è¾‘ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºå®ç°ç²¾ç¡®è¯­ä¹‰å¯¹é½ã€‚åœ¨PIE-BenchåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæ¶µç›–äº†â€œæ·»åŠ â€ã€â€œæ›´æ”¹â€å’Œâ€œåˆ é™¤â€ç¼–è¾‘æ“ä½œï¼Œè¯æ˜äº†æˆ‘ä»¬æ¨¡å‹ç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ‰©æ•£åŸºå‡†æ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/yx-chen-ust/EditInfinity%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yx-chen-ust/EditInfinityæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20217v1">PDF</a> 28 pages, 13 figures, accepted by The Thirty-ninth Annual Conference   on Neural Information Processing Systems (NeurIPS 2025)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£ç”Ÿæˆæ¨¡å‹è¿›è¡Œæ–‡æœ¬é©±åŠ¨å›¾åƒç¼–è¾‘ï¼Œæ— éœ€å¤§é‡è°ƒæ•´ï¼Œå±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚å½“å‰æ–¹æ³•é€šè¿‡é€†å‘æ¨æ–­ç»™å®šæºå›¾åƒçš„ç”Ÿæˆè½¨è¿¹ï¼Œç„¶åæ²¿æ¨æ–­è½¨è¿¹è¿›è¡Œå›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶å—ç›®æ ‡æ–‡æœ¬æç¤ºå¼•å¯¼ã€‚ç„¶è€Œï¼Œç”±äºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåè½¬è¿‡ç¨‹ä¸­çš„è¿‘ä¼¼è¯¯å·®ï¼Œé™åˆ¶äº†å›¾åƒç¼–è¾‘çš„æ•ˆæœã€‚è¿™äº›è¯¯å·®æºäºä¸­é—´ç”Ÿæˆæ­¥éª¤ä¸­ç¼ºä¹ç²¾ç¡®ç›‘ç£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†VQåŸºç”Ÿæˆæ¨¡å‹çš„å‚æ•°é«˜æ•ˆé€‚åº”å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œåˆ©ç”¨å…¶å›ºæœ‰çš„ç‰¹æ€§â€”â€”è·å–æºå›¾åƒçš„ä¸­é—´é‡åŒ–è¡¨ç¤ºï¼Œä¸ºç²¾ç¡®å›¾åƒåè½¬æä¾›æ›´æœ‰æ•ˆçš„ç›‘ç£ã€‚æˆ‘ä»¬æå‡ºäº†EditInfinityï¼Œå®ƒé€‚åº”äº†Infinityè¿™ä¸€äºŒå…ƒé‡åŒ–ç”Ÿæˆæ¨¡å‹ç”¨äºå›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé«˜æ•ˆè€Œç²¾ç¡®çš„å›¾åƒåè½¬æœºåˆ¶ï¼Œèåˆäº†æ–‡æœ¬æç¤ºæ ¡æ­£å’Œå›¾åƒé£æ ¼ä¿æŒï¼Œä½¿å›¾åƒåè½¬æ›´åŠ ç²¾ç¡®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ•´ä½“å¹³æ»‘ç­–ç•¥ï¼Œä½¿EditInfinityèƒ½å¤Ÿåœ¨ä¿æŒæºå›¾åƒé«˜ä¿çœŸåº¦çš„åŒæ—¶è¿›è¡Œç²¾ç¡®çš„è¯­ä¹‰æ–‡æœ¬å¯¹é½ç¼–è¾‘ã€‚åœ¨PIE-BenchåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ·»åŠ ã€æ›´æ”¹å’Œåˆ é™¤ç¼–è¾‘æ“ä½œä¸Šçš„æ€§èƒ½å‡ä¼˜äºæœ€å…ˆè¿›çš„æ‰©æ•£åŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘ä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šè¿‡æ¨æ–­ç”Ÿæˆè½¨è¿¹è¿›è¡Œå›¾åƒç¼–è¾‘ï¼Œä½†å­˜åœ¨è¿‘ä¼¼è¯¯å·®é—®é¢˜ã€‚</li>
<li>VQåŸºç”Ÿæˆæ¨¡å‹çš„ä¸­é—´é‡åŒ–è¡¨ç¤ºå¯æé«˜å›¾åƒåè½¬çš„ç²¾ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†EditInfinityæ¨¡å‹ï¼Œé€‚åº”äº†InfinityäºŒå…ƒé‡åŒ–ç”Ÿæˆæ¨¡å‹ç”¨äºå›¾åƒç¼–è¾‘ã€‚</li>
<li>EditInfinityè®¾è®¡äº†ä¸€ä¸ªé«˜æ•ˆç²¾ç¡®å›¾åƒåè½¬æœºåˆ¶ï¼Œèåˆæ–‡æœ¬æç¤ºæ ¡æ­£å’Œå›¾åƒé£æ ¼ä¿æŒã€‚</li>
<li>æ•´ä½“å¹³æ»‘ç­–ç•¥ä½¿EditInfinityèƒ½åœ¨ä¿æŒæºå›¾åƒé«˜ä¿çœŸåº¦çš„åŒæ—¶è¿›è¡Œç²¾ç¡®çš„è¯­ä¹‰æ–‡æœ¬å¯¹é½ç¼–è¾‘ã€‚</li>
<li>åœ¨PIE-BenchåŸºå‡†æµ‹è¯•ä¸Šï¼ŒEditInfinityæ€§èƒ½ä¼˜äºå…¶ä»–æ‰©æ•£åŸºå‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-714023223330813d5c8ec3a9ac248a25" align="middle">
<img src="https://picx.zhimg.com/v2-3a0e0f77c7c87758f012285351da3434" align="middle">
<img src="https://picx.zhimg.com/v2-e6b1a9eadfe2f137f9ad818b7b785edb" align="middle">
<img src="https://picx.zhimg.com/v2-ec8da7efba112a41b939c31296147368" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StableSketcher-Enhancing-Diffusion-Model-for-Pixel-based-Sketch-Generation-via-Visual-Question-Answering-Feedback"><a href="#StableSketcher-Enhancing-Diffusion-Model-for-Pixel-based-Sketch-Generation-via-Visual-Question-Answering-Feedback" class="headerlink" title="StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch   Generation via Visual Question Answering Feedback"></a>StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch   Generation via Visual Question Answering Feedback</h2><p><strong>Authors:Jiho Park, Sieun Choi, Jaeyoon Seo, Jihie Kim</strong></p>
<p>Although recent advancements in diffusion models have significantly enriched the quality of generated images, challenges remain in synthesizing pixel-based human-drawn sketches, a representative example of abstract expression. To combat these challenges, we propose StableSketcher, a novel framework that empowers diffusion models to generate hand-drawn sketches with high prompt fidelity. Within this framework, we fine-tune the variational autoencoder to optimize latent decoding, enabling it to better capture the characteristics of sketches. In parallel, we integrate a new reward function for reinforcement learning based on visual question answering, which improves text-image alignment and semantic consistency. Extensive experiments demonstrate that StableSketcher generates sketches with improved stylistic fidelity, achieving better alignment with prompts compared to the Stable Diffusion baseline. Additionally, we introduce SketchDUO, to the best of our knowledge, the first dataset comprising instance-level sketches paired with captions and question-answer pairs, thereby addressing the limitations of existing datasets that rely on image-label pairs. Our code and dataset will be made publicly available upon acceptance. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹æœ€è¿‘çš„è¿›å±•æå¤§åœ°æé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ï¼Œä½†åœ¨åˆæˆåŸºäºåƒç´ çš„æ‰‹ç»˜è‰å›¾ç­‰æŠ½è±¡è¡¨è¾¾ä»£è¡¨æ€§ç¤ºä¾‹æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†StableSketcherï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿèµ‹èƒ½æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…·æœ‰é«˜æç¤ºä¿çœŸåº¦çš„æ‰‹ç»˜è‰å›¾ã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œæˆ‘ä»¬å¾®è°ƒäº†å˜åˆ†è‡ªç¼–ç å™¨ä»¥ä¼˜åŒ–æ½œåœ¨è§£ç ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è‰å›¾çš„ç‰¹ç‚¹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ç»“åˆäº†ä¸€ç§åŸºäºè§†è§‰é—®ç­”çš„æ–°å‹å¥–åŠ±å‡½æ•°ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜æ–‡æœ¬-å›¾åƒå¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒStableSketcherç”Ÿæˆçš„è‰å›¾åœ¨é£æ ¼ä¿çœŸåº¦ä¸Šæœ‰æ‰€æé«˜ï¼Œä¸æç¤ºçš„å¯¹é½ç¨‹åº¦ä¼˜äºStable DiffusionåŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†SketchDUOæ•°æ®é›†ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªåŒ…å«å®ä¾‹çº§è‰å›¾ä¸æ ‡é¢˜å’Œé—®ç­”å¯¹é…å¯¹çš„æ•°æ®é›†ï¼Œä»è€Œè§£å†³äº†ç°æœ‰æ•°æ®é›†ä»…ä¾èµ–å›¾åƒæ ‡ç­¾å¯¹çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†åœ¨æ¥å—åå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.20093v1">PDF</a> Under review at IEEE Access. Author-submitted preprint. Not the   IEEE-published version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºStableSketcherçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºå¢å¼ºæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ‰‹ç»˜ç´ æå›¾çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒå˜åˆ†è‡ªç¼–ç å™¨ä»¥ä¼˜åŒ–æ½œåœ¨è§£ç ï¼Œå¹¶å¼•å…¥åŸºäºè§†è§‰é—®ç­”çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œæé«˜äº†æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒStableSketcherç”Ÿæˆçš„ç´ æå›¾åœ¨é£æ ¼ä¿çœŸåº¦ä¸Šæœ‰æ‰€æé«˜ï¼Œä¸æç¤ºå¯¹é½çš„æ•ˆæœæ›´å¥½ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†SketchDUOæ•°æ®é›†ï¼ŒåŒ…å«å¸¦æ³¨é‡Šçš„å®ä¾‹çº§ç´ æå›¾ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†ä¾èµ–å›¾åƒæ ‡ç­¾å¯¹çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StableSketcheræ¡†æ¶è¢«æå‡ºï¼Œæ—¨åœ¨å¢å¼ºæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ‰‹ç»˜ç´ æå›¾çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¾®è°ƒå˜åˆ†è‡ªç¼–ç å™¨ä¼˜åŒ–æ½œåœ¨è§£ç ï¼Œä»¥æ›´å¥½åœ°æ•æ‰ç´ æç‰¹å¾ã€‚</li>
<li>å¼•å…¥åŸºäºè§†è§‰é—®ç­”çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œæé«˜æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¯æ˜StableSketcherç”Ÿæˆçš„ç´ æå›¾åœ¨é£æ ¼ä¿çœŸåº¦ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>StableSketcherç›¸æ¯”åŸºçº¿æ¨¡å‹åœ¨æç¤ºå¯¹é½æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</li>
<li>æ¨å‡ºäº†SketchDUOæ•°æ®é›†ï¼ŒåŒ…å«å¸¦æ³¨é‡Šçš„å®ä¾‹çº§ç´ æå›¾ï¼Œè§£å†³ç°æœ‰æ•°æ®é›†çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.20093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e63434493113edf15da15546ea0a8b4" align="middle">
<img src="https://picx.zhimg.com/v2-2e93e938db4594ec3f177c118c8810b5" align="middle">
<img src="https://picx.zhimg.com/v2-7818b1bab88612f293f5bc0d7948b4fe" align="middle">
<img src="https://picx.zhimg.com/v2-250f8d3e511d0f4ad7384a64812c76d8" align="middle">
<img src="https://picx.zhimg.com/v2-621cf983f9243bb45841c824ef59c739" align="middle">
<img src="https://picx.zhimg.com/v2-c84efe55a4848745cbd7c3f13022abc8" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CBDiff-Conditional-Bernoulli-Diffusion-Models-for-Image-Forgery-Localization"><a href="#CBDiff-Conditional-Bernoulli-Diffusion-Models-for-Image-Forgery-Localization" class="headerlink" title="CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery   Localization"></a>CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery   Localization</h2><p><strong>Authors:Zhou Lei, Pan Gang, Wang Jiahao, Sun Di</strong></p>
<p>Image Forgery Localization (IFL) is a crucial task in image forensics, aimed at accurately identifying manipulated or tampered regions within an image at the pixel level. Existing methods typically generate a single deterministic localization map, which often lacks the precision and reliability required for high-stakes applications such as forensic analysis and security surveillance. To enhance the credibility of predictions and mitigate the risk of errors, we introduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a forged image, CBDiff generates multiple diverse and plausible localization maps, thereby offering a richer and more comprehensive representation of the forgery distribution. This approach addresses the uncertainty and variability inherent in tampered regions. Furthermore, CBDiff innovatively incorporates Bernoulli noise into the diffusion process to more faithfully reflect the inherent binary and sparse properties of forgery masks. Additionally, CBDiff introduces a Time-Step Cross-Attention (TSCAttention), which is specifically designed to leverage semantic feature guidance with temporal steps to improve manipulation detection. Extensive experiments on eight publicly benchmark datasets demonstrate that CBDiff significantly outperforms existing state-of-the-art methods, highlighting its strong potential for real-world deployment. </p>
<blockquote>
<p>å›¾åƒä¼ªé€ å®šä½ï¼ˆIFLï¼‰æ˜¯å›¾åƒå–è¯ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨å‡†ç¡®åœ°åœ¨åƒç´ çº§åˆ«è¯†åˆ«å›¾åƒä¸­è¢«æ“çºµæˆ–ç¯¡æ”¹çš„åŒºåŸŸã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç”Ÿæˆå•ä¸ªç¡®å®šæ€§å®šä½å›¾ï¼Œè¿™é€šå¸¸ç¼ºä¹ç”¨äºé«˜é£é™©åº”ç”¨ï¼ˆä¾‹å¦‚æ³•åŒ»åˆ†æå’Œå®‰å…¨ç›‘æ§ï¼‰æ‰€éœ€çš„é«˜ç²¾åº¦å’Œå¯é æ€§ã€‚ä¸ºäº†æé«˜é¢„æµ‹çš„å¯é æ€§å¹¶é™ä½é”™è¯¯é£é™©ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…ˆè¿›çš„æ¡ä»¶ä¼¯åŠªåˆ©æ‰©æ•£æ¨¡å‹ï¼ˆCBDiffï¼‰ã€‚ç»™å®šä¼ªé€ å›¾åƒï¼ŒCBDiffç”Ÿæˆå¤šä¸ªå¤šæ ·åŒ–å’Œåˆç†çš„å®šä½å›¾ï¼Œä»è€Œä¸ºä¼ªé€ åˆ†å¸ƒæä¾›æ›´ä¸°å¯Œå’Œå…¨é¢çš„è¡¨ç¤ºã€‚æ­¤æ–¹æ³•è§£å†³äº†ç¯¡æ”¹åŒºåŸŸå›ºæœ‰çš„ä¸ç¡®å®šæ€§å’Œå˜åŒ–æ€§ã€‚æ­¤å¤–ï¼ŒCBDiffåˆ›æ–°åœ°å°†ä¼¯åŠªåˆ©å™ªå£°èå…¥æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥æ›´çœŸå®åœ°åæ˜ ä¼ªé€ æ©ç çš„å›ºæœ‰äºŒè¿›åˆ¶å’Œç¨€ç–å±æ€§ã€‚æ­¤å¤–ï¼ŒCBDiffè¿˜å¼•å…¥äº†æ—¶é—´æ­¥äº¤å‰æ³¨æ„åŠ›ï¼ˆTSCAttentionï¼‰ï¼Œä¸“é—¨è®¾è®¡ç”¨äºåˆ©ç”¨è¯­ä¹‰ç‰¹å¾æŒ‡å¯¼ä¸æ—¶é—´æ­¥çš„ç»“åˆæ¥æé«˜æ“çºµæ£€æµ‹çš„æ€§èƒ½ã€‚åœ¨å…«ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCBDiffæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19597v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒä¼ªé€ å®šä½ï¼ˆIFLï¼‰ä»»åŠ¡çš„é‡è¦æ€§åŠå…¶åœ¨å›¾åƒå–è¯é¢†åŸŸçš„åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ç”Ÿæˆå•ä¸ªç¡®å®šæ€§å®šä½å›¾ï¼Œç¼ºä¹åœ¨é«˜çº§åº”ç”¨ï¼ˆå¦‚å–è¯åˆ†æå’Œå®‰å…¨ç›‘æ§ï¼‰ä¸­æ‰€éœ€çš„é«˜ç²¾åº¦å’Œå¯é æ€§ã€‚ä¸ºæ”¹è¿›é¢„æµ‹çš„å¯ä¿¡åº¦å¹¶é™ä½é”™è¯¯é£é™©ï¼Œæœ¬æ–‡å¼•å…¥äº†å…ˆè¿›çš„æ¡ä»¶ä¼¯åŠªåˆ©æ‰©æ•£æ¨¡å‹ï¼ˆCBDiffï¼‰ã€‚ç»™å®šä¼ªé€ å›¾åƒï¼ŒCBDiffèƒ½å¤Ÿç”Ÿæˆå¤šä¸ªå¤šæ ·åŒ–å’Œåˆç†çš„å®šä½å›¾ï¼Œä»è€Œæä¾›æ›´ä¸°å¯Œå’Œå…¨é¢çš„ä¼ªé€ åˆ†å¸ƒè¡¨ç¤ºã€‚æ­¤æ–¹æ³•è§£å†³äº†ç¯¡æ”¹åŒºåŸŸä¸­å›ºæœ‰çš„ä¸ç¡®å®šæ€§å’Œå˜åŒ–æ€§ã€‚æ­¤å¤–ï¼ŒCBDiffè¿˜å°†ä¼¯åŠªåˆ©å™ªå£°èå…¥æ‰©æ•£è¿‡ç¨‹ï¼Œæ›´çœŸå®åœ°åæ˜ äº†ä¼ªé€ æ©è†œçš„å›ºæœ‰äºŒè¿›åˆ¶å’Œç¨€ç–å±æ€§ã€‚åŒæ—¶ï¼ŒCBDiffå¼•å…¥äº†æ—¶é—´æ­¥äº¤å‰æ³¨æ„åŠ›ï¼ˆTSCAttentionï¼‰æœºåˆ¶ï¼Œä¸“é—¨è®¾è®¡ç”¨äºåˆ©ç”¨è¯­ä¹‰ç‰¹å¾æŒ‡å¯¼ä¸ä¸´æ—¶æ­¥éª¤æ¥æé«˜æ“ä½œæ£€æµ‹ã€‚åœ¨å…«ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCBDiffæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å›¾åƒä¼ªé€ å®šä½ï¼ˆIFLï¼‰æ˜¯å›¾åƒå–è¯ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨å‡†ç¡®è¯†åˆ«å›¾åƒä¸­çš„æ“çºµæˆ–ç¯¡æ”¹åŒºåŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸ç”Ÿæˆå•ä¸€ç¡®å®šæ€§å®šä½å›¾ï¼Œè¿™åœ¨é«˜é£é™©åº”ç”¨ä¸­å¯èƒ½ç¼ºä¹è¶³å¤Ÿçš„ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
<li>CBDiffæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šä¸ªå¤šæ ·åŒ–å’Œåˆç†çš„å®šä½å›¾ï¼Œæä¾›æ›´å…¨é¢å’ŒçœŸå®çš„ä¼ªé€ è¡¨ç¤ºã€‚</li>
<li>CBDiffå¤„ç†ç¯¡æ”¹åŒºåŸŸä¸­çš„ä¸ç¡®å®šæ€§å’Œå˜åŒ–æ€§ã€‚</li>
<li>CBDiffé€šè¿‡å°†ä¼¯åŠªåˆ©å™ªå£°èå…¥æ‰©æ•£è¿‡ç¨‹ï¼Œæ›´çœŸå®åœ°åæ˜ ä¼ªé€ æ©è†œçš„å›ºæœ‰å±æ€§ã€‚</li>
<li>TSCAttentionæœºåˆ¶æœ‰åŠ©äºæé«˜æ“ä½œæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc90fe8de41f2cd46c6986a843956c4b" align="middle">
<img src="https://picx.zhimg.com/v2-b4953ceb52e5fae9174405deb319a261" align="middle">
<img src="https://picx.zhimg.com/v2-602be111dec371fc3db9df47ed41b17e" align="middle">
<img src="https://picx.zhimg.com/v2-d00eb8a4d26f7fc0e370c87de7a35bce" align="middle">
<img src="https://picx.zhimg.com/v2-7834f442d08575b37191943771dc761a" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-Intricate-Dance-of-Prompt-Complexity-Quality-Diversity-and-Consistency-in-T2I-Models"><a href="#The-Intricate-Dance-of-Prompt-Complexity-Quality-Diversity-and-Consistency-in-T2I-Models" class="headerlink" title="The Intricate Dance of Prompt Complexity, Quality, Diversity, and   Consistency in T2I Models"></a>The Intricate Dance of Prompt Complexity, Quality, Diversity, and   Consistency in T2I Models</h2><p><strong>Authors:Xiaofeng Zhang, Aaron Courville, Michal Drozdzal, Adriana Romero-Soriano</strong></p>
<p>Text-to-image (T2I) models offer great potential for creating virtually limitless synthetic data, a valuable resource compared to fixed and finite real datasets. Previous works evaluate the utility of synthetic data from T2I models on three key desiderata: quality, diversity, and consistency. While prompt engineering is the primary means of interacting with T2I models, the systematic impact of prompt complexity on these critical utility axes remains underexplored. In this paper, we first conduct synthetic experiments to motivate the difficulty of generalization w.r.t. prompt complexity and explain the observed difficulty with theoretical derivations. Then, we introduce a new evaluation framework that can compare the utility of real data and synthetic data, and present a comprehensive analysis of how prompt complexity influences the utility of synthetic data generated by commonly used T2I models. We conduct our study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and evaluate different inference-time intervention methods. Our synthetic experiments show that generalizing to more general conditions is harder than the other way round, since the former needs an estimated likelihood that is not learned by diffusion models. Our large-scale empirical experiments reveal that increasing prompt complexity results in lower conditional diversity and prompt consistency, while reducing the synthetic-to-real distribution shift, which aligns with the synthetic experiments. Moreover, current inference-time interventions can augment the diversity of the generations at the expense of moving outside the support of real data. Among those interventions, prompt expansion, by deliberately using a pre-trained language model as a likelihood estimator, consistently achieves the highest performance in both image diversity and aesthetics, even higher than that of real data. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨åˆ›å»ºå‡ ä¹æ— é™åˆæˆæ•°æ®æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä¸å›ºå®šå’Œæœ‰é™çœŸå®æ•°æ®é›†ç›¸æ¯”ï¼Œè¿™æ˜¯ä¸€ç§æœ‰ä»·å€¼çš„èµ„æºã€‚ä»¥å‰çš„ç ”ç©¶å·¥ä½œè¯„ä¼°äº†æ¥è‡ªT2Iæ¨¡å‹çš„åˆæˆæ•°æ®åœ¨ä¸‰ä¸ªå…³é”®è¦ç´ ä¸Šçš„å®ç”¨æ€§ï¼šè´¨é‡ã€å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚è™½ç„¶æç¤ºå·¥ç¨‹æ˜¯ä¸T2Iæ¨¡å‹äº¤äº’çš„ä¸»è¦æ‰‹æ®µï¼Œä½†å…³äºæç¤ºå¤æ‚æ€§å¯¹è¿™äº›å…³é”®å®ç”¨è½´çš„ç³»ç»Ÿæ€§å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œåˆæˆå®éªŒï¼Œä»¥è¯æ˜å…³äºæç¤ºå¤æ‚æ€§çš„æ³›åŒ–éš¾åº¦ï¼Œå¹¶ç”¨ç†è®ºæ¨å¯¼æ¥è§£é‡Šè§‚å¯Ÿåˆ°çš„å›°éš¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯ä»¥æ¯”è¾ƒçœŸå®æ•°æ®å’Œåˆæˆæ•°æ®å®ç”¨æ€§çš„æ–°è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¯¹æç¤ºå¤æ‚æ€§å¦‚ä½•å½±å“å¸¸ç”¨T2Iæ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®çš„å®ç”¨æ€§è¿›è¡Œäº†ç»¼åˆåˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¶µç›–äº†CC12Mã€ImageNet-1kå’ŒDCIç­‰å¤šä¸ªæ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒçš„æ¨ç†æ—¶é—´å¹²é¢„æ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆæˆå®éªŒè¡¨æ˜ï¼Œæ¨å¹¿è‡³æ›´ä¸€èˆ¬çš„æ¡ä»¶æ¯”åæ–¹å‘æ›´éš¾ï¼Œå› ä¸ºå‰è€…éœ€è¦ä¼°è®¡çš„ä¼¼ç„¶æ€§æ˜¯æ‰©æ•£æ¨¡å‹æ²¡æœ‰å­¦åˆ°çš„ã€‚æˆ‘ä»¬çš„å¤§è§„æ¨¡å®è¯ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¢åŠ æç¤ºå¤æ‚æ€§ä¼šå¯¼è‡´æ¡ä»¶å¤šæ ·æ€§å’Œæç¤ºä¸€è‡´æ€§é™ä½ï¼ŒåŒæ—¶å‡å°‘åˆæˆåˆ°çœŸå®åˆ†å¸ƒè½¬ç§»ï¼Œè¿™ä¸åˆæˆå®éªŒç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ¨ç†æ—¶é—´å¹²é¢„æªæ–½å¯ä»¥é€šè¿‡å¢åŠ ç”Ÿæˆçš„å¤šæ ·æ€§æ¥ç‰ºç‰²å¯¹çœŸå®æ•°æ®æ”¯æŒèŒƒå›´çš„åç¦»ã€‚åœ¨è¿™äº›å¹²é¢„æªæ–½ä¸­ï¼Œé€šè¿‡æ•…æ„ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä½œä¸ºä¼¼ç„¶ä¼°è®¡å™¨æ¥è¿›è¡Œæç¤ºæ‰©å±•ï¼Œåœ¨å›¾åƒå¤šæ ·æ€§å’Œç¾è§‚åº¦æ–¹é¢å§‹ç»ˆè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡äº†çœŸå®æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19557v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åˆæˆæ•°æ®çš„æ½œåŠ›ï¼Œå¹¶å…³æ³¨äºå¦‚ä½•é€šè¿‡è°ƒæ•´æç¤ºå¤æ‚æ€§æ¥å½±å“åˆæˆæ•°æ®çš„æ•ˆç”¨ã€‚æ–‡ç« é€šè¿‡åˆæˆå®éªŒå’Œç†è®ºæ¨å¯¼ï¼Œåˆ†æäº†æç¤ºå¤æ‚æ€§å¯¹T2Iæ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®æ•ˆç”¨çš„å½±å“ï¼Œå¹¶å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶æ¥æ¯”è¾ƒçœŸå®æ•°æ®å’Œåˆæˆæ•°æ®çš„æ•ˆç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œå¢åŠ æç¤ºå¤æ‚æ€§ä¼šé™ä½æ¡ä»¶å¤šæ ·æ€§å’Œä¸€è‡´æ€§ï¼Œä½†å¯ä»¥å‡å°‘åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®åˆ†å¸ƒçš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰©å¤§æç¤ºæˆ–ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½œä¸ºä¼¼ç„¶ä¼°è®¡å™¨ç­‰æ–¹æ³•å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜ç”Ÿæˆçš„å›¾åƒçš„å¤šæ ·æ€§å’Œç¾å­¦æ•ˆæœã€‚ç»¼åˆæ¥çœ‹ï¼Œåˆç†åˆ©ç”¨æç¤ºå¤æ‚æ€§å¯¹ä¼˜åŒ–T2Iæ¨¡å‹çš„æ€§èƒ½å…·æœ‰ç§¯ææ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹èƒ½å¤Ÿåˆ›å»ºå‡ ä¹æ— é™åˆæˆæ•°æ®ï¼Œä¸ºæ•°æ®ç§‘å­¦å®¶æä¾›æœ‰ä»·å€¼çš„èµ„æºè¡¥å……æœ‰é™çš„çœŸå®æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¯¹åˆæˆå®éªŒçš„åˆ†æï¼Œæ–‡ç« æ­ç¤ºäº†å…³äºæç¤ºå¤æ‚æ€§å¯¹åˆæˆæ•°æ®æ•ˆç”¨å½±å“çš„è§‚å¯Ÿéš¾åº¦ï¼Œå¹¶é€šè¿‡ç†è®ºæ¨å¯¼è§£é‡Šäº†è¿™ä¸€ç°è±¡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæ¯”è¾ƒçœŸå®æ•°æ®å’Œç”±å¸¸è§T2Iæ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®çš„æ•ˆç”¨ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæé«˜æç¤ºå¤æ‚æ€§ä¼šé™ä½æ¡ä»¶å¤šæ ·æ€§å’Œä¸€è‡´æ€§ï¼Œä½†å¯ä»¥å‡å°‘åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®åˆ†å¸ƒçš„å·®å¼‚ã€‚</li>
<li>å½“å‰æ¨ç†æ—¶é—´å¹²é¢„æªæ–½èƒ½å¤Ÿå¢åŠ ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œä½†å¯èƒ½ä¼šåç¦»çœŸå®æ•°æ®çš„èŒƒå›´ã€‚</li>
<li>é€šè¿‡æ‰©å¤§æç¤ºæˆ–ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½œä¸ºä¼¼ç„¶ä¼°è®¡å™¨ç­‰æ–¹æ³•å¯ä»¥æœ‰æ•ˆæé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œç¾å­¦æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d52ed394dfa333240be417884a7ae651" align="middle">
<img src="https://picx.zhimg.com/v2-5d2f095382a0d750350d63475b7f8969" align="middle">
<img src="https://picx.zhimg.com/v2-e2641f51ce531a93342bd06ddd7d211c" align="middle">
<img src="https://picx.zhimg.com/v2-2a2e72deab7f8f5b550b38874a36a532" align="middle">
<img src="https://picx.zhimg.com/v2-11905d9ac87e6090878045f987146e18" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SCEESR-Semantic-Control-Edge-Enhancement-for-Diffusion-Based-Super-Resolution"><a href="#SCEESR-Semantic-Control-Edge-Enhancement-for-Diffusion-Based-Super-Resolution" class="headerlink" title="SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based   Super-Resolution"></a>SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based   Super-Resolution</h2><p><strong>Authors:Yun Kai Zhuang</strong></p>
<p>Real-world image super-resolution (Real-ISR) must handle complex degradations and inherent reconstruction ambiguities. While generative models have improved perceptual quality, a key trade-off remains with computational cost. One-step diffusion models offer speed but often produce structural inaccuracies due to distillation artifacts. To address this, we propose a novel SR framework that enhances a one-step diffusion model using a ControlNet mechanism for semantic edge guidance. This integrates edge information to provide dynamic structural control during single-pass inference. We also introduce a hybrid loss combining L2, LPIPS, and an edge-aware AME loss to optimize for pixel accuracy, perceptual quality, and geometric precision. Experiments show our method effectively improves structural integrity and realism while maintaining the efficiency of one-step generation, achieving a superior balance between output quality and inference speed. The results of test datasets will be published at <a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1amddXQ5orIyjbxHgGpzqFHZ6KTolinJF?usp=drive_link">https://drive.google.com/drive/folders/1amddXQ5orIyjbxHgGpzqFHZ6KTolinJF?usp=drive_link</a> and the related code will be published at <a target="_blank" rel="noopener" href="https://github.com/ARBEZ-ZEBRA/SCEESR">https://github.com/ARBEZ-ZEBRA/SCEESR</a>. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰éœ€è¦å¤„ç†å¤æ‚çš„é€€åŒ–å’Œå›ºæœ‰çš„é‡å»ºæ­§ä¹‰ã€‚è™½ç„¶ç”Ÿæˆæ¨¡å‹å·²ç»æé«˜äº†æ„ŸçŸ¥è´¨é‡ï¼Œä½†è®¡ç®—æˆæœ¬ä¹‹é—´çš„æƒè¡¡ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ä¸€æ­¥æ‰©æ•£æ¨¡å‹è™½ç„¶é€Ÿåº¦å¿«ï¼Œä½†ç”±äºè’¸é¦äº§ç”Ÿçš„ä¼ªå½±é€šå¸¸ä¼šå¯¼è‡´ç»“æ„ä¸å‡†ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„SRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ControlNetæœºåˆ¶å¢å¼ºäº†ä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè¯­ä¹‰è¾¹ç¼˜å¼•å¯¼ã€‚è¿™é€šè¿‡é›†æˆè¾¹ç¼˜ä¿¡æ¯åœ¨å•æ¬¡ä¼ é€’æ¨ç†è¿‡ç¨‹ä¸­æä¾›åŠ¨æ€ç»“æ„æ§åˆ¶ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ··åˆæŸå¤±ï¼Œç»“åˆäº†L2ã€LPIPSå’Œè¾¹ç¼˜æ„ŸçŸ¥AMEæŸå¤±ï¼Œä»¥ä¼˜åŒ–åƒç´ ç²¾åº¦ã€æ„ŸçŸ¥è´¨é‡å’Œå‡ ä½•ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒä¸€æ­¥ç”Ÿæˆçš„æ•ˆç‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°æé«˜äº†ç»“æ„çš„å®Œæ•´æ€§å’Œé€¼çœŸåº¦ï¼Œåœ¨è¾“å‡ºè´¨é‡å’Œæ¨ç†é€Ÿåº¦ä¹‹é—´è¾¾åˆ°äº†ä¼˜è¶Šçš„å¹³è¡¡ã€‚æµ‹è¯•æ•°æ®é›†çš„ç»“æœå°†å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1amddXQ5orIyjbxHgGpzqFHZ6KTolinJF?usp=drive_link%EF%BC%8C%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E5%B0%86%E5%8F%91%E5%B8%83%E5%9C%A8https://github.com/ARBEZ-ZEBRA/SCEESR%E3%80%82">https://drive.google.com/drive/folders/1amddXQ5orIyjbxHgGpzqFHZ6KTolinJF?usp=drive_linkï¼Œç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨https://github.com/ARBEZ-ZEBRA/SCEESRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19272v1">PDF</a> 10 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºControlNetæœºåˆ¶çš„æ–°å‹è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å¢å¼ºä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è¯­ä¹‰è¾¹ç¼˜å¼•å¯¼æ¥è§£å†³ç»“æ„å¤±çœŸé—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¾¹ç¼˜ä¿¡æ¯ï¼Œåœ¨å•æ¬¡ä¼ é€’æ¨ç†è¿‡ç¨‹ä¸­æä¾›åŠ¨æ€ç»“æ„æ§åˆ¶ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§ç»“åˆL2ã€LPIPSå’Œè¾¹ç¼˜æ„ŸçŸ¥AMEæŸå¤±çš„æ··åˆæŸå¤±ï¼Œä»¥ä¼˜åŒ–åƒç´ ç²¾åº¦ã€æ„ŸçŸ¥è´¨é‡å’Œå‡ ä½•ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒä¸€æ­¥ç”Ÿæˆæ•ˆç‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆæé«˜äº†ç»“æ„å®Œæ•´æ€§å’Œé€¼çœŸåº¦ï¼Œå®ç°äº†è¾“å‡ºè´¨é‡å’Œæ¨ç†é€Ÿåº¦ä¹‹é—´çš„å“è¶Šå¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„SRæ¡†æ¶åŸºäºControlNetæœºåˆ¶å¢å¼ºäº†ä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³ç»“æ„å¤±çœŸé—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç»“åˆè¾¹ç¼˜ä¿¡æ¯ï¼Œåœ¨å•æ¬¡ä¼ é€’æ¨ç†è¿‡ç¨‹ä¸­æä¾›åŠ¨æ€ç»“æ„æ§åˆ¶ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ··åˆæŸå¤±ï¼ŒåŒ…æ‹¬L2ã€LPIPSå’Œè¾¹ç¼˜æ„ŸçŸ¥AMEæŸå¤±ï¼Œä»¥ä¼˜åŒ–å›¾åƒè´¨é‡çš„ä¸åŒæ–¹é¢ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜ç»“æ„å®Œæ•´æ€§å’Œé€¼çœŸåº¦çš„åŒæ—¶ï¼Œä¿æŒäº†é«˜æ•ˆçš„ä¸€æ­¥ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†è¾“å‡ºè´¨é‡å’Œæ¨ç†é€Ÿåº¦ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>æµ‹è¯•æ•°æ®é›†çš„ç»“æœå°†åœ¨æŒ‡å®šçš„Google Driveé“¾æ¥ä¸Šå‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e252e5d6c8d9d9444b0146023d3a6aa" align="middle">
<img src="https://picx.zhimg.com/v2-7b985ed1f75102b02d2688656dffa3da" align="middle">
<img src="https://picx.zhimg.com/v2-8cb0a5365c92795778baf5c216e76d0c" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DP-2-O-SR-Direct-Perceptual-Preference-Optimization-for-Real-World-Image-Super-Resolution"><a href="#DP-2-O-SR-Direct-Perceptual-Preference-Optimization-for-Real-World-Image-Super-Resolution" class="headerlink" title="DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World   Image Super-Resolution"></a>DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World   Image Super-Resolution</h2><p><strong>Authors:Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang</strong></p>
<p>Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks. </p>
<blockquote>
<p>å—ç›Šäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œç°å®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰æ–¹æ³•å¯ä»¥åˆæˆä¸°å¯Œä¸”é€¼çœŸçš„ç»†èŠ‚ã€‚ç„¶è€Œï¼Œç”±äºT2Iæ¨¡å‹çš„å›ºæœ‰éšæœºæ€§ï¼Œä¸åŒçš„å™ªå£°è¾“å…¥é€šå¸¸ä¼šå¯¼è‡´è¾“å‡ºæ„ŸçŸ¥è´¨é‡æœ‰æ‰€ä¸åŒã€‚è™½ç„¶è¿™ç§éšæœºæ€§æœ‰æ—¶è¢«è§†ä¸ºé™åˆ¶ï¼Œä½†å®ƒä¹Ÿå¼•å…¥äº†æ›´å¹¿æ³›çš„æ„ŸçŸ¥è´¨é‡èŒƒå›´ï¼Œå¯ä»¥è¢«ç”¨æ¥æé«˜Real-ISRæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å‘Real-ISRçš„ç›´æ¥æ„ŸçŸ¥åå¥½ä¼˜åŒ–ï¼ˆDP$^2$O-SRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— éœ€æ˜‚è´µçš„äººåŠ›æ ‡æ³¨ï¼Œå³å¯ä½¿ç”Ÿæˆæ¨¡å‹ä¸æ„ŸçŸ¥åå¥½å¯¹é½ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆå…¨å‚è€ƒå’Œæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ¨¡å‹ï¼Œæ„å»ºäº†ä¸€ä¸ªæ··åˆå¥–åŠ±ä¿¡å·ï¼Œè¿™äº›æ¨¡å‹æ˜¯åœ¨å¤§è§„æ¨¡äººç±»åå¥½æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€‚è¯¥å¥–åŠ±é¼“åŠ±ç»“æ„ä¿çœŸå’Œè‡ªç„¶å¤–è§‚ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨æ„ŸçŸ¥å¤šæ ·æ€§ï¼Œæˆ‘ä»¬è¶…è¶Šäº†æ ‡å‡†çš„æœ€ä½³ä¸æœ€å·®é€‰æ‹©ï¼Œä»åŒä¸€æ¨¡å‹çš„è¾“å‡ºä¸­æ„å»ºå¤šä¸ªåå¥½å¯¹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæœ€ä½³é€‰æ‹©æ¯”ä¾‹å–å†³äºæ¨¡å‹å®¹é‡ï¼šå°å‹æ¨¡å‹ä»æ›´å¹¿æ³›çš„è¦†ç›–ä¸­å—ç›Šï¼Œè€Œå¤§å‹æ¨¡å‹å¯¹æ›´å¼ºçš„ç›‘ç£å¯¹æ¯”æœ‰æ›´å¥½çš„ååº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚åå¥½ä¼˜åŒ–ï¼Œå®ƒæ ¹æ®ç»„å†…å¥–åŠ±å·®è·å’Œç»„é—´å¤šæ ·æ€§è‡ªé€‚åº”åœ°åŠ æƒè®­ç»ƒå¯¹ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆå’Œç¨³å®šçš„å­¦ä¹ ã€‚åœ¨åŸºäºæ‰©æ•£å’Œæµä½“çš„T2Iä¸»å¹²ç½‘ç»œä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDP$^2$O-SRæ˜¾è‘—æé«˜äº†æ„ŸçŸ¥è´¨é‡ï¼Œå¹¶å¾ˆå¥½åœ°æ¨å¹¿åˆ°äº†ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18851v1">PDF</a> Accept by NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å—ç›Šäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œç°å®å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰æ–¹æ³•å¯ä»¥åˆæˆä¸°å¯Œä¸”é€¼çœŸçš„ç»†èŠ‚ã€‚ç„¶è€Œï¼Œç”±äºT2Iæ¨¡å‹å›ºæœ‰çš„éšæœºæ€§ï¼Œä¸åŒçš„å™ªå£°è¾“å…¥é€šå¸¸ä¼šå¯¼è‡´è¾“å‡ºå“çš„æ„ŸçŸ¥è´¨é‡æœ‰æ‰€ä¸åŒã€‚è™½ç„¶è¿™ç§éšæœºæ€§æœ‰æ—¶è¢«è§†ä¸ºé™åˆ¶ï¼Œä½†å®ƒä¹Ÿå¼•å…¥äº†æ›´å¹¿æ³›çš„æ„ŸçŸ¥è´¨é‡èŒƒå›´ï¼Œå¯ä»¥è¢«ç”¨æ¥æé«˜Real-ISRæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å‘Real-ISRçš„ç›´æ¥æ„ŸçŸ¥åå¥½ä¼˜åŒ–ï¼ˆDP$^2$O-SRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä½¿ç”Ÿæˆæ¨¡å‹ä¸æ„ŸçŸ¥åå¥½å¯¹é½ï¼Œè€Œæ— éœ€æ˜‚è´µçš„äººåŠ›æ³¨é‡Šã€‚æˆ‘ä»¬ç»“åˆå…¨å‚è€ƒå’Œæ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ¨¡å‹ï¼Œæ„å»ºäº†ä¸€ä¸ªæ··åˆå¥–åŠ±ä¿¡å·ï¼Œè¿™äº›æ¨¡å‹æ˜¯åœ¨å¤§è§„æ¨¡äººç±»åå¥½æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€‚è¯¥å¥–åŠ±é¼“åŠ±ç»“æ„ä¿çœŸå’Œè‡ªç„¶å¤–è§‚ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨æ„ŸçŸ¥å¤šæ ·æ€§ï¼Œæˆ‘ä»¬ä»åŒä¸€æ¨¡å‹çš„è¾“å‡ºä¸­æ„å»ºå¤šä¸ªåå¥½å¯¹ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€å¥½çš„ä¸æœ€å·®çš„é€‰é¡¹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæœ€ä½³é€‰æ‹©æ¯”ä¾‹å–å†³äºæ¨¡å‹å®¹é‡ï¼šå°å‹æ¨¡å‹ä»æ›´å¹¿æ³›çš„è¦†ç›–ä¸­å—ç›Šï¼Œè€Œå¤§å‹æ¨¡å‹å¯¹æ›´å¼ºçš„ç›‘ç£å¯¹æ¯”æœ‰æ›´å¥½çš„ååº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚åå¥½ä¼˜åŒ–ï¼Œå®ƒæ ¹æ®ç»„å†…å¥–åŠ±å·®è·å’Œç»„é—´å¤šæ ·æ€§è‡ªé€‚åº”åœ°åŠ æƒè®­ç»ƒå¯¹ï¼Œä½¿å­¦ä¹ æ›´åŠ é«˜æ•ˆå’Œç¨³å®šã€‚è·¨æ‰©æ•£å’ŒæµåŸºT2Iä¸»å¹²çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDP$^2$O-SRæ˜¾è‘—æé«˜äº†æ„ŸçŸ¥è´¨é‡ï¼Œå¹¶å¾ˆå¥½åœ°æ¨å¹¿åˆ°äº†ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å—ç›Šäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œç°å®å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰æ–¹æ³•å¯ä»¥åˆæˆé«˜è´¨é‡ç»†èŠ‚ã€‚</li>
<li>T2Iæ¨¡å‹çš„éšæœºæ€§å¯¼è‡´è¾“å‡ºæ„ŸçŸ¥è´¨é‡å­˜åœ¨å·®å¼‚ï¼Œè¿™æ—¢æ˜¯ä¸€ç§é™åˆ¶ä¹Ÿæ˜¯ä¸€ç§å¯ç”¨æ¥æé«˜Real-ISRæ€§èƒ½çš„èµ„æºã€‚</li>
<li>å¼•å…¥Direct Perceptual Preference Optimization for Real-ISRï¼ˆDP$^2$O-SRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸æ„ŸçŸ¥åå¥½å¯¹é½ï¼Œæ— éœ€æ˜‚è´µçš„äººåŠ›æ³¨é‡Šã€‚</li>
<li>é€šè¿‡ç»“åˆå…¨å‚è€ƒå’Œæ— å‚è€ƒIQAæ¨¡å‹ï¼Œæ„å»ºæ··åˆå¥–åŠ±ä¿¡å·ï¼Œé¼“åŠ±ç»“æ„ä¿çœŸå’Œè‡ªç„¶å¤–è§‚ã€‚</li>
<li>åˆ©ç”¨å¤šä¸ªåå¥½å¯¹æ¥æé«˜æ„ŸçŸ¥å¤šæ ·æ€§çš„åˆ©ç”¨ï¼Œæœ€ä½³é€‰æ‹©æ¯”ä¾‹å–å†³äºæ¨¡å‹å®¹é‡ã€‚</li>
<li>æå‡ºåˆ†å±‚åå¥½ä¼˜åŒ–ï¼Œä½¿å­¦ä¹ æ›´åŠ é«˜æ•ˆå’Œç¨³å®šã€‚</li>
<li>DP$^2$O-SRæ˜¾è‘—æé«˜äº†æ„ŸçŸ¥è´¨é‡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.18851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-594495eb66871a323e5e41607e3a75d2" align="middle">
<img src="https://picx.zhimg.com/v2-a85d37ce630fe5cf4be0067e0af18fef" align="middle">
<img src="https://picx.zhimg.com/v2-7cd2d90594112175b7bc85e4098da678" align="middle">
<img src="https://picx.zhimg.com/v2-ce3c1b282bc864a571b5c337c61fe4d6" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Identity-Preserving-Image-to-Video-Generation-via-Reward-Guided-Optimization"><a href="#Identity-Preserving-Image-to-Video-Generation-via-Reward-Guided-Optimization" class="headerlink" title="Identity-Preserving Image-to-Video Generation via Reward-Guided   Optimization"></a>Identity-Preserving Image-to-Video Generation via Reward-Guided   Optimization</h2><p><strong>Authors:Liao Shen, Wentao Jiang, Yiran Zhu, Jiahe Li, Tiezheng Ge, Zhiguo Cao, Bo Zheng</strong></p>
<p>Recent advances in image-to-video (I2V) generation have achieved remarkable progress in synthesizing high-quality, temporally coherent videos from static images. Among all the applications of I2V, human-centric video generation includes a large portion. However, existing I2V models encounter difficulties in maintaining identity consistency between the input human image and the generated video, especially when the person in the video exhibits significant expression changes and movements. This issue becomes critical when the human face occupies merely a small fraction of the image. Since humans are highly sensitive to identity variations, this poses a critical yet under-explored challenge in I2V generation. In this paper, we propose Identity-Preserving Reward-guided Optimization (IPRO), a novel video diffusion framework based on reinforcement learning to enhance identity preservation. Instead of introducing auxiliary modules or altering model architectures, our approach introduces a direct and effective tuning algorithm that optimizes diffusion models using a face identity scorer. To improve performance and accelerate convergence, our method backpropagates the reward signal through the last steps of the sampling chain, enabling richer gradient feedback. We also propose a novel facial scoring mechanism that treats faces in ground-truth videos as facial feature pools, providing multi-angle facial information to enhance generalization. A KL-divergence regularization is further incorporated to stabilize training and prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V model and our in-house I2V model demonstrate the effectiveness of our method. Our project and code are available at <a target="_blank" rel="noopener" href="https://ipro-alimama.github.io/">https://ipro-alimama.github.io/</a>. </p>
<blockquote>
<p>åœ¨å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ç”Ÿæˆæ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œå·²ç»åœ¨ä»é™æ€å›¾åƒåˆæˆé«˜è´¨é‡ã€æ—¶é—´è¿è´¯çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚åœ¨æ‰€æœ‰I2Våº”ç”¨ä¸­ï¼Œä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆå æ®å¾ˆå¤§ä¸€éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„I2Væ¨¡å‹åœ¨ä¿æŒè¾“å…¥äººåƒä¸ç”Ÿæˆè§†é¢‘ä¹‹é—´èº«ä»½ä¸€è‡´æ€§æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘ä¸­äººç‰©è¡¨æƒ…å˜åŒ–å’ŒåŠ¨ä½œæ˜¾è‘—æ—¶ã€‚å½“äººè„¸åªå å›¾åƒä¸€å°éƒ¨åˆ†æ—¶ï¼Œè¿™ä¸ªé—®é¢˜å˜å¾—æ›´ä¸ºå…³é”®ã€‚ç”±äºäººç±»å¯¹èº«ä»½å˜åŒ–é«˜åº¦æ•æ„Ÿï¼Œè¿™ä¸ºI2Vç”Ÿæˆæå‡ºäº†ä¸€ä¸ªè‡³å…³é‡è¦ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„èº«ä»½ä¿ç•™å¥–åŠ±å¼•å¯¼ä¼˜åŒ–ï¼ˆIPROï¼‰æ–°å‹è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œä»¥æé«˜èº«ä»½ä¿ç•™èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ˜¯å¼•å…¥è¾…åŠ©æ¨¡å—æˆ–æ”¹å˜æ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å¼•å…¥äº†ä¸€ç§ç›´æ¥æœ‰æ•ˆçš„è°ƒæ•´ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨é¢éƒ¨èº«ä»½è¯„åˆ†è€…å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚ä¸ºäº†æé«˜æ€§èƒ½å’ŒåŠ é€Ÿæ”¶æ•›ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é‡‡æ ·é“¾çš„æœ€åå‡ æ­¥åå‘ä¼ æ’­å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå®ç°æ›´ä¸°å¯Œçš„æ¢¯åº¦åé¦ˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„é¢éƒ¨è¯„åˆ†æœºåˆ¶ï¼Œå°†çœŸå®è§†é¢‘ä¸­çš„é¢éƒ¨è§†ä¸ºé¢éƒ¨ç‰¹å¾æ± ï¼Œæä¾›å¤šè§’åº¦çš„é¢éƒ¨ä¿¡æ¯ä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚è¿˜è¿›ä¸€æ­¥èå…¥äº†KLæ•£åº¦æ­£åˆ™åŒ–ï¼Œä»¥ç¨³å®šè®­ç»ƒå¹¶é˜²æ­¢å¯¹å¥–åŠ±ä¿¡å·çš„è¿‡åº¦æ‹Ÿåˆã€‚åœ¨Wan 2.2 I2Væ¨¡å‹å’Œæˆ‘ä»¬çš„å†…éƒ¨I2Væ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://ipro-alimama.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://ipro-alimama.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14255v3">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡é’ˆå¯¹å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­çš„äººä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„èº«ä»½ä¿ç•™å¥–åŠ±å¼•å¯¼ä¼˜åŒ–ï¼ˆIPROï¼‰æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨äººè„¸è¡¨æƒ…å˜åŒ–å’Œç§»åŠ¨æ—¶èº«ä»½ä¸€è‡´æ€§ç»´æŠ¤çš„å›°éš¾ï¼ŒIPROé€šè¿‡ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨é¢éƒ¨èº«ä»½è¯„åˆ†è€…è¿›è¡Œç›´æ¥æœ‰æ•ˆçš„è°ƒæ•´ç®—æ³•ã€‚è¯¥æ¡†æ¶è¿˜å¼•å…¥äº†é¢éƒ¨è¯„åˆ†æœºåˆ¶å’ŒKL-æ•£åº¦æ­£åˆ™åŒ–ï¼Œä»¥æé«˜æ€§èƒ½å’ŒåŠ é€Ÿæ”¶æ•›ï¼ŒåŒæ—¶ç¨³å®šè®­ç»ƒå¹¶é˜²æ­¢è¿‡åº¦æ‹Ÿåˆå¥–åŠ±ä¿¡å·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆï¼ˆI2Vï¼‰é¢†åŸŸé¢ä¸´èº«ä»½ä¸€è‡´æ€§ç»´æŠ¤çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨äººè„¸è¡¨æƒ…å˜åŒ–å’Œç§»åŠ¨æ—¶ã€‚</li>
<li>IPROæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œæä¾›äº†ä¸€ç§èº«ä»½ä¿ç•™å¥–åŠ±å¼•å¯¼çš„ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>IPROé‡‡ç”¨ç›´æ¥æœ‰æ•ˆçš„è°ƒæ•´ç®—æ³•ï¼Œä½¿ç”¨é¢éƒ¨èº«ä»½è¯„åˆ†è€…å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å¼•å…¥é¢éƒ¨è¯„åˆ†æœºåˆ¶ï¼Œåˆ©ç”¨åœ°é¢çœŸå®è§†é¢‘ä¸­çš„é¢éƒ¨ç‰¹å¾æ± æä¾›å¤šè§’åº¦é¢éƒ¨ä¿¡æ¯ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>KL-æ•£åº¦æ­£åˆ™åŒ–è¢«çº³å…¥ä»¥ç¨³å®šè®­ç»ƒå¹¶é˜²æ­¢è¿‡åº¦æ‹Ÿåˆå¥–åŠ±ä¿¡å·ã€‚</li>
<li>åœ¨Wan 2.2 I2Væ¨¡å‹å’Œå†…éƒ¨I2Væ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†IPROæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45a2212784c690cd31e61d3bfbb28304" align="middle">
<img src="https://picx.zhimg.com/v2-42c635d18009b5610a0633f7c86a70e5" align="middle">
<img src="https://picx.zhimg.com/v2-01c98ea8db09ccdc59a53bf97b3f8b54" align="middle">
<img src="https://picx.zhimg.com/v2-cdaa33f520f7d4971cdfb6c6b67192c4" align="middle">
<img src="https://picx.zhimg.com/v2-0741636324814ce13d38f5d7343259e2" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RODS-Robust-Optimization-Inspired-Diffusion-Sampling-for-Detecting-and-Reducing-Hallucination-in-Generative-Models"><a href="#RODS-Robust-Optimization-Inspired-Diffusion-Sampling-for-Detecting-and-Reducing-Hallucination-in-Generative-Models" class="headerlink" title="RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and   Reducing Hallucination in Generative Models"></a>RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and   Reducing Hallucination in Generative Models</h2><p><strong>Authors:Yiqi Tian, Pengfei Jin, Mingze Yuan, Na Li, Bo Zeng, Quanzheng Li</strong></p>
<p>Diffusion models have achieved state-of-the-art performance in generative modeling, yet their sampling procedures remain vulnerable to hallucinations-often stemming from inaccuracies in score approximation. In this work, we reinterpret diffusion sampling through the lens of optimization and introduce RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that detects and corrects high-risk sampling steps using geometric cues from the loss landscape. RODS enforces smoother sampling trajectories and adaptively adjusts perturbations, reducing hallucinations without retraining and at minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands demonstrate that RODS maintains comparable image quality and preserves generation diversity. More importantly, it improves both sampling fidelity and robustness, detecting over 70% of hallucinated samples and correcting more than 25%, all while avoiding the introduction of new artifacts. We release our code at <a target="_blank" rel="noopener" href="https://github.com/Yiqi-Verna-Tian/RODS">https://github.com/Yiqi-Verna-Tian/RODS</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å…¶é‡‡æ ·ç¨‹åºä»ç„¶å®¹æ˜“å—å¹»è§‰å½±å“ï¼Œè¿™é€šå¸¸æºäºè¯„åˆ†ä¼°è®¡ä¸å‡†ç¡®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»ä¼˜åŒ–çš„è§’åº¦é‡æ–°è§£é‡Šäº†æ‰©æ•£é‡‡æ ·ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹æ–¹æ³•RODSï¼ˆå—ç¨³å¥ä¼˜åŒ–å¯å‘çš„æ‰©æ•£é‡‡æ ·å™¨ï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æŸå¤±æ™¯è§‚çš„å‡ ä½•çº¿ç´¢æ¥æ£€æµ‹å’Œçº æ­£é«˜é£é™©çš„é‡‡æ ·æ­¥éª¤ã€‚RODSå¼ºåˆ¶å¹³æ»‘é‡‡æ ·è½¨è¿¹å¹¶è‡ªé€‚åº”è°ƒæ•´æ‰°åŠ¨ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯å‡å°‘å¹»è§‰ï¼Œå¹¶ä¸”åªéœ€æå°‘çš„é¢å¤–æ¨ç†æˆæœ¬ã€‚åœ¨AFHQv2ã€FFHQå’Œ11k-handsä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRODSä¿æŒäº†ç›¸å½“çš„å›¾ç‰‡è´¨é‡å¹¶ä¿æŒäº†ç”Ÿæˆå¤šæ ·æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒæé«˜äº†é‡‡æ ·ä¿çœŸåº¦å’Œç¨³å¥æ€§ï¼Œæ£€æµ‹åˆ°è¶…è¿‡70%çš„å¹»è§‰æ ·æœ¬å¹¶çº æ­£äº†è¶…è¿‡25%ï¼ŒåŒæ—¶é¿å…äº†æ–°ä¼ªå½±çš„äº§ç”Ÿã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yiqi-Verna-Tian/RODS">https://github.com/Yiqi-Verna-Tian/RODS</a>å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12201v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡ä¸­çš„æœ€æ–°è¿›å±•ã€‚å°½ç®¡å·²è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½ï¼Œä½†å…¶é‡‡æ ·è¿‡ç¨‹ä»å®¹æ˜“å—åˆ°hallucinationçš„å½±å“ï¼Œä¸»è¦æºäºè¯„åˆ†ä¼°ç®—çš„ä¸å‡†ç¡®ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹é‡‡æ ·æ–¹æ³•â€”â€”RODSï¼ˆåŸºäºç¨³å¥ä¼˜åŒ–çš„æ‰©æ•£é‡‡æ ·å™¨ï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡æŸå¤±æ™¯è§‚çš„å‡ ä½•çº¿ç´¢æ¥æ£€æµ‹å’Œçº æ­£é«˜é£é™©çš„é‡‡æ ·æ­¥éª¤ã€‚RODSå¯å®ç°æ›´å¹³æ»‘çš„é‡‡æ ·è½¨è¿¹ï¼Œè‡ªé€‚åº”è°ƒæ•´æ‰°åŠ¨ï¼Œåœ¨ä¸è¿›è¡Œå†è®­ç»ƒçš„æƒ…å†µä¸‹å‡å°‘äº†hallucinationï¼Œå¹¶ä¸”å‡ ä¹æ²¡æœ‰å¢åŠ é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚åœ¨AFHQv2ã€FFHQå’Œ11k-handsä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRODSä¿æŒäº†ç›¸å½“çš„å›¾ç‰‡è´¨é‡å¹¶ä¿æŒäº†ç”Ÿæˆå¤šæ ·æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒæé«˜äº†é‡‡æ ·ä¿çœŸåº¦å’Œç¨³å¥æ€§ï¼Œèƒ½å¤Ÿæ£€æµ‹å‡ºè¶…è¿‡70%çš„hallucinatedæ ·æœ¬å¹¶çº æ­£è¶…è¿‡25%ï¼ŒåŒæ—¶é¿å…äº†æ–°äº§ç”Ÿçš„ä¼ªå½±ã€‚æˆ‘ä»¬å·²å°†ä»£ç å‘å¸ƒåœ¨[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡é¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>é‡‡æ ·è¿‡ç¨‹ä¸­å®¹æ˜“å‡ºç°hallucinationé—®é¢˜ï¼Œä¸»è¦ç”±äºè¯„åˆ†ä¼°ç®—çš„ä¸å‡†ç¡®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹é‡‡æ ·æ–¹æ³•â€”â€”RODSï¼ˆåŸºäºç¨³å¥ä¼˜åŒ–çš„æ‰©æ•£é‡‡æ ·å™¨ï¼‰ã€‚</li>
<li>RODSåˆ©ç”¨æŸå¤±æ™¯è§‚çš„å‡ ä½•çº¿ç´¢æ¥æ£€æµ‹å’Œçº æ­£é«˜é£é™©é‡‡æ ·æ­¥éª¤ã€‚</li>
<li>RODSèƒ½è‡ªé€‚åº”è°ƒæ•´æ‰°åŠ¨å¹¶è¾¾åˆ°æ›´å¹³æ»‘çš„é‡‡æ ·è½¨è¿¹ã€‚</li>
<li>RODSåœ¨ä¸è¿›è¡Œå†è®­ç»ƒçš„æƒ…å†µä¸‹å‡å°‘äº†hallucinationï¼Œä¸”å‡ ä¹æ²¡æœ‰å¢åŠ é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12201">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e6a60f09188ee8a1942f6f4adf7340a" align="middle">
<img src="https://picx.zhimg.com/v2-055a38ada6a0f1e8a65015155fe2a29f" align="middle">
<img src="https://picx.zhimg.com/v2-85e3072e84a1c058611e52a1ebdc456b" align="middle">
<img src="https://picx.zhimg.com/v2-578b3c67cb658a4e5e53e039a024ebb3" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Latent-Diffusion-Models-with-Masked-AutoEncoders"><a href="#Latent-Diffusion-Models-with-Masked-AutoEncoders" class="headerlink" title="Latent Diffusion Models with Masked AutoEncoders"></a>Latent Diffusion Models with Masked AutoEncoders</h2><p><strong>Authors:Junho Lee, Jeongwoo Shin, Hyungwook Choi, Joonseok Lee</strong></p>
<p>In spite of the remarkable potential of Latent Diffusion Models (LDMs) in image generation, the desired properties and optimal design of the autoencoders have been underexplored. In this work, we analyze the role of autoencoders in LDMs and identify three key properties: latent smoothness, perceptual compression quality, and reconstruction quality. We demonstrate that existing autoencoders fail to simultaneously satisfy all three properties, and propose Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical features maintained by Masked AutoEncoders. We integrate VMAEs into the LDM framework, introducing Latent Diffusion Models with Masked AutoEncoders (LDMAEs). Our code is available at <a target="_blank" rel="noopener" href="https://github.com/isno0907/ldmae">https://github.com/isno0907/ldmae</a>. </p>
<blockquote>
<p>å°½ç®¡æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼Œç®€ç§°LDMï¼‰åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†è‡ªåŠ¨ç¼–ç å™¨çš„ç†æƒ³å±æ€§å’Œä¼˜åŒ–è®¾è®¡å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†è‡ªåŠ¨ç¼–ç å™¨åœ¨LDMä¸­çš„ä½œç”¨ï¼Œå¹¶ç¡®å®šäº†ä¸‰ä¸ªå…³é”®å±æ€§ï¼šæ½œåœ¨å¹³æ»‘æ€§ã€æ„ŸçŸ¥å‹ç¼©è´¨é‡å’Œé‡å»ºè´¨é‡ã€‚æˆ‘ä»¬è¯æ˜ç°æœ‰è‡ªåŠ¨ç¼–ç å™¨æ— æ³•æ»¡è¶³è¿™ä¸‰ä¸ªå±æ€§çš„åŒæ—¶ï¼Œå¹¶æå‡ºåˆ©ç”¨æ©ç è‡ªåŠ¨ç¼–ç å™¨æ‰€ä¿ç•™çš„åˆ†å±‚ç‰¹å¾è¿›è¡Œå˜åˆ†æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVariational Masked AutoEncodersï¼Œç®€ç§°VMAEsï¼‰ã€‚æˆ‘ä»¬å°†VMAEsé›†æˆåˆ°LDMæ¡†æ¶ä¸­ï¼Œå¼•å…¥å¸¦æœ‰æ©ç è‡ªåŠ¨ç¼–ç å™¨çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Models with Masked AutoEncodersï¼Œç®€ç§°LDMAEsï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/isno0907/ldmae%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/isno0907/ldmaeæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09984v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†è‡ªåŠ¨ç¼–ç å™¨çš„ç†æƒ³å±æ€§å’Œè®¾è®¡å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡åˆ†æäº†è‡ªåŠ¨ç¼–ç å™¨åœ¨LDMä¸­çš„ä½œç”¨ï¼Œå¹¶ç¡®å®šäº†ä¸‰ä¸ªå…³é”®å±æ€§ï¼šæ½œåœ¨å¹³æ»‘æ€§ã€æ„ŸçŸ¥å‹ç¼©è´¨é‡å’Œé‡å»ºè´¨é‡ã€‚æˆ‘ä»¬è¯æ˜ç°æœ‰è‡ªåŠ¨ç¼–ç å™¨æ— æ³•æ»¡è¶³æ‰€æœ‰è¿™ä¸‰ä¸ªå±æ€§ï¼Œå¹¶æå‡ºé‡‡ç”¨æ©è†œè‡ªåŠ¨ç¼–ç å™¨ï¼ˆMasked AutoEncodersï¼‰ä¼˜åŠ¿çš„å˜åˆ†æ©è†œè‡ªåŠ¨ç¼–ç å™¨ï¼ˆVMAEsï¼‰ã€‚å°†å…¶é›†æˆåˆ°LDMæ¡†æ¶ä¸­ï¼Œå¼•å…¥å¸¦æœ‰æ©è†œè‡ªåŠ¨ç¼–ç å™¨çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMAEsï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/isno0907/ldmae%E3%80%82">https://github.com/isno0907/ldmaeã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>è‡ªåŠ¨ç¼–ç å™¨åœ¨LDMä¸­çš„ä½œç”¨å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>ç¡®å®šè‡ªåŠ¨ç¼–ç å™¨çš„ä¸‰ä¸ªå…³é”®å±æ€§ï¼šæ½œåœ¨å¹³æ»‘æ€§ã€æ„ŸçŸ¥å‹ç¼©è´¨é‡å’Œé‡å»ºè´¨é‡ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨ç¼–ç å™¨æ— æ³•æ»¡è¶³æ‰€æœ‰è¿™ä¸‰ä¸ªå±æ€§ã€‚</li>
<li>æå‡ºå˜åˆ†æ©è†œè‡ªåŠ¨ç¼–ç å™¨ï¼ˆVMAEsï¼‰ï¼Œåˆ©ç”¨æ©è†œè‡ªåŠ¨ç¼–ç å™¨çš„å±‚æ¬¡ç‰¹å¾ã€‚</li>
<li>å°†VMAEsé›†æˆåˆ°LDMæ¡†æ¶ä¸­ï¼Œå½¢æˆå¸¦æœ‰æ©è†œè‡ªåŠ¨ç¼–ç å™¨çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMAEsï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fcce9d64d8b7547eec33f1626791e4a6" align="middle">
<img src="https://picx.zhimg.com/v2-7a6127569d506c926869ca425e3326a8" align="middle">
<img src="https://picx.zhimg.com/v2-b019116268ba163ca6badb3c125594c8" align="middle">
<img src="https://picx.zhimg.com/v2-3bc400b200f0f71a9c2e09968be3e4eb" align="middle">
<img src="https://picx.zhimg.com/v2-d237bde9c9c7aed254a9c1d9bb9f6670" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Doctor-Approved-Generating-Medically-Accurate-Skin-Disease-Images-through-AI-Expert-Feedback"><a href="#Doctor-Approved-Generating-Medically-Accurate-Skin-Disease-Images-through-AI-Expert-Feedback" class="headerlink" title="Doctor Approved: Generating Medically Accurate Skin Disease Images   through AI-Expert Feedback"></a>Doctor Approved: Generating Medically Accurate Skin Disease Images   through AI-Expert Feedback</h2><p><strong>Authors:Janet Wang, Yunbei Zhang, Zhengming Ding, Jihun Hamm</strong></p>
<p>Paucity of medical data severely limits the generalizability of diagnostic ML models, as the full spectrum of disease variability can not be represented by a small clinical dataset. To address this, diffusion models (DMs) have been considered as a promising avenue for synthetic image generation and augmentation. However, they frequently produce medically inaccurate images, deteriorating the model performance. Expert domain knowledge is critical for synthesizing images that correctly encode clinical information, especially when data is scarce and quality outweighs quantity. Existing approaches for incorporating human feedback, such as reinforcement learning (RL) and Direct Preference Optimization (DPO), rely on robust reward functions or demand labor-intensive expert evaluations. Recent progress in Multimodal Large Language Models (MLLMs) reveals their strong visual reasoning capabilities, making them adept candidates as evaluators. In this work, we propose a novel framework, coined MAGIC (Medically Accurate Generation of Images through AI-Expert Collaboration), that synthesizes clinically accurate skin disease images for data augmentation. Our method creatively translates expert-defined criteria into actionable feedback for image synthesis of DMs, significantly improving clinical accuracy while reducing the direct human workload. Experiments demonstrate that our method greatly improves the clinical quality of synthesized skin disease images, with outputs aligning with dermatologist assessments. Additionally, augmenting training data with these synthesized images improves diagnostic accuracy by +9.02% on a challenging 20-condition skin disease classification task, and by +13.89% in the few-shot setting. </p>
<blockquote>
<p>åŒ»ç–—æ•°æ®çš„ç¼ºä¹ä¸¥é‡é™åˆ¶äº†è¯Šæ–­æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå› ä¸ºå°è§„æ¨¡çš„ä¸´åºŠæ•°æ®é›†æ— æ³•ä»£è¡¨ç–¾ç—…çš„å…¨éƒ¨å˜å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²è¢«è§†ä¸ºåˆæˆå›¾åƒç”Ÿæˆå’Œå¢å¼ºçš„æœ‰å‰é€”çš„é€”å¾„ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸äº§ç”ŸåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒï¼Œä»è€Œé™ä½äº†æ¨¡å‹æ€§èƒ½ã€‚åœ¨æ•°æ®ç¨€ç¼ºä¸”è´¨é‡èƒœè¿‡æ•°é‡çš„æƒ…å†µä¸‹ï¼Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†å¯¹äºåˆæˆæ­£ç¡®ç¼–ç ä¸´åºŠä¿¡æ¯çš„å›¾åƒè‡³å…³é‡è¦ã€‚ç°æœ‰çš„èå…¥äººç±»åé¦ˆçš„æ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œä¾èµ–äºç¨³å¥çš„å¥–åŠ±åŠŸèƒ½æˆ–éœ€æ±‚åŠ³åŠ¨å¯†é›†å‹çš„ä¸“å®¶è¯„ä¼°ã€‚æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºå‡ºäº†å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºç†Ÿç»ƒçš„å€™é€‰è¯„ä¼°è€…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåä¸ºMAGICï¼ˆé€šè¿‡AI-Expertåä½œè¿›è¡ŒåŒ»å­¦å‡†ç¡®çš„å›¾åƒç”Ÿæˆï¼‰ï¼Œè¯¥æ¡†æ¶åˆæˆç”¨äºæ•°æ®å¢å¼ºçš„ä¸´åºŠå‡†ç¡®çš®è‚¤ç—…å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ›é€ æ€§åœ°å°†ä¸“å®¶å®šä¹‰çš„æ ‡å‡†è½¬åŒ–ä¸ºå¯¹æ‰©æ•£æ¨¡å‹å›¾åƒåˆæˆçš„å¯æ“ä½œåé¦ˆï¼Œæ˜¾è‘—æé«˜äº†ä¸´åºŠå‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†ç›´æ¥äººåŠ›å·¥ä½œé‡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§æé«˜äº†åˆæˆçš®è‚¤ç—…å›¾åƒçš„ä¸´åºŠè´¨é‡ï¼Œè¾“å‡ºç»“æœä¸çš®è‚¤ç§‘åŒ»ç”Ÿçš„è¯„ä¼°ç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è¿™äº›åˆæˆå›¾åƒå¢å¼ºè®­ç»ƒæ•°æ®ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„20ç§çš®è‚¤ç—…åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯Šæ–­å‡†ç¡®ç‡æé«˜äº†+9.02%ï¼Œåœ¨å°‘é•œå¤´è®¾ç½®ä¸‹æé«˜äº†+13.89%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12323v2">PDF</a> NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»ç–—æ•°æ®çš„åŒ®ä¹ä¸¥é‡é™åˆ¶äº†è¯Šæ–­æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ‰©æ•£æ¨¡å‹åœ¨åˆæˆå›¾åƒç”Ÿæˆå’Œå¢å¼ºæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¾€å¾€äº§ç”ŸåŒ»å­¦ä¸Šä¸å‡†ç¡®çš„å›¾åƒï¼Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚ä¸“å®¶é¢†åŸŸçŸ¥è¯†å¯¹äºåˆæˆæ­£ç¡®ç¼–ç ä¸´åºŠä¿¡æ¯çš„å›¾åƒè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºã€è´¨é‡èƒœè¿‡æ•°é‡çš„æƒ…å†µä¸‹ã€‚ç»“åˆäººç±»åé¦ˆçš„ç°æœ‰æ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼Œä¾èµ–äºç¨³å¥çš„å¥–åŠ±å‡½æ•°æˆ–éœ€è¦å¤§é‡ä¸“å®¶è¯„ä¼°ã€‚æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›å±•æ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºè¯„ä¼°è€…çš„åˆé€‚å€™é€‰ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºMAGICï¼ˆé€šè¿‡äººå·¥æ™ºèƒ½ä¸“å®¶åˆä½œè¿›è¡ŒåŒ»å­¦å‡†ç¡®å›¾åƒç”Ÿæˆï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºåˆæˆç”¨äºæ•°æ®å¢å¼ºçš„ä¸´åºŠå‡†ç¡®çš®è‚¤ç—…å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ›é€ æ€§åœ°å°†ä¸“å®¶å®šä¹‰çš„å‡†åˆ™è½¬åŒ–ä¸ºå¯¹æ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆæˆå¯æ“ä½œåé¦ˆï¼Œæ˜¾è‘—æé«˜äº†ä¸´åºŠå‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†ç›´æ¥äººåŠ›å·¥ä½œé‡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§æé«˜äº†åˆæˆçš®è‚¤ç—…å›¾åƒçš„ä¸´åºŠè´¨é‡ï¼Œè¾“å‡ºç»“æœä¸çš®è‚¤ç§‘åŒ»ç”Ÿçš„è¯„ä¼°ç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è¿™äº›åˆæˆå›¾åƒå¢å¼ºè®­ç»ƒæ•°æ®ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„20ç§çš®è‚¤ç—…åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯Šæ–­å‡†ç¡®ç‡æé«˜9.02%ï¼Œåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹æé«˜13.89%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒç”Ÿæˆå’Œå¢å¼ºä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†ç¼ºä¹åŒ»å­¦å‡†ç¡®æ€§ã€‚</li>
<li>ä¸“å®¶é¢†åŸŸçŸ¥è¯†å¯¹äºåˆæˆä¸´åºŠå‡†ç¡®å›¾åƒè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç»“åˆäººç±»åé¦ˆçš„æ–¹æ³•ä¾èµ–äºç¨³å¥çš„å¥–åŠ±å‡½æ•°æˆ–éœ€è¦å¤§é‡ä¸“å®¶è¯„ä¼°ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºå›¾åƒè¯„ä¼°ã€‚</li>
<li>æå‡ºçš„MAGICæ¡†æ¶é€šè¿‡äººå·¥æ™ºèƒ½ä¸ä¸“å®¶çš„åˆä½œï¼Œèƒ½åˆæˆä¸´åºŠå‡†ç¡®çš„çš®è‚¤ç—…å›¾åƒã€‚</li>
<li>MAGICæ–¹æ³•æé«˜äº†åˆæˆçš®è‚¤ç—…å›¾åƒçš„ä¸´åºŠè´¨é‡ï¼Œå¹¶ä¸çš®è‚¤ç§‘åŒ»ç”Ÿè¯„ä¼°ç›¸ç¬¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c50aa8ed00c0f2d6f2b631ab5ced920" align="middle">
<img src="https://picx.zhimg.com/v2-4a3fb120011fcfafaa2851c5fb95f845" align="middle">
<img src="https://picx.zhimg.com/v2-2fe52b39bc4308923cd78ed128ec6d42" align="middle">
<img src="https://picx.zhimg.com/v2-89706c45d139275fd6d1e54199726085" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Beyond-Masked-and-Unmasked-Discrete-Diffusion-Models-via-Partial-Masking"><a href="#Beyond-Masked-and-Unmasked-Discrete-Diffusion-Models-via-Partial-Masking" class="headerlink" title="Beyond Masked and Unmasked: Discrete Diffusion Models via Partial   Masking"></a>Beyond Masked and Unmasked: Discrete Diffusion Models via Partial   Masking</h2><p><strong>Authors:Chen-Hao Chao, Wei-Fang Sun, Hanwen Liang, Chun-Yi Lee, Rahul G. Krishnan</strong></p>
<p>Masked diffusion models (MDM) are powerful generative models for discrete data that generate samples by progressively unmasking tokens in a sequence. Each token can take one of two states: masked or unmasked. We observe that token sequences often remain unchanged between consecutive sampling steps; consequently, the model repeatedly processes identical inputs, leading to redundant computation. To address this inefficiency, we propose the Partial masking scheme (Prime), which augments MDM by allowing tokens to take intermediate states interpolated between the masked and unmasked states. This design enables the model to make predictions based on partially observed token information, and facilitates a fine-grained denoising process. We derive a variational training objective and introduce a simple architectural design to accommodate intermediate-state inputs. Our method demonstrates superior performance across a diverse set of generative modeling tasks. On text data, it achieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM (21.52), autoregressive models (17.54), and their hybrid variants (17.58), without relying on an autoregressive formulation. On image data, it attains competitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable to leading continuous generative models. </p>
<blockquote>
<p>æ©ç æ‰©æ•£æ¨¡å‹ï¼ˆMDMï¼‰æ˜¯ä¸ºç¦»æ•£æ•°æ®è®¾è®¡çš„å¼ºå¤§ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡é€æ­¥æ­ç¤ºåºåˆ—ä¸­çš„ä»¤ç‰Œæ¥ç”Ÿæˆæ ·æœ¬ã€‚æ¯ä¸ªä»¤ç‰Œå¯ä»¥å¤„äºä¸¤ç§çŠ¶æ€ä¹‹ä¸€ï¼šæ©ç çŠ¶æ€æˆ–æœªæ©ç çŠ¶æ€ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨è¿ç»­çš„é‡‡æ ·æ­¥éª¤ä¹‹é—´ï¼Œä»¤ç‰Œåºåˆ—å¾€å¾€ä¿æŒä¸å˜ï¼›å› æ­¤ï¼Œæ¨¡å‹ä¼šåå¤å¤„ç†ç›¸åŒçš„è¾“å…¥ï¼Œå¯¼è‡´å†—ä½™è®¡ç®—ã€‚ä¸ºäº†è§£å†³è¿™ç§ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†éƒ¨åˆ†æ©ç æ–¹æ¡ˆï¼ˆPrimeï¼‰ï¼Œå®ƒé€šè¿‡å…è®¸ä»¤ç‰Œå¤„äºæ©ç çŠ¶æ€å’Œæœªæ©ç çŠ¶æ€ä¹‹é—´çš„ä¸­é—´çŠ¶æ€ï¼Œæ¥å¢å¼ºMDMçš„åŠŸèƒ½ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºéƒ¨åˆ†è§‚å¯Ÿåˆ°çš„ä»¤ç‰Œä¿¡æ¯è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä¿ƒè¿›äº†ç²¾ç»†çš„é™å™ªè¿‡ç¨‹ã€‚æˆ‘ä»¬æ¨å¯¼äº†å˜åˆ†è®­ç»ƒç›®æ ‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç®€å•çš„æ¶æ„è®¾è®¡æ¥é€‚åº”ä¸­é—´çŠ¶æ€è¾“å…¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚åœ¨æ–‡æœ¬æ•°æ®ä¸Šï¼Œå®ƒåœ¨OpenWebTextä¸Šçš„å›°æƒ‘åº¦è¾¾åˆ°15.36ï¼Œä¼˜äºå…ˆå‰çš„MDMï¼ˆ21.52ï¼‰ã€è‡ªå›å½’æ¨¡å‹ï¼ˆ17.54ï¼‰åŠå…¶æ··åˆå˜ä½“ï¼ˆ17.58ï¼‰ï¼Œä¸”æ— éœ€ä¾èµ–è‡ªå›å½’å…¬å¼ã€‚åœ¨å›¾åƒæ•°æ®ä¸Šï¼Œå®ƒåœ¨CIFAR-10ä¸Šçš„FIDå¾—åˆ†ä¸º3.26ï¼Œåœ¨ImageNet-32ä¸Šçš„å¾—åˆ†ä¸º6.98ï¼Œä¸é¢†å…ˆçš„è¿ç»­ç”Ÿæˆæ¨¡å‹ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18495v2">PDF</a> Published at NeurIPS 2025. Project Page:   <a target="_blank" rel="noopener" href="https://chen-hao-chao.github.io/mdm-prime">https://chen-hao-chao.github.io/mdm-prime</a></p>
<p><strong>Summary</strong></p>
<p>éƒ¨åˆ†æ©ç›–æ‰©æ•£æ¨¡å‹ï¼ˆMDMï¼‰åœ¨ç¦»æ•£æ•°æ®ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå®ƒé€šè¿‡é€æ­¥æ­ç¤ºåºåˆ—ä¸­çš„ä»¤ç‰Œæ¥å®ç°æ ·æœ¬ç”Ÿæˆã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åä¸ºPrimeçš„éƒ¨åˆ†æ©ç›–æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå…è®¸ä»¤ç‰Œå¤„äºæ©ç›–ä¸æœªæ©ç›–ä¹‹é—´çš„ä¸­é—´çŠ¶æ€ï¼Œä»¥æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›å’Œç»†åŒ–å»å™ªè¿‡ç¨‹ã€‚æ–°æ–¹æ¡ˆåœ¨å¤šç§ç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¯¹OpenWebTextæ–‡æœ¬çš„å›°æƒ‘åº¦é™è‡³15.36ï¼Œä¼˜äºä¼ ç»ŸMDMã€è‡ªå›å½’æ¨¡å‹åŠå…¶æ··åˆå˜ä½“ï¼›åœ¨CIFAR-10å’ŒImageNet-32å›¾åƒæ•°æ®ä¸Šçš„FIDå¾—åˆ†ä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Masked Diffusion Models (MDM) æ˜¯ç¦»æ•£æ•°æ®ç”Ÿæˆçš„æœ‰åŠ›å·¥å…·ï¼Œé€šè¿‡é€æ­¥æ­ç¤ºä»¤ç‰Œæ¥å®ç°æ ·æœ¬ç”Ÿæˆã€‚</li>
<li>ä»¤ç‰Œåœ¨æ©ç›–ä¸æœªæ©ç›–ä¹‹é—´å¯ä»¥å­˜åœ¨ä¸­é—´çŠ¶æ€ï¼Œè¿™ä¸€ç‰¹æ€§ç§°ä¸ºPartial masking schemeï¼ˆPrimeï¼‰ã€‚</li>
<li>Primeæ–¹æ¡ˆæé«˜äº†æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œå¹¶ä¿ƒè¿›äº†æ›´ç²¾ç»†çš„å»å™ªè¿‡ç¨‹ã€‚</li>
<li>åœ¨ç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸Šï¼ŒPrimeæ–¹æ¡ˆè¡¨ç°å“è¶Šï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ–‡æœ¬å’Œå›¾åƒæ•°æ®æ—¶ã€‚</li>
<li>åœ¨OpenWebTextæ–‡æœ¬æ•°æ®ä¸Šï¼Œæ–°æ–¹æ³•çš„å›°æƒ‘åº¦ä½äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>åœ¨CIFAR-10å’ŒImageNet-32å›¾åƒæ•°æ®ä¸Šï¼Œæ–°æ–¹æ³•çš„FIDå¾—åˆ†å…·æœ‰ç«äº‰åŠ›ï¼Œä¸é¢†å…ˆçš„è¿ç»­ç”Ÿæˆæ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2e67a46ccf7b5f973de562b202f6243" align="middle">
<img src="https://picx.zhimg.com/v2-38dab630f1ecf553b6cc763828d73571" align="middle">
<img src="https://picx.zhimg.com/v2-0b74f05531384157f1b8deab88a77adb" align="middle">
<img src="https://picx.zhimg.com/v2-898dc5f6dfe98bfb110a08b0a2674723" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FairGen-Controlling-Sensitive-Attributes-for-Fair-Generations-in-Diffusion-Models-via-Adaptive-Latent-Guidance"><a href="#FairGen-Controlling-Sensitive-Attributes-for-Fair-Generations-in-Diffusion-Models-via-Adaptive-Latent-Guidance" class="headerlink" title="FairGen: Controlling Sensitive Attributes for Fair Generations in   Diffusion Models via Adaptive Latent Guidance"></a>FairGen: Controlling Sensitive Attributes for Fair Generations in   Diffusion Models via Adaptive Latent Guidance</h2><p><strong>Authors:Mintong Kang, Vinayshekhar Bannihatti Kumar, Shamik Roy, Abhishek Kumar, Sopan Khosla, Balakrishnan Murali Narayanaswamy, Rashmi Gangadharaiah</strong></p>
<p>Text-to-image diffusion models often exhibit biases toward specific demographic groups, such as generating more males than females when prompted to generate images of engineers, raising ethical concerns and limiting their adoption. In this paper, we tackle the challenge of mitigating generation bias towards any target attribute value (e.g., â€œmaleâ€ for â€œgenderâ€) in diffusion models while preserving generation quality. We propose FairGen, an adaptive latent guidance mechanism which controls the generation distribution during inference. In FairGen, a latent guidance module dynamically adjusts the diffusion process to enforce specific attributes, while a memory module tracks the generation statistics and steers latent guidance to align with the targeted fair distribution of the attribute values. Furthermore, we address the limitations of existing datasets by introducing the Holistic Bias Evaluation (HBE) benchmark, which covers diverse domains and incorporates complex prompts to assess bias more comprehensively. Extensive evaluations on HBE and Stable Bias datasets demonstrate that FairGen outperforms existing bias mitigation approaches, achieving substantial bias reduction (e.g., 68.5% gender bias reduction on Stable Diffusion 2). Ablation studies highlight FairGenâ€™s ability to flexibly control the output distribution at any user-specified granularity, ensuring adaptive and targeted bias mitigation. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å¾€å¾€ä¼šå¯¹ç‰¹å®šçš„ç¾¤ä½“äº§ç”Ÿåè§ï¼Œä¾‹å¦‚åœ¨æç¤ºç”Ÿæˆå·¥ç¨‹å¸ˆå›¾åƒæ—¶ï¼Œç”Ÿæˆæ›´å¤šçš„ç”·æ€§è€Œä¸æ˜¯å¥³æ€§ï¼Œè¿™å¼•å‘äº†ä¼¦ç†æ‹…å¿§ï¼Œå¹¶é™åˆ¶äº†å…¶é‡‡ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†åœ¨æ‰©æ•£æ¨¡å‹ä¸­ç¼“è§£å¯¹ä»»ä½•ç›®æ ‡å±æ€§å€¼çš„ç”Ÿæˆåè§ï¼ˆä¾‹å¦‚ï¼Œâ€œæ€§åˆ«â€ä¸­çš„â€œç”·æ€§â€ï¼‰çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†FairGenï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”çš„æ½œåœ¨æŒ‡å¯¼æœºåˆ¶ï¼Œç”¨äºæ§åˆ¶æ¨ç†è¿‡ç¨‹ä¸­çš„ç”Ÿæˆåˆ†å¸ƒã€‚åœ¨FairGenä¸­ï¼Œæ½œåœ¨æŒ‡å¯¼æ¨¡å—åŠ¨æ€è°ƒæ•´æ‰©æ•£è¿‡ç¨‹ä»¥å¼ºåˆ¶æ‰§è¡Œç‰¹å®šå±æ€§ï¼Œè€Œè®°å¿†æ¨¡å—åˆ™è·Ÿè¸ªç”Ÿæˆç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶å¼•å¯¼æ½œåœ¨æŒ‡å¯¼ä¸å±æ€§å€¼çš„å…¬å¹³åˆ†å¸ƒç›®æ ‡ä¿æŒä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å…¨é¢çš„åè§è¯„ä¼°ï¼ˆHBEï¼‰åŸºå‡†ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†çš„é™åˆ¶ï¼Œè¯¥åŸºå‡†æ¶µç›–å¤šä¸ªé¢†åŸŸå¹¶åŒ…å«å¤æ‚çš„æç¤ºï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°åè§ã€‚åœ¨HBEå’ŒStable Biasæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFairGenä¼˜äºç°æœ‰çš„åè§ç¼“è§£æ–¹æ³•ï¼Œå®ç°äº†æ˜¾è‘—çš„åè§å‡å°‘ï¼ˆä¾‹å¦‚åœ¨Stable Diffusion 2ä¸Šå®ç°äº†68.5%çš„æ€§åˆ«åè§å‡å°‘ï¼‰ã€‚æ¶ˆèç ”ç©¶çªå‡ºäº†FairGenåœ¨ç”¨æˆ·æŒ‡å®šçš„ä»»ä½•ç²’åº¦ä¸Šçµæ´»æ§åˆ¶è¾“å‡ºåˆ†å¸ƒçš„èƒ½åŠ›ï¼Œç¡®ä¿è‡ªé€‚åº”å’Œæœ‰é’ˆå¯¹æ€§çš„åè§ç¼“è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01872v2">PDF</a> EMNLP 2025 Main Conference (Camera Ready)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ç”Ÿæˆåè§é—®é¢˜ï¼Œå¹¶æå‡ºäº†FairGenæ–¹æ¡ˆä»¥ç¼“è§£å¯¹ç›®æ ‡å±æ€§å€¼çš„ç”Ÿæˆåè§ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ½œåœ¨æŒ‡å¯¼æœºåˆ¶ï¼ŒFairGenèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ§åˆ¶ç”Ÿæˆåˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†å…¨æ–°çš„Holistic Bias Evaluationï¼ˆHBEï¼‰åŸºå‡†æµ‹è¯•ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°ä¸åŒé¢†åŸŸçš„å¤æ‚æç¤ºä¸­çš„åè§ã€‚å®éªŒè¡¨æ˜ï¼ŒFairGenåœ¨HBEå’ŒStable Biasæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„åè§ç¼“è§£æ–¹æ³•ï¼Œå®ç°äº†æ˜¾è‘—çš„åè§å‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å­˜åœ¨å¯¹ç‰¹å®šäººç¾¤ï¼ˆå¦‚å·¥ç¨‹å¸ˆä¸­çš„ç”·æ€§ï¼‰çš„åè§ç”Ÿæˆé—®é¢˜ã€‚</li>
<li>FairGenæ–¹æ¡ˆè¢«æå‡ºä»¥ç¼“è§£å¯¹ç›®æ ‡å±æ€§å€¼çš„ç”Ÿæˆåè§ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚</li>
<li>FairGenå¼•å…¥äº†è‡ªé€‚åº”æ½œåœ¨æŒ‡å¯¼æœºåˆ¶ï¼Œèƒ½åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ§åˆ¶ç”Ÿæˆåˆ†å¸ƒã€‚</li>
<li>é¦–æ¬¡ä»‹ç»äº†Holistic Bias Evaluationï¼ˆHBEï¼‰åŸºå‡†æµ‹è¯•ï¼Œç”¨äºæ›´å…¨é¢åœ°è¯„ä¼°ä¸åŒé¢†åŸŸçš„å¤æ‚æç¤ºä¸­çš„åè§ã€‚</li>
<li>å®éªŒè¯æ˜FairGenåœ¨HBEå’ŒStable Biasæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°æ˜¾è‘—åè§å‡å°‘ã€‚</li>
<li>FairGenèƒ½å¤Ÿçµæ´»æ§åˆ¶ç”¨æˆ·æŒ‡å®šç²’åº¦çš„è¾“å‡ºåˆ†å¸ƒï¼Œå…·æœ‰è‡ªé€‚åº”å’Œé’ˆå¯¹æ€§çš„åè§ç¼“è§£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-881c9d6a882540844921698bd6d9b87c" align="middle">
<img src="https://picx.zhimg.com/v2-ea603147223668e94c6c840e934257e8" align="middle">
<img src="https://picx.zhimg.com/v2-9a4aaf1e1f0e2ddff121f84ed27de6f8" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Graph-Representation-Learning-with-Diffusion-Generative-Models"><a href="#Graph-Representation-Learning-with-Diffusion-Generative-Models" class="headerlink" title="Graph Representation Learning with Diffusion Generative Models"></a>Graph Representation Learning with Diffusion Generative Models</h2><p><strong>Authors:Daniel Wesego</strong></p>
<p>Diffusion models have established themselves as state-of-the-art generative models across various data modalities, including images and videos, due to their ability to accurately approximate complex data distributions. Unlike traditional generative approaches such as VAEs and GANs, diffusion models employ a progressive denoising process that transforms noise into meaningful data over multiple iterative steps. This gradual approach enhances their expressiveness and generation quality. Not only that, diffusion models have also been shown to extract meaningful representations from data while learning to generate samples. Despite their success, the application of diffusion models to graph-structured data remains relatively unexplored, primarily due to the discrete nature of graphs, which necessitates discrete diffusion processes distinct from the continuous methods used in other domains. In this work, we leverage the representational capabilities of diffusion models to learn meaningful embeddings for graph data. By training a discrete diffusion model within an autoencoder framework, we enable both effective autoencoding and representation learning tailored to the unique characteristics of graph-structured data. We extract the representation from the combination of the encoderâ€™s output and the decoderâ€™s first time step hidden embedding. Our approach demonstrates the potential of discrete diffusion models to be used for graph representation learning. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/DanielMitiku/Graph-Representation-Learning-with-Diffusion-Generative-Models">https://github.com/DanielMitiku/Graph-Representation-Learning-with-Diffusion-Generative-Models</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å‡­å€Ÿå…¶å‡†ç¡®é€¼è¿‘å¤æ‚æ•°æ®åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œå·²æˆä¸ºå›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ•°æ®æ¨¡æ€çš„å…ˆè¿›ç”Ÿæˆæ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„ç”Ÿæˆæ–¹æ³•ï¼ˆå¦‚å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰ä¸åŒï¼Œæ‰©æ•£æ¨¡å‹é‡‡ç”¨æ¸è¿›çš„å»å™ªè¿‡ç¨‹ï¼Œé€šè¿‡å¤šä¸ªè¿­ä»£æ­¥éª¤å°†å™ªå£°è½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„æ•°æ®ã€‚è¿™ç§é€æ­¥çš„æ–¹æ³•å¢å¼ºäº†å…¶è¡¨è¾¾åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚ä¸ä»…å¦‚æ­¤ï¼Œæ‰©æ•£æ¨¡å‹è¿˜æ˜¾ç¤ºå‡ºåœ¨ç”Ÿæˆæ ·æœ¬çš„åŒæ—¶ä»æ•°æ®ä¸­æå–æœ‰æ„ä¹‰è¡¨ç¤ºçš„èƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬å–å¾—äº†æˆåŠŸï¼Œä½†å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå›¾å½¢ç»“æ„æ•°æ®ä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå›¾å½¢çš„ç¦»æ•£æ€§è´¨éœ€è¦ä¸åŒäºå…¶ä»–é¢†åŸŸä½¿ç”¨çš„è¿ç»­æ–¹æ³•çš„ç¦»æ•£æ‰©æ•£è¿‡ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›æ¥å­¦ä¹ å›¾å½¢æ•°æ®çš„æœ‰æ„ä¹‰åµŒå…¥ã€‚é€šè¿‡åœ¨è‡ªç¼–ç å™¨æ¡†æ¶å†…è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬å®ç°äº†é’ˆå¯¹å›¾å½¢ç»“æ„æ•°æ®çš„æœ‰æ•ˆè‡ªç¼–ç å’Œè¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬ä»ç¼–ç å™¨è¾“å‡ºå’Œè§£ç å™¨ç¬¬ä¸€æ­¥éšè—åµŒå…¥çš„ç»„åˆä¸­æå–è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨å›¾å½¢è¡¨ç¤ºå­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/DanielMitiku/Graph-Representation-Learning-with-Diffusion-Generative-Models">https://github.com/DanielMitiku/Graph-Representation-Learning-with-Diffusion-Generative-Models</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13133v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥å»å™ªè¿‡ç¨‹ï¼Œå°†å™ªå£°è½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„æ•°æ®ï¼Œåœ¨å›¾åƒã€è§†é¢‘ç­‰å¤šç§æ•°æ®æ¨¡æ€ä¸Šè¡¨ç°å‡ºä¼˜ç§€çš„ç”Ÿæˆèƒ½åŠ›ã€‚å…¶åœ¨å›¾ç»“æ„æ•°æ®çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œä¸»è¦ç”±äºå›¾çš„ç¦»æ•£æ€§è´¨éœ€è¦ç¦»æ•£æ‰©æ•£è¿‡ç¨‹ï¼Œæœ‰åˆ«äºå…¶ä»–é¢†åŸŸçš„è¿ç»­æ–¹æ³•ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå­¦ä¹ å›¾æ•°æ®çš„æœ‰æ„ä¹‰åµŒå…¥ã€‚é€šè¿‡è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹äºè‡ªç¼–ç å™¨æ¡†æ¶å†…ï¼Œå®ç°äº†é’ˆå¯¹å›¾ç»“æ„æ•°æ®çš„æœ‰æ•ˆè‡ªç¼–ç å’Œè¡¨ç¤ºå­¦ä¹ ã€‚ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æ˜¯å½“ä¸‹å…ˆè¿›ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å‡†ç¡®è¿‘ä¼¼å¤æ‚æ•°æ®åˆ†å¸ƒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é‡‡ç”¨é€æ­¥å»å™ªè¿‡ç¨‹ï¼Œå¯å¢å¼ºè¡¨è¾¾æ€§å’Œç”Ÿæˆè´¨é‡ã€‚</li>
<li>ç¦»æ•£æ‰©æ•£æ¨¡å‹é€‚ç”¨äºå›¾ç»“æ„æ•°æ®å­¦ä¹ ï¼Œå› å›¾çš„ç¦»æ•£æ€§è´¨éœ€ç‰¹å®šå¤„ç†ã€‚</li>
<li>ç ”ç©¶é€šè¿‡è‡ªç¼–ç å™¨æ¡†æ¶è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œå®ç°å›¾æ•°æ®çš„æœ‰æ•ˆè‡ªç¼–ç å’Œè¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾è¡¨ç¤ºå­¦ä¹ ä¸­çš„æ½œåŠ›å¾—åˆ°éªŒè¯ã€‚</li>
<li>è¯¥ç ”ç©¶ä»£ç å¯é€šè¿‡æŒ‡å®šé“¾æ¥è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ba6dc26a168bdb8c802fda5197136e6" align="middle">
<img src="https://picx.zhimg.com/v2-2bb60f7d1500500cf09c3adf6f66aede" align="middle">
<img src="https://picx.zhimg.com/v2-b6e4f1c8c3c8b1e6ce2ab651043a61a2" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FairGen-Enhancing-Fairness-in-Text-to-Image-Diffusion-Models-via-Self-Discovering-Latent-Directions"><a href="#FairGen-Enhancing-Fairness-in-Text-to-Image-Diffusion-Models-via-Self-Discovering-Latent-Directions" class="headerlink" title="FairGen: Enhancing Fairness in Text-to-Image Diffusion Models via   Self-Discovering Latent Directions"></a>FairGen: Enhancing Fairness in Text-to-Image Diffusion Models via   Self-Discovering Latent Directions</h2><p><strong>Authors:Yilei Jiang, Weihong Li, Yiyuan Zhang, Minghong Cai, Xiangyu Yue</strong></p>
<p>While Diffusion Models (DM) exhibit remarkable performance across various image generative tasks, they nonetheless reflect the inherent bias presented in the training set. As DMs are now widely used in real-world applications, these biases could perpetuate a distorted worldview and hinder opportunities for minority groups. Existing methods on debiasing DMs usually requires model retraining with a human-crafted reference dataset or additional classifiers, which suffer from two major limitations: (1) collecting reference datasets causes expensive annotation cost; (2) the debiasing performance is heavily constrained by the quality of the reference dataset or the additional classifier. To address the above limitations, we propose FairGen, a plug-and-play method that learns attribute latent directions in a self-discovering manner, thus eliminating the reliance on such reference dataset. Specifically, FairGen consists of two parts: a set of attribute adapters and a distribution indicator. Each adapter in the set aims to learn an attribute latent direction, and is optimized via noise composition through a self-discovering process. Then, the distribution indicator is multiplied by the set of adapters to guide the generation process towards the prescribed distribution. Our method enables debiasing multiple attributes in DMs simultaneously, while remaining lightweight and easily integrable with other DMs, eliminating the need for retraining. Extensive experiments on debiasing gender, racial, and their intersectional biases show that our method outperforms previous SOTA by a large margin. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨å„ç§å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶åæ˜ å‡ºè®­ç»ƒé›†æ‰€å‘ˆç°çš„å›ºæœ‰åè§ã€‚ç”±äºæ‰©æ•£æ¨¡å‹ç°åœ¨å¹¿æ³›åº”ç”¨äºç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œè¿™äº›åè§å¯èƒ½ä¼šæŒç»­æ‰­æ›²ä¸–ç•Œè§‚å¹¶é˜»ç¢å°‘æ•°ç¾¤ä½“çš„æœºä¼šã€‚ç°æœ‰çš„å…³äºå»åæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•é€šå¸¸éœ€è¦ä¸äººå·¥æ„å»ºçš„å‚è€ƒæ•°æ®é›†æˆ–é¢å¤–çš„åˆ†ç±»å™¨ä¸€èµ·è¿›è¡Œæ¨¡å‹å†è®­ç»ƒï¼Œè¿™å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰æ”¶é›†å‚è€ƒæ•°æ®é›†å¯¼è‡´æ˜‚è´µçš„æ ‡æ³¨æˆæœ¬ï¼›ï¼ˆ2ï¼‰å»åæ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°å‚è€ƒæ•°æ®é›†æˆ–é™„åŠ åˆ†ç±»å™¨è´¨é‡çš„å½±å“ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FairGenï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨æ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥è‡ªæˆ‘å‘ç°çš„æ–¹å¼å­¦ä¹ å±æ€§æ½œåœ¨æ–¹å‘ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å‚è€ƒæ•°æ®é›†çš„ä¾èµ–ã€‚å…·ä½“æ¥è¯´ï¼ŒFairGenç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€ç»„å±æ€§é€‚é…å™¨å’Œä¸€ä¸ªåˆ†å¸ƒæŒ‡æ ‡ã€‚é›†åˆä¸­çš„æ¯ä¸ªé€‚é…å™¨æ—¨åœ¨å­¦ä¹ ä¸€ä¸ªå±æ€§æ½œåœ¨æ–¹å‘ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘å‘ç°è¿‡ç¨‹è¿›è¡Œä¼˜åŒ–å™ªå£°ç»„åˆã€‚ç„¶åï¼Œåˆ†å¸ƒæŒ‡æ ‡ä¸é€‚é…å™¨é›†åˆç›¸ä¹˜ï¼Œä»¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹æœå‘è§„å®šçš„åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤ŸåŒæ—¶å»é™¤æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šä¸ªå±æ€§åè§ï¼ŒåŒæ—¶ä¿æŒè½»é‡åŒ–ï¼Œå¹¶ä¸”å®¹æ˜“ä¸å…¶ä»–æ‰©æ•£æ¨¡å‹é›†æˆï¼Œæ— éœ€è¿›è¡Œå†è®­ç»ƒã€‚å¯¹æ¶ˆé™¤æ€§åˆ«ã€ç§æ—åŠå…¶äº¤å‰åè§çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§è¶…è¶Šäº†ä¹‹å‰çš„æœ€ä¼˜æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18810v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†Diffusion Modelsï¼ˆDMï¼‰åœ¨å¤„ç†å›¾åƒç”Ÿæˆä»»åŠ¡æ—¶å­˜åœ¨çš„åè§é—®é¢˜ã€‚ç°æœ‰å»åæ–¹æ³•éœ€è¦æ˜‚è´µçš„æ ‡æ³¨æˆæœ¬ï¼Œå¹¶å—é™äºå‚è€ƒæ•°æ®é›†æˆ–é™„åŠ åˆ†ç±»å™¨çš„è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºFairGençš„å³æ’å³ç”¨æ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘å‘ç°çš„æ–¹å¼å­¦ä¹ å±æ€§æ½œåœ¨æ–¹å‘ï¼Œæ— éœ€ä¾èµ–å‚è€ƒæ•°æ®é›†ã€‚FairGenåŒ…æ‹¬å±æ€§é€‚é…å™¨å’Œåˆ†å¸ƒæŒ‡ç¤ºå™¨ä¸¤éƒ¨åˆ†ï¼Œèƒ½åŒæ—¶å»åå¤šä¸ªå±æ€§ï¼Œä¸”æ˜“äºä¸å…¶ä»–DMé›†æˆï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§åˆ«ã€ç§æ—åŠå…¶äº¤å‰åè§çš„å»åä»»åŠ¡ä¸Šå¤§å¹…è¶…è¶Šäº†å…ˆå‰æœ€ä½³æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Modelsåœ¨å¤„ç†å›¾åƒç”Ÿæˆä»»åŠ¡æ—¶å­˜åœ¨åè§é—®é¢˜ï¼Œå¯èƒ½åŠ å‰§å¯¹å°‘æ•°ç¾¤ä½“çš„ä¸å…¬å¹³å¾…é‡ã€‚</li>
<li>ç°æœ‰å»åæ–¹æ³•é€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¾èµ–æ˜‚è´µçš„æ ‡æ³¨æˆæœ¬å’Œå‚è€ƒæ•°æ®é›†çš„è´¨é‡ã€‚</li>
<li>FairGenæ–¹æ³•é€šè¿‡è‡ªæˆ‘å‘ç°çš„æ–¹å¼å­¦ä¹ å±æ€§æ½œåœ¨æ–¹å‘ï¼Œæ— éœ€ä¾èµ–å‚è€ƒæ•°æ®é›†ã€‚</li>
<li>FairGenåŒ…æ‹¬å±æ€§é€‚é…å™¨å’Œåˆ†å¸ƒæŒ‡ç¤ºå™¨ä¸¤éƒ¨åˆ†ï¼Œå¯ä»¥åŒæ—¶å»åå¤šä¸ªå±æ€§ã€‚</li>
<li>FairGenæ–¹æ³•æ˜“äºä¸å…¶ä»–Diffusion Modelsé›†æˆï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFairGenåœ¨æ€§åˆ«ã€ç§æ—åŠå…¶äº¤å‰åè§çš„å»åä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¤§å¹…è¶…è¶Šäº†å…ˆå‰æœ€ä½³æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90884d4b6afd3f57fb7263b3daf78744" align="middle">
<img src="https://picx.zhimg.com/v2-6d0121c319e488e5e8c2fb6b82b10283" align="middle">
<img src="https://picx.zhimg.com/v2-c604646aa7d60929677f5724295f00d0" align="middle">
<img src="https://picx.zhimg.com/v2-c422898d1cfa864e284b69e268ac8b15" align="middle">
<img src="https://picx.zhimg.com/v2-264b6b6ce2ec1be2b97ff5dbc48dc863" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GenLit-Reformulating-Single-Image-Relighting-as-Video-Generation"><a href="#GenLit-Reformulating-Single-Image-Relighting-as-Video-Generation" class="headerlink" title="GenLit: Reformulating Single-Image Relighting as Video Generation"></a>GenLit: Reformulating Single-Image Relighting as Video Generation</h2><p><strong>Authors:Shrisha Bharadwaj, Haiwen Feng, Giorgio Becherini, Victoria Fernandez Abrevaya, Michael J. Black</strong></p>
<p>Manipulating the illumination of a 3D scene within a single image represents a fundamental challenge in computer vision and graphics. This problem has traditionally been addressed using inverse rendering techniques, which involve explicit 3D asset reconstruction and costly ray-tracing simulations. Meanwhile, recent advancements in visual foundation models suggest that a new paradigm could soon be possible â€“ one that replaces explicit physical models with networks that are trained on large amounts of image and video data. In this paper, we exploit the implicit scene understanding of a video diffusion model, particularly Stable Video Diffusion, to relight a single image. We introduce GenLit, a framework that distills the ability of a graphics engine to perform light manipulation into a video-generation model, enabling users to directly insert and manipulate a point light in the 3D world within a given image and generate results directly as a video sequence. We find that a model fine-tuned on only a small synthetic dataset generalizes to real-world scenes, enabling single-image relighting with plausible and convincing shadows and inter-reflections. Our results highlight the ability of video foundation models to capture rich information about lighting, material, and shape, and our findings indicate that such models, with minimal training, can be used to perform relighting without explicit asset reconstruction or ray-tracing. . Project page: <a target="_blank" rel="noopener" href="https://genlit.is.tue.mpg.de/">https://genlit.is.tue.mpg.de/</a>. </p>
<blockquote>
<p>åœ¨å•å¹…å›¾åƒå†…æ“çºµ3Dåœºæ™¯çš„ç…§æ˜æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­çš„åŸºæœ¬æŒ‘æˆ˜ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸ªé—®é¢˜æ˜¯é€šè¿‡é€†å‘æ¸²æŸ“æŠ€æœ¯æ¥è§£å†³çš„ï¼Œè¿™æ¶‰åŠåˆ°æ˜ç¡®çš„3Dèµ„äº§é‡å»ºå’Œæ˜‚è´µçš„å…‰çº¿è¿½è¸ªæ¨¡æ‹Ÿã€‚åŒæ—¶ï¼Œè§†è§‰åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œä¸€ç§æ–°çš„èŒƒå¼å³å°†åˆ°æ¥â€”â€”ä¸€ä¸ªç”¨å¤§é‡å›¾åƒå’Œè§†é¢‘æ•°æ®è®­ç»ƒçš„ç½‘ç»œæ¥æ›¿ä»£æ˜ç¡®çš„ç‰©ç†æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„éšå¼åœºæ™¯ç†è§£ï¼Œç‰¹åˆ«æ˜¯ç¨³å®šçš„è§†é¢‘æ‰©æ•£ï¼Œæ¥å¯¹å•å¹…å›¾åƒè¿›è¡Œé‡æ–°ç…§æ˜ã€‚æˆ‘ä»¬ä»‹ç»äº†GenLitæ¡†æ¶ï¼Œå®ƒå°†å›¾å½¢å¼•æ“è¿›è¡Œå…‰çº¿æ“ä½œçš„èƒ½åŠ›æç‚¼æˆè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨ç»™å®šçš„å›¾åƒä¸­çš„3Dä¸–ç•Œé‡Œç›´æ¥æ’å…¥å’Œæ“çºµç‚¹å…‰æºï¼Œå¹¶ç›´æ¥ç”Ÿæˆè§†é¢‘åºåˆ—ç»“æœã€‚æˆ‘ä»¬å‘ç°ä»…åœ¨å°å‹åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒåçš„æ¨¡å‹å¯ä»¥æ¨å¹¿åˆ°çœŸå®åœºæ™¯ï¼Œèƒ½å¤Ÿå®ç°å…·æœ‰å¯ä¿¡é˜´å½±å’Œç›¸äº’åå°„çš„å•å›¾åƒé‡æ–°ç…§æ˜ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†è§†é¢‘åŸºç¡€æ¨¡å‹åœ¨æ•æ‰å…³äºç…§æ˜ã€ææ–™å’Œå½¢çŠ¶ä¸°å¯Œä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ¨¡å‹å¯ä»¥åœ¨æå°‘çš„è®­ç»ƒä¸‹ç”¨äºæ‰§è¡Œé‡æ–°ç…§æ˜ï¼Œæ— éœ€æ˜ç¡®çš„èµ„äº§é‡å»ºæˆ–å…‰çº¿è¿½è¸ªã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://genlit.is.tue.mpg.de/%E3%80%82">https://genlit.is.tue.mpg.de/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11224v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç…§æ˜æ“æ§çš„æ–°æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥GenLitæ¡†æ¶ï¼Œå°†å›¾å½¢å¼•æ“çš„ç…§æ˜æ“æ§èƒ½åŠ›æç‚¼å¹¶èå…¥è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°åœ¨å•ä¸€å›¾åƒå†…å¯¹3Dä¸–ç•Œçš„ç‚¹å…‰æºè¿›è¡Œç›´æ¥æ’å…¥ä¸æ“æ§ï¼Œå¹¶ç”Ÿæˆè§†é¢‘åºåˆ—ã€‚è¯¥ç ”ç©¶å®ç°äº†åœ¨å°‘é‡åˆæˆæ•°æ®é›†ä¸Šçš„å¾®è°ƒï¼Œå¹¶èƒ½åœ¨çœŸå®åœºæ™¯ä¸­è¿›è¡Œå•å›¾åƒç…§æ˜ï¼Œç”Ÿæˆå…·æœ‰é€¼çœŸé˜´å½±å’Œåå°„æ•ˆæœçš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹æ“çºµå•ä¸€å›¾åƒå†…3Dåœºæ™¯ç…§æ˜çš„æ–°æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†GenLitæ¡†æ¶ï¼Œå°†å›¾å½¢å¼•æ“çš„ç…§æ˜æ“æ§èƒ½åŠ›èå…¥è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>å®ç°äº†åœ¨ç»™å®šå›¾åƒå†…ç›´æ¥æ’å…¥å’Œæ“æ§ç‚¹å…‰æºçš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆè§†é¢‘åºåˆ—å±•ç¤ºç…§æ˜æ“æ§ç»“æœã€‚</li>
<li>åœ¨å°‘é‡åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°çœŸå®åœºæ™¯ã€‚</li>
<li>ç”Ÿæˆç»“æœå…·æœ‰é€¼çœŸçš„é˜´å½±å’Œåå°„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d07d5771ca855ef8e748277cd0871cf" align="middle">
<img src="https://picx.zhimg.com/v2-90b9ec5f040979e97828dc67f02fbdff" align="middle">
<img src="https://picx.zhimg.com/v2-e242927b8051c4c915f5a3839be735ce" align="middle">
<img src="https://picx.zhimg.com/v2-572c2281d16c52823f941a65e85b7745" align="middle">
<img src="https://picx.zhimg.com/v2-f394b432a7c3560b7231dc6abceaadcd" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-25/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a1532839f8b84003f880d4f986b9f33c" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Asynchronous Distributed ECME Algorithm for Matrix Variate Non-Gaussian   Responses
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-25/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-78d4c19bbd169cca5e356900d70e8fa4" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-25  Extreme Views 3DGS Filter for Novel View Synthesis from   Out-of-Distribution Camera Poses
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
