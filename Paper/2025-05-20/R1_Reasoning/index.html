<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-20  SoftCoT++ Test-Time Scaling with Soft Chain-of-Thought Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-44e5f538fd47a6380935eca2cf22ce5f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    86 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-20-更新"><a href="#2025-05-20-更新" class="headerlink" title="2025-05-20 更新"></a>2025-05-20 更新</h1><h2 id="SoftCoT-Test-Time-Scaling-with-Soft-Chain-of-Thought-Reasoning"><a href="#SoftCoT-Test-Time-Scaling-with-Soft-Chain-of-Thought-Reasoning" class="headerlink" title="SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning"></a>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</h2><p><strong>Authors:Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao</strong></p>
<p>Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model’s parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/xuyige/SoftCoT">https://github.com/xuyige/SoftCoT</a>. </p>
<blockquote>
<p>测试时缩放（TTS）是指通过推理时分配额外的计算资源来提高推理性能的方法，而不会改变模型的参数。虽然现有的TTS方法在离散标记空间内运行，通过生成更多的中间步骤来工作，但最近在Coconut和SoftCoT中的研究表明，在连续潜在空间中进行思考可以进一步提高推理性能。这种潜在的想法可以编码信息丰富的思考过程，而不会产生与自回归标记生成相关的信息丢失，因此对连续空间推理产生了更大的兴趣。与离散解码不同，离散解码通过重复采样可以探索多样化的推理路径，而连续空间中的潜在表示对于给定输入是固定的，这限制了多样化的探索，因为所有解码路径都来源于同一潜在想法。为了克服这一局限性，我们通过引入SoftCoT++来将SoftCoT扩展到测试时缩放范式，通过采用多种专业初始标记来扰动潜在的想法，并应用对比学习来促进软思维表示之间的多样性。在五个推理基准测试和两种不同的大型语言模型架构上的实验表明，SoftCoT++显著提升了SoftCoT的性能，并且优于SoftCoT的自一致性缩放。此外，它显示出与常规缩放技术（如自一致性）的强大兼容性。源代码可在<a target="_blank" rel="noopener" href="https://github.com/xuyige/SoftCoT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xuyige/SoftCoT上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11484v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Test-Time Scaling（TTS）方法在提高推理性能方面的作用，通过推理时在连续潜在空间进行思考，无需改变模型参数即可分配额外的计算资源。最新研究Coconut和SoftCoT展示了连续潜在空间思考的优势。为克服连续空间推理中多样化探索的限制，提出SoftCoT++方法，通过多个专用初始标记扰动潜在思想，应用对比学习促进软思想表示之间的多样性。实验表明，SoftCoT++在五个推理基准测试和两个不同LLM架构上显著提升了SoftCoT的性能，且与传统扩展技术如自我一致性扩展兼容性强。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Test-Time Scaling (TTS) 通过在推理时分配额外计算资源，提高模型推理性能，且无需改变模型参数。</li>
<li>连续潜在空间思考是最近研究的热点，其在提高推理性能方面具有优势，可以避免离散标记生成带来的信息损失。</li>
<li>SoftCoT++克服了连续空间推理中多样化探索的限制，通过引入多个专用初始标记扰动潜在思想，并应用对比学习促进多样性。</li>
<li>SoftCoT++在多个基准测试上显著提升了SoftCoT的性能，显示出强大的推理能力。</li>
<li>SoftCoT++与其他传统扩展技术（如自我一致性扩展）兼容性强。</li>
<li>SoftCoT++方法有助于促进多样化探索的思考路径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11484">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4efe663a379b9baddddf48687e2b2b1d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2382ea27746d2e90e5e24c55497ccfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec8f495f6f4e58fe912ea8227a6d7907.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HelpSteer3-Preference-Open-Human-Annotated-Preference-Data-across-Diverse-Tasks-and-Languages"><a href="#HelpSteer3-Preference-Open-Human-Annotated-Preference-Data-across-Diverse-Tasks-and-Languages" class="headerlink" title="HelpSteer3-Preference: Open Human-Annotated Preference Data across   Diverse Tasks and Languages"></a>HelpSteer3-Preference: Open Human-Annotated Preference Data across   Diverse Tasks and Languages</h2><p><strong>Authors:Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans, Yi Dong, Oleksii Kuchaiev</strong></p>
<p>Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/HelpSteer3#preference">https://huggingface.co/datasets/nvidia/HelpSteer3#preference</a> </p>
<blockquote>
<p>偏好数据集对于使用人类反馈强化学习（RLHF）训练通用领域、遵循指令的语言模型至关重要。每次后续数据发布都提高了对未来数据收集的期望，这意味着需要不断改进公开可用偏好数据的质量和多样性。为了解决这一需求，我们推出了HelpSteer3-Preference，这是一个采用宽松许可（CC-BY-4.0）的高质量、人工标注的偏好数据集，包含超过40,000个样本。这些样本涵盖了大型语言模型（LLM）的多样化现实世界应用，包括与STEM、编码和多语言场景相关的任务。使用HelpSteer3-Preference，我们训练的奖励模型（RM）在RM-Bench上达到顶级性能（82.4%），在JudgeBench上达到（73.7%）。这相比现有RM的最佳报告结果有了实质性的改进（绝对提高了约10%）。我们还展示了HelpSteer3-Preference如何应用于训练生成式RM，以及如何使用我们的RM将政策模型与RLHF对齐。数据集（CC-BY-4.0）：[<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/HelpSteer3#preference]">https://huggingface.co/datasets/nvidia/HelpSteer3#preference]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11475v1">PDF</a> 38 pages, 2 figures</p>
<p><strong>Summary</strong>：</p>
<p>为了帮助训练通用领域的指令遵循语言模型，推出了HelpSteer3-Preference偏好数据集，该数据集拥有超过4万样本，涵盖了多样化的现实世界应用场景。通过使用该数据集训练的奖励模型，在RM-Bench和JudgeBench上的表现有了显著提升。此外，也展示了如何将HelpSteer3-Preference应用于训练生成式奖励模型以及如何使策略模型与RLHF对齐。数据集遵循CC-BY-4.0许可协议，公开可用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>HelpSteer3-Preference是一个高质量的偏好数据集，样本数量超过4万，涵盖大型语言模型的多种现实应用场景。</li>
<li>数据集采用CC-BY-4.0许可协议，公开可用，满足了训练通用领域语言模型的需求。</li>
<li>通过使用HelpSteer3-Preference数据集训练的奖励模型在RM-Bench和JudgeBench上的表现有所提升，达到领先水平。</li>
<li>数据集能够帮助提高语言模型的性能并推动其发展，使得训练结果更符合人类偏好。</li>
<li>该数据集适用于训练生成式奖励模型，展示了其广泛的应用潜力。</li>
<li>通过策略模型与RLHF的对齐，强化了语言模型的适应性和智能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11475">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4ccb60b90803797e1276c09922790813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c5f0bb4a4ca8738632eafa4151dacc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-833675bcc9041121755a0dac9fce6afb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GODBench-A-Benchmark-for-Multimodal-Large-Language-Models-in-Video-Comment-Art"><a href="#GODBench-A-Benchmark-for-Multimodal-Large-Language-Models-in-Video-Comment-Art" class="headerlink" title="GODBench: A Benchmark for Multimodal Large Language Models in Video   Comment Art"></a>GODBench: A Benchmark for Multimodal Large Language Models in Video   Comment Art</h2><p><strong>Authors:Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang</strong></p>
<p>Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs’ abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at <a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025">https://github.com/stan-lei/GODBench-ACL2025</a>. </p>
<blockquote>
<p>视频评论艺术通过提供传达幽默、讽刺或情感共鸣的创意内容，增强用户参与度，这要求微妙而全面地把握文化和语境的细微差别。尽管多模态大型语言模型（MLLMs）和思维链（CoT）在STEM任务（如数学和编码）中展示了强大的推理能力，但它们仍难以生成如产生共鸣的笑话和富有洞察力的讽刺等创意表达。此外，现有基准测试受限于其有限的模式和不足的分类，阻碍了基于视频的评论艺术创作的全面创造力的探索。为了解决这些局限性，我们引入了GODBench，这是一个新颖的基准测试，它整合了视频和文本模式，系统地评估MLLMs创作评论艺术的能力。此外，受物理学中波动传播模式的启发，我们提出了思想涟漪（RoT）多步推理框架，旨在提高MLLMs的创造力。大量实验表明，现有的MLLMs和CoT方法在理解和生成创造性视频评论方面仍面临巨大挑战。相比之下，RoT提供了一种提高创造性写作的有效方法，突显其在推动基于MLLM的创造力方面的潜力。GODBench可在<a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/stan-lei/GODBench-ACL2025上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11436v1">PDF</a> 69 pages, 66 figures, accepted by ACL 2025</p>
<p><strong>摘要</strong><br>     视频评论艺术通过提供传达幽默、讽刺或情感共鸣的创意内容，增强了用户参与度。这要求深入全面地理解文化和语境的细微差别。虽然多模态大型语言模型（MLLMs）和思维链（CoT）在STEM任务（如数学和编程）中展现出强大的推理能力，但它们仍然难以生成如共鸣笑话和深刻讽刺等创意表达。为解决现有基准测试在视频评论艺术创造力评估方面的局限性，我们引入了GODBench基准测试，它整合了视频和文本模态，系统地评估MLLMs创作评论艺术的能力。此外，受物理中波动传播模式的启发，我们提出了思想涟漪（RoT）多步推理框架，旨在增强MLLMs的创造力。实验表明，现有MLLMs和CoT方法在理解和生成创意视频评论方面仍面临巨大挑战。相比之下，RoT提供了一种有效的改进创作的方法，突显其在推动基于MLLM的创造力方面的潜力。GODBench已在<a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/stan-lei/GODBench-ACL2025公开可用。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频评论艺术通过创意内容增强用户参与度，这需要理解文化和语境的细微差别。</li>
<li>多模态大型语言模型和思维链在STEM任务中表现出强大的推理能力，但在生成创意表达方面仍有困难。</li>
<li>现有基准测试在评估视频评论艺术的创造力方面存在局限性。</li>
<li>引入GODBench基准测试，整合视频和文本模态，评估MLLMs创作评论艺术的能力。</li>
<li>提出思想涟漪（RoT）多步推理框架，增强MLLMs的创造力。</li>
<li>实验显示，现有MLLMs和CoT方法在理解和生成创意视频评论方面面临挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11436">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ad8734da6667e5e9e4115b0aeb767978.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-716e21106fd9c6f5ccd5b15c0537ccf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c24b0d34eff9882e6b379088fbcd1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f784b33ac157a05a7f830f7d0aba1e6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e58fc377c680b0d0c4374eecb840f3a5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="When-Thinking-Fails-The-Pitfalls-of-Reasoning-for-Instruction-Following-in-LLMs"><a href="#When-Thinking-Fails-The-Pitfalls-of-Reasoning-for-Instruction-Following-in-LLMs" class="headerlink" title="When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following   in LLMs"></a>When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following   in LLMs</h2><p><strong>Authors:Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal</strong></p>
<p>Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies. </p>
<blockquote>
<p>推理增强大型语言模型（RLLMs）无论是否经过明确的推理训练或通过思维链（CoT）进行提示，在许多复杂的推理任务上都达到了最先进的性能。然而，我们发现了一个令人惊讶且以前被忽视的现象：明确的CoT推理会显著降低指令执行准确性。我们在两个基准测试上对15个模型进行了评估：IFeval（具有简单、可验证的规则约束）和ComplexBench（具有复杂、组合约束），我们始终观察到当应用CoT提示时性能下降。通过大规模案例研究和基于注意力的分析，我们识别出推理有助于（例如，格式化或词汇精度）或有害（例如，忽视简单约束或引入不必要内容）的常见模式。我们提出一个指标，约束注意力，来量化生成过程中的模型关注点，并表明CoT推理通常会分散对指令相关标记的注意力。为了减轻这些影响，我们引入并评估了四种策略：上下文学习、自我反思、自我选择性推理和分类器选择性推理。我们的结果表明，选择性推理策略，特别是分类器选择性推理，可以大幅度恢复丢失的性能。据我们所知，这是第一项工作，系统地暴露了推理在指令执行中的失败并提供了实用的缓解策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11423v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在本文中，研究探讨了明确连贯推理（CoT）在大语言模型中对指令遵循准确性的潜在负面影响。作者们在多种模型与基准测试中发现，使用CoT推理时模型性能显著下降。对此，研究者进行了大规模的案例研究并引入了一个新的评价指标来量化模型生成过程中的关注焦点。文章提出了四种策略来缓解推理导致的注意力分散问题，其中选择性推理策略特别是分类器选择性推理在恢复模型性能上取得了显著效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RLLMs在复杂推理任务上表现出卓越性能，但发现明确的连贯推理（CoT）会显著降低指令遵循的准确性。</li>
<li>在两个基准测试中（IFEval和ComplexBench），使用CoT推理时模型性能下降。</li>
<li>通过大规模案例研究和注意力分析，识别出推理过程帮助或阻碍的不同模式。</li>
<li>提出新的评价指标来衡量模型在生成过程中的注意力焦点，发现CoT推理导致模型在指令相关标记上的注意力分散。</li>
<li>引入四种策略来缓解注意力分散问题，包括上下文学习、自我反思和自我选择性推理以及分类器选择性推理等。</li>
<li>分类器选择性推理策略在恢复模型性能方面尤为有效。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11423">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-458d5262f1a26b34ee993d185adf5b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5911c56e30adf0b0e1c76535f2292864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-450d0f386565be5c00036bc98f39f76b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Visual-Planning-Let’s-Think-Only-with-Images"><a href="#Visual-Planning-Let’s-Think-Only-with-Images" class="headerlink" title="Visual Planning: Let’s Think Only with Images"></a>Visual Planning: Let’s Think Only with Images</h2><p><strong>Authors:Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić</strong></p>
<p>Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference. </p>
<blockquote>
<p>近年来，大型语言模型（LLM）及其多模态扩展（MLLM）的进展极大地提高了跨不同任务的机器推理能力。然而，这些模型主要依赖纯文本作为表达和结构化推理的媒介，即使存在视觉信息也是如此。在这项工作中，我们主张语言并不总是最自然或最有效的推理方式，特别是在涉及空间和几何信息的任务中。受此启发，我们提出了一种新的范式——视觉规划，它能够通过纯粹的视觉表示进行规划，独立于文本。在这种范式中，规划是通过一系列图像执行的，这些图像在视觉领域编码了逐步推理，就像人类如何勾画或可视化未来行动一样。我们引入了一种新型强化学习框架——通过强化学习的视觉规划（VPRL），借助GRPO对大型视觉模型进行后训练，导致在具有代表性的视觉导航任务、FrozenLake、迷宫和MiniBehavior中的规划能力得到实质性提高。我们的视觉规划范式在只进行文本空间推理的规划中表现优于所有其他规划变体。我们的结果确立了视觉规划作为语言基础推理的一种可行且有前途的替代方案，为受益于直观、基于图像推理的任务开辟了新途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11409v1">PDF</a> 10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables   including references and appendices)</p>
<p><strong>Summary</strong></p>
<p>近期大型语言模型（LLM）和多模态扩展模型（MLLM）的进步极大地提高了跨不同任务的机器推理能力。然而，这些模型主要通过文本表达和结构推理，即使存在视觉信息也是如此。本文提出，在涉及空间和几何信息的任务中，语言可能并非最自然或有效的推理方式。为此，提出了一种新的推理方式——视觉规划，通过纯粹的视觉表示进行规划，独立于文本。视觉规划通过图像序列执行规划，这些图像编码了视觉领域的逐步推理，类似于人类如何绘制或可视化未来行动。通过强化学习框架和GRPO后训练大型视觉模型，实现了视觉规划的实质性改进，并在代表性视觉导航任务中取得了良好表现。本文的研究结果证明了视觉规划作为一种可行且有前途的替代语言基础推理方法，为受益于直观图像推理的任务开辟了新途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）和多模态扩展模型（MLLM）在机器推理方面取得了显著进展。</li>
<li>现有模型主要依赖文本进行表达和结构化推理，即使存在视觉信息时也是如此。</li>
<li>在涉及空间和几何信息的任务中，语言可能不是最自然或有效的推理方式。</li>
<li>提出了新的推理方法——视觉规划，该方法通过纯粹的视觉表示进行规划，不依赖于文本。</li>
<li>视觉规划通过图像序列执行，这些图像编码了类似于人类如何绘制或可视化未来行动的逐步推理。</li>
<li>强化学习框架和GRPO后训练大型视觉模型，实现了视觉规划的实质性改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11409">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-39e2fae2fb6e4a2b8d5c54c5ee658bdc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b087dca392b82352040f39806ccc878.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-003cb4251f762d33ea57f5b138daf6c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45836365d67a9715f197785ef545cf09.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Patho-R1-A-Multimodal-Reinforcement-Learning-Based-Pathology-Expert-Reasoner"><a href="#Patho-R1-A-Multimodal-Reinforcement-Learning-Based-Pathology-Expert-Reasoner" class="headerlink" title="Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert   Reasoner"></a>Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert   Reasoner</h2><p><strong>Authors:Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu</strong></p>
<p>Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose PathoCLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both PathoCLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: <a target="_blank" rel="noopener" href="https://github.com/Wenchuan-Zhang/Patho-R1">https://github.com/Wenchuan-Zhang/Patho-R1</a>. </p>
<blockquote>
<p>近期，视觉语言模型（VLMs）的进展为一般医疗领域带来了广泛的进步。然而，病理学仍然是一个更具挑战性的子领域。当前的病理学特定VLMs在诊断准确性和推理合理性方面存在局限性。这种缺陷在很大程度上归因于当前病理学数据集的性质，这些数据集主要由缺乏现实世界病理学家所采用的深度和结构化诊断范式的图像描述对组成。在这项研究中，我们利用病理学教材和现实世界病理学专家来构建高质量、以推理为导向的数据集。在此基础上，我们引入了Patho-R1，这是一个基于多模式强化学习的病理学推理器，通过三阶段管道进行训练：（1）在350万图像文本对上持续预训练，以注入知识；（2）在50万个高质量的思维链样本上进行监督微调，以激励推理；（3）使用集团相对策略优化和解耦剪辑以及动态采样策略优化的强化学习策略，以提高多模式推理质量。为了进一步评估我们数据集的对齐质量，我们提出了在同一图像-标题语料库上训练的PathoCLIP，该语料库用于持续预训练。综合实验结果表明，PathoCLIP和Patho-R1在广泛的病理学相关任务中表现稳健，包括零样本分类、跨模态检索、视觉问答和多选题。我们的项目可在Patho-R1仓库中找到：<a target="_blank" rel="noopener" href="https://github.com/Wenchuan-Zhang/Patho-R1%E3%80%82">https://github.com/Wenchuan-Zhang/Patho-R1。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11404v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了基于最新进展的医学影像与语言模型技术在病理学领域的广泛应用及存在的挑战。研究团队利用病理教科书和专家构建了高质量、以推理为导向的数据集，并基于此推出了Patho-R1模型，该模型通过三个阶段进行训练，包括知识灌输、推理激励和多模态推理质量优化。同时，为了评估数据集的质量，研究团队还提出了PathoCLIP模型。实验结果显示这两个模型在多种病理学任务中表现优异。项目的相关信息可通过Patho-R1仓库获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学影像与语言模型技术在病理学领域的应用面临挑战，尤其在诊断准确性和推理合理性方面存在局限。</li>
<li>当前病理数据集缺乏深度和结构化诊断范式，导致模型性能受限。</li>
<li>研究团队利用病理教科书和专家构建高质量、以推理为导向的数据集，以解决现有问题。</li>
<li>Patho-R1模型通过三个阶段进行训练：知识灌输、推理激励和多模态推理质量优化。</li>
<li>PathoCLIP模型的提出用于评估数据集质量，其训练基于相同的图像-文本对用于持续预训练。</li>
<li>实验结果显示PathoCLIP和Patho-R1模型在多种病理学任务中表现优秀，包括零样本分类、跨模态检索、视觉问答和多项选择题等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11404">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2a80232b582f35b48cc5c5839d7378f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-933359ff6a19ce46474a6f31df7bb350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afdbd0559742207ec25a6342125ec9c4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Critical-Questions-Generation-A-Challenging-Reasoning-Task-for-Large-Language-Models"><a href="#Benchmarking-Critical-Questions-Generation-A-Challenging-Reasoning-Task-for-Large-Language-Models" class="headerlink" title="Benchmarking Critical Questions Generation: A Challenging Reasoning Task   for Large Language Models"></a>Benchmarking Critical Questions Generation: A Challenging Reasoning Task   for Large Language Models</h2><p><strong>Authors:Banca Calvo Figueras, Rodrigo Agerri</strong></p>
<p>The task of Critical Questions Generation (CQs-Gen) aims to foster critical thinking by enabling systems to generate questions that expose assumptions and challenge the reasoning in arguments. Despite growing interest in this area, progress has been hindered by the lack of suitable datasets and automatic evaluation standards. This work presents a comprehensive approach to support the development and benchmarking of systems for this task. We construct the first large-scale manually-annotated dataset. We also investigate automatic evaluation methods and identify a reference-based technique using large language models (LLMs) as the strategy that best correlates with human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline while showcasing the difficulty of the task. Data, code, and a public leaderboard are provided to encourage further research not only in terms of model performance, but also to explore the practical benefits of CQs-Gen for both automated reasoning and human critical thinking. </p>
<blockquote>
<p>批判性问题生成（CQs-Gen）的任务旨在通过使系统能够生成揭示假设并挑战论证推理的问题来培养批判性思维。尽管这个领域日益受到关注，但由于缺乏合适的数据集和自动评估标准，进展受到了阻碍。这项工作提出了一个全面的方法来支持该任务的系统开发和评估。我们构建了第一个大规模手动注释的数据集。我们还研究了自动评估方法，并确定了使用大型语言模型（LLM）的参考基准技术是与人判断最相关的策略。我们对11个LLM进行的零样本评估建立了强大的基准线，同时展示了该任务的难度。为了鼓励进一步研究，不仅是从模型性能的角度，还从探索CQs-Gen对自动化推理和人类批判性思维的实际益处，我们提供了数据、代码和公开排行榜。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11341v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了批判性问题生成（CQs-Gen）的任务目标，即培养系统的批判性思维能力，生成能够揭露假设和质疑论证推理的问题。尽管该领域日益受到关注，但由于缺乏合适的数据集和自动评估标准，进展受到了阻碍。本研究提出了一种全面的方法来支持该任务的系统开发和基准测试。本研究构建了首个大规模手动标注数据集，并探讨了自动评估方法，确定了基于大型语言模型（LLMs）的参考技术作为与人类判断最相关的策略。对11种LLMs的零样本评估建立了强大的基准线，同时展示了该任务的难度。提供数据、代码和公开排行榜，以鼓励模型性能方面的进一步研究，并探索CQs-Gen对自动化推理和人类批判性思维的实际益处。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>批判性问题生成（CQs-Gen）旨在通过生成问题培养系统的批判性思维能力。</li>
<li>缺少合适的数据集和自动评估标准是阻碍该领域进展的主要原因。</li>
<li>研究构建了首个大规模手动标注数据集来支持CQs-Gen任务。</li>
<li>自动评估方法被探讨，其中基于大型语言模型（LLMs）的参考技术被确定为与人类判断最相关的策略。</li>
<li>对11种LLMs的零样本评估显示了该任务的难度。</li>
<li>提供数据、代码和公开排行榜以鼓励更多研究，不仅关注模型性能，还关注CQs-Gen对自动化推理和人类批判性思维的益处。</li>
<li>该研究为CQs-Gen任务的发展提供了全面支持，包括数据集、评估方法和公开平台。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11341">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd3ea7fad8589fe8fe258bf4d7508ab3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55aca43b4d15c49d90873ab83e4e8df3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f75b14ea7300c4ac475badd73ada6fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecf86bc79021382d2f309f84d28b3298.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Search-and-Refine-During-Think-Autonomous-Retrieval-Augmented-Reasoning-of-LLMs"><a href="#Search-and-Refine-During-Think-Autonomous-Retrieval-Augmented-Reasoning-of-LLMs" class="headerlink" title="Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning   of LLMs"></a>Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning   of LLMs</h2><p><strong>Authors:Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang</strong></p>
<p>Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new &#96;&#96;search-and-refine-during-think’’ paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively. </p>
<blockquote>
<p>大型语言模型已经展现出令人印象深刻的推理能力，但其本质上受到知识库的限制。检索增强推理通过允许大型语言模型查询外部资源来缓解这一限制，但现有方法经常检索到不相关或嘈杂的信息，阻碍了准确的推理。在本文中，我们提出了AutoRefine，这是一种采用新型“思考过程中的搜索与完善”范式的强化学习后训练框架。AutoRefine在连续的搜索调用之间引入了明确的知识完善步骤，使模型能够迭代地过滤、提炼和整理证据，然后生成答案。此外，我们通过使用群体相对策略优化，结合了定制的检索特定奖励和答案正确性奖励。在单跳和多跳问答基准测试上的实验表明，AutoRefine显著优于现有方法，特别是在复杂的多跳推理场景中。详细分析表明，AutoRefine能够进行频繁的高质量搜索，并能有效地合成证据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11277v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为AutoRefine的强化学习后训练框架，采用新的“边搜索边细化思考”模式来缓解大型语言模型在推理方面的固有局限性。该框架在连续搜索调用之间引入明确的知识细化步骤，使模型能够迭代地过滤、提炼和整理证据，生成答案。同时，通过结合针对检索的特定奖励和基于答案正确性的奖励，使用群组相对策略优化。在单跳和多跳问答基准测试上的实验表明，AutoRefine显著优于现有方法，特别是在复杂的多跳推理场景中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型虽然具备令人印象深刻的推理能力，但其知识库的限制影响了性能。</li>
<li>检索增强推理通过允许语言模型查询外部资源来减轻这一限制。</li>
<li>现有方法经常检索不相关或嘈杂的信息，阻碍准确推理。</li>
<li>AutoRefine是一个强化学习后训练框架，采用“边搜索边细化思考”模式来提高检索质量。</li>
<li>AutoRefine在连续搜索之间引入知识细化步骤，实现证据的迭代过滤、提炼和整理。</li>
<li>通过结合检索特定奖励和答案正确性奖励，使用群组相对策略优化来提高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11277">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c1b7f6e97f2feb3313c06612ab51b480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c630524472572e4123adc656bbb11014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d42cacf66dffd249ce76343aae3313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4446c1de7272a2a37d42415d65e1827.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5849377557489b948ec35a0555161fd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Is-PRM-Necessary-Problem-Solving-RL-Implicitly-Induces-PRM-Capability-in-LLMs"><a href="#Is-PRM-Necessary-Problem-Solving-RL-Implicitly-Induces-PRM-Capability-in-LLMs" class="headerlink" title="Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability   in LLMs"></a>Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability   in LLMs</h2><p><strong>Authors:Zhangying Feng, Qianglong Chen, Ning Lu, Yongqian Li, Siqi Cheng, Shuangmu Peng, Duyu Tang, Shengcai Liu, Zhirui Zhang</strong></p>
<p>The development of reasoning capabilities represents a critical frontier in large language models (LLMs) research, where reinforcement learning (RL) and process reward models (PRMs) have emerged as predominant methodological frameworks. Contrary to conventional wisdom, empirical evidence from DeepSeek-R1 demonstrates that pure RL training focused on mathematical problem-solving can progressively enhance reasoning abilities without PRM integration, challenging the perceived necessity of process supervision. In this study, we conduct a systematic investigation of the relationship between RL training and PRM capabilities. Our findings demonstrate that problem-solving proficiency and process supervision capabilities represent complementary dimensions of reasoning that co-evolve synergistically during pure RL training. In particular, current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To address this limitation, we propose Self-PRM, an introspective framework in which models autonomously evaluate and rerank their generated solutions through self-reward mechanisms. Although Self-PRM consistently improves the accuracy of the benchmark (particularly with larger sample sizes), analysis exposes persistent challenges: The approach exhibits low precision (&lt;10%) on difficult problems, frequently misclassifying flawed solutions as valid. These analyses underscore the need for continued RL scaling to improve reward alignment and introspective accuracy. Overall, our findings suggest that PRM may not be essential for enhancing complex reasoning, as pure RL not only improves problem-solving skills but also inherently fosters robust PRM capabilities. We hope these findings provide actionable insights for building more reliable and self-aware complex reasoning models. </p>
<blockquote>
<p>推理能力的发展在大规模语言模型（LLM）研究中代表了关键的前沿领域，其中强化学习（RL）和过程奖励模型（PRM）已成为主要的方法论框架。与常识相反，来自DeepSeek-R1的实证证据表明，专注于数学问题解决的纯RL训练可以逐步增强推理能力，而无需整合PRM，这挑战了过程监督的必要性。在这项研究中，我们对RL训练和PRM能力之间的关系进行了系统的调查。我们的研究结果表明，问题解决能力和过程监督能力代表了推理的互补维度，在纯RL训练过程中协同演化。特别是，当应用于最新模型（如DeepSeek-R1和QwQ-32B）时，当前的PRM表现甚至不如多数投票等简单基线。为了解决这一局限性，我们提出了Self-PRM，这是一种内省框架，模型通过自我奖励机制自主评估并重新排序其生成的解决方案。尽管Self-PRM始终提高了基准测试的准确性（特别是样本量较大时），但分析表明仍存在持久挑战：该方法在难题上的精度较低（&lt;10%），经常将错误的解决方案错误地分类为有效。这些分析强调，为了改善奖励对齐和内省精度，需要继续进行RL扩展。总的来说，我们的研究结果表明，PRM可能并不是增强复杂推理能力的关键，因为纯RL不仅提高了问题解决技能，而且内在地培养了稳健的PRM能力。我们希望这些发现能为构建更可靠、更自觉复杂推理模型提供实用见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11227v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于深度寻求R1模型的实证研究，强化学习（RL）在提升语言模型的推理能力方面展现出潜力，即使不结合过程奖励模型（PRM）也能增强推理能力。研究系统调查了RL训练和PRM能力之间的关系，发现解决问题的能力和过程监督能力代表了协同演化的互补推理维度。针对PRM在先进模型上的性能不佳问题，提出了自主评估解决方案的Self-PRM框架。虽然此框架改进了基准测试准确性，但仍存在精度低和误判问题。研究认为，PRM可能并非增强复杂推理能力的关键，纯RL不仅提升解题能力，还内在培养稳健的PRM能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习（RL）在大型语言模型（LLM）的推理能力发展中起到关键作用。</li>
<li>实证研究显示，纯RL训练能提升数学问题解决能力，挑战了过程监督的必要性。</li>
<li>问题解决能力与过程监督能力是互补的推理维度，协同演化。</li>
<li>当前PRM在先进模型上的表现不佳，低于简单基线如多数投票。</li>
<li>提出的Self-PRM框架旨在自主评估解决方案，虽改进基准测试准确性，但存在挑战。</li>
<li>Self-PRM在低精度和误判问题上仍有不足，需要更多RL扩展来提高奖励对齐和自省准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11227">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3578b6e81a922865407cf3447a34c59b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13a2342ccfd59571e78db1fac1d3ec81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73603b8dea5f2db2069751ab99600c87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2121916a28af1e52b13272cc484dd303.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd95c629cdd9b8989b2e00cb17134d5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CompAlign-Improving-Compositional-Text-to-Image-Generation-with-a-Complex-Benchmark-and-Fine-Grained-Feedback"><a href="#CompAlign-Improving-Compositional-Text-to-Image-Generation-with-a-Complex-Benchmark-and-Fine-Grained-Feedback" class="headerlink" title="CompAlign: Improving Compositional Text-to-Image Generation with a   Complex Benchmark and Fine-Grained Feedback"></a>CompAlign: Improving Compositional Text-to-Image Generation with a   Complex Benchmark and Fine-Grained Feedback</h2><p><strong>Authors:Yixin Wan, Kai-Wei Chang</strong></p>
<p>State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuest’s feedback as preference signals to improve diffusion models’ compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches. </p>
<blockquote>
<p>当前最先进的T2I模型已经能够根据文本提示生成高分辨率图像。然而，它们在准确描绘包含多个对象、属性和空间关系的组合场景时仍然面临挑战。我们推出了CompAlign，这是一个以评估3D空间关系描绘能力为重点的具有挑战性的基准测试，旨在评估和提高组合图像生成模型的性能。CompAlign包含900个复杂的跨主体图像生成提示，结合了数值和3D空间关系以及不同的属性绑定。我们的基准测试非常具有挑战性，融入了3个及以上生成主体的生成任务，并带有复杂的3D空间关系。此外，我们提出了CompQuest，这是一个可解释且准确的评估框架，它将复杂的提示分解成原子子问题，然后利用MLLM对模型生成图像中每个生成元素方面的正确性提供精细的二元反馈。这实现了生成图像与组合提示之间对齐的精确量化。此外，我们提出了一个使用CompQuest反馈作为偏好信号的对齐框架，以提高扩散模型的组合图像生成能力。使用可调整的每张图像偏好，我们的方法易于扩展，并且针对不同任务具有灵活性。对9个T2I模型的评估表明：（1）模型在具有更复杂3D空间配置的组合任务上面临更大的挑战；（2）开源可访问模型和封闭源代码商业模型之间存在明显的性能差距。关于使用CompAlign进行模型对齐的进一步实证研究产生了令人鼓舞的结果：对齐后的扩散模型在组合准确性方面取得了显著改进，特别是在复杂的生成任务上，超越了以前的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11178v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本描述了一种名为CompAlign的评估基准，用于评估和改进文本到图像模型在组合图像生成方面的表现。该基准强调对三维空间关系的描绘能力，包含挑战性的多主题图像生成提示。同时，提出了一种名为CompQuest的评估框架，可以精确量化生成图像与组合提示之间的对齐程度。此外，还提出了一种使用CompQuest反馈作为偏好信号的对齐框架，以提高扩散模型的组合图像生成能力。对现有模型的评估显示，复杂的三维空间配置组合任务是一大挑战，开源可访问模型与闭源商业模型之间存在性能差距。使用CompAlign进行模型对齐的进一步研究显示出可喜的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CompAlign是一个评估基准，强调对三维空间关系的描绘能力，用于评估和改进文本到图像模型在组合场景图像生成方面的性能。</li>
<li>CompAlign包含挑战性的多主题图像生成提示，这些提示结合了数字和三维空间关系以及不同的属性绑定。</li>
<li>CompQuest是一种评估框架，可以精确量化生成图像与组合提示之间的对齐程度，通过将复杂的提示分解为原子子问题并利用大型语言模型提供精细的二元反馈来实现。</li>
<li>提出的对齐框架使用CompQuest的反馈作为偏好信号，以提高扩散模型的组合图像生成能力，且该方法可调整、易于扩展，适用于不同任务。</li>
<li>对现有文本到图像模型的评估显示，处理具有复杂三维空间配置的图像生成任务是一大挑战。</li>
<li>开源可访问模型与闭源商业模型在组合图像生成任务上存在性能差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4dd40edf2cbaffb4a7709cc361e0b38c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c7baf423cbd584f86124d5252244cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37436045e6041dc7c5141960bd962469.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1385a7e7509115533a2a849a2acdc621.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Real-Time-Verification-of-Embodied-Reasoning-for-Generative-Skill-Acquisition"><a href="#Real-Time-Verification-of-Embodied-Reasoning-for-Generative-Skill-Acquisition" class="headerlink" title="Real-Time Verification of Embodied Reasoning for Generative Skill   Acquisition"></a>Real-Time Verification of Embodied Reasoning for Generative Skill   Acquisition</h2><p><strong>Authors:Bo Yue, Shuqi Guo, Kaiyu Hu, Chujiao Wang, Benyou Wang, Kui Jia, Guiliang Liu</strong></p>
<p>Generative skill acquisition enables embodied agents to actively learn a scalable and evolving repertoire of control skills, crucial for the advancement of large decision models. While prior approaches often rely on supervision signals from generalist agents (e.g., LLMs), their effectiveness in complex 3D environments remains unclear; exhaustive evaluation incurs substantial computational costs, significantly hindering the efficiency of skill learning. Inspired by recent successes in verification models for mathematical reasoning, we propose VERGSA (Verifying Embodied Reasoning in Generative Skill Acquisition), a framework that systematically integrates real-time verification principles into embodied skill learning. VERGSA establishes 1) a seamless extension from verification of mathematical reasoning into embodied learning by dynamically incorporating contextually relevant tasks into prompts and defining success metrics for both subtasks and overall tasks, and 2) an automated, scalable reward labeling scheme that synthesizes dense reward signals by iteratively finalizing the contribution of scene configuration and subtask learning to overall skill acquisition. To the best of our knowledge, this approach constitutes the first comprehensive training dataset for verification-driven generative skill acquisition, eliminating arduous manual reward engineering. Experiments validate the efficacy of our approach: 1) the exemplar task pool improves the average task success rates by 21%, 2) our verification model boosts success rates by 24% for novel tasks and 36% for encountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification quality. </p>
<blockquote>
<p>生成技能获取使实体代理能够主动学习可伸缩和不断发展的控制技能组合，这对于大型决策模型的进步至关重要。虽然之前的方法经常依赖于通用代理（例如大型语言模型）的监督信号，但它们在复杂的3D环境中的有效性尚不清楚；详尽的评估会产生巨大的计算成本，从而严重阻碍技能学习的效率。受最近数学推理验证模型成功的启发，我们提出了VERGSA（生成技能获取中的实体验证），这是一个将实时验证原则系统地融入实体技能学习框架。VERGSA建立了1）从数学推理验证无缝扩展到实体学习的机制，通过动态将上下文相关任务纳入提示并定义子任务和总体任务的成功指标，以及2）一种自动化、可伸缩的奖励标签方案，通过迭代确定场景配置和子任务学习对整体技能获取的贡献，从而合成密集的奖励信号。据我们所知，这种方法构成了以验证为驱动生成技能获取的首个综合训练数据集，消除了繁琐的手动奖励工程。实验验证了我们的方法的有效性：1）示例任务池提高了平均任务成功率21%，2）我们的验证模型提高了新任务的成功率24%，对已遇到的任务成功率提高36%，3）在验证质量方面优于以大型语言模型为评判员的基准线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11175v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了生成技能获取（Generative Skill Acquisition）在实体代理（embodied agents）中的应用。文章指出传统方法依赖于通用代理（如大型语言模型LLMs）的监督信号，在复杂的三维环境中效果不佳且评估成本高昂。为此，受数学推理验证模型成功的启发，文章提出了VERGSA框架，该框架将实时验证原则系统地集成到实体技能学习中。VERGSA框架不仅实现了数学推理验证到实体学习的无缝扩展，还通过自动、可扩展的奖励标注方案，合成密集奖励信号，简化了密集奖励信号的生成过程。实验验证，VERGSA提高了任务成功率并优于LLM作为评判员的基准测试。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成技能获取允许实体代理主动学习可拓展和不断发展的控制技能，这对大型决策模型的发展至关重要。</li>
<li>传统方法依赖通用代理的监督信号在复杂三维环境中的效果尚不清楚，且全面评估会带来巨大的计算成本。</li>
<li>VERGSA框架将实时验证原则集成到实体技能学习中，实现了从数学推理验证到实体学习的无缝扩展。</li>
<li>VERGSA框架通过动态融入上下文相关任务到提示中并定义子任务和总体任务的成功指标，增强了技能学习的效率。</li>
<li>VERGSA框架提供了一种自动、可扩展的奖励标注方案，通过迭代确定场景配置和子任务学习对整体技能获取的贡献，简化了密集奖励信号的生成。</li>
<li>VERGSA框架构成了一个全面的训练数据集，用于验证驱动生成技能获取，消除了繁琐的手动奖励工程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11175">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1ba42f4b646e412f876dd3927ff730af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61d3c798aac330c45b98228cc0469fb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff2551c4a097ae256a4af16f2fb36bb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b33d4c9e3584d370660f88c2a996f2de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66561176e94bbca5ee54e18b9197e17f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c6cc3c3e72dd601b11e1019a00b574ce.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GraphOracle-A-Foundation-Model-for-Knowledge-Graph-Reasoning"><a href="#GraphOracle-A-Foundation-Model-for-Knowledge-Graph-Reasoning" class="headerlink" title="GraphOracle: A Foundation Model for Knowledge Graph Reasoning"></a>GraphOracle: A Foundation Model for Knowledge Graph Reasoning</h2><p><strong>Authors:Enjun Du, Siyi Liu, Yongqi Zhang</strong></p>
<p>Foundation models have demonstrated remarkable capabilities across various domains, but developing analogous models for knowledge graphs presents unique challenges due to their dynamic nature and the need for cross-domain reasoning. To address these issues, we introduce \textbf{\textsc{GraphOracle}}, a relation-centric foundation model that unifies reasoning across knowledge graphs by converting them into Relation-Dependency Graphs (RDG), explicitly encoding compositional patterns with fewer edges than prior methods. A query-dependent attention mechanism is further developed to learn inductive representations for both relations and entities. Pre-training on diverse knowledge graphs, followed by minutes-level fine-tuning, enables effective generalization to unseen entities, relations, and entire graphs. Through comprehensive experiments on 31 diverse benchmarks spanning transductive, inductive, and cross-domain settings, we demonstrate consistent state-of-the-art performance with minimal adaptation, improving the prediction performance by up to 35% compared to the strongest baselines. </p>
<blockquote>
<p>基础模型在各个领域表现出了卓越的能力，但在知识图谱中开发类似模型却面临着独特的挑战，因为它们具有动态性和跨域推理的需求。为了解决这些问题，我们引入了关系中心的基础模型<strong>GraphOracle</strong>，它通过知识图谱转换为关系依赖图（RDG），明确编码组合模式（相比于先前的知识图谱有更少的边），从而统一了知识图谱中的推理过程。此外，我们还开发了一种查询相关的注意力机制，用于学习关系和实体的归纳表示。通过在多样的知识图谱上进行预训练，然后在短时间内进行微调，可以实现未知实体、关系和整个知识图谱的有效泛化。通过覆盖转导、归纳和跨域设置下的多种不同基准测试实验的综合实验验证，我们的模型表现出一致的最佳性能，即使最小程度的适应也能达到提升预测性能至高达35%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11125v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>知识图谱的跨域推理面临诸多挑战，如知识图谱的动态性和复杂性。为解决这些问题，我们提出了关系中心化基础模型GraphOracle，通过将知识图谱转换为关系依赖图（RDG）来统一跨知识图谱的推理。该模型能够显式编码组合模式，并减少边缘数量。此外，我们开发了一种查询依赖注意力机制，用于学习和表示关系和实体的归纳表示。通过在不同知识图谱上的预训练和在几分钟内的微调，模型可以有效地泛化到未见过的实体、关系和整个图谱。在涵盖归纳、归纳和跨域设置的31个不同基准测试上进行的综合实验表明，该模型在最小的适应情况下达到了最先进的性能，预测性能提高了高达35%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GraphOracle是一个关系中心化的基础模型，用于解决知识图谱的跨域推理挑战。</li>
<li>通过将知识图谱转换为关系依赖图（RDG），GraphOracle能够统一跨知识图谱的推理。</li>
<li>GraphOracle显式编码组合模式并减少边缘数量。</li>
<li>模型采用查询依赖注意力机制来学习和表示关系和实体的归纳表示。</li>
<li>模型通过预训练在不同知识图谱上进行训练，并通过微调适应未见过的实体、关系和整个图谱。</li>
<li>GraphOracle在多个基准测试中表现出卓越的性能，预测性能提高了高达35%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11125">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-858e4bfac7c2fe3898d029619d04337f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c55432bcc3e92429dd699f951e53b03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8ae8dca90d5f0f660b16ce4896d182f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Navigating-the-Alpha-Jungle-An-LLM-Powered-MCTS-Framework-for-Formulaic-Factor-Mining"><a href="#Navigating-the-Alpha-Jungle-An-LLM-Powered-MCTS-Framework-for-Formulaic-Factor-Mining" class="headerlink" title="Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic   Factor Mining"></a>Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic   Factor Mining</h2><p><strong>Authors:Yu Shi, Yitong Duan, Jian Li</strong></p>
<p>Alpha factor mining is pivotal in quantitative investment for identifying predictive signals from complex financial data. While traditional formulaic alpha mining relies on human expertise, contemporary automated methods, such as those based on genetic programming or reinforcement learning, often suffer from search inefficiency or yield poorly interpretable alpha factors. This paper introduces a novel framework that integrates Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS) to overcome these limitations. Our approach leverages the LLM’s instruction-following and reasoning capability to iteratively generate and refine symbolic alpha formulas within an MCTS-driven exploration. A key innovation is the guidance of MCTS exploration by rich, quantitative feedback from financial backtesting of each candidate factor, enabling efficient navigation of the vast search space. Furthermore, a frequent subtree avoidance mechanism is introduced to bolster search efficiency and alpha factor performance. Experimental results on real-world stock market data demonstrate that our LLM-based framework outperforms existing methods by mining alphas with superior predictive accuracy, trading performance, and improved interpretability, while offering a more efficient solution for formulaic alpha mining. </p>
<blockquote>
<p>阿尔法因子挖掘在量化投资中起着关键作用，能够从复杂的金融数据中识别出预测信号。虽然传统的公式化阿尔法挖掘依赖于人工专家经验，但当前的自动化方法，如基于遗传编程或强化学习的方法，常常面临搜索效率低下或产生的阿尔法因子解释性较差的问题。本文介绍了一个新型框架，它结合了大型语言模型（LLM）和蒙特卡洛树搜索（MCTS）来克服这些限制。我们的方法利用LLM的指令遵循能力和推理能力，在MCTS驱动的探索中迭代生成和细化符号阿尔法公式。一个关键的创新之处在于，MCTS探索受到来自每个候选因子的金融回测丰富定量反馈的引导，能够在庞大的搜索空间中实现高效导航。此外，还引入了一种常见的子树避免机制，以提高搜索效率和阿尔法因子性能。在真实股票市场数据上的实验结果表明，我们基于LLM的框架通过挖掘具有更高预测精度、交易性能和改善解释性的阿尔法因子，表现出优于现有方法的效果，同时为公式化阿尔法挖掘提供了更有效的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11122v1">PDF</a> 30 pages</p>
<p><strong>Summary</strong></p>
<p>在金融投资领域，挖掘阿尔法因子是关键环节之一，如何从复杂的数据中寻找预测信号尤为关键。传统的方法依赖于人为经验，而现代自动化方法如基于遗传编程或强化学习的方法常常面临搜索效率低下或产生的阿尔法因子难以解释的问题。本文提出了一种新型框架，融合了大型语言模型（LLM）与蒙特卡洛树搜索（MCTS），以克服这些难题。该方法利用LLM的指令遵循能力和推理能力，在MCTS驱动的搜索过程中迭代生成和精炼符号阿尔法公式。通过金融回测对候选因子进行定量反馈指导MCTS的搜索，实现了对广阔搜索空间的高效导航。此外，引入了一种频繁子树避免机制以提高搜索效率和阿尔法因子的性能。在真实股市数据上的实验结果表明，基于LLM的框架在挖掘阿尔法因子时表现出优异性能，具有更高的预测精度、交易表现和改进的可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>阿尔法因子挖掘在量化投资中至关重要，需从复杂数据中识别预测信号。</li>
<li>传统方法依赖人为经验，现代自动化方法存在搜索效率低下和解释性差的问题。</li>
<li>本文提出的新型框架融合了大型语言模型（LLM）与蒙特卡洛树搜索（MCTS），克服了这些难题。</li>
<li>LLM用于迭代生成和精炼符号阿尔法公式，提高搜索效率和因子性能。</li>
<li>通过金融回测反馈指导MCTS搜索，实现广阔搜索空间的高效导航。</li>
<li>引入频繁子树避免机制以进一步提高阿尔法因子的性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77b4946d9bff7036fa99997d7e9dec37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b84ab0f8fbb19d47788638d1cec2b2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7849fa285bb279ab5653303882442a8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="BLEUBERI-BLEU-is-a-surprisingly-effective-reward-for-instruction-following"><a href="#BLEUBERI-BLEU-is-a-surprisingly-effective-reward-for-instruction-following" class="headerlink" title="BLEUBERI: BLEU is a surprisingly effective reward for instruction   following"></a>BLEUBERI: BLEU is a surprisingly effective reward for instruction   following</h2><p><strong>Authors:Yapei Chang, Yekyung Kim, Michael Krumdick, Amir Zadeh, Chuan Li, Chris Tanner, Mohit Iyyer</strong></p>
<p>Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at <a target="_blank" rel="noopener" href="https://github.com/lilakk/BLEUBERI">https://github.com/lilakk/BLEUBERI</a>. </p>
<blockquote>
<p>奖励模型是使大型语言模型与人类偏好对齐的核心，但它们训练成本高昂，需要大量人工标记的偏好数据和强大的预训练大型语言模型骨架。同时，高质量合成指令跟随数据集的日益可用性提出了一个问题：在基于RL的对齐过程中，更简单的基于参考的度量标准能否作为奖励模型的可行替代品？在本文中，我们首先表明BLEU（一种基本的字符串匹配度量标准）与人类在通用指令跟随数据集上的偏好意外地匹配强大的奖励模型。基于这一见解，我们开发了BLEUBERI方法，该方法首先识别具有挑战性的指令，然后使用BLEU作为奖励函数应用组相对策略优化（GRPO）。我们证明，使用BLEUBERI训练的模型在四个具有挑战性的指令跟随基准测试以及三种不同的基础语言模型上，与通过奖励模型引导的RL训练的模型具有竞争力。人类评估进一步支持BLEUBERI模型输出的质量与奖励模型对齐的模型相当。而且，BLEUBERI模型产生的输出比竞争方法更加基于事实。总的来说，我们证明，在获得高质量参考输出（可通过现有指令跟随数据集或合成数据生成轻松获得）的情况下，字符串匹配基于度量的奖励模型是廉价而有效的代理。我们在<a target="_blank" rel="noopener" href="https://github.com/lilakk/BLEUBERI">https://github.com/lilakk/BLEUBERI</a>发布我们的代码和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11080v1">PDF</a> 28 pages, 11 figures, 15 tables</p>
<p><strong>Summary</strong></p>
<p>本文探讨了奖励模型在自然语言处理领域的重要性及其高昂的训练成本问题。随着高质量合成指令跟随数据集的普及，研究提出了一种基于BLEU评分的新方法BLEUBERI。该方法在识别挑战性指令后，使用BLEU作为奖励函数进行群体相对策略优化（GRPO）。实验结果显示，BLEUBERI训练出的模型在四个指令跟随基准测试上与奖励模型指导的强化学习模型表现相当，且生成输出更具事实依据。总体而言，BLEUBERI展示了基于高质量参考输出的字符串匹配指标在替代奖励模型进行对齐时的有效性和经济性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>奖励模型在自然语言处理领域中对齐大语言模型与人类偏好中占据核心地位，但其训练成本高昂。</li>
<li>随着高质量合成指令跟随数据集的增多，人们开始探索是否可以用更简单的基于参考的度量标准作为奖励模型的可行替代方案。</li>
<li>研究发现BLEU评分与人类的偏好高度一致，特别是在通用指令遵循数据集上。</li>
<li>基于这一发现，提出了BLEUBERI方法，该方法结合群体相对策略优化（GRPO）使用BLEU作为奖励函数。</li>
<li>BLEUBERI训练出的模型在多个基准测试中表现良好，与人类评价的结果相当。</li>
<li>与其他方法相比，BLEUBERI生成的输出更具事实依据。</li>
<li>研究最后公开了相关的代码和数据，为后来的研究者提供了方便。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0ed9eac6ac585ac61472f79fdd781a75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28058cc6bd943d7d7c7a4839f1644569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a164b276206767727e90873338fe7aa.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GuardReasoner-VL-Safeguarding-VLMs-via-Reinforced-Reasoning"><a href="#GuardReasoner-VL-Safeguarding-VLMs-via-Reinforced-Reasoning" class="headerlink" title="GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning"></a>GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning</h2><p><strong>Authors:Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, Junfeng Fang, Jiaheng Zhang, Bryan Hooi</strong></p>
<p>To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our model’s reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B&#x2F;7B) of GuardReasoner-VL at <a target="_blank" rel="noopener" href="https://github.com/yueliu1999/GuardReasoner-VL/">https://github.com/yueliu1999/GuardReasoner-VL/</a> </p>
<blockquote>
<p>本文介绍了一种基于推理的VLM保护模型，称为GuardReasoner-VL，以提高VLMs的安全性。核心理念是通过在线强化学习激励保护模型在做出适度决策之前进行审慎推理。首先，我们构建了GuardReasoner-VLTrain，一个包含12.3万个样本和63.1万个推理步骤的推理语料库，涵盖文本、图像和文本-图像输入。然后，在此基础上，我们通过SFT冷启动模型的推理能力。此外，我们还通过在线强化学习进一步提高了关于适度的推理能力。具体来说，为了提高样本的多样性和难度，我们进行了拒绝采样，然后通过提出的安全感知数据拼接进行数据增强。此外，我们使用动态裁剪参数来鼓励早期阶段的探索和在后期阶段的利用。为了平衡性能和令牌效率，我们设计了一个感知长度的安全奖励，融合了准确性、格式和令牌成本。大量实验证明了我们的模型的优越性。值得注意的是，它在平均F1分数上超过了第二名1.927个百分点。我们在<a target="_blank" rel="noopener" href="https://github.com/yueliu1999/GuardReasoner-VL/%E5%8F%91%E5%B8%83%E4%BA%86GuardReasoner-VL%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%81%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%A4%A7%E5%B0%8F%E4%B8%BAPB%E7%BA%A7%E6%88%96%E6%9B%B4%E5%A4%A7%EF%BC%89%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%BA%9B%E7%9B%B8%E5%85%B3%E7%BB%8F%E9%AA%8C%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9D%83%E9%87%8D%E9%85%8D%E7%BD%AE%EF%BC%88%E5%A6%82%E5%9C%A8%E5%AE%9E%E9%AA%8C%E4%B8%AD%E8%AE%AD%E7%BB%83%E7%9A%84%E6%A3%80%E6%9F%A5%E7%82%B9%EF%BC%89%E3%80%82">https://github.com/yueliu1999/GuardReasoner-VL/发布了GuardReasoner-VL的数据、代码和模型（大小为PB级或更大）以及一些相关经验模型和权重配置（如在实验中训练的检查点）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文介绍了一种名为GuardReasoner-VL的新型基于推理的VLM防护模型，旨在提高VLM的安全性。该模型通过在线强化学习激励推理决策，构建了包含文本、图像和文本-图像输入的推理语料库GuardReasoner-VLTrain，并通过SFT进行冷启动。采用拒绝采样和安全意识数据拼接进行数据增强，并使用动态裁剪参数鼓励早期探索与后期利用。设计了一种兼顾性能和令牌效率的基于长度的安全奖励机制。实验证明，该模型表现优异，平均F1分数较第二名高出19.27%。模型数据、代码（3B&#x2F;7B）及模型可在<a target="_blank" rel="noopener" href="https://github.com/yueliu1999/GuardReasoner-VL/">https://github.com/yueliu1999/GuardReasoner-VL/上获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种名为GuardReasoner-VL的新型VLM防护模型，旨在提高VLM的安全性。</li>
<li>通过在线强化学习激励推理决策，构建了包含文本、图像和文本-图像输入的推理语料库GuardReasoner-VLTrain。</li>
<li>采用拒绝采样和安全意识数据拼接进行数据增强，以增强样本的多样性和难度。</li>
<li>使用动态裁剪参数平衡早期探索和后期利用。</li>
<li>设计了一种基于长度的安全奖励机制，兼顾性能和令牌效率。</li>
<li>实验证明该模型表现优异，平均F1分数较第二名高出显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11049">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f2b1b9ee87f309281390ea28088be0f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5fe02e74a4f9af14e832951e75fcd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d347646fab8e0afb4371144e623e1260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d262dd993cddfd2b2a953a2a22a5d91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1af097b356c85cfcb2357cb215bbde9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Informed-but-Not-Always-Improved-Challenging-the-Benefit-of-Background-Knowledge-in-GNNs"><a href="#Informed-but-Not-Always-Improved-Challenging-the-Benefit-of-Background-Knowledge-in-GNNs" class="headerlink" title="Informed, but Not Always Improved: Challenging the Benefit of Background   Knowledge in GNNs"></a>Informed, but Not Always Improved: Challenging the Benefit of Background   Knowledge in GNNs</h2><p><strong>Authors:Kutalmış Coşkun, Ivo Kavisanczki, Amin Mirzaei, Tom Siegl, Bjarne C. Hiller, Stefan Lüdtke, Martin Becker</strong></p>
<p>In complex and low-data domains such as biomedical research, incorporating background knowledge (BK) graphs, such as protein-protein interaction (PPI) networks, into graph-based machine learning pipelines is a promising research direction. However, while BK is often assumed to improve model performance, its actual contribution and the impact of imperfect knowledge remain poorly understood. In this work, we investigate the role of BK in an important real-world task: cancer subtype classification. Surprisingly, we find that (i) state-of-the-art GNNs using BK perform no better than uninformed models like linear regression, and (ii) their performance remains largely unchanged even when the BK graph is heavily perturbed. To understand these unexpected results, we introduce an evaluation framework, which employs (i) a synthetic setting where the BK is clearly informative and (ii) a set of perturbations that simulate various imperfections in BK graphs. With this, we test the robustness of BK-aware models in both synthetic and real-world biomedical settings. Our findings reveal that careful alignment of GNN architectures and BK characteristics is necessary but holds the potential for significant performance improvements. </p>
<blockquote>
<p>在生物医学研究等复杂且数据稀缺的领域中，将背景知识（BK）图，如蛋白质-蛋白质相互作用（PPI）网络，融入基于图的机器学习流程是一个前景广阔的研究方向。虽然背景知识通常被认为可以提高模型性能，但其实际贡献以及不完美知识的影响仍鲜为人知。在这项工作中，我们研究了背景知识在一个重要的现实世界任务：癌症亚型分类中的作用。令人惊讶的是，我们发现（i）使用背景知识的最先进图神经网络（GNNs）并不比线性回归等未受训模型表现得更好，（ii）即使在背景知识图受到严重干扰的情况下，其性能也基本保持不变。为了理解这些意想不到的结果，我们引入了一个评估框架，该框架采用（i）一个背景知识明确有用的合成设置和（ii）一系列模拟背景知识图中各种缺陷的扰动。借此，我们在合成环境和现实世界的生物医学环境中测试了基于背景知识的模型的稳健性。我们的研究结果表明，需要仔细调整图神经网络架构和背景知识特性，这虽有可能提高潜在性能提升的可能性，但也需要注意存在的问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11023v1">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>在生物医学研究等复杂且数据稀缺的领域中，将背景知识（BK）图，如蛋白质-蛋白质相互作用（PPI）网络，融入基于图的机器学习管道是一个有前景的研究方向。然而，尽管BK通常被认为能提升模型性能，但其实际贡献以及不完美知识的影响仍知之甚少。本研究调查了BK在一个重要实际任务——癌症亚型分类中的作用。研究发现，使用BK的先进图神经网络（GNNs）并不比线性回归等未使用BK的模型表现更好，且即使BK图受到严重干扰，其表现也几乎不变。为了理解这些意外结果，研究引入了评估框架，采用信息清晰的合成设置和模拟BK图各种不完美的扰动设置。研究发现，GNN架构与BK特性的谨慎匹配是必要的，但具有提升性能的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在复杂且数据稀缺的领域中，如生物医学研究，背景知识（BK）图被纳入图基机器学习管道是一个有前途的研究方向。</li>
<li>虽然BK通常被认为能提高模型性能，但其实际贡献以及不完美知识对模型的影响仍不明确。</li>
<li>研究发现，在癌症亚型分类任务中，使用BK的先进图神经网络（GNNs）并不总是比未使用BK的模型表现更好。</li>
<li>即使BK图受到严重干扰，使用BK的模型的性能也几乎不变。</li>
<li>为了理解背景知识在模型中的实际作用，研究引入了评估框架，包括信息清晰的合成设置和模拟BK图各种不完美的扰动设置。</li>
<li>通过实验发现，GNN架构与BK特性的匹配对模型性能有重要影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11023">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d736b5c7feb85eb81315af7e7482747.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da4abc5167b1aca5e3791116bebdff56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aee9856a646f7c0147b7b1960bc9bff7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GenoArmory-A-Unified-Evaluation-Framework-for-Adversarial-Attacks-on-Genomic-Foundation-Models"><a href="#GenoArmory-A-Unified-Evaluation-Framework-for-Adversarial-Attacks-on-Genomic-Foundation-Models" class="headerlink" title="GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on   Genomic Foundation Models"></a>GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on   Genomic Foundation Models</h2><p><strong>Authors:Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, Yan Chen</strong></p>
<p>We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks, GenoArmory offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Additionally, we introduce GenoAdv, a new adversarial sample dataset designed to improve GFM safety. Empirically, classification models exhibit greater robustness to adversarial perturbations compared to generative models, highlighting the impact of task type on model vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features. </p>
<blockquote>
<p>我们提出了针对基因组基础模型（GFMs）的第一个统一对抗性攻击基准测试，名为GenoArmory。与现有的GFM基准测试不同，GenoArmory提供了一个全面的评估框架，系统地评估GFMs对抗对攻击性的脆弱性。在方法上，我们采用四种广泛采用的攻击算法和三种防御策略，评估了五种最先进的GFMs对抗攻击的鲁棒性。重要的是，我们的基准测试提供了一个易于访问的综合框架，用于分析GFM在模型结构、量化方案和训练数据集方面的脆弱性。此外，我们还介绍了GenoAdv，这是一种新的对抗样本数据集，旨在提高GFM的安全性。经验表明，与生成模型相比，分类模型对对抗性扰动表现出更大的鲁棒性，突出了任务类型对模型脆弱性的影响。而且，对抗性攻击经常针对生物学上重要的基因组区域，这表明这些模型有效地捕获了有意义的序列特征。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基因组模型对抗攻击基准测试（GenoArmory）被提出，它系统地评估了基因组基础模型（GFMs）对对抗攻击的脆弱性。该研究评估了五种最先进GFMs的对抗鲁棒性，采用四种广泛采用的攻击算法和三种防御策略。此外，它还引入了一个新的对抗样本数据集GenoAdv，以提高GFM的安全性。分类模型相对于生成模型展现出更强的对抗扰动鲁棒性，任务类型对模型脆弱性有影响。同时，对抗攻击经常针对生物学上重要的基因组区域，说明这些模型有效地捕捉了有意义的序列特征。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出首个统一的基因组基础模型（GFMs）对抗攻击基准测试（GenoArmory）。</li>
<li>系统地评估了GFMs对对抗攻击的脆弱性。</li>
<li>对五种最先进GFMs的对抗鲁棒性进行了评估，采用了四种攻击算法和三种防御策略。</li>
<li>引入新的对抗样本数据集GenoAdv以提升GFM的安全性。</li>
<li>分类模型相对于生成模型展现出更强的对抗扰动鲁棒性。</li>
<li>任务类型影响模型的脆弱性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10983">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-17eb7231ff9b76bce91e72999f692003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0963033f1b7a23c65f58c29be357e6e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Improving-the-Data-efficiency-of-Reinforcement-Learning-by-Warm-starting-with-LLM"><a href="#Improving-the-Data-efficiency-of-Reinforcement-Learning-by-Warm-starting-with-LLM" class="headerlink" title="Improving the Data-efficiency of Reinforcement Learning by Warm-starting   with LLM"></a>Improving the Data-efficiency of Reinforcement Learning by Warm-starting   with LLM</h2><p><strong>Authors:Thang Duong, Minglai Yang, Chicheng Zhang</strong></p>
<p>We investigate the usage of Large Language Model (LLM) in collecting high-quality data to warm-start Reinforcement Learning (RL) algorithms for learning in some classical Markov Decision Process (MDP) environments. In this work, we focus on using LLM to generate an off-policy dataset that sufficiently covers state-actions visited by optimal policies, then later using an RL algorithm to explore the environment and improve the policy suggested by the LLM. Our algorithm, LORO, can both converge to an optimal policy and have a high sample efficiency thanks to the LLM’s good starting policy. On multiple OpenAI Gym environments, such as CartPole and Pendulum, we empirically demonstrate that LORO outperforms baseline algorithms such as pure LLM-based policies, pure RL, and a naive combination of the two, achieving up to $4 \times$ the cumulative rewards of the pure RL baseline. </p>
<blockquote>
<p>我们研究了大型语言模型（LLM）在收集高质量数据以预热强化学习（RL）算法中的应用，用于在某些经典马尔可夫决策过程（MDP）环境中进行学习。在这项工作中，我们专注于使用LLM生成一种非策略数据集，该数据集能够充分覆盖由最优策略访问的状态动作，然后使用RL算法探索环境并改进由LLM提出的策略。我们的算法LORO可以收敛到最优策略，并且由于LLM的良好初始策略而具有较高的样本效率。在多个OpenAI Gym环境中，例如CartPole和Pendulum，我们实证表明LORO优于基线算法，如基于纯LLM的策略、纯RL和两者的简单组合，其累积奖励达到了纯RL基线的4倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10861v1">PDF</a> 31 pages (9 for the main paper), 27 figures, NeurIPS 25 submission</p>
<p><strong>摘要</strong></p>
<p>本文研究了使用大型语言模型（LLM）收集高质量数据，以启动强化学习（RL）算法在某些经典马尔可夫决策过程（MDP）环境中的学习。本文重点研究使用LLM生成涵盖最优策略访问的状态动作的离策略数据集，然后使用RL算法探索环境并改进由LLM提出的策略。我们的算法LORO，由于LLM的良好初始策略，既能收敛到最优策略，又具有较高的样本效率。在多个OpenAI Gym环境中，如CartPole和Pendulum，我们实证显示LORO优于基线算法，如纯LLM策略、纯RL和两者的简单组合，累计奖励达到纯RL基线的4倍。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究了大型语言模型（LLM）在强化学习（RL）中的应用，特别是在经典马尔可夫决策过程（MDP）环境中的使用。</li>
<li>提出了一种新的算法LORO，结合了LLM和RL的优势，旨在提高样本效率和策略优化。</li>
<li>LORO算法生成涵盖最优策略访问的状态动作的离策略数据集。</li>
<li>LORO在多个OpenAI Gym环境中进行了实证测试，包括CartPole和Pendulum。</li>
<li>LORO显著优于基线算法，如纯LLM策略、纯RL以及两者的组合。</li>
<li>LORO算法能够实现高达纯RL基线4倍的累计奖励。</li>
<li>LLM的初始策略对LORO算法的性能起到了关键作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10861">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-44e5f538fd47a6380935eca2cf22ce5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57fce7d35fa171aa6bde021597ade331.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cf301fcc5afd5a9d19eea23a7f22a9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34689d31bf4c796897e2a66f315667a1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="On-the-Evaluation-of-Engineering-Artificial-General-Intelligence"><a href="#On-the-Evaluation-of-Engineering-Artificial-General-Intelligence" class="headerlink" title="On the Evaluation of Engineering Artificial General Intelligence"></a>On the Evaluation of Engineering Artificial General Intelligence</h2><p><strong>Authors:Sandeep Neema, Susmit Jha, Adam Nagel, Ethan Lew, Chandrasekar Sureshkumar, Aleksa Gordic, Chase Shimmin, Hieu Nguygen, Paul Eremenko</strong></p>
<p>We discuss the challenges and propose a framework for evaluating engineering artificial general intelligence (eAGI) agents. We consider eAGI as a specialization of artificial general intelligence (AGI), deemed capable of addressing a broad range of problems in the engineering of physical systems and associated controllers. We exclude software engineering for a tractable scoping of eAGI and expect dedicated software engineering AI agents to address the software implementation challenges. Similar to human engineers, eAGI agents should possess a unique blend of background knowledge (recall and retrieve) of facts and methods, demonstrate familiarity with tools and processes, exhibit deep understanding of industrial components and well-known design families, and be able to engage in creative problem solving (analyze and synthesize), transferring ideas acquired in one context to another. Given this broad mandate, evaluating and qualifying the performance of eAGI agents is a challenge in itself and, arguably, a critical enabler to developing eAGI agents. In this paper, we address this challenge by proposing an extensible evaluation framework that specializes and grounds Bloom’s taxonomy - a framework for evaluating human learning that has also been recently used for evaluating LLMs - in an engineering design context. Our proposed framework advances the state of the art in benchmarking and evaluation of AI agents in terms of the following: (a) developing a rich taxonomy of evaluation questions spanning from methodological knowledge to real-world design problems; (b) motivating a pluggable evaluation framework that can evaluate not only textual responses but also evaluate structured design artifacts such as CAD models and SysML models; and (c) outlining an automatable procedure to customize the evaluation benchmark to different engineering contexts. </p>
<blockquote>
<p>我们讨论了面临的挑战，并提出一个评估工程人工智能通用智能（eAGI）代理的框架。我们认为eAGI是人工智能通用智能（AGI）的一种专业化形式，能够解决物理系统及其控制器工程中的一系列问题。我们将软件工程排除在外，以便对eAGI进行可行的范围界定，并期望专门的软件工程AI代理能解决软件实施挑战。与人类工程师类似，eAGI代理应具备独特的事实和方法背景知识（回忆和检索），熟悉工具和流程，深入了解工业组件和知名设计家族，并能够进行创造性解决问题（分析和综合），将在一个环境中获得的想法应用到另一个环境中。鉴于这一广泛的任务范围，评估eAGI代理的性能是一项挑战，并且是开发eAGI代理的关键推动因素。在本文中，我们通过提出一个可扩展的评估框架来解决这一挑战，该框架专门化和基于Bloom的分类法——一个评估人类学习的框架，最近也被用于评估大型语言模型——在工程设计的背景下。我们提出的框架在以下方面推动了人工智能代理的基准测试和评估的最新发展：（a）从方法论知识到现实世界设计问题，开发了丰富的评估问题分类；（b）激励了一个可扩展的评估框架，该框架不仅可以评估文本响应，还可以评估CAD模型和SysML模型等结构化设计成果；（c）概述了一个可自动化的程序来根据不同工程环境定制评估基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10653v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong><br>在挑战重重的背景下，本文探讨并提议一个针对工程化人工智能智能体（eAGI）代理的评价框架。本文强调人工智能的工程设计和创新实践的能力评估重要性。对于宽泛的任务指令执行工程自动化操作的评估分析有助于开启和发展下一代的人工智能代理。我们提议的框架通过专业化的扩展性评估方式，为人工智能代理提供了一个标准的性能评价系统，包括对丰富的评价问题的开发，不仅仅限于文本反馈评价，还包括CAD模型和SysML模型等结构化设计物体的评价。框架旨在能够自动化定制以适应不同的工程环境。对于推进人工智能代理在基准测试和技术评估方面的发展具有重要意义。总结为：工程化人工智能智能体（eAGI）评价框架的构建与扩展性评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章提出将人工通用智能的工程应用部分具体化进行人工智能智能体评估的框架和挑战性分析重要性认识讨论重要性认知领域的描绘形成了一更为针对的方法方法专评估智能化产业人物建设综合性运用自身综合能力要素的规范性依据支撑理论探讨价值创造具体价值讨论细节并持续推进提升总结过程自动化发展的路径应用推进更新流程的建设规划统一性与个性化特征相互结合的重要策略方案制定方向框架建设推进研究与发展过程的应用落地化探索框架在面向工程应用方面的重要意义和价值所在的关键见解要素要素领域信息架构结构讨论智能综合型工程技术体系实现以及应用场景不断优化的进程架构解决方案的特征考量跨应用场景信息的共性与特征挑战本身的分解能力的思想来源重构的技术运用高效问题解决复杂性与优越性水平输出思想核心的精确技术行业提出了一系列的专业测评具体问题解决环境突破系统的理论设计重要评估标准的支撑重要问题解决问题评估过程的细化操作依据可实践应用的核心领域特征及其研究价值和评估机制的价值贡献的实际应用性强的对策概念化和可视化的关联工程技术及其应用和系统方面设计与表现素养管理反馈高级人才培养智力利用的现状使用原则绩效问题的行动技能变革人才培养注重打造共识借鉴一体化效能大数据人宏观人文管理协同智能管理框架能力框架评价体系的核心内容能力发展等核心问题解决方案的核心问题核心思想及未来发展趋势和战略分析策略策略手段规划思维基础要素的创造性提出更加智能化评价应用层面的标准化研究标准落地推进的应用管理需求模型系统的科学适应性模块持续监控提出竞争发展新指标实时激励评价方法管理机制综合素质内容整体的延续与创新工具全生命周期平台的现实流程重要性中的跨界核心构建的能力传递激发新方法的数据基础的准确性素质创造元素提出了归纳表达模块化管理体系案例规律系统设计的方法和新型发展的策略的科研贡献现状布局规范化规模化产出创造性应用能力为应用指导产品工业的应用打造共创过程的理解通过需求转化的能力建设未来发展力的活力终端激励评估和差异化措施选择管理机制有效性转型提出了泛智能化技术发展在不同维度的评价方法一体化评测框架对智能系统的智能化程度进行评价的重要性</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10653">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8c8619300d87aae8c6e3663216f1c1fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73122ee4ba76f67fb0e059fdb11fbcdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0149fdd840a86573322cc2ac44425042.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CRPE-Expanding-The-Reasoning-Capability-of-Large-Language-Model-for-Code-Generation"><a href="#CRPE-Expanding-The-Reasoning-Capability-of-Large-Language-Model-for-Code-Generation" class="headerlink" title="CRPE: Expanding The Reasoning Capability of Large Language Model for   Code Generation"></a>CRPE: Expanding The Reasoning Capability of Large Language Model for   Code Generation</h2><p><strong>Authors:Ningxin Gui, Qianghuai Jia, Feijun Jiang, Yuling Jiao, dechun wang, Jerry Zhijian Yang</strong></p>
<p>We introduce CRPE (Code Reasoning Process Enhancer), an innovative three-stage framework for data synthesis and model training that advances the development of sophisticated code reasoning capabilities in large language models (LLMs). Building upon existing system-1 models, CRPE addresses the fundamental challenge of enhancing LLMs’ analytical and logical processing in code generation tasks. Our framework presents a methodologically rigorous yet implementable approach to cultivating advanced code reasoning abilities in language models. Through the implementation of CRPE, we successfully develop an enhanced COT-Coder that demonstrates marked improvements in code generation tasks. Evaluation results on LiveCodeBench (20240701-20240901) demonstrate that our COT-Coder-7B-StepDPO, derived from Qwen2.5-Coder-7B-Base, with a pass@1 accuracy of 21.88, exceeds all models with similar or even larger sizes. Furthermore, our COT-Coder-32B-StepDPO, based on Qwen2.5-Coder-32B-Base, exhibits superior performance with a pass@1 accuracy of 35.08, outperforming GPT4O on the benchmark. Overall, CRPE represents a comprehensive, open-source method that encompasses the complete pipeline from instruction data acquisition through expert code reasoning data synthesis, culminating in an autonomous reasoning enhancement mechanism. </p>
<blockquote>
<p>我们介绍了CRPE（代码推理过程增强器），这是一个创新的三阶段框架，用于数据合成和模型训练，推动大型语言模型（LLM）中复杂代码推理能力的发展。CRPE建立在现有的系统1模型之上，解决了提高LLM在代码生成任务中的分析和逻辑处理能力的根本挑战。我们的框架为在语言模型中培养先进的代码推理能力提供了一种方法严谨且切实可行的方法。通过CRPE的实施，我们成功开发了一种增强的COT-Coder，在代码生成任务中表现出显著改进。LiveCodeBench（2024年7月1日至2024年9月1日）的评估结果表明，我们的COT-Coder-7B-StepDPO，源于Qwen2.5-Coder-7B-Base，准确率为21.88%，超过了类似或更大规模的所有模型。此外，我们的COT-Coder-32B-StepDPO，基于Qwen2.5-Coder-32B-Base，表现出卓越的性能，准确率为35.08%，在基准测试中超越了GPT4O。总的来说，CRPE是一种全面、开源的方法，涵盖了从指令数据获取到专家代码推理数据合成的完整流程，最终形成了一个自主推理增强机制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10594v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>CRPE（代码推理过程增强器）是一个用于数据合成和模型训练的三阶段框架，旨在提升大型语言模型（LLM）的代码推理能力。通过系统-第一期模型的基础上增强模型分析力和逻辑处理，成功开发出具有先进代码推理能力的语言模型COT-Coder。在LiveCodeBench上的评估结果显示，新开发的COT-Coder在代码生成任务上表现卓越，超过其他相似或更大规模的模型。CRPE是一个全面、开源的方法，涵盖了从指令数据采集到专家代码推理数据合成，最终实现了自主推理增强机制。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>CRPE是一个三阶段框架，用于增强大型语言模型的代码推理能力。</li>
<li>它建立在现有的系统-第一期模型上，解决了提升LLM在代码生成任务中的分析和逻辑处理能力的挑战。</li>
<li>COT-Coder是CRPE框架下成功开发的具有先进代码推理能力的语言模型。</li>
<li>在LiveCodeBench上的评估显示，COT-Coder在代码生成任务上表现优异，超过了其他类似或更大的模型。</li>
<li>CRPE通过构建全面的数据合成和模型训练流程，实现了从指令数据采集到专家代码推理数据合成的完整管道。</li>
<li>CRPE具有自主推理增强机制的特点。</li>
<li>CRPE是开放源码的方法论。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10594">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-16fcfda0d02cf5c7522aa892e36f6f1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d37d66a0767a60b43950c7559053589d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf013824a04dd057dbfdebfb02dba7ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c81f091a1c845345c1b40c8e4e1b6b14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9daf230fcffcadc80e3dfd57ee5346b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e02542f6ea2bfc345931f653c3bc3cfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20d504803069e78ebf163ced33ee7c52.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c8676229573b6660592867fc418dd516.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-05-20  Modeling cognitive processes of natural reading with transformer-based   Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c5a3e44181e2069c853347aeeacb675a.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-18  RM-R1 Reward Modeling as Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
