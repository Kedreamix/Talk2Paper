<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-20  SoftCoT++ Test-Time Scaling with Soft Chain-of-Thought Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-44e5f538fd47a6380935eca2cf22ce5f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-20-æ›´æ–°"><a href="#2025-05-20-æ›´æ–°" class="headerlink" title="2025-05-20 æ›´æ–°"></a>2025-05-20 æ›´æ–°</h1><h2 id="SoftCoT-Test-Time-Scaling-with-Soft-Chain-of-Thought-Reasoning"><a href="#SoftCoT-Test-Time-Scaling-with-Soft-Chain-of-Thought-Reasoning" class="headerlink" title="SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning"></a>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</h2><p><strong>Authors:Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao</strong></p>
<p>Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the modelâ€™s parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/xuyige/SoftCoT">https://github.com/xuyige/SoftCoT</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ˜¯æŒ‡é€šè¿‡æ¨ç†æ—¶åˆ†é…é¢å¤–çš„è®¡ç®—èµ„æºæ¥æé«˜æ¨ç†æ€§èƒ½çš„æ–¹æ³•ï¼Œè€Œä¸ä¼šæ”¹å˜æ¨¡å‹çš„å‚æ•°ã€‚è™½ç„¶ç°æœ‰çš„TTSæ–¹æ³•åœ¨ç¦»æ•£æ ‡è®°ç©ºé—´å†…è¿è¡Œï¼Œé€šè¿‡ç”Ÿæˆæ›´å¤šçš„ä¸­é—´æ­¥éª¤æ¥å·¥ä½œï¼Œä½†æœ€è¿‘åœ¨Coconutå’ŒSoftCoTä¸­çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ€è€ƒå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨ç†æ€§èƒ½ã€‚è¿™ç§æ½œåœ¨çš„æƒ³æ³•å¯ä»¥ç¼–ç ä¿¡æ¯ä¸°å¯Œçš„æ€è€ƒè¿‡ç¨‹ï¼Œè€Œä¸ä¼šäº§ç”Ÿä¸è‡ªå›å½’æ ‡è®°ç”Ÿæˆç›¸å…³çš„ä¿¡æ¯ä¸¢å¤±ï¼Œå› æ­¤å¯¹è¿ç»­ç©ºé—´æ¨ç†äº§ç”Ÿäº†æ›´å¤§çš„å…´è¶£ã€‚ä¸ç¦»æ•£è§£ç ä¸åŒï¼Œç¦»æ•£è§£ç é€šè¿‡é‡å¤é‡‡æ ·å¯ä»¥æ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼Œè€Œè¿ç»­ç©ºé—´ä¸­çš„æ½œåœ¨è¡¨ç¤ºå¯¹äºç»™å®šè¾“å…¥æ˜¯å›ºå®šçš„ï¼Œè¿™é™åˆ¶äº†å¤šæ ·åŒ–çš„æ¢ç´¢ï¼Œå› ä¸ºæ‰€æœ‰è§£ç è·¯å¾„éƒ½æ¥æºäºåŒä¸€æ½œåœ¨æƒ³æ³•ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥SoftCoT++æ¥å°†SoftCoTæ‰©å±•åˆ°æµ‹è¯•æ—¶ç¼©æ”¾èŒƒå¼ï¼Œé€šè¿‡é‡‡ç”¨å¤šç§ä¸“ä¸šåˆå§‹æ ‡è®°æ¥æ‰°åŠ¨æ½œåœ¨çš„æƒ³æ³•ï¼Œå¹¶åº”ç”¨å¯¹æ¯”å­¦ä¹ æ¥ä¿ƒè¿›è½¯æ€ç»´è¡¨ç¤ºä¹‹é—´çš„å¤šæ ·æ€§ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’Œä¸¤ç§ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSoftCoT++æ˜¾è‘—æå‡äº†SoftCoTçš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¼˜äºSoftCoTçš„è‡ªä¸€è‡´æ€§ç¼©æ”¾ã€‚æ­¤å¤–ï¼Œå®ƒæ˜¾ç¤ºå‡ºä¸å¸¸è§„ç¼©æ”¾æŠ€æœ¯ï¼ˆå¦‚è‡ªä¸€è‡´æ€§ï¼‰çš„å¼ºå¤§å…¼å®¹æ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xuyige/SoftCoT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xuyige/SoftCoTä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11484v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Test-Time Scalingï¼ˆTTSï¼‰æ–¹æ³•åœ¨æé«˜æ¨ç†æ€§èƒ½æ–¹é¢çš„ä½œç”¨ï¼Œé€šè¿‡æ¨ç†æ—¶åœ¨è¿ç»­æ½œåœ¨ç©ºé—´è¿›è¡Œæ€è€ƒï¼Œæ— éœ€æ”¹å˜æ¨¡å‹å‚æ•°å³å¯åˆ†é…é¢å¤–çš„è®¡ç®—èµ„æºã€‚æœ€æ–°ç ”ç©¶Coconutå’ŒSoftCoTå±•ç¤ºäº†è¿ç»­æ½œåœ¨ç©ºé—´æ€è€ƒçš„ä¼˜åŠ¿ã€‚ä¸ºå…‹æœè¿ç»­ç©ºé—´æ¨ç†ä¸­å¤šæ ·åŒ–æ¢ç´¢çš„é™åˆ¶ï¼Œæå‡ºSoftCoT++æ–¹æ³•ï¼Œé€šè¿‡å¤šä¸ªä¸“ç”¨åˆå§‹æ ‡è®°æ‰°åŠ¨æ½œåœ¨æ€æƒ³ï¼Œåº”ç”¨å¯¹æ¯”å­¦ä¹ ä¿ƒè¿›è½¯æ€æƒ³è¡¨ç¤ºä¹‹é—´çš„å¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSoftCoT++åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªä¸åŒLLMæ¶æ„ä¸Šæ˜¾è‘—æå‡äº†SoftCoTçš„æ€§èƒ½ï¼Œä¸”ä¸ä¼ ç»Ÿæ‰©å±•æŠ€æœ¯å¦‚è‡ªæˆ‘ä¸€è‡´æ€§æ‰©å±•å…¼å®¹æ€§å¼ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Test-Time Scaling (TTS) é€šè¿‡åœ¨æ¨ç†æ—¶åˆ†é…é¢å¤–è®¡ç®—èµ„æºï¼Œæé«˜æ¨¡å‹æ¨ç†æ€§èƒ½ï¼Œä¸”æ— éœ€æ”¹å˜æ¨¡å‹å‚æ•°ã€‚</li>
<li>è¿ç»­æ½œåœ¨ç©ºé—´æ€è€ƒæ˜¯æœ€è¿‘ç ”ç©¶çš„çƒ­ç‚¹ï¼Œå…¶åœ¨æé«˜æ¨ç†æ€§èƒ½æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¯ä»¥é¿å…ç¦»æ•£æ ‡è®°ç”Ÿæˆå¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚</li>
<li>SoftCoT++å…‹æœäº†è¿ç»­ç©ºé—´æ¨ç†ä¸­å¤šæ ·åŒ–æ¢ç´¢çš„é™åˆ¶ï¼Œé€šè¿‡å¼•å…¥å¤šä¸ªä¸“ç”¨åˆå§‹æ ‡è®°æ‰°åŠ¨æ½œåœ¨æ€æƒ³ï¼Œå¹¶åº”ç”¨å¯¹æ¯”å­¦ä¹ ä¿ƒè¿›å¤šæ ·æ€§ã€‚</li>
<li>SoftCoT++åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æå‡äº†SoftCoTçš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SoftCoT++ä¸å…¶ä»–ä¼ ç»Ÿæ‰©å±•æŠ€æœ¯ï¼ˆå¦‚è‡ªæˆ‘ä¸€è‡´æ€§æ‰©å±•ï¼‰å…¼å®¹æ€§å¼ºã€‚</li>
<li>SoftCoT++æ–¹æ³•æœ‰åŠ©äºä¿ƒè¿›å¤šæ ·åŒ–æ¢ç´¢çš„æ€è€ƒè·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11484">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4efe663a379b9baddddf48687e2b2b1d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2382ea27746d2e90e5e24c55497ccfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec8f495f6f4e58fe912ea8227a6d7907.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HelpSteer3-Preference-Open-Human-Annotated-Preference-Data-across-Diverse-Tasks-and-Languages"><a href="#HelpSteer3-Preference-Open-Human-Annotated-Preference-Data-across-Diverse-Tasks-and-Languages" class="headerlink" title="HelpSteer3-Preference: Open Human-Annotated Preference Data across   Diverse Tasks and Languages"></a>HelpSteer3-Preference: Open Human-Annotated Preference Data across   Diverse Tasks and Languages</h2><p><strong>Authors:Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans, Yi Dong, Oleksii Kuchaiev</strong></p>
<p>Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/HelpSteer3#preference">https://huggingface.co/datasets/nvidia/HelpSteer3#preference</a> </p>
<blockquote>
<p>åå¥½æ•°æ®é›†å¯¹äºä½¿ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰è®­ç»ƒé€šç”¨é¢†åŸŸã€éµå¾ªæŒ‡ä»¤çš„è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚æ¯æ¬¡åç»­æ•°æ®å‘å¸ƒéƒ½æé«˜äº†å¯¹æœªæ¥æ•°æ®æ”¶é›†çš„æœŸæœ›ï¼Œè¿™æ„å‘³ç€éœ€è¦ä¸æ–­æ”¹è¿›å…¬å¼€å¯ç”¨åå¥½æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HelpSteer3-Preferenceï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨å®½æ¾è®¸å¯ï¼ˆCC-BY-4.0ï¼‰çš„é«˜è´¨é‡ã€äººå·¥æ ‡æ³¨çš„åå¥½æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡40,000ä¸ªæ ·æœ¬ã€‚è¿™äº›æ ·æœ¬æ¶µç›–äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ ·åŒ–ç°å®ä¸–ç•Œåº”ç”¨ï¼ŒåŒ…æ‹¬ä¸STEMã€ç¼–ç å’Œå¤šè¯­è¨€åœºæ™¯ç›¸å…³çš„ä»»åŠ¡ã€‚ä½¿ç”¨HelpSteer3-Preferenceï¼Œæˆ‘ä»¬è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨RM-Benchä¸Šè¾¾åˆ°é¡¶çº§æ€§èƒ½ï¼ˆ82.4%ï¼‰ï¼Œåœ¨JudgeBenchä¸Šè¾¾åˆ°ï¼ˆ73.7%ï¼‰ã€‚è¿™ç›¸æ¯”ç°æœ‰RMçš„æœ€ä½³æŠ¥å‘Šç»“æœæœ‰äº†å®è´¨æ€§çš„æ”¹è¿›ï¼ˆç»å¯¹æé«˜äº†çº¦10%ï¼‰ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†HelpSteer3-Preferenceå¦‚ä½•åº”ç”¨äºè®­ç»ƒç”Ÿæˆå¼RMï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„RMå°†æ”¿ç­–æ¨¡å‹ä¸RLHFå¯¹é½ã€‚æ•°æ®é›†ï¼ˆCC-BY-4.0ï¼‰ï¼š[<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/HelpSteer3#preference]">https://huggingface.co/datasets/nvidia/HelpSteer3#preference]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11475v1">PDF</a> 38 pages, 2 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä¸ºäº†å¸®åŠ©è®­ç»ƒé€šç”¨é¢†åŸŸçš„æŒ‡ä»¤éµå¾ªè¯­è¨€æ¨¡å‹ï¼Œæ¨å‡ºäº†HelpSteer3-Preferenceåå¥½æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ‹¥æœ‰è¶…è¿‡4ä¸‡æ ·æœ¬ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„ç°å®ä¸–ç•Œåº”ç”¨åœºæ™¯ã€‚é€šè¿‡ä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œåœ¨RM-Benchå’ŒJudgeBenchä¸Šçš„è¡¨ç°æœ‰äº†æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œä¹Ÿå±•ç¤ºäº†å¦‚ä½•å°†HelpSteer3-Preferenceåº”ç”¨äºè®­ç»ƒç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç­–ç•¥æ¨¡å‹ä¸RLHFå¯¹é½ã€‚æ•°æ®é›†éµå¾ªCC-BY-4.0è®¸å¯åè®®ï¼Œå…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>HelpSteer3-Preferenceæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„åå¥½æ•°æ®é›†ï¼Œæ ·æœ¬æ•°é‡è¶…è¿‡4ä¸‡ï¼Œæ¶µç›–å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šç§ç°å®åº”ç”¨åœºæ™¯ã€‚</li>
<li>æ•°æ®é›†é‡‡ç”¨CC-BY-4.0è®¸å¯åè®®ï¼Œå…¬å¼€å¯ç”¨ï¼Œæ»¡è¶³äº†è®­ç»ƒé€šç”¨é¢†åŸŸè¯­è¨€æ¨¡å‹çš„éœ€æ±‚ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨HelpSteer3-Preferenceæ•°æ®é›†è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹åœ¨RM-Benchå’ŒJudgeBenchä¸Šçš„è¡¨ç°æœ‰æ‰€æå‡ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
<li>æ•°æ®é›†èƒ½å¤Ÿå¸®åŠ©æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¹¶æ¨åŠ¨å…¶å‘å±•ï¼Œä½¿å¾—è®­ç»ƒç»“æœæ›´ç¬¦åˆäººç±»åå¥½ã€‚</li>
<li>è¯¥æ•°æ®é›†é€‚ç”¨äºè®­ç»ƒç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>é€šè¿‡ç­–ç•¥æ¨¡å‹ä¸RLHFçš„å¯¹é½ï¼Œå¼ºåŒ–äº†è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§å’Œæ™ºèƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ccb60b90803797e1276c09922790813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c5f0bb4a4ca8738632eafa4151dacc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-833675bcc9041121755a0dac9fce6afb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GODBench-A-Benchmark-for-Multimodal-Large-Language-Models-in-Video-Comment-Art"><a href="#GODBench-A-Benchmark-for-Multimodal-Large-Language-Models-in-Video-Comment-Art" class="headerlink" title="GODBench: A Benchmark for Multimodal Large Language Models in Video   Comment Art"></a>GODBench: A Benchmark for Multimodal Large Language Models in Video   Comment Art</h2><p><strong>Authors:Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang</strong></p>
<p>Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMsâ€™ abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at <a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025">https://github.com/stan-lei/GODBench-ACL2025</a>. </p>
<blockquote>
<p>è§†é¢‘è¯„è®ºè‰ºæœ¯é€šè¿‡æä¾›ä¼ è¾¾å¹½é»˜ã€è®½åˆºæˆ–æƒ…æ„Ÿå…±é¸£çš„åˆ›æ„å†…å®¹ï¼Œå¢å¼ºç”¨æˆ·å‚ä¸åº¦ï¼Œè¿™è¦æ±‚å¾®å¦™è€Œå…¨é¢åœ°æŠŠæ¡æ–‡åŒ–å’Œè¯­å¢ƒçš„ç»†å¾®å·®åˆ«ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰åœ¨STEMä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç ï¼‰ä¸­å±•ç¤ºäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»éš¾ä»¥ç”Ÿæˆå¦‚äº§ç”Ÿå…±é¸£çš„ç¬‘è¯å’Œå¯Œæœ‰æ´å¯ŸåŠ›çš„è®½åˆºç­‰åˆ›æ„è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å—é™äºå…¶æœ‰é™çš„æ¨¡å¼å’Œä¸è¶³çš„åˆ†ç±»ï¼Œé˜»ç¢äº†åŸºäºè§†é¢‘çš„è¯„è®ºè‰ºæœ¯åˆ›ä½œçš„å…¨é¢åˆ›é€ åŠ›çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†GODBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒæ•´åˆäº†è§†é¢‘å’Œæ–‡æœ¬æ¨¡å¼ï¼Œç³»ç»Ÿåœ°è¯„ä¼°MLLMsåˆ›ä½œè¯„è®ºè‰ºæœ¯çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå—ç‰©ç†å­¦ä¸­æ³¢åŠ¨ä¼ æ’­æ¨¡å¼çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ€æƒ³æ¶Ÿæ¼ªï¼ˆRoTï¼‰å¤šæ­¥æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜MLLMsçš„åˆ›é€ åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„MLLMså’ŒCoTæ–¹æ³•åœ¨ç†è§£å’Œç”Ÿæˆåˆ›é€ æ€§è§†é¢‘è¯„è®ºæ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRoTæä¾›äº†ä¸€ç§æé«˜åˆ›é€ æ€§å†™ä½œçš„æœ‰æ•ˆæ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨æ¨åŠ¨åŸºäºMLLMçš„åˆ›é€ åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚GODBenchå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/stan-lei/GODBench-ACL2025ä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11436v1">PDF</a> 69 pages, 66 figures, accepted by ACL 2025</p>
<p><strong>æ‘˜è¦</strong><br>     è§†é¢‘è¯„è®ºè‰ºæœ¯é€šè¿‡æä¾›ä¼ è¾¾å¹½é»˜ã€è®½åˆºæˆ–æƒ…æ„Ÿå…±é¸£çš„åˆ›æ„å†…å®¹ï¼Œå¢å¼ºäº†ç”¨æˆ·å‚ä¸åº¦ã€‚è¿™è¦æ±‚æ·±å…¥å…¨é¢åœ°ç†è§£æ–‡åŒ–å’Œè¯­å¢ƒçš„ç»†å¾®å·®åˆ«ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰åœ¨STEMä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥ç”Ÿæˆå¦‚å…±é¸£ç¬‘è¯å’Œæ·±åˆ»è®½åˆºç­‰åˆ›æ„è¡¨è¾¾ã€‚ä¸ºè§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è§†é¢‘è¯„è®ºè‰ºæœ¯åˆ›é€ åŠ›è¯„ä¼°æ–¹é¢çš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†GODBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ•´åˆäº†è§†é¢‘å’Œæ–‡æœ¬æ¨¡æ€ï¼Œç³»ç»Ÿåœ°è¯„ä¼°MLLMsåˆ›ä½œè¯„è®ºè‰ºæœ¯çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå—ç‰©ç†ä¸­æ³¢åŠ¨ä¼ æ’­æ¨¡å¼çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ€æƒ³æ¶Ÿæ¼ªï¼ˆRoTï¼‰å¤šæ­¥æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºMLLMsçš„åˆ›é€ åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰MLLMså’ŒCoTæ–¹æ³•åœ¨ç†è§£å’Œç”Ÿæˆåˆ›æ„è§†é¢‘è¯„è®ºæ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRoTæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ”¹è¿›åˆ›ä½œçš„æ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨æ¨åŠ¨åŸºäºMLLMçš„åˆ›é€ åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚GODBenchå·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/stan-lei/GODBench-ACL2025å…¬å¼€å¯ç”¨ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘è¯„è®ºè‰ºæœ¯é€šè¿‡åˆ›æ„å†…å®¹å¢å¼ºç”¨æˆ·å‚ä¸åº¦ï¼Œè¿™éœ€è¦ç†è§£æ–‡åŒ–å’Œè¯­å¢ƒçš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ€ç»´é“¾åœ¨STEMä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆåˆ›æ„è¡¨è¾¾æ–¹é¢ä»æœ‰å›°éš¾ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°è§†é¢‘è¯„è®ºè‰ºæœ¯çš„åˆ›é€ åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼•å…¥GODBenchåŸºå‡†æµ‹è¯•ï¼Œæ•´åˆè§†é¢‘å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¯„ä¼°MLLMsåˆ›ä½œè¯„è®ºè‰ºæœ¯çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºæ€æƒ³æ¶Ÿæ¼ªï¼ˆRoTï¼‰å¤šæ­¥æ¨ç†æ¡†æ¶ï¼Œå¢å¼ºMLLMsçš„åˆ›é€ åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œç°æœ‰MLLMså’ŒCoTæ–¹æ³•åœ¨ç†è§£å’Œç”Ÿæˆåˆ›æ„è§†é¢‘è¯„è®ºæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ad8734da6667e5e9e4115b0aeb767978.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-716e21106fd9c6f5ccd5b15c0537ccf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c24b0d34eff9882e6b379088fbcd1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f784b33ac157a05a7f830f7d0aba1e6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e58fc377c680b0d0c4374eecb840f3a5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="When-Thinking-Fails-The-Pitfalls-of-Reasoning-for-Instruction-Following-in-LLMs"><a href="#When-Thinking-Fails-The-Pitfalls-of-Reasoning-for-Instruction-Following-in-LLMs" class="headerlink" title="When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following   in LLMs"></a>When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following   in LLMs</h2><p><strong>Authors:Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal</strong></p>
<p>Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies. </p>
<blockquote>
<p>æ¨ç†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼‰æ— è®ºæ˜¯å¦ç»è¿‡æ˜ç¡®çš„æ¨ç†è®­ç»ƒæˆ–é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰è¿›è¡Œæç¤ºï¼Œåœ¨è®¸å¤šå¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªä»¤äººæƒŠè®¶ä¸”ä»¥å‰è¢«å¿½è§†çš„ç°è±¡ï¼šæ˜ç¡®çš„CoTæ¨ç†ä¼šæ˜¾è‘—é™ä½æŒ‡ä»¤æ‰§è¡Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹15ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼šIFevalï¼ˆå…·æœ‰ç®€å•ã€å¯éªŒè¯çš„è§„åˆ™çº¦æŸï¼‰å’ŒComplexBenchï¼ˆå…·æœ‰å¤æ‚ã€ç»„åˆçº¦æŸï¼‰ï¼Œæˆ‘ä»¬å§‹ç»ˆè§‚å¯Ÿåˆ°å½“åº”ç”¨CoTæç¤ºæ—¶æ€§èƒ½ä¸‹é™ã€‚é€šè¿‡å¤§è§„æ¨¡æ¡ˆä¾‹ç ”ç©¶å’ŒåŸºäºæ³¨æ„åŠ›çš„åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæ¨ç†æœ‰åŠ©äºï¼ˆä¾‹å¦‚ï¼Œæ ¼å¼åŒ–æˆ–è¯æ±‡ç²¾åº¦ï¼‰æˆ–æœ‰å®³ï¼ˆä¾‹å¦‚ï¼Œå¿½è§†ç®€å•çº¦æŸæˆ–å¼•å…¥ä¸å¿…è¦å†…å®¹ï¼‰çš„å¸¸è§æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªæŒ‡æ ‡ï¼Œçº¦æŸæ³¨æ„åŠ›ï¼Œæ¥é‡åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¨¡å‹å…³æ³¨ç‚¹ï¼Œå¹¶è¡¨æ˜CoTæ¨ç†é€šå¸¸ä¼šåˆ†æ•£å¯¹æŒ‡ä»¤ç›¸å…³æ ‡è®°çš„æ³¨æ„åŠ›ã€‚ä¸ºäº†å‡è½»è¿™äº›å½±å“ï¼Œæˆ‘ä»¬å¼•å…¥å¹¶è¯„ä¼°äº†å››ç§ç­–ç•¥ï¼šä¸Šä¸‹æ–‡å­¦ä¹ ã€è‡ªæˆ‘åæ€ã€è‡ªæˆ‘é€‰æ‹©æ€§æ¨ç†å’Œåˆ†ç±»å™¨é€‰æ‹©æ€§æ¨ç†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€‰æ‹©æ€§æ¨ç†ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åˆ†ç±»å™¨é€‰æ‹©æ€§æ¨ç†ï¼Œå¯ä»¥å¤§å¹…åº¦æ¢å¤ä¸¢å¤±çš„æ€§èƒ½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹å·¥ä½œï¼Œç³»ç»Ÿåœ°æš´éœ²äº†æ¨ç†åœ¨æŒ‡ä»¤æ‰§è¡Œä¸­çš„å¤±è´¥å¹¶æä¾›äº†å®ç”¨çš„ç¼“è§£ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11423v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œç ”ç©¶æ¢è®¨äº†æ˜ç¡®è¿è´¯æ¨ç†ï¼ˆCoTï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å¯¹æŒ‡ä»¤éµå¾ªå‡†ç¡®æ€§çš„æ½œåœ¨è´Ÿé¢å½±å“ã€‚ä½œè€…ä»¬åœ¨å¤šç§æ¨¡å‹ä¸åŸºå‡†æµ‹è¯•ä¸­å‘ç°ï¼Œä½¿ç”¨CoTæ¨ç†æ—¶æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚å¯¹æ­¤ï¼Œç ”ç©¶è€…è¿›è¡Œäº†å¤§è§„æ¨¡çš„æ¡ˆä¾‹ç ”ç©¶å¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä»·æŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…³æ³¨ç„¦ç‚¹ã€‚æ–‡ç« æå‡ºäº†å››ç§ç­–ç•¥æ¥ç¼“è§£æ¨ç†å¯¼è‡´çš„æ³¨æ„åŠ›åˆ†æ•£é—®é¢˜ï¼Œå…¶ä¸­é€‰æ‹©æ€§æ¨ç†ç­–ç•¥ç‰¹åˆ«æ˜¯åˆ†ç±»å™¨é€‰æ‹©æ€§æ¨ç†åœ¨æ¢å¤æ¨¡å‹æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RLLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å‘ç°æ˜ç¡®çš„è¿è´¯æ¨ç†ï¼ˆCoTï¼‰ä¼šæ˜¾è‘—é™ä½æŒ‡ä»¤éµå¾ªçš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ˆIFEvalå’ŒComplexBenchï¼‰ï¼Œä½¿ç”¨CoTæ¨ç†æ—¶æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡æ¡ˆä¾‹ç ”ç©¶å’Œæ³¨æ„åŠ›åˆ†æï¼Œè¯†åˆ«å‡ºæ¨ç†è¿‡ç¨‹å¸®åŠ©æˆ–é˜»ç¢çš„ä¸åŒæ¨¡å¼ã€‚</li>
<li>æå‡ºæ–°çš„è¯„ä»·æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›ç„¦ç‚¹ï¼Œå‘ç°CoTæ¨ç†å¯¼è‡´æ¨¡å‹åœ¨æŒ‡ä»¤ç›¸å…³æ ‡è®°ä¸Šçš„æ³¨æ„åŠ›åˆ†æ•£ã€‚</li>
<li>å¼•å…¥å››ç§ç­–ç•¥æ¥ç¼“è§£æ³¨æ„åŠ›åˆ†æ•£é—®é¢˜ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡å­¦ä¹ ã€è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘é€‰æ‹©æ€§æ¨ç†ä»¥åŠåˆ†ç±»å™¨é€‰æ‹©æ€§æ¨ç†ç­‰ã€‚</li>
<li>åˆ†ç±»å™¨é€‰æ‹©æ€§æ¨ç†ç­–ç•¥åœ¨æ¢å¤æ¨¡å‹æ€§èƒ½æ–¹é¢å°¤ä¸ºæœ‰æ•ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-458d5262f1a26b34ee993d185adf5b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5911c56e30adf0b0e1c76535f2292864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-450d0f386565be5c00036bc98f39f76b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Visual-Planning-Letâ€™s-Think-Only-with-Images"><a href="#Visual-Planning-Letâ€™s-Think-Only-with-Images" class="headerlink" title="Visual Planning: Letâ€™s Think Only with Images"></a>Visual Planning: Letâ€™s Think Only with Images</h2><p><strong>Authors:Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan VuliÄ‡</strong></p>
<p>Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠå…¶å¤šæ¨¡æ€æ‰©å±•ï¼ˆMLLMï¼‰çš„è¿›å±•æå¤§åœ°æé«˜äº†è·¨ä¸åŒä»»åŠ¡çš„æœºå™¨æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸»è¦ä¾èµ–çº¯æ–‡æœ¬ä½œä¸ºè¡¨è¾¾å’Œç»“æ„åŒ–æ¨ç†çš„åª’ä»‹ï¼Œå³ä½¿å­˜åœ¨è§†è§‰ä¿¡æ¯ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸»å¼ è¯­è¨€å¹¶ä¸æ€»æ˜¯æœ€è‡ªç„¶æˆ–æœ€æœ‰æ•ˆçš„æ¨ç†æ–¹å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç©ºé—´å’Œå‡ ä½•ä¿¡æ¯çš„ä»»åŠ¡ä¸­ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼â€”â€”è§†è§‰è§„åˆ’ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡çº¯ç²¹çš„è§†è§‰è¡¨ç¤ºè¿›è¡Œè§„åˆ’ï¼Œç‹¬ç«‹äºæ–‡æœ¬ã€‚åœ¨è¿™ç§èŒƒå¼ä¸­ï¼Œè§„åˆ’æ˜¯é€šè¿‡ä¸€ç³»åˆ—å›¾åƒæ‰§è¡Œçš„ï¼Œè¿™äº›å›¾åƒåœ¨è§†è§‰é¢†åŸŸç¼–ç äº†é€æ­¥æ¨ç†ï¼Œå°±åƒäººç±»å¦‚ä½•å‹¾ç”»æˆ–å¯è§†åŒ–æœªæ¥è¡ŒåŠ¨ä¸€æ ·ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”é€šè¿‡å¼ºåŒ–å­¦ä¹ çš„è§†è§‰è§„åˆ’ï¼ˆVPRLï¼‰ï¼Œå€ŸåŠ©GRPOå¯¹å¤§å‹è§†è§‰æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œå¯¼è‡´åœ¨å…·æœ‰ä»£è¡¨æ€§çš„è§†è§‰å¯¼èˆªä»»åŠ¡ã€FrozenLakeã€è¿·å®«å’ŒMiniBehaviorä¸­çš„è§„åˆ’èƒ½åŠ›å¾—åˆ°å®è´¨æ€§æé«˜ã€‚æˆ‘ä»¬çš„è§†è§‰è§„åˆ’èŒƒå¼åœ¨åªè¿›è¡Œæ–‡æœ¬ç©ºé—´æ¨ç†çš„è§„åˆ’ä¸­è¡¨ç°ä¼˜äºæ‰€æœ‰å…¶ä»–è§„åˆ’å˜ä½“ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†è§†è§‰è§„åˆ’ä½œä¸ºè¯­è¨€åŸºç¡€æ¨ç†çš„ä¸€ç§å¯è¡Œä¸”æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºå—ç›Šäºç›´è§‚ã€åŸºäºå›¾åƒæ¨ç†çš„ä»»åŠ¡å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11409v1">PDF</a> 10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables   including references and appendices)</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ‰©å±•æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿›æ­¥æå¤§åœ°æé«˜äº†è·¨ä¸åŒä»»åŠ¡çš„æœºå™¨æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸»è¦é€šè¿‡æ–‡æœ¬è¡¨è¾¾å’Œç»“æ„æ¨ç†ï¼Œå³ä½¿å­˜åœ¨è§†è§‰ä¿¡æ¯ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æœ¬æ–‡æå‡ºï¼Œåœ¨æ¶‰åŠç©ºé—´å’Œå‡ ä½•ä¿¡æ¯çš„ä»»åŠ¡ä¸­ï¼Œè¯­è¨€å¯èƒ½å¹¶éæœ€è‡ªç„¶æˆ–æœ‰æ•ˆçš„æ¨ç†æ–¹å¼ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ–¹å¼â€”â€”è§†è§‰è§„åˆ’ï¼Œé€šè¿‡çº¯ç²¹çš„è§†è§‰è¡¨ç¤ºè¿›è¡Œè§„åˆ’ï¼Œç‹¬ç«‹äºæ–‡æœ¬ã€‚è§†è§‰è§„åˆ’é€šè¿‡å›¾åƒåºåˆ—æ‰§è¡Œè§„åˆ’ï¼Œè¿™äº›å›¾åƒç¼–ç äº†è§†è§‰é¢†åŸŸçš„é€æ­¥æ¨ç†ï¼Œç±»ä¼¼äºäººç±»å¦‚ä½•ç»˜åˆ¶æˆ–å¯è§†åŒ–æœªæ¥è¡ŒåŠ¨ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’ŒGRPOåè®­ç»ƒå¤§å‹è§†è§‰æ¨¡å‹ï¼Œå®ç°äº†è§†è§‰è§„åˆ’çš„å®è´¨æ€§æ”¹è¿›ï¼Œå¹¶åœ¨ä»£è¡¨æ€§è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½è¡¨ç°ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœè¯æ˜äº†è§†è§‰è§„åˆ’ä½œä¸ºä¸€ç§å¯è¡Œä¸”æœ‰å‰é€”çš„æ›¿ä»£è¯­è¨€åŸºç¡€æ¨ç†æ–¹æ³•ï¼Œä¸ºå—ç›Šäºç›´è§‚å›¾åƒæ¨ç†çš„ä»»åŠ¡å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ‰©å±•æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨æœºå™¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¸»è¦ä¾èµ–æ–‡æœ¬è¿›è¡Œè¡¨è¾¾å’Œç»“æ„åŒ–æ¨ç†ï¼Œå³ä½¿å­˜åœ¨è§†è§‰ä¿¡æ¯æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>åœ¨æ¶‰åŠç©ºé—´å’Œå‡ ä½•ä¿¡æ¯çš„ä»»åŠ¡ä¸­ï¼Œè¯­è¨€å¯èƒ½ä¸æ˜¯æœ€è‡ªç„¶æˆ–æœ‰æ•ˆçš„æ¨ç†æ–¹å¼ã€‚</li>
<li>æå‡ºäº†æ–°çš„æ¨ç†æ–¹æ³•â€”â€”è§†è§‰è§„åˆ’ï¼Œè¯¥æ–¹æ³•é€šè¿‡çº¯ç²¹çš„è§†è§‰è¡¨ç¤ºè¿›è¡Œè§„åˆ’ï¼Œä¸ä¾èµ–äºæ–‡æœ¬ã€‚</li>
<li>è§†è§‰è§„åˆ’é€šè¿‡å›¾åƒåºåˆ—æ‰§è¡Œï¼Œè¿™äº›å›¾åƒç¼–ç äº†ç±»ä¼¼äºäººç±»å¦‚ä½•ç»˜åˆ¶æˆ–å¯è§†åŒ–æœªæ¥è¡ŒåŠ¨çš„é€æ­¥æ¨ç†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’ŒGRPOåè®­ç»ƒå¤§å‹è§†è§‰æ¨¡å‹ï¼Œå®ç°äº†è§†è§‰è§„åˆ’çš„å®è´¨æ€§æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11409">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39e2fae2fb6e4a2b8d5c54c5ee658bdc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b087dca392b82352040f39806ccc878.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-003cb4251f762d33ea57f5b138daf6c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45836365d67a9715f197785ef545cf09.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Patho-R1-A-Multimodal-Reinforcement-Learning-Based-Pathology-Expert-Reasoner"><a href="#Patho-R1-A-Multimodal-Reinforcement-Learning-Based-Pathology-Expert-Reasoner" class="headerlink" title="Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert   Reasoner"></a>Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert   Reasoner</h2><p><strong>Authors:Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu</strong></p>
<p>Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose PathoCLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both PathoCLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: <a target="_blank" rel="noopener" href="https://github.com/Wenchuan-Zhang/Patho-R1">https://github.com/Wenchuan-Zhang/Patho-R1</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›å±•ä¸ºä¸€èˆ¬åŒ»ç–—é¢†åŸŸå¸¦æ¥äº†å¹¿æ³›çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç—…ç†å­¦ä»ç„¶æ˜¯ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„å­é¢†åŸŸã€‚å½“å‰çš„ç—…ç†å­¦ç‰¹å®šVLMsåœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†åˆç†æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚è¿™ç§ç¼ºé™·åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½’å› äºå½“å‰ç—…ç†å­¦æ•°æ®é›†çš„æ€§è´¨ï¼Œè¿™äº›æ•°æ®é›†ä¸»è¦ç”±ç¼ºä¹ç°å®ä¸–ç•Œç—…ç†å­¦å®¶æ‰€é‡‡ç”¨çš„æ·±åº¦å’Œç»“æ„åŒ–è¯Šæ–­èŒƒå¼çš„å›¾åƒæè¿°å¯¹ç»„æˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ç—…ç†å­¦æ•™æå’Œç°å®ä¸–ç•Œç—…ç†å­¦ä¸“å®¶æ¥æ„å»ºé«˜è´¨é‡ã€ä»¥æ¨ç†ä¸ºå¯¼å‘çš„æ•°æ®é›†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†Patho-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡å¼å¼ºåŒ–å­¦ä¹ çš„ç—…ç†å­¦æ¨ç†å™¨ï¼Œé€šè¿‡ä¸‰é˜¶æ®µç®¡é“è¿›è¡Œè®­ç»ƒï¼šï¼ˆ1ï¼‰åœ¨350ä¸‡å›¾åƒæ–‡æœ¬å¯¹ä¸ŠæŒç»­é¢„è®­ç»ƒï¼Œä»¥æ³¨å…¥çŸ¥è¯†ï¼›ï¼ˆ2ï¼‰åœ¨50ä¸‡ä¸ªé«˜è´¨é‡çš„æ€ç»´é“¾æ ·æœ¬ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä»¥æ¿€åŠ±æ¨ç†ï¼›ï¼ˆ3ï¼‰ä½¿ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å’Œè§£è€¦å‰ªè¾‘ä»¥åŠåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜å¤šæ¨¡å¼æ¨ç†è´¨é‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°æˆ‘ä»¬æ•°æ®é›†çš„å¯¹é½è´¨é‡ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨åŒä¸€å›¾åƒ-æ ‡é¢˜è¯­æ–™åº“ä¸Šè®­ç»ƒçš„PathoCLIPï¼Œè¯¥è¯­æ–™åº“ç”¨äºæŒç»­é¢„è®­ç»ƒã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼ŒPathoCLIPå’ŒPatho-R1åœ¨å¹¿æ³›çš„ç—…ç†å­¦ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åˆ†ç±»ã€è·¨æ¨¡æ€æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œå¤šé€‰é¢˜ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨Patho-R1ä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Wenchuan-Zhang/Patho-R1%E3%80%82">https://github.com/Wenchuan-Zhang/Patho-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11404v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åŸºäºæœ€æ–°è¿›å±•çš„åŒ»å­¦å½±åƒä¸è¯­è¨€æ¨¡å‹æŠ€æœ¯åœ¨ç—…ç†å­¦é¢†åŸŸçš„å¹¿æ³›åº”ç”¨åŠå­˜åœ¨çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç—…ç†æ•™ç§‘ä¹¦å’Œä¸“å®¶æ„å»ºäº†é«˜è´¨é‡ã€ä»¥æ¨ç†ä¸ºå¯¼å‘çš„æ•°æ®é›†ï¼Œå¹¶åŸºäºæ­¤æ¨å‡ºäº†Patho-R1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬çŸ¥è¯†çŒè¾“ã€æ¨ç†æ¿€åŠ±å’Œå¤šæ¨¡æ€æ¨ç†è´¨é‡ä¼˜åŒ–ã€‚åŒæ—¶ï¼Œä¸ºäº†è¯„ä¼°æ•°æ®é›†çš„è´¨é‡ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æå‡ºäº†PathoCLIPæ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨å¤šç§ç—…ç†å­¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚é¡¹ç›®çš„ç›¸å…³ä¿¡æ¯å¯é€šè¿‡Patho-R1ä»“åº“è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å½±åƒä¸è¯­è¨€æ¨¡å‹æŠ€æœ¯åœ¨ç—…ç†å­¦é¢†åŸŸçš„åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†åˆç†æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>å½“å‰ç—…ç†æ•°æ®é›†ç¼ºä¹æ·±åº¦å’Œç»“æ„åŒ–è¯Šæ–­èŒƒå¼ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç—…ç†æ•™ç§‘ä¹¦å’Œä¸“å®¶æ„å»ºé«˜è´¨é‡ã€ä»¥æ¨ç†ä¸ºå¯¼å‘çš„æ•°æ®é›†ï¼Œä»¥è§£å†³ç°æœ‰é—®é¢˜ã€‚</li>
<li>Patho-R1æ¨¡å‹é€šè¿‡ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šçŸ¥è¯†çŒè¾“ã€æ¨ç†æ¿€åŠ±å’Œå¤šæ¨¡æ€æ¨ç†è´¨é‡ä¼˜åŒ–ã€‚</li>
<li>PathoCLIPæ¨¡å‹çš„æå‡ºç”¨äºè¯„ä¼°æ•°æ®é›†è´¨é‡ï¼Œå…¶è®­ç»ƒåŸºäºç›¸åŒçš„å›¾åƒ-æ–‡æœ¬å¯¹ç”¨äºæŒç»­é¢„è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºPathoCLIPå’ŒPatho-R1æ¨¡å‹åœ¨å¤šç§ç—…ç†å­¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬åˆ†ç±»ã€è·¨æ¨¡æ€æ£€ç´¢ã€è§†è§‰é—®ç­”å’Œå¤šé¡¹é€‰æ‹©é¢˜ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a80232b582f35b48cc5c5839d7378f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-933359ff6a19ce46474a6f31df7bb350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afdbd0559742207ec25a6342125ec9c4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Critical-Questions-Generation-A-Challenging-Reasoning-Task-for-Large-Language-Models"><a href="#Benchmarking-Critical-Questions-Generation-A-Challenging-Reasoning-Task-for-Large-Language-Models" class="headerlink" title="Benchmarking Critical Questions Generation: A Challenging Reasoning Task   for Large Language Models"></a>Benchmarking Critical Questions Generation: A Challenging Reasoning Task   for Large Language Models</h2><p><strong>Authors:Banca Calvo Figueras, Rodrigo Agerri</strong></p>
<p>The task of Critical Questions Generation (CQs-Gen) aims to foster critical thinking by enabling systems to generate questions that expose assumptions and challenge the reasoning in arguments. Despite growing interest in this area, progress has been hindered by the lack of suitable datasets and automatic evaluation standards. This work presents a comprehensive approach to support the development and benchmarking of systems for this task. We construct the first large-scale manually-annotated dataset. We also investigate automatic evaluation methods and identify a reference-based technique using large language models (LLMs) as the strategy that best correlates with human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline while showcasing the difficulty of the task. Data, code, and a public leaderboard are provided to encourage further research not only in terms of model performance, but also to explore the practical benefits of CQs-Gen for both automated reasoning and human critical thinking. </p>
<blockquote>
<p>æ‰¹åˆ¤æ€§é—®é¢˜ç”Ÿæˆï¼ˆCQs-Genï¼‰çš„ä»»åŠ¡æ—¨åœ¨é€šè¿‡ä½¿ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆæ­ç¤ºå‡è®¾å¹¶æŒ‘æˆ˜è®ºè¯æ¨ç†çš„é—®é¢˜æ¥åŸ¹å…»æ‰¹åˆ¤æ€§æ€ç»´ã€‚å°½ç®¡è¿™ä¸ªé¢†åŸŸæ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†ç”±äºç¼ºä¹åˆé€‚çš„æ•°æ®é›†å’Œè‡ªåŠ¨è¯„ä¼°æ ‡å‡†ï¼Œè¿›å±•å—åˆ°äº†é˜»ç¢ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ–¹æ³•æ¥æ”¯æŒè¯¥ä»»åŠ¡çš„ç³»ç»Ÿå¼€å‘å’Œè¯„ä¼°ã€‚æˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡æ‰‹åŠ¨æ³¨é‡Šçš„æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ï¼Œå¹¶ç¡®å®šäº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚è€ƒåŸºå‡†æŠ€æœ¯æ˜¯ä¸äººåˆ¤æ–­æœ€ç›¸å…³çš„ç­–ç•¥ã€‚æˆ‘ä»¬å¯¹11ä¸ªLLMè¿›è¡Œçš„é›¶æ ·æœ¬è¯„ä¼°å»ºç«‹äº†å¼ºå¤§çš„åŸºå‡†çº¿ï¼ŒåŒæ—¶å±•ç¤ºäº†è¯¥ä»»åŠ¡çš„éš¾åº¦ã€‚ä¸ºäº†é¼“åŠ±è¿›ä¸€æ­¥ç ”ç©¶ï¼Œä¸ä»…æ˜¯ä»æ¨¡å‹æ€§èƒ½çš„è§’åº¦ï¼Œè¿˜ä»æ¢ç´¢CQs-Genå¯¹è‡ªåŠ¨åŒ–æ¨ç†å’Œäººç±»æ‰¹åˆ¤æ€§æ€ç»´çš„å®é™…ç›Šå¤„ï¼Œæˆ‘ä»¬æä¾›äº†æ•°æ®ã€ä»£ç å’Œå…¬å¼€æ’è¡Œæ¦œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11341v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰¹åˆ¤æ€§é—®é¢˜ç”Ÿæˆï¼ˆCQs-Genï¼‰çš„ä»»åŠ¡ç›®æ ‡ï¼Œå³åŸ¹å…»ç³»ç»Ÿçš„æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ï¼Œç”Ÿæˆèƒ½å¤Ÿæ­éœ²å‡è®¾å’Œè´¨ç–‘è®ºè¯æ¨ç†çš„é—®é¢˜ã€‚å°½ç®¡è¯¥é¢†åŸŸæ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†ç”±äºç¼ºä¹åˆé€‚çš„æ•°æ®é›†å’Œè‡ªåŠ¨è¯„ä¼°æ ‡å‡†ï¼Œè¿›å±•å—åˆ°äº†é˜»ç¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…¨é¢çš„æ–¹æ³•æ¥æ”¯æŒè¯¥ä»»åŠ¡çš„ç³»ç»Ÿå¼€å‘å’ŒåŸºå‡†æµ‹è¯•ã€‚æœ¬ç ”ç©¶æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶æ¢è®¨äº†è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ï¼Œç¡®å®šäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‚è€ƒæŠ€æœ¯ä½œä¸ºä¸äººç±»åˆ¤æ–­æœ€ç›¸å…³çš„ç­–ç•¥ã€‚å¯¹11ç§LLMsçš„é›¶æ ·æœ¬è¯„ä¼°å»ºç«‹äº†å¼ºå¤§çš„åŸºå‡†çº¿ï¼ŒåŒæ—¶å±•ç¤ºäº†è¯¥ä»»åŠ¡çš„éš¾åº¦ã€‚æä¾›æ•°æ®ã€ä»£ç å’Œå…¬å¼€æ’è¡Œæ¦œï¼Œä»¥é¼“åŠ±æ¨¡å‹æ€§èƒ½æ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œå¹¶æ¢ç´¢CQs-Genå¯¹è‡ªåŠ¨åŒ–æ¨ç†å’Œäººç±»æ‰¹åˆ¤æ€§æ€ç»´çš„å®é™…ç›Šå¤„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰¹åˆ¤æ€§é—®é¢˜ç”Ÿæˆï¼ˆCQs-Genï¼‰æ—¨åœ¨é€šè¿‡ç”Ÿæˆé—®é¢˜åŸ¹å…»ç³»ç»Ÿçš„æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ã€‚</li>
<li>ç¼ºå°‘åˆé€‚çš„æ•°æ®é›†å’Œè‡ªåŠ¨è¯„ä¼°æ ‡å‡†æ˜¯é˜»ç¢è¯¥é¢†åŸŸè¿›å±•çš„ä¸»è¦åŸå› ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†é¦–ä¸ªå¤§è§„æ¨¡æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†æ¥æ”¯æŒCQs-Genä»»åŠ¡ã€‚</li>
<li>è‡ªåŠ¨è¯„ä¼°æ–¹æ³•è¢«æ¢è®¨ï¼Œå…¶ä¸­åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‚è€ƒæŠ€æœ¯è¢«ç¡®å®šä¸ºä¸äººç±»åˆ¤æ–­æœ€ç›¸å…³çš„ç­–ç•¥ã€‚</li>
<li>å¯¹11ç§LLMsçš„é›¶æ ·æœ¬è¯„ä¼°æ˜¾ç¤ºäº†è¯¥ä»»åŠ¡çš„éš¾åº¦ã€‚</li>
<li>æä¾›æ•°æ®ã€ä»£ç å’Œå…¬å¼€æ’è¡Œæ¦œä»¥é¼“åŠ±æ›´å¤šç ”ç©¶ï¼Œä¸ä»…å…³æ³¨æ¨¡å‹æ€§èƒ½ï¼Œè¿˜å…³æ³¨CQs-Genå¯¹è‡ªåŠ¨åŒ–æ¨ç†å’Œäººç±»æ‰¹åˆ¤æ€§æ€ç»´çš„ç›Šå¤„ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºCQs-Genä»»åŠ¡çš„å‘å±•æä¾›äº†å…¨é¢æ”¯æŒï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€è¯„ä¼°æ–¹æ³•å’Œå…¬å¼€å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11341">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd3ea7fad8589fe8fe258bf4d7508ab3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55aca43b4d15c49d90873ab83e4e8df3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f75b14ea7300c4ac475badd73ada6fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecf86bc79021382d2f309f84d28b3298.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Search-and-Refine-During-Think-Autonomous-Retrieval-Augmented-Reasoning-of-LLMs"><a href="#Search-and-Refine-During-Think-Autonomous-Retrieval-Augmented-Reasoning-of-LLMs" class="headerlink" title="Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning   of LLMs"></a>Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning   of LLMs</h2><p><strong>Authors:Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang</strong></p>
<p>Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new &#96;&#96;search-and-refine-during-thinkâ€™â€™ paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶æœ¬è´¨ä¸Šå—åˆ°çŸ¥è¯†åº“çš„é™åˆ¶ã€‚æ£€ç´¢å¢å¼ºæ¨ç†é€šè¿‡å…è®¸å¤§å‹è¯­è¨€æ¨¡å‹æŸ¥è¯¢å¤–éƒ¨èµ„æºæ¥ç¼“è§£è¿™ä¸€é™åˆ¶ï¼Œä½†ç°æœ‰æ–¹æ³•ç»å¸¸æ£€ç´¢åˆ°ä¸ç›¸å…³æˆ–å˜ˆæ‚çš„ä¿¡æ¯ï¼Œé˜»ç¢äº†å‡†ç¡®çš„æ¨ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AutoRefineï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨æ–°å‹â€œæ€è€ƒè¿‡ç¨‹ä¸­çš„æœç´¢ä¸å®Œå–„â€èŒƒå¼çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ã€‚AutoRefineåœ¨è¿ç»­çš„æœç´¢è°ƒç”¨ä¹‹é—´å¼•å…¥äº†æ˜ç¡®çš„çŸ¥è¯†å®Œå–„æ­¥éª¤ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿­ä»£åœ°è¿‡æ»¤ã€æç‚¼å’Œæ•´ç†è¯æ®ï¼Œç„¶åç”Ÿæˆç­”æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œç»“åˆäº†å®šåˆ¶çš„æ£€ç´¢ç‰¹å®šå¥–åŠ±å’Œç­”æ¡ˆæ­£ç¡®æ€§å¥–åŠ±ã€‚åœ¨å•è·³å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAutoRefineæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„å¤šè·³æ¨ç†åœºæ™¯ä¸­ã€‚è¯¦ç»†åˆ†æè¡¨æ˜ï¼ŒAutoRefineèƒ½å¤Ÿè¿›è¡Œé¢‘ç¹çš„é«˜è´¨é‡æœç´¢ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°åˆæˆè¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11277v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºAutoRefineçš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨æ–°çš„â€œè¾¹æœç´¢è¾¹ç»†åŒ–æ€è€ƒâ€æ¨¡å¼æ¥ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ–¹é¢çš„å›ºæœ‰å±€é™æ€§ã€‚è¯¥æ¡†æ¶åœ¨è¿ç»­æœç´¢è°ƒç”¨ä¹‹é—´å¼•å…¥æ˜ç¡®çš„çŸ¥è¯†ç»†åŒ–æ­¥éª¤ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿­ä»£åœ°è¿‡æ»¤ã€æç‚¼å’Œæ•´ç†è¯æ®ï¼Œç”Ÿæˆç­”æ¡ˆã€‚åŒæ—¶ï¼Œé€šè¿‡ç»“åˆé’ˆå¯¹æ£€ç´¢çš„ç‰¹å®šå¥–åŠ±å’ŒåŸºäºç­”æ¡ˆæ­£ç¡®æ€§çš„å¥–åŠ±ï¼Œä½¿ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ã€‚åœ¨å•è·³å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAutoRefineæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„å¤šè·³æ¨ç†åœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶å…·å¤‡ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶çŸ¥è¯†åº“çš„é™åˆ¶å½±å“äº†æ€§èƒ½ã€‚</li>
<li>æ£€ç´¢å¢å¼ºæ¨ç†é€šè¿‡å…è®¸è¯­è¨€æ¨¡å‹æŸ¥è¯¢å¤–éƒ¨èµ„æºæ¥å‡è½»è¿™ä¸€é™åˆ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç»å¸¸æ£€ç´¢ä¸ç›¸å…³æˆ–å˜ˆæ‚çš„ä¿¡æ¯ï¼Œé˜»ç¢å‡†ç¡®æ¨ç†ã€‚</li>
<li>AutoRefineæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨â€œè¾¹æœç´¢è¾¹ç»†åŒ–æ€è€ƒâ€æ¨¡å¼æ¥æé«˜æ£€ç´¢è´¨é‡ã€‚</li>
<li>AutoRefineåœ¨è¿ç»­æœç´¢ä¹‹é—´å¼•å…¥çŸ¥è¯†ç»†åŒ–æ­¥éª¤ï¼Œå®ç°è¯æ®çš„è¿­ä»£è¿‡æ»¤ã€æç‚¼å’Œæ•´ç†ã€‚</li>
<li>é€šè¿‡ç»“åˆæ£€ç´¢ç‰¹å®šå¥–åŠ±å’Œç­”æ¡ˆæ­£ç¡®æ€§å¥–åŠ±ï¼Œä½¿ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¥æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1b7f6e97f2feb3313c06612ab51b480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c630524472572e4123adc656bbb11014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d42cacf66dffd249ce76343aae3313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4446c1de7272a2a37d42415d65e1827.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5849377557489b948ec35a0555161fd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Is-PRM-Necessary-Problem-Solving-RL-Implicitly-Induces-PRM-Capability-in-LLMs"><a href="#Is-PRM-Necessary-Problem-Solving-RL-Implicitly-Induces-PRM-Capability-in-LLMs" class="headerlink" title="Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability   in LLMs"></a>Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability   in LLMs</h2><p><strong>Authors:Zhangying Feng, Qianglong Chen, Ning Lu, Yongqian Li, Siqi Cheng, Shuangmu Peng, Duyu Tang, Shengcai Liu, Zhirui Zhang</strong></p>
<p>The development of reasoning capabilities represents a critical frontier in large language models (LLMs) research, where reinforcement learning (RL) and process reward models (PRMs) have emerged as predominant methodological frameworks. Contrary to conventional wisdom, empirical evidence from DeepSeek-R1 demonstrates that pure RL training focused on mathematical problem-solving can progressively enhance reasoning abilities without PRM integration, challenging the perceived necessity of process supervision. In this study, we conduct a systematic investigation of the relationship between RL training and PRM capabilities. Our findings demonstrate that problem-solving proficiency and process supervision capabilities represent complementary dimensions of reasoning that co-evolve synergistically during pure RL training. In particular, current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To address this limitation, we propose Self-PRM, an introspective framework in which models autonomously evaluate and rerank their generated solutions through self-reward mechanisms. Although Self-PRM consistently improves the accuracy of the benchmark (particularly with larger sample sizes), analysis exposes persistent challenges: The approach exhibits low precision (&lt;10%) on difficult problems, frequently misclassifying flawed solutions as valid. These analyses underscore the need for continued RL scaling to improve reward alignment and introspective accuracy. Overall, our findings suggest that PRM may not be essential for enhancing complex reasoning, as pure RL not only improves problem-solving skills but also inherently fosters robust PRM capabilities. We hope these findings provide actionable insights for building more reliable and self-aware complex reasoning models. </p>
<blockquote>
<p>æ¨ç†èƒ½åŠ›çš„å‘å±•åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç ”ç©¶ä¸­ä»£è¡¨äº†å…³é”®çš„å‰æ²¿é¢†åŸŸï¼Œå…¶ä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å·²æˆä¸ºä¸»è¦çš„æ–¹æ³•è®ºæ¡†æ¶ã€‚ä¸å¸¸è¯†ç›¸åï¼Œæ¥è‡ªDeepSeek-R1çš„å®è¯è¯æ®è¡¨æ˜ï¼Œä¸“æ³¨äºæ•°å­¦é—®é¢˜è§£å†³çš„çº¯RLè®­ç»ƒå¯ä»¥é€æ­¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€æ•´åˆPRMï¼Œè¿™æŒ‘æˆ˜äº†è¿‡ç¨‹ç›‘ç£çš„å¿…è¦æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹RLè®­ç»ƒå’ŒPRMèƒ½åŠ›ä¹‹é—´çš„å…³ç³»è¿›è¡Œäº†ç³»ç»Ÿçš„è°ƒæŸ¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé—®é¢˜è§£å†³èƒ½åŠ›å’Œè¿‡ç¨‹ç›‘ç£èƒ½åŠ›ä»£è¡¨äº†æ¨ç†çš„äº’è¡¥ç»´åº¦ï¼Œåœ¨çº¯RLè®­ç»ƒè¿‡ç¨‹ä¸­ååŒæ¼”åŒ–ã€‚ç‰¹åˆ«æ˜¯ï¼Œå½“åº”ç”¨äºæœ€æ–°æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1å’ŒQwQ-32Bï¼‰æ—¶ï¼Œå½“å‰çš„PRMè¡¨ç°ç”šè‡³ä¸å¦‚å¤šæ•°æŠ•ç¥¨ç­‰ç®€å•åŸºçº¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Self-PRMï¼Œè¿™æ˜¯ä¸€ç§å†…çœæ¡†æ¶ï¼Œæ¨¡å‹é€šè¿‡è‡ªæˆ‘å¥–åŠ±æœºåˆ¶è‡ªä¸»è¯„ä¼°å¹¶é‡æ–°æ’åºå…¶ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡Self-PRMå§‹ç»ˆæé«˜äº†åŸºå‡†æµ‹è¯•çš„å‡†ç¡®æ€§ï¼ˆç‰¹åˆ«æ˜¯æ ·æœ¬é‡è¾ƒå¤§æ—¶ï¼‰ï¼Œä½†åˆ†æè¡¨æ˜ä»å­˜åœ¨æŒä¹…æŒ‘æˆ˜ï¼šè¯¥æ–¹æ³•åœ¨éš¾é¢˜ä¸Šçš„ç²¾åº¦è¾ƒä½ï¼ˆ&lt;10%ï¼‰ï¼Œç»å¸¸å°†é”™è¯¯çš„è§£å†³æ–¹æ¡ˆé”™è¯¯åœ°åˆ†ç±»ä¸ºæœ‰æ•ˆã€‚è¿™äº›åˆ†æå¼ºè°ƒï¼Œä¸ºäº†æ”¹å–„å¥–åŠ±å¯¹é½å’Œå†…çœç²¾åº¦ï¼Œéœ€è¦ç»§ç»­è¿›è¡ŒRLæ‰©å±•ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒPRMå¯èƒ½å¹¶ä¸æ˜¯å¢å¼ºå¤æ‚æ¨ç†èƒ½åŠ›çš„å…³é”®ï¼Œå› ä¸ºçº¯RLä¸ä»…æé«˜äº†é—®é¢˜è§£å†³æŠ€èƒ½ï¼Œè€Œä¸”å†…åœ¨åœ°åŸ¹å…»äº†ç¨³å¥çš„PRMèƒ½åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å‘ç°èƒ½ä¸ºæ„å»ºæ›´å¯é ã€æ›´è‡ªè§‰å¤æ‚æ¨ç†æ¨¡å‹æä¾›å®ç”¨è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11227v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å¯»æ±‚R1æ¨¡å‹çš„å®è¯ç ”ç©¶ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œå³ä½¿ä¸ç»“åˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä¹Ÿèƒ½å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ç³»ç»Ÿè°ƒæŸ¥äº†RLè®­ç»ƒå’ŒPRMèƒ½åŠ›ä¹‹é—´çš„å…³ç³»ï¼Œå‘ç°è§£å†³é—®é¢˜çš„èƒ½åŠ›å’Œè¿‡ç¨‹ç›‘ç£èƒ½åŠ›ä»£è¡¨äº†ååŒæ¼”åŒ–çš„äº’è¡¥æ¨ç†ç»´åº¦ã€‚é’ˆå¯¹PRMåœ¨å…ˆè¿›æ¨¡å‹ä¸Šçš„æ€§èƒ½ä¸ä½³é—®é¢˜ï¼Œæå‡ºäº†è‡ªä¸»è¯„ä¼°è§£å†³æ–¹æ¡ˆçš„Self-PRMæ¡†æ¶ã€‚è™½ç„¶æ­¤æ¡†æ¶æ”¹è¿›äº†åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§ï¼Œä½†ä»å­˜åœ¨ç²¾åº¦ä½å’Œè¯¯åˆ¤é—®é¢˜ã€‚ç ”ç©¶è®¤ä¸ºï¼ŒPRMå¯èƒ½å¹¶éå¢å¼ºå¤æ‚æ¨ç†èƒ½åŠ›çš„å…³é”®ï¼Œçº¯RLä¸ä»…æå‡è§£é¢˜èƒ½åŠ›ï¼Œè¿˜å†…åœ¨åŸ¹å…»ç¨³å¥çš„PRMèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å‘å±•ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>å®è¯ç ”ç©¶æ˜¾ç¤ºï¼Œçº¯RLè®­ç»ƒèƒ½æå‡æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›ï¼ŒæŒ‘æˆ˜äº†è¿‡ç¨‹ç›‘ç£çš„å¿…è¦æ€§ã€‚</li>
<li>é—®é¢˜è§£å†³èƒ½åŠ›ä¸è¿‡ç¨‹ç›‘ç£èƒ½åŠ›æ˜¯äº’è¡¥çš„æ¨ç†ç»´åº¦ï¼ŒååŒæ¼”åŒ–ã€‚</li>
<li>å½“å‰PRMåœ¨å…ˆè¿›æ¨¡å‹ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œä½äºç®€å•åŸºçº¿å¦‚å¤šæ•°æŠ•ç¥¨ã€‚</li>
<li>æå‡ºçš„Self-PRMæ¡†æ¶æ—¨åœ¨è‡ªä¸»è¯„ä¼°è§£å†³æ–¹æ¡ˆï¼Œè™½æ”¹è¿›åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§ï¼Œä½†å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Self-PRMåœ¨ä½ç²¾åº¦å’Œè¯¯åˆ¤é—®é¢˜ä¸Šä»æœ‰ä¸è¶³ï¼Œéœ€è¦æ›´å¤šRLæ‰©å±•æ¥æé«˜å¥–åŠ±å¯¹é½å’Œè‡ªçœå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3578b6e81a922865407cf3447a34c59b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13a2342ccfd59571e78db1fac1d3ec81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73603b8dea5f2db2069751ab99600c87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2121916a28af1e52b13272cc484dd303.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd95c629cdd9b8989b2e00cb17134d5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CompAlign-Improving-Compositional-Text-to-Image-Generation-with-a-Complex-Benchmark-and-Fine-Grained-Feedback"><a href="#CompAlign-Improving-Compositional-Text-to-Image-Generation-with-a-Complex-Benchmark-and-Fine-Grained-Feedback" class="headerlink" title="CompAlign: Improving Compositional Text-to-Image Generation with a   Complex Benchmark and Fine-Grained Feedback"></a>CompAlign: Improving Compositional Text-to-Image Generation with a   Complex Benchmark and Fine-Grained Feedback</h2><p><strong>Authors:Yixin Wan, Kai-Wei Chang</strong></p>
<p>State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuestâ€™s feedback as preference signals to improve diffusion modelsâ€™ compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„T2Iæ¨¡å‹å·²ç»èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å‡†ç¡®æç»˜åŒ…å«å¤šä¸ªå¯¹è±¡ã€å±æ€§å’Œç©ºé—´å…³ç³»çš„ç»„åˆåœºæ™¯æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†CompAlignï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è¯„ä¼°3Dç©ºé—´å…³ç³»æç»˜èƒ½åŠ›ä¸ºé‡ç‚¹çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæé«˜ç»„åˆå›¾åƒç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚CompAlignåŒ…å«900ä¸ªå¤æ‚çš„è·¨ä¸»ä½“å›¾åƒç”Ÿæˆæç¤ºï¼Œç»“åˆäº†æ•°å€¼å’Œ3Dç©ºé—´å…³ç³»ä»¥åŠä¸åŒçš„å±æ€§ç»‘å®šã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œèå…¥äº†3ä¸ªåŠä»¥ä¸Šç”Ÿæˆä¸»ä½“çš„ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¸¦æœ‰å¤æ‚çš„3Dç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†CompQuestï¼Œè¿™æ˜¯ä¸€ä¸ªå¯è§£é‡Šä¸”å‡†ç¡®çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒå°†å¤æ‚çš„æç¤ºåˆ†è§£æˆåŸå­å­é—®é¢˜ï¼Œç„¶ååˆ©ç”¨MLLMå¯¹æ¨¡å‹ç”Ÿæˆå›¾åƒä¸­æ¯ä¸ªç”Ÿæˆå…ƒç´ æ–¹é¢çš„æ­£ç¡®æ€§æä¾›ç²¾ç»†çš„äºŒå…ƒåé¦ˆã€‚è¿™å®ç°äº†ç”Ÿæˆå›¾åƒä¸ç»„åˆæç¤ºä¹‹é—´å¯¹é½çš„ç²¾ç¡®é‡åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½¿ç”¨CompQueståé¦ˆä½œä¸ºåå¥½ä¿¡å·çš„å¯¹é½æ¡†æ¶ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹çš„ç»„åˆå›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚ä½¿ç”¨å¯è°ƒæ•´çš„æ¯å¼ å›¾åƒåå¥½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜“äºæ‰©å±•ï¼Œå¹¶ä¸”é’ˆå¯¹ä¸åŒä»»åŠ¡å…·æœ‰çµæ´»æ€§ã€‚å¯¹9ä¸ªT2Iæ¨¡å‹çš„è¯„ä¼°è¡¨æ˜ï¼šï¼ˆ1ï¼‰æ¨¡å‹åœ¨å…·æœ‰æ›´å¤æ‚3Dç©ºé—´é…ç½®çš„ç»„åˆä»»åŠ¡ä¸Šé¢ä¸´æ›´å¤§çš„æŒ‘æˆ˜ï¼›ï¼ˆ2ï¼‰å¼€æºå¯è®¿é—®æ¨¡å‹å’Œå°é—­æºä»£ç å•†ä¸šæ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚å…³äºä½¿ç”¨CompAlignè¿›è¡Œæ¨¡å‹å¯¹é½çš„è¿›ä¸€æ­¥å®è¯ç ”ç©¶äº§ç”Ÿäº†ä»¤äººé¼“èˆçš„ç»“æœï¼šå¯¹é½åçš„æ‰©æ•£æ¨¡å‹åœ¨ç»„åˆå‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11178v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æè¿°äº†ä¸€ç§åä¸ºCompAlignçš„è¯„ä¼°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç»„åˆå›¾åƒç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†å¼ºè°ƒå¯¹ä¸‰ç»´ç©ºé—´å…³ç³»çš„æç»˜èƒ½åŠ›ï¼ŒåŒ…å«æŒ‘æˆ˜æ€§çš„å¤šä¸»é¢˜å›¾åƒç”Ÿæˆæç¤ºã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCompQuestçš„è¯„ä¼°æ¡†æ¶ï¼Œå¯ä»¥ç²¾ç¡®é‡åŒ–ç”Ÿæˆå›¾åƒä¸ç»„åˆæç¤ºä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ä½¿ç”¨CompQueståé¦ˆä½œä¸ºåå¥½ä¿¡å·çš„å¯¹é½æ¡†æ¶ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹çš„ç»„åˆå›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚å¯¹ç°æœ‰æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå¤æ‚çš„ä¸‰ç»´ç©ºé—´é…ç½®ç»„åˆä»»åŠ¡æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå¼€æºå¯è®¿é—®æ¨¡å‹ä¸é—­æºå•†ä¸šæ¨¡å‹ä¹‹é—´å­˜åœ¨æ€§èƒ½å·®è·ã€‚ä½¿ç”¨CompAlignè¿›è¡Œæ¨¡å‹å¯¹é½çš„è¿›ä¸€æ­¥ç ”ç©¶æ˜¾ç¤ºå‡ºå¯å–œçš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CompAlignæ˜¯ä¸€ä¸ªè¯„ä¼°åŸºå‡†ï¼Œå¼ºè°ƒå¯¹ä¸‰ç»´ç©ºé—´å…³ç³»çš„æç»˜èƒ½åŠ›ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç»„åˆåœºæ™¯å›¾åƒç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>CompAlignåŒ…å«æŒ‘æˆ˜æ€§çš„å¤šä¸»é¢˜å›¾åƒç”Ÿæˆæç¤ºï¼Œè¿™äº›æç¤ºç»“åˆäº†æ•°å­—å’Œä¸‰ç»´ç©ºé—´å…³ç³»ä»¥åŠä¸åŒçš„å±æ€§ç»‘å®šã€‚</li>
<li>CompQuestæ˜¯ä¸€ç§è¯„ä¼°æ¡†æ¶ï¼Œå¯ä»¥ç²¾ç¡®é‡åŒ–ç”Ÿæˆå›¾åƒä¸ç»„åˆæç¤ºä¹‹é—´çš„å¯¹é½ç¨‹åº¦ï¼Œé€šè¿‡å°†å¤æ‚çš„æç¤ºåˆ†è§£ä¸ºåŸå­å­é—®é¢˜å¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æä¾›ç²¾ç»†çš„äºŒå…ƒåé¦ˆæ¥å®ç°ã€‚</li>
<li>æå‡ºçš„å¯¹é½æ¡†æ¶ä½¿ç”¨CompQuestçš„åé¦ˆä½œä¸ºåå¥½ä¿¡å·ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹çš„ç»„åˆå›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œä¸”è¯¥æ–¹æ³•å¯è°ƒæ•´ã€æ˜“äºæ‰©å±•ï¼Œé€‚ç”¨äºä¸åŒä»»åŠ¡ã€‚</li>
<li>å¯¹ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå¤„ç†å…·æœ‰å¤æ‚ä¸‰ç»´ç©ºé—´é…ç½®çš„å›¾åƒç”Ÿæˆä»»åŠ¡æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>å¼€æºå¯è®¿é—®æ¨¡å‹ä¸é—­æºå•†ä¸šæ¨¡å‹åœ¨ç»„åˆå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå­˜åœ¨æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4dd40edf2cbaffb4a7709cc361e0b38c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c7baf423cbd584f86124d5252244cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37436045e6041dc7c5141960bd962469.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1385a7e7509115533a2a849a2acdc621.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Real-Time-Verification-of-Embodied-Reasoning-for-Generative-Skill-Acquisition"><a href="#Real-Time-Verification-of-Embodied-Reasoning-for-Generative-Skill-Acquisition" class="headerlink" title="Real-Time Verification of Embodied Reasoning for Generative Skill   Acquisition"></a>Real-Time Verification of Embodied Reasoning for Generative Skill   Acquisition</h2><p><strong>Authors:Bo Yue, Shuqi Guo, Kaiyu Hu, Chujiao Wang, Benyou Wang, Kui Jia, Guiliang Liu</strong></p>
<p>Generative skill acquisition enables embodied agents to actively learn a scalable and evolving repertoire of control skills, crucial for the advancement of large decision models. While prior approaches often rely on supervision signals from generalist agents (e.g., LLMs), their effectiveness in complex 3D environments remains unclear; exhaustive evaluation incurs substantial computational costs, significantly hindering the efficiency of skill learning. Inspired by recent successes in verification models for mathematical reasoning, we propose VERGSA (Verifying Embodied Reasoning in Generative Skill Acquisition), a framework that systematically integrates real-time verification principles into embodied skill learning. VERGSA establishes 1) a seamless extension from verification of mathematical reasoning into embodied learning by dynamically incorporating contextually relevant tasks into prompts and defining success metrics for both subtasks and overall tasks, and 2) an automated, scalable reward labeling scheme that synthesizes dense reward signals by iteratively finalizing the contribution of scene configuration and subtask learning to overall skill acquisition. To the best of our knowledge, this approach constitutes the first comprehensive training dataset for verification-driven generative skill acquisition, eliminating arduous manual reward engineering. Experiments validate the efficacy of our approach: 1) the exemplar task pool improves the average task success rates by 21%, 2) our verification model boosts success rates by 24% for novel tasks and 36% for encountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification quality. </p>
<blockquote>
<p>ç”ŸæˆæŠ€èƒ½è·å–ä½¿å®ä½“ä»£ç†èƒ½å¤Ÿä¸»åŠ¨å­¦ä¹ å¯ä¼¸ç¼©å’Œä¸æ–­å‘å±•çš„æ§åˆ¶æŠ€èƒ½ç»„åˆï¼Œè¿™å¯¹äºå¤§å‹å†³ç­–æ¨¡å‹çš„è¿›æ­¥è‡³å…³é‡è¦ã€‚è™½ç„¶ä¹‹å‰çš„æ–¹æ³•ç»å¸¸ä¾èµ–äºé€šç”¨ä»£ç†ï¼ˆä¾‹å¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„ç›‘ç£ä¿¡å·ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚çš„3Dç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å°šä¸æ¸…æ¥šï¼›è¯¦å°½çš„è¯„ä¼°ä¼šäº§ç”Ÿå·¨å¤§çš„è®¡ç®—æˆæœ¬ï¼Œä»è€Œä¸¥é‡é˜»ç¢æŠ€èƒ½å­¦ä¹ çš„æ•ˆç‡ã€‚å—æœ€è¿‘æ•°å­¦æ¨ç†éªŒè¯æ¨¡å‹æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†VERGSAï¼ˆç”ŸæˆæŠ€èƒ½è·å–ä¸­çš„å®ä½“éªŒè¯ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å®æ—¶éªŒè¯åŸåˆ™ç³»ç»Ÿåœ°èå…¥å®ä½“æŠ€èƒ½å­¦ä¹ æ¡†æ¶ã€‚VERGSAå»ºç«‹äº†1ï¼‰ä»æ•°å­¦æ¨ç†éªŒè¯æ— ç¼æ‰©å±•åˆ°å®ä½“å­¦ä¹ çš„æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€å°†ä¸Šä¸‹æ–‡ç›¸å…³ä»»åŠ¡çº³å…¥æç¤ºå¹¶å®šä¹‰å­ä»»åŠ¡å’Œæ€»ä½“ä»»åŠ¡çš„æˆåŠŸæŒ‡æ ‡ï¼Œä»¥åŠ2ï¼‰ä¸€ç§è‡ªåŠ¨åŒ–ã€å¯ä¼¸ç¼©çš„å¥–åŠ±æ ‡ç­¾æ–¹æ¡ˆï¼Œé€šè¿‡è¿­ä»£ç¡®å®šåœºæ™¯é…ç½®å’Œå­ä»»åŠ¡å­¦ä¹ å¯¹æ•´ä½“æŠ€èƒ½è·å–çš„è´¡çŒ®ï¼Œä»è€Œåˆæˆå¯†é›†çš„å¥–åŠ±ä¿¡å·ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ç§æ–¹æ³•æ„æˆäº†ä»¥éªŒè¯ä¸ºé©±åŠ¨ç”ŸæˆæŠ€èƒ½è·å–çš„é¦–ä¸ªç»¼åˆè®­ç»ƒæ•°æ®é›†ï¼Œæ¶ˆé™¤äº†ç¹ççš„æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹ã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼š1ï¼‰ç¤ºä¾‹ä»»åŠ¡æ± æé«˜äº†å¹³å‡ä»»åŠ¡æˆåŠŸç‡21%ï¼Œ2ï¼‰æˆ‘ä»¬çš„éªŒè¯æ¨¡å‹æé«˜äº†æ–°ä»»åŠ¡çš„æˆåŠŸç‡24%ï¼Œå¯¹å·²é‡åˆ°çš„ä»»åŠ¡æˆåŠŸç‡æé«˜36%ï¼Œ3ï¼‰åœ¨éªŒè¯è´¨é‡æ–¹é¢ä¼˜äºä»¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºè¯„åˆ¤å‘˜çš„åŸºå‡†çº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11175v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”ŸæˆæŠ€èƒ½è·å–ï¼ˆGenerative Skill Acquisitionï¼‰åœ¨å®ä½“ä»£ç†ï¼ˆembodied agentsï¼‰ä¸­çš„åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºé€šç”¨ä»£ç†ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹LLMsï¼‰çš„ç›‘ç£ä¿¡å·ï¼Œåœ¨å¤æ‚çš„ä¸‰ç»´ç¯å¢ƒä¸­æ•ˆæœä¸ä½³ä¸”è¯„ä¼°æˆæœ¬é«˜æ˜‚ã€‚ä¸ºæ­¤ï¼Œå—æ•°å­¦æ¨ç†éªŒè¯æ¨¡å‹æˆåŠŸçš„å¯å‘ï¼Œæ–‡ç« æå‡ºäº†VERGSAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å®æ—¶éªŒè¯åŸåˆ™ç³»ç»Ÿåœ°é›†æˆåˆ°å®ä½“æŠ€èƒ½å­¦ä¹ ä¸­ã€‚VERGSAæ¡†æ¶ä¸ä»…å®ç°äº†æ•°å­¦æ¨ç†éªŒè¯åˆ°å®ä½“å­¦ä¹ çš„æ— ç¼æ‰©å±•ï¼Œè¿˜é€šè¿‡è‡ªåŠ¨ã€å¯æ‰©å±•çš„å¥–åŠ±æ ‡æ³¨æ–¹æ¡ˆï¼Œåˆæˆå¯†é›†å¥–åŠ±ä¿¡å·ï¼Œç®€åŒ–äº†å¯†é›†å¥–åŠ±ä¿¡å·çš„ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒéªŒè¯ï¼ŒVERGSAæé«˜äº†ä»»åŠ¡æˆåŠŸç‡å¹¶ä¼˜äºLLMä½œä¸ºè¯„åˆ¤å‘˜çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”ŸæˆæŠ€èƒ½è·å–å…è®¸å®ä½“ä»£ç†ä¸»åŠ¨å­¦ä¹ å¯æ‹“å±•å’Œä¸æ–­å‘å±•çš„æ§åˆ¶æŠ€èƒ½ï¼Œè¿™å¯¹å¤§å‹å†³ç­–æ¨¡å‹çš„å‘å±•è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–é€šç”¨ä»£ç†çš„ç›‘ç£ä¿¡å·åœ¨å¤æ‚ä¸‰ç»´ç¯å¢ƒä¸­çš„æ•ˆæœå°šä¸æ¸…æ¥šï¼Œä¸”å…¨é¢è¯„ä¼°ä¼šå¸¦æ¥å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>VERGSAæ¡†æ¶å°†å®æ—¶éªŒè¯åŸåˆ™é›†æˆåˆ°å®ä½“æŠ€èƒ½å­¦ä¹ ä¸­ï¼Œå®ç°äº†ä»æ•°å­¦æ¨ç†éªŒè¯åˆ°å®ä½“å­¦ä¹ çš„æ— ç¼æ‰©å±•ã€‚</li>
<li>VERGSAæ¡†æ¶é€šè¿‡åŠ¨æ€èå…¥ä¸Šä¸‹æ–‡ç›¸å…³ä»»åŠ¡åˆ°æç¤ºä¸­å¹¶å®šä¹‰å­ä»»åŠ¡å’Œæ€»ä½“ä»»åŠ¡çš„æˆåŠŸæŒ‡æ ‡ï¼Œå¢å¼ºäº†æŠ€èƒ½å­¦ä¹ çš„æ•ˆç‡ã€‚</li>
<li>VERGSAæ¡†æ¶æä¾›äº†ä¸€ç§è‡ªåŠ¨ã€å¯æ‰©å±•çš„å¥–åŠ±æ ‡æ³¨æ–¹æ¡ˆï¼Œé€šè¿‡è¿­ä»£ç¡®å®šåœºæ™¯é…ç½®å’Œå­ä»»åŠ¡å­¦ä¹ å¯¹æ•´ä½“æŠ€èƒ½è·å–çš„è´¡çŒ®ï¼Œç®€åŒ–äº†å¯†é›†å¥–åŠ±ä¿¡å·çš„ç”Ÿæˆã€‚</li>
<li>VERGSAæ¡†æ¶æ„æˆäº†ä¸€ä¸ªå…¨é¢çš„è®­ç»ƒæ•°æ®é›†ï¼Œç”¨äºéªŒè¯é©±åŠ¨ç”ŸæˆæŠ€èƒ½è·å–ï¼Œæ¶ˆé™¤äº†ç¹ççš„æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ba42f4b646e412f876dd3927ff730af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61d3c798aac330c45b98228cc0469fb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff2551c4a097ae256a4af16f2fb36bb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b33d4c9e3584d370660f88c2a996f2de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66561176e94bbca5ee54e18b9197e17f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c6cc3c3e72dd601b11e1019a00b574ce.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GraphOracle-A-Foundation-Model-for-Knowledge-Graph-Reasoning"><a href="#GraphOracle-A-Foundation-Model-for-Knowledge-Graph-Reasoning" class="headerlink" title="GraphOracle: A Foundation Model for Knowledge Graph Reasoning"></a>GraphOracle: A Foundation Model for Knowledge Graph Reasoning</h2><p><strong>Authors:Enjun Du, Siyi Liu, Yongqi Zhang</strong></p>
<p>Foundation models have demonstrated remarkable capabilities across various domains, but developing analogous models for knowledge graphs presents unique challenges due to their dynamic nature and the need for cross-domain reasoning. To address these issues, we introduce \textbf{\textsc{GraphOracle}}, a relation-centric foundation model that unifies reasoning across knowledge graphs by converting them into Relation-Dependency Graphs (RDG), explicitly encoding compositional patterns with fewer edges than prior methods. A query-dependent attention mechanism is further developed to learn inductive representations for both relations and entities. Pre-training on diverse knowledge graphs, followed by minutes-level fine-tuning, enables effective generalization to unseen entities, relations, and entire graphs. Through comprehensive experiments on 31 diverse benchmarks spanning transductive, inductive, and cross-domain settings, we demonstrate consistent state-of-the-art performance with minimal adaptation, improving the prediction performance by up to 35% compared to the strongest baselines. </p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨çŸ¥è¯†å›¾è°±ä¸­å¼€å‘ç±»ä¼¼æ¨¡å‹å´é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰åŠ¨æ€æ€§å’Œè·¨åŸŸæ¨ç†çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…³ç³»ä¸­å¿ƒçš„åŸºç¡€æ¨¡å‹<strong>GraphOracle</strong>ï¼Œå®ƒé€šè¿‡çŸ¥è¯†å›¾è°±è½¬æ¢ä¸ºå…³ç³»ä¾èµ–å›¾ï¼ˆRDGï¼‰ï¼Œæ˜ç¡®ç¼–ç ç»„åˆæ¨¡å¼ï¼ˆç›¸æ¯”äºå…ˆå‰çš„çŸ¥è¯†å›¾è°±æœ‰æ›´å°‘çš„è¾¹ï¼‰ï¼Œä»è€Œç»Ÿä¸€äº†çŸ¥è¯†å›¾è°±ä¸­çš„æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§æŸ¥è¯¢ç›¸å…³çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå­¦ä¹ å…³ç³»å’Œå®ä½“çš„å½’çº³è¡¨ç¤ºã€‚é€šè¿‡åœ¨å¤šæ ·çš„çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨çŸ­æ—¶é—´å†…è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥å®ç°æœªçŸ¥å®ä½“ã€å…³ç³»å’Œæ•´ä¸ªçŸ¥è¯†å›¾è°±çš„æœ‰æ•ˆæ³›åŒ–ã€‚é€šè¿‡è¦†ç›–è½¬å¯¼ã€å½’çº³å’Œè·¨åŸŸè®¾ç½®ä¸‹çš„å¤šç§ä¸åŒåŸºå‡†æµ‹è¯•å®éªŒçš„ç»¼åˆå®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºä¸€è‡´çš„æœ€ä½³æ€§èƒ½ï¼Œå³ä½¿æœ€å°ç¨‹åº¦çš„é€‚åº”ä¹Ÿèƒ½è¾¾åˆ°æå‡é¢„æµ‹æ€§èƒ½è‡³é«˜è¾¾35%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11125v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†å›¾è°±çš„è·¨åŸŸæ¨ç†é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚çŸ¥è¯†å›¾è°±çš„åŠ¨æ€æ€§å’Œå¤æ‚æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…³ç³»ä¸­å¿ƒåŒ–åŸºç¡€æ¨¡å‹GraphOracleï¼Œé€šè¿‡å°†çŸ¥è¯†å›¾è°±è½¬æ¢ä¸ºå…³ç³»ä¾èµ–å›¾ï¼ˆRDGï¼‰æ¥ç»Ÿä¸€è·¨çŸ¥è¯†å›¾è°±çš„æ¨ç†ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ˜¾å¼ç¼–ç ç»„åˆæ¨¡å¼ï¼Œå¹¶å‡å°‘è¾¹ç¼˜æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æŸ¥è¯¢ä¾èµ–æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå­¦ä¹ å’Œè¡¨ç¤ºå…³ç³»å’Œå®ä½“çš„å½’çº³è¡¨ç¤ºã€‚é€šè¿‡åœ¨ä¸åŒçŸ¥è¯†å›¾è°±ä¸Šçš„é¢„è®­ç»ƒå’Œåœ¨å‡ åˆ†é’Ÿå†…çš„å¾®è°ƒï¼Œæ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„å®ä½“ã€å…³ç³»å’Œæ•´ä¸ªå›¾è°±ã€‚åœ¨æ¶µç›–å½’çº³ã€å½’çº³å’Œè·¨åŸŸè®¾ç½®çš„31ä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœ€å°çš„é€‚åº”æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé¢„æµ‹æ€§èƒ½æé«˜äº†é«˜è¾¾35%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GraphOracleæ˜¯ä¸€ä¸ªå…³ç³»ä¸­å¿ƒåŒ–çš„åŸºç¡€æ¨¡å‹ï¼Œç”¨äºè§£å†³çŸ¥è¯†å›¾è°±çš„è·¨åŸŸæ¨ç†æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å°†çŸ¥è¯†å›¾è°±è½¬æ¢ä¸ºå…³ç³»ä¾èµ–å›¾ï¼ˆRDGï¼‰ï¼ŒGraphOracleèƒ½å¤Ÿç»Ÿä¸€è·¨çŸ¥è¯†å›¾è°±çš„æ¨ç†ã€‚</li>
<li>GraphOracleæ˜¾å¼ç¼–ç ç»„åˆæ¨¡å¼å¹¶å‡å°‘è¾¹ç¼˜æ•°é‡ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æŸ¥è¯¢ä¾èµ–æ³¨æ„åŠ›æœºåˆ¶æ¥å­¦ä¹ å’Œè¡¨ç¤ºå…³ç³»å’Œå®ä½“çš„å½’çº³è¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒåœ¨ä¸åŒçŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡å¾®è°ƒé€‚åº”æœªè§è¿‡çš„å®ä½“ã€å…³ç³»å’Œæ•´ä¸ªå›¾è°±ã€‚</li>
<li>GraphOracleåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œé¢„æµ‹æ€§èƒ½æé«˜äº†é«˜è¾¾35%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-858e4bfac7c2fe3898d029619d04337f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c55432bcc3e92429dd699f951e53b03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8ae8dca90d5f0f660b16ce4896d182f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Navigating-the-Alpha-Jungle-An-LLM-Powered-MCTS-Framework-for-Formulaic-Factor-Mining"><a href="#Navigating-the-Alpha-Jungle-An-LLM-Powered-MCTS-Framework-for-Formulaic-Factor-Mining" class="headerlink" title="Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic   Factor Mining"></a>Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic   Factor Mining</h2><p><strong>Authors:Yu Shi, Yitong Duan, Jian Li</strong></p>
<p>Alpha factor mining is pivotal in quantitative investment for identifying predictive signals from complex financial data. While traditional formulaic alpha mining relies on human expertise, contemporary automated methods, such as those based on genetic programming or reinforcement learning, often suffer from search inefficiency or yield poorly interpretable alpha factors. This paper introduces a novel framework that integrates Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS) to overcome these limitations. Our approach leverages the LLMâ€™s instruction-following and reasoning capability to iteratively generate and refine symbolic alpha formulas within an MCTS-driven exploration. A key innovation is the guidance of MCTS exploration by rich, quantitative feedback from financial backtesting of each candidate factor, enabling efficient navigation of the vast search space. Furthermore, a frequent subtree avoidance mechanism is introduced to bolster search efficiency and alpha factor performance. Experimental results on real-world stock market data demonstrate that our LLM-based framework outperforms existing methods by mining alphas with superior predictive accuracy, trading performance, and improved interpretability, while offering a more efficient solution for formulaic alpha mining. </p>
<blockquote>
<p>é˜¿å°”æ³•å› å­æŒ–æ˜åœ¨é‡åŒ–æŠ•èµ„ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿä»å¤æ‚çš„é‡‘èæ•°æ®ä¸­è¯†åˆ«å‡ºé¢„æµ‹ä¿¡å·ã€‚è™½ç„¶ä¼ ç»Ÿçš„å…¬å¼åŒ–é˜¿å°”æ³•æŒ–æ˜ä¾èµ–äºäººå·¥ä¸“å®¶ç»éªŒï¼Œä½†å½“å‰çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå¦‚åŸºäºé—ä¼ ç¼–ç¨‹æˆ–å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œå¸¸å¸¸é¢ä¸´æœç´¢æ•ˆç‡ä½ä¸‹æˆ–äº§ç”Ÿçš„é˜¿å°”æ³•å› å­è§£é‡Šæ€§è¾ƒå·®çš„é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥å…‹æœè¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨MCTSé©±åŠ¨çš„æ¢ç´¢ä¸­è¿­ä»£ç”Ÿæˆå’Œç»†åŒ–ç¬¦å·é˜¿å°”æ³•å…¬å¼ã€‚ä¸€ä¸ªå…³é”®çš„åˆ›æ–°ä¹‹å¤„åœ¨äºï¼ŒMCTSæ¢ç´¢å—åˆ°æ¥è‡ªæ¯ä¸ªå€™é€‰å› å­çš„é‡‘èå›æµ‹ä¸°å¯Œå®šé‡åé¦ˆçš„å¼•å¯¼ï¼Œèƒ½å¤Ÿåœ¨åºå¤§çš„æœç´¢ç©ºé—´ä¸­å®ç°é«˜æ•ˆå¯¼èˆªã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§å¸¸è§çš„å­æ ‘é¿å…æœºåˆ¶ï¼Œä»¥æé«˜æœç´¢æ•ˆç‡å’Œé˜¿å°”æ³•å› å­æ€§èƒ½ã€‚åœ¨çœŸå®è‚¡ç¥¨å¸‚åœºæ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åŸºäºLLMçš„æ¡†æ¶é€šè¿‡æŒ–æ˜å…·æœ‰æ›´é«˜é¢„æµ‹ç²¾åº¦ã€äº¤æ˜“æ€§èƒ½å’Œæ”¹å–„è§£é‡Šæ€§çš„é˜¿å°”æ³•å› å­ï¼Œè¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ•ˆæœï¼ŒåŒæ—¶ä¸ºå…¬å¼åŒ–é˜¿å°”æ³•æŒ–æ˜æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11122v1">PDF</a> 30 pages</p>
<p><strong>Summary</strong></p>
<p>åœ¨é‡‘èæŠ•èµ„é¢†åŸŸï¼ŒæŒ–æ˜é˜¿å°”æ³•å› å­æ˜¯å…³é”®ç¯èŠ‚ä¹‹ä¸€ï¼Œå¦‚ä½•ä»å¤æ‚çš„æ•°æ®ä¸­å¯»æ‰¾é¢„æµ‹ä¿¡å·å°¤ä¸ºå…³é”®ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºäººä¸ºç»éªŒï¼Œè€Œç°ä»£è‡ªåŠ¨åŒ–æ–¹æ³•å¦‚åŸºäºé—ä¼ ç¼–ç¨‹æˆ–å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•å¸¸å¸¸é¢ä¸´æœç´¢æ•ˆç‡ä½ä¸‹æˆ–äº§ç”Ÿçš„é˜¿å°”æ³•å› å­éš¾ä»¥è§£é‡Šçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèåˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œä»¥å…‹æœè¿™äº›éš¾é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨MCTSé©±åŠ¨çš„æœç´¢è¿‡ç¨‹ä¸­è¿­ä»£ç”Ÿæˆå’Œç²¾ç‚¼ç¬¦å·é˜¿å°”æ³•å…¬å¼ã€‚é€šè¿‡é‡‘èå›æµ‹å¯¹å€™é€‰å› å­è¿›è¡Œå®šé‡åé¦ˆæŒ‡å¯¼MCTSçš„æœç´¢ï¼Œå®ç°äº†å¯¹å¹¿é˜”æœç´¢ç©ºé—´çš„é«˜æ•ˆå¯¼èˆªã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ç§é¢‘ç¹å­æ ‘é¿å…æœºåˆ¶ä»¥æé«˜æœç´¢æ•ˆç‡å’Œé˜¿å°”æ³•å› å­çš„æ€§èƒ½ã€‚åœ¨çœŸå®è‚¡å¸‚æ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„æ¡†æ¶åœ¨æŒ–æ˜é˜¿å°”æ³•å› å­æ—¶è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå…·æœ‰æ›´é«˜çš„é¢„æµ‹ç²¾åº¦ã€äº¤æ˜“è¡¨ç°å’Œæ”¹è¿›çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é˜¿å°”æ³•å› å­æŒ–æ˜åœ¨é‡åŒ–æŠ•èµ„ä¸­è‡³å…³é‡è¦ï¼Œéœ€ä»å¤æ‚æ•°æ®ä¸­è¯†åˆ«é¢„æµ‹ä¿¡å·ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äººä¸ºç»éªŒï¼Œç°ä»£è‡ªåŠ¨åŒ–æ–¹æ³•å­˜åœ¨æœç´¢æ•ˆç‡ä½ä¸‹å’Œè§£é‡Šæ€§å·®çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ–°å‹æ¡†æ¶èåˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œå…‹æœäº†è¿™äº›éš¾é¢˜ã€‚</li>
<li>LLMç”¨äºè¿­ä»£ç”Ÿæˆå’Œç²¾ç‚¼ç¬¦å·é˜¿å°”æ³•å…¬å¼ï¼Œæé«˜æœç´¢æ•ˆç‡å’Œå› å­æ€§èƒ½ã€‚</li>
<li>é€šè¿‡é‡‘èå›æµ‹åé¦ˆæŒ‡å¯¼MCTSæœç´¢ï¼Œå®ç°å¹¿é˜”æœç´¢ç©ºé—´çš„é«˜æ•ˆå¯¼èˆªã€‚</li>
<li>å¼•å…¥é¢‘ç¹å­æ ‘é¿å…æœºåˆ¶ä»¥è¿›ä¸€æ­¥æé«˜é˜¿å°”æ³•å› å­çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77b4946d9bff7036fa99997d7e9dec37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b84ab0f8fbb19d47788638d1cec2b2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7849fa285bb279ab5653303882442a8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="BLEUBERI-BLEU-is-a-surprisingly-effective-reward-for-instruction-following"><a href="#BLEUBERI-BLEU-is-a-surprisingly-effective-reward-for-instruction-following" class="headerlink" title="BLEUBERI: BLEU is a surprisingly effective reward for instruction   following"></a>BLEUBERI: BLEU is a surprisingly effective reward for instruction   following</h2><p><strong>Authors:Yapei Chang, Yekyung Kim, Michael Krumdick, Amir Zadeh, Chuan Li, Chris Tanner, Mohit Iyyer</strong></p>
<p>Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at <a target="_blank" rel="noopener" href="https://github.com/lilakk/BLEUBERI">https://github.com/lilakk/BLEUBERI</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹æ˜¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„æ ¸å¿ƒï¼Œä½†å®ƒä»¬è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¤§é‡äººå·¥æ ‡è®°çš„åå¥½æ•°æ®å’Œå¼ºå¤§çš„é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹éª¨æ¶ã€‚åŒæ—¶ï¼Œé«˜è´¨é‡åˆæˆæŒ‡ä»¤è·Ÿéšæ•°æ®é›†çš„æ—¥ç›Šå¯ç”¨æ€§æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šåœ¨åŸºäºRLçš„å¯¹é½è¿‡ç¨‹ä¸­ï¼Œæ›´ç®€å•çš„åŸºäºå‚è€ƒçš„åº¦é‡æ ‡å‡†èƒ½å¦ä½œä¸ºå¥–åŠ±æ¨¡å‹çš„å¯è¡Œæ›¿ä»£å“ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¡¨æ˜BLEUï¼ˆä¸€ç§åŸºæœ¬çš„å­—ç¬¦ä¸²åŒ¹é…åº¦é‡æ ‡å‡†ï¼‰ä¸äººç±»åœ¨é€šç”¨æŒ‡ä»¤è·Ÿéšæ•°æ®é›†ä¸Šçš„åå¥½æ„å¤–åœ°åŒ¹é…å¼ºå¤§çš„å¥–åŠ±æ¨¡å‹ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼€å‘äº†BLEUBERIæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¦–å…ˆè¯†åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤ï¼Œç„¶åä½¿ç”¨BLEUä½œä¸ºå¥–åŠ±å‡½æ•°åº”ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨BLEUBERIè®­ç»ƒçš„æ¨¡å‹åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä»¥åŠä¸‰ç§ä¸åŒçš„åŸºç¡€è¯­è¨€æ¨¡å‹ä¸Šï¼Œä¸é€šè¿‡å¥–åŠ±æ¨¡å‹å¼•å¯¼çš„RLè®­ç»ƒçš„æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥æ”¯æŒBLEUBERIæ¨¡å‹è¾“å‡ºçš„è´¨é‡ä¸å¥–åŠ±æ¨¡å‹å¯¹é½çš„æ¨¡å‹ç›¸å½“ã€‚è€Œä¸”ï¼ŒBLEUBERIæ¨¡å‹äº§ç”Ÿçš„è¾“å‡ºæ¯”ç«äº‰æ–¹æ³•æ›´åŠ åŸºäºäº‹å®ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œåœ¨è·å¾—é«˜è´¨é‡å‚è€ƒè¾“å‡ºï¼ˆå¯é€šè¿‡ç°æœ‰æŒ‡ä»¤è·Ÿéšæ•°æ®é›†æˆ–åˆæˆæ•°æ®ç”Ÿæˆè½»æ¾è·å¾—ï¼‰çš„æƒ…å†µä¸‹ï¼Œå­—ç¬¦ä¸²åŒ¹é…åŸºäºåº¦é‡çš„å¥–åŠ±æ¨¡å‹æ˜¯å»‰ä»·è€Œæœ‰æ•ˆçš„ä»£ç†ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/lilakk/BLEUBERI">https://github.com/lilakk/BLEUBERI</a>å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11080v1">PDF</a> 28 pages, 11 figures, 15 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¥–åŠ±æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„é‡è¦æ€§åŠå…¶é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬é—®é¢˜ã€‚éšç€é«˜è´¨é‡åˆæˆæŒ‡ä»¤è·Ÿéšæ•°æ®é›†çš„æ™®åŠï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºBLEUè¯„åˆ†çš„æ–°æ–¹æ³•BLEUBERIã€‚è¯¥æ–¹æ³•åœ¨è¯†åˆ«æŒ‘æˆ˜æ€§æŒ‡ä»¤åï¼Œä½¿ç”¨BLEUä½œä¸ºå¥–åŠ±å‡½æ•°è¿›è¡Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBLEUBERIè®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨å››ä¸ªæŒ‡ä»¤è·ŸéšåŸºå‡†æµ‹è¯•ä¸Šä¸å¥–åŠ±æ¨¡å‹æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œä¸”ç”Ÿæˆè¾“å‡ºæ›´å…·äº‹å®ä¾æ®ã€‚æ€»ä½“è€Œè¨€ï¼ŒBLEUBERIå±•ç¤ºäº†åŸºäºé«˜è´¨é‡å‚è€ƒè¾“å‡ºçš„å­—ç¬¦ä¸²åŒ¹é…æŒ‡æ ‡åœ¨æ›¿ä»£å¥–åŠ±æ¨¡å‹è¿›è¡Œå¯¹é½æ—¶çš„æœ‰æ•ˆæ€§å’Œç»æµæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­å¯¹é½å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œä½†å…¶è®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>éšç€é«˜è´¨é‡åˆæˆæŒ‡ä»¤è·Ÿéšæ•°æ®é›†çš„å¢å¤šï¼Œäººä»¬å¼€å§‹æ¢ç´¢æ˜¯å¦å¯ä»¥ç”¨æ›´ç®€å•çš„åŸºäºå‚è€ƒçš„åº¦é‡æ ‡å‡†ä½œä¸ºå¥–åŠ±æ¨¡å‹çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>ç ”ç©¶å‘ç°BLEUè¯„åˆ†ä¸äººç±»çš„åå¥½é«˜åº¦ä¸€è‡´ï¼Œç‰¹åˆ«æ˜¯åœ¨é€šç”¨æŒ‡ä»¤éµå¾ªæ•°æ®é›†ä¸Šã€‚</li>
<li>åŸºäºè¿™ä¸€å‘ç°ï¼Œæå‡ºäº†BLEUBERIæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä½¿ç”¨BLEUä½œä¸ºå¥–åŠ±å‡½æ•°ã€‚</li>
<li>BLEUBERIè®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä¸äººç±»è¯„ä»·çš„ç»“æœç›¸å½“ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒBLEUBERIç”Ÿæˆçš„è¾“å‡ºæ›´å…·äº‹å®ä¾æ®ã€‚</li>
<li>ç ”ç©¶æœ€åå…¬å¼€äº†ç›¸å…³çš„ä»£ç å’Œæ•°æ®ï¼Œä¸ºåæ¥çš„ç ”ç©¶è€…æä¾›äº†æ–¹ä¾¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ed9eac6ac585ac61472f79fdd781a75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28058cc6bd943d7d7c7a4839f1644569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a164b276206767727e90873338fe7aa.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GuardReasoner-VL-Safeguarding-VLMs-via-Reinforced-Reasoning"><a href="#GuardReasoner-VL-Safeguarding-VLMs-via-Reinforced-Reasoning" class="headerlink" title="GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning"></a>GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning</h2><p><strong>Authors:Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, Junfeng Fang, Jiaheng Zhang, Bryan Hooi</strong></p>
<p>To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our modelâ€™s reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B&#x2F;7B) of GuardReasoner-VL at <a target="_blank" rel="noopener" href="https://github.com/yueliu1999/GuardReasoner-VL/">https://github.com/yueliu1999/GuardReasoner-VL/</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ¨ç†çš„VLMä¿æŠ¤æ¨¡å‹ï¼Œç§°ä¸ºGuardReasoner-VLï¼Œä»¥æé«˜VLMsçš„å®‰å…¨æ€§ã€‚æ ¸å¿ƒç†å¿µæ˜¯é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¿€åŠ±ä¿æŠ¤æ¨¡å‹åœ¨åšå‡ºé€‚åº¦å†³ç­–ä¹‹å‰è¿›è¡Œå®¡æ…æ¨ç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†GuardReasoner-VLTrainï¼Œä¸€ä¸ªåŒ…å«12.3ä¸‡ä¸ªæ ·æœ¬å’Œ63.1ä¸‡ä¸ªæ¨ç†æ­¥éª¤çš„æ¨ç†è¯­æ–™åº“ï¼Œæ¶µç›–æ–‡æœ¬ã€å›¾åƒå’Œæ–‡æœ¬-å›¾åƒè¾“å…¥ã€‚ç„¶åï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡SFTå†·å¯åŠ¨æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æé«˜äº†å…³äºé€‚åº¦çš„æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†æé«˜æ ·æœ¬çš„å¤šæ ·æ€§å’Œéš¾åº¦ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ‹’ç»é‡‡æ ·ï¼Œç„¶åé€šè¿‡æå‡ºçš„å®‰å…¨æ„ŸçŸ¥æ•°æ®æ‹¼æ¥è¿›è¡Œæ•°æ®å¢å¼ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨åŠ¨æ€è£å‰ªå‚æ•°æ¥é¼“åŠ±æ—©æœŸé˜¶æ®µçš„æ¢ç´¢å’Œåœ¨åæœŸé˜¶æ®µçš„åˆ©ç”¨ã€‚ä¸ºäº†å¹³è¡¡æ€§èƒ½å’Œä»¤ç‰Œæ•ˆç‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ„ŸçŸ¥é•¿åº¦çš„å®‰å…¨å¥–åŠ±ï¼Œèåˆäº†å‡†ç¡®æ€§ã€æ ¼å¼å’Œä»¤ç‰Œæˆæœ¬ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨å¹³å‡F1åˆ†æ•°ä¸Šè¶…è¿‡äº†ç¬¬äºŒå1.927ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/yueliu1999/GuardReasoner-VL/%E5%8F%91%E5%B8%83%E4%BA%86GuardReasoner-VL%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%81%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%A4%A7%E5%B0%8F%E4%B8%BAPB%E7%BA%A7%E6%88%96%E6%9B%B4%E5%A4%A7%EF%BC%89%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%BA%9B%E7%9B%B8%E5%85%B3%E7%BB%8F%E9%AA%8C%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%9D%83%E9%87%8D%E9%85%8D%E7%BD%AE%EF%BC%88%E5%A6%82%E5%9C%A8%E5%AE%9E%E9%AA%8C%E4%B8%AD%E8%AE%AD%E7%BB%83%E7%9A%84%E6%A3%80%E6%9F%A5%E7%82%B9%EF%BC%89%E3%80%82">https://github.com/yueliu1999/GuardReasoner-VL/å‘å¸ƒäº†GuardReasoner-VLçš„æ•°æ®ã€ä»£ç å’Œæ¨¡å‹ï¼ˆå¤§å°ä¸ºPBçº§æˆ–æ›´å¤§ï¼‰ä»¥åŠä¸€äº›ç›¸å…³ç»éªŒæ¨¡å‹å’Œæƒé‡é…ç½®ï¼ˆå¦‚åœ¨å®éªŒä¸­è®­ç»ƒçš„æ£€æŸ¥ç‚¹ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11049v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGuardReasoner-VLçš„æ–°å‹åŸºäºæ¨ç†çš„VLMé˜²æŠ¤æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜VLMçš„å®‰å…¨æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¿€åŠ±æ¨ç†å†³ç­–ï¼Œæ„å»ºäº†åŒ…å«æ–‡æœ¬ã€å›¾åƒå’Œæ–‡æœ¬-å›¾åƒè¾“å…¥çš„æ¨ç†è¯­æ–™åº“GuardReasoner-VLTrainï¼Œå¹¶é€šè¿‡SFTè¿›è¡Œå†·å¯åŠ¨ã€‚é‡‡ç”¨æ‹’ç»é‡‡æ ·å’Œå®‰å…¨æ„è¯†æ•°æ®æ‹¼æ¥è¿›è¡Œæ•°æ®å¢å¼ºï¼Œå¹¶ä½¿ç”¨åŠ¨æ€è£å‰ªå‚æ•°é¼“åŠ±æ—©æœŸæ¢ç´¢ä¸åæœŸåˆ©ç”¨ã€‚è®¾è®¡äº†ä¸€ç§å…¼é¡¾æ€§èƒ½å’Œä»¤ç‰Œæ•ˆç‡çš„åŸºäºé•¿åº¦çš„å®‰å…¨å¥–åŠ±æœºåˆ¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡F1åˆ†æ•°è¾ƒç¬¬äºŒåé«˜å‡º19.27%ã€‚æ¨¡å‹æ•°æ®ã€ä»£ç ï¼ˆ3B&#x2F;7Bï¼‰åŠæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yueliu1999/GuardReasoner-VL/">https://github.com/yueliu1999/GuardReasoner-VL/ä¸Šè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºGuardReasoner-VLçš„æ–°å‹VLMé˜²æŠ¤æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜VLMçš„å®‰å…¨æ€§ã€‚</li>
<li>é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¿€åŠ±æ¨ç†å†³ç­–ï¼Œæ„å»ºäº†åŒ…å«æ–‡æœ¬ã€å›¾åƒå’Œæ–‡æœ¬-å›¾åƒè¾“å…¥çš„æ¨ç†è¯­æ–™åº“GuardReasoner-VLTrainã€‚</li>
<li>é‡‡ç”¨æ‹’ç»é‡‡æ ·å’Œå®‰å…¨æ„è¯†æ•°æ®æ‹¼æ¥è¿›è¡Œæ•°æ®å¢å¼ºï¼Œä»¥å¢å¼ºæ ·æœ¬çš„å¤šæ ·æ€§å’Œéš¾åº¦ã€‚</li>
<li>ä½¿ç”¨åŠ¨æ€è£å‰ªå‚æ•°å¹³è¡¡æ—©æœŸæ¢ç´¢å’ŒåæœŸåˆ©ç”¨ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºé•¿åº¦çš„å®‰å…¨å¥–åŠ±æœºåˆ¶ï¼Œå…¼é¡¾æ€§èƒ½å’Œä»¤ç‰Œæ•ˆç‡ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡F1åˆ†æ•°è¾ƒç¬¬äºŒåé«˜å‡ºæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2b1b9ee87f309281390ea28088be0f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf5fe02e74a4f9af14e832951e75fcd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d347646fab8e0afb4371144e623e1260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d262dd993cddfd2b2a953a2a22a5d91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1af097b356c85cfcb2357cb215bbde9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Informed-but-Not-Always-Improved-Challenging-the-Benefit-of-Background-Knowledge-in-GNNs"><a href="#Informed-but-Not-Always-Improved-Challenging-the-Benefit-of-Background-Knowledge-in-GNNs" class="headerlink" title="Informed, but Not Always Improved: Challenging the Benefit of Background   Knowledge in GNNs"></a>Informed, but Not Always Improved: Challenging the Benefit of Background   Knowledge in GNNs</h2><p><strong>Authors:KutalmÄ±ÅŸ CoÅŸkun, Ivo Kavisanczki, Amin Mirzaei, Tom Siegl, Bjarne C. Hiller, Stefan LÃ¼dtke, Martin Becker</strong></p>
<p>In complex and low-data domains such as biomedical research, incorporating background knowledge (BK) graphs, such as protein-protein interaction (PPI) networks, into graph-based machine learning pipelines is a promising research direction. However, while BK is often assumed to improve model performance, its actual contribution and the impact of imperfect knowledge remain poorly understood. In this work, we investigate the role of BK in an important real-world task: cancer subtype classification. Surprisingly, we find that (i) state-of-the-art GNNs using BK perform no better than uninformed models like linear regression, and (ii) their performance remains largely unchanged even when the BK graph is heavily perturbed. To understand these unexpected results, we introduce an evaluation framework, which employs (i) a synthetic setting where the BK is clearly informative and (ii) a set of perturbations that simulate various imperfections in BK graphs. With this, we test the robustness of BK-aware models in both synthetic and real-world biomedical settings. Our findings reveal that careful alignment of GNN architectures and BK characteristics is necessary but holds the potential for significant performance improvements. </p>
<blockquote>
<p>åœ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ç­‰å¤æ‚ä¸”æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸä¸­ï¼Œå°†èƒŒæ™¯çŸ¥è¯†ï¼ˆBKï¼‰å›¾ï¼Œå¦‚è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰ç½‘ç»œï¼Œèå…¥åŸºäºå›¾çš„æœºå™¨å­¦ä¹ æµç¨‹æ˜¯ä¸€ä¸ªå‰æ™¯å¹¿é˜”çš„ç ”ç©¶æ–¹å‘ã€‚è™½ç„¶èƒŒæ™¯çŸ¥è¯†é€šå¸¸è¢«è®¤ä¸ºå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†å…¶å®é™…è´¡çŒ®ä»¥åŠä¸å®Œç¾çŸ¥è¯†çš„å½±å“ä»é²œä¸ºäººçŸ¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†èƒŒæ™¯çŸ¥è¯†åœ¨ä¸€ä¸ªé‡è¦çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼šç™Œç—‡äºšå‹åˆ†ç±»ä¸­çš„ä½œç”¨ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ï¼ˆiï¼‰ä½¿ç”¨èƒŒæ™¯çŸ¥è¯†çš„æœ€å…ˆè¿›å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å¹¶ä¸æ¯”çº¿æ€§å›å½’ç­‰æœªå—è®­æ¨¡å‹è¡¨ç°å¾—æ›´å¥½ï¼Œï¼ˆiiï¼‰å³ä½¿åœ¨èƒŒæ™¯çŸ¥è¯†å›¾å—åˆ°ä¸¥é‡å¹²æ‰°çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½ä¹ŸåŸºæœ¬ä¿æŒä¸å˜ã€‚ä¸ºäº†ç†è§£è¿™äº›æ„æƒ³ä¸åˆ°çš„ç»“æœï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ï¼ˆiï¼‰ä¸€ä¸ªèƒŒæ™¯çŸ¥è¯†æ˜ç¡®æœ‰ç”¨çš„åˆæˆè®¾ç½®å’Œï¼ˆiiï¼‰ä¸€ç³»åˆ—æ¨¡æ‹ŸèƒŒæ™¯çŸ¥è¯†å›¾ä¸­å„ç§ç¼ºé™·çš„æ‰°åŠ¨ã€‚å€Ÿæ­¤ï¼Œæˆ‘ä»¬åœ¨åˆæˆç¯å¢ƒå’Œç°å®ä¸–ç•Œçš„ç”Ÿç‰©åŒ»å­¦ç¯å¢ƒä¸­æµ‹è¯•äº†åŸºäºèƒŒæ™¯çŸ¥è¯†çš„æ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´å›¾ç¥ç»ç½‘ç»œæ¶æ„å’ŒèƒŒæ™¯çŸ¥è¯†ç‰¹æ€§ï¼Œè¿™è™½æœ‰å¯èƒ½æé«˜æ½œåœ¨æ€§èƒ½æå‡çš„å¯èƒ½æ€§ï¼Œä½†ä¹Ÿéœ€è¦æ³¨æ„å­˜åœ¨çš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11023v1">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>åœ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ç­‰å¤æ‚ä¸”æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸä¸­ï¼Œå°†èƒŒæ™¯çŸ¥è¯†ï¼ˆBKï¼‰å›¾ï¼Œå¦‚è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰ç½‘ç»œï¼Œèå…¥åŸºäºå›¾çš„æœºå™¨å­¦ä¹ ç®¡é“æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œå°½ç®¡BKé€šå¸¸è¢«è®¤ä¸ºèƒ½æå‡æ¨¡å‹æ€§èƒ½ï¼Œä½†å…¶å®é™…è´¡çŒ®ä»¥åŠä¸å®Œç¾çŸ¥è¯†çš„å½±å“ä»çŸ¥ä¹‹ç”šå°‘ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†BKåœ¨ä¸€ä¸ªé‡è¦å®é™…ä»»åŠ¡â€”â€”ç™Œç—‡äºšå‹åˆ†ç±»ä¸­çš„ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨BKçš„å…ˆè¿›å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å¹¶ä¸æ¯”çº¿æ€§å›å½’ç­‰æœªä½¿ç”¨BKçš„æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä¸”å³ä½¿BKå›¾å—åˆ°ä¸¥é‡å¹²æ‰°ï¼Œå…¶è¡¨ç°ä¹Ÿå‡ ä¹ä¸å˜ã€‚ä¸ºäº†ç†è§£è¿™äº›æ„å¤–ç»“æœï¼Œç ”ç©¶å¼•å…¥äº†è¯„ä¼°æ¡†æ¶ï¼Œé‡‡ç”¨ä¿¡æ¯æ¸…æ™°çš„åˆæˆè®¾ç½®å’Œæ¨¡æ‹ŸBKå›¾å„ç§ä¸å®Œç¾çš„æ‰°åŠ¨è®¾ç½®ã€‚ç ”ç©¶å‘ç°ï¼ŒGNNæ¶æ„ä¸BKç‰¹æ€§çš„è°¨æ…åŒ¹é…æ˜¯å¿…è¦çš„ï¼Œä½†å…·æœ‰æå‡æ€§èƒ½çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤æ‚ä¸”æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸä¸­ï¼Œå¦‚ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ï¼ŒèƒŒæ™¯çŸ¥è¯†ï¼ˆBKï¼‰å›¾è¢«çº³å…¥å›¾åŸºæœºå™¨å­¦ä¹ ç®¡é“æ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>è™½ç„¶BKé€šå¸¸è¢«è®¤ä¸ºèƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†å…¶å®é™…è´¡çŒ®ä»¥åŠä¸å®Œç¾çŸ¥è¯†å¯¹æ¨¡å‹çš„å½±å“ä»ä¸æ˜ç¡®ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œåœ¨ç™Œç—‡äºšå‹åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨BKçš„å…ˆè¿›å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å¹¶ä¸æ€»æ˜¯æ¯”æœªä½¿ç”¨BKçš„æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚</li>
<li>å³ä½¿BKå›¾å—åˆ°ä¸¥é‡å¹²æ‰°ï¼Œä½¿ç”¨BKçš„æ¨¡å‹çš„æ€§èƒ½ä¹Ÿå‡ ä¹ä¸å˜ã€‚</li>
<li>ä¸ºäº†ç†è§£èƒŒæ™¯çŸ¥è¯†åœ¨æ¨¡å‹ä¸­çš„å®é™…ä½œç”¨ï¼Œç ”ç©¶å¼•å…¥äº†è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¿¡æ¯æ¸…æ™°çš„åˆæˆè®¾ç½®å’Œæ¨¡æ‹ŸBKå›¾å„ç§ä¸å®Œç¾çš„æ‰°åŠ¨è®¾ç½®ã€‚</li>
<li>é€šè¿‡å®éªŒå‘ç°ï¼ŒGNNæ¶æ„ä¸BKç‰¹æ€§çš„åŒ¹é…å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d736b5c7feb85eb81315af7e7482747.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da4abc5167b1aca5e3791116bebdff56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aee9856a646f7c0147b7b1960bc9bff7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GenoArmory-A-Unified-Evaluation-Framework-for-Adversarial-Attacks-on-Genomic-Foundation-Models"><a href="#GenoArmory-A-Unified-Evaluation-Framework-for-Adversarial-Attacks-on-Genomic-Foundation-Models" class="headerlink" title="GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on   Genomic Foundation Models"></a>GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on   Genomic Foundation Models</h2><p><strong>Authors:Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, Yan Chen</strong></p>
<p>We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks, GenoArmory offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Additionally, we introduce GenoAdv, a new adversarial sample dataset designed to improve GFM safety. Empirically, classification models exhibit greater robustness to adversarial perturbations compared to generative models, highlighting the impact of task type on model vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†é’ˆå¯¹åŸºå› ç»„åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰çš„ç¬¬ä¸€ä¸ªç»Ÿä¸€å¯¹æŠ—æ€§æ”»å‡»åŸºå‡†æµ‹è¯•ï¼Œåä¸ºGenoArmoryã€‚ä¸ç°æœ‰çš„GFMåŸºå‡†æµ‹è¯•ä¸åŒï¼ŒGenoArmoryæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°è¯„ä¼°GFMså¯¹æŠ—å¯¹æ”»å‡»æ€§çš„è„†å¼±æ€§ã€‚åœ¨æ–¹æ³•ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨å››ç§å¹¿æ³›é‡‡ç”¨çš„æ”»å‡»ç®—æ³•å’Œä¸‰ç§é˜²å¾¡ç­–ç•¥ï¼Œè¯„ä¼°äº†äº”ç§æœ€å…ˆè¿›çš„GFMså¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªæ˜“äºè®¿é—®çš„ç»¼åˆæ¡†æ¶ï¼Œç”¨äºåˆ†æGFMåœ¨æ¨¡å‹ç»“æ„ã€é‡åŒ–æ–¹æ¡ˆå’Œè®­ç»ƒæ•°æ®é›†æ–¹é¢çš„è„†å¼±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†GenoAdvï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¯¹æŠ—æ ·æœ¬æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜GFMçš„å®‰å…¨æ€§ã€‚ç»éªŒè¡¨æ˜ï¼Œä¸ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼Œåˆ†ç±»æ¨¡å‹å¯¹å¯¹æŠ—æ€§æ‰°åŠ¨è¡¨ç°å‡ºæ›´å¤§çš„é²æ£’æ€§ï¼Œçªå‡ºäº†ä»»åŠ¡ç±»å‹å¯¹æ¨¡å‹è„†å¼±æ€§çš„å½±å“ã€‚è€Œä¸”ï¼Œå¯¹æŠ—æ€§æ”»å‡»ç»å¸¸é’ˆå¯¹ç”Ÿç‰©å­¦ä¸Šé‡è¦çš„åŸºå› ç»„åŒºåŸŸï¼Œè¿™è¡¨æ˜è¿™äº›æ¨¡å‹æœ‰æ•ˆåœ°æ•è·äº†æœ‰æ„ä¹‰çš„åºåˆ—ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºå› ç»„æ¨¡å‹å¯¹æŠ—æ”»å‡»åŸºå‡†æµ‹è¯•ï¼ˆGenoArmoryï¼‰è¢«æå‡ºï¼Œå®ƒç³»ç»Ÿåœ°è¯„ä¼°äº†åŸºå› ç»„åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰å¯¹å¯¹æŠ—æ”»å‡»çš„è„†å¼±æ€§ã€‚è¯¥ç ”ç©¶è¯„ä¼°äº†äº”ç§æœ€å…ˆè¿›GFMsçš„å¯¹æŠ—é²æ£’æ€§ï¼Œé‡‡ç”¨å››ç§å¹¿æ³›é‡‡ç”¨çš„æ”»å‡»ç®—æ³•å’Œä¸‰ç§é˜²å¾¡ç­–ç•¥ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¯¹æŠ—æ ·æœ¬æ•°æ®é›†GenoAdvï¼Œä»¥æé«˜GFMçš„å®‰å…¨æ€§ã€‚åˆ†ç±»æ¨¡å‹ç›¸å¯¹äºç”Ÿæˆæ¨¡å‹å±•ç°å‡ºæ›´å¼ºçš„å¯¹æŠ—æ‰°åŠ¨é²æ£’æ€§ï¼Œä»»åŠ¡ç±»å‹å¯¹æ¨¡å‹è„†å¼±æ€§æœ‰å½±å“ã€‚åŒæ—¶ï¼Œå¯¹æŠ—æ”»å‡»ç»å¸¸é’ˆå¯¹ç”Ÿç‰©å­¦ä¸Šé‡è¦çš„åŸºå› ç»„åŒºåŸŸï¼Œè¯´æ˜è¿™äº›æ¨¡å‹æœ‰æ•ˆåœ°æ•æ‰äº†æœ‰æ„ä¹‰çš„åºåˆ—ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºé¦–ä¸ªç»Ÿä¸€çš„åŸºå› ç»„åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰å¯¹æŠ—æ”»å‡»åŸºå‡†æµ‹è¯•ï¼ˆGenoArmoryï¼‰ã€‚</li>
<li>ç³»ç»Ÿåœ°è¯„ä¼°äº†GFMså¯¹å¯¹æŠ—æ”»å‡»çš„è„†å¼±æ€§ã€‚</li>
<li>å¯¹äº”ç§æœ€å…ˆè¿›GFMsçš„å¯¹æŠ—é²æ£’æ€§è¿›è¡Œäº†è¯„ä¼°ï¼Œé‡‡ç”¨äº†å››ç§æ”»å‡»ç®—æ³•å’Œä¸‰ç§é˜²å¾¡ç­–ç•¥ã€‚</li>
<li>å¼•å…¥æ–°çš„å¯¹æŠ—æ ·æœ¬æ•°æ®é›†GenoAdvä»¥æå‡GFMçš„å®‰å…¨æ€§ã€‚</li>
<li>åˆ†ç±»æ¨¡å‹ç›¸å¯¹äºç”Ÿæˆæ¨¡å‹å±•ç°å‡ºæ›´å¼ºçš„å¯¹æŠ—æ‰°åŠ¨é²æ£’æ€§ã€‚</li>
<li>ä»»åŠ¡ç±»å‹å½±å“æ¨¡å‹çš„è„†å¼±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17eb7231ff9b76bce91e72999f692003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0963033f1b7a23c65f58c29be357e6e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Improving-the-Data-efficiency-of-Reinforcement-Learning-by-Warm-starting-with-LLM"><a href="#Improving-the-Data-efficiency-of-Reinforcement-Learning-by-Warm-starting-with-LLM" class="headerlink" title="Improving the Data-efficiency of Reinforcement Learning by Warm-starting   with LLM"></a>Improving the Data-efficiency of Reinforcement Learning by Warm-starting   with LLM</h2><p><strong>Authors:Thang Duong, Minglai Yang, Chicheng Zhang</strong></p>
<p>We investigate the usage of Large Language Model (LLM) in collecting high-quality data to warm-start Reinforcement Learning (RL) algorithms for learning in some classical Markov Decision Process (MDP) environments. In this work, we focus on using LLM to generate an off-policy dataset that sufficiently covers state-actions visited by optimal policies, then later using an RL algorithm to explore the environment and improve the policy suggested by the LLM. Our algorithm, LORO, can both converge to an optimal policy and have a high sample efficiency thanks to the LLMâ€™s good starting policy. On multiple OpenAI Gym environments, such as CartPole and Pendulum, we empirically demonstrate that LORO outperforms baseline algorithms such as pure LLM-based policies, pure RL, and a naive combination of the two, achieving up to $4 \times$ the cumulative rewards of the pure RL baseline. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ”¶é›†é«˜è´¨é‡æ•°æ®ä»¥é¢„çƒ­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•ä¸­çš„åº”ç”¨ï¼Œç”¨äºåœ¨æŸäº›ç»å…¸é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ç¯å¢ƒä¸­è¿›è¡Œå­¦ä¹ ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä½¿ç”¨LLMç”Ÿæˆä¸€ç§éç­–ç•¥æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†èƒ½å¤Ÿå……åˆ†è¦†ç›–ç”±æœ€ä¼˜ç­–ç•¥è®¿é—®çš„çŠ¶æ€åŠ¨ä½œï¼Œç„¶åä½¿ç”¨RLç®—æ³•æ¢ç´¢ç¯å¢ƒå¹¶æ”¹è¿›ç”±LLMæå‡ºçš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç®—æ³•LOROå¯ä»¥æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶ä¸”ç”±äºLLMçš„è‰¯å¥½åˆå§‹ç­–ç•¥è€Œå…·æœ‰è¾ƒé«˜çš„æ ·æœ¬æ•ˆç‡ã€‚åœ¨å¤šä¸ªOpenAI Gymç¯å¢ƒä¸­ï¼Œä¾‹å¦‚CartPoleå’ŒPendulumï¼Œæˆ‘ä»¬å®è¯è¡¨æ˜LOROä¼˜äºåŸºçº¿ç®—æ³•ï¼Œå¦‚åŸºäºçº¯LLMçš„ç­–ç•¥ã€çº¯RLå’Œä¸¤è€…çš„ç®€å•ç»„åˆï¼Œå…¶ç´¯ç§¯å¥–åŠ±è¾¾åˆ°äº†çº¯RLåŸºçº¿çš„4å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10861v1">PDF</a> 31 pages (9 for the main paper), 27 figures, NeurIPS 25 submission</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¶é›†é«˜è´¨é‡æ•°æ®ï¼Œä»¥å¯åŠ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•åœ¨æŸäº›ç»å…¸é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ç¯å¢ƒä¸­çš„å­¦ä¹ ã€‚æœ¬æ–‡é‡ç‚¹ç ”ç©¶ä½¿ç”¨LLMç”Ÿæˆæ¶µç›–æœ€ä¼˜ç­–ç•¥è®¿é—®çš„çŠ¶æ€åŠ¨ä½œçš„ç¦»ç­–ç•¥æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨RLç®—æ³•æ¢ç´¢ç¯å¢ƒå¹¶æ”¹è¿›ç”±LLMæå‡ºçš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç®—æ³•LOROï¼Œç”±äºLLMçš„è‰¯å¥½åˆå§‹ç­–ç•¥ï¼Œæ—¢èƒ½æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œåˆå…·æœ‰è¾ƒé«˜çš„æ ·æœ¬æ•ˆç‡ã€‚åœ¨å¤šä¸ªOpenAI Gymç¯å¢ƒä¸­ï¼Œå¦‚CartPoleå’ŒPendulumï¼Œæˆ‘ä»¬å®è¯æ˜¾ç¤ºLOROä¼˜äºåŸºçº¿ç®—æ³•ï¼Œå¦‚çº¯LLMç­–ç•¥ã€çº¯RLå’Œä¸¤è€…çš„ç®€å•ç»„åˆï¼Œç´¯è®¡å¥–åŠ±è¾¾åˆ°çº¯RLåŸºçº¿çš„4å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»å…¸é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ç¯å¢ƒä¸­çš„ä½¿ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•LOROï¼Œç»“åˆäº†LLMå’ŒRLçš„ä¼˜åŠ¿ï¼Œæ—¨åœ¨æé«˜æ ·æœ¬æ•ˆç‡å’Œç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>LOROç®—æ³•ç”Ÿæˆæ¶µç›–æœ€ä¼˜ç­–ç•¥è®¿é—®çš„çŠ¶æ€åŠ¨ä½œçš„ç¦»ç­–ç•¥æ•°æ®é›†ã€‚</li>
<li>LOROåœ¨å¤šä¸ªOpenAI Gymç¯å¢ƒä¸­è¿›è¡Œäº†å®è¯æµ‹è¯•ï¼ŒåŒ…æ‹¬CartPoleå’ŒPendulumã€‚</li>
<li>LOROæ˜¾è‘—ä¼˜äºåŸºçº¿ç®—æ³•ï¼Œå¦‚çº¯LLMç­–ç•¥ã€çº¯RLä»¥åŠä¸¤è€…çš„ç»„åˆã€‚</li>
<li>LOROç®—æ³•èƒ½å¤Ÿå®ç°é«˜è¾¾çº¯RLåŸºçº¿4å€çš„ç´¯è®¡å¥–åŠ±ã€‚</li>
<li>LLMçš„åˆå§‹ç­–ç•¥å¯¹LOROç®—æ³•çš„æ€§èƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10861">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44e5f538fd47a6380935eca2cf22ce5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57fce7d35fa171aa6bde021597ade331.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cf301fcc5afd5a9d19eea23a7f22a9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34689d31bf4c796897e2a66f315667a1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="On-the-Evaluation-of-Engineering-Artificial-General-Intelligence"><a href="#On-the-Evaluation-of-Engineering-Artificial-General-Intelligence" class="headerlink" title="On the Evaluation of Engineering Artificial General Intelligence"></a>On the Evaluation of Engineering Artificial General Intelligence</h2><p><strong>Authors:Sandeep Neema, Susmit Jha, Adam Nagel, Ethan Lew, Chandrasekar Sureshkumar, Aleksa Gordic, Chase Shimmin, Hieu Nguygen, Paul Eremenko</strong></p>
<p>We discuss the challenges and propose a framework for evaluating engineering artificial general intelligence (eAGI) agents. We consider eAGI as a specialization of artificial general intelligence (AGI), deemed capable of addressing a broad range of problems in the engineering of physical systems and associated controllers. We exclude software engineering for a tractable scoping of eAGI and expect dedicated software engineering AI agents to address the software implementation challenges. Similar to human engineers, eAGI agents should possess a unique blend of background knowledge (recall and retrieve) of facts and methods, demonstrate familiarity with tools and processes, exhibit deep understanding of industrial components and well-known design families, and be able to engage in creative problem solving (analyze and synthesize), transferring ideas acquired in one context to another. Given this broad mandate, evaluating and qualifying the performance of eAGI agents is a challenge in itself and, arguably, a critical enabler to developing eAGI agents. In this paper, we address this challenge by proposing an extensible evaluation framework that specializes and grounds Bloomâ€™s taxonomy - a framework for evaluating human learning that has also been recently used for evaluating LLMs - in an engineering design context. Our proposed framework advances the state of the art in benchmarking and evaluation of AI agents in terms of the following: (a) developing a rich taxonomy of evaluation questions spanning from methodological knowledge to real-world design problems; (b) motivating a pluggable evaluation framework that can evaluate not only textual responses but also evaluate structured design artifacts such as CAD models and SysML models; and (c) outlining an automatable procedure to customize the evaluation benchmark to different engineering contexts. </p>
<blockquote>
<p>æˆ‘ä»¬è®¨è®ºäº†é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºä¸€ä¸ªè¯„ä¼°å·¥ç¨‹äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆeAGIï¼‰ä»£ç†çš„æ¡†æ¶ã€‚æˆ‘ä»¬è®¤ä¸ºeAGIæ˜¯äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„ä¸€ç§ä¸“ä¸šåŒ–å½¢å¼ï¼Œèƒ½å¤Ÿè§£å†³ç‰©ç†ç³»ç»ŸåŠå…¶æ§åˆ¶å™¨å·¥ç¨‹ä¸­çš„ä¸€ç³»åˆ—é—®é¢˜ã€‚æˆ‘ä»¬å°†è½¯ä»¶å·¥ç¨‹æ’é™¤åœ¨å¤–ï¼Œä»¥ä¾¿å¯¹eAGIè¿›è¡Œå¯è¡Œçš„èŒƒå›´ç•Œå®šï¼Œå¹¶æœŸæœ›ä¸“é—¨çš„è½¯ä»¶å·¥ç¨‹AIä»£ç†èƒ½è§£å†³è½¯ä»¶å®æ–½æŒ‘æˆ˜ã€‚ä¸äººç±»å·¥ç¨‹å¸ˆç±»ä¼¼ï¼ŒeAGIä»£ç†åº”å…·å¤‡ç‹¬ç‰¹çš„äº‹å®å’Œæ–¹æ³•èƒŒæ™¯çŸ¥è¯†ï¼ˆå›å¿†å’Œæ£€ç´¢ï¼‰ï¼Œç†Ÿæ‚‰å·¥å…·å’Œæµç¨‹ï¼Œæ·±å…¥äº†è§£å·¥ä¸šç»„ä»¶å’ŒçŸ¥åè®¾è®¡å®¶æ—ï¼Œå¹¶èƒ½å¤Ÿè¿›è¡Œåˆ›é€ æ€§è§£å†³é—®é¢˜ï¼ˆåˆ†æå’Œç»¼åˆï¼‰ï¼Œå°†åœ¨ä¸€ä¸ªç¯å¢ƒä¸­è·å¾—çš„æƒ³æ³•åº”ç”¨åˆ°å¦ä¸€ä¸ªç¯å¢ƒä¸­ã€‚é‰´äºè¿™ä¸€å¹¿æ³›çš„ä»»åŠ¡èŒƒå›´ï¼Œè¯„ä¼°eAGIä»£ç†çš„æ€§èƒ½æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå¹¶ä¸”æ˜¯å¼€å‘eAGIä»£ç†çš„å…³é”®æ¨åŠ¨å› ç´ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ä¸ªå¯æ‰©å±•çš„è¯„ä¼°æ¡†æ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶ä¸“é—¨åŒ–å’ŒåŸºäºBloomçš„åˆ†ç±»æ³•â€”â€”ä¸€ä¸ªè¯„ä¼°äººç±»å­¦ä¹ çš„æ¡†æ¶ï¼Œæœ€è¿‘ä¹Ÿè¢«ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹â€”â€”åœ¨å·¥ç¨‹è®¾è®¡çš„èƒŒæ™¯ä¸‹ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶åœ¨ä»¥ä¸‹æ–¹é¢æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ä»£ç†çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°çš„æœ€æ–°å‘å±•ï¼šï¼ˆaï¼‰ä»æ–¹æ³•è®ºçŸ¥è¯†åˆ°ç°å®ä¸–ç•Œè®¾è®¡é—®é¢˜ï¼Œå¼€å‘äº†ä¸°å¯Œçš„è¯„ä¼°é—®é¢˜åˆ†ç±»ï¼›ï¼ˆbï¼‰æ¿€åŠ±äº†ä¸€ä¸ªå¯æ‰©å±•çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ä»…å¯ä»¥è¯„ä¼°æ–‡æœ¬å“åº”ï¼Œè¿˜å¯ä»¥è¯„ä¼°CADæ¨¡å‹å’ŒSysMLæ¨¡å‹ç­‰ç»“æ„åŒ–è®¾è®¡æˆæœï¼›ï¼ˆcï¼‰æ¦‚è¿°äº†ä¸€ä¸ªå¯è‡ªåŠ¨åŒ–çš„ç¨‹åºæ¥æ ¹æ®ä¸åŒå·¥ç¨‹ç¯å¢ƒå®šåˆ¶è¯„ä¼°åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10653v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong><br>åœ¨æŒ‘æˆ˜é‡é‡çš„èƒŒæ™¯ä¸‹ï¼Œæœ¬æ–‡æ¢è®¨å¹¶æè®®ä¸€ä¸ªé’ˆå¯¹å·¥ç¨‹åŒ–äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“ï¼ˆeAGIï¼‰ä»£ç†çš„è¯„ä»·æ¡†æ¶ã€‚æœ¬æ–‡å¼ºè°ƒäººå·¥æ™ºèƒ½çš„å·¥ç¨‹è®¾è®¡å’Œåˆ›æ–°å®è·µçš„èƒ½åŠ›è¯„ä¼°é‡è¦æ€§ã€‚å¯¹äºå®½æ³›çš„ä»»åŠ¡æŒ‡ä»¤æ‰§è¡Œå·¥ç¨‹è‡ªåŠ¨åŒ–æ“ä½œçš„è¯„ä¼°åˆ†ææœ‰åŠ©äºå¼€å¯å’Œå‘å±•ä¸‹ä¸€ä»£çš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚æˆ‘ä»¬æè®®çš„æ¡†æ¶é€šè¿‡ä¸“ä¸šåŒ–çš„æ‰©å±•æ€§è¯„ä¼°æ–¹å¼ï¼Œä¸ºäººå·¥æ™ºèƒ½ä»£ç†æä¾›äº†ä¸€ä¸ªæ ‡å‡†çš„æ€§èƒ½è¯„ä»·ç³»ç»Ÿï¼ŒåŒ…æ‹¬å¯¹ä¸°å¯Œçš„è¯„ä»·é—®é¢˜çš„å¼€å‘ï¼Œä¸ä»…ä»…é™äºæ–‡æœ¬åé¦ˆè¯„ä»·ï¼Œè¿˜åŒ…æ‹¬CADæ¨¡å‹å’ŒSysMLæ¨¡å‹ç­‰ç»“æ„åŒ–è®¾è®¡ç‰©ä½“çš„è¯„ä»·ã€‚æ¡†æ¶æ—¨åœ¨èƒ½å¤Ÿè‡ªåŠ¨åŒ–å®šåˆ¶ä»¥é€‚åº”ä¸åŒçš„å·¥ç¨‹ç¯å¢ƒã€‚å¯¹äºæ¨è¿›äººå·¥æ™ºèƒ½ä»£ç†åœ¨åŸºå‡†æµ‹è¯•å’ŒæŠ€æœ¯è¯„ä¼°æ–¹é¢çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ€»ç»“ä¸ºï¼šå·¥ç¨‹åŒ–äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“ï¼ˆeAGIï¼‰è¯„ä»·æ¡†æ¶çš„æ„å»ºä¸æ‰©å±•æ€§è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºå°†äººå·¥é€šç”¨æ™ºèƒ½çš„å·¥ç¨‹åº”ç”¨éƒ¨åˆ†å…·ä½“åŒ–è¿›è¡Œäººå·¥æ™ºèƒ½æ™ºèƒ½ä½“è¯„ä¼°çš„æ¡†æ¶å’ŒæŒ‘æˆ˜æ€§åˆ†æé‡è¦æ€§è®¤è¯†è®¨è®ºé‡è¦æ€§è®¤çŸ¥é¢†åŸŸçš„æç»˜å½¢æˆäº†ä¸€æ›´ä¸ºé’ˆå¯¹çš„æ–¹æ³•æ–¹æ³•ä¸“è¯„ä¼°æ™ºèƒ½åŒ–äº§ä¸šäººç‰©å»ºè®¾ç»¼åˆæ€§è¿ç”¨è‡ªèº«ç»¼åˆèƒ½åŠ›è¦ç´ çš„è§„èŒƒæ€§ä¾æ®æ”¯æ’‘ç†è®ºæ¢è®¨ä»·å€¼åˆ›é€ å…·ä½“ä»·å€¼è®¨è®ºç»†èŠ‚å¹¶æŒç»­æ¨è¿›æå‡æ€»ç»“è¿‡ç¨‹è‡ªåŠ¨åŒ–å‘å±•çš„è·¯å¾„åº”ç”¨æ¨è¿›æ›´æ–°æµç¨‹çš„å»ºè®¾è§„åˆ’ç»Ÿä¸€æ€§ä¸ä¸ªæ€§åŒ–ç‰¹å¾ç›¸äº’ç»“åˆçš„é‡è¦ç­–ç•¥æ–¹æ¡ˆåˆ¶å®šæ–¹å‘æ¡†æ¶å»ºè®¾æ¨è¿›ç ”ç©¶ä¸å‘å±•è¿‡ç¨‹çš„åº”ç”¨è½åœ°åŒ–æ¢ç´¢æ¡†æ¶åœ¨é¢å‘å·¥ç¨‹åº”ç”¨æ–¹é¢çš„é‡è¦æ„ä¹‰å’Œä»·å€¼æ‰€åœ¨çš„å…³é”®è§è§£è¦ç´ è¦ç´ é¢†åŸŸä¿¡æ¯æ¶æ„ç»“æ„è®¨è®ºæ™ºèƒ½ç»¼åˆå‹å·¥ç¨‹æŠ€æœ¯ä½“ç³»å®ç°ä»¥åŠåº”ç”¨åœºæ™¯ä¸æ–­ä¼˜åŒ–çš„è¿›ç¨‹æ¶æ„è§£å†³æ–¹æ¡ˆçš„ç‰¹å¾è€ƒé‡è·¨åº”ç”¨åœºæ™¯ä¿¡æ¯çš„å…±æ€§ä¸ç‰¹å¾æŒ‘æˆ˜æœ¬èº«çš„åˆ†è§£èƒ½åŠ›çš„æ€æƒ³æ¥æºé‡æ„çš„æŠ€æœ¯è¿ç”¨é«˜æ•ˆé—®é¢˜è§£å†³å¤æ‚æ€§ä¸ä¼˜è¶Šæ€§æ°´å¹³è¾“å‡ºæ€æƒ³æ ¸å¿ƒçš„ç²¾ç¡®æŠ€æœ¯è¡Œä¸šæå‡ºäº†ä¸€ç³»åˆ—çš„ä¸“ä¸šæµ‹è¯„å…·ä½“é—®é¢˜è§£å†³ç¯å¢ƒçªç ´ç³»ç»Ÿçš„ç†è®ºè®¾è®¡é‡è¦è¯„ä¼°æ ‡å‡†çš„æ”¯æ’‘é‡è¦é—®é¢˜è§£å†³é—®é¢˜è¯„ä¼°è¿‡ç¨‹çš„ç»†åŒ–æ“ä½œä¾æ®å¯å®è·µåº”ç”¨çš„æ ¸å¿ƒé¢†åŸŸç‰¹å¾åŠå…¶ç ”ç©¶ä»·å€¼å’Œè¯„ä¼°æœºåˆ¶çš„ä»·å€¼è´¡çŒ®çš„å®é™…åº”ç”¨æ€§å¼ºçš„å¯¹ç­–æ¦‚å¿µåŒ–å’Œå¯è§†åŒ–çš„å…³è”å·¥ç¨‹æŠ€æœ¯åŠå…¶åº”ç”¨å’Œç³»ç»Ÿæ–¹é¢è®¾è®¡ä¸è¡¨ç°ç´ å…»ç®¡ç†åé¦ˆé«˜çº§äººæ‰åŸ¹å…»æ™ºåŠ›åˆ©ç”¨çš„ç°çŠ¶ä½¿ç”¨åŸåˆ™ç»©æ•ˆé—®é¢˜çš„è¡ŒåŠ¨æŠ€èƒ½å˜é©äººæ‰åŸ¹å…»æ³¨é‡æ‰“é€ å…±è¯†å€Ÿé‰´ä¸€ä½“åŒ–æ•ˆèƒ½å¤§æ•°æ®äººå®è§‚äººæ–‡ç®¡ç†ååŒæ™ºèƒ½ç®¡ç†æ¡†æ¶èƒ½åŠ›æ¡†æ¶è¯„ä»·ä½“ç³»çš„æ ¸å¿ƒå†…å®¹èƒ½åŠ›å‘å±•ç­‰æ ¸å¿ƒé—®é¢˜è§£å†³æ–¹æ¡ˆçš„æ ¸å¿ƒé—®é¢˜æ ¸å¿ƒæ€æƒ³åŠæœªæ¥å‘å±•è¶‹åŠ¿å’Œæˆ˜ç•¥åˆ†æç­–ç•¥ç­–ç•¥æ‰‹æ®µè§„åˆ’æ€ç»´åŸºç¡€è¦ç´ çš„åˆ›é€ æ€§æå‡ºæ›´åŠ æ™ºèƒ½åŒ–è¯„ä»·åº”ç”¨å±‚é¢çš„æ ‡å‡†åŒ–ç ”ç©¶æ ‡å‡†è½åœ°æ¨è¿›çš„åº”ç”¨ç®¡ç†éœ€æ±‚æ¨¡å‹ç³»ç»Ÿçš„ç§‘å­¦é€‚åº”æ€§æ¨¡å—æŒç»­ç›‘æ§æå‡ºç«äº‰å‘å±•æ–°æŒ‡æ ‡å®æ—¶æ¿€åŠ±è¯„ä»·æ–¹æ³•ç®¡ç†æœºåˆ¶ç»¼åˆç´ è´¨å†…å®¹æ•´ä½“çš„å»¶ç»­ä¸åˆ›æ–°å·¥å…·å…¨ç”Ÿå‘½å‘¨æœŸå¹³å°çš„ç°å®æµç¨‹é‡è¦æ€§ä¸­çš„è·¨ç•Œæ ¸å¿ƒæ„å»ºçš„èƒ½åŠ›ä¼ é€’æ¿€å‘æ–°æ–¹æ³•çš„æ•°æ®åŸºç¡€çš„å‡†ç¡®æ€§ç´ è´¨åˆ›é€ å…ƒç´ æå‡ºäº†å½’çº³è¡¨è¾¾æ¨¡å—åŒ–ç®¡ç†ä½“ç³»æ¡ˆä¾‹è§„å¾‹ç³»ç»Ÿè®¾è®¡çš„æ–¹æ³•å’Œæ–°å‹å‘å±•çš„ç­–ç•¥çš„ç§‘ç ”è´¡çŒ®ç°çŠ¶å¸ƒå±€è§„èŒƒåŒ–è§„æ¨¡åŒ–äº§å‡ºåˆ›é€ æ€§åº”ç”¨èƒ½åŠ›ä¸ºåº”ç”¨æŒ‡å¯¼äº§å“å·¥ä¸šçš„åº”ç”¨æ‰“é€ å…±åˆ›è¿‡ç¨‹çš„ç†è§£é€šè¿‡éœ€æ±‚è½¬åŒ–çš„èƒ½åŠ›å»ºè®¾æœªæ¥å‘å±•åŠ›çš„æ´»åŠ›ç»ˆç«¯æ¿€åŠ±è¯„ä¼°å’Œå·®å¼‚åŒ–æªæ–½é€‰æ‹©ç®¡ç†æœºåˆ¶æœ‰æ•ˆæ€§è½¬å‹æå‡ºäº†æ³›æ™ºèƒ½åŒ–æŠ€æœ¯å‘å±•åœ¨ä¸åŒç»´åº¦çš„è¯„ä»·æ–¹æ³•ä¸€ä½“åŒ–è¯„æµ‹æ¡†æ¶å¯¹æ™ºèƒ½ç³»ç»Ÿçš„æ™ºèƒ½åŒ–ç¨‹åº¦è¿›è¡Œè¯„ä»·çš„é‡è¦æ€§</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8c8619300d87aae8c6e3663216f1c1fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73122ee4ba76f67fb0e059fdb11fbcdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0149fdd840a86573322cc2ac44425042.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CRPE-Expanding-The-Reasoning-Capability-of-Large-Language-Model-for-Code-Generation"><a href="#CRPE-Expanding-The-Reasoning-Capability-of-Large-Language-Model-for-Code-Generation" class="headerlink" title="CRPE: Expanding The Reasoning Capability of Large Language Model for   Code Generation"></a>CRPE: Expanding The Reasoning Capability of Large Language Model for   Code Generation</h2><p><strong>Authors:Ningxin Gui, Qianghuai Jia, Feijun Jiang, Yuling Jiao, dechun wang, Jerry Zhijian Yang</strong></p>
<p>We introduce CRPE (Code Reasoning Process Enhancer), an innovative three-stage framework for data synthesis and model training that advances the development of sophisticated code reasoning capabilities in large language models (LLMs). Building upon existing system-1 models, CRPE addresses the fundamental challenge of enhancing LLMsâ€™ analytical and logical processing in code generation tasks. Our framework presents a methodologically rigorous yet implementable approach to cultivating advanced code reasoning abilities in language models. Through the implementation of CRPE, we successfully develop an enhanced COT-Coder that demonstrates marked improvements in code generation tasks. Evaluation results on LiveCodeBench (20240701-20240901) demonstrate that our COT-Coder-7B-StepDPO, derived from Qwen2.5-Coder-7B-Base, with a pass@1 accuracy of 21.88, exceeds all models with similar or even larger sizes. Furthermore, our COT-Coder-32B-StepDPO, based on Qwen2.5-Coder-32B-Base, exhibits superior performance with a pass@1 accuracy of 35.08, outperforming GPT4O on the benchmark. Overall, CRPE represents a comprehensive, open-source method that encompasses the complete pipeline from instruction data acquisition through expert code reasoning data synthesis, culminating in an autonomous reasoning enhancement mechanism. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†CRPEï¼ˆä»£ç æ¨ç†è¿‡ç¨‹å¢å¼ºå™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºæ•°æ®åˆæˆå’Œæ¨¡å‹è®­ç»ƒï¼Œæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¤æ‚ä»£ç æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚CRPEå»ºç«‹åœ¨ç°æœ‰çš„ç³»ç»Ÿ1æ¨¡å‹ä¹‹ä¸Šï¼Œè§£å†³äº†æé«˜LLMåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„åˆ†æå’Œé€»è¾‘å¤„ç†èƒ½åŠ›çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºåœ¨è¯­è¨€æ¨¡å‹ä¸­åŸ¹å…»å…ˆè¿›çš„ä»£ç æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€ç§æ–¹æ³•ä¸¥è°¨ä¸”åˆ‡å®å¯è¡Œçš„æ–¹æ³•ã€‚é€šè¿‡CRPEçš„å®æ–½ï¼Œæˆ‘ä»¬æˆåŠŸå¼€å‘äº†ä¸€ç§å¢å¼ºçš„COT-Coderï¼Œåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚LiveCodeBenchï¼ˆ2024å¹´7æœˆ1æ—¥è‡³2024å¹´9æœˆ1æ—¥ï¼‰çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„COT-Coder-7B-StepDPOï¼ŒæºäºQwen2.5-Coder-7B-Baseï¼Œå‡†ç¡®ç‡ä¸º21.88%ï¼Œè¶…è¿‡äº†ç±»ä¼¼æˆ–æ›´å¤§è§„æ¨¡çš„æ‰€æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„COT-Coder-32B-StepDPOï¼ŒåŸºäºQwen2.5-Coder-32B-Baseï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º35.08%ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†GPT4Oã€‚æ€»çš„æ¥è¯´ï¼ŒCRPEæ˜¯ä¸€ç§å…¨é¢ã€å¼€æºçš„æ–¹æ³•ï¼Œæ¶µç›–äº†ä»æŒ‡ä»¤æ•°æ®è·å–åˆ°ä¸“å®¶ä»£ç æ¨ç†æ•°æ®åˆæˆçš„å®Œæ•´æµç¨‹ï¼Œæœ€ç»ˆå½¢æˆäº†ä¸€ä¸ªè‡ªä¸»æ¨ç†å¢å¼ºæœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10594v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>CRPEï¼ˆä»£ç æ¨ç†è¿‡ç¨‹å¢å¼ºå™¨ï¼‰æ˜¯ä¸€ä¸ªç”¨äºæ•°æ®åˆæˆå’Œæ¨¡å‹è®­ç»ƒçš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç³»ç»Ÿ-ç¬¬ä¸€æœŸæ¨¡å‹çš„åŸºç¡€ä¸Šå¢å¼ºæ¨¡å‹åˆ†æåŠ›å’Œé€»è¾‘å¤„ç†ï¼ŒæˆåŠŸå¼€å‘å‡ºå…·æœ‰å…ˆè¿›ä»£ç æ¨ç†èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹COT-Coderã€‚åœ¨LiveCodeBenchä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ–°å¼€å‘çš„COT-Coderåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œè¶…è¿‡å…¶ä»–ç›¸ä¼¼æˆ–æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚CRPEæ˜¯ä¸€ä¸ªå…¨é¢ã€å¼€æºçš„æ–¹æ³•ï¼Œæ¶µç›–äº†ä»æŒ‡ä»¤æ•°æ®é‡‡é›†åˆ°ä¸“å®¶ä»£ç æ¨ç†æ•°æ®åˆæˆï¼Œæœ€ç»ˆå®ç°äº†è‡ªä¸»æ¨ç†å¢å¼ºæœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>CRPEæ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®ƒå»ºç«‹åœ¨ç°æœ‰çš„ç³»ç»Ÿ-ç¬¬ä¸€æœŸæ¨¡å‹ä¸Šï¼Œè§£å†³äº†æå‡LLMåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„åˆ†æå’Œé€»è¾‘å¤„ç†èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>COT-Coderæ˜¯CRPEæ¡†æ¶ä¸‹æˆåŠŸå¼€å‘çš„å…·æœ‰å…ˆè¿›ä»£ç æ¨ç†èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åœ¨LiveCodeBenchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒCOT-Coderåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†å…¶ä»–ç±»ä¼¼æˆ–æ›´å¤§çš„æ¨¡å‹ã€‚</li>
<li>CRPEé€šè¿‡æ„å»ºå…¨é¢çš„æ•°æ®åˆæˆå’Œæ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œå®ç°äº†ä»æŒ‡ä»¤æ•°æ®é‡‡é›†åˆ°ä¸“å®¶ä»£ç æ¨ç†æ•°æ®åˆæˆçš„å®Œæ•´ç®¡é“ã€‚</li>
<li>CRPEå…·æœ‰è‡ªä¸»æ¨ç†å¢å¼ºæœºåˆ¶çš„ç‰¹ç‚¹ã€‚</li>
<li>CRPEæ˜¯å¼€æ”¾æºç çš„æ–¹æ³•è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16fcfda0d02cf5c7522aa892e36f6f1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d37d66a0767a60b43950c7559053589d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf013824a04dd057dbfdebfb02dba7ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c81f091a1c845345c1b40c8e4e1b6b14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9daf230fcffcadc80e3dfd57ee5346b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e02542f6ea2bfc345931f653c3bc3cfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20d504803069e78ebf163ced33ee7c52.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c8676229573b6660592867fc418dd516.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-20  Modeling cognitive processes of natural reading with transformer-based   Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c5a3e44181e2069c853347aeeacb675a.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-18  RM-R1 Reward Modeling as Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
