<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-20  QVGen Pushing the Limit of Quantized Video Generative Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4e9118e1f98bd6af8c0e55280683c781.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-20-更新"><a href="#2025-05-20-更新" class="headerlink" title="2025-05-20 更新"></a>2025-05-20 更新</h1><h2 id="QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models"><a href="#QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models" class="headerlink" title="QVGen: Pushing the Limit of Quantized Video Generative Models"></a>QVGen: Pushing the Limit of Quantized Video Generative Models</h2><p><strong>Authors:Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, Jun Zhang</strong></p>
<p>Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\Phi$, we propose a rank-decay strategy that progressively eliminates $\Phi$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\mathbf{\gamma}$ to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench. </p>
<blockquote>
<p>视频扩散模型（DMs）已经能够实现高质量的视频合成。然而，其巨大的计算和内存需求对实际部署带来了严峻挑战，即使在高端GPU上也是如此。作为一种常用的解决方案，量化在降低图像DM的成本方面取得了显著的成效，而直接应用于视频DM则仍然无效。在本文中，我们介绍了QVGen，这是一个针对极端低比特量化（例如4位及以下）下高性能和推理效率的视频DM定制的新型量化感知训练（QAT）框架。我们从理论分析开始，证明降低梯度范数是促进QAT收敛的关键。为此，我们引入了辅助模块（Φ）来缓解大量的量化误差，从而显著增强了收敛性。为了消除Φ的推理开销，我们提出了一种等级衰减策略，该策略逐步消除Φ。具体来说，我们反复使用奇异值分解（SVD）和提出的基于等级的规则γ来识别和衰减贡献较小的成分。此策略在消除推理开销的同时保留了性能。在4种最先进的视频DMs上的广泛实验，参数大小从1.3B到14B不等，表明QVGen首次在4位设置下达到全精度相当的质量，并且显著优于现有方法。例如，我们的3位CogVideoX-2B在VBench上的动态度和场景一致性分别提高了+25.28和+8.43。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11497v1">PDF</a> Our code will be released upon acceptance</p>
<p><strong>Summary</strong><br>     视频扩散模型（DMs）能够实现高质量视频合成，但其巨大的计算和内存需求对实际部署带来了挑战。针对视频DMs的量化训练框架QVGen被提出，该框架针对极低比特量化（如4位及以下）进行优化，通过引入辅助模块和排名衰减策略提高性能和推理效率。实验证明，QVGen在多种先进视频DMs中实现了全精度相当的质量，并显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频扩散模型（DMs）可实现高质量视频合成，但计算和内存需求巨大，对实际部署带来挑战。</li>
<li>QVGen是一种针对视频DMs的量化训练框架，旨在提高性能和推理效率，尤其适用于极低比特量化。</li>
<li>QVGen通过引入辅助模块来减轻量化误差，并通过理论分析和实验验证其有效性。</li>
<li>QVGen采用排名衰减策略消除辅助模块在推理过程中的开销，通过奇异值分解和基于排名的正则化策略逐步消除低贡献组件。</li>
<li>QVGen在多种先进视频DMs中实现了全精度相当的质量，并在某些指标上显著优于现有方法。</li>
<li>QVGen为视频DMs的实用部署提供了新的可能性和解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11497">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6313b87d21d88cf71900e34a05e944c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e624add7ca264450b9a69bfc9ff2c6cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b8a7b15cb9e035c4b2a7f5be4db9eb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be2747313516f4355f38e1fd38810f05.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Diff-Unfolding-A-Model-Based-Score-Learning-Framework-for-Inverse-Problems"><a href="#Diff-Unfolding-A-Model-Based-Score-Learning-Framework-for-Inverse-Problems" class="headerlink" title="Diff-Unfolding: A Model-Based Score Learning Framework for Inverse   Problems"></a>Diff-Unfolding: A Model-Based Score Learning Framework for Inverse   Problems</h2><p><strong>Authors:Yuanhao Wang, Shirin Shoushtari, Ulugbek S. Kamilov</strong></p>
<p>Diffusion models are extensively used for modeling image priors for inverse problems. We introduce \emph{Diff-Unfolding}, a principled framework for learning posterior score functions of \emph{conditional diffusion models} by explicitly incorporating the physical measurement operator into a modular network architecture. Diff-Unfolding formulates posterior score learning as the training of an unrolled optimization scheme, where the measurement model is decoupled from the learned image prior. This design allows our method to generalize across inverse problems at inference time by simply replacing the forward operator without retraining. We theoretically justify our unrolling approach by showing that the posterior score can be derived from a composite model-based optimization formulation. Extensive experiments on image restoration and accelerated MRI show that Diff-Unfolding achieves state-of-the-art performance, improving PSNR by up to 2 dB and reducing LPIPS by $22.7%$, while being both compact (47M parameters) and efficient (0.72 seconds per $256 \times 256$ image). An optimized C++&#x2F;LibTorch implementation further reduces inference time to 0.63 seconds, underscoring the practicality of our approach. </p>
<blockquote>
<p>扩散模型被广泛用于逆向问题的图像先验建模。我们介绍了Diff-Unfolding，这是一种通过明确地将物理测量算子融入模块化网络架构来学习条件扩散模型的后验得分函数的原理性框架。Diff-Unfolding将后验得分学习制定为展开优化方案的训练，其中测量模型与学习的图像先验解耦。这种设计使得我们的方法能够在推理时通过简单地替换正向算子来泛化到各种逆向问题，而无需重新训练。我们通过显示后验得分可以从基于模型的组合优化公式中得出，从理论上证明了我们的展开方法。在图像恢复和加速MRI的广泛实验表明，Diff-Unfolding达到了最先进的性能，PSNR提高了高达2dB，LPIPS降低了22.7%，同时既紧凑（4700万参数）又高效（每256 x 256图像0.72秒）。经过优化的C++&#x2F;LibTorch实现进一步将推理时间减少到0.63秒，突显了我们方法的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11393v1">PDF</a> 19 pages, 13 figures,</p>
<p><strong>Summary</strong></p>
<p>扩散模型被广泛用于逆向问题的图像先验建模。本文提出一种名为Diff-Unfolding的理论框架，它通过明确地将物理测量算子融入模块化网络架构，学习条件扩散模型的后验分数函数。Diff-Unfolding将后验分数学习制定为展开优化方案的训练，其中测量模型与学习的图像先验解耦。这种设计使得我们的方法能够在推理时通过简单地替换前向算子而适应各种逆向问题，无需重新训练。本文理论证明了展开方法的有效性，并通过广泛的图像恢复和加速MRI实验表明，Diff-Unfolding实现了最先进的性能，PSNR提高了高达2dB，LPIPS降低了22.7%，同时模型紧凑（47M参数）且高效（每张256x256图像处理时间为0.72秒）。使用优化的C++&#x2F;LibTorch实现可将推理时间进一步缩短至0.63秒，突显了方法的实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在逆向问题的图像先验建模中有广泛应用。</li>
<li>提出了Diff-Unfolding框架，结合物理测量算子和模块化网络架构，学习条件扩散模型的后验分数函数。</li>
<li>Diff-Unfolding将后验分数学习表述为展开优化方案的训练，测量模型与图像先验解耦，适应多种逆向问题。</li>
<li>理论证明了展开方法的有效性。</li>
<li>实验表明，Diff-Unfolding在图像恢复和加速MRI方面实现了最先进的性能。</li>
<li>Diff-Unfolding具有高效的推理时间，使用优化的C++&#x2F;LibTorch实现，处理速度更快。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11393">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-04b9c048e970fcdc1b0fc52ffeadf311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f52cd10c2dafc88b564eda5be73ab756.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f229547ab1e8b09f50ade0a81de0e993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1af47e187b5eba2348d82be4c653679.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d38ed3dab3d308f856365fb915c5e590.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Fourier-Space-Perspective-on-Diffusion-Models"><a href="#A-Fourier-Space-Perspective-on-Diffusion-Models" class="headerlink" title="A Fourier Space Perspective on Diffusion Models"></a>A Fourier Space Perspective on Diffusion Models</h2><p><strong>Authors:Fabian Falck, Teodora Pandeva, Kiarash Zahirnia, Rachel Lawrence, Richard Turner, Edward Meeds, Javier Zazo, Sushrut Karmalkar</strong></p>
<p>Diffusion models are state-of-the-art generative models on data modalities such as images, audio, proteins and materials. These modalities share the property of exponentially decaying variance and magnitude in the Fourier domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM) forward process of additive white noise, this property results in high-frequency components being corrupted faster and earlier in terms of their Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then generates low-frequency information before high-frequency details. In this work, we study the inductive bias of the forward process of diffusion models in Fourier space. We theoretically analyse and empirically demonstrate that the faster noising of high-frequency components in DDPM results in violations of the normality assumption in the reverse process. Our experiments show that this leads to degraded generation quality of high-frequency components. We then study an alternate forward process in Fourier space which corrupts all frequencies at the same rate, removing the typical frequency hierarchy during generation, and demonstrate marked performance improvements on datasets where high frequencies are primary, while performing on par with DDPM on standard imaging benchmarks. </p>
<blockquote>
<p>扩散模型是图像、音频、蛋白质和材料等数据模态领域的最前沿生成模型。这些模态在傅里叶域中共享指数衰减的方差和幅度属性。在标准的去噪扩散概率模型（DDPM）添加白噪声的前向过程中，此属性导致高频成分在信噪比（SNR）方面比低频成分更快、更早地被破坏。然后反向过程会先生成低频信息，再生成高频细节。在这项工作中，我们研究了扩散模型在傅里叶空间前向过程的归纳偏置。我们理论上分析和实证表明，DDPM中高频成分更快地被噪声干扰会导致反向过程中正态假设的违反。我们的实验表明，这导致了高频成分生成质量的下降。然后，我们研究了傅里叶空间中替代的前向过程，以相同的速率破坏所有频率，在生成过程中消除了典型的频率层次结构，并在高频为主要数据集的数据集上实现了显著的性能改进，同时在标准成像基准测试中与DDPM表现相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型是数据模态生成领域的最先进技术，如图像、音频、蛋白质和材料等。在傅里叶域中，这些模态具有指数衰减的方差和幅度特性。在标准的去噪扩散概率模型（DDPM）正向过程中，该特性导致高频成分在信噪比（SNR）方面较早地被噪声破坏。反向过程则生成低频信息，再生成高频细节。本文研究扩散模型在傅里叶空间中的正向过程的归纳偏置。我们理论分析和实证证明，DDPM中高频成分较快噪声化导致反向过程中正态假设的违反，进而降低高频成分的生成质量。我们研究了一种在傅里叶空间中替代的正向过程，以相同的速率破坏所有频率，在生成过程中消除了典型的频率层次结构，并在高频为主要特征的数据集上实现了显著的性能改进，同时在标准成像基准测试中表现与DDPM相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型是数据模态生成领域的最先进技术，应用于图像、音频、蛋白质和材料等。</li>
<li>在傅里叶域中，数据模态具有指数衰减的方差和幅度特性。</li>
<li>DDPM中的正向过程导致高频成分较早地被噪声破坏，影响生成质量。</li>
<li>反向过程先生成低频信息，再生成高频细节。</li>
<li>DDPM中高频快速噪声化会违反反向过程中的正态假设。</li>
<li>研究了替代的正向过程，以相同的速率破坏所有频率，提高了高频为主要特征数据集上的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11278">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-26e6cddc16839ae5a7d6f51af95e4c91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8db5b6cc5b0f026af4e19944d8bd0d54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-667c2a11a2d7c3568255abbdbcbb1e74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9eff0f4dcad79895e33d0a60dde2a4a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-NPO-Negative-Preference-Optimization-for-Better-Preference-Aligned-Generation-of-Diffusion-Models"><a href="#Diffusion-NPO-Negative-Preference-Optimization-for-Better-Preference-Aligned-Generation-of-Diffusion-Models" class="headerlink" title="Diffusion-NPO: Negative Preference Optimization for Better Preference   Aligned Generation of Diffusion Models"></a>Diffusion-NPO: Negative Preference Optimization for Better Preference   Aligned Generation of Diffusion Models</h2><p><strong>Authors:Fu-Yun Wang, Yunhao Shui, Jingtan Piao, Keqiang Sun, Hongsheng Li</strong></p>
<p>Diffusion models have made substantial advances in image generation, yet models trained on large, unfiltered datasets often yield outputs misaligned with human preferences. Numerous methods have been proposed to fine-tune pre-trained diffusion models, achieving notable improvements in aligning generated outputs with human preferences. However, we argue that existing preference alignment methods neglect the critical role of handling unconditional&#x2F;negative-conditional outputs, leading to a diminished capacity to avoid generating undesirable outcomes. This oversight limits the efficacy of classifier-free guidance~(CFG), which relies on the contrast between conditional generation and unconditional&#x2F;negative-conditional generation to optimize output quality. In response, we propose a straightforward but versatile effective approach that involves training a model specifically attuned to negative preferences. This method does not require new training strategies or datasets but rather involves minor modifications to existing techniques. Our approach integrates seamlessly with models such as SD1.5, SDXL, video diffusion models and models that have undergone preference optimization, consistently enhancing their alignment with human preferences. </p>
<blockquote>
<p>扩散模型在图像生成方面取得了重大进展，但在大型未过滤数据集上训练的模型通常产生的输出与人类偏好不符。人们已经提出了许多方法来微调预训练的扩散模型，并在使生成输出与人类偏好对齐方面取得了显著的改进。然而，我们认为现有的偏好对齐方法忽视了处理无条件&#x2F;负条件输出的关键作用，导致避免生成不良结果的能力降低。这一疏忽限制了无分类引导（CFG）的效用，无分类引导依赖于有条件生成和无条件&#x2F;负条件生成之间的对比来优化输出质量。作为回应，我们提出了一种简单但通用有效的方法，该方法专门训练一个模型，使其适应负面偏好。这种方法不需要新的训练策略或数据集，而是对现有技术进行了一些小修改。我们的方法与SD1.5、SDXL、视频扩散模型以及经过偏好优化的模型无缝集成，始终如一地增强其与人类偏好的对齐。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11245v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>扩散模型在图像生成领域取得了显著进展，但训练于大规模未筛选数据集上的模型常产生与人类偏好不符的输出。现有研究多关注微调预训练扩散模型以对齐生成输出与人类偏好，但忽视了处理无条件&#x2F;负条件输出的重要性，导致难以避免生成不良结果。本文提出了一种简单而有效的方法，通过训练专注于负偏好的模型来解决这一问题，该方法无需新的训练策略或数据集，而是对现有技术进行微小调整。此方法可无缝集成到SD1.5、SDXL等模型中，也可应用于视频扩散模型和经过偏好优化的模型，有效提高它们与人类偏好的对齐程度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像生成方面的进展显著，但面对生成输出与人类偏好不符的问题。</li>
<li>现有方法关注微调预训练扩散模型以对齐生成输出与人类偏好，但处理无条件&#x2F;负条件输出的重要性被忽视。</li>
<li>忽视处理负条件输出限制了无分类引导（CFG）的效果，其依赖于条件生成与无条件&#x2F;负条件生成的对比以优化输出质量。</li>
<li>本文提出了一种简单有效的方法，通过训练专注于负偏好的模型来解决上述问题。</li>
<li>所提方法无需新的训练策略或数据集，而是对现有技术进行微小调整，具有广泛的应用性。</li>
<li>所提方法可无缝集成到多种模型中，如SD1.5、SDXL、视频扩散模型和经过偏好优化的模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11245">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e00040cc876dae5525661bc3a437c32f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-983c5ef0bbd020d3124cc943aa8f3d9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-551e9762cf830cf11eec8b38438e1b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-171e01dda70bfd2ea555d946c2e92fc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77ae6db309bbb62b54e268e3b36e8553.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DiCo-Revitalizing-ConvNets-for-Scalable-and-Efficient-Diffusion-Modeling"><a href="#DiCo-Revitalizing-ConvNets-for-Scalable-and-Efficient-Diffusion-Modeling" class="headerlink" title="DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion   Modeling"></a>DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion   Modeling</h2><p><strong>Authors:Yuang Ai, Qihang Fan, Xuefeng Hu, Zhenheng Yang, Ran He, Huaibo Huang</strong></p>
<p>Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns-highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet benchmarks, DiCo outperforms previous diffusion models in both image quality and generation speed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53 at 512x512, with a 2.7x and 3.1x speedup over DiT-XL&#x2F;2, respectively. Furthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID of 1.90 on ImageNet 256x256-without any additional supervision during training. Code: <a target="_blank" rel="noopener" href="https://github.com/shallowdream204/DiCo">https://github.com/shallowdream204/DiCo</a>. </p>
<blockquote>
<p>扩散模型（Diffusion Transformer，简称DiT）是一种很有前景的视觉生成扩散模型，表现出令人印象深刻的性能，但同时也伴随着较大的计算开销。对预训练的DiT模型分析表明，全局自注意力往往是冗余的，主要捕捉的是局部模式，这突显了寻找更高效替代方案的潜力。在本文中，我们重新审视卷积作为一种构建高效且表达性强的扩散模型的替代构建块。然而，简单地用卷积替换自注意力通常会导致性能下降。我们将这种性能差距归因于与Transformer相比，卷积神经网络（ConvNets）中的通道冗余度较高。为解决这一问题，我们引入了一种紧凑的通道注意力机制，该机制促进了更多不同通道的激活，从而增强了特征多样性。这导致了完全由标准ConvNet模块构建的扩散模型家族——扩散卷积网络（Diffusion ConvNet，简称DiCo）。在面向类别的ImageNet基准测试中，DiCo在图像质量和生成速度方面均优于先前的扩散模型。值得一提的是，DiCo-XL在分辨率为256x256时取得了FID分数为2.05，分辨率为512x512时取得了FID分数为2.53，相较于DiT-XL&#x2F;2分别实现了2.7倍和3.1倍的加速。此外，我们规模最大的模型DiCo-H在训练过程中没有额外的监督下达到了ImageNet 256x256分辨率下的FID分数为1.90。代码地址：<a target="_blank" rel="noopener" href="https://github.com/shallowdream204/DiCo">https://github.com/shallowdream204/DiCo</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11196v1">PDF</a> 27 pages, 29 figures, 9 tables</p>
<p><strong>Summary</strong><br>     扩散模型Diffusion Transformer（DiT）在视觉生成任务中表现出卓越性能，但计算开销较大。研究发现预训练的DiT模型中全局自注意力常常是冗余的，更多地捕捉局部模式。本研究探讨了卷积作为构建高效扩散模型的替代方案，单纯替换会导致性能下降。为此引入了紧凑通道注意力机制，提高了特征多样性，从而构建了完全基于标准卷积网络模块的Diffusion ConvNet（DiCo）。在条件ImageNet基准测试中，DiCo在图像质量和生成速度上均优于先前的扩散模型。特别是DiCo-XL在256x256和512x512分辨率下FID分别为2.05和2.53，相较于DiT-XL&#x2F;2分别实现了2.7倍和3.1倍的加速。此外，最大的DiCo-H模型在ImageNet 256x256上达到了FID 1.90，且训练过程中无需额外监督。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformer (DiT) 在视觉生成领域展现出色性能，但计算开销大。</li>
<li>预训练的DiT模型中全局自注意力往往是冗余的，更多地捕捉局部模式。</li>
<li>研究提出使用卷积作为构建高效扩散模型的替代方案。</li>
<li>单纯替换自注意力为卷积会导致性能下降，这被归因于ConvNets中的通道冗余性较高。</li>
<li>引入了紧凑通道注意力机制，提高了特征多样性。</li>
<li>DiCo模型在条件ImageNet基准测试中具有卓越性能，特别是在图像质量和生成速度方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11196">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-970bf6a5aee69fee62ff9189adc82f73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-872ff6c53666ffe064405e16a0398bed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8d4caf527a90e151d8eaa300a1586fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c96dcc62a3d5d39ef83b4cd60d08853a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CompAlign-Improving-Compositional-Text-to-Image-Generation-with-a-Complex-Benchmark-and-Fine-Grained-Feedback"><a href="#CompAlign-Improving-Compositional-Text-to-Image-Generation-with-a-Complex-Benchmark-and-Fine-Grained-Feedback" class="headerlink" title="CompAlign: Improving Compositional Text-to-Image Generation with a   Complex Benchmark and Fine-Grained Feedback"></a>CompAlign: Improving Compositional Text-to-Image Generation with a   Complex Benchmark and Fine-Grained Feedback</h2><p><strong>Authors:Yixin Wan, Kai-Wei Chang</strong></p>
<p>State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuest’s feedback as preference signals to improve diffusion models’ compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches. </p>
<blockquote>
<p>当前最先进的文本到图像（T2I）模型已具备根据文本提示生成高分辨率图像的能力。然而，它们在准确描绘包含多个物体、属性和空间关系的组合场景方面仍面临挑战。我们推出了CompAlign，这是一个以评估3D空间关系描绘能力为重点的具有挑战性的基准测试，旨在评估和改进组合图像生成模型。CompAlign包含900个复杂的跨主题图像生成提示，结合了数值和3D空间关系以及不同的属性绑定。我们的基准测试非常具有挑战性，包含了带有3个以上生成主题的生成任务，这些任务具有复杂的3D空间关系。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11178v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一个名为CompAlign的基准测试平台，该平台专注于评估模型在生成复杂组合图像时的三维空间关系描绘能力。该平台包含900个复杂的跨多主题图像生成提示，涵盖了数字与三维空间关系的组合以及多变属性绑定。CompAlign具有挑战性，尤其包含超过三个生成主题且带有复杂三维空间关系的生成任务。此外，本文还提出了一个名为CompQuest的可解释和准确的评估框架，用于分解复杂的提示为原子子问题，并使用多标签学习模型（MLLM）对模型生成图像中每个生成元素的正确性提供精细的二元反馈。这为生成的图像与组合提示之间的对齐程度提供了精确量化。同时，本文介绍了一个使用CompQuest反馈作为偏好信号的对齐框架，以提高扩散模型在组合图像生成方面的能力。通过调整每张图像的偏好设置，该方法易于扩展且灵活适用于不同的任务。对九个文本到图像模型的评估显示，这些模型在处理具有更复杂三维空间配置的组成任务时面临更大的挑战，并且在开源可访问模型和封闭源代码商业模型之间也存在明显的性能差距。使用CompAlign进行模型对齐的进一步实证研究产生了令人鼓舞的结果：对齐后的扩散模型在组合准确性方面取得了显著改进，特别是在复杂的生成任务上表现优于以前的方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>CompAlign基准测试平台专注于评估模型在描述三维空间关系方面的表现，尤其在复杂的组合图像生成方面。</li>
<li>CompAlign包含复杂的跨多主题图像生成提示，涵盖数字与三维空间关系的组合以及多变属性绑定，挑战性强。</li>
<li>CompQuest评估框架可以分解复杂的提示为原子子问题，并提供精细的二元反馈，以量化模型生成的图像与提示之间的对齐程度。</li>
<li>使用CompQuest反馈的对齐框架有助于提高扩散模型在组合图像生成方面的能力，特别是在处理复杂的三维空间配置时。</li>
<li>评估发现，文本到图像模型在处理复杂的组合任务时面临困难，特别是在涉及更多三维空间配置的情况下。</li>
<li>在开源可访问模型和封闭源代码商业模型之间，存在明显的性能差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4dd40edf2cbaffb4a7709cc361e0b38c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c7baf423cbd584f86124d5252244cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37436045e6041dc7c5141960bd962469.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1385a7e7509115533a2a849a2acdc621.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="One-Image-is-Worth-a-Thousand-Words-A-Usability-Preservable-Text-Image-Collaborative-Erasing-Framework"><a href="#One-Image-is-Worth-a-Thousand-Words-A-Usability-Preservable-Text-Image-Collaborative-Erasing-Framework" class="headerlink" title="One Image is Worth a Thousand Words: A Usability Preservable Text-Image   Collaborative Erasing Framework"></a>One Image is Worth a Thousand Words: A Usability Preservable Text-Image   Collaborative Erasing Framework</h2><p><strong>Authors:Feiran Li, Qianqian Xu, Shilong Bao, Zhiyong Yang, Xiaochun Cao, Qingming Huang</strong></p>
<p>Concept erasing has recently emerged as an effective paradigm to prevent text-to-image diffusion models from generating visually undesirable or even harmful content. However, current removal methods heavily rely on manually crafted text prompts, making it challenging to achieve a high erasure (efficacy) while minimizing the impact on other benign concepts (usability). In this paper, we attribute the limitations to the inherent gap between the text and image modalities, which makes it hard to transfer the intricately entangled concept knowledge from text prompts to the image generation process. To address this, we propose a novel solution by directly integrating visual supervision into the erasure process, introducing the first text-image Collaborative Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the concept jointly by text prompts and the corresponding undesirable images induced by the prompts, and then reduces the generating probability of the target concept through negative guidance. This approach effectively bypasses the knowledge gap between text and image, significantly enhancing erasure efficacy. Additionally, we design a text-guided image concept refinement strategy that directs the model to focus on visual features most relevant to the specified text concept, minimizing disruption to other benign concepts. Finally, comprehensive experiments suggest that Co-Erasing outperforms state-of-the-art erasure approaches significantly with a better trade-off between efficacy and usability. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/Co-Erasing">https://github.com/Ferry-Li/Co-Erasing</a>. </p>
<blockquote>
<p>概念消除作为一种防止文本到图像扩散模型生成视觉不良甚至有害内容的有效范式，最近崭露头角。然而，当前的消除方法严重依赖于手动构建的文本提示，这使得在提高消除效果的同时尽量减少对其他良性概念的影响成为一个挑战。本文认为这些局限性源于文本和图像模态之间的固有差距，这使得从文本提示将复杂纠缠的概念知识转移到图像生成过程变得困难。针对这一问题，我们提出了一种通过直接在消除过程中引入视觉监督的解决方案，并引入了首个文本图像协同概念消除（Co-Erasing）框架。具体来说，Co-Erasing通过文本提示和由提示产生的相应的不良图像共同描述概念，然后通过负面指导降低目标概念的生成概率。这种方法有效地克服了文本和图像之间的知识差距，大大提高了消除效果。此外，我们还设计了一种文本引导的图像概念细化策略，引导模型关注与指定文本概念最相关的视觉特征，尽量减少对其他良性概念的干扰。最后，综合实验表明，Co-Erasing在效果和可用性之间取得了更好的权衡，显著优于最先进的消除方法。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/Co-Erasing%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ferry-Li/Co-Erasing上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11131v1">PDF</a> This paper has been accepeted to ICML 2025. Not Final Version</p>
<p><strong>Summary</strong></p>
<p>本文介绍了新兴的概念消除技术及其在文本到图像扩散模型中的应用。当前去除方法依赖手动构建的文本提示，存在挑战在于实现高效的消除效果同时最小化对其他良性概念的影响。为解决此问题，本文提出了一个新颖的解决策略，即通过直接集成视觉监督到消除过程中，引入首个文本图像协同概念消除（Co-Erasing）框架。该框架通过文本提示和相应的不理想图像共同描述概念，然后通过负指导降低目标概念的生成概率。这种方法有效地绕过了文本和图像之间的知识差距，大大提高了消除效果。同时，设计了一种文本引导的图像概念优化策略，使模型关注与指定文本概念最相关的视觉特征，尽量减少对其他良性概念的干扰。实验表明，Co-Erasing显著优于现有消除方法，在效果和可用性之间达到更好的平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>概念消除技术能有效防止文本到图像扩散模型生成不良内容。</li>
<li>当前去除方法存在挑战，难以实现高效消除同时最小化对其他良性概念的影响。</li>
<li>引入Co-Erasing框架，通过直接集成视觉监督到消除过程中解决此问题。</li>
<li>Co-Erasing通过文本提示和不良图像共同描述概念，降低目标概念的生成概率。</li>
<li>Co-Erasing有效绕过文本和图像之间的知识差距，提高消除效果。</li>
<li>设计文本引导的图像概念优化策略，使模型关注与指定文本概念最相关的视觉特征。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11131">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d1a977cee20edd6eec8387241bf1aac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-366e55be0464888c0dbacd9f7b3a8881.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-016182232ad7e9656dd22c9fbebcf50d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc4925130579ef796a3df0145eb0549f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8b1806c78253c43f3adc9d074decd7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="What’s-Inside-Your-Diffusion-Model-A-Score-Based-Riemannian-Metric-to-Explore-the-Data-Manifold"><a href="#What’s-Inside-Your-Diffusion-Model-A-Score-Based-Riemannian-Metric-to-Explore-the-Data-Manifold" class="headerlink" title="What’s Inside Your Diffusion Model? A Score-Based Riemannian Metric to   Explore the Data Manifold"></a>What’s Inside Your Diffusion Model? A Score-Based Riemannian Metric to   Explore the Data Manifold</h2><p><strong>Authors:Simone Azeglio, Arianna Di Bernardo</strong></p>
<p>Recent advances in diffusion models have demonstrated their remarkable ability to capture complex image distributions, but the geometric properties of the learned data manifold remain poorly understood. We address this gap by introducing a score-based Riemannian metric that leverages the Stein score function from diffusion models to characterize the intrinsic geometry of the data manifold without requiring explicit parameterization. Our approach defines a metric tensor in the ambient space that stretches distances perpendicular to the manifold while preserving them along tangential directions, effectively creating a geometry where geodesics naturally follow the manifold’s contours. We develop efficient algorithms for computing these geodesics and demonstrate their utility for both interpolation between data points and extrapolation beyond the observed data distribution. Through experiments on synthetic data with known geometry, Rotated MNIST, and complex natural images via Stable Diffusion, we show that our score-based geodesics capture meaningful transformations that respect the underlying data distribution. Our method consistently outperforms baseline approaches on perceptual metrics (LPIPS) and distribution-level metrics (FID, KID), producing smoother, more realistic image transitions. These results reveal the implicit geometric structure learned by diffusion models and provide a principled way to navigate the manifold of natural images through the lens of Riemannian geometry. </p>
<blockquote>
<p>扩散模型的最新进展表明其在捕捉复杂图像分布方面的卓越能力，但是对所学习数据流形的几何特性仍然了解不足。我们通过引入基于分数的黎曼度量来解决这一差距，该度量利用扩散模型的斯坦得分函数来表征数据流形的内在几何，而无需显式参数化。我们的方法在环境空间中定义一个度量张量，该张量会拉伸流形垂直方向的长度同时保留其沿切线方向的距离，从而有效地创建一个几何体，其中的测地线自然地遵循流形的轮廓。我们开发了计算这些测地线的有效算法，并展示了它们在数据点之间进行插值和超出观察到的数据分布进行外推方面的实用性。通过对具有已知几何的合成数据、旋转的MNIST以及通过稳定扩散的复杂自然图像的实验，我们证明了我们的基于得分的测地线能够捕获有意义的转换，这些转换尊重底层的数据分布。我们的方法在感知度量（LPIPS）和分布级度量（FID、KID）上始终优于基准方法，产生更平滑、更现实的图像过渡。这些结果揭示了扩散模型学习的隐式几何结构，并通过黎曼几何的透镜提供了一种在天然图像流形上导航的原则方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11128v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了扩散模型的新进展，通过引入基于分数的黎曼度量来解决数据流形几何属性理解不足的问题。该度量利用Stein得分函数来刻画数据流形的内在几何结构，无需显式参数化。该方法在合成数据、旋转MNIST和通过Stable Diffusion的复杂自然图像上的实验表明，基于分数的测地线能够捕捉有意义的转换，尊重底层数据分布。此方法在感知度量（LPIPS）和分布级度量（FID、KID）上优于基准方法，产生更平滑、更现实的图像过渡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型能够捕捉复杂的图像分布，但其数据流形的几何属性理解不足。</li>
<li>引入了一种基于分数的黎曼度量，用于刻画数据流形的内在几何结构，无需显式参数化。</li>
<li>该方法通过定义环境空间中的度量张量，在垂直于流形的方向上拉伸距离，同时沿切线方向保持距离，创建了一个自然的几何结构。</li>
<li>开发了计算这些测地线的有效算法，可用于数据点之间的插值和观察到的数据分布之外的插值。</li>
<li>实验表明，基于分数的测地线能够捕捉有意义的转换，尊重底层数据分布，并产生更平滑、更现实的图像过渡。</li>
<li>该方法优于基准方法，在感知度量（LPIPS）和分布级度量（FID、KID）上表现出更好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11128">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dee3b80e828d83541fb64e244ebf5b7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d4a3e849c52807107d384a39a76c64d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a828947f6ff4f00310bf871042d21f97.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Shackled-Dancing-A-Bit-Locked-Diffusion-Algorithm-for-Lossless-and-Controllable-Image-Steganography"><a href="#Shackled-Dancing-A-Bit-Locked-Diffusion-Algorithm-for-Lossless-and-Controllable-Image-Steganography" class="headerlink" title="Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and   Controllable Image Steganography"></a>Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and   Controllable Image Steganography</h2><p><strong>Authors:Tianshuo Zhang, Gao Jia, Wenzhe Zhai, Rui Yann, Xianglei Xing</strong></p>
<p>Data steganography aims to conceal information within visual content, yet existing spatial- and frequency-domain approaches suffer from trade-offs between security, capacity, and perceptual quality. Recent advances in generative models, particularly diffusion models, offer new avenues for adaptive image synthesis, but integrating precise information embedding into the generative process remains challenging. We introduce Shackled Dancing Diffusion, or SD$^2$, a plug-and-play generative steganography method that combines bit-position locking with diffusion sampling injection to enable controllable information embedding within the generative trajectory. SD$^2$ leverages the expressive power of diffusion models to synthesize diverse carrier images while maintaining full message recovery with $100%$ accuracy. Our method achieves a favorable balance between randomness and constraint, enhancing robustness against steganalysis without compromising image fidelity. Extensive experiments show that SD$^2$ substantially outperforms prior methods in security, embedding capacity, and stability. This algorithm offers new insights into controllable generation and opens promising directions for secure visual communication. </p>
<blockquote>
<p>数据隐写术旨在将信息隐藏在视觉内容中，但现有的空间和频率域方法在安全、容量和感知质量之间存存在权衡问题。生成模型的最新进展，特别是扩散模型，为自适应图像合成提供了新的途径，但将精确的信息嵌入生成过程仍然具有挑战性。我们引入了Shackled Dancing Diffusion，简称SD$^2$，这是一种即插即用的生成隐写术方法，它将位位置锁定与扩散采样注入相结合，实现在生成轨迹中可控的信息嵌入。SD$^2$利用扩散模型的表达能力合成多样的载体图像，同时以100%的准确率恢复全部信息。我们的方法在随机性和约束之间达到了有利的平衡，在提高抗隐写分析的能力的同时，不损害图像保真度。大量实验表明，SD$^2$在安全、嵌入容量和稳定性方面大大优于以前的方法。该算法为可控生成提供了新的见解，并为安全视觉通信提供了有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10950v1">PDF</a> </p>
<p><strong>Summary</strong><br>数据隐写术旨在将信息隐藏在视觉内容中，但现有的空间域和频域方法在安全、容量和感知质量之间存在此消彼长的问题。最近生成模型，尤其是扩散模型的发展，为自适应图像合成提供了新的途径，但将精确信息嵌入生成过程中仍具有挑战性。我们提出了Shackled Dancing Diffusion（SD$^2$）一种即插即用的生成隐写术方法，结合位位置锁定和扩散采样注入，实现在生成轨迹中的可控信息嵌入。SD$^2$利用扩散模型的表达能力合成多样的载体图像，同时保持100%的信息恢复准确性。我们的方法实现了随机性和约束性的平衡，提高了对隐写分析的鲁棒性，而不损害图像保真度。大量实验表明，SD$^2$在安全、嵌入容量和稳定性方面显著优于现有方法。该算法为可控生成提供了新的见解，并为安全视觉通信打开了有前景的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据隐写术旨在隐藏信息于视觉内容中，面临安全、容量和感知质量的权衡问题。</li>
<li>现有空间域和频域方法存在局限。</li>
<li>扩散模型提供新的图像合成途径，但信息嵌入具有挑战性。</li>
<li>提出Shackled Dancing Diffusion（SD$^2$）方法，结合位位置锁定和扩散采样注入。</li>
<li>SD$^2$能合成多样的载体图像，同时保持100%信息恢复准确性。</li>
<li>SD$^2$实现了随机性和约束性的平衡，提高鲁棒性并不损害图像质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10950">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a868b1e327e63d4dbdc50b0084ab1ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4662a01a2f2c7a8d2cbc997f3a927c5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e53fe55d9c823a17ec7e498757d2f7f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d90d186c2a510b8d24c6c661c1571951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e9118e1f98bd6af8c0e55280683c781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8fc61c3e19d02cf8644749abab7ecd0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SynRailObs-A-Synthetic-Dataset-for-Obstacle-Detection-in-Railway-Scenarios"><a href="#SynRailObs-A-Synthetic-Dataset-for-Obstacle-Detection-in-Railway-Scenarios" class="headerlink" title="SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway   Scenarios"></a>SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway   Scenarios</h2><p><strong>Authors:Qiushi Guo, Jason Rambach</strong></p>
<p>Detecting potential obstacles in railway environments is critical for preventing serious accidents. Identifying a broad range of obstacle categories under complex conditions requires large-scale datasets with precisely annotated, high-quality images. However, existing publicly available datasets fail to meet these requirements, thereby hindering progress in railway safety research. To address this gap, we introduce SynRailObs, a high-fidelity synthetic dataset designed to represent a diverse range of weather conditions and geographical features. Furthermore, diffusion models are employed to generate rare and difficult-to-capture obstacles that are typically challenging to obtain in real-world scenarios. To evaluate the effectiveness of SynRailObs, we perform experiments in real-world railway environments, testing on both ballasted and ballastless tracks across various weather conditions. The results demonstrate that SynRailObs holds substantial potential for advancing obstacle detection in railway safety applications. Models trained on this dataset show consistent performance across different distances and environmental conditions. Moreover, the model trained on SynRailObs exhibits zero-shot capabilities, which are essential for applications in security-sensitive domains. The data is available in <a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/qiushi910/synrailobs">https://www.kaggle.com/datasets/qiushi910/synrailobs</a>. </p>
<blockquote>
<p>在铁路环境中检测潜在障碍对于预防严重事故至关重要。在复杂条件下识别广泛障碍类别需要大规模数据集，其中包含精确标注的高质量图像。然而，现有公开数据集无法满足这些要求，从而阻碍了铁路安全研究的进展。为解决这一差距，我们推出了SynRailObs这一高保真合成数据集，旨在代表各种天气条件和地理特征。此外，还使用了扩散模型来生成罕见且难以捕获的障碍，这些障碍在真实场景中通常难以获得。为了评估SynRailObs的有效性，我们在真实铁路环境中进行实验，在多种天气条件下对球轨和无球轨轨道进行测试。结果表明，SynRailObs在铁路安全应用的障碍物检测方面具有巨大潜力。在此数据集上训练的模型在不同距离和环境条件下表现一致。此外，在SynRailObs上训练的模型具有零样本能力，这对于安全敏感领域的应用至关重要。数据可在<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/qiushi910/synrailobs%E6%89%BE%E5%88%B0%E3%80%82">https://www.kaggle.com/datasets/qiushi910/synrailobs找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10784v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在铁路环境中检测潜在障碍对于预防严重事故至关重要。为满足复杂条件下广泛障碍类别的识别需求，需要大规模、精确标注、高质量图像的数据集。然而，现有公开数据集无法满足这些要求，阻碍了铁路安全研究的进展。为解决这一缺口，我们推出SynRailObs高保真合成数据集，旨在代表各种天气和地理特征。此外，采用扩散模型生成罕见且难以捕获的障碍，这些障碍在真实场景中通常难以获取。通过真实铁路环境实验验证SynRailObs的有效性，在球粒和无球粒轨道上测试各种天气条件下的性能。结果表明，SynRailObs在铁路安全应用中的障碍检测方面具有巨大潜力。模型在此数据集上的表现距离和环境条件均表现一致。此外，在安全性敏感领域的应用中，经过SynRailObs训练的模型具有零射击能力。数据可在<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/qiushi910/synrailobs%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://www.kaggle.com/datasets/qiushi910/synrailobs上获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>铁路障碍检测对于预防事故至关重要，但现有数据集无法满足复杂条件下的多样障碍识别需求。</li>
<li>SynRailObs是一个高保真合成数据集，旨在代表各种天气和地理特征的障碍情况。</li>
<li>扩散模型被用于生成真实场景中难以捕获的罕见障碍。</li>
<li>SynRailObs数据集通过真实铁路环境实验验证其有效性。</li>
<li>模型在SynRailObs数据集上的表现稳定，能够适应不同距离和环境条件。</li>
<li>经过SynRailObs训练的模型具有零射击能力，这在安全性敏感领域的应用中至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10784">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b2ad85da39055b403f1d0fc798d98e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5128161cad17fc422f9eefb185d917ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-177211e63cf03a40f5c8f26d764ffa58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c6ccd72e887fe76df00d90b9d8456e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcfcd1f719f56b3d5eb764c454f142ef.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="IPGO-Indirect-Prompt-Gradient-Optimization-for-Parameter-Efficient-Prompt-level-Fine-Tuning-on-Text-to-Image-Models"><a href="#IPGO-Indirect-Prompt-Gradient-Optimization-for-Parameter-Efficient-Prompt-level-Fine-Tuning-on-Text-to-Image-Models" class="headerlink" title="IPGO: Indirect Prompt Gradient Optimization for Parameter-Efficient   Prompt-level Fine-Tuning on Text-to-Image Models"></a>IPGO: Indirect Prompt Gradient Optimization for Parameter-Efficient   Prompt-level Fine-Tuning on Text-to-Image Models</h2><p><strong>Authors:Jianping Ye, Michel Wedel, Kunpeng Zhang</strong></p>
<p>Text-to-Image Diffusion models excel at generating images from text prompts but often exhibit suboptimal alignment with content semantics, aesthetics, and human preferences. To address these limitations, this study proposes a novel parameter-efficient framework, Indirect Prompt Gradient Optimization (IPGO), for prompt-level diffusion model fine-tuning. IPGO enhances prompt embeddings by injecting continuously differentiable embeddings at the beginning and end of the prompt embeddings, leveraging low-rank structures with the flexibility and nonlinearity from rotations. This approach enables gradient-based optimization of injected embeddings under range, orthonormality, and conformity constraints, effectively narrowing the search space, promoting a stable solution, and ensuring alignment between the embeddings of the injected embeddings and the original prompt. Its extension IPGO+ adds a parameter-free cross-attention mechanism on the prompt embedding to enforce dependencies between the original prompt and the inserted embeddings. We conduct extensive evaluations through prompt-wise (IPGO) and prompt-batch (IPGO+) training using three reward models of image aesthetics, image-text alignment, and human preferences across three datasets of varying complexity. The results show that IPGO consistently outperforms SOTA benchmarks, including stable diffusion v1.5 with raw prompts, text-embedding-based methods (TextCraftor), training-based methods (DRaFT and DDPO), and training-free methods (DPO-Diffusion, Promptist, and ChatGPT-4o). Specifically, IPGO achieves a win-rate exceeding 99% in prompt-wise learning, and IPGO+ achieves a comparable, but often better performance against current SOTAs (a 75% win rate) in prompt-batch learning. Moreover, we illustrate IPGO’s generalizability and its capability to significantly enhance image quality while requiring minimal data and resources. </p>
<blockquote>
<p>文本转图像扩散模型擅长根据文本提示生成图像，但在内容语义、美学和人类偏好方面的对齐常常不够理想。为了解决这个问题，本研究提出了一种新的参数高效框架——间接提示梯度优化（IPGO），用于提示级扩散模型的微调。IPGO通过注入连续可微的嵌入来增强提示嵌入，这些嵌入位于提示嵌入的开始和结束处，利用低阶结构，同时拥有旋转的灵活性和非线性。这种方法允许在范围、正交性和一致性约束下对注入的嵌入进行基于梯度的优化，有效地缩小了搜索空间，促进了稳定解决方案的出现，并确保了注入嵌入的嵌入与原始提示之间的对齐。其扩展版IPGO+在提示嵌入上增加了一种无参数交叉注意机制，以强制执行原始提示和插入的嵌入之间的依赖关系。我们通过提示（IPGO）和提示批次（IPGO+）训练，使用三种图像美学、图像文本对齐和人类偏好奖励模型，在三个不同复杂度的数据集上进行了广泛评估。结果表明，IPGO一致地超越了最新基准，包括使用原始提示的稳定扩散v1.5、基于文本嵌入的方法（TextCraftor）、基于训练的方法（DRaFT和DDPO）和免训练方法（DPO-Diffusion、Promptist和ChatGPT-4o）。具体来说，IPGO在提示级学习中实现了超过99%的胜率，而IPGO+在提示批次学习中达到了与当前最新技术相当（75%的胜率）但往往更好的性能。此外，我们还展示了IPGO的通用性以及其在需要大量数据和资源的情况下显著提高图像质量的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21812v2">PDF</a> 9 pages, 2 figures, 4 tables</p>
<p><strong>摘要</strong></p>
<p>文本到图像扩散模型能够根据文本提示生成图像，但在内容语义、美学和人类偏好方面的对齐性常常不够理想。为解决这些问题，本研究提出了一种新型的参数高效框架——间接提示梯度优化（IPGO），用于扩散模型的提示级别微调。IPGO通过注入连续可微分的嵌入，增强提示嵌入，利用旋转的灵活性和非线性性，在提示嵌入的开头和结尾处进行注入。此方法能够实现注入嵌入的梯度优化，在范围、正交性和一致性约束下有效缩小搜索空间，促进稳定解决方案的出现，并确保注入嵌入与原始提示嵌入之间的对齐。其扩展版IPGO+在提示嵌入上增加了一种无参数交叉注意力机制，以强制执行原始提示和插入嵌入之间的依赖关系。我们通过使用三种奖励模型（图像美学、图像文本对齐和人类偏好）和三个不同复杂度的数据集进行提示（IPGO）和提示批次（IPGO+）的广泛评估。结果表明，IPGO在多个场景下表现超过其他尖端方法，包括原始提示的稳定扩散v1.5方法、基于文本嵌入的方法（TextCraftor）、基于训练的方法（DRaFT和DDPO）和无训练的方法（DPO-Diffusion、Promptist和ChatGPT-4o）。具体而言，IPGO在提示级学习中取得超过99%的胜率，而IPGO+在提示批次学习中取得了与当前最佳做法相当的（有时更好）表现，达到75%的胜率。此外，我们展示了IPGO的泛化能力及其增强图像质量的能力，同时需要最少的数据和资源。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本到图像扩散模型在根据文本提示生成图像方面表现出色，但在语义对齐、美学和人类偏好方面存在局限性。</li>
<li>IPGO框架通过注入连续可微分的嵌入增强提示嵌入，提高了扩散模型的性能。</li>
<li>IPGO利用低阶结构和旋转的灵活性和非线性性进行优化，确保嵌入之间的对齐并促进稳定解决方案的出现。</li>
<li>IPGO+通过添加交叉注意力机制进一步增强了IPGO的性能，强化了原始提示和插入嵌入之间的依赖关系。</li>
<li>使用三种奖励模型进行广泛评估表明IPGO在各种场景下均表现出色，与其他方法的比较显示出其优越性。</li>
<li>IPGO在提示级学习中取得显著成果，而IPGO+在提示批次学习中也表现出强大的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21812">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d69303016cfffd4753f633eac26cbc47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef0ce9b5d6f26a4a1178b42077c3cafa.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d7635e32016c1ff643bb2aff7cc428ab.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-20  GOUHFI a novel contrast- and resolution-agnostic segmentation tool for   Ultra-High Field MRI
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0ed2fe0750fb36794531fa20f8251421.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-20  MutualNeRF Improve the Performance of NeRF under Limited Samples with   Mutual Information Theory
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
