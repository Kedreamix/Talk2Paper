<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-20  QVGen Pushing the Limit of Quantized Video Generative Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4e9118e1f98bd6af8c0e55280683c781.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-20-æ›´æ–°"><a href="#2025-05-20-æ›´æ–°" class="headerlink" title="2025-05-20 æ›´æ–°"></a>2025-05-20 æ›´æ–°</h1><h2 id="QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models"><a href="#QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models" class="headerlink" title="QVGen: Pushing the Limit of Quantized Video Generative Models"></a>QVGen: Pushing the Limit of Quantized Video Generative Models</h2><p><strong>Authors:Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, Jun Zhang</strong></p>
<p>Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\Phi$, we propose a rank-decay strategy that progressively eliminates $\Phi$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\mathbf{\gamma}$ to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench. </p>
<blockquote>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„è§†é¢‘åˆæˆã€‚ç„¶è€Œï¼Œå…¶å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚å¯¹å®é™…éƒ¨ç½²å¸¦æ¥äº†ä¸¥å³»æŒ‘æˆ˜ï¼Œå³ä½¿åœ¨é«˜ç«¯GPUä¸Šä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä½œä¸ºä¸€ç§å¸¸ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œé‡åŒ–åœ¨é™ä½å›¾åƒDMçš„æˆæœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œè€Œç›´æ¥åº”ç”¨äºè§†é¢‘DMåˆ™ä»ç„¶æ— æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†QVGenï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æç«¯ä½æ¯”ç‰¹é‡åŒ–ï¼ˆä¾‹å¦‚4ä½åŠä»¥ä¸‹ï¼‰ä¸‹é«˜æ€§èƒ½å’Œæ¨ç†æ•ˆç‡çš„è§†é¢‘DMå®šåˆ¶çš„æ–°å‹é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬ä»ç†è®ºåˆ†æå¼€å§‹ï¼Œè¯æ˜é™ä½æ¢¯åº¦èŒƒæ•°æ˜¯ä¿ƒè¿›QATæ”¶æ•›çš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¾…åŠ©æ¨¡å—ï¼ˆÎ¦ï¼‰æ¥ç¼“è§£å¤§é‡çš„é‡åŒ–è¯¯å·®ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ”¶æ•›æ€§ã€‚ä¸ºäº†æ¶ˆé™¤Î¦çš„æ¨ç†å¼€é”€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç­‰çº§è¡°å‡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€æ­¥æ¶ˆé™¤Î¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åå¤ä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å’Œæå‡ºçš„åŸºäºç­‰çº§çš„è§„åˆ™Î³æ¥è¯†åˆ«å’Œè¡°å‡è´¡çŒ®è¾ƒå°çš„æˆåˆ†ã€‚æ­¤ç­–ç•¥åœ¨æ¶ˆé™¤æ¨ç†å¼€é”€çš„åŒæ—¶ä¿ç•™äº†æ€§èƒ½ã€‚åœ¨4ç§æœ€å…ˆè¿›çš„è§†é¢‘DMsä¸Šçš„å¹¿æ³›å®éªŒï¼Œå‚æ•°å¤§å°ä»1.3Båˆ°14Bä¸ç­‰ï¼Œè¡¨æ˜QVGené¦–æ¬¡åœ¨4ä½è®¾ç½®ä¸‹è¾¾åˆ°å…¨ç²¾åº¦ç›¸å½“çš„è´¨é‡ï¼Œå¹¶ä¸”æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„3ä½CogVideoX-2Båœ¨VBenchä¸Šçš„åŠ¨æ€åº¦å’Œåœºæ™¯ä¸€è‡´æ€§åˆ†åˆ«æé«˜äº†+25.28å’Œ+8.43ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11497v1">PDF</a> Our code will be released upon acceptance</p>
<p><strong>Summary</strong><br>     è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰èƒ½å¤Ÿå®ç°é«˜è´¨é‡è§†é¢‘åˆæˆï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚å¯¹å®é™…éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚é’ˆå¯¹è§†é¢‘DMsçš„é‡åŒ–è®­ç»ƒæ¡†æ¶QVGenè¢«æå‡ºï¼Œè¯¥æ¡†æ¶é’ˆå¯¹æä½æ¯”ç‰¹é‡åŒ–ï¼ˆå¦‚4ä½åŠä»¥ä¸‹ï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡å¼•å…¥è¾…åŠ©æ¨¡å—å’Œæ’åè¡°å‡ç­–ç•¥æé«˜æ€§èƒ½å’Œæ¨ç†æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒQVGenåœ¨å¤šç§å…ˆè¿›è§†é¢‘DMsä¸­å®ç°äº†å…¨ç²¾åº¦ç›¸å½“çš„è´¨é‡ï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å¯å®ç°é«˜è´¨é‡è§†é¢‘åˆæˆï¼Œä½†è®¡ç®—å’Œå†…å­˜éœ€æ±‚å·¨å¤§ï¼Œå¯¹å®é™…éƒ¨ç½²å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>QVGenæ˜¯ä¸€ç§é’ˆå¯¹è§†é¢‘DMsçš„é‡åŒ–è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½å’Œæ¨ç†æ•ˆç‡ï¼Œå°¤å…¶é€‚ç”¨äºæä½æ¯”ç‰¹é‡åŒ–ã€‚</li>
<li>QVGené€šè¿‡å¼•å…¥è¾…åŠ©æ¨¡å—æ¥å‡è½»é‡åŒ–è¯¯å·®ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>QVGené‡‡ç”¨æ’åè¡°å‡ç­–ç•¥æ¶ˆé™¤è¾…åŠ©æ¨¡å—åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„å¼€é”€ï¼Œé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£å’ŒåŸºäºæ’åçš„æ­£åˆ™åŒ–ç­–ç•¥é€æ­¥æ¶ˆé™¤ä½è´¡çŒ®ç»„ä»¶ã€‚</li>
<li>QVGenåœ¨å¤šç§å…ˆè¿›è§†é¢‘DMsä¸­å®ç°äº†å…¨ç²¾åº¦ç›¸å½“çš„è´¨é‡ï¼Œå¹¶åœ¨æŸäº›æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>QVGenä¸ºè§†é¢‘DMsçš„å®ç”¨éƒ¨ç½²æä¾›äº†æ–°çš„å¯èƒ½æ€§å’Œè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6313b87d21d88cf71900e34a05e944c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e624add7ca264450b9a69bfc9ff2c6cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b8a7b15cb9e035c4b2a7f5be4db9eb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be2747313516f4355f38e1fd38810f05.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Diff-Unfolding-A-Model-Based-Score-Learning-Framework-for-Inverse-Problems"><a href="#Diff-Unfolding-A-Model-Based-Score-Learning-Framework-for-Inverse-Problems" class="headerlink" title="Diff-Unfolding: A Model-Based Score Learning Framework for Inverse   Problems"></a>Diff-Unfolding: A Model-Based Score Learning Framework for Inverse   Problems</h2><p><strong>Authors:Yuanhao Wang, Shirin Shoushtari, Ulugbek S. Kamilov</strong></p>
<p>Diffusion models are extensively used for modeling image priors for inverse problems. We introduce \emph{Diff-Unfolding}, a principled framework for learning posterior score functions of \emph{conditional diffusion models} by explicitly incorporating the physical measurement operator into a modular network architecture. Diff-Unfolding formulates posterior score learning as the training of an unrolled optimization scheme, where the measurement model is decoupled from the learned image prior. This design allows our method to generalize across inverse problems at inference time by simply replacing the forward operator without retraining. We theoretically justify our unrolling approach by showing that the posterior score can be derived from a composite model-based optimization formulation. Extensive experiments on image restoration and accelerated MRI show that Diff-Unfolding achieves state-of-the-art performance, improving PSNR by up to 2 dB and reducing LPIPS by $22.7%$, while being both compact (47M parameters) and efficient (0.72 seconds per $256 \times 256$ image). An optimized C++&#x2F;LibTorch implementation further reduces inference time to 0.63 seconds, underscoring the practicality of our approach. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹è¢«å¹¿æ³›ç”¨äºé€†å‘é—®é¢˜çš„å›¾åƒå…ˆéªŒå»ºæ¨¡ã€‚æˆ‘ä»¬ä»‹ç»äº†Diff-Unfoldingï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ˜ç¡®åœ°å°†ç‰©ç†æµ‹é‡ç®—å­èå…¥æ¨¡å—åŒ–ç½‘ç»œæ¶æ„æ¥å­¦ä¹ æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åéªŒå¾—åˆ†å‡½æ•°çš„åŸç†æ€§æ¡†æ¶ã€‚Diff-Unfoldingå°†åéªŒå¾—åˆ†å­¦ä¹ åˆ¶å®šä¸ºå±•å¼€ä¼˜åŒ–æ–¹æ¡ˆçš„è®­ç»ƒï¼Œå…¶ä¸­æµ‹é‡æ¨¡å‹ä¸å­¦ä¹ çš„å›¾åƒå…ˆéªŒè§£è€¦ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ¨ç†æ—¶é€šè¿‡ç®€å•åœ°æ›¿æ¢æ­£å‘ç®—å­æ¥æ³›åŒ–åˆ°å„ç§é€†å‘é—®é¢˜ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬é€šè¿‡æ˜¾ç¤ºåéªŒå¾—åˆ†å¯ä»¥ä»åŸºäºæ¨¡å‹çš„ç»„åˆä¼˜åŒ–å…¬å¼ä¸­å¾—å‡ºï¼Œä»ç†è®ºä¸Šè¯æ˜äº†æˆ‘ä»¬çš„å±•å¼€æ–¹æ³•ã€‚åœ¨å›¾åƒæ¢å¤å’ŒåŠ é€ŸMRIçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDiff-Unfoldingè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒPSNRæé«˜äº†é«˜è¾¾2dBï¼ŒLPIPSé™ä½äº†22.7%ï¼ŒåŒæ—¶æ—¢ç´§å‡‘ï¼ˆ4700ä¸‡å‚æ•°ï¼‰åˆé«˜æ•ˆï¼ˆæ¯256 x 256å›¾åƒ0.72ç§’ï¼‰ã€‚ç»è¿‡ä¼˜åŒ–çš„C++&#x2F;LibTorchå®ç°è¿›ä¸€æ­¥å°†æ¨ç†æ—¶é—´å‡å°‘åˆ°0.63ç§’ï¼Œçªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11393v1">PDF</a> 19 pages, 13 figures,</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹è¢«å¹¿æ³›ç”¨äºé€†å‘é—®é¢˜çš„å›¾åƒå…ˆéªŒå»ºæ¨¡ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDiff-Unfoldingçš„ç†è®ºæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ˜ç¡®åœ°å°†ç‰©ç†æµ‹é‡ç®—å­èå…¥æ¨¡å—åŒ–ç½‘ç»œæ¶æ„ï¼Œå­¦ä¹ æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åéªŒåˆ†æ•°å‡½æ•°ã€‚Diff-Unfoldingå°†åéªŒåˆ†æ•°å­¦ä¹ åˆ¶å®šä¸ºå±•å¼€ä¼˜åŒ–æ–¹æ¡ˆçš„è®­ç»ƒï¼Œå…¶ä¸­æµ‹é‡æ¨¡å‹ä¸å­¦ä¹ çš„å›¾åƒå…ˆéªŒè§£è€¦ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ¨ç†æ—¶é€šè¿‡ç®€å•åœ°æ›¿æ¢å‰å‘ç®—å­è€Œé€‚åº”å„ç§é€†å‘é—®é¢˜ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æœ¬æ–‡ç†è®ºè¯æ˜äº†å±•å¼€æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„å›¾åƒæ¢å¤å’ŒåŠ é€ŸMRIå®éªŒè¡¨æ˜ï¼ŒDiff-Unfoldingå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒPSNRæé«˜äº†é«˜è¾¾2dBï¼ŒLPIPSé™ä½äº†22.7%ï¼ŒåŒæ—¶æ¨¡å‹ç´§å‡‘ï¼ˆ47Må‚æ•°ï¼‰ä¸”é«˜æ•ˆï¼ˆæ¯å¼ 256x256å›¾åƒå¤„ç†æ—¶é—´ä¸º0.72ç§’ï¼‰ã€‚ä½¿ç”¨ä¼˜åŒ–çš„C++&#x2F;LibTorchå®ç°å¯å°†æ¨ç†æ—¶é—´è¿›ä¸€æ­¥ç¼©çŸ­è‡³0.63ç§’ï¼Œçªæ˜¾äº†æ–¹æ³•çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é€†å‘é—®é¢˜çš„å›¾åƒå…ˆéªŒå»ºæ¨¡ä¸­æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>æå‡ºäº†Diff-Unfoldingæ¡†æ¶ï¼Œç»“åˆç‰©ç†æµ‹é‡ç®—å­å’Œæ¨¡å—åŒ–ç½‘ç»œæ¶æ„ï¼Œå­¦ä¹ æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åéªŒåˆ†æ•°å‡½æ•°ã€‚</li>
<li>Diff-Unfoldingå°†åéªŒåˆ†æ•°å­¦ä¹ è¡¨è¿°ä¸ºå±•å¼€ä¼˜åŒ–æ–¹æ¡ˆçš„è®­ç»ƒï¼Œæµ‹é‡æ¨¡å‹ä¸å›¾åƒå…ˆéªŒè§£è€¦ï¼Œé€‚åº”å¤šç§é€†å‘é—®é¢˜ã€‚</li>
<li>ç†è®ºè¯æ˜äº†å±•å¼€æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDiff-Unfoldingåœ¨å›¾åƒæ¢å¤å’ŒåŠ é€ŸMRIæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>Diff-Unfoldingå…·æœ‰é«˜æ•ˆçš„æ¨ç†æ—¶é—´ï¼Œä½¿ç”¨ä¼˜åŒ–çš„C++&#x2F;LibTorchå®ç°ï¼Œå¤„ç†é€Ÿåº¦æ›´å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04b9c048e970fcdc1b0fc52ffeadf311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f52cd10c2dafc88b564eda5be73ab756.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f229547ab1e8b09f50ade0a81de0e993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1af47e187b5eba2348d82be4c653679.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d38ed3dab3d308f856365fb915c5e590.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Fourier-Space-Perspective-on-Diffusion-Models"><a href="#A-Fourier-Space-Perspective-on-Diffusion-Models" class="headerlink" title="A Fourier Space Perspective on Diffusion Models"></a>A Fourier Space Perspective on Diffusion Models</h2><p><strong>Authors:Fabian Falck, Teodora Pandeva, Kiarash Zahirnia, Rachel Lawrence, Richard Turner, Edward Meeds, Javier Zazo, Sushrut Karmalkar</strong></p>
<p>Diffusion models are state-of-the-art generative models on data modalities such as images, audio, proteins and materials. These modalities share the property of exponentially decaying variance and magnitude in the Fourier domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM) forward process of additive white noise, this property results in high-frequency components being corrupted faster and earlier in terms of their Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then generates low-frequency information before high-frequency details. In this work, we study the inductive bias of the forward process of diffusion models in Fourier space. We theoretically analyse and empirically demonstrate that the faster noising of high-frequency components in DDPM results in violations of the normality assumption in the reverse process. Our experiments show that this leads to degraded generation quality of high-frequency components. We then study an alternate forward process in Fourier space which corrupts all frequencies at the same rate, removing the typical frequency hierarchy during generation, and demonstrate marked performance improvements on datasets where high frequencies are primary, while performing on par with DDPM on standard imaging benchmarks. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æ˜¯å›¾åƒã€éŸ³é¢‘ã€è›‹ç™½è´¨å’Œææ–™ç­‰æ•°æ®æ¨¡æ€é¢†åŸŸçš„æœ€å‰æ²¿ç”Ÿæˆæ¨¡å‹ã€‚è¿™äº›æ¨¡æ€åœ¨å‚…é‡Œå¶åŸŸä¸­å…±äº«æŒ‡æ•°è¡°å‡çš„æ–¹å·®å’Œå¹…åº¦å±æ€§ã€‚åœ¨æ ‡å‡†çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰æ·»åŠ ç™½å™ªå£°çš„å‰å‘è¿‡ç¨‹ä¸­ï¼Œæ­¤å±æ€§å¯¼è‡´é«˜é¢‘æˆåˆ†åœ¨ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ–¹é¢æ¯”ä½é¢‘æˆåˆ†æ›´å¿«ã€æ›´æ—©åœ°è¢«ç ´åã€‚ç„¶ååå‘è¿‡ç¨‹ä¼šå…ˆç”Ÿæˆä½é¢‘ä¿¡æ¯ï¼Œå†ç”Ÿæˆé«˜é¢‘ç»†èŠ‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ‰©æ•£æ¨¡å‹åœ¨å‚…é‡Œå¶ç©ºé—´å‰å‘è¿‡ç¨‹çš„å½’çº³åç½®ã€‚æˆ‘ä»¬ç†è®ºä¸Šåˆ†æå’Œå®è¯è¡¨æ˜ï¼ŒDDPMä¸­é«˜é¢‘æˆåˆ†æ›´å¿«åœ°è¢«å™ªå£°å¹²æ‰°ä¼šå¯¼è‡´åå‘è¿‡ç¨‹ä¸­æ­£æ€å‡è®¾çš„è¿åã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™å¯¼è‡´äº†é«˜é¢‘æˆåˆ†ç”Ÿæˆè´¨é‡çš„ä¸‹é™ã€‚ç„¶åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å‚…é‡Œå¶ç©ºé—´ä¸­æ›¿ä»£çš„å‰å‘è¿‡ç¨‹ï¼Œä»¥ç›¸åŒçš„é€Ÿç‡ç ´åæ‰€æœ‰é¢‘ç‡ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ¶ˆé™¤äº†å…¸å‹çš„é¢‘ç‡å±‚æ¬¡ç»“æ„ï¼Œå¹¶åœ¨é«˜é¢‘ä¸ºä¸»è¦æ•°æ®é›†çš„æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒåŒæ—¶åœ¨æ ‡å‡†æˆåƒåŸºå‡†æµ‹è¯•ä¸­ä¸DDPMè¡¨ç°ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹æ˜¯æ•°æ®æ¨¡æ€ç”Ÿæˆé¢†åŸŸçš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘ã€è›‹ç™½è´¨å’Œææ–™ç­‰ã€‚åœ¨å‚…é‡Œå¶åŸŸä¸­ï¼Œè¿™äº›æ¨¡æ€å…·æœ‰æŒ‡æ•°è¡°å‡çš„æ–¹å·®å’Œå¹…åº¦ç‰¹æ€§ã€‚åœ¨æ ‡å‡†çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰æ­£å‘è¿‡ç¨‹ä¸­ï¼Œè¯¥ç‰¹æ€§å¯¼è‡´é«˜é¢‘æˆåˆ†åœ¨ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰æ–¹é¢è¾ƒæ—©åœ°è¢«å™ªå£°ç ´åã€‚åå‘è¿‡ç¨‹åˆ™ç”Ÿæˆä½é¢‘ä¿¡æ¯ï¼Œå†ç”Ÿæˆé«˜é¢‘ç»†èŠ‚ã€‚æœ¬æ–‡ç ”ç©¶æ‰©æ•£æ¨¡å‹åœ¨å‚…é‡Œå¶ç©ºé—´ä¸­çš„æ­£å‘è¿‡ç¨‹çš„å½’çº³åç½®ã€‚æˆ‘ä»¬ç†è®ºåˆ†æå’Œå®è¯è¯æ˜ï¼ŒDDPMä¸­é«˜é¢‘æˆåˆ†è¾ƒå¿«å™ªå£°åŒ–å¯¼è‡´åå‘è¿‡ç¨‹ä¸­æ­£æ€å‡è®¾çš„è¿åï¼Œè¿›è€Œé™ä½é«˜é¢‘æˆåˆ†çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§åœ¨å‚…é‡Œå¶ç©ºé—´ä¸­æ›¿ä»£çš„æ­£å‘è¿‡ç¨‹ï¼Œä»¥ç›¸åŒçš„é€Ÿç‡ç ´åæ‰€æœ‰é¢‘ç‡ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ¶ˆé™¤äº†å…¸å‹çš„é¢‘ç‡å±‚æ¬¡ç»“æ„ï¼Œå¹¶åœ¨é«˜é¢‘ä¸ºä¸»è¦ç‰¹å¾çš„æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒåŒæ—¶åœ¨æ ‡å‡†æˆåƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸DDPMç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æ˜¯æ•°æ®æ¨¡æ€ç”Ÿæˆé¢†åŸŸçš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œåº”ç”¨äºå›¾åƒã€éŸ³é¢‘ã€è›‹ç™½è´¨å’Œææ–™ç­‰ã€‚</li>
<li>åœ¨å‚…é‡Œå¶åŸŸä¸­ï¼Œæ•°æ®æ¨¡æ€å…·æœ‰æŒ‡æ•°è¡°å‡çš„æ–¹å·®å’Œå¹…åº¦ç‰¹æ€§ã€‚</li>
<li>DDPMä¸­çš„æ­£å‘è¿‡ç¨‹å¯¼è‡´é«˜é¢‘æˆåˆ†è¾ƒæ—©åœ°è¢«å™ªå£°ç ´åï¼Œå½±å“ç”Ÿæˆè´¨é‡ã€‚</li>
<li>åå‘è¿‡ç¨‹å…ˆç”Ÿæˆä½é¢‘ä¿¡æ¯ï¼Œå†ç”Ÿæˆé«˜é¢‘ç»†èŠ‚ã€‚</li>
<li>DDPMä¸­é«˜é¢‘å¿«é€Ÿå™ªå£°åŒ–ä¼šè¿ååå‘è¿‡ç¨‹ä¸­çš„æ­£æ€å‡è®¾ã€‚</li>
<li>ç ”ç©¶äº†æ›¿ä»£çš„æ­£å‘è¿‡ç¨‹ï¼Œä»¥ç›¸åŒçš„é€Ÿç‡ç ´åæ‰€æœ‰é¢‘ç‡ï¼Œæé«˜äº†é«˜é¢‘ä¸ºä¸»è¦ç‰¹å¾æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-26e6cddc16839ae5a7d6f51af95e4c91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8db5b6cc5b0f026af4e19944d8bd0d54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-667c2a11a2d7c3568255abbdbcbb1e74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9eff0f4dcad79895e33d0a60dde2a4a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-NPO-Negative-Preference-Optimization-for-Better-Preference-Aligned-Generation-of-Diffusion-Models"><a href="#Diffusion-NPO-Negative-Preference-Optimization-for-Better-Preference-Aligned-Generation-of-Diffusion-Models" class="headerlink" title="Diffusion-NPO: Negative Preference Optimization for Better Preference   Aligned Generation of Diffusion Models"></a>Diffusion-NPO: Negative Preference Optimization for Better Preference   Aligned Generation of Diffusion Models</h2><p><strong>Authors:Fu-Yun Wang, Yunhao Shui, Jingtan Piao, Keqiang Sun, Hongsheng Li</strong></p>
<p>Diffusion models have made substantial advances in image generation, yet models trained on large, unfiltered datasets often yield outputs misaligned with human preferences. Numerous methods have been proposed to fine-tune pre-trained diffusion models, achieving notable improvements in aligning generated outputs with human preferences. However, we argue that existing preference alignment methods neglect the critical role of handling unconditional&#x2F;negative-conditional outputs, leading to a diminished capacity to avoid generating undesirable outcomes. This oversight limits the efficacy of classifier-free guidance~(CFG), which relies on the contrast between conditional generation and unconditional&#x2F;negative-conditional generation to optimize output quality. In response, we propose a straightforward but versatile effective approach that involves training a model specifically attuned to negative preferences. This method does not require new training strategies or datasets but rather involves minor modifications to existing techniques. Our approach integrates seamlessly with models such as SD1.5, SDXL, video diffusion models and models that have undergone preference optimization, consistently enhancing their alignment with human preferences. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨å¤§å‹æœªè¿‡æ»¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹é€šå¸¸äº§ç”Ÿçš„è¾“å‡ºä¸äººç±»åå¥½ä¸ç¬¦ã€‚äººä»¬å·²ç»æå‡ºäº†è®¸å¤šæ–¹æ³•æ¥å¾®è°ƒé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨ä½¿ç”Ÿæˆè¾“å‡ºä¸äººç±»åå¥½å¯¹é½æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºç°æœ‰çš„åå¥½å¯¹é½æ–¹æ³•å¿½è§†äº†å¤„ç†æ— æ¡ä»¶&#x2F;è´Ÿæ¡ä»¶è¾“å‡ºçš„å…³é”®ä½œç”¨ï¼Œå¯¼è‡´é¿å…ç”Ÿæˆä¸è‰¯ç»“æœçš„èƒ½åŠ›é™ä½ã€‚è¿™ä¸€ç–å¿½é™åˆ¶äº†æ— åˆ†ç±»å¼•å¯¼ï¼ˆCFGï¼‰çš„æ•ˆç”¨ï¼Œæ— åˆ†ç±»å¼•å¯¼ä¾èµ–äºæœ‰æ¡ä»¶ç”Ÿæˆå’Œæ— æ¡ä»¶&#x2F;è´Ÿæ¡ä»¶ç”Ÿæˆä¹‹é—´çš„å¯¹æ¯”æ¥ä¼˜åŒ–è¾“å‡ºè´¨é‡ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ä½†é€šç”¨æœ‰æ•ˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸“é—¨è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œä½¿å…¶é€‚åº”è´Ÿé¢åå¥½ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦æ–°çš„è®­ç»ƒç­–ç•¥æˆ–æ•°æ®é›†ï¼Œè€Œæ˜¯å¯¹ç°æœ‰æŠ€æœ¯è¿›è¡Œäº†ä¸€äº›å°ä¿®æ”¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸SD1.5ã€SDXLã€è§†é¢‘æ‰©æ•£æ¨¡å‹ä»¥åŠç»è¿‡åå¥½ä¼˜åŒ–çš„æ¨¡å‹æ— ç¼é›†æˆï¼Œå§‹ç»ˆå¦‚ä¸€åœ°å¢å¼ºå…¶ä¸äººç±»åå¥½çš„å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11245v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®­ç»ƒäºå¤§è§„æ¨¡æœªç­›é€‰æ•°æ®é›†ä¸Šçš„æ¨¡å‹å¸¸äº§ç”Ÿä¸äººç±»åå¥½ä¸ç¬¦çš„è¾“å‡ºã€‚ç°æœ‰ç ”ç©¶å¤šå…³æ³¨å¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä»¥å¯¹é½ç”Ÿæˆè¾“å‡ºä¸äººç±»åå¥½ï¼Œä½†å¿½è§†äº†å¤„ç†æ— æ¡ä»¶&#x2F;è´Ÿæ¡ä»¶è¾“å‡ºçš„é‡è¦æ€§ï¼Œå¯¼è‡´éš¾ä»¥é¿å…ç”Ÿæˆä¸è‰¯ç»“æœã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒä¸“æ³¨äºè´Ÿåå¥½çš„æ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–¹æ³•æ— éœ€æ–°çš„è®­ç»ƒç­–ç•¥æˆ–æ•°æ®é›†ï¼Œè€Œæ˜¯å¯¹ç°æœ‰æŠ€æœ¯è¿›è¡Œå¾®å°è°ƒæ•´ã€‚æ­¤æ–¹æ³•å¯æ— ç¼é›†æˆåˆ°SD1.5ã€SDXLç­‰æ¨¡å‹ä¸­ï¼Œä¹Ÿå¯åº”ç”¨äºè§†é¢‘æ‰©æ•£æ¨¡å‹å’Œç»è¿‡åå¥½ä¼˜åŒ–çš„æ¨¡å‹ï¼Œæœ‰æ•ˆæé«˜å®ƒä»¬ä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„è¿›å±•æ˜¾è‘—ï¼Œä½†é¢å¯¹ç”Ÿæˆè¾“å‡ºä¸äººç±»åå¥½ä¸ç¬¦çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å…³æ³¨å¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä»¥å¯¹é½ç”Ÿæˆè¾“å‡ºä¸äººç±»åå¥½ï¼Œä½†å¤„ç†æ— æ¡ä»¶&#x2F;è´Ÿæ¡ä»¶è¾“å‡ºçš„é‡è¦æ€§è¢«å¿½è§†ã€‚</li>
<li>å¿½è§†å¤„ç†è´Ÿæ¡ä»¶è¾“å‡ºé™åˆ¶äº†æ— åˆ†ç±»å¼•å¯¼ï¼ˆCFGï¼‰çš„æ•ˆæœï¼Œå…¶ä¾èµ–äºæ¡ä»¶ç”Ÿæˆä¸æ— æ¡ä»¶&#x2F;è´Ÿæ¡ä»¶ç”Ÿæˆçš„å¯¹æ¯”ä»¥ä¼˜åŒ–è¾“å‡ºè´¨é‡ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒä¸“æ³¨äºè´Ÿåå¥½çš„æ¨¡å‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æ‰€ææ–¹æ³•æ— éœ€æ–°çš„è®­ç»ƒç­–ç•¥æˆ–æ•°æ®é›†ï¼Œè€Œæ˜¯å¯¹ç°æœ‰æŠ€æœ¯è¿›è¡Œå¾®å°è°ƒæ•´ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</li>
<li>æ‰€ææ–¹æ³•å¯æ— ç¼é›†æˆåˆ°å¤šç§æ¨¡å‹ä¸­ï¼Œå¦‚SD1.5ã€SDXLã€è§†é¢‘æ‰©æ•£æ¨¡å‹å’Œç»è¿‡åå¥½ä¼˜åŒ–çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e00040cc876dae5525661bc3a437c32f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-983c5ef0bbd020d3124cc943aa8f3d9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-551e9762cf830cf11eec8b38438e1b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-171e01dda70bfd2ea555d946c2e92fc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77ae6db309bbb62b54e268e3b36e8553.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DiCo-Revitalizing-ConvNets-for-Scalable-and-Efficient-Diffusion-Modeling"><a href="#DiCo-Revitalizing-ConvNets-for-Scalable-and-Efficient-Diffusion-Modeling" class="headerlink" title="DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion   Modeling"></a>DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion   Modeling</h2><p><strong>Authors:Yuang Ai, Qihang Fan, Xuefeng Hu, Zhenheng Yang, Ran He, Huaibo Huang</strong></p>
<p>Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns-highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet benchmarks, DiCo outperforms previous diffusion models in both image quality and generation speed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53 at 512x512, with a 2.7x and 3.1x speedup over DiT-XL&#x2F;2, respectively. Furthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID of 1.90 on ImageNet 256x256-without any additional supervision during training. Code: <a target="_blank" rel="noopener" href="https://github.com/shallowdream204/DiCo">https://github.com/shallowdream204/DiCo</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Transformerï¼Œç®€ç§°DiTï¼‰æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„è§†è§‰ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿä¼´éšç€è¾ƒå¤§çš„è®¡ç®—å¼€é”€ã€‚å¯¹é¢„è®­ç»ƒçš„DiTæ¨¡å‹åˆ†æè¡¨æ˜ï¼Œå…¨å±€è‡ªæ³¨æ„åŠ›å¾€å¾€æ˜¯å†—ä½™çš„ï¼Œä¸»è¦æ•æ‰çš„æ˜¯å±€éƒ¨æ¨¡å¼ï¼Œè¿™çªæ˜¾äº†å¯»æ‰¾æ›´é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†å·ç§¯ä½œä¸ºä¸€ç§æ„å»ºé«˜æ•ˆä¸”è¡¨è¾¾æ€§å¼ºçš„æ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ„å»ºå—ã€‚ç„¶è€Œï¼Œç®€å•åœ°ç”¨å·ç§¯æ›¿æ¢è‡ªæ³¨æ„åŠ›é€šå¸¸ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬å°†è¿™ç§æ€§èƒ½å·®è·å½’å› äºä¸Transformerç›¸æ¯”ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetsï¼‰ä¸­çš„é€šé“å†—ä½™åº¦è¾ƒé«˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç´§å‡‘çš„é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¿ƒè¿›äº†æ›´å¤šä¸åŒé€šé“çš„æ¿€æ´»ï¼Œä»è€Œå¢å¼ºäº†ç‰¹å¾å¤šæ ·æ€§ã€‚è¿™å¯¼è‡´äº†å®Œå…¨ç”±æ ‡å‡†ConvNetæ¨¡å—æ„å»ºçš„æ‰©æ•£æ¨¡å‹å®¶æ—â€”â€”æ‰©æ•£å·ç§¯ç½‘ç»œï¼ˆDiffusion ConvNetï¼Œç®€ç§°DiCoï¼‰ã€‚åœ¨é¢å‘ç±»åˆ«çš„ImageNetåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDiCoåœ¨å›¾åƒè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢å‡ä¼˜äºå…ˆå‰çš„æ‰©æ•£æ¨¡å‹ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒDiCo-XLåœ¨åˆ†è¾¨ç‡ä¸º256x256æ—¶å–å¾—äº†FIDåˆ†æ•°ä¸º2.05ï¼Œåˆ†è¾¨ç‡ä¸º512x512æ—¶å–å¾—äº†FIDåˆ†æ•°ä¸º2.53ï¼Œç›¸è¾ƒäºDiT-XL&#x2F;2åˆ†åˆ«å®ç°äº†2.7å€å’Œ3.1å€çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§„æ¨¡æœ€å¤§çš„æ¨¡å‹DiCo-Håœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰é¢å¤–çš„ç›‘ç£ä¸‹è¾¾åˆ°äº†ImageNet 256x256åˆ†è¾¨ç‡ä¸‹çš„FIDåˆ†æ•°ä¸º1.90ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/shallowdream204/DiCo">https://github.com/shallowdream204/DiCo</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11196v1">PDF</a> 27 pages, 29 figures, 9 tables</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹Diffusion Transformerï¼ˆDiTï¼‰åœ¨è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†è®¡ç®—å¼€é”€è¾ƒå¤§ã€‚ç ”ç©¶å‘ç°é¢„è®­ç»ƒçš„DiTæ¨¡å‹ä¸­å…¨å±€è‡ªæ³¨æ„åŠ›å¸¸å¸¸æ˜¯å†—ä½™çš„ï¼Œæ›´å¤šåœ°æ•æ‰å±€éƒ¨æ¨¡å¼ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å·ç§¯ä½œä¸ºæ„å»ºé«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå•çº¯æ›¿æ¢ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤å¼•å…¥äº†ç´§å‡‘é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†ç‰¹å¾å¤šæ ·æ€§ï¼Œä»è€Œæ„å»ºäº†å®Œå…¨åŸºäºæ ‡å‡†å·ç§¯ç½‘ç»œæ¨¡å—çš„Diffusion ConvNetï¼ˆDiCoï¼‰ã€‚åœ¨æ¡ä»¶ImageNetåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDiCoåœ¨å›¾åƒè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ä¸Šå‡ä¼˜äºå…ˆå‰çš„æ‰©æ•£æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯DiCo-XLåœ¨256x256å’Œ512x512åˆ†è¾¨ç‡ä¸‹FIDåˆ†åˆ«ä¸º2.05å’Œ2.53ï¼Œç›¸è¾ƒäºDiT-XL&#x2F;2åˆ†åˆ«å®ç°äº†2.7å€å’Œ3.1å€çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œæœ€å¤§çš„DiCo-Hæ¨¡å‹åœ¨ImageNet 256x256ä¸Šè¾¾åˆ°äº†FID 1.90ï¼Œä¸”è®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€é¢å¤–ç›‘ç£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformer (DiT) åœ¨è§†è§‰ç”Ÿæˆé¢†åŸŸå±•ç°å‡ºè‰²æ€§èƒ½ï¼Œä½†è®¡ç®—å¼€é”€å¤§ã€‚</li>
<li>é¢„è®­ç»ƒçš„DiTæ¨¡å‹ä¸­å…¨å±€è‡ªæ³¨æ„åŠ›å¾€å¾€æ˜¯å†—ä½™çš„ï¼Œæ›´å¤šåœ°æ•æ‰å±€éƒ¨æ¨¡å¼ã€‚</li>
<li>ç ”ç©¶æå‡ºä½¿ç”¨å·ç§¯ä½œä¸ºæ„å»ºé«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>å•çº¯æ›¿æ¢è‡ªæ³¨æ„åŠ›ä¸ºå·ç§¯ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè¿™è¢«å½’å› äºConvNetsä¸­çš„é€šé“å†—ä½™æ€§è¾ƒé«˜ã€‚</li>
<li>å¼•å…¥äº†ç´§å‡‘é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†ç‰¹å¾å¤šæ ·æ€§ã€‚</li>
<li>DiCoæ¨¡å‹åœ¨æ¡ä»¶ImageNetåŸºå‡†æµ‹è¯•ä¸­å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-970bf6a5aee69fee62ff9189adc82f73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-872ff6c53666ffe064405e16a0398bed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8d4caf527a90e151d8eaa300a1586fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c96dcc62a3d5d39ef83b4cd60d08853a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CompAlign-Improving-Compositional-Text-to-Image-Generation-with-a-Complex-Benchmark-and-Fine-Grained-Feedback"><a href="#CompAlign-Improving-Compositional-Text-to-Image-Generation-with-a-Complex-Benchmark-and-Fine-Grained-Feedback" class="headerlink" title="CompAlign: Improving Compositional Text-to-Image Generation with a   Complex Benchmark and Fine-Grained Feedback"></a>CompAlign: Improving Compositional Text-to-Image Generation with a   Complex Benchmark and Fine-Grained Feedback</h2><p><strong>Authors:Yixin Wan, Kai-Wei Chang</strong></p>
<p>State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuestâ€™s feedback as preference signals to improve diffusion modelsâ€™ compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹å·²å…·å¤‡æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å‡†ç¡®æç»˜åŒ…å«å¤šä¸ªç‰©ä½“ã€å±æ€§å’Œç©ºé—´å…³ç³»çš„ç»„åˆåœºæ™¯æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†CompAlignï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è¯„ä¼°3Dç©ºé—´å…³ç³»æç»˜èƒ½åŠ›ä¸ºé‡ç‚¹çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›ç»„åˆå›¾åƒç”Ÿæˆæ¨¡å‹ã€‚CompAlignåŒ…å«900ä¸ªå¤æ‚çš„è·¨ä¸»é¢˜å›¾åƒç”Ÿæˆæç¤ºï¼Œç»“åˆäº†æ•°å€¼å’Œ3Dç©ºé—´å…³ç³»ä»¥åŠä¸åŒçš„å±æ€§ç»‘å®šã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒåŒ…å«äº†å¸¦æœ‰3ä¸ªä»¥ä¸Šç”Ÿæˆä¸»é¢˜çš„ç”Ÿæˆä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å…·æœ‰å¤æ‚çš„3Dç©ºé—´å…³ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11178v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCompAlignçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°ä¸“æ³¨äºè¯„ä¼°æ¨¡å‹åœ¨ç”Ÿæˆå¤æ‚ç»„åˆå›¾åƒæ—¶çš„ä¸‰ç»´ç©ºé—´å…³ç³»æç»˜èƒ½åŠ›ã€‚è¯¥å¹³å°åŒ…å«900ä¸ªå¤æ‚çš„è·¨å¤šä¸»é¢˜å›¾åƒç”Ÿæˆæç¤ºï¼Œæ¶µç›–äº†æ•°å­—ä¸ä¸‰ç»´ç©ºé—´å…³ç³»çš„ç»„åˆä»¥åŠå¤šå˜å±æ€§ç»‘å®šã€‚CompAlignå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶åŒ…å«è¶…è¿‡ä¸‰ä¸ªç”Ÿæˆä¸»é¢˜ä¸”å¸¦æœ‰å¤æ‚ä¸‰ç»´ç©ºé—´å…³ç³»çš„ç”Ÿæˆä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªåä¸ºCompQuestçš„å¯è§£é‡Šå’Œå‡†ç¡®çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºåˆ†è§£å¤æ‚çš„æç¤ºä¸ºåŸå­å­é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å¤šæ ‡ç­¾å­¦ä¹ æ¨¡å‹ï¼ˆMLLMï¼‰å¯¹æ¨¡å‹ç”Ÿæˆå›¾åƒä¸­æ¯ä¸ªç”Ÿæˆå…ƒç´ çš„æ­£ç¡®æ€§æä¾›ç²¾ç»†çš„äºŒå…ƒåé¦ˆã€‚è¿™ä¸ºç”Ÿæˆçš„å›¾åƒä¸ç»„åˆæç¤ºä¹‹é—´çš„å¯¹é½ç¨‹åº¦æä¾›äº†ç²¾ç¡®é‡åŒ–ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä½¿ç”¨CompQueståé¦ˆä½œä¸ºåå¥½ä¿¡å·çš„å¯¹é½æ¡†æ¶ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹åœ¨ç»„åˆå›¾åƒç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡è°ƒæ•´æ¯å¼ å›¾åƒçš„åå¥½è®¾ç½®ï¼Œè¯¥æ–¹æ³•æ˜“äºæ‰©å±•ä¸”çµæ´»é€‚ç”¨äºä¸åŒçš„ä»»åŠ¡ã€‚å¯¹ä¹ä¸ªæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†å…·æœ‰æ›´å¤æ‚ä¸‰ç»´ç©ºé—´é…ç½®çš„ç»„æˆä»»åŠ¡æ—¶é¢ä¸´æ›´å¤§çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸”åœ¨å¼€æºå¯è®¿é—®æ¨¡å‹å’Œå°é—­æºä»£ç å•†ä¸šæ¨¡å‹ä¹‹é—´ä¹Ÿå­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚ä½¿ç”¨CompAlignè¿›è¡Œæ¨¡å‹å¯¹é½çš„è¿›ä¸€æ­¥å®è¯ç ”ç©¶äº§ç”Ÿäº†ä»¤äººé¼“èˆçš„ç»“æœï¼šå¯¹é½åçš„æ‰©æ•£æ¨¡å‹åœ¨ç»„åˆå‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CompAlignåŸºå‡†æµ‹è¯•å¹³å°ä¸“æ³¨äºè¯„ä¼°æ¨¡å‹åœ¨æè¿°ä¸‰ç»´ç©ºé—´å…³ç³»æ–¹é¢çš„è¡¨ç°ï¼Œå°¤å…¶åœ¨å¤æ‚çš„ç»„åˆå›¾åƒç”Ÿæˆæ–¹é¢ã€‚</li>
<li>CompAlignåŒ…å«å¤æ‚çš„è·¨å¤šä¸»é¢˜å›¾åƒç”Ÿæˆæç¤ºï¼Œæ¶µç›–æ•°å­—ä¸ä¸‰ç»´ç©ºé—´å…³ç³»çš„ç»„åˆä»¥åŠå¤šå˜å±æ€§ç»‘å®šï¼ŒæŒ‘æˆ˜æ€§å¼ºã€‚</li>
<li>CompQuestè¯„ä¼°æ¡†æ¶å¯ä»¥åˆ†è§£å¤æ‚çš„æç¤ºä¸ºåŸå­å­é—®é¢˜ï¼Œå¹¶æä¾›ç²¾ç»†çš„äºŒå…ƒåé¦ˆï¼Œä»¥é‡åŒ–æ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¸æç¤ºä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚</li>
<li>ä½¿ç”¨CompQueståé¦ˆçš„å¯¹é½æ¡†æ¶æœ‰åŠ©äºæé«˜æ‰©æ•£æ¨¡å‹åœ¨ç»„åˆå›¾åƒç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„ä¸‰ç»´ç©ºé—´é…ç½®æ—¶ã€‚</li>
<li>è¯„ä¼°å‘ç°ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„ç»„åˆä»»åŠ¡æ—¶é¢ä¸´å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠæ›´å¤šä¸‰ç»´ç©ºé—´é…ç½®çš„æƒ…å†µä¸‹ã€‚</li>
<li>åœ¨å¼€æºå¯è®¿é—®æ¨¡å‹å’Œå°é—­æºä»£ç å•†ä¸šæ¨¡å‹ä¹‹é—´ï¼Œå­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4dd40edf2cbaffb4a7709cc361e0b38c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c7baf423cbd584f86124d5252244cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37436045e6041dc7c5141960bd962469.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1385a7e7509115533a2a849a2acdc621.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="One-Image-is-Worth-a-Thousand-Words-A-Usability-Preservable-Text-Image-Collaborative-Erasing-Framework"><a href="#One-Image-is-Worth-a-Thousand-Words-A-Usability-Preservable-Text-Image-Collaborative-Erasing-Framework" class="headerlink" title="One Image is Worth a Thousand Words: A Usability Preservable Text-Image   Collaborative Erasing Framework"></a>One Image is Worth a Thousand Words: A Usability Preservable Text-Image   Collaborative Erasing Framework</h2><p><strong>Authors:Feiran Li, Qianqian Xu, Shilong Bao, Zhiyong Yang, Xiaochun Cao, Qingming Huang</strong></p>
<p>Concept erasing has recently emerged as an effective paradigm to prevent text-to-image diffusion models from generating visually undesirable or even harmful content. However, current removal methods heavily rely on manually crafted text prompts, making it challenging to achieve a high erasure (efficacy) while minimizing the impact on other benign concepts (usability). In this paper, we attribute the limitations to the inherent gap between the text and image modalities, which makes it hard to transfer the intricately entangled concept knowledge from text prompts to the image generation process. To address this, we propose a novel solution by directly integrating visual supervision into the erasure process, introducing the first text-image Collaborative Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the concept jointly by text prompts and the corresponding undesirable images induced by the prompts, and then reduces the generating probability of the target concept through negative guidance. This approach effectively bypasses the knowledge gap between text and image, significantly enhancing erasure efficacy. Additionally, we design a text-guided image concept refinement strategy that directs the model to focus on visual features most relevant to the specified text concept, minimizing disruption to other benign concepts. Finally, comprehensive experiments suggest that Co-Erasing outperforms state-of-the-art erasure approaches significantly with a better trade-off between efficacy and usability. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/Co-Erasing">https://github.com/Ferry-Li/Co-Erasing</a>. </p>
<blockquote>
<p>æ¦‚å¿µæ¶ˆé™¤ä½œä¸ºä¸€ç§é˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰ä¸è‰¯ç”šè‡³æœ‰å®³å†…å®¹çš„æœ‰æ•ˆèŒƒå¼ï¼Œæœ€è¿‘å´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ¶ˆé™¤æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨æ„å»ºçš„æ–‡æœ¬æç¤ºï¼Œè¿™ä½¿å¾—åœ¨æé«˜æ¶ˆé™¤æ•ˆæœçš„åŒæ—¶å°½é‡å‡å°‘å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µçš„å½±å“æˆä¸ºä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡è®¤ä¸ºè¿™äº›å±€é™æ€§æºäºæ–‡æœ¬å’Œå›¾åƒæ¨¡æ€ä¹‹é—´çš„å›ºæœ‰å·®è·ï¼Œè¿™ä½¿å¾—ä»æ–‡æœ¬æç¤ºå°†å¤æ‚çº ç¼ çš„æ¦‚å¿µçŸ¥è¯†è½¬ç§»åˆ°å›¾åƒç”Ÿæˆè¿‡ç¨‹å˜å¾—å›°éš¾ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ç›´æ¥åœ¨æ¶ˆé™¤è¿‡ç¨‹ä¸­å¼•å…¥è§†è§‰ç›‘ç£çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªæ–‡æœ¬å›¾åƒååŒæ¦‚å¿µæ¶ˆé™¤ï¼ˆCo-Erasingï¼‰æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒCo-Erasingé€šè¿‡æ–‡æœ¬æç¤ºå’Œç”±æç¤ºäº§ç”Ÿçš„ç›¸åº”çš„ä¸è‰¯å›¾åƒå…±åŒæè¿°æ¦‚å¿µï¼Œç„¶åé€šè¿‡è´Ÿé¢æŒ‡å¯¼é™ä½ç›®æ ‡æ¦‚å¿µçš„ç”Ÿæˆæ¦‚ç‡ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å…‹æœäº†æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„çŸ¥è¯†å·®è·ï¼Œå¤§å¤§æé«˜äº†æ¶ˆé™¤æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ¦‚å¿µç»†åŒ–ç­–ç•¥ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ä¸æŒ‡å®šæ–‡æœ¬æ¦‚å¿µæœ€ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œå°½é‡å‡å°‘å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µçš„å¹²æ‰°ã€‚æœ€åï¼Œç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCo-Erasingåœ¨æ•ˆæœå’Œå¯ç”¨æ€§ä¹‹é—´å–å¾—äº†æ›´å¥½çš„æƒè¡¡ï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ¶ˆé™¤æ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/Co-Erasing%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ferry-Li/Co-Erasingä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11131v1">PDF</a> This paper has been accepeted to ICML 2025. Not Final Version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–°å…´çš„æ¦‚å¿µæ¶ˆé™¤æŠ€æœ¯åŠå…¶åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚å½“å‰å»é™¤æ–¹æ³•ä¾èµ–æ‰‹åŠ¨æ„å»ºçš„æ–‡æœ¬æç¤ºï¼Œå­˜åœ¨æŒ‘æˆ˜åœ¨äºå®ç°é«˜æ•ˆçš„æ¶ˆé™¤æ•ˆæœåŒæ—¶æœ€å°åŒ–å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µçš„å½±å“ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„è§£å†³ç­–ç•¥ï¼Œå³é€šè¿‡ç›´æ¥é›†æˆè§†è§‰ç›‘ç£åˆ°æ¶ˆé™¤è¿‡ç¨‹ä¸­ï¼Œå¼•å…¥é¦–ä¸ªæ–‡æœ¬å›¾åƒååŒæ¦‚å¿µæ¶ˆé™¤ï¼ˆCo-Erasingï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ–‡æœ¬æç¤ºå’Œç›¸åº”çš„ä¸ç†æƒ³å›¾åƒå…±åŒæè¿°æ¦‚å¿µï¼Œç„¶åé€šè¿‡è´ŸæŒ‡å¯¼é™ä½ç›®æ ‡æ¦‚å¿µçš„ç”Ÿæˆæ¦‚ç‡ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°ç»•è¿‡äº†æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„çŸ¥è¯†å·®è·ï¼Œå¤§å¤§æé«˜äº†æ¶ˆé™¤æ•ˆæœã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ¦‚å¿µä¼˜åŒ–ç­–ç•¥ï¼Œä½¿æ¨¡å‹å…³æ³¨ä¸æŒ‡å®šæ–‡æœ¬æ¦‚å¿µæœ€ç›¸å…³çš„è§†è§‰ç‰¹å¾ï¼Œå°½é‡å‡å°‘å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µçš„å¹²æ‰°ã€‚å®éªŒè¡¨æ˜ï¼ŒCo-Erasingæ˜¾è‘—ä¼˜äºç°æœ‰æ¶ˆé™¤æ–¹æ³•ï¼Œåœ¨æ•ˆæœå’Œå¯ç”¨æ€§ä¹‹é—´è¾¾åˆ°æ›´å¥½çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µæ¶ˆé™¤æŠ€æœ¯èƒ½æœ‰æ•ˆé˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸è‰¯å†…å®¹ã€‚</li>
<li>å½“å‰å»é™¤æ–¹æ³•å­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥å®ç°é«˜æ•ˆæ¶ˆé™¤åŒæ—¶æœ€å°åŒ–å¯¹å…¶ä»–è‰¯æ€§æ¦‚å¿µçš„å½±å“ã€‚</li>
<li>å¼•å…¥Co-Erasingæ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥é›†æˆè§†è§‰ç›‘ç£åˆ°æ¶ˆé™¤è¿‡ç¨‹ä¸­è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>Co-Erasingé€šè¿‡æ–‡æœ¬æç¤ºå’Œä¸è‰¯å›¾åƒå…±åŒæè¿°æ¦‚å¿µï¼Œé™ä½ç›®æ ‡æ¦‚å¿µçš„ç”Ÿæˆæ¦‚ç‡ã€‚</li>
<li>Co-Erasingæœ‰æ•ˆç»•è¿‡æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„çŸ¥è¯†å·®è·ï¼Œæé«˜æ¶ˆé™¤æ•ˆæœã€‚</li>
<li>è®¾è®¡æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ¦‚å¿µä¼˜åŒ–ç­–ç•¥ï¼Œä½¿æ¨¡å‹å…³æ³¨ä¸æŒ‡å®šæ–‡æœ¬æ¦‚å¿µæœ€ç›¸å…³çš„è§†è§‰ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d1a977cee20edd6eec8387241bf1aac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-366e55be0464888c0dbacd9f7b3a8881.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-016182232ad7e9656dd22c9fbebcf50d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc4925130579ef796a3df0145eb0549f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8b1806c78253c43f3adc9d074decd7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Whatâ€™s-Inside-Your-Diffusion-Model-A-Score-Based-Riemannian-Metric-to-Explore-the-Data-Manifold"><a href="#Whatâ€™s-Inside-Your-Diffusion-Model-A-Score-Based-Riemannian-Metric-to-Explore-the-Data-Manifold" class="headerlink" title="Whatâ€™s Inside Your Diffusion Model? A Score-Based Riemannian Metric to   Explore the Data Manifold"></a>Whatâ€™s Inside Your Diffusion Model? A Score-Based Riemannian Metric to   Explore the Data Manifold</h2><p><strong>Authors:Simone Azeglio, Arianna Di Bernardo</strong></p>
<p>Recent advances in diffusion models have demonstrated their remarkable ability to capture complex image distributions, but the geometric properties of the learned data manifold remain poorly understood. We address this gap by introducing a score-based Riemannian metric that leverages the Stein score function from diffusion models to characterize the intrinsic geometry of the data manifold without requiring explicit parameterization. Our approach defines a metric tensor in the ambient space that stretches distances perpendicular to the manifold while preserving them along tangential directions, effectively creating a geometry where geodesics naturally follow the manifoldâ€™s contours. We develop efficient algorithms for computing these geodesics and demonstrate their utility for both interpolation between data points and extrapolation beyond the observed data distribution. Through experiments on synthetic data with known geometry, Rotated MNIST, and complex natural images via Stable Diffusion, we show that our score-based geodesics capture meaningful transformations that respect the underlying data distribution. Our method consistently outperforms baseline approaches on perceptual metrics (LPIPS) and distribution-level metrics (FID, KID), producing smoother, more realistic image transitions. These results reveal the implicit geometric structure learned by diffusion models and provide a principled way to navigate the manifold of natural images through the lens of Riemannian geometry. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•è¡¨æ˜å…¶åœ¨æ•æ‰å¤æ‚å›¾åƒåˆ†å¸ƒæ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œä½†æ˜¯å¯¹æ‰€å­¦ä¹ æ•°æ®æµå½¢çš„å‡ ä½•ç‰¹æ€§ä»ç„¶äº†è§£ä¸è¶³ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥åŸºäºåˆ†æ•°çš„é»æ›¼åº¦é‡æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œè¯¥åº¦é‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¯å¦å¾—åˆ†å‡½æ•°æ¥è¡¨å¾æ•°æ®æµå½¢çš„å†…åœ¨å‡ ä½•ï¼Œè€Œæ— éœ€æ˜¾å¼å‚æ•°åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¯å¢ƒç©ºé—´ä¸­å®šä¹‰ä¸€ä¸ªåº¦é‡å¼ é‡ï¼Œè¯¥å¼ é‡ä¼šæ‹‰ä¼¸æµå½¢å‚ç›´æ–¹å‘çš„é•¿åº¦åŒæ—¶ä¿ç•™å…¶æ²¿åˆ‡çº¿æ–¹å‘çš„è·ç¦»ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ›å»ºä¸€ä¸ªå‡ ä½•ä½“ï¼Œå…¶ä¸­çš„æµ‹åœ°çº¿è‡ªç„¶åœ°éµå¾ªæµå½¢çš„è½®å»“ã€‚æˆ‘ä»¬å¼€å‘äº†è®¡ç®—è¿™äº›æµ‹åœ°çº¿çš„æœ‰æ•ˆç®—æ³•ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬åœ¨æ•°æ®ç‚¹ä¹‹é—´è¿›è¡Œæ’å€¼å’Œè¶…å‡ºè§‚å¯Ÿåˆ°çš„æ•°æ®åˆ†å¸ƒè¿›è¡Œå¤–æ¨æ–¹é¢çš„å®ç”¨æ€§ã€‚é€šè¿‡å¯¹å…·æœ‰å·²çŸ¥å‡ ä½•çš„åˆæˆæ•°æ®ã€æ—‹è½¬çš„MNISTä»¥åŠé€šè¿‡ç¨³å®šæ‰©æ•£çš„å¤æ‚è‡ªç„¶å›¾åƒçš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„åŸºäºå¾—åˆ†çš„æµ‹åœ°çº¿èƒ½å¤Ÿæ•è·æœ‰æ„ä¹‰çš„è½¬æ¢ï¼Œè¿™äº›è½¬æ¢å°Šé‡åº•å±‚çš„æ•°æ®åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ„ŸçŸ¥åº¦é‡ï¼ˆLPIPSï¼‰å’Œåˆ†å¸ƒçº§åº¦é‡ï¼ˆFIDã€KIDï¼‰ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œäº§ç”Ÿæ›´å¹³æ»‘ã€æ›´ç°å®çš„å›¾åƒè¿‡æ¸¡ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†æ‰©æ•£æ¨¡å‹å­¦ä¹ çš„éšå¼å‡ ä½•ç»“æ„ï¼Œå¹¶é€šè¿‡é»æ›¼å‡ ä½•çš„é€é•œæä¾›äº†ä¸€ç§åœ¨å¤©ç„¶å›¾åƒæµå½¢ä¸Šå¯¼èˆªçš„åŸåˆ™æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11128v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•ï¼Œé€šè¿‡å¼•å…¥åŸºäºåˆ†æ•°çš„é»æ›¼åº¦é‡æ¥è§£å†³æ•°æ®æµå½¢å‡ ä½•å±æ€§ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚è¯¥åº¦é‡åˆ©ç”¨Steinå¾—åˆ†å‡½æ•°æ¥åˆ»ç”»æ•°æ®æµå½¢çš„å†…åœ¨å‡ ä½•ç»“æ„ï¼Œæ— éœ€æ˜¾å¼å‚æ•°åŒ–ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®ã€æ—‹è½¬MNISTå’Œé€šè¿‡Stable Diffusionçš„å¤æ‚è‡ªç„¶å›¾åƒä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºåˆ†æ•°çš„æµ‹åœ°çº¿èƒ½å¤Ÿæ•æ‰æœ‰æ„ä¹‰çš„è½¬æ¢ï¼Œå°Šé‡åº•å±‚æ•°æ®åˆ†å¸ƒã€‚æ­¤æ–¹æ³•åœ¨æ„ŸçŸ¥åº¦é‡ï¼ˆLPIPSï¼‰å’Œåˆ†å¸ƒçº§åº¦é‡ï¼ˆFIDã€KIDï¼‰ä¸Šä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œäº§ç”Ÿæ›´å¹³æ»‘ã€æ›´ç°å®çš„å›¾åƒè¿‡æ¸¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¤æ‚çš„å›¾åƒåˆ†å¸ƒï¼Œä½†å…¶æ•°æ®æµå½¢çš„å‡ ä½•å±æ€§ç†è§£ä¸è¶³ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºåˆ†æ•°çš„é»æ›¼åº¦é‡ï¼Œç”¨äºåˆ»ç”»æ•°æ®æµå½¢çš„å†…åœ¨å‡ ä½•ç»“æ„ï¼Œæ— éœ€æ˜¾å¼å‚æ•°åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å®šä¹‰ç¯å¢ƒç©ºé—´ä¸­çš„åº¦é‡å¼ é‡ï¼Œåœ¨å‚ç›´äºæµå½¢çš„æ–¹å‘ä¸Šæ‹‰ä¼¸è·ç¦»ï¼ŒåŒæ—¶æ²¿åˆ‡çº¿æ–¹å‘ä¿æŒè·ç¦»ï¼Œåˆ›å»ºäº†ä¸€ä¸ªè‡ªç„¶çš„å‡ ä½•ç»“æ„ã€‚</li>
<li>å¼€å‘äº†è®¡ç®—è¿™äº›æµ‹åœ°çº¿çš„æœ‰æ•ˆç®—æ³•ï¼Œå¯ç”¨äºæ•°æ®ç‚¹ä¹‹é—´çš„æ’å€¼å’Œè§‚å¯Ÿåˆ°çš„æ•°æ®åˆ†å¸ƒä¹‹å¤–çš„æ’å€¼ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒåŸºäºåˆ†æ•°çš„æµ‹åœ°çº¿èƒ½å¤Ÿæ•æ‰æœ‰æ„ä¹‰çš„è½¬æ¢ï¼Œå°Šé‡åº•å±‚æ•°æ®åˆ†å¸ƒï¼Œå¹¶äº§ç”Ÿæ›´å¹³æ»‘ã€æ›´ç°å®çš„å›¾åƒè¿‡æ¸¡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œåœ¨æ„ŸçŸ¥åº¦é‡ï¼ˆLPIPSï¼‰å’Œåˆ†å¸ƒçº§åº¦é‡ï¼ˆFIDã€KIDï¼‰ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dee3b80e828d83541fb64e244ebf5b7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d4a3e849c52807107d384a39a76c64d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a828947f6ff4f00310bf871042d21f97.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Shackled-Dancing-A-Bit-Locked-Diffusion-Algorithm-for-Lossless-and-Controllable-Image-Steganography"><a href="#Shackled-Dancing-A-Bit-Locked-Diffusion-Algorithm-for-Lossless-and-Controllable-Image-Steganography" class="headerlink" title="Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and   Controllable Image Steganography"></a>Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and   Controllable Image Steganography</h2><p><strong>Authors:Tianshuo Zhang, Gao Jia, Wenzhe Zhai, Rui Yann, Xianglei Xing</strong></p>
<p>Data steganography aims to conceal information within visual content, yet existing spatial- and frequency-domain approaches suffer from trade-offs between security, capacity, and perceptual quality. Recent advances in generative models, particularly diffusion models, offer new avenues for adaptive image synthesis, but integrating precise information embedding into the generative process remains challenging. We introduce Shackled Dancing Diffusion, or SD$^2$, a plug-and-play generative steganography method that combines bit-position locking with diffusion sampling injection to enable controllable information embedding within the generative trajectory. SD$^2$ leverages the expressive power of diffusion models to synthesize diverse carrier images while maintaining full message recovery with $100%$ accuracy. Our method achieves a favorable balance between randomness and constraint, enhancing robustness against steganalysis without compromising image fidelity. Extensive experiments show that SD$^2$ substantially outperforms prior methods in security, embedding capacity, and stability. This algorithm offers new insights into controllable generation and opens promising directions for secure visual communication. </p>
<blockquote>
<p>æ•°æ®éšå†™æœ¯æ—¨åœ¨å°†ä¿¡æ¯éšè—åœ¨è§†è§‰å†…å®¹ä¸­ï¼Œä½†ç°æœ‰çš„ç©ºé—´å’Œé¢‘ç‡åŸŸæ–¹æ³•åœ¨å®‰å…¨ã€å®¹é‡å’Œæ„ŸçŸ¥è´¨é‡ä¹‹é—´å­˜å­˜åœ¨æƒè¡¡é—®é¢˜ã€‚ç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œä¸ºè‡ªé€‚åº”å›¾åƒåˆæˆæä¾›äº†æ–°çš„é€”å¾„ï¼Œä½†å°†ç²¾ç¡®çš„ä¿¡æ¯åµŒå…¥ç”Ÿæˆè¿‡ç¨‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†Shackled Dancing Diffusionï¼Œç®€ç§°SD$^2$ï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„ç”Ÿæˆéšå†™æœ¯æ–¹æ³•ï¼Œå®ƒå°†ä½ä½ç½®é”å®šä¸æ‰©æ•£é‡‡æ ·æ³¨å…¥ç›¸ç»“åˆï¼Œå®ç°åœ¨ç”Ÿæˆè½¨è¿¹ä¸­å¯æ§çš„ä¿¡æ¯åµŒå…¥ã€‚SD$^2$åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›åˆæˆå¤šæ ·çš„è½½ä½“å›¾åƒï¼ŒåŒæ—¶ä»¥100%çš„å‡†ç¡®ç‡æ¢å¤å…¨éƒ¨ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨éšæœºæ€§å’Œçº¦æŸä¹‹é—´è¾¾åˆ°äº†æœ‰åˆ©çš„å¹³è¡¡ï¼Œåœ¨æé«˜æŠ—éšå†™åˆ†æçš„èƒ½åŠ›çš„åŒæ—¶ï¼Œä¸æŸå®³å›¾åƒä¿çœŸåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSD$^2$åœ¨å®‰å…¨ã€åµŒå…¥å®¹é‡å’Œç¨³å®šæ€§æ–¹é¢å¤§å¤§ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚è¯¥ç®—æ³•ä¸ºå¯æ§ç”Ÿæˆæä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶ä¸ºå®‰å…¨è§†è§‰é€šä¿¡æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10950v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®éšå†™æœ¯æ—¨åœ¨å°†ä¿¡æ¯éšè—åœ¨è§†è§‰å†…å®¹ä¸­ï¼Œä½†ç°æœ‰çš„ç©ºé—´åŸŸå’Œé¢‘åŸŸæ–¹æ³•åœ¨å®‰å…¨ã€å®¹é‡å’Œæ„ŸçŸ¥è´¨é‡ä¹‹é—´å­˜åœ¨æ­¤æ¶ˆå½¼é•¿çš„é—®é¢˜ã€‚æœ€è¿‘ç”Ÿæˆæ¨¡å‹ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹çš„å‘å±•ï¼Œä¸ºè‡ªé€‚åº”å›¾åƒåˆæˆæä¾›äº†æ–°çš„é€”å¾„ï¼Œä½†å°†ç²¾ç¡®ä¿¡æ¯åµŒå…¥ç”Ÿæˆè¿‡ç¨‹ä¸­ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†Shackled Dancing Diffusionï¼ˆSD$^2$ï¼‰ä¸€ç§å³æ’å³ç”¨çš„ç”Ÿæˆéšå†™æœ¯æ–¹æ³•ï¼Œç»“åˆä½ä½ç½®é”å®šå’Œæ‰©æ•£é‡‡æ ·æ³¨å…¥ï¼Œå®ç°åœ¨ç”Ÿæˆè½¨è¿¹ä¸­çš„å¯æ§ä¿¡æ¯åµŒå…¥ã€‚SD$^2$åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›åˆæˆå¤šæ ·çš„è½½ä½“å›¾åƒï¼ŒåŒæ—¶ä¿æŒ100%çš„ä¿¡æ¯æ¢å¤å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†éšæœºæ€§å’Œçº¦æŸæ€§çš„å¹³è¡¡ï¼Œæé«˜äº†å¯¹éšå†™åˆ†æçš„é²æ£’æ€§ï¼Œè€Œä¸æŸå®³å›¾åƒä¿çœŸåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSD$^2$åœ¨å®‰å…¨ã€åµŒå…¥å®¹é‡å’Œç¨³å®šæ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç®—æ³•ä¸ºå¯æ§ç”Ÿæˆæä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶ä¸ºå®‰å…¨è§†è§‰é€šä¿¡æ‰“å¼€äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®éšå†™æœ¯æ—¨åœ¨éšè—ä¿¡æ¯äºè§†è§‰å†…å®¹ä¸­ï¼Œé¢ä¸´å®‰å…¨ã€å®¹é‡å’Œæ„ŸçŸ¥è´¨é‡çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>ç°æœ‰ç©ºé—´åŸŸå’Œé¢‘åŸŸæ–¹æ³•å­˜åœ¨å±€é™ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æä¾›æ–°çš„å›¾åƒåˆæˆé€”å¾„ï¼Œä½†ä¿¡æ¯åµŒå…¥å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºShackled Dancing Diffusionï¼ˆSD$^2$ï¼‰æ–¹æ³•ï¼Œç»“åˆä½ä½ç½®é”å®šå’Œæ‰©æ•£é‡‡æ ·æ³¨å…¥ã€‚</li>
<li>SD$^2$èƒ½åˆæˆå¤šæ ·çš„è½½ä½“å›¾åƒï¼ŒåŒæ—¶ä¿æŒ100%ä¿¡æ¯æ¢å¤å‡†ç¡®æ€§ã€‚</li>
<li>SD$^2$å®ç°äº†éšæœºæ€§å’Œçº¦æŸæ€§çš„å¹³è¡¡ï¼Œæé«˜é²æ£’æ€§å¹¶ä¸æŸå®³å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a868b1e327e63d4dbdc50b0084ab1ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4662a01a2f2c7a8d2cbc997f3a927c5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e53fe55d9c823a17ec7e498757d2f7f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d90d186c2a510b8d24c6c661c1571951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e9118e1f98bd6af8c0e55280683c781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8fc61c3e19d02cf8644749abab7ecd0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SynRailObs-A-Synthetic-Dataset-for-Obstacle-Detection-in-Railway-Scenarios"><a href="#SynRailObs-A-Synthetic-Dataset-for-Obstacle-Detection-in-Railway-Scenarios" class="headerlink" title="SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway   Scenarios"></a>SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway   Scenarios</h2><p><strong>Authors:Qiushi Guo, Jason Rambach</strong></p>
<p>Detecting potential obstacles in railway environments is critical for preventing serious accidents. Identifying a broad range of obstacle categories under complex conditions requires large-scale datasets with precisely annotated, high-quality images. However, existing publicly available datasets fail to meet these requirements, thereby hindering progress in railway safety research. To address this gap, we introduce SynRailObs, a high-fidelity synthetic dataset designed to represent a diverse range of weather conditions and geographical features. Furthermore, diffusion models are employed to generate rare and difficult-to-capture obstacles that are typically challenging to obtain in real-world scenarios. To evaluate the effectiveness of SynRailObs, we perform experiments in real-world railway environments, testing on both ballasted and ballastless tracks across various weather conditions. The results demonstrate that SynRailObs holds substantial potential for advancing obstacle detection in railway safety applications. Models trained on this dataset show consistent performance across different distances and environmental conditions. Moreover, the model trained on SynRailObs exhibits zero-shot capabilities, which are essential for applications in security-sensitive domains. The data is available in <a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/qiushi910/synrailobs">https://www.kaggle.com/datasets/qiushi910/synrailobs</a>. </p>
<blockquote>
<p>åœ¨é“è·¯ç¯å¢ƒä¸­æ£€æµ‹æ½œåœ¨éšœç¢å¯¹äºé¢„é˜²ä¸¥é‡äº‹æ•…è‡³å…³é‡è¦ã€‚åœ¨å¤æ‚æ¡ä»¶ä¸‹è¯†åˆ«å¹¿æ³›éšœç¢ç±»åˆ«éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç²¾ç¡®æ ‡æ³¨çš„é«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰å…¬å¼€æ•°æ®é›†æ— æ³•æ»¡è¶³è¿™äº›è¦æ±‚ï¼Œä»è€Œé˜»ç¢äº†é“è·¯å®‰å…¨ç ”ç©¶çš„è¿›å±•ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SynRailObsè¿™ä¸€é«˜ä¿çœŸåˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨ä»£è¡¨å„ç§å¤©æ°”æ¡ä»¶å’Œåœ°ç†ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨äº†æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆç½•è§ä¸”éš¾ä»¥æ•è·çš„éšœç¢ï¼Œè¿™äº›éšœç¢åœ¨çœŸå®åœºæ™¯ä¸­é€šå¸¸éš¾ä»¥è·å¾—ã€‚ä¸ºäº†è¯„ä¼°SynRailObsçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨çœŸå®é“è·¯ç¯å¢ƒä¸­è¿›è¡Œå®éªŒï¼Œåœ¨å¤šç§å¤©æ°”æ¡ä»¶ä¸‹å¯¹çƒè½¨å’Œæ— çƒè½¨è½¨é“è¿›è¡Œæµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒSynRailObsåœ¨é“è·¯å®‰å…¨åº”ç”¨çš„éšœç¢ç‰©æ£€æµ‹æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚åœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨ä¸åŒè·ç¦»å’Œç¯å¢ƒæ¡ä»¶ä¸‹è¡¨ç°ä¸€è‡´ã€‚æ­¤å¤–ï¼Œåœ¨SynRailObsä¸Šè®­ç»ƒçš„æ¨¡å‹å…·æœ‰é›¶æ ·æœ¬èƒ½åŠ›ï¼Œè¿™å¯¹äºå®‰å…¨æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨è‡³å…³é‡è¦ã€‚æ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/qiushi910/synrailobs%E6%89%BE%E5%88%B0%E3%80%82">https://www.kaggle.com/datasets/qiushi910/synrailobsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10784v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨é“è·¯ç¯å¢ƒä¸­æ£€æµ‹æ½œåœ¨éšœç¢å¯¹äºé¢„é˜²ä¸¥é‡äº‹æ•…è‡³å…³é‡è¦ã€‚ä¸ºæ»¡è¶³å¤æ‚æ¡ä»¶ä¸‹å¹¿æ³›éšœç¢ç±»åˆ«çš„è¯†åˆ«éœ€æ±‚ï¼Œéœ€è¦å¤§è§„æ¨¡ã€ç²¾ç¡®æ ‡æ³¨ã€é«˜è´¨é‡å›¾åƒçš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œç°æœ‰å…¬å¼€æ•°æ®é›†æ— æ³•æ»¡è¶³è¿™äº›è¦æ±‚ï¼Œé˜»ç¢äº†é“è·¯å®‰å…¨ç ”ç©¶çš„è¿›å±•ã€‚ä¸ºè§£å†³è¿™ä¸€ç¼ºå£ï¼Œæˆ‘ä»¬æ¨å‡ºSynRailObsé«˜ä¿çœŸåˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨ä»£è¡¨å„ç§å¤©æ°”å’Œåœ°ç†ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç½•è§ä¸”éš¾ä»¥æ•è·çš„éšœç¢ï¼Œè¿™äº›éšœç¢åœ¨çœŸå®åœºæ™¯ä¸­é€šå¸¸éš¾ä»¥è·å–ã€‚é€šè¿‡çœŸå®é“è·¯ç¯å¢ƒå®éªŒéªŒè¯SynRailObsçš„æœ‰æ•ˆæ€§ï¼Œåœ¨çƒç²’å’Œæ— çƒç²’è½¨é“ä¸Šæµ‹è¯•å„ç§å¤©æ°”æ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒSynRailObsåœ¨é“è·¯å®‰å…¨åº”ç”¨ä¸­çš„éšœç¢æ£€æµ‹æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æ¨¡å‹åœ¨æ­¤æ•°æ®é›†ä¸Šçš„è¡¨ç°è·ç¦»å’Œç¯å¢ƒæ¡ä»¶å‡è¡¨ç°ä¸€è‡´ã€‚æ­¤å¤–ï¼Œåœ¨å®‰å…¨æ€§æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œç»è¿‡SynRailObsè®­ç»ƒçš„æ¨¡å‹å…·æœ‰é›¶å°„å‡»èƒ½åŠ›ã€‚æ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/qiushi910/synrailobs%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://www.kaggle.com/datasets/qiushi910/synrailobsä¸Šè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“è·¯éšœç¢æ£€æµ‹å¯¹äºé¢„é˜²äº‹æ•…è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ•°æ®é›†æ— æ³•æ»¡è¶³å¤æ‚æ¡ä»¶ä¸‹çš„å¤šæ ·éšœç¢è¯†åˆ«éœ€æ±‚ã€‚</li>
<li>SynRailObsæ˜¯ä¸€ä¸ªé«˜ä¿çœŸåˆæˆæ•°æ®é›†ï¼Œæ—¨åœ¨ä»£è¡¨å„ç§å¤©æ°”å’Œåœ°ç†ç‰¹å¾çš„éšœç¢æƒ…å†µã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨äºç”ŸæˆçœŸå®åœºæ™¯ä¸­éš¾ä»¥æ•è·çš„ç½•è§éšœç¢ã€‚</li>
<li>SynRailObsæ•°æ®é›†é€šè¿‡çœŸå®é“è·¯ç¯å¢ƒå®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¨¡å‹åœ¨SynRailObsæ•°æ®é›†ä¸Šçš„è¡¨ç°ç¨³å®šï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒè·ç¦»å’Œç¯å¢ƒæ¡ä»¶ã€‚</li>
<li>ç»è¿‡SynRailObsè®­ç»ƒçš„æ¨¡å‹å…·æœ‰é›¶å°„å‡»èƒ½åŠ›ï¼Œè¿™åœ¨å®‰å…¨æ€§æ•æ„Ÿé¢†åŸŸçš„åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b2ad85da39055b403f1d0fc798d98e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5128161cad17fc422f9eefb185d917ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-177211e63cf03a40f5c8f26d764ffa58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c6ccd72e887fe76df00d90b9d8456e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcfcd1f719f56b3d5eb764c454f142ef.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="IPGO-Indirect-Prompt-Gradient-Optimization-for-Parameter-Efficient-Prompt-level-Fine-Tuning-on-Text-to-Image-Models"><a href="#IPGO-Indirect-Prompt-Gradient-Optimization-for-Parameter-Efficient-Prompt-level-Fine-Tuning-on-Text-to-Image-Models" class="headerlink" title="IPGO: Indirect Prompt Gradient Optimization for Parameter-Efficient   Prompt-level Fine-Tuning on Text-to-Image Models"></a>IPGO: Indirect Prompt Gradient Optimization for Parameter-Efficient   Prompt-level Fine-Tuning on Text-to-Image Models</h2><p><strong>Authors:Jianping Ye, Michel Wedel, Kunpeng Zhang</strong></p>
<p>Text-to-Image Diffusion models excel at generating images from text prompts but often exhibit suboptimal alignment with content semantics, aesthetics, and human preferences. To address these limitations, this study proposes a novel parameter-efficient framework, Indirect Prompt Gradient Optimization (IPGO), for prompt-level diffusion model fine-tuning. IPGO enhances prompt embeddings by injecting continuously differentiable embeddings at the beginning and end of the prompt embeddings, leveraging low-rank structures with the flexibility and nonlinearity from rotations. This approach enables gradient-based optimization of injected embeddings under range, orthonormality, and conformity constraints, effectively narrowing the search space, promoting a stable solution, and ensuring alignment between the embeddings of the injected embeddings and the original prompt. Its extension IPGO+ adds a parameter-free cross-attention mechanism on the prompt embedding to enforce dependencies between the original prompt and the inserted embeddings. We conduct extensive evaluations through prompt-wise (IPGO) and prompt-batch (IPGO+) training using three reward models of image aesthetics, image-text alignment, and human preferences across three datasets of varying complexity. The results show that IPGO consistently outperforms SOTA benchmarks, including stable diffusion v1.5 with raw prompts, text-embedding-based methods (TextCraftor), training-based methods (DRaFT and DDPO), and training-free methods (DPO-Diffusion, Promptist, and ChatGPT-4o). Specifically, IPGO achieves a win-rate exceeding 99% in prompt-wise learning, and IPGO+ achieves a comparable, but often better performance against current SOTAs (a 75% win rate) in prompt-batch learning. Moreover, we illustrate IPGOâ€™s generalizability and its capability to significantly enhance image quality while requiring minimal data and resources. </p>
<blockquote>
<p>æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹æ“…é•¿æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒï¼Œä½†åœ¨å†…å®¹è¯­ä¹‰ã€ç¾å­¦å’Œäººç±»åå¥½æ–¹é¢çš„å¯¹é½å¸¸å¸¸ä¸å¤Ÿç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å‚æ•°é«˜æ•ˆæ¡†æ¶â€”â€”é—´æ¥æç¤ºæ¢¯åº¦ä¼˜åŒ–ï¼ˆIPGOï¼‰ï¼Œç”¨äºæç¤ºçº§æ‰©æ•£æ¨¡å‹çš„å¾®è°ƒã€‚IPGOé€šè¿‡æ³¨å…¥è¿ç»­å¯å¾®çš„åµŒå…¥æ¥å¢å¼ºæç¤ºåµŒå…¥ï¼Œè¿™äº›åµŒå…¥ä½äºæç¤ºåµŒå…¥çš„å¼€å§‹å’Œç»“æŸå¤„ï¼Œåˆ©ç”¨ä½é˜¶ç»“æ„ï¼ŒåŒæ—¶æ‹¥æœ‰æ—‹è½¬çš„çµæ´»æ€§å’Œéçº¿æ€§ã€‚è¿™ç§æ–¹æ³•å…è®¸åœ¨èŒƒå›´ã€æ­£äº¤æ€§å’Œä¸€è‡´æ€§çº¦æŸä¸‹å¯¹æ³¨å…¥çš„åµŒå…¥è¿›è¡ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ï¼Œæœ‰æ•ˆåœ°ç¼©å°äº†æœç´¢ç©ºé—´ï¼Œä¿ƒè¿›äº†ç¨³å®šè§£å†³æ–¹æ¡ˆçš„å‡ºç°ï¼Œå¹¶ç¡®ä¿äº†æ³¨å…¥åµŒå…¥çš„åµŒå…¥ä¸åŸå§‹æç¤ºä¹‹é—´çš„å¯¹é½ã€‚å…¶æ‰©å±•ç‰ˆIPGO+åœ¨æç¤ºåµŒå…¥ä¸Šå¢åŠ äº†ä¸€ç§æ— å‚æ•°äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œä»¥å¼ºåˆ¶æ‰§è¡ŒåŸå§‹æç¤ºå’Œæ’å…¥çš„åµŒå…¥ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬é€šè¿‡æç¤ºï¼ˆIPGOï¼‰å’Œæç¤ºæ‰¹æ¬¡ï¼ˆIPGO+ï¼‰è®­ç»ƒï¼Œä½¿ç”¨ä¸‰ç§å›¾åƒç¾å­¦ã€å›¾åƒæ–‡æœ¬å¯¹é½å’Œäººç±»åå¥½å¥–åŠ±æ¨¡å‹ï¼Œåœ¨ä¸‰ä¸ªä¸åŒå¤æ‚åº¦çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒIPGOä¸€è‡´åœ°è¶…è¶Šäº†æœ€æ–°åŸºå‡†ï¼ŒåŒ…æ‹¬ä½¿ç”¨åŸå§‹æç¤ºçš„ç¨³å®šæ‰©æ•£v1.5ã€åŸºäºæ–‡æœ¬åµŒå…¥çš„æ–¹æ³•ï¼ˆTextCraftorï¼‰ã€åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼ˆDRaFTå’ŒDDPOï¼‰å’Œå…è®­ç»ƒæ–¹æ³•ï¼ˆDPO-Diffusionã€Promptistå’ŒChatGPT-4oï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒIPGOåœ¨æç¤ºçº§å­¦ä¹ ä¸­å®ç°äº†è¶…è¿‡99%çš„èƒœç‡ï¼Œè€ŒIPGO+åœ¨æç¤ºæ‰¹æ¬¡å­¦ä¹ ä¸­è¾¾åˆ°äº†ä¸å½“å‰æœ€æ–°æŠ€æœ¯ç›¸å½“ï¼ˆ75%çš„èƒœç‡ï¼‰ä½†å¾€å¾€æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†IPGOçš„é€šç”¨æ€§ä»¥åŠå…¶åœ¨éœ€è¦å¤§é‡æ•°æ®å’Œèµ„æºçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜å›¾åƒè´¨é‡çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21812v2">PDF</a> 9 pages, 2 figures, 4 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒï¼Œä½†åœ¨å†…å®¹è¯­ä¹‰ã€ç¾å­¦å’Œäººç±»åå¥½æ–¹é¢çš„å¯¹é½æ€§å¸¸å¸¸ä¸å¤Ÿç†æƒ³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å‚æ•°é«˜æ•ˆæ¡†æ¶â€”â€”é—´æ¥æç¤ºæ¢¯åº¦ä¼˜åŒ–ï¼ˆIPGOï¼‰ï¼Œç”¨äºæ‰©æ•£æ¨¡å‹çš„æç¤ºçº§åˆ«å¾®è°ƒã€‚IPGOé€šè¿‡æ³¨å…¥è¿ç»­å¯å¾®åˆ†çš„åµŒå…¥ï¼Œå¢å¼ºæç¤ºåµŒå…¥ï¼Œåˆ©ç”¨æ—‹è½¬çš„çµæ´»æ€§å’Œéçº¿æ€§æ€§ï¼Œåœ¨æç¤ºåµŒå…¥çš„å¼€å¤´å’Œç»“å°¾å¤„è¿›è¡Œæ³¨å…¥ã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿå®ç°æ³¨å…¥åµŒå…¥çš„æ¢¯åº¦ä¼˜åŒ–ï¼Œåœ¨èŒƒå›´ã€æ­£äº¤æ€§å’Œä¸€è‡´æ€§çº¦æŸä¸‹æœ‰æ•ˆç¼©å°æœç´¢ç©ºé—´ï¼Œä¿ƒè¿›ç¨³å®šè§£å†³æ–¹æ¡ˆçš„å‡ºç°ï¼Œå¹¶ç¡®ä¿æ³¨å…¥åµŒå…¥ä¸åŸå§‹æç¤ºåµŒå…¥ä¹‹é—´çš„å¯¹é½ã€‚å…¶æ‰©å±•ç‰ˆIPGO+åœ¨æç¤ºåµŒå…¥ä¸Šå¢åŠ äº†ä¸€ç§æ— å‚æ•°äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¼ºåˆ¶æ‰§è¡ŒåŸå§‹æç¤ºå’Œæ’å…¥åµŒå…¥ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸‰ç§å¥–åŠ±æ¨¡å‹ï¼ˆå›¾åƒç¾å­¦ã€å›¾åƒæ–‡æœ¬å¯¹é½å’Œäººç±»åå¥½ï¼‰å’Œä¸‰ä¸ªä¸åŒå¤æ‚åº¦çš„æ•°æ®é›†è¿›è¡Œæç¤ºï¼ˆIPGOï¼‰å’Œæç¤ºæ‰¹æ¬¡ï¼ˆIPGO+ï¼‰çš„å¹¿æ³›è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒIPGOåœ¨å¤šä¸ªåœºæ™¯ä¸‹è¡¨ç°è¶…è¿‡å…¶ä»–å°–ç«¯æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸå§‹æç¤ºçš„ç¨³å®šæ‰©æ•£v1.5æ–¹æ³•ã€åŸºäºæ–‡æœ¬åµŒå…¥çš„æ–¹æ³•ï¼ˆTextCraftorï¼‰ã€åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼ˆDRaFTå’ŒDDPOï¼‰å’Œæ— è®­ç»ƒçš„æ–¹æ³•ï¼ˆDPO-Diffusionã€Promptistå’ŒChatGPT-4oï¼‰ã€‚å…·ä½“è€Œè¨€ï¼ŒIPGOåœ¨æç¤ºçº§å­¦ä¹ ä¸­å–å¾—è¶…è¿‡99%çš„èƒœç‡ï¼Œè€ŒIPGO+åœ¨æç¤ºæ‰¹æ¬¡å­¦ä¹ ä¸­å–å¾—äº†ä¸å½“å‰æœ€ä½³åšæ³•ç›¸å½“çš„ï¼ˆæœ‰æ—¶æ›´å¥½ï¼‰è¡¨ç°ï¼Œè¾¾åˆ°75%çš„èƒœç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†IPGOçš„æ³›åŒ–èƒ½åŠ›åŠå…¶å¢å¼ºå›¾åƒè´¨é‡çš„èƒ½åŠ›ï¼ŒåŒæ—¶éœ€è¦æœ€å°‘çš„æ•°æ®å’Œèµ„æºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¯­ä¹‰å¯¹é½ã€ç¾å­¦å’Œäººç±»åå¥½æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>IPGOæ¡†æ¶é€šè¿‡æ³¨å…¥è¿ç»­å¯å¾®åˆ†çš„åµŒå…¥å¢å¼ºæç¤ºåµŒå…¥ï¼Œæé«˜äº†æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>IPGOåˆ©ç”¨ä½é˜¶ç»“æ„å’Œæ—‹è½¬çš„çµæ´»æ€§å’Œéçº¿æ€§æ€§è¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿åµŒå…¥ä¹‹é—´çš„å¯¹é½å¹¶ä¿ƒè¿›ç¨³å®šè§£å†³æ–¹æ¡ˆçš„å‡ºç°ã€‚</li>
<li>IPGO+é€šè¿‡æ·»åŠ äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›ä¸€æ­¥å¢å¼ºäº†IPGOçš„æ€§èƒ½ï¼Œå¼ºåŒ–äº†åŸå§‹æç¤ºå’Œæ’å…¥åµŒå…¥ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚</li>
<li>ä½¿ç”¨ä¸‰ç§å¥–åŠ±æ¨¡å‹è¿›è¡Œå¹¿æ³›è¯„ä¼°è¡¨æ˜IPGOåœ¨å„ç§åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºè‰²ï¼Œä¸å…¶ä»–æ–¹æ³•çš„æ¯”è¾ƒæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>IPGOåœ¨æç¤ºçº§å­¦ä¹ ä¸­å–å¾—æ˜¾è‘—æˆæœï¼Œè€ŒIPGO+åœ¨æç¤ºæ‰¹æ¬¡å­¦ä¹ ä¸­ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d69303016cfffd4753f633eac26cbc47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef0ce9b5d6f26a4a1178b42077c3cafa.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d7635e32016c1ff643bb2aff7cc428ab.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-20  GOUHFI a novel contrast- and resolution-agnostic segmentation tool for   Ultra-High Field MRI
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0ed2fe0750fb36794531fa20f8251421.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-20  MutualNeRF Improve the Performance of NeRF under Limited Samples with   Mutual Information Theory
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
