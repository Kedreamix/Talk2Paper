<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-05-20  MTevent A Multi-Task Event Camera Dataset for 6D Pose Estimation and   Moving Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-de78ed98224b317cf960665d1c4aacbd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-20-更新"><a href="#2025-05-20-更新" class="headerlink" title="2025-05-20 更新"></a>2025-05-20 更新</h1><h2 id="MTevent-A-Multi-Task-Event-Camera-Dataset-for-6D-Pose-Estimation-and-Moving-Object-Detection"><a href="#MTevent-A-Multi-Task-Event-Camera-Dataset-for-6D-Pose-Estimation-and-Moving-Object-Detection" class="headerlink" title="MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and   Moving Object Detection"></a>MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and   Moving Object Detection</h2><p><strong>Authors:Shrutarv Awasthi, Anas Gouda, Sven Franke, Jérôme Rutinowski, Frank Hoffmann, Moritz Roidl</strong></p>
<p>Mobile robots are reaching unprecedented speeds, with platforms like Unitree B2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m&#x2F;s. However, effectively utilizing such speeds remains a challenge due to the limitations of RGB cameras, which suffer from motion blur and fail to provide real-time responsiveness. Event cameras, with their asynchronous operation, and low-latency sensing, offer a promising alternative for high-speed robotic perception. In this work, we introduce MTevent, a dataset designed for 6D pose estimation and moving object detection in highly dynamic environments with large detection distances. Our setup consists of a stereo-event camera and an RGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16 unique objects under challenging conditions such as extreme viewing angles, varying lighting, and occlusions. MTevent is the first dataset to combine high-speed motion, long-range perception, and real-world object interactions, making it a valuable resource for advancing event-based vision in robotics. To establish a baseline, we evaluate the task of 6D pose estimation using NVIDIA’s FoundationPose on RGB images, achieving an Average Recall of 0.22 with ground-truth masks, highlighting the limitations of RGB-based approaches in such dynamic settings. With MTevent, we provide a novel resource to improve perception models and foster further research in high-speed robotic vision. The dataset is available for download <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/anas-gouda/MTevent">https://huggingface.co/datasets/anas-gouda/MTevent</a> </p>
<blockquote>
<p>移动机器人的速度已达到前所未有的水平，例如Unitree B2和Fraunhofer O3dyn等平台的最快速度已达到5至10米&#x2F;秒。然而，由于RGB相机的局限性，例如运动模糊以及无法提供实时响应，因此有效利用这种速度仍然是一个挑战。事件相机具有异步操作和低延迟传感功能，为高速机器人感知提供了有前景的替代方案。在这项工作中，我们介绍了MTevent数据集，该数据集旨在用于高度动态环境中的6D姿态估计和运动目标检测，并可在大检测距离中使用。我们的设置包括一个立体事件相机和一个RGB相机，拍摄了75个场景，每个场景平均拍摄16秒，并在具有挑战性的条件下展示了16个唯一对象，例如极端视角、光线变化和遮挡。MTevent是第一个结合了高速运动、远程感知和现实世界对象交互的数据集，使其成为推进机器人事件中基于视觉研究的宝贵资源。为了建立基准线，我们使用NVIDIA的FoundationPose评估RGB图像上的6D姿态估计任务，使用真实遮罩获得平均召回率为0.22，这突显了RGB相机在这种动态环境中的局限性。通过MTevent数据集，我们提供了一个改进感知模型的宝贵资源，并促进高速机器人视觉的进一步研究。该数据集可通过以下链接下载：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/anas-gouda/MTevent">https://huggingface.co/datasets/anas-gouda/MTevent</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11282v1">PDF</a> accepted to CVPR 2025 Workshop on Event-based Vision</p>
<p><strong>Summary</strong>：随着移动机器人技术的飞速发展，高速机器人感知技术面临新的挑战。传统RGB相机在高动态环境下存在运动模糊和实时响应能力不足的问题。事件相机为高速机器人感知提供了有前景的替代方案。本文介绍了MTevent数据集，该数据集适用于高动态环境下的6D姿态估计和移动物体检测。该数据集包含立体事件相机和RGB相机捕获的75个场景，每个场景平均持续16秒，展示了具有挑战性的条件下的16个独特物体。MTevent数据集是首个结合高速运动、长距离感知和现实世界物体交互的数据集，对于推进基于事件的机器人视觉研究具有重要价值。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>移动机器人技术正在迅速发展，最大速度达到5至10米&#x2F;秒。</li>
<li>RGB相机在高动态环境下存在运动模糊和实时响应问题。</li>
<li>事件相机为高速机器人感知提供了有前景的替代方案。</li>
<li>介绍了MTevent数据集，适用于高动态环境下的6D姿态估计和移动物体检测。</li>
<li>MTevent包含立体事件相机和RGB相机捕获的75个场景，每个场景平均时长16秒，展示具有挑战性的条件下的多个物体。</li>
<li>MTevent数据集是首个结合高速运动、长距离感知和现实世界物体交互的数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d6716556d4badf30793ad3ad0b5793b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cde84cf8fec9bf4e4f5107473d8ac572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38562829ce146e244494acf454f9bf0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b8c88ece0a09a2ed84a147cd8740c3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83914e27600900ab4a64520afb06d7ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa050908ddd6867f1b574e2673813560.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Pseudo-Label-Quality-Decoupling-and-Correction-for-Semi-Supervised-Instance-Segmentation"><a href="#Pseudo-Label-Quality-Decoupling-and-Correction-for-Semi-Supervised-Instance-Segmentation" class="headerlink" title="Pseudo-Label Quality Decoupling and Correction for Semi-Supervised   Instance Segmentation"></a>Pseudo-Label Quality Decoupling and Correction for Semi-Supervised   Instance Segmentation</h2><p><strong>Authors:Jianghang Lin, Yilin Lu, Yunhang Shen, Chaoyang Zhu, Shengchuan Zhang, Liujuan Cao, Rongrong Ji</strong></p>
<p>Semi-Supervised Instance Segmentation (SSIS) involves classifying and grouping image pixels into distinct object instances using limited labeled data. This learning paradigm usually faces a significant challenge of unstable performance caused by noisy pseudo-labels of instance categories and pixel masks. We find that the prevalent practice of filtering instance pseudo-labels assessing both class and mask quality with a single score threshold, frequently leads to compromises in the trade-off between the qualities of class and mask labels. In this paper, we introduce a novel Pseudo-Label Quality Decoupling and Correction (PL-DC) framework for SSIS to tackle the above challenges. Firstly, at the instance level, a decoupled dual-threshold filtering mechanism is designed to decouple class and mask quality estimations for instance-level pseudo-labels, thereby independently controlling pixel classifying and grouping qualities. Secondly, at the category level, we introduce a dynamic instance category correction module to dynamically correct the pseudo-labels of instance categories, effectively alleviating category confusion. Lastly, we introduce a pixel-level mask uncertainty-aware mechanism at the pixel level to re-weight the mask loss for different pixels, thereby reducing the impact of noise introduced by pixel-level mask pseudo-labels. Extensive experiments on the COCO and Cityscapes datasets demonstrate that the proposed PL-DC achieves significant performance improvements, setting new state-of-the-art results for SSIS. Notably, our PL-DC shows substantial gains even with minimal labeled data, achieving an improvement of +11.6 mAP with just 1% COCO labeled data and +15.5 mAP with 5% Cityscapes labeled data. The code will be public. </p>
<blockquote>
<p>半监督实例分割（SSIS）是利用有限的标记数据将图像像素分类并分组为不同的对象实例。这种学习范式通常面临一个由实例类别的噪声伪标签和像素掩膜引起的性能不稳定性的重大挑战。我们发现，普遍的做法是使用单个得分阈值来评估类和掩膜质量，进而过滤实例伪标签，这通常会在类和掩膜标签质量之间陷入权衡取舍的困境。在本文中，我们为SSIS引入了一个新颖的伪标签质量解耦与校正（PL-DC）框架，以解决上述挑战。首先，在实例层面，设计了一个解耦的双阈值过滤机制，以解耦实例级伪标签的类和掩膜质量估计，从而独立控制像素分类和分组质量。其次，在类别层面，我们引入了一个动态实例类别校正模块，以动态校正实例类别的伪标签，有效地减轻了类别混淆。最后，我们在像素级别引入了一种掩膜不确定性感知机制，对不同像素的掩膜损失进行重新加权，从而减少了由像素级掩膜伪标签引入的噪声的影响。在COCO和Cityscapes数据集上的大量实验表明，所提出的PL-DC实现了显著的性能改进，为SSIS设定了新的最新结果。值得注意的是，我们的PL-DC即使在标记数据很少的情况下也表现出极大的优势，在仅使用COCO 1%标记数据的情况下提高了+11.6 mAP，在Cityscapes 5%标记数据的情况下提高了+15.5 mAP。代码将公开。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11075v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种针对半监督实例分割（SSIS）的新框架——伪标签质量解耦与校正（PL-DC）。该框架解决了由于实例伪标签的类别和像素掩膜噪声导致的不稳定性能问题。它采用了三层级的策略，包括解耦的实例层级双阈值过滤机制、动态的实例类别校正模块以及像素级别的掩膜不确定性感知机制。这些策略提高了伪标签的质量，从而提高了模型的性能。在COCO和Cityscapes数据集上的实验表明，PL-DC取得了显著的性能提升，特别是在少量标注数据的情况下。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>半监督实例分割（SSIS）面临由于实例伪标签的类别和像素掩膜噪声导致的性能不稳定问题。</li>
<li>伪标签质量解耦与校正（PL-DC）框架通过解耦的实例层级双阈值过滤机制提高伪标签质量。</li>
<li>动态实例类别校正模块用于动态校正实例类别的伪标签，减少类别混淆。</li>
<li>像素级别的掩膜不确定性感知机制用于降低像素级掩膜伪标签噪声的影响。</li>
<li>PL-DC在COCO和Cityscapes数据集上取得了显著的性能提升，特别是在少量标注数据的情况下。</li>
<li>PL-DC框架有望为半监督学习领域提供一种有效的解决方案，尤其是在图像分割任务中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11075">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-500c1342b0f8e16ee1bfc0fa4e52fc9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c8c2997002c9d9b9f7a701c18ad8b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d219bcae9707c38dc61285020207ad57.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="M4-SAR-A-Multi-Resolution-Multi-Polarization-Multi-Scene-Multi-Source-Dataset-and-Benchmark-for-Optical-SAR-Fusion-Object-Detection"><a href="#M4-SAR-A-Multi-Resolution-Multi-Polarization-Multi-Scene-Multi-Source-Dataset-and-Benchmark-for-Optical-SAR-Fusion-Object-Detection" class="headerlink" title="M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene,   Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection"></a>M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene,   Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection</h2><p><strong>Authors:Chao Wang, Wei Lu, Xiang Li, Jian Yang, Lei Luo</strong></p>
<p>Single-source remote sensing object detection using optical or SAR images struggles in complex environments. Optical images offer rich textural details but are often affected by low-light, cloud-obscured, or low-resolution conditions, reducing the detection performance. SAR images are robust to weather, but suffer from speckle noise and limited semantic expressiveness. Optical and SAR images provide complementary advantages, and fusing them can significantly improve the detection accuracy. However, progress in this field is hindered by the lack of large-scale, standardized datasets. To address these challenges, we propose the first comprehensive dataset for optical-SAR fusion object detection, named Multi-resolution, Multi-polarization, Multi-scene, Multi-source SAR dataset (M4-SAR). It contains 112,184 precisely aligned image pairs and nearly one million labeled instances with arbitrary orientations, spanning six key categories. To enable standardized evaluation, we develop a unified benchmarking toolkit that integrates six state-of-the-art multi-source fusion methods. Furthermore, we propose E2E-OSDet, a novel end-to-end multi-source fusion detection framework that mitigates cross-domain discrepancies and establishes a robust baseline for future studies. Extensive experiments on M4-SAR demonstrate that fusing optical and SAR data can improve $mAP$ by 5.7% over single-source inputs, with particularly significant gains in complex environments. The dataset and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/wchao0601/M4-SAR">https://github.com/wchao0601/M4-SAR</a>. </p>
<blockquote>
<p>在复杂环境中，使用光学或SAR图像进行单源遥感对象检测面临挑战。光学图像虽然具有丰富的纹理细节，但常常受到低光、云层遮蔽或低分辨率条件的影响，降低了检测性能。SAR图像对天气具有鲁棒性，但受到斑点噪声和有限的语义表达能力的限制。光学和SAR图像具有互补优势，融合它们可以显著提高检测精度。然而，该领域的进展受到缺乏大规模、标准化数据集的阻碍。为了应对这些挑战，我们提出了首个用于光学-SAR融合对象检测的综合数据集，命名为多分辨率、多极化、多场景、多源SAR数据集（M4-SAR）。它包含112,184个精确对齐的图像对和近百万个具有任意方向的标记实例，涵盖六个关键类别。为了进行标准化评估，我们开发了一个统一的基准测试工具包，集成了六种最先进的多源融合方法。此外，我们提出了E2E-OSDet，这是一种新型端到端多源融合检测框架，它减轻了跨域差异，并为未来的研究建立了稳健的基准。在M4-SAR上的广泛实验表明，与单源输入相比，融合光学和SAR数据可以提高mAP的5.7%，在复杂环境中尤其显著。数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/wchao0601/M4-SAR%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/wchao0601/M4-SAR公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10931v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>该文针对单一遥感图像在复杂环境中的目标检测困难问题，提出结合光学图像与SAR图像进行多源融合检测的方法。为解决此领域缺乏大规模标准化数据集的问题，构建了首个综合数据集M4-SAR，包含精确对齐的图像对和大量标记实例。同时，开发了一个统一的基准测试工具包，并提出了E2E-OSDet这一端到端的多源融合检测框架，以缓解跨域差异并建立稳健的基线。实验证明，融合光学和SAR数据可以提高目标检测的准确度。数据集和代码已公开于GitHub上。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>单一遥感图像在复杂环境中目标检测存在困难。</li>
<li>光学图像和SAR图像具有互补优势，融合可提高检测准确性。</li>
<li>缺乏大规模标准化数据集是此领域发展的主要挑战之一。</li>
<li>提出首个综合数据集M4-SAR，包含精确对齐的图像对和大量标记实例。</li>
<li>开发统一的基准测试工具包以进行标准化评估。</li>
<li>提出E2E-OSDet这一端到端的多源融合检测框架，缓解跨域差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10931">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-683e67cc1d5b27f42989b7f15bd3daeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9e35e9446494206927f99587e551eb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67488de79d3a24d5f869dadcad338e6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8119ddbd63a75ad98e36fc7111e43c0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee4f098c6367148d5531ff71682987d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50cda3d35e0a1d245955700b97aa4d97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58e7effcf3e9e3edbcc914f9bd9b5ccd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-High-Performance-Thermal-Infrared-Object-Detection-Framework-with-Centralized-Regulation"><a href="#A-High-Performance-Thermal-Infrared-Object-Detection-Framework-with-Centralized-Regulation" class="headerlink" title="A High-Performance Thermal Infrared Object Detection Framework with   Centralized Regulation"></a>A High-Performance Thermal Infrared Object Detection Framework with   Centralized Regulation</h2><p><strong>Authors:Jinke Li, Yue Wu, Xiaoyan Yang</strong></p>
<p>Thermal Infrared (TIR) technology involves the use of sensors to detect and measure infrared radiation emitted by objects, and it is widely utilized across a broad spectrum of applications. The advancements in object detection methods utilizing TIR images have sparked significant research interest. However, most traditional methods lack the capability to effectively extract and fuse local-global information, which is crucial for TIR-domain feature attention. In this study, we present a novel and efficient thermal infrared object detection framework, known as CRT-YOLO, that is based on centralized feature regulation, enabling the establishment of global-range interaction on TIR information. Our proposed model integrates efficient multi-scale attention (EMA) modules, which adeptly capture long-range dependencies while incurring minimal computational overhead. Additionally, it leverages the Centralized Feature Pyramid (CFP) network, which offers global regulation of TIR features. Extensive experiments conducted on two benchmark datasets demonstrate that our CRT-YOLO model significantly outperforms conventional methods for TIR image object detection. Furthermore, the ablation study provides compelling evidence of the effectiveness of our proposed modules, reinforcing the potential impact of our approach on advancing the field of thermal infrared object detection. </p>
<blockquote>
<p>热红外（TIR）技术涉及使用传感器检测和测量物体发射的红外辐射，广泛应用于广泛的领域。利用TIR图像进行目标检测方法的进步引起了大量的研究兴趣。然而，大多数传统方法无法有效地提取和融合局部全局信息，这对于红外领域的特征注意力至关重要。本研究提出了一种新型高效的热红外目标检测框架，称为CRT-YOLO，基于集中特征调控，实现了红外信息的全局范围交互。我们提出的模型集成了高效的跨尺度注意力（EMA）模块，该模块能够巧妙地捕捉长距离依赖关系，同时产生极低的计算开销。此外，它利用中央特征金字塔（CFP）网络，对红外特征进行全局调控。在两大基准数据集上进行的广泛实验表明，我们的CRT-YOLO模型在红外图像目标检测方面显著优于传统方法。此外，消融研究为我们提出的模块的有效性提供了有力证据，证明了我们的方法对于推动热红外目标检测领域发展的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10825v1">PDF</a> This manuscript has been accepted for publication in the   International Journal for Housing Science and Its Applications (IJHSA), 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于集中式特征调控的高效热红外目标检测框架——CRT-YOLO。该模型能够建立全局范围内的热红外信息交互，并集成高效的多尺度注意力模块，捕捉长距离依赖关系。此外，它采用集中式特征金字塔网络，实现对热红外特征的全局调控。在基准数据集上的实验表明，CRT-YOLO在热红外图像目标检测方面显著优于传统方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIR技术通过传感器检测并测量物体发射的红外辐射，广泛应用于多个领域。</li>
<li>大多数传统方法无法有效地提取和融合局部-全局信息，这对于热红外特征注意力至关重要。</li>
<li>CRT-YOLO是一种新型高效的热红外目标检测框架，基于集中式特征调控，可实现全局范围内的热红外信息交互。</li>
<li>CRT-YOLO集成了高效的多尺度注意力模块，能够捕捉长距离依赖关系，同时减少计算开销。</li>
<li>该模型采用集中式特征金字塔网络，实现热红外特征的全局调控。</li>
<li>在基准数据集上的实验表明，CRT-YOLO在热红外图像目标检测方面显著优于传统方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10825">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c7f125329a2800f97ea7004d5a1b8deb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98fcca03580005c840e7f9c1233be0fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49a91ef92402dad7b39222d929ccdc05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6562eb3d4b9650be8e12d848265802f8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Completely-Weakly-Supervised-Class-Incremental-Learning-for-Semantic-Segmentation"><a href="#Completely-Weakly-Supervised-Class-Incremental-Learning-for-Semantic-Segmentation" class="headerlink" title="Completely Weakly Supervised Class-Incremental Learning for Semantic   Segmentation"></a>Completely Weakly Supervised Class-Incremental Learning for Semantic   Segmentation</h2><p><strong>Authors:David Minkwan Kim, Soeun Lee, Byeongkeun Kang</strong></p>
<p>This work addresses the task of completely weakly supervised class-incremental learning for semantic segmentation to learn segmentation for both base and additional novel classes using only image-level labels. While class-incremental semantic segmentation (CISS) is crucial for handling diverse and newly emerging objects in the real world, traditional CISS methods require expensive pixel-level annotations for training. To overcome this limitation, partially weakly-supervised approaches have recently been proposed. However, to the best of our knowledge, this is the first work to introduce a completely weakly-supervised method for CISS. To achieve this, we propose to generate robust pseudo-labels by combining pseudo-labels from a localizer and a sequence of foundation models based on their uncertainty. Moreover, to mitigate catastrophic forgetting, we introduce an exemplar-guided data augmentation method that generates diverse images containing both previous and novel classes with guidance. Finally, we conduct experiments in three common experimental settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint and overlap. The experimental results demonstrate that our completely weakly supervised method outperforms even partially weakly supervised methods in the 15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the COCO-to-VOC setting. </p>
<blockquote>
<p>这篇论文针对完全弱监督的类增量学习语义分割任务展开研究，旨在仅使用图像级别的标签来学习基础类和新增类的分割。虽然类增量语义分割（CISS）对于处理现实世界中多样且新出现的物体至关重要，但传统的CISS方法需要昂贵的像素级标注来进行训练。为了克服这一局限性，最近提出了部分弱监督的方法。然而，据我们所知，这是第一项引入完全弱监督方法用于CISS的工作。为了实现这一点，我们提出通过结合定位器和基于不确定性的基础模型序列的伪标签来生成稳健的伪标签。此外，为了缓解灾难性遗忘，我们引入了一种示例引导的数据增强方法，该方法生成包含先前和新型类别的各种图像以进行引导。最后，我们在三种常见的实验设置（即15-5 VOC、10-10 VOC和COCO-to-VOC）和两个场景（即不相关和重叠）下进行实验。实验结果表明，我们的完全弱监督方法在15-5 VOC和10-10 VOC设置中甚至超越了部分弱监督方法的表现，同时在COCO-to-VOC设置中也达到了具有竞争力的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10781v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种全新的完全弱监督类增量学习方法，用于语义分割。该方法利用仅图像级别的标签，实现对基础类和新增类的学习。通过结合定位器和基础模型的伪标签及其不确定性，生成稳健的伪标签。同时，为缓解灾难性遗忘问题，引入实例引导的数据增强方法。实验结果表明，该方法在弱监督环境下表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究解决了类增量语义分割（CISS）的完全弱监督学习问题，仅使用图像级别的标签就能学习基础类和新增类的分割。</li>
<li>提出了一种结合定位器和基础模型的伪标签生成方法，考虑其不确定性以生成稳健的伪标签。</li>
<li>为缓解灾难性遗忘问题，引入了实例引导的数据增强方法，可以生成包含新旧类别的多样化图像。</li>
<li>在不同的实验设置和场景下进行了实验验证，包括15-5 VOC、10-10 VOC和COCO-to-VOC的设置，以及分立和重叠场景。</li>
<li>在15-5 VOC和10-10 VOC的设置中，完全弱监督方法甚至超越了部分弱监督方法。</li>
<li>在COCO-to-VOC设置中，该方法具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10781">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-85e0ce43e02df18bf2694f6dc0798c5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea9fe09d3d639325abccf2b59d063e82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70e927f488c7d5ea00b924951fe38358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ca44060915577705697ea950280b0d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc6488db16e5ea3ff5dcc040a75e0ccb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-399408b57fb5ad4bed37bcb7cf817aba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-992c3ff481dfabde78929a40b766cf3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-808837cb0eeb4393cc90355d75c11076.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de78ed98224b317cf960665d1c4aacbd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FreeA-Human-object-Interaction-Detection-using-Free-Annotation-Labels"><a href="#FreeA-Human-object-Interaction-Detection-using-Free-Annotation-Labels" class="headerlink" title="FreeA: Human-object Interaction Detection using Free Annotation Labels"></a>FreeA: Human-object Interaction Detection using Free Annotation Labels</h2><p><strong>Authors:Qi Liu, Yuxiao Wang, Xinyu Jiang, Wolin Liang, Zhenao Wei, Yu Lei, Nan Zhuang, Weiying Xue</strong></p>
<p>Recent human-object interaction (HOI) detection methods depend on extensively annotated image datasets, which require a significant amount of manpower. In this paper, we propose a novel self-adaptive, language-driven HOI detection method, termed FreeA. This method leverages the adaptability of the text-image model to generate latent HOI labels without requiring manual annotation. Specifically, FreeA aligns image features of human-object pairs with HOI text templates and employs a knowledge-based masking technique to decrease improbable interactions. Furthermore, FreeA implements a proposed method for matching interaction correlations to increase the probability of actions associated with a particular action, thereby improving the generated HOI labels. Experiments on two benchmark datasets showcase that FreeA achieves state-of-the-art performance among weakly supervised HOI competitors. Our proposal gets +\textbf{13.29} (\textbf{159%$\uparrow$}) mAP and +\textbf{17.30} (\textbf{98%$\uparrow$}) mAP than the newest <code>Weakly&#39;&#39; supervised model, and +\textbf&#123;7.19&#125; (\textbf&#123;28\%$\uparrow$&#125;) mAP and +\textbf&#123;14.69&#125; (\textbf&#123;34\%$\uparrow$&#125;) mAP than the latest </code>Weakly+’’ supervised model, respectively, on HICO-DET and V-COCO datasets, more accurate in localizing and classifying the interactive actions. The source code will be made public. </p>
<blockquote>
<p>近期的人机交互（HOI）检测方法严重依赖于大量标注的图像数据集，这需要大量的人工操作。在本文中，我们提出了一种新型的自适应语言驱动HOI检测方法，称为FreeA。该方法利用文本图像模型的适应性，无需手动注释即可生成潜在的HOI标签。具体来说，FreeA将人机对图像特征与HOI文本模板对齐，并采用基于知识的遮罩技术来减少不可能的交互。此外，FreeA实现了一种匹配交互关联性的方法，以提高与特定动作相关的动作概率，从而改进生成的HOI标签。在两个基准数据集上的实验表明，FreeA在弱监督HOI竞争对手中实现了最先进的性能。我们的提案在HICO-DET和V-COCO数据集上比最新的“弱”监督模型提高了+13.29（提高159%）和+17.30（提高98%）的mAP，比最新的“弱+”监督模型分别提高了+7.19（提高28%）和+14.69（提高34%）的mAP。在定位和理解交互动作方面更加准确。源代码将公开。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.01840v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的自适应语言驱动的人机交互（HOI）检测模型FreeA，无需手动标注即可生成潜在的HOI标签。该模型通过文本图像模型的适应性，将人机对图像特征对齐，并采用基于知识的遮蔽技术减少不可能的交互。此外，FreeA还提出了一种匹配交互关联的方法，以提高特定动作的关联概率，从而改进生成的HOI标签。实验表明，FreeA在两大基准数据集上均取得了最先进的性能表现。相较于最新的弱监督HOI模型，其在HICO-DET和V-COCO数据集上的平均精度（mAP）分别提升了13.29%（相当于提升159%）和17.30%（相当于提升98%），更能准确地进行交互动作的定位和分类。源代码将公开。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FreeA模型是一种新型的自适应语言驱动的人机交互（HOI）检测模型。</li>
<li>该模型通过文本图像模型的适应性生成潜在的HOI标签，无需手动标注。</li>
<li>FreeA利用基于知识的遮蔽技术减少不可能的交互，并采用了匹配交互关联的方法提高特定动作的关联概率。</li>
<li>实验结果显示，FreeA在两大基准数据集上取得了最先进的性能表现，相较于其他模型有明显的精度提升。</li>
<li>该模型的源代码将公开。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.01840">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-44711841dc8686c238d0fb6e9fc5e304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-653f4489753bf19c61e3e04bd76174ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bd598f84a56581251da0ffb62ebdcab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-097883cc9ef7932a411045b1438b95d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10ae74f99ef44a3323a358bb9221127e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fc3a7620480f3f55911402d1251469ae.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-05-20  CROC Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled   Contrastive Robustness Checks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f8a6bd307d65a7fba5bcc5f44bb9e542.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-05-20  Unifying Segment Anything in Microscopy with Multimodal Large Language   Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
