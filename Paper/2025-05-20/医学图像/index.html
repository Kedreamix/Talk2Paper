<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-20  GOUHFI a novel contrast- and resolution-agnostic segmentation tool for   Ultra-High Field MRI">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-d7635e32016c1ff643bb2aff7cc428ab.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    42 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-20-更新"><a href="#2025-05-20-更新" class="headerlink" title="2025-05-20 更新"></a>2025-05-20 更新</h1><h2 id="GOUHFI-a-novel-contrast-and-resolution-agnostic-segmentation-tool-for-Ultra-High-Field-MRI"><a href="#GOUHFI-a-novel-contrast-and-resolution-agnostic-segmentation-tool-for-Ultra-High-Field-MRI" class="headerlink" title="GOUHFI: a novel contrast- and resolution-agnostic segmentation tool for   Ultra-High Field MRI"></a>GOUHFI: a novel contrast- and resolution-agnostic segmentation tool for   Ultra-High Field MRI</h2><p><strong>Authors:Marc-Antoine Fortin, Anne Louise Kristoffersen, Michael Staff Larsen, Laurent Lamalle, Ruediger Stirnberg, Paal Erik Goa</strong></p>
<p>Recently, Ultra-High Field MRI (UHF-MRI) has become more available and one of the best tools to study the brain. One common step in quantitative neuroimaging is the brain segmentation. However, the differences between UHF-MRI and 1.5-3T images are such that the automatic segmentation techniques optimized at these field strengths usually produce unsatisfactory segmentation results for UHF images. It has been particularly challenging to perform quantitative analyses as typically done with 1.5-3T data, considerably limiting the potential of UHF-MRI. Hence, we propose a novel Deep Learning (DL)-based segmentation technique called GOUHFI: Generalized and Optimized segmentation tool for Ultra-High Field Images, designed to segment UHF images of various contrasts and resolutions. For training, we used a total of 206 label maps from four datasets acquired at 3T, 7T and 9.4T. In contrast to most DL strategies, we used a previously proposed domain randomization approach, where synthetic images generated from the label maps were used for training a 3D U-Net. GOUHFI was tested on seven different datasets and compared to techniques like FastSurferVINN and CEREBRUM-7T. GOUHFI was able to the segment six contrasts and seven resolutions tested at 3T, 7T and 9.4T. Average Dice-Sorensen Similarity Coefficient (DSC) scores of 0.87, 0.84, 0.91 were computed against the ground truth segmentations at 3T, 7T and 9.4T. Moreover, GOUHFI demonstrated impressive resistance to the typical inhomogeneities observed at UHF-MRI, making it a new powerful segmentation tool that allows to apply the usual quantitative analysis pipelines also at UHF. Ultimately, GOUHFI is a promising new segmentation tool, being the first of its kind proposing a contrast- and resolution-agnostic alternative for UHF-MRI, making it the forthcoming alternative for neuroscientists working with UHF-MRI or even lower field strengths. </p>
<blockquote>
<p>最近，超高场磁共振成像（UHF-MRI）变得越来越普及，已成为研究大脑的最佳工具之一。定量神经影像学中的一个常见步骤是大脑分割。然而，UHF-MRI与1.5-3T图像之间的差异使得针对这些场强优化的自动分割技术通常会产生令人不满意的UHF图像分割结果。使用通常用于1.5-3T数据的定量分析方法具有挑战性，这极大地限制了UHF-MRI的潜力。因此，我们提出了一种新型的基于深度学习的分割技术，称为GOUHFI：适用于超高场图像的通用优化分割工具，旨在分割具有各种对比度和分辨率的UHF图像。为了训练，我们使用了从3T、7T和9.4T采集的四个数据集的共206个标签图。与大多数深度学习策略不同，我们采用了先前提出的域随机化方法，其中根据标签图生成的合成图像用于训练3D U-Net。GOUHFI在七个不同的数据集上进行了测试，并与FastSurferVINN和CEREBRUM-7T等技术进行了比较。GOUHFI能够在3T、7T和9.4T测试的六种对比度和七种分辨率下进行分割。与3T、7T和9.4T的基准分割相比，计算出的平均Dice-Sorensen相似系数（DSC）分数分别为0.87、0.84、0.91。此外，GOUHFI对UHF-MRI中常见的非均匀性表现出了令人印象深刻的抗性，成为了一种强大的新分割工具，允许在UHF上应用通常的定量分析流水线。最终，GOUHFI是一种有前途的新分割工具，它是第一个提出适用于UHF-MRI的对比度和分辨率无关替代方案的工具，使其成为从事UHF-MRI研究的神经科学家或甚至使用较低场强的科学家的未来替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11445v1">PDF</a> 45 pages, 9 Figures, 6 Tables, Submitted to Imaging Neuroscience on   16-05-25</p>
<p><strong>Summary</strong><br>    本文介绍了一种基于深度学习的新型超高频磁共振成像（UHF-MRI）分割工具GOUHFI。该工具可用于对各种对比度和分辨率的UHF图像进行分割，并通过使用合成图像进行训练来提高性能。在多个数据集上测试后，GOUHFI展现出良好的分割效果，并具有较高的抵抗UHF-MRI常见的不均匀性能力。它为神经科学家提供了一种有前途的新工具，可作为超高频和较低场强的MRI的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UHF-MRI已成为研究大脑的最佳工具之一，但在定量分析中面临挑战。</li>
<li>自动分割技术在超高场MRI上优化通常产生不满意的分割结果。</li>
<li>GOUHFI是一个基于深度学习的新型分割工具，旨在解决UHF图像的分割问题。</li>
<li>GOUHFI可用于多种对比度和分辨率的UHF图像分割。</li>
<li>使用合成图像进行训练，提高GOUHFI的性能。</li>
<li>GOUHFI在多个数据集上表现出良好的分割效果，并具有较高的抵抗UHF-MRI不均匀性的能力。</li>
<li>GOUHFI为神经科学家提供了一种有前途的新工具，可应用于超高频和较低场强的MRI分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11445">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-73572fa160f532c0fc70cfaf7d3892c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c448197cc3a62d77fdb5c156cb25ff7c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Diff-Unfolding-A-Model-Based-Score-Learning-Framework-for-Inverse-Problems"><a href="#Diff-Unfolding-A-Model-Based-Score-Learning-Framework-for-Inverse-Problems" class="headerlink" title="Diff-Unfolding: A Model-Based Score Learning Framework for Inverse   Problems"></a>Diff-Unfolding: A Model-Based Score Learning Framework for Inverse   Problems</h2><p><strong>Authors:Yuanhao Wang, Shirin Shoushtari, Ulugbek S. Kamilov</strong></p>
<p>Diffusion models are extensively used for modeling image priors for inverse problems. We introduce \emph{Diff-Unfolding}, a principled framework for learning posterior score functions of \emph{conditional diffusion models} by explicitly incorporating the physical measurement operator into a modular network architecture. Diff-Unfolding formulates posterior score learning as the training of an unrolled optimization scheme, where the measurement model is decoupled from the learned image prior. This design allows our method to generalize across inverse problems at inference time by simply replacing the forward operator without retraining. We theoretically justify our unrolling approach by showing that the posterior score can be derived from a composite model-based optimization formulation. Extensive experiments on image restoration and accelerated MRI show that Diff-Unfolding achieves state-of-the-art performance, improving PSNR by up to 2 dB and reducing LPIPS by $22.7%$, while being both compact (47M parameters) and efficient (0.72 seconds per $256 \times 256$ image). An optimized C++&#x2F;LibTorch implementation further reduces inference time to 0.63 seconds, underscoring the practicality of our approach. </p>
<blockquote>
<p>扩散模型被广泛用于逆向问题的图像先验建模。我们介绍了”Diff-Unfolding”，这是一种通过明确地将物理测量算子融入模块化网络架构来学习”条件扩散模型”的后验评分函数的原理性框架。Diff-Unfolding将后验评分学习制定为展开优化方案的训练，其中测量模型与学习的图像先验解耦。这种设计使我们的方法能够在推理时通过简单地替换正向算子而泛化到各种逆向问题，而无需重新训练。我们通过理论证明，从基于复合模型的优化公式中可以推导出后验评分，从而证明我们的展开方法是合理的。在图像恢复和加速MRI的广泛实验表明，Diff-Unfolding达到了最先进的性能，PSNR提高了高达2分贝，LPIPS降低了22.7%，同时既紧凑（47M参数）又高效（每张256x256图像0.72秒）。经过优化的C++&#x2F;LibTorch实现进一步将推理时间缩短至0.63秒，凸显了我们方法的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11393v1">PDF</a> 19 pages, 13 figures,</p>
<p><strong>Summary</strong></p>
<p>扩散模型广泛用于图像先验建模中的反问题。本文提出一种名为Diff-Unfolding的理论框架，它通过明确地将物理测量算子融入模块化网络架构，学习条件扩散模型的后验得分函数。Diff-Unfolding将后验得分学习公式化为一种展开的优化方案训练，将测量模型与学习的图像先验解耦。这种设计使我们在推理时能灵活应对不同的反问题，只需替换前向算子而无需重新训练。本文理论证明了展开方法的合理性，即通过复合模型为基础的优化公式推导后验得分。在图像恢复和加速MRI的广泛实验表明，Diff-Unfolding达到了最先进的性能，PSNR提高了高达2分贝，LPIPS降低了22.7%，同时模型紧凑（47M参数）且高效（每处理一张256x256图像需0.72秒）。使用优化的C++&#x2F;LibTorch实现进一步将推理时间缩短至0.63秒，凸显了方法的实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型广泛应用于图像先验建模中的反问题。</li>
<li>提出了Diff-Unfolding框架，通过结合物理测量算子学习条件扩散模型的后验得分函数。</li>
<li>Diff-Unfolding将后验得分学习表述为一种展开的优化方案，实现了测量模型与图像先验的解耦。</li>
<li>该方法在推理阶段能灵活应用于多种反问题，只需替换前向算子。</li>
<li>理论证明了展开方法的合理性，即通过复合模型优化公式推导后验得分。</li>
<li>在图像恢复和加速MRI的实验中，Diff-Unfolding表现出卓越性能，PSNR和LPIPS指标均有显著改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11393">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-04b9c048e970fcdc1b0fc52ffeadf311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f52cd10c2dafc88b564eda5be73ab756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f229547ab1e8b09f50ade0a81de0e993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1af47e187b5eba2348d82be4c653679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d38ed3dab3d308f856365fb915c5e590.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CheX-DS-Improving-Chest-X-ray-Image-Classification-with-Ensemble-Learning-Based-on-DenseNet-and-Swin-Transformer"><a href="#CheX-DS-Improving-Chest-X-ray-Image-Classification-with-Ensemble-Learning-Based-on-DenseNet-and-Swin-Transformer" class="headerlink" title="CheX-DS: Improving Chest X-ray Image Classification with Ensemble   Learning Based on DenseNet and Swin Transformer"></a>CheX-DS: Improving Chest X-ray Image Classification with Ensemble   Learning Based on DenseNet and Swin Transformer</h2><p><strong>Authors:Xinran Li, Yu Liu, Xiujuan Xu, Xiaowei Zhao</strong></p>
<p>The automatic diagnosis of chest diseases is a popular and challenging task. Most current methods are based on convolutional neural networks (CNNs), which focus on local features while neglecting global features. Recently, self-attention mechanisms have been introduced into the field of computer vision, demonstrating superior performance. Therefore, this paper proposes an effective model, CheX-DS, for classifying long-tail multi-label data in the medical field of chest X-rays. The model is based on the excellent CNN model DenseNet for medical imaging and the newly popular Swin Transformer model, utilizing ensemble deep learning techniques to combine the two models and leverage the advantages of both CNNs and Transformers. The loss function of CheX-DS combines weighted binary cross-entropy loss with asymmetric loss, effectively addressing the issue of data imbalance. The NIH ChestX-ray14 dataset is selected to evaluate the model’s effectiveness. The model outperforms previous studies with an excellent average AUC score of 83.76%, demonstrating its superior performance. </p>
<blockquote>
<p>胸部疾病的自动诊断是一项受欢迎且具有挑战性的任务。当前大多数方法都是基于卷积神经网络（CNNs），这些网络专注于局部特征而忽略了全局特征。最近，计算机视觉领域引入了自注意力机制，表现出了卓越的性能。因此，本文提出了一种有效的模型CheX-DS，用于医学领域胸部X射线长尾多标签数据的分类。该模型基于优秀的医学成像DenseNet模型和目前流行的Swin Transformer模型，采用集成深度学习技术结合这两种模型的优势，充分利用CNN和Transformer的优点。CheX-DS的损失函数结合了加权二元交叉熵损失和不对称损失，有效地解决了数据不平衡的问题。选用NIH ChestX-ray14数据集对模型的有效性进行评估。该模型的平均AUC得分高达83.76%，表现出优异的性能，超过了以前的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11168v1">PDF</a> BIBM</p>
<p><strong>Summary</strong></p>
<p>基于卷积神经网络（CNNs）的局限性，本文提出了一种新的模型CheX-DS，用于诊断胸部疾病。该模型结合了DenseNet和Swin Transformer模型的优势，利用集成深度学习技术进行分类。模型的损失函数结合了加权二元交叉熵损失和不对称损失，以解决数据不平衡问题。在NIH ChestX-ray14数据集上的实验结果表明，该模型具有出色的平均AUC得分，表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前诊断胸部疾病的方法大多基于卷积神经网络（CNNs），但忽略了全局特征。</li>
<li>自注意力机制在计算机视觉领域被引入，表现出卓越的性能。</li>
<li>CheX-DS模型结合了DenseNet模型和Swin Transformer模型的优势。</li>
<li>CheX-DS模型的损失函数结合了加权二元交叉熵损失和不对称损失，以解决数据不平衡问题。</li>
<li>CheX-DS模型在NIH ChestX-ray14数据集上的平均AUC得分为83.76%，表现出卓越的性能。</li>
<li>该模型可以有效地处理医学领域的长尾多标签数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11168">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c1a044421981ae89af75da41039a817c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-816a44117eea6c8f973214d038d744fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7635e32016c1ff643bb2aff7cc428ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-271c7fb72c2a3460b664bc65eddf4b69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-495a25bd1d970c4786cecb71745572fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0eb75744439aadfa8949655c8d53a01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5585810bd3d9e70fd7a9b077b50eb75e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba0c25216a9b8fae75fd5dec1eb760d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db491cdd57b375dfcfe62e2033a809ce.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Controlling-spatial-correlation-in-k-space-interpolation-networks-for-MRI-reconstruction-denoising-versus-apparent-blurring"><a href="#Controlling-spatial-correlation-in-k-space-interpolation-networks-for-MRI-reconstruction-denoising-versus-apparent-blurring" class="headerlink" title="Controlling spatial correlation in k-space interpolation networks for   MRI reconstruction: denoising versus apparent blurring"></a>Controlling spatial correlation in k-space interpolation networks for   MRI reconstruction: denoising versus apparent blurring</h2><p><strong>Authors:Istvan Homolya, Peter Dawood, Jannik Stebani, Felix Breuer, Grit Hein, Matthias Gamer, Florian Knoll, Martin Blaimer</strong></p>
<p>Purpose: To improve the interpretability of noise amplification and apparent blurring of k-space interpolation networks, and to optimize for them in the loss function as a model-based regularizer in k-space interpolation networks.   Methods: Network is subjected to noise amplification analysis through automatic differentiation of the input with respect to the input. Noise variance maps are decomposed into terms accounting for the linear and nonlinear characteristics of the network. Variance maps are derived in each iteration, allowing for runtime quality monitoring. Maximum variance (eigenpixel) and residual variance maps (pixel contamination) are introduced, which describe the network noise amplification and apparent blurring, respectively. By including the variance maps in the training, the loss function is enriched with a model-based regularizer beyond the k-space data consistency term. Accordingly, the proposed g-factor-informed RAKI (GIF-RAKI) establishes a recurrent flow of noise and apparent blurring information into the training, that drives the denoising via the trainable nonlinear activation function.   Results: GIF-RAKI outperforms other RAKI implementations, supported by difference maps, and image quality metrics. Eigenpixel and pixel contamination maps provide quantitative metrics for noise amplification and apparent blurring, respectively, without the need for a gold standard reference. RAKI with tuneable Leaky ReLU is capable of adjusting its own nonlinearity automatically.   Conclusion: The additional model-based loss terms allow to optimize for the trade-off between denoising and apparent blurring during RAKI training. This has the potential to eliminate the need for heuristic hyperparameter tweaking. </p>
<blockquote>
<p>目的：旨在提高k空间插值网络的噪声放大和明显模糊度的可解释性，并在损失函数中对它们进行优化，作为k空间插值网络的基于模型的正规化器。方法：网络通过输入相对于自身的自动微分来进行噪声放大分析。噪声方差图被分解成反映网络线性特征和非线性特征的项。每次迭代都会推导出方差图，从而实现运行时质量监控。引入了最大方差（特征像素）和残差方差图（像素污染），分别描述网络噪声放大和明显模糊度。通过将方差图纳入训练，损失函数除了k空间数据一致性项之外，还包含了基于模型的正规化器。因此，所提出的g因子信息RAKI（GIF-RAKI）将噪声和明显模糊度信息不断融入训练，通过可训练的非线性激活函数驱动去噪。结果：GIF-RAKI在其他RAKI实现中表现出色，这得到了差异图和图像质量指标的支持。特征像素和像素污染图分别为噪声放大和明显模糊度提供了定量指标，而无需金标准参考。具有可调泄漏ReLU的RAKI能够自动调整其非线性。结论：额外的基于模型的损失项允许在RAKI训练过程中优化去噪和明显模糊度之间的权衡。这有可能消除对启发式超参数调整的需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11155v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>该研究旨在改善和提升k空间插值网络的噪声放大和模糊表现的解释性，并在损失函数中对其进行优化作为模型正则化器。通过自动区分网络输入，进行噪声放大分析，分解噪声方差图以考虑网络的线性和非线性特征。在每个迭代过程中推导方差图，实现运行时质量监控。引入最大方差（特征像素）和残差方差图（像素污染）来描述网络噪声放大和模糊现象。通过将方差图纳入训练，损失函数除了k空间数据一致性项外还加入了基于模型的正规化器。因此，所提出的基于g因子的RAKI（GIF-RAKI）将噪声和模糊信息不断融入训练，通过可训练的非线性激活函数驱动去噪。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究旨在提高k空间插值网络中的噪声放大和模糊表现的解释性，并对其进行优化。</li>
<li>通过自动区分网络输入进行噪声放大分析，并分解噪声方差图。</li>
<li>在每个迭代过程中推导方差图，实现运行时质量监控，引入最大方差和残差方差图来描述网络噪声放大和模糊现象。</li>
<li>将方差图纳入训练，丰富损失函数，包括基于模型的正规化器。</li>
<li>GIF-RAKI建立了一个将噪声和模糊信息不断融入训练的流程，通过可训练的非线性激活函数驱动去噪。</li>
<li>GIF-RAKI在其他RAKI实现中表现出优越性能，并提供定量指标来衡量噪声放大和模糊现象。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11155">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e690d8aaf25d70475bae69f32c35b854.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="In-silico-tool-for-identification-of-colorectal-cancer-from-cell-free-DNA-biomarkers"><a href="#In-silico-tool-for-identification-of-colorectal-cancer-from-cell-free-DNA-biomarkers" class="headerlink" title="In silico tool for identification of colorectal cancer from cell-free   DNA biomarkers"></a>In silico tool for identification of colorectal cancer from cell-free   DNA biomarkers</h2><p><strong>Authors:Kartavya Mathur, Shipra Jain, Nisha Bajiya, Nishant Kumar, Gajendra P. S. Raghava</strong></p>
<p>Colorectal cancer remains a major global health concern, with early detection being pivotal for improving patient outcomes. In this study, we leveraged high throughput methylation profiling of cellfree DNA to identify and validate diagnostic biomarkers for CRC. The GSE124600 study data were downloaded from the Gene Expression Omnibus, as the discovery cohort, comprising 142 CRC and 132 normal cfDNA methylation profiles obtained via MCTA seq. After preprocessing and filtering, 97,863 CpG sites were retained for further analysis. Differential methylation analysis using statistical tests identified 30,791 CpG sites as significantly altered in CRC samples, where p is less than 0.05. Univariate scoring enabled the selection of top ranking features, which were further refined using multiple feature selection algorithms, including Recursive Feature Elimination, Sequential Feature Selection, and SVC L1. Various machine learning models such as Logistic Regression, Support Vector Machines, Random Forest, and Multi layer Perceptron were trained and tested using independent validation datasets. The best performance was achieved with an MLP model trained on 25 features selected by RFE, reaching an AUROC of 0.89 and MCC of 0.78 on validation data. Additionally, a deep learning based convolutional neural network achieved an AUROC of 0.78. Functional annotation of the most predictive CpG sites identified several genes involved in key cellular processes, some of which were validated for differential expression in CRC using the GEPIA2 platform. Our study highlights the potential of cfDNA methylation markers combined with ML and DL models for noninvasive and accurate CRC detection, paving the way for clinically relevant diagnostic tools. </p>
<blockquote>
<p>结直肠癌仍然是一个全球性的重大健康问题，早期发现对于改善患者预后至关重要。在这项研究中，我们利用细胞游离DNA的高通量甲基化谱分析技术，来识别和验证CRC的诊断生物标志物。GSE124600研究数据从基因表达综合数据库下载，作为发现队列，包含142例CRC和132例正常cfDNA甲基化谱，通过MCTA seq获得。经过预处理和筛选后，保留了97,863个CpG位点用于进一步分析。使用统计测试进行差异甲基化分析，确定了30,791个在CRC样本中显著改变的CpG位点（p &lt; 0.05）。单变量评分使得能够选择排名靠前的特征，这些特征进一步使用多种特征选择算法进行精炼，包括递归特征消除、序列特征选择和SVC L1。使用独立验证数据集训练和测试了各种机器学习模型，如逻辑回归、支持向量机、随机森林和多层感知器。使用RFE选择的25个特征训练的MLP模型表现最佳，在验证数据上的AUROC达到0.89，MCC达到0.78。此外，基于深度学习的卷积神经网络达到了AUROC为0.78。预测性最高的CpG位点的功能注释确定了涉及关键细胞过程的几个基因，其中一些基因在CRC中的差异表达已通过GEPIA2平台进行验证。我们的研究强调了cfDNA甲基化标志物结合机器学习和深度学习模型在无创且准确的CRC检测中的潜力，为开发临床相关的诊断工具奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11041v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究利用细胞游离DNA的高通量甲基化谱分析技术，成功鉴定并验证了结直肠癌（CRC）的诊断生物标志物。研究采用GSE124600数据，通过差异甲基化分析和机器学习模型，筛选关键特征并构建模型，其中多层感知器模型表现最佳，验证数据集的受试者工作特征曲线下面积达到0.89，显示cfDNA甲基化标记结合机器学习在结直肠癌无创检测中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究利用细胞游离DNA的高通量甲基化谱分析技术，旨在鉴定结直肠癌（CRC）的诊断生物标志物。</li>
<li>通过差异甲基化分析，确定了30,791个在CRC样本中显著改变的CpG位点。</li>
<li>使用多种机器学习模型进行训练和测试，其中多层感知器模型表现最佳，验证数据集的受试者工作特征曲线下面积达到0.89。</li>
<li>深度学习卷积神经网络也表现出良好的诊断潜力，其受试者工作特征曲线下面积达到0.78。</li>
<li>最具预测性的CpG位点的功能注释显示，涉及关键细胞过程的几个基因被确定，并在CRC中验证了其差异表达。</li>
<li>研究结果强调了细胞游离DNA甲基化标记与机器学习结合在结直肠癌无创检测中的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11041">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e398b57283d5e2198b3902d0a4104fa7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rethinking-the-Mean-Teacher-Strategy-from-the-Perspective-of-Self-paced-Learning"><a href="#Rethinking-the-Mean-Teacher-Strategy-from-the-Perspective-of-Self-paced-Learning" class="headerlink" title="Rethinking the Mean Teacher Strategy from the Perspective of Self-paced   Learning"></a>Rethinking the Mean Teacher Strategy from the Perspective of Self-paced   Learning</h2><p><strong>Authors:Pengchen Zhang, Alan J. X. Guo, Sipin Luo, Zhe Han, Lin Guo</strong></p>
<p>Semi-supervised medical image segmentation has attracted significant attention due to its potential to reduce manual annotation costs. The mean teacher (MT) strategy, commonly understood as introducing smoothed, temporally lagged consistency regularization, has demonstrated strong performance across various tasks in this field. In this work, we reinterpret the MT strategy on supervised data as a form of self-paced learning, regulated by the output agreement between the temporally lagged teacher model and the ground truth labels. This idea is further extended to incorporate agreement between a temporally lagged model and a cross-architectural model, which offers greater flexibility in regulating the learning pace and enables application to unlabeled data. Specifically, we propose dual teacher-student learning (DTSL), a framework that introduces two groups of teacher-student models with different architectures. The output agreement between the cross-group teacher and student models is used as pseudo-labels, generated via a Jensen-Shannon divergence-based consensus label generator (CLG). Extensive experiments on popular datasets demonstrate that the proposed method consistently outperforms existing state-of-the-art approaches. Ablation studies further validate the effectiveness of the proposed modules. </p>
<blockquote>
<p>半监督医学图像分割因其降低手动标注成本的潜力而受到广泛关注。均值教师（Mean Teacher，简称MT）策略，通常理解为引入平滑、时间滞后一致性正则化，在该领域各种任务中表现出强大的性能。在这项工作中，我们将MT策略在监督数据上的应用重新解释为一种自我节奏学习，受时间滞后教师模型与真实标签之间输出协议的控制。这个想法进一步扩展到包含时间滞后模型与跨架构模型之间的协议，这为调节学习进度提供了更大的灵活性，并使其能够应用于无标签数据。具体来说，我们提出了双教师学生学习（Dual Teacher-Student Learning，简称DTSL）框架，该框架引入了两组不同架构的教师-学生模型。跨组教师和学生模型之间的输出协议作为通过Jensen-Shannon散度共识标签生成器（CLG）生成的伪标签。在流行数据集上的大量实验表明，所提出的方法始终优于现有的最先进的方法。消融研究进一步验证了所提出模块的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11018v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>半监督医学图像分割中，均值教师（MT）策略被重新解释为一种由输出协议监管的自我进度学习方式，并扩展应用于无标签数据。提出双教师学生学习（DTSL）框架，引入两组不同架构的教师-学生模型，通过基于Jensen-Shannon散度的共识标签生成器（CLG）生成伪标签。实验证明该方法优于现有先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>半监督医学图像分割旨在降低手动注释成本。</li>
<li>均值教师（MT）策略被重新解释为自我进度学习，由输出协议监管。</li>
<li>扩展MT策略以纳入不同架构模型之间的协议，提供更灵活的学习速度调控。</li>
<li>提出双教师学生学习（DTSL）框架，包含两组不同架构的教师-学生模型。</li>
<li>使用基于Jensen-Shannon散度的共识标签生成器（CLG）生成伪标签。</li>
<li>在流行数据集上的广泛实验证明该方法优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11018">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b4017a02df3d8f6e86eef739c185965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86d231e6b4f201f6bcb651a565edfa41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a22a98d02d2238fdccee350d429655b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-062b8cbd80dad1876721ab8e8418ef1c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="WeGA-Weakly-Supervised-Global-Local-Affinity-Learning-Framework-for-Lymph-Node-Metastasis-Prediction-in-Rectal-Cancer"><a href="#WeGA-Weakly-Supervised-Global-Local-Affinity-Learning-Framework-for-Lymph-Node-Metastasis-Prediction-in-Rectal-Cancer" class="headerlink" title="WeGA: Weakly-Supervised Global-Local Affinity Learning Framework for   Lymph Node Metastasis Prediction in Rectal Cancer"></a>WeGA: Weakly-Supervised Global-Local Affinity Learning Framework for   Lymph Node Metastasis Prediction in Rectal Cancer</h2><p><strong>Authors:Yifan Gao, Yaoxian Dong, Wenbin Wu, Chaoyang Ge, Feng Yuan, Jiaxi Sheng, Haoyue Li, Xin Gao</strong></p>
<p>Accurate lymph node metastasis (LNM) assessment in rectal cancer is essential for treatment planning, yet current MRI-based evaluation shows unsatisfactory accuracy, leading to suboptimal clinical decisions. Developing automated systems also faces significant obstacles, primarily the lack of node-level annotations. Previous methods treat lymph nodes as isolated entities rather than as an interconnected system, overlooking valuable spatial and contextual information. To solve this problem, we present WeGA, a novel weakly-supervised global-local affinity learning framework that addresses these challenges through three key innovations: 1) a dual-branch architecture with DINOv2 backbone for global context and residual encoder for local node details; 2) a global-local affinity extractor that aligns features across scales through cross-attention fusion; and 3) a regional affinity loss that enforces structural coherence between classification maps and anatomical regions. Experiments across one internal and two external test centers demonstrate that WeGA outperforms existing methods, achieving AUCs of 0.750, 0.822, and 0.802 respectively. By effectively modeling the relationships between individual lymph nodes and their collective context, WeGA provides a more accurate and generalizable approach for lymph node metastasis prediction, potentially enhancing diagnostic precision and treatment selection for rectal cancer patients. </p>
<blockquote>
<p>对直肠癌淋巴结转移（LNM）的准确评估是治疗计划的关键，但目前的MRI评估准确性不佳，可能导致临床决策失误。自动系统的开发也面临重大障碍，主要是缺乏节点级别的注释。以往的方法将淋巴结视为孤立的实体，而不是相互关联的系统，从而忽略了宝贵的空间和上下文信息。为了解决这一问题，我们提出了WeGA，这是一种新型的弱监督全局-局部亲和力学习框架，通过以下三个关键创新解决了这些挑战：1）具有DINOv2主干和残差编码器的双分支架构，用于全局上下文和局部节点细节；2）全局-局部亲和力提取器，通过跨注意力融合来对齐跨尺度的特征；3）区域亲和力损失，在分类图和解剖区域之间强制执行结构一致性。在一个内部和两个外部测试中心的实验表明，WeGA优于现有方法，分别实现了AUC值为0.750、0.822和0.802。通过有效地建模单个淋巴结之间的关系及其集体上下文，WeGA为淋巴结转移预测提供了更准确和可推广的方法，可能提高了直肠癌患者的诊断精度和治疗选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10502v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的弱监督全局局部亲和力学习框架WeGA，用于准确评估直肠癌淋巴结节转移情况。该方法通过融合全局上下文和局部节点细节、跨尺度特征对齐以及结构连贯性，提高了预测准确性和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>直肠癌中淋巴节点转移（LNM）的准确评估对治疗计划至关重要。</li>
<li>当前MRI评估存在准确性问题，导致临床决策失误。</li>
<li>开发自动化系统的难点在于缺乏节点级别的注释。</li>
<li>现有方法忽略淋巴节点的相互关联性和空间上下文信息。</li>
<li>WeGA通过全局和局部分支架构、全局局部亲和力提取器和区域亲和力损失来解决这些问题。</li>
<li>实验结果表明，WeGA在内部和外部测试中心均表现出优异性能，提高了预测准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10502">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d48c4ce32e03665e92ef6019f9c68006.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0c3cfcfcb8371f4fb35016db2f0b63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea13d034f63c5c07fb9783cb4ff421b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d3c72e94119894a999f3f1371bf7b17.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-Automatic-CAD-Annotations-for-Supervised-Learning-in-3D-Scene-Understanding"><a href="#Leveraging-Automatic-CAD-Annotations-for-Supervised-Learning-in-3D-Scene-Understanding" class="headerlink" title="Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene   Understanding"></a>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene   Understanding</h2><p><strong>Authors:Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer</strong></p>
<p>High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models. </p>
<blockquote>
<p>高级别的三维场景理解在许多应用中至关重要。然而，生成准确的三维标注的挑战使得开发深度学习模型变得困难。我们转向最近自动检索合成CAD模型的进展，并证明由这些方法生成的数据可以用作高质量的真实标签，用于训练有监督的深度学习模型。更确切地说，我们采用了一种类似于之前用于ScanNet场景中自动标注对象的管道，这些对象带有其9D姿态和CAD模型。这次，我们将其应用于最新的ScanNet++ v1数据集，该数据集之前缺乏此类注释。我们的研究结果表明，不仅可以在这些自动获得的注释上训练深度学习模型，而且所得模型的性能优于手动注释数据训练的模型。我们在两个不同的任务上对此进行了验证：点云补全和单视图CAD模型检索与对齐。我们的结果强调了自动三维标注在提高模型性能的同时显著降低标注成本的潜力。为了支持未来对三维场景理解的研究，我们将发布我们的注释，我们称之为SCANnotate++，以及我们训练的模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13580v4">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://stefan-ainetter.github.io/SCANnotatepp">https://stefan-ainetter.github.io/SCANnotatepp</a>; CVPR’25   Workshop</p>
<p><strong>Summary</strong></p>
<p>这篇文本介绍了自动检索合成CAD模型的新进展，并将其应用于ScanNet++ v1数据集的自动标注。研究结果表明，使用自动获得的标注训练深度模型不仅可行，而且其性能优于使用手动标注数据训练的模型。该研究为增强模型性能并显著降低标注成本提供了潜力，并将发布其标注和训练模型以支持未来研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动检索合成CAD模型技术被应用于ScanNet++ v1数据集的标注，为深度学习任务提供了高质量的地标数据。</li>
<li>使用自动获得的标注训练的深度模型性能优于使用手动标注数据训练的模型。</li>
<li>自动进行3D标注的方法显著降低了标注成本。</li>
<li>通过应用类似ScanNet场景的自动标注技术，实现了对物体进行精确标注的目标。</li>
<li>研究结果验证了自动进行3D场景理解的可行性，并展示了其在多个应用中的潜力。</li>
<li>研究结果强调了自动获取高质量标注的重要性，特别是在缺乏手动标注数据的场景中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13580">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c388eab897e9fb3439eb46d1e69b9edd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63032ce7a7c64077e664f27e6a061354.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-261fc97f44c75d77faa745b096a25866.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe3ea7b428a57cad58f9a68d93a6a9a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0303bb75b63c6b19cecda0d4ca074201.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Resolving-the-Ambiguity-of-Complete-to-Partial-Point-Cloud-Registration-for-Image-Guided-Liver-Surgery-with-Patches-to-Partial-Matching"><a href="#Resolving-the-Ambiguity-of-Complete-to-Partial-Point-Cloud-Registration-for-Image-Guided-Liver-Surgery-with-Patches-to-Partial-Matching" class="headerlink" title="Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration   for Image-Guided Liver Surgery with Patches-to-Partial Matching"></a>Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration   for Image-Guided Liver Surgery with Patches-to-Partial Matching</h2><p><strong>Authors:Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte</strong></p>
<p>In image-guided liver surgery, the initial rigid alignment between preoperative and intraoperative data, often represented as point clouds, is crucial for providing sub-surface information from preoperative CT&#x2F;MRI images to the surgeon during the procedure. Currently, this alignment is typically performed using semi-automatic methods, which, while effective to some extent, are prone to errors that demand manual correction. Point cloud correspondence-based registration methods are promising to serve as a fully automatic solution. However, they may struggle in scenarios with limited intraoperative surface visibility, a common challenge in liver surgery, particularly in laparoscopic procedures, which we refer to as complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating the performance of state-of-the-art learning-based point cloud registration methods on our carefully constructed in silico and in vitro datasets. Then, we propose a patches-to-partial matching strategy as a plug-and-play module to resolve the ambiguity, which can be seamlessly integrated into learning-based registration methods without disrupting their end-to-end structure. It has proven effective and efficient in improving registration performance for cases with limited intraoperative visibility. The constructed benchmark and the proposed module establish a solid foundation for advancing applications of point cloud correspondence-based registration methods in image-guided liver surgery. </p>
<blockquote>
<p>在图像引导下的肝脏手术中，术前和术中数据之间的初始刚性对齐至关重要。这些数据通常以点云的形式呈现，以便为外科医生提供从术前CT&#x2F;MRI图像中获得的亚表面信息。目前，这种对齐通常使用半自动方法执行，这些方法尽管在一定程度上有效，但仍容易出现需要手动纠正的错误。基于点云对应的注册方法有望成为全自动解决方案。然而，在肝内表面可见性有限的场景中，它们可能会遇到困难，这是肝脏手术中常见的挑战，特别是在腹腔镜手术中，我们称之为完全到部分的模糊性。我们首先通过评估最先进的学习型点云注册方法在精心构建的体内和体外数据集上的表现来说明这种模糊性。然后，我们提出了一种补丁到部分的匹配策略，作为一种即插即用的模块来解决模糊性问题。它可以无缝集成到基于学习的注册方法中，而不会破坏其端到端的结构。对于术中视野有限的病例，该策略已被证明能够有效提高注册性能。构建的基准测试和提出的模块为推进点云对应注册方法在图像引导肝脏手术中的应用奠定了坚实的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19328v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在图像引导下的肝脏手术中，对术前和术中数据进行初步精确对齐极为关键，此过程常表现为点云形式，旨在向手术医生提供来自术前CT&#x2F;MRI图像的表面下信息。当前，通常采用半自动方法进行此对齐，虽然在一定程度上有效，但易出现错误需要人工校正。基于点云对应的注册方法有望成为全自动解决方案，但在术中表面可见性有限的情况下可能会遇到困难，这在肝脏手术中尤为常见，特别是在腹腔镜手术中我们称之为完整到部分的歧义性。为解决此歧义性，本文提出一种即插即用的“补丁到部分”匹配策略，该策略可无缝集成到基于学习的注册方法中，且不影响其端到端的结构。对于术中可见性有限的情况，该策略在改善注册性能方面被证明是有效且高效的。本文构建的基准和提出的模块为推进点云对应注册方法在图像引导肝脏手术中的应用奠定了坚实基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在图像引导肝脏手术中，术前与术中数据的初始精确对齐至关重要，为手术医生提供来自术前影像的表面下信息。</li>
<li>当前的对齐方法主要依赖半自动技术，尽管有效，但存在需要人工校正的错误风险。</li>
<li>点云对应的注册方法被视为全自动解决方案的候选，但在术中表面可见性有限的情况下可能会遭遇挑战。</li>
<li>本文定义了完整到部分的歧义性问题，尤其在腹腔镜肝脏手术中尤为突出。</li>
<li>为解决上述歧义性，提出了一种“补丁到部分”匹配策略，可无缝集成到现有基于学习的注册方法中。</li>
<li>此策略在改善注册性能上被证实有效且高效，特别是在术中可见性有限的情况下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19328">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-600426655e969fa25fdfa10e3fa85af3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-837ccce401f569033b43f4edc9ef7e0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c552527dbf1de37c1376db9340a27ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0362b2097230aa174b85e92ec7f50fd3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="reBEN-Refined-BigEarthNet-Dataset-for-Remote-Sensing-Image-Analysis"><a href="#reBEN-Refined-BigEarthNet-Dataset-for-Remote-Sensing-Image-Analysis" class="headerlink" title="reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis"></a>reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis</h2><p><strong>Authors:Kai Norman Clasen, Leonard Hackel, Tom Burgert, Gencer Sumbul, Begüm Demir, Volker Markl</strong></p>
<p>This paper presents refined BigEarthNet (reBEN) that is a large-scale, multi-modal remote sensing dataset constructed to support deep learning (DL) studies for remote sensing image analysis. The reBEN dataset consists of 549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN, we initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the BigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m. We apply atmospheric correction to the Sentinel-2 patches using the latest version of the sen2cor tool, resulting in higher-quality patches compared to those present in BigEarthNet. Each patch is then associated with a pixel-level reference map and scene-level multi-labels. This makes reBEN suitable for pixel- and scene-based learning tasks. The labels are derived from the most recent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class nomenclature as in BigEarthNet. The use of the most recent CLC map results in overcoming the label noise present in BigEarthNet. Furthermore, we introduce a new geographical-based split assignment algorithm that significantly reduces the spatial correlation among the train, validation, and test sets with respect to those present in BigEarthNet. This increases the reliability of the evaluation of DL models. To minimize the DL model training time, we introduce software tools that convert the reBEN dataset into a DL-optimized data format. In our experiments, we show the potential of reBEN for multi-modal multi-label image classification problems by considering several state-of-the-art DL models. The pre-trained model weights, associated code, and complete dataset are available at <a target="_blank" rel="noopener" href="https://bigearth.net/">https://bigearth.net</a>. </p>
<blockquote>
<p>本文介绍了精细化的BigEarthNet（reBEN），这是一个大规模、多模态的遥感数据集，旨在支持用于遥感图像分析的深度学习（DL）研究。reBEN数据集包含549,488对Sentinel-1和Sentinel-2图像斑块。为了构建reBEN，我们首先考虑用于构建BigEarthNet数据集的Sentinel-1和Sentinel-2瓦片，然后将其划分为大小为1200米x 1200米的斑块。我们对Sentinel-2斑块应用大气校正，使用sen2cor工具的最新版本，从而得到与BigEarthNet中现有的斑块相比质量更高的斑块。然后，每个斑块都与像素级参考地图和场景级多标签相关联。这使得reBEN适合用于基于像素和场景的学习任务。标签是通过使用与BigEarthNet相同的19类命名法，从最新的2018年CORINE土地覆盖（CLC）地图中得出的。使用最新的CLC地图克服了BigEarthNet中存在的标签噪声。此外，我们引入了一种新的基于地理的分割分配算法，该算法显著减少了训练集、验证集和测试集之间的空间相关性，与BigEarthNet中的相关性相比。这增加了深度学习模型评估的可靠性。为了最小化深度学习模型训练时间，我们引入了将reBEN数据集转换为深度学习优化数据格式的软件工具。在我们的实验中，我们通过考虑一些最先进的深度学习模型，展示了reBEN在多模态多标签图像分类问题上的潜力。预训练模型权重、相关代码和完整数据集可在<a target="_blank" rel="noopener" href="https://bigearth.net上找到./">https://bigearth.net上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03653v5">PDF</a> Accepted at IEEE International Geoscience and Remote Sensing   Symposium (IGARSS) 2025. Our code is available at   <a target="_blank" rel="noopener" href="https://github.com/rsim-tu-berlin/bigearthnet-pipeline">https://github.com/rsim-tu-berlin/bigearthnet-pipeline</a></p>
<p><strong>摘要</strong></p>
<p>本论文推出精细化BigEarthNet（reBEN），这是一套大型、多模式遥感数据集，专为支持遥感图像分析的深度学习（DL）研究而构建。reBEN数据集包含549,488对Sentinel-1和Sentinel-2图像补丁。相较于BigEarthNet，reBEN通过应用大气校正技术提高了补丁质量，并引入新的地理分割分配算法以增加评估深度学习模型的可靠性。此外，为缩短深度学习模型训练时间，我们提供软件工具将reBEN数据集转换为优化的数据格式。实验显示，reBEN在多模态多标签图像分类问题方面具有潜力。预训练模型权重、相关代码和完整数据集可在<a target="_blank" rel="noopener" href="https://bigearth.net访问./">https://bigearth.net访问。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>reBEN是一个大型、多模式遥感数据集，专为支持深度学习研究而构建。</li>
<li>reBEN包含经过大气校正的高质量图像补丁，适用于像素和场景基础学习任务。</li>
<li>利用最新的CORINE Land Cover地图克服BigEarthNet中的标签噪声问题。</li>
<li>引入新的地理分割分配算法以提高评估深度学习模型的可信度。</li>
<li>提供软件工具以优化深度学习模型的训练时间。</li>
<li>实验证明reBEN在多模态多标签图像分类问题上的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.03653">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a9844b7f3a16709fd87d654a508b3eb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa791f7729df39b9aa372f0d7d0a99be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aa521d347dd0ac94c100c0c69944d41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e5f8208787f9f52221eabba2a6a035f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4efe663a379b9baddddf48687e2b2b1d.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-05-20  SoftCoT++ Test-Time Scaling with Soft Chain-of-Thought Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4e9118e1f98bd6af8c0e55280683c781.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-20  QVGen Pushing the Limit of Quantized Video Generative Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
