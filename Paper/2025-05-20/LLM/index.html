<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-20  Modeling cognitive processes of natural reading with transformer-based   Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c8676229573b6660592867fc418dd516.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-20-æ›´æ–°"><a href="#2025-05-20-æ›´æ–°" class="headerlink" title="2025-05-20 æ›´æ–°"></a>2025-05-20 æ›´æ–°</h1><h2 id="Modeling-cognitive-processes-of-natural-reading-with-transformer-based-Language-Models"><a href="#Modeling-cognitive-processes-of-natural-reading-with-transformer-based-Language-Models" class="headerlink" title="Modeling cognitive processes of natural reading with transformer-based   Language Models"></a>Modeling cognitive processes of natural reading with transformer-based   Language Models</h2><p><strong>Authors:Bruno Bianchi, FermÃ­n Travi, Juan E. Kamienkowski</strong></p>
<p>Recent advances in Natural Language Processing (NLP) have led to the development of highly sophisticated language models for text generation. In parallel, neuroscience has increasingly employed these models to explore cognitive processes involved in language comprehension. Previous research has shown that models such as N-grams and LSTM networks can partially account for predictability effects in explaining eye movement behaviors, specifically Gaze Duration, during reading. In this study, we extend these findings by evaluating transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate this relationship. Our results indicate that these architectures outperform earlier models in explaining the variance in Gaze Durations recorded from Rioplantense Spanish readers. However, similar to previous studies, these models still fail to account for the entirety of the variance captured by human predictability. These findings suggest that, despite their advancements, state-of-the-art language models continue to predict language in ways that differ from human readers. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„å…ˆè¿›è¯­è¨€æ¨¡å‹çš„å¼€å‘ã€‚ä¸æ­¤åŒæ—¶ï¼Œç¥ç»ç§‘å­¦ä¹Ÿè¶Šæ¥è¶Šå€¾å‘äºåˆ©ç”¨è¿™äº›æ¨¡å‹æ¥æ¢ç´¢è¯­è¨€ç†è§£æ‰€æ¶‰åŠçš„è®¤çŸ¥è¿‡ç¨‹ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒN-gramså’ŒLSTMç½‘ç»œç­‰æ¨¡å‹å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šè§£é‡Šé˜…è¯»è¿‡ç¨‹ä¸­çš„çœ¼åŠ¨è¡Œä¸ºï¼Œç‰¹åˆ«æ˜¯æ³¨è§†æŒç»­æ—¶é—´ï¼ˆGaze Durationï¼‰çš„é¢„æµ‹æ•ˆæœã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¯„ä¼°åŸºäºtransformerçš„æ¨¡å‹ï¼ˆGPT2ã€LLaMA-7Bå’ŒLLaMA2-7Bï¼‰æ¥è¿›ä¸€æ­¥æ¢è®¨è¿™ç§å…³ç³»ï¼Œä»¥æ‰©å±•è¿™äº›å‘ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¶æ„åœ¨è§£é‡Šé‡Œç¿å…°ç‰¹è¥¿ç­ç‰™è¯»è€…è®°å½•çš„æ³¨è§†æŒç»­æ—¶é—´ï¼ˆGaze Durationsï¼‰æ–¹é¢çš„æ–¹å·®æ—¶è¡¨ç°å¾—ä¼˜äºæ—©æœŸæ¨¡å‹ã€‚ç„¶è€Œï¼Œä¸å…ˆå‰çš„ç ”ç©¶ç±»ä¼¼ï¼Œè¿™äº›æ¨¡å‹ä»ç„¶ä¸èƒ½å®Œå…¨è§£é‡Šäººç±»é¢„æµ‹æ‰€æ•è·çš„æ–¹å·®ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹è¯­è¨€æ–¹é¢ä»ç„¶ä¸äººç±»è¯»è€…çš„æ–¹å¼å­˜åœ¨å·®å¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11485v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯çš„è¿›å±•æ¨åŠ¨äº†ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„å…ˆè¿›è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚åŒæ—¶ï¼Œç¥ç»ç§‘å­¦å¼€å§‹è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨è¿™äº›æ¨¡å‹æ¢ç´¢è¯­è¨€ç†è§£è¿‡ç¨‹ä¸­çš„è®¤çŸ¥è¿‡ç¨‹ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†åŸºäºTransformerçš„æ¨¡å‹ï¼ˆGPT2ã€LLaMA-7Bå’ŒLLaMA2-7Bï¼‰ï¼Œä»¥è¿›ä¸€æ­¥ç ”ç©¶è¯­è¨€æ¨¡å‹å’Œçœ¼åŠ¨è¡Œä¸ºï¼ˆå°¤å…¶æ˜¯é˜…è¯»æ—¶çš„æ³¨è§†æŒç»­æ—¶é—´ï¼‰ä¹‹é—´çš„å…³ç³»ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ–°æ¨¡å‹åœ¨è§£é‡Šæ³¨è§†æŒç»­æ—¶é—´æ–¹é¢çš„æ–¹å·®è¡¨ç°ä¼˜äºæ—©æœŸæ¨¡å‹ï¼Œä½†ä»ä¸èƒ½å®Œå…¨è§£é‡Šäººç±»é¢„æµ‹æ€§æ‰€äº§ç”Ÿçš„å…¨éƒ¨æ–¹å·®ã€‚è¿™è¡¨æ˜å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹äººç±»é˜…è¯»æ–¹å¼æ–¹é¢ä»å­˜åœ¨å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„å…ˆè¿›è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚</li>
<li>ç¥ç»ç§‘å­¦å¼€å§‹åˆ©ç”¨è¿™äº›æ¨¡å‹æ¢ç´¢è¯­è¨€ç†è§£è¿‡ç¨‹ä¸­çš„è®¤çŸ¥è¿‡ç¨‹ã€‚</li>
<li>åŸºäºTransformerçš„æ¨¡å‹ï¼ˆGPT2ã€LLaMA-7Bå’ŒLLaMA2-7Bï¼‰åœ¨è§£é‡Šé˜…è¯»æ—¶çš„æ³¨è§†æŒç»­æ—¶é—´æ–¹é¢çš„è¡¨ç°ä¼˜äºæ—©æœŸæ¨¡å‹ã€‚</li>
<li>è¿™äº›æ–°æ¨¡å‹èƒ½å¤Ÿéƒ¨åˆ†è§£é‡Šäººç±»é¢„æµ‹æ€§åœ¨é˜…è¯»è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚</li>
<li>å°½ç®¡è¿™äº›æ¨¡å‹æœ‰æ‰€è¿›å±•ï¼Œä½†å®ƒä»¬ä»æ— æ³•å®Œå…¨æ•æ‰äººç±»é˜…è¯»æ–¹å¼çš„å…¨éƒ¨ç»†èŠ‚ã€‚</li>
<li>æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹äººç±»é˜…è¯»æ–¹å¼æ–¹é¢å­˜åœ¨å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11485">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca089764c781ab0a7b590938d5f02345.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33d88ce45baddc72a1bd9c7fbd948542.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a0f0413d6d23dcea86a64807f3b6e2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbfd9594ebf7e073f8fdf46513226723.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HelpSteer3-Preference-Open-Human-Annotated-Preference-Data-across-Diverse-Tasks-and-Languages"><a href="#HelpSteer3-Preference-Open-Human-Annotated-Preference-Data-across-Diverse-Tasks-and-Languages" class="headerlink" title="HelpSteer3-Preference: Open Human-Annotated Preference Data across   Diverse Tasks and Languages"></a>HelpSteer3-Preference: Open Human-Annotated Preference Data across   Diverse Tasks and Languages</h2><p><strong>Authors:Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans, Yi Dong, Oleksii Kuchaiev</strong></p>
<p>Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/HelpSteer3#preference">https://huggingface.co/datasets/nvidia/HelpSteer3#preference</a> </p>
<blockquote>
<p>åå¥½æ•°æ®é›†å¯¹äºä½¿ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰è®­ç»ƒé€šç”¨é¢†åŸŸã€éµå¾ªæŒ‡ä»¤çš„è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚æ¯æ¬¡åç»­çš„æ•°æ®å‘å¸ƒéƒ½æé«˜äº†å¯¹æœªæ¥æ•°æ®æ”¶é›†çš„æœŸæœ›ï¼Œè¿™æ„å‘³ç€éœ€è¦ä¸æ–­æé«˜å…¬å¼€å¯ç”¨åå¥½æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HelpSteer3-Preferenceï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨CC-BY-4.0è®¸å¯ã€é«˜è´¨é‡ã€äººç±»æ³¨é‡Šçš„åå¥½æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡40,000ä¸ªæ ·æœ¬ã€‚è¿™äº›æ ·æœ¬æ¶µç›–äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ ·åŒ–ç°å®ä¸–ç•Œåº”ç”¨ï¼ŒåŒ…æ‹¬ä¸STEMã€ç¼–ç å’Œå¤šè¯­è¨€åœºæ™¯ç›¸å…³çš„ä»»åŠ¡ã€‚ä½¿ç”¨HelpSteer3-Preferenceï¼Œæˆ‘ä»¬è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨RM-Benchï¼ˆ82.4%ï¼‰å’ŒJudgeBenchï¼ˆ73.7%ï¼‰ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚è¿™ç›¸æ¯”ç°æœ‰RMçš„æœ€ä½³æŠ¥å‘Šç»“æœæœ‰äº†å®è´¨æ€§çš„æ”¹è¿›ï¼ˆç»å¯¹æé«˜äº†çº¦10%ï¼‰ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†HelpSteer3-Preferenceå¦‚ä½•ç”¨äºè®­ç»ƒç”Ÿæˆå¼RMï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„RMå°†æ”¿ç­–æ¨¡å‹ä¸RLHFå¯¹é½ã€‚æ•°æ®é›†ï¼ˆCC-BY-4.0ï¼‰ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/HelpSteer3#preference">https://huggingface.co/datasets/nvidia/HelpSteer3#preference</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11475v1">PDF</a> 38 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>å¸®åŠ©Steer3åå¥½æ•°æ®é›†å¯¹äºä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰è®­ç»ƒé€šç”¨é¢†åŸŸã€éµå¾ªæŒ‡ä»¤çš„è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚è¯¥æ•°æ®é›†æ˜¯è®¸å¯çš„ï¼ˆCC-BY-4.0ï¼‰ï¼Œé«˜è´¨é‡ï¼ŒåŒ…å«è¶…è¿‡40ï¼Œ000ä¸ªäººå·¥æ³¨é‡Šçš„åå¥½æ ·æœ¬ã€‚è¿™äº›æ ·æœ¬æ¶µç›–äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šç§ç°å®ä¸–ç•Œåº”ç”¨ï¼ŒåŒ…æ‹¬STEMã€ç¼–ç å’Œå¤šè¯­ç§ä»»åŠ¡ã€‚ä½¿ç”¨å¸®åŠ©Steer3åå¥½æ•°æ®é›†è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨RM-Benchå’ŒJudgeBenchä¸Šè¡¨ç°å“è¶Šï¼Œç›¸è¾ƒäºç°æœ‰RMè¾¾åˆ°ç»å¯¹æå‡çº¦10%ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•å°†å¸®åŠ©Steer3åå¥½æ•°æ®é›†åº”ç”¨äºè®­ç»ƒç”Ÿæˆå¼RMï¼Œä»¥åŠå¦‚ä½•é€šè¿‡å¯¹é½ç­–ç•¥æ¨¡å‹ä½¿ç”¨æˆ‘ä»¬çš„RMå®ç°RLHFã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸®åŠ©Steer3åå¥½æ•°æ®é›†æ˜¯è®­ç»ƒé€šç”¨é¢†åŸŸè¯­è¨€æ¨¡å‹çš„é‡è¦èµ„æºï¼Œå°¤å…¶åœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ—¶ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¶…è¿‡40ï¼Œ000ä¸ªäººå·¥æ³¨é‡Šçš„åå¥½æ ·æœ¬ï¼Œä¿è¯äº†æ•°æ®è´¨é‡ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬STEMã€ç¼–ç å’Œå¤šè¯­ç§ä»»åŠ¡ï¼Œä½“ç°äº†æ•°æ®çš„å¤šæ ·æ€§ã€‚</li>
<li>ä½¿ç”¨å¸®åŠ©Steer3åå¥½æ•°æ®é›†è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨RM-Benchå’ŒJudgeBenchä¸Šçš„è¡¨ç°æ˜¾è‘—ï¼Œç›¸è¾ƒäºç°æœ‰æ¨¡å‹æœ‰ç»å¯¹æå‡çº¦10%ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•å°†å¸®åŠ©Steer3åå¥½æ•°æ®é›†åº”ç”¨äºè®­ç»ƒç”Ÿæˆå¼RMã€‚</li>
<li>é€šè¿‡ç­–ç•¥æ¨¡å‹ä¸RMçš„å¯¹é½ï¼Œå®ç°äº†RLHFçš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ccb60b90803797e1276c09922790813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c5f0bb4a4ca8738632eafa4151dacc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-833675bcc9041121755a0dac9fce6afb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ProxyPrompt-Securing-System-Prompts-against-Prompt-Extraction-Attacks"><a href="#ProxyPrompt-Securing-System-Prompts-against-Prompt-Extraction-Attacks" class="headerlink" title="ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks"></a>ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks</h2><p><strong>Authors:Zhixiong Zhuang, Maria-Irina Nicolae, Hui-Po Wang, Mario Fritz</strong></p>
<p>The integration of large language models (LLMs) into a wide range of applications has highlighted the critical role of well-crafted system prompts, which require extensive testing and domain expertise. These prompts enhance task performance but may also encode sensitive information and filtering criteria, posing security risks if exposed. Recent research shows that system prompts are vulnerable to extraction attacks, while existing defenses are either easily bypassed or require constant updates to address new threats. In this work, we introduce ProxyPrompt, a novel defense mechanism that prevents prompt leakage by replacing the original prompt with a proxy. This proxy maintains the original taskâ€™s utility while obfuscating the extracted prompt, ensuring attackers cannot reproduce the task or access sensitive information. Comprehensive evaluations on 264 LLM and system prompt pairs show that ProxyPrompt protects 94.70% of prompts from extraction attacks, outperforming the next-best defense, which only achieves 42.80%. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°å„ç§åº”ç”¨ä¸­ï¼Œå‡¸æ˜¾äº†ç²¾å¿ƒè®¾è®¡çš„ç³»ç»Ÿæç¤ºçš„å…³é”®ä½œç”¨ï¼Œè¿™éœ€è¦å¹¿æ³›çš„æµ‹è¯•å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚è¿™äº›æç¤ºæé«˜äº†ä»»åŠ¡æ€§èƒ½ï¼Œä½†ä¹Ÿå¯èƒ½ç¼–ç æ•æ„Ÿä¿¡æ¯å’Œè¿‡æ»¤æ ‡å‡†ï¼Œå¦‚æœæš´éœ²å‡ºæ¥ä¼šæ„æˆå®‰å…¨é£é™©ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç³»ç»Ÿæç¤ºå®¹æ˜“å—åˆ°æå–æ”»å‡»ï¼Œè€Œç°æœ‰çš„é˜²å¾¡æ‰‹æ®µè¦ä¹ˆå®¹æ˜“è¢«ç»•è¿‡ï¼Œè¦ä¹ˆéœ€è¦ä¸æ–­æ›´æ–°ä»¥åº”å¯¹æ–°å¨èƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ProxyPromptï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é˜²å¾¡æœºåˆ¶ï¼Œé€šè¿‡ç”¨ä»£ç†æ›¿æ¢åŸå§‹æç¤ºæ¥é˜²æ­¢æç¤ºæ³„éœ²ã€‚è¯¥ä»£ç†ä¿æŒåŸå§‹ä»»åŠ¡çš„å®ç”¨æ€§ï¼ŒåŒæ—¶æ¨¡ç³Šæå–çš„æç¤ºï¼Œç¡®ä¿æ”»å‡»è€…æ— æ³•å¤åˆ¶ä»»åŠ¡æˆ–è®¿é—®æ•æ„Ÿä¿¡æ¯ã€‚å¯¹264ä¸ªLLMå’Œç³»ç»Ÿæç¤ºå¯¹çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒProxyPromptåœ¨é˜²æ­¢æå–æ”»å‡»æ–¹é¢ä¿æŠ¤äº†94.70%çš„æç¤ºï¼Œä¼˜äºæ¬¡ä¹‹çš„æœ€ä½³é˜²å¾¡æ‰‹æ®µï¼ˆä»…è¾¾åˆ°42.80%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11459v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåº”ç”¨å‡¸æ˜¾ç²¾å¿ƒè®¾è®¡çš„ç³»ç»Ÿæç¤ºé‡è¦æ€§ï¼Œéœ€æµ‹è¯•ä¸é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚ç³»ç»Ÿæç¤ºæå‡ä»»åŠ¡æ€§èƒ½ï¼Œä½†å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯åŠç­›é€‰æ ‡å‡†ï¼Œæ³„éœ²å­˜åœ¨å®‰å…¨é£é™©ã€‚æœ€æ–°ç ”ç©¶æ˜¾ç¤ºç³»ç»Ÿæç¤ºæ˜“å—æå–æ”»å‡»ï¼Œç°æœ‰é˜²å¾¡æ‰‹æ®µæ˜“å¤±æ•ˆæˆ–éœ€é¢‘ç¹æ›´æ–°ã€‚æœ¬ç ”ç©¶ä»‹ç»ProxyPromptï¼Œä¸€ç§æ–°å‹é˜²å¾¡æœºåˆ¶ï¼Œé€šè¿‡æ›¿æ¢åŸå§‹æç¤ºæ¥é˜²æ­¢æç¤ºæ³„éœ²ã€‚ProxyPromptä¿æŒä»»åŠ¡å®ç”¨æ€§åŒæ—¶æ©ç›–æå–çš„æç¤ºï¼Œç¡®ä¿æ”»å‡»è€…æ— æ³•å¤åˆ¶ä»»åŠ¡æˆ–è®¿é—®æ•æ„Ÿä¿¡æ¯ã€‚å¯¹264ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œç³»ç»Ÿæç¤ºå¯¹çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼ŒProxyPromptä¿æŠ¤94.7%çš„æç¤ºå…å—æå–æ”»å‡»ï¼Œä¼˜äºç°æœ‰æœ€ä½³é˜²å¾¡æ‰‹æ®µçš„42.8%ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåº”ç”¨éœ€è¦é‡è§†ç³»ç»Ÿæç¤ºçš„è®¾è®¡å’Œæµ‹è¯•ã€‚</li>
<li>ç³»ç»Ÿæç¤ºåœ¨æå‡ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œå­˜åœ¨å®‰å…¨é£é™©ã€‚</li>
<li>æœ€æ–°ç ”ç©¶æ˜¾ç¤ºç³»ç»Ÿæç¤ºå®¹æ˜“å—åˆ°æå–æ”»å‡»ã€‚</li>
<li>ç°æœ‰é˜²å¾¡æ‰‹æ®µå­˜åœ¨ç¼ºé™·ï¼Œæ˜“å¤±æ•ˆæˆ–éœ€è¦é¢‘ç¹æ›´æ–°ã€‚</li>
<li>ProxyPromptæ˜¯ä¸€ç§æ–°å‹é˜²å¾¡æœºåˆ¶ï¼Œé€šè¿‡æ›¿æ¢åŸå§‹æç¤ºæ¥é˜²æ­¢æç¤ºæ³„éœ²ã€‚</li>
<li>ProxyPromptèƒ½æœ‰æ•ˆä¿æŠ¤å¤§éƒ¨åˆ†æç¤ºå…å—æå–æ”»å‡»ï¼Œæ•ˆæœä¼˜äºç°æœ‰æœ€ä½³é˜²å¾¡æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc4e25c004065576b42267f65a531077.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f991d3f482900b517f262fc16542b4d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd38c04e8b5d4a06da8eaf362f0802fb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLMs-unlock-new-paths-to-monetizing-exploits"><a href="#LLMs-unlock-new-paths-to-monetizing-exploits" class="headerlink" title="LLMs unlock new paths to monetizing exploits"></a>LLMs unlock new paths to monetizing exploits</h2><p><strong>Authors:Nicholas Carlini, Milad Nasr, Edoardo Debenedetti, Barry Wang, Christopher A. Choquette-Choo, Daphne Ippolito, Florian TramÃ¨r, Matthew Jagielski</strong></p>
<p>We argue that Large language models (LLMs) will soon alter the economics of cyberattacks. Instead of attacking the most commonly used software and monetizing exploits by targeting the lowest common denominator among victims, LLMs enable adversaries to launch tailored attacks on a user-by-user basis. On the exploitation front, instead of human attackers manually searching for one difficult-to-identify bug in a product with millions of users, LLMs can find thousands of easy-to-identify bugs in products with thousands of users. And on the monetization front, instead of generic ransomware that always performs the same attack (encrypt all your data and request payment to decrypt), an LLM-driven ransomware attack could tailor the ransom demand based on the particular content of each exploited device.   We show that these two attacks (and several others) are imminently practical using state-of-the-art LLMs. For example, we show that without any human intervention, an LLM finds highly sensitive personal information in the Enron email dataset (e.g., an executive having an affair with another employee) that could be used for blackmail. While some of our attacks are still too expensive to scale widely today, the incentives to implement these attacks will only increase as LLMs get cheaper. Thus, we argue that LLMs create a need for new defense-in-depth approaches. </p>
<blockquote>
<p>æˆ‘ä»¬è®¤ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†å¾ˆå¿«æ”¹å˜ç½‘ç»œæ”»å‡»çš„ç»æµæ¨¡å¼ã€‚LLMä½¿å¯¹æ‰‹èƒ½å¤Ÿé’ˆå¯¹æ¯ä¸ªç”¨æˆ·å‘èµ·å®šåˆ¶æ”»å‡»ï¼Œè€Œä¸æ˜¯æ”»å‡»æœ€å¸¸ç”¨çš„è½¯ä»¶å¹¶é€šè¿‡å¯¹å—å®³è€…ä¸­çš„æœ€ä½å…¬åˆ†æ¯è¿›è¡Œå®šä½æ¥åˆ©ç”¨æ¼æ´å®ç°ç›ˆåˆ©ã€‚åœ¨åˆ©ç”¨æ¼æ´æ–¹é¢ï¼Œåˆ©ç”¨äººç±»æ”»å‡»è€…æ‰‹åŠ¨åœ¨æ‹¥æœ‰æ•°ç™¾ä¸‡ç”¨æˆ·çš„äº§å“ä¸­å¯»æ‰¾éš¾ä»¥è¯†åˆ«çš„æ¼æ´ï¼Œè€ŒLLMå¯ä»¥åœ¨æ‹¥æœ‰æ•°åƒç”¨æˆ·çš„äº§å“ä¸­æ‰¾åˆ°æˆåƒä¸Šä¸‡å®¹æ˜“è¯†åˆ«çš„æ¼æ´ã€‚åœ¨ç›ˆåˆ©æ–¹é¢ï¼Œä¸å§‹ç»ˆæ‰§è¡Œç›¸åŒæ”»å‡»çš„é€šç”¨å‹’ç´¢è½¯ä»¶ç›¸æ¯”ï¼Œç”±LLMé©±åŠ¨çš„å‹’ç´¢è½¯ä»¶æ”»å‡»å¯ä»¥æ ¹æ®æ¯ä¸ªå—æ”»å‡»è®¾å¤‡çš„ç‰¹å®šå†…å®¹å®šåˆ¶èµé‡‘è¦æ±‚ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ä¸¤ç§æ”»å‡»ï¼ˆä»¥åŠå…¶ä»–å‡ ç§æ”»å‡»ï¼‰éƒ½æ˜¯å³å°†å‘ç”Ÿå¹¶ä¸”æ˜¯å®ç”¨çš„ï¼Œé‡‡ç”¨çš„éƒ½æ˜¯æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨æ²¡æœ‰äººç±»å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨Enronç”µå­é‚®ä»¶æ•°æ®é›†ä¸­æ‰¾åˆ°é«˜åº¦æ•æ„Ÿçš„ä¸ªäººä¿¡æ¯ï¼ˆä¾‹å¦‚é«˜ç®¡ä¸å¦ä¸€å‘˜å·¥å‘ç”Ÿå¤–é‡ï¼‰ï¼Œè¿™äº›ä¿¡æ¯å¯è¢«ç”¨äºæ•²è¯ˆå‹’ç´¢ã€‚è™½ç„¶æˆ‘ä»¬ä»Šå¤©å®æ–½çš„ä¸€äº›æ”»å‡»ä»ç„¶æˆæœ¬è¿‡é«˜ï¼Œéš¾ä»¥å¤§è§„æ¨¡è¿›è¡Œï¼Œä½†éšç€å¤§å‹è¯­è¨€æ¨¡å‹æˆæœ¬çš„ä¸æ–­ä¸‹é™ï¼Œå®æ–½è¿™äº›æ”»å‡»çš„æ¿€åŠ±åªä¼šå¢åŠ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦é‡‡ç”¨æ–°çš„æ·±åº¦é˜²å¾¡æ–¹æ³•æ¥è¿›è¡Œé˜²å¾¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11449v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°†æ”¹å˜ç½‘ç»œæ”»å‡»çš„ç»æµæ¨¡å¼ã€‚LLMsä½¿å¯¹æ‰‹èƒ½å¤Ÿé’ˆå¯¹æ¯ä¸ªç”¨æˆ·å‘èµ·å®šåˆ¶æ”»å‡»ï¼Œæ”¹å˜ä¼ ç»Ÿçš„æ”»å‡»æœ€å¸¸è§è½¯ä»¶å¹¶é€šè¿‡é’ˆå¯¹å—å®³è€…æœ€ä½å…¬å› æ•°æ¥ç›ˆåˆ©çš„æ¨¡å¼ã€‚åœ¨æ”»å‡»æ–¹é¢ï¼ŒLLMsèƒ½å¤Ÿæ‰¾åˆ°æˆåƒä¸Šä¸‡å®¹æ˜“è¯†åˆ«çš„ç”¨æˆ·äº§å“æ¼æ´ï¼Œè€Œæ— éœ€äººä¸ºæ”»å‡»è€…æ‰‹åŠ¨æœç´¢éš¾ä»¥è¯†åˆ«çš„äº§å“æ¼æ´ã€‚åœ¨ç›ˆåˆ©æ–¹é¢ï¼ŒåŸºäºLLMçš„å‹’ç´¢è½¯ä»¶æ”»å‡»å¯ä»¥æ ¹æ®æ¯ä¸ªå—æ”»å‡»è®¾å¤‡çš„ç‰¹å®šå†…å®¹å®šåˆ¶å‹’ç´¢è¦æ±‚ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ä¸¤ç§æ”»å‡»ï¼ˆä»¥åŠå…¶ä»–å‡ ç§æ”»å‡»ï¼‰ä½¿ç”¨æœ€æ–°LLMæ˜¯å³åˆ»å¯è¡Œçš„ã€‚è™½ç„¶ç›®å‰æœ‰äº›æ”»å‡»æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥å¹¿æ³›å®æ–½ï¼Œä½†éšç€LLMæˆæœ¬çš„é™ä½ï¼Œå®æ–½è¿™äº›æ”»å‡»çš„æ¿€åŠ±æªæ–½åªä¼šå¢åŠ ã€‚å› æ­¤ï¼ŒLLMsçš„å‡ºç°éœ€è¦æ–°çš„æ·±åº¦é˜²å¾¡æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså°†æ”¹å˜ç½‘ç»œæ”»å‡»çš„ç»æµæ¨¡å¼ï¼Œä»æ”»å‡»æœ€å¸¸è§è½¯ä»¶è½¬å˜ä¸ºé’ˆå¯¹æ¯ä¸ªç”¨æˆ·å®šåˆ¶çš„æ”»å‡»æ–¹å¼ã€‚</li>
<li>LLMsèƒ½å¤Ÿå¿«é€Ÿæ‰¾åˆ°äº§å“ä¸­çš„æ¼æ´ï¼Œæé«˜æ”»å‡»æ•ˆç‡ã€‚</li>
<li>åŸºäºLLMçš„å‹’ç´¢è½¯ä»¶å¯ä»¥æ ¹æ®å—å®³è€…çš„å…·ä½“ä¿¡æ¯å®šåˆ¶å‹’ç´¢è¦æ±‚ã€‚</li>
<li>å½“å‰ä¸€äº›é’ˆå¯¹LLMçš„ç½‘ç»œæ”»å‡»æˆæœ¬é«˜æ˜‚ï¼Œéš¾ä»¥å¹¿æ³›å®æ–½ã€‚</li>
<li>éšç€LLMæˆæœ¬çš„é™ä½ï¼Œå®æ–½ç½‘ç»œæ”»å‡»çš„æ¿€åŠ±å°†å¢åŠ ã€‚</li>
<li>LLMsçš„å‡ºç°éœ€è¦æ–°çš„æ·±åº¦é˜²å¾¡ç­–ç•¥æ¥åº”å¯¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4d89a6b0010e4ff23d3a6c34882f12dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9ed47913f142a440fdc422dc0768e11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75dccf0d417547155569e8143ba9aa43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aaec4ce9d72b0bb79acdea78b3ac49aa.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GODBench-A-Benchmark-for-Multimodal-Large-Language-Models-in-Video-Comment-Art"><a href="#GODBench-A-Benchmark-for-Multimodal-Large-Language-Models-in-Video-Comment-Art" class="headerlink" title="GODBench: A Benchmark for Multimodal Large Language Models in Video   Comment Art"></a>GODBench: A Benchmark for Multimodal Large Language Models in Video   Comment Art</h2><p><strong>Authors:Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang</strong></p>
<p>Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMsâ€™ abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at <a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025">https://github.com/stan-lei/GODBench-ACL2025</a>. </p>
<blockquote>
<p>è§†é¢‘è¯„è®ºè‰ºæœ¯é€šè¿‡æä¾›ä¼ é€’å¹½é»˜ã€è®½åˆºæˆ–æƒ…æ„Ÿå…±é¸£çš„åˆ›æ„å†…å®¹ï¼Œå¢å¼ºäº†ç”¨æˆ·å‚ä¸åº¦ï¼Œè¿™è¦æ±‚å¾®å¦™è€Œå…¨é¢åœ°æŒæ¡æ–‡åŒ–å’Œä¸Šä¸‹æ–‡ç»†å¾®å·®åˆ«ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰å·²åœ¨STEMä»»åŠ¡ï¼ˆä¾‹å¦‚æ•°å­¦å’Œç¼–ç ï¼‰ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥ç”Ÿæˆè¯¸å¦‚å¼•äººå…±é¸£çš„ç¬‘è¯å’Œå¯Œæœ‰æ´å¯ŸåŠ›çš„è®½åˆºç­‰åˆ›æ„è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å—é™äºå…¶æœ‰é™çš„æ¨¡æ€å’Œä¸è¶³çš„ç±»åˆ«ï¼Œé˜»ç¢äº†åŸºäºè§†é¢‘çš„è¯„è®ºè‰ºæœ¯åˆ›ä½œçš„å…¨é¢åˆ›é€ åŠ›çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GODBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå°†è§†é¢‘å’Œæ–‡æœ¬æ¨¡æ€ç»“åˆåœ¨ä¸€èµ·çš„å…¨æ–°åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥ç³»ç»Ÿåœ°è¯„ä¼°MLLMsåˆ›ä½œè¯„è®ºè‰ºæœ¯çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå—ç‰©ç†å­¦ä¸­æ³¢çš„ä¼ æ’­æ¨¡å¼çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ€ç»´æ¶Ÿæ¼ªï¼ˆRoTï¼‰è¿™ä¸€å¤šæ­¥éª¤æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºMLLMçš„åˆ›é€ åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„MLLMå’ŒCoTæ–¹æ³•åœ¨ç†è§£å’Œç”Ÿæˆåˆ›é€ æ€§è§†é¢‘è¯„è®ºæ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRoTæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥æ”¹å–„åˆ›é€ æ€§æ„æˆï¼Œçªå‡ºäº†å…¶åœ¨æ¨åŠ¨åŸºäºMLLMçš„åˆ›é€ åŠ›çš„æœ‰æ„ä¹‰çš„è¿›æ­¥æ–¹é¢çš„æ½œåŠ›ã€‚GODBenchå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/stan-lei/GODBench-ACL2025ä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11436v1">PDF</a> 69 pages, 66 figures, accepted by ACL 2025</p>
<p><strong>æ‘˜è¦</strong><br>è§†é¢‘è¯„è®ºè‰ºæœ¯é€šè¿‡æä¾›ä¼ è¾¾å¹½é»˜ã€è®½åˆºæˆ–æƒ…æ„Ÿå…±é¸£çš„åˆ›æ„å†…å®¹ï¼Œå¢å¼ºäº†ç”¨æˆ·å‚ä¸åº¦ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰åœ¨STEMä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç ï¼‰ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»éš¾ä»¥ç”Ÿæˆå¦‚åŠ¨äººç¬‘è¯å’Œæ·±åˆ»è®½åˆºç­‰åˆ›æ„è¡¨è¾¾ã€‚ä¸ºè§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è§†é¢‘è¯„è®ºè‰ºæœ¯åˆ›ä½œæ–¹é¢çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºGODBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ•´åˆè§†é¢‘å’Œæ–‡æœ¬æ¨¡æ€æ¥ç³»ç»Ÿè¯„ä¼°MLLMsåˆ›ä½œè¯„è®ºè‰ºæœ¯çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå—ç‰©ç†ä¸­æ³¢åŠ¨ä¼ æ’­æ¨¡å¼çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºRipple of Thoughtï¼ˆRoTï¼‰å¤šæ­¥æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºMLLMsçš„åˆ›é€ åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰MLLMså’ŒCoTæ–¹æ³•åœ¨ç†è§£å’Œç”Ÿæˆåˆ›æ„è§†é¢‘è¯„è®ºæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRoTæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æé«˜åˆ›æ„å†™ä½œçš„æ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨æ¨åŠ¨åŸºäºMLLMçš„åˆ›é€ åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚GODBenchå·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/stan-lei/GODBench-ACL2025å…¬å¼€å¯ç”¨ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘è¯„è®ºè‰ºæœ¯é€šè¿‡åˆ›æ„å†…å®¹å¢å¼ºç”¨æˆ·å‚ä¸åº¦ï¼Œéœ€è¦å…¨é¢ç†è§£æ–‡åŒ–å’Œè¯­å¢ƒç»†å¾®å·®åˆ«ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨STEMä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆåˆ›æ„è¡¨è¾¾æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è§†é¢‘è¯„è®ºè‰ºæœ¯åˆ›ä½œæ–¹é¢çš„è¯„ä¼°å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹ç»¼åˆæ€§å’Œè¶³å¤Ÿçš„ç±»åˆ«å¤šæ ·æ€§ã€‚</li>
<li>æ¨å‡ºGODBenchåŸºå‡†æµ‹è¯•ï¼Œæ•´åˆè§†é¢‘å’Œæ–‡æœ¬æ¨¡æ€è¯„ä¼°MLLMsåˆ›ä½œè¯„è®ºè‰ºæœ¯çš„èƒ½åŠ›ã€‚</li>
<li>å—ç‰©ç†æ³¢åŠ¨ä¼ æ’­æ¨¡å¼å¯å‘ï¼Œæå‡ºRipple of Thoughtï¼ˆRoTï¼‰å¤šæ­¥æ¨ç†æ¡†æ¶ï¼Œå¢å¼ºMLLMsçš„åˆ›é€ åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºç°æœ‰MLLMså’ŒCoTæ–¹æ³•åœ¨ç†è§£å’Œç”Ÿæˆåˆ›æ„è§†é¢‘è¯„è®ºæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>RoTæ–¹æ³•æœ‰æ•ˆæé«˜äº†åˆ›æ„å†™ä½œèƒ½åŠ›ï¼Œçªæ˜¾å…¶åœ¨æ¨åŠ¨åŸºäºMLLMçš„åˆ›é€ åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad8734da6667e5e9e4115b0aeb767978.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-716e21106fd9c6f5ccd5b15c0537ccf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c24b0d34eff9882e6b379088fbcd1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f784b33ac157a05a7f830f7d0aba1e6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e58fc377c680b0d0c4374eecb840f3a5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="When-Thinking-Fails-The-Pitfalls-of-Reasoning-for-Instruction-Following-in-LLMs"><a href="#When-Thinking-Fails-The-Pitfalls-of-Reasoning-for-Instruction-Following-in-LLMs" class="headerlink" title="When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following   in LLMs"></a>When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following   in LLMs</h2><p><strong>Authors:Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal</strong></p>
<p>Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies. </p>
<blockquote>
<p>æ¨ç†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼‰æ— è®ºæ˜¯å¦ç»è¿‡æ˜ç¡®çš„æ¨ç†è®­ç»ƒæˆ–é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰è¿›è¡Œæç¤ºï¼Œå·²åœ¨è®¸å¤šå¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªä»¤äººæƒŠè®¶ä¸”ä»¥å‰è¢«å¿½è§†çš„ç°è±¡ï¼šæ˜ç¡®çš„CoTæ¨ç†ä¼šæ˜¾è‘—é™ä½éµå¾ªæŒ‡ä»¤çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ï¼šIFEvalï¼ˆå…·æœ‰ç®€å•ã€å¯éªŒè¯çš„è§„åˆ™çº¦æŸï¼‰å’ŒComplexBenchï¼ˆå…·æœ‰å¤æ‚ã€ç»„åˆçº¦æŸï¼‰ä¸Šè¯„ä¼°äº†15ä¸ªæ¨¡å‹ï¼Œå§‹ç»ˆè§‚å¯Ÿåˆ°åº”ç”¨CoTæç¤ºæ—¶æ€§èƒ½ä¸‹é™ã€‚é€šè¿‡å¤§è§„æ¨¡æ¡ˆä¾‹ç ”ç©¶å’ŒåŸºäºæ³¨æ„åŠ›çš„åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæ¨ç†æœ‰åŠ©äºï¼ˆä¾‹å¦‚ï¼Œåœ¨æ ¼å¼æˆ–è¯æ±‡ç²¾åº¦æ–¹é¢ï¼‰æˆ–æœ‰å®³ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡å¿½ç•¥ç®€å•çº¦æŸæˆ–å¼•å…¥ä¸å¿…è¦å†…å®¹ï¼‰çš„å¸¸è§æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæŒ‡æ ‡â€”â€”çº¦æŸæ³¨æ„åŠ›ï¼Œæ¥é‡åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¨¡å‹å…³æ³¨ç‚¹ï¼Œå¹¶è¡¨æ˜CoTæ¨ç†é€šå¸¸ä¼šåˆ†æ•£å¯¹æŒ‡ä»¤ç›¸å…³æ ‡è®°çš„æ³¨æ„åŠ›ã€‚ä¸ºäº†å‡è½»è¿™äº›å½±å“ï¼Œæˆ‘ä»¬å¼•å…¥å¹¶è¯„ä¼°äº†å››ç§ç­–ç•¥ï¼šä¸Šä¸‹æ–‡å­¦ä¹ ã€è‡ªæˆ‘åæ€ã€è‡ªæˆ‘é€‰æ‹©æ€§æ¨ç†å’Œåˆ†ç±»å™¨é€‰æ‹©æ€§æ¨ç†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€‰æ‹©æ€§æ¨ç†ç­–ç•¥ï¼Œå°¤å…¶æ˜¯åˆ†ç±»å™¨é€‰æ‹©æ€§æ¨ç†ï¼Œå¯ä»¥å¤§å¹…åº¦æ¢å¤ä¸¢å¤±çš„æ€§èƒ½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹ç³»ç»Ÿæ€§åœ°æ­ç¤ºæ¨ç†å¯¼è‡´çš„æŒ‡ä»¤éµå¾ªå¤±è´¥å¹¶æä¾›å®ç”¨ç¼“è§£ç­–ç•¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11423v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæ˜¾æ€§çš„é“¾å¼æ€ç»´æ¨ç†ä¼šåœ¨éµå¾ªæŒ‡ä»¤æ—¶æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡åœ¨ä¸€ç³»åˆ—æ¨¡å‹å’Œä¸¤ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆå…·æœ‰ç®€å•å¯éªŒè¯çº¦æŸçš„IFEvalå’Œå…·æœ‰å¤æ‚ç»„åˆçº¦æŸçš„ComplexBenchï¼‰ä¸Šçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°å½“åº”ç”¨é“¾å¼æ€ç»´æ¨ç†æ—¶ï¼Œæ¨¡å‹æ€§èƒ½å§‹ç»ˆå‡ºç°ä¸‹é™ã€‚ç ”ç©¶å‘ç°å››ç§ç¼“è§£ç­–ç•¥èƒ½æœ‰æ•ˆå‡è½»è¿™ç§æƒ…å†µã€‚å…¶ä¸­é€‰æ‹©æ€§æ¨ç†ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åˆ†ç±»å™¨é€‰æ‹©æ€§æ¨ç†ï¼Œèƒ½æ˜¾è‘—æ¢å¤æ€§èƒ½æŸå¤±ã€‚è¿™æ˜¯é¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ­ç¤ºæ¨ç†å¯¼è‡´çš„æŒ‡ä»¤éµå¾ªå¤±è´¥å¹¶æå‡ºå®ç”¨ç¼“è§£ç­–ç•¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ˜¾å¼é“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTï¼‰åœ¨éµå¾ªæŒ‡ä»¤æ—¶ä¼šå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚</li>
<li>åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œåº”ç”¨CoTæ¨ç†ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡æ¡ˆä¾‹ç ”ç©¶å’ŒåŸºäºæ³¨æ„åŠ›çš„åˆ†æï¼Œå‘ç°æ¨ç†åœ¨å¸®åŠ©å’Œä¼¤å®³æ¨¡å‹æ€§èƒ½æ–¹é¢çš„å¸¸è§æ¨¡å¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é‡åŒ–æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ä¸­å…³æ³¨åº¦çš„æŒ‡æ ‡â€”â€”çº¦æŸæ³¨æ„åŠ›ï¼Œå‘ç°CoTæ¨ç†å¸¸å¸¸ä½¿æ¨¡å‹åç¦»æŒ‡ä»¤ç›¸å…³çš„æ ‡è®°ã€‚</li>
<li>ä»‹ç»äº†å››ç§ç¼“è§£ç­–ç•¥æ¥å‡è½»æ¨ç†å¯¹æŒ‡ä»¤éµå¾ªçš„å½±å“ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡å­¦ä¹ ã€è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘é€‰æ‹©æ€§æ¨ç†ç­‰ã€‚</li>
<li>é€‰æ‹©æ€§æ¨ç†ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åˆ†ç±»å™¨é€‰æ‹©æ€§æ¨ç†ï¼Œèƒ½æœ‰æ•ˆæ¢å¤å› æ¨ç†å¯¼è‡´çš„æ€§èƒ½æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-458d5262f1a26b34ee993d185adf5b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5911c56e30adf0b0e1c76535f2292864.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-450d0f386565be5c00036bc98f39f76b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="EdgeWisePersona-A-Dataset-for-On-Device-User-Profiling-from-Natural-Language-Interactions"><a href="#EdgeWisePersona-A-Dataset-for-On-Device-User-Profiling-from-Natural-Language-Interactions" class="headerlink" title="EdgeWisePersona: A Dataset for On-Device User Profiling from Natural   Language Interactions"></a>EdgeWisePersona: A Dataset for On-Device User Profiling from Natural   Language Interactions</h2><p><strong>Authors:Patryk Bartkowiak, Michal Podstawski</strong></p>
<p>This paper introduces a novel dataset and evaluation benchmark designed to assess and improve small language models deployable on edge devices, with a focus on user profiling from multi-session natural language interactions in smart home environments. At the core of the dataset are structured user profiles, each defined by a set of routines - context-triggered, repeatable patterns of behavior that govern how users interact with their home systems. Using these profiles as input, a large language model (LLM) generates corresponding interaction sessions that simulate realistic, diverse, and context-aware dialogues between users and their devices.   The primary task supported by this dataset is profile reconstruction: inferring user routines and preferences solely from interactions history. To assess how well current models can perform this task under realistic conditions, we benchmarked several state-of-the-art compact language models and compared their performance against large foundation models. Our results show that while small models demonstrate some capability in reconstructing profiles, they still fall significantly short of large models in accurately capturing user behavior. This performance gap poses a major challenge - particularly because on-device processing offers critical advantages, such as preserving user privacy, minimizing latency, and enabling personalized experiences without reliance on the cloud. By providing a realistic, structured testbed for developing and evaluating behavioral modeling under these constraints, our dataset represents a key step toward enabling intelligent, privacy-respecting AI systems that learn and adapt directly on user-owned devices. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæé«˜å¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²çš„å°å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒé‡ç‚¹å…³æ³¨æ™ºèƒ½å®¶åº­ç¯å¢ƒä¸­å¤šä¼šè¯è‡ªç„¶è¯­è¨€äº¤äº’çš„ç”¨æˆ·ç”»åƒæ„å»ºã€‚æ•°æ®é›†çš„æ ¸å¿ƒæ˜¯ç»“æ„åŒ–ç”¨æˆ·ç”»åƒï¼Œæ¯ä¸ªç”¨æˆ·ç”»åƒç”±ä¸€ç»„ä¾‹è¡Œç¨‹åºå®šä¹‰â€”â€”ä¾‹è¡Œç¨‹åºæ˜¯ç”±ä¸Šä¸‹æ–‡è§¦å‘çš„ã€å¯é‡å¤çš„è¡Œä¸ºæ¨¡å¼ï¼Œå†³å®šäº†ç”¨æˆ·å¦‚ä½•ä¸å®¶åº­ç³»ç»Ÿè¿›è¡Œäº¤äº’ã€‚ä½¿ç”¨è¿™äº›ç”¨æˆ·ç”»åƒä½œä¸ºè¾“å…¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç›¸åº”çš„äº¤äº’ä¼šè¯ï¼Œæ¨¡æ‹Ÿç”¨æˆ·ä¸è®¾å¤‡ä¹‹é—´ç°å®ã€å¤šæ ·åŒ–å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹è¯ã€‚è¯¥æ•°æ®é›†æ”¯æŒçš„ä¸»è¦ä»»åŠ¡æ˜¯ç”»åƒé‡å»ºï¼šä»…æ ¹æ®äº¤äº’å†å²æ¥æ¨æ–­ç”¨æˆ·ä¾‹è¡Œç¨‹å’Œåå¥½ã€‚ä¸ºäº†è¯„ä¼°å½“å‰æ¨¡å‹åœ¨çœŸå®æ¡ä»¶ä¸‹æ‰§è¡Œæ­¤ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¯¹å¤šä¸ªæœ€æ–°ç´§å‡‘è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å°†å…¶æ€§èƒ½ä¸å¤§å‹åŸºç¡€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å°å‹æ¨¡å‹åœ¨é‡å»ºç”»åƒæ–¹é¢è¡¨ç°å‡ºä¸€äº›èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»è¿œè¿œä¸èƒ½å‡†ç¡®åœ°æ•æ‰ç”¨æˆ·è¡Œä¸ºã€‚è¿™ç§æ€§èƒ½å·®è·æ„æˆäº†ä¸€å¤§æŒ‘æˆ˜â€”â€”å°¤å…¶æ˜¯ç”±äºè®¾å¤‡ç«¯å¤„ç†æä¾›äº†å…³é”®ä¼˜åŠ¿ï¼Œå¦‚ä¿æŠ¤ç”¨æˆ·éšç§ã€æœ€å°åŒ–å»¶è¿Ÿå’Œå®ç°æ— éœ€ä¾èµ–äº‘çš„ä¸ªäººåŒ–ä½“éªŒã€‚é€šè¿‡ä¸ºåœ¨è¿™äº›çº¦æŸä¸‹å¼€å‘å’Œè¯„ä¼°è¡Œä¸ºå»ºæ¨¡æä¾›ä¸€ä¸ªç°å®ã€ç»“æ„åŒ–æµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æ˜¯å®ç°åœ¨ç”¨æˆ·æ‹¥æœ‰çš„è®¾å¤‡ä¸Šç›´æ¥å­¦ä¹ å¹¶è‡ªé€‚åº”çš„æ™ºèƒ½ã€å°Šé‡éšç§çš„AIç³»ç»Ÿçš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11417v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è‰¯å¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²çš„å°å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥æ•°æ®é›†ä»¥ç”¨æˆ·ç”»åƒä¸ºæ ¸å¿ƒï¼Œé€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·ä¸æ™ºèƒ½å®¶åº­ç³»ç»Ÿçš„å¤šä¼šè¯è‡ªç„¶è¯­è¨€äº¤äº’ï¼Œç”Ÿæˆç›¸åº”çš„äº¤äº’ä¼šè¯ã€‚ä¸»è¦ä»»åŠ¡æ˜¯é€šè¿‡ç”¨æˆ·äº¤äº’å†å²æ¥æ¨æ–­ç”¨æˆ·çš„è¡Œä¸ºæ¨¡å¼å’Œåå¥½è®¾ç½®ã€‚é€šè¿‡å¯¹æ¯”å½“å‰å°å‹æ¨¡å‹ä¸å¤§å‹åŸºç¡€æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ï¼Œå‘ç°å°å‹æ¨¡å‹åœ¨é‡å»ºç”¨æˆ·ç”»åƒæ–¹é¢å­˜åœ¨ä¸€å®šèƒ½åŠ›ï¼Œä½†åœ¨å‡†ç¡®æ•æ‰ç”¨æˆ·è¡Œä¸ºæ–¹é¢ä»ä¸å¤§å‹æ¨¡å‹å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¿™ä¸€æ€§èƒ½å·®è·å¯¹äºåœ¨è®¾å¤‡ä¸Šç›´æ¥å¤„ç†å…·æœ‰ä¿æŠ¤ç”¨æˆ·éšç§ã€å‡å°‘å»¶è¿Ÿå’Œä¾èµ–ä¸ªæ€§åŒ–ä½“éªŒç­‰å…³é”®ä¼˜åŠ¿çš„ç³»ç»Ÿæ„æˆé‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æ‰€æä¾›çš„æ•°æ®é›†ä¸ºåœ¨è¿™äº›é™åˆ¶æ¡ä»¶ä¸‹å¼€å‘å’Œè¯„ä¼°è¡Œä¸ºå»ºæ¨¡æä¾›äº†ä¸€ä¸ªçœŸå®ã€ç»“æ„åŒ–çš„æµ‹è¯•å¹³å°ï¼Œæœç€å®ç°åœ¨ç”¨æˆ·æ‹¥æœ‰çš„è®¾å¤‡ä¸Šç›´æ¥è¿›è¡Œæ™ºèƒ½ã€å°Šé‡éšç§çš„AIç³»ç»Ÿçš„ç›®æ ‡è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹è¾¹ç¼˜è®¾å¤‡ä¸Šå°å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ•°æ®é›†ä»¥ç”¨æˆ·ç”»åƒä¸ºæ ¸å¿ƒï¼Œé€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·åœ¨æ™ºèƒ½å®¶åº­ç¯å¢ƒä¸­çš„å¤šä¼šè¯è‡ªç„¶è¯­è¨€äº¤äº’æ¥ç”Ÿæˆæ•°æ®ã€‚</li>
<li>ä¸»è¦ä»»åŠ¡æ˜¯é‡å»ºç”¨æˆ·ç”»åƒï¼Œå³åŸºäºç”¨æˆ·äº¤äº’å†å²æ¨æ–­å…¶æ—¥å¸¸è¡Œä¸ºå’Œåå¥½è®¾ç½®ã€‚</li>
<li>å¯¹æ¯”äº†å°å‹æ¨¡å‹å’Œå¤§å‹åŸºç¡€æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°å°å‹æ¨¡å‹åœ¨æ•æ‰ç”¨æˆ·è¡Œä¸ºæ–¹é¢ä»æœ‰æ˜¾è‘—å·®è·ã€‚</li>
<li>å°å‹æ¨¡å‹ä¸å¤§å‹æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·å¯¹åœ¨è®¾å¤‡ä¸Šç›´æ¥å¤„ç†ç³»ç»Ÿæ„æˆæŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›ç³»ç»Ÿéœ€è¦ä¿æŠ¤ç”¨æˆ·éšç§ã€å‡å°‘å»¶è¿Ÿå¹¶ä¾èµ–ä¸ªæ€§åŒ–ä½“éªŒã€‚</li>
<li>æ‰€æä¾›çš„æ•°æ®é›†ä¸ºåœ¨é™åˆ¶æ¡ä»¶ä¸‹å¼€å‘å’Œè¯„ä¼°è¡Œä¸ºå»ºæ¨¡æä¾›äº†ä¸€ä¸ªçœŸå®ã€ç»“æ„åŒ–çš„æµ‹è¯•å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11417">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c930f059aab213bb8ef2cd146604cc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-830ea07d0f583eb6fb728eba64133d89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4ef57d7a66bb69a5eac1d98335b032d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cc0eed3bc59ff909af16ce1b33b833e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CARES-Comprehensive-Evaluation-of-Safety-and-Adversarial-Robustness-in-Medical-LLMs"><a href="#CARES-Comprehensive-Evaluation-of-Safety-and-Adversarial-Robustness-in-Medical-LLMs" class="headerlink" title="CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in   Medical LLMs"></a>CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in   Medical LLMs</h2><p><strong>Authors:Sijia Chen, Xiaomin Li, Mengxue Zhang, Eric Hanchen Jiang, Qingcheng Zeng, Chen-Hsiang Yu</strong></p>
<p>Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES (Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for evaluating LLM safety in healthcare. CARES includes over 18,000 prompts spanning eight medical safety principles, four harm levels, and four prompting styles: direct, indirect, obfuscated, and role-play, to simulate both malicious and benign use cases. We propose a three-way response evaluation protocol (Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess model behavior. Our analysis reveals that many state-of-the-art LLMs remain vulnerable to jailbreaks that subtly rephrase harmful prompts, while also over-refusing safe but atypically phrased queries. Finally, we propose a mitigation strategy using a lightweight classifier to detect jailbreak attempts and steer models toward safer behavior via reminder-based conditioning. CARES provides a rigorous framework for testing and improving medical LLM safety under adversarial and ambiguous conditions. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—ç¯å¢ƒä¸­å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„åº”ç”¨ï¼Œè¿™å¼•å‘äº†å…³äºå…¶å®‰å…¨æ€§ã€å¯¹é½æ€§å’Œæ˜“å—å¯¹æ‰‹æ“æ§çš„ä¸¥é‡å…³åˆ‡ã€‚è™½ç„¶ä¹‹å‰çš„åŸºå‡†æµ‹è¯•è¯„ä¼°äº†æ¨¡å‹æ‹’ç»æœ‰å®³æç¤ºçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹ä¸´åºŠç‰¹å¼‚æ€§ã€åˆ†çº§ä¼¤å®³æ°´å¹³å’Œçªç ´å¼æ”»å‡»è¦†ç›–ç‡ã€‚æˆ‘ä»¬å¼•å…¥CARESï¼ˆä¸´åºŠå¯¹æŠ—ç¨³å¥æ€§å’Œå®‰å…¨æ€§è¯„ä¼°ï¼‰ï¼ˆClinical Adversarial Robustness and Evaluation of Safetyï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°åŒ»ç–—ä¿å¥ä¸­LLMå®‰å…¨æ€§çš„åŸºå‡†æµ‹è¯•ã€‚CARESåŒ…å«è¶…è¿‡18ï¼Œ000ä¸ªæç¤ºï¼Œæ¶µç›–å…«ä¸ªåŒ»ç–—å®‰å…¨åŸåˆ™ã€å››ä¸ªä¼¤å®³ç¨‹åº¦å’Œå››ç§æç¤ºé£æ ¼ï¼šç›´æ¥ã€é—´æ¥ã€æ¨¡ç³Šå’Œè§’è‰²æ‰®æ¼”ï¼Œä»¥æ¨¡æ‹Ÿæ¶æ„å’Œè‰¯æ€§ç”¨ä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰å‘å“åº”è¯„ä¼°åè®®ï¼ˆæ¥å—ã€è°¨æ…ã€æ‹’ç»ï¼‰å’Œä¸€ä¸ªç²¾ç»†çš„å®‰å…¨åˆ†æ•°æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹è¡Œä¸ºã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè®¸å¤šæœ€å…ˆè¿›çš„LLMä»ç„¶å®¹æ˜“å—åˆ°çªç ´å¼æ”»å‡»ï¼Œè¿™äº›æ”»å‡»ä¼šå¾®å¦™åœ°é‡æ–°è¡¨è¿°æœ‰å®³æç¤ºï¼ŒåŒæ—¶ä¹Ÿä¼šæ‹’ç»å®‰å…¨ä½†æªè¾ä¸å…¸å‹çš„æŸ¥è¯¢ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¼“è§£ç­–ç•¥ï¼Œä½¿ç”¨è½»é‡çº§åˆ†ç±»å™¨æ¥æ£€æµ‹çªç ´å°è¯•ï¼Œå¹¶é€šè¿‡åŸºäºæé†’çš„æ¡ä»¶æ¥å¼•å¯¼æ¨¡å‹é‡‡å–æ›´å®‰å…¨çš„è¡ŒåŠ¨ã€‚CARESä¸ºæµ‹è¯•å’Œæé«˜å¯¹æ•Œæ¡ä»¶å’Œæ¨¡ç³Šæ¡ä»¶ä¸‹çš„åŒ»ç–—LLMå®‰å…¨æ€§æä¾›äº†ä¸¥æ ¼æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11413v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å¼•å‘äº†å…³äºå®‰å…¨ã€å¯¹é½å’Œæ˜“å—æ•Œå¯¹æ“ä½œå½±å“çš„æ‹…å¿§ã€‚ç°æœ‰çš„è¯„ä¼°æ¨¡å‹æ‹’ç»æœ‰å®³æç¤ºçš„èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œå¾€å¾€ç¼ºä¹ä¸´åºŠç‰¹å¼‚æ€§ã€åˆ†çº§å±å®³æ€§å’Œè¶Šç‹±å¼æ”»å‡»è¦†ç›–ã€‚æœ¬æ–‡ä»‹ç»CARESï¼ˆä¸´åºŠå¯¹æŠ—ç¨³å¥æ€§å’Œå®‰å…¨æ€§è¯„ä¼°ï¼‰ï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°åŒ»ç–—é¢†åŸŸLLMå®‰å…¨æ€§çš„åŸºå‡†æµ‹è¯•ã€‚CARESåŒ…å«è¶…è¿‡18ï¼Œ000ä¸ªæç¤ºï¼Œæ¶µç›–å…«ä¸ªåŒ»ç–—å®‰å…¨åŸåˆ™ã€å››ä¸ªå±å®³çº§åˆ«å’Œå››ç§æç¤ºé£æ ¼ï¼šç›´æ¥ã€é—´æ¥ã€æ¨¡ç³Šå’Œè§’è‰²æ‰®æ¼”ï¼Œä»¥æ¨¡æ‹Ÿæ¶æ„å’Œè‰¯æ€§ç”¨ä¾‹ã€‚æœ¬æ–‡æå‡ºä¸€ç§ä¸‰æ–¹å“åº”è¯„ä¼°åè®®ï¼ˆæ¥å—ã€è°¨æ…ã€æ‹’ç»ï¼‰å’Œç²¾ç»†çš„å®‰å…¨è¯„åˆ†æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹è¡Œä¸ºã€‚åˆ†æè¡¨æ˜ï¼Œè®¸å¤šæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»ç„¶å®¹æ˜“å—åˆ°å·§å¦™é‡æ–°è¡¨è¿°çš„æœ‰å®³æç¤ºçš„æ”»å‡»ï¼ŒåŒæ—¶ä¹Ÿä¼šè¿‡äºæ‹’ç»å®‰å…¨ä½†æªè¾ä¸å…¸å‹çš„æŸ¥è¯¢ã€‚æœ€åï¼Œæœ¬æ–‡æå‡ºä¸€ç§ä½¿ç”¨è½»é‡çº§åˆ†ç±»å™¨æ¥æ£€æµ‹è¶Šç‹±å°è¯•çš„ç¼“è§£ç­–ç•¥ï¼Œå¹¶é€šè¿‡åŸºäºæé†’çš„æ¡ä»¶å¼•å¯¼æ¨¡å‹æœç€æ›´å®‰å…¨çš„è¡Œä¸ºå‘å±•ã€‚CARESä¸ºæµ‹è¯•å’Œæé«˜åŒ»ç–—LLMåœ¨æ•Œå¯¹å’Œæ¨¡ç³Šæ¡ä»¶ä¸‹çš„å®‰å…¨æ€§æä¾›äº†ä¸¥æ ¼æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å¼•å‘äº†å¯¹å®‰å…¨ã€å¯¹é½å’Œæ˜“å—æ•Œå¯¹æ“ä½œå½±å“çš„æ‹…å¿§ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ç¼ºä¹ä¸´åºŠç‰¹å¼‚æ€§ã€åˆ†çº§å±å®³æ€§å’Œè¶Šç‹±å¼æ”»å‡»è¦†ç›–ã€‚</li>
<li>å¼•å…¥CARESåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šç§æç¤ºé£æ ¼å’Œè¯„ä¼°åè®®ï¼Œä»¥å…¨é¢è¯„ä¼°LLMåœ¨åŒ»ç–—é¢†åŸŸçš„å®‰å…¨æ€§ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºï¼Œå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ï¼Œå¹¶å¯èƒ½è¿‡äºæ‹’ç»å®‰å…¨æŸ¥è¯¢ã€‚</li>
<li>æå‡ºä½¿ç”¨è½»é‡çº§åˆ†ç±»å™¨æ£€æµ‹è¶Šç‹±å°è¯•çš„ç¼“è§£ç­–ç•¥ã€‚</li>
<li>CARESä¸ºæµ‹è¯•å’Œæé«˜åŒ»ç–—LLMåœ¨æ•Œå¯¹å’Œæ¨¡ç³Šæ¡ä»¶ä¸‹çš„å®‰å…¨æ€§æä¾›äº†ä¸¥æ ¼æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28d78d292a7f3a4b688c20abbc75eadf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-201ffbac50911ed6bc9c1911a0aec010.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-476b2c1cf67eb5fa7be75e283c5ffcb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10671724acd38e8caafb26c95e6f2909.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Visual-Planning-Letâ€™s-Think-Only-with-Images"><a href="#Visual-Planning-Letâ€™s-Think-Only-with-Images" class="headerlink" title="Visual Planning: Letâ€™s Think Only with Images"></a>Visual Planning: Letâ€™s Think Only with Images</h2><p><strong>Authors:Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan VuliÄ‡</strong></p>
<p>Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠå…¶å¤šæ¨¡æ€æ‰©å±•ï¼ˆMLLMï¼‰çš„è¿›å±•ï¼Œå·²åœ¨å¤šç§ä»»åŠ¡ä¸Šæå¤§åœ°å¢å¼ºäº†æœºå™¨æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸»è¦ä¾èµ–çº¯æ–‡æœ¬ä½œä¸ºè¡¨è¾¾å’Œç»“æ„åŒ–æ¨ç†çš„åª’ä»‹ï¼Œå³ä½¿å­˜åœ¨è§†è§‰ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸»å¼ è¯­è¨€å¹¶ä¸æ€»æ˜¯æœ€è‡ªç„¶æˆ–æœ€æœ‰æ•ˆçš„æ¨ç†æ–¹å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç©ºé—´å’Œå‡ ä½•ä¿¡æ¯çš„ä»»åŠ¡ä¸­ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼â€”â€”è§†è§‰è§„åˆ’ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡çº¯è§†è§‰è¡¨ç¤ºè¿›è¡Œè§„åˆ’ï¼Œç‹¬ç«‹äºæ–‡æœ¬ã€‚åœ¨è¿™ç§èŒƒå¼ä¸­ï¼Œè§„åˆ’æ˜¯é€šè¿‡ä¸€ç³»åˆ—å›¾åƒæ‰§è¡Œçš„ï¼Œè¿™äº›å›¾åƒåœ¨è§†è§‰é¢†åŸŸç¼–ç äº†é€æ­¥æ¨ç†ï¼Œç±»ä¼¼äºäººç±»å¦‚ä½•å‹¾ç”»æˆ–å¯è§†åŒ–æœªæ¥è¡ŒåŠ¨ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”é€šè¿‡å¼ºåŒ–å­¦ä¹ çš„è§†è§‰è§„åˆ’ï¼ˆVPRLï¼‰ï¼Œå€ŸåŠ©GRPOå¯¹å¤§å‹è§†è§‰æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œä»è€Œåœ¨å…·æœ‰ä»£è¡¨æ€§çš„è§†è§‰å¯¼èˆªä»»åŠ¡ã€FrozenLakeã€è¿·å®«å’ŒMiniBehaviorä¸­çš„è§„åˆ’æ–¹é¢å–å¾—äº†å®è´¨æ€§æ”¹è¿›ã€‚æˆ‘ä»¬çš„è§†è§‰è§„åˆ’èŒƒå¼åœ¨åªè¿›è¡Œæ–‡æœ¬æ¨ç†çš„å…¶å®ƒæ‰€æœ‰è§„åˆ’å˜ç§ä¸­è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†è§†è§‰è§„åˆ’ä½œä¸ºè¯­è¨€åŸºç¡€æ¨ç†çš„å¯è¡Œä¸”æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºå—ç›Šäºç›´è§‚ã€åŸºäºå›¾åƒçš„æ¨ç†çš„ä»»åŠ¡å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11409v1">PDF</a> 10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables   including references and appendices)</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ‰©å±•æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿›å±•æå¤§åœ°ä¿ƒè¿›äº†è·¨ä¸åŒä»»åŠ¡çš„æœºå™¨æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸»è¦ä¾èµ–çº¯æ–‡æœ¬ä½œä¸ºè¡¨è¾¾å’Œæ„å»ºæ¨ç†çš„åª’ä»‹ï¼Œå³ä½¿å­˜åœ¨è§†è§‰ä¿¡æ¯ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¨ç†æ¨¡å¼â€”â€”è§†è§‰è§„åˆ’ï¼Œå®ƒé€šè¿‡çº¯ç²¹çš„è§†è§‰è¡¨å¾è¿›è¡Œè§„åˆ’ï¼Œç‹¬ç«‹äºæ–‡æœ¬ã€‚è§†è§‰è§„åˆ’é€šè¿‡å›¾åƒåºåˆ—æ‰§è¡Œè§„åˆ’ï¼Œåœ¨è§†è§‰é¢†åŸŸé€æ­¥è¿›è¡Œæ¨ç†ï¼Œç±»ä¼¼äºäººç±»å¦‚ä½•å‹¾ç”»æˆ–å¯è§†åŒ–æœªæ¥è¡ŒåŠ¨ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”åŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰è§„åˆ’ï¼ˆVPRLï¼‰ï¼Œé€šè¿‡GRPOå¯¹å¤§å‹è§†è§‰æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œåœ¨å…¸å‹çš„è§†è§‰å¯¼èˆªä»»åŠ¡ï¼ˆå¦‚FrozenLakeã€è¿·å®«å’ŒMiniBehaviorï¼‰ä¸­å®ç°äº†æ˜¾è‘—çš„è§„åˆ’æ”¹è¿›ã€‚è§†è§‰è§„åˆ’èŒƒå¼åœ¨çº¯æ–‡æœ¬ç©ºé—´è¿›è¡Œæ¨ç†çš„è§„åˆ’å˜ä½“ä¸­è¡¨ç°æœ€ä½³ã€‚æœ¬æ–‡ç»“æœè¯æ˜äº†è§†è§‰è§„åˆ’ä½œä¸ºä¸€ç§å¯è¡Œä¸”æœ‰å‰é€”çš„æ›¿ä»£è¯­è¨€åŸºç¡€æ¨ç†çš„æ–¹æ³•ï¼Œä¸ºå—ç›Šäºç›´è§‚å›¾åƒåŸºç¡€æ¨ç†çš„ä»»åŠ¡å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€æ‰©å±•ï¼ˆMLLMï¼‰å¢å¼ºäº†æœºå™¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ¨¡å‹ä¸»è¦ä¾èµ–æ–‡æœ¬è¿›è¡Œæ¨ç†ï¼Œå³ä½¿å­˜åœ¨è§†è§‰ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¨ç†æ¨¡å¼â€”â€”è§†è§‰è§„åˆ’ï¼Œé€šè¿‡çº¯ç²¹çš„è§†è§‰è¡¨å¾è¿›è¡Œè§„åˆ’ï¼Œç‹¬ç«‹äºæ–‡æœ¬ã€‚</li>
<li>è§†è§‰è§„åˆ’é€šè¿‡å›¾åƒåºåˆ—æ‰§è¡Œï¼Œç±»ä¼¼äºäººç±»å¦‚ä½•å‹¾ç”»æˆ–å¯è§†åŒ–æœªæ¥è¡ŒåŠ¨ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”åŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰è§„åˆ’ï¼ˆVPRLï¼‰ã€‚</li>
<li>VPRLåœ¨å…¸å‹çš„è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„è§„åˆ’æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11409">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39e2fae2fb6e4a2b8d5c54c5ee658bdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b087dca392b82352040f39806ccc878.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-003cb4251f762d33ea57f5b138daf6c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45836365d67a9715f197785ef545cf09.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EmotionHallucer-Evaluating-Emotion-Hallucinations-in-Multimodal-Large-Language-Models"><a href="#EmotionHallucer-Evaluating-Emotion-Hallucinations-in-Multimodal-Large-Language-Models" class="headerlink" title="EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large   Language Models"></a>EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large   Language Models</h2><p><strong>Authors:Bohao Xing, Xin Liu, Guoying Zhao, Chengyu Liu, Xiaolan Fu, Heikki KÃ¤lviÃ¤inen</strong></p>
<p>Emotion understanding is a critical yet challenging task. Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities in this area. However, MLLMs often suffer from hallucinations, generating irrelevant or nonsensical content. To the best of our knowledge, despite the importance of this issue, there has been no dedicated effort to evaluate emotion-related hallucinations in MLLMs. In this work, we introduce EmotionHallucer, the first benchmark for detecting and analyzing emotion hallucinations in MLLMs. Unlike humans, whose emotion understanding stems from the interplay of biology and social learning, MLLMs rely solely on data-driven learning and lack innate emotional instincts. Fortunately, emotion psychology provides a solid foundation of knowledge about human emotions. Building on this, we assess emotion hallucinations from two dimensions: emotion psychology knowledge and real-world multimodal perception. To support robust evaluation, we utilize an adversarial binary question-answer (QA) framework, which employs carefully crafted basic and hallucinated pairs to assess the emotion hallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on EmotionHallucer, we reveal that: i) most current models exhibit substantial issues with emotion hallucinations; ii) closed-source models outperform open-source ones in detecting emotion hallucinations, and reasoning capability provides additional advantages; iii) existing models perform better in emotion psychology knowledge than in multimodal emotion perception. As a byproduct, these findings inspire us to propose the PEP-MEK framework, which yields an average improvement of 9.90% in emotion hallucination detection across selected models. Resources will be available at <a target="_blank" rel="noopener" href="https://github.com/xxtars/EmotionHallucer">https://github.com/xxtars/EmotionHallucer</a>. </p>
<blockquote>
<p>æƒ…æ„Ÿç†è§£æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—å¢å¼ºäº†è¯¥é¢†åŸŸçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒMLLMså¸¸å¸¸å‡ºç°å¹»è§‰ï¼Œç”Ÿæˆä¸ç›¸å…³æˆ–æ— æ„ä¹‰çš„å†…å®¹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°½ç®¡è¿™ä¸ªé—®é¢˜å¾ˆé‡è¦ï¼Œä½†è¿˜æ²¡æœ‰ä¸“é—¨é’ˆå¯¹MLLMsä¸­çš„æƒ…æ„Ÿç›¸å…³å¹»è§‰è¿›è¡Œè¯„ä¼°çš„åŠªåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†EmotionHallucerï¼Œå®ƒæ˜¯æ£€æµ‹å’Œåˆ†æMLLMsä¸­æƒ…æ„Ÿå¹»è§‰çš„ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ã€‚ä¸äººç±»ä¸åŒï¼Œäººç±»çš„æƒ…æ„Ÿç†è§£æ¥æºäºç”Ÿç‰©å­¦å’Œç¤¾ä¼šå­¦ä¹ çš„ç›¸äº’ä½œç”¨ï¼Œè€ŒMLLMså®Œå…¨ä¾èµ–äºæ•°æ®é©±åŠ¨çš„å­¦ä¹ ï¼Œç¼ºä¹å¤©ç”Ÿçš„æƒ…æ„Ÿç›´è§‰ã€‚å¹¸è¿çš„æ˜¯ï¼Œæƒ…æ„Ÿå¿ƒç†å­¦ä¸ºäººç±»æƒ…æ„Ÿæä¾›äº†åšå®çš„çŸ¥è¯†åŸºç¡€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªç»´åº¦è¯„ä¼°æƒ…æ„Ÿå¹»è§‰ï¼šæƒ…æ„Ÿå¿ƒç†å­¦çŸ¥è¯†å’Œç°å®ä¸–ç•Œçš„å¤šæ¨¡æ€æ„ŸçŸ¥ã€‚ä¸ºäº†æ”¯æŒç¨³å¥çš„è¯„ä¼°ï¼Œæˆ‘ä»¬é‡‡ç”¨å¯¹æŠ—æ€§äºŒå…ƒé—®ç­”ï¼ˆQAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç²¾å¿ƒåˆ¶ä½œçš„åŸºæœ¬å’Œå¹»è§‰é…å¯¹æ¥è¯„ä¼°MLLMsçš„æƒ…æ„Ÿå¹»è§‰å€¾å‘ã€‚é€šè¿‡å¯¹EmotionHallucerä¸Š38ä¸ªLLMså’ŒMLLMsçš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ï¼šiï¼‰å½“å‰å¤§å¤šæ•°æ¨¡å‹åœ¨æƒ…æ„Ÿå¹»è§‰æ–¹é¢å­˜åœ¨é‡å¤§é—®é¢˜ï¼›iiï¼‰å°é—­æºæ¨¡å‹åœ¨æ£€æµ‹æƒ…æ„Ÿå¹»è§‰æ–¹é¢ä¼˜äºå¼€æºæ¨¡å‹ï¼Œæ¨ç†èƒ½åŠ›ä¼šæä¾›é¢å¤–ä¼˜åŠ¿ï¼›iiiï¼‰ç°æœ‰æ¨¡å‹åœ¨æƒ…æ„Ÿå¿ƒç†å­¦çŸ¥è¯†æ–¹é¢çš„è¡¨ç°ä¼˜äºå¤šæ¨¡æ€æƒ…æ„Ÿæ„ŸçŸ¥ã€‚ä½œä¸ºå‰¯äº§å“ï¼Œè¿™äº›å‘ç°æ¿€åŠ±æˆ‘ä»¬æå‡ºäº†PEP-MEKæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ‰€é€‰æ¨¡å‹çš„æƒ…æ„Ÿå¹»è§‰æ£€æµ‹ä¸­å¹³å‡æé«˜äº†9.90%ã€‚ç›¸å…³èµ„æºå°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/xxtars/EmotionHallucer">https://github.com/xxtars/EmotionHallucer</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11405v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æƒ…æ„Ÿç†è§£åœ¨Multimodalå¤§è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚å°½ç®¡è¿‘æœŸæŠ€æœ¯è¿›æ­¥æ˜¾è‘—ï¼Œä½†è¿™äº›æ¨¡å‹å¸¸å¸¸å‡ºç°æƒ…æ„Ÿå¹»è§‰é—®é¢˜ï¼Œç”Ÿæˆæ— å…³æˆ–ä¸åˆç†çš„å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†EmotionHallucerï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºæ£€æµ‹å’Œè¯„ä¼°Multimodalå¤§è¯­è¨€æ¨¡å‹ä¸­æƒ…æ„Ÿå¹»è§‰çš„åŸºå‡†æµ‹è¯•ã€‚æ–‡ç« ä»æƒ…æ„Ÿå¿ƒç†å­¦çŸ¥è¯†å’Œç°å®ä¸–ç•Œçš„å¤šæ¨¡å¼æ„ŸçŸ¥ä¸¤ä¸ªç»´åº¦è¯„ä¼°æƒ…æ„Ÿå¹»è§‰ã€‚åˆ©ç”¨å¯¹æŠ—æ€§äºŒå…ƒé—®ç­”æ¡†æ¶è¿›è¡Œè¯„ä¼°ï¼Œå¯¹38ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿå¹»è§‰å€¾å‘çš„æµ‹è¯•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜å¤§å¤šæ•°å½“å‰æ¨¡å‹å­˜åœ¨æ˜æ˜¾çš„æƒ…æ„Ÿå¹»è§‰é—®é¢˜ï¼Œå°é—­æºæ¨¡å‹åœ¨æ£€æµ‹æƒ…æ„Ÿå¹»è§‰æ–¹é¢è¡¨ç°ä¼˜äºå¼€æºæ¨¡å‹ï¼Œä¸”æ¨ç†èƒ½åŠ›æä¾›é¢å¤–ä¼˜åŠ¿ã€‚ç°æœ‰æ¨¡å‹åœ¨æƒ…æ„Ÿå¿ƒç†å­¦çŸ¥è¯†æ–¹é¢çš„è¡¨ç°ä¼˜äºå¤šæ¨¡æ€æƒ…æ„Ÿæ„ŸçŸ¥ã€‚åŸºäºæ­¤ï¼Œæ–‡ç« æå‡ºäº†PEP-MEKæ¡†æ¶ï¼Œå¯ä»¥åœ¨é€‰å®šçš„æ¨¡å‹ä¸­å¹³å‡æé«˜æƒ…æ„Ÿå¹»è§‰æ£€æµ‹çš„æ•ˆæœã€‚ç›¸å…³èµ„æºå¯åœ¨æŒ‡å®šç½‘ç«™è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿç†è§£åœ¨Multimodalå¤§è¯­è¨€æ¨¡å‹ä¸­è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Multimodalå¤§è¯­è¨€æ¨¡å‹å¸¸å¸¸å‡ºç°æƒ…æ„Ÿå¹»è§‰é—®é¢˜ã€‚</li>
<li>EmotionHalluceræ˜¯é¦–ä¸ªç”¨äºæ£€æµ‹å’Œè¯„ä¼°Multimodalå¤§è¯­è¨€æ¨¡å‹ä¸­æƒ…æ„Ÿå¹»è§‰çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ä»æƒ…æ„Ÿå¿ƒç†å­¦çŸ¥è¯†å’Œç°å®ä¸–ç•Œçš„å¤šæ¨¡å¼æ„ŸçŸ¥ä¸¤ä¸ªç»´åº¦è¯„ä¼°æƒ…æ„Ÿå¹»è§‰ã€‚</li>
<li>å¯¹æŠ—æ€§äºŒå…ƒé—®ç­”æ¡†æ¶ç”¨äºè¯„ä¼°æƒ…æ„Ÿå¹»è§‰å€¾å‘ã€‚</li>
<li>å¤§å¤šæ•°å½“å‰æ¨¡å‹å­˜åœ¨æ˜æ˜¾çš„æƒ…æ„Ÿå¹»è§‰é—®é¢˜ï¼Œå°é—­æºæ¨¡å‹è¡¨ç°è¾ƒå¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-126002d0c859f8501d50c8588752d44b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-947ca07f1bc27799c8ffa01688204db4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28ed3a40b69208e07fd498a2b7aceeaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8ef7819ac862b08b1c284536d08bf49.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Phare-A-Safety-Probe-for-Large-Language-Models"><a href="#Phare-A-Safety-Probe-for-Large-Language-Models" class="headerlink" title="Phare: A Safety Probe for Large Language Models"></a>Phare: A Safety Probe for Large Language Models</h2><p><strong>Authors:Pierre Le Jeune, BenoÃ®t MalÃ©sieux, Weixuan Xiao, Matteo Dora</strong></p>
<p>Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems. </p>
<blockquote>
<p>ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨å¯¹äºè´Ÿè´£ä»»çš„éƒ¨ç½²è‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰çš„è¯„ä¼°å¾€å¾€æ›´ä¾§é‡äºæ€§èƒ½è€Œéè¯†åˆ«æ•…éšœæ¨¡å¼ã€‚æˆ‘ä»¬ä»‹ç»äº†Phareï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€è¯Šæ–­æ¡†æ¶ï¼Œç”¨äºæ¢ç©¶å’Œè¯„ä¼°LLMåœ¨ä¸‰æ–¹é¢çš„è¡Œä¸ºï¼šå¹»è§‰å’Œå¯é æ€§ã€ç¤¾ä¼šåè§ä»¥åŠæœ‰å®³å†…å®¹ç”Ÿæˆã€‚æˆ‘ä»¬å¯¹17ä¸ªæœ€æ–°LLMçš„è¯„ä¼°æ­ç¤ºäº†æ‰€æœ‰å®‰å…¨ç»´åº¦ä¸Šçš„ç³»ç»Ÿæ€§æ¼æ´æ¨¡å¼ï¼ŒåŒ…æ‹¬æ‹é©¬å±ã€æç¤ºæ•æ„Ÿæ€§å’Œåˆ»æ¿å°è±¡å¤åˆ¶ã€‚Phareé€šè¿‡çªå‡ºè¿™äº›ç‰¹å®šçš„æ•…éšœæ¨¡å¼è€Œä¸æ˜¯ç®€å•åœ°æ’åæ¨¡å‹ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†æ„å»ºæ›´ç¨³å¥ã€å¯¹é½å’Œå¯ä¿¡èµ–çš„è¯­è¨€ç³»ç»Ÿçš„å¯æ“ä½œè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11365v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºPhareçš„å¤šè¯­è¨€è¯Šæ–­æ¡†æ¶ï¼Œç”¨äºå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡Œä¸ºè¿›è¡Œå…¨é¢è¯„ä¼°ã€‚é€šè¿‡ä¸‰ä¸ªæ–¹é¢â€”â€”å¹»æƒ³ä¸å¯é æ€§ã€ç¤¾ä¼šåè§å’Œæœ‰å®³å†…å®¹ç”Ÿæˆï¼Œå¯¹ç°æœ‰17ç§å‰æ²¿LLMçš„è¯„ä¼°æ­ç¤ºäº†ç³»ç»Ÿæ¼æ´æ¨¡å¼ã€‚Phareé€šè¿‡å¼ºè°ƒç‰¹å®šå¤±è´¥æ¨¡å¼è€Œä¸æ˜¯ç®€å•çš„æ’åæ¨¡å‹ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›å…·æœ‰æ“ä½œæ€§çš„è§è§£ï¼Œç”¨äºæ„å»ºæ›´åŠ ç¨³å¥ã€ä¸€è‡´å’Œå¯é çš„è¯­ç³»ç»Ÿã€‚æ€»ä½“æ¥è¯´ï¼Œä¿éšœLLMçš„å®‰å…¨å¯¹äºè´Ÿè´£éƒ¨ç½²è‡³å…³é‡è¦ï¼ŒPhareæ¡†æ¶å¯¹äºæ­ç¤ºLLMçš„ç³»ç»Ÿæ€§è„†å¼±ç‚¹å¹¶å¼•å¯¼æœªæ¥å‘å±•å…·æœ‰æ·±è¿œæ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Phareæ˜¯ä¸€ä¸ªå¤šè¯­è¨€è¯Šæ–­æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œè¯Šæ–­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡Œä¸ºã€‚</li>
<li>Phareä»ä¸‰ä¸ªæ–¹é¢è¯„ä¼°LLMçš„è¡Œä¸ºï¼šå¹»æƒ³ä¸å¯é æ€§ã€ç¤¾ä¼šåè§å’Œæœ‰å®³å†…å®¹ç”Ÿæˆã€‚</li>
<li>å¯¹ç°æœ‰LLMçš„è¯„ä¼°æ­ç¤ºäº†ç³»ç»Ÿæ€§æ¼æ´æ¨¡å¼ï¼Œè¿™äº›æ¼æ´å¯èƒ½å½±å“æ¨¡å‹çš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚</li>
<li>Phareæ¡†æ¶å¼ºè°ƒäº†ç‰¹å®šå¤±è´¥æ¨¡å¼è€Œéç®€å•æ’åæ¨¡å‹çš„é‡è¦æ€§ã€‚</li>
<li>é€šè¿‡è¯†åˆ«è¿™äº›å¤±è´¥æ¨¡å¼ï¼ŒPhareä¸ºæ„å»ºæ›´åŠ ç¨³å¥ã€ä¸€è‡´å’Œå¯é çš„LLMæä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚</li>
<li>Phareè¯„ä¼°æœ‰åŠ©äºæŒ‡å¯¼ç ”ç©¶è€…å’Œä»ä¸šè€…æ›´å¥½åœ°ç†è§£LLMçš„ä¼˜åŠ¿å’Œæ½œåœ¨ç¼ºé™·ï¼Œä»¥æ›´å¥½åœ°æ¨åŠ¨æ¨¡å‹çš„æœªæ¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e870f5e4b992bd7cbc9a0e7b7b078c89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f798cbf74880c10e4393bbae6615ebc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff49a79f0bb4f148ddb10972158f0eb6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LegoSLM-Connecting-LLM-with-Speech-Encoder-using-CTC-Posteriors"><a href="#LegoSLM-Connecting-LLM-with-Speech-Encoder-using-CTC-Posteriors" class="headerlink" title="LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors"></a>LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors</h2><p><strong>Authors:Rao Ma, Tongzhou Chen, Kartik Audhkhasi, Bhuvana Ramabhadran</strong></p>
<p>Recently, large-scale pre-trained speech encoders and Large Language Models (LLMs) have been released, which show state-of-the-art performance on a range of spoken language processing tasks including Automatic Speech Recognition (ASR). To effectively combine both models for better performance, continuous speech prompts, and ASR error correction have been adopted. However, these methods are prone to suboptimal performance or are inflexible. In this paper, we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using the ASR posterior matrices. The speech encoder is trained to generate Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary, which are used to reconstruct pseudo-audio embeddings by computing a weighted sum of the LLM input embeddings. These embeddings are concatenated with text embeddings in the LLM input space. Using the well-performing USM and Gemma models as an example, we demonstrate that our proposed LegoSLM method yields good performance on both ASR and speech translation tasks. By connecting USM with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline on 8 MLS testsets. The trained model also exhibits modularity in a range of settings â€“ after fine-tuning the Gemma model weights, the speech encoder can be switched and combined with the LLM in a zero-shot fashion. Additionally, we propose to control the decode-time influence of the USM and LLM using a softmax temperature, which shows effectiveness in domain adaptation. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå·²ç»å‘å¸ƒäº†ä¸€ç³»åˆ—å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ƒä»¬åœ¨åŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨å†…çš„å„ç§å£è¯­å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°ç»“åˆè¿™ä¸¤ç§æ¨¡å‹ä»¥å–å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå·²ç»é‡‡ç”¨äº†è¿ç»­è¯­éŸ³æç¤ºå’ŒASRé”™è¯¯æ ¡æ­£ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å®¹æ˜“å‡ºç°æ€§èƒ½ä¸ä½³æˆ–ä¸å¤Ÿçµæ´»çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼LegoSLMï¼Œå®ƒä½¿ç”¨ASRåéªŒçŸ©é˜µæ¥è¿æ¥è¯­éŸ³ç¼–ç å™¨å’ŒLLMã€‚è¯­éŸ³ç¼–ç å™¨è¢«è®­ç»ƒä»¥ç”Ÿæˆé’ˆå¯¹LLMè¯æ±‡è¡¨çš„è¿æ¥æ—¶åºåˆ†ç±»ï¼ˆCTCï¼‰åéªŒæ¦‚ç‡ï¼Œè¿™äº›åéªŒæ¦‚ç‡è¢«ç”¨æ¥é€šè¿‡è®¡ç®—LLMè¾“å…¥åµŒå…¥çš„åŠ æƒå’Œæ¥é‡å»ºä¼ªéŸ³é¢‘åµŒå…¥ã€‚è¿™äº›åµŒå…¥ä¸LLMè¾“å…¥ç©ºé—´ä¸­çš„æ–‡æœ¬åµŒå…¥ç›¸è¿æ¥ã€‚ä»¥æ€§èƒ½è‰¯å¥½çš„USMå’ŒGemmaæ¨¡å‹ä¸ºä¾‹ï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„LegoSLMæ–¹æ³•åœ¨ASRå’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸Šéƒ½è¡¨ç°è‰¯å¥½ã€‚é€šè¿‡å°†USMä¸Gemmaæ¨¡å‹è¿æ¥èµ·æ¥ï¼Œæˆ‘ä»¬åœ¨8ä¸ªMLSæµ‹è¯•é›†ä¸Šç›¸å¯¹äºUSM-CTCåŸºçº¿è·å¾—äº†å¹³å‡49%çš„WERRã€‚è®­ç»ƒå¥½çš„æ¨¡å‹è¿˜è¡¨ç°å‡ºåœ¨å„ç§è®¾ç½®ä¸‹çš„æ¨¡å—åŒ–â€”â€”åœ¨å¾®è°ƒGemmaæ¨¡å‹æƒé‡åï¼Œå¯ä»¥åˆ‡æ¢è¯­éŸ³ç¼–ç å™¨å¹¶ä»¥é›¶æ ·æœ¬çš„æ–¹å¼ä¸LLMç»“åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨softmaxæ¸©åº¦æ¥æ§åˆ¶USMå’ŒLLMåœ¨è§£ç æ—¶é—´çš„å½±å“ï¼Œè¿™åœ¨é¢†åŸŸé€‚åº”ä¸­æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11352v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç»“åˆæ–¹æ³•LegoSLMï¼Œæ—¨åœ¨è§£å†³è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ASRåéªŒçŸ©é˜µæ¡¥æ¥è¯­éŸ³ç¼–ç å™¨å’ŒLLMï¼Œè®­ç»ƒè¯­éŸ³ç¼–ç å™¨ç”ŸæˆCTCåéªŒï¼Œç”¨äºé‡å»ºä¼ªéŸ³é¢‘åµŒå…¥ï¼Œå¹¶å°†å…¶ä¸LLMè¾“å…¥ç©ºé—´ä¸­çš„æ–‡æœ¬åµŒå…¥åˆå¹¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ASRå’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä¸ç¾å›½USMæ¨¡å‹å’ŒGemmaæ¨¡å‹çš„ç»“åˆå¹³å‡æé«˜äº†çº¦49%çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å…·æœ‰æ¨¡å—åŒ–ç‰¹æ€§ï¼Œå¯ä»¥åœ¨ä¸åŒè®¾ç½®ä¸‹è¿›è¡Œå¾®è°ƒå¹¶ä¸å…¶ä»–æ¨¡å‹ç»„åˆä½¿ç”¨ã€‚åŒæ—¶ï¼Œé€šè¿‡è°ƒæ•´softmaxæ¸©åº¦æ§åˆ¶è§£ç æ—¶çš„å½±å“ï¼Œä¸ºé¢†åŸŸé€‚åº”æ€§æä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LegoSLMç»“åˆäº†è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºæ”¹å–„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ASRåéªŒçŸ©é˜µæ¡¥æ¥è¯­éŸ³ç¼–ç å™¨å’ŒLLMï¼Œè®­ç»ƒè¯­éŸ³ç¼–ç å™¨ç”ŸæˆCTCåéªŒã€‚</li>
<li>ä¼ªéŸ³é¢‘åµŒå…¥æ˜¯é€šè¿‡è®¡ç®—LLMè¾“å…¥åµŒå…¥çš„åŠ æƒå’Œæ¥é‡å»ºçš„ï¼Œå¹¶ä¸æ–‡æœ¬åµŒå…¥åˆå¹¶ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºLegoSLMåœ¨ASRå’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸USMå’ŒGemmaæ¨¡å‹çš„ç»“åˆï¼Œæ€§èƒ½å¹³å‡æå‡çº¦49%ã€‚</li>
<li>æ¨¡å‹å…·å¤‡æ¨¡å—åŒ–ç‰¹æ€§ï¼Œå¯åœ¨ä¸åŒè®¾ç½®ä¸‹è¿›è¡Œå¾®è°ƒå¹¶ä¸å…¶ä»–æ¨¡å‹ç»„åˆä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4ba46440e42129c99760bbc2ab76162.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ebe810e6380a78828bdd21c127cc21d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8809b33003316d618210ea3dccf7423b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64af55f0fc6795592761f2d11179d198.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7279fa04349e3c5e715eaf9a5561fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61b2f397f7d7f1b23e51ab7c3881dddd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Context-parroting-A-simple-but-tough-to-beat-baseline-for-foundation-models-in-scientific-machine-learning"><a href="#Context-parroting-A-simple-but-tough-to-beat-baseline-for-foundation-models-in-scientific-machine-learning" class="headerlink" title="Context parroting: A simple but tough-to-beat baseline for foundation   models in scientific machine learning"></a>Context parroting: A simple but tough-to-beat baseline for foundation   models in scientific machine learning</h2><p><strong>Authors:Yuanzhao Zhang, William Gilpin</strong></p>
<p>Recently-developed time series foundation models for scientific machine learning exhibit emergent abilities to predict physical systems. These abilities include zero-shot forecasting, in which a model forecasts future states of a system given only a short trajectory as context. Here, we show that foundation models applied to physical systems can give accurate predictions, but that they fail to develop meaningful representations of the underlying physics. Instead, foundation models often forecast by context parroting, a simple zero-shot forecasting strategy that copies directly from the context. As a result, a naive direct context parroting model scores higher than state-of-the-art time-series foundation models on predicting a diverse range of dynamical systems, at a tiny fraction of the computational cost. We draw a parallel between context parroting and induction heads, which explains why large language models trained on text can be repurposed for time series forecasting. Our dynamical systems perspective also ties the scaling between forecast accuracy and context length to the fractal dimension of the attractor, providing insight into the previously observed in-context neural scaling laws. Context parroting thus serves as a simple but tough-to-beat baseline for future time-series foundation models and can help identify in-context learning strategies beyond parroting. </p>
<blockquote>
<p>æœ€è¿‘å¼€å‘çš„ç”¨äºç§‘å­¦æœºå™¨å­¦ä¹ çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹å±•ç°å‡ºé¢„æµ‹ç‰©ç†ç³»ç»Ÿçš„èƒ½åŠ›ã€‚è¿™äº›èƒ½åŠ›åŒ…æ‹¬é›¶æ ·æœ¬é¢„æµ‹ï¼Œå³ä»…æ ¹æ®ç³»ç»Ÿçš„çŸ­æœŸè½¨è¿¹ä½œä¸ºä¸Šä¸‹æ–‡è¿›è¡Œæœªæ¥çŠ¶æ€é¢„æµ‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å±•ç¤ºäº†åº”ç”¨äºç‰©ç†ç³»ç»Ÿçš„åŸºç¡€æ¨¡å‹å¯ä»¥åšå‡ºå‡†ç¡®çš„é¢„æµ‹ï¼Œä½†å®ƒä»¬æ— æ³•å‘å±•å¯¹åº•å±‚ç‰©ç†å­¦çš„æœ‰æ„ä¹‰è¡¨ç¤ºã€‚ç›¸åï¼ŒåŸºç¡€æ¨¡å‹é€šå¸¸é€šè¿‡ä¸Šä¸‹æ–‡æ¨¡ä»¿è¿›è¡Œé¢„æµ‹ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„é›¶æ ·æœ¬é¢„æµ‹ç­–ç•¥ï¼Œç›´æ¥ä»ä¸Šä¸‹æ–‡ä¸­å¤åˆ¶ã€‚å› æ­¤ï¼Œç®€å•çš„ç›´æ¥ä¸Šä¸‹æ–‡æ¨¡ä»¿æ¨¡å‹åœ¨é¢„æµ‹å„ç§åŠ¨æ€ç³»ç»Ÿæ—¶å¾—åˆ†é«˜äºæœ€æ–°æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼Œä¸”è®¡ç®—æˆæœ¬æä½ã€‚æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡æ¨¡ä»¿ä¸å½’çº³å¤´è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆè®­ç»ƒäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥é‡æ–°ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹ã€‚æˆ‘ä»¬çš„åŠ¨æ€ç³»ç»Ÿè§†è§’è¿˜å°†é¢„æµ‹ç²¾åº¦ä¸ä¸Šä¸‹æ–‡é•¿åº¦ä¹‹é—´çš„ç¼©æ”¾ä¸å¸å¼•å­çš„åˆ†å½¢ç»´åº¦è”ç³»èµ·æ¥ï¼Œä¸ºä¹‹å‰è§‚å¯Ÿåˆ°çš„ä¸Šä¸‹æ–‡ç¥ç»ç¼©æ”¾å®šå¾‹æä¾›äº†è§è§£ã€‚å› æ­¤ï¼Œä¸Šä¸‹æ–‡æ¨¡ä»¿æˆä¸ºæœªæ¥æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„ä¸€ä¸ªç®€å•ä½†éš¾ä»¥è¶…è¶Šçš„åŸºå‡†ï¼Œå¹¶æœ‰åŠ©äºè¯†åˆ«é™¤æ¨¡ä»¿ä»¥å¤–çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11349v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼€å‘çš„ç§‘å­¦æœºå™¨å­¦ä¹ æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹å±•ç°å‡ºé¢„æµ‹ç‰©ç†ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œå¦‚é›¶èµ·ç‚¹é¢„æµ‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨é¢„æµ‹æ—¶å¹¶æœªçœŸæ­£æ•æ‰åˆ°ç‰©ç†ç³»ç»Ÿçš„å†…åœ¨è§„å¾‹ï¼Œè€Œæ˜¯é€šè¿‡æ¨¡ä»¿ä¸Šä¸‹æ–‡è¿›è¡Œé¢„æµ‹ï¼Œç§°ä¸ºâ€œä¸Šä¸‹æ–‡é¹¦é¹‰å­¦èˆŒâ€ã€‚è¿™ç§ç®€å•çš„é¢„æµ‹ç­–ç•¥åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°ç”šè‡³ä¼˜äºå½“å‰å…ˆè¿›çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼Œä¸”è®¡ç®—æˆæœ¬ä½ã€‚ç ”ç©¶å°†ä¸Šä¸‹æ–‡é¹¦é¹‰å­¦èˆŒä¸å½’çº³å¤´è”ç³»èµ·æ¥ï¼Œè§£é‡Šä¸ºä½•å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥åº”ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹ã€‚åŒæ—¶ï¼Œé€šè¿‡åŠ¨æ€ç³»ç»Ÿè§†è§’æ­ç¤ºäº†é¢„æµ‹ç²¾åº¦ä¸ä¸Šä¸‹æ–‡é•¿åº¦ä¹‹é—´çš„å…³è”ã€‚å› æ­¤ï¼Œä¸Šä¸‹æ–‡æ¨¡ä»¿ä¸ºæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹æä¾›äº†ä¸€ä¸ªç®€å•ä½†éš¾ä»¥è¶…è¶Šçš„åŸºå‡†çº¿ï¼Œå¹¶æœ‰åŠ©äºè¯†åˆ«æ›´å¤šçš„ä¸Šä¸‹æ–‡å­¦ä¹ ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹å¯ä»¥é¢„æµ‹ç‰©ç†ç³»ç»Ÿï¼ŒåŒ…æ‹¬é›¶èµ·ç‚¹é¢„æµ‹ã€‚</li>
<li>è¿™äº›æ¨¡å‹ä¸»è¦é€šè¿‡æ¨¡ä»¿ä¸Šä¸‹æ–‡è¿›è¡Œé¢„æµ‹ï¼Œè€ŒéçœŸæ­£æ•æ‰ç‰©ç†ç³»ç»Ÿçš„å†…åœ¨è§„å¾‹ã€‚</li>
<li>ä¸Šä¸‹æ–‡é¹¦é¹‰å­¦èˆŒç­–ç•¥åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸”è®¡ç®—æˆæœ¬ä½ã€‚</li>
<li>ä¸Šä¸‹æ–‡é¹¦é¹‰å­¦èˆŒä¸å½’çº³å¤´ä¹‹é—´çš„è”ç³»è¢«æ­ç¤ºã€‚</li>
<li>é¢„æµ‹ç²¾åº¦ä¸ä¸Šä¸‹æ–‡é•¿åº¦ä¹‹é—´çš„å…³ç³»è¢«é˜æ˜ï¼Œä¸å¸å¼•å­çš„åˆ†å½¢ç»´åº¦æœ‰å…³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9c7d398c490099d17e4a9488618f240.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e077d970dcfa8e480922ae41cff388bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7893f2a5db1a5fc4e16cd581d8c4f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fdc4faa2f68175481ce04b5bb472a00.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Attention-via-Pre-Scoring-Prioritizing-Informative-Keys-in-Transformers"><a href="#Efficient-Attention-via-Pre-Scoring-Prioritizing-Informative-Keys-in-Transformers" class="headerlink" title="Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in   Transformers"></a>Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in   Transformers</h2><p><strong>Authors:Zhexiang Li, Haoyu Wang, Yutong Bao, David Woodruff</strong></p>
<p>Recent advances in transformer architectures deeply enhance long-context language modeling. Among them, HyperAttention achieves competitive efficiency by combining a single-level LSH-based clustering with uniform residual sampling. However,such a sampling limits crucial keysâ€™ capturing, which in turn raises the overall perplexity. In this paper, we propose a pre-scoring mechanism to assist HyperAttention to prioritize significant keys. Specifically, we introduce three scoring methods: K-means clustering, K-median clustering, and leverage score-based ranking (inspired by LevAttention) to filter keys effectively. We further replace HyperAttentionâ€™s original uniform residual sampling entirely, relying exclusively on our pre-scoring mechanism. Experiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3, which outperforms standard HyperAttention. Moreover, when running on the Vision-Transformer (ViT), our method shows that it can guarantee similar accuracy compared with LevAttention, and will surpass LevAttention given specific parameters. Although this method introduces computational overhead, its combination with HyperAttention remains 20 times faster than FlashAttention, providing a balanced trade-off between speed and modeling accuracy. Our results highlight the effectiveness of integrating pre-scoring into hierarchical attention mechanisms, significantly improving Transformerâ€™s efficiency. </p>
<blockquote>
<p>è¿‘æœŸTransformeræ¶æ„çš„è¿›å±•æå¤§åœ°æå‡äº†é•¿è¯­å¢ƒè¯­è¨€å»ºæ¨¡çš„èƒ½åŠ›ã€‚å…¶ä¸­ï¼ŒHyperAttentioné€šè¿‡ç»“åˆå•çº§LSHï¼ˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰èšç±»ä¸å‡åŒ€å‰©ä½™é‡‡æ ·å®ç°äº†ç«äº‰æ€§çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œè¿™ç§é‡‡æ ·æ–¹å¼é™åˆ¶äº†å…³é”®ä¿¡æ¯çš„æ•è·ï¼Œä»è€Œæé«˜äº†æ•´ä½“å›°æƒ‘åº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§é¢„è¯„åˆ†æœºåˆ¶æ¥å¸®åŠ©HyperAttentionä¼˜å…ˆå¤„ç†é‡è¦çš„å…³é”®ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§è¯„åˆ†æ–¹æ³•ï¼šK-å‡å€¼èšç±»ã€K-ä¸­ä½æ•°èšç±»å’ŒåŸºäºæ æ†å¾—åˆ†çš„æ’åï¼ˆå—LevAttentionå¯å‘ï¼‰ä»¥æœ‰æ•ˆåœ°è¿‡æ»¤å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å®Œå…¨æ›¿æ¢HyperAttentionçš„åŸå§‹å‡åŒ€å‰©ä½™é‡‡æ ·ï¼Œä»…ä¾èµ–æˆ‘ä»¬çš„é¢„è¯„åˆ†æœºåˆ¶ã€‚åœ¨ChatGLM2ï¼ˆ13.1ä¸‡ä»¤ç‰Œä¸Šä¸‹æ–‡ï¼‰ä¸Šçš„å®éªŒå°†å›°æƒ‘åº¦ä»12é™ä½åˆ°äº†8.3ï¼Œè¿™ä¼˜äºæ ‡å‡†çš„HyperAttentionã€‚æ­¤å¤–ï¼Œå½“åœ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸Šè¿è¡Œæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¿è¯ä¸LevAttentionç±»ä¼¼çš„ç²¾åº¦ï¼Œå¹¶åœ¨ç»™å®šç‰¹å®šå‚æ•°æ—¶è¶…è¿‡LevAttentionã€‚å°½ç®¡è¿™ç§æ–¹æ³•å¼•å…¥äº†è®¡ç®—å¼€é”€ï¼Œä½†å®ƒä¸HyperAttentionçš„ç»“åˆä»ç„¶æ¯”FlashAttentionå¿«20å€ï¼Œåœ¨é€Ÿåº¦å’Œå»ºæ¨¡ç²¾åº¦ä¹‹é—´æä¾›äº†å¹³è¡¡çš„æŠ˜è¡·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªæ˜¾äº†å°†é¢„è¯„åˆ†é›†æˆåˆ°åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå¤§åœ°æé«˜äº†Transformerçš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11040v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼ŒHyperAttentionç»“åˆå•çº§LSHèšç±»ä¸å‡åŒ€å‰©ä½™é‡‡æ ·æŠ€æœ¯æé«˜äº†é•¿æ–‡æœ¬è¯­å¢ƒå»ºæ¨¡çš„æ•ˆç‡ï¼Œä½†ä»å­˜åœ¨å…³é”®ä¿¡æ¯æ•æ‰å—é™çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºä¸€ç§é¢„è¯„åˆ†æœºåˆ¶ï¼Œå¼•å…¥K-meansèšç±»ã€K-medianèšç±»å’ŒåŸºäºæ æ†å¾—åˆ†çš„æ’åç­‰æ–¹æ³•ä»¥ä¼˜åŒ–å…³é”®ä¿¡æ¯çš„ç­›é€‰ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æœºåˆ¶åœ¨ChatGLM2ä¸Šé™ä½äº†å›°æƒ‘åº¦ï¼Œå¹¶èƒ½åœ¨Vision-Transformerä¸Šä¿è¯ä¸LevAttentionç›¸ä¼¼çš„å‡†ç¡®ç‡ã€‚è™½ç„¶å¢åŠ äº†è®¡ç®—å¼€é”€ï¼Œä½†ä¸HyperAttentionç»“åˆåé€Ÿåº¦ä»æ˜¯FlashAttentionçš„20å€ï¼Œå®ç°äº†é€Ÿåº¦ä¸å»ºæ¨¡å‡†ç¡®æ€§çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HyperAttentioné€šè¿‡ç»“åˆLSH-basedèšç±»å’Œå‡åŒ€å‰©ä½™é‡‡æ ·æé«˜äº†é•¿æ–‡æœ¬è¯­å¢ƒå»ºæ¨¡æ•ˆç‡ã€‚</li>
<li>é¢„è¯„åˆ†æœºåˆ¶è¢«å¼•å…¥ä»¥ä¼˜åŒ–HyperAttentionä¸­å…³é”®ä¿¡æ¯çš„æ•æ‰ã€‚</li>
<li>å¼•å…¥çš„é¢„è¯„åˆ†åŒ…æ‹¬K-meansèšç±»ã€K-medianèšç±»å’ŒåŸºäºæ æ†å¾—åˆ†çš„æ’åæ–¹æ³•ã€‚</li>
<li>å‡åŒ€å‰©ä½™é‡‡æ ·è¢«å®Œå…¨æ›¿æ¢ä¸ºé¢„è¯„åˆ†æœºåˆ¶ã€‚</li>
<li>åœ¨ChatGLM2ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–°çš„æœºåˆ¶é™ä½äº†å›°æƒ‘åº¦ï¼Œå¹¶ä¼˜äºæ ‡å‡†HyperAttentionã€‚</li>
<li>åœ¨Vision-Transformerä¸Šï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®ç‡ä¸LevAttentionç›¸å½“ï¼Œç‰¹å®šå‚æ•°ä¸‹å¯è¶…è¶ŠLevAttentionã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db08f5bbe02f298281442e6012a900fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ae3cc4b7e055376af18cf21f8b836ee.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Let-the-Trial-Begin-A-Mock-Court-Approach-to-Vulnerability-Detection-using-LLM-Based-Agents"><a href="#Let-the-Trial-Begin-A-Mock-Court-Approach-to-Vulnerability-Detection-using-LLM-Based-Agents" class="headerlink" title="Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection   using LLM-Based Agents"></a>Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection   using LLM-Based Agents</h2><p><strong>Authors:Ratnadira Widyasari, Martin Weyssow, Ivana Clairine Irsan, Han Wei Ang, Frank Liauw, Eng Lieh Ouh, Lwin Khin Shar, Hong Jin Kang, David Lo</strong></p>
<p>Detecting vulnerabilities in source code remains a critical yet challenging task, especially when benign and vulnerable functions share significant similarities. In this work, we introduce VulTrial, a courtroom-inspired multi-agent framework designed to enhance automated vulnerability detection. It employs four role-specific agents, which are security researcher, code author, moderator, and review board. Through extensive experiments using GPT-3.5 and GPT-4o we demonstrate that Vultrial outperforms single-agent and multi-agent baselines. Using GPT-4o, VulTrial improves the performance by 102.39% and 84.17% over its respective baseline. Additionally, we show that role-specific instruction tuning in multi-agent with small data (50 pair samples) improves the performance of VulTrial further by 139.89% and 118.30%. Furthermore, we analyze the impact of increasing the number of agent interactions on VulTrialâ€™s overall performance. While multi-agent setups inherently incur higher costs due to increased token usage, our findings reveal that applying VulTrial to a cost-effective model like GPT-3.5 can improve its performance by 69.89% compared to GPT-4o in a single-agent setting, at a lower overall cost. </p>
<blockquote>
<p>æ£€æµ‹æºä»£ç ä¸­çš„æ¼æ´ä»ç„¶æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œä½†ä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å½“è‰¯æ€§å‡½æ•°å’Œæ˜“é­å—æ”»å‡»çš„å‡½æ•°çš„ç›¸ä¼¼åº¦å¾ˆé«˜æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VulTrialï¼Œè¿™æ˜¯ä¸€ä¸ªå—æ³•åº­å¯å‘çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹ã€‚å®ƒé‡‡ç”¨äº†å››ä¸ªè§’è‰²ç‰¹å®šçš„æ™ºèƒ½ä½“ï¼Œåˆ†åˆ«æ˜¯å®‰å…¨ç ”ç©¶å‘˜ã€ä»£ç ä½œè€…ã€ä¸»æŒäººå’Œè¯„å®¡å›¢ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨GPT-3.5å’ŒGPT-4oçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVulTrialåœ¨å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“åŸºçº¿æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä½¿ç”¨GPT-4oæ—¶ï¼ŒVulTrialçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†102.39%å’Œ84.17%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨å¤šæ™ºèƒ½ä½“ä¸­ä½¿ç”¨å°æ•°æ®ï¼ˆ50å¯¹æ ·æœ¬ï¼‰è¿›è¡Œç‰¹å®šè§’è‰²æŒ‡ä»¤è°ƒæ•´å¯ä»¥è¿›ä¸€æ­¥æé«˜VulTrialçš„æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº†139.89%å’Œ118.30%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†å¢åŠ æ™ºèƒ½ä½“äº¤äº’æ¬¡æ•°å¯¹VulTrialæ€»ä½“æ€§èƒ½çš„å½±å“ã€‚è™½ç„¶å¤šæ™ºèƒ½ä½“è®¾ç½®ç”±äºå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨é‡è€Œå¤©ç„¶åœ°å¯¼è‡´æ›´é«˜çš„æˆæœ¬ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†VulTrialåº”ç”¨äºæˆæœ¬æ•ˆç›Šé«˜çš„æ¨¡å‹ï¼ˆå¦‚GPT-3.5ï¼‰ï¼Œåœ¨å•æ™ºèƒ½ä½“ç¯å¢ƒä¸­ç›¸æ¯”GPT-4oå¯ä»¥æé«˜æ€§èƒ½è¾¾69.89%ï¼ŒåŒæ—¶æ€»ä½“æˆæœ¬æ›´ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10961v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVulTrialçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå¢å¼ºè‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å››ç§ç‰¹å®šè§’è‰²æ™ºèƒ½ä½“ï¼ŒåŒ…æ‹¬å®‰å…¨ç ”ç©¶äººå‘˜ã€ä»£ç ä½œè€…ã€è°ƒè§£å‘˜å’Œå®¡æŸ¥å§”å‘˜ä¼šã€‚å®éªŒè¡¨æ˜ï¼ŒVulTrialåœ¨å•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨GPT-4oæ—¶æ€§èƒ½æå‡102.39%å’Œ84.17%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¡¨æ˜ï¼Œåœ¨å°‘é‡æ•°æ®ä¸‹å¯¹å¤šæ™ºèƒ½ä½“çš„ç‰¹å®šè§’è‰²æŒ‡ä»¤è¿›è¡Œè°ƒæ•´ï¼Œå¯è¿›ä¸€æ­¥æé«˜VulTrialçš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œæ¢è®¨äº†å¢åŠ æ™ºèƒ½ä½“äº¤äº’å¯¹VulTrialæ€§èƒ½çš„å½±å“ã€‚è™½ç„¶å¤šæ™ºèƒ½ä½“è®¾ç½®å¯¼è‡´æ›´é«˜çš„æˆæœ¬ï¼Œä½†ä½¿ç”¨åƒGPT-3.5è¿™æ ·çš„ä½æˆæœ¬æ¨¡å‹ï¼ŒVulTrialçš„æ€§èƒ½åœ¨å•æ™ºèƒ½ä½“è®¾ç½®ä¸­å¯ä»¥æé«˜69.89%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VulTrialæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹ã€‚</li>
<li>VulTrialåŒ…å«å››ç§ç‰¹å®šè§’è‰²æ™ºèƒ½ä½“ï¼šå®‰å…¨ç ”ç©¶äººå‘˜ã€ä»£ç ä½œè€…ã€è°ƒè§£å‘˜å’Œå®¡æŸ¥å§”å‘˜ä¼šã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜ï¼ŒVulTrialåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨GPT-4oæ—¶æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>åœ¨å°‘é‡æ•°æ®ä¸‹ï¼Œå¯¹å¤šæ™ºèƒ½ä½“çš„ç‰¹å®šè§’è‰²æŒ‡ä»¤è°ƒæ•´å¯è¿›ä¸€æ­¥æé«˜VulTrialçš„æ€§èƒ½ã€‚</li>
<li>å¢åŠ æ™ºèƒ½ä½“äº¤äº’å¯¹VulTrialçš„æ€§èƒ½æœ‰ç§¯æå½±å“ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“è®¾ç½®è™½ç„¶æˆæœ¬è¾ƒé«˜ï¼Œä½†ä½¿ç”¨ä½æˆæœ¬æ¨¡å‹å¦‚GPT-3.5å¯ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>VulTrialæ¡†æ¶å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ï¼Œä¸ºè‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8b5894aa5a1181b69a2b6b0e5542ae8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f02dfd7ae9bf5213fda5b835a601313f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb30327ae2cc17c4c81d7ca7a5648fce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84c953f5f56b635657adde6622d152f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a75b8ae405923d798ad76eeb1893e9f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6bae18a09773e9af69968c91d6ec562.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model"><a href="#Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model" class="headerlink" title="Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model"></a>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model</h2><p><strong>Authors:Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He</strong></p>
<p>In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>. </p>
<blockquote>
<p>åœ¨çœ¼ç§‘æ‰‹æœ¯ä¸­ï¼Œå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè§£è¯»æ‰‹æœ¯è§†é¢‘å¹¶é¢„æµ‹åç»­æ“ä½œçš„AIç³»ç»Ÿï¼Œéœ€è¦å¤§é‡çš„å¸¦æœ‰é«˜è´¨é‡æ³¨é‡Šçš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ç”±äºéšç§é—®é¢˜å’ŒåŠ³åŠ¨æ¶ˆè€—ï¼Œè¿™äº›è§†é¢‘çš„æ”¶é›†éå¸¸å›°éš¾ã€‚æ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œå®ƒå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Ophoraï¼Œä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘çš„å¼€åˆ›æ€§æ¨¡å‹ã€‚ä¸ºäº†æ„å»ºOphoraï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ•´ç†ç®¡é“ï¼Œå°†å™è¿°æ€§çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡çš„é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡16ä¸‡ä¸ªè§†é¢‘æŒ‡ä»¤å¯¹ï¼Œå³Ophora-160Kã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¸è¿›çš„è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆï¼Œä»¥ä»ä¸€ä¸ªåœ¨å¤©ç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„T2Væ¨¡å‹è½¬ç§»ä¸°å¯Œçš„æ—¶ç©ºçŸ¥è¯†ï¼Œç”¨äºåŸºäºOphora-160Kçš„éšç§ä¿æŠ¤çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆã€‚é€šè¿‡å¯¹è§†é¢‘è´¨é‡çš„å®šé‡åˆ†æå’Œçœ¼ç§‘åŒ»ç”Ÿçš„åé¦ˆè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒOphoraå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆé€¼çœŸå’Œå¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†Ophoraåœ¨çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07449v3">PDF</a> Early accepted in MICCAI25</p>
<p><strong>Summary</strong></p>
<p>ä¸€ä¸ªçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆçš„æ–°æ¨¡å‹è¢«æå‡ºï¼Œåä¸ºâ€œOphoraâ€ã€‚å®ƒé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ï¼Œè§£å†³äº†ç”±äºéšç§é—®é¢˜å’ŒåŠ³åŠ¨åŠ›æ¶ˆè€—å¯¼è‡´çš„é«˜è´¨é‡çœ¼ç§‘æ‰‹æœ¯è§†é¢‘éš¾ä»¥æ”¶é›†çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ç»¼åˆæ•°æ®æ•´ç†ç®¡é“å°†å™äº‹çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆä»é¢„å…ˆè®­ç»ƒçš„è‡ªç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸­è½¬ç§»æ—¶ç©ºçŸ¥è¯†ã€‚å®éªŒè¯æ˜ï¼ŒOphoraå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”ŸæˆçœŸå®å¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ï¼Œå¹¶æ”¯æŒçœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.Ophoraæ˜¯ä¸€ä¸ªèƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘çš„æ¨¡å‹ã€‚<br>2.ä¸ºäº†è§£å†³é«˜è´¨é‡çœ¼ç§‘æ‰‹æœ¯è§†é¢‘éš¾ä»¥æ”¶é›†çš„é—®é¢˜ï¼Œä½¿ç”¨äº†ç»¼åˆæ•°æ®æ•´ç†ç®¡é“ï¼ˆComprehensive Data Curation pipelineï¼‰ã€‚<br>3.æå‡ºäº†ä¸€ä¸ªæ¸è¿›å¼è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆï¼ˆProgressive Video-Instruction Tuning schemeï¼‰ï¼Œç”¨äºä»é¢„å…ˆè®­ç»ƒçš„è‡ªç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸­è½¬ç§»æ—¶ç©ºçŸ¥è¯†ã€‚<br>4.å®éªŒè¯æ˜äº†Ophoraå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”ŸæˆçœŸå®å¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚<br>5.Ophoraå…·æœ‰æ”¯æŒçœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡çš„èƒ½åŠ›ã€‚<br>6.è¯¥æ¨¡å‹çš„åº”ç”¨å¯ä»¥ç¼“è§£é«˜è´¨é‡çœ¼ç§‘æ‰‹æœ¯è§†é¢‘èµ„æºçš„ç¨€ç¼ºé—®é¢˜ï¼Œä¿ƒè¿›çœ¼ç§‘æ‰‹æœ¯ç›¸å…³ç ”ç©¶å’ŒåŸ¹è®­çš„å‘å±•ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6b725defca02e998337fa7052f69802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4ec3b71a8c06c8f6d824bb3db7290c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cca6f327a9e0a5dece7366cde8a1f565.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding"><a href="#VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding" class="headerlink" title="VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding"></a>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding</h2><p><strong>Authors:Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber</strong></p>
<p>Synthetic video generation has gained significant attention for its realism and broad applications, but remains prone to violations of common sense and physical laws. This highlights the need for reliable abnormality detectors that understand such principles and are robust to hallucinations. To address this, we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from synthetic videos generated by models like Veo2, Sora, and Kling, paired with expert-crafted counterintuitive QA to evaluate the critical thinking abilities of Multi-modal Large Language Models (MLLMs) on abnormalities that are perceptually obvious to humans but often hallucinated due to language priors. VideoHallu evaluates MLLMsâ€™ abnormality detection abilities with examples across alignment, consistency, commonsense, and physics. We benchmark SOTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and VideoChat-R1. We observe that these models perform well on many real-world benchmarks like MVBench and MovieChat, but still struggle with basic physics-based and commonsense reasoning in synthetic videos. We further show that post-training with Group Relative Policy Optimization (GRPO), using curriculum learning on datasets combining video QA with counterintuitive commonsense and physics reasoning over real and synthetic videos, improves MLLMsâ€™ abnormality detection and critical thinking, demonstrating the value of targeted training for improving their understanding of commonsense and physical laws. </p>
<blockquote>
<p>è§†é¢‘ç”ŸæˆæŠ€æœ¯å› å…¶çœŸå®æ„Ÿå’Œå¹¿æ³›åº”ç”¨è€Œå¤‡å—å…³æ³¨ï¼Œä½†ä»ç„¶å­˜åœ¨è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„é—®é¢˜ã€‚è¿™å¼ºè°ƒäº†å¯¹å¯é å¼‚å¸¸æ£€æµ‹å™¨çš„éœ€æ±‚ï¼Œè¿™äº›æ£€æµ‹å™¨éœ€è¦ç†è§£è¿™äº›åŸç†ï¼Œå¹¶å¯¹å¹»è§‰å…·æœ‰é²æ£’æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoHalluï¼Œè¿™æ˜¯ä¸€ä¸ªç”±åˆæˆè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Veo2ã€Soraå’ŒKlingï¼‰ç”Ÿæˆçš„è¶…è¿‡3000ä¸ªè§†é¢‘é—®ç­”å¯¹ç»„æˆçš„åŸºå‡†æµ‹è¯•é›†ã€‚è¿™äº›é—®ç­”å¯¹ä¸ä¸“å®¶è®¾è®¡çš„åç›´è§‰é—®ç­”ç›¸ç»“åˆï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ï¼Œè¿™äº›å¼‚å¸¸å¯¹äººç±»æ¥è¯´æ˜¯æ„ŸçŸ¥æ˜æ˜¾çš„ï¼Œä½†ç”±äºè¯­è¨€å…ˆéªŒçŸ¥è¯†å¾€å¾€ä¼šäº§ç”Ÿå¹»è§‰ã€‚VideoHallué€šè¿‡å®ä¾‹è¯„ä¼°MLLMsåœ¨å¯¹é½ã€ä¸€è‡´æ€§ã€å¸¸è¯†å’Œç‰©ç†æ–¹é¢çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬GPT-4oã€Gemini-2.5-Proã€Qwen2.5-VLã€Video-R1å’ŒVideoChat-Råœ¨å†…çš„æœ€æ–°MLLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨MVBenchå’ŒMovieChatç­‰ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆæˆè§†é¢‘çš„åŸºäºç‰©ç†å’Œå¸¸è¯†çš„æ¨ç†æ–¹é¢ä»ç„¶é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œä½¿ç”¨ç»„åˆè§†é¢‘é—®ç­”ä¸åç›´è§‰å¸¸è¯†å’Œç‰©ç†æ¨ç†çš„æ•°æ®é›†è¿›è¡Œè¯¾ç¨‹å­¦ä¹ çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åè®­ç»ƒï¼Œå¯ä»¥æé«˜MLLMsçš„å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ï¼Œè¯æ˜äº†æœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒå¯¹äºæé«˜ä»–ä»¬å¯¹å¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„ç†è§£çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01481v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åˆæˆè§†é¢‘ç”ŸæˆæŠ€æœ¯çš„ç°å®åº”ç”¨åŠå…¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„è¿åã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†VideoHalluåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡è¶…è¿‡3000ä¸ªè§†é¢‘é—®ç­”å¯¹æ¥è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚æ–‡ç« è¿˜æ¢è®¨äº†å½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆæˆè§†é¢‘ä¸­çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†é€šè¿‡è¯¾ç¨‹å­¦ä¹ ä½¿ç”¨ç»„åˆç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥æ”¹å–„æ¨¡å‹å¯¹å¸¸è¯†å’Œç‰©ç†å®šå¾‹ç†è§£çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆè§†é¢‘ç”ŸæˆæŠ€æœ¯å—åˆ°å…³æ³¨ï¼Œä½†å­˜åœ¨è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„é—®é¢˜ã€‚</li>
<li>éœ€è¦å¯é çš„å¼‚å¸¸æ£€æµ‹å™¨æ¥ç†è§£å¸¸è¯†å¹¶æŠµæŠ—å¹»è§‰ã€‚</li>
<li>VideoHalluåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆæˆè§†é¢‘ä¸­çš„è¡¨ç°å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰©ç†å’Œå¸¸è¯†æ¨ç†æ–¹é¢ã€‚</li>
<li>æ–‡ç« å¼•å…¥Group Relative Policy Optimization (GRPO) æ–¹æ³•è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒï¼Œæé«˜æ¨¡å‹å¯¹å¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„ç†è§£ã€‚</li>
<li>é€šè¿‡è¯¾ç¨‹å­¦ä¹ ä½¿ç”¨GRPOå¯ä»¥æ”¹å–„MLLMsçš„å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a1dd7cb2c1924ce6e6e388a2cfd747a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c0e565c33c36de724192494cf59dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd791c7ebf56cd412d8edaf50d4d8b75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c07288c94070b78a2e6183c618ed95a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d4f30a27d12edd17a705e7e98a27eba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eba42bf105f2f69700259d4c77c4224.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Task-Specific-Data-Selection-for-Instruction-Tuning-via-Monosemantic-Neuronal-Activations"><a href="#Task-Specific-Data-Selection-for-Instruction-Tuning-via-Monosemantic-Neuronal-Activations" class="headerlink" title="Task-Specific Data Selection for Instruction Tuning via Monosemantic   Neuronal Activations"></a>Task-Specific Data Selection for Instruction Tuning via Monosemantic   Neuronal Activations</h2><p><strong>Authors:Da Ma, Gonghu Shang, Zhi Chen, Libo Qin, Yijie Luo, Lei Pan, Shuai Fan, Lu Chen, Kai Yu</strong></p>
<p>Instruction tuning improves the ability of large language models (LLMs) to follow diverse human instructions, but achieving strong performance on specific target tasks remains challenging. A critical bottleneck is selecting the most relevant data to maximize task-specific performance. Existing data selection approaches include unstable influence-based methods and more stable distribution alignment methods, the latter of which critically rely on the underlying sample representation. In practice, most distribution alignment methods, from shallow features (e.g., BM25) to neural embeddings (e.g., BGE, LLM2Vec), may fail to capture how the model internally processes samples. To bridge this gap, we adopt a model-centric strategy in which each sample is represented by its neuronal activation pattern in the model, directly reflecting internal computation. However, directly using raw neuron activations leads to spurious similarity between unrelated samples due to neuron polysemanticity, where a single neuron may respond to multiple, unrelated concepts. To address this, we employ sparse autoencoders to disentangle polysemantic activations into sparse, monosemantic representations, and introduce a dedicated similarity metric for this space to better identify task-relevant data. Comprehensive experiments across multiple instruction datasets, models, tasks, and selection ratios show that our approach consistently outperforms existing data selection baselines in both stability and task-specific performance. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éµå¾ªå„ç§äººç±»æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œä½†åœ¨ç‰¹å®šç›®æ ‡ä»»åŠ¡ä¸Šå®ç°å¼ºåŠ²è¡¨ç°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸€ä¸ªå…³é”®çš„ç“¶é¢ˆåœ¨äºé€‰æ‹©æœ€ç›¸å…³çš„æ•°æ®ä»¥æœ€å¤§åŒ–ç‰¹å®šä»»åŠ¡æ€§èƒ½ã€‚ç°æœ‰çš„æ•°æ®é€‰æ‹©æ–¹æ³•åŒ…æ‹¬åŸºäºä¸ç¨³å®šå½±å“çš„æ–¹æ³•å’Œæ›´ç¨³å®šçš„åˆ†å¸ƒå¯¹é½æ–¹æ³•ï¼Œåè€…ä¸¥é‡ä¾èµ–äºåº•å±‚æ ·æœ¬è¡¨ç¤ºã€‚åœ¨å®è·µä¸­ï¼Œå¤§å¤šæ•°åˆ†å¸ƒå¯¹é½æ–¹æ³•ï¼Œä»æµ…å±‚ç‰¹å¾ï¼ˆä¾‹å¦‚BM25ï¼‰åˆ°ç¥ç»åµŒå…¥ï¼ˆä¾‹å¦‚BGEï¼ŒLLM2Vecï¼‰ï¼Œéƒ½å¯èƒ½æ— æ³•æ•æ‰æ¨¡å‹å†…éƒ¨å¦‚ä½•å¤„ç†æ ·æœ¬çš„æ–¹å¼ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„ç­–ç•¥ï¼Œå…¶ä¸­æ¯ä¸ªæ ·æœ¬éƒ½ç”±å…¶åœ¨æ¨¡å‹ä¸­çš„ç¥ç»å…ƒæ¿€æ´»æ¨¡å¼è¡¨ç¤ºï¼Œç›´æ¥åæ˜ å†…éƒ¨è®¡ç®—ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç¥ç»å…ƒæ¿€æ´»ä¼šå¯¼è‡´ç”±äºç¥ç»å…ƒçš„å¤šä¹‰æ€§ï¼Œå¯¼è‡´ä¸ç›¸å…³æ ·æœ¬ä¹‹é—´å‡ºç°è™šå‡ç›¸ä¼¼æ€§ï¼Œå•ä¸ªç¥ç»å…ƒå¯èƒ½å¯¹å¤šä¸ªä¸ç›¸å…³çš„æ¦‚å¿µéƒ½æœ‰ååº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨æ¥è§£è€¦å¤šä¹‰æ¿€æ´»ï¼Œå½¢æˆç¨€ç–ã€å•ä¹‰è¡¨ç¤ºï¼Œå¹¶ä¸ºæ­¤ç©ºé—´å¼•å…¥ä¸“ç”¨ç›¸ä¼¼æ€§åº¦é‡ï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«ä»»åŠ¡ç›¸å…³æ•°æ®ã€‚åœ¨å¤šä¸ªæŒ‡ä»¤æ•°æ®é›†ã€æ¨¡å‹ã€ä»»åŠ¡å’Œé€‰æ‹©æ¯”ä¾‹ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¨³å®šæ€§å’Œç‰¹å®šä»»åŠ¡æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰æ•°æ®é€‰æ‹©åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15573v2">PDF</a> preprint, (20 pages, 7 figures, 13 tables)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éµå¾ªå¤šæ ·åŒ–çš„äººç±»æŒ‡ä»¤æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é€‰æ‹©æœ€ç›¸å…³çš„æ•°æ®ä»¥æœ€å¤§åŒ–ç‰¹å®šä»»åŠ¡æ€§èƒ½æ–¹é¢çš„ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„ç­–ç•¥ï¼Œé€šè¿‡ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨è§£å†³ç¥ç»å…ƒæ¿€æ´»çš„å¤šä¹‰æ€§é—®é¢˜ï¼Œå¹¶å¼•å…¥ä¸“é—¨çš„ç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«ä»»åŠ¡ç›¸å…³æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¨³å®šæ€§å’Œç‰¹å®šä»»åŠ¡æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰æ•°æ®é€‰æ‹©åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éµå¾ªå¤šæ ·åŒ–çš„äººç±»æŒ‡ä»¤æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡ã€‚</li>
<li>æ•°æ®é€‰æ‹©å¯¹äºLLMçš„ä»»åŠ¡ç‰¹å®šæ€§èƒ½è‡³å…³é‡è¦ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨ä¸ç¨³å®šå’Œå¤šä¹‰æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„æ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡ç¥ç»æ¿€æ´»æ¨¡å¼åæ˜ æ ·æœ¬çš„å†…éƒ¨è®¡ç®—ã€‚</li>
<li>ç¥ç»å…ƒæ¿€æ´»çš„å¤šä¹‰æ€§ä¼šå¯¼è‡´æ— å…³æ ·æœ¬ä¹‹é—´çš„è™šå‡ç›¸ä¼¼æ€§ã€‚</li>
<li>é‡‡ç”¨äº†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨æ¥è§£å†³ç¥ç»å…ƒæ¿€æ´»çš„å¤šä¹‰æ€§é—®é¢˜ï¼Œå½¢æˆç¨€ç–ã€å•è¯­ä¹‰çš„è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«ä¸ä»»åŠ¡ç›¸å…³çš„æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71e39fe0208e731a195786bc2b229a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-940dada67fe1cf25ea0a56ceb894deb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1397cd8c9853ba5aa199c227bb6ffd43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6baece3178eeda22b50efeffdbad09ae.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="The-Lazy-Studentâ€™s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own"><a href="#The-Lazy-Studentâ€™s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own" class="headerlink" title="The Lazy Studentâ€™s Dream: ChatGPT Passing an Engineering Course on Its   Own"></a>The Lazy Studentâ€™s Dream: ChatGPT Passing an Engineering Course on Its   Own</h2><p><strong>Authors:Gokul Puthumanaillam, Timothy Bretl, Melkior Ornik</strong></p>
<p>This paper presents a comprehensive investigation into the capability of Large Language Models (LLMs) to successfully complete a semester-long undergraduate control systems course. Through evaluation of 115 course deliverables, we assess LLM performance using ChatGPT under a â€œminimal effortâ€ protocol that simulates realistic student usage patterns. The investigation employs a rigorous testing methodology across multiple assessment formats, from auto-graded multiple choice questions to complex Python programming tasks and long-form analytical writing. Our analysis provides quantitative insights into AIâ€™s strengths and limitations in handling mathematical formulations, coding challenges, and theoretical concepts in control systems engineering. The LLM achieved a B-grade performance (82.24%), approaching but not exceeding the class average (84.99%), with strongest results in structured assignments and greatest limitations in open-ended projects. The findings inform discussions about course design adaptation in response to AI advancement, moving beyond simple prohibition towards thoughtful integration of these tools in engineering education. Additional materials including syllabus, examination papers, design projects, and example responses can be found at the project website: <a target="_blank" rel="noopener" href="https://gradegpt.github.io/">https://gradegpt.github.io</a>. </p>
<blockquote>
<p>æœ¬æ–‡å…¨é¢æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆåŠŸå®Œæˆä¸€å­¦æœŸæœ¬ç§‘æ§åˆ¶ç³»ç»Ÿè¯¾ç¨‹çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹115ä»½è¯¾ç¨‹äº¤ä»˜æˆæœçš„è¯„ä»·ï¼Œæˆ‘ä»¬é‡‡ç”¨ChatGPTè¯„ä¼°LLMæ€§èƒ½ï¼Œéµå¾ªæ¨¡æ‹Ÿç°å®å­¦ç”Ÿä½¿ç”¨æ¨¡å¼çš„â€œæœ€å°åŠªåŠ›â€åè®®ã€‚è°ƒæŸ¥é‡‡ç”¨ä¸¥æ ¼çš„æµ‹è¯•æ–¹æ³•ï¼Œæ¶µç›–å¤šç§è¯„ä¼°å½¢å¼ï¼Œä»è‡ªåŠ¨åˆ†çº§çš„å®¢è§‚é¢˜åˆ°å¤æ‚çš„Pythonç¼–ç¨‹ä»»åŠ¡å’Œé•¿æ ¼å¼åˆ†æå†™ä½œã€‚æˆ‘ä»¬çš„åˆ†æä¸ºAIåœ¨å¤„ç†æ§åˆ¶ç³»ç»Ÿå·¥ç¨‹ä¸­çš„æ•°å­¦å…¬å¼ã€ç¼–ç¨‹æŒ‘æˆ˜å’Œç†è®ºæ¦‚å¿µæ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§æä¾›äº†å®šé‡è§è§£ã€‚LLMå–å¾—äº†Bçº§è¡¨ç°ï¼ˆ82.24%ï¼‰ï¼Œæ¥è¿‘ä½†æœªè¶…è¿‡ç­çº§å¹³å‡æ°´å¹³ï¼ˆ84.99%ï¼‰ï¼Œåœ¨ç»“æ„åŒ–ä½œä¸šæ–¹é¢è¡¨ç°æœ€ä½³ï¼Œåœ¨å¼€æ”¾å¼é¡¹ç›®ä¸­è¡¨ç°æœ€ä¸ºå—é™ã€‚è¿™äº›å‘ç°å¼•å‘äº†å…³äºé€‚åº”è¯¾ç¨‹è®¾è®¡çš„è®¨è®ºï¼Œä»¥åº”å¯¹äººå·¥æ™ºèƒ½çš„è¿›æ­¥ï¼Œè¶…è¶Šç®€å•çš„ç¦ä»¤ï¼Œæœç€åœ¨å·¥ç¨‹æ•™è‚²ä¸­æ·±æ€ç†Ÿè™‘åœ°æ•´åˆè¿™äº›å·¥å…·çš„æ–¹å‘å‘å±•ã€‚é¢å¤–ææ–™åŒ…æ‹¬æ•™å­¦å¤§çº²ã€è€ƒè¯•è¯•å·ã€è®¾è®¡é¡¹ç›®å’Œç¤ºä¾‹ç­”æ¡ˆï¼Œå¯åœ¨é¡¹ç›®ç½‘ç«™æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gradegpt.github.io./">https://gradegpt.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05760v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®Œæˆä¸€ä¸ªå­¦æœŸé•¿çš„æ§åˆ¶å·¥ç¨‹è¯¾ç¨‹æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹115ä»½è¯¾ç¨‹ä½œå“è¿›è¡Œè¯„ä»·ï¼Œé‡‡ç”¨æ¨¡æ‹ŸçœŸå®å­¦ç”Ÿä½¿ç”¨æ¨¡å¼çš„â€œæœ€å°åŠªåŠ›â€åè®®ï¼Œå¯¹ChatGPTè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶é‡‡ç”¨ä¸¥æ ¼çš„æµ‹è¯•æ–¹æ³•ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯„åˆ†çš„é€‰æ‹©é¢˜ã€å¤æ‚çš„Pythonç¼–ç¨‹ä»»åŠ¡å’Œé•¿å½¢å¼çš„åˆ†æå†™ä½œã€‚åˆ†ææä¾›äº†å…³äºAIåœ¨å¤„ç†æ•°å­¦å…¬å¼ã€ç¼–ç¨‹æŒ‘æˆ˜å’Œç†è®ºæ¦‚å¿µçš„ä¼˜ç‚¹å’Œå±€é™æ€§çš„å®šé‡è§è§£ã€‚LLMçš„æˆç»©è¾¾åˆ°äº†Bçº§ï¼ˆ82.24%ï¼‰ï¼Œæ¥è¿‘ä½†æœªè¶…è¿‡ç­çº§å¹³å‡æ°´å¹³ï¼ˆ84.99%ï¼‰ï¼Œåœ¨ç»“æ„åŒ–ä½œä¸šä¸­çš„è¡¨ç°æœ€ä½³ï¼Œåœ¨å¼€æ”¾æ€§é¡¹ç›®ä¸­çš„è¡¨ç°æœ€ä¸ºæœ‰é™ã€‚è¿™äº›å‘ç°å¼•å‘äº†å…³äºé€‚åº”AIå‘å±•çš„è¯¾ç¨‹è®¾è®¡é€‚åº”çš„è®¨è®ºï¼Œæå€¡è¶…è¶Šç®€å•ç¦æ­¢ï¼Œä»¥æœ‰è§åœ°çš„æ€åº¦æ•´åˆè¿™äº›å·¥å…·åœ¨å·¥ç¨‹æ•™è‚²ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®Œæˆæ§åˆ¶å·¥ç¨‹è¯¾ç¨‹çš„èƒ½åŠ›å¾—åˆ°å…¨é¢ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¯¹115ä»½è¯¾ç¨‹ä½œå“è¯„ä»·ï¼Œé‡‡ç”¨â€œæœ€å°åŠªåŠ›â€åè®®ä¸‹çš„ChatGPTè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¤šç§è¯„ä¼°å½¢å¼ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯„åˆ†é€‰æ‹©é¢˜ã€Pythonç¼–ç¨‹ä»»åŠ¡å’Œåˆ†æå†™ä½œã€‚</li>
<li>LLMåœ¨å¤„ç†æ•°å­¦å…¬å¼ã€ç¼–ç¨‹æŒ‘æˆ˜å’Œç†è®ºæ¦‚å¿µæ–¹é¢å±•ç°å‡ºä¼˜ç‚¹å’Œå±€é™æ€§ã€‚</li>
<li>LLMçš„æˆç»©è¾¾åˆ°Bçº§æ°´å¹³ï¼Œæ¥è¿‘ä½†æœªè¶…è¿‡ç­çº§å¹³å‡æ°´å¹³ã€‚</li>
<li>åœ¨ç»“æ„åŒ–ä½œä¸šä¸­çš„è¡¨ç°æœ€ä½³ï¼Œè€Œåœ¨å¼€æ”¾æ€§é¡¹ç›®ä¸­çš„è¡¨ç°ç›¸å¯¹æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-09a16e38fae383d356f3326f88953312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67bdee915f3f1854ce7502388fdf0d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89ff0a65bdd64b9f97d6185862a6f43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3b91291511b7b3b642c59b7d5279932.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8ab861defd4399182b8ef0f4a9ae66.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Mask-Enhanced-Autoregressive-Prediction-Pay-Less-Attention-to-Learn-More"><a href="#Mask-Enhanced-Autoregressive-Prediction-Pay-Less-Attention-to-Learn-More" class="headerlink" title="Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn   More"></a>Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn   More</h2><p><strong>Authors:Xialie Zhuang, Zhikai Jia, Jianjin Li, Zhenyu Zhang, Li Shen, Zheng Cao, Shiwei Liu</strong></p>
<p>Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latterâ€™s in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAPâ€™s effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the modelâ€™s focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«å‘ç°å­˜åœ¨å‡†ç¡®æ£€ç´¢å…³é”®ä¿¡æ¯çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Maskå¢å¼ºè‡ªå›å½’é¢„æµ‹ï¼ˆMEAPï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼ï¼Œå®ƒå°†Masked Language Modelingï¼ˆMLMï¼‰æ— ç¼é›†æˆåˆ°Next-Token Predictionï¼ˆNTPï¼‰ä¸­ï¼Œä»¥å¢å¼ºåè€…çš„ä¸Šä¸‹æ–‡å†…æ£€ç´¢èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒMEAPé¦–å…ˆéšæœºæ©ç›–ä¸€å°éƒ¨åˆ†è¾“å…¥æ ‡è®°ï¼Œç„¶åç›´æ¥ä½¿ç”¨ä»…è§£ç å™¨Transformerè¿›è¡Œæ ‡å‡†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è‡ªå›å½’ã€‚MEAPæ¶ˆé™¤äº†MLMå¯¹åŒå‘æ³¨æ„åŠ›æˆ–ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„éœ€æ±‚ï¼Œåœ¨é¢„è®­ç»ƒæˆ–æ¨ç†æœŸé—´æ²¡æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚å¯†é›†çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å…³é”®ä¿¡æ¯æ£€ç´¢å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸Šï¼ŒMEAPå¤§å¤§ä¼˜äºNTPï¼Œè€Œåœ¨å¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°åˆ™ä¸ä¹‹ç›¸å½“æˆ–æ›´å¥½ã€‚MEAPçš„ä¼˜åŠ¿è¿˜æ‰©å±•åˆ°æœ‰ç›‘ç£å¾®è°ƒï¼Œåœ¨â€œè¿·å¤±ä¸­é—´â€åœºæ™¯ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œæ¯”NTPé«˜å‡º11.77ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒMEAPçš„æœ‰æ•ˆæ€§æºäºå…¶é€šè¿‡å…³æ³¨å‡å°‘çš„éæ©ç æ ‡è®°é›†ä¿ƒè¿›æ›´å¯åˆ†è¾¨çš„æ³¨æ„åŠ›åˆ†æ•°çš„èƒ½åŠ›ã€‚è¿™ç§æœºåˆ¶æé«˜äº†æ¨¡å‹å¯¹ä»»åŠ¡ç›¸å…³ä¿¡å·çš„å…³æ³¨ï¼ŒåŒæ—¶å‡è½»äº†å‘¨è¾¹ä¸Šä¸‹æ–‡çš„å½±å“ã€‚è¿™äº›å‘ç°ä½¿MEAPæˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰å‰é€”çš„è®­ç»ƒèŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07490v2">PDF</a> 17 pages,7 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å‡†ç¡®æ£€ç´¢å…³é”®ä¿¡æ¯æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºMask-Enhanced Autoregressive Predictionï¼ˆMEAPï¼‰çš„ç®€å•æœ‰æ•ˆè®­ç»ƒèŒƒå¼ï¼Œå®ƒå°†Masked Language Modelingï¼ˆMLMï¼‰æ— ç¼é›†æˆåˆ°Next-Token Predictionï¼ˆNTPï¼‰ä¸­ï¼Œä»¥å¢å¼ºå…¶ä¸Šä¸‹æ–‡æ£€ç´¢èƒ½åŠ›ã€‚MEAPé€šè¿‡éšæœºæ©ç›–ä¸€å°éƒ¨åˆ†è¾“å…¥æ ‡è®°ï¼Œç„¶åä½¿ç”¨ä»…è§£ç å™¨Transformerè¿›è¡Œæ ‡å‡†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è‡ªå›å½’ï¼Œæ— éœ€åŒå‘æ³¨æ„åŠ›æˆ–ç¼–ç å™¨-è§£ç å™¨æ¶æ„è¿›è¡ŒMLMï¼Œå› æ­¤åœ¨é¢„è®­ç»ƒæˆ–æ¨ç†æœŸé—´æ²¡æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚å®éªŒè¡¨æ˜ï¼ŒMEAPåœ¨å…³é”®ä¿¡æ¯æ£€ç´¢å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºNTPï¼ŒåŒæ—¶åœ¨å¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æŒå¹³æˆ–æ›´å¥½ã€‚MEAPçš„ä¼˜åŠ¿è¿˜æ‰©å±•åˆ°æœ‰ç›‘ç£å¾®è°ƒï¼Œåœ¨ä¸¢å¤±ä¸­é—´åœºæ™¯çš„æƒ…å†µä¸‹è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œæ¯”NTPé«˜å‡º11.77ä¸ªç™¾åˆ†ç‚¹ã€‚åˆ†æè¡¨æ˜ï¼ŒMEAPçš„æœ‰æ•ˆæ€§æºäºå…¶é€šè¿‡é›†ä¸­åœ¨å‡å°‘çš„éæ©ç æ ‡è®°ä¸Šè€Œä¿ƒè¿›æ›´æ˜æ˜¾çš„æ³¨æ„åŠ›å¾—åˆ†çš„èƒ½åŠ›ã€‚è¿™ç§æœºåˆ¶æé«˜äº†æ¨¡å‹å¯¹ä»»åŠ¡ç›¸å…³ä¿¡å·çš„å…³æ³¨ï¼ŒåŒæ—¶å‡è½»äº†å‘¨è¾¹ä¸Šä¸‹æ–‡çš„å½±å“ã€‚è¿™äº›å‘ç°ä½¿MEAPæˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰å‰é€”çš„è®­ç»ƒèŒƒå¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨å‡†ç¡®æ£€ç´¢å…³é”®ä¿¡æ¯æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Mask-Enhanced Autoregressive Prediction (MEAP)è®­ç»ƒèŒƒå¼ç»“åˆäº†Masked Language Modeling (MLM)å’ŒNext-Token Prediction (NTP)ã€‚</li>
<li>MEAPæé«˜äº†LLMçš„ä¸Šä¸‹æ–‡æ£€ç´¢èƒ½åŠ›ï¼Œä¸”æ— éœ€é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚</li>
<li>MEAPåœ¨å…³é”®ä¿¡æ¯æ£€ç´¢å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºNTPã€‚</li>
<li>MEAPåœ¨ä¸¢å¤±ä¸­é—´åœºæ™¯çš„æœ‰ç›‘ç£å¾®è°ƒä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>MEAPé€šè¿‡ä¿ƒè¿›æ›´æ˜æ˜¾çš„æ³¨æ„åŠ›å¾—åˆ†ï¼Œæé«˜äº†æ¨¡å‹å¯¹ä»»åŠ¡ç›¸å…³ä¿¡å·çš„å…³æ³¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c23df6711654eedbc0dfeb5f98e913bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8676229573b6660592867fc418dd516.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df906f85a73cfc1218684c0d7882c8d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40fbe24a1a2ade884846750c842f9d4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0405eb1b7b995a22297ba6d12fe008a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9903c19398dec17a2298e0fc5680ac0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1851c3b489296b1915fc0f2a6ff0c84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a38603ae0dd3c72537de8c52f8b62aff.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-129b9c165e2e601f65dd2ea8be3f865e.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-20  Let the Trial Begin A Mock-Court Approach to Vulnerability Detection   using LLM-Based Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-44e5f538fd47a6380935eca2cf22ce5f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-20  SoftCoT++ Test-Time Scaling with Soft Chain-of-Thought Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
