<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-05-20  Modeling cognitive processes of natural reading with transformer-based   Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c8676229573b6660592867fc418dd516.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-20-更新"><a href="#2025-05-20-更新" class="headerlink" title="2025-05-20 更新"></a>2025-05-20 更新</h1><h2 id="Modeling-cognitive-processes-of-natural-reading-with-transformer-based-Language-Models"><a href="#Modeling-cognitive-processes-of-natural-reading-with-transformer-based-Language-Models" class="headerlink" title="Modeling cognitive processes of natural reading with transformer-based   Language Models"></a>Modeling cognitive processes of natural reading with transformer-based   Language Models</h2><p><strong>Authors:Bruno Bianchi, Fermín Travi, Juan E. Kamienkowski</strong></p>
<p>Recent advances in Natural Language Processing (NLP) have led to the development of highly sophisticated language models for text generation. In parallel, neuroscience has increasingly employed these models to explore cognitive processes involved in language comprehension. Previous research has shown that models such as N-grams and LSTM networks can partially account for predictability effects in explaining eye movement behaviors, specifically Gaze Duration, during reading. In this study, we extend these findings by evaluating transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate this relationship. Our results indicate that these architectures outperform earlier models in explaining the variance in Gaze Durations recorded from Rioplantense Spanish readers. However, similar to previous studies, these models still fail to account for the entirety of the variance captured by human predictability. These findings suggest that, despite their advancements, state-of-the-art language models continue to predict language in ways that differ from human readers. </p>
<blockquote>
<p>自然语言处理（NLP）的最新进展推动了用于文本生成的先进语言模型的开发。与此同时，神经科学也越来越倾向于利用这些模型来探索语言理解所涉及的认知过程。先前的研究表明，N-grams和LSTM网络等模型可以在一定程度上解释阅读过程中的眼动行为，特别是注视持续时间（Gaze Duration）的预测效果。在这项研究中，我们通过评估基于transformer的模型（GPT2、LLaMA-7B和LLaMA2-7B）来进一步探讨这种关系，以扩展这些发现。我们的结果表明，这些架构在解释里翁兰特西班牙读者记录的注视持续时间（Gaze Durations）方面的方差时表现得优于早期模型。然而，与先前的研究类似，这些模型仍然不能完全解释人类预测所捕获的方差。这些发现表明，尽管取得了进展，但最先进的语言模型在预测语言方面仍然与人类读者的方式存在差异。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11485v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期自然语言处理（NLP）技术的进展推动了用于文本生成的先进语言模型的发展。同时，神经科学开始越来越多地利用这些模型探索语言理解过程中的认知过程。本研究评估了基于Transformer的模型（GPT2、LLaMA-7B和LLaMA2-7B），以进一步研究语言模型和眼动行为（尤其是阅读时的注视持续时间）之间的关系。结果显示，这些新模型在解释注视持续时间方面的方差表现优于早期模型，但仍不能完全解释人类预测性所产生的全部方差。这表明尽管取得了进展，但最先进的语言模型在预测人类阅读方式方面仍存在差异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自然语言处理（NLP）技术的最新进展推动了用于文本生成的先进语言模型的发展。</li>
<li>神经科学开始利用这些模型探索语言理解过程中的认知过程。</li>
<li>基于Transformer的模型（GPT2、LLaMA-7B和LLaMA2-7B）在解释阅读时的注视持续时间方面的表现优于早期模型。</li>
<li>这些新模型能够部分解释人类预测性在阅读过程中的作用。</li>
<li>尽管这些模型有所进展，但它们仍无法完全捕捉人类阅读方式的全部细节。</li>
<li>最先进的语言模型在预测人类阅读方式方面存在差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11485">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca089764c781ab0a7b590938d5f02345.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33d88ce45baddc72a1bd9c7fbd948542.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a0f0413d6d23dcea86a64807f3b6e2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbfd9594ebf7e073f8fdf46513226723.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HelpSteer3-Preference-Open-Human-Annotated-Preference-Data-across-Diverse-Tasks-and-Languages"><a href="#HelpSteer3-Preference-Open-Human-Annotated-Preference-Data-across-Diverse-Tasks-and-Languages" class="headerlink" title="HelpSteer3-Preference: Open Human-Annotated Preference Data across   Diverse Tasks and Languages"></a>HelpSteer3-Preference: Open Human-Annotated Preference Data across   Diverse Tasks and Languages</h2><p><strong>Authors:Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans, Yi Dong, Oleksii Kuchaiev</strong></p>
<p>Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/HelpSteer3#preference">https://huggingface.co/datasets/nvidia/HelpSteer3#preference</a> </p>
<blockquote>
<p>偏好数据集对于使用人类反馈强化学习（RLHF）训练通用领域、遵循指令的语言模型至关重要。每次后续的数据发布都提高了对未来数据收集的期望，这意味着需要不断提高公开可用偏好数据的质量和多样性。为了解决这一需求，我们推出了HelpSteer3-Preference，这是一个采用CC-BY-4.0许可、高质量、人类注释的偏好数据集，包含超过40,000个样本。这些样本涵盖了大型语言模型（LLM）的多样化现实世界应用，包括与STEM、编码和多语言场景相关的任务。使用HelpSteer3-Preference，我们训练的奖励模型（RM）在RM-Bench（82.4%）和JudgeBench（73.7%）上实现了最佳性能。这相比现有RM的最佳报告结果有了实质性的改进（绝对提高了约10%）。我们还展示了HelpSteer3-Preference如何用于训练生成式RM，以及如何使用我们的RM将政策模型与RLHF对齐。数据集（CC-BY-4.0）：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/HelpSteer3#preference">https://huggingface.co/datasets/nvidia/HelpSteer3#preference</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11475v1">PDF</a> 38 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>帮助Steer3偏好数据集对于使用强化学习从人类反馈（RLHF）训练通用领域、遵循指令的语言模型至关重要。该数据集是许可的（CC-BY-4.0），高质量，包含超过40，000个人工注释的偏好样本。这些样本涵盖了大型语言模型（LLM）的多种现实世界应用，包括STEM、编码和多语种任务。使用帮助Steer3偏好数据集训练的奖励模型（RM）在RM-Bench和JudgeBench上表现卓越，相较于现有RM达到绝对提升约10%。此外，本文展示了如何将帮助Steer3偏好数据集应用于训练生成式RM，以及如何通过对齐策略模型使用我们的RM实现RLHF。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>帮助Steer3偏好数据集是训练通用领域语言模型的重要资源，尤其在使用强化学习从人类反馈（RLHF）时。</li>
<li>数据集包含超过40，000个人工注释的偏好样本，保证了数据质量。</li>
<li>数据集涵盖大型语言模型（LLM）的多种应用，包括STEM、编码和多语种任务，体现了数据的多样性。</li>
<li>使用帮助Steer3偏好数据集训练的奖励模型（RM）在RM-Bench和JudgeBench上的表现显著，相较于现有模型有绝对提升约10%。</li>
<li>该研究展示了如何将帮助Steer3偏好数据集应用于训练生成式RM。</li>
<li>通过策略模型与RM的对齐，实现了RLHF的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11475">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4ccb60b90803797e1276c09922790813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c5f0bb4a4ca8738632eafa4151dacc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-833675bcc9041121755a0dac9fce6afb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ProxyPrompt-Securing-System-Prompts-against-Prompt-Extraction-Attacks"><a href="#ProxyPrompt-Securing-System-Prompts-against-Prompt-Extraction-Attacks" class="headerlink" title="ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks"></a>ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks</h2><p><strong>Authors:Zhixiong Zhuang, Maria-Irina Nicolae, Hui-Po Wang, Mario Fritz</strong></p>
<p>The integration of large language models (LLMs) into a wide range of applications has highlighted the critical role of well-crafted system prompts, which require extensive testing and domain expertise. These prompts enhance task performance but may also encode sensitive information and filtering criteria, posing security risks if exposed. Recent research shows that system prompts are vulnerable to extraction attacks, while existing defenses are either easily bypassed or require constant updates to address new threats. In this work, we introduce ProxyPrompt, a novel defense mechanism that prevents prompt leakage by replacing the original prompt with a proxy. This proxy maintains the original task’s utility while obfuscating the extracted prompt, ensuring attackers cannot reproduce the task or access sensitive information. Comprehensive evaluations on 264 LLM and system prompt pairs show that ProxyPrompt protects 94.70% of prompts from extraction attacks, outperforming the next-best defense, which only achieves 42.80%. </p>
<blockquote>
<p>将大型语言模型（LLM）集成到各种应用中，凸显了精心设计的系统提示的关键作用，这需要广泛的测试和领域专业知识。这些提示提高了任务性能，但也可能编码敏感信息和过滤标准，如果暴露出来会构成安全风险。最近的研究表明，系统提示容易受到提取攻击，而现有的防御手段要么容易被绕过，要么需要不断更新以应对新威胁。在这项工作中，我们引入了ProxyPrompt，这是一种新型防御机制，通过用代理替换原始提示来防止提示泄露。该代理保持原始任务的实用性，同时模糊提取的提示，确保攻击者无法复制任务或访问敏感信息。对264个LLM和系统提示对的全面评估表明，ProxyPrompt在防止提取攻击方面保护了94.70%的提示，优于次之的最佳防御手段（仅达到42.80%）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11459v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>大型语言模型集成应用凸显精心设计的系统提示重要性，需测试与领域专业知识。系统提示提升任务性能，但可能包含敏感信息及筛选标准，泄露存在安全风险。最新研究显示系统提示易受提取攻击，现有防御手段易失效或需频繁更新。本研究介绍ProxyPrompt，一种新型防御机制，通过替换原始提示来防止提示泄露。ProxyPrompt保持任务实用性同时掩盖提取的提示，确保攻击者无法复制任务或访问敏感信息。对264个大型语言模型和系统提示对的全面评估显示，ProxyPrompt保护94.7%的提示免受提取攻击，优于现有最佳防御手段的42.8%。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>大型语言模型集成应用需要重视系统提示的设计和测试。</li>
<li>系统提示在提升任务性能的同时可能包含敏感信息，存在安全风险。</li>
<li>最新研究显示系统提示容易受到提取攻击。</li>
<li>现有防御手段存在缺陷，易失效或需要频繁更新。</li>
<li>ProxyPrompt是一种新型防御机制，通过替换原始提示来防止提示泄露。</li>
<li>ProxyPrompt能有效保护大部分提示免受提取攻击，效果优于现有最佳防御手段。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11459">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bc4e25c004065576b42267f65a531077.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f991d3f482900b517f262fc16542b4d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd38c04e8b5d4a06da8eaf362f0802fb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLMs-unlock-new-paths-to-monetizing-exploits"><a href="#LLMs-unlock-new-paths-to-monetizing-exploits" class="headerlink" title="LLMs unlock new paths to monetizing exploits"></a>LLMs unlock new paths to monetizing exploits</h2><p><strong>Authors:Nicholas Carlini, Milad Nasr, Edoardo Debenedetti, Barry Wang, Christopher A. Choquette-Choo, Daphne Ippolito, Florian Tramèr, Matthew Jagielski</strong></p>
<p>We argue that Large language models (LLMs) will soon alter the economics of cyberattacks. Instead of attacking the most commonly used software and monetizing exploits by targeting the lowest common denominator among victims, LLMs enable adversaries to launch tailored attacks on a user-by-user basis. On the exploitation front, instead of human attackers manually searching for one difficult-to-identify bug in a product with millions of users, LLMs can find thousands of easy-to-identify bugs in products with thousands of users. And on the monetization front, instead of generic ransomware that always performs the same attack (encrypt all your data and request payment to decrypt), an LLM-driven ransomware attack could tailor the ransom demand based on the particular content of each exploited device.   We show that these two attacks (and several others) are imminently practical using state-of-the-art LLMs. For example, we show that without any human intervention, an LLM finds highly sensitive personal information in the Enron email dataset (e.g., an executive having an affair with another employee) that could be used for blackmail. While some of our attacks are still too expensive to scale widely today, the incentives to implement these attacks will only increase as LLMs get cheaper. Thus, we argue that LLMs create a need for new defense-in-depth approaches. </p>
<blockquote>
<p>我们认为大型语言模型（LLM）将很快改变网络攻击的经济模式。LLM使对手能够针对每个用户发起定制攻击，而不是攻击最常用的软件并通过对受害者中的最低公分母进行定位来利用漏洞实现盈利。在利用漏洞方面，利用人类攻击者手动在拥有数百万用户的产品中寻找难以识别的漏洞，而LLM可以在拥有数千用户的产品中找到成千上万容易识别的漏洞。在盈利方面，与始终执行相同攻击的通用勒索软件相比，由LLM驱动的勒索软件攻击可以根据每个受攻击设备的特定内容定制赎金要求。我们展示了这两种攻击（以及其他几种攻击）都是即将发生并且是实用的，采用的都是最新的大型语言模型。例如，我们展示了在没有人类干预的情况下，大型语言模型能够在Enron电子邮件数据集中找到高度敏感的个人信息（例如高管与另一员工发生外遇），这些信息可被用于敲诈勒索。虽然我们今天实施的一些攻击仍然成本过高，难以大规模进行，但随着大型语言模型成本的不断下降，实施这些攻击的激励只会增加。因此，我们认为大型语言模型需要采用新的深度防御方法来进行防御。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11449v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）将改变网络攻击的经济模式。LLMs使对手能够针对每个用户发起定制攻击，改变传统的攻击最常见软件并通过针对受害者最低公因数来盈利的模式。在攻击方面，LLMs能够找到成千上万容易识别的用户产品漏洞，而无需人为攻击者手动搜索难以识别的产品漏洞。在盈利方面，基于LLM的勒索软件攻击可以根据每个受攻击设备的特定内容定制勒索要求。我们展示了这两种攻击（以及其他几种攻击）使用最新LLM是即刻可行的。虽然目前有些攻击成本高昂，难以广泛实施，但随着LLM成本的降低，实施这些攻击的激励措施只会增加。因此，LLMs的出现需要新的深度防御方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs将改变网络攻击的经济模式，从攻击最常见软件转变为针对每个用户定制的攻击方式。</li>
<li>LLMs能够快速找到产品中的漏洞，提高攻击效率。</li>
<li>基于LLM的勒索软件可以根据受害者的具体信息定制勒索要求。</li>
<li>当前一些针对LLM的网络攻击成本高昂，难以广泛实施。</li>
<li>随着LLM成本的降低，实施网络攻击的激励将增加。</li>
<li>LLMs的出现需要新的深度防御策略来应对。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11449">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4d89a6b0010e4ff23d3a6c34882f12dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9ed47913f142a440fdc422dc0768e11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75dccf0d417547155569e8143ba9aa43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aaec4ce9d72b0bb79acdea78b3ac49aa.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GODBench-A-Benchmark-for-Multimodal-Large-Language-Models-in-Video-Comment-Art"><a href="#GODBench-A-Benchmark-for-Multimodal-Large-Language-Models-in-Video-Comment-Art" class="headerlink" title="GODBench: A Benchmark for Multimodal Large Language Models in Video   Comment Art"></a>GODBench: A Benchmark for Multimodal Large Language Models in Video   Comment Art</h2><p><strong>Authors:Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang</strong></p>
<p>Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs’ abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at <a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025">https://github.com/stan-lei/GODBench-ACL2025</a>. </p>
<blockquote>
<p>视频评论艺术通过提供传递幽默、讽刺或情感共鸣的创意内容，增强了用户参与度，这要求微妙而全面地掌握文化和上下文细微差别。尽管多模态大型语言模型（MLLM）和思维链（CoT）已在STEM任务（例如数学和编码）中展现出强大的推理能力，但它们仍然难以生成诸如引人共鸣的笑话和富有洞察力的讽刺等创意表达。此外，现有基准测试受限于其有限的模态和不足的类别，阻碍了基于视频的评论艺术创作的全面创造力的探索。为了解决这个问题，我们引入了GODBench，这是一个将视频和文本模态结合在一起的全新基准测试，可以系统地评估MLLMs创作评论艺术的能力。此外，受物理学中波的传播模式的启发，我们提出了思维涟漪（RoT）这一多步骤推理框架，旨在增强MLLM的创造力。大量实验表明，现有的MLLM和CoT方法在理解和生成创造性视频评论方面仍面临巨大挑战。相比之下，RoT提供了一种有效的方法来改善创造性构成，突出了其在推动基于MLLM的创造力的有意义的进步方面的潜力。GODBench可在<a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/stan-lei/GODBench-ACL2025上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11436v1">PDF</a> 69 pages, 66 figures, accepted by ACL 2025</p>
<p><strong>摘要</strong><br>视频评论艺术通过提供传达幽默、讽刺或情感共鸣的创意内容，增强了用户参与度。虽然多模态大型语言模型（MLLM）和思维链（CoT）在STEM任务（如数学和编码）中展现出强大的推理能力，但它们仍难以生成如动人笑话和深刻讽刺等创意表达。为解决现有基准测试在视频评论艺术创作方面的局限性，我们推出GODBench基准测试，它整合视频和文本模态来系统评估MLLMs创作评论艺术的能力。此外，受物理中波动传播模式的启发，我们提出Ripple of Thought（RoT）多步推理框架，旨在增强MLLMs的创造力。实验表明，现有MLLMs和CoT方法在理解和生成创意视频评论方面仍面临挑战。相比之下，RoT提供了一种有效的提高创意写作的方法，突显其在推动基于MLLM的创造力方面的潜力。GODBench已在<a target="_blank" rel="noopener" href="https://github.com/stan-lei/GODBench-ACL2025%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/stan-lei/GODBench-ACL2025公开可用。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频评论艺术通过创意内容增强用户参与度，需要全面理解文化和语境细微差别。</li>
<li>多模态大型语言模型（MLLMs）在STEM任务中展现出强大的推理能力，但在生成创意表达方面仍面临挑战。</li>
<li>现有基准测试在视频评论艺术创作方面的评估存在局限性，缺乏综合性和足够的类别多样性。</li>
<li>推出GODBench基准测试，整合视频和文本模态评估MLLMs创作评论艺术的能力。</li>
<li>受物理波动传播模式启发，提出Ripple of Thought（RoT）多步推理框架，增强MLLMs的创造力。</li>
<li>实验显示现有MLLMs和CoT方法在理解和生成创意视频评论方面存在挑战。</li>
<li>RoT方法有效提高了创意写作能力，突显其在推动基于MLLM的创造力方面的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11436">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ad8734da6667e5e9e4115b0aeb767978.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-716e21106fd9c6f5ccd5b15c0537ccf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c24b0d34eff9882e6b379088fbcd1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f784b33ac157a05a7f830f7d0aba1e6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e58fc377c680b0d0c4374eecb840f3a5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="When-Thinking-Fails-The-Pitfalls-of-Reasoning-for-Instruction-Following-in-LLMs"><a href="#When-Thinking-Fails-The-Pitfalls-of-Reasoning-for-Instruction-Following-in-LLMs" class="headerlink" title="When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following   in LLMs"></a>When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following   in LLMs</h2><p><strong>Authors:Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal</strong></p>
<p>Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies. </p>
<blockquote>
<p>推理增强的大型语言模型（RLLMs）无论是否经过明确的推理训练或通过思维链（CoT）进行提示，已在许多复杂的推理任务上实现了最先进的性能。然而，我们发现了一个令人惊讶且以前被忽视的现象：明确的CoT推理会显著降低遵循指令的准确性。我们在两个基准测试：IFEval（具有简单、可验证的规则约束）和ComplexBench（具有复杂、组合约束）上评估了15个模型，始终观察到应用CoT提示时性能下降。通过大规模案例研究和基于注意力的分析，我们识别出推理有助于（例如，在格式或词汇精度方面）或有害（例如，通过忽略简单约束或引入不必要内容）的常见模式。我们提出了一个指标——约束注意力，来量化生成过程中的模型关注点，并表明CoT推理通常会分散对指令相关标记的注意力。为了减轻这些影响，我们引入并评估了四种策略：上下文学习、自我反思、自我选择性推理和分类器选择性推理。我们的结果表明，选择性推理策略，尤其是分类器选择性推理，可以大幅度恢复丢失的性能。据我们所知，这是第一项系统性地揭示推理导致的指令遵循失败并提供实用缓解策略的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11423v1">PDF</a> </p>
<p><strong>Summary</strong>：显性的链式思维推理会在遵循指令时显著影响模型性能。通过在一系列模型和两个基准测试（具有简单可验证约束的IFEval和具有复杂组合约束的ComplexBench）上的研究，我们发现当应用链式思维推理时，模型性能始终出现下降。研究发现四种缓解策略能有效减轻这种情况。其中选择性推理策略，特别是分类器选择性推理，能显著恢复性能损失。这是首次系统性地揭示推理导致的指令遵循失败并提出实用缓解策略的研究。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>显式链式思维推理（CoT）在遵循指令时会对模型性能产生显著影响。</li>
<li>在两个基准测试中，应用CoT推理会导致模型性能下降。</li>
<li>通过大规模案例研究和基于注意力的分析，发现推理在帮助和伤害模型性能方面的常见模式。</li>
<li>提出了一种量化模型生成过程中关注度的指标——约束注意力，发现CoT推理常常使模型偏离指令相关的标记。</li>
<li>介绍了四种缓解策略来减轻推理对指令遵循的影响，包括上下文学习、自我反思和自我选择性推理等。</li>
<li>选择性推理策略，特别是分类器选择性推理，能有效恢复因推理导致的性能损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11423">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-458d5262f1a26b34ee993d185adf5b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5911c56e30adf0b0e1c76535f2292864.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-450d0f386565be5c00036bc98f39f76b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="EdgeWisePersona-A-Dataset-for-On-Device-User-Profiling-from-Natural-Language-Interactions"><a href="#EdgeWisePersona-A-Dataset-for-On-Device-User-Profiling-from-Natural-Language-Interactions" class="headerlink" title="EdgeWisePersona: A Dataset for On-Device User Profiling from Natural   Language Interactions"></a>EdgeWisePersona: A Dataset for On-Device User Profiling from Natural   Language Interactions</h2><p><strong>Authors:Patryk Bartkowiak, Michal Podstawski</strong></p>
<p>This paper introduces a novel dataset and evaluation benchmark designed to assess and improve small language models deployable on edge devices, with a focus on user profiling from multi-session natural language interactions in smart home environments. At the core of the dataset are structured user profiles, each defined by a set of routines - context-triggered, repeatable patterns of behavior that govern how users interact with their home systems. Using these profiles as input, a large language model (LLM) generates corresponding interaction sessions that simulate realistic, diverse, and context-aware dialogues between users and their devices.   The primary task supported by this dataset is profile reconstruction: inferring user routines and preferences solely from interactions history. To assess how well current models can perform this task under realistic conditions, we benchmarked several state-of-the-art compact language models and compared their performance against large foundation models. Our results show that while small models demonstrate some capability in reconstructing profiles, they still fall significantly short of large models in accurately capturing user behavior. This performance gap poses a major challenge - particularly because on-device processing offers critical advantages, such as preserving user privacy, minimizing latency, and enabling personalized experiences without reliance on the cloud. By providing a realistic, structured testbed for developing and evaluating behavioral modeling under these constraints, our dataset represents a key step toward enabling intelligent, privacy-respecting AI systems that learn and adapt directly on user-owned devices. </p>
<blockquote>
<p>本文介绍了一个新型数据集和评估基准，旨在评估和提高可在边缘设备上部署的小型语言模型。它重点关注智能家庭环境中多会话自然语言交互的用户画像构建。数据集的核心是结构化用户画像，每个用户画像由一组例行程序定义——例行程序是由上下文触发的、可重复的行为模式，决定了用户如何与家庭系统进行交互。使用这些用户画像作为输入，大型语言模型（LLM）生成相应的交互会话，模拟用户与设备之间现实、多样化和上下文感知的对话。该数据集支持的主要任务是画像重建：仅根据交互历史来推断用户例行程和偏好。为了评估当前模型在真实条件下执行此任务的能力，我们对多个最新紧凑语言模型进行了基准测试，并将其性能与大型基础模型进行了比较。我们的结果表明，虽然小型模型在重建画像方面表现出一些能力，但它们仍远远不能准确地捕捉用户行为。这种性能差距构成了一大挑战——尤其是由于设备端处理提供了关键优势，如保护用户隐私、最小化延迟和实现无需依赖云的个人化体验。通过为在这些约束下开发和评估行为建模提供一个现实、结构化测试平台，我们的数据集是实现在用户拥有的设备上直接学习并自适应的智能、尊重隐私的AI系统的重要一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11417v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个新型数据集和评估基准测试，旨在评估和改良可在边缘设备上部署的小型语言模型。该数据集以用户画像为核心，通过模拟用户与智能家庭系统的多会话自然语言交互，生成相应的交互会话。主要任务是通过用户交互历史来推断用户的行为模式和偏好设置。通过对比当前小型模型与大型基础模型的性能表现，发现小型模型在重建用户画像方面存在一定能力，但在准确捕捉用户行为方面仍与大型模型存在显著差距。这一性能差距对于在设备上直接处理具有保护用户隐私、减少延迟和依赖个性化体验等关键优势的系统构成重大挑战。本文所提供的数据集为在这些限制条件下开发和评估行为建模提供了一个真实、结构化的测试平台，朝着实现在用户拥有的设备上直接进行智能、尊重隐私的AI系统的目标迈出了重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文介绍了一个针对边缘设备上小型语言模型的评估数据集和基准测试。</li>
<li>数据集以用户画像为核心，通过模拟用户在智能家庭环境中的多会话自然语言交互来生成数据。</li>
<li>主要任务是重建用户画像，即基于用户交互历史推断其日常行为和偏好设置。</li>
<li>对比了小型模型和大型基础模型在该任务上的表现，发现小型模型在捕捉用户行为方面仍有显著差距。</li>
<li>小型模型与大型模型之间的性能差距对在设备上直接处理系统构成挑战，因为这些系统需要保护用户隐私、减少延迟并依赖个性化体验。</li>
<li>所提供的数据集为在限制条件下开发和评估行为建模提供了一个真实、结构化的测试平台。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11417">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3c930f059aab213bb8ef2cd146604cc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-830ea07d0f583eb6fb728eba64133d89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4ef57d7a66bb69a5eac1d98335b032d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cc0eed3bc59ff909af16ce1b33b833e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CARES-Comprehensive-Evaluation-of-Safety-and-Adversarial-Robustness-in-Medical-LLMs"><a href="#CARES-Comprehensive-Evaluation-of-Safety-and-Adversarial-Robustness-in-Medical-LLMs" class="headerlink" title="CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in   Medical LLMs"></a>CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in   Medical LLMs</h2><p><strong>Authors:Sijia Chen, Xiaomin Li, Mengxue Zhang, Eric Hanchen Jiang, Qingcheng Zeng, Chen-Hsiang Yu</strong></p>
<p>Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES (Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for evaluating LLM safety in healthcare. CARES includes over 18,000 prompts spanning eight medical safety principles, four harm levels, and four prompting styles: direct, indirect, obfuscated, and role-play, to simulate both malicious and benign use cases. We propose a three-way response evaluation protocol (Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess model behavior. Our analysis reveals that many state-of-the-art LLMs remain vulnerable to jailbreaks that subtly rephrase harmful prompts, while also over-refusing safe but atypically phrased queries. Finally, we propose a mitigation strategy using a lightweight classifier to detect jailbreak attempts and steer models toward safer behavior via reminder-based conditioning. CARES provides a rigorous framework for testing and improving medical LLM safety under adversarial and ambiguous conditions. </p>
<blockquote>
<p>大型语言模型（LLM）在医疗环境中得到了越来越多的应用，这引发了关于其安全性、对齐性和易受对手操控的严重关切。虽然之前的基准测试评估了模型拒绝有害提示的能力，但它们通常缺乏临床特异性、分级伤害水平和突破式攻击覆盖率。我们引入CARES（临床对抗稳健性和安全性评估）（Clinical Adversarial Robustness and Evaluation of Safety），这是一个用于评估医疗保健中LLM安全性的基准测试。CARES包含超过18，000个提示，涵盖八个医疗安全原则、四个伤害程度和四种提示风格：直接、间接、模糊和角色扮演，以模拟恶意和良性用例。我们提出了一个三向响应评估协议（接受、谨慎、拒绝）和一个精细的安全分数指标来评估模型行为。我们的分析表明，许多最先进的LLM仍然容易受到突破式攻击，这些攻击会微妙地重新表述有害提示，同时也会拒绝安全但措辞不典型的查询。最后，我们提出了一种缓解策略，使用轻量级分类器来检测突破尝试，并通过基于提醒的条件来引导模型采取更安全的行动。CARES为测试和提高对敌条件和模糊条件下的医疗LLM安全性提供了严格框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11413v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在医疗领域的应用引发了关于安全、对齐和易受敌对操作影响的担忧。现有的评估模型拒绝有害提示的能力的基准测试，往往缺乏临床特异性、分级危害性和越狱式攻击覆盖。本文介绍CARES（临床对抗稳健性和安全性评估），一个用于评估医疗领域LLM安全性的基准测试。CARES包含超过18，000个提示，涵盖八个医疗安全原则、四个危害级别和四种提示风格：直接、间接、模糊和角色扮演，以模拟恶意和良性用例。本文提出一种三方响应评估协议（接受、谨慎、拒绝）和精细的安全评分指标来评估模型行为。分析表明，许多最先进的大型语言模型仍然容易受到巧妙重新表述的有害提示的攻击，同时也会过于拒绝安全但措辞不典型的查询。最后，本文提出一种使用轻量级分类器来检测越狱尝试的缓解策略，并通过基于提醒的条件引导模型朝着更安全的行为发展。CARES为测试和提高医疗LLM在敌对和模糊条件下的安全性提供了严格框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在医疗领域的应用引发了对安全、对齐和易受敌对操作影响的担忧。</li>
<li>现有基准测试缺乏临床特异性、分级危害性和越狱式攻击覆盖。</li>
<li>引入CARES基准测试，包含多种提示风格和评估协议，以全面评估LLM在医疗领域的安全性。</li>
<li>分析显示，先进的大型语言模型仍易受到越狱攻击，并可能过于拒绝安全查询。</li>
<li>提出使用轻量级分类器检测越狱尝试的缓解策略。</li>
<li>CARES为测试和提高医疗LLM在敌对和模糊条件下的安全性提供了严格框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11413">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-28d78d292a7f3a4b688c20abbc75eadf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-201ffbac50911ed6bc9c1911a0aec010.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-476b2c1cf67eb5fa7be75e283c5ffcb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10671724acd38e8caafb26c95e6f2909.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Visual-Planning-Let’s-Think-Only-with-Images"><a href="#Visual-Planning-Let’s-Think-Only-with-Images" class="headerlink" title="Visual Planning: Let’s Think Only with Images"></a>Visual Planning: Let’s Think Only with Images</h2><p><strong>Authors:Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić</strong></p>
<p>Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference. </p>
<blockquote>
<p>近期，大型语言模型（LLM）及其多模态扩展（MLLM）的进展，已在多种任务上极大地增强了机器推理能力。然而，这些模型主要依赖纯文本作为表达和结构化推理的媒介，即使存在视觉信息。在这项工作中，我们主张语言并不总是最自然或最有效的推理方式，特别是在涉及空间和几何信息的任务中。受此启发，我们提出了一种新的范式——视觉规划，它能够通过纯视觉表示进行规划，独立于文本。在这种范式中，规划是通过一系列图像执行的，这些图像在视觉领域编码了逐步推理，类似于人类如何勾画或可视化未来行动。我们引入了一种新的强化学习框架——通过强化学习的视觉规划（VPRL），借助GRPO对大型视觉模型进行后训练，从而在具有代表性的视觉导航任务、FrozenLake、迷宫和MiniBehavior中的规划方面取得了实质性改进。我们的视觉规划范式在只进行文本推理的其它所有规划变种中表现最佳。我们的结果证明了视觉规划作为语言基础推理的可行且有前途的替代方案，为受益于直观、基于图像的推理的任务开辟了新途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11409v1">PDF</a> 10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables   including references and appendices)</p>
<p><strong>Summary</strong><br>大型语言模型（LLM）和多模态扩展模型（MLLM）的进展极大地促进了跨不同任务的机器推理能力。然而，这些模型主要依赖纯文本作为表达和构建推理的媒介，即使存在视觉信息也是如此。本文提出一种新型推理模式——视觉规划，它通过纯粹的视觉表征进行规划，独立于文本。视觉规划通过图像序列执行规划，在视觉领域逐步进行推理，类似于人类如何勾画或可视化未来行动。本文引入了一种新型强化学习框架——基于强化学习的视觉规划（VPRL），通过GRPO对大型视觉模型进行后训练，在典型的视觉导航任务（如FrozenLake、迷宫和MiniBehavior）中实现了显著的规划改进。视觉规划范式在纯文本空间进行推理的规划变体中表现最佳。本文结果证明了视觉规划作为一种可行且有前途的替代语言基础推理的方法，为受益于直观图像基础推理的任务开辟了新途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）和多模态扩展（MLLM）增强了机器推理能力。</li>
<li>当前模型主要依赖文本进行推理，即使存在视觉信息。</li>
<li>提出了一个新的推理模式——视觉规划，通过纯粹的视觉表征进行规划，独立于文本。</li>
<li>视觉规划通过图像序列执行，类似于人类如何勾画或可视化未来行动。</li>
<li>引入了一种新型强化学习框架——基于强化学习的视觉规划（VPRL）。</li>
<li>VPRL在典型的视觉导航任务中实现了显著的规划改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11409">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-39e2fae2fb6e4a2b8d5c54c5ee658bdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b087dca392b82352040f39806ccc878.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-003cb4251f762d33ea57f5b138daf6c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45836365d67a9715f197785ef545cf09.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EmotionHallucer-Evaluating-Emotion-Hallucinations-in-Multimodal-Large-Language-Models"><a href="#EmotionHallucer-Evaluating-Emotion-Hallucinations-in-Multimodal-Large-Language-Models" class="headerlink" title="EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large   Language Models"></a>EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large   Language Models</h2><p><strong>Authors:Bohao Xing, Xin Liu, Guoying Zhao, Chengyu Liu, Xiaolan Fu, Heikki Kälviäinen</strong></p>
<p>Emotion understanding is a critical yet challenging task. Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities in this area. However, MLLMs often suffer from hallucinations, generating irrelevant or nonsensical content. To the best of our knowledge, despite the importance of this issue, there has been no dedicated effort to evaluate emotion-related hallucinations in MLLMs. In this work, we introduce EmotionHallucer, the first benchmark for detecting and analyzing emotion hallucinations in MLLMs. Unlike humans, whose emotion understanding stems from the interplay of biology and social learning, MLLMs rely solely on data-driven learning and lack innate emotional instincts. Fortunately, emotion psychology provides a solid foundation of knowledge about human emotions. Building on this, we assess emotion hallucinations from two dimensions: emotion psychology knowledge and real-world multimodal perception. To support robust evaluation, we utilize an adversarial binary question-answer (QA) framework, which employs carefully crafted basic and hallucinated pairs to assess the emotion hallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on EmotionHallucer, we reveal that: i) most current models exhibit substantial issues with emotion hallucinations; ii) closed-source models outperform open-source ones in detecting emotion hallucinations, and reasoning capability provides additional advantages; iii) existing models perform better in emotion psychology knowledge than in multimodal emotion perception. As a byproduct, these findings inspire us to propose the PEP-MEK framework, which yields an average improvement of 9.90% in emotion hallucination detection across selected models. Resources will be available at <a target="_blank" rel="noopener" href="https://github.com/xxtars/EmotionHallucer">https://github.com/xxtars/EmotionHallucer</a>. </p>
<blockquote>
<p>情感理解是一项至关重要的且具有挑战性的任务。最近，多模态大语言模型（MLLMs）的进步显著增强了该领域的能力。然而，MLLMs常常出现幻觉，生成不相关或无意义的内容。据我们所知，尽管这个问题很重要，但还没有专门针对MLLMs中的情感相关幻觉进行评估的努力。在这项工作中，我们介绍了EmotionHallucer，它是检测和分析MLLMs中情感幻觉的第一个基准测试。与人类不同，人类的情感理解来源于生物学和社会学习的相互作用，而MLLMs完全依赖于数据驱动的学习，缺乏天生的情感直觉。幸运的是，情感心理学为人类情感提供了坚实的知识基础。在此基础上，我们从两个维度评估情感幻觉：情感心理学知识和现实世界的多模态感知。为了支持稳健的评估，我们采用对抗性二元问答（QA）框架，该框架利用精心制作的基本和幻觉配对来评估MLLMs的情感幻觉倾向。通过对EmotionHallucer上38个LLMs和MLLMs的评估，我们发现：i）当前大多数模型在情感幻觉方面存在重大问题；ii）封闭源模型在检测情感幻觉方面优于开源模型，推理能力会提供额外优势；iii）现有模型在情感心理学知识方面的表现优于多模态情感感知。作为副产品，这些发现激励我们提出了PEP-MEK框架，该框架在所选模型的情感幻觉检测中平均提高了9.90%。相关资源将在<a target="_blank" rel="noopener" href="https://github.com/xxtars/EmotionHallucer">https://github.com/xxtars/EmotionHallucer</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11405v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了情感理解在Multimodal大语言模型中的重要性及其面临的挑战。尽管近期技术进步显著，但这些模型常常出现情感幻觉问题，生成无关或不合理的内容。为此，本文引入了EmotionHallucer，这是首个用于检测和评估Multimodal大语言模型中情感幻觉的基准测试。文章从情感心理学知识和现实世界的多模式感知两个维度评估情感幻觉。利用对抗性二元问答框架进行评估，对38个大型语言模型和多模态语言模型进行情感幻觉倾向的测试。研究结果表明大多数当前模型存在明显的情感幻觉问题，封闭源模型在检测情感幻觉方面表现优于开源模型，且推理能力提供额外优势。现有模型在情感心理学知识方面的表现优于多模态情感感知。基于此，文章提出了PEP-MEK框架，可以在选定的模型中平均提高情感幻觉检测的效果。相关资源可在指定网站获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>情感理解在Multimodal大语言模型中至关重要，但存在挑战。</li>
<li>Multimodal大语言模型常常出现情感幻觉问题。</li>
<li>EmotionHallucer是首个用于检测和评估Multimodal大语言模型中情感幻觉的基准测试。</li>
<li>从情感心理学知识和现实世界的多模式感知两个维度评估情感幻觉。</li>
<li>对抗性二元问答框架用于评估情感幻觉倾向。</li>
<li>大多数当前模型存在明显的情感幻觉问题，封闭源模型表现较好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11405">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-126002d0c859f8501d50c8588752d44b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-947ca07f1bc27799c8ffa01688204db4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28ed3a40b69208e07fd498a2b7aceeaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8ef7819ac862b08b1c284536d08bf49.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Phare-A-Safety-Probe-for-Large-Language-Models"><a href="#Phare-A-Safety-Probe-for-Large-Language-Models" class="headerlink" title="Phare: A Safety Probe for Large Language Models"></a>Phare: A Safety Probe for Large Language Models</h2><p><strong>Authors:Pierre Le Jeune, Benoît Malésieux, Weixuan Xiao, Matteo Dora</strong></p>
<p>Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems. </p>
<blockquote>
<p>确保大型语言模型（LLM）的安全对于负责任的部署至关重要，然而现有的评估往往更侧重于性能而非识别故障模式。我们介绍了Phare，这是一个多语言诊断框架，用于探究和评估LLM在三方面的行为：幻觉和可靠性、社会偏见以及有害内容生成。我们对17个最新LLM的评估揭示了所有安全维度上的系统性漏洞模式，包括拍马屁、提示敏感性和刻板印象复制。Phare通过突出这些特定的故障模式而不是简单地排名模型，为研究人员和从业者提供了构建更稳健、对齐和可信赖的语言系统的可操作见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11365v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了一种名为Phare的多语言诊断框架，用于对大型语言模型（LLM）的行为进行全面评估。通过三个方面——幻想与可靠性、社会偏见和有害内容生成，对现有17种前沿LLM的评估揭示了系统漏洞模式。Phare通过强调特定失败模式而不是简单的排名模型，为研究人员和从业者提供具有操作性的见解，用于构建更加稳健、一致和可靠的语系统。总体来说，保障LLM的安全对于负责部署至关重要，Phare框架对于揭示LLM的系统性脆弱点并引导未来发展具有深远意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Phare是一个多语言诊断框架，用于评估和诊断大型语言模型（LLM）的行为。</li>
<li>Phare从三个方面评估LLM的行为：幻想与可靠性、社会偏见和有害内容生成。</li>
<li>对现有LLM的评估揭示了系统性漏洞模式，这些漏洞可能影响模型的可靠性和安全性。</li>
<li>Phare框架强调了特定失败模式而非简单排名模型的重要性。</li>
<li>通过识别这些失败模式，Phare为构建更加稳健、一致和可靠的LLM提供了实用指导。</li>
<li>Phare评估有助于指导研究者和从业者更好地理解LLM的优势和潜在缺陷，以更好地推动模型的未来发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11365">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e870f5e4b992bd7cbc9a0e7b7b078c89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f798cbf74880c10e4393bbae6615ebc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff49a79f0bb4f148ddb10972158f0eb6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LegoSLM-Connecting-LLM-with-Speech-Encoder-using-CTC-Posteriors"><a href="#LegoSLM-Connecting-LLM-with-Speech-Encoder-using-CTC-Posteriors" class="headerlink" title="LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors"></a>LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors</h2><p><strong>Authors:Rao Ma, Tongzhou Chen, Kartik Audhkhasi, Bhuvana Ramabhadran</strong></p>
<p>Recently, large-scale pre-trained speech encoders and Large Language Models (LLMs) have been released, which show state-of-the-art performance on a range of spoken language processing tasks including Automatic Speech Recognition (ASR). To effectively combine both models for better performance, continuous speech prompts, and ASR error correction have been adopted. However, these methods are prone to suboptimal performance or are inflexible. In this paper, we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using the ASR posterior matrices. The speech encoder is trained to generate Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary, which are used to reconstruct pseudo-audio embeddings by computing a weighted sum of the LLM input embeddings. These embeddings are concatenated with text embeddings in the LLM input space. Using the well-performing USM and Gemma models as an example, we demonstrate that our proposed LegoSLM method yields good performance on both ASR and speech translation tasks. By connecting USM with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline on 8 MLS testsets. The trained model also exhibits modularity in a range of settings – after fine-tuning the Gemma model weights, the speech encoder can be switched and combined with the LLM in a zero-shot fashion. Additionally, we propose to control the decode-time influence of the USM and LLM using a softmax temperature, which shows effectiveness in domain adaptation. </p>
<blockquote>
<p>最近，已经发布了一系列大规模预训练语音编码器和大型语言模型（LLM），它们在包括自动语音识别（ASR）在内的各种口语处理任务上表现出卓越的性能。为了更有效地结合这两种模型以取得更好的性能，已经采用了连续语音提示和ASR错误校正。然而，这些方法容易出现性能不佳或不够灵活的问题。在本文中，我们提出了一种新的范式LegoSLM，它使用ASR后验矩阵来连接语音编码器和LLM。语音编码器被训练以生成针对LLM词汇表的连接时序分类（CTC）后验概率，这些后验概率被用来通过计算LLM输入嵌入的加权和来重建伪音频嵌入。这些嵌入与LLM输入空间中的文本嵌入相连接。以性能良好的USM和Gemma模型为例，我们证明了所提出的LegoSLM方法在ASR和语音翻译任务上都表现良好。通过将USM与Gemma模型连接起来，我们在8个MLS测试集上相对于USM-CTC基线获得了平均49%的WERR。训练好的模型还表现出在各种设置下的模块化——在微调Gemma模型权重后，可以切换语音编码器并以零样本的方式与LLM结合。此外，我们提出使用softmax温度来控制USM和LLM在解码时间的影响，这在领域适应中显示出有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11352v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大规模预训练语音编码器和大型语言模型（LLM），本文提出了一种新的结合方法LegoSLM，旨在解决语音处理任务中的自动语音识别（ASR）问题。该方法通过ASR后验矩阵桥接语音编码器和LLM，训练语音编码器生成CTC后验，用于重建伪音频嵌入，并将其与LLM输入空间中的文本嵌入合并。实验表明，该方法在ASR和语音翻译任务上表现良好，与美国USM模型和Gemma模型的结合平均提高了约49%的性能。此外，该模型具有模块化特性，可以在不同设置下进行微调并与其他模型组合使用。同时，通过调整softmax温度控制解码时的影响，为领域适应性提供了有效手段。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LegoSLM结合了语音编码器和大型语言模型（LLM），用于改善自动语音识别（ASR）任务的性能。</li>
<li>通过ASR后验矩阵桥接语音编码器和LLM，训练语音编码器生成CTC后验。</li>
<li>伪音频嵌入是通过计算LLM输入嵌入的加权和来重建的，并与文本嵌入合并。</li>
<li>实验显示LegoSLM在ASR和语音翻译任务上表现优异。</li>
<li>与USM和Gemma模型的结合，性能平均提升约49%。</li>
<li>模型具备模块化特性，可在不同设置下进行微调并与其他模型组合使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11352">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4ba46440e42129c99760bbc2ab76162.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ebe810e6380a78828bdd21c127cc21d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8809b33003316d618210ea3dccf7423b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64af55f0fc6795592761f2d11179d198.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7279fa04349e3c5e715eaf9a5561fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61b2f397f7d7f1b23e51ab7c3881dddd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Context-parroting-A-simple-but-tough-to-beat-baseline-for-foundation-models-in-scientific-machine-learning"><a href="#Context-parroting-A-simple-but-tough-to-beat-baseline-for-foundation-models-in-scientific-machine-learning" class="headerlink" title="Context parroting: A simple but tough-to-beat baseline for foundation   models in scientific machine learning"></a>Context parroting: A simple but tough-to-beat baseline for foundation   models in scientific machine learning</h2><p><strong>Authors:Yuanzhao Zhang, William Gilpin</strong></p>
<p>Recently-developed time series foundation models for scientific machine learning exhibit emergent abilities to predict physical systems. These abilities include zero-shot forecasting, in which a model forecasts future states of a system given only a short trajectory as context. Here, we show that foundation models applied to physical systems can give accurate predictions, but that they fail to develop meaningful representations of the underlying physics. Instead, foundation models often forecast by context parroting, a simple zero-shot forecasting strategy that copies directly from the context. As a result, a naive direct context parroting model scores higher than state-of-the-art time-series foundation models on predicting a diverse range of dynamical systems, at a tiny fraction of the computational cost. We draw a parallel between context parroting and induction heads, which explains why large language models trained on text can be repurposed for time series forecasting. Our dynamical systems perspective also ties the scaling between forecast accuracy and context length to the fractal dimension of the attractor, providing insight into the previously observed in-context neural scaling laws. Context parroting thus serves as a simple but tough-to-beat baseline for future time-series foundation models and can help identify in-context learning strategies beyond parroting. </p>
<blockquote>
<p>最近开发的用于科学机器学习的时间序列基础模型展现出预测物理系统的能力。这些能力包括零样本预测，即仅根据系统的短期轨迹作为上下文进行未来状态预测。在这里，我们展示了应用于物理系统的基础模型可以做出准确的预测，但它们无法发展对底层物理学的有意义表示。相反，基础模型通常通过上下文模仿进行预测，这是一种简单的零样本预测策略，直接从上下文中复制。因此，简单的直接上下文模仿模型在预测各种动态系统时得分高于最新时间序列基础模型，且计算成本极低。我们将上下文模仿与归纳头进行了比较，这解释了为什么训练于文本的大型语言模型可以重新用于时间序列预测。我们的动态系统视角还将预测精度与上下文长度之间的缩放与吸引子的分形维度联系起来，为之前观察到的上下文神经缩放定律提供了见解。因此，上下文模仿成为未来时间序列基础模型的一个简单但难以超越的基准，并有助于识别除模仿以外的上下文学习策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11349v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期开发的科学机器学习时间序列基础模型展现出预测物理系统的能力，如零起点预测。然而，这些模型在预测时并未真正捕捉到物理系统的内在规律，而是通过模仿上下文进行预测，称为“上下文鹦鹉学舌”。这种简单的预测策略在某些情况下表现甚至优于当前先进的时间序列基础模型，且计算成本低。研究将上下文鹦鹉学舌与归纳头联系起来，解释为何大型语言模型可以应用于时间序列预测。同时，通过动态系统视角揭示了预测精度与上下文长度之间的关联。因此，上下文模仿为时间序列基础模型提供了一个简单但难以超越的基准线，并有助于识别更多的上下文学习策略。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>时间序列基础模型可以预测物理系统，包括零起点预测。</li>
<li>这些模型主要通过模仿上下文进行预测，而非真正捕捉物理系统的内在规律。</li>
<li>上下文鹦鹉学舌策略在某些情况下表现优异，且计算成本低。</li>
<li>上下文鹦鹉学舌与归纳头之间的联系被揭示。</li>
<li>预测精度与上下文长度之间的关系被阐明，与吸引子的分形维度有关。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b9c7d398c490099d17e4a9488618f240.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e077d970dcfa8e480922ae41cff388bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7893f2a5db1a5fc4e16cd581d8c4f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fdc4faa2f68175481ce04b5bb472a00.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Attention-via-Pre-Scoring-Prioritizing-Informative-Keys-in-Transformers"><a href="#Efficient-Attention-via-Pre-Scoring-Prioritizing-Informative-Keys-in-Transformers" class="headerlink" title="Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in   Transformers"></a>Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in   Transformers</h2><p><strong>Authors:Zhexiang Li, Haoyu Wang, Yutong Bao, David Woodruff</strong></p>
<p>Recent advances in transformer architectures deeply enhance long-context language modeling. Among them, HyperAttention achieves competitive efficiency by combining a single-level LSH-based clustering with uniform residual sampling. However,such a sampling limits crucial keys’ capturing, which in turn raises the overall perplexity. In this paper, we propose a pre-scoring mechanism to assist HyperAttention to prioritize significant keys. Specifically, we introduce three scoring methods: K-means clustering, K-median clustering, and leverage score-based ranking (inspired by LevAttention) to filter keys effectively. We further replace HyperAttention’s original uniform residual sampling entirely, relying exclusively on our pre-scoring mechanism. Experiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3, which outperforms standard HyperAttention. Moreover, when running on the Vision-Transformer (ViT), our method shows that it can guarantee similar accuracy compared with LevAttention, and will surpass LevAttention given specific parameters. Although this method introduces computational overhead, its combination with HyperAttention remains 20 times faster than FlashAttention, providing a balanced trade-off between speed and modeling accuracy. Our results highlight the effectiveness of integrating pre-scoring into hierarchical attention mechanisms, significantly improving Transformer’s efficiency. </p>
<blockquote>
<p>近期Transformer架构的进展极大地提升了长语境语言建模的能力。其中，HyperAttention通过结合单级LSH（局部敏感哈希）聚类与均匀剩余采样实现了竞争性的效率。然而，这种采样方式限制了关键信息的捕获，从而提高了整体困惑度。在本文中，我们提出一种预评分机制来帮助HyperAttention优先处理重要的关键信息。具体来说，我们引入了三种评分方法：K-均值聚类、K-中位数聚类和基于杠杆得分的排名（受LevAttention启发）以有效地过滤关键信息。我们进一步完全替换HyperAttention的原始均匀剩余采样，仅依赖我们的预评分机制。在ChatGLM2（13.1万令牌上下文）上的实验将困惑度从12降低到了8.3，这优于标准的HyperAttention。此外，当在视觉转换器（ViT）上运行时，我们的方法可以保证与LevAttention类似的精度，并在给定特定参数时超过LevAttention。尽管这种方法引入了计算开销，但它与HyperAttention的结合仍然比FlashAttention快20倍，在速度和建模精度之间提供了平衡的折衷。我们的研究结果突显了将预评分集成到分层注意力机制中的有效性，极大地提高了Transformer的效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11040v1">PDF</a> </p>
<p><strong>Summary</strong><br>     最新研究表明，HyperAttention结合单级LSH聚类与均匀剩余采样技术提高了长文本语境建模的效率，但仍存在关键信息捕捉受限的问题。为此，研究团队提出一种预评分机制，引入K-means聚类、K-median聚类和基于杠杆得分的排名等方法以优化关键信息的筛选。实验证明，该机制在ChatGLM2上降低了困惑度，并能在Vision-Transformer上保证与LevAttention相似的准确率。虽然增加了计算开销，但与HyperAttention结合后速度仍是FlashAttention的20倍，实现了速度与建模准确性的平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HyperAttention通过结合LSH-based聚类和均匀剩余采样提高了长文本语境建模效率。</li>
<li>预评分机制被引入以优化HyperAttention中关键信息的捕捉。</li>
<li>引入的预评分包括K-means聚类、K-median聚类和基于杠杆得分的排名方法。</li>
<li>均匀剩余采样被完全替换为预评分机制。</li>
<li>在ChatGLM2上的实验表明，新的机制降低了困惑度，并优于标准HyperAttention。</li>
<li>在Vision-Transformer上，该方法的准确率与LevAttention相当，特定参数下可超越LevAttention。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db08f5bbe02f298281442e6012a900fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ae3cc4b7e055376af18cf21f8b836ee.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Let-the-Trial-Begin-A-Mock-Court-Approach-to-Vulnerability-Detection-using-LLM-Based-Agents"><a href="#Let-the-Trial-Begin-A-Mock-Court-Approach-to-Vulnerability-Detection-using-LLM-Based-Agents" class="headerlink" title="Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection   using LLM-Based Agents"></a>Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection   using LLM-Based Agents</h2><p><strong>Authors:Ratnadira Widyasari, Martin Weyssow, Ivana Clairine Irsan, Han Wei Ang, Frank Liauw, Eng Lieh Ouh, Lwin Khin Shar, Hong Jin Kang, David Lo</strong></p>
<p>Detecting vulnerabilities in source code remains a critical yet challenging task, especially when benign and vulnerable functions share significant similarities. In this work, we introduce VulTrial, a courtroom-inspired multi-agent framework designed to enhance automated vulnerability detection. It employs four role-specific agents, which are security researcher, code author, moderator, and review board. Through extensive experiments using GPT-3.5 and GPT-4o we demonstrate that Vultrial outperforms single-agent and multi-agent baselines. Using GPT-4o, VulTrial improves the performance by 102.39% and 84.17% over its respective baseline. Additionally, we show that role-specific instruction tuning in multi-agent with small data (50 pair samples) improves the performance of VulTrial further by 139.89% and 118.30%. Furthermore, we analyze the impact of increasing the number of agent interactions on VulTrial’s overall performance. While multi-agent setups inherently incur higher costs due to increased token usage, our findings reveal that applying VulTrial to a cost-effective model like GPT-3.5 can improve its performance by 69.89% compared to GPT-4o in a single-agent setting, at a lower overall cost. </p>
<blockquote>
<p>检测源代码中的漏洞仍然是一项至关重要的任务，但也具有挑战性，特别是当良性函数和易遭受攻击的函数的相似度很高时更是如此。在这项研究中，我们引入了VulTrial，这是一个受法庭启发的多智能体框架，旨在增强自动化漏洞检测。它采用了四个角色特定的智能体，分别是安全研究员、代码作者、主持人和评审团。我们通过使用GPT-3.5和GPT-4o的大量实验表明，VulTrial在单智能体和多智能体基线方面表现出色。使用GPT-4o时，VulTrial的性能分别提高了102.39%和84.17%。此外，我们还展示了在多智能体中使用小数据（50对样本）进行特定角色指令调整可以进一步提高VulTrial的性能，分别提高了139.89%和118.30%。此外，我们还分析了增加智能体交互次数对VulTrial总体性能的影响。虽然多智能体设置由于增加了令牌使用量而天然地导致更高的成本，但我们的研究结果表明，将VulTrial应用于成本效益高的模型（如GPT-3.5），在单智能体环境中相比GPT-4o可以提高性能达69.89%，同时总体成本更低。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10961v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为VulTrial的多智能体框架，用于增强自动化漏洞检测。该框架采用四种特定角色智能体，包括安全研究人员、代码作者、调解员和审查委员会。实验表明，VulTrial在单智能体和多智能体基准测试中表现出色，使用GPT-4o时性能提升102.39%和84.17%。此外，研究表明，在少量数据下对多智能体的特定角色指令进行调整，可进一步提高VulTrial的性能。同时，探讨了增加智能体交互对VulTrial性能的影响。虽然多智能体设置导致更高的成本，但使用像GPT-3.5这样的低成本模型，VulTrial的性能在单智能体设置中可以提高69.89%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VulTrial是一个多智能体框架，旨在增强自动化漏洞检测。</li>
<li>VulTrial包含四种特定角色智能体：安全研究人员、代码作者、调解员和审查委员会。</li>
<li>通过实验证明，VulTrial在基准测试中表现出色，使用GPT-4o时性能提升显著。</li>
<li>在少量数据下，对多智能体的特定角色指令调整可进一步提高VulTrial的性能。</li>
<li>增加智能体交互对VulTrial的性能有积极影响。</li>
<li>多智能体设置虽然成本较高，但使用低成本模型如GPT-3.5可以提高性能。</li>
<li>VulTrial框架具有潜在的应用价值，为自动化漏洞检测提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10961">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8b5894aa5a1181b69a2b6b0e5542ae8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f02dfd7ae9bf5213fda5b835a601313f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb30327ae2cc17c4c81d7ca7a5648fce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84c953f5f56b635657adde6622d152f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a75b8ae405923d798ad76eeb1893e9f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6bae18a09773e9af69968c91d6ec562.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model"><a href="#Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model" class="headerlink" title="Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model"></a>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model</h2><p><strong>Authors:Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He</strong></p>
<p>In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>. </p>
<blockquote>
<p>在眼科手术中，开发一个能够解读手术视频并预测后续操作的AI系统，需要大量的带有高质量注释的眼科手术视频。由于隐私问题和劳动消耗，这些视频的收集非常困难。文本引导的视频生成（T2V）作为一种有前途的解决方案应运而生，它可以根据外科医生的指令生成眼科手术视频。在本文中，我们介绍了Ophora，一个能够根据自然语言指令生成眼科手术视频的开创性模型。为了构建Ophora，我们首先提出了一个全面的数据整理管道，将叙述性的眼科手术视频转化为大规模的高质量数据集，包含超过16万个视频指令对，即Ophora-160K。然后，我们提出了一个渐进的视频指令调整方案，以从一个在天然视频文本数据集上预训练的T2V模型转移丰富的时空知识，用于基于Ophora-160K的隐私保护眼科手术视频生成。通过对视频质量的定量分析和眼科医生的反馈进行的实验表明，Ophora可以根据外科医生的指令生成逼真和可靠的眼科手术视频。我们还验证了Ophora在眼科手术工作流程理解下游任务中的能力。代码可在<a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07449v3">PDF</a> Early accepted in MICCAI25</p>
<p><strong>Summary</strong></p>
<p>一个眼科手术视频生成的新模型被提出，名为“Ophora”。它通过自然语言指令生成眼科手术视频，解决了由于隐私问题和劳动力消耗导致的高质量眼科手术视频难以收集的问题。该模型使用综合数据整理管道将叙事眼科手术视频转化为大规模高质量数据集，并采用渐进式视频指令调整方案从预先训练的自然视频文本数据集中转移时空知识。实验证明，Ophora可以根据外科医生的指令生成真实可靠的眼科手术视频，并支持眼科手术工作流程理解等下游任务。</p>
<p><strong>Key Takeaways</strong></p>
<p>1.Ophora是一个能够基于自然语言指令生成眼科手术视频的模型。<br>2.为了解决高质量眼科手术视频难以收集的问题，使用了综合数据整理管道（Comprehensive Data Curation pipeline）。<br>3.提出了一个渐进式视频指令调整方案（Progressive Video-Instruction Tuning scheme），用于从预先训练的自然视频文本数据集中转移时空知识。<br>4.实验证明了Ophora可以根据外科医生的指令生成真实可靠的眼科手术视频。<br>5.Ophora具有支持眼科手术工作流程理解等下游任务的能力。<br>6.该模型的应用可以缓解高质量眼科手术视频资源的稀缺问题，促进眼科手术相关研究和培训的发展。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07449">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a6b725defca02e998337fa7052f69802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4ec3b71a8c06c8f6d824bb3db7290c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cca6f327a9e0a5dece7366cde8a1f565.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding"><a href="#VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding" class="headerlink" title="VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding"></a>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding</h2><p><strong>Authors:Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber</strong></p>
<p>Synthetic video generation has gained significant attention for its realism and broad applications, but remains prone to violations of common sense and physical laws. This highlights the need for reliable abnormality detectors that understand such principles and are robust to hallucinations. To address this, we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from synthetic videos generated by models like Veo2, Sora, and Kling, paired with expert-crafted counterintuitive QA to evaluate the critical thinking abilities of Multi-modal Large Language Models (MLLMs) on abnormalities that are perceptually obvious to humans but often hallucinated due to language priors. VideoHallu evaluates MLLMs’ abnormality detection abilities with examples across alignment, consistency, commonsense, and physics. We benchmark SOTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and VideoChat-R1. We observe that these models perform well on many real-world benchmarks like MVBench and MovieChat, but still struggle with basic physics-based and commonsense reasoning in synthetic videos. We further show that post-training with Group Relative Policy Optimization (GRPO), using curriculum learning on datasets combining video QA with counterintuitive commonsense and physics reasoning over real and synthetic videos, improves MLLMs’ abnormality detection and critical thinking, demonstrating the value of targeted training for improving their understanding of commonsense and physical laws. </p>
<blockquote>
<p>视频生成技术因其真实感和广泛应用而备受关注，但仍然存在违反常识和物理定律的问题。这强调了对可靠异常检测器的需求，这些检测器需要理解这些原理，并对幻觉具有鲁棒性。为解决这一问题，我们引入了VideoHallu，这是一个由合成视频生成模型（如Veo2、Sora和Kling）生成的超过3000个视频问答对组成的基准测试集。这些问答对与专家设计的反直觉问答相结合，旨在评估多模态大型语言模型（MLLMs）在异常检测方面的批判性思维能力，这些异常对人类来说是感知明显的，但由于语言先验知识往往会产生幻觉。VideoHallu通过实例评估MLLMs在对齐、一致性、常识和物理方面的异常检测能力。我们对包括GPT-4o、Gemini-2.5-Pro、Qwen2.5-VL、Video-R1和VideoChat-R在内的最新MLLMs进行了基准测试。我们发现这些模型在MVBench和MovieChat等现实世界基准测试中表现良好，但在合成视频的基于物理和常识的推理方面仍然遇到困难。我们进一步表明，使用组合视频问答与反直觉常识和物理推理的数据集进行课程学习的集团相对策略优化（GRPO）后训练，可以提高MLLMs的异常检测和批判性思维能力，证明了有针对性的训练对于提高他们对常识和物理定律的理解的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01481v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了合成视频生成技术的现实应用及其所面临的挑战，包括常识和物理定律的违反。为解决这一问题，引入了VideoHallu基准测试，通过超过3000个视频问答对来评估多模态大型语言模型（MLLMs）在异常检测方面的能力。文章还探讨了当前最先进的大型语言模型在合成视频中的表现，并提出了通过课程学习使用组合相对策略优化（GRPO）来改善模型对常识和物理定律理解的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>合成视频生成技术受到关注，但存在违反常识和物理定律的问题。</li>
<li>需要可靠的异常检测器来理解常识并抵抗幻觉。</li>
<li>VideoHallu基准测试用于评估多模态大型语言模型（MLLMs）在异常检测方面的能力。</li>
<li>现有大型语言模型在合成视频中的表现存在挑战，特别是在物理和常识推理方面。</li>
<li>文章引入Group Relative Policy Optimization (GRPO) 方法进行有针对性的训练，提高模型对常识和物理定律的理解。</li>
<li>通过课程学习使用GRPO可以改善MLLMs的异常检测和批判性思维能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a1dd7cb2c1924ce6e6e388a2cfd747a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c0e565c33c36de724192494cf59dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd791c7ebf56cd412d8edaf50d4d8b75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c07288c94070b78a2e6183c618ed95a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d4f30a27d12edd17a705e7e98a27eba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eba42bf105f2f69700259d4c77c4224.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Task-Specific-Data-Selection-for-Instruction-Tuning-via-Monosemantic-Neuronal-Activations"><a href="#Task-Specific-Data-Selection-for-Instruction-Tuning-via-Monosemantic-Neuronal-Activations" class="headerlink" title="Task-Specific Data Selection for Instruction Tuning via Monosemantic   Neuronal Activations"></a>Task-Specific Data Selection for Instruction Tuning via Monosemantic   Neuronal Activations</h2><p><strong>Authors:Da Ma, Gonghu Shang, Zhi Chen, Libo Qin, Yijie Luo, Lei Pan, Shuai Fan, Lu Chen, Kai Yu</strong></p>
<p>Instruction tuning improves the ability of large language models (LLMs) to follow diverse human instructions, but achieving strong performance on specific target tasks remains challenging. A critical bottleneck is selecting the most relevant data to maximize task-specific performance. Existing data selection approaches include unstable influence-based methods and more stable distribution alignment methods, the latter of which critically rely on the underlying sample representation. In practice, most distribution alignment methods, from shallow features (e.g., BM25) to neural embeddings (e.g., BGE, LLM2Vec), may fail to capture how the model internally processes samples. To bridge this gap, we adopt a model-centric strategy in which each sample is represented by its neuronal activation pattern in the model, directly reflecting internal computation. However, directly using raw neuron activations leads to spurious similarity between unrelated samples due to neuron polysemanticity, where a single neuron may respond to multiple, unrelated concepts. To address this, we employ sparse autoencoders to disentangle polysemantic activations into sparse, monosemantic representations, and introduce a dedicated similarity metric for this space to better identify task-relevant data. Comprehensive experiments across multiple instruction datasets, models, tasks, and selection ratios show that our approach consistently outperforms existing data selection baselines in both stability and task-specific performance. </p>
<blockquote>
<p>指令微调提高了大型语言模型（LLM）遵循各种人类指令的能力，但在特定目标任务上实现强劲表现仍然具有挑战性。一个关键的瓶颈在于选择最相关的数据以最大化特定任务性能。现有的数据选择方法包括基于不稳定影响的方法和更稳定的分布对齐方法，后者严重依赖于底层样本表示。在实践中，大多数分布对齐方法，从浅层特征（例如BM25）到神经嵌入（例如BGE，LLM2Vec），都可能无法捕捉模型内部如何处理样本的方式。为了弥补这一差距，我们采用了一种以模型为中心的策略，其中每个样本都由其在模型中的神经元激活模式表示，直接反映内部计算。然而，直接使用原始神经元激活会导致由于神经元的多义性，导致不相关样本之间出现虚假相似性，单个神经元可能对多个不相关的概念都有反应。为了解决这一问题，我们采用稀疏自动编码器来解耦多义激活，形成稀疏、单义表示，并为此空间引入专用相似性度量，以更好地识别任务相关数据。在多个指令数据集、模型、任务和选择比例上的综合实验表明，我们的方法在稳定性和特定任务性能上始终优于现有数据选择基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15573v2">PDF</a> preprint, (20 pages, 7 figures, 13 tables)</p>
<p><strong>Summary</strong></p>
<p>该文本讨论了大型语言模型（LLM）在遵循多样化的人类指令时面临的挑战，尤其是在选择最相关的数据以最大化特定任务性能方面的瓶颈。为此，提出了一种以模型为中心的策略，通过稀疏自动编码器解决神经元激活的多义性问题，并引入专门的相似性度量方法，以更好地识别任务相关数据。实验表明，该方法在稳定性和特定任务性能上均优于现有数据选择基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在遵循多样化的人类指令时面临挑战，特别是在特定任务上的性能提升。</li>
<li>数据选择对于LLM的任务特定性能至关重要，现有方法存在不稳定和多义性问题。</li>
<li>提出了一种以模型为中心的数据表示方法，通过神经激活模式反映样本的内部计算。</li>
<li>神经元激活的多义性会导致无关样本之间的虚假相似性。</li>
<li>采用了稀疏自动编码器来解决神经元激活的多义性问题，形成稀疏、单语义的表示。</li>
<li>引入了一种新的相似性度量方法，以更好地识别与任务相关的数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15573">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-71e39fe0208e731a195786bc2b229a42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-940dada67fe1cf25ea0a56ceb894deb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1397cd8c9853ba5aa199c227bb6ffd43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6baece3178eeda22b50efeffdbad09ae.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="The-Lazy-Student’s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own"><a href="#The-Lazy-Student’s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own" class="headerlink" title="The Lazy Student’s Dream: ChatGPT Passing an Engineering Course on Its   Own"></a>The Lazy Student’s Dream: ChatGPT Passing an Engineering Course on Its   Own</h2><p><strong>Authors:Gokul Puthumanaillam, Timothy Bretl, Melkior Ornik</strong></p>
<p>This paper presents a comprehensive investigation into the capability of Large Language Models (LLMs) to successfully complete a semester-long undergraduate control systems course. Through evaluation of 115 course deliverables, we assess LLM performance using ChatGPT under a “minimal effort” protocol that simulates realistic student usage patterns. The investigation employs a rigorous testing methodology across multiple assessment formats, from auto-graded multiple choice questions to complex Python programming tasks and long-form analytical writing. Our analysis provides quantitative insights into AI’s strengths and limitations in handling mathematical formulations, coding challenges, and theoretical concepts in control systems engineering. The LLM achieved a B-grade performance (82.24%), approaching but not exceeding the class average (84.99%), with strongest results in structured assignments and greatest limitations in open-ended projects. The findings inform discussions about course design adaptation in response to AI advancement, moving beyond simple prohibition towards thoughtful integration of these tools in engineering education. Additional materials including syllabus, examination papers, design projects, and example responses can be found at the project website: <a target="_blank" rel="noopener" href="https://gradegpt.github.io/">https://gradegpt.github.io</a>. </p>
<blockquote>
<p>本文全面探讨了大型语言模型（LLM）成功完成一学期本科控制系统课程的能力。通过对115份课程交付成果的评价，我们采用ChatGPT评估LLM性能，遵循模拟现实学生使用模式的“最小努力”协议。调查采用严格的测试方法，涵盖多种评估形式，从自动分级的客观题到复杂的Python编程任务和长格式分析写作。我们的分析为AI在处理控制系统工程中的数学公式、编程挑战和理论概念方面的优势和局限性提供了定量见解。LLM取得了B级表现（82.24%），接近但未超过班级平均水平（84.99%），在结构化作业方面表现最佳，在开放式项目中表现最为受限。这些发现引发了关于适应课程设计的讨论，以应对人工智能的进步，超越简单的禁令，朝着在工程教育中深思熟虑地整合这些工具的方向发展。额外材料包括教学大纲、考试试卷、设计项目和示例答案，可在项目网站找到：<a target="_blank" rel="noopener" href="https://gradegpt.github.io./">https://gradegpt.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05760v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文探讨了大型语言模型（LLM）在完成一个学期长的控制工程课程方面的能力。通过对115份课程作品进行评价，采用模拟真实学生使用模式的“最小努力”协议，对ChatGPT进行了评估。研究采用严格的测试方法，包括自动评分的选择题、复杂的Python编程任务和长形式的分析写作。分析提供了关于AI在处理数学公式、编程挑战和理论概念的优点和局限性的定量见解。LLM的成绩达到了B级（82.24%），接近但未超过班级平均水平（84.99%），在结构化作业中的表现最佳，在开放性项目中的表现最为有限。这些发现引发了关于适应AI发展的课程设计适应的讨论，提倡超越简单禁止，以有见地的态度整合这些工具在工程教育中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）完成控制工程课程的能力得到全面研究。</li>
<li>研究通过对115份课程作品评价，采用“最小努力”协议下的ChatGPT进行评估。</li>
<li>研究采用多种评估形式，包括自动评分选择题、Python编程任务和分析写作。</li>
<li>LLM在处理数学公式、编程挑战和理论概念方面展现出优点和局限性。</li>
<li>LLM的成绩达到B级水平，接近但未超过班级平均水平。</li>
<li>在结构化作业中的表现最佳，而在开放性项目中的表现相对有限。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05760">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-09a16e38fae383d356f3326f88953312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67bdee915f3f1854ce7502388fdf0d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89ff0a65bdd64b9f97d6185862a6f43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3b91291511b7b3b642c59b7d5279932.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e8ab861defd4399182b8ef0f4a9ae66.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Mask-Enhanced-Autoregressive-Prediction-Pay-Less-Attention-to-Learn-More"><a href="#Mask-Enhanced-Autoregressive-Prediction-Pay-Less-Attention-to-Learn-More" class="headerlink" title="Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn   More"></a>Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn   More</h2><p><strong>Authors:Xialie Zhuang, Zhikai Jia, Jianjin Li, Zhenyu Zhang, Li Shen, Zheng Cao, Shiwei Liu</strong></p>
<p>Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter’s in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP’s effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model’s focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models. </p>
<blockquote>
<p>大型语言模型（LLM）被发现存在准确检索关键信息的问题。为了解决这一问题，我们提出了Mask增强自回归预测（MEAP）方法，这是一种简单有效的训练范式，它将Masked Language Modeling（MLM）无缝集成到Next-Token Prediction（NTP）中，以增强后者的上下文内检索能力。具体来说，MEAP首先随机掩盖一小部分输入标记，然后直接使用仅解码器Transformer进行标准下一个标记预测自回归。MEAP消除了MLM对双向注意力或编码器-解码器架构的需求，在预训练或推理期间没有额外的计算开销。密集的实验表明，在关键信息检索和长上下文推理任务上，MEAP大大优于NTP，而在常识推理任务上的表现则与之相当或更好。MEAP的优势还扩展到有监督微调，在“迷失中间”场景中表现出显著优势，比NTP高出11.77个百分点。我们的分析表明，MEAP的有效性源于其通过关注减少的非掩码标记集促进更可分辨的注意力分数的能力。这种机制提高了模型对任务相关信号的关注，同时减轻了周边上下文的影响。这些发现使MEAP成为大型语言模型的有前途的训练范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07490v2">PDF</a> 17 pages,7 figures</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）在准确检索关键信息方面存在缺陷。为此，本文提出一种名为Mask-Enhanced Autoregressive Prediction（MEAP）的简单有效训练范式，它将Masked Language Modeling（MLM）无缝集成到Next-Token Prediction（NTP）中，以增强其上下文检索能力。MEAP通过随机掩盖一小部分输入标记，然后使用仅解码器Transformer进行标准下一个标记预测自回归，无需双向注意力或编码器-解码器架构进行MLM，因此在预训练或推理期间没有额外的计算开销。实验表明，MEAP在关键信息检索和长上下文推理任务上的性能优于NTP，同时在常识推理任务上的表现持平或更好。MEAP的优势还扩展到有监督微调，在丢失中间场景的情况下表现出显著优势，比NTP高出11.77个百分点。分析表明，MEAP的有效性源于其通过集中在减少的非掩码标记上而促进更明显的注意力得分的能力。这种机制提高了模型对任务相关信号的关注，同时减轻了周边上下文的影响。这些发现使MEAP成为大型语言模型的有前途的训练范式。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM在准确检索关键信息方面存在挑战。</li>
<li>Mask-Enhanced Autoregressive Prediction (MEAP)训练范式结合了Masked Language Modeling (MLM)和Next-Token Prediction (NTP)。</li>
<li>MEAP提高了LLM的上下文检索能力，且无需额外的计算开销。</li>
<li>MEAP在关键信息检索和长上下文推理任务上的性能优于NTP。</li>
<li>MEAP在丢失中间场景的有监督微调中表现出显著优势。</li>
<li>MEAP通过促进更明显的注意力得分，提高了模型对任务相关信号的关注。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07490">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c23df6711654eedbc0dfeb5f98e913bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8676229573b6660592867fc418dd516.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df906f85a73cfc1218684c0d7882c8d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40fbe24a1a2ade884846750c842f9d4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0405eb1b7b995a22297ba6d12fe008a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9903c19398dec17a2298e0fc5680ac0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1851c3b489296b1915fc0f2a6ff0c84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a38603ae0dd3c72537de8c52f8b62aff.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-129b9c165e2e601f65dd2ea8be3f865e.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-05-20  Let the Trial Begin A Mock-Court Approach to Vulnerability Detection   using LLM-Based Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-44e5f538fd47a6380935eca2cf22ce5f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-20  SoftCoT++ Test-Time Scaling with Soft Chain-of-Thought Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
