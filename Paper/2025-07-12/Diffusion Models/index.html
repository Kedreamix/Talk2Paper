<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-07-12  Single-Step Latent Diffusion for Underwater Image Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-26f2170c29889b50443a202b56a12b11.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    63 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-12-更新"><a href="#2025-07-12-更新" class="headerlink" title="2025-07-12 更新"></a>2025-07-12 更新</h1><h2 id="Single-Step-Latent-Diffusion-for-Underwater-Image-Restoration"><a href="#Single-Step-Latent-Diffusion-for-Underwater-Image-Restoration" class="headerlink" title="Single-Step Latent Diffusion for Underwater Image Restoration"></a>Single-Step Latent Diffusion for Underwater Image Restoration</h2><p><strong>Authors:Jiayi Wu, Tianfu Wang, Md Abu Bakr Siddique, Md Jahidul Islam, Cornelia Fermuller, Yiannis Aloimonos, Christopher A. Metzler</strong></p>
<p>Underwater image restoration algorithms seek to restore the color, contrast, and appearance of a scene that is imaged underwater. They are a critical tool in applications ranging from marine ecology and aquaculture to underwater construction and archaeology. While existing pixel-domain diffusion-based image restoration approaches are effective at restoring simple scenes with limited depth variation, they are computationally intensive and often generate unrealistic artifacts when applied to scenes with complex geometry and significant depth variation. In this work we overcome these limitations by combining a novel network architecture (SLURPP) with an accurate synthetic data generation pipeline. SLURPP combines pretrained latent diffusion models – which encode strong priors on the geometry and depth of scenes – with an explicit scene decomposition – which allows one to model and account for the effects of light attenuation and backscattering. To train SLURPP we design a physics-based underwater image synthesis pipeline that applies varied and realistic underwater degradation effects to existing terrestrial image datasets. This approach enables the generation of diverse training data with dense medium&#x2F;degradation annotations. We evaluate our method extensively on both synthetic and real-world benchmarks and demonstrate state-of-the-art performance. Notably, SLURPP is over 200X faster than existing diffusion-based methods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It also offers compelling qualitative improvements on real-world data. Project website <a target="_blank" rel="noopener" href="https://tianfwang.github.io/slurpp/">https://tianfwang.github.io/slurpp/</a>. </p>
<blockquote>
<p>水下图像恢复算法旨在恢复水下场景的色泽、对比度和外观。它们在海洋生态、水产养殖、水下建设和考古等应用中都是至关重要的工具。虽然现有的像素域扩散型图像恢复方法在恢复深度变化有限的简单场景时很有效，但在处理具有复杂几何和显著深度变化的场景时，它们计算量大并且常常产生不真实的伪影。在这项工作中，我们通过结合新型网络架构（SLURPP）与精准合成数据生成流程来克服这些局限。SLURPP结合了预训练的潜在扩散模型，该模型对场景的几何和深度具有强烈的先验知识，以及明确的场景分解，这允许建模和考虑光衰减和后向散射的影响。为了训练SLURPP，我们设计了一个基于物理的水下图像合成管道，该管道对现有的地面图像数据集应用多样化和现实的水下退化效果。这种方法能够生成具有密集介质&#x2F;退化注释的多样化训练数据。我们在合成和实际基准测试上对我们的方法进行了广泛评估，并展示了其最先进的性能。值得注意的是，SLURPP是现有扩散方法的200倍以上，同时在合成基准测试上PSNR提高了约3分贝。它在真实世界数据上也提供了引人注目的定性改进。项目网站<a target="_blank" rel="noopener" href="https://tianfwang.github.io/slurpp/%E3%80%82">https://tianfwang.github.io/slurpp/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07878v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种结合新型网络架构SLURPP与精确合成数据生成流程的水下图像恢复方法。该方法通过利用预训练的潜在扩散模型与明确的场景分解，能够更有效地处理复杂场景和显著深度变化的水下图像。通过物理基础的水下图像合成管道设计训练数据，能够在陆地图像数据集上应用各种真实水下退化效应。该方法在合成和真实世界基准测试上表现出卓越性能，相较于现有扩散方法，速度提升超过200倍，同时在合成基准测试上的峰值信噪比（PSNR）提高约3分贝。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下图像恢复算法旨在恢复水下场景的色泽、对比度和外观，广泛应用于海洋生态、水产养殖业、水下建设和考古等领域。</li>
<li>现有像素域扩散型图像恢复方法在处理简单场景时效果显著，但在面对复杂场景和显著深度变化时，计算量大且易产生不真实的人工痕迹。</li>
<li>本文提出的SLURPP网络架构结合了预训练的潜在扩散模型和明确的场景分解，能更有效地处理复杂的水下图像。</li>
<li>设计了基于物理的水下图像合成管道，能够在陆地图像数据集上应用各种真实水下退化效应，生成丰富的训练数据。</li>
<li>SLURPP方法在合成和真实世界基准测试上表现出卓越性能，速度大幅超越现有扩散方法，同时提高了图像质量。</li>
<li>SLURPP方法提供了强大的定性改进，特别是在处理真实世界数据时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c74b8aaf7eed48957edc2fbc06ae5cde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb537cfb9da269838e9c391caa3be003.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82ff1d6a163085ec5a5be9fbd70e9c49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad79e38ac4b2877f9ab97a8b30f02e3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-427702c23a50056833be6150f5bc0161.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbe6906a401e7d5b08ca87b21a47dac7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Capture-Stage-Environments-A-Guide-to-Better-Matting"><a href="#Capture-Stage-Environments-A-Guide-to-Better-Matting" class="headerlink" title="Capture Stage Environments: A Guide to Better Matting"></a>Capture Stage Environments: A Guide to Better Matting</h2><p><strong>Authors:Hannah Dröge, Janelle Pfeifer, Saskia Rabich, Markus Plack, Reinhard Klein, Matthias B. Hullin</strong></p>
<p>Capture stages are high-end sources of state-of-the-art recordings for downstream applications in movies, games, and other media. One crucial step in almost all pipelines is the matting of images to isolate the captured performances from the background. While common matting algorithms deliver remarkable performance in other applications like teleconferencing and mobile entertainment, we found that they struggle significantly with the peculiarities of capture stage content. The goal of our work is to share insights into those challenges as a curated list of those characteristics along with a constructive discussion for proactive intervention and present a guideline to practitioners for an improved workflow to mitigate unresolved challenges. To this end, we also demonstrate an efficient pipeline to adapt state-of-the-art approaches to such custom setups without the need of extensive annotations, both offline and real-time. For an objective evaluation, we propose a validation methodology based on a leading diffusion model that highlights the benefits of our approach. </p>
<blockquote>
<p>采集阶段是为电影、游戏和其他媒体等下游应用提供最新技术录音的高端来源。在几乎所有流程中，一个至关重要的步骤是通过图像抠像技术将捕捉到的表演与背景分离。虽然常见的抠像算法在视频会议和移动娱乐等其他应用中表现出卓越的性能，但我们发现它们在应对采集阶段内容的特殊性时遇到了很大困难。我们工作的目标是分享关于这些挑战的看法，列出这些特性的精选列表，并进行建设性讨论以采取积极干预的措施，同时为从业者提供一个改进工作流程指南，以缓解未解决的挑战。为此，我们还展示了一个高效的管道，可以适应最新的技术方法来适应这种定制设置，无需大量的注释标注工作，并且适用于离线处理和实时环境。为了进行客观评估，我们提出了一种基于前沿扩散模型的验证方法，突出我们方法的优势所在。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07623v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了捕获阶段在影视、游戏等媒体下游应用中的重要性，并指出图像抠图是将捕获的表演与背景分离的关键步骤。尽管常见的抠图算法在其他应用如视频会议和手机娱乐中表现卓越，但在捕获阶段内容方面存在诸多挑战。本文旨在分享这些挑战的特性，并讨论积极应对措施，为从业者提供改进工作流程的指导。同时，展示了一种高效管道，可适应最新技术方法，无需大量标注即可应用于此类自定义设置，包括离线与实时环境。通过基于领先的扩散模型的验证方法，客观地评估了我们的方法的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>捕获阶段是影视、游戏等媒体下游应用中的高端先进技术录制重要环节。</li>
<li>图像抠图是从捕获的表演中分离背景的关键步骤。</li>
<li>常见抠图算法在捕获阶段内容方面存在挑战。</li>
<li>本文分享了对这些挑战的洞察，并讨论了积极应对措施。</li>
<li>为从业者提供了改进工作流程的指导。</li>
<li>展示了一种高效管道，可适应最新技术方法，适用于自定义设置，无需大量标注。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07623">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5199fbf7ef11096a46b40592d3c36d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b89a088d99263f56c680a695e19348.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-550922c15c4e8270748a4983f0e12f15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20640e0279490e8af4fc73f71462c6f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55edd82b50f2d97cbdf74ac3d1a1c8e7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Stable-Hair-v2-Real-World-Hair-Transfer-via-Multiple-View-Diffusion-Model"><a href="#Stable-Hair-v2-Real-World-Hair-Transfer-via-Multiple-View-Diffusion-Model" class="headerlink" title="Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion   Model"></a>Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion   Model</h2><p><strong>Authors:Kuiyuan Sun, Yuxuan Zhang, Jichao Zhang, Jiaming Liu, Wei Wang, Niculae Sebe, Yao Zhao</strong></p>
<p>While diffusion-based methods have shown impressive capabilities in capturing diverse and complex hairstyles, their ability to generate consistent and high-quality multi-view outputs – crucial for real-world applications such as digital humans and virtual avatars – remains underexplored. In this paper, we propose Stable-Hair v2, a novel diffusion-based multi-view hair transfer framework. To the best of our knowledge, this is the first work to leverage multi-view diffusion models for robust, high-fidelity, and view-consistent hair transfer across multiple perspectives. We introduce a comprehensive multi-view training data generation pipeline comprising a diffusion-based Bald Converter, a data-augment inpainting model, and a face-finetuned multi-view diffusion model to generate high-quality triplet data, including bald images, reference hairstyles, and view-aligned source-bald pairs. Our multi-view hair transfer model integrates polar-azimuth embeddings for pose conditioning and temporal attention layers to ensure smooth transitions between views. To optimize this model, we design a novel multi-stage training strategy consisting of pose-controllable latent IdentityNet training, hair extractor training, and temporal attention training. Extensive experiments demonstrate that our method accurately transfers detailed and realistic hairstyles to source subjects while achieving seamless and consistent results across views, significantly outperforming existing methods and establishing a new benchmark in multi-view hair transfer. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/sunkymepro/StableHairV2">https://github.com/sunkymepro/StableHairV2</a>. </p>
<blockquote>
<p>基于扩散的方法在捕捉多样且复杂的发型方面展现出了令人印象深刻的能力，但它们在生成一致且高质量的多视角输出方面的能力——对于数字人类和虚拟化身等现实世界应用至关重要——仍然未被充分探索。在本文中，我们提出了Stable-Hair v2，这是一种基于扩散的多视角头发转移框架。据我们所知，这是首次利用多视角扩散模型进行稳健、高保真、跨多个视角一致性的头发转移的工作。我们引入了一个全面的多视角训练数据生成流程，包括基于扩散的秃头转换器、数据增强填充模型，以及针对面部微调的多视角扩散模型，以生成高质量的三元组数据，包括秃头图像、参考发型和视角对齐的源秃头对。我们的多视角头发转移模型集成了极坐标方位嵌入进行姿态条件设置和临时注意力层，以确保不同视角之间的平滑过渡。为了优化这个模型，我们设计了一种新的多阶段训练策略，包括姿态可控潜在身份网训练、头发提取器训练和临时注意力训练。大量实验表明，我们的方法能够准确地将详细的现实发型转移到源主体上，同时在各视角实现无缝且一致的结果，显著优于现有方法，并在多视角头发转移方面建立了新的基准。代码已公开在：<a target="_blank" rel="noopener" href="https://github.com/sunkymepro/StableHairV2%E3%80%82">https://github.com/sunkymepro/StableHairV2。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07591v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于扩散模型的多视角头发转移框架Stable-Hair v2。该框架利用多视角扩散模型实现了稳健、高保真、跨不同视角的连续头发转移。为生成高质量的三元组数据，引入全面的多视角训练数据生成管道，包括基于扩散的秃顶转换器、数据增强填充模型和对脸进行微调的多视角扩散模型。该方法集成了方位嵌入用于姿态调节和临时注意力层以确保不同视角之间的平滑过渡。通过设计包含姿态可控潜在IdentityNet训练、头发提取器训练和临时注意力训练的多阶段训练策略，实现了优化模型的目的。该方法准确地将详细的现实发型转移到源主体上，同时实现跨视角的无缝和一致结果，显著优于现有方法，为多视角头发转移建立了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文提出了Stable-Hair v2，一个基于扩散模型的多视角头发转移框架。</li>
<li>这是首次利用多视角扩散模型进行头发转移的工作，可实现稳健、高保真和连续性的头发转移。</li>
<li>引入全面的多视角训练数据生成管道，包括秃顶转换器、数据增强填充模型以及面向多视角的扩散模型。</li>
<li>集成方位嵌入用于姿态调节和临时注意力层确保不同视角间的平滑过渡。</li>
<li>通过设计多阶段训练策略优化模型，包括姿态可控的IdentityNet训练、头发提取器训练和临时注意力训练。</li>
<li>实验结果表明，该方法在将详细和现实的发型转移到源主体上表现优异，实现了跨视角的无缝和一致结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07591">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3be245c4ee45527f37c4379c7d95120f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e787a68cb79a710a76a6a8f8ae5167e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2474f08c057f8ee19f39d692b6b46258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b98dbb838e6bbe6f9b327023a2e9ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-224f3aaf7f90c80ad5b7fcbce935a558.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cbf9ee6b541785971c048a6e830986e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Divergence-Minimization-Preference-Optimization-for-Diffusion-Model-Alignment"><a href="#Divergence-Minimization-Preference-Optimization-for-Diffusion-Model-Alignment" class="headerlink" title="Divergence Minimization Preference Optimization for Diffusion Model   Alignment"></a>Divergence Minimization Preference Optimization for Diffusion Model   Alignment</h2><p><strong>Authors:Binxu Li, Minkai Xu, Meihua Dang, Stefano Ermon</strong></p>
<p>Diffusion models have achieved remarkable success in generating realistic and versatile images from text prompts. Inspired by the recent advancements of language models, there is an increasing interest in further improving the models by aligning with human preferences. However, we investigate alignment from a divergence minimization perspective and reveal that existing preference optimization methods are typically trapped in suboptimal mean-seeking optimization. In this paper, we introduce Divergence Minimization Preference Optimization (DMPO), a novel and principled method for aligning diffusion models by minimizing reverse KL divergence, which asymptotically enjoys the same optimization direction as original RL. We provide rigorous analysis to justify the effectiveness of DMPO and conduct comprehensive experiments to validate its empirical strength across both human evaluations and automatic metrics. Our extensive results show that diffusion models fine-tuned with DMPO can consistently outperform or match existing techniques, specifically outperforming all existing diffusion alignment baselines by at least 64.6% in PickScore across all evaluation datasets, demonstrating the method’s superiority in aligning generative behavior with desired outputs. Overall, DMPO unlocks a robust and elegant pathway for preference alignment, bridging principled theory with practical performance in diffusion models. </p>
<blockquote>
<p>扩散模型通过文本提示生成现实且多样化的图像，取得了显著的成功。受语言模型最新进展的启发，人们越来越感兴趣通过符合人类偏好来进一步改进这些模型。然而，我们从分歧最小化角度研究对齐问题，并揭示现有的偏好优化方法通常陷入次优均值寻求优化。在本文中，我们介绍了分歧最小化偏好优化（DMPO），这是一种通过最小化反向KL分歧来对齐扩散模型的新型且基于原则的方法，它渐近地享有与原始强化学习相同的优化方向。我们提供了严格的分析来证明DMPO的有效性，并进行了全面的实验来验证其在人类评估和自动指标方面的实证实力。我们的广泛结果表明，使用DMPO微调过的扩散模型可以持续超越或匹配现有技术，特别是在PickScore上，相较于所有现有扩散对齐基准测试，至少高出64.6%，这证明了该方法在将生成行为与期望输出对齐方面的优越性。总体而言，DMPO为偏好对齐解锁了稳健而优雅的途径，在理论原则与实际性能之间架起了桥梁。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07510v1">PDF</a> 24 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型在生成图像时的偏好对齐问题。作者从分歧最小化角度进行研究，提出了Divergence Minimization Preference Optimization（DMPO）方法，该方法通过最小化反向KL分歧来实现扩散模型的偏好对齐。理论分析证明了DMPO的有效性，并通过实验验证了其在人类评估和自动指标上的优越性。使用DMPO微调后的扩散模型在所有评估数据集上的PickScore至少优于现有技术基线64.6%，展现出其在对齐生成行为与期望输出方面的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型已成功生成与文本提示相符的真实图像。</li>
<li>随着语言模型的发展，如何进一步改善扩散模型以满足人类偏好成为研究热点。</li>
<li>本文从分歧最小化的角度研究偏好对齐问题。</li>
<li>提出了一种新的方法——Divergence Minimization Preference Optimization（DMPO），用于优化扩散模型的偏好对齐。</li>
<li>DMPO方法通过最小化反向KL分歧来实现扩散模型的偏好对齐，与原始强化学习的优化方向一致。</li>
<li>实验结果显示，使用DMPO方法微调后的扩散模型在多个评估指标上表现优异，特别是PickScore。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07510">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eaf848c9412681195d966ca193c9e861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99051acf766b09a1ed3468854eef3a6f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Degradation-Agnostic-Statistical-Facial-Feature-Transformation-for-Blind-Face-Restoration-in-Adverse-Weather-Conditions"><a href="#Degradation-Agnostic-Statistical-Facial-Feature-Transformation-for-Blind-Face-Restoration-in-Adverse-Weather-Conditions" class="headerlink" title="Degradation-Agnostic Statistical Facial Feature Transformation for Blind   Face Restoration in Adverse Weather Conditions"></a>Degradation-Agnostic Statistical Facial Feature Transformation for Blind   Face Restoration in Adverse Weather Conditions</h2><p><strong>Authors:Chang-Hwan Son</strong></p>
<p>With the increasing deployment of intelligent CCTV systems in outdoor environments, there is a growing demand for face recognition systems optimized for challenging weather conditions. Adverse weather significantly degrades image quality, which in turn reduces recognition accuracy. Although recent face image restoration (FIR) models based on generative adversarial networks (GANs) and diffusion models have shown progress, their performance remains limited due to the lack of dedicated modules that explicitly address weather-induced degradations. This leads to distorted facial textures and structures. To address these limitations, we propose a novel GAN-based blind FIR framework that integrates two key components: local Statistical Facial Feature Transformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The local SFFT module enhances facial structure and color fidelity by aligning the local statistical distributions of low-quality (LQ) facial regions with those of high-quality (HQ) counterparts. Complementarily, the DAFE module enables robust statistical facial feature extraction under adverse weather conditions by aligning LQ and HQ encoder representations, thereby making the restoration process adaptive to severe weather-induced degradations. Experimental results demonstrate that the proposed degradation-agnostic SFFT model outperforms existing state-of-the-art FIR methods based on GAN and diffusion models, particularly in suppressing texture distortions and accurately reconstructing facial structures. Furthermore, both the SFFT and DAFE modules are empirically validated in enhancing structural fidelity and perceptual quality in face restoration under challenging weather scenarios. </p>
<blockquote>
<p>随着智能CCTV系统在户外环境中的部署越来越多，对于适应恶劣天气条件的面部识别系统的需求也在日益增长。恶劣天气会严重降低图像质量，进而导致识别精度下降。尽管基于生成对抗网络（GANs）和扩散模型的面部图像恢复（FIR）模型已经取得了进展，但由于缺乏专门解决天气引起的退化的模块，其性能仍然有限。这会导致面部纹理和结构失真。为了解决这些局限性，我们提出了一种基于GAN的盲FIR框架，该框架包含两个关键组件：局部统计面部特征变换（SFFT）和退化无关特征嵌入（DAFE）。局部SFFT模块通过对齐低质量（LQ）面部区域的局部统计分布与高质量（HQ）对应区域的统计分布，增强面部结构和颜色保真度。互补地，DAFE模块通过对齐LQ和HQ编码器表示，实现在恶劣天气条件下的稳健统计面部特征提取，从而使恢复过程适应于由恶劣天气引起的退化。实验结果表明，所提出的退化无关SFFT模型在抑制纹理失真和准确重建面部结构方面优于现有的基于GAN和扩散模型的先进FIR方法。此外，SFFT和DAFE模块在增强结构保真度和感知质量方面也得到了实证验证，在具有挑战性的天气情况下进行面部恢复时效果显著。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07464v1">PDF</a> </p>
<p><strong>摘要</strong><br>    针对户外环境中智能监控系统的广泛应用，对面部识别系统在恶劣天气条件下的优化需求日益增长。恶劣天气会显著降图像质量，进而影响识别准确率。尽管基于生成对抗网络（GAN）和扩散模型的面部图像恢复（FIR）模型已取得进展，但由于缺乏专门应对天气引起的降质的模块，其性能仍然有限。为此，提出一种新颖的基于GAN的盲FIR框架，包含两个关键组件：局部统计面部特征变换（SFFT）和降质无关特征嵌入（DAFE）。SFFT模块通过对齐低质量（LQ）面部区域与高质量（HQ）区域的局部统计分布，增强面部结构和色彩保真度。同时，DAFE模块通过对齐LQ和HQ编码器表示，使面部特征提取在恶劣天气条件下更为稳健，从而使恢复过程适应严重的天气引起的降质。实验结果表明，所提出的降质无关SFFT模型在基于GAN和扩散模型的现有先进FIR方法中表现优异，尤其在抑制纹理失真和准确重建面部结构方面。此外，SFFT和DAFE模块在提高结构保真度和感知质量方面也得到了实证。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>恶劣天气对面部识别系统造成挑战，降低图像质量和识别准确率。</li>
<li>基于GAN和扩散模型的现有面部图像恢复（FIR）模型虽有所进展，但仍存在性能限制。</li>
<li>提出的盲FIR框架结合局部统计面部特征变换（SFFT）和降质无关特征嵌入（DAFE）两大模块以应对天气引起的图像降质。</li>
<li>SFFT模块通过局部统计分布对齐增强面部结构和色彩保真度。</li>
<li>DAFE模块使面部特征提取在恶劣天气下更为稳健，适应严重天气引起的降质。</li>
<li>实证结果显示，所提出模型在抑制纹理失真和面部结构重建方面表现优异。</li>
<li>SFFT和DAFE模块在提高结构保真度和感知质量方面得到验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07464">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19bf915952f0961f0bd4826afaadac3e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EscherNet-Simultaneous-Amodal-Completion-and-Scalable-View-Synthesis-through-Masked-Fine-Tuning-and-Enhanced-Feed-Forward-3D-Reconstruction"><a href="#EscherNet-Simultaneous-Amodal-Completion-and-Scalable-View-Synthesis-through-Masked-Fine-Tuning-and-Enhanced-Feed-Forward-3D-Reconstruction" class="headerlink" title="EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis   through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction"></a>EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis   through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction</h2><p><strong>Authors:Xinan Zhang, Muhammad Zubair Irshad, Anthony Yezzi, Yi-Chang Tsai, Zsolt Kira</strong></p>
<p>We propose EscherNet++, a masked fine-tuned diffusion model that can synthesize novel views of objects in a zero-shot manner with amodal completion ability. Existing approaches utilize multiple stages and complex pipelines to first hallucinate missing parts of the image and then perform novel view synthesis, which fail to consider cross-view dependencies and require redundant storage and computing for separate stages. Instead, we apply masked fine-tuning including input-level and feature-level masking to enable an end-to-end model with the improved ability to synthesize novel views and conduct amodal completion. In addition, we empirically integrate our model with other feed-forward image-to-mesh models without extra training and achieve competitive results with reconstruction time decreased by 95%, thanks to its ability to synthesize arbitrary query views. Our method’s scalable nature further enhances fast 3D reconstruction. Despite fine-tuning on a smaller dataset and batch size, our method achieves state-of-the-art results, improving PSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings, while also generalizing to real-world occluded reconstruction. </p>
<blockquote>
<p>我们提出了EscherNet++，这是一种经过掩码微调（fine-tuning）的扩散模型，能够以零样本方式合成对象的新视角，并具有模态完成能力。现有方法采用多阶段和复杂的流水线，首先虚构图像缺失的部分，然后进行新视角合成，这种方法忽略了跨视角依赖性，需要针对各个阶段进行冗余存储和计算。相反，我们应用包括输入级别和特征级别的掩码进行掩码微调，使端到端模型能够合成新视角并具备模态完成能力。此外，我们通过实证将我们的模型与其他前馈图像到网格模型相结合，无需额外训练，由于模型能够合成任意查询视角，重建时间减少了95%。我们方法的可扩展性进一步加强了快速三维重建。尽管在较小的数据集和批次大小上进行微调，我们的方法取得了最先进的成果，在遮挡任务的PSNR指标上提高了3.9，体积IoU提高了0.28的10输入设置上，同时推广到真实世界的遮挡重建任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07410v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为EscherNet++的扩散模型改进方案，通过采用掩码微调技术，能够零样本方式合成对象的新视角并具有无模态完成能力。现有方法采用多阶段和复杂的流水线来先模拟图像缺失部分，然后进行新视角合成，这种方法忽略了跨视角的依赖关系，并需要冗余的存储和计算资源。相比之下，EscherNet++通过输入级和特征级的掩码微调，实现了端到端的模型，提高了合成新视角和无模态完成的能力。此外，该模型还可以与其他前馈图像到网格模型进行实证集成，无需额外训练，由于能够合成任意查询视角，重建时间减少了95%。该方法的可扩展性进一步提高了快速三维重建。即使在较小的数据集和批量大小上进行微调，该方法也达到了最先进的成果，在遮挡任务的PSNR指标上提高了3.9，体积IoU提高了0.28的10输入设置中，而且还能够推广到真实世界的遮挡重建。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EscherNet++是一个通过掩码微调优化的扩散模型，可零样本方式合成对象的新视角。</li>
<li>与多阶段和复杂流水线方法相比，EscherNet++实现了端到端的模型，提高了新视角合成和无模态完成能力。</li>
<li>EscherNet++通过输入级和特征级掩码微调技术来提升性能。</li>
<li>该模型可与其他前馈图像到网格模型集成，无需额外训练。</li>
<li>EscherNet++能够合成任意查询视角，大大降低了三维重建的时间。</li>
<li>EscherNet++具有可扩展性，在遮挡任务中表现出色，并在PSNR和体积IoU指标上达到了先进水准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07410">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-71d60be4e0a74705b9da6c311af876c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6155b3d8fb9b568f4eab52e82ca651d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f68d5f54bd3c1c851e089d4bce091540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcb2b7254d68fb34ea07f10a014f61e2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Interpretable-EEG-to-Image-Generation-with-Semantic-Prompts"><a href="#Interpretable-EEG-to-Image-Generation-with-Semantic-Prompts" class="headerlink" title="Interpretable EEG-to-Image Generation with Semantic Prompts"></a>Interpretable EEG-to-Image Generation with Semantic Prompts</h2><p><strong>Authors:Arshak Rezvani, Ali Akbari, Kosar Sanjar Arani, Maryam Mirian, Emad Arasteh, Martin J. McKeown</strong></p>
<p>Decoding visual experience from brain signals offers exciting possibilities for neuroscience and interpretable AI. While EEG is accessible and temporally precise, its limitations in spatial detail hinder image reconstruction. Our model bypasses direct EEG-to-image generation by aligning EEG signals with multilevel semantic captions – ranging from object-level to abstract themes – generated by a large language model. A transformer-based EEG encoder maps brain activity to these captions through contrastive learning. During inference, caption embeddings retrieved via projection heads condition a pretrained latent diffusion model for image generation. This text-mediated framework yields state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable alignment to known neurocognitive pathways. Dominant EEG-caption associations reflected the importance of different semantic levels extracted from perceived images. Saliency maps and t-SNE projections reveal semantic topography across the scalp. Our model demonstrates how structured semantic mediation enables cognitively aligned visual decoding from EEG. </p>
<blockquote>
<p>从脑信号解码视觉经验为神经科学和可解释的AI提供了令人兴奋的可能性。脑电图（EEG）易于获取且时间精确，但其空间细节的局限性阻碍了图像重建。我们的模型通过使脑电图信号与大型语言模型生成的多层次语义标题（从对象级别到抽象主题）对齐，从而绕过了直接的脑电图到图像生成过程。基于变压器的脑电图编码器通过对比学习将这些标题映射到大脑活动上。在推理过程中，通过投影头检索的标题嵌入为图像生成提供了预训练的潜在扩散模型的先验条件。这种文本介导的框架在EEGCVPR数据集上实现了最先进的视觉解码效果，并与已知的神经认知途径具有可解释的对齐性。主导的脑电图标题关联反映了从感知图像中提取的不同语义级别的重要性。显著图（saliency maps）和t-SNE投影揭示了头皮上的语义地形。我们的模型展示了结构化语义中介如何使脑电图的认知对齐视觉解码成为可能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07157v1">PDF</a> Actionable Interpretability Workshop (non-archival) at the 42   International Conference on Machine Learning</p>
<p><strong>Summary</strong></p>
<p>基于脑电信号解码视觉体验对于神经科学和可解释人工智能领域具有巨大潜力。本文提出一种新方法，通过语义字幕将EEG信号与图像生成相联系，避免了直接生成EEG图像的技术难题。该方法使用对比学习训练基于变压器的EEG编码器，将脑活动与语义字幕相对应，语义字幕涵盖从对象级别到抽象主题的不同层次。通过投影头检索字幕嵌入，进而在预训练的潜在扩散模型中生成图像。该文本介导框架在EEGCVPR数据集上实现了最先进的视觉解码效果，与已知神经认知途径的对齐解释性较强。EEG与字幕的主要关联反映了从感知图像中提取的不同语义层次的重要性。显著性图和t-SNE投影揭示了头皮表面的语义地形。该研究展示了结构化语义调解如何实现与认知对齐的EEG视觉解码。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EEG在视觉解码方面具有巨大潜力，可以通过语义字幕与图像生成相联系。</li>
<li>使用对比学习训练基于变压器的EEG编码器，将脑活动与不同层次的语义字幕相对应。</li>
<li>通过投影头检索字幕嵌入，在预训练的潜在扩散模型中生成图像。</li>
<li>文本介导框架在EEGCVPR数据集上实现了先进的视觉解码效果。</li>
<li>EEG与字幕的主要关联反映了从感知图像中提取的不同语义层次的重要性。</li>
<li>显著性图和t-SNE投影揭示了语义在头皮表面的分布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49d6141374562c9add2c28504fbb9994.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5f1f4e4c5d24b83a56dd075dedc023a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3caa5e14cad86a232b4a07716f04b51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00593c5583479a49ce771ddc58bfe6c6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PWD-Prior-Guided-and-Wavelet-Enhanced-Diffusion-Model-for-Limited-Angle-CT"><a href="#PWD-Prior-Guided-and-Wavelet-Enhanced-Diffusion-Model-for-Limited-Angle-CT" class="headerlink" title="PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT"></a>PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT</h2><p><strong>Authors:Yi Liu, Yiyang Wen, Zekun Zhou, Junqi Ma, Linghang Wang, Yucheng Yao, Liu Shi, Qiegen Liu</strong></p>
<p>Generative diffusion models have received increasing attention in medical imaging, particularly in limited-angle computed tomography (LACT). Standard diffusion models achieve high-quality image reconstruction but require a large number of sampling steps during inference, resulting in substantial computational overhead. Although skip-sampling strategies have been proposed to improve efficiency, they often lead to loss of fine structural details. To address this issue, we propose a prior information embedding and wavelet feature fusion fast sampling diffusion model for LACT reconstruction. The PWD enables efficient sampling while preserving reconstruction fidelity in LACT, and effectively mitigates the degradation typically introduced by skip-sampling. Specifically, during the training phase, PWD maps the distribution of LACT images to that of fully sampled target images, enabling the model to learn structural correspondences between them. During inference, the LACT image serves as an explicit prior to guide the sampling trajectory, allowing for high-quality reconstruction with significantly fewer steps. In addition, PWD performs multi-scale feature fusion in the wavelet domain, effectively enhancing the reconstruction of fine details by leveraging both low-frequency and high-frequency information. Quantitative and qualitative evaluations on clinical dental arch CBCT and periapical datasets demonstrate that PWD outperforms existing methods under the same sampling condition. Using only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and 10% gain in SSIM. </p>
<blockquote>
<p>生成扩散模型在医学成像中得到了越来越多的关注，特别是在有限角度计算机断层扫描（LACT）中。标准扩散模型能够实现高质量图像重建，但在推理过程中需要大量采样步骤，导致计算开销大。虽然提出了跳过采样策略来提高效率，但它们往往会导致精细结构细节的丢失。为了解决这一问题，我们提出了一种基于先验信息嵌入和小波特征融合的快速采样扩散模型，用于LACT重建。PWD（Prior Wavelet Diffusion）能够在保持LACT重建保真度的同时实现高效采样，并有效缓解跳过采样通常引起的退化。具体而言，在训练阶段，PWD将LACT图像的分布映射到完全采样目标图像的分布，使模型能够学习两者之间的结构对应关系。在推理阶段，LACT图像作为明确的先验来引导采样轨迹，允许以较少的步骤实现高质量重建。此外，PWD在小波域执行多尺度特征融合，通过利用低频和高频信息有效地增强了精细细节的重建。在临床牙科全景CBCT和根尖周数据集上的定量和定性评估表明，在相同的采样条件下，PWD优于现有方法。仅使用50个采样步骤，PWD在PSNR上实现了至少1.7 dB的提升，在SSIM上获得了10%的增益。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05317v2">PDF</a> </p>
<p><strong>Summary</strong><br>    针对有限角度计算机断层扫描（LACT）的重构问题，提出了一种基于先验信息嵌入和小波特征融合的快速采样扩散模型。该模型在训练阶段将LACT图像分布映射到完全采样的目标图像分布，学习两者之间的结构对应关系。在推理阶段，LACT图像作为显式先验引导采样轨迹，实现高质量重构并大大减少采样步骤。同时，该模型利用小波域多尺度特征融合，有效增强精细细节的重构。实验证明，该模型在相同采样条件下优于现有方法，50步采样即可实现至少1.7 dB的峰值信噪比（PSNR）提升和10%的结构相似性指数（SSIM）增长。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成性扩散模型在医学成像领域，特别是在有限角度计算机断层扫描（LACT）中受到关注。</li>
<li>标准扩散模型需要大量采样步骤，计算开销大，而跳过采样策略可能导致细节损失。</li>
<li>提出的模型利用先验信息嵌入和小波特征融合进行快速采样扩散。</li>
<li>在训练阶段，模型学习LACT图像与完全采样目标图像之间的结构对应关系。</li>
<li>在推理阶段，LACT图像作为显式先验引导采样轨迹，实现高效高质量重构。</li>
<li>模型利用小波域多尺度特征融合，增强细节重构能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05317">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab3e69ab774fe780bcd12a12de07891e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c77d2f231bcd03bfb13565de40981da4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab472ee626d855e79e9d3b582d27684b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ffe0bf2d6eeeb6d75f26e36f95c53a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f7169be35bc1757d66024e170114c28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1e3343762ed90af0c65f348b8b99008.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation"><a href="#Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation" class="headerlink" title="Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation"></a>Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation</h2><p><strong>Authors:Fangyijie Wang, Kevin Whelan, Félix Balado, Kathleen M. Curran, Guénolé Silvestre</strong></p>
<p>Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66% and 94.38% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub. </p>
<blockquote>
<p>医学图像数据由于隐私和监管限制，其可访问性比其他领域的数据要低。此外，标注需要临床专家进行昂贵且耗时的手动图像标注。为了克服这些挑战，合成医学数据生成提供了一个有前景的解决方案。采用生成式深度学习模型的生成式人工智能（GenAI）在生成逼真的合成图像方面已证明是有效的。本研究提出了一种使用扩散模型的新型掩膜引导GenAI方法，生成合成胎儿头部超声图像及其分割掩膜配对。这些合成配对数据增强了真实数据集，用于监督微调Segment Anything Model（SAM）。结果表明，合成数据有效地捕捉了真实图像的特征，尤其是在使用有限数量的真实图像-掩膜配对进行训练时，该方法达到了最先进的胎儿头部分割效果。尤其是，使用来自西班牙和非洲队列的少量超声图像，分割达到迪克分数（Dice Scores）分别为94.66%和94.38%。我们的代码、模型和数据均可在GitHub上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23664v2">PDF</a> Accepted at Irish Machine Vision and Image Processing Conference   (IMVIP) 2025</p>
<p><strong>Summary</strong><br>     本研究提出一种使用扩散模型生成带有分割掩膜合成胎儿头部超声图像的新方法。这种方法克服了医学图像数据难以获取和标注成本高昂的问题，通过生成合成数据增强真实数据集，用于监督微调SAM模型。结果显示合成数据有效捕捉真实图像特征，并在有限真实图像-掩膜对训练下达到先进胎儿头部分割效果，Dice得分分别为94.66%和94.38%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像数据因隐私和监管限制而难以获取。</li>
<li>手工标注医学图像耗时且成本高昂。</li>
<li>生成式AI（GenAI）采用深度生成模型可有效生成逼真的合成图像。</li>
<li>本研究提出一种新颖的mask-guided GenAI方法，使用扩散模型生成配对的合成胎儿头部超声图像和分割掩膜。</li>
<li>合成数据增强现实数据集，用于监督微调SAM模型。</li>
<li>合成数据有效捕捉真实图像特征，在有限真实图像-掩膜对训练下达到先进胎儿头部分割效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23664">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0bb205423e332b4317b0c0b676f0f8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a6e2e20b23c412a54f5b1a6d39a4562d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e224c2d283809a9b64b55df257ff009.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HadaNorm-Diffusion-Transformer-Quantization-through-Mean-Centered-Transformations"><a href="#HadaNorm-Diffusion-Transformer-Quantization-through-Mean-Centered-Transformations" class="headerlink" title="HadaNorm: Diffusion Transformer Quantization through Mean-Centered   Transformations"></a>HadaNorm: Diffusion Transformer Quantization through Mean-Centered   Transformations</h2><p><strong>Authors:Marco Federici, Riccardo Del Chiaro, Boris van Breugel, Paul Whatmough, Markus Nagel</strong></p>
<p>Diffusion models represent the cutting edge in image generation, but their high memory and computational demands hinder deployment on resource-constrained devices. Post-Training Quantization (PTQ) offers a promising solution by reducing the bitwidth of matrix operations. However, standard PTQ methods struggle with outliers, and achieving higher compression often requires transforming model weights and activations before quantization. In this work, we propose HadaNorm, a novel linear transformation that extends existing approaches by both normalizing channels activations and applying Hadamard transforms to effectively mitigate outliers and enable aggressive activation quantization. We demonstrate that HadaNorm consistently reduces quantization error across the various components of transformer blocks, outperforming state-of-the-art methods. </p>
<blockquote>
<p>扩散模型是图像生成领域的前沿技术，但其对内存和计算资源的高需求限制了其在资源受限设备上的部署。后训练量化（PTQ）通过降低矩阵操作的位宽提供了一种有前景的解决方案。然而，标准PTQ方法难以处理异常值，为实现更高的压缩，通常需要在量化前对模型权重和激活进行转换。在这项工作中，我们提出了HadaNorm，这是一种新型线性转换，它通过标准化通道激活和应用哈达玛变换来有效缓解异常值问题，从而实现激烈的激活量化。我们证明，HadaNorm在变压器块的各种组件中一致地减少了量化误差，超越了最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09932v2">PDF</a> 8 Pages, 6 Figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散模型在图像生成领域的最新进展，但其高内存和计算需求限制了其在资源受限设备上的应用。为解决这一问题，研究人员提出了采用后训练量化（PTQ）方法降低矩阵操作位宽的策略。然而，标准PTQ方法面临处理异常值的问题，并且为了在量化前变换模型权重和激活值来实现更高的压缩率。本研究提出了HadaNorm，一种新型线性变换方法，它通过归一化通道激活和采用Hadamard变换来有效减轻异常值问题，从而实现激烈的激活量化。实验证明，HadaNorm在变压器块的各个组件中一致降低了量化误差，优于现有最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型是图像生成的前沿技术，但其在资源受限设备上的部署受限。</li>
<li>后训练量化（PTQ）是解决扩散模型高内存和计算需求的有效方法。</li>
<li>标准PTQ方法面临处理异常值的问题。</li>
<li>HadaNorm是一种新型线性变换方法，用于在量化过程中有效减轻异常值问题。</li>
<li>HadaNorm通过归一化通道激活和采用Hadamard变换来实现激活量化的优化。</li>
<li>HadaNorm在不同组件的变压器块中一致降低了量化误差。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09932">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e65ed53f6ed2be9e9d39675ff52de529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd62f7222df75230a7a2c71c268bf3be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99aaa658915aa3bb17ef250df466e2cd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Revisiting-Likelihood-Based-Out-of-Distribution-Detection-by-Modeling-Representations"><a href="#Revisiting-Likelihood-Based-Out-of-Distribution-Detection-by-Modeling-Representations" class="headerlink" title="Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling   Representations"></a>Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling   Representations</h2><p><strong>Authors:Yifan Ding, Arturas Aleksandraus, Amirhossein Ahmadian, Jonas Unger, Fredrik Lindsten, Gabriel Eilertsen</strong></p>
<p>Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\href{<a target="_blank" rel="noopener" href="https://github.com/limchaos/Likelihood-OOD.git%7D%7B/texttt%7Bhttps://github.com/limchaos/Likelihood-OOD.git%7D%7D$">https://github.com/limchaos/Likelihood-OOD.git}{\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$</a>. </p>
<blockquote>
<p>在深度学习的可靠性保障方面，尤其是在关键安全应用中，超出分布范围的检测（Out-of-Distribution Detection，简称OOD）是非常关键的。基于可能性的深度生成模型在历史中因其对超出分布范围检测的欠佳表现而受到批评。当应用于图像数据时，这些模型往往会对超出分布范围的数据分配更高的可能性，高于对内部分布样本的分配可能性。在这项工作中，我们证明了可能性并非天生存在问题。相反，图像空间中的某些属性阻止了可能性作为一个有效的检测分数。给定一个足够好的可能性估计器，特别是使用扩散模型的概率流公式，我们展示了在预训练编码器的表示空间中应用时，基于可能性的方法仍然可以与最先进的方法表现相当。我们工作的代码可以在<a target="_blank" rel="noopener" href="https://github.com/limchaos/Likelihood-OOD.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/limchaos/Likelihood-OOD.git找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07793v3">PDF</a> Scandinavian Conference on Image Analysis 2025 (oral)</p>
<p><strong>Summary</strong><br>     基于似然的方法在进行异常检测（OOD检测）时能够表现良好，但需要选择正确的图像特征空间并使用合适的预训练编码器。本工作指出，利用扩散模型的概率流公式构建似然估计器，可以在预训练编码器的特征空间上实现与最新技术相当的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OOD检测对于确保深度学习系统的可靠性至关重要，特别是在安全关键应用中。</li>
<li>传统上基于似然的深度生成模型在OOD检测中表现不佳，容易对OOD数据赋予较高的似然值。</li>
<li>在图像空间中，某些特性阻碍了似然作为有效的检测分数。</li>
<li>使用良好的似然估计器，特别是基于扩散模型的概率流公式，可以改善基于似然的方法的性能。</li>
<li>在预训练编码器的特征空间上应用基于似然的方法，可以达到与最新技术相当的效果。</li>
<li>该工作的代码已公开，可供进一步研究参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07793">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-38c75be78c9965de94664607bc230cfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61fc3b5ec20e0b4908265527832f9765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a5d7b2d126d6163d0396a867c39b66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a2be2096465d08fc90357df24ea8d8b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Localized-Concept-Erasure-for-Text-to-Image-Diffusion-Models-Using-Training-Free-Gated-Low-Rank-Adaptation"><a href="#Localized-Concept-Erasure-for-Text-to-Image-Diffusion-Models-Using-Training-Free-Gated-Low-Rank-Adaptation" class="headerlink" title="Localized Concept Erasure for Text-to-Image Diffusion Models Using   Training-Free Gated Low-Rank Adaptation"></a>Localized Concept Erasure for Text-to-Image Diffusion Models Using   Training-Free Gated Low-Rank Adaptation</h2><p><strong>Authors:Byung Hyun Lee, Sungjin Lim, Se Young Chun</strong></p>
<p>Fine-tuning based concept erasing has demonstrated promising results in preventing generation of harmful contents from text-to-image diffusion models by removing target concepts while preserving remaining concepts. To maintain the generation capability of diffusion models after concept erasure, it is necessary to remove only the image region containing the target concept when it locally appears in an image, leaving other regions intact. However, prior arts often compromise fidelity of the other image regions in order to erase the localized target concept appearing in a specific area, thereby reducing the overall performance of image generation. To address these limitations, we first introduce a framework called localized concept erasure, which allows for the deletion of only the specific area containing the target concept in the image while preserving the other regions. As a solution for the localized concept erasure, we propose a training-free approach, dubbed Gated Low-rank adaptation for Concept Erasure (GLoCE), that injects a lightweight module into the diffusion model. GLoCE consists of low-rank matrices and a simple gate, determined only by several generation steps for concepts without training. By directly applying GLoCE to image embeddings and designing the gate to activate only for target concepts, GLoCE can selectively remove only the region of the target concepts, even when target and remaining concepts coexist within an image. Extensive experiments demonstrated GLoCE not only improves the image fidelity to text prompts after erasing the localized target concepts, but also outperforms prior arts in efficacy, specificity, and robustness by large margin and can be extended to mass concept erasure. </p>
<blockquote>
<p>基于概念擦除的微调在防止文本到图像扩散模型生成有害内容方面显示出良好的前景。它通过消除目标概念同时保留剩余概念来实现这一点。为了保持扩散模型在概念擦除后的生成能力，有必要只移除图像中包含目标概念的区域，同时保持其他区域不变。然而，现有的技术往往为了消除特定区域中出现的局部目标概念而牺牲了其他图像区域的保真度，从而降低了图像生成的总体性能。为了解决这些局限性，我们首先引入了一个名为局部概念擦除的框架，它允许只删除图像中包含目标概念的特定区域，同时保留其他区域。作为局部概念擦除的解决方案，我们提出了一种无需训练的方法，称为用于概念擦除的带门控的低秩自适应（GLoCE），它将一个轻量级模块注入到扩散模型中。GLoCE由低秩矩阵和简单门组成，仅由几代的生成步骤决定概念，无需训练。通过将GLoCE直接应用于图像嵌入并设计仅在目标概念出现时激活的门，GLoCE可以仅选择性地删除目标概念的区域，即使目标概念和剩余的概念共存于图像中也是如此。大量实验表明，GLoCE不仅在擦除局部目标概念后对文本提示的图像保真度有所提高，而且在有效性、特异性和稳健性方面大大超越了现有技术，并且可以扩展到大规模的概念擦除。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12356v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong>：<br>基于概念擦除的微调在防止文本到图像扩散模型生成有害内容方面展现出良好效果。为了维护扩散模型在概念擦除后的生成能力，只需删除包含目标概念的图像区域，同时保留其他区域。为解决现有技术在擦除特定区域目标概念时损害其他图像区域保真度的问题，本文引入了一个名为局部概念擦除的框架，并提出了一种无需训练的解决方案——GLoCE（门控低秩适应概念擦除）。GLoCE通过在扩散模型中注入轻量级模块来实现选择性擦除目标概念区域，即使目标概念和剩余概念共存于图像中也是如此。实验表明，GLoCE不仅能提高擦除目标概念后的图像对文本提示的保真度，而且在有效性、特异性和稳健性方面大大优于现有技术，并可扩展到大规模概念擦除。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>概念擦除在防止文本到图像扩散模型生成有害内容方面至关重要。</li>
<li>局部概念擦除框架允许仅删除图像中包含目标概念的特定区域，同时保持其他区域的完整性。</li>
<li>GLoCE是一种无需训练的解决方案，通过注入轻量级模块到扩散模型中实现选择性擦除目标概念区域。</li>
<li>GLoCE通过直接在图像嵌入上应用并设计仅针对目标概念的门来控制激活，实现选择性擦除。</li>
<li>实验表明，GLoCE在擦除局部目标概念后提高了图像对文本提示的保真度。</li>
<li>GLoCE在有效性、特异性和稳健性方面显著优于现有技术。</li>
<li>GLoCE可扩展到大规模概念擦除。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12356">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-26f2170c29889b50443a202b56a12b11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d196095f55585ddcb9d61d6de16975fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-141712c7a0ae182942897243edffcd0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2eb732eaccbcf8fb7462bad3d1c651c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f13502c0508bf83613907869935ad38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89013911ec3360982e50460ab4a59391.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Solving-Inverse-Problems-using-Diffusion-with-Iterative-Colored-Renoising"><a href="#Solving-Inverse-Problems-using-Diffusion-with-Iterative-Colored-Renoising" class="headerlink" title="Solving Inverse Problems using Diffusion with Iterative Colored   Renoising"></a>Solving Inverse Problems using Diffusion with Iterative Colored   Renoising</h2><p><strong>Authors:Matt C. Bendel, Saurav K. Shastri, Rizwan Ahmad, Philip Schniter</strong></p>
<p>Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relatively poor, especially early in the reverse process, and so we propose a new approach that iteratively reestimates and “renoises” the estimate several times per diffusion step. This iterative approach, which we call Fast Iterative REnoising (FIRE), injects colored noise that is shaped to ensure that the pre-trained diffusion model always sees white noise, in accordance with how it was trained. We then embed FIRE into the DDIM reverse process and show that the resulting “DDfire” offers state-of-the-art accuracy and runtime on several linear inverse problems, as well as phase retrieval. Our implementation is at <a target="_blank" rel="noopener" href="https://github.com/matt-bendel/DDfire">https://github.com/matt-bendel/DDfire</a> </p>
<blockquote>
<p>使用预训练的扩散模型以无监督的方式解决成像反问题，需要在扩散反向过程中近似测量条件得分函数的梯度。我们显示现有方法产生的近似值相对较差，尤其是在反向过程的早期，因此，我们提出了一种新方法，该方法在每个扩散步骤中多次重新估计和“增加噪声”。我们称这种迭代方法为快速迭代重新噪声化（FIRE），它注入有色噪声，以确保预训练的扩散模型始终看到白噪声，符合其训练方式。然后，我们将FIRE嵌入到DDIM反向过程中，并证明由此产生的“DDfire”在几个线性反问题以及相位检索方面提供了最先进的准确性和运行时。我们的实现位于<a target="_blank" rel="noopener" href="https://github.com/matt-bendel/DDfire%E3%80%82">https://github.com/matt-bendel/DDfire。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17468v3">PDF</a> </p>
<p><strong>Summary</strong>：利用预训练的扩散模型以无监督的方式解决成像反问题，需要估计测量条件分数函数的梯度。现有方法产生的近似值在早期逆向过程中相对较差，因此提出了一种新的方法——快速迭代去噪（FIRE），在每个扩散步骤中多次重新估计和“加入噪声”。将FIRE嵌入DDIM逆向过程，得到的DDfire在多个线性反问题以及相位检索方面具有最佳精度和运行时效率。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>扩散模型可以无监督地解决成像反问题。</li>
<li>现有方法估计测量条件分数函数的梯度时存在不足。</li>
<li>提出了快速迭代去噪（FIRE）方法，以提高估计的准确性。</li>
<li>FIRE方法通过注入彩色噪声，确保预训练的扩散模型始终看到白噪声。</li>
<li>DDfire方法将FIRE嵌入DDIM逆向过程。</li>
<li>DDfire在多个线性反问题和相位检索方面具有最佳性能和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8699ffb487311ce20fbd12bd11f55f53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07f5a0d543b4b18350a3d7847871ef27.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Diffusion-Augmented-Retrieval-A-Training-Free-Approach-to-Interactive-Text-to-Image-Retrieval"><a href="#Diffusion-Augmented-Retrieval-A-Training-Free-Approach-to-Interactive-Text-to-Image-Retrieval" class="headerlink" title="Diffusion Augmented Retrieval: A Training-Free Approach to Interactive   Text-to-Image Retrieval"></a>Diffusion Augmented Retrieval: A Training-Free Approach to Interactive   Text-to-Image Retrieval</h2><p><strong>Authors:Zijun Long, Kangheng Liang, Gerardo Aragon-Camarasa, Richard Mccreadie, Paul Henderson</strong></p>
<p>Interactive Text-to-image retrieval (I-TIR) is an important enabler for a wide range of state-of-the-art services in domains such as e-commerce and education. However, current methods rely on finetuned Multimodal Large Language Models (MLLMs), which are costly to train and update, and exhibit poor generalizability. This latter issue is of particular concern, as: 1) finetuning narrows the pretrained distribution of MLLMs, thereby reducing generalizability; and 2) I-TIR introduces increasing query diversity and complexity. As a result, I-TIR solutions are highly likely to encounter queries and images not well represented in any training dataset. To address this, we propose leveraging Diffusion Models (DMs) for text-to-image mapping, to avoid finetuning MLLMs while preserving robust performance on complex queries. Specifically, we introduce Diffusion Augmented Retrieval (DAR), a framework that generates multiple intermediate representations via LLM-based dialogue refinements and DMs, producing a richer depiction of the user’s information needs. This augmented representation facilitates more accurate identification of semantically and visually related images. Extensive experiments on four benchmarks show that for simple queries, DAR achieves results on par with finetuned I-TIR models, yet without incurring their tuning overhead. Moreover, as queries become more complex through additional conversational turns, DAR surpasses finetuned I-TIR models by up to 7.61% in Hits@10 after ten turns, illustrating its improved generalization for more intricate queries. </p>
<blockquote>
<p>交互式文本到图像检索（I-TIR）是电子商务和教育等领域先进服务广泛应用的重要推动者。然而，当前的方法依赖于微调的多模态大型语言模型（MLLMs），这些模型训练和更新的成本很高，并且通用性较差。后者的问题尤其值得关注，因为：1）微调会缩小MLLMs的预训练分布，从而降低其通用性；2）I-TIR引入了日益增长的查询多样性和复杂性。因此，I-TIR解决方案很可能会遇到任何训练数据集中表示不佳的查询和图像。为了解决这一问题，我们提出利用扩散模型（DMs）进行文本到图像的映射，以避免对MLLMs进行微调，同时在复杂查询上保持稳健的性能。具体来说，我们引入了扩散增强检索（DAR）框架，该框架通过基于LLM的对话精炼和DMs生成多个中间表示，从而提供更丰富的用户信息需求的描述。这种增强的表示有助于更准确地识别语义和视觉相关的图像。在四个基准测试上的大量实验表明，对于简单查询，DAR在不产生额外的微调开销的情况下，实现了与微调过的I-TIR模型相当的结果。此外，随着查询通过额外的对话回合变得更为复杂，DAR在十回合后的命中率提高了高达7.61%，这证明了其在更复杂的查询中具有更好的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15379v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Interactive Text-to-image Retrieval（I-TIR）的重要性及其在电子商务和教育等领域的应用。然而，当前的方法依赖于微调的多模态大型语言模型（MLLMs），训练成本高且通用性较差。为解决这一问题，本文提出利用Diffusion Models（DMs）进行文本到图像的映射，避免微调MLLMs，同时在复杂查询上保持稳健性能。实验表明，对于简单查询，新提出的方法与微调I-TIR模型结果相当，而对于复杂查询，则表现出更好的泛化能力和性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>I-TIR对于电子商务和教育等领域至关重要。</li>
<li>当前I-TIR方法依赖昂贵的训练成本且通用性差的微调多模态大型语言模型（MLLMs）。</li>
<li>利用Diffusion Models（DMs）进行文本到图像的映射可以避免微调MLLMs并保持对复杂查询的稳健性能。</li>
<li>新方法通过引入Diffusion Augmented Retrieval（DAR）框架生成多个中间表示，通过LLM基于对话的细化修正和DMs产生更丰富用户信息需求的描述。</li>
<li>对于简单查询，DAR与微调的I-TIR模型结果相当。</li>
<li>对于复杂查询，DAR在多次对话回合后超越调参模型，在Hits@10上提高了7.61%，显示出更好的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84bccd6cdda7958623f355ff235d0f65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a9b2b509621b36923dac1301199fa86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b343aab34d13422f2957b227f809e4e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e148749e08b3fff34d497d1d4af58a94.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Diffusion-Driven-Semantic-Communication-for-Generative-Models-with-Bandwidth-Constraints"><a href="#Diffusion-Driven-Semantic-Communication-for-Generative-Models-with-Bandwidth-Constraints" class="headerlink" title="Diffusion-Driven Semantic Communication for Generative Models with   Bandwidth Constraints"></a>Diffusion-Driven Semantic Communication for Generative Models with   Bandwidth Constraints</h2><p><strong>Authors:Lei Guo, Wei Chen, Yuxuan Sun, Bo Ai, Nikolaos Pappas, Tony Q. S. Quek</strong></p>
<p>Diffusion models have been extensively utilized in AI-generated content (AIGC) in recent years, thanks to the superior generation capabilities. Combining with semantic communications, diffusion models are used for tasks such as denoising, data reconstruction, and content generation. However, existing diffusion-based generative models do not consider the stringent bandwidth limitation, which limits its application in wireless communication. This paper introduces a diffusion-driven semantic communication framework with advanced VAE-based compression for bandwidth-constrained generative model. Our designed architecture utilizes the diffusion model, where the signal transmission process through the wireless channel acts as the forward process in diffusion. To reduce bandwidth requirements, we incorporate a downsampling module and a paired upsampling module based on a variational auto-encoder with reparameterization at the receiver to ensure that the recovered features conform to the Gaussian distribution. Furthermore, we derive the loss function for our proposed system and evaluate its performance through comprehensive experiments. Our experimental results demonstrate significant improvements in pixel-level metrics such as peak signal to noise ratio (PSNR) and semantic metrics like learned perceptual image patch similarity (LPIPS). These enhancements are more profound regarding the compression rates and SNR compared to deep joint source-channel coding (DJSCC). We release the code at <a target="_blank" rel="noopener" href="https://github.com/import-sudo/Diffusion-Driven-Semantic-Communication">https://github.com/import-sudo/Diffusion-Driven-Semantic-Communication</a>. </p>
<blockquote>
<p>扩散模型因其出色的生成能力，近年来在人工智能生成内容（AIGC）中得到了广泛应用。结合语义通信，扩散模型被用于去噪、数据重建和内容生成等任务。然而，现有的基于扩散的生成模型并未考虑严格的带宽限制，这限制了其在无线通信中的应用。本文介绍了一个扩散驱动的语义通信框架，该框架具有先进的基于VAE的压缩技术，适用于带宽受限的生成模型。我们设计的架构利用扩散模型，其中通过无线信道传输信号的过程作为扩散的前向过程。为了减少带宽要求，我们融入了下采样模块和基于变分自动编码器的配对上采样模块，并在接收器处进行重参数化，以确保恢复的特征符合高斯分布。此外，我们推导了所提出系统的损失函数，并通过综合实验评估了其性能。实验结果证明，在像素级指标（如峰值信噪比）和语义指标（如学习感知图像补丁相似性）方面，我们的系统有显著改进。与深度联合源信道编码相比，我们的系统在压缩率和信噪比方面获得了更显著的改进。我们在<a target="_blank" rel="noopener" href="https://github.com/import-sudo/Diffusion-Driven-Semantic-Communication%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/import-sudo/Diffusion-Driven-Semantic-Communication上发布了代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18468v4">PDF</a> accepted to IEEE for possible publication</p>
<p><strong>Summary</strong><br>     扩散模型结合语义通信，用于去噪、数据重建和内容生成等任务。针对现有扩散生成模型在带宽限制方面的应用瓶颈，本文提出一种基于变分自编码器压缩的扩散驱动语义通信框架。设计架构利用扩散模型，通过无线信道传输信号的过程作为扩散中的正向过程。通过引入下采样模块和配对上采样模块，减少带宽要求，并在接收器处采用重参数化确保恢复的特征符合高斯分布。实验结果在像素级和语义度量上都有显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在AI生成内容（AIGC）中广泛应用，具有出色的生成能力。</li>
<li>扩散模型与语义通信结合，用于去噪、数据重建和内容生成。</li>
<li>现有扩散生成模型在无线通信中面临带宽限制的挑战。</li>
<li>本文提出一种基于变分自编码器压缩的扩散驱动语义通信框架，以适应带宽限制。</li>
<li>设计架构利用无线信道传输信号的扩散模型，引入下采样和上采样模块以减少带宽需求。</li>
<li>通过实验，该框架在像素级度量（如峰值信噪比PSNR）和语义度量（如学习感知图像补丁相似性LPIPS）上表现出显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eb88b87733df7f31f2015be883b0d407.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97056f0b43e006e54b2c0c0cf8a8659f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f35745e2f1cbe47b1b629f5ebb90a1bb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning"><a href="#QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning" class="headerlink" title="QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning"></a>QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning</h2><p><strong>Authors:Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Junchi Yan, Yan Yan</strong></p>
<p>The practical deployment of diffusion models is still hindered by the high memory and computational overhead. Although quantization paves a way for model compression and acceleration, existing methods face challenges in achieving low-bit quantization efficiently. In this paper, we identify imbalanced activation distributions as a primary source of quantization difficulty, and propose to adjust these distributions through weight finetuning to be more quantization-friendly. We provide both theoretical and empirical evidence supporting finetuning as a practical and reliable solution. Building on this approach, we further distinguish two critical types of quantized layers: those responsible for retaining essential temporal information and those particularly sensitive to bit-width reduction. By selectively finetuning these layers under both local and global supervision, we mitigate performance degradation while enhancing quantization efficiency. Our method demonstrates its efficacy across three high-resolution image generation tasks, obtaining state-of-the-art performance across multiple bit-width settings. </p>
<blockquote>
<p>扩散模型的实际应用仍然受到高内存和计算开销的阻碍。尽管量化可以为模型压缩和加速铺平道路，但现有方法在实现低位量化时面临挑战。在本文中，我们将激活分布不平衡作为量化的主要困难来源，并提出通过权重微调调整这些分布，使其更利于量化。我们提供了理论和实证证据，支持微调作为一种实用且可靠的解决方案。在此基础上，我们进一步区分了两种关键的量化层：那些负责保留重要时间信息的层和那些对位宽减少特别敏感的层。通过局部和全局监督下有选择地微调这些层，我们在提高量化效率的同时缓解了性能下降。我们的方法在三个高分辨率图像生成任务中证明了其有效性，并在多个位宽设置下达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.03666v5">PDF</a> ICCV 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/hatchetProject/QuEST">https://github.com/hatchetProject/QuEST</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型在实际部署中遇到的内存和计算开销问题。针对量化压缩和加速的需求，本文识别出激活分布不平衡是量化困难的主要原因，并提出通过权重微调来调整这些分布，使其更利于量化。同时，本文还区分了两种关键的量化层类型，并通过局部和全局监督下的选择性微调来增强量化效率，同时在高分辨率图像生成任务中取得了先进性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在实际部署中面临高内存和计算开销的挑战。</li>
<li>量化是模型压缩和加速的一种可行方法，但现有方法在实现低位量化时面临挑战。</li>
<li>激活分布不平衡被识别为量化的主要困难。</li>
<li>通过权重微调调整激活分布，使其更利于量化。</li>
<li>区分了两种关键的量化层类型：保留重要时间信息的层和对比特宽度减少敏感的层。</li>
<li>通过局部和全局监督下的选择性微调，缓解了性能下降，提高了量化效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.03666">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee314fdf808d5701b34ee0d0d20268b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f838e90ed63bb3599ceacd153b10ac8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f155e65b9c876b0acb10a9d79b6c802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dfdc538b35f7928c0ed3460b91d77af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f93b8cfb10211e4ed36e2345eac31f6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2708e2891246883ca60f3bd7ecea99b9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-12/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c77d2f231bcd03bfb13565de40981da4.jpg" class="responsive-img" alt="牙齿修复">
                        
                        <span class="card-title">牙齿修复</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            牙齿修复 方向最新论文已更新，请持续关注 Update in 2025-07-12  PWD Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    牙齿修复
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">牙齿修复</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8daa22cddaaa53d9a573a5a13956572a.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-07-12  MUVOD A Novel Multi-view Video Object Segmentation Dataset and A   Benchmark for 3D Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
