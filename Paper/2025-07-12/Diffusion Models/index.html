<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  Single-Step Latent Diffusion for Underwater Image Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-26f2170c29889b50443a202b56a12b11.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-12-æ›´æ–°"><a href="#2025-07-12-æ›´æ–°" class="headerlink" title="2025-07-12 æ›´æ–°"></a>2025-07-12 æ›´æ–°</h1><h2 id="Single-Step-Latent-Diffusion-for-Underwater-Image-Restoration"><a href="#Single-Step-Latent-Diffusion-for-Underwater-Image-Restoration" class="headerlink" title="Single-Step Latent Diffusion for Underwater Image Restoration"></a>Single-Step Latent Diffusion for Underwater Image Restoration</h2><p><strong>Authors:Jiayi Wu, Tianfu Wang, Md Abu Bakr Siddique, Md Jahidul Islam, Cornelia Fermuller, Yiannis Aloimonos, Christopher A. Metzler</strong></p>
<p>Underwater image restoration algorithms seek to restore the color, contrast, and appearance of a scene that is imaged underwater. They are a critical tool in applications ranging from marine ecology and aquaculture to underwater construction and archaeology. While existing pixel-domain diffusion-based image restoration approaches are effective at restoring simple scenes with limited depth variation, they are computationally intensive and often generate unrealistic artifacts when applied to scenes with complex geometry and significant depth variation. In this work we overcome these limitations by combining a novel network architecture (SLURPP) with an accurate synthetic data generation pipeline. SLURPP combines pretrained latent diffusion models â€“ which encode strong priors on the geometry and depth of scenes â€“ with an explicit scene decomposition â€“ which allows one to model and account for the effects of light attenuation and backscattering. To train SLURPP we design a physics-based underwater image synthesis pipeline that applies varied and realistic underwater degradation effects to existing terrestrial image datasets. This approach enables the generation of diverse training data with dense medium&#x2F;degradation annotations. We evaluate our method extensively on both synthetic and real-world benchmarks and demonstrate state-of-the-art performance. Notably, SLURPP is over 200X faster than existing diffusion-based methods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It also offers compelling qualitative improvements on real-world data. Project website <a target="_blank" rel="noopener" href="https://tianfwang.github.io/slurpp/">https://tianfwang.github.io/slurpp/</a>. </p>
<blockquote>
<p>æ°´ä¸‹å›¾åƒæ¢å¤ç®—æ³•æ—¨åœ¨æ¢å¤æ°´ä¸‹åœºæ™¯çš„è‰²æ³½ã€å¯¹æ¯”åº¦å’Œå¤–è§‚ã€‚å®ƒä»¬åœ¨æµ·æ´‹ç”Ÿæ€ã€æ°´äº§å…»æ®–ã€æ°´ä¸‹å»ºè®¾å’Œè€ƒå¤ç­‰åº”ç”¨ä¸­éƒ½æ˜¯è‡³å…³é‡è¦çš„å·¥å…·ã€‚è™½ç„¶ç°æœ‰çš„åƒç´ åŸŸæ‰©æ•£å‹å›¾åƒæ¢å¤æ–¹æ³•åœ¨æ¢å¤æ·±åº¦å˜åŒ–æœ‰é™çš„ç®€å•åœºæ™¯æ—¶å¾ˆæœ‰æ•ˆï¼Œä½†åœ¨å¤„ç†å…·æœ‰å¤æ‚å‡ ä½•å’Œæ˜¾è‘—æ·±åº¦å˜åŒ–çš„åœºæ™¯æ—¶ï¼Œå®ƒä»¬è®¡ç®—é‡å¤§å¹¶ä¸”å¸¸å¸¸äº§ç”Ÿä¸çœŸå®çš„ä¼ªå½±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆæ–°å‹ç½‘ç»œæ¶æ„ï¼ˆSLURPPï¼‰ä¸ç²¾å‡†åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹æ¥å…‹æœè¿™äº›å±€é™ã€‚SLURPPç»“åˆäº†é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹åœºæ™¯çš„å‡ ä½•å’Œæ·±åº¦å…·æœ‰å¼ºçƒˆçš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åŠæ˜ç¡®çš„åœºæ™¯åˆ†è§£ï¼Œè¿™å…è®¸å»ºæ¨¡å’Œè€ƒè™‘å…‰è¡°å‡å’Œåå‘æ•£å°„çš„å½±å“ã€‚ä¸ºäº†è®­ç»ƒSLURPPï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºç‰©ç†çš„æ°´ä¸‹å›¾åƒåˆæˆç®¡é“ï¼Œè¯¥ç®¡é“å¯¹ç°æœ‰çš„åœ°é¢å›¾åƒæ•°æ®é›†åº”ç”¨å¤šæ ·åŒ–å’Œç°å®çš„æ°´ä¸‹é€€åŒ–æ•ˆæœã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¯†é›†ä»‹è´¨&#x2F;é€€åŒ–æ³¨é‡Šçš„å¤šæ ·åŒ–è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬åœ¨åˆæˆå’Œå®é™…åŸºå‡†æµ‹è¯•ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†å…¶æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSLURPPæ˜¯ç°æœ‰æ‰©æ•£æ–¹æ³•çš„200å€ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸ŠPSNRæé«˜äº†çº¦3åˆ†è´ã€‚å®ƒåœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šä¹Ÿæä¾›äº†å¼•äººæ³¨ç›®çš„å®šæ€§æ”¹è¿›ã€‚é¡¹ç›®ç½‘ç«™<a target="_blank" rel="noopener" href="https://tianfwang.github.io/slurpp/%E3%80%82">https://tianfwang.github.io/slurpp/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07878v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆæ–°å‹ç½‘ç»œæ¶æ„SLURPPä¸ç²¾ç¡®åˆæˆæ•°æ®ç”Ÿæˆæµç¨‹çš„æ°´ä¸‹å›¾åƒæ¢å¤æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸æ˜ç¡®çš„åœºæ™¯åˆ†è§£ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚åœºæ™¯å’Œæ˜¾è‘—æ·±åº¦å˜åŒ–çš„æ°´ä¸‹å›¾åƒã€‚é€šè¿‡ç‰©ç†åŸºç¡€çš„æ°´ä¸‹å›¾åƒåˆæˆç®¡é“è®¾è®¡è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿåœ¨é™†åœ°å›¾åƒæ•°æ®é›†ä¸Šåº”ç”¨å„ç§çœŸå®æ°´ä¸‹é€€åŒ–æ•ˆåº”ã€‚è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰æ‰©æ•£æ–¹æ³•ï¼Œé€Ÿåº¦æå‡è¶…è¿‡200å€ï¼ŒåŒæ—¶åœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šçš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜çº¦3åˆ†è´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å›¾åƒæ¢å¤ç®—æ³•æ—¨åœ¨æ¢å¤æ°´ä¸‹åœºæ™¯çš„è‰²æ³½ã€å¯¹æ¯”åº¦å’Œå¤–è§‚ï¼Œå¹¿æ³›åº”ç”¨äºæµ·æ´‹ç”Ÿæ€ã€æ°´äº§å…»æ®–ä¸šã€æ°´ä¸‹å»ºè®¾å’Œè€ƒå¤ç­‰é¢†åŸŸã€‚</li>
<li>ç°æœ‰åƒç´ åŸŸæ‰©æ•£å‹å›¾åƒæ¢å¤æ–¹æ³•åœ¨å¤„ç†ç®€å•åœºæ™¯æ—¶æ•ˆæœæ˜¾è‘—ï¼Œä½†åœ¨é¢å¯¹å¤æ‚åœºæ™¯å’Œæ˜¾è‘—æ·±åº¦å˜åŒ–æ—¶ï¼Œè®¡ç®—é‡å¤§ä¸”æ˜“äº§ç”Ÿä¸çœŸå®çš„äººå·¥ç—•è¿¹ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„SLURPPç½‘ç»œæ¶æ„ç»“åˆäº†é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œæ˜ç¡®çš„åœºæ™¯åˆ†è§£ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„æ°´ä¸‹å›¾åƒã€‚</li>
<li>è®¾è®¡äº†åŸºäºç‰©ç†çš„æ°´ä¸‹å›¾åƒåˆæˆç®¡é“ï¼Œèƒ½å¤Ÿåœ¨é™†åœ°å›¾åƒæ•°æ®é›†ä¸Šåº”ç”¨å„ç§çœŸå®æ°´ä¸‹é€€åŒ–æ•ˆåº”ï¼Œç”Ÿæˆä¸°å¯Œçš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>SLURPPæ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé€Ÿåº¦å¤§å¹…è¶…è¶Šç°æœ‰æ‰©æ•£æ–¹æ³•ï¼ŒåŒæ—¶æé«˜äº†å›¾åƒè´¨é‡ã€‚</li>
<li>SLURPPæ–¹æ³•æä¾›äº†å¼ºå¤§çš„å®šæ€§æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†çœŸå®ä¸–ç•Œæ•°æ®æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c74b8aaf7eed48957edc2fbc06ae5cde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb537cfb9da269838e9c391caa3be003.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82ff1d6a163085ec5a5be9fbd70e9c49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad79e38ac4b2877f9ab97a8b30f02e3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-427702c23a50056833be6150f5bc0161.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbe6906a401e7d5b08ca87b21a47dac7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Capture-Stage-Environments-A-Guide-to-Better-Matting"><a href="#Capture-Stage-Environments-A-Guide-to-Better-Matting" class="headerlink" title="Capture Stage Environments: A Guide to Better Matting"></a>Capture Stage Environments: A Guide to Better Matting</h2><p><strong>Authors:Hannah DrÃ¶ge, Janelle Pfeifer, Saskia Rabich, Markus Plack, Reinhard Klein, Matthias B. Hullin</strong></p>
<p>Capture stages are high-end sources of state-of-the-art recordings for downstream applications in movies, games, and other media. One crucial step in almost all pipelines is the matting of images to isolate the captured performances from the background. While common matting algorithms deliver remarkable performance in other applications like teleconferencing and mobile entertainment, we found that they struggle significantly with the peculiarities of capture stage content. The goal of our work is to share insights into those challenges as a curated list of those characteristics along with a constructive discussion for proactive intervention and present a guideline to practitioners for an improved workflow to mitigate unresolved challenges. To this end, we also demonstrate an efficient pipeline to adapt state-of-the-art approaches to such custom setups without the need of extensive annotations, both offline and real-time. For an objective evaluation, we propose a validation methodology based on a leading diffusion model that highlights the benefits of our approach. </p>
<blockquote>
<p>é‡‡é›†é˜¶æ®µæ˜¯ä¸ºç”µå½±ã€æ¸¸æˆå’Œå…¶ä»–åª’ä½“ç­‰ä¸‹æ¸¸åº”ç”¨æä¾›æœ€æ–°æŠ€æœ¯å½•éŸ³çš„é«˜ç«¯æ¥æºã€‚åœ¨å‡ ä¹æ‰€æœ‰æµç¨‹ä¸­ï¼Œä¸€ä¸ªè‡³å…³é‡è¦çš„æ­¥éª¤æ˜¯é€šè¿‡å›¾åƒæŠ åƒæŠ€æœ¯å°†æ•æ‰åˆ°çš„è¡¨æ¼”ä¸èƒŒæ™¯åˆ†ç¦»ã€‚è™½ç„¶å¸¸è§çš„æŠ åƒç®—æ³•åœ¨è§†é¢‘ä¼šè®®å’Œç§»åŠ¨å¨±ä¹ç­‰å…¶ä»–åº”ç”¨ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨åº”å¯¹é‡‡é›†é˜¶æ®µå†…å®¹çš„ç‰¹æ®Šæ€§æ—¶é‡åˆ°äº†å¾ˆå¤§å›°éš¾ã€‚æˆ‘ä»¬å·¥ä½œçš„ç›®æ ‡æ˜¯åˆ†äº«å…³äºè¿™äº›æŒ‘æˆ˜çš„çœ‹æ³•ï¼Œåˆ—å‡ºè¿™äº›ç‰¹æ€§çš„ç²¾é€‰åˆ—è¡¨ï¼Œå¹¶è¿›è¡Œå»ºè®¾æ€§è®¨è®ºä»¥é‡‡å–ç§¯æå¹²é¢„çš„æªæ–½ï¼ŒåŒæ—¶ä¸ºä»ä¸šè€…æä¾›ä¸€ä¸ªæ”¹è¿›å·¥ä½œæµç¨‹æŒ‡å—ï¼Œä»¥ç¼“è§£æœªè§£å†³çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ç®¡é“ï¼Œå¯ä»¥é€‚åº”æœ€æ–°çš„æŠ€æœ¯æ–¹æ³•æ¥é€‚åº”è¿™ç§å®šåˆ¶è®¾ç½®ï¼Œæ— éœ€å¤§é‡çš„æ³¨é‡Šæ ‡æ³¨å·¥ä½œï¼Œå¹¶ä¸”é€‚ç”¨äºç¦»çº¿å¤„ç†å’Œå®æ—¶ç¯å¢ƒã€‚ä¸ºäº†è¿›è¡Œå®¢è§‚è¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå‰æ²¿æ‰©æ•£æ¨¡å‹çš„éªŒè¯æ–¹æ³•ï¼Œçªå‡ºæˆ‘ä»¬æ–¹æ³•çš„ä¼˜åŠ¿æ‰€åœ¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07623v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ•è·é˜¶æ®µåœ¨å½±è§†ã€æ¸¸æˆç­‰åª’ä½“ä¸‹æ¸¸åº”ç”¨ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå›¾åƒæŠ å›¾æ˜¯å°†æ•è·çš„è¡¨æ¼”ä¸èƒŒæ™¯åˆ†ç¦»çš„å…³é”®æ­¥éª¤ã€‚å°½ç®¡å¸¸è§çš„æŠ å›¾ç®—æ³•åœ¨å…¶ä»–åº”ç”¨å¦‚è§†é¢‘ä¼šè®®å’Œæ‰‹æœºå¨±ä¹ä¸­è¡¨ç°å“è¶Šï¼Œä½†åœ¨æ•è·é˜¶æ®µå†…å®¹æ–¹é¢å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨åˆ†äº«è¿™äº›æŒ‘æˆ˜çš„ç‰¹æ€§ï¼Œå¹¶è®¨è®ºç§¯æåº”å¯¹æªæ–½ï¼Œä¸ºä»ä¸šè€…æä¾›æ”¹è¿›å·¥ä½œæµç¨‹çš„æŒ‡å¯¼ã€‚åŒæ—¶ï¼Œå±•ç¤ºäº†ä¸€ç§é«˜æ•ˆç®¡é“ï¼Œå¯é€‚åº”æœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œæ— éœ€å¤§é‡æ ‡æ³¨å³å¯åº”ç”¨äºæ­¤ç±»è‡ªå®šä¹‰è®¾ç½®ï¼ŒåŒ…æ‹¬ç¦»çº¿ä¸å®æ—¶ç¯å¢ƒã€‚é€šè¿‡åŸºäºé¢†å…ˆçš„æ‰©æ•£æ¨¡å‹çš„éªŒè¯æ–¹æ³•ï¼Œå®¢è§‚åœ°è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•è·é˜¶æ®µæ˜¯å½±è§†ã€æ¸¸æˆç­‰åª’ä½“ä¸‹æ¸¸åº”ç”¨ä¸­çš„é«˜ç«¯å…ˆè¿›æŠ€æœ¯å½•åˆ¶é‡è¦ç¯èŠ‚ã€‚</li>
<li>å›¾åƒæŠ å›¾æ˜¯ä»æ•è·çš„è¡¨æ¼”ä¸­åˆ†ç¦»èƒŒæ™¯çš„å…³é”®æ­¥éª¤ã€‚</li>
<li>å¸¸è§æŠ å›¾ç®—æ³•åœ¨æ•è·é˜¶æ®µå†…å®¹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡åˆ†äº«äº†å¯¹è¿™äº›æŒ‘æˆ˜çš„æ´å¯Ÿï¼Œå¹¶è®¨è®ºäº†ç§¯æåº”å¯¹æªæ–½ã€‚</li>
<li>ä¸ºä»ä¸šè€…æä¾›äº†æ”¹è¿›å·¥ä½œæµç¨‹çš„æŒ‡å¯¼ã€‚</li>
<li>å±•ç¤ºäº†ä¸€ç§é«˜æ•ˆç®¡é“ï¼Œå¯é€‚åº”æœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œé€‚ç”¨äºè‡ªå®šä¹‰è®¾ç½®ï¼Œæ— éœ€å¤§é‡æ ‡æ³¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5199fbf7ef11096a46b40592d3c36d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b89a088d99263f56c680a695e19348.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-550922c15c4e8270748a4983f0e12f15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20640e0279490e8af4fc73f71462c6f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55edd82b50f2d97cbdf74ac3d1a1c8e7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Stable-Hair-v2-Real-World-Hair-Transfer-via-Multiple-View-Diffusion-Model"><a href="#Stable-Hair-v2-Real-World-Hair-Transfer-via-Multiple-View-Diffusion-Model" class="headerlink" title="Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion   Model"></a>Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion   Model</h2><p><strong>Authors:Kuiyuan Sun, Yuxuan Zhang, Jichao Zhang, Jiaming Liu, Wei Wang, Niculae Sebe, Yao Zhao</strong></p>
<p>While diffusion-based methods have shown impressive capabilities in capturing diverse and complex hairstyles, their ability to generate consistent and high-quality multi-view outputs â€“ crucial for real-world applications such as digital humans and virtual avatars â€“ remains underexplored. In this paper, we propose Stable-Hair v2, a novel diffusion-based multi-view hair transfer framework. To the best of our knowledge, this is the first work to leverage multi-view diffusion models for robust, high-fidelity, and view-consistent hair transfer across multiple perspectives. We introduce a comprehensive multi-view training data generation pipeline comprising a diffusion-based Bald Converter, a data-augment inpainting model, and a face-finetuned multi-view diffusion model to generate high-quality triplet data, including bald images, reference hairstyles, and view-aligned source-bald pairs. Our multi-view hair transfer model integrates polar-azimuth embeddings for pose conditioning and temporal attention layers to ensure smooth transitions between views. To optimize this model, we design a novel multi-stage training strategy consisting of pose-controllable latent IdentityNet training, hair extractor training, and temporal attention training. Extensive experiments demonstrate that our method accurately transfers detailed and realistic hairstyles to source subjects while achieving seamless and consistent results across views, significantly outperforming existing methods and establishing a new benchmark in multi-view hair transfer. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/sunkymepro/StableHairV2">https://github.com/sunkymepro/StableHairV2</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨æ•æ‰å¤šæ ·ä¸”å¤æ‚çš„å‘å‹æ–¹é¢å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ç”Ÿæˆä¸€è‡´ä¸”é«˜è´¨é‡çš„å¤šè§†è§’è¾“å‡ºæ–¹é¢çš„èƒ½åŠ›â€”â€”å¯¹äºæ•°å­—äººç±»å’Œè™šæ‹ŸåŒ–èº«ç­‰ç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦â€”â€”ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Stable-Hair v2ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„å¤šè§†è§’å¤´å‘è½¬ç§»æ¡†æ¶ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹è¿›è¡Œç¨³å¥ã€é«˜ä¿çœŸã€è·¨å¤šä¸ªè§†è§’ä¸€è‡´æ€§çš„å¤´å‘è½¬ç§»çš„å·¥ä½œã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„å¤šè§†è§’è®­ç»ƒæ•°æ®ç”Ÿæˆæµç¨‹ï¼ŒåŒ…æ‹¬åŸºäºæ‰©æ•£çš„ç§ƒå¤´è½¬æ¢å™¨ã€æ•°æ®å¢å¼ºå¡«å……æ¨¡å‹ï¼Œä»¥åŠé’ˆå¯¹é¢éƒ¨å¾®è°ƒçš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰å…ƒç»„æ•°æ®ï¼ŒåŒ…æ‹¬ç§ƒå¤´å›¾åƒã€å‚è€ƒå‘å‹å’Œè§†è§’å¯¹é½çš„æºç§ƒå¤´å¯¹ã€‚æˆ‘ä»¬çš„å¤šè§†è§’å¤´å‘è½¬ç§»æ¨¡å‹é›†æˆäº†æåæ ‡æ–¹ä½åµŒå…¥è¿›è¡Œå§¿æ€æ¡ä»¶è®¾ç½®å’Œä¸´æ—¶æ³¨æ„åŠ›å±‚ï¼Œä»¥ç¡®ä¿ä¸åŒè§†è§’ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚ä¸ºäº†ä¼˜åŒ–è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å§¿æ€å¯æ§æ½œåœ¨èº«ä»½ç½‘è®­ç»ƒã€å¤´å‘æå–å™¨è®­ç»ƒå’Œä¸´æ—¶æ³¨æ„åŠ›è®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®åœ°å°†è¯¦ç»†çš„ç°å®å‘å‹è½¬ç§»åˆ°æºä¸»ä½“ä¸Šï¼ŒåŒæ—¶åœ¨å„è§†è§’å®ç°æ— ç¼ä¸”ä¸€è‡´çš„ç»“æœï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å¤šè§†è§’å¤´å‘è½¬ç§»æ–¹é¢å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚ä»£ç å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/sunkymepro/StableHairV2%E3%80%82">https://github.com/sunkymepro/StableHairV2ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07591v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šè§†è§’å¤´å‘è½¬ç§»æ¡†æ¶Stable-Hair v2ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹å®ç°äº†ç¨³å¥ã€é«˜ä¿çœŸã€è·¨ä¸åŒè§†è§’çš„è¿ç»­å¤´å‘è½¬ç§»ã€‚ä¸ºç”Ÿæˆé«˜è´¨é‡çš„ä¸‰å…ƒç»„æ•°æ®ï¼Œå¼•å…¥å…¨é¢çš„å¤šè§†è§’è®­ç»ƒæ•°æ®ç”Ÿæˆç®¡é“ï¼ŒåŒ…æ‹¬åŸºäºæ‰©æ•£çš„ç§ƒé¡¶è½¬æ¢å™¨ã€æ•°æ®å¢å¼ºå¡«å……æ¨¡å‹å’Œå¯¹è„¸è¿›è¡Œå¾®è°ƒçš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ–¹æ³•é›†æˆäº†æ–¹ä½åµŒå…¥ç”¨äºå§¿æ€è°ƒèŠ‚å’Œä¸´æ—¶æ³¨æ„åŠ›å±‚ä»¥ç¡®ä¿ä¸åŒè§†è§’ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚é€šè¿‡è®¾è®¡åŒ…å«å§¿æ€å¯æ§æ½œåœ¨IdentityNetè®­ç»ƒã€å¤´å‘æå–å™¨è®­ç»ƒå’Œä¸´æ—¶æ³¨æ„åŠ›è®­ç»ƒçš„å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†ä¼˜åŒ–æ¨¡å‹çš„ç›®çš„ã€‚è¯¥æ–¹æ³•å‡†ç¡®åœ°å°†è¯¦ç»†çš„ç°å®å‘å‹è½¬ç§»åˆ°æºä¸»ä½“ä¸Šï¼ŒåŒæ—¶å®ç°è·¨è§†è§’çš„æ— ç¼å’Œä¸€è‡´ç»“æœï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå¤šè§†è§’å¤´å‘è½¬ç§»å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†Stable-Hair v2ï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šè§†è§’å¤´å‘è½¬ç§»æ¡†æ¶ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡åˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤´å‘è½¬ç§»çš„å·¥ä½œï¼Œå¯å®ç°ç¨³å¥ã€é«˜ä¿çœŸå’Œè¿ç»­æ€§çš„å¤´å‘è½¬ç§»ã€‚</li>
<li>å¼•å…¥å…¨é¢çš„å¤šè§†è§’è®­ç»ƒæ•°æ®ç”Ÿæˆç®¡é“ï¼ŒåŒ…æ‹¬ç§ƒé¡¶è½¬æ¢å™¨ã€æ•°æ®å¢å¼ºå¡«å……æ¨¡å‹ä»¥åŠé¢å‘å¤šè§†è§’çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>é›†æˆæ–¹ä½åµŒå…¥ç”¨äºå§¿æ€è°ƒèŠ‚å’Œä¸´æ—¶æ³¨æ„åŠ›å±‚ç¡®ä¿ä¸åŒè§†è§’é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚</li>
<li>é€šè¿‡è®¾è®¡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ä¼˜åŒ–æ¨¡å‹ï¼ŒåŒ…æ‹¬å§¿æ€å¯æ§çš„IdentityNetè®­ç»ƒã€å¤´å‘æå–å™¨è®­ç»ƒå’Œä¸´æ—¶æ³¨æ„åŠ›è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°†è¯¦ç»†å’Œç°å®çš„å‘å‹è½¬ç§»åˆ°æºä¸»ä½“ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†è·¨è§†è§’çš„æ— ç¼å’Œä¸€è‡´ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3be245c4ee45527f37c4379c7d95120f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e787a68cb79a710a76a6a8f8ae5167e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2474f08c057f8ee19f39d692b6b46258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b98dbb838e6bbe6f9b327023a2e9ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-224f3aaf7f90c80ad5b7fcbce935a558.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cbf9ee6b541785971c048a6e830986e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Divergence-Minimization-Preference-Optimization-for-Diffusion-Model-Alignment"><a href="#Divergence-Minimization-Preference-Optimization-for-Diffusion-Model-Alignment" class="headerlink" title="Divergence Minimization Preference Optimization for Diffusion Model   Alignment"></a>Divergence Minimization Preference Optimization for Diffusion Model   Alignment</h2><p><strong>Authors:Binxu Li, Minkai Xu, Meihua Dang, Stefano Ermon</strong></p>
<p>Diffusion models have achieved remarkable success in generating realistic and versatile images from text prompts. Inspired by the recent advancements of language models, there is an increasing interest in further improving the models by aligning with human preferences. However, we investigate alignment from a divergence minimization perspective and reveal that existing preference optimization methods are typically trapped in suboptimal mean-seeking optimization. In this paper, we introduce Divergence Minimization Preference Optimization (DMPO), a novel and principled method for aligning diffusion models by minimizing reverse KL divergence, which asymptotically enjoys the same optimization direction as original RL. We provide rigorous analysis to justify the effectiveness of DMPO and conduct comprehensive experiments to validate its empirical strength across both human evaluations and automatic metrics. Our extensive results show that diffusion models fine-tuned with DMPO can consistently outperform or match existing techniques, specifically outperforming all existing diffusion alignment baselines by at least 64.6% in PickScore across all evaluation datasets, demonstrating the methodâ€™s superiority in aligning generative behavior with desired outputs. Overall, DMPO unlocks a robust and elegant pathway for preference alignment, bridging principled theory with practical performance in diffusion models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆç°å®ä¸”å¤šæ ·åŒ–çš„å›¾åƒï¼Œå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚å—è¯­è¨€æ¨¡å‹æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œäººä»¬è¶Šæ¥è¶Šæ„Ÿå…´è¶£é€šè¿‡ç¬¦åˆäººç±»åå¥½æ¥è¿›ä¸€æ­¥æ”¹è¿›è¿™äº›æ¨¡å‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä»åˆ†æ­§æœ€å°åŒ–è§’åº¦ç ”ç©¶å¯¹é½é—®é¢˜ï¼Œå¹¶æ­ç¤ºç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•é€šå¸¸é™·å…¥æ¬¡ä¼˜å‡å€¼å¯»æ±‚ä¼˜åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åˆ†æ­§æœ€å°åŒ–åå¥½ä¼˜åŒ–ï¼ˆDMPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æœ€å°åŒ–åå‘KLåˆ†æ­§æ¥å¯¹é½æ‰©æ•£æ¨¡å‹çš„æ–°å‹ä¸”åŸºäºåŸåˆ™çš„æ–¹æ³•ï¼Œå®ƒæ¸è¿‘åœ°äº«æœ‰ä¸åŸå§‹å¼ºåŒ–å­¦ä¹ ç›¸åŒçš„ä¼˜åŒ–æ–¹å‘ã€‚æˆ‘ä»¬æä¾›äº†ä¸¥æ ¼çš„åˆ†ææ¥è¯æ˜DMPOçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¿›è¡Œäº†å…¨é¢çš„å®éªŒæ¥éªŒè¯å…¶åœ¨äººç±»è¯„ä¼°å’Œè‡ªåŠ¨æŒ‡æ ‡æ–¹é¢çš„å®è¯å®åŠ›ã€‚æˆ‘ä»¬çš„å¹¿æ³›ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DMPOå¾®è°ƒè¿‡çš„æ‰©æ•£æ¨¡å‹å¯ä»¥æŒç»­è¶…è¶Šæˆ–åŒ¹é…ç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨PickScoreä¸Šï¼Œç›¸è¾ƒäºæ‰€æœ‰ç°æœ‰æ‰©æ•£å¯¹é½åŸºå‡†æµ‹è¯•ï¼Œè‡³å°‘é«˜å‡º64.6%ï¼Œè¿™è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å°†ç”Ÿæˆè¡Œä¸ºä¸æœŸæœ›è¾“å‡ºå¯¹é½æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒDMPOä¸ºåå¥½å¯¹é½è§£é”äº†ç¨³å¥è€Œä¼˜é›…çš„é€”å¾„ï¼Œåœ¨ç†è®ºåŸåˆ™ä¸å®é™…æ€§èƒ½ä¹‹é—´æ¶èµ·äº†æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07510v1">PDF</a> 24 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶çš„åå¥½å¯¹é½é—®é¢˜ã€‚ä½œè€…ä»åˆ†æ­§æœ€å°åŒ–è§’åº¦è¿›è¡Œç ”ç©¶ï¼Œæå‡ºäº†Divergence Minimization Preference Optimizationï¼ˆDMPOï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æœ€å°åŒ–åå‘KLåˆ†æ­§æ¥å®ç°æ‰©æ•£æ¨¡å‹çš„åå¥½å¯¹é½ã€‚ç†è®ºåˆ†æè¯æ˜äº†DMPOçš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨äººç±»è¯„ä¼°å’Œè‡ªåŠ¨æŒ‡æ ‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚ä½¿ç”¨DMPOå¾®è°ƒåçš„æ‰©æ•£æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°æ•°æ®é›†ä¸Šçš„PickScoreè‡³å°‘ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿64.6%ï¼Œå±•ç°å‡ºå…¶åœ¨å¯¹é½ç”Ÿæˆè¡Œä¸ºä¸æœŸæœ›è¾“å‡ºæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆåŠŸç”Ÿæˆä¸æ–‡æœ¬æç¤ºç›¸ç¬¦çš„çœŸå®å›¾åƒã€‚</li>
<li>éšç€è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œå¦‚ä½•è¿›ä¸€æ­¥æ”¹å–„æ‰©æ•£æ¨¡å‹ä»¥æ»¡è¶³äººç±»åå¥½æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>æœ¬æ–‡ä»åˆ†æ­§æœ€å°åŒ–çš„è§’åº¦ç ”ç©¶åå¥½å¯¹é½é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”Divergence Minimization Preference Optimizationï¼ˆDMPOï¼‰ï¼Œç”¨äºä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„åå¥½å¯¹é½ã€‚</li>
<li>DMPOæ–¹æ³•é€šè¿‡æœ€å°åŒ–åå‘KLåˆ†æ­§æ¥å®ç°æ‰©æ•£æ¨¡å‹çš„åå¥½å¯¹é½ï¼Œä¸åŸå§‹å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–æ–¹å‘ä¸€è‡´ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨DMPOæ–¹æ³•å¾®è°ƒåçš„æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯PickScoreã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eaf848c9412681195d966ca193c9e861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99051acf766b09a1ed3468854eef3a6f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Degradation-Agnostic-Statistical-Facial-Feature-Transformation-for-Blind-Face-Restoration-in-Adverse-Weather-Conditions"><a href="#Degradation-Agnostic-Statistical-Facial-Feature-Transformation-for-Blind-Face-Restoration-in-Adverse-Weather-Conditions" class="headerlink" title="Degradation-Agnostic Statistical Facial Feature Transformation for Blind   Face Restoration in Adverse Weather Conditions"></a>Degradation-Agnostic Statistical Facial Feature Transformation for Blind   Face Restoration in Adverse Weather Conditions</h2><p><strong>Authors:Chang-Hwan Son</strong></p>
<p>With the increasing deployment of intelligent CCTV systems in outdoor environments, there is a growing demand for face recognition systems optimized for challenging weather conditions. Adverse weather significantly degrades image quality, which in turn reduces recognition accuracy. Although recent face image restoration (FIR) models based on generative adversarial networks (GANs) and diffusion models have shown progress, their performance remains limited due to the lack of dedicated modules that explicitly address weather-induced degradations. This leads to distorted facial textures and structures. To address these limitations, we propose a novel GAN-based blind FIR framework that integrates two key components: local Statistical Facial Feature Transformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The local SFFT module enhances facial structure and color fidelity by aligning the local statistical distributions of low-quality (LQ) facial regions with those of high-quality (HQ) counterparts. Complementarily, the DAFE module enables robust statistical facial feature extraction under adverse weather conditions by aligning LQ and HQ encoder representations, thereby making the restoration process adaptive to severe weather-induced degradations. Experimental results demonstrate that the proposed degradation-agnostic SFFT model outperforms existing state-of-the-art FIR methods based on GAN and diffusion models, particularly in suppressing texture distortions and accurately reconstructing facial structures. Furthermore, both the SFFT and DAFE modules are empirically validated in enhancing structural fidelity and perceptual quality in face restoration under challenging weather scenarios. </p>
<blockquote>
<p>éšç€æ™ºèƒ½CCTVç³»ç»Ÿåœ¨æˆ·å¤–ç¯å¢ƒä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œå¯¹äºé€‚åº”æ¶åŠ£å¤©æ°”æ¡ä»¶çš„é¢éƒ¨è¯†åˆ«ç³»ç»Ÿçš„éœ€æ±‚ä¹Ÿåœ¨æ—¥ç›Šå¢é•¿ã€‚æ¶åŠ£å¤©æ°”ä¼šä¸¥é‡é™ä½å›¾åƒè´¨é‡ï¼Œè¿›è€Œå¯¼è‡´è¯†åˆ«ç²¾åº¦ä¸‹é™ã€‚å°½ç®¡åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨å›¾åƒæ¢å¤ï¼ˆFIRï¼‰æ¨¡å‹å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“é—¨è§£å†³å¤©æ°”å¼•èµ·çš„é€€åŒ–çš„æ¨¡å—ï¼Œå…¶æ€§èƒ½ä»ç„¶æœ‰é™ã€‚è¿™ä¼šå¯¼è‡´é¢éƒ¨çº¹ç†å’Œç»“æ„å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºGANçš„ç›²FIRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå±€éƒ¨ç»Ÿè®¡é¢éƒ¨ç‰¹å¾å˜æ¢ï¼ˆSFFTï¼‰å’Œé€€åŒ–æ— å…³ç‰¹å¾åµŒå…¥ï¼ˆDAFEï¼‰ã€‚å±€éƒ¨SFFTæ¨¡å—é€šè¿‡å¯¹é½ä½è´¨é‡ï¼ˆLQï¼‰é¢éƒ¨åŒºåŸŸçš„å±€éƒ¨ç»Ÿè®¡åˆ†å¸ƒä¸é«˜è´¨é‡ï¼ˆHQï¼‰å¯¹åº”åŒºåŸŸçš„ç»Ÿè®¡åˆ†å¸ƒï¼Œå¢å¼ºé¢éƒ¨ç»“æ„å’Œé¢œè‰²ä¿çœŸåº¦ã€‚äº’è¡¥åœ°ï¼ŒDAFEæ¨¡å—é€šè¿‡å¯¹é½LQå’ŒHQç¼–ç å™¨è¡¨ç¤ºï¼Œå®ç°åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„ç¨³å¥ç»Ÿè®¡é¢éƒ¨ç‰¹å¾æå–ï¼Œä»è€Œä½¿æ¢å¤è¿‡ç¨‹é€‚åº”äºç”±æ¶åŠ£å¤©æ°”å¼•èµ·çš„é€€åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é€€åŒ–æ— å…³SFFTæ¨¡å‹åœ¨æŠ‘åˆ¶çº¹ç†å¤±çœŸå’Œå‡†ç¡®é‡å»ºé¢éƒ¨ç»“æ„æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„å…ˆè¿›FIRæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSFFTå’ŒDAFEæ¨¡å—åœ¨å¢å¼ºç»“æ„ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¹Ÿå¾—åˆ°äº†å®è¯éªŒè¯ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤©æ°”æƒ…å†µä¸‹è¿›è¡Œé¢éƒ¨æ¢å¤æ—¶æ•ˆæœæ˜¾è‘—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07464v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹æˆ·å¤–ç¯å¢ƒä¸­æ™ºèƒ½ç›‘æ§ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹é¢éƒ¨è¯†åˆ«ç³»ç»Ÿåœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„ä¼˜åŒ–éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚æ¶åŠ£å¤©æ°”ä¼šæ˜¾è‘—é™å›¾åƒè´¨é‡ï¼Œè¿›è€Œå½±å“è¯†åˆ«å‡†ç¡®ç‡ã€‚å°½ç®¡åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨å›¾åƒæ¢å¤ï¼ˆFIRï¼‰æ¨¡å‹å·²å–å¾—è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“é—¨åº”å¯¹å¤©æ°”å¼•èµ·çš„é™è´¨çš„æ¨¡å—ï¼Œå…¶æ€§èƒ½ä»ç„¶æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§æ–°é¢–çš„åŸºäºGANçš„ç›²FIRæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå±€éƒ¨ç»Ÿè®¡é¢éƒ¨ç‰¹å¾å˜æ¢ï¼ˆSFFTï¼‰å’Œé™è´¨æ— å…³ç‰¹å¾åµŒå…¥ï¼ˆDAFEï¼‰ã€‚SFFTæ¨¡å—é€šè¿‡å¯¹é½ä½è´¨é‡ï¼ˆLQï¼‰é¢éƒ¨åŒºåŸŸä¸é«˜è´¨é‡ï¼ˆHQï¼‰åŒºåŸŸçš„å±€éƒ¨ç»Ÿè®¡åˆ†å¸ƒï¼Œå¢å¼ºé¢éƒ¨ç»“æ„å’Œè‰²å½©ä¿çœŸåº¦ã€‚åŒæ—¶ï¼ŒDAFEæ¨¡å—é€šè¿‡å¯¹é½LQå’ŒHQç¼–ç å™¨è¡¨ç¤ºï¼Œä½¿é¢éƒ¨ç‰¹å¾æå–åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹æ›´ä¸ºç¨³å¥ï¼Œä»è€Œä½¿æ¢å¤è¿‡ç¨‹é€‚åº”ä¸¥é‡çš„å¤©æ°”å¼•èµ·çš„é™è´¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é™è´¨æ— å…³SFFTæ¨¡å‹åœ¨åŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„ç°æœ‰å…ˆè¿›FIRæ–¹æ³•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æŠ‘åˆ¶çº¹ç†å¤±çœŸå’Œå‡†ç¡®é‡å»ºé¢éƒ¨ç»“æ„æ–¹é¢ã€‚æ­¤å¤–ï¼ŒSFFTå’ŒDAFEæ¨¡å—åœ¨æé«˜ç»“æ„ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¹Ÿå¾—åˆ°äº†å®è¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¶åŠ£å¤©æ°”å¯¹é¢éƒ¨è¯†åˆ«ç³»ç»Ÿé€ æˆæŒ‘æˆ˜ï¼Œé™ä½å›¾åƒè´¨é‡å’Œè¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
<li>åŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„ç°æœ‰é¢éƒ¨å›¾åƒæ¢å¤ï¼ˆFIRï¼‰æ¨¡å‹è™½æœ‰æ‰€è¿›å±•ï¼Œä½†ä»å­˜åœ¨æ€§èƒ½é™åˆ¶ã€‚</li>
<li>æå‡ºçš„ç›²FIRæ¡†æ¶ç»“åˆå±€éƒ¨ç»Ÿè®¡é¢éƒ¨ç‰¹å¾å˜æ¢ï¼ˆSFFTï¼‰å’Œé™è´¨æ— å…³ç‰¹å¾åµŒå…¥ï¼ˆDAFEï¼‰ä¸¤å¤§æ¨¡å—ä»¥åº”å¯¹å¤©æ°”å¼•èµ·çš„å›¾åƒé™è´¨ã€‚</li>
<li>SFFTæ¨¡å—é€šè¿‡å±€éƒ¨ç»Ÿè®¡åˆ†å¸ƒå¯¹é½å¢å¼ºé¢éƒ¨ç»“æ„å’Œè‰²å½©ä¿çœŸåº¦ã€‚</li>
<li>DAFEæ¨¡å—ä½¿é¢éƒ¨ç‰¹å¾æå–åœ¨æ¶åŠ£å¤©æ°”ä¸‹æ›´ä¸ºç¨³å¥ï¼Œé€‚åº”ä¸¥é‡å¤©æ°”å¼•èµ·çš„é™è´¨ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºæ¨¡å‹åœ¨æŠ‘åˆ¶çº¹ç†å¤±çœŸå’Œé¢éƒ¨ç»“æ„é‡å»ºæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>SFFTå’ŒDAFEæ¨¡å—åœ¨æé«˜ç»“æ„ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢å¾—åˆ°éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19bf915952f0961f0bd4826afaadac3e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EscherNet-Simultaneous-Amodal-Completion-and-Scalable-View-Synthesis-through-Masked-Fine-Tuning-and-Enhanced-Feed-Forward-3D-Reconstruction"><a href="#EscherNet-Simultaneous-Amodal-Completion-and-Scalable-View-Synthesis-through-Masked-Fine-Tuning-and-Enhanced-Feed-Forward-3D-Reconstruction" class="headerlink" title="EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis   through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction"></a>EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis   through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction</h2><p><strong>Authors:Xinan Zhang, Muhammad Zubair Irshad, Anthony Yezzi, Yi-Chang Tsai, Zsolt Kira</strong></p>
<p>We propose EscherNet++, a masked fine-tuned diffusion model that can synthesize novel views of objects in a zero-shot manner with amodal completion ability. Existing approaches utilize multiple stages and complex pipelines to first hallucinate missing parts of the image and then perform novel view synthesis, which fail to consider cross-view dependencies and require redundant storage and computing for separate stages. Instead, we apply masked fine-tuning including input-level and feature-level masking to enable an end-to-end model with the improved ability to synthesize novel views and conduct amodal completion. In addition, we empirically integrate our model with other feed-forward image-to-mesh models without extra training and achieve competitive results with reconstruction time decreased by 95%, thanks to its ability to synthesize arbitrary query views. Our methodâ€™s scalable nature further enhances fast 3D reconstruction. Despite fine-tuning on a smaller dataset and batch size, our method achieves state-of-the-art results, improving PSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings, while also generalizing to real-world occluded reconstruction. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†EscherNet++ï¼Œè¿™æ˜¯ä¸€ç§ç»è¿‡æ©ç å¾®è°ƒï¼ˆfine-tuningï¼‰çš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿä»¥é›¶æ ·æœ¬æ–¹å¼åˆæˆå¯¹è±¡çš„æ–°è§†è§’ï¼Œå¹¶å…·æœ‰æ¨¡æ€å®Œæˆèƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é‡‡ç”¨å¤šé˜¶æ®µå’Œå¤æ‚çš„æµæ°´çº¿ï¼Œé¦–å…ˆè™šæ„å›¾åƒç¼ºå¤±çš„éƒ¨åˆ†ï¼Œç„¶åè¿›è¡Œæ–°è§†è§’åˆæˆï¼Œè¿™ç§æ–¹æ³•å¿½ç•¥äº†è·¨è§†è§’ä¾èµ–æ€§ï¼Œéœ€è¦é’ˆå¯¹å„ä¸ªé˜¶æ®µè¿›è¡Œå†—ä½™å­˜å‚¨å’Œè®¡ç®—ã€‚ç›¸åï¼Œæˆ‘ä»¬åº”ç”¨åŒ…æ‹¬è¾“å…¥çº§åˆ«å’Œç‰¹å¾çº§åˆ«çš„æ©ç è¿›è¡Œæ©ç å¾®è°ƒï¼Œä½¿ç«¯åˆ°ç«¯æ¨¡å‹èƒ½å¤Ÿåˆæˆæ–°è§†è§’å¹¶å…·å¤‡æ¨¡æ€å®Œæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯å°†æˆ‘ä»¬çš„æ¨¡å‹ä¸å…¶ä»–å‰é¦ˆå›¾åƒåˆ°ç½‘æ ¼æ¨¡å‹ç›¸ç»“åˆï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œç”±äºæ¨¡å‹èƒ½å¤Ÿåˆæˆä»»æ„æŸ¥è¯¢è§†è§’ï¼Œé‡å»ºæ—¶é—´å‡å°‘äº†95%ã€‚æˆ‘ä»¬æ–¹æ³•çš„å¯æ‰©å±•æ€§è¿›ä¸€æ­¥åŠ å¼ºäº†å¿«é€Ÿä¸‰ç»´é‡å»ºã€‚å°½ç®¡åœ¨è¾ƒå°çš„æ•°æ®é›†å’Œæ‰¹æ¬¡å¤§å°ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œåœ¨é®æŒ¡ä»»åŠ¡çš„PSNRæŒ‡æ ‡ä¸Šæé«˜äº†3.9ï¼Œä½“ç§¯IoUæé«˜äº†0.28çš„10è¾“å…¥è®¾ç½®ä¸Šï¼ŒåŒæ—¶æ¨å¹¿åˆ°çœŸå®ä¸–ç•Œçš„é®æŒ¡é‡å»ºä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07410v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºEscherNet++çš„æ‰©æ•£æ¨¡å‹æ”¹è¿›æ–¹æ¡ˆï¼Œé€šè¿‡é‡‡ç”¨æ©ç å¾®è°ƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿé›¶æ ·æœ¬æ–¹å¼åˆæˆå¯¹è±¡çš„æ–°è§†è§’å¹¶å…·æœ‰æ— æ¨¡æ€å®Œæˆèƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é‡‡ç”¨å¤šé˜¶æ®µå’Œå¤æ‚çš„æµæ°´çº¿æ¥å…ˆæ¨¡æ‹Ÿå›¾åƒç¼ºå¤±éƒ¨åˆ†ï¼Œç„¶åè¿›è¡Œæ–°è§†è§’åˆæˆï¼Œè¿™ç§æ–¹æ³•å¿½ç•¥äº†è·¨è§†è§’çš„ä¾èµ–å…³ç³»ï¼Œå¹¶éœ€è¦å†—ä½™çš„å­˜å‚¨å’Œè®¡ç®—èµ„æºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒEscherNet++é€šè¿‡è¾“å…¥çº§å’Œç‰¹å¾çº§çš„æ©ç å¾®è°ƒï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œæé«˜äº†åˆæˆæ–°è§†è§’å’Œæ— æ¨¡æ€å®Œæˆçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å¯ä»¥ä¸å…¶ä»–å‰é¦ˆå›¾åƒåˆ°ç½‘æ ¼æ¨¡å‹è¿›è¡Œå®è¯é›†æˆï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œç”±äºèƒ½å¤Ÿåˆæˆä»»æ„æŸ¥è¯¢è§†è§’ï¼Œé‡å»ºæ—¶é—´å‡å°‘äº†95%ã€‚è¯¥æ–¹æ³•çš„å¯æ‰©å±•æ€§è¿›ä¸€æ­¥æé«˜äº†å¿«é€Ÿä¸‰ç»´é‡å»ºã€‚å³ä½¿åœ¨è¾ƒå°çš„æ•°æ®é›†å’Œæ‰¹é‡å¤§å°ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¯¥æ–¹æ³•ä¹Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œåœ¨é®æŒ¡ä»»åŠ¡çš„PSNRæŒ‡æ ‡ä¸Šæé«˜äº†3.9ï¼Œä½“ç§¯IoUæé«˜äº†0.28çš„10è¾“å…¥è®¾ç½®ä¸­ï¼Œè€Œä¸”è¿˜èƒ½å¤Ÿæ¨å¹¿åˆ°çœŸå®ä¸–ç•Œçš„é®æŒ¡é‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EscherNet++æ˜¯ä¸€ä¸ªé€šè¿‡æ©ç å¾®è°ƒä¼˜åŒ–çš„æ‰©æ•£æ¨¡å‹ï¼Œå¯é›¶æ ·æœ¬æ–¹å¼åˆæˆå¯¹è±¡çš„æ–°è§†è§’ã€‚</li>
<li>ä¸å¤šé˜¶æ®µå’Œå¤æ‚æµæ°´çº¿æ–¹æ³•ç›¸æ¯”ï¼ŒEscherNet++å®ç°äº†ç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œæé«˜äº†æ–°è§†è§’åˆæˆå’Œæ— æ¨¡æ€å®Œæˆèƒ½åŠ›ã€‚</li>
<li>EscherNet++é€šè¿‡è¾“å…¥çº§å’Œç‰¹å¾çº§æ©ç å¾®è°ƒæŠ€æœ¯æ¥æå‡æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹å¯ä¸å…¶ä»–å‰é¦ˆå›¾åƒåˆ°ç½‘æ ¼æ¨¡å‹é›†æˆï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>EscherNet++èƒ½å¤Ÿåˆæˆä»»æ„æŸ¥è¯¢è§†è§’ï¼Œå¤§å¤§é™ä½äº†ä¸‰ç»´é‡å»ºçš„æ—¶é—´ã€‚</li>
<li>EscherNet++å…·æœ‰å¯æ‰©å±•æ€§ï¼Œåœ¨é®æŒ¡ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨PSNRå’Œä½“ç§¯IoUæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ°´å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71d60be4e0a74705b9da6c311af876c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6155b3d8fb9b568f4eab52e82ca651d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f68d5f54bd3c1c851e089d4bce091540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcb2b7254d68fb34ea07f10a014f61e2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Interpretable-EEG-to-Image-Generation-with-Semantic-Prompts"><a href="#Interpretable-EEG-to-Image-Generation-with-Semantic-Prompts" class="headerlink" title="Interpretable EEG-to-Image Generation with Semantic Prompts"></a>Interpretable EEG-to-Image Generation with Semantic Prompts</h2><p><strong>Authors:Arshak Rezvani, Ali Akbari, Kosar Sanjar Arani, Maryam Mirian, Emad Arasteh, Martin J. McKeown</strong></p>
<p>Decoding visual experience from brain signals offers exciting possibilities for neuroscience and interpretable AI. While EEG is accessible and temporally precise, its limitations in spatial detail hinder image reconstruction. Our model bypasses direct EEG-to-image generation by aligning EEG signals with multilevel semantic captions â€“ ranging from object-level to abstract themes â€“ generated by a large language model. A transformer-based EEG encoder maps brain activity to these captions through contrastive learning. During inference, caption embeddings retrieved via projection heads condition a pretrained latent diffusion model for image generation. This text-mediated framework yields state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable alignment to known neurocognitive pathways. Dominant EEG-caption associations reflected the importance of different semantic levels extracted from perceived images. Saliency maps and t-SNE projections reveal semantic topography across the scalp. Our model demonstrates how structured semantic mediation enables cognitively aligned visual decoding from EEG. </p>
<blockquote>
<p>ä»è„‘ä¿¡å·è§£ç è§†è§‰ç»éªŒä¸ºç¥ç»ç§‘å­¦å’Œå¯è§£é‡Šçš„AIæä¾›äº†ä»¤äººå…´å¥‹çš„å¯èƒ½æ€§ã€‚è„‘ç”µå›¾ï¼ˆEEGï¼‰æ˜“äºè·å–ä¸”æ—¶é—´ç²¾ç¡®ï¼Œä½†å…¶ç©ºé—´ç»†èŠ‚çš„å±€é™æ€§é˜»ç¢äº†å›¾åƒé‡å»ºã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡ä½¿è„‘ç”µå›¾ä¿¡å·ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å¤šå±‚æ¬¡è¯­ä¹‰æ ‡é¢˜ï¼ˆä»å¯¹è±¡çº§åˆ«åˆ°æŠ½è±¡ä¸»é¢˜ï¼‰å¯¹é½ï¼Œä»è€Œç»•è¿‡äº†ç›´æ¥çš„è„‘ç”µå›¾åˆ°å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚åŸºäºå˜å‹å™¨çš„è„‘ç”µå›¾ç¼–ç å™¨é€šè¿‡å¯¹æ¯”å­¦ä¹ å°†è¿™äº›æ ‡é¢˜æ˜ å°„åˆ°å¤§è„‘æ´»åŠ¨ä¸Šã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æŠ•å½±å¤´æ£€ç´¢çš„æ ‡é¢˜åµŒå…¥ä¸ºå›¾åƒç”Ÿæˆæä¾›äº†é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒæ¡ä»¶ã€‚è¿™ç§æ–‡æœ¬ä»‹å¯¼çš„æ¡†æ¶åœ¨EEGCVPRæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§†è§‰è§£ç æ•ˆæœï¼Œå¹¶ä¸å·²çŸ¥çš„ç¥ç»è®¤çŸ¥é€”å¾„å…·æœ‰å¯è§£é‡Šçš„å¯¹é½æ€§ã€‚ä¸»å¯¼çš„è„‘ç”µå›¾æ ‡é¢˜å…³è”åæ˜ äº†ä»æ„ŸçŸ¥å›¾åƒä¸­æå–çš„ä¸åŒè¯­ä¹‰çº§åˆ«çš„é‡è¦æ€§ã€‚æ˜¾è‘—å›¾ï¼ˆsaliency mapsï¼‰å’Œt-SNEæŠ•å½±æ­ç¤ºäº†å¤´çš®ä¸Šçš„è¯­ä¹‰åœ°å½¢ã€‚æˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†ç»“æ„åŒ–è¯­ä¹‰ä¸­ä»‹å¦‚ä½•ä½¿è„‘ç”µå›¾çš„è®¤çŸ¥å¯¹é½è§†è§‰è§£ç æˆä¸ºå¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07157v1">PDF</a> Actionable Interpretability Workshop (non-archival) at the 42   International Conference on Machine Learning</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè„‘ç”µä¿¡å·è§£ç è§†è§‰ä½“éªŒå¯¹äºç¥ç»ç§‘å­¦å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è¯­ä¹‰å­—å¹•å°†EEGä¿¡å·ä¸å›¾åƒç”Ÿæˆç›¸è”ç³»ï¼Œé¿å…äº†ç›´æ¥ç”ŸæˆEEGå›¾åƒçš„æŠ€æœ¯éš¾é¢˜ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒåŸºäºå˜å‹å™¨çš„EEGç¼–ç å™¨ï¼Œå°†è„‘æ´»åŠ¨ä¸è¯­ä¹‰å­—å¹•ç›¸å¯¹åº”ï¼Œè¯­ä¹‰å­—å¹•æ¶µç›–ä»å¯¹è±¡çº§åˆ«åˆ°æŠ½è±¡ä¸»é¢˜çš„ä¸åŒå±‚æ¬¡ã€‚é€šè¿‡æŠ•å½±å¤´æ£€ç´¢å­—å¹•åµŒå…¥ï¼Œè¿›è€Œåœ¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆå›¾åƒã€‚è¯¥æ–‡æœ¬ä»‹å¯¼æ¡†æ¶åœ¨EEGCVPRæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§†è§‰è§£ç æ•ˆæœï¼Œä¸å·²çŸ¥ç¥ç»è®¤çŸ¥é€”å¾„çš„å¯¹é½è§£é‡Šæ€§è¾ƒå¼ºã€‚EEGä¸å­—å¹•çš„ä¸»è¦å…³è”åæ˜ äº†ä»æ„ŸçŸ¥å›¾åƒä¸­æå–çš„ä¸åŒè¯­ä¹‰å±‚æ¬¡çš„é‡è¦æ€§ã€‚æ˜¾è‘—æ€§å›¾å’Œt-SNEæŠ•å½±æ­ç¤ºäº†å¤´çš®è¡¨é¢çš„è¯­ä¹‰åœ°å½¢ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†ç»“æ„åŒ–è¯­ä¹‰è°ƒè§£å¦‚ä½•å®ç°ä¸è®¤çŸ¥å¯¹é½çš„EEGè§†è§‰è§£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EEGåœ¨è§†è§‰è§£ç æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯ä»¥é€šè¿‡è¯­ä¹‰å­—å¹•ä¸å›¾åƒç”Ÿæˆç›¸è”ç³»ã€‚</li>
<li>ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒåŸºäºå˜å‹å™¨çš„EEGç¼–ç å™¨ï¼Œå°†è„‘æ´»åŠ¨ä¸ä¸åŒå±‚æ¬¡çš„è¯­ä¹‰å­—å¹•ç›¸å¯¹åº”ã€‚</li>
<li>é€šè¿‡æŠ•å½±å¤´æ£€ç´¢å­—å¹•åµŒå…¥ï¼Œåœ¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆå›¾åƒã€‚</li>
<li>æ–‡æœ¬ä»‹å¯¼æ¡†æ¶åœ¨EEGCVPRæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„è§†è§‰è§£ç æ•ˆæœã€‚</li>
<li>EEGä¸å­—å¹•çš„ä¸»è¦å…³è”åæ˜ äº†ä»æ„ŸçŸ¥å›¾åƒä¸­æå–çš„ä¸åŒè¯­ä¹‰å±‚æ¬¡çš„é‡è¦æ€§ã€‚</li>
<li>æ˜¾è‘—æ€§å›¾å’Œt-SNEæŠ•å½±æ­ç¤ºäº†è¯­ä¹‰åœ¨å¤´çš®è¡¨é¢çš„åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49d6141374562c9add2c28504fbb9994.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5f1f4e4c5d24b83a56dd075dedc023a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3caa5e14cad86a232b4a07716f04b51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00593c5583479a49ce771ddc58bfe6c6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PWD-Prior-Guided-and-Wavelet-Enhanced-Diffusion-Model-for-Limited-Angle-CT"><a href="#PWD-Prior-Guided-and-Wavelet-Enhanced-Diffusion-Model-for-Limited-Angle-CT" class="headerlink" title="PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT"></a>PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT</h2><p><strong>Authors:Yi Liu, Yiyang Wen, Zekun Zhou, Junqi Ma, Linghang Wang, Yucheng Yao, Liu Shi, Qiegen Liu</strong></p>
<p>Generative diffusion models have received increasing attention in medical imaging, particularly in limited-angle computed tomography (LACT). Standard diffusion models achieve high-quality image reconstruction but require a large number of sampling steps during inference, resulting in substantial computational overhead. Although skip-sampling strategies have been proposed to improve efficiency, they often lead to loss of fine structural details. To address this issue, we propose a prior information embedding and wavelet feature fusion fast sampling diffusion model for LACT reconstruction. The PWD enables efficient sampling while preserving reconstruction fidelity in LACT, and effectively mitigates the degradation typically introduced by skip-sampling. Specifically, during the training phase, PWD maps the distribution of LACT images to that of fully sampled target images, enabling the model to learn structural correspondences between them. During inference, the LACT image serves as an explicit prior to guide the sampling trajectory, allowing for high-quality reconstruction with significantly fewer steps. In addition, PWD performs multi-scale feature fusion in the wavelet domain, effectively enhancing the reconstruction of fine details by leveraging both low-frequency and high-frequency information. Quantitative and qualitative evaluations on clinical dental arch CBCT and periapical datasets demonstrate that PWD outperforms existing methods under the same sampling condition. Using only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and 10% gain in SSIM. </p>
<blockquote>
<p>ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™è§’åº¦è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLACTï¼‰ä¸­ã€‚æ ‡å‡†æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿå®ç°é«˜è´¨é‡å›¾åƒé‡å»ºï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦å¤§é‡é‡‡æ ·æ­¥éª¤ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§ã€‚è™½ç„¶æå‡ºäº†è·³è¿‡é‡‡æ ·ç­–ç•¥æ¥æé«˜æ•ˆç‡ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå¯¼è‡´ç²¾ç»†ç»“æ„ç»†èŠ‚çš„ä¸¢å¤±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå…ˆéªŒä¿¡æ¯åµŒå…¥å’Œå°æ³¢ç‰¹å¾èåˆçš„å¿«é€Ÿé‡‡æ ·æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºLACTé‡å»ºã€‚PWDï¼ˆPrior Wavelet Diffusionï¼‰èƒ½å¤Ÿåœ¨ä¿æŒLACTé‡å»ºä¿çœŸåº¦çš„åŒæ—¶å®ç°é«˜æ•ˆé‡‡æ ·ï¼Œå¹¶æœ‰æ•ˆç¼“è§£è·³è¿‡é‡‡æ ·é€šå¸¸å¼•èµ·çš„é€€åŒ–ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨è®­ç»ƒé˜¶æ®µï¼ŒPWDå°†LACTå›¾åƒçš„åˆ†å¸ƒæ˜ å°„åˆ°å®Œå…¨é‡‡æ ·ç›®æ ‡å›¾åƒçš„åˆ†å¸ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä¸¤è€…ä¹‹é—´çš„ç»“æ„å¯¹åº”å…³ç³»ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒLACTå›¾åƒä½œä¸ºæ˜ç¡®çš„å…ˆéªŒæ¥å¼•å¯¼é‡‡æ ·è½¨è¿¹ï¼Œå…è®¸ä»¥è¾ƒå°‘çš„æ­¥éª¤å®ç°é«˜è´¨é‡é‡å»ºã€‚æ­¤å¤–ï¼ŒPWDåœ¨å°æ³¢åŸŸæ‰§è¡Œå¤šå°ºåº¦ç‰¹å¾èåˆï¼Œé€šè¿‡åˆ©ç”¨ä½é¢‘å’Œé«˜é¢‘ä¿¡æ¯æœ‰æ•ˆåœ°å¢å¼ºäº†ç²¾ç»†ç»†èŠ‚çš„é‡å»ºã€‚åœ¨ä¸´åºŠç‰™ç§‘å…¨æ™¯CBCTå’Œæ ¹å°–å‘¨æ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„é‡‡æ ·æ¡ä»¶ä¸‹ï¼ŒPWDä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»…ä½¿ç”¨50ä¸ªé‡‡æ ·æ­¥éª¤ï¼ŒPWDåœ¨PSNRä¸Šå®ç°äº†è‡³å°‘1.7 dBçš„æå‡ï¼Œåœ¨SSIMä¸Šè·å¾—äº†10%çš„å¢ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05317v2">PDF</a> </p>
<p><strong>Summary</strong><br>    é’ˆå¯¹æœ‰é™è§’åº¦è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLACTï¼‰çš„é‡æ„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå…ˆéªŒä¿¡æ¯åµŒå…¥å’Œå°æ³¢ç‰¹å¾èåˆçš„å¿«é€Ÿé‡‡æ ·æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µå°†LACTå›¾åƒåˆ†å¸ƒæ˜ å°„åˆ°å®Œå…¨é‡‡æ ·çš„ç›®æ ‡å›¾åƒåˆ†å¸ƒï¼Œå­¦ä¹ ä¸¤è€…ä¹‹é—´çš„ç»“æ„å¯¹åº”å…³ç³»ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒLACTå›¾åƒä½œä¸ºæ˜¾å¼å…ˆéªŒå¼•å¯¼é‡‡æ ·è½¨è¿¹ï¼Œå®ç°é«˜è´¨é‡é‡æ„å¹¶å¤§å¤§å‡å°‘é‡‡æ ·æ­¥éª¤ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å°æ³¢åŸŸå¤šå°ºåº¦ç‰¹å¾èåˆï¼Œæœ‰æ•ˆå¢å¼ºç²¾ç»†ç»†èŠ‚çš„é‡æ„ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç›¸åŒé‡‡æ ·æ¡ä»¶ä¸‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œ50æ­¥é‡‡æ ·å³å¯å®ç°è‡³å°‘1.7 dBçš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æå‡å’Œ10%çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰å¢é•¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™è§’åº¦è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLACTï¼‰ä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>æ ‡å‡†æ‰©æ•£æ¨¡å‹éœ€è¦å¤§é‡é‡‡æ ·æ­¥éª¤ï¼Œè®¡ç®—å¼€é”€å¤§ï¼Œè€Œè·³è¿‡é‡‡æ ·ç­–ç•¥å¯èƒ½å¯¼è‡´ç»†èŠ‚æŸå¤±ã€‚</li>
<li>æå‡ºçš„æ¨¡å‹åˆ©ç”¨å…ˆéªŒä¿¡æ¯åµŒå…¥å’Œå°æ³¢ç‰¹å¾èåˆè¿›è¡Œå¿«é€Ÿé‡‡æ ·æ‰©æ•£ã€‚</li>
<li>åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ LACTå›¾åƒä¸å®Œå…¨é‡‡æ ·ç›®æ ‡å›¾åƒä¹‹é—´çš„ç»“æ„å¯¹åº”å…³ç³»ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µï¼ŒLACTå›¾åƒä½œä¸ºæ˜¾å¼å…ˆéªŒå¼•å¯¼é‡‡æ ·è½¨è¿¹ï¼Œå®ç°é«˜æ•ˆé«˜è´¨é‡é‡æ„ã€‚</li>
<li>æ¨¡å‹åˆ©ç”¨å°æ³¢åŸŸå¤šå°ºåº¦ç‰¹å¾èåˆï¼Œå¢å¼ºç»†èŠ‚é‡æ„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05317">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab3e69ab774fe780bcd12a12de07891e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c77d2f231bcd03bfb13565de40981da4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab472ee626d855e79e9d3b582d27684b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ffe0bf2d6eeeb6d75f26e36f95c53a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f7169be35bc1757d66024e170114c28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1e3343762ed90af0c65f348b8b99008.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation"><a href="#Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation" class="headerlink" title="Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation"></a>Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation</h2><p><strong>Authors:Fangyijie Wang, Kevin Whelan, FÃ©lix Balado, Kathleen M. Curran, GuÃ©nolÃ© Silvestre</strong></p>
<p>Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66% and 94.38% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ•°æ®ç”±äºéšç§å’Œç›‘ç®¡é™åˆ¶ï¼Œå…¶å¯è®¿é—®æ€§æ¯”å…¶ä»–é¢†åŸŸçš„æ•°æ®è¦ä½ã€‚æ­¤å¤–ï¼Œæ ‡æ³¨éœ€è¦ä¸´åºŠä¸“å®¶è¿›è¡Œæ˜‚è´µä¸”è€—æ—¶çš„æ‰‹åŠ¨å›¾åƒæ ‡æ³¨ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼ŒåˆæˆåŒ»å­¦æ•°æ®ç”Ÿæˆæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚é‡‡ç”¨ç”Ÿæˆå¼æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰åœ¨ç”Ÿæˆé€¼çœŸçš„åˆæˆå›¾åƒæ–¹é¢å·²è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ©è†œå¼•å¯¼GenAIæ–¹æ³•ï¼Œç”Ÿæˆåˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒåŠå…¶åˆ†å‰²æ©è†œé…å¯¹ã€‚è¿™äº›åˆæˆé…å¯¹æ•°æ®å¢å¼ºäº†çœŸå®æ•°æ®é›†ï¼Œç”¨äºç›‘ç£å¾®è°ƒSegment Anything Modelï¼ˆSAMï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œåˆæˆæ•°æ®æœ‰æ•ˆåœ°æ•æ‰äº†çœŸå®å›¾åƒçš„ç‰¹å¾ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨æœ‰é™æ•°é‡çš„çœŸå®å›¾åƒ-æ©è†œé…å¯¹è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœã€‚å°¤å…¶æ˜¯ï¼Œä½¿ç”¨æ¥è‡ªè¥¿ç­ç‰™å’Œéæ´²é˜Ÿåˆ—çš„å°‘é‡è¶…å£°å›¾åƒï¼Œåˆ†å‰²è¾¾åˆ°è¿ªå…‹åˆ†æ•°ï¼ˆDice Scoresï¼‰åˆ†åˆ«ä¸º94.66%å’Œ94.38%ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å‡å¯åœ¨GitHubä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23664v2">PDF</a> Accepted at Irish Machine Vision and Image Processing Conference   (IMVIP) 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¸¦æœ‰åˆ†å‰²æ©è†œåˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒçš„æ–°æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å…‹æœäº†åŒ»å­¦å›¾åƒæ•°æ®éš¾ä»¥è·å–å’Œæ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®å¢å¼ºçœŸå®æ•°æ®é›†ï¼Œç”¨äºç›‘ç£å¾®è°ƒSAMæ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºåˆæˆæ•°æ®æœ‰æ•ˆæ•æ‰çœŸå®å›¾åƒç‰¹å¾ï¼Œå¹¶åœ¨æœ‰é™çœŸå®å›¾åƒ-æ©è†œå¯¹è®­ç»ƒä¸‹è¾¾åˆ°å…ˆè¿›èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœï¼ŒDiceå¾—åˆ†åˆ†åˆ«ä¸º94.66%å’Œ94.38%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ•°æ®å› éšç§å’Œç›‘ç®¡é™åˆ¶è€Œéš¾ä»¥è·å–ã€‚</li>
<li>æ‰‹å·¥æ ‡æ³¨åŒ»å­¦å›¾åƒè€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ç”Ÿæˆå¼AIï¼ˆGenAIï¼‰é‡‡ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹å¯æœ‰æ•ˆç”Ÿæˆé€¼çœŸçš„åˆæˆå›¾åƒã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°é¢–çš„mask-guided GenAIæ–¹æ³•ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé…å¯¹çš„åˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒå’Œåˆ†å‰²æ©è†œã€‚</li>
<li>åˆæˆæ•°æ®å¢å¼ºç°å®æ•°æ®é›†ï¼Œç”¨äºç›‘ç£å¾®è°ƒSAMæ¨¡å‹ã€‚</li>
<li>åˆæˆæ•°æ®æœ‰æ•ˆæ•æ‰çœŸå®å›¾åƒç‰¹å¾ï¼Œåœ¨æœ‰é™çœŸå®å›¾åƒ-æ©è†œå¯¹è®­ç»ƒä¸‹è¾¾åˆ°å…ˆè¿›èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0bb205423e332b4317b0c0b676f0f8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a6e2e20b23c412a54f5b1a6d39a4562d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e224c2d283809a9b64b55df257ff009.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HadaNorm-Diffusion-Transformer-Quantization-through-Mean-Centered-Transformations"><a href="#HadaNorm-Diffusion-Transformer-Quantization-through-Mean-Centered-Transformations" class="headerlink" title="HadaNorm: Diffusion Transformer Quantization through Mean-Centered   Transformations"></a>HadaNorm: Diffusion Transformer Quantization through Mean-Centered   Transformations</h2><p><strong>Authors:Marco Federici, Riccardo Del Chiaro, Boris van Breugel, Paul Whatmough, Markus Nagel</strong></p>
<p>Diffusion models represent the cutting edge in image generation, but their high memory and computational demands hinder deployment on resource-constrained devices. Post-Training Quantization (PTQ) offers a promising solution by reducing the bitwidth of matrix operations. However, standard PTQ methods struggle with outliers, and achieving higher compression often requires transforming model weights and activations before quantization. In this work, we propose HadaNorm, a novel linear transformation that extends existing approaches by both normalizing channels activations and applying Hadamard transforms to effectively mitigate outliers and enable aggressive activation quantization. We demonstrate that HadaNorm consistently reduces quantization error across the various components of transformer blocks, outperforming state-of-the-art methods. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æ˜¯å›¾åƒç”Ÿæˆé¢†åŸŸçš„å‰æ²¿æŠ€æœ¯ï¼Œä½†å…¶å¯¹å†…å­˜å’Œè®¡ç®—èµ„æºçš„é«˜éœ€æ±‚é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰é€šè¿‡é™ä½çŸ©é˜µæ“ä½œçš„ä½å®½æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ ‡å‡†PTQæ–¹æ³•éš¾ä»¥å¤„ç†å¼‚å¸¸å€¼ï¼Œä¸ºå®ç°æ›´é«˜çš„å‹ç¼©ï¼Œé€šå¸¸éœ€è¦åœ¨é‡åŒ–å‰å¯¹æ¨¡å‹æƒé‡å’Œæ¿€æ´»è¿›è¡Œè½¬æ¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HadaNormï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çº¿æ€§è½¬æ¢ï¼Œå®ƒé€šè¿‡æ ‡å‡†åŒ–é€šé“æ¿€æ´»å’Œåº”ç”¨å“ˆè¾¾ç›å˜æ¢æ¥æœ‰æ•ˆç¼“è§£å¼‚å¸¸å€¼é—®é¢˜ï¼Œä»è€Œå®ç°æ¿€çƒˆçš„æ¿€æ´»é‡åŒ–ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒHadaNormåœ¨å˜å‹å™¨å—çš„å„ç§ç»„ä»¶ä¸­ä¸€è‡´åœ°å‡å°‘äº†é‡åŒ–è¯¯å·®ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09932v2">PDF</a> 8 Pages, 6 Figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä½†å…¶é«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†é‡‡ç”¨åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æ–¹æ³•é™ä½çŸ©é˜µæ“ä½œä½å®½çš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œæ ‡å‡†PTQæ–¹æ³•é¢ä¸´å¤„ç†å¼‚å¸¸å€¼çš„é—®é¢˜ï¼Œå¹¶ä¸”ä¸ºäº†åœ¨é‡åŒ–å‰å˜æ¢æ¨¡å‹æƒé‡å’Œæ¿€æ´»å€¼æ¥å®ç°æ›´é«˜çš„å‹ç¼©ç‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†HadaNormï¼Œä¸€ç§æ–°å‹çº¿æ€§å˜æ¢æ–¹æ³•ï¼Œå®ƒé€šè¿‡å½’ä¸€åŒ–é€šé“æ¿€æ´»å’Œé‡‡ç”¨Hadamardå˜æ¢æ¥æœ‰æ•ˆå‡è½»å¼‚å¸¸å€¼é—®é¢˜ï¼Œä»è€Œå®ç°æ¿€çƒˆçš„æ¿€æ´»é‡åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒHadaNormåœ¨å˜å‹å™¨å—çš„å„ä¸ªç»„ä»¶ä¸­ä¸€è‡´é™ä½äº†é‡åŒ–è¯¯å·®ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æ˜¯å›¾åƒç”Ÿæˆçš„å‰æ²¿æŠ€æœ¯ï¼Œä½†å…¶åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²å—é™ã€‚</li>
<li>åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æ˜¯è§£å†³æ‰©æ•£æ¨¡å‹é«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>æ ‡å‡†PTQæ–¹æ³•é¢ä¸´å¤„ç†å¼‚å¸¸å€¼çš„é—®é¢˜ã€‚</li>
<li>HadaNormæ˜¯ä¸€ç§æ–°å‹çº¿æ€§å˜æ¢æ–¹æ³•ï¼Œç”¨äºåœ¨é‡åŒ–è¿‡ç¨‹ä¸­æœ‰æ•ˆå‡è½»å¼‚å¸¸å€¼é—®é¢˜ã€‚</li>
<li>HadaNormé€šè¿‡å½’ä¸€åŒ–é€šé“æ¿€æ´»å’Œé‡‡ç”¨Hadamardå˜æ¢æ¥å®ç°æ¿€æ´»é‡åŒ–çš„ä¼˜åŒ–ã€‚</li>
<li>HadaNormåœ¨ä¸åŒç»„ä»¶çš„å˜å‹å™¨å—ä¸­ä¸€è‡´é™ä½äº†é‡åŒ–è¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e65ed53f6ed2be9e9d39675ff52de529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd62f7222df75230a7a2c71c268bf3be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99aaa658915aa3bb17ef250df466e2cd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Revisiting-Likelihood-Based-Out-of-Distribution-Detection-by-Modeling-Representations"><a href="#Revisiting-Likelihood-Based-Out-of-Distribution-Detection-by-Modeling-Representations" class="headerlink" title="Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling   Representations"></a>Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling   Representations</h2><p><strong>Authors:Yifan Ding, Arturas Aleksandraus, Amirhossein Ahmadian, Jonas Unger, Fredrik Lindsten, Gabriel Eilertsen</strong></p>
<p>Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\href{<a target="_blank" rel="noopener" href="https://github.com/limchaos/Likelihood-OOD.git%7D%7B/texttt%7Bhttps://github.com/limchaos/Likelihood-OOD.git%7D%7D$">https://github.com/limchaos/Likelihood-OOD.git}{\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$</a>. </p>
<blockquote>
<p>åœ¨æ·±åº¦å­¦ä¹ çš„å¯é æ€§ä¿éšœæ–¹é¢ï¼Œå°¤å…¶æ˜¯åœ¨å…³é”®å®‰å…¨åº”ç”¨ä¸­ï¼Œè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ£€æµ‹ï¼ˆOut-of-Distribution Detectionï¼Œç®€ç§°OODï¼‰æ˜¯éå¸¸å…³é”®çš„ã€‚åŸºäºå¯èƒ½æ€§çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å†å²ä¸­å› å…¶å¯¹è¶…å‡ºåˆ†å¸ƒèŒƒå›´æ£€æµ‹çš„æ¬ ä½³è¡¨ç°è€Œå—åˆ°æ‰¹è¯„ã€‚å½“åº”ç”¨äºå›¾åƒæ•°æ®æ—¶ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¼šå¯¹è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ•°æ®åˆ†é…æ›´é«˜çš„å¯èƒ½æ€§ï¼Œé«˜äºå¯¹å†…éƒ¨åˆ†å¸ƒæ ·æœ¬çš„åˆ†é…å¯èƒ½æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯èƒ½æ€§å¹¶éå¤©ç”Ÿå­˜åœ¨é—®é¢˜ã€‚ç›¸åï¼Œå›¾åƒç©ºé—´ä¸­çš„æŸäº›å±æ€§é˜»æ­¢äº†å¯èƒ½æ€§ä½œä¸ºä¸€ä¸ªæœ‰æ•ˆçš„æ£€æµ‹åˆ†æ•°ã€‚ç»™å®šä¸€ä¸ªè¶³å¤Ÿå¥½çš„å¯èƒ½æ€§ä¼°è®¡å™¨ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå…¬å¼ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨é¢„è®­ç»ƒç¼–ç å™¨çš„è¡¨ç¤ºç©ºé—´ä¸­åº”ç”¨æ—¶ï¼ŒåŸºäºå¯èƒ½æ€§çš„æ–¹æ³•ä»ç„¶å¯ä»¥ä¸æœ€å…ˆè¿›çš„æ–¹æ³•è¡¨ç°ç›¸å½“ã€‚æˆ‘ä»¬å·¥ä½œçš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/limchaos/Likelihood-OOD.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/limchaos/Likelihood-OOD.gitæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07793v3">PDF</a> Scandinavian Conference on Image Analysis 2025 (oral)</p>
<p><strong>Summary</strong><br>     åŸºäºä¼¼ç„¶çš„æ–¹æ³•åœ¨è¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼ˆOODæ£€æµ‹ï¼‰æ—¶èƒ½å¤Ÿè¡¨ç°è‰¯å¥½ï¼Œä½†éœ€è¦é€‰æ‹©æ­£ç¡®çš„å›¾åƒç‰¹å¾ç©ºé—´å¹¶ä½¿ç”¨åˆé€‚çš„é¢„è®­ç»ƒç¼–ç å™¨ã€‚æœ¬å·¥ä½œæŒ‡å‡ºï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå…¬å¼æ„å»ºä¼¼ç„¶ä¼°è®¡å™¨ï¼Œå¯ä»¥åœ¨é¢„è®­ç»ƒç¼–ç å™¨çš„ç‰¹å¾ç©ºé—´ä¸Šå®ç°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OODæ£€æµ‹å¯¹äºç¡®ä¿æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å¯é æ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­ã€‚</li>
<li>ä¼ ç»Ÿä¸ŠåŸºäºä¼¼ç„¶çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨OODæ£€æµ‹ä¸­è¡¨ç°ä¸ä½³ï¼Œå®¹æ˜“å¯¹OODæ•°æ®èµ‹äºˆè¾ƒé«˜çš„ä¼¼ç„¶å€¼ã€‚</li>
<li>åœ¨å›¾åƒç©ºé—´ä¸­ï¼ŒæŸäº›ç‰¹æ€§é˜»ç¢äº†ä¼¼ç„¶ä½œä¸ºæœ‰æ•ˆçš„æ£€æµ‹åˆ†æ•°ã€‚</li>
<li>ä½¿ç”¨è‰¯å¥½çš„ä¼¼ç„¶ä¼°è®¡å™¨ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¦‚ç‡æµå…¬å¼ï¼Œå¯ä»¥æ”¹å–„åŸºäºä¼¼ç„¶çš„æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>åœ¨é¢„è®­ç»ƒç¼–ç å™¨çš„ç‰¹å¾ç©ºé—´ä¸Šåº”ç”¨åŸºäºä¼¼ç„¶çš„æ–¹æ³•ï¼Œå¯ä»¥è¾¾åˆ°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ•ˆæœã€‚</li>
<li>è¯¥å·¥ä½œçš„ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07793">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-38c75be78c9965de94664607bc230cfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61fc3b5ec20e0b4908265527832f9765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a5d7b2d126d6163d0396a867c39b66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a2be2096465d08fc90357df24ea8d8b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Localized-Concept-Erasure-for-Text-to-Image-Diffusion-Models-Using-Training-Free-Gated-Low-Rank-Adaptation"><a href="#Localized-Concept-Erasure-for-Text-to-Image-Diffusion-Models-Using-Training-Free-Gated-Low-Rank-Adaptation" class="headerlink" title="Localized Concept Erasure for Text-to-Image Diffusion Models Using   Training-Free Gated Low-Rank Adaptation"></a>Localized Concept Erasure for Text-to-Image Diffusion Models Using   Training-Free Gated Low-Rank Adaptation</h2><p><strong>Authors:Byung Hyun Lee, Sungjin Lim, Se Young Chun</strong></p>
<p>Fine-tuning based concept erasing has demonstrated promising results in preventing generation of harmful contents from text-to-image diffusion models by removing target concepts while preserving remaining concepts. To maintain the generation capability of diffusion models after concept erasure, it is necessary to remove only the image region containing the target concept when it locally appears in an image, leaving other regions intact. However, prior arts often compromise fidelity of the other image regions in order to erase the localized target concept appearing in a specific area, thereby reducing the overall performance of image generation. To address these limitations, we first introduce a framework called localized concept erasure, which allows for the deletion of only the specific area containing the target concept in the image while preserving the other regions. As a solution for the localized concept erasure, we propose a training-free approach, dubbed Gated Low-rank adaptation for Concept Erasure (GLoCE), that injects a lightweight module into the diffusion model. GLoCE consists of low-rank matrices and a simple gate, determined only by several generation steps for concepts without training. By directly applying GLoCE to image embeddings and designing the gate to activate only for target concepts, GLoCE can selectively remove only the region of the target concepts, even when target and remaining concepts coexist within an image. Extensive experiments demonstrated GLoCE not only improves the image fidelity to text prompts after erasing the localized target concepts, but also outperforms prior arts in efficacy, specificity, and robustness by large margin and can be extended to mass concept erasure. </p>
<blockquote>
<p>åŸºäºæ¦‚å¿µæ“¦é™¤çš„å¾®è°ƒåœ¨é˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹æ–¹é¢æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚å®ƒé€šè¿‡æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µåŒæ—¶ä¿ç•™å‰©ä½™æ¦‚å¿µæ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†ä¿æŒæ‰©æ•£æ¨¡å‹åœ¨æ¦‚å¿µæ“¦é™¤åçš„ç”Ÿæˆèƒ½åŠ›ï¼Œæœ‰å¿…è¦åªç§»é™¤å›¾åƒä¸­åŒ…å«ç›®æ ‡æ¦‚å¿µçš„åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒå…¶ä»–åŒºåŸŸä¸å˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æŠ€æœ¯å¾€å¾€ä¸ºäº†æ¶ˆé™¤ç‰¹å®šåŒºåŸŸä¸­å‡ºç°çš„å±€éƒ¨ç›®æ ‡æ¦‚å¿µè€Œç‰ºç‰²äº†å…¶ä»–å›¾åƒåŒºåŸŸçš„ä¿çœŸåº¦ï¼Œä»è€Œé™ä½äº†å›¾åƒç”Ÿæˆçš„æ€»ä½“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªåä¸ºå±€éƒ¨æ¦‚å¿µæ“¦é™¤çš„æ¡†æ¶ï¼Œå®ƒå…è®¸åªåˆ é™¤å›¾åƒä¸­åŒ…å«ç›®æ ‡æ¦‚å¿µçš„ç‰¹å®šåŒºåŸŸï¼ŒåŒæ—¶ä¿ç•™å…¶ä»–åŒºåŸŸã€‚ä½œä¸ºå±€éƒ¨æ¦‚å¿µæ“¦é™¤çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œç§°ä¸ºç”¨äºæ¦‚å¿µæ“¦é™¤çš„å¸¦é—¨æ§çš„ä½ç§©è‡ªé€‚åº”ï¼ˆGLoCEï¼‰ï¼Œå®ƒå°†ä¸€ä¸ªè½»é‡çº§æ¨¡å—æ³¨å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚GLoCEç”±ä½ç§©çŸ©é˜µå’Œç®€å•é—¨ç»„æˆï¼Œä»…ç”±å‡ ä»£çš„ç”Ÿæˆæ­¥éª¤å†³å®šæ¦‚å¿µï¼Œæ— éœ€è®­ç»ƒã€‚é€šè¿‡å°†GLoCEç›´æ¥åº”ç”¨äºå›¾åƒåµŒå…¥å¹¶è®¾è®¡ä»…åœ¨ç›®æ ‡æ¦‚å¿µå‡ºç°æ—¶æ¿€æ´»çš„é—¨ï¼ŒGLoCEå¯ä»¥ä»…é€‰æ‹©æ€§åœ°åˆ é™¤ç›®æ ‡æ¦‚å¿µçš„åŒºåŸŸï¼Œå³ä½¿ç›®æ ‡æ¦‚å¿µå’Œå‰©ä½™çš„æ¦‚å¿µå…±å­˜äºå›¾åƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGLoCEä¸ä»…åœ¨æ“¦é™¤å±€éƒ¨ç›®æ ‡æ¦‚å¿µåå¯¹æ–‡æœ¬æç¤ºçš„å›¾åƒä¿çœŸåº¦æœ‰æ‰€æé«˜ï¼Œè€Œä¸”åœ¨æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œç¨³å¥æ€§æ–¹é¢å¤§å¤§è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡çš„æ¦‚å¿µæ“¦é™¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12356v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºæ¦‚å¿µæ“¦é™¤çš„å¾®è°ƒåœ¨é˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹æ–¹é¢å±•ç°å‡ºè‰¯å¥½æ•ˆæœã€‚ä¸ºäº†ç»´æŠ¤æ‰©æ•£æ¨¡å‹åœ¨æ¦‚å¿µæ“¦é™¤åçš„ç”Ÿæˆèƒ½åŠ›ï¼Œåªéœ€åˆ é™¤åŒ…å«ç›®æ ‡æ¦‚å¿µçš„å›¾åƒåŒºåŸŸï¼ŒåŒæ—¶ä¿ç•™å…¶ä»–åŒºåŸŸã€‚ä¸ºè§£å†³ç°æœ‰æŠ€æœ¯åœ¨æ“¦é™¤ç‰¹å®šåŒºåŸŸç›®æ ‡æ¦‚å¿µæ—¶æŸå®³å…¶ä»–å›¾åƒåŒºåŸŸä¿çœŸåº¦çš„é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåä¸ºå±€éƒ¨æ¦‚å¿µæ“¦é™¤çš„æ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£å†³æ–¹æ¡ˆâ€”â€”GLoCEï¼ˆé—¨æ§ä½ç§©é€‚åº”æ¦‚å¿µæ“¦é™¤ï¼‰ã€‚GLoCEé€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ä¸­æ³¨å…¥è½»é‡çº§æ¨¡å—æ¥å®ç°é€‰æ‹©æ€§æ“¦é™¤ç›®æ ‡æ¦‚å¿µåŒºåŸŸï¼Œå³ä½¿ç›®æ ‡æ¦‚å¿µå’Œå‰©ä½™æ¦‚å¿µå…±å­˜äºå›¾åƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å®éªŒè¡¨æ˜ï¼ŒGLoCEä¸ä»…èƒ½æé«˜æ“¦é™¤ç›®æ ‡æ¦‚å¿µåçš„å›¾åƒå¯¹æ–‡æœ¬æç¤ºçš„ä¿çœŸåº¦ï¼Œè€Œä¸”åœ¨æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œç¨³å¥æ€§æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶å¯æ‰©å±•åˆ°å¤§è§„æ¨¡æ¦‚å¿µæ“¦é™¤ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¦‚å¿µæ“¦é™¤åœ¨é˜²æ­¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆæœ‰å®³å†…å®¹æ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>å±€éƒ¨æ¦‚å¿µæ“¦é™¤æ¡†æ¶å…è®¸ä»…åˆ é™¤å›¾åƒä¸­åŒ…å«ç›®æ ‡æ¦‚å¿µçš„ç‰¹å®šåŒºåŸŸï¼ŒåŒæ—¶ä¿æŒå…¶ä»–åŒºåŸŸçš„å®Œæ•´æ€§ã€‚</li>
<li>GLoCEæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡æ³¨å…¥è½»é‡çº§æ¨¡å—åˆ°æ‰©æ•£æ¨¡å‹ä¸­å®ç°é€‰æ‹©æ€§æ“¦é™¤ç›®æ ‡æ¦‚å¿µåŒºåŸŸã€‚</li>
<li>GLoCEé€šè¿‡ç›´æ¥åœ¨å›¾åƒåµŒå…¥ä¸Šåº”ç”¨å¹¶è®¾è®¡ä»…é’ˆå¯¹ç›®æ ‡æ¦‚å¿µçš„é—¨æ¥æ§åˆ¶æ¿€æ´»ï¼Œå®ç°é€‰æ‹©æ€§æ“¦é™¤ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒGLoCEåœ¨æ“¦é™¤å±€éƒ¨ç›®æ ‡æ¦‚å¿µåæé«˜äº†å›¾åƒå¯¹æ–‡æœ¬æç¤ºçš„ä¿çœŸåº¦ã€‚</li>
<li>GLoCEåœ¨æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œç¨³å¥æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>GLoCEå¯æ‰©å±•åˆ°å¤§è§„æ¨¡æ¦‚å¿µæ“¦é™¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26f2170c29889b50443a202b56a12b11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d196095f55585ddcb9d61d6de16975fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-141712c7a0ae182942897243edffcd0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2eb732eaccbcf8fb7462bad3d1c651c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f13502c0508bf83613907869935ad38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89013911ec3360982e50460ab4a59391.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Solving-Inverse-Problems-using-Diffusion-with-Iterative-Colored-Renoising"><a href="#Solving-Inverse-Problems-using-Diffusion-with-Iterative-Colored-Renoising" class="headerlink" title="Solving Inverse Problems using Diffusion with Iterative Colored   Renoising"></a>Solving Inverse Problems using Diffusion with Iterative Colored   Renoising</h2><p><strong>Authors:Matt C. Bendel, Saurav K. Shastri, Rizwan Ahmad, Philip Schniter</strong></p>
<p>Imaging inverse problems can be solved in an unsupervised manner using pre-trained diffusion models, but doing so requires approximating the gradient of the measurement-conditional score function in the diffusion reverse process. We show that the approximations produced by existing methods are relatively poor, especially early in the reverse process, and so we propose a new approach that iteratively reestimates and â€œrenoisesâ€ the estimate several times per diffusion step. This iterative approach, which we call Fast Iterative REnoising (FIRE), injects colored noise that is shaped to ensure that the pre-trained diffusion model always sees white noise, in accordance with how it was trained. We then embed FIRE into the DDIM reverse process and show that the resulting â€œDDfireâ€ offers state-of-the-art accuracy and runtime on several linear inverse problems, as well as phase retrieval. Our implementation is at <a target="_blank" rel="noopener" href="https://github.com/matt-bendel/DDfire">https://github.com/matt-bendel/DDfire</a> </p>
<blockquote>
<p>ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä»¥æ— ç›‘ç£çš„æ–¹å¼è§£å†³æˆåƒåé—®é¢˜ï¼Œéœ€è¦åœ¨æ‰©æ•£åå‘è¿‡ç¨‹ä¸­è¿‘ä¼¼æµ‹é‡æ¡ä»¶å¾—åˆ†å‡½æ•°çš„æ¢¯åº¦ã€‚æˆ‘ä»¬æ˜¾ç¤ºç°æœ‰æ–¹æ³•äº§ç”Ÿçš„è¿‘ä¼¼å€¼ç›¸å¯¹è¾ƒå·®ï¼Œå°¤å…¶æ˜¯åœ¨åå‘è¿‡ç¨‹çš„æ—©æœŸï¼Œå› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­å¤šæ¬¡é‡æ–°ä¼°è®¡å’Œâ€œå¢åŠ å™ªå£°â€ã€‚æˆ‘ä»¬ç§°è¿™ç§è¿­ä»£æ–¹æ³•ä¸ºå¿«é€Ÿè¿­ä»£é‡æ–°å™ªå£°åŒ–ï¼ˆFIREï¼‰ï¼Œå®ƒæ³¨å…¥æœ‰è‰²å™ªå£°ï¼Œä»¥ç¡®ä¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å§‹ç»ˆçœ‹åˆ°ç™½å™ªå£°ï¼Œç¬¦åˆå…¶è®­ç»ƒæ–¹å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†FIREåµŒå…¥åˆ°DDIMåå‘è¿‡ç¨‹ä¸­ï¼Œå¹¶è¯æ˜ç”±æ­¤äº§ç”Ÿçš„â€œDDfireâ€åœ¨å‡ ä¸ªçº¿æ€§åé—®é¢˜ä»¥åŠç›¸ä½æ£€ç´¢æ–¹é¢æä¾›äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œè¿è¡Œæ—¶ã€‚æˆ‘ä»¬çš„å®ç°ä½äº<a target="_blank" rel="noopener" href="https://github.com/matt-bendel/DDfire%E3%80%82">https://github.com/matt-bendel/DDfireã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17468v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä»¥æ— ç›‘ç£çš„æ–¹å¼è§£å†³æˆåƒåé—®é¢˜ï¼Œéœ€è¦ä¼°è®¡æµ‹é‡æ¡ä»¶åˆ†æ•°å‡½æ•°çš„æ¢¯åº¦ã€‚ç°æœ‰æ–¹æ³•äº§ç”Ÿçš„è¿‘ä¼¼å€¼åœ¨æ—©æœŸé€†å‘è¿‡ç¨‹ä¸­ç›¸å¯¹è¾ƒå·®ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å¿«é€Ÿè¿­ä»£å»å™ªï¼ˆFIREï¼‰ï¼Œåœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­å¤šæ¬¡é‡æ–°ä¼°è®¡å’Œâ€œåŠ å…¥å™ªå£°â€ã€‚å°†FIREåµŒå…¥DDIMé€†å‘è¿‡ç¨‹ï¼Œå¾—åˆ°çš„DDfireåœ¨å¤šä¸ªçº¿æ€§åé—®é¢˜ä»¥åŠç›¸ä½æ£€ç´¢æ–¹é¢å…·æœ‰æœ€ä½³ç²¾åº¦å’Œè¿è¡Œæ—¶æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¯ä»¥æ— ç›‘ç£åœ°è§£å†³æˆåƒåé—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¼°è®¡æµ‹é‡æ¡ä»¶åˆ†æ•°å‡½æ•°çš„æ¢¯åº¦æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†å¿«é€Ÿè¿­ä»£å»å™ªï¼ˆFIREï¼‰æ–¹æ³•ï¼Œä»¥æé«˜ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>FIREæ–¹æ³•é€šè¿‡æ³¨å…¥å½©è‰²å™ªå£°ï¼Œç¡®ä¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å§‹ç»ˆçœ‹åˆ°ç™½å™ªå£°ã€‚</li>
<li>DDfireæ–¹æ³•å°†FIREåµŒå…¥DDIMé€†å‘è¿‡ç¨‹ã€‚</li>
<li>DDfireåœ¨å¤šä¸ªçº¿æ€§åé—®é¢˜å’Œç›¸ä½æ£€ç´¢æ–¹é¢å…·æœ‰æœ€ä½³æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8699ffb487311ce20fbd12bd11f55f53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07f5a0d543b4b18350a3d7847871ef27.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Diffusion-Augmented-Retrieval-A-Training-Free-Approach-to-Interactive-Text-to-Image-Retrieval"><a href="#Diffusion-Augmented-Retrieval-A-Training-Free-Approach-to-Interactive-Text-to-Image-Retrieval" class="headerlink" title="Diffusion Augmented Retrieval: A Training-Free Approach to Interactive   Text-to-Image Retrieval"></a>Diffusion Augmented Retrieval: A Training-Free Approach to Interactive   Text-to-Image Retrieval</h2><p><strong>Authors:Zijun Long, Kangheng Liang, Gerardo Aragon-Camarasa, Richard Mccreadie, Paul Henderson</strong></p>
<p>Interactive Text-to-image retrieval (I-TIR) is an important enabler for a wide range of state-of-the-art services in domains such as e-commerce and education. However, current methods rely on finetuned Multimodal Large Language Models (MLLMs), which are costly to train and update, and exhibit poor generalizability. This latter issue is of particular concern, as: 1) finetuning narrows the pretrained distribution of MLLMs, thereby reducing generalizability; and 2) I-TIR introduces increasing query diversity and complexity. As a result, I-TIR solutions are highly likely to encounter queries and images not well represented in any training dataset. To address this, we propose leveraging Diffusion Models (DMs) for text-to-image mapping, to avoid finetuning MLLMs while preserving robust performance on complex queries. Specifically, we introduce Diffusion Augmented Retrieval (DAR), a framework that generates multiple intermediate representations via LLM-based dialogue refinements and DMs, producing a richer depiction of the userâ€™s information needs. This augmented representation facilitates more accurate identification of semantically and visually related images. Extensive experiments on four benchmarks show that for simple queries, DAR achieves results on par with finetuned I-TIR models, yet without incurring their tuning overhead. Moreover, as queries become more complex through additional conversational turns, DAR surpasses finetuned I-TIR models by up to 7.61% in Hits@10 after ten turns, illustrating its improved generalization for more intricate queries. </p>
<blockquote>
<p>äº¤äº’å¼æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ï¼ˆI-TIRï¼‰æ˜¯ç”µå­å•†åŠ¡å’Œæ•™è‚²ç­‰é¢†åŸŸå…ˆè¿›æœåŠ¡å¹¿æ³›åº”ç”¨çš„é‡è¦æ¨åŠ¨è€…ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºå¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œè¿™äº›æ¨¡å‹è®­ç»ƒå’Œæ›´æ–°çš„æˆæœ¬å¾ˆé«˜ï¼Œå¹¶ä¸”é€šç”¨æ€§è¾ƒå·®ã€‚åè€…çš„é—®é¢˜å°¤å…¶å€¼å¾—å…³æ³¨ï¼Œå› ä¸ºï¼š1ï¼‰å¾®è°ƒä¼šç¼©å°MLLMsçš„é¢„è®­ç»ƒåˆ†å¸ƒï¼Œä»è€Œé™ä½å…¶é€šç”¨æ€§ï¼›2ï¼‰I-TIRå¼•å…¥äº†æ—¥ç›Šå¢é•¿çš„æŸ¥è¯¢å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚å› æ­¤ï¼ŒI-TIRè§£å†³æ–¹æ¡ˆå¾ˆå¯èƒ½ä¼šé‡åˆ°ä»»ä½•è®­ç»ƒæ•°æ®é›†ä¸­è¡¨ç¤ºä¸ä½³çš„æŸ¥è¯¢å’Œå›¾åƒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒçš„æ˜ å°„ï¼Œä»¥é¿å…å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶åœ¨å¤æ‚æŸ¥è¯¢ä¸Šä¿æŒç¨³å¥çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ‰©æ•£å¢å¼ºæ£€ç´¢ï¼ˆDARï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŸºäºLLMçš„å¯¹è¯ç²¾ç‚¼å’ŒDMsç”Ÿæˆå¤šä¸ªä¸­é—´è¡¨ç¤ºï¼Œä»è€Œæä¾›æ›´ä¸°å¯Œçš„ç”¨æˆ·ä¿¡æ¯éœ€æ±‚çš„æè¿°ã€‚è¿™ç§å¢å¼ºçš„è¡¨ç¤ºæœ‰åŠ©äºæ›´å‡†ç¡®åœ°è¯†åˆ«è¯­ä¹‰å’Œè§†è§‰ç›¸å…³çš„å›¾åƒã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå¯¹äºç®€å•æŸ¥è¯¢ï¼ŒDARåœ¨ä¸äº§ç”Ÿé¢å¤–çš„å¾®è°ƒå¼€é”€çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸å¾®è°ƒè¿‡çš„I-TIRæ¨¡å‹ç›¸å½“çš„ç»“æœã€‚æ­¤å¤–ï¼Œéšç€æŸ¥è¯¢é€šè¿‡é¢å¤–çš„å¯¹è¯å›åˆå˜å¾—æ›´ä¸ºå¤æ‚ï¼ŒDARåœ¨åå›åˆåçš„å‘½ä¸­ç‡æé«˜äº†é«˜è¾¾7.61%ï¼Œè¿™è¯æ˜äº†å…¶åœ¨æ›´å¤æ‚çš„æŸ¥è¯¢ä¸­å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15379v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Interactive Text-to-image Retrievalï¼ˆI-TIRï¼‰çš„é‡è¦æ€§åŠå…¶åœ¨ç”µå­å•†åŠ¡å’Œæ•™è‚²ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºå¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œè®­ç»ƒæˆæœ¬é«˜ä¸”é€šç”¨æ€§è¾ƒå·®ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåˆ©ç”¨Diffusion Modelsï¼ˆDMsï¼‰è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒçš„æ˜ å°„ï¼Œé¿å…å¾®è°ƒMLLMsï¼ŒåŒæ—¶åœ¨å¤æ‚æŸ¥è¯¢ä¸Šä¿æŒç¨³å¥æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œå¯¹äºç®€å•æŸ¥è¯¢ï¼Œæ–°æå‡ºçš„æ–¹æ³•ä¸å¾®è°ƒI-TIRæ¨¡å‹ç»“æœç›¸å½“ï¼Œè€Œå¯¹äºå¤æ‚æŸ¥è¯¢ï¼Œåˆ™è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>I-TIRå¯¹äºç”µå­å•†åŠ¡å’Œæ•™è‚²ç­‰é¢†åŸŸè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰I-TIRæ–¹æ³•ä¾èµ–æ˜‚è´µçš„è®­ç»ƒæˆæœ¬ä¸”é€šç”¨æ€§å·®çš„å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
<li>åˆ©ç”¨Diffusion Modelsï¼ˆDMsï¼‰è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒçš„æ˜ å°„å¯ä»¥é¿å…å¾®è°ƒMLLMså¹¶ä¿æŒå¯¹å¤æ‚æŸ¥è¯¢çš„ç¨³å¥æ€§èƒ½ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡å¼•å…¥Diffusion Augmented Retrievalï¼ˆDARï¼‰æ¡†æ¶ç”Ÿæˆå¤šä¸ªä¸­é—´è¡¨ç¤ºï¼Œé€šè¿‡LLMåŸºäºå¯¹è¯çš„ç»†åŒ–ä¿®æ­£å’ŒDMsäº§ç”Ÿæ›´ä¸°å¯Œç”¨æˆ·ä¿¡æ¯éœ€æ±‚çš„æè¿°ã€‚</li>
<li>å¯¹äºç®€å•æŸ¥è¯¢ï¼ŒDARä¸å¾®è°ƒçš„I-TIRæ¨¡å‹ç»“æœç›¸å½“ã€‚</li>
<li>å¯¹äºå¤æ‚æŸ¥è¯¢ï¼ŒDARåœ¨å¤šæ¬¡å¯¹è¯å›åˆåè¶…è¶Šè°ƒå‚æ¨¡å‹ï¼Œåœ¨Hits@10ä¸Šæé«˜äº†7.61%ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84bccd6cdda7958623f355ff235d0f65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a9b2b509621b36923dac1301199fa86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b343aab34d13422f2957b227f809e4e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e148749e08b3fff34d497d1d4af58a94.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Diffusion-Driven-Semantic-Communication-for-Generative-Models-with-Bandwidth-Constraints"><a href="#Diffusion-Driven-Semantic-Communication-for-Generative-Models-with-Bandwidth-Constraints" class="headerlink" title="Diffusion-Driven Semantic Communication for Generative Models with   Bandwidth Constraints"></a>Diffusion-Driven Semantic Communication for Generative Models with   Bandwidth Constraints</h2><p><strong>Authors:Lei Guo, Wei Chen, Yuxuan Sun, Bo Ai, Nikolaos Pappas, Tony Q. S. Quek</strong></p>
<p>Diffusion models have been extensively utilized in AI-generated content (AIGC) in recent years, thanks to the superior generation capabilities. Combining with semantic communications, diffusion models are used for tasks such as denoising, data reconstruction, and content generation. However, existing diffusion-based generative models do not consider the stringent bandwidth limitation, which limits its application in wireless communication. This paper introduces a diffusion-driven semantic communication framework with advanced VAE-based compression for bandwidth-constrained generative model. Our designed architecture utilizes the diffusion model, where the signal transmission process through the wireless channel acts as the forward process in diffusion. To reduce bandwidth requirements, we incorporate a downsampling module and a paired upsampling module based on a variational auto-encoder with reparameterization at the receiver to ensure that the recovered features conform to the Gaussian distribution. Furthermore, we derive the loss function for our proposed system and evaluate its performance through comprehensive experiments. Our experimental results demonstrate significant improvements in pixel-level metrics such as peak signal to noise ratio (PSNR) and semantic metrics like learned perceptual image patch similarity (LPIPS). These enhancements are more profound regarding the compression rates and SNR compared to deep joint source-channel coding (DJSCC). We release the code at <a target="_blank" rel="noopener" href="https://github.com/import-sudo/Diffusion-Driven-Semantic-Communication">https://github.com/import-sudo/Diffusion-Driven-Semantic-Communication</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ï¼Œè¿‘å¹´æ¥åœ¨äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç»“åˆè¯­ä¹‰é€šä¿¡ï¼Œæ‰©æ•£æ¨¡å‹è¢«ç”¨äºå»å™ªã€æ•°æ®é‡å»ºå’Œå†…å®¹ç”Ÿæˆç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹å¹¶æœªè€ƒè™‘ä¸¥æ ¼çš„å¸¦å®½é™åˆ¶ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æ— çº¿é€šä¿¡ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ‰©æ•£é©±åŠ¨çš„è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰å…ˆè¿›çš„åŸºäºVAEçš„å‹ç¼©æŠ€æœ¯ï¼Œé€‚ç”¨äºå¸¦å®½å—é™çš„ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬è®¾è®¡çš„æ¶æ„åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­é€šè¿‡æ— çº¿ä¿¡é“ä¼ è¾“ä¿¡å·çš„è¿‡ç¨‹ä½œä¸ºæ‰©æ•£çš„å‰å‘è¿‡ç¨‹ã€‚ä¸ºäº†å‡å°‘å¸¦å®½è¦æ±‚ï¼Œæˆ‘ä»¬èå…¥äº†ä¸‹é‡‡æ ·æ¨¡å—å’ŒåŸºäºå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨çš„é…å¯¹ä¸Šé‡‡æ ·æ¨¡å—ï¼Œå¹¶åœ¨æ¥æ”¶å™¨å¤„è¿›è¡Œé‡å‚æ•°åŒ–ï¼Œä»¥ç¡®ä¿æ¢å¤çš„ç‰¹å¾ç¬¦åˆé«˜æ–¯åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å¯¼äº†æ‰€æå‡ºç³»ç»Ÿçš„æŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡ç»¼åˆå®éªŒè¯„ä¼°äº†å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨åƒç´ çº§æŒ‡æ ‡ï¼ˆå¦‚å³°å€¼ä¿¡å™ªæ¯”ï¼‰å’Œè¯­ä¹‰æŒ‡æ ‡ï¼ˆå¦‚å­¦ä¹ æ„ŸçŸ¥å›¾åƒè¡¥ä¸ç›¸ä¼¼æ€§ï¼‰æ–¹é¢ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæœ‰æ˜¾è‘—æ”¹è¿›ã€‚ä¸æ·±åº¦è”åˆæºä¿¡é“ç¼–ç ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å‹ç¼©ç‡å’Œä¿¡å™ªæ¯”æ–¹é¢è·å¾—äº†æ›´æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/import-sudo/Diffusion-Driven-Semantic-Communication%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/import-sudo/Diffusion-Driven-Semantic-Communicationä¸Šå‘å¸ƒäº†ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18468v4">PDF</a> accepted to IEEE for possible publication</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹ç»“åˆè¯­ä¹‰é€šä¿¡ï¼Œç”¨äºå»å™ªã€æ•°æ®é‡å»ºå’Œå†…å®¹ç”Ÿæˆç­‰ä»»åŠ¡ã€‚é’ˆå¯¹ç°æœ‰æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨å¸¦å®½é™åˆ¶æ–¹é¢çš„åº”ç”¨ç“¶é¢ˆï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºå˜åˆ†è‡ªç¼–ç å™¨å‹ç¼©çš„æ‰©æ•£é©±åŠ¨è¯­ä¹‰é€šä¿¡æ¡†æ¶ã€‚è®¾è®¡æ¶æ„åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ— çº¿ä¿¡é“ä¼ è¾“ä¿¡å·çš„è¿‡ç¨‹ä½œä¸ºæ‰©æ•£ä¸­çš„æ­£å‘è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥ä¸‹é‡‡æ ·æ¨¡å—å’Œé…å¯¹ä¸Šé‡‡æ ·æ¨¡å—ï¼Œå‡å°‘å¸¦å®½è¦æ±‚ï¼Œå¹¶åœ¨æ¥æ”¶å™¨å¤„é‡‡ç”¨é‡å‚æ•°åŒ–ç¡®ä¿æ¢å¤çš„ç‰¹å¾ç¬¦åˆé«˜æ–¯åˆ†å¸ƒã€‚å®éªŒç»“æœåœ¨åƒç´ çº§å’Œè¯­ä¹‰åº¦é‡ä¸Šéƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰ä¸­å¹¿æ³›åº”ç”¨ï¼Œå…·æœ‰å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä¸è¯­ä¹‰é€šä¿¡ç»“åˆï¼Œç”¨äºå»å™ªã€æ•°æ®é‡å»ºå’Œå†…å®¹ç”Ÿæˆã€‚</li>
<li>ç°æœ‰æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨æ— çº¿é€šä¿¡ä¸­é¢ä¸´å¸¦å®½é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå˜åˆ†è‡ªç¼–ç å™¨å‹ç¼©çš„æ‰©æ•£é©±åŠ¨è¯­ä¹‰é€šä¿¡æ¡†æ¶ï¼Œä»¥é€‚åº”å¸¦å®½é™åˆ¶ã€‚</li>
<li>è®¾è®¡æ¶æ„åˆ©ç”¨æ— çº¿ä¿¡é“ä¼ è¾“ä¿¡å·çš„æ‰©æ•£æ¨¡å‹ï¼Œå¼•å…¥ä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·æ¨¡å—ä»¥å‡å°‘å¸¦å®½éœ€æ±‚ã€‚</li>
<li>é€šè¿‡å®éªŒï¼Œè¯¥æ¡†æ¶åœ¨åƒç´ çº§åº¦é‡ï¼ˆå¦‚å³°å€¼ä¿¡å™ªæ¯”PSNRï¼‰å’Œè¯­ä¹‰åº¦é‡ï¼ˆå¦‚å­¦ä¹ æ„ŸçŸ¥å›¾åƒè¡¥ä¸ç›¸ä¼¼æ€§LPIPSï¼‰ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb88b87733df7f31f2015be883b0d407.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97056f0b43e006e54b2c0c0cf8a8659f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f35745e2f1cbe47b1b629f5ebb90a1bb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning"><a href="#QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning" class="headerlink" title="QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning"></a>QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning</h2><p><strong>Authors:Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Junchi Yan, Yan Yan</strong></p>
<p>The practical deployment of diffusion models is still hindered by the high memory and computational overhead. Although quantization paves a way for model compression and acceleration, existing methods face challenges in achieving low-bit quantization efficiently. In this paper, we identify imbalanced activation distributions as a primary source of quantization difficulty, and propose to adjust these distributions through weight finetuning to be more quantization-friendly. We provide both theoretical and empirical evidence supporting finetuning as a practical and reliable solution. Building on this approach, we further distinguish two critical types of quantized layers: those responsible for retaining essential temporal information and those particularly sensitive to bit-width reduction. By selectively finetuning these layers under both local and global supervision, we mitigate performance degradation while enhancing quantization efficiency. Our method demonstrates its efficacy across three high-resolution image generation tasks, obtaining state-of-the-art performance across multiple bit-width settings. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å®é™…åº”ç”¨ä»ç„¶å—åˆ°é«˜å†…å­˜å’Œè®¡ç®—å¼€é”€çš„é˜»ç¢ã€‚å°½ç®¡é‡åŒ–å¯ä»¥ä¸ºæ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿé“ºå¹³é“è·¯ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å®ç°ä½ä½é‡åŒ–æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¿€æ´»åˆ†å¸ƒä¸å¹³è¡¡ä½œä¸ºé‡åŒ–çš„ä¸»è¦å›°éš¾æ¥æºï¼Œå¹¶æå‡ºé€šè¿‡æƒé‡å¾®è°ƒè°ƒæ•´è¿™äº›åˆ†å¸ƒï¼Œä½¿å…¶æ›´åˆ©äºé‡åŒ–ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºå’Œå®è¯è¯æ®ï¼Œæ”¯æŒå¾®è°ƒä½œä¸ºä¸€ç§å®ç”¨ä¸”å¯é çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åŒºåˆ†äº†ä¸¤ç§å…³é”®çš„é‡åŒ–å±‚ï¼šé‚£äº›è´Ÿè´£ä¿ç•™é‡è¦æ—¶é—´ä¿¡æ¯çš„å±‚å’Œé‚£äº›å¯¹ä½å®½å‡å°‘ç‰¹åˆ«æ•æ„Ÿçš„å±‚ã€‚é€šè¿‡å±€éƒ¨å’Œå…¨å±€ç›‘ç£ä¸‹æœ‰é€‰æ‹©åœ°å¾®è°ƒè¿™äº›å±‚ï¼Œæˆ‘ä»¬åœ¨æé«˜é‡åŒ–æ•ˆç‡çš„åŒæ—¶ç¼“è§£äº†æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å¤šä¸ªä½å®½è®¾ç½®ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.03666v5">PDF</a> ICCV 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/hatchetProject/QuEST">https://github.com/hatchetProject/QuEST</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­é‡åˆ°çš„å†…å­˜å’Œè®¡ç®—å¼€é”€é—®é¢˜ã€‚é’ˆå¯¹é‡åŒ–å‹ç¼©å’ŒåŠ é€Ÿçš„éœ€æ±‚ï¼Œæœ¬æ–‡è¯†åˆ«å‡ºæ¿€æ´»åˆ†å¸ƒä¸å¹³è¡¡æ˜¯é‡åŒ–å›°éš¾çš„ä¸»è¦åŸå› ï¼Œå¹¶æå‡ºé€šè¿‡æƒé‡å¾®è°ƒæ¥è°ƒæ•´è¿™äº›åˆ†å¸ƒï¼Œä½¿å…¶æ›´åˆ©äºé‡åŒ–ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜åŒºåˆ†äº†ä¸¤ç§å…³é”®çš„é‡åŒ–å±‚ç±»å‹ï¼Œå¹¶é€šè¿‡å±€éƒ¨å’Œå…¨å±€ç›‘ç£ä¸‹çš„é€‰æ‹©æ€§å¾®è°ƒæ¥å¢å¼ºé‡åŒ–æ•ˆç‡ï¼ŒåŒæ—¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´é«˜å†…å­˜å’Œè®¡ç®—å¼€é”€çš„æŒ‘æˆ˜ã€‚</li>
<li>é‡åŒ–æ˜¯æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿçš„ä¸€ç§å¯è¡Œæ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å®ç°ä½ä½é‡åŒ–æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ¿€æ´»åˆ†å¸ƒä¸å¹³è¡¡è¢«è¯†åˆ«ä¸ºé‡åŒ–çš„ä¸»è¦å›°éš¾ã€‚</li>
<li>é€šè¿‡æƒé‡å¾®è°ƒè°ƒæ•´æ¿€æ´»åˆ†å¸ƒï¼Œä½¿å…¶æ›´åˆ©äºé‡åŒ–ã€‚</li>
<li>åŒºåˆ†äº†ä¸¤ç§å…³é”®çš„é‡åŒ–å±‚ç±»å‹ï¼šä¿ç•™é‡è¦æ—¶é—´ä¿¡æ¯çš„å±‚å’Œå¯¹æ¯”ç‰¹å®½åº¦å‡å°‘æ•æ„Ÿçš„å±‚ã€‚</li>
<li>é€šè¿‡å±€éƒ¨å’Œå…¨å±€ç›‘ç£ä¸‹çš„é€‰æ‹©æ€§å¾®è°ƒï¼Œç¼“è§£äº†æ€§èƒ½ä¸‹é™ï¼Œæé«˜äº†é‡åŒ–æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.03666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee314fdf808d5701b34ee0d0d20268b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f838e90ed63bb3599ceacd153b10ac8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f155e65b9c876b0acb10a9d79b6c802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dfdc538b35f7928c0ed3460b91d77af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f93b8cfb10211e4ed36e2345eac31f6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2708e2891246883ca60f3bd7ecea99b9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-12/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c77d2f231bcd03bfb13565de40981da4.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  PWD Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8daa22cddaaa53d9a573a5a13956572a.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  MUVOD A Novel Multi-view Video Object Segmentation Dataset and A   Benchmark for 3D Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
