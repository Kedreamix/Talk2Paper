<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  Baryonification II Constraining feedback with X-ray and kinematic   Sunyaev-Zel&#39;dovich observations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-49d6141374562c9add2c28504fbb9994.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-12-æ›´æ–°"><a href="#2025-07-12-æ›´æ–°" class="headerlink" title="2025-07-12 æ›´æ–°"></a>2025-07-12 æ›´æ–°</h1><h2 id="Baryonification-II-Constraining-feedback-with-X-ray-and-kinematic-Sunyaev-Zelâ€™dovich-observations"><a href="#Baryonification-II-Constraining-feedback-with-X-ray-and-kinematic-Sunyaev-Zelâ€™dovich-observations" class="headerlink" title="Baryonification II: Constraining feedback with X-ray and kinematic   Sunyaev-Zelâ€™dovich observations"></a>Baryonification II: Constraining feedback with X-ray and kinematic   Sunyaev-Zelâ€™dovich observations</h2><p><strong>Authors:Michael KovaÄ, Andrina Nicola, Jozef Bucko, Aurel Schneider, Robert Reischke, Sambit K. Giri, Romain Teyssier, Matthieu Schaller, Joop Schaye</strong></p>
<p>Baryonic feedback alters the matter distribution on small and intermediate scales, posing a challenge for precision cosmology. The new, component-wise baryonification (BFC) approach provides a self-consistent framework to model feedback effects for different observables. In this paper we use this framework to fit kinematic Sunyaev-Zelâ€™dovich (kSZ) observations from the Atacama Cosmology Telescope (ACT) alongside halo X-ray gas fractions from eROSITA, investigating baryonic feedback in a cosmological context. We first show that the kSZ data from ACT is consistent with the gas fractions from eROSITA, both suggesting a feedback model that is stronger than what is assumed in most hydrodynamical simulations. This finding is in contrast to older, pre-eROSITA gas fraction measurements that point towards weaker feedback in tension with the kSZ results. We suspect these discrepancies to be due to selection bias in the pre-eROSITA sample, or differences in halo mass estimation between the two data sets. In a further step, we use the BFC model to predict the baryonic suppression of the matter power spectrum. Based on our combined fit to data from ACT and eROSITA, we find a power spectrum suppression that exceeds the percent-level at modes above $k&#x3D;0.3-0.6 ,h,\mathrm{Mpc}^{-1}$, growing to 2-8 percent at $k&#x3D;1,h,\mathrm{Mpc}^{-1}$, and to 20-25 percent at $k&#x3D;5,h,\mathrm{Mpc}^{-1}$, consistent with strong-feedback hydrodynamical simulations. Finally, we compare our best-fitting model to the observed gas density and pressure profiles of massive galaxy clusters from the X-COP sample, finding excellent agreement. These results show that BFC provides a self-consistent picture of feedback across mass- and length scales as well as different cosmological observables, thus making it promising for applications to multiwavelength studies to jointly constrain cosmology and baryonic effects. </p>
<blockquote>
<p>é‡å­åé¦ˆæ”¹å˜äº†å°å°ºåº¦å’Œä¸­å°ºåº¦çš„ç‰©è´¨åˆ†å¸ƒï¼Œå¯¹ç²¾ç¡®å®‡å®™å­¦æå‡ºäº†æŒ‘æˆ˜ã€‚æ–°çš„é€ä¸ªé‡å­åŒ–ï¼ˆBFCï¼‰æ–¹æ³•ä¸ºä¸åŒçš„è§‚æµ‹å¯¹è±¡æä¾›äº†æ¨¡æ‹Ÿåé¦ˆæ•ˆåº”çš„è‡ªæ´½æ¡†æ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸€æ¡†æ¶æ¥æ‹Ÿåˆæ¥è‡ªé˜¿å¡”å¡é©¬å®‡å®™å­¦æœ›è¿œé•œï¼ˆACTï¼‰çš„åŠ¨åŠ›å­¦Sunyaev-Zelâ€™dovichï¼ˆkSZï¼‰è§‚æµ‹ä»¥åŠæ¥è‡ªeROSITAçš„æ˜Ÿç³»å›¢Xå°„çº¿æ°”ä½“åˆ†æ•°ï¼Œå¹¶åœ¨å®‡å®™å­¦çš„èƒŒæ™¯ä¸‹ç ”ç©¶é‡å­åé¦ˆã€‚æˆ‘ä»¬é¦–å…ˆè¡¨æ˜ï¼ŒACTçš„kSZæ•°æ®ä¸eROSITAçš„æ°”ä½“åˆ†æ•°æ˜¯ä¸€è‡´çš„ï¼Œä¸¤è€…éƒ½æš—ç¤ºåé¦ˆæ¨¡å‹æ¯”å¤§å¤šæ•°æµä½“åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ‰€å‡è®¾çš„è¦å¼ºã€‚è¿™ä¸€å‘ç°ä¸æ—§çš„ã€pre-eROSITAæ°”ä½“åˆ†æ•°æµ‹é‡ç»“æœç›¸åï¼Œåè€…æŒ‡å‘è¾ƒå¼±çš„åé¦ˆä¸kSZç»“æœå­˜åœ¨å¼ åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›å·®å¼‚å¯èƒ½æ˜¯ç”±äºpre-eROSITAæ ·æœ¬çš„é€‰æ‹©åè§ï¼Œæˆ–æ˜¯ä¸¤ç»„æ•°æ®ä¹‹é—´æ˜Ÿç³»å›¢è´¨é‡ä¼°è®¡çš„å·®å¼‚ã€‚è¿›ä¸€æ­¥åœ°ï¼Œæˆ‘ä»¬ä½¿ç”¨BFCæ¨¡å‹æ¥é¢„æµ‹ç‰©è´¨åŠŸç‡è°±çš„é‡å­æŠ‘åˆ¶ã€‚åŸºäºæˆ‘ä»¬å¯¹ACTå’ŒeROSITAæ•°æ®çš„ç»¼åˆæ‹Ÿåˆï¼Œæˆ‘ä»¬å‘ç°åŠŸç‡è°±æŠ‘åˆ¶åœ¨æ¨¡å¼é«˜äº$ k&#x3D;0.3-0.6 ,h,\mathrm{Mpc}^{-1} $æ—¶è¶…è¿‡äº†ç™¾åˆ†ä¹‹ä¸€æ°´å¹³ï¼Œåœ¨$ k&#x3D;1 ,h,\mathrm{Mpc}^{-1} $æ—¶å¢é•¿åˆ°2-8%ï¼Œåœ¨$ k&#x3D;5 ,h,\mathrm{Mpc}^{-1} $æ—¶è¾¾åˆ°20-25%ï¼Œè¿™ä¸å¼ºåé¦ˆæµä½“åŠ¨åŠ›å­¦æ¨¡æ‹Ÿç›¸ä¸€è‡´ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æœ€ä½³æ‹Ÿåˆæ¨¡å‹ä¸X-COPæ ·æœ¬ä¸­å¤§è§„æ¨¡æ˜Ÿç³»å›¢è§‚æµ‹åˆ°çš„æ°”ä½“å¯†åº¦å’Œå‹åŠ›åˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒï¼Œå‘ç°å»åˆå¾—å¾ˆå¥½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒBFCæä¾›äº†è·¨è´¨é‡å’Œé•¿åº¦å°ºåº¦ä»¥åŠä¸åŒå®‡å®™å­¦è§‚æµ‹å€¼çš„è‡ªæ´½åé¦ˆå›¾åƒï¼Œå› æ­¤å°†å…¶åº”ç”¨äºå¤šæ³¢é•¿ç ”ç©¶ä»¥å…±åŒçº¦æŸå®‡å®™å­¦å’Œé‡å­æ•ˆåº”æ–¹é¢å‰æ™¯å¹¿é˜”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07991v1">PDF</a> 40 pages, 12 figures, 2 tables, to be submitted to JCAP</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–°çš„æˆåˆ†çº§é‡å­åŒ–ï¼ˆBFCï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸ºä¸åŒçš„è§‚æµ‹å€¼æä¾›äº†æ¨¡æ‹Ÿåé¦ˆæ•ˆåº”çš„è‡ªæ´½æ¡†æ¶ã€‚ç ”ç©¶ä¸­åˆ©ç”¨æ­¤æ–¹æ³•æ‹ŸåˆAtacamaå®‡å®™å­¦æœ›è¿œé•œçš„åŠ¨æ€Sunyaev-Zelâ€™dovichï¼ˆkSZï¼‰è§‚æµ‹ä¸eROSITAçš„æ˜Ÿç³»æ™•Xå°„çº¿æ°”ä½“åˆ†æ•°æ•°æ®ï¼Œç ”ç©¶å®‡å®™å­¦èƒŒæ™¯ä¸‹çš„é‡å­åé¦ˆã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºkSZæ•°æ®ä¸eROSITAæ°”ä½“åˆ†æ•°æ•°æ®ä¸€è‡´ï¼Œä¸¤è€…å‡æ”¯æŒä¸€ç§æ¯”å¤§å¤šæ•°æµä½“åŠ¨åŠ›å­¦æ¨¡æ‹Ÿä¸­å‡è®¾çš„æ›´å¼ºçƒˆçš„åé¦ˆæ¨¡å‹ã€‚ç»“åˆBFCæ¨¡å‹é¢„æµ‹ç‰©è´¨åŠŸç‡è°±çš„æŠ‘åˆ¶æƒ…å†µï¼Œå‘ç°åœ¨ä¸€å®šæ¨¡å¼ä¸‹æŠ‘åˆ¶è¶…è¿‡ç™¾åˆ†ä¹‹ä¸€ï¼Œæœ€é«˜å¯è¾¾ç™¾åˆ†ä¹‹äºŒåäº”ä»¥ä¸Šã€‚æœ€åï¼Œå°†æœ€ä½³æ‹Ÿåˆæ¨¡å‹ä¸X-COPæ ·æœ¬è§‚å¯Ÿåˆ°çš„æ°”ä½“å¯†åº¦å’Œå‹åŠ›åˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒï¼Œç»“æœå»åˆè‰¯å¥½ã€‚è¿™æ˜¾ç¤ºå‡ºBFCæ–¹æ³•åœ¨ä¸åŒè´¨é‡å’Œé•¿åº¦å°ºåº¦ä»¥åŠä¸åŒçš„å®‡å®™å­¦è§‚æµ‹å€¼ä¸Šæä¾›äº†è‡ªæ´½çš„åé¦ˆå›¾åƒï¼Œå› æ­¤æœ‰æœ›åº”ç”¨äºå¤šæ³¢é•¿ç ”ç©¶ï¼Œå…±åŒçº¦æŸå®‡å®™å­¦å’Œé‡å­æ•ˆåº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»æ–°çš„æˆåˆ†çº§é‡å­åŒ–ï¼ˆBFCï¼‰æ–¹æ³•ä½œä¸ºæ¨¡æ‹Ÿä¸åŒè§‚æµ‹å€¼çš„åé¦ˆæ•ˆåº”çš„è‡ªæ´½æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨Atacamaå®‡å®™å­¦æœ›è¿œé•œçš„kSZè§‚æµ‹æ•°æ®å’ŒeROSITAçš„æ˜Ÿç³»æ™•Xå°„çº¿æ°”ä½“åˆ†æ•°æ•°æ®è¿›è¡Œç ”ç©¶ã€‚</li>
<li>kSZæ•°æ®å’ŒeROSITAæ•°æ®å‡æŒ‡å‘ä¸€ç§æ¯”å¤§å¤šæ•°æµä½“åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ›´å¼ºçƒˆçš„åé¦ˆæ¨¡å‹ã€‚</li>
<li>BFCæ¨¡å‹é¢„æµ‹ç‰©è´¨åŠŸç‡è°±çš„æŠ‘åˆ¶ç¨‹åº¦è¶…è¿‡ç™¾åˆ†ä¹‹ä¸€ï¼Œæœ€é«˜å¯è¾¾ç™¾åˆ†ä¹‹äºŒåäº”ä»¥ä¸Šã€‚</li>
<li>ç ”ç©¶ä¸­å‘ç°æ•°æ®ä¸æ¨¡æ‹Ÿä¹‹é—´çš„ä¸ä¸€è‡´å¯èƒ½æ˜¯ç”±äºæ ·æœ¬é€‰æ‹©åè§æˆ–æ•°æ®é›†åˆä¹‹é—´åœ¨æ˜Ÿç³»æ™•è´¨é‡ä¼°è®¡ä¸Šçš„å·®å¼‚é€ æˆçš„ã€‚</li>
<li>æœ€ä½³æ‹Ÿåˆæ¨¡å‹ä¸X-COPæ ·æœ¬è§‚æµ‹åˆ°çš„æ°”ä½“å¯†åº¦å’Œå‹åŠ›åˆ†å¸ƒæ¯”è¾ƒå»åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3b5e8bfbb9c02abc5b649856d1f89395.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Rethinking-Query-based-Transformer-for-Continual-Image-Segmentation"><a href="#Rethinking-Query-based-Transformer-for-Continual-Image-Segmentation" class="headerlink" title="Rethinking Query-based Transformer for Continual Image Segmentation"></a>Rethinking Query-based Transformer for Continual Image Segmentation</h2><p><strong>Authors:Yuchen Zhu, Cheng Shi, Dingyou Wang, Jiajin Tang, Zhengxuan Wei, Yu Wu, Guanbin Li, Sibei Yang</strong></p>
<p>Class-incremental&#x2F;Continual image segmentation (CIS) aims to train an image segmenter in stages, where the set of available categories differs at each stage. To leverage the built-in objectness of query-based transformers, which mitigates catastrophic forgetting of mask proposals, current methods often decouple mask generation from the continual learning process. This study, however, identifies two key issues with decoupled frameworks: loss of plasticity and heavy reliance on input data order. To address these, we conduct an in-depth investigation of the built-in objectness and find that highly aggregated image features provide a shortcut for queries to generate masks through simple feature alignment. Based on this, we propose SimCIS, a simple yet powerful baseline for CIS. Its core idea is to directly select image features for query assignment, ensuring â€œperfect alignmentâ€ to preserve objectness, while simultaneously allowing queries to select new classes to promote plasticity. To further combat catastrophic forgetting of categories, we introduce cross-stage consistency in selection and an innovative â€œvisual queryâ€-based replay mechanism. Experiments demonstrate that SimCIS consistently outperforms state-of-the-art methods across various segmentation tasks, settings, splits, and input data orders. All models and codes will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/SooLab/SimCIS">https://github.com/SooLab/SimCIS</a>. </p>
<blockquote>
<p>ç±»å¢é‡&#x2F;æŒç»­å›¾åƒåˆ†å‰²ï¼ˆCISï¼‰æ—¨åœ¨åˆ†é˜¶æ®µè®­ç»ƒå›¾åƒåˆ†å‰²å™¨ï¼Œæ¯ä¸ªé˜¶æ®µçš„å¯ç”¨ç±»åˆ«é›†å„ä¸ç›¸åŒã€‚ä¸ºäº†åˆ©ç”¨åŸºäºæŸ¥è¯¢çš„å˜å‹å™¨ï¼ˆtransformerï¼‰çš„å†…ç½®å¯¹è±¡æ€§ï¼Œå‡è½»æ©è†œææ¡ˆçš„ç¾éš¾æ€§é—å¿˜ï¼Œå½“å‰æ–¹æ³•é€šå¸¸å°†æ©è†œç”Ÿæˆä¸æŒç»­å­¦ä¹ è¿‡ç¨‹è§£è€¦ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶å‘ç°äº†åŸºäºè§£è€¦æ¡†æ¶çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šå¯å¡‘æ€§ä¸§å¤±å’Œå¯¹è¾“å…¥æ•°æ®é¡ºåºçš„ä¸¥é‡ä¾èµ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹å†…ç½®çš„å¯¹è±¡æ€§è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶å‘ç°é«˜åº¦èšåˆçš„å›¾åƒç‰¹å¾ä¸ºæŸ¥è¯¢ç”Ÿæˆæ©è†œæä¾›äº†é€šè¿‡ç®€å•ç‰¹å¾å¯¹é½çš„æ·å¾„ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SimCISï¼Œè¿™æ˜¯CISçš„ç®€å•è€Œå¼ºå¤§çš„åŸºçº¿ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç›´æ¥é€‰æ‹©å›¾åƒç‰¹å¾è¿›è¡Œåˆ†é…æŸ¥è¯¢çš„ä»»åŠ¡ï¼Œç¡®ä¿ä¸å¯¹è±¡æ€§çš„â€œå®Œç¾å¯¹é½â€ï¼ŒåŒæ—¶å…è®¸æŸ¥è¯¢é€‰æ‹©æ–°ç±»åˆ«ä»¥ä¿ƒè¿›å¯å¡‘æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥å…‹æœç±»åˆ«çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨é˜¶æ®µçš„é€‰å‹ä¸€è‡´æ€§ä»¥åŠåŸºäºâ€œè§†è§‰æŸ¥è¯¢â€çš„å›æ”¾æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§åˆ†å‰²ä»»åŠ¡ã€è®¾ç½®ã€åˆ†å‰²å’Œè¾“å…¥æ•°æ®é¡ºåºä¸‹ï¼ŒSimCISå§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æ‰€æœ‰æ¨¡å‹å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/SooLab/SimCIS%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/SooLab/SimCISå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07831v1">PDF</a> This work is accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºæŸ¥è¯¢çš„å¢é‡å›¾åƒåˆ†å‰²ï¼ˆCISï¼‰æŠ€æœ¯ï¼ŒæŒ‡å‡ºå½“å‰æ–¹æ³•å­˜åœ¨çš„ä¸¤ä¸ªé—®é¢˜å¹¶è¿›è¡Œäº†æ”¹è¿›ã€‚é€šè¿‡æ·±å…¥åˆ†æå†…ç½®å¯¹è±¡æ€§ï¼Œå‘ç°é«˜åº¦èšåˆçš„å›¾åƒç‰¹å¾å¯ä½œä¸ºæŸ¥è¯¢ç”Ÿæˆæ©ç çš„æ·å¾„ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SimCISæ–¹æ¡ˆï¼Œé€šè¿‡ç›´æ¥é€‰æ‹©å›¾åƒç‰¹å¾è¿›è¡Œåˆ†é…æ¥ä¿ç•™å¯¹è±¡æ€§å¹¶ä¿ƒè¿›å¯å¡‘æ€§ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è·¨é˜¶æ®µä¸€è‡´æ€§é€‰æ‹©å’ŒåŸºäºè§†è§‰æŸ¥è¯¢çš„å›æ”¾æœºåˆ¶æ¥å¯¹æŠ—ç±»åˆ«é—å¿˜é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSimCISåœ¨å„ç§åˆ†å‰²ä»»åŠ¡ã€è®¾ç½®ã€åˆ†å‰²å’Œè¾“å…¥æ•°æ®é¡ºåºä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¢é‡å›¾åƒåˆ†å‰²æ–¹æ³•å­˜åœ¨å¤±å»çµæ´»æ€§å’Œä¾èµ–è¾“å…¥æ•°æ®é¡ºåºçš„é—®é¢˜ã€‚</li>
<li>é«˜åº¦èšåˆçš„å›¾åƒç‰¹å¾å¯ä»¥ä½œä¸ºæŸ¥è¯¢ç”Ÿæˆæ©ç çš„æ·å¾„ã€‚</li>
<li>SimCISé€šè¿‡ç›´æ¥é€‰æ‹©å›¾åƒç‰¹å¾è¿›è¡Œåˆ†é…æ¥ä¿ç•™å¯¹è±¡æ€§å¹¶ä¿ƒè¿›å¯å¡‘æ€§ã€‚</li>
<li>SimCISå¼•å…¥äº†è·¨é˜¶æ®µä¸€è‡´æ€§é€‰æ‹©æ¥å¯¹æŠ—ç±»åˆ«é—å¿˜é—®é¢˜ã€‚</li>
<li>SimCISé‡‡ç”¨åŸºäºè§†è§‰æŸ¥è¯¢çš„å›æ”¾æœºåˆ¶æ¥å¢å¼ºå­¦ä¹ æ•ˆæœã€‚</li>
<li>å®éªŒè¡¨æ˜SimCISåœ¨å„ç§åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e7bd15557a5054fa762d8c1fcfd478b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8ab8f88a673d69e5c31637c837fe64e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e395f6c653a85269358e471bf1be0a65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b068ab83be976fe61e66120de2d15e84.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Adaptive-Attention-Residual-U-Net-for-curvilinear-structure-segmentation-in-fluorescence-microscopy-and-biomedical-images"><a href="#Adaptive-Attention-Residual-U-Net-for-curvilinear-structure-segmentation-in-fluorescence-microscopy-and-biomedical-images" class="headerlink" title="Adaptive Attention Residual U-Net for curvilinear structure segmentation   in fluorescence microscopy and biomedical images"></a>Adaptive Attention Residual U-Net for curvilinear structure segmentation   in fluorescence microscopy and biomedical images</h2><p><strong>Authors:Achraf Ait Laydi, Louis Cueff, Mewen Crespo, Yousef El Mourabit, HÃ©lÃ¨ne Bouvrais</strong></p>
<p>Segmenting curvilinear structures in fluorescence microscopy remains a challenging task, particularly under noisy conditions and in dense filament networks commonly seen in vivo. To address this, we created two original datasets consisting of hundreds of synthetic images of fluorescently labelled microtubules within cells. These datasets are precisely annotated and closely mimic real microscopy images, including realistic noise. The second dataset presents an additional challenge, by simulating varying fluorescence intensities along filaments that complicate segmentation. While deep learning has shown strong potential in biomedical image analysis, its performance often declines in noisy or low-contrast conditions. To overcome this limitation, we developed a novel advanced architecture: the Adaptive Squeeze-and-Excitation Residual U-Net (ASE_Res_UNet). This model enhanced the standard U-Net by integrating residual blocks in the encoder and adaptive SE attention mechanisms in the decoder. Through ablation studies and comprehensive visual and quantitative evaluations, ASE_Res_UNet consistently outperformed its variants, namely standard U-Net, ASE_UNet and Res_UNet architectures. These improvements, particularly in noise resilience and detecting fine, low-intensity structures, were largely attributed to the adaptive SE attention module that we created. We further benchmarked ASE_Res_UNet against various state-of-the-art models, and found it achieved superior performance on our most challenging dataset. Finally, the model also generalized well to real microscopy images of stained microtubules as well as to other curvilinear structures. Indeed, it successfully segmented retinal blood vessels and nerves in noisy or low-contrast biomedical images, demonstrating its strong potential for applications in disease diagnosis and treatment. </p>
<blockquote>
<p>åœ¨è§å…‰æ˜¾å¾®é•œä¸‹å¯¹æ›²çº¿ç»“æ„è¿›è¡Œåˆ†å‰²ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°æ¡ä»¶ä¸‹å’Œåœ¨ä½“å†…å¸¸è§çš„å¯†é›†çº¤ç»´ç½‘ç»œä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªåŸå§‹æ•°æ®é›†ï¼ŒåŒ…å«æ•°ç™¾å¼ åˆæˆç»†èƒå›¾åƒä¸­çš„è§å…‰æ ‡è®°å¾®ç®¡å›¾åƒã€‚è¿™äº›æ•°æ®é›†ç»è¿‡äº†ç²¾ç¡®æ ‡æ³¨ï¼Œå¹¶æ¨¡æ‹Ÿäº†çœŸå®çš„æ˜¾å¾®é•œå›¾åƒï¼ŒåŒ…æ‹¬çœŸå®å™ªå£°ã€‚ç¬¬äºŒä¸ªæ•°æ®é›†é€šè¿‡æ¨¡æ‹Ÿçº¤ç»´ä¸­ä¸åŒçš„è§å…‰å¼ºåº¦ï¼Œå¢åŠ äº†é¢å¤–çš„æŒ‘æˆ˜ï¼Œè¿™ä½¿å¾—åˆ†å‰²å¤æ‚åŒ–ã€‚æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œä½†åœ¨å™ªå£°æˆ–ä½å¯¹æ¯”åº¦æ¡ä»¶ä¸‹ï¼Œå…¶æ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹é«˜çº§æ¶æ„ï¼šè‡ªé€‚åº”å‹ç¼©ä¸æ¿€å‘æ®‹å·®U-Netï¼ˆASE_Res_UNetï¼‰ã€‚æ­¤æ¨¡å‹é€šè¿‡é›†æˆç¼–ç å™¨ä¸­çš„æ®‹å·®å—å’Œè§£ç å™¨ä¸­çš„è‡ªé€‚åº”SEæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¢å¼ºäº†æ ‡å‡†U-Netçš„åŠŸèƒ½ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ä»¥åŠå…¨é¢çš„è§†è§‰å’Œå®šé‡è¯„ä¼°ï¼ŒASE_Res_UNetå§‹ç»ˆä¼˜äºå…¶å˜ä½“ï¼Œå³æ ‡å‡†U-Netã€ASE_UNetå’ŒRes_UNetæ¶æ„ã€‚è¿™äº›æ”¹è¿›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½’åŠŸäºæˆ‘ä»¬åˆ›å»ºçš„è‡ªé€‚åº”SEæ³¨æ„åŠ›æ¨¡å—ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°éŸ§æ€§å’Œæ£€æµ‹ç»†å¾®ã€ä½å¼ºåº¦ç»“æ„æ–¹é¢ã€‚æˆ‘ä»¬å°†ASE_Res_UNetä¸å„ç§æœ€æ–°æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°å®ƒåœ¨æˆ‘ä»¬çš„æœ€å…·æŒ‘æˆ˜çš„æ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚æœ€åï¼Œè¯¥æ¨¡å‹ä¹Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°äº†çœŸå®çš„æ˜¾å¾®é•œå›¾åƒæŸ“è‰²å¾®ç®¡å’Œå…¶ä»–çš„æ›²çº¿ç»“æ„ä¸Šã€‚äº‹å®ä¸Šï¼Œå®ƒèƒ½å¤ŸæˆåŠŸåˆ†å‰²å˜ˆæ‚æˆ–ä½å¯¹æ¯”åº¦çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­çš„è§†ç½‘è†œè¡€ç®¡å’Œç¥ç»ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—åº”ç”¨ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§å…‰æ˜¾å¾®é•œä¸‹ç»†èƒå†…å¾®ç®¡è›‹ç™½çš„åˆ†å‰²é—®é¢˜ï¼Œåˆ›å»ºçš„ä¸¤ä¸ªåˆæˆå›¾åƒæ•°æ®é›†åŠæå‡ºçš„è‡ªé€‚åº”æŒ¤å‹ä¸æ¿€åŠ±æ®‹å·®U-Netæ¨¡å‹ï¼ˆASE_Res_UNetï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡é›†æˆæ®‹å·®å—å’Œè‡ªé€‚åº”SEæ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†æ ‡å‡†U-Netçš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒASE_Res_UNetåœ¨å™ªå£°å¤„ç†å’Œä½å¯¹æ¯”åº¦ç»“æ„æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡æ‹Ÿä¸åŒè§å…‰å¼ºåº¦çš„å¾®ç®¡è›‹ç™½åˆ†å‰²é—®é¢˜ä¸Šã€‚è¯¥æ¨¡å‹è¿˜å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤ŸæˆåŠŸåº”ç”¨äºçœŸå®æ˜¾å¾®é•œå›¾åƒä¸­çš„å¾®ç®¡è›‹ç™½ä»¥åŠå…¶ä»–æ›²ç‡ç»“æ„ï¼Œå¦‚è§†ç½‘è†œè¡€ç®¡å’Œç¥ç»çš„åˆ†å‰²ï¼Œå…·æœ‰ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—åº”ç”¨çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹è§å…‰æ˜¾å¾®é•œä¸‹ç»†èƒå†…å¾®ç®¡è›‹ç™½çš„åˆ†å‰²é—®é¢˜ï¼Œåˆ›å»ºäº†ä¸¤ä¸ªåˆæˆå›¾åƒæ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†æ¨¡æ‹ŸçœŸå®æ˜¾å¾®é•œå›¾åƒï¼ŒåŒ…æ‹¬å™ªå£°å’Œä¸åŒçš„è§å…‰å¼ºåº¦ã€‚</li>
<li>æå‡ºäº†è‡ªé€‚åº”æŒ¤å‹ä¸æ¿€åŠ±æ®‹å·®U-Netæ¨¡å‹ï¼ˆASE_Res_UNetï¼‰ï¼Œç»“åˆäº†æ®‹å·®å—å’Œè‡ªé€‚åº”SEæ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>ASE_Res_UNetåœ¨å™ªå£°å¤„ç†å’Œä½å¯¹æ¯”åº¦ç»“æ„æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æ¨¡æ‹Ÿä¸åŒè§å…‰å¼ºåº¦çš„å¾®ç®¡è›‹ç™½åˆ†å‰²é—®é¢˜ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</li>
<li>ASE_Res_UNetå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒæˆåŠŸåº”ç”¨äºçœŸå®æ˜¾å¾®é•œå›¾åƒä¸­çš„å¾®ç®¡è›‹ç™½ä»¥åŠå…¶ä»–æ›²ç‡ç»“æ„ï¼ˆå¦‚è§†ç½‘è†œè¡€ç®¡å’Œç¥ç»ï¼‰çš„åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d06311fd78590f31e3d08cf59dd3aafc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6a24b4cadb9f55b5acb9b445ddaf0be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-298823d0d442bd24e46222dd482a6451.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RAPS-3D-Efficient-interactive-segmentation-for-3D-radiological-imaging"><a href="#RAPS-3D-Efficient-interactive-segmentation-for-3D-radiological-imaging" class="headerlink" title="RAPS-3D: Efficient interactive segmentation for 3D radiological imaging"></a>RAPS-3D: Efficient interactive segmentation for 3D radiological imaging</h2><p><strong>Authors:ThÃ©o Danielou, Daniel Tordjman, Pierre Manceron, Corentin Dancette</strong></p>
<p>Promptable segmentation, introduced by the Segment Anything Model (SAM), is a promising approach for medical imaging, as it enables clinicians to guide and refine model predictions interactively. However, SAMâ€™s architecture is designed for 2D images and does not extend naturally to 3D volumetric data such as CT or MRI scans. Adapting 2D models to 3D typically involves autoregressive strategies, where predictions are propagated slice by slice, resulting in increased inference complexity. Processing large 3D volumes also requires significant computational resources, often leading existing 3D methods to also adopt complex strategies like sliding-window inference to manage memory usage, at the cost of longer inference times and greater implementation complexity. In this paper, we present a simplified 3D promptable segmentation method, inspired by SegVol, designed to reduce inference time and eliminate prompt management complexities associated with sliding windows while achieving state-of-the-art performance. </p>
<blockquote>
<p>ç”±Segment Anything Modelï¼ˆSAMï¼‰å¼•å…¥çš„æç¤ºåˆ†å‰²æ˜¯åŒ»å­¦æˆåƒçš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿä½¿ä¸´åºŠåŒ»ç”Ÿä»¥äº¤äº’å¼æ–¹å¼å¼•å¯¼å’Œç»†åŒ–æ¨¡å‹é¢„æµ‹ã€‚ç„¶è€Œï¼ŒSAMçš„æ¶æ„æ˜¯ä¸ºäºŒç»´å›¾åƒè®¾è®¡çš„ï¼Œå¹¶ä¸èƒ½è‡ªç„¶åœ°æ‰©å±•åˆ°ä¸‰ç»´ä½“ç§¯æ•°æ®ï¼Œå¦‚CTæˆ–MRIæ‰«æã€‚å°†äºŒç»´æ¨¡å‹é€‚åº”åˆ°ä¸‰ç»´é€šå¸¸é‡‡ç”¨è‡ªå›å½’ç­–ç•¥ï¼Œé¢„æµ‹é€å±‚ä¼ æ’­ï¼Œå¯¼è‡´æ¨ç†å¤æ‚åº¦å¢åŠ ã€‚å¤„ç†å¤§å‹ä¸‰ç»´ä½“ç§¯è¿˜éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¿™å¯¼è‡´ç°æœ‰çš„ä¸‰ç»´æ–¹æ³•ä¹Ÿé‡‡ç”¨å¤æ‚çš„ç­–ç•¥ï¼Œå¦‚æ»‘åŠ¨çª—å£æ¨ç†æ¥ç®¡ç†å†…å­˜ä½¿ç”¨ï¼Œä½†è¿™ä¼šå¢åŠ æ¨ç†æ—¶é—´å’Œå®ç°å¤æ‚åº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€åŒ–çš„ä¸‰ç»´æç¤ºåˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å—åˆ°SegVolçš„å¯å‘ï¼Œæ—¨åœ¨å‡å°‘æ¨ç†æ—¶é—´å¹¶æ¶ˆé™¤ä¸æ»‘åŠ¨çª—å£ç›¸å…³çš„æç¤ºç®¡ç†å¤æ‚æ€§ï¼ŒåŒæ—¶å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07730v1">PDF</a> Abstract accepted at MIUA 2025</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒé¢†åŸŸå¼•å…¥çš„å¯æç¤ºåˆ†å‰²æ³•ï¼Œå³Segment Anything Modelï¼ˆSAMï¼‰ï¼Œæ˜¯ä¸€ç§æœ‰æœ›ç”¨äºåŒ»å­¦æˆåƒçš„äº¤äº’å¼æ–¹æ³•ï¼Œèƒ½å¤Ÿè®©ä¸´åºŠåŒ»ç”Ÿå¯¹æ¨¡å‹é¢„æµ‹è¿›è¡Œå¼•å¯¼å’Œç²¾ç»†è°ƒæ•´ã€‚ç„¶è€Œï¼ŒSAMçš„è®¾è®¡åˆè¡·æ˜¯é’ˆå¯¹äºŒç»´å›¾åƒï¼Œå¹¶ä¸è‡ªç„¶é€‚ç”¨äºä¸‰ç»´ä½“ç§¯æ•°æ®å¦‚CTæˆ–MRIæ‰«æã€‚å°†äºŒç»´æ¨¡å‹é€‚åº”åˆ°ä¸‰ç»´é€šå¸¸é‡‡ç”¨è‡ªå›å½’ç­–ç•¥ï¼Œè¿™ç§ç­–ç•¥é€ç‰‡é¢„æµ‹å¹¶ä¼ æ’­ï¼Œå¢åŠ äº†æ¨ç†å¤æ‚æ€§ã€‚é’ˆå¯¹å¤„ç†å¤§é‡ä¸‰ç»´ä½“ç§¯æ•°æ®éœ€è¦å¤§é‡è®¡ç®—èµ„æºçš„é—®é¢˜ï¼Œç°æœ‰çš„ä¸‰ç»´æ–¹æ³•å¾€å¾€é‡‡ç”¨å¦‚æ»‘åŠ¨çª—å£æ¨ç†ç­‰å¤æ‚ç­–ç•¥ä»¥ç®¡ç†å†…å­˜ä½¿ç”¨ï¼Œä½†åŒæ—¶ä¹Ÿç‰ºç‰²äº†æ¨ç†æ—¶é—´å’Œå®æ–½å¤æ‚æ€§ã€‚æœ¬æ–‡å—SegVolå¯å‘ï¼Œæå‡ºäº†ä¸€ç§ç®€åŒ–ä¸‰ç»´å¯æç¤ºåˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘æ¨ç†æ—¶é—´å¹¶æ¶ˆé™¤ä¸æ»‘åŠ¨çª—å£ç›¸å…³çš„æç¤ºç®¡ç†å¤æ‚æ€§ï¼ŒåŒæ—¶è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMæ¨¡å‹å¼•å…¥äº†ä¸€ç§å¯æç¤ºåˆ†å‰²æ³•ç”¨äºåŒ»å­¦æˆåƒï¼Œå…è®¸äº¤äº’å¼åœ°å¼•å¯¼å’Œç²¾ç»†è°ƒæ•´æ¨¡å‹é¢„æµ‹ã€‚</li>
<li>SAMä¸»è¦é€‚ç”¨äºäºŒç»´å›¾åƒï¼Œå¯¹äºä¸‰ç»´ä½“ç§¯æ•°æ®çš„å¤„ç†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å°†äºŒç»´æ¨¡å‹åº”ç”¨äºä¸‰ç»´æ•°æ®é€šå¸¸é‡‡å–è‡ªå›å½’ç­–ç•¥ï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹å¤æ‚åŒ–ã€‚</li>
<li>å¤„ç†å¤§é‡ä¸‰ç»´ä½“ç§¯æ•°æ®éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚</li>
<li>ç°å­˜çš„ä¸‰ç»´æ–¹æ³•å¦‚æ»‘åŠ¨çª—å£æ¨ç†ï¼Œåœ¨ç®¡ç†å†…å­˜ä½¿ç”¨çš„åŒæ—¶ç‰ºç‰²äº†æ¨ç†æ—¶é—´å’Œå®æ–½å¤æ‚æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€åŒ–ä¸‰ç»´å¯æç¤ºåˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘æ¨ç†æ—¶é—´å¹¶æ¶ˆé™¤ä¸æ»‘åŠ¨çª—å£ç›¸å…³çš„æç¤ºç®¡ç†å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af6f535dbd5f9eb882dffb16bb520f57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02d5ec0ff76a188cc9cc8f34f202fbaf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Compressive-Imaging-Reconstruction-via-Tensor-Decomposed-Multi-Resolution-Grid-Encoding"><a href="#Compressive-Imaging-Reconstruction-via-Tensor-Decomposed-Multi-Resolution-Grid-Encoding" class="headerlink" title="Compressive Imaging Reconstruction via Tensor Decomposed   Multi-Resolution Grid Encoding"></a>Compressive Imaging Reconstruction via Tensor Decomposed   Multi-Resolution Grid Encoding</h2><p><strong>Authors:Zhenyu Jin, Yisi Luo, Xile Zhao, Deyu Meng</strong></p>
<p>Compressive imaging (CI) reconstruction, such as snapshot compressive imaging (SCI) and compressive sensing magnetic resonance imaging (MRI), aims to recover high-dimensional images from low-dimensional compressed measurements. This process critically relies on learning an accurate representation of the underlying high-dimensional image. However, existing unsupervised representations may struggle to achieve a desired balance between representation ability and efficiency. To overcome this limitation, we propose Tensor Decomposed multi-resolution Grid encoding (GridTD), an unsupervised continuous representation framework for CI reconstruction. GridTD optimizes a lightweight neural network and the input tensor decomposition model whose parameters are learned via multi-resolution hash grid encoding. It inherently enjoys the hierarchical modeling ability of multi-resolution grid encoding and the compactness of tensor decomposition, enabling effective and efficient reconstruction of high-dimensional images. Theoretical analyses for the algorithmâ€™s Lipschitz property, generalization error bound, and fixed-point convergence reveal the intrinsic superiority of GridTD as compared with existing continuous representation models. Extensive experiments across diverse CI tasks, including video SCI, spectral SCI, and compressive dynamic MRI reconstruction, consistently demonstrate the superiority of GridTD over existing methods, positioning GridTD as a versatile and state-of-the-art CI reconstruction method. </p>
<blockquote>
<p>å‹ç¼©æˆåƒï¼ˆCIï¼‰é‡å»ºï¼Œå¦‚å¿«ç…§å‹ç¼©æˆåƒï¼ˆSCIï¼‰å’Œå‹ç¼©æ„ŸçŸ¥ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œæ—¨åœ¨ä»ä½ç»´å‹ç¼©æµ‹é‡ä¸­æ¢å¤é«˜ç»´å›¾åƒã€‚è¿™ä¸€è¿‡ç¨‹ä¸¥é‡ä¾èµ–äºå­¦ä¹ é«˜ç»´å›¾åƒçš„æ­£ç¡®è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ— ç›‘ç£è¡¨ç¤ºå¯èƒ½åœ¨è¡¨ç¤ºèƒ½åŠ›å’Œæ•ˆç‡ä¹‹é—´éš¾ä»¥å®ç°å¹³è¡¡ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Tensor Decomposedå¤šåˆ†è¾¨ç‡ç½‘æ ¼ç¼–ç ï¼ˆGridTDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºCIé‡å»ºçš„æ— ç›‘ç£è¿ç»­è¡¨ç¤ºæ¡†æ¶ã€‚GridTDä¼˜åŒ–äº†è½»é‡çº§ç¥ç»ç½‘ç»œå’Œè¾“å…¥å¼ é‡åˆ†è§£æ¨¡å‹ï¼Œå…¶å‚æ•°é€šè¿‡å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼ç¼–ç å­¦ä¹ ã€‚å®ƒå¤©ç„¶åœ°äº«å—å¤šåˆ†è¾¨ç‡ç½‘æ ¼ç¼–ç çš„åˆ†å±‚å»ºæ¨¡èƒ½åŠ›å’Œå¼ é‡åˆ†è§£çš„ç´§å‡‘æ€§ï¼Œèƒ½å¤Ÿå®ç°é«˜ç»´å›¾åƒçš„æœ‰æ•ˆä¸”é«˜æ•ˆçš„é‡å»ºã€‚è¯¥ç®—æ³•çš„ææ™®å¸ŒèŒ¨å±æ€§ã€æ³›åŒ–è¯¯å·®ç•Œå’Œå®šç‚¹æ”¶æ•›çš„ç†è®ºåˆ†ææ­ç¤ºäº†GridTDç›¸è¾ƒäºç°æœ‰è¿ç»­è¡¨ç¤ºæ¨¡å‹çš„å†…åœ¨ä¼˜åŠ¿ã€‚åœ¨åŒ…æ‹¬è§†é¢‘SCIã€å…‰è°±SCIå’Œå‹ç¼©åŠ¨æ€MRIé‡å»ºç­‰å¤šç§CIä»»åŠ¡çš„å¤§é‡å®éªŒä¸­ï¼ŒGridTDå§‹ç»ˆè¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ•ˆæœï¼Œç¡®ç«‹äº†GridTDä½œä¸ºä¸€ç§é€šç”¨ä¸”æœ€å…ˆè¿›çš„CIé‡å»ºæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07707v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å‹ç¼©æˆåƒï¼ˆCIï¼‰é‡å»ºæŠ€æœ¯ï¼Œå¦‚å¿«ç…§å‹ç¼©æˆåƒï¼ˆSCIï¼‰å’Œå‹ç¼©æ„ŸçŸ¥ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºTensor Decomposedå¤šåˆ†è¾¨ç‡ç½‘æ ¼ç¼–ç ï¼ˆGridTDï¼‰çš„æ— ç›‘ç£è¿ç»­è¡¨ç¤ºæ¡†æ¶ï¼Œç”¨äºCIé‡å»ºã€‚GridTDä¼˜åŒ–äº†ä¸€ä¸ªè½»é‡çº§ç¥ç»ç½‘ç»œå’Œè¾“å…¥å¼ é‡åˆ†è§£æ¨¡å‹ï¼Œé€šè¿‡å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼ç¼–ç å­¦ä¹ å‚æ•°ã€‚GridTDç»“åˆäº†å¤šåˆ†è¾¨ç‡ç½‘æ ¼ç¼–ç çš„å±‚æ¬¡å»ºæ¨¡èƒ½åŠ›å’Œå¼ é‡åˆ†è§£çš„ç´§å‡‘æ€§ï¼Œèƒ½å¤Ÿé«˜æ•ˆã€æœ‰æ•ˆåœ°é‡å»ºé«˜ç»´å›¾åƒã€‚ç†è®ºåˆ†æå’Œåœ¨å¤šç§CIä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGridTDç›¸è¾ƒäºç°æœ‰æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‹ç¼©æˆåƒï¼ˆCIï¼‰æŠ€æœ¯æ—¨åœ¨ä»ä½ç»´å‹ç¼©æµ‹é‡ä¸­æ¢å¤é«˜ç»´å›¾åƒã€‚</li>
<li>ç°æœ‰æ— ç›‘ç£è¡¨ç¤ºæ–¹æ³•å¯èƒ½åœ¨è¡¨ç¤ºèƒ½åŠ›å’Œæ•ˆç‡ä¹‹é—´éš¾ä»¥è¾¾åˆ°å¹³è¡¡ã€‚</li>
<li>æå‡ºçš„Tensor Decomposedå¤šåˆ†è¾¨ç‡ç½‘æ ¼ç¼–ç ï¼ˆGridTDï¼‰æ¡†æ¶ç»“åˆäº†å¤šåˆ†è¾¨ç‡ç½‘æ ¼ç¼–ç çš„å±‚æ¬¡å»ºæ¨¡èƒ½åŠ›å’Œå¼ é‡åˆ†è§£çš„ç´§å‡‘æ€§ã€‚</li>
<li>GridTDé€šè¿‡ä¼˜åŒ–è½»é‡çº§ç¥ç»ç½‘ç»œå’Œè¾“å…¥å¼ é‡åˆ†è§£æ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆçš„é«˜ç»´å›¾åƒé‡å»ºã€‚</li>
<li>GridTDå…·æœ‰ä¼˜å¼‚çš„ç†è®ºæ€§èƒ½ï¼ŒåŒ…æ‹¬Lipschitzæ€§è´¨ã€æ³›åŒ–è¯¯å·®ç•Œå’Œå›ºå®šç‚¹æ”¶æ•›æ€§åˆ†æã€‚</li>
<li>åœ¨å¤šç§CIä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬è§†é¢‘SCIã€å…‰è°±SCIå’Œå‹ç¼©åŠ¨æ€MRIé‡å»ºï¼ŒGridTDç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-76c5b247fe093265ddb1f3898ddaf444.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c18136b0a7adeb6e1bcfe82cfe4303f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-654f753c460cfa58839b3315fbc58bfe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TransformEEG-Towards-Improving-Model-Generalizability-in-Deep-Learning-based-EEG-Parkinsonâ€™s-Disease-Detection"><a href="#TransformEEG-Towards-Improving-Model-Generalizability-in-Deep-Learning-based-EEG-Parkinsonâ€™s-Disease-Detection" class="headerlink" title="TransformEEG: Towards Improving Model Generalizability in Deep   Learning-based EEG Parkinsonâ€™s Disease Detection"></a>TransformEEG: Towards Improving Model Generalizability in Deep   Learning-based EEG Parkinsonâ€™s Disease Detection</h2><p><strong>Authors:Federico Del Pup, Riccardo Brun, Filippo Iotti, Edoardo Paccagnella, Mattia Pezzato, Sabrina Bertozzo, Andrea Zanola, Louis Fabrice Tshimanga, Henning MÃ¼ller, Manfredo Atzori</strong></p>
<p>Electroencephalography (EEG) is establishing itself as an important, low-cost, noninvasive diagnostic tool for the early detection of Parkinsonâ€™s Disease (PD). In this context, EEG-based Deep Learning (DL) models have shown promising results due to their ability to discover highly nonlinear patterns within the signal. However, current state-of-the-art DL models suffer from poor generalizability caused by high inter-subject variability. This high variability underscores the need for enhancing model generalizability by developing new architectures better tailored to EEG data. This paper introduces TransformEEG, a hybrid Convolutional-Transformer designed for Parkinsonâ€™s disease detection using EEG data. Unlike transformer models based on the EEGNet structure, TransformEEG incorporates a depthwise convolutional tokenizer. This tokenizer is specialized in generating tokens composed by channel-specific features, which enables more effective feature mixing within the self-attention layers of the transformer encoder. To evaluate the proposed model, four public datasets comprising 290 subjects (140 PD patients, 150 healthy controls) were harmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out (N-LNSO) cross-validation was performed to provide an unbiased comparison against seven other consolidated EEG deep learning models. TransformEEG achieved the highest balanced accuracyâ€™s median (78.45%) as well as the lowest interquartile range (6.37%) across all the N-LNSO partitions. When combined with data augmentation and threshold correction, median accuracy increased to 80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG produces more consistent and less skewed results. It demonstrates a substantial reduction in variability and more reliable PD detection using EEG data compared to the other investigated models. </p>
<blockquote>
<p>è„‘ç”µå›¾ï¼ˆEEGï¼‰æ­£åœ¨æˆä¸ºä¸€ç§é‡è¦çš„ã€ä½æˆæœ¬çš„ã€éä¾µå…¥æ€§çš„å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ—©æœŸè¯Šæ–­å·¥å…·ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼ŒåŸºäºè„‘ç”µå›¾çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ç”±äºå…¶å‘ç°ä¿¡å·å†…é«˜åº¦éçº¿æ€§æ¨¡å¼çš„èƒ½åŠ›è€Œæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœã€‚ç„¶è€Œï¼Œç›®å‰æœ€å…ˆè¿›çš„DLæ¨¡å‹ç”±äºä¸»ä½“é—´å­˜åœ¨é«˜å˜å¼‚æ€§è€Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚è¿™ç§é«˜å˜å¼‚æ€§å¼ºè°ƒäº†é€šè¿‡å¼€å‘æ›´å¥½åœ°é€‚åº”è„‘ç”µå›¾æ•°æ®çš„æ–°æ¶æ„æ¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å¿…è¦æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†TransformEEGï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¸•é‡‘æ£®ç—…æ£€æµ‹çš„æ··åˆå·ç§¯-è½¬æ¢å™¨ï¼Œä½¿ç”¨è„‘ç”µå›¾æ•°æ®ã€‚ä¸åŸºäºEEGNetç»“æ„çš„è½¬æ¢å™¨æ¨¡å‹ä¸åŒï¼ŒTransformEEGé‡‡ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯åˆ†è¯å™¨ã€‚è¯¥åˆ†è¯å™¨ä¸“é—¨ç”Ÿæˆç”±é€šé“ç‰¹å®šç‰¹å¾ç»„æˆçš„ä»¤ç‰Œï¼Œè¿™å¯ä»¥åœ¨è½¬æ¢å™¨ç¼–ç å™¨çš„è‡ªæ³¨æ„åŠ›å±‚ä¸­å®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾æ··åˆã€‚ä¸ºäº†è¯„ä¼°æ‰€æå‡ºçš„æ¨¡å‹ï¼Œå››ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆåŒ…æ‹¬290ä¸ªå—è¯•è€…ï¼Œå…¶ä¸­140åä¸ºPDæ‚£è€…ï¼Œ150åä¸ºå¥åº·å¯¹ç…§ï¼‰è¢«åè°ƒå¹¶æ±‡æ€»ã€‚è¿›è¡Œäº†10æ¬¡å¤–éƒ¨å’Œ10æ¬¡å†…éƒ¨åµŒå¥—ç•™Nä¸ªä¸»ä½“å¤–ï¼ˆN-LNSOï¼‰çš„äº¤å‰éªŒè¯ï¼Œä»¥ä¸å…¶ä»–ä¸ƒä¸ªæˆç†Ÿçš„è„‘ç”µå›¾æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå…¬æ­£æ¯”è¾ƒã€‚TransformEEGåœ¨æ‰€æœ‰çš„N-LNSOåˆ†åŒºä¸­è¾¾åˆ°äº†æœ€é«˜çš„å¹³è¡¡å‡†ç¡®ç‡çš„ä¸­ä½æ•°ï¼ˆ78.45%ï¼‰ï¼Œä»¥åŠæœ€ä½çš„å››åˆ†ä½æ•°èŒƒå›´ï¼ˆ6.37%ï¼‰ã€‚å½“ä¸æ•°æ®å¢å¼ºå’Œé˜ˆå€¼æ ¡æ­£ç›¸ç»“åˆæ—¶ï¼Œä¸­ä½æ•°å‡†ç¡®ç‡æé«˜åˆ°80.1%ï¼Œå››åˆ†ä½æ•°èŒƒå›´ä¸º5.74%ã€‚æ€»ä¹‹ï¼ŒTransformEEGäº§ç”Ÿçš„ç»“æœæ›´åŠ ä¸€è‡´ä¸”åå·®è¾ƒå°ã€‚ä¸å…¶ä»–è°ƒæŸ¥æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒæ˜¾ç¤ºäº†æ˜¾è‘—çš„å˜å¼‚æ€§é™ä½ï¼Œå¹¶ä¸”ä½¿ç”¨è„‘ç”µå›¾æ•°æ®æ›´å¯é åœ°æ£€æµ‹PDã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07622v1">PDF</a> Submitted for possible publication. GitHub repository: see   <a target="_blank" rel="noopener" href="https://github.com/MedMaxLab/transformeeg">https://github.com/MedMaxLab/transformeeg</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†EEGåœ¨å¸•é‡‘æ£®ç—…æ—©æœŸè¯Šæ–­ä¸­çš„é‡è¦ä½œç”¨ï¼Œä»¥åŠåŸºäºEEGçš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº”ç”¨å‰æ™¯ã€‚é’ˆå¯¹å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹å­˜åœ¨çš„æ³›åŒ–èƒ½åŠ›ä¸å¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆå·ç§¯-Transformeræ¨¡å‹TransformEEGã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ·±åº¦å·ç§¯åˆ†è¯å™¨ç”Ÿæˆé€šé“ç‰¹å®šç‰¹å¾æ ‡è®°ï¼Œæé«˜äº†ç‰¹å¾æ··åˆæ•ˆç‡ï¼Œå¹¶åœ¨å¸•é‡‘æ£®ç—…æ£€æµ‹ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EEGä½œä¸ºå¸•é‡‘æ£®ç—…æ—©æœŸè¯Šæ–­çš„éä¾µå…¥æ€§å·¥å…·ï¼Œå…·æœ‰ä½æˆæœ¬å’Œé‡è¦æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨EEGä¿¡å·å¤„ç†ä¸­å…·æœ‰å‘ç°éçº¿æ€§æ¨¡å¼çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹å­˜åœ¨æ³›åŒ–èƒ½åŠ›ä¸å¼ºçš„é—®é¢˜ï¼Œä¸»è¦ç”±äºä¸ªä½“é—´å·®å¼‚å¤§ã€‚</li>
<li>TransformEEGæ¨¡å‹å¼•å…¥æ·±åº¦å·ç§¯åˆ†è¯å™¨ï¼Œæé«˜ç‰¹å¾æ··åˆæ•ˆç‡ã€‚</li>
<li>TransformEEGæ¨¡å‹åœ¨å¸•é‡‘æ£®ç—…æ£€æµ‹ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>TransformEEGæ¨¡å‹ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œç»“æœæ›´ä¸€è‡´ã€å¯é æ€§æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4f1f1ef1185a194286069c43e0d958e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ddc2e593cd19d8c1bb255ea9f5a3d8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7873758f8a755ce11ccdc70f400f7137.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Semantic-guided-Masked-Mutual-Learning-for-Multi-modal-Brain-Tumor-Segmentation-with-Arbitrary-Missing-Modalities"><a href="#Semantic-guided-Masked-Mutual-Learning-for-Multi-modal-Brain-Tumor-Segmentation-with-Arbitrary-Missing-Modalities" class="headerlink" title="Semantic-guided Masked Mutual Learning for Multi-modal Brain Tumor   Segmentation with Arbitrary Missing Modalities"></a>Semantic-guided Masked Mutual Learning for Multi-modal Brain Tumor   Segmentation with Arbitrary Missing Modalities</h2><p><strong>Authors:Guoyan Liang, Qin Zhou, Jingyuan Chen, Bingcang Huang, Kai Chen, Lin Gu, Zhe Wang, Sai Wu, Chang Yao</strong></p>
<p>Malignant brain tumors have become an aggressive and dangerous disease that leads to death worldwide.Multi-modal MRI data is crucial for accurate brain tumor segmentation, but missing modalities common in clinical practice can severely degrade the segmentation performance. While incomplete multi-modal learning methods attempt to address this, learning robust and discriminative features from arbitrary missing modalities remains challenging. To address this challenge, we propose a novel Semantic-guided Masked Mutual Learning (SMML) approach to distill robust and discriminative knowledge across diverse missing modality scenarios.Specifically, we propose a novel dual-branch masked mutual learning scheme guided by Hierarchical Consistency Constraints (HCC) to ensure multi-level consistency, thereby enhancing mutual learning in incomplete multi-modal scenarios. The HCC framework comprises a pixel-level constraint that selects and exchanges reliable knowledge to guide the mutual learning process. Additionally, it includes a feature-level constraint that uncovers robust inter-sample and inter-class relational knowledge within the latent feature space. To further enhance multi-modal learning from missing modality data, we integrate a refinement network into each student branch. This network leverages semantic priors from the Segment Anything Model (SAM) to provide supplementary information, effectively complementing the masked mutual learning strategy in capturing auxiliary discriminative knowledge. Extensive experiments on three challenging brain tumor segmentation datasets demonstrate that our method significantly improves performance over state-of-the-art methods in diverse missing modality settings. </p>
<blockquote>
<p>æ¶æ€§è„‘è‚¿ç˜¤å·²æˆä¸ºä¸€ç§å…¨çƒèŒƒå›´å†…å…·æœ‰ä¾µè¢­æ€§å’Œè‡´å‘½æ€§çš„ç–¾ç—…ã€‚å¤šæ¨¡æ€MRIæ•°æ®å¯¹äºå‡†ç¡®çš„è„‘è‚¿ç˜¤åˆ†å‰²è‡³å…³é‡è¦ï¼Œä½†åœ¨ä¸´åºŠå®è·µä¸­çš„ç¼ºå¤±æ¨¡å¼ä¼šä¸¥é‡é™ä½åˆ†å‰²æ€§èƒ½ã€‚è™½ç„¶ä¸å®Œæ•´çš„å¤šæ¨¡å¼å­¦ä¹ æ–¹æ³•è¯•å›¾è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ä»ä»»æ„ç¼ºå¤±çš„æ¨¡å¼ä¸­å­¦ä¹ ç¨³å¥å’Œå…·æœ‰åŒºåˆ†æ€§çš„ç‰¹å¾ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰å¼•å¯¼æ©è†œäº’å­¦ä¹ ï¼ˆSMMLï¼‰æ–¹æ³•ï¼Œä»¥åœ¨å¤šç§ç¼ºå¤±æ¨¡å¼åœºæ™¯ä¸­æç‚¼ç¨³å¥å’Œå…·æœ‰åŒºåˆ†æ€§çš„çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”±å±‚æ¬¡ä¸€è‡´æ€§çº¦æŸï¼ˆHCCï¼‰å¼•å¯¼çš„åŒé‡åˆ†æ”¯æ©è†œäº’å­¦ä¹ æ–¹æ¡ˆï¼Œä»¥ç¡®ä¿å¤šçº§ä¸€è‡´æ€§ï¼Œä»è€Œå¢å¼ºä¸å®Œæ•´å¤šæ¨¡å¼åœºæ™¯ä¸­çš„ç›¸äº’å­¦ä¹ ã€‚HCCæ¡†æ¶åŒ…æ‹¬åƒç´ çº§çº¦æŸï¼Œç”¨äºé€‰æ‹©å’Œäº¤æ¢å¯é çš„çŸ¥è¯†ä»¥æŒ‡å¯¼äº’å­¦ä¹ è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬ç‰¹å¾çº§çº¦æŸï¼Œç”¨äºæ­ç¤ºæ½œåœ¨ç‰¹å¾ç©ºé—´å†…çš„ç¨³å¥è·¨æ ·æœ¬å’Œè·¨ç±»åˆ«å…³ç³»çŸ¥è¯†ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä»ç¼ºå¤±æ¨¡å¼æ•°æ®ä¸­å¢å¼ºå¤šæ¨¡å¼å­¦ä¹ ï¼Œæˆ‘ä»¬å°†ç»†åŒ–ç½‘ç»œé›†æˆåˆ°æ¯ä¸ªå­¦ç”Ÿåˆ†æ”¯ä¸­ã€‚è¯¥ç½‘ç»œåˆ©ç”¨æ¥è‡ªåˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰çš„è¯­ä¹‰å…ˆéªŒä¿¡æ¯æä¾›è¡¥å……ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°è¡¥å……äº†æ©è†œäº’å­¦ä¹ ç­–ç•¥åœ¨æ•è·è¾…åŠ©åŒºåˆ†æ€§çŸ¥è¯†æ–¹é¢çš„ä¸è¶³ã€‚åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è„‘è‚¿ç˜¤åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ç¼ºå¤±æ¨¡å¼è®¾ç½®ä¸­çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07592v1">PDF</a> 9 pages, 3 figures,conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºè¯­ä¹‰å¼•å¯¼æ©è†œäº’å­¦ä¹ ï¼ˆSMMLï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨å¤šç§ç¼ºå¤±æ¨¡æ€åœºæ™¯ä¸‹æç‚¼ç¨³å¥å’Œå…·æœ‰åŒºåˆ†åŠ›çš„çŸ¥è¯†ã€‚è¯¥æ–¹æ³•é€šè¿‡åŒåˆ†æ”¯æ©è†œäº’å­¦ä¹ æ–¹æ¡ˆï¼Œç»“åˆå±‚æ¬¡ä¸€è‡´æ€§çº¦æŸï¼ˆHCCï¼‰ç¡®ä¿å¤šçº§ä¸€è‡´æ€§ï¼Œä»è€Œæå‡ä¸å®Œæ•´å¤šæ¨¡æ€åœºæ™¯ä¸­çš„äº’å­¦ä¹ æ•ˆæœã€‚æ­¤å¤–ï¼Œæ•´åˆç²¾ç‚¼ç½‘ç»œä»¥æ•è·è¾…åŠ©åŒºåˆ†æ€§çŸ¥è¯†ï¼Œæœ‰æ•ˆè¡¥å……æ©è†œäº’å­¦ä¹ ç­–ç•¥çš„ä¸è¶³ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç¼ºå¤±æ¨¡æ€è®¾ç½®ä¸‹ï¼Œå¯¹è„‘è‚¿ç˜¤åˆ†å‰²æ•°æ®é›†çš„æ€§èƒ½æå‡æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¶æ€§è„‘è‚¿ç˜¤æ˜¯ä¸€ç§å…¨çƒæ€§çš„è‡´å‘½ç–¾ç—…ï¼Œå¤šæ¨¡æ€MRIæ•°æ®å¯¹å‡†ç¡®è„‘è‚¿ç˜¤åˆ†å‰²è‡³å…³é‡è¦ã€‚</li>
<li>ç¼ºå¤±æ¨¡æ€æ˜¯ä¸´åºŠå®è·µä¸­å¸¸è§é—®é¢˜ï¼Œä¸¥é‡å½±å“åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„è¯­ä¹‰å¼•å¯¼æ©è†œäº’å­¦ä¹ ï¼ˆSMMLï¼‰æ–¹æ³•ï¼Œä»¥å¤„ç†ç¼ºå¤±æ¨¡æ€åœºæ™¯ä¸­çš„ç¨³å¥å’ŒåŒºåˆ†æ€§ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>é€šè¿‡åŒåˆ†æ”¯æ©è†œäº’å­¦ä¹ æ–¹æ¡ˆå’Œå±‚æ¬¡ä¸€è‡´æ€§çº¦æŸï¼ˆHCCï¼‰ç¡®ä¿å¤šçº§ä¸€è‡´æ€§ã€‚</li>
<li>HCCæ¡†æ¶åŒ…æ‹¬åƒç´ çº§çº¦æŸå’Œç‰¹å¾çº§çº¦æŸï¼Œåˆ†åˆ«ç”¨äºé€‰æ‹©å’Œäº¤æ¢å¯é çŸ¥è¯†ä»¥åŠæ­ç¤ºæ ·æœ¬å’Œç±»åˆ«ä¹‹é—´çš„ç¨³å¥å…³ç³»ã€‚</li>
<li>æ•´åˆç²¾ç‚¼ç½‘ç»œä»¥æä¾›è¡¥å……ä¿¡æ¯ï¼Œæœ‰æ•ˆè¡¥å……æ©è†œäº’å­¦ä¹ ç­–ç•¥çš„ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-646da4467b286d91d4a886174e081e79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-230fa74e9abca3df579eb0ebcdf51387.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f635879178b06eea2861abed9ebd2d48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-153356d09fa389e61fff37faf0eebb0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aab625e41536d81790861efef19b6931.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Learnable-Retrieval-Enhanced-Visual-Text-Alignment-and-Fusion-for-Radiology-Report-Generation"><a href="#Learnable-Retrieval-Enhanced-Visual-Text-Alignment-and-Fusion-for-Radiology-Report-Generation" class="headerlink" title="Learnable Retrieval Enhanced Visual-Text Alignment and Fusion for   Radiology Report Generation"></a>Learnable Retrieval Enhanced Visual-Text Alignment and Fusion for   Radiology Report Generation</h2><p><strong>Authors:Qin Zhou, Guoyan Liang, Xindi Li, Jingyuan Chen, Wang Zhe, Chang Yao, Sai Wu</strong></p>
<p>Automated radiology report generation is essential for improving diagnostic efficiency and reducing the workload of medical professionals. However, existing methods face significant challenges, such as disease class imbalance and insufficient cross-modal fusion. To address these issues, we propose the learnable Retrieval Enhanced Visual-Text Alignment and Fusion (REVTAF) framework, which effectively tackles both class imbalance and visual-text fusion in report generation. REVTAF incorporates two core components: (1) a Learnable Retrieval Enhancer (LRE) that utilizes semantic hierarchies from hyperbolic space and intra-batch context through a ranking-based metric. LRE adaptively retrieves the most relevant reference reports, enhancing image representations, particularly for underrepresented (tail) class inputs; and (2) a fine-grained visual-text alignment and fusion strategy that ensures consistency across multi-source cross-attention maps for precise alignment. This component further employs an optimal transport-based cross-attention mechanism to dynamically integrate task-relevant textual knowledge for improved report generation. By combining adaptive retrieval with multi-source alignment and fusion, REVTAF achieves fine-grained visual-text integration under weak image-report level supervision while effectively mitigating data imbalance issues. The experiments demonstrate that REVTAF outperforms state-of-the-art methods, achieving an average improvement of 7.4% on the MIMIC-CXR dataset and 2.9% on the IU X-Ray dataset. Comparisons with mainstream multimodal LLMs (e.g., GPT-series models), further highlight its superiority in radiology report generation <a target="_blank" rel="noopener" href="https://github.com/banbooliang/REVTAF-RRG">https://github.com/banbooliang/REVTAF-RRG</a>. </p>
<blockquote>
<p>è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå¯¹äºæé«˜è¯Šæ–­æ•ˆç‡ã€å‡è½»åŒ»ç–—ä¸“ä¸šäººå‘˜çš„å·¥ä½œé‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¦‚ç–¾ç—…ç±»åˆ«ä¸å¹³è¡¡å’Œè·¨æ¨¡æ€èåˆä¸è¶³ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯å­¦ä¹ çš„æ£€ç´¢å¢å¼ºè§†è§‰æ–‡æœ¬å¯¹é½ä¸èåˆï¼ˆREVTAFï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡å’ŒæŠ¥å‘Šç”Ÿæˆä¸­çš„è§†è§‰æ–‡æœ¬èåˆé—®é¢˜ã€‚REVTAFåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰å¯å­¦ä¹ æ£€ç´¢å¢å¼ºå™¨ï¼ˆLREï¼‰ï¼Œå®ƒåˆ©ç”¨è¶…çƒé¢çš„è¯­ä¹‰å±‚æ¬¡ç»“æ„å’ŒåŸºäºæ’åçš„æŒ‡æ ‡åœ¨æ‰¹æ¬¡å†…éƒ¨ä¸Šä¸‹æ–‡è¿›è¡Œè‡ªé€‚åº”æ£€ç´¢ã€‚LREèƒ½å¤Ÿè‡ªé€‚åº”åœ°æ£€ç´¢æœ€ç›¸å…³çš„å‚è€ƒæŠ¥å‘Šï¼Œå¢å¼ºå›¾åƒè¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ¬ ä»£è¡¨çš„å°¾éƒ¨ç±»åˆ«è¾“å…¥ï¼›ï¼ˆ2ï¼‰ç²¾ç»†çš„è§†è§‰æ–‡æœ¬å¯¹é½ä¸èåˆç­–ç•¥ï¼Œç¡®ä¿è·¨å¤šæºäº¤å‰æ³¨æ„åŠ›å›¾çš„ä¸€è‡´æ€§ï¼Œä»¥å®ç°ç²¾ç¡®å¯¹é½ã€‚è¯¥ç»„ä»¶è¿›ä¸€æ­¥é‡‡ç”¨åŸºäºæœ€ä¼˜ä¼ è¾“çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥åŠ¨æ€é›†æˆä»»åŠ¡ç›¸å…³çš„æ–‡æœ¬çŸ¥è¯†ï¼Œæ”¹è¿›æŠ¥å‘Šç”Ÿæˆã€‚é€šè¿‡è‡ªé€‚åº”æ£€ç´¢ä¸å¤šæºå¯¹é½å’Œèåˆç›¸ç»“åˆï¼ŒREVTAFåœ¨å¼±å›¾åƒæŠ¥å‘Šçº§åˆ«ç›‘ç£ä¸‹å®ç°äº†ç²¾ç»†çš„è§†è§‰æ–‡æœ¬é›†æˆï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ç¼“è§£äº†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒREVTAFä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨MIMIC-CXRæ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†7.4%ï¼Œåœ¨IU Xå…‰æ•°æ®é›†ä¸Šæé«˜äº†2.9%ã€‚å®ƒä¸ä¸»æµçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPTç³»åˆ—æ¨¡å‹ï¼‰çš„æ¯”è¾ƒï¼Œè¿›ä¸€æ­¥å‡¸æ˜¾å…¶åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„ä¼˜è¶Šæ€§ã€‚ç›¸å…³ä»£ç å·²ä¸Šä¼ è‡³GitHubï¼š<a target="_blank" rel="noopener" href="https://github.com/banbooliang/REVTAF-RRG%E3%80%82%EF%BC%88%E5%AD%98%E5%9C%A8%E7%9B%B4%E6%8E%A5%E4%BF%AE%E6%94%B9GitHub%E9%93%BE%E6%8E%A5%EF%BC%89">https://github.com/banbooliang/REVTAF-RRGã€‚ï¼ˆæ­¤å¤„ç›´æ¥é™„ä¸Šäº†GitHubé“¾æ¥ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07568v1">PDF</a> 10 pages,3 figures, conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºREVTAFçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä¸­çš„è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆé—®é¢˜ã€‚å®ƒé€šè¿‡ç»“åˆå¯å­¦ä¹ çš„æ£€ç´¢å¢å¼ºå™¨å’Œç²¾ç»†çš„è§†è§‰æ–‡æœ¬å¯¹é½èåˆç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³äº†ç–¾ç—…ç±»åˆ«ä¸å¹³è¡¡å’Œè·¨æ¨¡æ€èåˆçš„æŒ‘æˆ˜ã€‚REVTAFå®ç°äº†ç²¾ç»†çš„è§†è§‰æ–‡æœ¬é›†æˆï¼Œå¹¶åœ¨å¼±å›¾åƒæŠ¥å‘Šçº§åˆ«ç›‘ç£ä¸‹æœ‰æ•ˆç¼“è§£äº†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒREVTAFåœ¨MIMIC-CXRå’ŒIU X-Rayæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡æé«˜äº†7.4%å’Œ2.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆèƒ½æé«˜è¯Šæ–­æ•ˆç‡å¹¶é™ä½åŒ»ç–—ä¸“ä¸šäººå‘˜çš„å·¥ä½œé‡ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´ç–¾ç—…ç±»åˆ«ä¸å¹³è¡¡å’Œè·¨æ¨¡æ€èåˆçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„REVTAFæ¡†æ¶åŒ…æ‹¬å¯å­¦ä¹ çš„æ£€ç´¢å¢å¼ºå™¨å’Œç²¾ç»†çš„è§†è§‰æ–‡æœ¬å¯¹é½èåˆç­–ç•¥ï¼Œæœ‰æ•ˆè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>REVTAFé€šè¿‡ç»“åˆè‡ªé€‚åº”æ£€ç´¢å’Œå¤šæºå¯¹é½èåˆï¼Œå®ç°äº†åœ¨å¼±å›¾åƒæŠ¥å‘Šçº§åˆ«ç›‘ç£ä¸‹çš„ç²¾ç»†è§†è§‰æ–‡æœ¬é›†æˆã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒREVTAFåœ¨MIMIC-CXRå’ŒIU X-Rayæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6ee542c3d6d9296f97c58034a8505a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0abf873b42dbeb00693ce90cd4ef9de5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01c10d929793b3400031cb8ba8269fc9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26bbd090ed42dda65d8fd55b996357c2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Toward-All-2D-based-Printed-Raindrop-Triboelectric-Nanogenerators"><a href="#Toward-All-2D-based-Printed-Raindrop-Triboelectric-Nanogenerators" class="headerlink" title="Toward All 2D-based Printed Raindrop Triboelectric Nanogenerators"></a>Toward All 2D-based Printed Raindrop Triboelectric Nanogenerators</h2><p><strong>Authors:Foad Ghasemi, Jonas Heirich, Dimitri Sharikow, Sebastian Klenk, Jonathan N. Coleman, Georg S. Duesberg, Claudia Backes</strong></p>
<p>The raindrop triboelectric nanogenerator (RD-TENG) is an emerging technology that is designed to harvest energy from raindrops. This application requires materials with negative triboelectric effect, high surface charge density, mechanical flexibility, and a large surface area, which are key characteristics of 2D materials. However, fundamental research is necessary to understand the potential of 2D materials in this context. This study introduces all-2D-based RD-TENG devices using graphene and transition metal dichalcogenide (TMD) nanosheets. Liquid phase exfoliation (LPE) and liquid cascade centrifugation are used for nanosheet preparation and size selection. The TENGs are fabricated through a rapid, low-cost solution deposition technique based on liquid-liquid interface deposition, which allows screening of different active films and device geometries. Among the tested layered materials, medium-sized molybdenum disulfide (MoS2) nanosheets (average lateral size~160 nm, volume-fraction weighted average layer number ~9) exhibit the highest short-circuit current (microampere per drop) and voltage (mV per drop) output due to their most suited electron affinity, capacitance, and surface charge exchange properties. The variations in the performance of the TMD films were further evaluated with X-ray photoelectron spectroscopy (XPS), showing the influence of oxidation differences on charge transfer and charge decay time. </p>
<blockquote>
<p>é›¨æ»´é™ç”µçº³ç±³å‘ç”µæœºï¼ˆRD-TENGï¼‰æ˜¯ä¸€é¡¹æ–°å…´æŠ€æœ¯ï¼Œæ—¨åœ¨ä»é›¨æ»´ä¸­æ”¶é›†èƒ½é‡ã€‚è¯¥åº”ç”¨éœ€è¦å…·æœ‰è´Ÿé™ç”µæ•ˆåº”ã€é«˜è¡¨é¢ç”µè·å¯†åº¦ã€æœºæ¢°æŸ”éŸ§æ€§å’Œå¤§é¢ç§¯ç­‰å…³é”®ç‰¹æ€§çš„ææ–™ï¼Œè¿™äº›éƒ½æ˜¯äºŒç»´ææ–™çš„å…³é”®ç‰¹æ€§ã€‚ç„¶è€Œï¼Œè¦åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­äº†è§£äºŒç»´ææ–™çš„æ½œåŠ›ï¼ŒåŸºç¡€ç ”ç©¶æ˜¯å¿…è¦çš„ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†å…¨äºŒç»´çš„RD-TENGå™¨ä»¶ï¼Œä½¿ç”¨çŸ³å¢¨çƒ¯å’Œè¿‡æ¸¡é‡‘å±äºŒå¤åŒ–ç‰©ï¼ˆTMDï¼‰çº³ç±³ç‰‡ã€‚æ¶²ç›¸å‰¥ç¦»ï¼ˆLPEï¼‰å’Œæ¶²ä½“çº§è”ç¦»å¿ƒç”¨äºçº³ç±³ç‰‡çš„åˆ¶å¤‡å’Œå°ºå¯¸é€‰æ‹©ã€‚TENGæ˜¯é€šè¿‡å¿«é€Ÿã€ä½æˆæœ¬çš„æº¶æ¶²æ²‰ç§¯æŠ€æœ¯åˆ¶é€ çš„ï¼Œè¯¥æŠ€æœ¯åŸºäºæ¶²-æ¶²ç•Œé¢æ²‰ç§¯ï¼Œå…è®¸ç­›é€‰ä¸åŒçš„æ´»æ€§è–„è†œå’Œå™¨ä»¶å‡ ä½•å½¢çŠ¶ã€‚åœ¨æµ‹è¯•çš„å±‚çŠ¶ææ–™ä¸­ï¼Œä¸­ç­‰å°ºå¯¸çš„ç¡«åŒ–é’¼ï¼ˆMoS2ï¼‰çº³ç±³ç‰‡ï¼ˆå¹³å‡æ¨ªå‘å°ºå¯¸çº¦ä¸º160çº³ç±³ï¼Œä½“ç§¯åˆ†æ•°åŠ æƒå¹³å‡å±‚æ•°çº¦ä¸º9ï¼‰è¡¨ç°å‡ºæœ€é«˜çš„çŸ­è·¯ç”µæµï¼ˆå¾®å®‰&#x2F;æ»´ï¼‰å’Œç”µå‹ï¼ˆæ¯«ä¼&#x2F;æ»´ï¼‰è¾“å‡ºï¼Œè¿™ä¸»è¦å¾—ç›Šäºå…¶é€‚åˆçš„ç”µå­äº²å’ŒåŠ›ã€ç”µå®¹å’Œè¡¨é¢ç”µè·äº¤æ¢ç‰¹æ€§ã€‚è¿›ä¸€æ­¥åˆ©ç”¨Xå°„çº¿å…‰ç”µå­å…‰è°±ä»ªï¼ˆXPSï¼‰è¯„ä¼°äº†TMDè–„è†œçš„æ€§èƒ½å˜åŒ–ï¼Œæ˜¾ç¤ºäº†æ°§åŒ–å·®å¼‚å¯¹ç”µè·è½¬ç§»å’Œç”µè·è¡°å‡æ—¶é—´çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07504v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™é¡¹ç ”ç©¶ä»‹ç»äº†åŸºäºäºŒç»´ææ–™çš„å…¨äºŒç»´é›¨æ»´æ‘©æ“¦çº³ç±³å‘ç”µæœºï¼ˆRD-TENGï¼‰å™¨ä»¶ã€‚è¯¥ç ”ç©¶ä½¿ç”¨çŸ³å¢¨çƒ¯å’Œè¿‡æ¸¡é‡‘å±äºŒå¤åŒ–ç‰©ï¼ˆTMDï¼‰çº³ç±³ç‰‡ï¼Œé€šè¿‡æ¶²ç›¸å‰¥ç¦»ï¼ˆLPEï¼‰å’Œæ¶²ä½“çº§è”ç¦»å¿ƒæ³•è¿›è¡Œçº³ç±³ç‰‡åˆ¶å¤‡å’Œå°ºå¯¸é€‰æ‹©ã€‚åŸºäºæ¶²-æ¶²ç•Œé¢æ²‰ç§¯çš„å¿«é€Ÿã€ä½æˆæœ¬æº¶æ¶²æ²‰ç§¯æŠ€æœ¯ç”¨äºåˆ¶é€ TENGå™¨ä»¶ï¼Œå¹¶ç­›é€‰ä¸åŒçš„æ´»æ€§è–„è†œå’Œå™¨ä»¶å‡ ä½•å½¢çŠ¶ã€‚å…¶ä¸­ï¼Œä¸­ç­‰å°ºå¯¸çš„é’¼äºŒç¡«åŒ–ç‰©ï¼ˆMoSâ‚‚ï¼‰çº³ç±³ç‰‡å±•ç°å‡ºæœ€ä½³çš„è¾“å‡ºæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RD-TENGæ˜¯ä¸€ç§æ–°å…´æŠ€æœ¯ï¼Œç”¨äºä»é›¨æ»´ä¸­æ”¶é›†èƒ½é‡ã€‚</li>
<li>äºŒç»´ææ–™åœ¨RD-TENGåº”ç”¨ä¸­å…·æœ‰å…³é”®ç‰¹æ€§ï¼Œå¦‚è´Ÿæ‘©æ“¦ç”µæ•ˆåº”ã€é«˜è¡¨é¢ç”µè·å¯†åº¦ã€æœºæ¢°çµæ´»æ€§å’Œå¤§è¡¨é¢ç§¯ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å…¨äºŒç»´çš„RD-TENGå™¨ä»¶ï¼Œä½¿ç”¨çŸ³å¢¨çƒ¯å’Œè¿‡æ¸¡é‡‘å±äºŒå¤åŒ–ç‰©ï¼ˆTMDï¼‰çº³ç±³ç‰‡ã€‚</li>
<li>æ¶²ç›¸å‰¥ç¦»ï¼ˆLPEï¼‰å’Œæ¶²ä½“çº§è”ç¦»å¿ƒæ³•ç”¨äºçº³ç±³ç‰‡çš„åˆ¶å¤‡å’Œå°ºå¯¸é€‰æ‹©ã€‚</li>
<li>åŸºäºæ¶²-æ¶²ç•Œé¢æ²‰ç§¯çš„å¿«é€Ÿã€ä½æˆæœ¬æº¶æ¶²æ²‰ç§¯æŠ€æœ¯ç”¨äºåˆ¶é€ TENGå™¨ä»¶ã€‚</li>
<li>ä¸­ç­‰å°ºå¯¸çš„é’¼äºŒç¡«åŒ–ç‰©ï¼ˆMoSâ‚‚ï¼‰çº³ç±³ç‰‡åœ¨æµ‹è¯•ä¸­è¡¨ç°å‡ºæœ€ä½³è¾“å‡ºæ€§èƒ½ï¼Œè¿™å½’å› äºå…¶é€‚åˆçš„ç”µå­äº²å’ŒåŠ›ã€ç”µå®¹å’Œè¡¨é¢ç”µè·äº¤æ¢å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e28dfb19a9a3bfd66d57cb2b97a142ab.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Dual-Semantic-Aware-Network-for-Noise-Suppressed-Ultrasound-Video-Segmentation"><a href="#Dual-Semantic-Aware-Network-for-Noise-Suppressed-Ultrasound-Video-Segmentation" class="headerlink" title="Dual Semantic-Aware Network for Noise Suppressed Ultrasound Video   Segmentation"></a>Dual Semantic-Aware Network for Noise Suppressed Ultrasound Video   Segmentation</h2><p><strong>Authors:Ling Zhou, Runtian Yuan, Yi Liu, Yuejie Zhang, Rui Feng, Shang Gao</strong></p>
<p>Ultrasound imaging is a prevalent diagnostic tool known for its simplicity and non-invasiveness. However, its inherent characteristics often introduce substantial noise, posing considerable challenges for automated lesion or organ segmentation in ultrasound video sequences. To address these limitations, we propose the Dual Semantic-Aware Network (DSANet), a novel framework designed to enhance noise robustness in ultrasound video segmentation by fostering mutual semantic awareness between local and global features. Specifically, we introduce an Adjacent-Frame Semantic-Aware (AFSA) module, which constructs a channel-wise similarity matrix to guide feature fusion across adjacent frames, effectively mitigating the impact of random noise without relying on pixel-level relationships. Additionally, we propose a Local-and-Global Semantic-Aware (LGSA) module that reorganizes and fuses temporal unconditional local features, which capture spatial details independently at each frame, with conditional global features that incorporate temporal context from adjacent frames. This integration facilitates multi-level semantic representation, significantly improving the modelâ€™s resilience to noise interference. Extensive evaluations on four benchmark datasets demonstrate that DSANet substantially outperforms state-of-the-art methods in segmentation accuracy. Moreover, since our model avoids pixel-level feature dependencies, it achieves significantly higher inference FPS than video-based methods, and even surpasses some image-based models. Code can be found in \href{<a target="_blank" rel="noopener" href="https://github.com/ZhouL2001/DSANet%7D%7BDSANet%7D">https://github.com/ZhouL2001/DSANet}{DSANet}</a> </p>
<blockquote>
<p>è¶…å£°æˆåƒæ˜¯ä¸€ç§æ™®éçš„è¯Šæ–­å·¥å…·ï¼Œä»¥å…¶ç®€å•å’Œéä¾µå…¥æ€§è€Œé—»åã€‚ç„¶è€Œï¼Œå…¶å›ºæœ‰ç‰¹æ€§å¸¸å¸¸å¼•å…¥å¤§é‡å™ªå£°ï¼Œç»™è¶…å£°è§†é¢‘åºåˆ—ä¸­çš„è‡ªåŠ¨åŒ–ç—…å˜æˆ–å™¨å®˜åˆ†å‰²å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŒè¯­ä¹‰æ„ŸçŸ¥ç½‘ç»œï¼ˆDSANetï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æå‡å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ä¹‹é—´çš„ç›¸äº’è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¢å¼ºè¶…å£°è§†é¢‘åˆ†å‰²ä¸­çš„å™ªå£°é²æ£’æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†é‚»å¸§è¯­ä¹‰æ„ŸçŸ¥ï¼ˆAFSAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—æ„å»ºé€šé“ç›¸ä¼¼åº¦çŸ©é˜µï¼Œä»¥å¼•å¯¼ç›¸é‚»å¸§ä¹‹é—´çš„ç‰¹å¾èåˆï¼Œæœ‰æ•ˆåœ°å‡è½»éšæœºå™ªå£°çš„å½±å“ï¼Œè€Œæ— éœ€ä¾èµ–åƒç´ çº§å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å±€éƒ¨å’Œå…¨å±€è¯­ä¹‰æ„ŸçŸ¥ï¼ˆLGSAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é‡æ–°ç»„ç»‡å’Œèåˆäº†æ—¶é—´ä¸Šçš„æ— æ¡ä»¶å±€éƒ¨ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾èƒ½å¤Ÿåœ¨æ¯ä¸ªå¸§ä¸Šç‹¬ç«‹æ•æ‰ç©ºé—´ç»†èŠ‚ï¼Œä¸è€ƒè™‘ç›¸é‚»å¸§æ—¶é—´ä¸Šä¸‹æ–‡çš„æ¡ä»¶å…¨å±€ç‰¹å¾ç›¸ç»“åˆã€‚è¿™ç§ç»“åˆä¿ƒè¿›äº†å¤šçº§åˆ«è¯­ä¹‰è¡¨ç¤ºï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹å™ªå£°å¹²æ‰°çš„æŠ—æ€§ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDSANetåœ¨åˆ†å‰²ç²¾åº¦ä¸Šå¤§å¤§ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚è€Œä¸”ï¼Œç”±äºæˆ‘ä»¬çš„æ¨¡å‹é¿å…äº†åƒç´ çº§ç‰¹å¾ä¾èµ–æ€§ï¼Œå› æ­¤ä¸åŸºäºè§†é¢‘çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå®ç°äº†æ›´é«˜çš„æ¨ç†FPSï¼Œç”šè‡³è¶…è¿‡äº†æŸäº›åŸºäºå›¾åƒæ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚ä»£ç å¯è§äºDSANetï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/ZhouL2001/DSANet%EF%BC%89%E2%80%9D">https://github.com/ZhouL2001/DSANetï¼‰</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07443v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDual Semantic-Aware Networkï¼ˆDSANetï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºæé«˜è¶…å£°è§†é¢‘åˆ†å‰²ä¸­çš„å™ªå£°é²æ£’æ€§ã€‚å®ƒé€šè¿‡ä¿ƒè¿›å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ä¹‹é—´çš„ç›¸äº’è¯­ä¹‰æ„ŸçŸ¥ï¼Œè§£å†³äº†è¶…å£°æˆåƒä¸­å™ªå£°å¸¦æ¥çš„è‡ªåŠ¨åŒ–ç—…å˜æˆ–å™¨å®˜åˆ†å‰²æŒ‘æˆ˜ã€‚DSANetåŒ…æ‹¬Adjacent-Frame Semantic-Awareï¼ˆAFSAï¼‰æ¨¡å—å’ŒLocal-and-Global Semantic-Awareï¼ˆLGSAï¼‰æ¨¡å—ï¼Œå¯åˆ†åˆ«é€šè¿‡æ„å»ºé€šé“ç›¸ä¼¼åº¦çŸ©é˜µå’Œå¼•å¯¼ç‰¹å¾èåˆæ¥å‡è½»å™ªå£°å½±å“ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDSANetåœ¨åˆ†å‰²ç²¾åº¦ä¸Šå¤§å¹…ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”ç”±äºé¿å…äº†åƒç´ çº§ç‰¹å¾ä¾èµ–ï¼Œå…¶æ¨ç†é€Ÿåº¦é«˜äºè§†é¢‘æ–¹æ³•ï¼Œç”šè‡³è¶…è¶ŠæŸäº›å›¾åƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DSANetæ¡†æ¶æ—¨åœ¨æé«˜è¶…å£°è§†é¢‘åˆ†å‰²ä¸­çš„å™ªå£°é²æ£’æ€§ã€‚</li>
<li>é€šè¿‡ä¿ƒè¿›å±€éƒ¨å’Œå…¨å±€ç‰¹å¾é—´çš„ç›¸äº’è¯­ä¹‰æ„ŸçŸ¥æ¥è§£å†³è¶…å£°æˆåƒä¸­çš„å™ªå£°é—®é¢˜ã€‚</li>
<li>AFSAæ¨¡å—æ„å»ºé€šé“ç›¸ä¼¼åº¦çŸ©é˜µï¼ŒæŒ‡å¯¼ç›¸é‚»å¸§çš„ç‰¹å¾èåˆã€‚</li>
<li>LGSAæ¨¡å—é‡ç»„å¹¶èåˆç‹¬ç«‹çš„ç©ºé—´ç»†èŠ‚å±€éƒ¨ç‰¹å¾ä¸åŒ…å«æ—¶é—´ä¸Šä¸‹æ–‡çš„æ¡ä»¶å…¨å±€ç‰¹å¾ã€‚</li>
<li>DSANetåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>DSANeté¿å…äº†åƒç´ çº§ç‰¹å¾ä¾èµ–ï¼Œå®ç°æ›´é«˜çš„æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-002b4c7968f05f476225d71319eb0b42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1bf3eb0c7e946ba1de4cd5ba7d166e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-442290af52932a1d2446c4b8e61e1c99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b397ed6b4cd50f5c75d3b7f394ed7e94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0bd013f32f3d468d49baa31e57dcfa6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Label-Efficient-Chest-X-ray-Diagnosis-via-Partial-CLIP-Adaptation"><a href="#Label-Efficient-Chest-X-ray-Diagnosis-via-Partial-CLIP-Adaptation" class="headerlink" title="Label-Efficient Chest X-ray Diagnosis via Partial CLIP Adaptation"></a>Label-Efficient Chest X-ray Diagnosis via Partial CLIP Adaptation</h2><p><strong>Authors:Heet Nitinkumar Dalsania</strong></p>
<p>Modern deep learning implementations for medical imaging usually rely on large labeled datasets. These datasets are often difficult to obtain due to privacy concerns, high costs, and even scarcity of cases. In this paper, a label-efficient strategy is proposed for chest X-ray diagnosis that seeks to reflect real-world hospital scenarios. The experiments use the NIH Chest X-ray14 dataset and a pre-trained CLIP ViT-B&#x2F;32 model. The model is adapted via partial fine-tuning of its visual encoder and then evaluated using zero-shot and few-shot learning with 1-16 labeled examples per disease class. The tests demonstrate that CLIPâ€™s pre-trained vision-language features can be effectively adapted to few-shot medical imaging tasks, achieving over 20% improvement in mean AUC score as compared to the zero-shot baseline. The key aspect of this work is to attempt to simulate internal hospital workflows, where image archives exist but annotations are sparse. This work evaluates a practical and scalable solution for both common and rare disease diagnosis. Additionally this research is intended for academic and experimental purposes only and has not been peer reviewed yet. All code is found at <a target="_blank" rel="noopener" href="https://github.com/heet007-code/CLIP-disease-xray">https://github.com/heet007-code/CLIP-disease-xray</a>. </p>
<blockquote>
<p>ç°ä»£åŒ»å­¦æˆåƒæ·±åº¦å­¦ä¹ çš„å®ç°é€šå¸¸ä¾èµ–äºå¤§é‡æ ‡è®°æ•°æ®é›†ã€‚ç”±äºéšç§æ‹…å¿§ã€é«˜æ˜‚çš„æˆæœ¬ä»¥åŠç—…ä¾‹ç¨€ç¼ºç­‰åŸå› ï¼Œè¿™äº›æ•°æ®é›†å¾€å¾€éš¾ä»¥è·å–ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºèƒ¸éƒ¨Xå°„çº¿è¯Šæ–­çš„æ ‡ç­¾é«˜æ•ˆç­–ç•¥ï¼Œæ—¨åœ¨åæ˜ ç°å®åŒ»é™¢åœºæ™¯ã€‚å®éªŒä½¿ç”¨NIH Chest X-ray14æ•°æ®é›†å’Œé¢„è®­ç»ƒçš„CLIP ViT-B&#x2F;32æ¨¡å‹ã€‚é€šè¿‡å¯¹è§†è§‰ç¼–ç å™¨çš„éƒ¨åˆ†å¾®è°ƒæ¥é€‚åº”æ¨¡å‹ï¼Œç„¶åä½¿ç”¨æ¯ç–¾ç—…ç±»åˆ«1-16ä¸ªæ ‡è®°æ ·æœ¬è¿›è¡Œé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ è¿›è¡Œè¯„ä¼°ã€‚æµ‹è¯•è¡¨æ˜ï¼ŒCLIPçš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€ç‰¹å¾å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”å°‘æ ·æœ¬åŒ»å­¦æˆåƒä»»åŠ¡ï¼Œä¸é›¶æ ·æœ¬åŸºçº¿ç›¸æ¯”ï¼Œå¹³å‡AUCå¾—åˆ†æé«˜äº†è¶…è¿‡20%ã€‚è¿™é¡¹å·¥ä½œçš„å…³é”®æ˜¯å°è¯•æ¨¡æ‹ŸåŒ»é™¢å†…éƒ¨å·¥ä½œæµç¨‹ï¼Œå…¶ä¸­å›¾åƒæ¡£æ¡ˆå­˜åœ¨ä½†æ³¨é‡Šç¨€ç–ã€‚è¿™é¡¹å·¥ä½œä¸ºå¸¸è§å’Œç½•è§ç–¾ç—…çš„è¯Šæ–­æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚å¦å¤–ï¼Œæœ¬ç ”ç©¶ä»…ç”¨äºå­¦æœ¯å’Œå®éªŒç›®çš„ï¼Œå°šæœªç»è¿‡åŒè¡Œè¯„å®¡ã€‚æ‰€æœ‰ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/heet007-code/CLIP-disease-xray%E5%8D%A0%E6%9C%AC%E5%AD%A4%E7%BC%96%E5%BD%A9%E5%AD%97%E6%AF%94%E5%AF%BC%E5%BC%BA/">https://github.com/heet007-code/CLIP-disease-xrayæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07254v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹åŒ»å­¦å½±åƒä¸­çš„æ·±åº¦å­¦ä¹ åº”ç”¨æå‡ºäº†ä¸€ä¸ªæ ‡ç­¾é«˜æ•ˆçš„ç­–ç•¥ï¼Œä¸»è¦ç”¨äºèƒ¸Xå…‰è¯Šæ–­å¹¶æ¨¡æ‹ŸåŒ»é™¢å®é™…åœºæ™¯ã€‚ç ”ç©¶ä½¿ç”¨NIH Chest X-ray14æ•°æ®é›†å’Œé¢„è®­ç»ƒçš„CLIP ViT-B&#x2F;32æ¨¡å‹ï¼Œé€šè¿‡éƒ¨åˆ†å¾®è°ƒè§†è§‰ç¼–ç å™¨è¿›è¡Œæ¨¡å‹é€‚åº”ï¼Œå¹¶é‡‡ç”¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ è¿›è¡Œæµ‹è¯•ï¼Œæ¯ä¸ªç–¾ç—…ç±»åˆ«åªéœ€1-16ä¸ªæ ‡ç­¾æ ·æœ¬ã€‚å®éªŒè¯æ˜CLIPçš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€ç‰¹å¾å¯æœ‰æ•ˆåº”ç”¨äºå°‘æ ·æœ¬åŒ»å­¦å½±åƒä»»åŠ¡ï¼Œå¹³å‡AUCå¾—åˆ†è¾ƒé›¶æ ·æœ¬åŸºçº¿æé«˜äº†è¶…è¿‡20%ã€‚æœ¬ç ”ç©¶çš„å…³é”®åœ¨äºæ¨¡æ‹ŸåŒ»é™¢å†…éƒ¨å·¥ä½œæµç¨‹ï¼Œå›¾åƒæ¡£æ¡ˆä¸°å¯Œä½†æ ‡æ³¨ç¨€ç¼ºã€‚è¯¥ç ”ç©¶ä¸ºå¸¸è§å’Œç½•è§ç–¾ç—…çš„è¯Šæ–­æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤ç ”ç©¶ä»…ç”¨äºå­¦æœ¯å’Œå®éªŒç›®çš„ï¼Œå°šæœªç»è¿‡åŒè¡Œè¯„å®¡ã€‚æ‰€æœ‰ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/heet007-code/CLIP-disease-xray%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/heet007-code/CLIP-disease-xrayæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ ‡ç­¾é«˜æ•ˆçš„ç­–ç•¥ï¼Œç”¨äºåŒ»å­¦å½±åƒä¸­çš„èƒ¸Xå…‰è¯Šæ–­ï¼Œæ¨¡æ‹ŸåŒ»é™¢å®é™…åœºæ™¯ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼Œå¹¶é€šè¿‡éƒ¨åˆ†å¾®è°ƒè§†è§‰ç¼–ç å™¨è¿›è¡Œæ¨¡å‹é€‚åº”ã€‚</li>
<li>å®éªŒé‡‡ç”¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œå±•ç¤ºäº†å¯¹å°‘æ ·æœ¬åŒ»å­¦å½±åƒä»»åŠ¡çš„é€‚åº”æ€§ã€‚</li>
<li>å®éªŒç»“æœè¾ƒé›¶æ ·æœ¬åŸºçº¿æé«˜äº†è¶…è¿‡20%çš„å¹³å‡AUCå¾—åˆ†ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹åœ¨äºæ¨¡æ‹Ÿå›¾åƒæ¡£æ¡ˆä¸°å¯Œä½†æ ‡æ³¨ç¨€ç¼ºçš„åŒ»é™¢å†…éƒ¨å·¥ä½œæµç¨‹ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¸¸è§å’Œç½•è§ç–¾ç—…çš„è¯Šæ–­æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23fb01f9af84edf2962f7e20477eb0d5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Interpretable-EEG-to-Image-Generation-with-Semantic-Prompts"><a href="#Interpretable-EEG-to-Image-Generation-with-Semantic-Prompts" class="headerlink" title="Interpretable EEG-to-Image Generation with Semantic Prompts"></a>Interpretable EEG-to-Image Generation with Semantic Prompts</h2><p><strong>Authors:Arshak Rezvani, Ali Akbari, Kosar Sanjar Arani, Maryam Mirian, Emad Arasteh, Martin J. McKeown</strong></p>
<p>Decoding visual experience from brain signals offers exciting possibilities for neuroscience and interpretable AI. While EEG is accessible and temporally precise, its limitations in spatial detail hinder image reconstruction. Our model bypasses direct EEG-to-image generation by aligning EEG signals with multilevel semantic captions â€“ ranging from object-level to abstract themes â€“ generated by a large language model. A transformer-based EEG encoder maps brain activity to these captions through contrastive learning. During inference, caption embeddings retrieved via projection heads condition a pretrained latent diffusion model for image generation. This text-mediated framework yields state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable alignment to known neurocognitive pathways. Dominant EEG-caption associations reflected the importance of different semantic levels extracted from perceived images. Saliency maps and t-SNE projections reveal semantic topography across the scalp. Our model demonstrates how structured semantic mediation enables cognitively aligned visual decoding from EEG. </p>
<blockquote>
<p>ä»è„‘ä¿¡å·è§£ç è§†è§‰ç»éªŒä¸ºç¥ç»ç§‘å­¦å’Œå¯è§£é‡Šçš„AIæä¾›äº†ä»¤äººå…´å¥‹çš„å¯èƒ½æ€§ã€‚è„‘ç”µå›¾æ˜¯å¯è®¿é—®çš„å¹¶ä¸”åœ¨æ—¶é—´ä¸Šç²¾ç¡®ï¼Œä½†å…¶ç©ºé—´ç»†èŠ‚ä¸Šçš„å±€é™æ€§é˜»ç¢äº†å›¾åƒé‡å»ºã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡å°†è„‘ç”µå›¾ä¿¡å·ä¸ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å¤šå±‚æ¬¡è¯­ä¹‰æ ‡é¢˜å¯¹é½ï¼Œä»è€Œç»•è¿‡ç›´æ¥çš„è„‘ç”µå›¾åˆ°å›¾åƒçš„ç”Ÿæˆã€‚åŸºäºå˜å‹å™¨çš„è„‘ç”µå›¾ç¼–ç å™¨é€šè¿‡å¯¹æ¯”å­¦ä¹ å°†è„‘æ´»åŠ¨æ˜ å°„åˆ°è¿™äº›æ ‡é¢˜ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æŠ•å½±å¤´æ£€ç´¢çš„æ ‡é¢˜åµŒå…¥æ¡ä»¶ç”¨äºå›¾åƒç”Ÿæˆçš„é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚è¿™ç§æ–‡æœ¬ä»‹å¯¼çš„æ¡†æ¶åœ¨EEGCVPRæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§†è§‰è§£ç æ•ˆæœï¼Œå¹¶ä¸”ä¸å·²çŸ¥çš„ç¥ç»è®¤çŸ¥é€”å¾„æœ‰å¯è§£é‡Šçš„å¯¹é½æ€§ã€‚ä¸»è¦çš„è„‘ç”µå›¾-æ ‡é¢˜å…³è”åæ˜ äº†ä»ä¸åŒè¯­ä¹‰å±‚æ¬¡ä¸­æå–çš„è§†è§‰æ„ŸçŸ¥å›¾åƒçš„é‡è¦æ€§ã€‚æ˜¾è‘—å›¾å’Œæ—¶é—´å°ºåº¦ä¸Šçš„t-SNEæŠ•å½±æ­ç¤ºäº†å¤´çš®ä¸Šçš„è¯­ä¹‰åœ°å½¢ã€‚æˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç»“æ„åŒ–çš„è¯­ä¹‰è°ƒè§£å®ç°ä¸è®¤çŸ¥å¯¹é½çš„è„‘ç”µå›¾è§†è§‰è§£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07157v1">PDF</a> Actionable Interpretability Workshop (non-archival) at the 42   International Conference on Machine Learning</p>
<p><strong>Summary</strong><br>è§£ç å¤§è„‘ä¿¡å·ä¸­çš„è§†è§‰ä½“éªŒå¯¹ç¥ç»ç§‘å­¦å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ã€‚å°½ç®¡è„‘ç”µå›¾æŠ€æœ¯æ˜“äºè·å–ä¸”æ—¶é—´ç²¾ç¡®ï¼Œä½†åœ¨ç©ºé—´ç»†èŠ‚æ–¹é¢çš„å±€é™æ€§é˜»ç¢äº†å›¾åƒé‡å»ºã€‚æœ¬ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§åŸºäºè¯­ä¹‰æ ‡é¢˜å¯¹é½çš„æ¨¡å‹ï¼Œç»•è¿‡ç›´æ¥çš„è„‘ç”µå›¾è‡³å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚æ¨¡å‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»å¯¹è±¡çº§åˆ«åˆ°æŠ½è±¡ä¸»é¢˜çš„å„çº§è¯­ä¹‰æ ‡é¢˜ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå°†è„‘ç”µå›¾ç¼–ç å™¨æ˜ å°„åˆ°è¿™äº›æ ‡é¢˜ä¸Šã€‚åœ¨æ¨æ–­è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡æŠ•å½±å¤´è·å–çš„æ ‡é¢˜åµŒå…¥ç”¨äºè°ƒèŠ‚é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆå›¾åƒã€‚è¿™ç§æ–‡æœ¬ä»‹å¯¼çš„æ¡†æ¶åœ¨EEGCVPRæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§†è§‰è§£ç æ•ˆæœï¼Œä¸å·²çŸ¥çš„ç¥ç»è®¤çŸ¥é€”å¾„å…·æœ‰å¯è§£é‡Šçš„å¯¹é½æ€§ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†ä¸åŒè¯­ä¹‰çº§åˆ«åœ¨æ„ŸçŸ¥å›¾åƒä¸­çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†ç»“æ„åŒ–è¯­ä¹‰è°ƒè§£å¦‚ä½•ä¿ƒè¿›ä¸è®¤çŸ¥å¯¹é½çš„è„‘ç”µå›¾è§†è§‰è§£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£ç å¤§è„‘ä¿¡å·ä¸­çš„è§†è§‰ä½“éªŒå¯¹ç¥ç»ç§‘å­¦å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>è„‘ç”µå›¾æŠ€æœ¯è™½æ˜“äºè·å–ä¸”æ—¶é—´ç²¾ç¡®ï¼Œä½†åœ¨ç©ºé—´ç»†èŠ‚æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨è¯­ä¹‰æ ‡é¢˜å¯¹é½çš„æ–¹å¼ç»•è¿‡ç›´æ¥çš„è„‘ç”µå›¾è‡³å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç”¨äºç”Ÿæˆå„çº§è¯­ä¹‰æ ‡é¢˜ï¼Œä»å¯¹è±¡çº§åˆ«åˆ°æŠ½è±¡ä¸»é¢˜ã€‚</li>
<li>è„‘ç”µå›¾ç¼–ç å™¨é€šè¿‡å¯¹æ¯”å­¦ä¹ æ˜ å°„åˆ°è¿™äº›æ ‡é¢˜ä¸Šã€‚</li>
<li>æŠ•å½±å¤´è·å–çš„æ ‡é¢˜åµŒå…¥ç”¨äºè°ƒèŠ‚é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49d6141374562c9add2c28504fbb9994.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5f1f4e4c5d24b83a56dd075dedc023a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3caa5e14cad86a232b4a07716f04b51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00593c5583479a49ce771ddc58bfe6c6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CoPT-Unsupervised-Domain-Adaptive-Segmentation-using-Domain-Agnostic-Text-Embeddings"><a href="#CoPT-Unsupervised-Domain-Adaptive-Segmentation-using-Domain-Agnostic-Text-Embeddings" class="headerlink" title="CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic   Text Embeddings"></a>CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic   Text Embeddings</h2><p><strong>Authors:Cristina Mata, Kanchana Ranasinghe, Michael S. Ryoo</strong></p>
<p>Unsupervised domain adaptation (UDA) involves learning class semantics from labeled data within a source domain that generalize to an unseen target domain. UDA methods are particularly impactful for semantic segmentation, where annotations are more difficult to collect than in image classification. Despite recent advances in large-scale vision-language representation learning, UDA methods for segmentation have not taken advantage of the domain-agnostic properties of text. To address this, we present a novel Covariance-based Pixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn domain-invariant features in an image segmentation encoder. The text embeddings are generated through our LLM Domain Template process, where an LLM is used to generate source and target domain descriptions that are fed to a frozen CLIP model and combined. In experiments on four benchmarks we show that a model trained using CoPT achieves the new state of the art performance on UDA for segmentation. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/cfmata/CoPT">https://github.com/cfmata/CoPT</a>. </p>
<blockquote>
<p>æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ¶‰åŠä»æºåŸŸä¸­çš„æ ‡è®°æ•°æ®ä¸­å­¦ä¹ ç±»åˆ«è¯­ä¹‰ï¼Œå¹¶å°†å…¶æ¨å¹¿åˆ°æœªè§è¿‡çš„ç›®æ ‡åŸŸã€‚UDAæ–¹æ³•å¯¹è¯­ä¹‰åˆ†å‰²ç‰¹åˆ«æœ‰å½±å“ï¼Œå› ä¸ºåœ¨é‚£é‡Œæ”¶é›†æ³¨é‡Šæ¯”å›¾åƒåˆ†ç±»æ›´åŠ å›°éš¾ã€‚å°½ç®¡æœ€è¿‘åœ¨å¤§å‹è§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹ æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åˆ†å‰²çš„UDAæ–¹æ³•å¹¶æ²¡æœ‰åˆ©ç”¨æ–‡æœ¬çš„åŸŸæ— å…³å±æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåæ–¹å·®çš„æ–°é¢–åƒç´ æ–‡æœ¬æŸå¤±ï¼ˆCoPTï¼‰ï¼Œå®ƒä½¿ç”¨åŸŸæ— å…³çš„æ–‡æœ¬åµŒå…¥æ¥å­¦ä¹ å›¾åƒåˆ†å‰²ç¼–ç å™¨ä¸­ä¸å˜çš„ç‰¹æ€§ã€‚æ–‡æœ¬åµŒå…¥æ˜¯é€šè¿‡æˆ‘ä»¬çš„LLMåŸŸæ¨¡æ¿è¿‡ç¨‹ç”Ÿæˆçš„ï¼Œè¯¥è¿‡ç¨‹ä½¿ç”¨LLMç”Ÿæˆæºå’Œç›®æ ‡åŸŸçš„æè¿°ï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°å†»ç»“çš„CLIPæ¨¡å‹ä¸­å¹¶è¿›è¡Œç»„åˆã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨CoPTè®­ç»ƒçš„æ¨¡å‹åœ¨åˆ†å‰²UDAä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ã€‚ä»£ç å¯ä»¥åœ¨ <a target="_blank" rel="noopener" href="https://github.com/cfmata/CoPT">https://github.com/cfmata/CoPT</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07125v1">PDF</a> ECCV 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰åœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåæ–¹å·®å’Œåƒç´ æ–‡æœ¬çš„æŸå¤±å‡½æ•°CoPTã€‚è¯¥æŸå¤±å‡½æ•°åˆ©ç”¨é¢†åŸŸä¸å¯çŸ¥çš„æ–‡æœ¬åµŒå…¥æ¥åœ¨å›¾åƒåˆ†å‰²ç¼–ç å™¨ä¸­å­¦ä¹ é¢†åŸŸä¸å˜ç‰¹å¾ã€‚ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæºå’Œç›®æ ‡åŸŸæè¿°ï¼Œç»“åˆCLIPæ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨CoPTè®­ç»ƒçš„æ¨¡å‹åœ¨UDAåˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UDAæ–¹æ³•ç”¨äºè¯­ä¹‰åˆ†å‰²ï¼Œé€šè¿‡ä»æºåŸŸçš„æœ‰æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ ç±»åˆ«è¯­ä¹‰ï¼Œå¹¶æ¨å¹¿åˆ°æœªè§è¿‡çš„ç›®æ ‡åŸŸã€‚</li>
<li>å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ å–å¾—äº†è¿›å±•ï¼Œä½†UDAåˆ†å‰²æ–¹æ³•å°šæœªåˆ©ç”¨æ–‡æœ¬çš„é¢†åŸŸä¸å¯çŸ¥å±æ€§ã€‚</li>
<li>æå‡ºäº†åŸºäºåæ–¹å·®å’Œåƒç´ æ–‡æœ¬çš„æŸå¤±å‡½æ•°CoPTï¼Œåˆ©ç”¨é¢†åŸŸä¸å¯çŸ¥çš„æ–‡æœ¬åµŒå…¥æ¥å­¦ä¹ å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸä¸å˜ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨LLMç”Ÿæˆæºå’Œç›®æ ‡åŸŸæè¿°ï¼Œé€šè¿‡CLIPæ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚</li>
<li>CoPTåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†UDAåˆ†å‰²çš„æ–°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>CoPTæŸå¤±å‡½æ•°å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”ä¸åŒçš„é¢†åŸŸï¼Œæé«˜æ¨¡å‹åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a25ee390075456d8d9942483f7335814.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37b7249bc50f3fc66708e5a695309836.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b04261ad41aef8ca4a6508dee2dc5131.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7720917031f793097be2e9ab5094b47b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Hybrid-View-Attention-Network-for-Clinically-Significant-Prostate-Cancer-Classification-in-Transrectal-Ultrasound"><a href="#Hybrid-View-Attention-Network-for-Clinically-Significant-Prostate-Cancer-Classification-in-Transrectal-Ultrasound" class="headerlink" title="Hybrid-View Attention Network for Clinically Significant Prostate Cancer   Classification in Transrectal Ultrasound"></a>Hybrid-View Attention Network for Clinically Significant Prostate Cancer   Classification in Transrectal Ultrasound</h2><p><strong>Authors:Zetian Feng, Juan Fu, Xuebin Zou, Hongsheng Ye, Hong Wu, Jianhua Zhou, Yi Wang</strong></p>
<p>Prostate cancer (PCa) is a leading cause of cancer-related mortality in men, and accurate identification of clinically significant PCa (csPCa) is critical for timely intervention. Transrectal ultrasound (TRUS) is widely used for prostate biopsy; however, its low contrast and anisotropic spatial resolution pose diagnostic challenges. To address these limitations, we propose a novel hybrid-view attention (HVA) network for csPCa classification in 3D TRUS that leverages complementary information from transverse and sagittal views. Our approach integrates a CNN-transformer hybrid architecture, where convolutional layers extract fine-grained local features and transformer-based HVA models global dependencies. Specifically, the HVA comprises intra-view attention to refine features within a single view and cross-view attention to incorporate complementary information across views. Furthermore, a hybrid-view adaptive fusion module dynamically aggregates features along both channel and spatial dimensions, enhancing the overall representation. Experiments are conducted on an in-house dataset containing 590 subjects who underwent prostate biopsy. Comparative and ablation results prove the efficacy of our method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/mock1ngbrd/HVAN">https://github.com/mock1ngbrd/HVAN</a>. </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰æ˜¯ç”·æ€§ç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå‡†ç¡®è¯†åˆ«ä¸´åºŠä¸Šé‡è¦çš„å‰åˆ—è…ºç™Œï¼ˆcsPCaï¼‰å¯¹äºåŠæ—¶å¹²é¢„è‡³å…³é‡è¦ã€‚ç»ç›´è‚ è¶…å£°ï¼ˆTRUSï¼‰å¹¿æ³›åº”ç”¨äºå‰åˆ—è…ºæ´»æ£€ï¼Œä½†å…¶ä½å¯¹æ¯”åº¦å’Œå„å‘å¼‚æ€§çš„ç©ºé—´åˆ†è¾¨ç‡ç»™è¯Šæ–­å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆè§†å›¾æ³¨æ„åŠ›ï¼ˆHVAï¼‰ç½‘ç»œï¼Œç”¨äºåœ¨3D TRUSä¸­å¯¹csPCaè¿›è¡Œåˆ†ç±»ï¼Œè¯¥ç½‘ç»œåˆ©ç”¨æ¨ªæ–­é¢å’ŒçŸ¢çŠ¶é¢çš„äº’è¡¥ä¿¡æ¯è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬çš„æ–¹æ³•èåˆäº†CNN-transformeræ··åˆæ¶æ„ï¼Œå…¶ä¸­å·ç§¯å±‚æå–ç²¾ç»†çš„å±€éƒ¨ç‰¹å¾ï¼Œè€ŒåŸºäºtransformerçš„HVAæ¨¡å‹å…¨å±€ä¾èµ–æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒHVAåŒ…æ‹¬è§†å›¾å†…æ³¨æ„åŠ›ï¼Œç”¨äºç»†åŒ–å•ä¸€è§†å›¾å†…çš„ç‰¹å¾ï¼Œä»¥åŠè·¨è§†å›¾æ³¨æ„åŠ›ï¼Œç”¨äºç»“åˆä¸åŒè§†å›¾ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæ··åˆè§†å›¾è‡ªé€‚åº”èåˆæ¨¡å—æ²¿é€šé“å’Œç©ºé—´ç»´åº¦åŠ¨æ€èšåˆç‰¹å¾ï¼Œå¢å¼ºäº†æ•´ä½“è¡¨ç¤ºã€‚å®éªŒæ˜¯åœ¨åŒ…å«590åæ¥å—å‰åˆ—è…ºæ´»æ£€å—è¯•è€…çš„å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œçš„ã€‚å¯¹æ¯”å’Œæ¶ˆèç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mock1ngbrd/HVAN%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mock1ngbrd/HVANä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03421v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹ä¸‰ç»´è¶…å£°æˆåƒï¼ˆTRUSï¼‰ä¸­ä¸´åºŠæ˜¾è‘—å‰åˆ—è…ºç™Œï¼ˆcsPCaï¼‰åˆ†ç±»çš„æ··åˆè§†å›¾æ³¨æ„åŠ›ï¼ˆHVAï¼‰ç½‘ç»œã€‚è¯¥ç½‘ç»œç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå˜æ¢å™¨ï¼ˆtransformerï¼‰æ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¶…å£°æˆåƒçš„ä½å¯¹æ¯”åº¦å’Œç©ºé—´åˆ†è¾¨ç‡é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨æ¨ªæ–­é¢å’ŒçŸ¢çŠ¶é¢çš„äº’è¡¥ä¿¡æ¯ï¼Œè¯¥ç½‘ç»œé€šè¿‡æ”¹è¿›çš„å•è§†å›¾å†…æ³¨æ„åŠ›å’Œè·¨è§†å›¾æ³¨æ„åŠ›ï¼Œå®ç°äº†ç‰¹å¾ç²¾ç»†åŒ–å¹¶èå…¥äº†å¤šè§†å›¾ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæ··åˆè§†å›¾è‡ªé€‚åº”èåˆæ¨¡å—èƒ½å¤ŸåŠ¨æ€èšåˆé€šé“å’Œç©ºé—´ç»´åº¦çš„ç‰¹å¾ï¼Œæé«˜æ•´ä½“è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰æ˜¯ç”·æ€§ç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå‡†ç¡®è¯†åˆ«ä¸´åºŠæ˜¾è‘—å‰åˆ—è…ºç™Œï¼ˆcsPCaï¼‰å¯¹åŠæ—¶å¹²é¢„è‡³å…³é‡è¦ã€‚</li>
<li>ç›®å‰å¹¿æ³›ä½¿ç”¨çš„ç»ç›´è‚ è¶…å£°ï¼ˆTRUSï¼‰æˆåƒå­˜åœ¨ä½å¯¹æ¯”åº¦å’Œç©ºé—´åˆ†è¾¨ç‡é—®é¢˜ï¼Œå¯¹è¯Šæ–­æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºæ··åˆè§†å›¾æ³¨æ„åŠ›ï¼ˆHVAï¼‰ç½‘ç»œçš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸‰ç»´TRUSä¸­å¯¹csPCaè¿›è¡Œåˆ†ç±»ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå˜æ¢å™¨ï¼ˆtransformerï¼‰æ¶æ„ï¼Œä»¥æå–å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>HVAç½‘ç»œåŒ…æ‹¬å•è§†å›¾å†…æ³¨æ„åŠ›å’Œè·¨è§†å›¾æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåˆ©ç”¨ä¸åŒè§†å›¾çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡æ··åˆè§†å›¾è‡ªé€‚åº”èåˆæ¨¡å—ï¼Œç½‘ç»œèƒ½å¤Ÿåœ¨é€šé“å’Œç©ºé—´ç»´åº¦ä¸ŠåŠ¨æ€èšåˆç‰¹å¾ï¼Œå¢å¼ºæ•´ä½“æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e9d58bca4c65831e3f1d94cebd1f47b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-960f9df0d8b83b1585dd49563d69703e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e31612bda332cae3e5a263606fce478.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Zero-shot-Inexact-CAD-Model-Alignment-from-a-Single-Image"><a href="#Zero-shot-Inexact-CAD-Model-Alignment-from-a-Single-Image" class="headerlink" title="Zero-shot Inexact CAD Model Alignment from a Single Image"></a>Zero-shot Inexact CAD Model Alignment from a Single Image</h2><p><strong>Authors:Pattaramanee Arsomngern, Sasikarn Khwanmuang, Matthias NieÃŸner, Supasorn Suwajanakorn</strong></p>
<p>One practical approach to infer 3D scene structure from a single image is to retrieve a closely matching 3D model from a database and align it with the object in the image. Existing methods rely on supervised training with images and pose annotations, which limits them to a narrow set of object categories. To address this, we propose a weakly supervised 9-DoF alignment method for inexact 3D models that requires no pose annotations and generalizes to unseen categories. Our approach derives a novel feature space based on foundation features that ensure multi-view consistency and overcome symmetry ambiguities inherent in foundation features using a self-supervised triplet loss. Additionally, we introduce a texture-invariant pose refinement technique that performs dense alignment in normalized object coordinates, estimated through the enhanced feature space. We conduct extensive evaluations on the real-world ScanNet25k dataset, where our method outperforms SOTA weakly supervised baselines by +4.3% mean alignment accuracy and is the only weakly supervised approach to surpass the supervised ROCA by +2.7%. To assess generalization, we introduce SUN2CAD, a real-world test set with 20 novel object categories, where our method achieves SOTA results without prior training on them. </p>
<blockquote>
<p>ä»å•ä¸€å›¾åƒæ¨æ–­3Dåœºæ™¯ç»“æ„çš„ä¸€ç§å®ç”¨æ–¹æ³•æ˜¯ä»æ•°æ®åº“ä¸­æ£€ç´¢ä¸å›¾åƒåŒ¹é…åº¦é«˜çš„3Dæ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸å›¾åƒä¸­çš„å¯¹è±¡å¯¹é½ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºä½¿ç”¨å›¾åƒå’Œå§¿æ€æ³¨é‡Šè¿›è¡Œçš„æœ‰ç›‘ç£è®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç‰¹å®šå¯¹è±¡ç±»åˆ«ä¸­çš„ä½¿ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸ç²¾ç¡®3Dæ¨¡å‹çš„å¼±ç›‘ç£9è‡ªç”±åº¦å¯¹é½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€å§¿æ€æ³¨é‡Šï¼Œå¹¶å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„ç±»åˆ«ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºåŸºç¡€ç‰¹å¾æ„å»ºäº†ä¸€ä¸ªæ–°çš„ç‰¹å¾ç©ºé—´ï¼Œç¡®ä¿å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå¹¶ä½¿ç”¨è‡ªç›‘ç£ä¸‰å…ƒæŸå¤±å…‹æœåŸºç¡€ç‰¹å¾ä¸­çš„å›ºæœ‰å¯¹ç§°æ€§æ­§ä¹‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çº¹ç†ä¸å˜çš„å§¿æ€ä¼˜åŒ–æŠ€æœ¯ï¼Œåœ¨å½’ä¸€åŒ–å¯¹è±¡åæ ‡ä¸­è¿›è¡Œå¯†é›†å¯¹é½ï¼Œé€šè¿‡å¢å¼ºçš„ç‰¹å¾ç©ºé—´è¿›è¡Œä¼°è®¡ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•ŒScanNet25kæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾ƒå…ˆè¿›çš„å¼±ç›‘ç£åŸºçº¿æé«˜äº†+4.3%çš„å¹³å‡å¯¹é½ç²¾åº¦ï¼Œå¹¶ä¸”æ˜¯å”¯ä¸€ä¸€ä¸ªåœ¨æœªè§è¿‡çš„ç±»åˆ«ä¸Šè¶…è¶Šç›‘ç£ROCAçš„å¼±ç›‘ç£æ–¹æ³•ã€‚ä¸ºäº†è¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†SUN2CADè¿™ä¸€çœŸå®ä¸–ç•Œæµ‹è¯•é›†ï¼Œå…¶ä¸­åŒ…å«20ä¸ªæ–°å‹å¯¹è±¡ç±»åˆ«ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ²¡æœ‰äº‹å…ˆå¯¹å…¶è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03292v1">PDF</a> ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://zerocad9d.github.io/">https://zerocad9d.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§ä»å•å¹…å›¾åƒä¸­æ¨æ–­ä¸‰ç»´åœºæ™¯ç»“æ„çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»æ•°æ®åº“ä¸­æ£€ç´¢ä¸å›¾åƒä¸­çš„å¯¹è±¡ç›¸åŒ¹é…çš„ä¸‰ç»´æ¨¡å‹ï¼Œå¹¶ä¸è¯¥å¯¹è±¡è¿›è¡Œå¯¹é½æ¥å®ç°ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–äºå›¾åƒå’Œå§¿æ€æ³¨é‡Šè¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒçš„å±€é™æ€§ï¼Œè¯¥æ–‡æå‡ºäº†ä¸€ç§æ— éœ€å§¿æ€æ³¨é‡Šçš„å¼±ç›‘ç£9è‡ªç”±åº¦å¯¹é½æ–¹æ³•ï¼Œå¹¶èƒ½å¤Ÿå¯¹æœªè§è¿‡çš„ç±»åˆ«è¿›è¡Œæ³›åŒ–ã€‚è¯¥æ–¹æ³•åŸºäºåŸºç¡€ç‰¹å¾æ„å»ºäº†ä¸€ä¸ªæ–°çš„ç‰¹å¾ç©ºé—´ï¼Œç¡®ä¿å¤šè§†è§’ä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨è‡ªç›‘ç£ä¸‰å…ƒæŸå¤±å…‹æœåŸºç¡€ç‰¹å¾ä¸­çš„å›ºæœ‰å¯¹ç§°æ€§æ­§ä¹‰ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§çº¹ç†æ— å…³çš„å§¿æ€ä¼˜åŒ–æŠ€æœ¯ï¼Œåœ¨å½’ä¸€åŒ–å¯¹è±¡åæ ‡ä¸­è¿›è¡Œå¯†é›†å¯¹é½ï¼Œé€šè¿‡å¢å¼ºçš„ç‰¹å¾ç©ºé—´è¿›è¡Œä¼°è®¡ã€‚åœ¨çœŸå®ä¸–ç•Œçš„ScanNet25kæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼±ç›‘ç£åŸºå‡†ä¸Šçš„å¹³å‡å¯¹é½ç²¾åº¦æé«˜äº†4.3%ï¼Œå¹¶ä¸”æ˜¯å”¯ä¸€ä¸€ç§åœ¨æœªç»è¿™äº›ç±»åˆ«é¢„å…ˆè®­ç»ƒçš„æƒ…å†µä¸‹è¶…è¶Šæœ‰ç›‘ç£ROCAæ–¹æ³•çš„å¼±ç›‘ç£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåœ¨å…·æœ‰20ä¸ªæ–°å¯¹è±¡ç±»åˆ«çš„çœŸå®ä¸–ç•Œæµ‹è¯•é›†SUN2CADä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°çŠ¶æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ä»å•å¹…å›¾åƒä¸­æ¨æ–­ä¸‰ç»´åœºæ™¯ç»“æ„çš„æ–°æ–¹æ³•ï¼Œå³é€šè¿‡æ£€ç´¢ä¸å›¾åƒä¸­çš„å¯¹è±¡ç›¸åŒ¹é…çš„ä¸‰ç»´æ¨¡å‹å¹¶è¿›è¡Œå¯¹é½ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•é‡‡ç”¨å¼±ç›‘ç£æ–¹å¼ï¼Œæ— éœ€å§¿æ€æ³¨é‡Šï¼Œå¯ä»¥åº”ç”¨äºæœªè§è¿‡çš„å¯¹è±¡ç±»åˆ«ã€‚</li>
<li>é€šè¿‡æ„å»ºæ–°çš„ç‰¹å¾ç©ºé—´å’Œå¤šè§†è§’ä¸€è‡´æ€§ç¡®ä¿æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨è‡ªç›‘ç£ä¸‰å…ƒæŸå¤±å…‹æœåŸºç¡€ç‰¹å¾ä¸­çš„å¯¹ç§°æ€§æ­§ä¹‰é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§çº¹ç†æ— å…³çš„å§¿æ€ä¼˜åŒ–æŠ€æœ¯ï¼Œç”¨äºåœ¨å½’ä¸€åŒ–å¯¹è±¡åæ ‡ä¸­è¿›è¡Œå¯†é›†å¯¹é½ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œçš„ScanNet25kæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼±ç›‘ç£åŸºå‡†ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-57fa02051296b393bc2f0cd5cfd9fdc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-612ef56d96cca49bd213736d060b50ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b9696ad394d0de8a516ed00369ac232.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9372c7419d145c01a8267a946ea137ad.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Continual-Multiple-Instance-Learning-with-Enhanced-Localization-for-Histopathological-Whole-Slide-Image-Analysis"><a href="#Continual-Multiple-Instance-Learning-with-Enhanced-Localization-for-Histopathological-Whole-Slide-Image-Analysis" class="headerlink" title="Continual Multiple Instance Learning with Enhanced Localization for   Histopathological Whole Slide Image Analysis"></a>Continual Multiple Instance Learning with Enhanced Localization for   Histopathological Whole Slide Image Analysis</h2><p><strong>Authors:Byung Hyun Lee, Wongi Jeong, Woojae Han, Kyoungbun Lee, Se Young Chun</strong></p>
<p>Multiple instance learning (MIL) significantly reduced annotation costs via bag-level weak labels for large-scale images, such as histopathological whole slide images (WSIs). However, its adaptability to continual tasks with minimal forgetting has been rarely explored, especially on instance classification for localization. Weakly incremental learning for semantic segmentation has been studied for continual localization, but it focused on natural images, leveraging global relationships among hundreds of small patches (e.g., $16 \times 16$) using pre-trained models. This approach seems infeasible for MIL localization due to enormous amounts ($\sim 10^5$) of large patches (e.g., $256 \times 256$) and no available global relationships such as cancer cells. To address these challenges, we propose Continual Multiple Instance Learning with Enhanced Localization (CoMEL), an MIL framework for both localization and adaptability with minimal forgetting. CoMEL consists of (1) Grouped Double Attention Transformer (GDAT) for efficient instance encoding, (2) Bag Prototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling, and (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting in both bag and instance classification. Extensive experiments on three public WSI datasets demonstrate superior performance of CoMEL, outperforming the prior arts by up to $11.00%$ in bag-level accuracy and up to $23.4%$ in localization accuracy under the continual MIL setup. </p>
<blockquote>
<p>å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMILï¼‰é€šè¿‡å¤§è§„æ¨¡å›¾åƒï¼ˆå¦‚ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼‰çš„è¢‹çº§å¼±æ ‡ç­¾æ˜¾è‘—é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚ç„¶è€Œï¼Œå…¶åœ¨è¿ç»­ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ä»¥åŠæœ€å°åŒ–é—å¿˜çš„ç ”ç©¶å¾ˆå°‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ä¾‹åˆ†ç±»å®šä½æ–¹é¢çš„åº”ç”¨ã€‚é’ˆå¯¹è¯­ä¹‰åˆ†å‰²çš„å¼±å¢é‡å­¦ä¹ å·²ç»ç”¨äºè¿ç»­å®šä½ç ”ç©¶ï¼Œä½†å®ƒä¸»è¦é›†ä¸­åœ¨è‡ªç„¶å›¾åƒä¸Šï¼Œåˆ©ç”¨æ•°ç™¾ä¸ªå°è¡¥ä¸ä¹‹é—´çš„å…¨å±€å…³ç³»ï¼ˆä¾‹å¦‚ï¼Œ$16 \times 16$ï¼‰ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•å¯¹äºå¤šä»»åŠ¡å­¦ä¹ çš„å®šä½ä¼¼ä¹ä¸å¯è¡Œï¼Œå› ä¸ºå­˜åœ¨å¤§é‡ï¼ˆ$\sim 10^5$ï¼‰çš„å¤§å‹è¡¥ä¸ï¼ˆä¾‹å¦‚ï¼Œ$256 \times 256$ï¼‰ï¼Œå¹¶ä¸”æ²¡æœ‰å¯ç”¨çš„å…¨å±€å…³ç³»ï¼Œå¦‚ç™Œç»†èƒç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¢å¼ºå®šä½çš„è¿ç»­å¤šä»»åŠ¡å­¦ä¹ ï¼ˆCoMELï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå®šä½å’Œé€‚åº”æ€§çš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰æœ€å°çš„é—å¿˜æ€§ã€‚CoMELåŒ…æ‹¬ï¼ˆ1ï¼‰åˆ†ç»„åŒé‡æ³¨æ„åŠ›è½¬æ¢å™¨ï¼ˆGDATï¼‰è¿›è¡Œé«˜æ•ˆå®ä¾‹ç¼–ç ï¼Œï¼ˆ2ï¼‰åŸºäºè¢‹åŸå‹ä¼ªæ ‡ç­¾ï¼ˆBPPLï¼‰çš„å¯é å®ä¾‹ä¼ªæ ‡ç­¾åŒ–ï¼Œï¼ˆ3ï¼‰æ­£äº¤åŠ æƒä½ç§©é€‚åº”ï¼ˆOWLoRAï¼‰ä»¥å‡è½»è¢‹å’Œå®ä¾‹åˆ†ç±»ä¸­çš„é—å¿˜ã€‚åœ¨ä¸‰ä¸ªå…¬å…±WSIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoMELåœ¨è¿ç»­å¤šä»»åŠ¡è®¾ç½®ä¸‹çš„æ€§èƒ½ä¼˜äºå…ˆå‰æŠ€æœ¯ï¼Œè¢‹çº§ç²¾åº¦æé«˜é«˜è¾¾$11.00%$ï¼Œå®šä½ç²¾åº¦æé«˜é«˜è¾¾$23.4%$ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02395v2">PDF</a> Accepted at ICCV 2025</p>
<p><strong>Summary</strong><br>    å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡å›¾åƒï¼ˆå¦‚ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼‰çš„è¢‹çº§å¼±æ ‡ç­¾å¤§å¹…é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚é’ˆå¯¹è¿ç»­ä»»åŠ¡ä¸­çš„é—å¿˜é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å®ä¾‹åˆ†ç±»çš„å®šä½é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é¢å‘è¯­ä¹‰åˆ†å‰²çš„å¼±å¢é‡å­¦ä¹ æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¯¹äºå¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰å®šä½ï¼Œç”±äºå…¶å¤§é‡çš„å¤§å‹è¡¥ä¸å’Œç¼ºä¹å¯ç”¨çš„å…¨å±€å…³ç³»ï¼ˆå¦‚ç™Œç»†èƒï¼‰ï¼Œè¿™ç§æ–¹æ³•ä¼¼ä¹å¹¶ä¸å¯è¡Œã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰æœ€å°é—å¿˜é€‚åº”æ€§çš„æŒç»­å¤šå®ä¾‹å­¦ä¹ å¢å¼ºå®šä½ï¼ˆCoMELï¼‰ã€‚CoMELåŒ…æ‹¬ï¼ˆ1ï¼‰åˆ†ç»„åŒæ³¨æ„åŠ›è½¬æ¢å™¨ï¼ˆGDATï¼‰è¿›è¡Œé«˜æ•ˆå®ä¾‹ç¼–ç ï¼Œï¼ˆ2ï¼‰åŸºäºè¢‹åŸå‹ä¼ªæ ‡ç­¾ï¼ˆBPPLï¼‰çš„å¯é å®ä¾‹ä¼ªæ ‡ç­¾ç”Ÿæˆï¼Œä»¥åŠï¼ˆ3ï¼‰æ­£äº¤åŠ æƒä½ç§©é€‚åº”ï¼ˆOWLoRAï¼‰æ¥ç¼“è§£åŒ…å’Œå®ä¾‹åˆ†ç±»ä¸­çš„é—å¿˜é—®é¢˜ã€‚åœ¨ä¸‰ä¸ªå…¬å…±WSIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoMELåœ¨è¿ç»­MILè®¾ç½®ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨è¢‹çº§ç²¾åº¦ä¸Šæ¯”ç°æœ‰æŠ€æœ¯é«˜å‡ºæœ€å¤š11.00ï¼…ï¼Œåœ¨å®šä½ç²¾åº¦ä¸Šé«˜å‡ºæœ€å¤š23.4ï¼…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡å›¾åƒçš„è¢‹çº§å¼±æ ‡ç­¾é™ä½äº†æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>è¿ç»­ä»»åŠ¡ä¸­çš„é—å¿˜é—®é¢˜åœ¨å¤šå®ä¾‹å­¦ä¹ å®šä½ä¸­å°¤ä¸ºçªå‡ºã€‚</li>
<li>æå‡ºçš„CoMELæ¡†æ¶ç»“åˆäº†åˆ†ç»„åŒæ³¨æ„åŠ›è½¬æ¢å™¨ï¼ˆGDATï¼‰ã€åŸºäºè¢‹åŸå‹ä¼ªæ ‡ç­¾ï¼ˆBPPLï¼‰å’Œæ­£äº¤åŠ æƒä½ç§©é€‚åº”ï¼ˆOWLoRAï¼‰æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>GDATç”¨äºé«˜æ•ˆå®ä¾‹ç¼–ç ã€‚</li>
<li>BPPLç”¨äºç”Ÿæˆå¯é çš„å®ä¾‹ä¼ªæ ‡ç­¾ã€‚</li>
<li>OWLoRAç”¨äºç¼“è§£åŒ…å’Œå®ä¾‹åˆ†ç±»ä¸­çš„é—å¿˜é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68a5c3fbecc612f74e9ccabc1dc08ca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce80285f8f19fdbc397a2c8e639d397f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c91cd515a55ec54a3a28abf5040c3576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2611ff4095521b9d64a369ffe9f1ee09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f12a2412220c9653945a241cd318447.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Are-Vision-Transformer-Representations-Semantically-Meaningful-A-Case-Study-in-Medical-Imaging"><a href="#Are-Vision-Transformer-Representations-Semantically-Meaningful-A-Case-Study-in-Medical-Imaging" class="headerlink" title="Are Vision Transformer Representations Semantically Meaningful? A Case   Study in Medical Imaging"></a>Are Vision Transformer Representations Semantically Meaningful? A Case   Study in Medical Imaging</h2><p><strong>Authors:Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu</strong></p>
<p>Vision transformers (ViTs) have rapidly gained prominence in medical imaging tasks such as disease classification, segmentation, and detection due to their superior accuracy compared to conventional deep learning models. However, due to their size and complex interactions via the self-attention mechanism, they are not well understood. In particular, it is unclear whether the representations produced by such models are semantically meaningful. In this paper, using a projected gradient-based algorithm, we show that their representations are not semantically meaningful and they are inherently vulnerable to small changes. Images with imperceptible differences can have very different representations; on the other hand, images that should belong to different semantic classes can have nearly identical representations. Such vulnerability can lead to unreliable classification results; for example, unnoticeable changes cause the classification accuracy to be reduced by over 60%. %. To the best of our knowledge, this is the first work to systematically demonstrate this fundamental lack of semantic meaningfulness in ViT representations for medical image classification, revealing a critical challenge for their deployment in safety-critical systems. </p>
<blockquote>
<p>è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰å› å…¶ç›¸å¯¹äºä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å“è¶Šå‡†ç¡®æ€§ï¼Œåœ¨åŒ»å­¦æˆåƒä»»åŠ¡ï¼ˆå¦‚ç–¾ç—…åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ï¼‰ä¸­è¿…é€Ÿå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºå…¶è§„æ¨¡å’Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶äº§ç”Ÿçš„å¤æ‚äº¤äº’ï¼Œäººä»¬å¯¹å…¶ç†è§£å¹¶ä¸å……åˆ†ã€‚å°¤å…¶ä¸æ¸…æ¥šçš„æ˜¯ï¼Œæ­¤ç±»æ¨¡å‹äº§ç”Ÿçš„è¡¨ç¤ºæ˜¯å¦å…·æœ‰è¯­ä¹‰æ„ä¹‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºæŠ•å½±çš„æ¢¯åº¦ç®—æ³•æ¥å±•ç¤ºï¼Œä»–ä»¬çš„è¡¨ç¤ºä¸å…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼Œå¹¶ä¸”æœ¬è´¨ä¸Šå®¹æ˜“å—å¾®å°å˜åŒ–çš„å½±å“ã€‚å…·æœ‰éš¾ä»¥å¯Ÿè§‰å·®å¼‚çš„å›¾åƒå¯ä»¥å…·æœ‰å®Œå…¨ä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼›å¦ä¸€æ–¹é¢ï¼Œæœ¬åº”å±äºä¸åŒè¯­ä¹‰ç±»çš„å›¾åƒå´å¯ä»¥æœ‰å‡ ä¹ç›¸åŒçš„è¡¨ç¤ºå½¢å¼ã€‚è¿™ç§è„†å¼±æ€§å¯èƒ½å¯¼è‡´åˆ†ç±»ç»“æœä¸å¯é ï¼›ä¾‹å¦‚ï¼Œä¸æ˜æ˜¾çš„å˜åŒ–å¯¼è‡´åˆ†ç±»ç²¾åº¦é™ä½è¶…è¿‡60%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹ç³»ç»Ÿå±•ç¤ºåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ViTè¡¨ç¤ºç¼ºä¹åŸºæœ¬è¯­ä¹‰æ„ä¹‰çš„å·¥ä½œï¼Œè¿™æ­ç¤ºäº†å°†å…¶éƒ¨ç½²åœ¨å®‰å…¨å…³é”®ç³»ç»Ÿä¸­çš„é‡å¤§æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01788v2">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>ViTæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šå‡†ç¡®æ€§ï¼Œä½†å­˜åœ¨è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚æœ¬æ–‡åˆ©ç”¨æ¢¯åº¦æŠ•å½±ç®—æ³•æ˜¾ç¤ºViTæ¨¡å‹çš„è¡¨ç¤ºä¸å…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼Œå®¹æ˜“å—åˆ°å¾®å°å˜åŒ–çš„å¹²æ‰°ã€‚ç›¸åŒè¯­ä¹‰ç±»åˆ«çš„å›¾åƒå¯èƒ½å…·æœ‰ä¸åŒçš„è¡¨ç¤ºï¼Œä¸åŒè¯­ä¹‰ç±»åˆ«çš„å›¾åƒå¯èƒ½å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºï¼Œå¯¼è‡´åˆ†ç±»ç»“æœä¸å¯é ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViTæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒä»»åŠ¡ä¸­å…·æœ‰é«˜å‡†ç¡®æ€§ã€‚</li>
<li>ViTæ¨¡å‹çš„è¡¨ç¤ºç¼ºä¹è¯­ä¹‰æ„ä¹‰ã€‚</li>
<li>ViTæ¨¡å‹å®¹æ˜“å—åˆ°å¾®å°å˜åŒ–çš„å½±å“ï¼Œå¯¼è‡´ä¸åŒçš„è¡¨ç¤ºã€‚</li>
<li>ç›¸åŒè¯­ä¹‰ç±»åˆ«çš„å›¾åƒå¯èƒ½å…·æœ‰ä¸åŒçš„è¡¨ç¤ºã€‚</li>
<li>ä¸åŒè¯­ä¹‰ç±»åˆ«çš„å›¾åƒå¯èƒ½å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºã€‚</li>
<li>è¿™ç§ç¼ºä¹è¯­ä¹‰æ„ä¹‰çš„é—®é¢˜ä¼šå¯¼è‡´åˆ†ç±»ç»“æœä¸å¯é ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01788">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a0163c81c6d97087756c12b3854bc25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5c3198cd89c601336447e74c06bce8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6551925d301b74203057df105206caae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28103e4f2932f18b4ac7db0d2501fbb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7688f3c9db5da72c0685988635e14749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd2c7411e81b619f5340b8105fad2ea6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Bridging-Classical-and-Learning-based-Iterative-Registration-through-Deep-Equilibrium-Models"><a href="#Bridging-Classical-and-Learning-based-Iterative-Registration-through-Deep-Equilibrium-Models" class="headerlink" title="Bridging Classical and Learning-based Iterative Registration through   Deep Equilibrium Models"></a>Bridging Classical and Learning-based Iterative Registration through   Deep Equilibrium Models</h2><p><strong>Authors:Yi Zhang, Yidong Zhao, Qian Tao</strong></p>
<p>Deformable medical image registration is traditionally formulated as an optimization problem. While classical methods solve this problem iteratively, recent learning-based approaches use recurrent neural networks (RNNs) to mimic this process by unrolling the prediction of deformation fields in a fixed number of steps. However, classical methods typically converge after sufficient iterations, but learning-based unrolling methods lack a theoretical convergence guarantee and show instability empirically. In addition, unrolling methods have a practical bottleneck at training time: GPU memory usage grows linearly with the unrolling steps due to backpropagation through time (BPTT). To address both theoretical and practical challenges, we propose DEQReg, a novel registration framework based on Deep Equilibrium Models (DEQ), which formulates registration as an equilibrium-seeking problem, establishing a natural connection between classical optimization and learning-based unrolling methods. DEQReg maintains constant memory usage, enabling theoretically unlimited iteration steps. Through extensive evaluation on the public brain MRI and lung CT datasets, we show that DEQReg can achieve competitive registration performance, while substantially reducing memory consumption compared to state-of-the-art unrolling methods. We also reveal an intriguing phenomenon: the performance of existing unrolling methods first increases slightly then degrades irreversibly when the inference steps go beyond the training configuration. In contrast, DEQReg achieves stable convergence with its inbuilt equilibrium-seeking mechanism, bridging the gap between classical optimization-based and modern learning-based registration methods. </p>
<blockquote>
<p>ä¼ ç»Ÿä¸Šï¼Œå¯å˜å½¢åŒ»å­¦å›¾åƒé…å‡†è¢«åˆ¶å®šä¸ºä¸€ä¸ªä¼˜åŒ–é—®é¢˜ã€‚è™½ç„¶ç»å…¸æ–¹æ³•é€šè¿‡è¿­ä»£æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æœ€è¿‘çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ¥æ¨¡æ‹Ÿè¿™ä¸ªè¿‡ç¨‹ï¼Œé€šè¿‡åœ¨å›ºå®šæ•°é‡çš„æ­¥éª¤ä¸­å±•å¼€å˜å½¢åœºçš„é¢„æµ‹ã€‚ç„¶è€Œï¼Œç»å…¸æ–¹æ³•é€šå¸¸åœ¨è¶³å¤Ÿçš„è¿­ä»£åæ”¶æ•›ï¼Œä½†åŸºäºå±•å¼€çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ç¼ºä¹ç†è®ºæ”¶æ•›ä¿è¯ï¼Œå¹¶ä¸”åœ¨ç»éªŒä¸Šæ˜¾ç¤ºå‡ºä¸ç¨³å®šã€‚æ­¤å¤–ï¼Œå±•å¼€æ–¹æ³•åœ¨è®­ç»ƒæ—¶é—´æ–¹é¢å­˜åœ¨å®é™…ç“¶é¢ˆï¼šç”±äºæ—¶é—´åå‘ä¼ æ’­ï¼ˆBPTTï¼‰ï¼ŒGPUå†…å­˜ä½¿ç”¨é‡ä¸å±•å¼€æ­¥éª¤çº¿æ€§å¢é•¿ã€‚ä¸ºäº†è§£å†³ç†è®ºå’Œå®è·µä¸Šçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ·±åº¦å‡è¡¡æ¨¡å‹ï¼ˆDEQï¼‰çš„æ–°å‹é…å‡†æ¡†æ¶DEQRegï¼Œå®ƒå°†é…å‡†åˆ¶å®šä¸ºå‡è¡¡æ±‚è§£é—®é¢˜ï¼Œåœ¨ç»å…¸ä¼˜åŒ–å’ŒåŸºäºå­¦ä¹ çš„å±•å¼€æ–¹æ³•ä¹‹é—´å»ºç«‹äº†è‡ªç„¶è”ç³»ã€‚DEQRegä¿æŒæ’å®šçš„å†…å­˜ä½¿ç”¨ï¼Œæ”¯æŒç†è®ºä¸Šçš„æ— é™è¿­ä»£æ­¥éª¤ã€‚é€šè¿‡å¯¹å…¬å…±è„‘éƒ¨MRIå’Œè‚ºéƒ¨CTæ•°æ®é›†çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†DEQRegå¯ä»¥å®ç°å…·æœ‰ç«äº‰åŠ›çš„é…å‡†æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†ä¸æœ€æ–°å±•å¼€æ–¹æ³•ç›¸æ¯”çš„å†…å­˜æ¶ˆè€—ã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šå½“æ¨æ–­æ­¥éª¤è¶…å‡ºè®­ç»ƒé…ç½®æ—¶ï¼Œç°æœ‰å±•å¼€æ–¹æ³•çš„æ€§èƒ½ä¼šå…ˆç•¥æœ‰æé«˜ç„¶åä¸å¯é€†è½¬åœ°ä¸‹é™ã€‚ä¸æ­¤ç›¸åï¼ŒDEQRegåˆ©ç”¨å…¶å†…ç½®çš„å‡è¡¡æ±‚è§£æœºåˆ¶å®ç°äº†ç¨³å®šçš„æ”¶æ•›ï¼Œç¼©å°äº†åŸºäºç»å…¸ä¼˜åŒ–çš„ç°ä»£å­¦ä¹ å’ŒåŸºäºå­¦ä¹ çš„é…å‡†æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00582v2">PDF</a> Submitted version. Accepted by MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>    å˜å½¢åŒ»å­¦å›¾åƒé…å‡†å¯çœ‹ä½œä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•æ˜¯è¿­ä»£è§£å†³ã€‚è¿‘å¹´å‡ºç°çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•é‡‡ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ¨¡æ‹Ÿæ­¤è¿‡ç¨‹ã€‚ä½†ç°æœ‰å­¦ä¹ æ–¹æ³•çš„è¿­ä»£è¿‡ç¨‹ç¼ºä¹ç†è®ºæ”¶æ•›ä¿è¯ï¼Œä¸”å­˜åœ¨ä¸ç¨³å®šç°è±¡ã€‚æ­¤å¤–ï¼Œå…¶è®­ç»ƒæ—¶çš„GPUå†…å­˜ä½¿ç”¨éšè¿­ä»£æ­¥éª¤çº¿æ€§å¢é•¿ï¼Œå­˜åœ¨å®é™…ç“¶é¢ˆã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåŸºäºæ·±åº¦å‡è¡¡æ¨¡å‹ï¼ˆDEQï¼‰çš„DEQRegæ¡†æ¶ï¼Œå°†é…å‡†çœ‹ä½œä¸€ä¸ªå¹³è¡¡å¯»æ‰¾é—®é¢˜ï¼Œåœ¨ç»å…¸ä¼˜åŒ–ä¸å­¦ä¹ å±•å¼€æ–¹æ³•é—´å»ºç«‹è‡ªç„¶è”ç³»ã€‚DEQRegä¿æŒæ’å®šå†…å­˜ä½¿ç”¨ï¼Œå®ç°ç†è®ºä¸Šæ— é™è¿­ä»£æ­¥éª¤ã€‚åœ¨å…¬å…±è„‘MRIå’Œè‚ºCTæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDEQRegå¯å®ç°ç«äº‰æ€§çš„é…å‡†æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘å†…å­˜æ¶ˆè€—ï¼Œä¼˜äºæœ€æ–°å±•å¼€æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å‘ç°ä¸€ä¸ªæœ‰è¶£ç°è±¡ï¼šç°æœ‰å±•å¼€æ–¹æ³•çš„æ€§èƒ½åœ¨æ¨ç†æ­¥éª¤è¶…å‡ºè®­ç»ƒé…ç½®æ—¶ä¼šå…ˆç•¥å¾®æå‡ç„¶åä¸å¯é€†åœ°ä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDEQRegå‡­å€Ÿå†…ç½®çš„å¹³è¡¡å¯»æ‰¾æœºåˆ¶å®ç°äº†ç¨³å®šçš„æ”¶æ•›ï¼Œç¼©å°äº†åŸºäºç»å…¸ä¼˜åŒ–å’Œç°ä»£åŸºäºå­¦ä¹ çš„é…å‡†æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å˜å½¢åŒ»å­¦å›¾åƒé…å‡†è¢«è§†ä¸ºä¼˜åŒ–é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•é€šè¿‡è¿­ä»£è§£å†³ã€‚</li>
<li>åŸºäºå­¦ä¹ çš„æ–¹æ³•ä½¿ç”¨RNNæ¨¡æ‹Ÿé…å‡†è¿‡ç¨‹ï¼Œä½†ç¼ºä¹ç†è®ºæ”¶æ•›ä¿è¯ä¸”è¡¨ç°ä¸ç¨³å®šã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨GPUå†…å­˜ä½¿ç”¨éšè¿­ä»£æ­¥éª¤çº¿æ€§å¢é•¿çš„å®è·µç“¶é¢ˆã€‚</li>
<li>DEQRegæ¡†æ¶åŸºäºæ·±åº¦å‡è¡¡æ¨¡å‹ï¼ˆDEQï¼‰ï¼Œå°†é…å‡†è§†ä¸ºå¹³è¡¡å¯»æ‰¾é—®é¢˜ï¼Œç»“åˆäº†ä¼ ç»Ÿä¸ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>DEQRegå®ç°äº†ç†è®ºä¸Šæ— é™çš„è¿­ä»£æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒæ’å®šçš„å†…å­˜ä½¿ç”¨ã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºDEQRegæ€§èƒ½å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”ç›¸æ¯”æœ€æ–°æ–¹æ³•å¤§å¹…å‡å°‘å†…å­˜æ¶ˆè€—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-86dc726b990cf0045fb8411c2cd32cb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0e7bb0286ecc7dd5ce68ddff8dca304.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ed80a53b1c8445273e77bc9c107fcd9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Medical-Image-Segmentation-Using-Advanced-Unet-VMSE-Unet-and-VM-Unet-CBAM"><a href="#Medical-Image-Segmentation-Using-Advanced-Unet-VMSE-Unet-and-VM-Unet-CBAM" class="headerlink" title="Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet   CBAM+"></a>Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet   CBAM+</h2><p><strong>Authors:Sayandeep Kanrar, Raja Piyush, Qaiser Razi, Debanshi Chakraborty, Vikas Hassija, GSS Chalapathi</strong></p>
<p>In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two cutting-edge deep learning architectures designed to enhance medical image segmentation. Our approach integrates Squeeze-and-Excitation (SE) and Convolutional Block Attention Module (CBAM) techniques into the traditional VM U-Net framework, significantly improving segmentation accuracy, feature localization, and computational efficiency. Both models show superior performance compared to the baseline VM-Unet across multiple datasets. Notably, VMSEUnet achieves the highest accuracy, IoU, precision, and recall while maintaining low loss values. It also exhibits exceptional computational efficiency with faster inference times and lower memory usage on both GPU and CPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a valuable tool for medical image analysis. These findings highlight its potential for real-world clinical applications, emphasizing the importance of further research to optimize accuracy, robustness, and computational efficiency. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VMSE U-Netå’ŒVM-Unet CBAM+æ¨¡å‹ï¼Œè¿™æ˜¯ä¸¤ç§å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†Squeeze-and-Excitationï¼ˆSEï¼‰å’Œå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰æŠ€æœ¯é›†æˆåˆ°ä¼ ç»Ÿçš„VM U-Netæ¡†æ¶ä¸­ï¼Œæ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦ã€ç‰¹å¾å®šä½èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚ä¸åŸºçº¿VM-Unetç›¸æ¯”ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨å¤šæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVMSE U-Netåœ¨ä¿æŒä½æŸå¤±å€¼çš„åŒæ—¶ï¼Œå®ç°äº†æœ€é«˜çš„å‡†ç¡®åº¦ã€IoUã€ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨GPUå’ŒCPUä¸Šéƒ½è¡¨ç°å‡ºéå‡¡çš„è®¡ç®—æ•ˆç‡ï¼Œæ¨ç†æ—¶é—´æ›´å¿«ï¼Œå†…å­˜ä½¿ç”¨æ›´ä½ã€‚æ€»ä½“è€Œè¨€ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¢å¼ºå‹æ¶æ„VMSE-Unetæ˜¯åŒ»å­¦å›¾åƒåˆ†æçš„æœ‰ç”¨å·¥å…·ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å…¶åœ¨çœŸå®ä¸–ç•Œä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥ä¼˜åŒ–å‡†ç¡®åº¦ã€ç¨³å¥æ€§å’Œè®¡ç®—æ•ˆç‡çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00511v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VMSE U-Netå’ŒVM-Unet CBAM+ä¸¤ä¸ªå…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå®ƒä»¬è¢«è®¾è®¡ç”¨äºæé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚é€šè¿‡é›†æˆSqueeze-and-Excitationï¼ˆSEï¼‰å’Œå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰æŠ€æœ¯åˆ°ä¼ ç»Ÿçš„VM U-Netæ¡†æ¶ä¸­ï¼Œæ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦ã€ç‰¹å¾å®šä½èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚ç›¸è¾ƒäºåŸºç¡€VM-Unetæ¨¡å‹ï¼Œä¸¤ä¸ªæ¨¡å‹åœ¨å¤šæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯VMSE U-Netåœ¨å‡†ç¡®åº¦ã€IoUã€ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¸Šå–å¾—äº†æœ€é«˜æˆç»©ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„æŸå¤±å€¼ã€‚å®ƒåœ¨GPUå’ŒCPUä¸Šå‡å…·æœ‰å‡ºè‰²çš„è®¡ç®—æ•ˆç‡ï¼Œæ¨ç†é€Ÿåº¦å¿«ã€å†…å­˜å ç”¨ä½ã€‚æ€»ä½“è€Œè¨€ï¼Œä¼˜åŒ–çš„VMSE U-Netæ¶æ„å¯¹äºåŒ»å­¦å›¾åƒåˆ†æå…·æœ‰é‡è¦ä»·å€¼ï¼Œå…·æœ‰å®é™…åº”ç”¨äºä¸´åºŠçš„æ½œåŠ›ã€‚ç ”ç©¶å¼ºè°ƒäº†è¿›ä¸€æ­¥ä¼˜åŒ–å…¶å‡†ç¡®æ€§ã€ç¨³å¥æ€§å’Œè®¡ç®—æ•ˆç‡çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ®µæ–‡æœ¬çš„ä¸»è¦è§‚ç‚¹æˆ–è§è§£ï¼š</p>
<ul>
<li>VMSE U-Netå’ŒVM-Unet CBAM+æ˜¯è®¾è®¡ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚</li>
<li>å®ƒä»¬é›†æˆäº†Squeeze-and-Excitationï¼ˆSEï¼‰å’Œå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰æŠ€æœ¯ï¼Œä»¥æé«˜åˆ†å‰²ç²¾åº¦ã€ç‰¹å¾å®šä½å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>å¯¹æ¯”åŸºç¡€VM-Unetæ¨¡å‹ï¼Œä¸¤ç§æ¨¡å‹åœ¨å¤šæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>VMSE U-Netåœ¨å‡†ç¡®åº¦ã€IoUã€ç²¾ç¡®åº¦å’Œå¬å›ç‡ç­‰å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰è¾ƒä½æŸå¤±å€¼ã€‚</li>
<li>VMSE U-Netæ¶æ„åœ¨GPUå’ŒCPUä¸Šéƒ½è¡¨ç°å‡ºä¼˜ç§€çš„è®¡ç®—æ•ˆç‡ï¼Œå…·æœ‰å¿«é€Ÿçš„æ¨ç†é€Ÿåº¦å’Œä½å†…å­˜å ç”¨ã€‚</li>
<li>æ•´ä½“è€Œè¨€ï¼ŒVMSE U-Netæ¶æ„å¯¹äºåŒ»å­¦å›¾åƒåˆ†æå…·æœ‰å®ç”¨ä»·å€¼ï¼Œå¹¶æœ‰æœ›åº”ç”¨äºå®é™…ä¸´åºŠç¯å¢ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-84e2da153e3c2669a7fa7aac050d9ea0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-084968aa2c56e186772b8e72690c106c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb2ae97f6eecf0cc70214a69e8a720f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-279aaa8202d90a7385d80cf25005b568.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39ee2e730dd0afc94ae3de3a3063c8b5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation"><a href="#Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation" class="headerlink" title="Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation"></a>Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation</h2><p><strong>Authors:Fangyijie Wang, Kevin Whelan, FÃ©lix Balado, Kathleen M. Curran, GuÃ©nolÃ© Silvestre</strong></p>
<p>Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66% and 94.38% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ•°æ®ç”±äºéšç§å’Œç›‘ç®¡é™åˆ¶ï¼Œå…¶å¯è®¿é—®æ€§æ¯”å…¶ä»–é¢†åŸŸçš„æ•°æ®è¦ä½ã€‚æ­¤å¤–ï¼Œæ ‡æ³¨éœ€è¦ä¸´åºŠä¸“å®¶è¿›è¡Œæ˜‚è´µä¸”è€—æ—¶çš„æ‰‹åŠ¨å›¾åƒæ³¨é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼ŒåˆæˆåŒ»å­¦æ•°æ®ç”Ÿæˆæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚é‡‡ç”¨ç”Ÿæˆå¼æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰å·²è¢«è¯æ˜èƒ½å¤Ÿäº§ç”Ÿé€¼çœŸçš„åˆæˆå›¾åƒã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„maskå¼•å¯¼GenAIæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆä¸åˆ†å‰²æ©è†œé…å¯¹çš„åˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒã€‚è¿™äº›åˆæˆå›¾åƒå¯¹çœŸå®æ•°æ®é›†è¿›è¡Œäº†æ‰©å……ï¼Œç”¨äºç›‘ç£å¾®è°ƒSegment Anything Modelï¼ˆSAMï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåˆæˆæ•°æ®æœ‰æ•ˆåœ°æ•æ‰äº†çœŸå®å›¾åƒçš„ç‰¹å¾ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨æœ‰é™æ•°é‡çš„çœŸå®å›¾åƒ-æ©è†œå¯¹è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨æ¥è‡ªè¥¿ç­ç‰™å’Œéæ´²é˜Ÿåˆ—çš„å°‘é‡è¶…å£°å›¾åƒï¼Œåˆ†å‰²è¾¾åˆ°Diceåˆ†æ•°åˆ†åˆ«ä¸º94.66%å’Œ94.38%ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å‡å¯åœ¨GitHubä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23664v2">PDF</a> Accepted at Irish Machine Vision and Image Processing Conference   (IMVIP) 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸé¢ä¸´éšç§å’Œç›‘ç®¡é™åˆ¶å¯¼è‡´æ•°æ®è®¿é—®å›°éš¾ä»¥åŠæ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆå¸¦æœ‰åˆ†å‰²æ©ç çš„åˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒã€‚åˆæˆæ•°æ®æœ‰æ•ˆæ•æ‰çœŸå®å›¾åƒç‰¹å¾ï¼Œå¹¶ç”¨äºç›‘ç£ç²¾ç»†è°ƒæ•´SAMæ¨¡å‹ã€‚æœ¬ç ”ç©¶åœ¨æœ‰é™çœŸå®å›¾åƒ-æ©ç å¯¹çš„æƒ…å†µä¸‹å–å¾—äº†é¢†å…ˆæ°´å¹³çš„èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœï¼ŒDiceå¾—åˆ†è¾¾åˆ°94.66%å’Œ94.38%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ•°æ®ç”±äºéšç§å’Œç›‘ç®¡é™åˆ¶è€Œä¸æ˜“è·å–ã€‚</li>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰å¯è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹ç”ŸæˆåˆæˆåŒ»å­¦å›¾åƒã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨æ–°å‹mask-guided GenAIæ–¹æ³•ï¼Œç”Ÿæˆåˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒåŠåˆ†å‰²æ©ç ã€‚</li>
<li>åˆæˆæ•°æ®å¯æœ‰æ•ˆæ•æ‰çœŸå®å›¾åƒç‰¹å¾ã€‚</li>
<li>æ­¤æ–¹æ³•å®ç°äº†å…ˆè¿›çš„èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®å›¾åƒ-æ©ç å¯¹æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>åˆæˆæ•°æ®å¯¹æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆæœèµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚</li>
<li>ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0bb205423e332b4317b0c0b676f0f8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6e2e20b23c412a54f5b1a6d39a4562d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e224c2d283809a9b64b55df257ff009.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-12/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-999b2aed19ba40817b043ed7a7181add.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  Toward Real-World Chinese Psychological Support Dialogues CPsDD Dataset   and a Co-Evolving Multi-Agent System
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-12/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c77d2f231bcd03bfb13565de40981da4.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  PWD Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle   CT
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
