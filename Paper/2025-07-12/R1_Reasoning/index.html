<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  Traceable Evidence Enhanced Visual Grounded Reasoning Evaluation and   Methodology">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5bf8838811dda82d849d4705d479bce7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-12-æ›´æ–°"><a href="#2025-07-12-æ›´æ–°" class="headerlink" title="2025-07-12 æ›´æ–°"></a>2025-07-12 æ›´æ–°</h1><h2 id="Traceable-Evidence-Enhanced-Visual-Grounded-Reasoning-Evaluation-and-Methodology"><a href="#Traceable-Evidence-Enhanced-Visual-Grounded-Reasoning-Evaluation-and-Methodology" class="headerlink" title="Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and   Methodology"></a>Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and   Methodology</h2><p><strong>Authors:Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, Zhuochen Wang, Zhaoxiang Zhang</strong></p>
<p>Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human â€œthinking with imagesâ€. However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Haochen-Wang409/TreeVGR">https://github.com/Haochen-Wang409/TreeVGR</a>. </p>
<blockquote>
<p>åƒOpenAI-o3è¿™æ ·çš„æ¨¡å‹é€šè¿‡åŠ¨æ€å‚è€ƒè§†è§‰åŒºåŸŸæ¥å¼€åˆ›è§†è§‰æ¨ç†çš„å…ˆæ²³ï¼Œå°±åƒäººç±»â€œä»¥å›¾åƒæ€è€ƒâ€ä¸€æ ·ã€‚ç„¶è€Œï¼Œè¿˜æ²¡æœ‰ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ¥è¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†TreeBenchï¼ˆå¯è¿½æº¯è¯æ®è¯„ä¼°åŸºå‡†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸‰ä¸ªåŸåˆ™çš„è¯Šæ–­åŸºå‡†ï¼šï¼ˆ1ï¼‰å¯¹å¤æ‚åœºæ™¯ä¸­ç»†å¾®ç›®æ ‡çš„é›†ä¸­è§†è§‰æ„ŸçŸ¥ï¼›ï¼ˆ2ï¼‰é€šè¿‡è¾¹ç•Œæ¡†è¯„ä¼°çš„å¯è¿½æº¯è¯æ®ï¼›ï¼ˆ3ï¼‰äºŒé˜¶æ¨ç†ï¼Œä»¥æµ‹è¯•å¯¹è±¡äº¤äº’å’Œç©ºé—´å±‚æ¬¡ç»“æ„ï¼Œè€Œä¸ä»…ä»…æ˜¯ç®€å•çš„å¯¹è±¡å®šä½ã€‚æˆ‘ä»¬ä¼˜å…ˆå¤„ç†å¯†é›†ç‰©ä½“çš„å›¾åƒï¼Œæœ€åˆä»SA-1Bä¸­é‡‡æ ·äº†1Kå¼ é«˜è´¨é‡å›¾åƒï¼Œå¹¶èå…¥äº†å…«ä½LMMä¸“å®¶ä¸ºæ¯å¼ å›¾åƒæ‰‹åŠ¨æ ‡æ³¨é—®é¢˜ã€å€™é€‰é€‰é¡¹å’Œç­”æ¡ˆã€‚ç»è¿‡ä¸‰ä¸ªé˜¶æ®µçš„è´¨é‡æ§åˆ¶ï¼ŒTreeBenchåŒ…å«äº†405ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰é—®ç­”å¯¹ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¿™ä¸ªåŸºå‡†ä¸Šä¹Ÿä¼šé‡åˆ°å›°éš¾ï¼Œå…¶ä¸­æ²¡æœ‰å“ªä¸ªæ¨¡å‹çš„å‡†ç¡®ç‡èƒ½è¾¾åˆ°60%ï¼Œä¾‹å¦‚OpenAI-o3çš„å¾—åˆ†ä»…ä¸º54.87ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†TreeVGRï¼ˆå¯è¿½æº¯è¯æ®å¢å¼ºè§†è§‰æ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨å¼ºåŒ–å­¦ä¹ è”åˆç›‘ç£å®šä½å’Œæ¨ç†çš„è®­ç»ƒèŒƒå¼ï¼Œèƒ½å¤Ÿå®ç°å‡†ç¡®çš„å®šä½å’Œå¯è§£é‡Šçš„æ¨ç†è·¯å¾„ã€‚ä»Qwen2.5-VL-7Bå¼€å§‹åˆå§‹åŒ–ï¼Œå®ƒåœ¨V* Benchï¼ˆ+16.8ï¼‰ã€MME-RealWorldï¼ˆ+12.6ï¼‰å’ŒTreeBenchï¼ˆ+13.4ï¼‰ä¸Šéƒ½æœ‰æ‰€æ”¹è¿›ï¼Œè¯æ˜è¿½æº¯æ€§æ˜¯æ¨è¿›è§†è§‰æ¨ç†çš„å…³é”®ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Haochen-Wang409/TreeVGR%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Haochen-Wang409/TreeVGRä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07999v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†OpenAI-o3ç­‰æ¨¡å‹åœ¨è§†è§‰æ¨ç†æ–¹é¢çš„è¿›å±•ï¼Œä½†ç¼ºä¹å…¨é¢çš„è¯„ä¼°åŸºå‡†ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†TreeBenchè¯„ä¼°åŸºå‡†ï¼Œä»¥æµ‹è¯•æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç»†å¾®ç›®æ ‡æ„ŸçŸ¥ã€è¾¹ç•Œæ¡†è¯„ä¼°çš„å¯è¿½è¸ªè¯æ®å’ŒäºŒé˜¶æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†TreeVGRè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è”åˆç›‘ç£å®šä½å’Œæ¨ç†ï¼Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†OpenAI-o3ç­‰æ¨¡å‹åœ¨è§†è§‰æ¨ç†æ–¹é¢çš„è¿›å±•ï¼ŒåŠ¨æ€å‚è€ƒè§†è§‰åŒºåŸŸï¼Œç±»ä¼¼äººç±»â€œå›¾åƒæ€è€ƒâ€ã€‚</li>
<li>ç¼ºä¹å…¨é¢çš„è¯„ä¼°åŸºå‡†æ¥å…¨é¢è¯„ä¼°è¿™äº›èƒ½åŠ›ï¼Œä¸ºæ­¤æå‡ºäº†TreeBenchè¯„ä¼°åŸºå‡†ã€‚</li>
<li>TreeBenchåŸºäºä¸‰ä¸ªåŸåˆ™æ„å»ºï¼šç»†å¾®ç›®æ ‡çš„èšç„¦è§†è§‰æ„ŸçŸ¥ã€é€šè¿‡è¾¹ç•Œæ¡†è¯„ä¼°çš„å¯è¿½è¸ªè¯æ®å’ŒäºŒé˜¶æ¨ç†èƒ½åŠ›æµ‹è¯•ã€‚</li>
<li>TreeBenchåŒ…å«405ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰é—®ç­”å¯¹ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿéš¾ä»¥è¾¾åˆ°60%çš„å‡†ç¡®ç‡ã€‚</li>
<li>å¼•å…¥äº†TreeVGRè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è”åˆç›‘ç£å®šä½å’Œæ¨ç†ï¼Œæé«˜æ¨¡å‹å‡†ç¡®ç‡å’Œè§£é‡Šæ€§ã€‚</li>
<li>TreeVGRä»Qwen2.5-VL-7Bå¼€å§‹åˆå§‹åŒ–ï¼Œå¯ä»¥æ”¹å–„V* Benchã€MME-RealWorldå’ŒTreeBenchçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d9a55289070c56ea84ec7e6fba53344.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8852240c08ce669fc30599d185401b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff375b7b211ddb23a18b9727545382ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c5c219d5f8f5d89566874d9c1e89b1c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automating-Expert-Level-Medical-Reasoning-Evaluation-of-Large-Language-Models"><a href="#Automating-Expert-Level-Medical-Reasoning-Evaluation-of-Large-Language-Models" class="headerlink" title="Automating Expert-Level Medical Reasoning Evaluation of Large Language   Models"></a>Automating Expert-Level Medical Reasoning Evaluation of Large Language   Models</h2><p><strong>Authors:Shuang Zhou, Wenya Xie, Jiaxi Li, Zaifu Zhan, Meijia Song, Han Yang, Cheyenna Espinoza, Lindsay Welton, Xinnie Mai, Yanwei Jin, Zidu Xu, Yuen-Hei Chung, Yiyun Xing, Meng-Han Tsai, Emma Schaffer, Yucheng Shi, Ninghao Liu, Zirui Liu, Rui Zhang</strong></p>
<p>As large language models (LLMs) become increasingly integrated into clinical decision-making, ensuring transparent and trustworthy reasoning is essential. However, existing evaluation strategies of LLMsâ€™ medical reasoning capability either suffer from unsatisfactory assessment or poor scalability, and a rigorous benchmark remains lacking. To address this, we introduce MedThink-Bench, a benchmark designed for rigorous, explainable, and scalable assessment of LLMsâ€™ medical reasoning. MedThink-Bench comprises 500 challenging questions across ten medical domains, each annotated with expert-crafted step-by-step rationales. Building on this, we propose LLM-w-Ref, a novel evaluation framework that leverages fine-grained rationales and LLM-as-a-Judge mechanisms to assess intermediate reasoning with expert-level fidelity while maintaining scalability. Experiments show that LLM-w-Ref exhibits a strong positive correlation with expert judgments. Benchmarking twelve state-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can surpass larger proprietary counterparts (e.g., OpenAI-o3). Overall, MedThink-Bench offers a foundational tool for evaluating LLMsâ€™ medical reasoning, advancing their safe and responsible deployment in clinical practice. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸´åºŠå†³ç­–ä¸­çš„é›†æˆåº¦è¶Šæ¥è¶Šé«˜ï¼Œç¡®ä¿é€æ˜å’Œå¯ä¿¡èµ–çš„æ¨ç†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMsåŒ»ç–—æ¨ç†èƒ½åŠ›è¯„ä¼°ç­–ç•¥è¦ä¹ˆå­˜åœ¨è¯„ä¼°ä¸å°½å¦‚äººæ„çš„é—®é¢˜ï¼Œè¦ä¹ˆå­˜åœ¨å¯æ‰©å±•æ€§å·®çš„é—®é¢˜ï¼Œä¸”ç¼ºä¹ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MedThink-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºLLMsåŒ»ç–—æ¨ç†è¿›è¡Œä¸¥æ ¼ã€å¯è§£é‡Šå’Œå¯æ‰©å±•è¯„ä¼°è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚MedThink-BenchåŒ…å«500ä¸ªè·¨è¶Šåä¸ªåŒ»å­¦é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é™„æœ‰ä¸“å®¶åˆ¶ä½œçš„é€æ­¥æ¨ç†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†LLM-w-Refè¿™ä¸€æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç²¾ç»†çš„æ¨ç†å’ŒLLM-as-a-Judgeæœºåˆ¶æ¥è¯„ä¼°ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œä»¥ä¸“å®¶çº§ä¿çœŸåº¦ä¿æŒå¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒLLM-w-Refä¸ä¸“å®¶åˆ¤æ–­å‘ˆå¼ºçƒˆæ­£ç›¸å…³ã€‚æˆ‘ä»¬å¯¹12æ¬¾æœ€æ–°LLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°å°å‹æ¨¡å‹ï¼ˆä¾‹å¦‚MedGemma-27Bï¼‰å¯ä»¥è¶…è¶Šå¤§å‹ä¸“æœ‰æ¨¡å‹ï¼ˆä¾‹å¦‚OpenAI-o3ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼ŒMedThink-Benchä¸ºè¯„ä¼°LLMsçš„åŒ»ç–—æ¨ç†èƒ½åŠ›æä¾›äº†åŸºç¡€å·¥å…·ï¼Œæ¨åŠ¨äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„å®‰å…¨å’Œè´Ÿè´£ä»»éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07988v1">PDF</a> 22 pages,6 figures</p>
<p><strong>Summary</strong>ï¼šéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸´åºŠå†³ç­–ä¸­çš„é›†æˆåº¦ä¸æ–­æé«˜ï¼Œç¡®ä¿é€æ˜å’Œå¯ä¿¡èµ–çš„æ¨ç†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMsåŒ»ç–—æ¨ç†èƒ½åŠ›è¯„ä¼°ç­–ç•¥å­˜åœ¨è¯„ä¼°ç»“æœä¸å°½å¦‚äººæ„æˆ–å¯æ‰©å±•æ€§å·®çš„é—®é¢˜ï¼Œç¼ºä¹ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†MedThink-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹LLMsçš„åŒ»ç–—æ¨ç†èƒ½åŠ›è¿›è¡Œä¸¥è°¨ã€å¯è§£é‡Šå’Œå¯æ‰©å±•çš„è¯„ä¼°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«500ä¸ªè·¨è¶Šåä¸ªåŒ»å­¦é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸“å®¶ç²¾å¿ƒåˆ¶ä½œçš„é€æ­¥æ¨ç†ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†LLM-w-Refè¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨ç²¾ç»†çš„æ¨ç†å’ŒLLM-as-a-Judgeæœºåˆ¶æ¥è¯„ä¼°ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œä¸ä¸“å®¶æ°´å¹³çš„ä¿çœŸåº¦ä¿æŒä¸€è‡´ï¼ŒåŒæ—¶ä¿æŒå¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒLLM-w-Refä¸ä¸“å®¶åˆ¤æ–­ä¹‹é—´å­˜åœ¨å¼ºçƒˆæ­£ç›¸å…³ã€‚é€šè¿‡å¯¹12ä¸ªæœ€å…ˆè¿›çš„LLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°å°å‹æ¨¡å‹ï¼ˆå¦‚MedGemma-27Bï¼‰å¯ä»¥è¶…è¶Šå¤§å‹ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚OpenAI-o3ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼ŒMedThink-Benchä¸ºè¯„ä¼°LLMsçš„åŒ»ç–—æ¨ç†èƒ½åŠ›æä¾›äº†åŸºç¡€å·¥å…·ï¼Œæ¨åŠ¨äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„å®‰å…¨å’Œè´Ÿè´£ä»»éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸´åºŠå†³ç­–ä¸­çš„é›†æˆåº¦æé«˜ï¼Œéœ€è¦ç¡®ä¿é€æ˜å’Œå¯ä¿¡èµ–çš„æ¨ç†ã€‚</li>
<li>ç°æœ‰çš„LLMsåŒ»ç–—æ¨ç†èƒ½åŠ›è¯„ä¼°ç­–ç•¥å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>MedThink-BenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°LLMsçš„åŒ»ç–—æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«500ä¸ªè·¨è¶Šåä¸ªåŒ»å­¦é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>LLM-w-Refè¯„ä¼°æ¡†æ¶åˆ©ç”¨ç²¾ç»†çš„æ¨ç†å’ŒLLMæœºåˆ¶æ¥è¯„ä¼°ä¸­é—´æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>LLM-w-Refä¸ä¸“å®¶åˆ¤æ–­ä¹‹é—´å­˜åœ¨å¼ºçƒˆæ­£ç›¸å…³ã€‚</li>
<li>åŸºå‡†æµ‹è¯•å‘ç°å°å‹æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥è¶…è¶Šå¤§å‹ä¸“æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3fa8f20d07109e92ad02f91f9fad96af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45bae99e2b559c16f1d4eea9aa94e312.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-260b58b6b5afd84c260fc498a92cc817.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39be266f0f2d3a4df0de8a030941cf22.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Scaling-RL-to-Long-Videos"><a href="#Scaling-RL-to-Long-Videos" class="headerlink" title="Scaling RL to Long Videos"></a>Scaling RL to Long Videos</h2><p><strong>Authors:Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han</strong></p>
<p>We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames &#x2F; around 256k tokens). </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨æ ˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†æ‰©å±•åˆ°é•¿è§†é¢‘ã€‚é€šè¿‡é›†æˆä¸‰ä¸ªå…³é”®ç»„ä»¶æ¥è§£å†³é•¿è§†é¢‘æ¨ç†çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¤§è§„æ¨¡æ•°æ®é›†LongVideo-Reasonï¼ŒåŒ…å«52Ké•¿è§†é¢‘é—®ç­”å¯¹ï¼Œæ¶µç›–ä½“è‚²ã€æ¸¸æˆã€Vlogç­‰ä¸åŒé¢†åŸŸçš„é«˜è´¨é‡æ¨ç†æ³¨é‡Šï¼›ï¼ˆ2ï¼‰ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œé€šè¿‡é“¾å¼æ€ç»´ç›‘ç£å¾®è°ƒï¼ˆCoT-SFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ‰©å±•VLMsï¼›ï¼ˆ3ï¼‰é•¿è§†é¢‘RLçš„è®­ç»ƒåŸºç¡€è®¾æ–½ï¼Œåä¸ºå¤šæ¨¡æ€å¼ºåŒ–åºåˆ—å¹¶è¡Œæ€§ï¼ˆMR-SPï¼‰ï¼Œå®ƒç»“åˆäº†åºåˆ—å¹¶è¡Œæ€§å’Œé’ˆå¯¹é•¿è§†é¢‘çš„vLLMå¼•æ“ï¼Œä½¿ç”¨ç¼“å­˜çš„è§†é¢‘åµŒå…¥è¿›è¡Œé«˜æ•ˆrolloutå’Œé¢„å¡«å……ã€‚åœ¨å®éªŒä¸­ï¼ŒLongVILA-R1-7Båœ¨é•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMEï¼‰ä¸Šè¡¨ç°å¼ºåŠ²ã€‚å®ƒåœ¨æˆ‘ä»¬çš„LongVideo-Reason-evalåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨æ—¶åºæ¨ç†ã€ç›®æ ‡å’Œç›®çš„æ¨ç†ã€ç©ºé—´æ¨ç†å’Œæƒ…èŠ‚æ¨ç†æ–¹é¢éƒ½ä¼˜äºVideo-R1-7Bï¼Œç”šè‡³ä¸Gemini-1.5-Proç›¸åŒ¹é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„MR-SPç³»ç»Ÿå®ç°äº†é•¿è¾¾è§†é¢‘RLè®­ç»ƒçš„2.1å€åŠ é€Ÿã€‚éšç€è¾“å…¥è§†é¢‘å¸§æ•°é‡çš„å¢åŠ ï¼ŒLongVILA-R1æŒç»­å±•ç°å‡ºæ€§èƒ½çš„æå‡ã€‚LongVILA-R1æ ‡å¿—ç€å‘VLMsä¸­çš„é•¿è§†é¢‘æ¨ç†è¿ˆå‡ºäº†åšå®çš„ä¸€æ­¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†æ”¯æŒå„ç§æ¨¡æ€ï¼ˆè§†é¢‘ã€æ–‡æœ¬å’ŒéŸ³é¢‘ï¼‰çš„è®­ç»ƒç³»ç»Ÿï¼Œæ”¯æŒVILAå’ŒQwenç³»åˆ—ç­‰å¤šç§æ¨¡å‹ï¼Œç”šè‡³æ”¯æŒå›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚åœ¨å•ä¸ªA100èŠ‚ç‚¹ï¼ˆ8ä¸ªGPUï¼‰ä¸Šï¼Œå®ƒæ”¯æŒé•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘çš„RLè®­ç»ƒï¼ˆä¾‹å¦‚ï¼Œ3600å¸§&#x2F;çº¦256kä»¤ç‰Œï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07966v1">PDF</a> Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/NVlabs/Long-RL">https://github.com/NVlabs/Long-RL</a></p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æ¨å‡ºäº†ä¸€å¥—é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é•¿è§†é¢‘æ¨ç†çš„å…¨æ ˆæ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥è§£å†³é•¿è§†é¢‘æ¨ç†çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŒ…å«å¤§è§„æ¨¡æ•°æ®é›†LongVideo-Reasonã€ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“å’Œé’ˆå¯¹é•¿è§†é¢‘RLçš„è®­ç»ƒåŸºç¡€è®¾æ–½Multi-modal Reinforcement Sequence Parallelismï¼ˆMR-SPï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒLongVILA-R1åœ¨é•¿è§†é¢‘QAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å¼ºåŠ²ï¼Œå¹¶å®ç°äº†é•¿è§†é¢‘æ¨ç†é¢†åŸŸçš„é‡è¦çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é•¿è§†é¢‘æ¨ç†çš„å…¨æ ˆæ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡æ€§èƒ½ã€‚</li>
<li>è§£å†³äº†é•¿è§†é¢‘æ¨ç†çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œé€šè¿‡é›†æˆä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå¤§è§„æ¨¡æ•°æ®é›†LongVideo-Reasonã€ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“å’ŒMR-SPè®­ç»ƒåŸºç¡€è®¾æ–½ã€‚</li>
<li>LongVILA-R1åœ¨é•¿è§†é¢‘QAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ¹é…æˆ–è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>MR-SPç³»ç»Ÿå®ç°äº†é•¿è§†é¢‘RLè®­ç»ƒçš„é€Ÿåº¦æå‡ï¼Œè¾¾åˆ°2.1å€ã€‚</li>
<li>LongVILA-R1åœ¨è¾“å…¥è§†é¢‘å¸§æ•°å¢åŠ æ—¶ï¼Œè¡¨ç°æŒç»­å¢å¼ºã€‚</li>
<li>LongVILA-R1çš„æ¨å‡ºæ ‡å¿—ç€é•¿è§†é¢‘æ¨ç†åœ¨VLMsé¢†åŸŸçš„é‡è¦è¿›å±•ã€‚</li>
<li>ç ”ç©¶è€…å…¬å¼€äº†è®­ç»ƒç³»ç»Ÿï¼Œæ”¯æŒå¤šç§æ¨¡æ€ï¼ˆè§†é¢‘ã€æ–‡æœ¬ã€éŸ³é¢‘ï¼‰çš„RLè®­ç»ƒï¼Œä»¥åŠä¸åŒæ¨¡å‹ï¼ˆVILAå’ŒQwenç³»åˆ—ï¼‰å’Œå›¾åƒã€è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b97ef616fd6dd7a2a25b8b5d949b973b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2354949fb76cea30979515b7f159e5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f12d8cc60cb633d0684f6e8f14b714ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfeafa24537c615ea34021aa517a0d67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-186eebbf19367b06d4f461147e37b231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a9be1839109b43f1885c4c3b7426f0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Measuring-AI-Alignment-with-Human-Flourishing"><a href="#Measuring-AI-Alignment-with-Human-Flourishing" class="headerlink" title="Measuring AI Alignment with Human Flourishing"></a>Measuring AI Alignment with Human Flourishing</h2><p><strong>Authors:Elizabeth Hilliard, Akshaya Jagadeesh, Alex Cook, Steele Billings, Nicholas Skytland, Alicia Llewellyn, Jackson Paull, Nathan Paull, Nolan Kurylo, Keatra Nesbitt, Robert Gruenewald, Anthony Jantzi, Omar Chavez</strong></p>
<p>This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel evaluation framework that assesses AI alignment with human flourishing across seven dimensions: Character and Virtue, Close Social Relationships, Happiness and Life Satisfaction, Meaning and Purpose, Mental and Physical Health, Financial and Material Stability, and Faith and Spirituality. Unlike traditional benchmarks that focus on technical capabilities or harm prevention, the FAI Benchmark measures AI performance on how effectively models contribute to the flourishing of a person across these dimensions. The benchmark evaluates how effectively LLM AI systems align with current research models of holistic human well-being through a comprehensive methodology that incorporates 1,229 objective and subjective questions. Using specialized judge Large Language Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs geometric mean scoring to ensure balanced performance across all flourishing dimensions. Initial testing of 28 leading language models reveals that while some models approach holistic alignment (with the highest-scoring models achieving 72&#x2F;100), none are acceptably aligned across all dimensions, particularly in Faith and Spirituality, Character and Virtue, and Meaning and Purpose. This research establishes a framework for developing AI systems that actively support human flourishing rather than merely avoiding harm, offering significant implications for AI development, ethics, and evaluation. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æ–°å…´çš„ç¹è£äººå·¥æ™ºèƒ½åŸºå‡†æµ‹è¯•ï¼ˆFAI Benchmarkï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå¯ä»¥ä»ä¸ƒä¸ªç»´åº¦è¯„ä¼°äººå·¥æ™ºèƒ½ä¸äººç±»ç¹è£çš„å¥‘åˆåº¦ï¼šæ€§æ ¼ä¸ç¾å¾·ã€äº²å¯†ç¤¾ä¼šå…³ç³»ã€å¹¸ç¦ä¸ç”Ÿæ´»æ»¡æ„åº¦ã€æ„ä¹‰ä¸ç›®çš„ã€èº«å¿ƒå¥åº·ã€è´¢åŠ¡ä¸ç‰©è´¨ç¨³å®šä»¥åŠä¿¡ä»°ä¸çµæ€§ã€‚ä¸ä¼ ç»Ÿçš„ä»¥æŠ€æœ¯èƒ½åŠ›æˆ–ä¼¤å®³é¢„é˜²ä¸ºé‡ç‚¹çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒFAI Benchmarkè¡¡é‡çš„æ˜¯äººå·¥æ™ºèƒ½åœ¨è¿™äº›ç»´åº¦ä¸Šå¦‚ä½•æœ‰æ•ˆåœ°ä¿ƒè¿›äººçš„ç¹è£ã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡ä¸€ç§ç»¼åˆæ–¹æ³•ï¼Œé‡‡ç”¨åŒ…å«1,229ä¸ªå®¢è§‚å’Œä¸»è§‚é—®é¢˜çš„å…¨é¢è¯„ä¼°ä½“ç³»ï¼Œè¯„ä»·å¤§å‹è¯­è¨€æ¨¡å‹AIç³»ç»Ÿå¦‚ä½•ä¸ç›®å‰çš„å…¨äººç±»æ•´ä½“ç¦ç¥‰ç ”ç©¶æ¨¡å‹ç›¸å¥‘åˆã€‚é€šè¿‡è¿ç”¨ä¸“ä¸šåŒ–çš„è£åˆ¤å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè·¨ç»´åº¦è¯„ä¼°ï¼ŒFAI Benchmarké‡‡ç”¨å‡ ä½•å‡å€¼è¯„åˆ†æ³•ï¼Œç¡®ä¿åœ¨æ‰€æœ‰ç¹è£ç»´åº¦ä¸Šçš„è¡¨ç°å‡è¡¡ã€‚å¯¹28ç§é¢†å…ˆçš„è¯­è¨€æ¨¡å‹çš„åˆæ­¥æµ‹è¯•è¡¨æ˜ï¼Œå°½ç®¡ä¸€äº›æ¨¡å‹åœ¨æ•´ä½“å¯¹é½æ–¹é¢å–å¾—äº†è¿›å±•ï¼ˆå¾—åˆ†æœ€é«˜çš„æ¨¡å‹å¾—åˆ†ä¸º72åˆ†ï¼‰ï¼Œä½†æ²¡æœ‰ä¸€æ¬¾åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½è¾¾åˆ°äº†å¯æ¥å—çš„å¥‘åˆåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡ä»°ä¸çµæ€§ã€æ€§æ ¼ä¸ç¾å¾·ä»¥åŠæ„ä¹‰ä¸ç›®çš„æ–¹é¢ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘ç§¯ææ”¯æŒäººç±»ç¹è£çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿè€Œä¸æ˜¯ä»…ä»…é¿å…å±å®³çš„ç³»ç»Ÿçš„å¼€å‘å»ºç«‹äº†æ¡†æ¶ï¼Œå¯¹äººå·¥æ™ºèƒ½å‘å±•ã€ä¼¦ç†å’Œè¯„ä¼°äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07787v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ­¤è®ºæ–‡ä»‹ç»äº†ç¹è£äººå·¥æ™ºèƒ½åŸºå‡†ï¼ˆFAI Benchmarkï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½åœ¨ä¸ƒä¸ªç»´åº¦ä¸Šå¯¹äººç±»ç¹è£çš„è´¡çŒ®ç¨‹åº¦ã€‚è¿™äº›ç»´åº¦åŒ…æ‹¬æ€§æ ¼ä¸ç¾å¾·ã€äº²å¯†ç¤¾ä¼šå…³ç³»ã€å¹¸ç¦ä¸äººç”Ÿæ»¡æ„åº¦ã€æ„ä¹‰ä¸ç›®çš„ã€èº«å¿ƒå¥åº·ã€è´¢åŠ¡ä¸ç‰©è´¨ç¨³å®šä»¥åŠä¿¡ä»°ä¸ç²¾ç¥ã€‚ä¸ä¼ ç»Ÿçš„ä»¥æŠ€æœ¯èƒ½åŠ›æˆ–ä¼¤å®³é¢„é˜²ä¸ºé‡ç‚¹çš„åŸºå‡†ä¸åŒï¼ŒFAI Benchmarké€šè¿‡ä¸€ç§ç»¼åˆæ–¹æ³•è¡¡é‡äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨è¿™äº›ç»´åº¦ä¸Šå¯¹äººç±»ç¹è£çš„è´¡çŒ®ç¨‹åº¦ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å®¢è§‚å’Œä¸»è§‚é—®é¢˜ã€‚è¯¥åŸºå‡†ä½¿ç”¨ä¸“é—¨çš„åˆ¤æ–­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè·¨ç»´åº¦è¯„ä¼°ï¼Œå¹¶é‡‡ç”¨å‡ ä½•å¹³å‡è¯„åˆ†ç¡®ä¿åœ¨æ‰€æœ‰ç¹è£ç»´åº¦ä¸Šçš„å‡è¡¡è¡¨ç°ã€‚åˆæ­¥æµ‹è¯•è¡¨æ˜ï¼Œå°½ç®¡ä¸€äº›æ¨¡å‹åœ¨æ•´ä½“å¯¹é½æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†æ²¡æœ‰æ¨¡å‹åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½èƒ½è¾¾åˆ°å¯æ¥å—çš„å¯¹é½ç¨‹åº¦ã€‚è¿™ä¸ºAIç³»ç»Ÿçš„å¼€å‘æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œä»¥æ”¯æŒäººç±»ç¹è£è€Œä¸ä»…ä»…æ˜¯é¿å…ä¼¤å®³ï¼Œå¯¹AIå‘å±•ã€ä¼¦ç†å’Œè¯„ä¼°å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Flourishing AI Benchmarkï¼ˆFAI Benchmarkï¼‰æ˜¯ä¸€ä¸ªæ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°AIåœ¨å¤šä¸ªç»´åº¦ä¸Šå¯¹äººç±»ç¹è£çš„è´¡çŒ®ã€‚</li>
<li>è¯¥æ¡†æ¶æ¶µç›–äº†æ€§æ ¼ä¸ç¾å¾·ã€ç¤¾ä¼šå…³ç³»ã€å¹¸ç¦ä¸æ»¡æ„åº¦ç­‰ä¸ƒä¸ªç»´åº¦ã€‚</li>
<li>FAI Benchmarkä¸åŒäºä¼ ç»Ÿçš„äººå·¥æ™ºèƒ½è¯„ä¼°åŸºå‡†ï¼Œå®ƒä¾§é‡äºè¯„ä¼°AIå¦‚ä½•ä¿ƒè¿›äººç±»çš„å…¨é¢ç¹è£ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ç»¼åˆæ–¹æ³•ï¼Œç»“åˆå®¢è§‚å’Œä¸»è§‚é—®é¢˜è¿›è¡Œè¯„ä»·ã€‚</li>
<li>ä½¿ç”¨Large Language Modelsï¼ˆLLMsï¼‰è¿›è¡Œè·¨ç»´åº¦è¯„ä¼°å’Œå‡ ä½•å¹³å‡è¯„åˆ†ã€‚</li>
<li>åˆæ­¥æµ‹è¯•è¡¨æ˜ï¼Œå°½ç®¡ä¸€äº›æ¨¡å‹åœ¨æ•´ä½“å¯¹é½æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†æ²¡æœ‰æ¨¡å‹åœ¨æ‰€æœ‰ç»´åº¦ä¸Šå®ç°å®Œç¾å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f364ab316e8118ca3770fcee6c87e2c8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SURPRISE3D-A-Dataset-for-Spatial-Understanding-and-Reasoning-in-Complex-3D-Scenes"><a href="#SURPRISE3D-A-Dataset-for-Spatial-Understanding-and-Reasoning-in-Complex-3D-Scenes" class="headerlink" title="SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex   3D Scenes"></a>SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex   3D Scenes</h2><p><strong>Authors:Jiaxin Huang, Ziwen Li, Hanlve Zhang, Runnan Chen, Xiao He, Yandong Guo, Wenping Wang, Tongliang Liu, Mingming Gong</strong></p>
<p>The integration of language and 3D perception is critical for embodied AI and robotic systems to perceive, understand, and interact with the physical world. Spatial reasoning, a key capability for understanding spatial relationships between objects, remains underexplored in current 3D vision-language research. Existing datasets often mix semantic cues (e.g., object name) with spatial context, leading models to rely on superficial shortcuts rather than genuinely interpreting spatial relationships. To address this gap, we introduce S\textsc{urprise}3D, a novel dataset designed to evaluate language-guided spatial reasoning segmentation in complex 3D scenes. S\textsc{urprise}3D consists of more than 200k vision language pairs across 900+ detailed indoor scenes from ScanNet++ v2, including more than 2.8k unique object classes. The dataset contains 89k+ human-annotated spatial queries deliberately crafted without object name, thereby mitigating shortcut biases in spatial understanding. These queries comprehensively cover various spatial reasoning skills, such as relative position, narrative perspective, parametric perspective, and absolute distance reasoning. Initial benchmarks demonstrate significant challenges for current state-of-the-art expert 3D visual grounding methods and 3D-LLMs, underscoring the necessity of our dataset and the accompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite. S\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially aware AI, paving the way for effective embodied interaction and robotic planning. The code and datasets can be found in <a target="_blank" rel="noopener" href="https://github.com/liziwennba/SUPRISE">https://github.com/liziwennba/SUPRISE</a>. </p>
<blockquote>
<p>è¯­è¨€ä¸3Dæ„ŸçŸ¥çš„èåˆå¯¹äºå®ä½“äººå·¥æ™ºèƒ½å’Œæœºå™¨äººç³»ç»Ÿæ„ŸçŸ¥ã€ç†è§£å’Œä¸ç‰©ç†ä¸–ç•Œäº¤äº’è‡³å…³é‡è¦ã€‚ç©ºé—´æ¨ç†æ˜¯ç†è§£ç‰©ä½“ä¹‹é—´ç©ºé—´å…³ç³»çš„å…³é”®èƒ½åŠ›ï¼Œåœ¨å½“å‰çš„3Dè§†è§‰è¯­è¨€ç ”ç©¶ä¸­ä»ç„¶è¢«å¿½è§†ã€‚ç°æœ‰çš„æ•°æ®é›†å¾€å¾€å°†è¯­ä¹‰çº¿ç´¢ï¼ˆå¦‚å¯¹è±¡åç§°ï¼‰ä¸ç©ºé—´ä¸Šä¸‹æ–‡æ··åˆï¼Œå¯¼è‡´æ¨¡å‹ä¾èµ–äºè¡¨é¢çš„æ·å¾„ï¼Œè€Œä¸æ˜¯çœŸæ­£è§£é‡Šç©ºé—´å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†S\textsc{urprise}3Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤æ‚3Dåœºæ™¯ä¸­è¯­è¨€å¼•å¯¼çš„ç©ºé—´æ¨ç†åˆ†å‰²èƒ½åŠ›çš„æ–°æ•°æ®é›†ã€‚S\textsc{urprise}3Dç”±æ¥è‡ªScanNet++ v2çš„900å¤šä¸ªè¯¦ç»†å®¤å†…åœºæ™¯ä¸­çš„è¶…è¿‡20ä¸‡ä¸ªè§†è§‰è¯­è¨€å¯¹ç»„æˆï¼ŒåŒ…æ‹¬è¶…è¿‡2800ä¸ªç‹¬ç‰¹çš„å¯¹è±¡ç±»åˆ«ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡8ä¸‡ä¸ªäººç±»æ³¨é‡Šçš„ç©ºé—´æŸ¥è¯¢ï¼Œè¿™äº›æŸ¥è¯¢æ˜¯ç²¾å¿ƒè®¾è®¡çš„ï¼Œä¸åŒ…å«å¯¹è±¡åç§°ï¼Œä»è€Œå‡è½»äº†ç©ºé—´ç†è§£ä¸­çš„æ·å¾„åè§ã€‚è¿™äº›æŸ¥è¯¢æ¶µç›–äº†å„ç§ç©ºé—´æ¨ç†æŠ€èƒ½ï¼Œå¦‚ç›¸å¯¹ä½ç½®ã€å™äº‹è§†è§’ã€å‚æ•°è§†è§’å’Œç»å¯¹è·ç¦»æ¨ç†ç­‰ã€‚åˆæ­¥åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œå¯¹äºç›®å‰æœ€å…ˆè¿›çš„3Dè§†è§‰å®šä½æ–¹æ³•å’Œ3D-LLMæ¥è¯´ï¼Œä»ç„¶å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œè¿™çªæ˜¾äº†æˆ‘ä»¬æ•°æ®é›†å’Œé…å¥—çš„3Dç©ºé—´æ¨ç†åˆ†å‰²ï¼ˆ3D-SRSï¼‰åŸºå‡†æµ‹è¯•å¥—ä»¶çš„é‡è¦æ€§ã€‚S\textsc{urprise}3Då’Œ3D-SRSæ—¨åœ¨ä¿ƒè¿›ç©ºé—´æ„ŸçŸ¥äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œä¸ºæœ‰æ•ˆçš„å®ä½“äº¤äº’å’Œæœºå™¨äººè§„åˆ’é“ºå¹³é“è·¯ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/liziwennba/SUPRISE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liziwennba/SUPRISEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07781v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†è¯­è¨€ä¸3Dæ„ŸçŸ¥èåˆåœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½å’Œæœºå™¨äººç³»ç»Ÿä¸­çš„é‡è¦æ€§å’ŒæŒ‘æˆ˜ã€‚é’ˆå¯¹å½“å‰æ•°æ®é›†åœ¨è¯„ä¼°ç©ºé—´æ¨ç†æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ•°æ®é›†S\textsc{urprise}3Dï¼Œç”¨äºè¯„ä¼°å¤æ‚3Dåœºæ™¯ä¸­çš„è¯­è¨€å¼•å¯¼ç©ºé—´æ¨ç†åˆ†å‰²èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡20ä¸‡ä¸ªè§†è§‰è¯­è¨€å¯¹ï¼Œæ¶µç›–äº†å¤šç§ç©ºé—´æ¨ç†æŠ€èƒ½ï¼Œå¦‚ç›¸å¯¹ä½ç½®ã€å™äº‹è§†è§’ã€å‚æ•°è§†è§’å’Œç»å¯¹è·ç¦»æ¨ç†ç­‰ã€‚æ•°æ®é›†ä¸­çš„ç©ºé—´æŸ¥è¯¢æœªåŒ…å«å¯¹è±¡åç§°ï¼Œä»è€Œå‡å°‘äº†ç©ºé—´ç†è§£çš„æ·å¾„åè§ã€‚S\textsc{urprise}3DåŠå…¶ä¼´éšçš„3Dç©ºé—´æ¨ç†åˆ†å‰²ï¼ˆ3D-SRSï¼‰åŸºå‡†å¥—ä»¶æ—¨åœ¨æ¨åŠ¨ç©ºé—´æ„ŸçŸ¥äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œä¸ºå®ç°æœ‰æ•ˆçš„åµŒå…¥å¼äº¤äº’å’Œæœºå™¨äººè§„åˆ’é“ºå¹³é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€ä¸3Dæ„ŸçŸ¥çš„èåˆå¯¹åµŒå…¥å¼AIå’Œæœºå™¨äººç³»ç»Ÿçš„ç‰©ç†ä¸–ç•Œäº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ•°æ®é›†åœ¨è¯„ä¼°ç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ–°å‹æ•°æ®é›†æ¥æ”¹è¿›ã€‚</li>
<li>S\textsc{urprise}3Dæ•°æ®é›†ç”¨äºè¯„ä¼°è¯­è¨€å¼•å¯¼çš„ç©ºé—´æ¨ç†åˆ†å‰²èƒ½åŠ›ï¼ŒåŒ…å«å¤šç§ç©ºé—´æ¨ç†æŠ€èƒ½ã€‚</li>
<li>æ•°æ®é›†ä¸­çš„ç©ºé—´æŸ¥è¯¢æœªåŒ…å«å¯¹è±¡åç§°ï¼Œä»¥å‡å°‘å¯¹ç©ºé—´ç†è§£çš„æ·å¾„åè§ã€‚</li>
<li>S\textsc{urprise}3Då’Œ3D-SRSåŸºå‡†å¥—ä»¶æ—¨åœ¨æ¨åŠ¨ç©ºé—´æ„ŸçŸ¥äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</li>
<li>åŸºå‡†å¥—ä»¶ä¸ºç°æœ‰çš„äººå·¥æ™ºèƒ½æŠ€æœ¯æä¾›äº†æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„3Dè§†è§‰å®šä½æ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d38252a25bde4224bd22dc2b34c6945b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad5a2479befef1cc83c8ced4c8e7395e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fe1fd1f432d899ade641dce1c575cee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f65d091f9a0772f323a01dd20fbabb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e66b8b103a181f34e18e41f11fdd02dc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BEAVER-Building-Environments-with-Assessable-Variation-for-Evaluating-Multi-Objective-Reinforcement-Learning"><a href="#BEAVER-Building-Environments-with-Assessable-Variation-for-Evaluating-Multi-Objective-Reinforcement-Learning" class="headerlink" title="BEAVER: Building Environments with Assessable Variation for Evaluating   Multi-Objective Reinforcement Learning"></a>BEAVER: Building Environments with Assessable Variation for Evaluating   Multi-Objective Reinforcement Learning</h2><p><strong>Authors:Ruohong Liu, Jack Umenberger, Yize Chen</strong></p>
<p>Recent years have seen significant advancements in designing reinforcement learning (RL)-based agents for building energy management. While individual success is observed in simulated or controlled environments, the scalability of RL approaches in terms of efficiency and generalization across building dynamics and operational scenarios remains an open question. In this work, we formally characterize the generalization space for the cross-environment, multi-objective building energy management task, and formulate the multi-objective contextual RL problem. Such a formulation helps understand the challenges of transferring learned policies across varied operational contexts such as climate and heat convection dynamics under multiple control objectives such as comfort level and energy consumption. We provide a principled way to parameterize such contextual information in realistic building RL environments, and construct a novel benchmark to facilitate the evaluation of generalizable RL algorithms in practical building control tasks. Our results show that existing multi-objective RL methods are capable of achieving reasonable trade-offs between conflicting objectives. However, their performance degrades under certain environment variations, underscoring the importance of incorporating dynamics-dependent contextual information into the policy learning process. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œåœ¨è®¾è®¡åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„èƒ½æºç®¡ç†æ™ºèƒ½ä½“æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚è™½ç„¶æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿæˆ–å—æ§ç¯å¢ƒä¸­è§‚å¯Ÿåˆ°äº†ä¸ªåˆ«çš„æˆåŠŸï¼Œä½†å¼ºåŒ–å­¦ä¹ åœ¨æ•ˆç‡å’Œè·¨å»ºç­‘åŠ¨æ€åŠæ“ä½œåœºæ™¯çš„æ³›åŒ–æ–¹é¢çš„å¯æ‰©å±•æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ­£å¼æè¿°äº†è·¨ç¯å¢ƒã€å¤šç›®æ ‡å»ºç­‘èƒ½æºç®¡ç†ä»»åŠ¡çš„æ³›åŒ–ç©ºé—´ï¼Œå¹¶åˆ¶å®šäº†å¤šç›®æ ‡ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚è¿™ç§è¡¨è¿°æœ‰åŠ©äºç†è§£åœ¨ä¸åŒæ“ä½œä¸Šä¸‹æ–‡ä¸­è½¬ç§»å­¦ä¹ ç­–ç•¥çš„éš¾é¢˜ï¼Œä¾‹å¦‚åœ¨æ°”å€™å’Œä¼ çƒ­åŠ¨åŠ›å­¦æ–¹é¢çš„å¤šé‡æ§åˆ¶ç›®æ ‡ï¼Œå¦‚èˆ’é€‚åº¦å’Œèƒ½æºæ¶ˆè€—ç­‰ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ç§åŸåˆ™æ€§çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨ç°å®çš„å»ºç­‘å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­å¯¹ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œå‚æ•°åŒ–ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥ä¿ƒè¿›åœ¨å®é™…å»ºç­‘æ§åˆ¶ä»»åŠ¡ä¸­å¯¹é€šç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿåœ¨ç›¸äº’å†²çªçš„ç›®æ ‡ä¹‹é—´å–å¾—åˆç†çš„å¹³è¡¡ã€‚ä½†åœ¨æŸäº›ç¯å¢ƒå‘ç”Ÿå˜åŒ–çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½ä¼šä¸‹é™ï¼Œè¿™çªæ˜¾äº†åœ¨ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­èå…¥åŠ¨æ€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07769v1">PDF</a> Accepted at the Workshop on Computational Optimization of Buildings   (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML   2025), Vancouver, Canada</p>
<p><strong>Summary</strong><br>åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯è®¾è®¡åº”ç”¨äºå»ºç­‘èƒ½æºç®¡ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚å°½ç®¡åœ¨æ¨¡æ‹Ÿæˆ–å—æ§ç¯å¢ƒä¸­è·å¾—äº†æˆåŠŸï¼Œä½†åœ¨æ•ˆç‡æ–¹é¢çš„å®é™…åº”ç”¨å’Œæ¨å¹¿å»ºç­‘åŠ¨æ€å’Œæ“ä½œåœºæ™¯ä»å­˜åœ¨é—®é¢˜ã€‚è¯¥ç ”ç©¶ä¸ºè·¨ç¯å¢ƒçš„ã€å¤šç›®æ ‡çš„å»ºç­‘èƒ½æºç®¡ç†ä»»åŠ¡æä¾›äº†å½¢å¼åŒ–è¡¨å¾ï¼Œå½¢æˆäº†å¤šç›®æ ‡æƒ…å¢ƒå¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚å®ƒå¸®åŠ©ç†è§£åœ¨ä¸åŒæ“ä½œç¯å¢ƒã€æ°”å€™ã€çƒ­å¯¹æµåŠ¨åŠ›å­¦å’Œå¤šç›®æ ‡æ§åˆ¶ä¸‹çš„ç­–ç•¥è½¬ç§»æŒ‘æˆ˜ï¼Œå¦‚èˆ’é€‚åº¦æ°´å¹³å’Œèƒ½æºæ¶ˆè€—ç­‰ã€‚ç ”ç©¶ä¸ºå®é™…åº”ç”¨ä¸­çš„å»ºç­‘å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­æ­¤ç±»æƒ…å¢ƒä¿¡æ¯çš„å‚æ•°åŒ–æä¾›äº†åŸåˆ™æ€§æ–¹æ³•ï¼Œå¹¶å»ºç«‹äº†æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥ä¿ƒè¿›é€šç”¨æ€§å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å®é™…å»ºç­‘æ§åˆ¶ä»»åŠ¡ä¸­çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ç°æœ‰å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•èƒ½å¤Ÿåœ¨å†²çªç›®æ ‡ä¹‹é—´å–å¾—åˆç†çš„å¹³è¡¡ï¼Œä½†åœ¨æŸäº›ç¯å¢ƒå˜åŠ¨ä¸‹æ€§èƒ½ä¸‹é™ï¼Œçªæ˜¾äº†å°†åŠ¨æ€ç›¸å…³çš„æƒ…å¢ƒä¿¡æ¯çº³å…¥ç­–ç•¥å­¦ä¹ è¿‡ç¨‹çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å»ºç­‘èƒ½æºç®¡ç†é¢†åŸŸå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨æ•ˆç‡å’Œæ¨å¹¿æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æ­£å¼å®šä¹‰äº†è·¨ç¯å¢ƒã€å¤šç›®æ ‡çš„å»ºç­‘èƒ½æºç®¡ç†ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†å¤šç›®æ ‡æƒ…å¢ƒå¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ¦‚å¿µæ¡†æ¶ã€‚</li>
<li>åˆ†æäº†è½¬ç§»å­¦ä¹ åœ¨å¤šç§æ“ä½œç¯å¢ƒã€æ°”å€™æ¡ä»¶å’Œä¸åŒæ§åˆ¶ç›®æ ‡ï¼ˆå¦‚èˆ’é€‚åº¦å’Œèƒ½æºæ¶ˆè€—ï¼‰ä¸‹çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºåœ¨çœŸå®å»ºç­‘ç¯å¢ƒä¸­å‚æ•°åŒ–æƒ…å¢ƒä¿¡æ¯æä¾›äº†æ–¹æ³•ï¼Œå¹¶å»ºç«‹äº†è¯„ä¼°é€šç”¨æ€§å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ–°åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨è§£å†³å¤šç›®æ ‡é—®é¢˜æ—¶æœ‰è¾ƒå¥½çš„è¡¨ç°ï¼Œä½†åœ¨ç‰¹å®šç¯å¢ƒå˜åŠ¨ä¸‹æ€§èƒ½ä¸‹æ»‘ã€‚</li>
<li>ç»“æœå¼ºè°ƒäº†å°†åŠ¨æ€ç›¸å…³çš„æƒ…å¢ƒä¿¡æ¯çº³å…¥ç­–ç•¥å­¦ä¹ è¿‡ç¨‹çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6fdc883fa8cc8eb33cedaedb7af67f09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a19f27240964ecc3dc5d2ad25d3dea5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9108063cd158e5d7d26e78d52fc064be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8f4e5a4a124f8ad523d19f8018a2af3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be9cd007dda21d0b53f2d05c372d97e8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FrugalRAG-Learning-to-retrieve-and-reason-for-multi-hop-QA"><a href="#FrugalRAG-Learning-to-retrieve-and-reason-for-multi-hop-QA" class="headerlink" title="FrugalRAG: Learning to retrieve and reason for multi-hop QA"></a>FrugalRAG: Learning to retrieve and reason for multi-hop QA</h2><p><strong>Authors:Abhinav Java, Srivathsan Koundinyan, Nagarajan Natarajan, Amit Sharma</strong></p>
<p>We consider the problem of answering complex questions, given access to a large unstructured document corpus. The de facto approach to solving the problem is to leverage language models that (iteratively) retrieve and reason through the retrieved documents, until the model has sufficient information to generate an answer. Attempts at improving this approach focus on retrieval-augmented generation (RAG) metrics such as accuracy and recall and can be categorized into two types: (a) fine-tuning on large question answering (QA) datasets augmented with chain-of-thought traces, and (b) leveraging RL-based fine-tuning techniques that rely on question-document relevance signals. However, efficiency in the number of retrieval searches is an equally important metric, which has received less attention. In this work, we show that: (1) Large-scale fine-tuning is not needed to improve RAG metrics, contrary to popular claims in recent literature. Specifically, a standard ReAct pipeline with improved prompts can outperform state-of-the-art methods on benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help RAG from the perspective of frugality, i.e., the latency due to number of searches at inference time. For example, we show that we can achieve competitive RAG metrics at nearly half the cost (in terms of number of searches) on popular RAG benchmarks, using the same base model, and at a small training cost (1000 examples). </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘äº†åœ¨å¯ä»¥è®¿é—®å¤§è§„æ¨¡éç»“æ„åŒ–æ–‡æ¡£è¯­æ–™åº“çš„æƒ…å†µä¸‹å›ç­”å¤æ‚é—®é¢˜çš„èƒ½åŠ›ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„å®é™…æ–¹æ³•æ˜¯åˆ©ç”¨è¯­è¨€æ¨¡å‹ï¼ˆé€šè¿‡è¿­ä»£æ–¹å¼ï¼‰æ£€ç´¢å’Œæ¨ç†æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œç›´åˆ°æ¨¡å‹æ‹¥æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥ç”Ÿæˆç­”æ¡ˆã€‚æ”¹è¿›è¿™ä¸€æ–¹æ³•çš„å°è¯•ä¸»è¦é›†ä¸­åœ¨æé«˜æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å‡†ç¡®æ€§å’Œå¬å›ç‡æ–¹é¢ï¼Œè¿™äº›å°è¯•å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼šï¼ˆaï¼‰åœ¨å¤§å‹é—®ç­”æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶å¢åŠ æ€ç»´é“¾è¿½è¸ªï¼›ï¼ˆbï¼‰åˆ©ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä¾èµ–äºé—®é¢˜æ–‡æ¡£ç›¸å…³æ€§ä¿¡å·ã€‚ç„¶è€Œï¼Œæ£€ç´¢æœç´¢çš„æ•°é‡åŒæ ·æ˜¯ä¸€ä¸ªé‡è¦çš„æ•ˆç‡æŒ‡æ ‡ï¼Œä½†è¿™ä¸€æŒ‡æ ‡å—åˆ°çš„å…³æ³¨åº¦è¾ƒå°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ï¼šï¼ˆ1ï¼‰æ— éœ€å¤§è§„æ¨¡å¾®è°ƒå°±èƒ½æé«˜RAGæŒ‡æ ‡ï¼Œè¿™ä¸è¿‘æœŸæ–‡çŒ®ä¸­çš„æ™®éè§‚ç‚¹ç›¸åã€‚å…·ä½“æ¥è¯´ï¼Œé‡‡ç”¨æ”¹è¿›æç¤ºçš„æ ‡å‡†ReActæµç¨‹å¯ä»¥åœ¨HotPotQAç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè¶…è¶Šæœ€æ–°æ–¹æ³•çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ä»èŠ‚ä¿­çš„è§’åº¦æ¥çœ‹ï¼Œæœ‰ç›‘ç£çš„å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒå¯ä»¥å¸®åŠ©RAGå‡å°‘æ¨ç†æ—¶é—´çš„æœç´¢æ•°é‡å»¶è¿Ÿã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨ç›¸åŒçš„åŸºå‡†æ¨¡å‹åœ¨æµè¡Œçš„RAGåŸºå‡†æµ‹è¯•ä¸Šå¯ä»¥åœ¨æœç´¢æ•°é‡æˆæœ¬é™ä½ä¸€åŠçš„æƒ…å†µä¸‹å®ç°æœ‰ç«äº‰åŠ›çš„RAGæŒ‡æ ‡ï¼Œå¹¶ä¸”è®­ç»ƒæˆæœ¬è¾ƒä½ï¼ˆä»…ä½¿ç”¨1000ä¸ªç¤ºä¾‹ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07634v1">PDF</a> Accepted at ICML Workshop: Efficient Systems for Foundation Models</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§è§„æ¨¡éç»“æ„åŒ–æ–‡æ¡£è¯­æ–™åº“ä¸­å›ç­”å¤æ‚é—®é¢˜çš„é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä¸éœ€è¦å¤§è§„æ¨¡å¾®è°ƒå³å¯æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„è¯„ä¼°æŒ‡æ ‡ï¼Œæ”¹è¿›æç¤ºçš„æ ‡å‡†ReActç®¡é“èƒ½å¤Ÿè¶…è¶Šå½“å‰æœ€å¥½æ–¹æ³•åœ¨HotPotQAç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚åŒæ—¶ï¼Œç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒæœ‰åŠ©äºå‡å°‘æ¨ç†æ—¶é—´æˆæœ¬ï¼Œé€šè¿‡å‡å°‘æœç´¢æ¬¡æ•°å®ç°ç«äº‰æ€§çš„RAGæŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›ç­”é—®é¢˜æ—¶ï¼Œå¯¹äºå¤§è§„æ¨¡éç»“æ„åŒ–æ–‡æ¡£è¯­æ–™åº“çš„æ£€ç´¢å’Œæ¨ç†æ˜¯å…³é”®é—®é¢˜ã€‚</li>
<li>ä¸éœ€è¦å¤§è§„æ¨¡å¾®è°ƒä¹Ÿèƒ½æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>æ ‡å‡†ReActç®¡é“é€šè¿‡æ”¹è¿›æç¤ºå¯è¶…è¶Šå½“å‰æœ€å¥½æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</li>
<li>ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒæœ‰åŠ©äºå‡å°‘æ¨ç†çš„æ—¶é—´æˆæœ¬ã€‚</li>
<li>å‡å°‘æœç´¢æ¬¡æ•°å¯å®ç°ç«äº‰æ€§çš„RAGæŒ‡æ ‡ã€‚</li>
<li>æ•ˆç‡æ˜¯é™¤äº†å‡†ç¡®æ€§ä¹‹å¤–åŒæ ·é‡è¦çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67842400e275b7a4969efac4ecb32cfd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SpatialViz-Bench-Automatically-Generated-Spatial-Visualization-Reasoning-Tasks-for-MLLMs"><a href="#SpatialViz-Bench-Automatically-Generated-Spatial-Visualization-Reasoning-Tasks-for-MLLMs" class="headerlink" title="SpatialViz-Bench: Automatically Generated Spatial Visualization   Reasoning Tasks for MLLMs"></a>SpatialViz-Bench: Automatically Generated Spatial Visualization   Reasoning Tasks for MLLMs</h2><p><strong>Authors:Siting Wang, Luoyang Sun, Cheng Deng, Kun Shao, Minnan Pei, Zheng Tian, Haifeng Zhang, Jun Wang</strong></p>
<p>Humans can directly imagine and manipulate visual images in their minds, a capability known as spatial visualization. While multi-modal Large Language Models (MLLMs) support imagination-based reasoning, spatial visualization remains insufficiently evaluated, typically embedded within broader mathematical and logical assessments. Existing evaluations often rely on IQ tests or math competitions that may overlap with training data, compromising assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 automatically generated problems. Our evaluation of 33 state-of-the-art MLLMs not only reveals wide performance variations and demonstrates the benchmarkâ€™s strong discriminative power, but also uncovers counter-intuitive findings: models exhibit unexpected behaviors by showing difficulty perception that misaligns with human intuition, displaying dramatic 2D-to-3D performance cliffs, and defaulting to formula derivation despite spatial tasks requiring visualization alone. SpatialVizBench empirically demonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark is publicly available. </p>
<blockquote>
<p>äººç±»èƒ½å¤Ÿåœ¨å¤§è„‘ä¸­ç›´æ¥æƒ³è±¡å¹¶æ“ä½œè§†è§‰å›¾åƒï¼Œè¿™ç§èƒ½åŠ›è¢«ç§°ä¸ºç©ºé—´å¯è§†åŒ–ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ”¯æŒåŸºäºæƒ³è±¡çš„æ¨ç†ï¼Œä½†ç©ºé—´å¯è§†åŒ–èƒ½åŠ›çš„è¯„ä¼°ä»ç„¶ä¸è¶³ï¼Œé€šå¸¸åªåµŒå…¥æ›´å¹¿æ³›çš„æ•°å­¦å’Œé€»è¾‘è¯„ä¼°ä¸­ã€‚ç°æœ‰çš„è¯„ä¼°é€šå¸¸ä¾èµ–äºå¯èƒ½ä¸è®­ç»ƒæ•°æ®é‡å çš„æ™ºå•†æµ‹è¯•æˆ–æ•°å­¦ç«èµ›ï¼Œä»è€Œå½±å“è¯„ä¼°çš„å¯é æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpatialViz-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´å¯è§†åŒ–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«12é¡¹ä»»åŠ¡ï¼Œæ¶µç›–4ç§å­èƒ½åŠ›ï¼Œå…±æœ‰1180ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜ã€‚æˆ‘ä»¬å¯¹33ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä»·ä¸ä»…æ­ç¤ºäº†æ€§èƒ½ä¸Šçš„å·¨å¤§å·®å¼‚ï¼Œè¯æ˜äº†åŸºå‡†æµ‹è¯•çš„å¼ºé‰´åˆ«åŠ›ï¼Œè¿˜å‘ç°äº†ä¸€äº›è¿èƒŒç›´è§‰çš„ç»“æœï¼šæ¨¡å‹è¡¨ç°å‡ºéš¾ä»¥ç†è§£äººç±»ç›´è§‰çš„æ„ŸçŸ¥å›°éš¾è¡Œä¸ºï¼Œåœ¨äºŒç»´åˆ°ä¸‰ç»´è½¬æ¢ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå·¨å¤§è½å·®ï¼Œä»¥åŠåœ¨éœ€è¦å•ç‹¬ä½¿ç”¨å¯è§†åŒ–è§£å†³çš„ç©ºé—´ä»»åŠ¡ä¸­é»˜è®¤ä½¿ç”¨å…¬å¼æ¨å¯¼ã€‚SpatialVizBenchå®è¯è¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å¯è§†åŒ–ä»»åŠ¡ä¸Šä»å­˜åœ¨ç¼ºé™·ï¼Œä»è€Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚è¯¥åŸºå‡†æµ‹è¯•å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07610v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>ç©ºé—´å¯è§†åŒ–æ˜¯äººç±»èƒ½å¤Ÿåœ¨å¤§è„‘ä¸­ç›´æ¥æƒ³è±¡å’Œæ“ä½œè§†è§‰å›¾åƒçš„èƒ½åŠ›ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ”¯æŒåŸºäºæƒ³è±¡çš„æ¨ç†ï¼Œä½†ç©ºé—´å¯è§†åŒ–å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ï¼Œé€šå¸¸åªæ˜¯æ›´å¹¿æ³›çš„æ•°å­¦å’Œé€»è¾‘è¯„ä¼°çš„ä¸€éƒ¨åˆ†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpatialViz-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´å¯è§†åŒ–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«4ä¸ªå­èƒ½åŠ›çš„12é¡¹ä»»åŠ¡ï¼Œå…±è®¡1ï¼Œ180ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•é—®é¢˜ã€‚æˆ‘ä»¬å¯¹ç›®å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å…¶æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œå¹¶è¯æ˜äº†è¯¥åŸºå‡†æµ‹è¯•å…·æœ‰è¾ƒå¼ºçš„é‰´åˆ«åŠ›ã€‚è¯„ä¼°è¿˜å‘ç°ä¸€äº›æ„æƒ³ä¸åˆ°çš„è¡Œä¸ºï¼Œå¦‚æ¨¡å‹åœ¨æ„ŸçŸ¥æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´äºŒç»´åˆ°ä¸‰ç»´æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œä»¥åŠåœ¨ä»…éœ€è¦å¯è§†åŒ–çš„æƒ…å†µä¸‹é»˜è®¤ä½¿ç”¨å…¬å¼æ¨å¯¼ç­‰ã€‚SpatialVizBenchå®è¯ç ”ç©¶è¯æ˜ï¼Œç›®å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å¯è§†åŒ–ä»»åŠ¡ä¸Šä»å­˜åœ¨ç¼ºé™·ã€‚è¯¥åŸºå‡†æµ‹è¯•å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>äººç±»æ‹¥æœ‰ç©ºé—´å¯è§†åŒ–èƒ½åŠ›ï¼Œèƒ½ç›´æ¥æƒ³è±¡å’Œæ“ä½œè§†è§‰å›¾åƒã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ”¯æŒåŸºäºæƒ³è±¡çš„æ¨ç†ï¼Œä½†ç©ºé—´å¯è§†åŒ–èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚</li>
<li>SpatialViz-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´å¯è§†åŒ–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šç§ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„ç©ºé—´å¯è§†åŒ–èƒ½åŠ›ã€‚</li>
<li>å¯¹ç°æœ‰æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨ç©ºé—´å¯è§†åŒ–æ–¹é¢çš„æ€§èƒ½å­˜åœ¨è¾ƒå¤§å·®å¼‚ã€‚</li>
<li>æ¨¡å‹åœ¨æŸäº›ç©ºé—´å¯è§†åŒ–ä»»åŠ¡ä¸Šå­˜åœ¨å›°éš¾ï¼Œå¦‚æ„ŸçŸ¥æ–¹é¢å­˜åœ¨çš„é—®é¢˜å¯¼è‡´äºŒç»´åˆ°ä¸‰ç»´æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚</li>
<li>æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹é»˜è®¤ä½¿ç”¨å…¬å¼æ¨å¯¼ï¼Œè€Œéä»…ä¾èµ–ç©ºé—´å¯è§†åŒ–èƒ½åŠ›å®Œæˆä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-94b9776280ebb5efe01c097260564869.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64ec35a3c3008d1c27862e904eff0e53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-553d800ac6766443411e90f9b8627746.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-028d2e8ac30eb50ba0c8537ec1524339.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e2bd081953150145f5899a10cd413d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e0b6077cb15e1f7a2db6466971e2464.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RLEP-Reinforcement-Learning-with-Experience-Replay-for-LLM-Reasoning"><a href="#RLEP-Reinforcement-Learning-with-Experience-Replay-for-LLM-Reasoning" class="headerlink" title="RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning"></a>RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning</h2><p><strong>Authors:Hongzhi Zhang, Jia Fu, Jingyuan Zhang, Kai Fu, Qi Wang, Fuzheng Zhang, Guorui Zhou</strong></p>
<p>Reinforcement learning (RL) for large language models is an energy-intensive endeavor: training can be unstable, and the policy may gradually drift away from its pretrained weights. We present \emph{RLEP}, â€“ ,Reinforcement Learning with Experience rePlay, â€“ ,a two-phase framework that first collects verified trajectories and then replays them during subsequent training. At every update step, the policy is optimized on mini-batches that blend newly generated rollouts with these replayed successes. By replaying high-quality examples, RLEP steers the model away from fruitless exploration, focuses learning on promising reasoning paths, and delivers both faster convergence and stronger final performance. On the Qwen2.5-Math-7B base model, RLEP reaches baseline peak accuracy with substantially fewer updates and ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%, on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our code, datasets, and checkpoints are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/RLEP">https://github.com/Kwai-Klear/RLEP</a> to facilitate reproducibility and further research. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€é¡¹è€—èƒ½å¯†é›†å‹çš„ä»»åŠ¡ï¼šè®­ç»ƒå¯èƒ½ä¸ç¨³å®šï¼Œç­–ç•¥å¯èƒ½ä¼šé€æ¸åç¦»å…¶é¢„è®­ç»ƒæƒé‡ã€‚æˆ‘ä»¬æå‡ºäº†â€œRLEPâ€â€”â€”å¸¦æœ‰ç»éªŒé‡æ’­çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning with Experience rePlayï¼‰â€”â€”è¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé¦–å…ˆæ”¶é›†éªŒè¯è¿‡çš„è½¨è¿¹ï¼Œç„¶ååœ¨éšåçš„è®­ç»ƒä¸­è¿›è¡Œé‡æ’­ã€‚åœ¨æ¯ä¸ªæ›´æ–°æ­¥éª¤ä¸­ï¼Œç­–ç•¥éƒ½ä¼šåœ¨æ–°ç”Ÿæˆçš„æ»šåŠ¨æ•°æ®ä¸è¿™äº›é‡æ’­çš„æˆåŠŸæ¡ˆä¾‹çš„æ··åˆå°æ‰¹é‡æ•°æ®ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡é‡æ’­é«˜è´¨é‡ç¤ºä¾‹ï¼ŒRLEPå¼•å¯¼æ¨¡å‹è¿œç¦»æ— æ•ˆçš„æ¢ç´¢ï¼Œä¸“æ³¨äºæœ‰å‰æ™¯çš„æ¨ç†è·¯å¾„çš„å­¦ä¹ ï¼Œå¹¶å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¼ºçš„æœ€ç»ˆæ€§èƒ½ã€‚åœ¨Qwen2.5-Math-7BåŸºå‡†æ¨¡å‹ä¸Šï¼ŒRLEPåœ¨æ›´å°‘çš„æ›´æ–°æ¬¡æ•°å†…è¾¾åˆ°äº†åŸºå‡†å³°å€¼ç²¾åº¦ï¼Œå¹¶æœ€ç»ˆè¶…è¶Šäº†å®ƒï¼Œå°†AIME-2024çš„å‡†ç¡®æ€§ä»38.2%æé«˜åˆ°39.9%ï¼Œå°†AIME-2025çš„å‡†ç¡®æ€§ä»19.8%æé«˜åˆ°22.3%ï¼Œå¹¶å°†AMC-2023çš„å‡†ç¡®æ€§ä»77.0%æé«˜åˆ°82.2%ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ£€æŸ¥ç‚¹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/RLEP%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%EF%BC%8C%E4%BB%A5%E6%96%B9%E4%BE%BF%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E5%92%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/Kwai-Klear/RLEPä¸Šå…¬å¼€æä¾›ï¼Œä»¥æ–¹ä¾¿å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07451v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/RLEP">https://github.com/Kwai-Klear/RLEP</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€é¡¹èƒ½è€—é«˜çš„å·¥ä½œï¼šè®­ç»ƒä¸ç¨³å®šï¼Œç­–ç•¥å¯èƒ½ä¼šé€æ¸åç¦»å…¶é¢„è®­ç»ƒæƒé‡ã€‚æœ¬æ–‡æå‡ºRLEPâ€”â€”ç»“åˆç»éªŒå›æ”¾ï¼ˆReinforcement Learning with Experience rePlayï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé¦–å…ˆæ”¶é›†éªŒè¯è¿‡çš„è½¨è¿¹ï¼Œç„¶ååœ¨éšåçš„è®­ç»ƒä¸­è¿›è¡Œå›æ”¾ã€‚åœ¨æ¯ä¸ªæ›´æ–°æ­¥éª¤ä¸­ï¼Œç­–ç•¥éƒ½ä¼šåœ¨æ–°ç”Ÿæˆçš„æ»šåŠ¨æ•°æ®ä¸è¿™äº›å›æ”¾çš„æˆåŠŸæ¡ˆä¾‹çš„æ··åˆå°æ‰¹é‡æ•°æ®ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡å›æ”¾é«˜è´¨é‡ç¤ºä¾‹ï¼ŒRLEPå¼•å¯¼æ¨¡å‹è¿œç¦»æ— æ•ˆæ¢ç´¢ï¼Œä¸“æ³¨äºæœ‰å‰é€”çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œå®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´å¼ºçš„æœ€ç»ˆæ€§èƒ½ã€‚åœ¨Qwen2.5-Math-7BåŸºå‡†æ¨¡å‹ä¸Šï¼ŒRLEPåœ¨æ˜¾è‘—æ›´å°‘çš„æ›´æ–°æ¬¡æ•°å†…è¾¾åˆ°äº†å³°å€¼ç²¾åº¦ï¼Œå¹¶æœ€ç»ˆè¶…è¶Šäº†å®ƒï¼Œåœ¨AIME-2024ä¸Šçš„å‡†ç¡®ç‡ä»38.2%æé«˜åˆ°39.9%ï¼Œåœ¨AIME-2025ä¸Šçš„å‡†ç¡®ç‡ä»19.8%æé«˜åˆ°22.3%ï¼Œåœ¨AMC-2023ä¸Šçš„å‡†ç¡®ç‡ä»77.0%æé«˜åˆ°82.2%ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œæ£€æŸ¥ç‚¹å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Kwai-Klear/RLEP%EF%BC%8C%E4%BB%A5%E6%96%B9%E4%BE%BF%E5%A4%8D%E7%8E%B0%E5%92%8C%E8%BF%9B%E4%B8%80%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/Kwai-Klear/RLEPï¼Œä»¥æ–¹ä¾¿å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå­˜åœ¨ä¸ç¨³å®šæ€§å’Œç­–ç•¥åç¦»çš„é—®é¢˜ã€‚</li>
<li>RLEPæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡æ”¶é›†éªŒè¯è½¨è¿¹å¹¶åœ¨è®­ç»ƒä¸­è¿›è¡Œå›æ”¾æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>RLEPé€šè¿‡å›æ”¾é«˜è´¨é‡ç¤ºä¾‹æ¥å¼•å¯¼æ¨¡å‹ï¼Œé¿å…æ— æ•ˆæ¢ç´¢ï¼Œå¹¶ä¸“æ³¨äºæœ‰å‰é€”çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>RLEPèƒ½åŠ å¿«æ¨¡å‹æ”¶æ•›é€Ÿåº¦å¹¶æå‡æœ€ç»ˆæ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRLEPæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚</li>
<li>RLEPæ¡†æ¶çš„ä»£ç ã€æ•°æ®é›†å’Œæ£€æŸ¥ç‚¹å·²å…¬å¼€å‘å¸ƒï¼Œæ–¹ä¾¿å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31fcbad3a5ce7ccfba1e0e70bfd7dcca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19a06b2fce34f648cc8b0fcd7115d78b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d3218b862eb86497954106e1cb6a513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0f72fbd8139367639adba0453544825.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-562f366a354f531f387923d3ed9d34d3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Interpretable-Time-Series-Foundation-Models"><a href="#Towards-Interpretable-Time-Series-Foundation-Models" class="headerlink" title="Towards Interpretable Time Series Foundation Models"></a>Towards Interpretable Time Series Foundation Models</h2><p><strong>Authors:Matthieu Boileau, Philippe Helluy, Jeremy Pawlus, Svitlana Vyetrenko</strong></p>
<p>In this paper, we investigate the distillation of time series reasoning capabilities into small, instruction-tuned language models as a step toward building interpretable time series foundation models. Leveraging a synthetic dataset of mean-reverting time series with systematically varied trends and noise levels, we generate natural language annotations using a large multimodal model and use these to supervise the fine-tuning of compact Qwen models. We introduce evaluation metrics that assess the quality of the distilled reasoning - focusing on trend direction, noise intensity, and extremum localization - and show that the post-trained models acquire meaningful interpretive capabilities. Our results highlight the feasibility of compressing time series understanding into lightweight, language-capable models suitable for on-device or privacy-sensitive deployment. This work contributes a concrete foundation toward developing small, interpretable models that explain temporal patterns in natural language. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›æç‚¼æˆå°å‹ã€å¯æŒ‡å¯¼çš„è¯­è¨€æ¨¡å‹ï¼Œä½œä¸ºæ„å»ºå¯è§£é‡Šæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„ä¸€æ­¥ã€‚æˆ‘ä»¬åˆ©ç”¨ä¸€ä¸ªå…·æœ‰ç³»ç»ŸåŒ–è¶‹åŠ¿å˜åŒ–å’Œå™ªå£°æ°´å¹³çš„å‡å€¼å›å½’åˆæˆæ•°æ®é›†ï¼Œå€ŸåŠ©å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€æ³¨é‡Šï¼Œå¹¶ç”¨è¿™äº›æ³¨é‡Šå¯¹ç´§å‡‘Qwenæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å¼•å…¥äº†è¯„ä¼°æç‚¼æ¨ç†è´¨é‡çš„è¯„ä»·æŒ‡æ ‡ï¼Œé‡ç‚¹è€ƒå¯Ÿè¶‹åŠ¿æ–¹å‘ã€å™ªå£°å¼ºåº¦å’Œæå€¼å®šä½ï¼Œå¹¶å±•ç¤ºç»è¿‡è®­ç»ƒåçš„æ¨¡å‹è·å¾—äº†æœ‰æ„ä¹‰çš„è§£é‡Šèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†å°†æ—¶é—´åºåˆ—ç†è§£å‹ç¼©æˆå°å‹ã€å…·å¤‡è¯­è¨€èƒ½åŠ›çš„æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œè¿™äº›æ¨¡å‹é€‚åˆåœ¨è®¾å¤‡ç«¯æˆ–éšç§æ•æ„Ÿçš„åœºæ™¯ä¸‹è¿›è¡Œéƒ¨ç½²ã€‚è¿™é¡¹ç ”ç©¶ä¸ºå®ç°å°å‹å¯è§£é‡Šæ¨¡å‹çš„ç ”å‘å¥ å®šäº†åŸºç¡€ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿä»¥è‡ªç„¶è¯­è¨€è§£é‡Šæ—¶é—´åºåˆ—æ¨¡å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07439v1">PDF</a> International Conference on Machine Leaning (ICML) 2025 Workshop on   Foundation Models for Structured Data</p>
<p><strong>Summary</strong>:<br>æœ¬æ–‡ç ”ç©¶äº†å°†æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å°å‹ã€æŒ‡ä»¤è°ƒä¼˜çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œä½œä¸ºæ„å»ºå¯è§£é‡Šæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„ä¸€æ­¥ã€‚é€šè¿‡åˆ©ç”¨å…·æœ‰ç³»ç»Ÿå˜åŒ–è¶‹åŠ¿å’Œå™ªå£°æ°´å¹³çš„å¹³å‡å›å½’æ—¶é—´åºåˆ—çš„åˆæˆæ•°æ®é›†ï¼Œä½¿ç”¨å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€æ³¨é‡Šï¼Œå¹¶ç”¨äºç›‘ç£ç´§å‡‘æ¨¡å‹çš„å¾®è°ƒã€‚å¼•å…¥è¯„ä¼°æŒ‡æ ‡ï¼Œè¯„ä¼°è’¸é¦æ¨ç†çš„è´¨é‡ï¼Œé‡ç‚¹å…³æ³¨è¶‹åŠ¿æ–¹å‘ã€å™ªå£°å¼ºåº¦å’Œæå€¼å®šä½ã€‚ç»“æœæ˜¾ç¤ºï¼Œè®­ç»ƒåçš„æ¨¡å‹è·å¾—äº†æœ‰æ„ä¹‰çš„è§£é‡Šèƒ½åŠ›ï¼Œè¯æ˜äº†å°†æ—¶é—´åºåˆ—ç†è§£å‹ç¼©æˆå…·æœ‰è¯­è¨€èƒ½åŠ›çš„è½»å‹æ¨¡å‹ï¼Œé€‚åˆåœ¨è®¾å¤‡ç«¯æˆ–éšç§æ•æ„Ÿç¯å¢ƒä¸­éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚æœ¬æ–‡ä¸ºå®ç°å°å‹ã€å¯è§£é‡Šæ¨¡å‹çš„ç ”å‘å¥ å®šäº†åŸºç¡€ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè§£é‡Šè‡ªç„¶è¯­è¨€ä¸­çš„æ—¶é—´æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç ”ç©¶äº†å¦‚ä½•å°†æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å°å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
<li>åˆ©ç”¨åˆæˆæ•°æ®é›†ç”Ÿæˆè‡ªç„¶è¯­è¨€æ³¨é‡Šï¼Œç”¨äºç›‘ç£æ¨¡å‹å¾®è°ƒã€‚</li>
<li>å¼•å…¥äº†è¯„ä¼°è’¸é¦æ¨ç†è´¨é‡çš„æŒ‡æ ‡ï¼ŒåŒ…æ‹¬è¶‹åŠ¿æ–¹å‘ã€å™ªå£°å¼ºåº¦å’Œæå€¼å®šä½ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºè®­ç»ƒåçš„æ¨¡å‹è·å¾—äº†æœ‰æ„ä¹‰çš„è§£é‡Šèƒ½åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå®ç°å…·æœ‰è¯­è¨€èƒ½åŠ›çš„è½»å‹æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œé€‚åˆåœ¨è®¾å¤‡ç«¯æˆ–éšç§æ•æ„Ÿç¯å¢ƒä¸­éƒ¨ç½²ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå®ç°å°å‹ã€å¯è§£é‡Šæ¨¡å‹çš„ç ”å‘é“ºå¹³äº†é“è·¯ï¼Œèƒ½å¤Ÿè§£é‡Šè‡ªç„¶è¯­è¨€ä¸­çš„æ—¶é—´æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c1eea1391c183096f3c2cefd87a9acb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf8838811dda82d849d4705d479bce7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74e2aadcb83e6645800962e2ada5a587.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Corvid-Improving-Multimodal-Large-Language-Models-Towards-Chain-of-Thought-Reasoning"><a href="#Corvid-Improving-Multimodal-Large-Language-Models-Towards-Chain-of-Thought-Reasoning" class="headerlink" title="Corvid: Improving Multimodal Large Language Models Towards   Chain-of-Thought Reasoning"></a>Corvid: Improving Multimodal Large Language Models Towards   Chain-of-Thought Reasoning</h2><p><strong>Authors:Jingjing Jiang, Chao Ma, Xurui Song, Hanwang Zhang, Jun Luo</strong></p>
<p>Recent advancements in multimodal large language models (MLLMs) have demonstrated exceptional performance in multimodal perception and understanding. However, leading open-source MLLMs exhibit significant limitations in complex and structured reasoning, particularly in tasks requiring deep reasoning for decision-making and problem-solving. In this work, we present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning capabilities. Architecturally, Corvid incorporates a hybrid vision encoder for informative visual representation and a meticulously designed connector (GateMixer) to facilitate cross-modal alignment. To enhance Corvidâ€™s CoT reasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality multimodal CoT instruction-following dataset, refined and standardized from diverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid with a two-stage CoT-formatted training approach to progressively enhance its step-by-step reasoning abilities. Furthermore, we propose an effective inference-time scaling strategy that enables Corvid to mitigate over-reasoning and under-reasoning through self-verification. Extensive experiments demonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art MLLMs with similar parameter scales, with notable strengths in mathematical reasoning and science problem-solving. Project page: <a target="_blank" rel="noopener" href="https://mm-vl.github.io/corvid">https://mm-vl.github.io/corvid</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•åœ¨å¤šæ¨¡æ€æ„ŸçŸ¥å’Œç†è§£æ–¹é¢å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œé¢†å…ˆçš„å¼€æºMLLMåœ¨å¤æ‚å’Œç»“æ„åŒ–çš„æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ·±åº¦æ¨ç†è¿›è¡Œå†³ç­–å’Œé—®é¢˜è§£å†³çš„ä»»åŠ¡ä¸­å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Corvidï¼Œä¸€ä¸ªå…·æœ‰å¢å¼ºæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›çš„MLLMã€‚åœ¨ç»“æ„ä¸Šï¼ŒCorvidé‡‡ç”¨æ··åˆè§†è§‰ç¼–ç å™¨è¿›è¡Œä¿¡æ¯è§†è§‰è¡¨ç¤ºï¼Œå¹¶ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªè¿æ¥å™¨ï¼ˆGateMixerï¼‰ä»¥ä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½ã€‚ä¸ºäº†å¢å¼ºCorvidçš„CoTæ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†MCoT-Instruct-287Kï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€CoTæŒ‡ä»¤éµå¾ªæ•°æ®é›†ï¼Œç»è¿‡ç­›é€‰å’Œæ ‡å‡†åŒ–æ¥è‡ªä¸åŒçš„å…¬å…±æ¨ç†æ¥æºã€‚åˆ©ç”¨æ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µçš„CoTæ ¼å¼åŒ–è®­ç»ƒæ–¹æ³•å¯¹Corvidè¿›è¡Œå¾®è°ƒï¼Œä»¥é€æ­¥æé«˜å…¶åˆ†æ­¥æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ¨ç†æ—¶é—´ç¼©æ”¾ç­–ç•¥ï¼Œä½¿Corvidèƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘éªŒè¯æ¥å‡è½»è¿‡åº¦æ¨ç†å’Œæ¨ç†ä¸è¶³çš„é—®é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCorvidä¼˜äºç°æœ‰çš„ç±»ä¼¼ChatGPTçš„MLLMså’Œå…·æœ‰ç›¸ä¼¼å‚æ•°è§„æ¨¡çš„æœ€æ–°MLLMsï¼Œåœ¨æ•°å­¦æ¨ç†å’Œç§‘å­¦é—®é¢˜è§£å†³æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mm-vl.github.io/corvid%E3%80%82">https://mm-vl.github.io/corvidã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07424v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Corvidè¿™ä¸€å…·å¤‡å¢å¼ºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚Corvidé‡‡ç”¨æ··åˆè§†è§‰ç¼–ç å™¨å’Œç²¾å¿ƒè®¾è®¡è¿æ¥å™¨ï¼ˆGateMixerï¼‰å®ç°è·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶å¼•å…¥MCoT-Instruct-287Ké«˜è´¨é‡å¤šæ¨¡æ€CoTæŒ‡ä»¤éµå¾ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒä»¥æå‡æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡é‡‡ç”¨ä¸¤é˜¶æ®µçš„CoTæ ¼å¼åŒ–è®­ç»ƒæ–¹æ³•ï¼ŒCorvidé€æ­¥å¢å¼ºäº†å…¶åˆ†æ­¥æ¨ç†èƒ½åŠ›ï¼Œå¹¶æå‡ºä¸€ç§æœ‰æ•ˆçš„æ¨ç†æ—¶é—´ç¼©æ”¾ç­–ç•¥ï¼Œé€šè¿‡è‡ªæˆ‘éªŒè¯æ¥å‡è½»è¿‡åº¦æ¨ç†å’Œæ¨ç†ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒCorvidåœ¨æ•°å­¦æ¨ç†å’Œç§‘å­¦é—®é¢˜è§£å†³æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰çš„ç±»ä¼¼å‚æ•°è§„æ¨¡çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Corvidæ˜¯ä¸€ä¸ªå…·å¤‡å¢å¼ºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚</li>
<li>Corvidé‡‡ç”¨æ··åˆè§†è§‰ç¼–ç å™¨å’Œè¿æ¥å™¨å®ç°è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>MCoT-Instruct-287Kæ•°æ®é›†çš„å¼•å…¥ç”¨äºæå‡Corvidçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µçš„CoTæ ¼å¼åŒ–è®­ç»ƒï¼ŒCorvidé€æ­¥å¢å¼ºåˆ†æ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Corvidé€šè¿‡è‡ªæˆ‘éªŒè¯æ¥å‡è½»è¿‡åº¦æ¨ç†å’Œæ¨ç†ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>Corvidåœ¨æ•°å­¦æ¨ç†å’Œç§‘å­¦é—®é¢˜è§£å†³æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e60db7f20f675eeaad4f3204571b336c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ca6ad142006feea6b2162a525ebbd6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d946f83c509f4d7fc3828b82fbd2f149.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d26690d8404083d2220a013620086711.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1cf7c27842e6ca07b86214eaee8182d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Generalized-Tree-Edit-Distance-GTED-A-Faithful-Evaluation-Metric-for-Statement-Autoformalization"><a href="#Generalized-Tree-Edit-Distance-GTED-A-Faithful-Evaluation-Metric-for-Statement-Autoformalization" class="headerlink" title="Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for   Statement Autoformalization"></a>Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for   Statement Autoformalization</h2><p><strong>Authors:Yuntian Liu, Tao Zhu, Xiaoyang Liu, Yu Chen, Zhaoxuan Liu, Qingfeng Guo, Jiashuo Zhang, Kangjie Bao, Tao Luo</strong></p>
<p>Statement autoformalization, the automated translation of statement from natural language into formal languages, has become a subject of extensive research, yet the development of robust automated evaluation metrics remains limited. Existing evaluation methods often lack semantic understanding, face challenges with high computational costs, and are constrained by the current progress of automated theorem proving. To address these issues, we propose GTED (Generalized Tree Edit Distance), a novel evaluation framework that first standardizes formal statements and converts them into operator trees, then determines the semantic similarity using the eponymous GTED metric. On the miniF2F and ProofNet benchmarks, GTED outperforms all baseline metrics by achieving the highest accuracy and Kappa scores, thus providing the community with a more faithful metric for automated evaluation. The code and experimental results are available at <a target="_blank" rel="noopener" href="https://github.com/XiaoyangLiu-sjtu/GTED">https://github.com/XiaoyangLiu-sjtu/GTED</a>. </p>
<blockquote>
<p>å‘½é¢˜è‡ªåŠ¨å½¢å¼åŒ–æ˜¯å°†è‡ªç„¶è¯­è¨€ä¸­çš„å‘½é¢˜è‡ªåŠ¨è½¬æ¢ä¸ºå½¢å¼åŒ–è¯­è¨€çš„è¿‡ç¨‹ï¼Œç›®å‰å·²ç»æˆä¸ºä¸€ä¸ªå¹¿æ³›ç ”ç©¶çš„è¯¾é¢˜ï¼Œç„¶è€Œå¯é çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å¼€å‘ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€ç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œé¢ä¸´è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œå¹¶ä¸”å—åˆ°å½“å‰è‡ªåŠ¨å®šç†è¯æ˜å‘å±•çš„åˆ¶çº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GTEDï¼ˆå¹¿ä¹‰æ ‘ç¼–è¾‘è·ç¦»ï¼‰è¿™ä¸€æ–°çš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆæ ‡å‡†åŒ–å½¢å¼åŒ–è¯­å¥å¹¶å°†å…¶è½¬æ¢ä¸ºè¿ç®—ç¬¦æ ‘ï¼Œç„¶åä½¿ç”¨åŒåçš„GTEDæŒ‡æ ‡ç¡®å®šè¯­ä¹‰ç›¸ä¼¼æ€§ã€‚åœ¨miniF2Få’ŒProofNetåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGTEDåœ¨å‡†ç¡®ç‡å’ŒKappaå¾—åˆ†æ–¹é¢å‡ä¼˜äºæ‰€æœ‰åŸºçº¿æŒ‡æ ‡ï¼Œä¸ºç¤¾åŒºæä¾›äº†æ›´å¯é çš„è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚ç›¸å…³ä»£ç å’Œå®éªŒç»“æœå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/XiaoyangLiu-sjtu/GTED%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/XiaoyangLiu-sjtu/GTEDè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07399v1">PDF</a> Accepted to AI4Math@ICML25</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨è¯­å¥å½¢å¼åŒ–æ˜¯è‡ªç„¶è¯­è¨€å‘å½¢å¼è¯­è¨€è‡ªåŠ¨ç¿»è¯‘çš„ç ”ç©¶è¯¾é¢˜ï¼Œä½†å¯é çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å¼€å‘ä»ç„¶æœ‰é™ã€‚ä¸ºè§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•ç¼ºä¹è¯­ä¹‰ç†è§£ã€è®¡ç®—æˆæœ¬é«˜ä»¥åŠä¸è‡ªåŠ¨å®šç†è¯æ˜è¿›å±•å—é™çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GTEDï¼ˆå¹¿ä¹‰æ ‘ç¼–è¾‘è·ç¦»ï¼‰è¿™ä¸€æ–°å‹è¯„ä¼°æ¡†æ¶ã€‚å®ƒé€šè¿‡æ ‡å‡†åŒ–å½¢å¼è¯­å¥å¹¶å°†å…¶è½¬æ¢ä¸ºæ“ä½œç¬¦æ ‘ï¼Œç„¶åä½¿ç”¨åŒåGTEDæŒ‡æ ‡ç¡®å®šè¯­ä¹‰ç›¸ä¼¼æ€§ã€‚åœ¨miniF2Få’ŒProofNetåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGTEDä»¥æœ€é«˜å‡†ç¡®ç‡å’ŒKappaåˆ†æ•°ä¼˜äºæ‰€æœ‰åŸºçº¿æŒ‡æ ‡ï¼Œä¸ºç¤¾åŒºæä¾›äº†æ›´å‡†ç¡®çš„è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚ç›¸å…³ä»£ç å’Œå®éªŒç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XiaoyangLiu-sjtu/GTED">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­å¥å½¢å¼åŒ–æ˜¯ä¸€ä¸ªçƒ­é—¨ç ”ç©¶é¢†åŸŸï¼Œä½†è¯„ä¼°æŒ‡æ ‡çš„å¯é æ€§ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•å­˜åœ¨è¯­ä¹‰ç†è§£ä¸è¶³ã€è®¡ç®—æˆæœ¬é«˜ä»¥åŠä¸è‡ªåŠ¨å®šç†è¯æ˜è¿›å±•å—é™çš„é—®é¢˜ã€‚</li>
<li>GTEDæ¡†æ¶é€šè¿‡æ ‡å‡†åŒ–å½¢å¼è¯­å¥å¹¶è½¬æ¢ä¸ºæ“ä½œç¬¦æ ‘æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>GTEDåˆ©ç”¨å¹¿ä¹‰æ ‘ç¼–è¾‘è·ç¦»æŒ‡æ ‡ç¡®å®šè¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</li>
<li>åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGTEDè¡¨ç°å‡ºä¼˜äºå…¶ä»–åŸºçº¿æŒ‡æ ‡çš„æ€§èƒ½ã€‚</li>
<li>GTEDæä¾›äº†æ›´å‡†ç¡®çš„è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºç¤¾åŒºæä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33e67c35961da7c27a3274bb835ffa06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f79ba380bf672c8ec75b999c36c17d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-544f53a5cd215917838958fc9aade3a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f96bd8e52b6c3575670d2ff926cd424.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc85aa1b2e584ffaee7555fe3f7ba13c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-929188dc3dc95a63c03c4b90b0953331.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Bradley-Terry-and-Multi-Objective-Reward-Modeling-Are-Complementary"><a href="#Bradley-Terry-and-Multi-Objective-Reward-Modeling-Are-Complementary" class="headerlink" title="Bradley-Terry and Multi-Objective Reward Modeling Are Complementary"></a>Bradley-Terry and Multi-Objective Reward Modeling Are Complementary</h2><p><strong>Authors:Zhiwei Zhang, Hui Liu, Xiaomin Li, Zhenwei Dai, Jingying Zeng, Fali Wang, Minhua Lin, Ramraj Chandradevan, Zhen Li, Chen Luo, Xianfeng Tang, Qi He, Suhang Wang</strong></p>
<p>Reward models trained on human preference data have demonstrated strong effectiveness in aligning Large Language Models (LLMs) with human intent under the framework of Reinforcement Learning from Human Feedback (RLHF). However, RLHF remains vulnerable to reward hacking, where the policy exploits imperfections in the reward function rather than genuinely learning the intended behavior. Although significant efforts have been made to mitigate reward hacking, they predominantly focus on and evaluate in-distribution scenarios, where the training and testing data for the reward model share the same distribution. In this paper, we empirically show that state-of-the-art methods struggle in more challenging out-of-distribution (OOD) settings. We further demonstrate that incorporating fine-grained multi-attribute scores helps address this challenge. However, the limited availability of high-quality data often leads to weak performance of multi-objective reward functions, which can negatively impact overall performance and become the bottleneck. To address this issue, we propose a unified reward modeling framework that jointly trains Bradleyâ€“Terry (BT) single-objective and multi-objective regression-based reward functions using a shared embedding space. We theoretically establish a connection between the BT loss and the regression objective and highlight their complementary benefits. Specifically, the regression task enhances the single-objective reward functionâ€™s ability to mitigate reward hacking in challenging OOD settings, while BT-based training improves the scoring capability of the multi-objective reward function, enabling a 7B model to outperform a 70B baseline. Extensive experimental results demonstrate that our framework significantly improves both the robustness and the scoring performance of reward models. </p>
<blockquote>
<p>åŸºäºäººç±»åå¥½æ•°æ®çš„å¥–åŠ±æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰çš„æ¡†æ¶ä¸‹ï¼Œå·²ç»æ˜¾ç¤ºå‡ºåœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»æ„å›¾å¯¹é½æ–¹é¢çš„å¼ºå¤§æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼ŒRLHFä»ç„¶å®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»çš„å¨èƒï¼Œç­–ç•¥ä¼šåˆ©ç”¨å¥–åŠ±å‡½æ•°ä¸­çš„ç¼ºé™·ï¼Œè€Œä¸æ˜¯çœŸæ­£å­¦ä¹ é¢„æœŸçš„è¡Œä¸ºã€‚å°½ç®¡å·²ç»ä»˜å‡ºäº†å¾ˆå¤§çš„åŠªåŠ›æ¥ç¼“è§£å¥–åŠ±é»‘å®¢æ”»å‡»çš„é—®é¢˜ï¼Œä½†å®ƒä»¬ä¸»è¦ä¸“æ³¨äºå¹¶è¯„ä¼°åœ¨åˆ†å¸ƒå†…çš„åœºæ™¯ï¼Œå³å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ¥è‡ªåŒä¸€åˆ†å¸ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å®è¯è¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ–¹æ³•åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰è®¾ç½®ä¸‹ä¼šé™·å…¥å›°å¢ƒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œèå…¥ç²¾ç»†ç²’åº¦çš„å¤šå±æ€§åˆ†æ•°æœ‰åŠ©äºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡æ•°æ®çš„æœ‰é™å¯ç”¨æ€§å¸¸å¸¸å¯¼è‡´å¤šç›®æ ‡å¥–åŠ±å‡½æ•°è¡¨ç°ä¸ä½³ï¼Œå¯èƒ½ä¼šç»™æ•´ä½“æ€§èƒ½å¸¦æ¥è´Ÿé¢å½±å“å¹¶æˆä¸ºç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å…±äº«åµŒå…¥ç©ºé—´æ¥è”åˆè®­ç»ƒBradley-Terryï¼ˆBTï¼‰å•ç›®æ ‡å¥–åŠ±å‡½æ•°å’Œå¤šç›®æ ‡å›å½’å¥–åŠ±å‡½æ•°ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šå»ºç«‹äº†BTæŸå¤±å’Œå›å½’ç›®æ ‡ä¹‹é—´çš„è”ç³»ï¼Œå¹¶å¼ºè°ƒäº†å®ƒä»¬çš„äº’è¡¥ä¼˜åŠ¿ã€‚å…·ä½“è€Œè¨€ï¼Œå›å½’ä»»åŠ¡å¢å¼ºäº†å•ç›®æ ‡å¥–åŠ±å‡½æ•°åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„OODç¯å¢ƒä¸­ç¼“è§£å¥–åŠ±é»‘å®¢æ”»å‡»çš„èƒ½åŠ›ï¼Œè€ŒåŸºäºBTçš„è®­ç»ƒæé«˜äº†å¤šç›®æ ‡å¥–åŠ±å‡½æ•°çš„è¯„åˆ†èƒ½åŠ›ï¼Œä½¿å¾—ä¸€ä¸ªè§„æ¨¡ä¸º7Bçš„æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šè§„æ¨¡ä¸º70Bçš„åŸºçº¿æ¨¡å‹ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—æé«˜äº†å¥–åŠ±æ¨¡å‹çš„ç¨³å¥æ€§å’Œè¯„åˆ†æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07375v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¥–åŠ±æ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥åŸºäºäººç±»åå¥½æ•°æ®æœ‰æ•ˆè°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»æ„å›¾å¯¹é½çš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨å¥–åŠ±ç ´è§£çš„è„†å¼±æ€§é£é™©ï¼Œæ”¿ç­–åˆ¶å®šè€…å¯èƒ½é€šè¿‡å‘ç°å¥–åŠ±æœºåˆ¶çš„æ¼æ´è€Œæ‰§è¡Œæ„æƒ³ä¸åˆ°çš„è¡Œä¸ºã€‚å°½ç®¡æœ‰è®¸å¤šå°è¯•é™ä½å¥–åŠ±ç ´è§£é£é™©çš„åŠªåŠ›ï¼Œä½†è¿™äº›åŠªåŠ›ä¸»è¦å…³æ³¨å¹¶è¯„ä¼°åœ¨åˆ†å¸ƒå†…çš„åœºæ™¯ï¼Œå³å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®å…±äº«ç›¸åŒçš„åˆ†å¸ƒã€‚æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶å‘ç°ï¼Œæœ€å…ˆè¿›çš„å¥–åŠ±æ¨¡å‹åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„åˆ†å¸ƒå¤–è®¾ç½®ç¯å¢ƒä¸­ä»å­˜åœ¨çŸ­æ¿ã€‚å®éªŒæ˜¾ç¤ºå°†ç²¾ç»†ç²’åº¦çš„å¤šå±æ€§åˆ†æ•°çº³å…¥å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿåº”å¯¹è¯¥æŒ‘æˆ˜ã€‚ä½†æ˜¯ï¼Œé«˜è´¨é‡æ•°æ®çš„æœ‰é™å¯ç”¨æ€§é™åˆ¶äº†å¤šç›®æ ‡å¥–åŠ±å‡½æ•°çš„æ€§èƒ½æå‡ï¼Œè¿™å¯èƒ½ä¼šæˆä¸ºç“¶é¢ˆé—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†Bradley-Terryå•ç›®æ ‡å›å½’å’Œå¤šç›®æ ‡å›å½’å¥–åŠ±å‡½æ•°åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸Šçš„è®­ç»ƒæ–¹å¼ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¸ä»…æ˜¾è‘—æé«˜äº†å¥–åŠ±æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œè¿˜æé«˜äº†å…¶è¯„åˆ†æ€§èƒ½ã€‚ </p>
<p><strong>Key Takeaways</strong>ï¼š </p>
<ul>
<li>å¥–åŠ±æ¨¡å‹åœ¨è®­ç»ƒåå¯ä»¥æœ‰æ•ˆåœ°ä¸äººç±»åå¥½å¯¹é½ä»¥æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ </li>
<li>å­˜åœ¨ä¸€ç§é£é™©å«åšå¥–åŠ±ç ´è§£ï¼Œå³æ¨¡å‹å¯èƒ½å‘ç°å¥–åŠ±æœºåˆ¶çš„æ¼æ´å¹¶æ‰§è¡Œæ„æƒ³ä¸åˆ°çš„è¡Œä¸ºã€‚ </li>
<li>å½“å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨åˆ†å¸ƒå†…çš„åœºæ™¯ï¼Œå°šæœªè§£å†³æ›´å…·æŒ‘æˆ˜æ€§çš„åˆ†å¸ƒå¤–åœºæ™¯ä¸­çš„é—®é¢˜ã€‚ </li>
<li>å°†ç²¾ç»†ç²’åº¦çš„å¤šå±æ€§åˆ†æ•°çº³å…¥å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿåº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚ </li>
<li>é«˜è´¨é‡æ•°æ®çš„æœ‰é™å¯ç”¨æ€§é™åˆ¶äº†å¤šç›®æ ‡å¥–åŠ±å‡½æ•°çš„æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a19f7578c3e101550029c17e917b365.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d322886928ba4b8faeab07fe1e82911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f35710ef56b8aee5f36bcd1a89c6bd3b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Plausibility-Validity-Gap-by-Fine-Tuning-a-Reasoning-Enhanced-LLM-for-Chemical-Synthesis-and-Discovery"><a href="#Bridging-the-Plausibility-Validity-Gap-by-Fine-Tuning-a-Reasoning-Enhanced-LLM-for-Chemical-Synthesis-and-Discovery" class="headerlink" title="Bridging the Plausibility-Validity Gap by Fine-Tuning a   Reasoning-Enhanced LLM for Chemical Synthesis and Discovery"></a>Bridging the Plausibility-Validity Gap by Fine-Tuning a   Reasoning-Enhanced LLM for Chemical Synthesis and Discovery</h2><p><strong>Authors: Malikussaid, Hilal Hudan Nuha</strong></p>
<p>Large Language Models (LLMs) often generate scientifically plausible but factually invalid information, a challenge we term the â€œplausibility-validity gap,â€ particularly in specialized domains like chemistry. This paper presents a systematic methodology to bridge this gap by developing a specialized scientific assistant. We utilized the Magistral Small model, noted for its integrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation (LoRA). A key component of our approach was the creation of a â€œdual-domain dataset,â€ a comprehensive corpus curated from various sources encompassing both molecular properties and chemical reactions, which was standardized to ensure quality. Our evaluation demonstrates that the fine-tuned model achieves significant improvements over the baseline model in format adherence, chemical validity of generated molecules, and the feasibility of proposed synthesis routes. The results indicate a hierarchical learning pattern, where syntactic correctness is learned more readily than chemical possibility and synthesis feasibility. While a comparative analysis with human experts revealed competitive performance in areas like chemical creativity and reasoning, it also highlighted key limitations, including persistent errors in stereochemistry, a static knowledge cutoff, and occasional reference hallucination. This work establishes a viable framework for adapting generalist LLMs into reliable, specialized tools for chemical research, while also delineating critical areas for future improvement. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸ç”Ÿæˆç§‘å­¦ä¸Šåˆç†ä½†äº‹å®æ— æ•ˆçš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºâ€œå¯è¡Œæ€§-æœ‰æ•ˆæ€§å·®è·â€ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ–å­¦ç­‰ç‰¹å®šé¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¼€å‘ä¸“ç”¨ç§‘å­¦åŠ©ç†æ¥å¼¥è¡¥è¿™ä¸€å·®è·çš„ç³»ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨ä»¥ç»¼åˆæ¨ç†èƒ½åŠ›è‘—ç§°çš„Magistral Smallæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®ç»„æˆéƒ¨åˆ†æ˜¯åˆ›å»ºâ€œåŒåŸŸæ•°æ®é›†â€ï¼Œè¿™æ˜¯ä¸€ä¸ªä»å„ç§æ¥æºç²¾å¿ƒæŒ‘é€‰çš„ç»¼åˆè¯­æ–™åº“ï¼Œæ¶µç›–äº†åˆ†å­å±æ€§å’ŒåŒ–å­¦ååº”ï¼Œå¹¶è¿›è¡Œäº†æ ‡å‡†åŒ–ä»¥ç¡®ä¿è´¨é‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œç»è¿‡å¾®è°ƒåçš„æ¨¡å‹åœ¨æ ¼å¼éµå®ˆã€ç”Ÿæˆåˆ†å­çš„åŒ–å­¦æœ‰æ•ˆæ€§å’Œæè®®çš„åˆæˆè·¯çº¿çš„å¯è¡Œæ€§æ–¹é¢å–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚ç»“æœè¡¨æ˜ï¼Œä¸€ç§åˆ†å±‚å­¦ä¹ æ¨¡å¼ï¼Œå…¶ä¸­è¯­æ³•æ­£ç¡®æ€§çš„å­¦ä¹ æ¯”åŒ–å­¦å¯èƒ½æ€§å’Œåˆæˆå¯è¡Œæ€§æ›´å®¹æ˜“ã€‚è™½ç„¶ä¸äººç±»ä¸“å®¶çš„æ¯”è¾ƒåˆ†ææ˜¾ç¤ºäº†åœ¨åŒ–å­¦åˆ›é€ åŠ›å’Œæ¨ç†ç­‰é¢†åŸŸçš„ç«äº‰åŠ›ï¼Œä½†ä¹Ÿçªå‡ºäº†å…³é”®é™åˆ¶ï¼ŒåŒ…æ‹¬ç«‹ä½“åŒ–å­¦çš„æŒä¹…æ€§é”™è¯¯ã€é™æ€çŸ¥è¯†æˆªæ­¢å’Œå¶å°”çš„å‚è€ƒå¹»è§‰ã€‚è¿™é¡¹å·¥ä½œä¸ºå°†é€šç”¨LLMé€‚åº”ä¸ºå¯é çš„åŒ–å­¦ç ”ç©¶ä¸“ç”¨å·¥å…·å»ºç«‹äº†å¯è¡Œçš„æ¡†æ¶ï¼ŒåŒæ—¶åˆ’å®šäº†æœªæ¥æ”¹è¿›çš„å…³é”®é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07328v1">PDF</a> 42 pages, 8 figures, 1 equation, 2 algorithms, 31 tables, to be   published in ISPACS Conference 2025, unabridged version</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸä¼šç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…ä¸æ­£ç¡®ä¿¡æ¯ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–æ–¹æ³•æ¥å¼¥è¡¥è¿™ä¸€ç¼ºé™·ã€‚é€šè¿‡ä½¿ç”¨Magistral Smallæ¨¡å‹å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œç»“åˆåˆ›å»ºåŒ…å«åˆ†å­å±æ€§å’ŒåŒ–å­¦ååº”çš„â€œåŒåŸŸæ•°æ®é›†â€ï¼Œæå‡æ¨¡å‹å¯¹åŒ–å­¦é¢†åŸŸçš„é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚å°½ç®¡ä»æœ‰å±€é™æ€§ï¼Œä½†æœ¬æ–‡ä¸ºé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºå¯é åŒ–å­¦ç ”ç©¶å·¥å…·å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸå­˜åœ¨ç”Ÿæˆçš„ä¿¡æ¯çœ‹ä¼¼åˆç†ä½†å®é™…ä¸å‡†ç¡®çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ–å­¦ç­‰ç‰¹å®šé¢†åŸŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒ…æ‹¬ä½¿ç”¨Magistral Smallæ¨¡å‹å’ŒLoRAæŠ€æœ¯è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>åˆ›å»ºäº†åŒ…å«åˆ†å­å±æ€§å’ŒåŒ–å­¦ååº”çš„â€œåŒåŸŸæ•°æ®é›†â€ï¼Œä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨æ ¼å¼éµå¾ªã€åŒ–å­¦åˆ†å­æœ‰æ•ˆæ€§ä»¥åŠåˆæˆè·¯çº¿å¯è¡Œæ€§æ–¹é¢æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
<li>æ¨¡å‹å­¦ä¹ å‘ˆç°å‡ºå±‚æ¬¡ç»“æ„ï¼Œå³è¯­æ³•çš„æ­£ç¡®æ€§æ›´å®¹æ˜“å­¦ä¹ ï¼Œè€ŒåŒ–å­¦å¯èƒ½æ€§ä¸åˆæˆè·¯çº¿çš„å¯è¡Œæ€§éœ€è¦è¿›ä¸€æ­¥æé«˜ã€‚</li>
<li>ä¸äººç±»ä¸“å®¶çš„æ¯”è¾ƒåˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨åŒ–å­¦åˆ›é€ åŠ›å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨å…³é”®å±€é™æ€§ï¼Œå¦‚ç«‹ä½“åŒ–å­¦çš„æŒä¹…é”™è¯¯ã€çŸ¥è¯†æˆªæ–­é™æ€ä»¥åŠå¶å°”çš„å‚è€ƒå¹»è§‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c0a884b6dc392dc4058a0f53c98d3bd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-123707a7aa7f23fdada11e9751126f34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35e4283325b551ff72368924e175bcfe.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Frontier-LLMs-Still-Struggle-with-Simple-Reasoning-Tasks"><a href="#Frontier-LLMs-Still-Struggle-with-Simple-Reasoning-Tasks" class="headerlink" title="Frontier LLMs Still Struggle with Simple Reasoning Tasks"></a>Frontier LLMs Still Struggle with Simple Reasoning Tasks</h2><p><strong>Authors:Alan Malek, Jiawei Ge, Nevena Lazic, Chi Jin, AndrÃ¡s GyÃ¶rgy, Csaba SzepesvÃ¡ri</strong></p>
<p>While state-of-the-art large language models (LLMs) demonstrate advanced reasoning capabilities-achieving remarkable performance on challenging competitive math and coding benchmarks-they also frequently fail on tasks that are easy for humans. This work studies the performance of frontier LLMs on a broad set of such â€œeasyâ€ reasoning problems. By extending previous work in the literature, we create a suite of procedurally generated simple reasoning tasks, including counting, first-order logic, proof trees, and travel planning, with changeable parameters (such as document length. or the number of variables in a math problem) that can arbitrarily increase the amount of computation required to produce the answer while preserving the fundamental difficulty. While previous work showed that traditional, non-thinking models can be made to fail on such problems, we demonstrate that even state-of-the-art thinking models consistently fail on such problems and for similar reasons (e.g. statistical shortcuts, errors in intermediate steps, and difficulties in processing long contexts). To further understand the behavior of the models, we introduce the unpuzzles dataset, a different â€œeasyâ€ benchmark consisting of trivialized versions of well-known math and logic puzzles. Interestingly, while modern LLMs excel at solving the original puzzles, they tend to fail on the trivialized versions, exhibiting several systematic failure patterns related to memorizing the originals. We show that this happens even if the models are otherwise able to solve problems with different descriptions but requiring the same logic. Our results highlight that out-of-distribution generalization is still problematic for frontier language models and the new generation of thinking models, even for simple reasoning tasks, and making tasks easier does not necessarily imply improved performance. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œç¼–ç åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºé«˜çº§æ¨ç†èƒ½åŠ›ï¼Œå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆç»©ã€‚ä½†å®ƒä»¬ä¹Ÿç»å¸¸åœ¨å¯¹äºäººç±»æ¥è¯´å¾ˆå®¹æ˜“çš„ä»»åŠ¡ä¸Šå¤±è´¥ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å‰æ²¿LLMåœ¨å¹¿æ³›çš„ä¸€ç³»åˆ—è¿™æ ·çš„â€œç®€å•â€æ¨ç†é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡æ‰©å±•å‰äººæ–‡çŒ®ä¸­çš„å·¥ä½œï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€å¥—ç¨‹åºç”Ÿæˆçš„ç®€å•æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è®¡æ•°ã€ä¸€é˜¶é€»è¾‘ã€è¯æ˜æ ‘å’Œæ—…è¡Œè®¡åˆ’ç­‰ï¼Œè¿™äº›ä»»åŠ¡å…·æœ‰å¯æ›´æ”¹çš„å‚æ•°ï¼ˆå¦‚æ–‡æ¡£é•¿åº¦æˆ–æ•°å­¦é—®é¢˜çš„å˜é‡æ•°é‡ï¼‰ï¼Œè¿™äº›å‚æ•°å¯ä»¥ä»»æ„å¢åŠ è®¡ç®—ç­”æ¡ˆæ‰€éœ€çš„è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒåŸºæœ¬éš¾åº¦ä¸å˜ã€‚è™½ç„¶ä»¥å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„éæ€è€ƒæ¨¡å‹å¯ä»¥åœ¨è¿™äº›é—®é¢˜ä¸Šå¤±è´¥ï¼Œä½†æˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ€è€ƒæ¨¡å‹ä¹Ÿä¼šåœ¨è¿™äº›é—®é¢˜ä¸ŠæŒç»­å¤±è´¥ï¼Œå¹¶ä¸”å¤±è´¥åŸå› ç±»ä¼¼ï¼ˆä¾‹å¦‚ç»Ÿè®¡æ·å¾„ã€ä¸­é—´æ­¥éª¤é”™è¯¯ä»¥åŠå¤„ç†é•¿æ–‡æœ¬æ—¶çš„å›°éš¾ç­‰ï¼‰ã€‚ä¸ºäº†æ›´æ·±å…¥åœ°äº†è§£æ¨¡å‹çš„è¡Œä¸ºï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œè°œé¢˜è§£å¯†æ•°æ®é›†â€ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸åŒçš„â€œç®€å•â€åŸºå‡†æµ‹è¯•ï¼Œç”±è‘—åæ•°å­¦å’Œé€»è¾‘è°œé¢˜çš„ç®€åŒ–ç‰ˆç»„æˆã€‚æœ‰è¶£çš„æ˜¯ï¼Œå°½ç®¡ç°ä»£LLMæ“…é•¿è§£å†³åŸå§‹è°œé¢˜ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šåœ¨ç®€åŒ–ç‰ˆä¸Šå¤±è´¥ï¼Œè¡¨ç°å‡ºä¸è®°å¿†åŸå§‹é¢˜ç›®ç›¸å…³çš„å¤šç§ç³»ç»Ÿæ€§å¤±è´¥æ¨¡å¼ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå³ä½¿æ¨¡å‹èƒ½å¤Ÿè§£å†³æè¿°ä¸åŒä½†éœ€è¦ç›¸åŒé€»è¾‘çš„é—®é¢˜ï¼Œè¿™ç§æƒ…å†µä¹Ÿä¼šå‘ç”Ÿã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œå³ä½¿åœ¨ç®€å•æ¨ç†ä»»åŠ¡ä¸Šï¼Œå‰æ²¿è¯­è¨€æ¨¡å‹å’Œæ–°ä¸€ä»£æ€è€ƒæ¨¡å‹åœ¨è¶…å‡ºå…¶åˆ†å¸ƒèŒƒå›´çš„æƒ…å†µä¸‹è¿›è¡Œæ³›åŒ–æ—¶ä»ç„¶å­˜åœ¨é—®é¢˜ï¼Œå¹¶ä¸”ä»»åŠ¡çš„ç®€åŒ–å¹¶ä¸ä¸€å®šæ„å‘³ç€æ€§èƒ½çš„æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07313v1">PDF</a> 53 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œç¼–ç¨‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¸åœ¨äººç±»å¯ä»¥è½»æ¾å®Œæˆçš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶å‰æ²¿LLMåœ¨å¹¿æ³›â€œç®€å•â€æ¨ç†é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬æ‰©å±•äº†æ–‡çŒ®ä¸­çš„å…ˆå‰å·¥ä½œï¼Œåˆ›å»ºäº†ä¸€ç³»åˆ—ç¨‹åºç”Ÿæˆçš„ç®€å•æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è®¡æ•°ã€ä¸€é˜¶é€»è¾‘ã€è¯æ˜æ ‘å’Œæ—…è¡Œè§„åˆ’ç­‰ã€‚è¿™äº›ä»»åŠ¡å¯ä»¥é€šè¿‡æ”¹å˜å‚æ•°ï¼ˆå¦‚æ–‡æ¡£é•¿åº¦æˆ–æ•°å­¦é—®é¢˜çš„å˜é‡æ•°é‡ï¼‰æ¥ä»»æ„å¢åŠ è®¡ç®—ç­”æ¡ˆæ‰€éœ€çš„é‡ï¼ŒåŒæ—¶ä¿æŒåŸºæœ¬éš¾åº¦ä¸å˜ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œå·²ç»è¯æ˜ä¼ ç»Ÿéæ€è€ƒæ¨¡å‹å¯ä»¥è§£å†³è¿™äº›é—®é¢˜å¤±è´¥ï¼Œä½†æˆ‘ä»¬è¯æ˜å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ€è€ƒæ¨¡å‹ä¹Ÿä¼šåœ¨è¿™äº›é—®é¢˜ä¸Šä¸æ–­å¤±è´¥ï¼ŒåŸå› ä¹Ÿç›¸åŒï¼ˆä¾‹å¦‚ç»Ÿè®¡æ·å¾„ã€ä¸­é—´æ­¥éª¤ä¸­çš„é”™è¯¯å’Œå¤„ç†é•¿æ–‡æœ¬çš„å›°éš¾ç­‰ï¼‰ã€‚ä¸ºäº†æ›´å¥½åœ°äº†è§£æ¨¡å‹çš„è¡Œä¸ºï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œè°œé¢˜ä¹‹å¤–â€æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸åŒçš„â€œç®€å•â€åŸºå‡†æµ‹è¯•ï¼Œç”±è‘—åæ•°å­¦å’Œé€»è¾‘è°œé¢˜çš„ç®€åŒ–ç‰ˆç»„æˆã€‚æœ‰è¶£çš„æ˜¯ï¼Œè™½ç„¶ç°ä»£LLMæ“…é•¿è§£å†³åŸå§‹è°œé¢˜ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šåœ¨ç®€åŒ–ç‰ˆä¸Šå¤±è´¥ï¼Œè¡¨ç°å‡ºå¤šç§ç³»ç»Ÿæ€§å¤±è´¥æ¨¡å¼ï¼Œä¸è®°å¿†åŸå§‹é¢˜ç›®æœ‰å…³ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ç®€å•çš„æ¨ç†ä»»åŠ¡ä¸Šï¼Œæœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹å’Œæ–°ä¸€ä»£æ€è€ƒæ¨¡å‹åœ¨è¶…å‡ºå…¶åˆ†å¸ƒèŒƒå›´çš„æ³›åŒ–æ–¹é¢ä»å­˜åœ¨ç¼ºé™·ï¼Œè€Œä¸”ä½¿ä»»åŠ¡å˜å¾—æ›´ç®€å•å¹¶ä¸ä¸€å®šæ„å‘³ç€æ€§èƒ½ä¼šæœ‰æ‰€æ”¹å–„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œç¼–ç¨‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨ç®€å•çš„æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
<li>åˆ›å»ºäº†ç¨‹åºç”Ÿæˆçš„ç®€å•æ¨ç†ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å¯ä»¥é€šè¿‡æ”¹å˜å‚æ•°æ¥ä¿æŒéš¾åº¦ä¸å˜çš„åŒæ—¶å¢åŠ è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>å³ä½¿åœ¨ç®€å•çš„ä»»åŠ¡ä¸Šï¼Œå‰æ²¿çš„LLMä¹Ÿä¼šå› ä¸ºç»Ÿè®¡æ·å¾„ã€ä¸­é—´æ­¥éª¤é”™è¯¯å’Œé•¿æ–‡æœ¬å¤„ç†å›°éš¾ç­‰åŸå› è€Œå¤±è´¥ã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†â€”â€”â€œè°œé¢˜ä¹‹å¤–â€ï¼ŒåŒ…å«ç®€åŒ–ç‰ˆçš„æ•°å­¦å’Œé€»è¾‘è°œé¢˜ï¼Œæ—¨åœ¨æ¢ç©¶LLMåœ¨ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>ç°ä»£LLMåœ¨ç®€åŒ–ç‰ˆè°œé¢˜ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºä¸è®°å¿†åŸå§‹é¢˜ç›®ç›¸å…³çš„ç³»ç»Ÿæ€§å¤±è´¥æ¨¡å¼ã€‚</li>
<li>LLMåœ¨è¶…å‡ºå…¶åˆ†å¸ƒèŒƒå›´çš„ç®€å•æ¨ç†ä»»åŠ¡çš„æ³›åŒ–æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-067689f5b33112e5b61daf3b81213637.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ViDove-A-Translation-Agent-System-with-Multimodal-Context-and-Memory-Augmented-Reasoning"><a href="#ViDove-A-Translation-Agent-System-with-Multimodal-Context-and-Memory-Augmented-Reasoning" class="headerlink" title="ViDove: A Translation Agent System with Multimodal Context and   Memory-Augmented Reasoning"></a>ViDove: A Translation Agent System with Multimodal Context and   Memory-Augmented Reasoning</h2><p><strong>Authors:Yichen Lu, Wei Dai, Jiaen Liu, Ching Wing Kwok, Zongheng Wu, Xudong Xiao, Ao Sun, Sheng Fu, Jianyuan Zhan, Yian Wang, Takatomo Saito, Sicheng Lai</strong></p>
<p>LLM-based translation agents have achieved highly human-like translation results and are capable of handling longer and more complex contexts with greater efficiency. However, they are typically limited to text-only inputs. In this paper, we introduce ViDove, a translation agent system designed for multimodal input. Inspired by the workflow of human translators, ViDove leverages visual and contextual background information to enhance the translation process. Additionally, we integrate a multimodal memory system and long-short term memory modules enriched with domain-specific knowledge, enabling the agent to perform more accurately and adaptively in real-world scenarios. As a result, ViDove achieves significantly higher translation quality in both subtitle generation and general translation tasks, with a 28% improvement in BLEU scores and a 15% improvement in SubER compared to previous state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark for long-form automatic video subtitling and translation, featuring 17 hours of high-quality, human-annotated data. Our code is available here: <a target="_blank" rel="noopener" href="https://github.com/pigeonai-org/ViDove">https://github.com/pigeonai-org/ViDove</a> </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¿»è¯‘ä»£ç†å·²ç»å®ç°äº†é«˜åº¦äººæ€§åŒ–çš„ç¿»è¯‘ç»“æœï¼Œå¹¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†æ›´é•¿ã€æ›´å¤æ‚çš„ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸ä»…é™äºæ–‡æœ¬è¾“å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ViDoveï¼Œä¸€ä¸ªä¸ºå¤šåª’ä½“è¾“å…¥è®¾è®¡çš„ç¿»è¯‘ä»£ç†ç³»ç»Ÿã€‚ViDoveå€Ÿé‰´äº†äººå·¥ç¿»è¯‘çš„å·¥ä½œæµç¨‹ï¼Œåˆ©ç”¨è§†è§‰å’Œä¸Šä¸‹æ–‡èƒŒæ™¯ä¿¡æ¯æ¥å¢å¼ºç¿»è¯‘è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ•´åˆäº†å¤šåª’ä½“è®°å¿†ç³»ç»Ÿå’Œä¸°å¯Œçš„é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„é•¿çŸ­æ—¶è®°å¿†æ¨¡å—ï¼Œä½¿ä»£ç†èƒ½å¤Ÿåœ¨ç°å®åœºæ™¯ä¸­æ›´ç²¾ç¡®ã€æ›´è‡ªé€‚åº”åœ°æ‰§è¡Œã€‚å› æ­¤ï¼ŒViDoveåœ¨å­—å¹•ç”Ÿæˆå’Œä¸€èˆ¬ç¿»è¯‘ä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„ç¿»è¯‘è´¨é‡ï¼Œä¸æœ€æ–°çš„å…ˆè¿›åŸºçº¿ç›¸æ¯”ï¼ŒBLEUå¾—åˆ†æé«˜äº†28%ï¼ŒSubERæé«˜äº†15%ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DoveBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„é•¿æ ¼å¼è‡ªåŠ¨è§†é¢‘å­—å¹•å’Œç¿»è¯‘çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«17å°æ—¶é«˜è´¨é‡ã€ç»è¿‡äººå·¥æ³¨é‡Šçš„æ•°æ®ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/pigeonai-org/ViDove">https://github.com/pigeonai-org/ViDove</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07306v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLMçš„ç¿»è¯‘ä»£ç†å·²ç»å®ç°äº†é«˜åº¦äººæ€§åŒ–çš„ç¿»è¯‘ç»“æœï¼Œå¹¶èƒ½å¤Ÿåœ¨å¤„ç†æ›´é•¿ã€æ›´å¤æ‚çš„ä¸Šä¸‹æ–‡æ—¶è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡ï¼Œä½†å®ƒä»¬é€šå¸¸ä»…é™äºæ–‡æœ¬è¾“å…¥ã€‚æœ¬æ–‡ä»‹ç»äº†ViDoveï¼Œä¸€ä¸ªä¸ºå¤šåª’ä½“è¾“å…¥è®¾è®¡çš„ç¿»è¯‘ä»£ç†ç³»ç»Ÿã€‚ViDoveå—åˆ°äººç±»ç¿»è¯‘å·¥ä½œæµçš„å¯å‘ï¼Œåˆ©ç”¨è§†è§‰å’Œä¸Šä¸‹æ–‡èƒŒæ™¯ä¿¡æ¯å¢å¼ºç¿»è¯‘è¿‡ç¨‹ã€‚é€šè¿‡æ•´åˆå¤šåª’ä½“å†…å­˜ç³»ç»Ÿå’Œé•¿çŸ­æ—¶è®°å¿†æ¨¡å—ï¼Œå¹¶èå…¥é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼ŒViDoveåœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°æ›´åŠ ç²¾å‡†å’Œè‡ªé€‚åº”ã€‚å› æ­¤ï¼ŒViDoveåœ¨å­—å¹•ç”Ÿæˆå’Œä¸€èˆ¬ç¿»è¯‘ä»»åŠ¡ä¸­çš„ç¿»è¯‘è´¨é‡æ˜¾è‘—æé«˜ï¼Œä¸æœ€æ–°çš„å…ˆè¿›åŸºçº¿ç›¸æ¯”ï¼ŒBLEUå¾—åˆ†æé«˜äº†28%ï¼ŒSubERæé«˜äº†15%ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†DoveBenchï¼Œä¸€ä¸ªæ–°çš„é•¿å½¢å¼è‡ªåŠ¨è§†é¢‘å­—å¹•å’Œç¿»è¯‘åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«17å°æ—¶é«˜è´¨é‡ã€ç»è¿‡äººç±»æ³¨é‡Šçš„æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based translation agents can achieve human-like translation results and handle complex contexts efficiently.</li>
<li>ViDoveç³»ç»Ÿæ˜¯ä¸€ä¸ªä¸ºå¤šåª’ä½“è¾“å…¥è®¾è®¡çš„ç¿»è¯‘ä»£ç†ï¼Œåˆ©ç”¨è§†è§‰å’Œä¸Šä¸‹æ–‡èƒŒæ™¯ä¿¡æ¯å¢å¼ºç¿»è¯‘è¿‡ç¨‹ã€‚</li>
<li>ViDoveé€šè¿‡æ•´åˆå¤šåª’ä½“å†…å­˜ç³»ç»Ÿå’Œé•¿çŸ­æ—¶è®°å¿†æ¨¡å—ï¼Œå¹¶èå…¥é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œæé«˜åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°ã€‚</li>
<li>ViDoveåœ¨å­—å¹•ç”Ÿæˆå’Œä¸€èˆ¬ç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ç¿»è¯‘è´¨é‡ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›åŸºçº¿ç›¸æ¯”ï¼ŒViDoveåœ¨BLEUå¾—åˆ†å’ŒSubERæ–¹é¢æœ‰æ‰€æ”¹è¿›ã€‚</li>
<li>æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„é•¿å½¢å¼è‡ªåŠ¨è§†é¢‘å­—å¹•å’Œç¿»è¯‘åŸºå‡†æµ‹è¯•DoveBenchï¼ŒåŒ…å«å¤§é‡é«˜è´¨é‡ã€ç»è¿‡äººç±»æ³¨é‡Šçš„æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea1772e954eda01c01d88ab97402d15d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39acad041d18f7b3f7dbec0c9be8d199.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-099fdd8e59a02e599a46352ae3a93527.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26a24c827328e6e1a751f471e80bfc4f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Squeeze-the-Soaked-Sponge-Efficient-Off-policy-Reinforcement-Finetuning-for-Large-Language-Model"><a href="#Squeeze-the-Soaked-Sponge-Efficient-Off-policy-Reinforcement-Finetuning-for-Large-Language-Model" class="headerlink" title="Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning   for Large Language Model"></a>Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning   for Large Language Model</h2><p><strong>Authors:Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao</strong></p>
<p>Reinforcement Learning (RL) has demonstrated its potential to improve the reasoning ability of Large Language Models (LLMs). One major limitation of most existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL in nature, i.e., data generated during the past learning process is not fully utilized. This inevitably comes at a significant cost of compute and time, posing a stringent bottleneck on continuing economic and efficient scaling. To this end, we launch the renaissance of off-policy RL and propose Reincarnating Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix consists of three major components: (1) Mix-policy proximal policy gradient with an increased Update-To-Data (UTD) ratio for efficient training; (2) KL-Convex policy constraint to balance the trade-off between stability and flexibility; (3) Policy reincarnation to achieve a seamless transition from efficient early-stage learning to steady asymptotic improvement. In our experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with 0.079M response rollouts, 350 training steps and achieves 63.27%&#x2F;64.39% (for 7B model) with 0.007M&#x2F;0.011M response rollouts, 50&#x2F;75 training steps, on five math reasoning benchmarks (i.e., AIMEâ€™24, AMCâ€™23, Minerva, OlympiadBench, and MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level performance with an over 30x to 450x reduction in training cost in terms of rollout data volume. In addition, we reveal insightful findings via multifaceted analysis, including the implicit preference for shorter responses due to the Whipping Effect of off-policy discrepancy, the collapse mode of self-reflection behavior under the presence of severe off-policyness, etc. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æ˜¾ç¤ºå‡ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚å¤§å¤šæ•°ç°æœ‰å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ–¹æ³•çš„ä¸€ä¸ªä¸»è¦å±€é™æ€§åœ¨äºå®ƒä»¬æœ¬è´¨ä¸Šæ˜¯åŸºäºæœ‰ç­–ç•¥å¼ºåŒ–å­¦ä¹ ï¼Œå³è¿‡å»å­¦ä¹ è¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ•°æ®æ²¡æœ‰å¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚è¿™ä¸å¯é¿å…åœ°éœ€è¦å·¨å¤§çš„è®¡ç®—å’Œæ—¶é—´çš„æŠ•å…¥ï¼Œæˆä¸ºç»æµé«˜æ•ˆæ‰©å±•çš„ä¸¥æ ¼ç“¶é¢ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº†ç¦»çº¿ç­–ç•¥å¼ºåŒ–çš„å¤å…´ï¼Œå¹¶æå‡ºäº†å†ç”Ÿçš„æ··åˆç­–ç•¥è¿‘ç«¯ç­–ç•¥æ¢¯åº¦ï¼ˆReMixï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œä½¿åƒPPOå’ŒGRPOè¿™æ ·çš„æœ‰ç­–ç•¥RFTæ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨ç¦»çº¿æ•°æ®ã€‚ReMixç”±ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†æ„æˆï¼šï¼ˆ1ï¼‰æ··åˆç­–ç•¥è¿‘ç«¯ç­–ç•¥æ¢¯åº¦ï¼Œé€šè¿‡å¢åŠ æ›´æ–°åˆ°æ•°æ®ï¼ˆUTDï¼‰çš„æ¯”ç‡ä»¥å®ç°é«˜æ•ˆè®­ç»ƒï¼›ï¼ˆ2ï¼‰KL-å‡¸ç­–ç•¥çº¦æŸä»¥å¹³è¡¡ç¨³å®šæ€§å’Œçµæ´»æ€§çš„æƒè¡¡ï¼›ï¼ˆ3ï¼‰ç­–ç•¥å†ç”Ÿä»¥å®ç°ä»é«˜æ•ˆæ—©æœŸå­¦ä¹ åˆ°ç¨³å®šæ¸è¿›æ”¹è¿›çš„æ— ç¼è¿‡æ¸¡ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬åœ¨PPOã€GRPOå’ŒåŸºäºæ¨¡å‹çš„1.5Bã€7Bä¸Šè®­ç»ƒäº†ä¸€ç³»åˆ—ReMixæ¨¡å‹ã€‚ReMixåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå³AIMEâ€™24ã€AMCâ€™23ã€Minervaã€OlympiadBenchå’ŒMATH500ï¼‰ä¸Šï¼Œä½¿ç”¨è¾ƒå°‘çš„å“åº”æ»šåŠ¨æ•°æ®ï¼ˆå¯¹äº1.5Bæ¨¡å‹ä¸ºå¹³å‡Pass@1å‡†ç¡®ç‡ä¸º52.1%ï¼Œä½¿ç”¨0.079Må“åº”æ»šåŠ¨æ•°æ®å’Œ350ä¸ªè®­ç»ƒæ­¥éª¤ï¼‰ï¼Œå¯¹äº7Bæ¨¡å‹ä¸º63.27%&#x2F;64.39%ï¼Œä½¿ç”¨è¾ƒå°‘çš„å“åº”æ»šåŠ¨æ•°æ®ï¼ˆåˆ†åˆ«ä¸º0.007Må’Œ0.011Må“åº”æ»šåŠ¨æ•°æ®ä»¥åŠç›¸åº”çš„è®­ç»ƒæ­¥éª¤ï¼‰ã€‚ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼ŒReMixå±•ç°äº†æœ€ä½³æ°´å¹³çš„æ€§èƒ½ï¼Œåœ¨æ»šåŠ¨æ•°æ®é‡æ–¹é¢å®ç°äº†é«˜è¾¾å‡ åå€çš„åŸ¹è®­æˆæœ¬ç¼©å‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å¤šæ–¹é¢çš„åˆ†ææ­ç¤ºäº†æ·±å…¥çš„è§è§£ï¼ŒåŒ…æ‹¬ç”±äºç¦»çº¿å·®å¼‚çš„é­æ‰“æ•ˆåº”å¯¼è‡´çš„å¯¹è¾ƒçŸ­å›ç­”çš„éšæ€§åå¥½ï¼Œåœ¨ä¸¥é‡ç¦»çº¿çŠ¶æ€ä¸‹è‡ªæˆ‘åæ€è¡Œä¸ºçš„å´©æºƒæ¨¡å¼ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06892v2">PDF</a> Preliminary version, v2, added more details and corrected some minor   mistakes. Project page: <a target="_blank" rel="noopener" href="https://anitaleungxx.github.io/ReMix">https://anitaleungxx.github.io/ReMix</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ–¹æ³•çš„ä¸€ä¸ªä¸»è¦å±€é™åœ¨äºå®ƒä»¬æœ¬è´¨ä¸Šæ˜¯åŸºäºç­–ç•¥çš„ï¼Œå¯¼è‡´è®¡ç®—å’Œæ—¶é—´æˆæœ¬é«˜æ˜‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆç­–ç•¥çš„è¿‘ç«¯ç­–ç•¥æ¢¯åº¦ï¼ˆReMixï¼‰ï¼Œä½¿åŸºäºç­–ç•¥çš„RFTæ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨éç­–ç•¥æ•°æ®ã€‚ReMixåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€å¹³è¡¡ç¨³å®šæ€§å’Œçµæ´»æ€§ä»¥åŠå®ç°æ—©æœŸå­¦ä¹ å’Œæ¸è¿›æ”¹å–„çš„å¹³ç¨³è¿‡æ¸¡ã€‚åœ¨å®éªŒä¸­ï¼ŒReMixåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¼˜å¼‚æˆç»©ï¼Œç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼Œå¤§å¹…é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹å¤šä¸ªæ–¹é¢çš„åˆ†æï¼Œæ­ç¤ºäº†ä¸€äº›æœ‰è¶£çš„å‘ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å¾®è°ƒæ–¹æ³•å­˜åœ¨è®¡ç®—å’Œæ—¶é—´æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>ReMixæ–¹æ³•èƒ½ä½¿åŸºäºç­–ç•¥çš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•åˆ©ç”¨éç­–ç•¥æ•°æ®ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ReMixåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šæé«˜è®­ç»ƒæ•ˆç‡ã€å¹³è¡¡ç¨³å®šæ€§å’Œçµæ´»æ€§ã€å®ç°æ—©æœŸå­¦ä¹ ä¸æ¸è¿›æ”¹å–„çš„å¹³ç¨³è¿‡æ¸¡ã€‚</li>
<li>ReMixåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>ReMixç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ¨¡å‹å¤§å¹…é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7f4df69c47e974c39f80f8be36a9803.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab707d92dae445c8b4c6f8ae21011878.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Latent-Reasoning"><a href="#A-Survey-on-Latent-Reasoning" class="headerlink" title="A Survey on Latent Reasoning"></a>A Survey on Latent Reasoning</h2><p><strong>Authors:Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, Jason Eshraghian</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the modelâ€™s expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the modelâ€™s continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: <a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/LatentCoT-Horizon/">https://github.com/multimodal-art-projection/LatentCoT-Horizon/</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å¼•å¯¼ä¸‹ï¼Œé€šè¿‡å£å¤´è¡¨è¾¾ä¸­é—´æ­¥éª¤ã€‚è™½ç„¶æ€ç»´é“¾æé«˜äº†å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œä½†å®ƒå¯¹è‡ªç„¶è¯­è¨€æ¨ç†çš„ä¾èµ–é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾å¸¦å®½ã€‚æ½œåœ¨æ¨ç†é€šè¿‡å®Œå…¨åˆ©ç”¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€è¿›è¡Œå¤šæ­¥æ¨ç†æ¥è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæ¶ˆé™¤äº†å¯¹æ ‡è®°çº§ç›‘ç£çš„ä¾èµ–ã€‚ä¸ºäº†æ¨åŠ¨æ½œåœ¨æ¨ç†ç ”ç©¶çš„å‘å±•ï¼Œè¿™ç¯‡ç»¼è¿°æä¾›äº†å¯¹æ–°å…´æ½œåœ¨æ¨ç†é¢†åŸŸçš„å…¨é¢æ¦‚è¿°ã€‚æˆ‘ä»¬é¦–å…ˆç ”ç©¶ç¥ç»ç½‘ç»œå±‚ä½œä¸ºæ¨ç†è®¡ç®—åŸºç¡€çš„ä½œç”¨ï¼Œå¹¶å¼ºè°ƒåˆ†å±‚è¡¨ç¤ºå¦‚ä½•æ”¯æŒå¤æ‚è½¬æ¢ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¿€æ´»çš„é€’å½’ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠå¾®è°ƒç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å‹ç¼©æˆ–å†…åŒ–æ˜¾æ€§æ¨ç†ç—•è¿¹ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†é€šè¿‡æ©ç æ‰©æ•£æ¨¡å‹å®ç°æ— é™æ·±åº¦æ½œåœ¨æ¨ç†ç­‰å…ˆè¿›èŒƒå¼ï¼Œè¿™äº›èŒƒå¼å¯å®ç°å…¨å±€ä¸€è‡´ä¸”å¯é€†çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ç»Ÿä¸€è¿™äº›è§‚ç‚¹ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¾„æ¸…æ½œåœ¨æ¨ç†çš„æ¦‚å¿µæ ¼å±€ï¼Œå¹¶ä¸ºå‰æ²¿çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®¤çŸ¥ç ”ç©¶æŒ‡æ˜æœªæ¥æ–¹å‘ã€‚ç›¸å…³çš„GitHubä»“åº“å¯åœ¨æ­¤å¤„æ‰¾åˆ°æœ€æ–°è®ºæ–‡å’Œä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/LatentCoT-Horizon/">https://github.com/multimodal-art-projection/LatentCoT-Horizon/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06203v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æ˜ç¡®çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å±•ç¤ºå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œè¯¥æ–¹å¼èƒ½å¤Ÿæè¿°ä¸­é—´æ­¥éª¤ã€‚è™½ç„¶CoTæé«˜äº†å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œä½†å®ƒå¯¹è‡ªç„¶è¯­è¨€æ¨ç†çš„ä¾èµ–é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾å¸¦å®½ã€‚æ½œåœ¨æ¨ç†é€šè¿‡å®Œå…¨åœ¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†æ¥è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæ¶ˆé™¤äº†ä»¤ç‰Œçº§åˆ«çš„ç›‘ç£ã€‚æœ¬æ–‡æä¾›äº†å¯¹æ½œåœ¨æ¨ç†è¿™ä¸€æ–°å…´é¢†åŸŸçš„å…¨é¢æ¦‚è¿°ï¼Œæ¢è®¨äº†ç¥ç»ç½‘ç»œå±‚ä½œä¸ºæ¨ç†è®¡ç®—åŸºç¡€çš„ä½œç”¨ï¼Œå¹¶å±•ç¤ºäº†å±‚æ¬¡è¡¨ç¤ºå¦‚ä½•æ”¯æŒå¤æ‚è½¬æ¢ã€‚æ–‡ç« è¿˜æ¢è®¨äº†å„ç§æ½œåœ¨æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¿€æ´»çš„å¤å‘ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠå¾®è°ƒç­–ç•¥ç­‰ï¼Œè¿™äº›ç­–ç•¥èƒ½å¤Ÿå‹ç¼©æˆ–å†…åŒ–æ˜¾å¼çš„æ¨ç†ç—•è¿¹ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†é€šè¿‡æ©ç æ‰©æ•£æ¨¡å‹å®ç°æ— é™æ·±åº¦çš„æ½œåœ¨æ¨ç†ç­‰å…ˆè¿›èŒƒå¼ï¼Œè¿™äº›èŒƒå¼èƒ½å¤Ÿå®ç°å…¨å±€ä¸€è‡´ä¸”å¯é€†çš„æ¨ç†è¿‡ç¨‹ã€‚æœ¬æ–‡æ—¨åœ¨ç»Ÿä¸€è¿™äº›è§‚ç‚¹ï¼Œæ¾„æ¸…æ½œåœ¨æ¨ç†çš„æ¦‚å¿µæ ¼å±€ï¼Œå¹¶ä¸ºå‰æ²¿çš„LLMè®¤çŸ¥ç ”ç©¶æŒ‡æ˜æœªæ¥æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>æ½œåœ¨æ¨ç†æ˜¯è§£å†³è‡ªç„¶è¯­è¨€æ¨ç†ç“¶é¢ˆçš„ä¸€ç§æ–°æ–¹æ³•ï¼Œå®ƒåœ¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†ã€‚</li>
<li>ç¥ç»ç½‘ç»œå±‚ä½œä¸ºè®¡ç®—åŸºç¡€åœ¨æ½œåœ¨æ¨ç†ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚</li>
<li>å±‚æ¬¡è¡¨ç¤ºæ”¯æŒå¤æ‚è½¬æ¢ï¼Œæ˜¯æ½œåœ¨æ¨ç†çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•åŒ…æ‹¬åŸºäºæ¿€æ´»çš„å¤å‘ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠå¾®è°ƒç­–ç•¥ç­‰ã€‚</li>
<li>å…ˆè¿›èŒƒå¼å¦‚é€šè¿‡æ©ç æ‰©æ•£æ¨¡å‹å®ç°æ— é™æ·±åº¦çš„æ½œåœ¨æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69633fab06a62cf6a9509e59a3392aa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-800810ee39ef683322f1d02ed49e3ffc.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Skywork-R1V3-Technical-Report"><a href="#Skywork-R1V3-Technical-Report" class="headerlink" title="Skywork-R1V3 Technical Report"></a>Skywork-R1V3 Technical Report</h2><p><strong>Authors:Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Jianhao Zhang, Yahui Zhou</strong></p>
<p>We introduce Skywork-R1V3, an advanced, open-source vision-language model (VLM) that pioneers a new approach to visual reasoning. Its key innovation lies in effectively transferring reasoning skills from text-only Large Language Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily stems from our elaborate post-training RL framework, which effectively activates and enhances the modelâ€™s reasoning ability, without the need for additional continue pre-training. Through this framework, we further uncover the fundamental role of the connector module in achieving robust cross-modal alignment for multimodal reasoning models. In addition, we introduce a unique indicator of reasoning capability, the entropy of critical reasoning tokens, which has proven highly effective for checkpoint selection during RL training. Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%. This performance matches entry-level human capabilities. Remarkably, our RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs. The implementation successfully transfers mathematical reasoning to other subject-related reasoning tasks. We also include an analysis of curriculum learning and reinforcement finetuning strategies, along with a broader discussion on multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Skywork-R1V3ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒåœ¨è§†è§‰æ¨ç†æ–¹é¢å¼€åˆ›äº†æ–°çš„æ–¹æ³•ã€‚å…¶ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºæœ‰æ•ˆåœ°å°†ä»çº¯æ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚Skywork-R1V3çš„å‡ºè‰²æ€§èƒ½ä¸»è¦æºäºæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆæ¿€æ´»å¹¶å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€é¢å¤–çš„ç»§ç»­é¢„è®­ç»ƒã€‚é€šè¿‡è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°äº†è¿æ¥å™¨æ¨¡å—åœ¨å®ç°ç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ä¸­çš„åŸºç¡€ä½œç”¨ï¼Œè¿™å¯¹äºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›æŒ‡æ ‡â€”â€”å…³é”®æ¨ç†æ ‡è®°çš„ç†µï¼Œè¿™åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ£€æŸ¥ç‚¹é€‰æ‹©ä¸­å·²è¢«è¯æ˜æ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚Skywork-R1V3åœ¨MMMUä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œä»64.3%æ˜¾è‘—æé«˜è‡³76.0%ï¼Œä¸äººç±»å…¥é—¨çº§èƒ½åŠ›ç›¸åŒ¹é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹æ³•ç”šè‡³ä½¿38Bå‚æ•°æ¨¡å‹èƒ½å¤Ÿä¸é¡¶çº§é—­æºVLMç›¸åŒ¹æ•Œã€‚æˆ‘ä»¬çš„å®ç°æˆåŠŸåœ°å°†æ•°å­¦æ¨ç†è½¬ç§»åˆ°å…¶ä»–ç›¸å…³æ¨ç†ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬è¿˜å¯¹è¯¾ç¨‹å­¦ä¹ å’Œå¼ºåŒ–å¾®è°ƒç­–ç•¥è¿›è¡Œäº†åˆ†æï¼Œå¹¶å¯¹å¤šæ¨¡æ€æ¨ç†è¿›è¡Œäº†æ›´å¹¿æ³›çš„è®¨è®ºã€‚Skywork-R1V3åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­å®ç°äº†é‡å¤§çªç ´ï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä½œä¸ºæ¨åŠ¨å¼€æºVLMèƒ½åŠ›å‘å±•çš„å¼ºå¤§å¼•æ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06167v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Skywork-R1V3æ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå¼€åˆ›äº†ä¸€ç§æ–°çš„è§†è§‰æ¨ç†æ–¹æ³•ã€‚è¯¥æ¨¡å‹çš„å…³é”®åˆ›æ–°åœ¨äºå°†çº¯æ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æœ‰æ•ˆåœ°è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚å…¶å¼ºå¤§çš„æ€§èƒ½ä¸»è¦æ¥è‡ªäºç²¾ç»†çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶è®­ç»ƒåæµç¨‹ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ¿€æ´»å¹¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„é¢„è®­ç»ƒã€‚Skywork-R1V3åœ¨MMMUä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå®ç°äº†ä»64.3%åˆ°76.0%çš„æ˜¾è‘—æ”¹è¿›ï¼Œä¸äººç±»åˆçº§æ°´å¹³ç›¸åŒ¹é…ã€‚æ­¤æˆæœå±•ç°äº†å¼ºåŒ–å­¦ä¹ åœ¨æ¨åŠ¨å¼€æºVLMèƒ½åŠ›ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Skywork-R1V3æ˜¯ä¸€æ¬¾åˆ›æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å°†æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨ç²¾ç»†çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒåä¼˜åŒ–ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿æ¥å™¨æ¨¡å—åœ¨è¾¾æˆç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªè¡¡é‡æ¨ç†èƒ½åŠ›çš„ç‹¬ç‰¹æŒ‡æ ‡â€”â€”å…³é”®æ¨ç†ä»¤ç‰Œçš„ç†µå€¼ï¼Œè¿™åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¯¹è®­ç»ƒæ£€æŸ¥ç‚¹çš„é€‰æ‹©éå¸¸æœ‰æ•ˆã€‚</li>
<li>Skywork-R1V3åœ¨MMMUä¸Šçš„è¡¨ç°è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
<li>æ¨¡å‹æˆåŠŸåœ°å°†æ•°å­¦æ¨ç†è½¬ç§»åˆ°å…¶ä»–ç›¸å…³ç§‘ç›®æ¨ç†ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f68b8782ffae6493c9eb36bbed2ce46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6828544e25f077d8f088b10a12f9b2dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2da3aa29206d112763a07ee22e77fa36.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Beyond-Overcorrection-Evaluating-Diversity-in-T2I-Models-with-DivBench"><a href="#Beyond-Overcorrection-Evaluating-Diversity-in-T2I-Models-with-DivBench" class="headerlink" title="Beyond Overcorrection: Evaluating Diversity in T2I Models with DivBench"></a>Beyond Overcorrection: Evaluating Diversity in T2I Models with DivBench</h2><p><strong>Authors:Felix Friedrich, Thiemo Ganesha Welsch, Manuel Brack, Patrick Schramowski, Kristian Kersting</strong></p>
<p>Current diversification strategies for text-to-image (T2I) models often ignore contextual appropriateness, leading to over-diversification where demographic attributes are modified even when explicitly specified in prompts. This paper introduces DIVBENCH, a benchmark and evaluation framework for measuring both under- and over-diversification in T2I generation. Through systematic evaluation of state-of-the-art T2I models, we find that while most models exhibit limited diversity, many diversification approaches overcorrect by inappropriately altering contextually-specified attributes. We demonstrate that context-aware methods, particularly LLM-guided FairDiffusion and prompt rewriting, can already effectively address under-diversity while avoiding over-diversification, achieving a better balance between representation and semantic fidelity. </p>
<blockquote>
<p>å½“å‰æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„å¤šæ ·åŒ–ç­–ç•¥å¾€å¾€å¿½ç•¥äº†ä¸Šä¸‹æ–‡æ°å½“æ€§ï¼Œå¯¼è‡´è¿‡åº¦å¤šæ ·åŒ–ï¼Œå³ä½¿æç¤ºä¸­æ˜ç¡®æŒ‡å®šäº†äººå£ç»Ÿè®¡ç‰¹å¾ä¹Ÿä¼šè¢«ä¿®æ”¹ã€‚æœ¬æ–‡ä»‹ç»äº†DIVBENCHï¼Œä¸€ä¸ªç”¨äºæµ‹é‡T2Iç”Ÿæˆä¸­ä¸è¶³å’Œè¿‡åº¦å¤šæ ·åŒ–çš„ç¨‹åº¦çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„T2Iæ¨¡å‹è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°æ¨¡å‹è¡¨ç°å‡ºæœ‰é™çš„å¤šæ ·æ€§ï¼Œè®¸å¤šå¤šæ ·åŒ–æ–¹æ³•è¿‡åº¦çº æ­£äº†ä¸Šä¸‹æ–‡æŒ‡å®šçš„å±æ€§ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯LLMå¼•å¯¼çš„FairDiffusionå’Œæç¤ºé‡å†™ï¼Œå·²ç»å¯ä»¥æœ‰æ•ˆè§£å†³å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼ŒåŒæ—¶é¿å…è¿‡åº¦å¤šæ ·åŒ–ï¼Œåœ¨è¡¨ç¤ºå’Œè¯­ä¹‰ä¿çœŸåº¦ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03015v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†T2Iæ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶é¢ä¸´çš„é—®é¢˜ï¼Œå³å½“å‰çš„å¤šæ ·åŒ–ç­–ç•¥å¿½ç•¥äº†ä¸Šä¸‹æ–‡é€‚å½“æ€§ï¼Œå¯¼è‡´è¿‡åº¦å¤šæ ·åŒ–ï¼Œå³ä½¿æç¤ºä¸­æ˜ç¡®æŒ‡å®šäº†äººå£ç»Ÿè®¡ç‰¹å¾ä¹Ÿä¼šè¢«ä¿®æ”¹ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†DIVBENCHï¼Œä¸€ä¸ªç”¨äºè¡¡é‡T2Iç”Ÿæˆä¸­è¿‡åº¦å’Œä¸è¶³çš„å¤šæ ·åŒ–é—®é¢˜çš„åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡å¯¹å…ˆè¿›çš„T2Iæ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°å¤§å¤šæ•°æ¨¡å‹è¡¨ç°å‡ºæœ‰é™çš„å¤šæ ·æ€§ï¼Œè®¸å¤šå¤šæ ·åŒ–æ–¹æ³•ä¼šè¿‡åº¦çº æ­£ä¸Šä¸‹æ–‡æŒ‡å®šçš„å±æ€§ã€‚é€šè¿‡å±•ç¤ºä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯LLMå¼•å¯¼çš„FairDiffusionå’Œæç¤ºé‡å†™ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³ä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ï¼ŒåŒæ—¶é¿å…è¿‡åº¦å¤šæ ·åŒ–ï¼Œå®ç°ä»£è¡¨æ€§åŠè¯­ä¹‰å¿ å®åº¦çš„è‰¯å¥½å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ¨¡å‹çš„ç°æœ‰å¤šæ ·åŒ–ç­–ç•¥å¯èƒ½å¿½è§†ä¸Šä¸‹æ–‡é€‚å½“æ€§ã€‚</li>
<li>DIVBENCHæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°T2Iæ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶é¢ä¸´çš„è¿‡åº¦å’Œä¸è¶³å¤šæ ·åŒ–é—®é¢˜çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚</li>
<li>å¤§å¤šæ•°T2Iæ¨¡å‹è¡¨ç°å‡ºæœ‰é™çš„å¤šæ ·æ€§ã€‚</li>
<li>ä¸€äº›å¤šæ ·åŒ–æ–¹æ³•å¯èƒ½ä¼šè¿‡åº¦çº æ­£ä¸Šä¸‹æ–‡æŒ‡å®šçš„å±æ€§ã€‚</li>
<li>ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹æ³•ï¼ˆå¦‚LLMå¼•å¯¼çš„FairDiffusionå’Œæç¤ºé‡å†™ï¼‰å¯ä»¥æœ‰æ•ˆè§£å†³ä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>è¿™äº›æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒè¯­ä¹‰å¿ å®åº¦çš„åŒæ—¶å®ç°æ›´å¥½çš„ä»£è¡¨æ€§å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9175c823adcd67eadc3e44a1bb52005.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74dfdfb235ff03eccbfc2f74bf5c3a49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98d0aee653b801cc3cf298d10564e9bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ad0bb33568af291ca1b1df7eb185fa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3148107004b8e7b80a643638797b284.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-761089c4b6a6698f306d441b68c0664e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  Multi-Granular Spatio-Temporal Token Merging for Training-Free   Acceleration of Video LLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-11/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ce9b6a1ee4b3acba1f4aa0e226a0905e.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-11  Go to Zero Towards Zero-shot Motion Generation with Million-scale Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
