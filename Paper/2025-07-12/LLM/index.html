<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  Multi-Granular Spatio-Temporal Token Merging for Training-Free   Acceleration of Video LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-761089c4b6a6698f306d441b68c0664e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-12-æ›´æ–°"><a href="#2025-07-12-æ›´æ–°" class="headerlink" title="2025-07-12 æ›´æ–°"></a>2025-07-12 æ›´æ–°</h1><h2 id="Multi-Granular-Spatio-Temporal-Token-Merging-for-Training-Free-Acceleration-of-Video-LLMs"><a href="#Multi-Granular-Spatio-Temporal-Token-Merging-for-Training-Free-Acceleration-of-Video-LLMs" class="headerlink" title="Multi-Granular Spatio-Temporal Token Merging for Training-Free   Acceleration of Video LLMs"></a>Multi-Granular Spatio-Temporal Token Merging for Training-Free   Acceleration of Video LLMs</h2><p><strong>Authors:Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim</strong></p>
<p>Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose a training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves a 2$\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and a 3$\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is query-agnostic, allowing KV cache reuse across different questions for the same video. The project page is available at <a target="_blank" rel="noopener" href="https://www.jshyun.me/projects/sttm">https://www.jshyun.me/projects/sttm</a>. </p>
<blockquote>
<p>è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åˆ©ç”¨å¤§é‡æ—¶ç©ºä»¤ç‰Œå®ç°äº†å¼ºå¤§çš„è§†é¢‘ç†è§£ï¼Œä½†éšç€ä»¤ç‰Œæ•°é‡çš„å¢åŠ ï¼Œå…¶è®¡ç®—è§„æ¨¡å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ—¶ç©ºä»¤ç‰Œåˆå¹¶æ–¹æ³•ï¼Œç§°ä¸ºSTTMã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œåˆ©ç”¨è§†é¢‘æ•°æ®ä¸­å±€éƒ¨ç©ºé—´å’Œæ—¶é—´çš„å†—ä½™æ€§ï¼Œè¿™æ˜¯ä»¥å‰çš„ç ”ç©¶ä¸­è¢«å¿½è§†çš„éƒ¨åˆ†ã€‚STTMé¦–å…ˆé€šè¿‡å››å‰æ ‘ç»“æ„çš„ç²—åˆ°ç»†æœç´¢å°†æ¯ä¸€å¸§è½¬æ¢ä¸ºå¤šç²’åº¦ç©ºé—´ä»¤ç‰Œï¼Œç„¶ååœ¨æ—¶é—´ç»´åº¦ä¸Šæ‰§è¡Œæœ‰æ–¹å‘çš„æˆå¯¹åˆå¹¶ã€‚è¿™ç§åˆ†è§£åˆå¹¶æ–¹æ³•åœ¨æ‰€æœ‰å…­ä¸ªè§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºç°æœ‰çš„ä»¤ç‰Œç¼©å‡æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨50%ä»¤ç‰Œé¢„ç®—ä¸‹ï¼ŒSTTMå®ç°äº†2å€çš„é€Ÿåº¦æå‡ï¼Œå‡†ç¡®åº¦ä»…ä¸‹é™0.5%ï¼Œåœ¨30%é¢„ç®—ä¸‹ï¼Œå®ç°äº†3å€çš„é€Ÿåº¦æå‡ï¼Œå‡†ç¡®åº¦ä»…ä¸‹é™2%ã€‚æ­¤å¤–ï¼ŒSTTMå¯¹æŸ¥è¯¢æ— æ„ŸçŸ¥ï¼Œå…è®¸é’ˆå¯¹åŒä¸€è§†é¢‘çš„å¤šä¸ªé—®é¢˜é‡å¤ä½¿ç”¨KVç¼“å­˜ã€‚é¡¹ç›®é¡µé¢å¯åœ¨[<a target="_blank" rel="noopener" href="https://www.jshyun.me/projects/sttm%E8%AE%BF%E9%97%AE%E3%80%82]">https://www.jshyun.me/projects/sttmè®¿é—®ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07990v1">PDF</a> Accepted at ICCV2025; Project page:   <a target="_blank" rel="noopener" href="https://www.jshyun.me/projects/sttm">https://www.jshyun.me/projects/sttm</a></p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åˆ©ç”¨å¤§é‡æ—¶ç©ºä»¤ç‰Œå®ç°å¼ºå¤§çš„è§†é¢‘ç†è§£ï¼Œä½†é¢ä¸´è®¡ç®—é‡éšä»¤ç‰Œæ•°é‡äºŒæ¬¡å¢é•¿çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ—¶ç©ºä»¤ç‰Œåˆå¹¶æ–¹æ³•ï¼Œåä¸ºSTTMã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯å‘æ˜è§†é¢‘æ•°æ®ä¸­å±€éƒ¨ç©ºé—´å’Œæ—¶é—´çš„å†—ä½™æ€§ï¼Œè¿™ä¸€ç‚¹åœ¨ä»¥å‰çš„ç ”ç©¶ä¸­å¸¸è¢«å¿½è§†ã€‚STTMé¦–å…ˆå°†æ¯å¸§è½¬æ¢ä¸ºå¤šç²’åº¦ç©ºé—´ä»¤ç‰Œï¼Œé€šè¿‡å››å‰æ ‘ç»“æ„çš„ç²—ç»†æœç´¢ï¼Œç„¶ååœ¨æ—¶é—´ç»´åº¦ä¸Šè¿›è¡Œæœ‰æ–¹å‘çš„æˆå¯¹åˆå¹¶ã€‚è¿™ç§åˆ†è§£åˆå¹¶æ–¹æ³•åœ¨ä¼—å¤šè§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰ä»¤ç‰Œç¼©å‡æ–¹æ³•ã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨50%ä»¤ç‰Œé¢„ç®—ä¸‹ï¼ŒSTTMå®ç°äº†2å€é€Ÿåº¦æå‡ï¼Œå‡†ç¡®åº¦ä»…ä¸‹é™0.5%ï¼›åœ¨30%é¢„ç®—ä¸‹ï¼Œå®ç°äº†3å€é€Ÿåº¦æå‡ï¼Œå‡†ç¡®åº¦ä»…ä¸‹é™2%ã€‚æ­¤å¤–ï¼ŒSTTMå¯¹æŸ¥è¯¢ä¸æ•æ„Ÿï¼Œå…è®¸è·¨ä¸åŒé—®é¢˜å¯¹åŒä¸€è§†é¢‘è¿›è¡ŒKVç¼“å­˜é‡ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åˆ©ç”¨å¤§é‡æ—¶ç©ºä»¤ç‰Œå®ç°è§†é¢‘ç†è§£ï¼Œä½†å­˜åœ¨è®¡ç®—é‡éšä»¤ç‰Œæ•°é‡äºŒæ¬¡å¢é•¿çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ—¶ç©ºä»¤ç‰Œåˆå¹¶æ–¹æ³•ï¼ˆSTTMï¼‰ï¼Œé€šè¿‡å‘æ˜è§†é¢‘æ•°æ®ä¸­å±€éƒ¨ç©ºé—´å’Œæ—¶é—´çš„å†—ä½™æ€§æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>STTMé¦–å…ˆå°†æ¯å¸§è½¬æ¢ä¸ºå¤šç²’åº¦ç©ºé—´ä»¤ç‰Œï¼Œåˆ©ç”¨å››å‰æ ‘ç»“æ„çš„ç²—ç»†æœç´¢ã€‚</li>
<li>STTMåœ¨æ—¶ç©ºç»´åº¦ä¸Šè¿›è¡Œä»¤ç‰Œåˆå¹¶ï¼Œå®ç°äº†ä¼˜äºç°æœ‰ä»¤ç‰Œç¼©å‡æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ç‰¹å®šçš„é¢„ç®—ä¸‹ï¼ŒSTTMåœ¨é€Ÿåº¦æå‡å’Œå‡†ç¡®åº¦æŸå¤±ä¹‹é—´è¾¾åˆ°äº†è‰¯å¥½çš„å¹³è¡¡ã€‚</li>
<li>STTMå¯¹æŸ¥è¯¢ä¸æ•æ„Ÿï¼Œå…è®¸è·¨ä¸åŒé—®é¢˜å¯¹åŒä¸€è§†é¢‘è¿›è¡ŒKVç¼“å­˜é‡ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-195a3b636dacd58d3c20128f851ffefb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4d887c98625ea4fdf3d8a274fad4fe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e951c4b878547f855d02d8b0648fcbdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-715783a920eb6366fc7765fb6298162f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-091b0b5bce335d6c5a5cac6808367789.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automating-Expert-Level-Medical-Reasoning-Evaluation-of-Large-Language-Models"><a href="#Automating-Expert-Level-Medical-Reasoning-Evaluation-of-Large-Language-Models" class="headerlink" title="Automating Expert-Level Medical Reasoning Evaluation of Large Language   Models"></a>Automating Expert-Level Medical Reasoning Evaluation of Large Language   Models</h2><p><strong>Authors:Shuang Zhou, Wenya Xie, Jiaxi Li, Zaifu Zhan, Meijia Song, Han Yang, Cheyenna Espinoza, Lindsay Welton, Xinnie Mai, Yanwei Jin, Zidu Xu, Yuen-Hei Chung, Yiyun Xing, Meng-Han Tsai, Emma Schaffer, Yucheng Shi, Ninghao Liu, Zirui Liu, Rui Zhang</strong></p>
<p>As large language models (LLMs) become increasingly integrated into clinical decision-making, ensuring transparent and trustworthy reasoning is essential. However, existing evaluation strategies of LLMsâ€™ medical reasoning capability either suffer from unsatisfactory assessment or poor scalability, and a rigorous benchmark remains lacking. To address this, we introduce MedThink-Bench, a benchmark designed for rigorous, explainable, and scalable assessment of LLMsâ€™ medical reasoning. MedThink-Bench comprises 500 challenging questions across ten medical domains, each annotated with expert-crafted step-by-step rationales. Building on this, we propose LLM-w-Ref, a novel evaluation framework that leverages fine-grained rationales and LLM-as-a-Judge mechanisms to assess intermediate reasoning with expert-level fidelity while maintaining scalability. Experiments show that LLM-w-Ref exhibits a strong positive correlation with expert judgments. Benchmarking twelve state-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can surpass larger proprietary counterparts (e.g., OpenAI-o3). Overall, MedThink-Bench offers a foundational tool for evaluating LLMsâ€™ medical reasoning, advancing their safe and responsible deployment in clinical practice. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸´åºŠå†³ç­–ä¸­çš„é›†æˆåº¦è¶Šæ¥è¶Šé«˜ï¼Œç¡®ä¿é€æ˜å’Œå¯ä¿¡èµ–çš„æ¨ç†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMåŒ»ç–—æ¨ç†èƒ½åŠ›è¯„ä¼°ç­–ç•¥è¦ä¹ˆè¯„ä¼°ç»“æœä¸å°½å¦‚äººæ„ï¼Œè¦ä¹ˆæ‰©å±•æ€§è¾ƒå·®ï¼Œä¸”ç¼ºä¹ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedThink-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºLLMåŒ»ç–—æ¨ç†è¿›è¡Œä¸¥æ ¼ã€å¯è§£é‡Šå’Œå¯æ‰©å±•è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ã€‚MedThink-BenchåŒ…å«500ä¸ªè·¨è¶Šåä¸ªåŒ»å­¦é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½é…æœ‰ä¸“å®¶é€æ­¥åˆ¶å®šçš„ç†ç”±ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†LLM-w-Refè¿™ä¸€æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç²¾ç»†åŒ–çš„ç†ç”±å’ŒLLMä½œä¸ºæ³•å®˜çš„æœºåˆ¶ï¼Œä»¥ä¸“å®¶çº§çš„å¿ å®åº¦è¯„ä¼°ä¸­é—´æ¨ç†ï¼ŒåŒæ—¶ä¿æŒå¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒLLM-w-Refä¸ä¸“å®¶åˆ¤æ–­ä¹‹é—´å‘ˆç°å‡ºå¼ºçƒˆæ­£ç›¸å…³ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸­å¯¹æ¯”äº†æœ€å…ˆè¿›çš„åäºŒæ¬¾LLMï¼Œæˆ‘ä»¬å‘ç°è¾ƒå°çš„æ¨¡å‹ï¼ˆä¾‹å¦‚MedGemma-27Bï¼‰å¯ä»¥è¶…è¶Šè¾ƒå¤§çš„ä¸“æœ‰æ¨¡å‹ï¼ˆä¾‹å¦‚OpenAI-o3ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼ŒMedThink-Benchä¸ºè¯„ä¼°LLMçš„åŒ»ç–—æ¨ç†èƒ½åŠ›æä¾›äº†åŸºç¡€å·¥å…·ï¼Œæ¨åŠ¨äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„å®‰å…¨å’Œè´Ÿè´£ä»»éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07988v1">PDF</a> 22 pages,6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸´åºŠå†³ç­–ä¸­çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦è‡³å…³é‡è¦ã€‚å½“å‰è¯„ä¼°ç­–ç•¥å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ä¸¥æ ¼çš„æ ‡å‡†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºMedThink-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸¥æ ¼ã€å¯è§£é‡Šå’Œå¯æ‰©å±•åœ°è¯„ä¼°LLMçš„åŒ»ç–—æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«500ä¸ªè·¨è¶Šåå¤§åŒ»å­¦é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸“å®¶é€æ­¥åˆ¶å®šçš„ç†ç”±ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†LLM-w-Refè¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨ç²¾ç»†åŒ–çš„ç†ç”±å’ŒLLM-as-a-Judgeæœºåˆ¶æ¥è¯„ä¼°ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼Œä¿æŒä¸ä¸“å®¶åˆ¤æ–­çš„ä¸€è‡´æ€§åŒæ—¶æé«˜å¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒLLM-w-Refä¸ä¸“å®¶åˆ¤æ–­å‘ˆç°æ˜¾è‘—æ­£ç›¸å…³ã€‚å¯¹12æ¬¾æœ€æ–°LLMæ¨¡å‹çš„åŸºå‡†æµ‹è¯•å‘ç°ï¼Œå°å‹æ¨¡å‹ï¼ˆå¦‚MedGemma-27Bï¼‰è¡¨ç°ä¼˜äºå¤§å‹ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚OpenAI-o3ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼ŒMedThink-Benchä¸ºè¯„ä¼°LLMçš„åŒ»ç–—æ¨ç†èƒ½åŠ›æä¾›äº†åŸºç¡€å·¥å…·ï¼Œæ¨åŠ¨äº†å…¶åœ¨ä¸´åºŠå®è·µä¸­çš„å®‰å…¨å’Œè´Ÿè´£ä»»éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸´åºŠå†³ç­–ä¸­çš„é€æ˜åº¦å’Œå¯ä¿¡åº¦è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰LLMåŒ»ç–—æ¨ç†èƒ½åŠ›è¯„ä¼°ç­–ç•¥å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ä¸¥æ ¼æ ‡å‡†ã€‚</li>
<li>MedThink-BenchåŸºå‡†æµ‹è¯•æ—¨åœ¨ä¸¥æ ¼ã€å¯è§£é‡Šå’Œå¯æ‰©å±•åœ°è¯„ä¼°LLMçš„åŒ»ç–—æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MedThink-BenchåŒ…å«500ä¸ªè·¨è¶Šåå¤§åŒ»å­¦é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸“å®¶ç†ç”±ã€‚</li>
<li>LLM-w-Refè¯„ä¼°æ¡†æ¶åˆ©ç”¨ç²¾ç»†åŒ–çš„ç†ç”±å’ŒLLMæœºåˆ¶æ¥è¯„ä¼°æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>LLM-w-Refä¸ä¸“å®¶åˆ¤æ–­å‘ˆç°æ˜¾è‘—æ­£ç›¸å…³ã€‚</li>
<li>å¯¹æœ€æ–°LLMæ¨¡å‹çš„åŸºå‡†æµ‹è¯•å‘ç°ï¼Œå°å‹æ¨¡å‹è¡¨ç°ä¼˜äºæŸäº›å¤§å‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3fa8f20d07109e92ad02f91f9fad96af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45bae99e2b559c16f1d4eea9aa94e312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-260b58b6b5afd84c260fc498a92cc817.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39be266f0f2d3a4df0de8a030941cf22.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OST-Bench-Evaluating-the-Capabilities-of-MLLMs-in-Online-Spatio-temporal-Scene-Understanding"><a href="#OST-Bench-Evaluating-the-Capabilities-of-MLLMs-in-Online-Spatio-temporal-Scene-Understanding" class="headerlink" title="OST-Bench: Evaluating the Capabilities of MLLMs in Online   Spatio-temporal Scene Understanding"></a>OST-Bench: Evaluating the Capabilities of MLLMs in Online   Spatio-temporal Scene Understanding</h2><p><strong>Authors:JingLi Lin, Chenming Zhu, Runsen Xu, Xiaohan Mao, Xihui Liu, Tai Wang, Jiangmiao Pang</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is: <a target="_blank" rel="noopener" href="https://rbler1234.github.io/OSTBench.github.io/">https://rbler1234.github.io/OSTBench.github.io/</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•åœ¨æ•´åˆè§†è§‰å’Œè¯­è¨€è¿›è¡Œå¤æ‚æ¨ç†æ–¹é¢å±•ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰çš„åŸºå‡†æµ‹è¯•éƒ½åœ¨ç¦»çº¿è®¾ç½®ä¸‹è¯„ä¼°æ¨¡å‹ï¼Œä½¿ç”¨ä¸€ç»„å›ºå®šçš„é¢„å…ˆå½•åˆ¶çš„è¾“å…¥ï¼Œä½†æˆ‘ä»¬å¼•å…¥äº†OST-BenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ˜¯ä»ä»£ç†ç§¯ææ¢ç´¢åœºæ™¯çš„è§’åº¦è®¾è®¡çš„ï¼Œæ—¨åœ¨è¯„ä¼°åœ¨çº¿æ—¶ç©ºç†è§£ã€‚åœ¨çº¿æ–¹é¢å¼ºè°ƒäº†å¤„ç†å’Œå¢é‡è·å–çš„è§‚å¯Ÿç»“æœè¿›è¡Œæ¨ç†çš„éœ€æ±‚ï¼Œè€Œæ—¶ç©ºç»„ä»¶åˆ™è¦æ±‚å°†å½“å‰çš„è§†è§‰è¾“å…¥ä¸å†å²è®°å¿†ç›¸ç»“åˆï¼Œä»¥æ”¯æŒåŠ¨æ€ç©ºé—´æ¨ç†ã€‚OST-Benchæ›´å¥½åœ°åæ˜ äº†ç°å®ä¸–ç•Œä¸­ä½“æ„Ÿæ„ŸçŸ¥çš„æŒ‘æˆ˜ã€‚å»ºç«‹åœ¨é«˜æ•ˆçš„æ•°æ®æ”¶é›†æµç¨‹ä¹‹ä¸Šï¼ŒOST-Benchç”±æ¥è‡ªScanNetã€Matterport3Då’ŒARKitScenesçš„1400ä¸ªåœºæ™¯å’Œ1ä¸‡ä¸ªé—®é¢˜ç­”æ¡ˆå¯¹ç»„æˆã€‚æˆ‘ä»¬åœ¨OST-Benchä¸Šè¯„ä¼°äº†å‡ æ¬¾é¢†å…ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶å‘ç°å®ƒä»¬åœ¨éœ€è¦å¤æ‚æ—¶ç©ºæ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ã€‚åœ¨çº¿è®¾ç½®ä¸‹ï¼Œéšç€æ¢ç´¢èŒƒå›´çš„æ‰©å¤§å’Œè®°å¿†çš„å¢é•¿ï¼Œä»–ä»¬çš„å‡†ç¡®æ€§ä¼šä¸‹é™ã€‚é€šè¿‡è¿›ä¸€æ­¥çš„å®éªŒåˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†æ¨¡å‹ä¹‹é—´çš„å¸¸è§é”™è¯¯æ¨¡å¼ï¼Œå¹¶å‘ç°åŸºäºå¤æ‚çº¿ç´¢çš„ç©ºé—´æ¨ç†éœ€æ±‚å’Œé•¿æœŸè®°å¿†æ£€ç´¢è¦æ±‚æ˜¾è‘—é™ä½æ¨¡å‹æ€§èƒ½çš„ä¸¤ä¸ªç‹¬ç«‹è½´ï¼Œè¿™çªå‡ºäº†å¿…é¡»è§£å†³çš„æ ¸å¿ƒæŒ‘æˆ˜ä»¥æé«˜åœ¨çº¿ä½“æ„Ÿæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ï¼Œæˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å‡å¯ç”¨ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯ï¼š[ç½‘ç«™é“¾æ¥]ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…ç½‘ç«™é“¾æ¥ï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07984v1">PDF</a> 28 pages, a benchmark designed to evaluate Online Spatio-Temporal   understanding from the perspective of an agent actively exploring a scene.   Project Page: <a target="_blank" rel="noopener" href="https://rbler1234.github.io/OSTBench.github.io/">https://rbler1234.github.io/OSTBench.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ–°åŸºå‡†æµ‹è¯•â€”â€”OST-Benchã€‚ä¸ä¼ ç»Ÿçš„ç¦»çº¿åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒOST-Benchæ³¨é‡åœ¨çº¿æ—¶ç©ºç†è§£çš„è¯„ä»·ï¼Œå°¤å…¶å¼ºè°ƒäº†å¤„ç†å’Œç†è§£è¿ç»­è·å¾—çš„ä¿¡æ¯å’Œåœºæ™¯çš„æ—¶ç©ºç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘ç°å½“å‰çš„ä¸»æµMLLMsåœ¨å¤„ç†å¤æ‚æ—¶ç©ºæ¨ç†ä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ï¼Œéšç€æ¢ç´¢è§†é‡çš„æ‰©å¤§å’Œè®°å¿†çš„å¢é•¿ï¼Œå…¶å‡†ç¡®æ€§ä¼šä¸‹é™ã€‚æ–‡ç« è¿˜æŒ‡å‡ºäº†æ¨¡å‹å­˜åœ¨çš„å¸¸è§é”™è¯¯æ¨¡å¼ï¼Œå¹¶å¼ºè°ƒäº†è§£å†³åœ¨çº¿å®ä½“æ¨ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ€åï¼Œè¯¥é¡¹ç›®çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¯ä¾›å…¬ä¼—è®¿é—®ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥OST-BenchåŸºå‡†æµ‹è¯•ï¼šä¸“é—¨ä¸ºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åœ¨çº¿æ—¶ç©ºç†è§£èƒ½åŠ›è€Œè®¾è®¡ã€‚</li>
<li>åœ¨çº¿æ—¶ç©ºç†è§£çš„å¼ºè°ƒï¼šæ³¨é‡å¤„ç†å’Œç†è§£è¿ç»­è·å¾—çš„ä¿¡æ¯å’Œåœºæ™¯çš„æ—¶ç©ºç‰¹å¾ã€‚</li>
<li>ç°æœ‰MLLMsçš„æŒ‘æˆ˜ï¼šåœ¨å¤„ç†å¤æ‚æ—¶ç©ºæ¨ç†ä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ï¼Œéšç€æ¢ç´¢è§†é‡çš„æ‰©å¤§å’Œè®°å¿†çš„å¢é•¿ï¼Œå‡†ç¡®æ€§ä¸‹é™ã€‚</li>
<li>æ¨¡å‹é”™è¯¯æ¨¡å¼åˆ†æï¼šå‘ç°ä¸¤ç§å¸¸è§çš„é”™è¯¯æ¨¡å¼ï¼Œåˆ†åˆ«æ˜¯å¤æ‚çš„çº¿ç´¢åŸºç¡€ç©ºé—´æ¨ç†éœ€æ±‚å’Œé•¿æœŸè®°å¿†æ£€ç´¢è¦æ±‚ã€‚</li>
<li>æ ¸å¿ƒæŒ‘æˆ˜è¯†åˆ«ï¼šä¸ºäº†æå‡åœ¨çº¿å®ä½“æ¨ç†èƒ½åŠ›ï¼Œå¿…é¡»è§£å†³ç‰¹å®šçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•çš„å…¬å¼€ï¼šä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-901b4531925f631ff6f133caf21f020d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0974b2a675b0af65b252894c17b26feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4aa1eecf2c6d9ead79d5777e9c468ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdc01210d431fff34a6e011af9938ca6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Defending-Against-Prompt-Injection-With-a-Few-DefensiveTokens"><a href="#Defending-Against-Prompt-Injection-With-a-Few-DefensiveTokens" class="headerlink" title="Defending Against Prompt Injection With a Few DefensiveTokens"></a>Defending Against Prompt Injection With a Few DefensiveTokens</h2><p><strong>Authors:Sizhe Chen, Yizhu Wang, Nicholas Carlini, Chawin Sitawarin, David Wagner</strong></p>
<p>When large language model (LLM) systems interact with external data to perform complex tasks, a new attack, namely prompt injection, becomes a significant threat. By injecting instructions into the data accessed by the system, the attacker is able to override the initial user task with an arbitrary task directed by the attacker. To secure the system, test-time defenses, e.g., defensive prompting, have been proposed for system developers to attain security only when needed in a flexible manner. However, they are much less effective than training-time defenses that change the model parameters. Motivated by this, we propose DefensiveToken, a test-time defense with prompt injection robustness comparable to training-time alternatives. DefensiveTokens are newly inserted as special tokens, whose embeddings are optimized for security. In security-sensitive cases, system developers can append a few DefensiveTokens before the LLM input to achieve security with a minimal utility drop. In scenarios where security is less of a concern, developers can simply skip DefensiveTokens; the LLM system remains the same as there is no defense, generating high-quality responses. Thus, DefensiveTokens, if released alongside the model, allow a flexible switch between the state-of-the-art (SOTA) utility and almost-SOTA security at test time. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Sizhe-Chen/DefensiveToken">https://github.com/Sizhe-Chen/DefensiveToken</a>. </p>
<blockquote>
<p>å½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿä¸å¤–éƒ¨æ•°æ®äº¤äº’ä»¥æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶ï¼Œä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•ï¼Œå³æç¤ºæ³¨å…¥ï¼Œæˆä¸ºé‡å¤§å¨èƒã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡å‘ç³»ç»Ÿè®¿é—®çš„æ•°æ®æ³¨å…¥æŒ‡ä»¤ï¼Œæ¥è¦†ç›–åˆå§‹ç”¨æˆ·ä»»åŠ¡ï¼Œæ‰§è¡Œæ”»å‡»è€…æŒ‡å®šçš„ä»»æ„ä»»åŠ¡ã€‚ä¸ºäº†ç³»ç»Ÿå®‰å…¨ï¼Œç³»ç»Ÿå¼€å‘è€…æå‡ºäº†æµ‹è¯•æ—¶é˜²å¾¡æªæ–½ï¼Œä¾‹å¦‚é˜²å¾¡æ€§æç¤ºï¼Œä»¥ä¸€ç§çµæ´»çš„æ–¹å¼åœ¨éœ€è¦æ—¶æ‰å®ç°å®‰å…¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬è¿œä¸å¦‚è®­ç»ƒæ—¶æ”¹å˜æ¨¡å‹å‚æ•°çš„é˜²å¾¡æªæ–½æœ‰æ•ˆã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†DefensiveTokenï¼Œè¿™æ˜¯ä¸€ç§æµ‹è¯•æ—¶é˜²å¾¡æªæ–½ï¼Œå…·æœ‰ä¸è®­ç»ƒæ—¶æ›¿ä»£æ–¹æ¡ˆç›¸å½“çš„æç¤ºæ³¨å…¥ç¨³å¥æ€§ã€‚DefensiveTokenä½œä¸ºæ–°æ’å…¥çš„ç‰¹æ®Šä»¤ç‰Œï¼Œå…¶åµŒå…¥ä¼˜åŒ–ä»¥æé«˜å®‰å…¨æ€§ã€‚åœ¨å®‰å…¨æ•æ„Ÿçš„æƒ…å†µä¸‹ï¼Œç³»ç»Ÿå¼€å‘è€…å¯ä»¥åœ¨LLMè¾“å…¥ä¹‹å‰é™„åŠ å‡ ä¸ªDefensiveTokenï¼Œä»¥æœ€å°çš„æ•ˆç”¨æŸå¤±å®ç°å®‰å…¨ã€‚åœ¨å®‰å…¨ä¸å¤ªé‡è¦çš„åœºæ™¯ä¸­ï¼Œå¼€å‘è€…å¯ä»¥ç®€å•åœ°è·³è¿‡DefensiveTokenï¼›LLMç³»ç»Ÿä¿æŒåŸæ ·ï¼Œæ— éœ€é˜²å¾¡æªæ–½ï¼Œç”Ÿæˆé«˜è´¨é‡å“åº”ã€‚å› æ­¤ï¼Œå¦‚æœä¸æ¨¡å‹ä¸€èµ·å‘å¸ƒDefensiveTokenï¼Œåˆ™å…è®¸åœ¨æµ‹è¯•æ—¶åœ¨æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰æ•ˆç”¨å’Œæ¥è¿‘æœ€æ–°æŠ€æœ¯çš„å®‰å…¨æ€§ä¹‹é—´è¿›è¡Œçµæ´»åˆ‡æ¢ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Sizhe-Chen/DefensiveToken%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Sizhe-Chen/DefensiveTokenæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07974v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´ä¸€ç§æ–°çš„å¨èƒâ€”â€”æŒ‡ä»¤æ³¨å…¥æ”»å‡»ã€‚æ”»å‡»è€…é€šè¿‡æ³¨å…¥æ•°æ®æŒ‡ä»¤æ¥è¦†ç›–ç”¨æˆ·åŸå§‹ä»»åŠ¡ï¼Œæ‰§è¡Œæ”»å‡»è€…è®¾å®šçš„ä»»åŠ¡ã€‚ç›®å‰æå‡ºçš„æµ‹è¯•æ—¶é˜²å¾¡ç­–ç•¥ï¼Œå¦‚é˜²å¾¡æ€§æç¤ºï¼Œåœ¨ç³»ç»Ÿå¼€å‘è€…éœ€è¦æ—¶æ‰æä¾›å®‰å…¨æ€§ï¼Œä½†å…¶æ•ˆæœè¿œä¸å¦‚è®­ç»ƒæ—¶çš„é˜²å¾¡ç­–ç•¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DefensiveTokenï¼Œè¿™æ˜¯ä¸€ç§æµ‹è¯•æ—¶çš„é˜²å¾¡ç­–ç•¥ï¼Œå…·æœ‰ä¸è®­ç»ƒæ—¶ç­–ç•¥ç›¸å½“çš„æŒ‡ä»¤æ³¨å…¥ç¨³å¥æ€§ã€‚DefensiveTokenä½œä¸ºç‰¹æ®Šä»¤ç‰Œè¢«æ’å…¥ï¼Œå…¶åµŒå…¥ä¼˜åŒ–ä»¥æé«˜å®‰å…¨æ€§ã€‚å¼€å‘è€…å¯ä»¥åœ¨å®‰å…¨æ•æ„Ÿçš„æƒ…å†µä¸‹ï¼Œåœ¨LLMè¾“å…¥å‰æ·»åŠ å°‘é‡DefensiveTokenä»¥å®ç°å®‰å…¨æ€§ï¼Œå¹¶ä»¥æœ€å°çš„æ•ˆç”¨æŸå¤±è¾¾åˆ°å®‰å…¨ç›®çš„ã€‚åœ¨ä¸å…³æ³¨å®‰å…¨æ€§çš„åœºæ™¯ä¸­ï¼Œå¼€å‘è€…å¯ä»¥é€‰æ‹©è·³è¿‡DefensiveTokenï¼Œä¸å½±å“LLMç³»ç»Ÿçš„æ­£å¸¸è¿è¡Œã€‚å› æ­¤ï¼ŒDefensiveTokenä¸æ¨¡å‹ä¸€èµ·å‘å¸ƒï¼Œå¯åœ¨æµ‹è¯•æ—¶çµæ´»åˆ‡æ¢æœ€é«˜æ•ˆç”¨å’Œè¿‘ä¼¼æœ€é«˜å®‰å…¨æ€§ä¹‹é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´æŒ‡ä»¤æ³¨å…¥æ”»å‡»çš„é£é™©ã€‚</li>
<li>æµ‹è¯•æ—¶çš„é˜²å¾¡ç­–ç•¥ï¼ˆå¦‚é˜²å¾¡æ€§æç¤ºï¼‰è™½ç„¶çµæ´»ä½†æ•ˆæœæœ‰é™ã€‚</li>
<li>DefensiveTokenæ˜¯ä¸€ç§æ–°çš„æµ‹è¯•æ—¶é˜²å¾¡ç­–ç•¥ï¼Œèƒ½æœ‰æ•ˆæé«˜ç³»ç»Ÿå¯¹æŒ‡ä»¤æ³¨å…¥æ”»å‡»çš„ç¨³å¥æ€§ã€‚</li>
<li>DefensiveTokené€šè¿‡æ’å…¥ç‰¹æ®Šä»¤ç‰Œæ¥å®ç°å®‰å…¨æ€§ï¼Œè¿™äº›ä»¤ç‰Œçš„åµŒå…¥ç»è¿‡ä¼˜åŒ–ä»¥æé«˜å®‰å…¨æ€§ã€‚</li>
<li>å¼€å‘è€…å¯ä»¥æ ¹æ®éœ€è¦çµæ´»ä½¿ç”¨DefensiveTokenï¼Œä»¥å®ç°å®‰å…¨æ€§å’Œæ•ˆç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>DefensiveTokençš„æ•ˆç”¨ä¸æ¨¡å‹ä¸€èµ·å‘å¸ƒï¼Œæ–¹ä¾¿å¼€å‘è€…åœ¨æµ‹è¯•æ—¶è¿›è¡Œçµæ´»è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-884889e3b08f4b36941c109cd8532fb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15395be229e55a07ed9eb921ec5b505d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d559945147760192e65abfdda5d7337b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f467026826a69dc4ea4fb49b4bfa545.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MIRIX-Multi-Agent-Memory-System-for-LLM-Based-Agents"><a href="#MIRIX-Multi-Agent-Memory-System-for-LLM-Based-Agents" class="headerlink" title="MIRIX: Multi-Agent Memory System for LLM-Based Agents"></a>MIRIX: Multi-Agent Memory System for LLM-Based Agents</h2><p><strong>Authors:Yu Wang, Xi Chen</strong></p>
<p>Although memory capabilities of AI agents are gaining increasing attention, existing solutions remain fundamentally limited. Most rely on flat, narrowly scoped memory components, constraining their ability to personalize, abstract, and reliably recall user-specific information over time. To this end, we introduce MIRIX, a modular, multi-agent memory system that redefines the future of AI memory by solving the fieldâ€™s most critical challenge: enabling language models to truly remember. Unlike prior approaches, MIRIX transcends text to embrace rich visual and multimodal experiences, making memory genuinely useful in real-world scenarios. MIRIX consists of six distinct, carefully structured memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and Knowledge Vault, coupled with a multi-agent framework that dynamically controls and coordinates updates and retrieval. This design enables agents to persist, reason over, and accurately retrieve diverse, long-term user data at scale. We validate MIRIX in two demanding settings. First, on ScreenshotVQA, a challenging multimodal benchmark comprising nearly 20,000 high-resolution computer screenshots per sequence, requiring deep contextual understanding and where no existing memory systems can be applied, MIRIX achieves 35% higher accuracy than the RAG baseline while reducing storage requirements by 99.9%. Second, on LOCOMO, a long-form conversation benchmark with single-modal textual input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing existing baselines. These results show that MIRIX sets a new performance standard for memory-augmented LLM agents. To allow users to experience our memory system, we provide a packaged application powered by MIRIX. It monitors the screen in real time, builds a personalized memory base, and offers intuitive visualization and secure local storage to ensure privacy. </p>
<blockquote>
<p>å°½ç®¡äººå·¥æ™ºèƒ½ä¸»ä½“çš„è®°å¿†èƒ½åŠ›æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†ç°æœ‰è§£å†³æ–¹æ¡ˆä»å­˜åœ¨æ ¹æœ¬æ€§å±€é™ã€‚å¤§å¤šæ•°è§£å†³æ–¹æ¡ˆä¾èµ–äºå¹³é¢ã€èŒƒå›´ç‹­çª„çš„è®°å¿†ç»„ä»¶ï¼Œé™åˆ¶äº†å®ƒä»¬éšæ—¶é—´ä¸ªæ€§åŒ–ã€æŠ½è±¡å’Œå¯é åœ°å›å¿†ç”¨æˆ·ç‰¹å®šä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MIRIXï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ã€å¤šä¸»ä½“çš„è®°å¿†ç³»ç»Ÿï¼Œå®ƒé€šè¿‡è§£å†³è¯¥é¢†åŸŸæœ€å…³é”®çš„æŒ‘æˆ˜æ¥é‡æ–°å®šä¹‰AIè®°å¿†çš„æœªæ¥ï¼Œå³è®©è¯­è¨€æ¨¡å‹çœŸæ­£èƒ½å¤Ÿè®°å¿†ã€‚ä¸åŒäºä»¥å‰çš„æ–¹æ³•ï¼ŒMIRIXè¶…è¶Šäº†æ–‡æœ¬ï¼Œæ‹¥æŠ±ä¸°å¯Œçš„è§†è§‰å’Œå¤šæ¨¡å¼ä½“éªŒï¼Œä½¿è®°å¿†åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çœŸæ­£æœ‰ç”¨ã€‚MIRIXç”±å…­ç§ç‹¬ç‰¹ã€ç²¾å¿ƒæ„å»ºçš„è®°å¿†ç±»å‹ç»„æˆï¼šæ ¸å¿ƒè®°å¿†ã€æƒ…æ™¯è®°å¿†ã€è¯­ä¹‰è®°å¿†ã€ç¨‹åºè®°å¿†ã€èµ„æºè®°å¿†å’ŒçŸ¥è¯†å®åº“ï¼Œå†åŠ ä¸Šä¸€ä¸ªå¤šåŠ¨ä¸»ä½“æ¡†æ¶ï¼ŒåŠ¨æ€æ§åˆ¶å’Œåè°ƒæ›´æ–°å’Œæ£€ç´¢ã€‚è¿™ç§è®¾è®¡ä½¿ä¸»ä½“èƒ½å¤Ÿå¤§è§„æ¨¡æŒä¹…åŒ–ã€æ¨ç†å’Œå‡†ç¡®æ£€ç´¢å¤šæ ·ã€é•¿æœŸçš„ç”¨æˆ·æ•°æ®ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªæœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­éªŒè¯äº†MIRIXçš„æœ‰æ•ˆæ€§ã€‚é¦–å…ˆï¼Œåœ¨ScreenshotVQAä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¿‘2ä¸‡å¥—é«˜åˆ†è¾¨ç‡è®¡ç®—æœºæˆªå›¾åºåˆ—çš„å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ï¼Œéœ€è¦æ·±åº¦ä¸Šä¸‹æ–‡ç†è§£ï¼Œä¸”æ— æ³•åº”ç”¨ç°æœ‰çš„è®°å¿†ç³»ç»Ÿã€‚MIRIXçš„å‡†ç¡®åº¦æ¯”RAGåŸºå‡†é«˜å‡º35%ï¼ŒåŒæ—¶å­˜å‚¨éœ€æ±‚é™ä½äº†99.9%ã€‚å…¶æ¬¡ï¼Œåœ¨LOCOMOä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å•æ¨¡æ€æ–‡æœ¬è¾“å…¥çš„é•¿å¯¹è¯åŸºå‡†æµ‹è¯•ï¼ŒMIRIXè¾¾åˆ°äº†æœ€å…ˆè¿›çš„85.4%æ€§èƒ½ï¼Œè¿œè¿œè¶…è¿‡äº†ç°æœ‰åŸºå‡†ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒMIRIXä¸ºå†…å­˜å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸»ä½“è®¾å®šäº†æ–°çš„æ€§èƒ½æ ‡å‡†ã€‚ä¸ºäº†è®©ç”¨æˆ·ä½“éªŒæˆ‘ä»¬çš„è®°å¿†ç³»ç»Ÿï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç”±MIRIXé©±åŠ¨çš„åº”ç”¨ç¨‹åºã€‚å®ƒå®æ—¶ç›‘æ§å±å¹•ï¼Œå»ºç«‹ä¸ªæ€§åŒ–è®°å¿†åŸºç¡€ï¼Œå¹¶æä¾›ç›´è§‚çš„å¯è§†åŒ–å’Œå®‰å…¨çš„æœ¬åœ°å­˜å‚¨æ¥ç¡®ä¿éšç§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07957v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§æ–°å‹çš„AIå¤šä»£ç†è®°å¿†ç³»ç»Ÿâ€”â€”MIRIXã€‚ä¼ ç»Ÿçš„AIè®°å¿†èƒ½åŠ›å­˜åœ¨å±€é™ï¼ŒMIRIXé€šè¿‡ç»“åˆå…­å¤§ç±»è®°å¿†ï¼ˆæ ¸å¿ƒè®°å¿†ã€æƒ…æ™¯è®°å¿†ã€è¯­ä¹‰è®°å¿†ã€ç¨‹åºè®°å¿†ã€èµ„æºè®°å¿†å’ŒçŸ¥è¯†å®åº“ï¼‰å’Œå¤šä»£ç†æ¡†æ¶ï¼Œå®ç°äº†å¯¹ç”¨æˆ·çš„ä¸ªæ€§åŒ–ã€æŠ½è±¡åŒ–å’Œé•¿æœŸè®°å¿†çš„å¢å¼ºã€‚MIRIXåœ¨æˆªå›¾è§†è§‰é—®ç­”å’Œé•¿å¯¹è¯ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºï¼Œè¶…è¶Šäº†ç°æœ‰ç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒMIRIXè¿˜æä¾›äº†ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œå¯ä»¥å®æ—¶ç›‘æ§å±å¹•ã€å»ºç«‹ä¸ªæ€§åŒ–è®°å¿†åº“ã€ç›´è§‚å¯è§†åŒ–åŠä¿éšœæœ¬åœ°å­˜å‚¨çš„éšç§æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIRIXæ˜¯ä¸€ç§æ–°å‹çš„å¤šä»£ç†è®°å¿†ç³»ç»Ÿï¼Œä¸“ä¸ºè§£å†³AIè¯­è¨€æ¨¡å‹çš„è®°å¿†é—®é¢˜è€Œè®¾è®¡ã€‚</li>
<li>MIRIXç»“åˆäº†å…­å¤§ç±»è®°å¿†ï¼ŒåŒ…æ‹¬æ ¸å¿ƒè®°å¿†ã€æƒ…æ™¯è®°å¿†ç­‰ï¼Œå¢å¼ºäº†AIçš„ä¸ªæ€§åŒ–ã€æŠ½è±¡åŒ–å’Œé•¿æœŸè®°å¿†èƒ½åŠ›ã€‚</li>
<li>MIRIXé‡‡ç”¨å¤šä»£ç†æ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€æ§åˆ¶å’Œåè°ƒè®°å¿†æ›´æ–°å’Œæ£€ç´¢ã€‚</li>
<li>MIRIXåœ¨æˆªå›¾è§†è§‰é—®ç­”å’Œé•¿å¯¹è¯ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰ç³»ç»Ÿã€‚</li>
<li>MIRIXæä¾›äº†ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œå¯å®æ—¶ç›‘æ§å±å¹•ã€å»ºç«‹ä¸ªæ€§åŒ–è®°å¿†åº“ã€‚</li>
<li>MIRIXåº”ç”¨ç¨‹åºå…·æœ‰ç›´è§‚å¯è§†åŒ–å’Œä¿éšœæœ¬åœ°å­˜å‚¨éšç§æ€§çš„åŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0bf542b4356fc691ab7ef7e1fcdfc516.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8f9933212e66073dcf5a841f9f38de6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-008ecc74c82d6d85ef78cea9d2e1e135.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e37d57a7dc0e3a1add7a98d70058d04b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MIRA-A-Novel-Framework-for-Fusing-Modalities-in-Medical-RAG"><a href="#MIRA-A-Novel-Framework-for-Fusing-Modalities-in-Medical-RAG" class="headerlink" title="MIRA: A Novel Framework for Fusing Modalities in Medical RAG"></a>MIRA: A Novel Framework for Fusing Modalities in Medical RAG</h2><p><strong>Authors:Jinhong Wang, Tajamul Ashraf, Zongyan Han, Jorma Laaksonen, Rao Mohammad Anwer</strong></p>
<p>Multimodal Large Language Models (MLLMs) have significantly advanced AI-assisted medical diagnosis, but they often generate factually inconsistent responses that deviate from established medical knowledge. Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external sources, but it presents two key challenges. First, insufficient retrieval can miss critical information, whereas excessive retrieval can introduce irrelevant or misleading content, disrupting model output. Second, even when the model initially provides correct answers, over-reliance on retrieved data can lead to factual errors. To address these issues, we introduce the Multimodal Intelligent Retrieval and Augmentation (MIRA) framework, designed to optimize factual accuracy in MLLM. MIRA consists of two key components: (1) a calibrated Rethinking and Rearrangement module that dynamically adjusts the number of retrieved contexts to manage factual risk, and (2) A medical RAG framework integrating image embeddings and a medical knowledge base with a query-rewrite module for efficient multimodal reasoning. This enables the model to effectively integrate both its inherent knowledge and external references. Our evaluation of publicly available medical VQA and report generation benchmarks demonstrates that MIRA substantially enhances factual accuracy and overall performance, achieving new state-of-the-art results. Code is released at <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/MIRA">https://github.com/mbzuai-oryx/MIRA</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨AIè¾…åŠ©åŒ»å­¦è¯Šæ–­æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬ç»å¸¸ç”Ÿæˆä¸æ—¢å®šåŒ»å­¦çŸ¥è¯†ä¸ä¸€è‡´çš„å“åº”ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡æ•´åˆå¤–éƒ¨èµ„æºæé«˜äº†äº‹å®å‡†ç¡®æ€§ï¼Œä½†å­˜åœ¨ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæ£€ç´¢ä¸è¶³å¯èƒ½ä¼šé—æ¼å…³é”®ä¿¡æ¯ï¼Œè€Œè¿‡åº¦æ£€ç´¢åˆ™ä¼šå¼•å…¥æ— å…³æˆ–è¯¯å¯¼æ€§çš„å†…å®¹ï¼Œå¹²æ‰°æ¨¡å‹è¾“å‡ºã€‚å…¶æ¬¡ï¼Œå³ä½¿æ¨¡å‹æœ€åˆç»™å‡ºæ­£ç¡®çš„ç­”æ¡ˆï¼Œè¿‡åº¦ä¾èµ–æ£€ç´¢æ•°æ®ä¹Ÿå¯èƒ½å¯¼è‡´äº‹å®é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€æ™ºèƒ½æ£€ç´¢å’Œå¢å¼ºï¼ˆMIRAï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–MLLMä¸­çš„äº‹å®å‡†ç¡®æ€§ã€‚MIRAç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼šï¼ˆ1ï¼‰ç»è¿‡æ ¡å‡†çš„åæ€å’Œé‡ç»„æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯åŠ¨æ€è°ƒæ•´æ£€ç´¢ä¸Šä¸‹æ–‡æ•°é‡ä»¥ç®¡ç†äº‹å®é£é™©ï¼›ï¼ˆ2ï¼‰åŒ»å­¦RAGæ¡†æ¶èåˆäº†å›¾åƒåµŒå…¥å’ŒåŒ»å­¦çŸ¥è¯†åº“ï¼Œå¹¶é…å¤‡äº†æŸ¥è¯¢æ”¹å†™æ¨¡å—ä»¥å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•´åˆå…¶å†…åœ¨çŸ¥è¯†å’Œå¤–éƒ¨å‚è€ƒã€‚æˆ‘ä»¬å¯¹å…¬å¼€å¯ç”¨çš„åŒ»å­¦VQAå’ŒæŠ¥å‘Šç”ŸæˆåŸºå‡†ç‚¹çš„è¯„ä¼°è¡¨æ˜ï¼ŒMIRAå¤§å¤§æé«˜äº†äº‹å®å‡†ç¡®æ€§å’Œæ€»ä½“æ€§èƒ½ï¼Œå®ç°äº†æœ€æ–°çš„æœ€å…ˆè¿›çš„æˆæœã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/MIRA%E3%80%82">https://github.com/mbzuai-oryx/MIRAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07902v1">PDF</a> ACM Multimedia 2025</p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨è¾…åŠ©åŒ»ç–—è¯Šæ–­æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨äº‹å®ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¸ºæé«˜äº‹å®å‡†ç¡®æ€§ï¼Œç ”ç©¶è€…æå‡ºäº†ç»“åˆå¤–éƒ¨èµ„æºçš„RAGæŠ€æœ¯ï¼Œä½†ä»é¢ä¸´ä¿¡æ¯ä¸è¶³æˆ–å†—ä½™çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†æ—¨åœ¨ä¼˜åŒ–MLLMäº‹å®å‡†ç¡®æ€§çš„Multimodal Intelligent Retrieval and Augmentationï¼ˆMIRAï¼‰æ¡†æ¶ã€‚MIRAåŒ…æ‹¬æ ¡å‡†çš„åæ€å’Œè°ƒæ•´æ¨¡å—ä»¥åŠåŒ»å­¦RAGæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå›¾åƒåµŒå…¥å’ŒåŒ»å­¦çŸ¥è¯†åº“è¿›è¡Œé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒMIRAæ˜¾è‘—æé«˜äº‹å®å‡†ç¡®æ€§å’Œæ€»ä½“æ€§èƒ½ï¼Œè¾¾åˆ°æ–°çš„ç ”ç©¶æ°´å¹³ã€‚ä»£ç å·²å‘å¸ƒåœ¨å…¬å¼€é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MLLMsåœ¨åŒ»ç–—è¯Šæ–­æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨äº‹å®ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>RAGæŠ€æœ¯æ—¨åœ¨é€šè¿‡ç»“åˆå¤–éƒ¨èµ„æºæé«˜äº‹å®å‡†ç¡®æ€§ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>MIRAæ¡†æ¶åŒ…æ‹¬æ ¡å‡†çš„åæ€å’Œè°ƒæ•´æ¨¡å—ä»¥åŠåŒ»å­¦RAGæ¡†æ¶æ¥ä¼˜åŒ–äº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>MIRAé€šè¿‡æ•´åˆå›¾åƒåµŒå…¥å’ŒåŒ»å­¦çŸ¥è¯†åº“è¿›è¡Œé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>MIRAæ˜¾è‘—æé«˜äº‹å®å‡†ç¡®æ€§å’Œæ€»ä½“æ€§èƒ½ï¼Œè¾¾åˆ°æ–°çš„ç ”ç©¶æ°´å¹³ã€‚</li>
<li>ä»£ç å·²å‘å¸ƒåœ¨å…¬å¼€é“¾æ¥ä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e778f5fad9e9644da2469e7e04afd07d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cecbb24a7514824bca102047f658e825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc614305ad4566f6c20b7e27dc918256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7389b511146d20edccb2699a9c38147.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Opting-Out-of-Generative-AI-a-Behavioral-Experiment-on-the-Role-of-Education-in-Perplexity-AI-Avoidance"><a href="#Opting-Out-of-Generative-AI-a-Behavioral-Experiment-on-the-Role-of-Education-in-Perplexity-AI-Avoidance" class="headerlink" title="Opting Out of Generative AI: a Behavioral Experiment on the Role of   Education in Perplexity AI Avoidance"></a>Opting Out of Generative AI: a Behavioral Experiment on the Role of   Education in Perplexity AI Avoidance</h2><p><strong>Authors:Roberto Ulloa, Juhi Kulshrestha, Celina Kacperski</strong></p>
<p>The rise of conversational AI (CAI), powered by large language models, is transforming how individuals access and interact with digital information. However, these tools may inadvertently amplify existing digital inequalities. This study investigates whether differences in formal education are associated with CAI avoidance, leveraging behavioral data from an online experiment (N &#x3D; 1,636). Participants were randomly assigned to a control or an information-seeking task, either a traditional online search or a CAI (Perplexity AI). Task avoidance (operationalized as survey abandonment or providing unrelated responses during task assignment) was significantly higher in the CAI group (51%) compared to the search (30.9%) and control (16.8%) groups, with the highest CAI avoidance among participants with lower education levels (~74.4%). Structural equation modeling based on the theoretical framework UTAUT2 and LASSO regressions reveal that education is strongly associated with CAI avoidance, even after accounting for various cognitive and affective predictors of technology adoption. These findings underscore educationâ€™s central role in shaping AI adoption and the role of self-selection biases in AI-related research, stressing the need for inclusive design to ensure equitable access to emerging technologies. </p>
<blockquote>
<p>å¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼ˆCAIï¼‰çš„å´›èµ·ï¼Œä»¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºåŠ¨åŠ›ï¼Œæ­£åœ¨æ”¹å˜ä¸ªäººè®¿é—®å’Œä¸æ•°å­—ä¿¡æ¯äº¤äº’çš„æ–¹å¼ã€‚ç„¶è€Œï¼Œè¿™äº›å·¥å…·å¯èƒ½ä¼šæ— æ„ä¸­æ”¾å¤§ç°æœ‰çš„æ•°å­—é¸¿æ²Ÿã€‚æœ¬ç ”ç©¶å€ŸåŠ©æ¥è‡ªåœ¨çº¿å®éªŒï¼ˆN&#x3D;1636ï¼‰çš„è¡Œä¸ºæ•°æ®ï¼Œæ¢è®¨æ­£è§„æ•™è‚²çš„å·®å¼‚æ˜¯å¦ä¼šå¯¼è‡´é¿å…ä½¿ç”¨CAIã€‚å‚ä¸è€…è¢«éšæœºåˆ†é…æ‰§è¡Œæ§åˆ¶ä»»åŠ¡ã€ä¼ ç»Ÿç½‘ä¸Šæœç´¢ä»»åŠ¡æˆ–ä½¿ç”¨CAIï¼ˆPerplexity AIï¼‰è¿›è¡Œä¿¡æ¯æœç´¢ä»»åŠ¡ã€‚ç›¸è¾ƒäºæœç´¢ç»„ï¼ˆ30.9%ï¼‰å’Œæ§åˆ¶ç»„ï¼ˆ16.8%ï¼‰ï¼Œä»»åŠ¡å›é¿ï¼ˆè¡¨ç°ä¸ºè°ƒæŸ¥æ”¾å¼ƒæˆ–åœ¨ä»»åŠ¡åˆ†é…æœŸé—´æä¾›ä¸ç›¸å…³ç­”æ¡ˆï¼‰åœ¨CAIç»„æ˜æ˜¾æ›´é«˜ï¼ˆ51%ï¼‰ï¼Œä¸”å—æ•™è‚²ç¨‹åº¦è¾ƒä½å‚ä¸è€…å›é¿ä½¿ç”¨CAIçš„æƒ…å†µå°¤ä¸ºä¸¥é‡ï¼ˆçº¦å 74.4%ï¼‰ã€‚åŸºäºUTAUT2ç†è®ºæ¡†æ¶çš„ç»“æ„æ–¹ç¨‹æ¨¡å‹å’ŒLASSOå›å½’åˆ†ææ­ç¤ºï¼Œå³ä½¿è€ƒè™‘åˆ°å„ç§å…³äºæŠ€æœ¯é‡‡çº³çš„è®¤çŸ¥å’Œæƒ…æ„Ÿé¢„æµ‹å› ç´ ï¼Œæ•™è‚²ç¨‹åº¦ä¸é¿å…ä½¿ç”¨CAIä¹‹é—´å­˜åœ¨å¼ºçƒˆå…³è”ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æ•™è‚²åœ¨å¡‘é€ äººå·¥æ™ºèƒ½é‡‡çº³æ–¹é¢çš„é‡è¦ä½œç”¨ï¼Œä»¥åŠåœ¨äººå·¥æ™ºèƒ½ç›¸å…³ç ”ç©¶ä¸­çš„è‡ªæˆ‘é€‰æ‹©åè§é—®é¢˜ï¼Œå¼ºè°ƒäº†éœ€è¦åŒ…å®¹æ€§è®¾è®¡ä»¥ç¡®ä¿äººä»¬èƒ½å…¬å¹³åœ°æ¥è§¦æ–°å…´æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07881v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶çš„è°ƒæŸ¥è¡¨æ˜ï¼Œå¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼ˆCAIï¼‰çš„å…´èµ·å¯èƒ½ä¼šåŠ å‰§ç°æœ‰çš„æ•°å­—é¸¿æ²Ÿã€‚é€šè¿‡å¯¹åœ¨çº¿å®éªŒçš„è¡Œä¸ºæ•°æ®è¿›è¡Œåˆ†æï¼Œå‘ç°å‚ä¸è€…çš„å—æ•™è‚²ç¨‹åº¦ä¸å…¶åœ¨å¯¹è¯å¼äººå·¥æ™ºèƒ½ä»»åŠ¡ä¸­çš„å›é¿è¡Œä¸ºå­˜åœ¨å…³è”ã€‚ç›¸è¾ƒäºä¼ ç»Ÿåœ¨çº¿æœç´¢å’Œå¯¹ç…§å‚ä¸è€…ï¼Œå¯¹è¯å¼äººå·¥æ™ºèƒ½ç»„çš„å›é¿è¡Œä¸ºæ˜¾è‘—æ›´é«˜ï¼ˆè¾¾åˆ°çº¦51%ï¼‰ï¼Œå°¤å…¶å—æ•™è‚²ç¨‹åº¦è¾ƒä½çš„å‚ä¸è€…ä¸­è¡¨ç°æ›´ä¸ºæ˜æ˜¾ï¼ˆè¾¾åˆ°çº¦74.4%ï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå—æ•™è‚²ç¨‹åº¦åœ¨å½±å“AIæ¥å—åº¦æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ï¼Œä¹Ÿæ­ç¤ºäº†è‡ªæˆ‘é€‰æ‹©åè§åœ¨AIç›¸å…³ç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚ä¸ºæ­¤éœ€è¦é‡‡å–åŒ…å®¹æ€§è®¾è®¡ç¡®ä¿æ‰€æœ‰äººå¹³ç­‰äº«å—æŠ€æœ¯æ‰€å¸¦æ¥çš„ä¾¿åˆ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼ˆCAIï¼‰æ­£åœ¨æ”¹å˜äººä»¬è·å–å’Œä½¿ç”¨æ•°å­—ä¿¡æ¯çš„æ–¹å¼ã€‚</li>
<li>å¯¹è¯å¼äººå·¥æ™ºèƒ½å¯èƒ½ä¼šåŠ å‰§ç°æœ‰çš„æ•°å­—é¸¿æ²Ÿã€‚</li>
<li>å—æ•™è‚²ç¨‹åº¦æ˜¯å½±å“å¯¹è¯å¼äººå·¥æ™ºèƒ½æ¥å—åº¦çš„é‡è¦å› ç´ ä¹‹ä¸€ã€‚</li>
<li>åœ¨å¯¹è¯å¼äººå·¥æ™ºèƒ½ä»»åŠ¡ä¸­ï¼Œå—æ•™è‚²ç¨‹åº¦è¾ƒä½çš„å‚ä¸è€…è¡¨ç°å‡ºæ›´é«˜çš„å›é¿è¡Œä¸ºã€‚</li>
<li>è‡ªæˆ‘é€‰æ‹©åè§åœ¨AIç›¸å…³ç ”ç©¶ä¸­çš„é‡è¦æ€§ä¸å¯å¿½è§†ã€‚</li>
<li>ç»“æ„æ–¹ç¨‹æ¨¡å‹å’ŒLASSOå›å½’åˆ†ææ­ç¤ºæ•™è‚²åœ¨AIé‡‡ç”¨æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9987ce59db68dd293950cbcabfceebb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9f022bf4c197bd5d8653dba3bedb1b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20fada5adc07375f200df939e5ba486b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DocCHA-Towards-LLM-Augmented-Interactive-Online-diagnosis-System"><a href="#DocCHA-Towards-LLM-Augmented-Interactive-Online-diagnosis-System" class="headerlink" title="DocCHA: Towards LLM-Augmented Interactive Online diagnosis System"></a>DocCHA: Towards LLM-Augmented Interactive Online diagnosis System</h2><p><strong>Authors:Xinyi Liu, Dachun Sun, Yi R. Fung, Dilek Hakkani-TÃ¼r, Tarek Abdelzaher</strong></p>
<p>Despite the impressive capabilities of Large Language Models (LLMs), existing Conversational Health Agents (CHAs) remain static and brittle, incapable of adaptive multi-turn reasoning, symptom clarification, or transparent decision-making. This hinders their real-world applicability in clinical diagnosis, where iterative and structured dialogue is essential. We propose DocCHA, a confidence-aware, modular framework that emulates clinical reasoning by decomposing the diagnostic process into three stages: (1) symptom elicitation, (2) history acquisition, and (3) causal graph construction. Each module uses interpretable confidence scores to guide adaptive questioning, prioritize informative clarifications, and refine weak reasoning links.   Evaluated on two real-world Chinese consultation datasets (IMCS21, DX), DocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5, GPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and over 30 percent improvement in symptom recall, with only modest increase in dialogue turns. These results demonstrate the effectiveness of DocCHA in enabling structured, transparent, and efficient diagnostic conversations â€“ paving the way for trustworthy LLM-powered clinical assistants in multilingual and resource-constrained settings. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†ç°æœ‰çš„ä¼šè¯å¥åº·ä»£ç†ï¼ˆCHAï¼‰ä»ç„¶é™æ€ä¸”è„†å¼±ï¼Œæ— æ³•è¿›è¡Œè‡ªé€‚åº”çš„å¤šè½®æ¨ç†ã€ç—‡çŠ¶æ¾„æ¸…æˆ–é€æ˜çš„å†³ç­–åˆ¶å®šã€‚è¿™é˜»ç¢äº†å®ƒä»¬åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„å®é™…åº”ç”¨ï¼Œè€Œåœ¨ä¸´åºŠè¯Šæ–­ä¸­ï¼Œè¿­ä»£å’Œç»“æ„åŒ–å¯¹è¯è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†DocCHAï¼Œè¿™æ˜¯ä¸€ä¸ªæ„è¯†åˆ°çš„ã€æ¨¡å—åŒ–çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£è¯Šæ–­è¿‡ç¨‹ä¸ºä¸‰ä¸ªé˜¶æ®µæ¥æ¨¡æ‹Ÿä¸´åºŠæ¨ç†ï¼šï¼ˆ1ï¼‰ç—‡çŠ¶å¼•å¯¼ï¼Œï¼ˆ2ï¼‰ç—…å²é‡‡é›†ï¼Œå’Œï¼ˆ3ï¼‰å› æœå›¾æ„å»ºã€‚æ¯ä¸ªæ¨¡å—éƒ½ä½¿ç”¨å¯è§£é‡Šçš„ç½®ä¿¡åº¦åˆ†æ•°æ¥æŒ‡å¯¼è‡ªé€‚åº”æé—®ã€ä¼˜å…ˆæä¾›ä¿¡æ¯æ¾„æ¸…å¹¶æ”¹è¿›è–„å¼±çš„æ¨ç†è”ç³»ã€‚åœ¨ä¸¤é¡¹çœŸå®ä¸–ç•Œçš„ä¸­æ–‡å’¨è¯¢æ•°æ®é›†ï¼ˆIMCS21ã€DXï¼‰ä¸Šè¯„ä¼°ï¼ŒDocCHAæŒç»­ä¼˜äºå¼ºå¤§çš„åŸºäºæç¤ºçš„LLMåŸºå‡†æµ‹è¯•ï¼ˆGPT-3.5ã€GPT-4oã€LLaMA-3ï¼‰ï¼Œè¯Šæ–­å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾5.18%ï¼Œç—‡çŠ¶å›å¿†ç‡æé«˜äº†è¶…è¿‡30%ï¼Œè€Œå¯¹è¯å›åˆä»…é€‚åº¦å¢åŠ ã€‚è¿™äº›ç»“æœè¯æ˜äº†DocCHAåœ¨ç»“æ„åŒ–ã€é€æ˜å’Œé«˜æ•ˆçš„è¯Šæ–­å¯¹è¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåœ¨å¤šç§è¯­è¨€å’Œèµ„æºå—é™ç¯å¢ƒä¸­ä½¿ç”¨å¯ä¿¡çš„LLMé©±åŠ¨çš„ä¸´åºŠåŠ©æ‰‹é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07870v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶èƒ½åŠ›å¼ºå¤§ï¼Œä½†ç°æœ‰çš„ä¼šè¯å¥åº·ä»£ç†ï¼ˆCHAï¼‰ä»ç„¶é™æ€ä¸”è„†å¼±ï¼Œç¼ºä¹è‡ªé€‚åº”å¤šè½®æ¨ç†ã€ç—‡çŠ¶æ¾„æ¸…å’Œé€æ˜å†³ç­–èƒ½åŠ›ã€‚è¿™é˜»ç¢äº†å…¶åœ¨ä¸´åºŠè¯Šç–—ä¸­çš„å®é™…åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DocCHAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿä¸´åºŠæ¨ç†è¿‡ç¨‹ï¼Œå°†å…¶åˆ†è§£ä¸ºç—‡çŠ¶æç¤ºã€ç—…å²é‡‡é›†å’Œå› æœå›¾æ„å»ºä¸‰ä¸ªé˜¶æ®µï¼Œå®ç°è‡ªä¿¡åº¦æ„ŸçŸ¥å’Œæ¨¡å—åŒ–ã€‚æ¯ä¸ªæ¨¡å—éƒ½ä½¿ç”¨å¯è§£é‡Šçš„ç½®ä¿¡åº¦åˆ†æ•°æ¥å¼•å¯¼è‡ªé€‚åº”æé—®ã€ä¼˜å…ˆå¤„ç†ä¿¡æ¯æ¾„æ¸…å’Œç»†åŒ–å¼±æ¨ç†é“¾æ¥ã€‚åœ¨ä¸¤é¡¹çœŸå®ä¸–ç•Œä¸­æ–‡å’¨è¯¢æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒDocCHAåœ¨è¯Šæ–­å‡†ç¡®ç‡ä¸Šæ¯”LLMåŸºçº¿æ¨¡å‹é«˜å‡º5.18%ï¼Œç—‡çŠ¶å¬å›ç‡æé«˜äº†è¶…è¿‡30%ï¼Œä¸”å¯¹è¯è½®æ¬¡ä»…é€‚åº¦å¢åŠ ã€‚è¿™è¯æ˜äº†DocCHAåœ¨ç»“æ„åŒ–ã€é€æ˜å’Œé«˜æ•ˆçš„è¯Šæ–­å¯¹è¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºåœ¨å¤šç§è¯­è¨€å’Œèµ„æºå—é™ç¯å¢ƒä¸­ä½¿ç”¨å¯ä¿¡çš„LLMé©±åŠ¨çš„ä¸´åºŠåŠ©æ‰‹é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰CHAså­˜åœ¨é™æ€å’Œè„†å¼±çš„é—®é¢˜ï¼Œç¼ºä¹è‡ªé€‚åº”å¤šè½®æ¨ç†å’Œé€æ˜å†³ç­–èƒ½åŠ›ã€‚</li>
<li>DocCHAæ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿä¸´åºŠæ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œæé«˜CHAsçš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚</li>
<li>DocCHAä½¿ç”¨è‡ªä¿¡åº¦æ„ŸçŸ¥å’Œæ¨¡å—åŒ–è®¾è®¡ï¼Œé€šè¿‡è‡ªé€‚åº”æé—®å’Œä¼˜å…ˆå¤„ç†ä¿¡æ¯æ¾„æ¸…æ¥å¼•å¯¼å¯¹è¯ã€‚</li>
<li>DocCHAåœ¨çœŸå®ä¸–ç•Œä¸­æ–‡å’¨è¯¢æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºLLMåŸºçº¿æ¨¡å‹ã€‚</li>
<li>DocCHAåœ¨è¯Šæ–­å‡†ç¡®ç‡å’Œç—‡çŠ¶å¬å›ç‡æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
<li>DocCHAæ¡†æ¶æœ‰åŠ©äºå®ç°ç»“æ„åŒ–ã€é€æ˜å’Œé«˜æ•ˆçš„è¯Šæ–­å¯¹è¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e016c9ab0597bf342ee928bb5981ed9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edb7c301cbf8ec00b8b8f4a7b00bd7c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f819388bd191883a58bf28857a3b48de.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="On-the-Effect-of-Instruction-Tuning-Loss-on-Generalization"><a href="#On-the-Effect-of-Instruction-Tuning-Loss-on-Generalization" class="headerlink" title="On the Effect of Instruction Tuning Loss on Generalization"></a>On the Effect of Instruction Tuning Loss on Generalization</h2><p><strong>Authors:Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty</strong></p>
<p>Instruction Tuning has emerged as a pivotal post-training paradigm that enables pre-trained language models to better follow user instructions. Despite its significance, little attention has been given to optimizing the loss function used. A fundamental, yet often overlooked, question is whether the conventional auto-regressive objective - where loss is computed only on response tokens, excluding prompt tokens - is truly optimal for instruction tuning. In this work, we systematically investigate the impact of differentially weighting prompt and response tokens in instruction tuning loss, and propose Weighted Instruction Tuning (WIT) as a better alternative to conventional instruction tuning. Through extensive experiments on five language models of different families and scale, three finetuning datasets of different sizes, and five diverse evaluation benchmarks, we show that the standard instruction tuning loss often yields suboptimal performance and limited robustness to input prompt variations. We find that a low-to-moderate weight for prompt tokens coupled with a moderate-to-high weight for response tokens yields the best-performing models across settings and also serve as better starting points for the subsequent preference alignment training. These findings highlight the need to reconsider instruction tuning loss and offer actionable insights for developing more robust and generalizable models. Our code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/kowndinya-renduchintala/WIT">https://github.com/kowndinya-renduchintala/WIT</a>. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå·²æˆä¸ºä¸€ç§å…³é”®çš„åè®­ç»ƒæ¨¡å¼ï¼Œä½¿é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°éµå¾ªç”¨æˆ·æŒ‡ä»¤ã€‚å°½ç®¡å…¶æ„ä¹‰é‡å¤§ï¼Œä½†å¾ˆå°‘æœ‰äººå…³æ³¨ä¼˜åŒ–æ‰€ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€‚ä¸€ä¸ªåŸºæœ¬ä½†å¸¸è¢«å¿½è§†çš„é—®é¢˜æ˜¯ï¼Œä¼ ç»Ÿçš„è‡ªå›å½’ç›®æ ‡ï¼ˆæŸå¤±ä»…è®¡ç®—å“åº”æ ‡è®°ï¼Œä¸åŒ…æ‹¬æç¤ºæ ‡è®°ï¼‰æ˜¯å¦çœŸæ­£é€‚åˆæŒ‡ä»¤å¾®è°ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†åœ¨æŒ‡ä»¤å¾®è°ƒæŸå¤±ä¸­å·®å¼‚åŒ–åŠ æƒæç¤ºå’Œå“åº”æ ‡è®°çš„å½±å“ï¼Œå¹¶æå‡ºåŠ æƒæŒ‡ä»¤å¾®è°ƒï¼ˆWITï¼‰ä½œä¸ºä¼ ç»ŸæŒ‡ä»¤è°ƒè°çš„æ›´å¥½æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡å¯¹äº”ä¸ªä¸åŒå®¶æ—å’Œè§„æ¨¡çš„è¯­è¨€æ¨¡å‹ã€ä¸‰ä¸ªä¸åŒå¤§å°çš„å¾®è°ƒæ•°æ®é›†å’Œäº”ä¸ªä¸åŒçš„è¯„ä¼°åŸºå‡†è¿›è¡Œå¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¡¨æ˜æ ‡å‡†æŒ‡ä»¤è°ƒè°æŸå¤±é€šå¸¸ä¼šäº§ç”Ÿæ¬¡ä¼˜æ€§èƒ½ï¼Œå¹¶ä¸”å¯¹è¾“å…¥æç¤ºå˜åŒ–çš„é²æ£’æ€§æœ‰é™ã€‚æˆ‘ä»¬å‘ç°ï¼Œæç¤ºæ ‡è®°çš„æƒé‡è¾ƒä½è‡³ä¸­ç­‰ï¼Œå“åº”æ ‡è®°çš„æƒé‡ä¸­è‡³é«˜æ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½è¡¨ç°æœ€ä½³ï¼Œå¹¶ä¸”åœ¨å„ç§è®¾ç½®ä¸­ä¹Ÿä½œä¸ºåç»­åå¥½å¯¹é½è®­ç»ƒçš„æ›´å¥½èµ·ç‚¹ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†éœ€è¦é‡æ–°è€ƒè™‘æŒ‡ä»¤è°ƒè°æŸå¤±çš„å¿…è¦æ€§å’Œå¼€å‘æ›´ç¨³å¥å’Œé€šç”¨æ¨¡å‹çš„å®ç”¨è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/kowndinya-renduchintala/WIT%E3%80%82">https://github.com/kowndinya-renduchintala/WITã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07817v1">PDF</a> Transactions of the Association for Computational Linguistics (TACL)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æŒ‡ä»¤å¾®è°ƒä¸­çš„æŸå¤±å‡½æ•°ä¼˜åŒ–é—®é¢˜ï¼Œæå‡ºåŠ æƒæŒ‡ä»¤å¾®è°ƒï¼ˆWITï¼‰ä½œä¸ºå¯¹å¸¸è§„æŒ‡ä»¤è°ƒå‚çš„æ”¹è¿›ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼Œä½œè€…å‘ç°æ ‡å‡†æŒ‡ä»¤è°ƒå‚æŸå¤±ä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³å’Œåº”å¯¹è¾“å…¥æç¤ºå˜åŒ–æ—¶é²æ£’æ€§æœ‰é™çš„é—®é¢˜ã€‚é€‚å½“é™ä½æç¤ºä»¤ç‰Œçš„æƒé‡å¹¶å¢åŠ å“åº”ä»¤ç‰Œçš„æƒé‡ï¼Œå¯ä»¥åœ¨ä¸åŒè®¾ç½®ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä½œä¸ºåç»­åå¥½å¯¹é½è®­ç»ƒçš„æ›´å¥½èµ·ç‚¹ã€‚è¿™è¦æ±‚é‡æ–°è€ƒè™‘æŒ‡ä»¤è°ƒå‚æŸå¤±ï¼Œå¹¶ä¸ºå¼€å‘æ›´ç¨³å¥å’Œé€šç”¨çš„æ¨¡å‹æä¾›å®é™…æ“ä½œå»ºè®®ã€‚ä»£ç å·²å…¬å¼€äºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒæ˜¯ä½¿é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ›´å¥½åœ°éµå¾ªç”¨æˆ·æŒ‡ä»¤çš„å…³é”®è®­ç»ƒèŒƒå¼ã€‚</li>
<li>å¸¸è§„çš„è‡ªå›å½’ç›®æ ‡åœ¨æŒ‡ä»¤å¾®è°ƒä¸­å¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ã€‚</li>
<li>åŠ æƒæŒ‡ä»¤å¾®è°ƒï¼ˆWITï¼‰ä½œä¸ºä¸€ç§æ”¹è¿›æ–¹æ³•è¢«æå‡ºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ ‡å‡†æŒ‡ä»¤è°ƒå‚æŸå¤±å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³å’Œé²æ£’æ€§æœ‰é™ã€‚</li>
<li>æç¤ºä»¤ç‰Œçš„æƒé‡è¿‡ä½è‡³ä¸­ç­‰ï¼Œç»“åˆå“åº”ä»¤ç‰Œçš„æƒé‡é€‚ä¸­è‡³è¾ƒé«˜ï¼Œèƒ½åœ¨ä¸åŒè®¾ç½®ä¸­å®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¿™ç§è°ƒå‚æ–¹æ³•ä¸ºåç»­åå¥½å¯¹é½è®­ç»ƒæä¾›äº†æ›´å¥½çš„èµ·ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3199bd18190a95617fe3d03284668cc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-855b31429a38a3703667212cd83e8d22.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2874aa1588eca465199cee4f4faacc88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a379211f800640d3d29a7322bb01ccf.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Hallucination-Stations-On-Some-Basic-Limitations-of-Transformer-Based-Language-Models"><a href="#Hallucination-Stations-On-Some-Basic-Limitations-of-Transformer-Based-Language-Models" class="headerlink" title="Hallucination Stations: On Some Basic Limitations of Transformer-Based   Language Models"></a>Hallucination Stations: On Some Basic Limitations of Transformer-Based   Language Models</h2><p><strong>Authors:Varin Sikka, Vishal Sikka</strong></p>
<p>With widespread adoption of transformer-based language models in AI, there is significant interest in the limits of LLMs capabilities, specifically so-called hallucinations, occurrences in which LLMs provide spurious, factually incorrect or nonsensical information when prompted on certain subjects. Furthermore, there is growing interest in agentic uses of LLMs - that is, using LLMs to create agents that act autonomously or semi-autonomously to carry out various tasks, including tasks with applications in the real world. This makes it important to understand the types of tasks LLMs can and cannot perform. We explore this topic from the perspective of the computational complexity of LLM inference. We show that LLMs are incapable of carrying out computational and agentic tasks beyond a certain complexity, and further that LLMs are incapable of verifying the accuracy of tasks beyond a certain complexity. We present examples of both, then discuss some consequences of this work. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ä¸­åŸºäºå˜å‹å™¨æ¶æ„çš„è¯­è¨€æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æé™å­˜åœ¨æµ“åšçš„å…´è¶£ï¼Œç‰¹åˆ«æ˜¯æ‰€è°“çš„â€œå¹»è§‰â€ç°è±¡ã€‚å½“æç¤ºæŸäº›ä¸»é¢˜æ—¶ï¼ŒLLMä¼šäº§ç”Ÿè™šå‡ã€äº‹å®é”™è¯¯æˆ–æ— æ„ä¹‰çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå¯¹äºLLMçš„èƒ½åŠ¨æ€§åº”ç”¨ä¹Ÿè¶Šæ¥è¶Šæ„Ÿå…´è¶£â€”â€”å³ä½¿ç”¨LLMåˆ›å»ºèƒ½å¤Ÿè‡ªä¸»æˆ–åŠè‡ªä¸»æ‰§è¡Œå„ç§ä»»åŠ¡çš„ä»£ç†ï¼ŒåŒ…æ‹¬åœ¨ç°å®ä¸–ç•Œä¸­åº”ç”¨çš„ä»»åŠ¡ã€‚è¿™ä½¿æˆ‘ä»¬ç†è§£LLMèƒ½å¤Ÿæ‰§è¡Œå’Œä¸èƒ½æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹å˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ä»è®¡ç®—å¤æ‚æ€§çš„è§’åº¦æ¢è®¨è¿™ä¸€ä¸»é¢˜ï¼Œæ¢è®¨LLMæ¨ç†çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬è¯æ˜LLMæ— æ³•æ‰§è¡Œè¶…å‡ºä¸€å®šå¤æ‚åº¦çš„è®¡ç®—å’Œä»£ç†ä»»åŠ¡ï¼Œå¹¶ä¸”æ— æ³•éªŒè¯è¶…å‡ºä¸€å®šå¤æ‚åº¦çš„ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬ä¸¾å‡ºä¸¤ä¸ªæ–¹é¢çš„ä¾‹å­ï¼Œç„¶åè®¨è®ºè¿™é¡¹å·¥ä½œçš„åæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07505v1">PDF</a> 6 pages; to be submitted to AAAI-26 after reviews</p>
<p><strong>Summary</strong></p>
<p>éšç€åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹å…¶èƒ½åŠ›æé™çš„ç ”ç©¶æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç‰¹åˆ«æ˜¯å…³äºLLMsäº§ç”Ÿçš„æ‰€è°“â€œå¹»è§‰â€ç°è±¡ï¼Œå³LLMsåœ¨æŸäº›ä¸»é¢˜æç¤ºä¸‹æä¾›è™šå‡ã€é”™è¯¯æˆ–éç†æ€§çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œäººä»¬è¿˜è¶Šæ¥è¶Šå…³æ³¨LLMsçš„ä»£ç†ä½¿ç”¨ï¼Œå³ä½¿ç”¨LLMsåˆ›å»ºèƒ½å¤Ÿè‡ªä¸»æˆ–åŠè‡ªä¸»æ‰§è¡Œå„ç§ä»»åŠ¡çš„ä»£ç†ï¼ŒåŒ…æ‹¬åœ¨ç°å®ä¸–ç•Œä¸­åº”ç”¨çš„ä»»åŠ¡ã€‚æœ¬æ–‡ä»è®¡ç®—å¤æ‚æ€§çš„è§’åº¦æ¢è®¨äº†è¿™ä¸€ä¸»é¢˜ï¼Œå‘ç°LLMsæ— æ³•æ‰§è¡Œè¶…è¿‡ä¸€å®šå¤æ‚åº¦çš„è®¡ç®—å’Œä»£ç†ä»»åŠ¡ï¼Œä¹Ÿæ— æ³•éªŒè¯è¶…è¿‡ä¸€å®šå¤æ‚åº¦çš„ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså­˜åœ¨èƒ½åŠ›æé™ï¼Œæ— æ³•å¤„ç†è¶…è¿‡ä¸€å®šå¤æ‚åº¦çš„è®¡ç®—å’Œä»£ç†ä»»åŠ¡ã€‚</li>
<li>LLMsä¼šäº§ç”Ÿâ€œå¹»è§‰â€ç°è±¡ï¼Œå³æä¾›è™šå‡ã€é”™è¯¯æˆ–éç†æ€§çš„ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨LLMsåˆ›å»ºçš„ä»£ç†å¯ä»¥è‡ªä¸»æˆ–åŠè‡ªä¸»æ‰§è¡Œä»»åŠ¡ã€‚</li>
<li>LLMsæ— æ³•éªŒè¯è¶…è¿‡ä¸€å®šå¤æ‚åº¦çš„ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¯¹LLMsçš„èƒ½åŠ›æé™çš„ç ”ç©¶å¯¹äºç†è§£å’Œä¼˜åŒ–å…¶æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>LLMsåœ¨å®é™…ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›å·¨å¤§ï¼Œä½†éœ€è¦å¯¹å…¶èƒ½åŠ›è¿›è¡Œæ·±å…¥ç ”ç©¶ä»¥å……åˆ†åˆ©ç”¨å…¶æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bf85b44bd3140c008b78bff6a8acf49d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Interpretable-Time-Series-Foundation-Models"><a href="#Towards-Interpretable-Time-Series-Foundation-Models" class="headerlink" title="Towards Interpretable Time Series Foundation Models"></a>Towards Interpretable Time Series Foundation Models</h2><p><strong>Authors:Matthieu Boileau, Philippe Helluy, Jeremy Pawlus, Svitlana Vyetrenko</strong></p>
<p>In this paper, we investigate the distillation of time series reasoning capabilities into small, instruction-tuned language models as a step toward building interpretable time series foundation models. Leveraging a synthetic dataset of mean-reverting time series with systematically varied trends and noise levels, we generate natural language annotations using a large multimodal model and use these to supervise the fine-tuning of compact Qwen models. We introduce evaluation metrics that assess the quality of the distilled reasoning - focusing on trend direction, noise intensity, and extremum localization - and show that the post-trained models acquire meaningful interpretive capabilities. Our results highlight the feasibility of compressing time series understanding into lightweight, language-capable models suitable for on-device or privacy-sensitive deployment. This work contributes a concrete foundation toward developing small, interpretable models that explain temporal patterns in natural language. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›è’¸é¦æˆå°å‹æŒ‡ä»¤è°ƒä¼˜è¯­è¨€æ¨¡å‹ï¼Œä½œä¸ºæ„å»ºå¯è§£é‡Šæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„ä¸€æ­¥ã€‚æˆ‘ä»¬åˆ©ç”¨ä¸€ä¸ªç”±å¹³å‡å›å½’æ—¶é—´åºåˆ—æ„æˆçš„åˆæˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«æœ‰ç³»ç»Ÿæ€§å˜åŒ–è¶‹åŠ¿å’Œå™ªå£°æ°´å¹³ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€æ³¨é‡Šï¼Œå¹¶ç”¨è¿™äº›æ³¨é‡Šæ¥ç›‘ç£ç´§å‡‘Qwenæ¨¡å‹çš„å¾®è°ƒã€‚æˆ‘ä»¬å¼•å…¥äº†è¯„ä¼°è’¸é¦æ¨ç†è´¨é‡çš„è¯„ä»·æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡ä¾§é‡äºè¶‹åŠ¿æ–¹å‘ã€å™ªå£°å¼ºåº¦å’Œæå€¼å®šä½ï¼Œå¹¶è¡¨æ˜ç»è¿‡è®­ç»ƒåçš„æ¨¡å‹è·å¾—äº†æœ‰æ„ä¹‰çš„è§£é‡Šèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å°†æ—¶é—´åºåˆ—ç†è§£å‹ç¼©æˆé€‚åˆè®¾å¤‡ç«¯æˆ–éšç§æ•æ„Ÿéƒ¨ç½²çš„è½»å‹è¯­è¨€æ¨¡å‹çš„å¯èƒ½æ€§ã€‚è¿™é¡¹å·¥ä½œæœç€å¼€å‘èƒ½å¤Ÿè§£é‡Šè‡ªç„¶è¯­è¨€ä¸­çš„æ—¶é—´æ¨¡å¼çš„å°å‹ã€å¯è§£é‡Šæ¨¡å‹çš„å…·ä½“åŸºç¡€è¿ˆå‡ºäº†åšå®çš„æ­¥ä¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07439v1">PDF</a> International Conference on Machine Leaning (ICML) 2025 Workshop on   Foundation Models for Structured Data</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å°†æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å°å‹ã€æŒ‡ä»¤è°ƒä¼˜çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œä½œä¸ºæ„å»ºå¯è§£é‡Šæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹çš„ä¸€æ­¥ã€‚ç ”ç©¶ä½¿ç”¨åˆæˆå¹³å‡å›å½’æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œé€šè¿‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€æ³¨é‡Šï¼Œç”¨äºç›‘ç£ç²¾ç»†è°ƒæ•´ç´§å‡‘æ¨¡å‹ã€‚å¼•å…¥è¯„ä¼°æŒ‡æ ‡ï¼Œè¯„ä¼°è’¸é¦æ¨ç†çš„è´¨é‡ï¼Œé‡ç‚¹å…³æ³¨è¶‹åŠ¿æ–¹å‘ã€å™ªå£°å¼ºåº¦å’Œæå€¼å®šä½ã€‚ç»“æœæ˜¾ç¤ºï¼Œè®­ç»ƒåçš„æ¨¡å‹è·å¾—äº†æœ‰æ„ä¹‰çš„è§£é‡Šèƒ½åŠ›ï¼Œè¯æ˜äº†å°†æ—¶é—´åºåˆ—ç†è§£å‹ç¼©æˆé€‚åˆè®¾å¤‡ç«¯æˆ–éšç§æ•æ„Ÿéƒ¨ç½²çš„è½»å‹è¯­è¨€æ¨¡å‹æ˜¯å¯è¡Œçš„ã€‚æœ¬ç ”ç©¶ä¸ºå¼€å‘èƒ½å¤Ÿè§£é‡Šè‡ªç„¶è¯­è¨€ä¸­æ—¶é—´æ¨¡å¼çš„å°å‹ã€å¯è§£é‡Šæ¨¡å‹å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨å°†æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å°å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥æ„å»ºå¯è§£é‡Šçš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨åˆæˆå¹³å‡å›å½’æ—¶é—´åºåˆ—æ•°æ®é›†è¿›è¡Œç ”ç©¶ã€‚</li>
<li>åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€æ³¨é‡Šï¼Œç”¨äºç›‘ç£æ¨¡å‹çš„ç²¾ç»†è°ƒæ•´ã€‚</li>
<li>å¼•å…¥è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°è’¸é¦æ¨ç†çš„è´¨é‡ï¼ŒåŒ…æ‹¬è¶‹åŠ¿æ–¹å‘ã€å™ªå£°å¼ºåº¦å’Œæå€¼å®šä½ã€‚</li>
<li>è®­ç»ƒåçš„æ¨¡å‹å±•ç°å‡ºæœ‰æ„ä¹‰çš„è§£é‡Šèƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶è¯æ˜äº†å°†æ—¶é—´åºåˆ—ç†è§£å‹ç¼©æˆé€‚åˆè®¾å¤‡ç«¯æˆ–éšç§æ•æ„Ÿéƒ¨ç½²çš„è½»å‹è¯­è¨€æ¨¡å‹çš„å¯è¡Œæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1eea1391c183096f3c2cefd87a9acb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bf8838811dda82d849d4705d479bce7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74e2aadcb83e6645800962e2ada5a587.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="The-Dark-Side-of-LLMs-Agent-based-Attacks-for-Complete-Computer-Takeover"><a href="#The-Dark-Side-of-LLMs-Agent-based-Attacks-for-Complete-Computer-Takeover" class="headerlink" title="The Dark Side of LLMs: Agent-based Attacks for Complete Computer   Takeover"></a>The Dark Side of LLMs: Agent-based Attacks for Complete Computer   Takeover</h2><p><strong>Authors:Matteo Lupinacci, Francesco Aurelio Pironti, Francesco Blefari, Francesco Romeo, Luigi Arena, Angelo Furfaro</strong></p>
<p>The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables unprecedented capabilities in natural language processing and generation. However, these systems have introduced unprecedented security vulnerabilities that extend beyond traditional prompt injection attacks. This paper presents the first comprehensive evaluation of LLM agents as attack vectors capable of achieving complete computer takeover through the exploitation of trust boundaries within agentic AI systems where autonomous entities interact and influence each other. We demonstrate that adversaries can leverage three distinct attack surfaces - direct prompt injection, RAG backdoor attacks, and inter-agent trust exploitation - to coerce popular LLMs (including GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals an alarming vulnerability hierarchy: while 41.2% of models succumb to direct prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical 82.4% can be compromised through inter-agent trust exploitation. Notably, we discovered that LLMs which successfully resist direct malicious commands will execute identical payloads when requested by peer agents, revealing a fundamental flaw in current multi-agent security models. Our findings demonstrate that only 5.9% of tested models (1&#x2F;17) proved resistant to all attack vectors, with the majority exhibiting context-dependent security behaviors that create exploitable blind spots. Our findings also highlight the need to increase awareness and research on the security risks of LLMs, showing a paradigm shift in cybersecurity threats, where AI tools themselves become sophisticated attack vectors. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å’Œå¤šä»£ç†ç³»ç»Ÿçš„å¿«é€Ÿé‡‡çº³ï¼Œä½¿è‡ªç„¶è¯­è¨€å¤„ç†å’Œç”Ÿæˆèƒ½åŠ›è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„æ°´å¹³ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿä¹Ÿå¼•å…¥äº†å‰æ‰€æœªæœ‰çš„å®‰å…¨æ¼æ´ï¼Œè¿™äº›æ¼æ´è¶…å‡ºäº†ä¼ ç»Ÿçš„æç¤ºæ³¨å…¥æ”»å‡»çš„èŒƒå›´ã€‚æœ¬æ–‡é’ˆå¯¹LLMä»£ç†ä½œä¸ºæ”»å‡»å‘é‡è¿›è¡Œäº†é¦–æ¬¡å…¨é¢è¯„ä¼°ï¼Œè¿™äº›æ”»å‡»å‘é‡èƒ½å¤Ÿé€šè¿‡åˆ©ç”¨ä»£ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿå†…çš„ä¿¡ä»»è¾¹ç•Œï¼Œå®ç°è®¡ç®—æœºç³»ç»Ÿçš„å®Œå…¨æ¥ç®¡ï¼Œåœ¨è¿™äº›ç³»ç»Ÿä¸­ï¼Œè‡ªä¸»å®ä½“ç›¸äº’äº¤äº’å’Œå½±å“ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ•Œäººå¯ä»¥åˆ©ç”¨ä¸‰ç§ä¸åŒçš„æ”»å‡»é¢â€”â€”ç›´æ¥æç¤ºæ³¨å…¥ã€RAGåé—¨æ”»å‡»å’Œè·¨ä»£ç†ä¿¡ä»»æ¼æ´â€”â€”æ¥è¿«ä½¿æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oã€Claude-4å’ŒGemini-2.5ï¼‰åœ¨å—å®³æœºå™¨ä¸Šè‡ªä¸»å®‰è£…å’Œæ‰§è¡Œæ¶æ„è½¯ä»¶ã€‚æˆ‘ä»¬å¯¹17ç§æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°äº†ä¸€ä¸ªä»¤äººè­¦è§‰çš„æ¼æ´å±‚æ¬¡ç»“æ„ï¼šè™½ç„¶41.2%çš„æ¨¡å‹ä¼šå—åˆ°ç›´æ¥æç¤ºæ³¨å…¥çš„å½±å“ï¼Œä½†52.9%çš„æ¨¡å‹å®¹æ˜“å—åˆ°RAGåé—¨æ”»å‡»ï¼Œé«˜è¾¾82.4%çš„æ¨¡å‹å¯ä»¥é€šè¿‡è·¨ä»£ç†ä¿¡ä»»æ¼æ´è¢«åˆ©ç”¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é‚£äº›æˆåŠŸæŠµæŠ—ç›´æ¥æ¶æ„å‘½ä»¤çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼šåœ¨æ”¶åˆ°åŒè¡Œä»£ç†çš„è¯·æ±‚æ—¶æ‰§è¡Œç›¸åŒçš„è½½è·ï¼Œè¿™æ­ç¤ºäº†å½“å‰å¤šä»£ç†å®‰å…¨æ¨¡å‹ä¸­çš„åŸºæœ¬ç¼ºé™·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåªæœ‰5.9%çš„æµ‹è¯•æ¨¡å‹ï¼ˆå³åä¸ƒåˆ†ä¹‹ä¸€ï¼‰èƒ½å¤ŸæŠµæŠ—æ‰€æœ‰æ”»å‡»å‘é‡ï¼Œå¤§å¤šæ•°æ¨¡å‹è¡¨ç°å‡ºä¾èµ–äºä¸Šä¸‹æ–‡çš„å®‰å…¨è¡Œä¸ºï¼Œè¿™ä¼šäº§ç”Ÿå¯åˆ©ç”¨çš„ç›²ç‚¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜å¼ºè°ƒäº†æé«˜äººä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨é£é™©çš„è®¤è¯†å’Œç ”ç©¶çš„å¿…è¦æ€§ï¼Œæ˜¾ç¤ºå‡ºç½‘ç»œå®‰å…¨å¨èƒçš„è½¬å˜ï¼Œäººå·¥æ™ºèƒ½å·¥å…·æœ¬èº«æˆä¸ºå¤æ‚çš„æ”»å‡»å‘é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06850v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>LLMä»£ç†äººå’Œç³»ç»Ÿä»¥å…¶å¯¹è‡ªç„¶è¯­è¨€å¤„ç†åŠç”Ÿæˆçš„çªç ´èƒ½åŠ›å¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚ä¸è¿‡è¿™äº›ç³»ç»Ÿå­˜åœ¨ç€ç©ºå‰åºå¤§çš„å®‰å…¨æ¼æ´éšæ‚£ï¼Œä¼ ç»ŸåŸºäºæç¤ºæ³¨å…¥çš„æ”»å‡»æ–¹å¼åªæ˜¯å†°å±±ä¸€è§’ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢è¯„ä¼°äº†LLMä»£ç†äººä½œä¸ºæ”»å‡»åª’ä»‹çš„èƒ½åŠ›ï¼Œæ”»å‡»è€…èƒ½å¤Ÿé€šè¿‡ä¿¡ä»»è¾¹ç•Œæ¼æ´æ“çºµæ™ºèƒ½ä½“AIç³»ç»Ÿå†…çš„è‡ªä¸»å®ä½“ï¼Œä»è€Œå®ç°å¯¹è®¡ç®—æœºçš„å®Œå…¨æ¥ç®¡ã€‚é€šè¿‡æ¼”ç¤ºä¸‰ç§ç‹¬ç‰¹æ”»å‡»é€”å¾„ï¼šç›´æ¥æç¤ºæ³¨å…¥ã€RAGåé—¨æ”»å‡»å’Œè·¨ä»£ç†ä¿¡ä»»å‰¥å‰Šï¼Œæˆ‘ä»¬éªŒè¯äº†æµè¡Œçš„LLMæ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oã€Claude-4å’ŒGemini-2.5ï¼‰èƒ½å¤Ÿåœ¨å—å®³æœºå™¨ä¸Šè‡ªä¸»å®‰è£…å¹¶æ‰§è¡Œæ¶æ„è½¯ä»¶ã€‚å¯¹æœ€æ–°å‰æ²¿çš„17æ¬¾LLMçš„è¯„ä¼°ç»“æœæ­ç¤ºä»¤äººè­¦æƒ•çš„æ¼æ´å±‚æ¬¡åˆ†å¸ƒï¼šå¤§çº¦æœ‰å››æˆæ¨¡å‹ä¼šå—åˆ°ç›´æ¥æç¤ºæ³¨å…¥çš„å½±å“ï¼Œè¿‡åŠä¼šé­é‡åˆ°RAGåé—¨æ”»å‡»ï¼Œæ›´ä»¤äººæ‹…å¿§çš„æ˜¯è¶…è¿‡å…«æˆçš„æ¨¡å‹ä¼šè¢«è·¨ä»£ç†ä¿¡ä»»å‰¥å‰Šã€‚å°½ç®¡éƒ¨åˆ†LLMèƒ½æŠµæŠ—ç›´æ¥çš„æ¶æ„æŒ‡ä»¤ï¼Œä½†å®ƒä»¬ä¼šåœ¨æ¥æ”¶åˆ°å…¶ä»–æ™ºèƒ½ä½“è¯·æ±‚æ—¶æ‰§è¡Œç›¸åŒçš„æ¶æ„è½½è·ä»»åŠ¡ï¼Œæ­ç¤ºå‡ºå½“å‰å¤šæ™ºèƒ½ä½“å®‰å…¨æ¨¡å‹çš„åŸºæœ¬ç¼ºé™·ã€‚ä»…æœ‰ä¸åˆ°ä¸€æˆï¼ˆä¸åˆ°æ¯åä¸ªä¸­çš„ä¸€æ¬¾ï¼‰çš„æ¨¡å‹èƒ½æŠµå¾¡æ‰€æœ‰æ”»å‡»åª’ä»‹ã€‚å¤§éƒ¨åˆ†æ¨¡å‹çš„ä¿æŠ¤å­˜åœ¨ä¸Šä¸‹æ–‡çš„ä¾èµ–å› ç´ ï¼Œè¿™ç§ç‰¹å®šçš„è¡¨ç°æ„æˆäº†æ˜“è¢«å¿½è§†çš„æ¼æ´ç›²ç‚¹ã€‚å‘ç°åŒæ—¶ä¹Ÿåæ˜ å‡ºéœ€è¦å¯¹LLMçš„å®‰å…¨é£é™©æœ‰æ›´é«˜æ„è¯†ä¸ç ”ç©¶å…³æ³¨ï¼Œè¿™ä»£è¡¨ç€ç½‘ç»œå®‰å…¨å¨èƒçš„è½¬å˜â€”â€”AIå·¥å…·æœ¬èº«æˆä¸ºäº†é«˜æ˜çš„æ”»å‡»åª’ä»‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>LLMä»£ç†å’Œç³»ç»Ÿå¼•å…¥äº†å‰æ‰€æœªæœ‰çš„å®‰å…¨æ¼æ´éšæ‚£ã€‚</li>
<li>å‘ç°äº†ä¸‰ç§æ”»å‡»é€”å¾„ï¼šç›´æ¥æç¤ºæ³¨å…¥ã€RAGåé—¨æ”»å‡»å’Œè·¨ä»£ç†ä¿¡ä»»å‰¥å‰Šã€‚</li>
<li>å¯¹æœ€æ–°å‰æ²¿çš„LLMæ¨¡å‹çš„å®‰å…¨è¯„ä¼°æ˜¾ç¤ºå‡ºäº†é«˜è„†å¼±æ€§ç­‰çº§ï¼Œç»å¤§å¤šæ•°æ˜“å—æ”»å‡»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0f06208d3b4499ee86f914c046f51416.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb30ade488ab8d6950b300d12eb2d276.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c213c1e3cbfce5771a7ea0c75ac6efe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f56a393a613ad5e63e8b550d39f04ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8392915aa8f538a5a9dac2c1342dfdb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fb059459b3c503815875a6f49b5360e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Nexus-Taming-Throughput-Latency-Tradeoff-in-LLM-Serving-via-Efficient-GPU-Sharing"><a href="#Nexus-Taming-Throughput-Latency-Tradeoff-in-LLM-Serving-via-Efficient-GPU-Sharing" class="headerlink" title="Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient   GPU Sharing"></a>Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient   GPU Sharing</h2><p><strong>Authors:Xiaoxiang Shi, Colin Cai, Junjia Du, Zhanda Zhu, Zhihao Jia</strong></p>
<p>Current prefill-decode (PD) disaggregation is typically deployed at the level of entire serving engines, assigning separate GPUs to handle prefill and decode phases. While effective at reducing latency, this approach demands more hardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode requests within the same batch, but introduces phase interference between prefill and decode.   While existing PD disaggregation solutions separate the phases across GPUs, we ask: can the same decoupling be achieved within a single serving engine? The key challenge lies in managing the conflicting resource requirements of prefill and decode when they share the same hardware. In this paper, we first show that chunked prefill requests cause interference with decode requests due to their distinct requirements for GPU resources. Second, we find that GPU resources exhibit diminishing returns. Beyond a saturation point, increasing GPU allocation yields negligible latency improvements. This insight enables us to split a single GPUâ€™s resources and dynamically allocate them to prefill and decode on the fly, effectively disaggregating the two phases within the same GPU.   Across a range of models and workloads, our system Nexus achieves up to 2.2x higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also outperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x lower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using only half the number of GPUs. </p>
<blockquote>
<p>å½“å‰çš„prefill-decodeï¼ˆPDï¼‰è§£èšé€šå¸¸åœ¨æ•´ä¸ªæœåŠ¡å¼•æ“çº§åˆ«è¿›è¡Œéƒ¨ç½²ï¼Œå°†å•ç‹¬çš„GPUåˆ†é…ç»™prefillå’Œdecodeé˜¶æ®µã€‚è™½ç„¶è¿™ç§æ–¹æ³•åœ¨å‡å°‘å»¶è¿Ÿæ–¹é¢å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒéœ€è¦æ›´å¤šçš„ç¡¬ä»¶ã€‚ä¸ºäº†æå‡GPUåˆ©ç”¨ç‡ï¼ŒChunked Prefillå°†prefillå’Œdecodeè¯·æ±‚æ··åˆåœ¨åŒä¸€æ‰¹æ¬¡ä¸­ï¼Œä½†ä¼šåœ¨prefillå’Œdecodeä¹‹é—´å¼•å…¥é˜¶æ®µå¹²æ‰°ã€‚è™½ç„¶ç°æœ‰çš„PDè§£èšè§£å†³æ–¹æ¡ˆåœ¨GPUä¹‹é—´åˆ†ç¦»é˜¶æ®µï¼Œæˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šæ˜¯å¦å¯ä»¥åœ¨å•ä¸ªæœåŠ¡å¼•æ“ä¸­å®ç°ç›¸åŒçš„è§£è€¦ï¼Ÿå…³é”®æŒ‘æˆ˜åœ¨äºç®¡ç†prefillå’Œdecodeåœ¨å…±äº«ç›¸åŒç¡¬ä»¶æ—¶çš„å†²çªèµ„æºéœ€æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¡¨æ˜ç”±äºGPUèµ„æºçš„ä¸åŒéœ€æ±‚ï¼Œåˆ†å—çš„prefillè¯·æ±‚ä¼šå¯¹decodeè¯·æ±‚é€ æˆå¹²æ‰°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘ç°GPUèµ„æºè¡¨ç°å‡ºæ”¶ç›Šé€’å‡çš„ç°è±¡ã€‚è¶…è¿‡é¥±å’Œç‚¹åï¼Œå¢åŠ GPUåˆ†é…å¯¹å»¶è¿Ÿçš„æ”¹è¿›å¾®ä¹å…¶å¾®ã€‚è¿™ä¸€è§è§£ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ‹†åˆ†å•ä¸ªGPUçš„èµ„æºï¼Œå¹¶åŠ¨æ€åœ°å®æ—¶å°†å®ƒä»¬åˆ†é…ç»™prefillå’Œdecodeï¼Œæœ‰æ•ˆåœ°åœ¨åŒä¸€GPUå†…è§£èšä¸¤ä¸ªé˜¶æ®µã€‚åœ¨æˆ‘ä»¬çš„ç³»ç»ŸNexusä¸­ï¼Œä¸vLLMç›¸æ¯”ï¼Œå®ƒå®ç°äº†é«˜è¾¾2.2å€çš„ååé‡ï¼ŒTTFé™ä½äº†20å€ï¼ŒTBTé™ä½äº†2.5å€ã€‚å®ƒè¿˜ä¼˜äºSGLangï¼Œååé‡æé«˜äº†ä¸¤å€ï¼ŒTTFé™ä½äº†ä¸¤å€ï¼ŒTBTé™ä½äº†1.7å€ï¼Œå¹¶ä¸”ä½¿ç”¨åªæœ‰ä¸€åŠçš„GPUæ•°é‡å°±å®ç°äº†æ¯”vLLM-è§£èšæ›´é«˜çš„1.4å€ååé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06608v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å½“å‰é¢„å¡«å……è§£ç ï¼ˆPDï¼‰å»èšåˆæ–¹æ³•çš„é—®é¢˜ï¼Œå³åœ¨ç¡¬ä»¶èµ„æºåˆ†é…ä¸Šå­˜åœ¨çš„å±€é™æ€§ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªæœåŠ¡å¼•æ“å†…å®ç°è§£è€¦ï¼Œä»è€Œæé«˜GPUåˆ©ç”¨ç‡ã€‚ç ”ç©¶å‘ç°ï¼Œåˆ†å—é¢„å¡«å……è¯·æ±‚ä¸è§£ç è¯·æ±‚ä¹‹é—´å­˜åœ¨èµ„æºå†²çªé—®é¢˜ï¼ŒåŒæ—¶GPUèµ„æºå‘ˆç°æ”¶ç›Šé€’å‡çš„ç°è±¡ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€åˆ†é…GPUèµ„æºçš„æ–¹æ³•ï¼Œå®ç°åœ¨å•ä¸ªGPUå†…å¯¹é¢„å¡«å……å’Œè§£ç ä¸¤ä¸ªé˜¶æ®µçš„å»èšåˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæé«˜ç³»ç»Ÿæ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–ç³»ç»Ÿï¼Œå…·æœ‰æ›´é«˜çš„ååé‡å’Œæ›´ä½çš„å»¶è¿Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰é¢„å¡«å……è§£ç å»èšåˆæ–¹æ³•ä¸»è¦åœ¨æ•´ä¸ªæœåŠ¡å¼•æ“å±‚é¢éƒ¨ç½²ï¼Œé€šè¿‡åˆ†é…å•ç‹¬çš„GPUæ¥å¤„ç†é¢„å¡«å……å’Œè§£ç é˜¶æ®µï¼Œè™½ç„¶é™ä½äº†å»¶è¿Ÿï¼Œä½†ç¡¬ä»¶éœ€æ±‚è¾ƒé«˜ã€‚</li>
<li>åˆ†å—é¢„å¡«å……æ–¹æ³•è™½ç„¶æé«˜äº†GPUåˆ©ç”¨ç‡ï¼Œä½†é¢„å¡«å……å’Œè§£ç è¯·æ±‚ä¹‹é—´å­˜åœ¨ç›¸ä½å¹²æ‰°é—®é¢˜ã€‚</li>
<li>åœ¨å•ä¸ªæœåŠ¡å¼•æ“å†…å®ç°è§£è€¦çš„å…³é”®æŒ‘æˆ˜åœ¨äºç®¡ç†é¢„å¡«å……å’Œè§£ç åœ¨å…±äº«ç¡¬ä»¶æ—¶çš„å†²çªèµ„æºéœ€æ±‚ã€‚</li>
<li>åˆ†å—é¢„å¡«å……è¯·æ±‚å› å¯¹GPUèµ„æºçš„ç‹¬ç‰¹éœ€æ±‚è€Œä¸è§£ç è¯·æ±‚äº§ç”Ÿå¹²æ‰°ã€‚</li>
<li>GPUèµ„æºå‘ˆç°æ”¶ç›Šé€’å‡çš„ç°è±¡ï¼Œè¶…è¿‡é¥±å’Œç‚¹åï¼Œå¢åŠ GPUåˆ†é…å¯¹å»¶è¿Ÿæ”¹è¿›ç”šå¾®ã€‚</li>
<li>é€šè¿‡åŠ¨æ€åˆ†é…GPUèµ„æºï¼Œå®ç°åœ¨å•ä¸ªGPUå†…å¯¹é¢„å¡«å……å’Œè§£ç ä¸¤ä¸ªé˜¶æ®µçš„å»èšåˆï¼Œæé«˜äº†ç³»ç»Ÿæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bef06f72306e3e597043f4c9f9c8bfe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad3368b94c8dcee50d6794ec25028d37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8b3b8926e463211e2fe7096c469d908.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f84c267b8c48c01947cbb059916daaaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c743c37de23affc1eb383eea2d9e4c7d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Latent-Reasoning"><a href="#A-Survey-on-Latent-Reasoning" class="headerlink" title="A Survey on Latent Reasoning"></a>A Survey on Latent Reasoning</h2><p><strong>Authors:Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, Jason Eshraghian</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the modelâ€™s expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the modelâ€™s continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: <a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/LatentCoT-Horizon/">https://github.com/multimodal-art-projection/LatentCoT-Horizon/</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å½“å—åˆ°æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å¼•å¯¼æ—¶ï¼Œèƒ½å¤Ÿå£å¤´è¡¨è¾¾ä¸­é—´æ­¥éª¤ã€‚è™½ç„¶æ€ç»´é“¾æé«˜äº†å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œä½†å®ƒå¯¹è‡ªç„¶è¯­è¨€æ¨ç†çš„ä¾èµ–é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾å¸¦å®½ã€‚æ½œåœ¨æ¨ç†é€šè¿‡å®Œå…¨åœ¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†æ¥è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæ¶ˆé™¤äº†ä»¤ç‰Œçº§åˆ«çš„ç›‘ç£ã€‚ä¸ºäº†æ¨åŠ¨æ½œåœ¨æ¨ç†ç ”ç©¶çš„å‘å±•ï¼Œè¿™ç¯‡ç»¼è¿°å¯¹æ–°å…´çš„é¢†åŸŸâ€”â€”æ½œåœ¨æ¨ç†è¿›è¡Œäº†å…¨é¢çš„æ¦‚è¿°ã€‚æˆ‘ä»¬é¦–å…ˆç ”ç©¶ç¥ç»ç½‘ç»œå±‚ä½œä¸ºæ¨ç†è®¡ç®—åŸºç¡€çš„ä½œç”¨ï¼Œå¼ºè°ƒåˆ†å±‚è¡¨ç¤ºå¦‚ä½•æ”¯æŒå¤æ‚è½¬æ¢ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¿€æ´»çš„å¤å‘ã€éšè—çŠ¶æ€ä¼ æ’­ï¼Œä»¥åŠå‹ç¼©æˆ–å†…åŒ–æ˜¾æ€§æ¨ç†ç—•è¿¹çš„ç²¾è°ƒç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†é€šè¿‡æ©è†œæ‰©æ•£æ¨¡å‹å®ç°æ— é™æ·±åº¦æ½œåœ¨æ¨ç†ç­‰å…ˆè¿›èŒƒå¼ï¼Œè¿™èƒ½å¤Ÿæ”¯æŒå…¨å±€ä¸€è‡´æ€§å’Œå¯é€†çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ç»Ÿä¸€è¿™äº›è§‚ç‚¹ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¾„æ¸…æ½œåœ¨æ¨ç†çš„æ¦‚å¿µæ™¯è§‚ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è®¤çŸ¥å‰æ²¿çš„ç ”ç©¶ç»˜åˆ¶æœªæ¥æ–¹å‘ã€‚ç›¸å…³çš„GitHubä»“åº“ï¼Œæ”¶é›†æœ€æ–°çš„è®ºæ–‡å’Œä»“åº“èµ„æºï¼Œå¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/LatentCoT-Horizon/">https://github.com/multimodal-art-projection/LatentCoT-Horizon/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06203v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å±•ç¤ºå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™ç§æ¨ç†æ–¹å¼èƒ½å¤Ÿå£å¤´è¡¨è¾¾ä¸­é—´æ­¥éª¤ã€‚è™½ç„¶CoTæé«˜äº†å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œä½†å®ƒå¯¹è‡ªç„¶è¯­è¨€æ¨ç†çš„ä¾èµ–é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾å¸¦å®½ã€‚æ½œåœ¨æ¨ç†é€šè¿‡å®Œå…¨åœ¨æ¨¡å‹çš„è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†æ¥è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œä»è€Œæ¶ˆé™¤äº†ä»¤ç‰Œçº§åˆ«çš„ç›‘ç£ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†æ½œåœ¨æ¨ç†è¿™ä¸€æ–°å…´é¢†åŸŸçš„å‘å±•ã€‚æ–‡ç« é¦–å…ˆç ”ç©¶ç¥ç»ç½‘ç»œå±‚ä½œä¸ºæ¨ç†è®¡ç®—åŸºåº•çš„ä½œç”¨ï¼Œå¼ºè°ƒåˆ†å±‚è¡¨ç¤ºå¦‚ä½•æ”¯æŒå¤æ‚è½¬æ¢ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¿€æ´»çš„å¤å‘ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠå¾®è°ƒç­–ç•¥ç­‰ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥å‹ç¼©æˆ–å†…åŒ–æ˜¾æ€§æ¨ç†ç—•è¿¹ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†é€šè¿‡æ©æ¨¡æ‰©æ•£æ¨¡å‹å®ç°æ— é™æ·±åº¦æ½œåœ¨æ¨ç†ç­‰å…ˆè¿›èŒƒå¼ï¼Œè¿™äº›èŒƒå¼å¯å®ç°å…¨å±€ä¸€è‡´ä¸”å¯é€†çš„æ¨ç†è¿‡ç¨‹ã€‚æœ¬æ–‡æ—¨åœ¨ç»Ÿä¸€è¿™äº›è§‚ç‚¹ï¼Œå˜æ¸…æ½œåœ¨æ¨ç†çš„æ¦‚å¿µæ™¯è§‚ï¼Œå¹¶ç»˜åˆ¶LLMè®¤çŸ¥é¢†åŸŸå‰æ²¿çš„ç ”ç©¶æœªæ¥æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡é€šè¿‡æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œè¿™å¢å¼ºäº†å…¶å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æ½œåœ¨æ¨ç†æ–¹æ³•æ—¨åœ¨è§£å†³åœ¨è¿ç»­éšè—çŠ¶æ€ä¸­è¿›è¡Œå¤šæ­¥æ¨ç†çš„é—®é¢˜ï¼Œæ¶ˆé™¤å¯¹ä»¤ç‰Œçº§åˆ«ç›‘ç£çš„ä¾èµ–ã€‚</li>
<li>ç¥ç»ç½‘ç»œå±‚åœ¨æ½œåœ¨æ¨ç†ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œåˆ†å±‚è¡¨ç¤ºæ”¯æŒå¤æ‚è½¬æ¢ã€‚</li>
<li>å­˜åœ¨å¤šç§æ½œåœ¨æ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¿€æ´»çš„å¤å‘ã€éšè—çŠ¶æ€ä¼ æ’­ä»¥åŠå¾®è°ƒç­–ç•¥ç­‰ã€‚</li>
<li>å…ˆè¿›çš„èŒƒå¼å¦‚é€šè¿‡æ©æ¨¡æ‰©æ•£æ¨¡å‹å®ç°æ— é™æ·±åº¦æ½œåœ¨æ¨ç†ï¼Œå®ç°å…¨å±€ä¸€è‡´ä¸”å¯é€†çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†æ½œåœ¨æ¨ç†çš„æœ€æ–°å‘å±•å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69633fab06a62cf6a9509e59a3392aa1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-800810ee39ef683322f1d02ed49e3ffc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Skywork-R1V3-Technical-Report"><a href="#Skywork-R1V3-Technical-Report" class="headerlink" title="Skywork-R1V3 Technical Report"></a>Skywork-R1V3 Technical Report</h2><p><strong>Authors:Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Jianhao Zhang, Yahui Zhou</strong></p>
<p>We introduce Skywork-R1V3, an advanced, open-source vision-language model (VLM) that pioneers a new approach to visual reasoning. Its key innovation lies in effectively transferring reasoning skills from text-only Large Language Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily stems from our elaborate post-training RL framework, which effectively activates and enhances the modelâ€™s reasoning ability, without the need for additional continue pre-training. Through this framework, we further uncover the fundamental role of the connector module in achieving robust cross-modal alignment for multimodal reasoning models. In addition, we introduce a unique indicator of reasoning capability, the entropy of critical reasoning tokens, which has proven highly effective for checkpoint selection during RL training. Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%. This performance matches entry-level human capabilities. Remarkably, our RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs. The implementation successfully transfers mathematical reasoning to other subject-related reasoning tasks. We also include an analysis of curriculum learning and reinforcement finetuning strategies, along with a broader discussion on multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Skywork-R1V3ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå¼€åˆ›äº†ä¸€ç§æ–°çš„è§†è§‰æ¨ç†æ–¹æ³•ã€‚å…¶ä¸»è¦åˆ›æ–°åœ¨äºæœ‰æ•ˆåœ°å°†ä»æ–‡æœ¬ä¸­è·å¾—çš„æ¨ç†èƒ½åŠ›ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚Skywork-R1V3çš„å‡ºè‰²æ€§èƒ½ä¸»è¦æºäºæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ¿€æ´»å¹¶å¢å¼ºäº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€é¢å¤–çš„ç»§ç»­é¢„è®­ç»ƒã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°äº†è¿æ¥å™¨æ¨¡å—åœ¨å®ç°ç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ä¸­çš„åŸºæœ¬ä½œç”¨ï¼Œè¿™å¯¹äºå¤šæ¨¡æ€æ¨ç†æ¨¡å‹è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‹¬ç‰¹çš„æ¨ç†èƒ½åŠ›æŒ‡æ ‡â€”â€”å…³é”®æ¨ç†ä»¤ç‰Œçš„ç†µï¼Œè¿™å·²è¢«è¯æ˜åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ£€æŸ¥ç‚¹é€‰æ‹©ä¸­éå¸¸æœ‰æ•ˆã€‚Skywork-R1V3åœ¨MMMUä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œä»64.3%æ˜¾è‘—æé«˜åˆ°76.0%ï¼Œä¸äººç±»å…¥é—¨çº§èƒ½åŠ›ç›¸åŒ¹é…ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒæ–¹æ³•ç”šè‡³ä½¿38Bå‚æ•°æ¨¡å‹èƒ½å¤Ÿä¸é¡¶çº§é—­æºVLMç›¸åŒ¹æ•Œã€‚è¯¥å®ç°æˆåŠŸåœ°å°†åœ¨æ•°å­¦æ¨ç†è½¬ç§»åˆ°å…¶ä»–ç›¸å…³æ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è¯¾ç¨‹å­¦ä¹ å’Œå¼ºåŒ–å¾®è°ƒç­–ç•¥ï¼Œå¹¶å¯¹å¤šæ¨¡æ€æ¨ç†è¿›è¡Œäº†æ›´å¹¿æ³›çš„è®¨è®ºã€‚Skywork-R1V ä»£è¡¨äº†å¤šæ¨¡æ€æ¨ç†çš„é‡å¤§é£è·ƒï¼Œå±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä½œä¸ºæ¨åŠ¨å¼€æºVLMèƒ½åŠ›å‘å±•çš„å¼ºå¤§å¼•æ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06167v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Skywork-R1V3æ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå°†æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æœ‰æ•ˆåœ°è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºé‡‡ç”¨ç²¾ç»†çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒåä¼˜åŒ–ï¼Œæ— éœ€é¢å¤–çš„æŒç»­é¢„è®­ç»ƒå³å¯æ¿€æ´»å’Œæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚Skywork-R1V3å®ç°äº†å…ˆè¿›çš„å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨MMMUæµ‹è¯•ä¸­ä»64.3%æå‡è‡³76.0%ï¼Œä¸äººç±»å…¥é—¨çº§èƒ½åŠ›ç›¸å½“ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æˆåŠŸåœ°å°†æ•°å­¦æ¨ç†èƒ½åŠ›åº”ç”¨äºå…¶ä»–ç›¸å…³æ¨ç†ä»»åŠ¡ã€‚RLé©±åŠ¨çš„åè®­ç»ƒæ–¹æ³•ä½¿è¯¥æ¨¡å‹èƒ½åœ¨è§„æ¨¡è¾ƒå°ï¼ˆå‚æ•°è¾¾æ•°åäº¿çº§åˆ«ï¼‰çš„æƒ…å†µä¸‹åª²ç¾é¡¶å°–çš„å•†ä¸šæ€§VLMæ¨¡å‹ã€‚æˆ‘ä»¬æ¢è®¨äº†æ•™æ¡ˆå­¦ä¹ ä¸å¼ºåŒ–å¾®è°ƒç­–ç•¥çš„åˆ†æä»¥åŠå¤šæ¨¡æ€æ¨ç†çš„æ›´å¹¿æ³›è®¨è®ºã€‚Skywork-R1V3ä»£è¡¨äº†å¤šæ¨¡æ€æ¨ç†é¢†åŸŸçš„é‡å¤§çªç ´ï¼Œå¹¶å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ä½œä¸ºæ¨åŠ¨å¼€æºVLMèƒ½åŠ›çš„å¼ºå¤§å¼•æ“ã€‚Skywork-R1Våœ¨ä¼—å¤šåœºæ™¯ä¸­éƒ½èƒ½å–å¾—è¾ƒå¥½çš„è¡¨ç°ã€‚æ¯”å¦‚é‡‘èäº¤æ˜“å‘˜çš„åˆ†æå¸‚åœºæ•°æ®å’Œåˆ†æè‡ªç„¶é£æ™¯æ‘„å½±çš„ä½œå“å¯ä»¥é€‰ç”¨å®ƒè¿›è¡Œé«˜æ•ˆæ¨ç†å¤„ç†ã€‚æ€»ä½“æ¥è¯´ï¼ŒSkywork-R1Vä»£è¡¨äº†æ–°ä¸€ä»£çš„æ™ºèƒ½å†³ç­–è¾…åŠ©å·¥å…·çš„å‡ºç°ã€‚å®ƒçš„æ¨å‡ºä¸ä»…èƒ½å¸®åŠ©æˆ‘ä»¬å¤„ç†å¤æ‚çš„è§†è§‰ä»»åŠ¡ï¼Œè€Œä¸”æœ‰åŠ©äºæˆ‘ä»¬ç†è§£è¯­è¨€ç»“æ„å’Œå†…åœ¨å«ä¹‰ä»¥åŠå®ƒçš„å…·ä½“åº”ç”¨å±‚é¢çš„å…³é”®å˜åŒ–æ–¹å‘å’Œå½±å“å¤§å°è¶‹åŠ¿è¯„ä¼°åˆ†æç­‰é‡è¦æ„ä¹‰æ·±è¿œçš„è¡ŒåŠ¨å’Œä»·å€¼æ‹“å±•çš„å¯èƒ½ä¸ä»·å€¼çš„å°è¯•åˆ¤æ–­å’Œå®ç°é—®é¢˜é¢„åˆ¤å’Œæ“ä½œåè°ƒæ›´åˆç†æœ‰åºçš„è§£å†³è¿‡ç¨‹æ¨è¿›é—®é¢˜åé¦ˆå’Œä¼˜åŒ–æœºåˆ¶å®ç°æ›´å¥½åº”ç”¨æ•ˆæœçš„æå‡ä»¥åŠæ›´é«˜æ•ˆè§£å†³å®é™…åº”ç”¨é—®é¢˜ä¸­é‡åˆ°çš„å„ç§æŒ‘æˆ˜å’Œé—®é¢˜æä¾›æ–°çš„æ€è·¯å’Œå·¥å…·æ”¯æŒä»¥åŠè§£å†³æ–¹å¼æ–¹æ³•çš„å¼€å‘æ”¯æŒå’Œä¿æŠ¤ç­‰æ–¹å¼æœ€ç»ˆå®ç°è¾…åŠ©å·¥å…·æœåŠ¡çš„ä¸“ä¸šåŒ–ä¸æŠ€æœ¯æ‰‹æ®µçš„åˆ›æ–°åŠå…¶å¹¿é˜”çš„å¸‚åœºåº”ç”¨å‰æ™¯é¢„æœŸæ›´å¥½åœ°èµ‹èƒ½äºäººæˆå°±åŒèµ¢ç›®æ ‡çš„å®ç°é•¿è¿œå¥åº·å‘å±•é—­ç¯åŒæ­¥å½±å“å„ç§æ–¹å¼çš„æ”¹å–„åŠå…±äº«æ›´åŠ æ™ºæ…§çš„æœåŠ¡ä¸æ”¯æŒåŠå…¶ä¸šåŠ¡åˆè§„ä¸å®‰å…¨ä¿éšœæœºåˆ¶ä¿ƒè¿›å¯æŒç»­æ€§å‘å±•å’Œç›®æ ‡è½åœ°å¯èƒ½æ€§åœ¨å‡å°‘èƒ½è€—èµ„æºæŸå¤±ç­‰æ™ºèƒ½åŒ–çš„å‘å±•ä¸­å‘æŒ¥å·¨å¤§ä¼˜åŠ¿æä¾›ä»·å€¼ä½“ç°çš„è·¯å¾„æ‹“å±•éœ€æ±‚ä½“ç°çš„éœ€è¦ä¾æ‰˜çš„å…·ä½“ç¯èŠ‚éœ€è¦è¯¦ç»†è§„åˆ’å’Œå¼€å‘å¹¶è¿›è¡Œæ•°æ®çš„å®‰å…¨ä¿å¯†ç­‰æ–¹é¢é‡‡å–åˆ‡å®å¯è¡Œçš„æ–¹æ¡ˆå’ŒæŠ€æœ¯æªæ–½æ”¯æ’‘è¡Œä¸šé«˜è´¨é‡å‘å±•çš„è¿«åˆ‡éœ€æ±‚ç­‰ç›¸å…³å†…å®¹è¡¨è¾¾ä¸°å¯Œå…·ä½“è¯¦å°½å®Œæ•´é€»è¾‘æ¸…æ™°å…·æœ‰ç°å®æ„ä¹‰å’Œç¤¾ä¼šä»·å€¼å¯¹äºæœªæ¥ç§‘æŠ€å‘å±•è¶‹åŠ¿å’Œäººç±»ç¤¾ä¼šè¿›æ­¥å…·æœ‰é‡è¦çš„æ¨åŠ¨å’Œä¿ƒè¿›ä½œç”¨<strong>Key Takeaways</strong></p>
<ol>
<li>Skywork-R1V3æ˜¯ä¸€ä¸ªå…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œèƒ½å¤Ÿå°†æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°è§†è§‰ä»»åŠ¡ä¸Šã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒåä¼˜åŒ–ï¼Œæ— éœ€é¢å¤–çš„æŒç»­é¢„è®­ç»ƒå³å¯æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Skywork-R1V3å®ç°äº†å…ˆè¿›çš„å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œåœ¨MMMUæµ‹è¯•ä¸­æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
<li>æ¨¡å‹å…·å¤‡æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œå¹¶èƒ½åº”ç”¨äºå…¶ä»–ç›¸å…³æ¨ç†ä»»åŠ¡ã€‚</li>
<li>RLé©±åŠ¨çš„åè®­ç»ƒæ–¹æ³•ä½¿è¯¥æ¨¡å‹åœ¨è¾ƒå°è§„æ¨¡ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œåª²ç¾é¡¶å°–çš„å•†ä¸šæ€§VLMæ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åˆ†æåŒ…æ‹¬æ•™æ¡ˆå­¦ä¹ ä¸å¼ºåŒ–å¾®è°ƒç­–ç•¥çš„è®¨è®ºï¼Œä»¥åŠæ›´å¹¿æ³›çš„å¤šæ¨¡æ€æ¨ç†æ¢è®¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1f68b8782ffae6493c9eb36bbed2ce46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6828544e25f077d8f088b10a12f9b2dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2da3aa29206d112763a07ee22e77fa36.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Theory-of-Inference-Compute-Scaling-Reasoning-through-Directed-Stochastic-Skill-Search"><a href="#A-Theory-of-Inference-Compute-Scaling-Reasoning-through-Directed-Stochastic-Skill-Search" class="headerlink" title="A Theory of Inference Compute Scaling: Reasoning through Directed   Stochastic Skill Search"></a>A Theory of Inference Compute Scaling: Reasoning through Directed   Stochastic Skill Search</h2><p><strong>Authors:Austin R. Ellis-Mohr, Anuj K. Nayak, Lav R. Varshney</strong></p>
<p>Large language models (LLMs) demand considerable computational, energy, and financial resources during both training and deployment. While scaling laws for training have guided much of the fieldâ€™s recent progress, inference costs now represent a significant and growing component of the overall resource burden, particularly for reasoning-focused models. Existing characterizations of compute-optimality that consider model size, dataset size, and inference tokens in isolation or in fixed combinations risk overlooking more efficient operating points. We introduce directed stochastic skill search (DS3), a general framework that represents inference as stochastic traversal over a learned skill graph. From a simplified yet expressive instantiation, we derive closed-form expressions for task success and compute cost across a wide range of inference strategies â€“ including chain-of-thought (CoT) and tree-of-thought (ToT) â€“ enabling comparative analysis as a function of task difficulty and model capability. To that end, we extend a prior first-principles tripartite graph framework of LLM training to incorporate inference, and separately bridge DS3 with empirical methods that characterize LLM scaling behavior. We theoretically recover empirically observed patterns, including: linear accuracy scaling with logarithmic compute; variation in preferred inference strategies as a function of task difficulty and model capability; emergent behavior elicited by reasoning even when performance plateaus under parameter scaling; and both best-of-N (BoN) and majority voting behavior captured within a unified analytical framework. By explicitly characterizing training-inference interdependencies, our framework deepens theoretical understanding and supports principled algorithmic design and resource allocation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒå’Œéƒ¨ç½²è¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„è®¡ç®—ã€èƒ½æºå’Œè´¢åŠ¡èµ„æºã€‚è™½ç„¶è®­ç»ƒçš„å¯æ‰©å±•æ€§å®šå¾‹æŒ‡å¯¼äº†è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä½†æ¨ç†æˆæœ¬ç°åœ¨å·²æˆä¸ºæ€»ä½“èµ„æºè´Ÿæ‹…çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ¨¡å‹ã€‚ç°æœ‰çš„è®¡ç®—æœ€ä¼˜æ€§çš„è¡¨å¾ï¼Œåœ¨è€ƒè™‘æ¨¡å‹å¤§å°ã€æ•°æ®é›†å¤§å°å’Œæ¨ç†ä»¤ç‰Œæ—¶å¤„äºå­¤ç«‹çŠ¶æ€æˆ–ä»¥å›ºå®šç»„åˆå­˜åœ¨ï¼Œè¿™å¯èƒ½ä¼šå¿½è§†æ›´æœ‰æ•ˆçš„æ“ä½œç‚¹ã€‚æˆ‘ä»¬å¼•å…¥äº†æœ‰å‘éšæœºæŠ€èƒ½æœç´¢ï¼ˆDS3ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå®ƒå°†æ¨ç†è¡¨ç¤ºä¸ºåœ¨å­¦åˆ°çš„æŠ€èƒ½å›¾ä¸Šçš„éšæœºéå†ã€‚ä»ä¸€ä¸ªç®€åŒ–è€Œå¯Œæœ‰è¡¨ç°åŠ›çš„å®ä¾‹åŒ–å‡ºå‘ï¼Œæˆ‘ä»¬å¾—å‡ºäº†è·¨å„ç§æ¨ç†ç­–ç•¥çš„å°é—­å½¢å¼è¡¨è¾¾å¼ï¼Œè¿™äº›ç­–ç•¥åŒ…æ‹¬æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œæ€ç»´æ ‘ï¼ˆToTï¼‰ï¼Œå¹¶æ ¹æ®ä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›è¿›è¡Œæ¯”è¾ƒåˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å…ˆå‰å…³äºLLMè®­ç»ƒçš„ç¬¬ä¸€æ€§åŸç†ä¸‰æ–¹å›¾æ¡†æ¶æ‰©å±•åˆ°æ¨ç†ï¼Œå¹¶å°†DS3ä¸åˆ»ç”»LLMæ‰©å±•è¡Œä¸ºçš„å®è¯æ–¹æ³•ç›¸è”ç³»ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šé‡æ–°å‘ç°äº†è§‚å¯Ÿåˆ°çš„æ¨¡å¼ï¼ŒåŒ…æ‹¬ï¼šéšç€å¯¹æ•°è®¡ç®—çš„å¢åŠ ï¼Œå‡†ç¡®æ€§å‘ˆçº¿æ€§æ‰©å±•ï¼›éšç€ä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›çš„å˜åŒ–ï¼Œé¦–é€‰æ¨ç†ç­–ç•¥çš„å˜åŒ–ï¼›å³ä½¿åœ¨å‚æ•°æ‰©å±•ä¸‹æ€§èƒ½è¾¾åˆ°ç¨³å®šçŠ¶æ€ï¼Œæ¨ç†ä¹Ÿèƒ½æ¿€å‘å‡ºç°çš„è¡Œä¸ºï¼›ä»¥åŠåœ¨ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ææ¡†æ¶å†…æ•è·çš„æœ€ä½³Nï¼ˆBoNï¼‰å’Œå¤šæ•°æŠ•ç¥¨è¡Œä¸ºã€‚é€šè¿‡æ˜ç¡®åˆ»ç”»è®­ç»ƒä¸æ¨ç†ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ·±åŒ–äº†ç†è®ºç†è§£ï¼Œå¹¶æ”¯æŒæœ‰åŸåˆ™çš„ç®—æ³•è®¾è®¡å’Œèµ„æºåˆ†é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00004v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒå’Œéƒ¨ç½²è¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„è®¡ç®—ã€èƒ½æºå’Œè´¢åŠ¡èµ„æºã€‚å°½ç®¡è®­ç»ƒè§„æ¨¡æ³•åˆ™æŒ‡å¯¼äº†è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä½†æ¨ç†æˆæœ¬ç°åœ¨å·²æˆä¸ºæ•´ä½“èµ„æºè´Ÿæ‹…ä¸­æ˜¾è‘—ä¸”æ—¥ç›Šé‡è¦çš„éƒ¨åˆ†ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ³¨é‡æ¨ç†çš„æ¨¡å‹ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šç”¨æ¡†æ¶â€”â€”å®šå‘éšæœºæŠ€èƒ½æœç´¢ï¼ˆDS3ï¼‰ï¼Œå®ƒå°†æ¨ç†è¡¨ç°ä¸ºå­¦ä¹ æŠ€èƒ½å›¾ä¸Šçš„éšæœºéå†ã€‚é€šè¿‡ç®€æ´è€Œå¯Œæœ‰è¡¨ç°åŠ›çš„å®ä¾‹åŒ–ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†è·¨å„ç§æ¨ç†ç­–ç•¥çš„ä»»åŠ¡æˆåŠŸå’Œè®¡ç®—æˆæœ¬çš„å°é—­å½¢å¼è¡¨è¾¾å¼ï¼ŒåŒ…æ‹¬é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œæ ‘çŠ¶æ€ç»´ï¼ˆToTï¼‰ï¼Œä»¥ä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›ä¸ºå‡½æ•°è¿›è¡Œæ¯”è¾ƒåˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ‰©å±•äº†å…ˆå‰çš„LLMè®­ç»ƒä¸‰æ–¹å›¾æ¡†æ¶ä»¥çº³å…¥æ¨ç†ï¼Œå¹¶å°†DS3ä¸è¡¨å¾LLMè§„æ¨¡è¡Œä¸ºçš„ç»éªŒæ–¹æ³•è¿›è¡Œæ¡¥æ¥ã€‚æˆ‘ä»¬çš„ç†è®ºæ¢å¤äº†è§‚å¯Ÿåˆ°çš„æ¨¡å¼ï¼ŒåŒ…æ‹¬ï¼šéšç€å¯¹æ•°è®¡ç®—çš„å¢åŠ ï¼Œå‡†ç¡®æ€§å‘ˆçº¿æ€§æ‰©å±•ï¼›ä½œä¸ºä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›å‡½æ•°çš„ä¼˜é€‰æ¨ç†ç­–ç•¥çš„å˜åŒ–ï¼›å³ä½¿åœ¨å‚æ•°è§„æ¨¡ä¸‹æ€§èƒ½è¾¾åˆ°å¹³å°æœŸæ—¶ä¹Ÿä¼šå‡ºç°ç”±æ¨ç†å¼•å‘çš„è¡Œä¸ºï¼›ä»¥åŠä¸€ä¸ªç»Ÿä¸€çš„åˆ†ææ¡†æ¶å†…æ•è·çš„æœ€ä½³Nï¼ˆBoNï¼‰å’Œå¤šæ•°æŠ•ç¥¨è¡Œä¸ºã€‚é€šè¿‡æ˜ç¡®è®­ç»ƒæ¨ç†ç›¸äº’ä¾èµ–å…³ç³»ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ·±åŒ–äº†ç†è®ºç†è§£ï¼Œå¹¶æ”¯æŒäº†åŸåˆ™æ€§çš„ç®—æ³•è®¾è®¡å’Œèµ„æºåˆ†é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è®­ç»ƒå’Œéƒ¨ç½²è¿‡ç¨‹ä¸­éœ€è¦å·¨å¤§çš„èµ„æºï¼Œç‰¹åˆ«æ˜¯æ¨ç†é˜¶æ®µçš„æˆæœ¬é€æ¸å¢é•¿ã€‚</li>
<li>å®šå‘éšæœºæŠ€èƒ½æœç´¢ï¼ˆDS3ï¼‰æ¡†æ¶å°†æ¨ç†è¡¨ç¤ºä¸ºå­¦ä¹ æŠ€èƒ½å›¾ä¸Šçš„éšæœºéå†ï¼Œæä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ã€‚</li>
<li>é€šè¿‡ç®€æ´çš„å®ä¾‹åŒ–ï¼Œæ¨å¯¼å‡ºäº†ä»»åŠ¡æˆåŠŸå’Œè®¡ç®—æˆæœ¬çš„å°é—­å½¢å¼è¡¨è¾¾å¼ï¼Œèƒ½åˆ†æä¸åŒæ¨ç†ç­–ç•¥çš„æ•ˆæœã€‚</li>
<li>æ‰©å±•äº†LLMè®­ç»ƒçš„ä¸‰æ–¹å›¾æ¡†æ¶ä»¥åŒ…å«æ¨ç†é˜¶æ®µã€‚</li>
<li>ç†è®ºç¡®è®¤äº†å¤šç§è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§éšè®¡ç®—çš„å¯¹æ•°å¢é•¿è€Œçº¿æ€§æ‰©å±•ã€‚</li>
<li>ä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹èƒ½åŠ›ä¼šå½±å“ä¼˜é€‰çš„æ¨ç†ç­–ç•¥ã€‚</li>
<li>åœ¨å‚æ•°è§„æ¨¡ä¸å˜çš„æƒ…å†µä¸‹ï¼Œæ¨ç†ä¼šå¼•å‘æŸäº›æ–°å…´è¡Œä¸ºï¼Œè€Œæœ€ä½³Nå’Œå¤šæ•°æŠ•ç¥¨è¡Œä¸ºå¯ä»¥åœ¨ç»Ÿä¸€æ¡†æ¶å†…è¢«ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c597f0d56802c54a89bb7989bbc7f382.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0e467ee082f13153bed689fc7c87b15.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Watermarking-Degrades-Alignment-in-Language-Models-Analysis-and-Mitigation"><a href="#Watermarking-Degrades-Alignment-in-Language-Models-Analysis-and-Mitigation" class="headerlink" title="Watermarking Degrades Alignment in Language Models: Analysis and   Mitigation"></a>Watermarking Degrades Alignment in Language Models: Analysis and   Mitigation</h2><p><strong>Authors:Apurv Verma, NhatHai Phan, Shubhendu Trivedi</strong></p>
<p>Watermarking techniques for large language models (LLMs) can significantly impact output quality, yet their effects on truthfulness, safety, and helpfulness remain critically underexamined. This paper presents a systematic analysis of how two popular watermarking approaches-Gumbel and KGW-affect these core alignment properties across four aligned LLMs. Our experiments reveal two distinct degradation patterns: guard attenuation, where enhanced helpfulness undermines model safety, and guard amplification, where excessive caution reduces model helpfulness. These patterns emerge from watermark-induced shifts in token distribution, surfacing the fundamental tension that exists between alignment objectives.   To mitigate these degradations, we propose Alignment Resampling (AR), an inference-time sampling method that uses an external reward model to restore alignment. We establish a theoretical lower bound on the improvement in expected reward score as the sample size is increased and empirically demonstrate that sampling just 2-4 watermarked generations effectively recovers or surpasses baseline (unwatermarked) alignment scores. To overcome the limited response diversity of standard Gumbel watermarking, our modified implementation sacrifices strict distortion-freeness while maintaining robust detectability, ensuring compatibility with AR. Experimental results confirm that AR successfully recovers baseline alignment in both watermarking approaches, while maintaining strong watermark detectability. This work reveals the critical balance between watermark strength and model alignment, providing a simple inference-time solution to responsibly deploy watermarked LLMs in practice. </p>
<blockquote>
<p>æ°´å°æŠ€æœ¯å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºè´¨é‡æœ‰ç€æ˜¾è‘—å½±å“ï¼Œç„¶è€Œå®ƒä»¬å¯¹äºçœŸå®æ€§ã€å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°åˆ†æäº†ä¸¤ç§æµè¡Œæ°´å°æ–¹æ³•â€”â€”Gumbelå’ŒKGWå¯¹å››ä¸ªå¯¹é½LLMçš„è¿™äº›æ ¸å¿ƒå¯¹é½å±æ€§çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†ä¸¤ç§ç‹¬ç‰¹çš„é€€åŒ–æ¨¡å¼ï¼šå®ˆå«è¡°å‡ï¼Œå…¶ä¸­å¢å¼ºæœ‰ç”¨æ€§ä¼šç ´åæ¨¡å‹å®‰å…¨æ€§ï¼›å®ˆå«æ”¾å¤§ï¼Œå…¶ä¸­è¿‡åº¦è°¨æ…ä¼šé™ä½æ¨¡å‹çš„æœ‰ç”¨æ€§ã€‚è¿™äº›æ¨¡å¼æ˜¯ç”±æ°´å°å¼•èµ·çš„ä»¤ç‰Œåˆ†å¸ƒå˜åŒ–è€Œäº§ç”Ÿçš„ï¼Œæš´éœ²äº†å¯¹é½ç›®æ ‡ä¹‹é—´å­˜åœ¨çš„æ ¹æœ¬æ€§ç´§å¼ å…³ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04462v2">PDF</a> Published at the 1st Workshop on GenAI Watermarking, collocated with   ICLR 2025. OpenReview: <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=SIBkIV48gF">https://openreview.net/forum?id=SIBkIV48gF</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ°´å°æŠ€æœ¯å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºè´¨é‡æœ‰ç€æ˜¾è‘—å½±å“ï¼Œä½†å…¶å¯¹çœŸå®æ€§ã€å®‰å…¨æ€§å’Œæœ‰åŠ©ç›Šæ€§çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†çš„æ¢è®¨ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°åˆ†æäº†ä¸¤ç§æµè¡Œæ°´å°æ–¹æ³•â€”â€”Gumbelå’ŒKGWå¯¹è¿™å››ä¸ªæ ¸å¿ƒå¯¹é½å±æ€§çš„å½±å“ã€‚å®éªŒæ­ç¤ºäº†ä¸¤ç§ç‹¬ç‰¹çš„é€€åŒ–æ¨¡å¼ï¼šè­¦å«è¡°å‡å’Œè­¦å«æ”¾å¤§ï¼Œå…¶ä¸­è­¦å«è¡°å‡è¡¨ç¤ºå¢å¼ºæœ‰åŠ©æ€§ä¼šé™ä½æ¨¡å‹å®‰å…¨æ€§ï¼Œè€Œè­¦å«æ”¾å¤§åˆ™æ„å‘³ç€è¿‡åº¦è°¨æ…ä¼šå‡å°‘æ¨¡å‹çš„æœ‰åŠ©æ€§ã€‚è¿™äº›æ¨¡å¼æºäºæ°´å°å¼•èµ·çš„ç¬¦å·åˆ†å¸ƒå˜åŒ–ï¼Œå‡¸æ˜¾äº†å¯¹é½ç›®æ ‡ä¹‹é—´å­˜åœ¨çš„æ ¹æœ¬æ€§ç´§å¼ å…³ç³»ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é€€åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨ç†æ—¶é—´é‡‡æ ·æ–¹æ³•â€”â€”å¯¹é½é‡é‡‡æ ·ï¼ˆARï¼‰ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¤–éƒ¨å¥–åŠ±æ¨¡å‹æ¥æ¢å¤å¯¹é½æ€§ã€‚æˆ‘ä»¬å»ºç«‹äº†ç†è®ºä¸Šçš„æ”¹è¿›é¢„æœŸå¥–åŠ±åˆ†æ•°çš„ä¸‹é™éšç€æ ·æœ¬æ•°é‡çš„å¢åŠ è€Œå¢åŠ ï¼Œå¹¶å®è¯è¡¨æ˜ä»…å¯¹2-4ä¸ªå¸¦æ°´å°çš„ç”Ÿæˆç‰©è¿›è¡Œé‡‡æ ·å³å¯æ¢å¤æˆ–è¶…è¿‡åŸºçº¿ï¼ˆä¸å¸¦æ°´å°ï¼‰çš„å¯¹é½åˆ†æ•°ã€‚ä¸ºäº†å…‹æœæ ‡å‡†Gumbelæ°´å°çš„æœ‰é™å“åº”å¤šæ ·æ€§ï¼Œæˆ‘ä»¬çš„æ”¹è¿›å®ç°ç‰ºç‰²äº†å¯¹ç²¾ç¡®ç•¸å˜çš„ä¸¥æ ¼è¦æ±‚ä»¥ä¿æŒå¼ºå¤§çš„æ£€æµ‹èƒ½åŠ›ï¼Œç¡®ä¿äº†ä¸ARçš„å…¼å®¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARæˆåŠŸæ¢å¤äº†ä¸¤ç§æ°´å°æ–¹æ³•ä¸­çš„åŸºçº¿å¯¹é½æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ°´å°æ£€æµ‹èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†æ°´å°å¼ºåº¦å’Œæ¨¡å‹å¯¹é½ä¹‹é—´çš„å…³é”®å¹³è¡¡ï¼Œä¸ºå®è·µä¸­è´Ÿè´£ä»»åœ°éƒ¨ç½²å¸¦æ°´å°çš„LLMæä¾›äº†ç®€å•çš„æ¨ç†æ—¶é—´è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ°´å°æŠ€æœ¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒå±æ€§ï¼ˆçœŸå®æ€§ã€å®‰å…¨æ€§å’Œæœ‰åŠ©ç›Šæ€§ï¼‰äº§ç”Ÿå½±å“ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§æ°´å°æ–¹æ³•çš„é€€åŒ–æ¨¡å¼ï¼šè­¦å«è¡°å‡å’Œè­¦å«æ”¾å¤§ã€‚</li>
<li>æ­ç¤ºæ°´å°å¼•èµ·çš„ç¬¦å·åˆ†å¸ƒå˜åŒ–æ˜¯å¯¹é½ç›®æ ‡é—´ç´§å¼ å…³ç³»çš„æ ¹æºã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ—¶é—´é‡‡æ ·æ–¹æ³•â€”â€”å¯¹é½é‡é‡‡æ ·ï¼ˆARï¼‰ï¼Œä»¥æ¢å¤æ¨¡å‹çš„å¯¹é½æ€§ã€‚</li>
<li>ARæ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒæ°´å°æ£€æµ‹èƒ½åŠ›çš„åŒæ—¶æˆåŠŸæ¢å¤åŸºçº¿å¯¹é½æ€§ã€‚</li>
<li>å®éªŒç»“æœè¯å®äº†ARçš„æœ‰æ•ˆæ€§ï¼Œåªéœ€å¯¹å°‘é‡å¸¦æ°´å°çš„ç”Ÿæˆç‰©è¿›è¡Œé‡‡æ ·å³å¯æ¢å¤æˆ–è¶…è¶ŠæœªåŠ æ°´å°çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-515a47b4f8e42b100dea0000a02e922b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ea8058b0c9f72c2a7e90b2de20bf11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b469e43afba12b8f2f5cff721eb36939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84159dc31423fbd6c7d680fae205b983.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5c089a150a1ade396af4729b438b94b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Parallel-CPU-GPU-Execution-for-LLM-Inference-on-Constrained-GPUs"><a href="#Parallel-CPU-GPU-Execution-for-LLM-Inference-on-Constrained-GPUs" class="headerlink" title="Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"></a>Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs</h2><p><strong>Authors:Jiakun Fan, Yanglin Zhang, Xiangchen Li, Dimitrios S. Nikolopoulos</strong></p>
<p>Deploying large language models (LLMs) for online inference is often constrained by limited GPU memory, particularly due to the growing KV cache during auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a promising solution by offloading KV cache management and parts of attention computation to the CPU. However, a key bottleneck remains: existing schedulers fail to effectively overlap CPU-offloaded tasks with GPU execution during the latency-critical, bandwidth-bound decode phase. This particularly penalizes real-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning) which are currently underserved by existing systems, especially under memory pressure typical of edge or low-cost deployments.   We present APEX, a novel, profiling-informed scheduling strategy that maximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems relying on static rules or purely heuristic approaches, APEX dynamically dispatches compute across heterogeneous resources by predicting execution times of CPU and GPU subtasks to maximize overlap while avoiding scheduling overheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA T4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only schedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89% on A10 GPUs, while preserving latency. Against the best existing hybrid schedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in long-output settings. APEX significantly advances hybrid LLM inference efficiency on such memory-constrained hardware and provides a blueprint for scheduling in heterogeneous AI systems, filling a critical gap for efficient real-time LLM applications. </p>
<blockquote>
<p>åœ¨åœ¨çº¿æ¨ç†ä¸­éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸å—åˆ°æœ‰é™GPUå†…å­˜çš„åˆ¶çº¦ï¼Œç‰¹åˆ«æ˜¯ç”±äºåœ¨è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­KVç¼“å­˜çš„å¢é•¿ã€‚æ··åˆGPU-CPUæ‰§è¡Œå·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å°†KVç¼“å­˜ç®¡ç†å’Œéƒ¨åˆ†æ³¨æ„åŠ›è®¡ç®—ä»»åŠ¡è½¬ç§»åˆ°CPUä¸Šã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼šç°æœ‰è°ƒåº¦å™¨æ— æ³•åœ¨å»¶è¿Ÿå…³é”®ã€å¸¦å®½å—é™çš„è§£ç é˜¶æ®µæœ‰æ•ˆåœ°å°†CPUå¸è½½çš„ä»»åŠ¡ä¸GPUæ‰§è¡Œé‡å ã€‚è¿™å¯¹äºå®æ—¶ã€è§£ç å¯†é›†å‹åº”ç”¨ï¼ˆä¾‹å¦‚èŠå¤©ã€æ€ç»´é“¾æ¨ç†ï¼‰ç‰¹åˆ«ä¸åˆ©ï¼Œè¿™äº›åº”ç”¨åœ¨å½“å‰ç³»ç»Ÿä¸­å—åˆ°çš„é™åˆ¶å°¤ä¸ºä¸¥é‡ï¼Œå°¤å…¶æ˜¯åœ¨è¾¹ç¼˜æˆ–ä½æˆæœ¬éƒ¨ç½²ä¸­å…¸å‹çš„å†…å­˜å‹åŠ›ä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03296v3">PDF</a> Preprint, under review</p>
<p><strong>Summary</strong></p>
<p>åœ¨éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œåœ¨çº¿æ¨ç†æ—¶ï¼ŒGPUå†…å­˜é™åˆ¶æ˜¯ä¸€ä¸ªå¸¸è§çš„çº¦æŸï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨å›å½’è§£ç è¿‡ç¨‹ä¸­KVç¼“å­˜ä¸æ–­å¢é•¿çš„æƒ…å†µä¸‹ã€‚è™½ç„¶æ··åˆGPU-CPUæ‰§è¡Œå·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å°†KVç¼“å­˜ç®¡ç†å’Œéƒ¨åˆ†æ³¨æ„åŠ›è®¡ç®—ä»»åŠ¡è½¬ç§»åˆ°CPUæ¥å‡è½»GPUçš„è´Ÿæ‹…ï¼Œä½†ç°æœ‰è°ƒåº¦å™¨åœ¨å»¶è¿Ÿå…³é”®ã€å¸¦å®½ç»‘å®šçš„è§£ç é˜¶æ®µæ— æ³•æœ‰æ•ˆåœ°é‡å CPUå¸è½½çš„ä»»åŠ¡ä¸GPUæ‰§è¡Œï¼Œè¿™ç‰¹åˆ«æƒ©ç½šäº†å®æ—¶ã€è§£ç å¯†é›†å‹çš„åº”ç”¨ç¨‹åºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ã€ä»¥åˆ†æä¸ºåŸºç¡€çš„è°ƒåº¦ç­–ç•¥â€”â€”APEXï¼Œä»¥åœ¨æ··åˆLLMæ¨ç†è¿‡ç¨‹ä¸­æœ€å¤§é™åº¦åœ°å®ç°CPU-GPUå¹¶è¡Œæ€§ã€‚ä¸ä¾èµ–é™æ€è§„åˆ™æˆ–çº¯ç²¹å¯å‘å¼æ–¹æ³•çš„ç³»ç»Ÿä¸åŒï¼ŒAPEXé€šè¿‡é¢„æµ‹CPUå’ŒGPUå­ä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´æ¥åŠ¨æ€åœ°åœ¨å¼‚æ„èµ„æºä¹‹é—´åˆ†é…è®¡ç®—ä»»åŠ¡ï¼Œä»¥æœ€å¤§é™åº¦åœ°å®ç°é‡å ï¼ŒåŒæ—¶é¿å…è°ƒåº¦å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨çº¿æ¨ç†ä¸­GPUå†…å­˜é™åˆ¶æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨å›å½’è§£ç è¿‡ç¨‹ä¸­ã€‚</li>
<li>æ··åˆGPU-CPUæ‰§è¡Œæ˜¯ç¼“è§£GPUå‹åŠ›çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ç°æœ‰è°ƒåº¦å™¨åœ¨CPU-offloadedä»»åŠ¡ä¸GPUæ‰§è¡Œçš„é‡å æ–¹é¢å­˜åœ¨ç“¶é¢ˆã€‚</li>
<li>APEXæ˜¯ä¸€ç§æ–°å‹çš„è°ƒåº¦ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è°ƒåº¦å™¨çš„é—®é¢˜ï¼Œé€šè¿‡åŠ¨æ€åœ°é¢„æµ‹CPUå’ŒGPUå­ä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´æ¥å®ç°CPUå’ŒGPUä¹‹é—´çš„æœ€å¤§å¹¶è¡Œæ€§ã€‚</li>
<li>APEXåœ¨å¤šç§å·¥ä½œè´Ÿè½½å’ŒGPUæ¶æ„ä¸Šçš„æ€§èƒ½è¯„ä¼°è¡¨æ˜ï¼Œä¸ä»…ä½¿ç”¨GPUçš„è°ƒåº¦å™¨ç›¸æ¯”ï¼ŒAPEXæé«˜äº†ååé‡å¹¶ä¿æŒäº†å»¶è¿Ÿã€‚</li>
<li>APEXå¡«è¡¥äº†å®æ—¶LLMåº”ç”¨ç¨‹åºä¸­é«˜æ•ˆè°ƒåº¦ç­–ç•¥çš„ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯åœ¨å†…å­˜å—é™çš„ç¡¬ä»¶ä¸Šã€‚</li>
<li>APEXä¸ä»…è§£å†³äº†å½“å‰LLMæ¨ç†ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œè¿˜ä¸ºå¼‚æ„AIç³»ç»Ÿä¸­çš„è°ƒåº¦ç­–ç•¥æä¾›äº†è“å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96c42ee5761cf2c611bca599b7b680b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37fb1b52265ddd71b71ecc7f43757648.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87907899ed7c6e1c59af1c1f59dd6953.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d401e2470585e9f76b830d64f265d2d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3775130ae7caa949c838316fdc3eaab7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="STAR-R1-Spatial-TrAnsformation-Reasoning-by-Reinforcing-Multimodal-LLMs"><a href="#STAR-R1-Spatial-TrAnsformation-Reasoning-by-Reinforcing-Multimodal-LLMs" class="headerlink" title="STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs"></a>STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs</h2><p><strong>Authors:Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1â€™s anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/zongzhao23/STAR-R1">https://github.com/zongzhao23/STAR-R1</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†åœ¨ç©ºé—´æ¨ç†æ–¹é¢ä¸äººç±»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æˆ‘ä»¬é€šè¿‡è½¬æ¢é©±åŠ¨è§†è§‰æ¨ç†ï¼ˆTVRï¼‰æ¥ç ”ç©¶è¿™ä¸€å·®è·ï¼Œè¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¦æ±‚åœ¨ä¸åŒè§†è§’ä¸‹è¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡è½¬æ¢ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ— æ³•åœ¨è·¨è§†å›¾è®¾ç½®ä¸­ç”Ÿæˆè¿è´¯çš„æ¨ç†è·¯å¾„ï¼Œè€ŒåŸºäºç¨€ç–å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ™é¢ä¸´æ¢ç´¢æ•ˆç‡ä½ä¸‹å’Œæ”¶æ•›ç¼“æ…¢çš„å›°å¢ƒã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†STAR-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å•é˜¶æ®µRLèŒƒå¼å’Œé’ˆå¯¹TVRå®šåˆ¶çš„ç²¾ç»†å¥–åŠ±æœºåˆ¶çš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒSTAR-R1å¥–åŠ±éƒ¨åˆ†æ­£ç¡®æ€§ï¼ŒåŒæ—¶æƒ©ç½šè¿‡åº¦æšä¸¾å’Œè¢«åŠ¨æ— ä½œä¸ºï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ¢ç´¢å’Œç²¾ç¡®æ¨ç†ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒSTAR-R1åœ¨æ‰€æœ‰11é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨è·¨è§†å›¾åœºæ™¯ä¸­è¾ƒSFTé«˜å‡º23%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†STAR-R1çš„äººç±»è¡Œä¸ºç‰¹å¾ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨æ¯”è¾ƒæ‰€æœ‰å¯¹è±¡æ–¹é¢æé«˜ç©ºé—´æ¨ç†çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œå¯¹æ¨åŠ¨MLLMså’Œæ¨ç†æ¨¡å‹çš„ç ”ç©¶æä¾›äº†å…³é”®è§è§£ã€‚ä»£ç ã€æ¨¡å‹æƒé‡å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zongzhao23/STAR-R">https://github.com/zongzhao23/STAR-R</a> å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15804v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œä½†åœ¨ç©ºé—´æ¨ç†æ–¹é¢ä¸äººç±»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æœ¬ç ”ç©¶é€šè¿‡Transformation-Driven Visual Reasoningï¼ˆTVRï¼‰ä»»åŠ¡æ¢ç©¶è¿™ä¸€å·®è·ï¼Œå¹¶æå‡ºSTAR-R1æ¡†æ¶ï¼Œç»“åˆå•é˜¶æ®µå¼ºåŒ–å­¦ä¹ ä¸ç²¾ç»†å¥–åŠ±æœºåˆ¶ï¼Œæœ‰æ•ˆåº”å¯¹ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å±€é™æ€§ã€‚STAR-R1åœ¨TVRä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨è·¨è§†å›¾åœºæ™¯ä¸­è¾ƒSFTé«˜å‡º23%ã€‚åˆ†ææ˜¾ç¤ºSTAR-R1å±•ç°æ‹ŸäººåŒ–è¡Œä¸ºï¼Œå¹¶å¯é€šè¿‡æ¯”è¾ƒæ‰€æœ‰å¯¹è±¡æ¥æ”¹å–„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å¯¹æ¨è¿›MLLMä¸æ¨ç†æ¨¡å‹ç ”ç©¶æä¾›é‡è¦è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ç©ºé—´æ¨ç†æ–¹é¢ä¸äººç±»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>Transformation-Driven Visual Reasoningï¼ˆTVRï¼‰ä»»åŠ¡ç”¨äºæ¢ç©¶è¿™ä¸€å·®è·ã€‚</li>
<li>ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨è·¨è§†å›¾è®¾ç½®ä¸‹æ— æ³•ç”Ÿæˆè¿è´¯çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>ç¨€ç–å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å­˜åœ¨æ¢ç´¢æ•ˆç‡ä½ä¸‹å’Œæ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ã€‚</li>
<li>STAR-R1æ¡†æ¶ç»“åˆå•é˜¶æ®µRLä¸ç²¾ç»†å¥–åŠ±æœºåˆ¶ï¼Œåº”å¯¹ä¸Šè¿°å±€é™æ€§ã€‚</li>
<li>STAR-R1åœ¨TVRä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œè¾ƒSFTåœ¨è·¨è§†å›¾åœºæ™¯ä¸­é«˜å‡º23%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20a20bb50df4cca252a0de5f01e8196e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db79b8fdab5db12a0df10ffc8c504d0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64d507ff00417d60ec92ecb7b99014a1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="EditLord-Learning-Code-Transformation-Rules-for-Code-Editing"><a href="#EditLord-Learning-Code-Transformation-Rules-for-Code-Editing" class="headerlink" title="EditLord: Learning Code Transformation Rules for Code Editing"></a>EditLord: Learning Code Transformation Rules for Code Editing</h2><p><strong>Authors:Weichen Li, Albert Jan, Baishakhi Ray, Junfeng Yang, Chengzhi Mao, Kexin Pei</strong></p>
<p>Code editing is a foundational task in software development, where its effectiveness depends on whether it introduces desired code property changes without changing the original codeâ€™s intended functionality. Existing approaches often formulate code editing as an implicit end-to-end task, omitting the fact that code-editing procedures inherently consist of discrete and explicit steps. Thus, they suffer from suboptimal performance and lack of robustness and generalization. We introduce EditLord, a code editing framework that makes the code transformation steps explicit. Our key insight is to employ a language model (LM) as an inductive learner to extract code editing rules from the training code pairs as concise meta-rule sets. Such rule sets will be manifested for each training sample to augment them for finetuning or assist in prompting- and iterative-based code editing. EditLord outperforms the state-of-the-art by an average of 22.7% in editing performance and 58.1% in robustness while achieving 20.2% higher functional correctness across critical software engineering and security applications, LM models, and editing modes. </p>
<blockquote>
<p>ä»£ç ç¼–è¾‘æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºæ˜¯å¦åœ¨ä¸æ”¹å˜åŸå§‹ä»£ç é¢„æœŸåŠŸèƒ½çš„æƒ…å†µä¸‹å¼•å…¥äº†æ‰€éœ€çš„ä»£ç å±æ€§æ›´æ”¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†ä»£ç ç¼–è¾‘åˆ¶å®šä¸ºéšå¼çš„ç«¯åˆ°ç«¯ä»»åŠ¡ï¼Œå¿½ç•¥äº†ä»£ç ç¼–è¾‘è¿‡ç¨‹æœ¬è´¨ä¸ŠåŒ…å«ç¦»æ•£å’Œæ˜ç¡®æ­¥éª¤è¿™ä¸€äº‹å®ã€‚å› æ­¤ï¼Œå®ƒä»¬é¢ä¸´æ€§èƒ½ä¸ä½³ã€ç¼ºä¹ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†EditLordï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ä»£ç è½¬æ¢æ­¥éª¤æ˜ç¡®çš„ä»£ç ç¼–è¾‘æ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œé‡‡ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä½œä¸ºå½’çº³å­¦ä¹ è€…ï¼Œä»è®­ç»ƒä»£ç å¯¹ä¸­æå–ä»£ç ç¼–è¾‘è§„åˆ™ï¼Œå½¢æˆç®€æ´çš„å…ƒè§„åˆ™é›†ã€‚è¿™æ ·çš„è§„åˆ™é›†å°†ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬æä¾›è¡¨ç°ï¼Œä»¥å¢å¼ºå®ƒä»¬è¿›è¡Œå¾®è°ƒæˆ–è¾…åŠ©åŸºäºæç¤ºå’Œè¿­ä»£çš„ä»£ç ç¼–è¾‘ã€‚EditLordåœ¨ç¼–è¾‘æ€§èƒ½ä¸Šå¹³å‡é«˜å‡ºæœ€æ–°æŠ€æœ¯22.7%ï¼Œåœ¨ç¨³å¥æ€§ä¸Šé«˜å‡º58.1%ï¼ŒåŒæ—¶åœ¨å…³é”®è½¯ä»¶å·¥ç¨‹å’Œå®‰å…¨åº”ç”¨ç¨‹åºã€LMæ¨¡å‹å’Œç¼–è¾‘æ¨¡å¼ä¸‹å®ç°20.2%æ›´é«˜çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15284v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä»£ç ç¼–è¾‘æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºæ˜¯å¦åœ¨æ”¹å˜åŸå§‹ä»£ç é¢„æœŸåŠŸèƒ½çš„åŒæ—¶å¼•å…¥æ‰€éœ€çš„ä»£ç å±æ€§æ›´æ”¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†ä»£ç ç¼–è¾‘åˆ¶å®šä¸ºéšå¼çš„ç«¯åˆ°ç«¯ä»»åŠ¡ï¼Œå¿½ç•¥äº†ä»£ç ç¼–è¾‘è¿‡ç¨‹æœ¬è´¨ä¸ŠåŒ…å«ç¦»æ•£å’Œæ˜¾å¼æ­¥éª¤è¿™ä¸€äº‹å®ã€‚å› æ­¤ï¼Œå®ƒä»¬å­˜åœ¨æ€§èƒ½ä¸ä½³ã€ç¼ºä¹ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†EditLordï¼Œä¸€ä¸ªä½¿ä»£ç è½¬æ¢æ­¥éª¤æ˜¾å¼çš„ä»£ç ç¼–è¾‘æ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œé‡‡ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä½œä¸ºå½’çº³å­¦ä¹ å™¨ï¼Œä»è®­ç»ƒä»£ç å¯¹ä¸­æå–ä»£ç ç¼–è¾‘è§„åˆ™ï¼Œå½¢æˆç®€æ´çš„å…ƒè§„åˆ™é›†ã€‚è¿™æ ·çš„è§„åˆ™é›†å°†ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬æä¾›è¾…åŠ©ï¼Œç”¨äºå¾®è°ƒæˆ–è¾…åŠ©åŸºäºæç¤ºå’Œè¿­ä»£çš„ä»£ç ç¼–è¾‘ã€‚EditLordåœ¨ç¼–è¾‘æ€§èƒ½ã€ç¨³å¥æ€§å’ŒåŠŸèƒ½æ­£ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹³å‡æé«˜22.7%ã€58.1%å’Œ20.2%ï¼Œé€‚ç”¨äºå…³é”®è½¯ä»¶å·¥ç¨‹å’Œå®‰å…¨åº”ç”¨ç¨‹åºã€LMæ¨¡å‹å’Œç¼–è¾‘æ¨¡å¼ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä»£ç ç¼–è¾‘æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œè¦æ±‚åœ¨ä¸æ”¹å˜åŸå§‹ä»£ç é¢„æœŸåŠŸèƒ½çš„å‰æä¸‹å¼•å…¥æ‰€éœ€çš„ä»£ç å±æ€§æ›´æ”¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°†ä»£ç ç¼–è¾‘è§†ä¸ºéšå¼çš„ç«¯åˆ°ç«¯ä»»åŠ¡ï¼Œå¿½ç•¥äº†ä»£ç ç¼–è¾‘è¿‡ç¨‹çš„ç¦»æ•£å’Œæ˜¾å¼æ­¥éª¤ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³å’Œç¼ºä¹ç¨³å¥æ€§ã€‚</li>
<li>EditLordæ¡†æ¶ä½¿ä»£ç è½¬æ¢æ­¥éª¤æ˜¾å¼åŒ–ï¼Œé€šè¿‡é‡‡ç”¨è¯­è¨€æ¨¡å‹ä½œä¸ºå½’çº³å­¦ä¹ å™¨ï¼Œä»è®­ç»ƒä»£ç å¯¹ä¸­æå–ç®€æ´çš„å…ƒè§„åˆ™é›†ã€‚</li>
<li>è¿™äº›è§„åˆ™é›†å¯ç”¨äºä¸ºè®­ç»ƒæ ·æœ¬æä¾›è¾…åŠ©ï¼Œç”¨äºå¾®è°ƒæˆ–åŸºäºæç¤ºå’Œè¿­ä»£çš„ä»£ç ç¼–è¾‘ã€‚</li>
<li>EditLordåœ¨ç¼–è¾‘æ€§èƒ½ã€ç¨³å¥æ€§å’ŒåŠŸèƒ½æ­£ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œé€‚ç”¨äºå¤šç§è½¯ä»¶å·¥ç¨‹å’Œåº”ç”¨ç¨‹åºåœºæ™¯ã€‚</li>
<li>EditLordçš„å¹³å‡ç¼–è¾‘æ€§èƒ½æé«˜22.7%ï¼Œç¨³å¥æ€§æé«˜58.1%ï¼ŒåŠŸèƒ½æ­£ç¡®æ€§æé«˜20.2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-761089c4b6a6698f306d441b68c0664e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15264410816cd52f80f8d74350f5ba33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a1d85c0e98bae79079f2f6d8d7a061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65e5aa27aba09d9764491a66544b5ac2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66dcd7c6ad52512cebe76c6755fda936.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8e4695ba8c675a41df6074f72e8d36bb.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  MIRIX Multi-Agent Memory System for LLM-Based Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-12/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5bf8838811dda82d849d4705d479bce7.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-12  Traceable Evidence Enhanced Visual Grounded Reasoning Evaluation and   Methodology
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
