<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-01  VideoReasonBench Can MLLMs Perform Vision-Centric Complex Video   Reasoning?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-937915e313a6e036e2c588a707cf9053.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    92 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-01-æ›´æ–°"><a href="#2025-06-01-æ›´æ–°" class="headerlink" title="2025-06-01 æ›´æ–°"></a>2025-06-01 æ›´æ–°</h1><h2 id="VideoReasonBench-Can-MLLMs-Perform-Vision-Centric-Complex-Video-Reasoning"><a href="#VideoReasonBench-Can-MLLMs-Perform-Vision-Centric-Complex-Video-Reasoning" class="headerlink" title="VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video   Reasoning?"></a>VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video   Reasoning?</h2><p><strong>Authors:Yuanxin Liu, Kun Ouyang, Haoning Wu, Yi Liu, Lin Sui, Xinhao Li, Yan Zhong, Y. Charles, Xinyu Zhou, Xu Sun</strong></p>
<p>Recent studies have shown that long chain-of-thought (CoT) reasoning can significantly enhance the performance of large language models (LLMs) on complex tasks. However, this benefit is yet to be demonstrated in the domain of video understanding, since most existing benchmarks lack the reasoning depth required to demonstrate the advantages of extended CoT chains. While recent efforts have proposed benchmarks aimed at video reasoning, the tasks are often knowledge-driven and do not rely heavily on visual content. To bridge this gap, we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric, complex video reasoning. To ensure visual richness and high reasoning complexity, each video in VideoReasonBench depicts a sequence of fine-grained operations on a latent state that is only visible in part of the video. The questions evaluate three escalating levels of video reasoning skills: recalling observed visual information, inferring the content of latent states, and predicting information beyond the video. Under such task setting, models have to precisely recall multiple operations in the video, and perform step-by-step reasoning to get correct final answers for these questions. Using VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal LLMs (MLLMs), finding that most perform poorly on complex video reasoning, e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our investigations on â€œtest-time scalingâ€ further reveal that extended thinking budget, while offering none or minimal benefits on existing video benchmarks, is essential for improving the performance on VideoReasonBench. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å¯ä»¥æ˜¾è‘—å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä¼˜åŠ¿åœ¨è§†é¢‘ç†è§£é¢†åŸŸå°šæœªå¾—åˆ°è¯æ˜ï¼Œå› ä¸ºå¤§å¤šæ•°ç°æœ‰çš„åŸºå‡†æµ‹è¯•ç¼ºä¹å±•ç¤ºé•¿é“¾æ€ç»´ä¼˜åŠ¿æ‰€éœ€çš„æ¨ç†æ·±åº¦ã€‚å°½ç®¡æœ€è¿‘æœ‰åŠªåŠ›æå‡ºäº†æ—¨åœ¨è§†é¢‘æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼Œä½†è¿™äº›ä»»åŠ¡é€šå¸¸æ˜¯çŸ¥è¯†é©±åŠ¨çš„ï¼Œå¹¶ä¸å¤ªä¾èµ–è§†è§‰å†…å®¹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VideoReasonBenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ä»¥è§†è§‰ä¸ºä¸­å¿ƒã€å¤æ‚çš„è§†é¢‘æ¨ç†ã€‚ä¸ºç¡®ä¿è§†è§‰ä¸°å¯Œæ€§å’Œé«˜æ¨ç†å¤æ‚æ€§ï¼ŒVideoReasonBenchä¸­çš„æ¯ä¸ªè§†é¢‘éƒ½æè¿°äº†ä¸€ç³»åˆ—æ½œåœ¨çŠ¶æ€çš„ç²¾ç»†æ“ä½œï¼Œè¿™äº›çŠ¶æ€ä»…åœ¨è§†é¢‘çš„ä¸€éƒ¨åˆ†ä¸­å¯è§ã€‚é—®é¢˜éƒ¨åˆ†è¯„ä¼°äº†ä¸‰ä¸ªé€’å¢çº§åˆ«çš„è§†é¢‘æ¨ç†æŠ€èƒ½ï¼šå›å¿†è§‚å¯Ÿåˆ°çš„è§†è§‰ä¿¡æ¯ã€æ¨æ–­æ½œåœ¨çŠ¶æ€çš„å†…å®¹å’Œé¢„æµ‹è§†é¢‘ä¹‹å¤–çš„ä¿¡æ¯ã€‚åœ¨è¿™æ ·çš„ä»»åŠ¡è®¾ç½®ä¸­ï¼Œæ¨¡å‹å¿…é¡»ç²¾ç¡®å›å¿†è§†é¢‘ä¸­çš„å¤šä¸ªæ“ä½œï¼Œå¹¶è¿›è¡Œé€æ­¥æ¨ç†ï¼Œä»¥æ­£ç¡®å›ç­”è¿™äº›é—®é¢˜ã€‚ä½¿ç”¨VideoReasonBenchåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹18ä¸ªæœ€æ–°å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°å¤§å¤šæ•°æ¨¡å‹åœ¨å¤æ‚çš„è§†é¢‘æ¨ç†ä¸Šè¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚GPT-4oçš„å‡†ç¡®ç‡ä»…ä¸º6.9%ï¼Œè€Œæ€ç»´å¢å¼ºçš„Gemini-2.5-Proåˆ™æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¾¾åˆ°56.0%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬å¯¹â€œæµ‹è¯•æ—¶ç¼©æ”¾â€çš„è¿›ä¸€æ­¥è°ƒæŸ¥è¡¨æ˜ï¼Œåœ¨ç°æœ‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¢åŠ æ€è€ƒæ—¶é—´å¹¶æ²¡æœ‰å¸¦æ¥å¤šå°‘å¥½å¤„ï¼Œä½†åœ¨VideoReasonBenchä¸Šå´å¯¹æé«˜æ€§èƒ½è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23359v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://llyx97.github.io/video_reason_bench/">https://llyx97.github.io/video_reason_bench/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹é’ˆå¯¹è§†é¢‘ç†è§£é¢†åŸŸçš„æ–°ç ”ç©¶ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†VideoReasonBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è¯„ä¼°ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¤æ‚è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚è¯¥å¹³å°å¼ºè°ƒè§†é¢‘çš„è§†è§‰ä¸°å¯Œæ€§å’Œé«˜æ¨ç†å¤æ‚æ€§ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿç²¾ç¡®å›å¿†è§†é¢‘ä¸­çš„å¤šä¸ªæ“ä½œï¼Œå¹¶è¿›è¡Œé€æ­¥æ¨ç†ä»¥è·å–æ­£ç¡®çš„ç­”æ¡ˆã€‚ç ”ç©¶å¯¹18ç§æœ€å…ˆè¿›çš„å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°å¤§å¤šæ•°æ¨¡å‹åœ¨å¤æ‚è§†é¢‘æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œè€Œæ€è€ƒå¢å¼ºçš„Gemini-2.5-Proæ¨¡å‹æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†â€œæµ‹è¯•æ—¶é—´ç¼©æ”¾â€çš„å½±å“ï¼Œå‘ç°å¢åŠ æ€è€ƒé¢„ç®—å¯¹äºæé«˜VideoReasonBenchä¸Šçš„æ€§èƒ½è‡³å…³é‡è¦ï¼Œè€Œå¯¹ç°æœ‰è§†é¢‘åŸºå‡†æµ‹è¯•çš„å½±å“å¾®ä¹å…¶å¾®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoReasonBenchæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤æ‚è§†é¢‘æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>è¯¥å¹³å°å¼ºè°ƒè§†é¢‘çš„è§†è§‰ä¸°å¯Œæ€§å’Œé«˜æ¨ç†å¤æ‚æ€§ã€‚</li>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§†é¢‘æ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ã€‚</li>
<li>Gemini-2.5-Proæ¨¡å‹åœ¨VideoReasonBenchä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>â€œæµ‹è¯•æ—¶é—´ç¼©æ”¾â€ç ”ç©¶æ˜¾ç¤ºï¼Œå¢åŠ æ€è€ƒé¢„ç®—å¯¹æ”¹å–„å¤æ‚è§†é¢‘æ¨ç†æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>å¢åŠ æ€è€ƒé¢„ç®—å¯¹ç°æœ‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•å½±å“ç”šå¾®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23359">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-37cff7473dd8987b61ef6949a634de7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e61f9fa79958b694098ef903e690415.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f011b92bfea57c818b00c6f1f84df8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca839f94e1b9293fbc82d2739ad0572e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a181f8fe7aa535065e8ee8e2f5f6041.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ScEdit-Script-based-Assessment-of-Knowledge-Editing"><a href="#ScEdit-Script-based-Assessment-of-Knowledge-Editing" class="headerlink" title="ScEdit: Script-based Assessment of Knowledge Editing"></a>ScEdit: Script-based Assessment of Knowledge Editing</h2><p><strong>Authors:Xinye Li, Zunwen Zheng, Qian Zhang, Dekai Zhuang, Jiabao Kang, Liyan Xu, Qingbin Liu, Xi Chen, Zhiying Tu, Dianhui Chu, Dianbo Sui</strong></p>
<p>Knowledge Editing (KE) has gained increasing attention, yet current KE tasks remain relatively simple. Under current evaluation frameworks, many editing methods achieve exceptionally high scores, sometimes nearing perfection. However, few studies integrate KE into real-world application scenarios (e.g., recent interest in LLM-as-agent). To support our analysis, we introduce a novel script-based benchmark â€“ ScEdit (Script-based Knowledge Editing Benchmark) â€“ which encompasses both counterfactual and temporal edits. We integrate token-level and text-level evaluation methods, comprehensively analyzing existing KE techniques. The benchmark extends traditional fact-based (â€œWhatâ€-type question) evaluation to action-based (â€œHowâ€-type question) evaluation. We observe that all KE methods exhibit a drop in performance on established metrics and face challenges on text-level metrics, indicating a challenging task. Our benchmark is available at <a target="_blank" rel="noopener" href="https://github.com/asdfo123/ScEdit">https://github.com/asdfo123/ScEdit</a>. </p>
<blockquote>
<p>çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œä½†å½“å‰çš„KEä»»åŠ¡ä»ç„¶ç›¸å¯¹ç®€å•ã€‚åœ¨ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸‹ï¼Œè®¸å¤šç¼–è¾‘æ–¹æ³•éƒ½å–å¾—äº†å¼‚å¸¸é«˜çš„åˆ†æ•°ï¼Œæœ‰æ—¶ç”šè‡³æ¥è¿‘å®Œç¾ã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰ç ”ç©¶å°†KEé›†æˆåˆ°ç°å®ä¸–ç•Œçš„åº”ç”¨åœºæ™¯ä¸­ï¼ˆä¾‹å¦‚æœ€è¿‘å¯¹LLM-as-agentçš„å…´è¶£ï¼‰ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºè„šæœ¬çš„åŸºå‡†æµ‹è¯•â€”â€”ScEditï¼ˆåŸºäºè„šæœ¬çš„çŸ¥è¯†ç¼–è¾‘åŸºå‡†æµ‹è¯•ï¼‰ï¼Œå®ƒæ¶µç›–äº†äº‹å®æ€§å’Œæ—¶é—´æ€§çš„ç¼–è¾‘ã€‚æˆ‘ä»¬æ•´åˆäº†ä»¤ç‰Œçº§åˆ«å’Œæ–‡æœ¬çº§åˆ«çš„è¯„ä¼°æ–¹æ³•ï¼Œå…¨é¢åˆ†æäº†ç°æœ‰çš„KEæŠ€æœ¯ã€‚è¯¥åŸºå‡†æµ‹è¯•å°†ä¼ ç»Ÿçš„åŸºäºäº‹å®ï¼ˆâ€œæ˜¯ä»€ä¹ˆâ€ç±»å‹é—®é¢˜ï¼‰çš„è¯„ä¼°æ‰©å±•åˆ°åŸºäºè¡ŒåŠ¨ï¼ˆâ€œå¦‚ä½•åšâ€ç±»å‹é—®é¢˜ï¼‰çš„è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ‰€æœ‰KEæ–¹æ³•åœ¨æ—¢å®šæŒ‡æ ‡ä¸Šçš„è¡¨ç°éƒ½æœ‰æ‰€ä¸‹é™ï¼Œå¹¶ä¸”åœ¨æ–‡æœ¬çº§åˆ«æŒ‡æ ‡ä¸Šé‡åˆ°äº†æŒ‘æˆ˜ï¼Œè¡¨æ˜ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åœ¨<a target="_blank" rel="noopener" href="https://github.com/asdfo123/ScEdit%E5%8F%AF%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/asdfo123/ScEditå¯è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23291v1">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰çš„å½“å‰çŠ¶å†µåŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚å°½ç®¡KEå·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½†ç°æœ‰ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œè¯„ä¼°æ¡†æ¶ä¸‹çš„è®¸å¤šç¼–è¾‘æ–¹æ³•å¾—åˆ†æé«˜ï¼Œä½†å®é™…åº”ç”¨åœºæ™¯ä¸­çš„é›†æˆè¾ƒå°‘ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è„šæœ¬åŸºå‡†â€”â€”ScEditï¼Œå®ƒåŒ…æ‹¬äº‹å®æ€§å’Œæ—¶åºæ€§ç¼–è¾‘ï¼Œå¯¹ç°æœ‰çš„KEæŠ€æœ¯è¿›è¡Œäº†å…¨é¢çš„åˆ†æã€‚è¯¥åŸºå‡†æ‰©å±•äº†ä¼ ç»Ÿçš„äº‹å®æ€§è¯„ä»·ï¼Œå¼•å…¥äº†è¡ŒåŠ¨æ€§è¯„ä»·ï¼Œå‘ç°æ‰€æœ‰KEæ–¹æ³•åœ¨æ–°åŸºå‡†ä¸Šçš„æ€§èƒ½éƒ½æœ‰æ‰€ä¸‹é™ï¼Œè¡¨æ˜è¯¥ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œè¯„ä¼°æ–¹æ³•ä¸‹çš„è®¸å¤šç¼–è¾‘æ–¹æ³•è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å®é™…åº”ç”¨ä¸­å°†KEé›†æˆåˆ°ç°å®åœºæ™¯ï¼ˆå¦‚LLM-as-agentï¼‰çš„ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>å¼•å…¥æ–°å‹è„šæœ¬åŸºå‡†ScEditï¼Œæ¶µç›–äº‹å®æ€§å’Œæ—¶åºæ€§ç¼–è¾‘ã€‚</li>
<li>ScEditåŸºå‡†æ‰©å±•äº†ä¼ ç»Ÿçš„äº‹å®æ€§è¯„ä»·ï¼Œå¼•å…¥äº†è¡ŒåŠ¨æ€§è¯„ä»·ã€‚</li>
<li>KEæ–¹æ³•åœ¨ScEditåŸºå‡†ä¸Šçš„æ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œè¡¨æ˜è¯¥ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ScEditåŸºå‡†æä¾›äº†å…¨é¢çš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬token-levelå’Œtext-levelè¯„ä»·æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6b349fbb908e259612e4529d948613d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12bc17cc76811fed42d8e3076fcffb01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a0f6c04e3fd147931c67e8118f840c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8018e501f9f4afd405ebb05e9cf2bf44.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Are-MLMs-Trapped-in-the-Visual-Room"><a href="#Are-MLMs-Trapped-in-the-Visual-Room" class="headerlink" title="Are MLMs Trapped in the Visual Room?"></a>Are MLMs Trapped in the Visual Room?</h2><p><strong>Authors:Yazhou Zhang, Chunwang Zou, Qimeng Liu, Lu Rong, Ben Yao, Zheng Lian, Qiuchi Li, Peng Zhang, Jing Qin</strong></p>
<p>Can multi-modal large models (MLMs) that can <code>see&#39;&#39; an image be said to </code>understandâ€™â€™ it? Drawing inspiration from Searleâ€™s Chinese Room, we propose the \textbf{Visual Room} argument: a system may process and describe every detail of visual inputs by following algorithmic rules, without genuinely comprehending the underlying intention. This dilemma challenges the prevailing assumption that perceptual mastery implies genuine understanding. In implementation, we introduce a two-tier evaluation framework spanning perception and cognition. The perception component evaluates whether MLMs can accurately capture the surface-level details of visual contents, where the cognitive component examines their ability to infer sarcasm polarity. To support this framework, We further introduce a high-quality multi-modal sarcasm dataset comprising both 924 static images and 100 dynamic videos. All sarcasm labels are annotated by the original authors and verified by independent reviewers to ensure clarity and consistency. We evaluate eight state-of-the-art (SoTA) MLMs. Our results highlight three key findings: (1) MLMs perform well on perception tasks; (2) even with correct perception, models exhibit an average error rate of ~16.1% in sarcasm understanding, revealing a significant gap between seeing and understanding; (3) error analysis attributes this gap to deficiencies in emotional reasoning, commonsense inference, and context alignment. This work provides empirical grounding for the proposed Visual Room argument and offers a new evaluation paradigm for MLMs. </p>
<blockquote>
<p>èƒ½å¦è¯´èƒ½å¤Ÿâ€œçœ‹åˆ°â€å›¾åƒçš„å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆMLMsï¼‰å°±â€œç†è§£â€äº†å®ƒï¼Ÿä»å¡å°”çš„ä¸­æ–‡æˆ¿é—´ï¼ˆChinese Roomï¼‰ä¸­è·å¾—çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†<strong>è§†è§‰æˆ¿é—´ï¼ˆVisual Roomï¼‰</strong>è®ºè¯ï¼šä¸€ä¸ªç³»ç»Ÿå¯èƒ½ä¼šæŒ‰ç…§ç®—æ³•è§„åˆ™å¤„ç†å’Œæè¿°è§†è§‰è¾“å…¥çš„æ¯ä¸€ä¸ªç»†èŠ‚ï¼Œè€Œå¹¶æ²¡æœ‰çœŸæ­£ç†è§£å…¶èƒŒåçš„æ„å›¾ã€‚è¿™ç§å›°å¢ƒæŒ‘æˆ˜äº†æ™®éå­˜åœ¨çš„å‡è®¾ï¼Œå³æ„ŸçŸ¥æŒæ¡æ„å‘³ç€çœŸæ­£çš„ç†è§£ã€‚åœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè·¨è¶Šæ„ŸçŸ¥å’Œè®¤çŸ¥çš„äºŒå±‚è¯„ä¼°æ¡†æ¶ã€‚æ„ŸçŸ¥ç»„ä»¶è¯„ä¼°MLMsæ˜¯å¦èƒ½å‡†ç¡®æ•æ‰è§†è§‰å†…å®¹çš„è¡¨å±‚ç»†èŠ‚ï¼Œè®¤çŸ¥ç»„ä»¶åˆ™è€ƒå¯Ÿå®ƒä»¬æ¨æ–­è®½åˆºææ€§çš„èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€è®½åˆºæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«924å¼ é™æ€å›¾åƒå’Œ100ä¸ªåŠ¨æ€è§†é¢‘ã€‚æ‰€æœ‰çš„è®½åˆºæ ‡ç­¾éƒ½ç”±åŸå§‹ä½œè€…æ ‡æ³¨ï¼Œå¹¶ç”±ç‹¬ç«‹å®¡æŸ¥äººå‘˜éªŒè¯ï¼Œä»¥ç¡®ä¿æ¸…æ™°å’Œä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†å…«ç§æœ€å…ˆè¿›çš„MLMsã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šï¼ˆ1ï¼‰MLMsåœ¨æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼›ï¼ˆ2ï¼‰å³ä½¿åœ¨æ„ŸçŸ¥æ­£ç¡®çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨ç†è§£è®½åˆºæ–¹é¢çš„å¹³å‡é”™è¯¯ç‡ä»è¾¾åˆ°çº¦16.1%ï¼Œæ­ç¤ºäº†â€œçœ‹åˆ°â€å’Œâ€œç†è§£â€ä¹‹é—´çš„æ˜¾è‘—å·®è·ï¼›ï¼ˆ3ï¼‰é”™è¯¯åˆ†æå°†è¿™ç§å·®è·å½’å› äºæƒ…æ„Ÿæ¨ç†ã€å¸¸è¯†æ¨ç†å’Œä¸Šä¸‹æ–‡å¯¹é½æ–¹é¢çš„ä¸è¶³ã€‚è¿™é¡¹å·¥ä½œä¸ºæå‡ºçš„è§†è§‰æˆ¿é—´è®ºè¯æä¾›äº†å®è¯ä¾æ®ï¼Œå¹¶ä¸ºMLMsæä¾›äº†æ–°çš„è¯„ä¼°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23272v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆMLMsï¼‰èƒ½å¦é€šè¿‡è§†è§‰ç†è§£å›¾åƒä»å­˜åœ¨äº‰è®®ã€‚å€Ÿé‰´å¡è€¶å°”çš„ä¸­æ–‡æˆ¿é—´ç†è®ºï¼Œæˆ‘ä»¬æå‡ºè§†è§‰æˆ¿é—´è®ºè¯ï¼šç³»ç»Ÿå¯ä»¥å¤„ç†å¹¶æè¿°è§†è§‰è¾“å…¥çš„æ¯ä¸€ä¸ªç»†èŠ‚ï¼Œéµå¾ªç®—æ³•è§„åˆ™ï¼Œä½†ä¸ä¸€å®šçœŸæ­£ç†è§£äº†å…¶å†…åœ¨æ„å›¾ã€‚æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ï¼Œå‘ç°å³ä½¿æ˜¯é¡¶å°–æ¨¡å‹ä¹Ÿå­˜åœ¨æƒ…æ„Ÿæ¨ç†ã€å¸¸è¯†æ¨æ–­å’Œè¯­å¢ƒå¯¹é½ä¸Šçš„ä¸è¶³ï¼Œè¿™è¯å®äº†è§†è§‰æˆ¿é—´è®ºè¯çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆMLMsï¼‰å¯ä»¥å‡†ç¡®æ•æ‰è§†è§‰å†…å®¹çš„è¡¨é¢ç»†èŠ‚ï¼Œä½†åœ¨ç†è§£å±‚é¢å­˜åœ¨å±€é™ã€‚</li>
<li>è§†è§‰æˆ¿é—´è®ºè¯æŒ‘æˆ˜äº†æ„ŸçŸ¥æŒæ¡å³çœŸæ­£ç†è§£çš„å‡è®¾ã€‚</li>
<li>å¼•å…¥çš„ä¸¤å±‚è¯„ä¼°æ¡†æ¶åŒ…æ‹¬æ„ŸçŸ¥å’Œè®¤çŸ¥ï¼Œå…¶ä¸­è®¤çŸ¥éƒ¨åˆ†è€ƒå¯Ÿæ¨¡å‹æ¨ç†è®½åˆºçš„èƒ½åŠ›ã€‚</li>
<li>é«˜è´¨é‡çš„å¤šæ¨¡æ€è®½åˆºæ•°æ®é›†åŒ…å«é™æ€å›¾åƒå’ŒåŠ¨æ€è§†é¢‘ã€‚</li>
<li>å…«æ¬¾æœ€æ–°MLMsçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåœ¨è®½åˆºç†è§£æ–¹é¢å­˜åœ¨çº¦16.1%çš„å¹³å‡é”™è¯¯ç‡ã€‚</li>
<li>é”™è¯¯åˆ†æè¡¨æ˜ï¼Œè¿™ä¸€ç†è§£å·®è·æºäºæƒ…æ„Ÿæ¨ç†ã€å¸¸è¯†æ¨æ–­å’Œè¯­å¢ƒå¯¹é½ä¸Šçš„ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9353369d2bf81b89ce41ea74e52438e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db5c576747676bd5bf8a5e903bb89617.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd11a58887a521e6431732d49d353d2e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Image-Aesthetic-Reasoning-A-New-Benchmark-for-Medical-Image-Screening-with-MLLMs"><a href="#Image-Aesthetic-Reasoning-A-New-Benchmark-for-Medical-Image-Screening-with-MLLMs" class="headerlink" title="Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening   with MLLMs"></a>Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening   with MLLMs</h2><p><strong>Authors:Zheng Sun, Yi Wei, Long Yu</strong></p>
<p>Multimodal Large Language Models (MLLMs) are of great application across many domains, such as multimodal understanding and generation. With the development of diffusion models (DM) and unified MLLMs, the performance of image generation has been significantly improved, however, the study of image screening is rare and its performance with MLLMs is unsatisfactory due to the lack of data and the week image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive medical image screening dataset with 1500+ samples, each sample consists of a medical image, four generated images, and a multiple-choice answer. The dataset evaluates the aesthetic reasoning ability under four aspects: \textit{(1) Appearance Deformation, (2) Principles of Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}. For methodology, we utilize long chains of thought (CoT) and Group Relative Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO, to enhance the image aesthetic reasoning ability of MLLMs. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the reinforcement learning approach, we are able to surpass the score of both large-scale models and leading closed-source models using a much smaller model. We hope our attempt on medical image screening will serve as a regular configuration in image aesthetic reasoning in the future. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¼—å¤šé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œå¦‚å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚éšç€æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å’Œç»Ÿä¸€MLLMsçš„å‘å±•ï¼Œå›¾åƒç”Ÿæˆçš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œå›¾åƒç­›é€‰çš„ç ”ç©¶å¾ˆå°‘è§ï¼Œå…¶ä¸MLLMsçš„æ€§èƒ½ä¹Ÿä¸å°½äººæ„ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ•°æ®ç¼ºä¹ä»¥åŠMLLMsä¸­å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›çš„è–„å¼±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æ•°æ®å’Œæ–¹æ³•çš„è¿™äº›é—®é¢˜æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„åŒ»å­¦å›¾åƒç­›é€‰æ•°æ®é›†ï¼ŒåŒ…å«1500å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…æ‹¬ä¸€å¼ åŒ»å­¦å›¾åƒã€å››å¼ ç”Ÿæˆçš„å›¾åƒå’Œä¸€ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ã€‚è¯¥æ•°æ®é›†ä»ä»¥ä¸‹å››ä¸ªæ–¹é¢è¯„ä¼°ç¾å­¦æ¨ç†èƒ½åŠ›ï¼šï¼ˆ1ï¼‰å¤–è§‚å˜å½¢ï¼›ï¼ˆ2ï¼‰ç‰©ç†å…‰ç…§å’Œé˜´å½±åŸç†ï¼›ï¼ˆ3ï¼‰å¸ƒå±€æ”¾ç½®ï¼›ï¼ˆ4ï¼‰æ‰©å±•åˆç†æ€§ã€‚åœ¨æ–¹æ³•ä¸Šï¼Œæˆ‘ä»¬åˆ©ç”¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰å’ŒåŠ¨æ€æ¯”ä¾‹ç²¾åº¦å¥–åŠ±ä¸‹çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDPA-GRPOï¼‰ï¼Œä»¥æé«˜MLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ­ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„é—­æºMLLMsï¼Œå¦‚GPT-4oå’ŒQwen-VL-Maxï¼Œåœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ä¹Ÿå¦‚åŒéšæœºçŒœæµ‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨æ›´å°çš„æ¨¡å‹è¶…è¶Šå¤§å‹æ¨¡å‹å’Œé¢†å…ˆé—­æºæ¨¡å‹çš„å¾—åˆ†ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åŒ»å­¦å›¾åƒç­›é€‰å°è¯•èƒ½ä¸ºæœªæ¥çš„å›¾åƒç¾å­¦æ¨ç†æä¾›å¸¸è§„é…ç½®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23265v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šé¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ï¼Œå¦‚å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚éšç€æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å’Œç»Ÿä¸€MLLMsçš„å‘å±•ï¼Œå›¾åƒç”Ÿæˆæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œä½†å¯¹å›¾åƒç­›é€‰çš„ç ”ç©¶è¾ƒå°‘ï¼Œä¸”å…¶ä¸MLLMsçš„æ€§èƒ½å¹¶ä¸ç†æƒ³ï¼Œä¸»è¦ç”±äºæ•°æ®ç¼ºä¹ä»¥åŠMLLMsä¸­å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›è¾ƒå¼±ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶ä»æ•°æ®å’Œæ–¹æ³•çš„ç»´åº¦æå‡ºäº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„åŒ»å­¦å›¾åƒç­›é€‰æ•°æ®é›†ï¼ŒåŒ…å«1500å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…æ‹¬ä¸€å¼ åŒ»å­¦å›¾åƒã€å››å¼ ç”Ÿæˆå›¾åƒå’Œå¤šä¸ªé€‰æ‹©é¢˜ã€‚è¯¥æ•°æ®é›†ä»å››ä¸ªæ–¹é¢è¯„ä¼°ç¾å­¦æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤–è§‚å˜å½¢ã€ç‰©ç†å…‰çº¿å’Œé˜´å½±åŸç†ã€å¸ƒå±€æ”¾ç½®ã€æ‰©å±•åˆç†æ€§ã€‚åœ¨æ–¹æ³•ä¸Šï¼Œæˆ‘ä»¬åˆ©ç”¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰å’ŒåŠ¨æ€æ¯”ä¾‹ç²¾åº¦å¥–åŠ±ä¸‹çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDPA-GRPOï¼‰ï¼Œæé«˜MLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„é—­æºMLLMsï¼Œå¦‚GPT-4oå’ŒQwen-VL-Maxï¼Œåœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ä¹Ÿä¸éšæœºçŒœæµ‹æ— å¼‚ã€‚è€Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å³ä½¿ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä¹Ÿèƒ½è¶…è¶Šå¤§å‹æ¨¡å‹å’Œé¢†å…ˆé—­æºæ¨¡å‹çš„æˆç»©ã€‚æˆ‘ä»¬æœŸæœ›æœ¬ç ”ç©¶çš„åŒ»å­¦å›¾åƒç­›é€‰å°è¯•èƒ½ä¸ºæœªæ¥çš„å›¾åƒç¾å­¦æ¨ç†æä¾›å¸¸è§„é…ç½®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨å›¾åƒç­›é€‰æ–¹é¢çš„æ€§èƒ½æœ‰å¾…æé«˜ã€‚</li>
<li>ç¼ºä¹æ•°æ®å’ŒMLLMsä¸­çš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›å¼±æ˜¯ä¸»è¦åŸå› ã€‚</li>
<li>æå‡ºé€šè¿‡æ”¶é›†å…¨é¢çš„åŒ»å­¦å›¾åƒç­›é€‰æ•°æ®é›†æ¥è§£å†³æ•°æ®é—®é¢˜ï¼Œè¯¥æ•°æ®é›†æ³¨é‡è¯„ä¼°ç¾å­¦æ¨ç†çš„å¤šä¸ªæ–¹é¢ã€‚</li>
<li>é‡‡ç”¨é•¿é“¾æ€ç»´å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆDPA-GRPOï¼‰æ¥æé«˜MLLMsçš„å›¾åƒç¾å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„é—­æºMLLMsåœ¨å›¾åƒç¾å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚</li>
<li>åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•èƒ½åœ¨è¾ƒå°çš„æ¨¡å‹ä¸Šå®ç°è¶…è¶Šå¤§å‹å’Œé¢†å…ˆé—­æºæ¨¡å‹çš„æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d19a4c72639245ea283e992fd20ca2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94f800422ed87ab8d8f6aeab3fe7c84d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6950cabc4284cd495fc92aaacd877a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration"><a href="#MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration" class="headerlink" title="MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration"></a>MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration</h2><p><strong>Authors:Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R.,  Fung</strong></p>
<p>In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»é¢ä¸´å›ºæœ‰æŒ‘æˆ˜ï¼Œè¿™éœ€è¦è¿›è¡Œå¤šå±‚æ¬¡ï¼ˆä¾‹å¦‚ï¼Œæ„ŸçŸ¥ã€æ¨ç†ï¼‰å’Œå¤šç²’åº¦ï¼ˆä¾‹å¦‚ï¼Œå¤šæ­¥æ¨ç†é“¾ï¼‰çš„é«˜çº§æ¨æ–­ã€‚ä»¥å¾€å…³äºä¼°è®¡æ¨¡å‹ä¿¡å¿ƒçš„å·¥ä½œå¾€å¾€é›†ä¸­åœ¨è®­ç»ƒå’Œæ ¡å‡†çš„æ•´ä½“å“åº”ä¸Šï¼Œä½†æœªèƒ½è¯„ä¼°æ¯ä¸€æ­¥æ¨ç†çš„ä¿¡å¿ƒï¼Œå¯¼è‡´ä¸ç†æƒ³çš„å¹»è§‰ç´¯ç§¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MMBoundaryï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡æ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒæ ¡å‡†æé«˜MLLMsçš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºç»“åˆäº’è¡¥æ–‡æœ¬å’Œè·¨æ¨¡æ€è‡ªæˆ‘å¥–åŠ±ä¿¡å·æ¥ä¼°è®¡MLLMæ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ä¿¡å¿ƒã€‚é™¤äº†ä½¿ç”¨è‡ªæˆ‘å¥–åŠ±çš„ä¿¡å¿ƒä¼°è®¡ä¿¡å·å¯¹åˆå§‹ä¿¡å¿ƒè¡¨è¾¾è¿›è¡Œå¾®è°ƒå¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä½¿ç”¨å¤šä¸ªå¥–åŠ±å‡½æ•°è¿›ä¸€æ­¥å¯¹é½æ¨¡å‹çŸ¥è¯†å¹¶æ ¡å‡†æ¯ä¸€æ­¥æ¨ç†çš„ä¿¡å¿ƒï¼Œå¢å¼ºæ¨ç†é“¾çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒMMBoundaryåœ¨å¤šç§é¢†åŸŸæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡å‡å°‘7.5%çš„å¤šæ¨¡æ€ä¿¡å¿ƒæ ¡å‡†è¯¯å·®ï¼Œä»»åŠ¡æ€§èƒ½æé«˜8.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23224v1">PDF</a> Accepted to ACL 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿‘å¹´æ¥åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å¤šæ¨¡æ€æ¨ç†çš„å†…åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹å¯¹ä¿¡å¿ƒè¯„ä¼°å¾€å¾€é›†ä¸­åœ¨æ•´ä½“å“åº”çš„åŸ¹è®­å’Œæ ¡å‡†ä¸Šï¼Œè€Œæ— æ³•è¯„ä¼°æ¯ä¸€æ­¥æ¨ç†çš„ä¿¡å¿ƒï¼Œå¯¼è‡´å‡ºç°ä¸å¸Œæœ›å‡ºç°çš„å¹»è§‰ç´¯ç§¯ã€‚æœ¬ç ”ç©¶æå‡ºMMBoundaryæ¡†æ¶ï¼Œé€šè¿‡æ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒæ ¡å‡†æé«˜MLLMsçš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥è¡¥å……æ–‡æœ¬å’Œè·¨æ¨¡æ€è‡ªå¥–åŠ±ä¿¡å·æ¥ä¼°è®¡MLLMæ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ä¿¡å¿ƒã€‚é™¤äº†å¯¹åˆå§‹ä¿¡å¿ƒè¡¨è¾¾è¿›è¡Œå¾®è°ƒå¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œé€šè¿‡å¤šé‡å¥–åŠ±å‡½æ•°è¿›ä¸€æ­¥å¯¹é½æ¨¡å‹çŸ¥è¯†å’Œæ ¡å‡†æ¯ä¸€æ­¥æ¨ç†çš„ä¿¡å¿ƒï¼Œå¢å¼ºæ¨ç†é“¾çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒMMBoundaryåœ¨è·¨åŸŸæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡å‡å°‘7.5%çš„å¤šæ¨¡æ€ä¿¡å¿ƒæ ¡å‡†è¯¯å·®ï¼Œä»»åŠ¡æ€§èƒ½æé«˜8.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´å¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¤šçº§åˆ«å’Œå¤šç²’åº¦çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¿¡å¿ƒè¯„ä¼°ä¸»è¦é›†ä¸­åœ¨æ•´ä½“å“åº”çš„åŸ¹è®­å’Œæ ¡å‡†ï¼Œå¿½è§†äº†æ¯ä¸€æ­¥æ¨ç†çš„ä¿¡å¿ƒè¯„ä¼°ã€‚</li>
<li>MMBoundaryæ¡†æ¶é€šè¿‡æ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒæ ¡å‡†æé«˜MLLMsçš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ã€‚</li>
<li>MMBoundaryé€šè¿‡å¼•å…¥è¡¥å……æ–‡æœ¬å’Œè·¨æ¨¡æ€è‡ªå¥–åŠ±ä¿¡å·æ¥ä¼°è®¡MLLMæ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ä¿¡å¿ƒã€‚</li>
<li>MMBoundaryé‡‡ç”¨å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥å¯¹é½æ¨¡å‹çŸ¥è¯†å’Œæ ¡å‡†ä¿¡å¿ƒï¼Œå¢å¼ºæ¨ç†é“¾çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼ŒMMBoundaryåœ¨è·¨åŸŸæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d3a82488fe6458edbc2d0f5c8b086b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4e84b70fafdec136fbf20f34fc34b8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98fa6945fb647e0c77a6f1f4b4455683.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-724e70cad27d2693c6cc005ac8896228.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DIP-R1-Deep-Inspection-and-Perception-with-RL-Looking-Through-and-Understanding-Complex-Scenes"><a href="#DIP-R1-Deep-Inspection-and-Perception-with-RL-Looking-Through-and-Understanding-Complex-Scenes" class="headerlink" title="DIP-R1: Deep Inspection and Perception with RL Looking Through and   Understanding Complex Scenes"></a>DIP-R1: Deep Inspection and Perception with RL Looking Through and   Understanding Complex Scenes</h2><p><strong>Authors:Sungjune Park, Hyunjun Kim, Junho Kim, Seongho Kim, Yong Man Ro</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated significant visual understanding capabilities, yet their fine-grained visual perception in complex real-world scenarios, such as densely crowded public areas, remains limited. Inspired by the recent success of reinforcement learning (RL) in both LLMs and MLLMs, in this paper, we explore how RL can enhance visual perception ability of MLLMs. Then we develop a novel RL-based framework, Deep Inspection and Perception with RL (DIP-R1) designed to enhance the visual perception capabilities of MLLMs, by comprehending complex scenes and looking through visual instances closely. DIP-R1 guides MLLMs through detailed inspection of visual scene via three simply designed rule-based reward modelings. First, we adopt a standard reasoning reward encouraging the model to include three step-by-step processes: 1) reasoning for understanding visual scenes, 2) observing for looking through interested but ambiguous regions, and 3) decision-making for predicting answer. Second, a variance-guided looking reward is designed to examine uncertain regions for the second observing process. It explicitly enables the model to inspect ambiguous areas, improving its ability to mitigate perceptual uncertainties. Third, we model a weighted precision-recall accuracy reward enhancing accurate decision-making. We explore its effectiveness across diverse fine-grained object detection data consisting of challenging real-world environments, such as densely crowded scenes. Built upon existing MLLMs, DIP-R1 achieves consistent and significant improvement across various in-domain and out-of-domain scenarios. It also outperforms various existing baseline models and supervised fine-tuning methods. Our findings highlight the substantial potential of integrating RL into MLLMs for enhancing capabilities in complex real-world perception tasks. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»å±•ç°å‡ºæ˜¾è‘—çš„ç†è§£è§†è§‰ä¿¡æ¯çš„èƒ½åŠ›ï¼Œç„¶è€Œå®ƒä»¬åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„ç²¾ç»†è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‹¥æŒ¤çš„å…¬å…±åœºæ‰€ç­‰åœºæ™¯ã€‚æœ¬æ–‡å—å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å–å¾—æˆåŠŸçš„å¯å‘ï¼Œæ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºMLLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶â€”â€”ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ·±åº¦æ£€æµ‹ä¸æ„ŸçŸ¥ï¼ˆDIP-R1ï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ç†è§£å¤æ‚åœºæ™¯å’Œè¿‘è·ç¦»è§‚å¯Ÿè§†è§‰å®ä¾‹æ¥å¢å¼ºMLLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚DIP-R1é€šè¿‡ä¸‰ç§ç®€å•è®¾è®¡çš„åŸºäºè§„åˆ™çš„å¥–åŠ±å»ºæ¨¡æ¥å¼•å¯¼MLLMså¯¹è§†è§‰åœºæ™¯è¿›è¡Œè¯¦ç»†æ£€æŸ¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨æ ‡å‡†æ¨ç†å¥–åŠ±æ¥é¼“åŠ±æ¨¡å‹é‡‡ç”¨ä¸‰ä¸ªé€æ­¥è¿‡ç¨‹ï¼š1ï¼‰ç†è§£è§†è§‰åœºæ™¯çš„æ¨ç†è¿‡ç¨‹ï¼›2ï¼‰è§‚å¯Ÿæ„Ÿå…´è¶£ä½†æ¨¡ç³Šçš„åŒºåŸŸï¼›ä»¥åŠ3ï¼‰é¢„æµ‹ç­”æ¡ˆçš„å†³ç­–è¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†ä¸€ç§æ–¹å·®å¼•å¯¼çš„è§‚å¯Ÿå¥–åŠ±ï¼Œç”¨äºæ£€æŸ¥ç¬¬äºŒä¸ªè§‚å¯Ÿè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šåŒºåŸŸã€‚è¿™æ˜ç¡®åœ°ä½¿æ¨¡å‹èƒ½å¤Ÿæ£€æŸ¥æ¨¡ç³ŠåŒºåŸŸï¼Œæé«˜å…¶è§£å†³æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ç§åŠ æƒç²¾åº¦å¬å›å‡†ç¡®æ€§å¥–åŠ±ï¼Œä»¥æé«˜å†³ç­–çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ç”±å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ç¯å¢ƒç»„æˆçš„å¤šç§ç²¾ç»†å¯¹è±¡æ£€æµ‹æ•°æ®ä¸Šæ¢ç´¢äº†å…¶æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚æ‹¥æŒ¤çš„åœºæ™¯ã€‚åŸºäºç°æœ‰çš„MLLMsï¼ŒDIP-R1åœ¨å„ç§é¢†åŸŸå†…å’Œè·¨é¢†åŸŸçš„åœºæ™¯ä¸­å®ç°äº†æŒç»­ä¸”æ˜¾è‘—çš„æ”¹è¿›ã€‚å®ƒä¹Ÿä¼˜äºå„ç§ç°æœ‰çš„åŸºçº¿æ¨¡å‹å’Œç»è¿‡ç›‘ç£å¾®è°ƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†å°†å¼ºåŒ–å­¦ä¹ æ•´åˆåˆ°MLLMsä¸­ä»¥æé«˜å¤æ‚ç°å®æ„ŸçŸ¥ä»»åŠ¡èƒ½åŠ›çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23179v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²å±•ç°å‡ºæ˜¾è‘—çš„ç†è§£è§†è§‰èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„ç²¾ç»†è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ä»ç„¶å—é™ã€‚æœ¬æ–‡å—å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨LLMså’ŒMLLMsä¸­æˆåŠŸçš„å¯å‘ï¼Œæ¢ç´¢äº†RLå¦‚ä½•å¢å¼ºMLLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚éšåï¼Œå¼€å‘äº†ä¸€ç§æ–°å‹çš„åŸºäºRLçš„æ¡†æ¶â€”â€”æ·±åº¦æ£€æµ‹ä¸æ„ŸçŸ¥RLï¼ˆDIP-R1ï¼‰ï¼Œæ—¨åœ¨é€šè¿‡ç†è§£å¤æ‚åœºæ™¯å’Œä»”ç»†è§‚å¯Ÿè§†è§‰å®ä¾‹æ¥å¢å¼ºMLLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚DIP-R1é€šè¿‡ä¸‰ä¸ªç®€å•çš„åŸºäºè§„åˆ™çš„å¥–åŠ±æ¨¡å‹æŒ‡å¯¼MLLMså¯¹è§†è§‰åœºæ™¯è¿›è¡Œè¯¦ç»†æ£€æŸ¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨æ ‡å‡†æ¨ç†å¥–åŠ±é¼“åŠ±æ¨¡å‹åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤çš„è¿‡ç¨‹ï¼šç†è§£è§†è§‰åœºæ™¯è¿›è¡Œæ¨ç†ã€è§‚å¯Ÿæœ‰è¶£ä½†æ¨¡ç³Šçš„åŒºåŸŸã€é¢„æµ‹ç­”æ¡ˆè¿›è¡Œå†³ç­–ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†ä¸€ç§æ–¹å·®å¼•å¯¼è§‚å¯Ÿå¥–åŠ±ï¼Œç”¨äºæ£€æŸ¥ç¬¬äºŒä¸ªè§‚å¯Ÿè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šåŒºåŸŸã€‚å®ƒä½¿æ¨¡å‹èƒ½å¤Ÿæ£€æŸ¥æ¨¡ç³ŠåŒºåŸŸï¼Œæé«˜å…¶å‡å°‘æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŠ æƒç²¾åº¦å¬å›å‡†ç¡®æ€§å¥–åŠ±ä»¥æé«˜å†³ç­–çš„å‡†ç¡®æ€§ã€‚åœ¨ç”±å¯†é›†æ‹¥æŒ¤åœºæ™¯ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ç¯å¢ƒç»„æˆçš„å¤šæ ·åŒ–ç²¾ç»†ç›®æ ‡æ£€æµ‹æ•°æ®ä¸Šï¼ŒéªŒè¯äº†å…¶åœ¨å„ç§é¢†åŸŸå†…å¤–åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œå¹¶ä¼˜äºå„ç§ç°æœ‰çš„åŸºçº¿æ¨¡å‹å’Œç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†å¼ºåŒ–å­¦ä¹ æ•´åˆåˆ°MLLMsä¸­ï¼Œå¯¹äºæé«˜å¤æ‚ç°å®æ„ŸçŸ¥ä»»åŠ¡çš„æ€§èƒ½å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MLLMsåœ¨å¤æ‚ç°å®åœºæ™¯ä¸­çš„ç²¾ç»†è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å—é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶DIP-R1ï¼Œæ—¨åœ¨å¢å¼ºMLLMsçš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>DIP-R1åŒ…å«ä¸‰ä¸ªåŸºäºè§„åˆ™çš„å¥–åŠ±æ¨¡å‹ï¼Œä»¥æŒ‡å¯¼MLLMsè¯¦ç»†æ£€æŸ¥è§†è§‰åœºæ™¯ã€‚</li>
<li>é€šè¿‡æ ‡å‡†æ¨ç†å¥–åŠ±é¼“åŠ±æ¨¡å‹è¿›è¡Œä¸‰ä¸ªæ­¥éª¤çš„è¿‡ç¨‹ï¼šç†è§£è§†è§‰åœºæ™¯ã€è§‚å¯Ÿæœ‰è¶£åŒºåŸŸã€åšå‡ºé¢„æµ‹å†³ç­–ã€‚</li>
<li>è®¾è®¡äº†æ–¹å·®å¼•å¯¼è§‚å¯Ÿå¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ£€æŸ¥æ¨¡ç³ŠåŒºåŸŸå¹¶å‡å°‘æ„ŸçŸ¥ä¸ç¡®å®šæ€§ã€‚</li>
<li>åŠ æƒç²¾åº¦å¬å›å‡†ç¡®æ€§å¥–åŠ±æé«˜å†³ç­–çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51b1d865219faea4dd26ca2f433d7f72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97ca7710d8ca573a0198d379d0a2790e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c31ac52512d13807e89fc7359edfbda1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PBEBench-A-Multi-Step-Programming-by-Examples-Reasoning-Benchmark-inspired-by-Historical-Linguistics"><a href="#PBEBench-A-Multi-Step-Programming-by-Examples-Reasoning-Benchmark-inspired-by-Historical-Linguistics" class="headerlink" title="PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark   inspired by Historical Linguistics"></a>PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark   inspired by Historical Linguistics</h2><p><strong>Authors:Atharva Naik, Darsh Agrawal, Manav Kapadnis, Yuwei An, Yash Mathur, Carolyn Rose, David Mortensen</strong></p>
<p>Recently, long chain of thought (LCoT), Large Language Models (LLMs), have taken the machine learning world by storm with their breathtaking reasoning capabilities. However, are the abstract reasoning abilities of these models general enough for problems of practical importance? Unlike past work, which has focused mainly on math, coding, and data wrangling, we focus on a historical linguistics-inspired inductive reasoning problem, formulated as Programming by Examples. We develop a fully automated pipeline for dynamically generating a benchmark for this task with controllable difficulty in order to tackle scalability and contamination issues to which many reasoning benchmarks are subject. Using our pipeline, we generate a test set with nearly 1k instances that is challenging for all state-of-the-art reasoning LLMs, with the best model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating that LCoT LLMs still struggle with a class or reasoning that is ubiquitous in historical linguistics as well as many other domains. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé•¿é“¾æ€ç»´ï¼ˆLong Chain of Thoughtï¼Œç®€ç§°LCoTï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼Œç®€ç§°LLMï¼‰ä»¥å…¶æƒŠäººçš„æ¨ç†èƒ½åŠ›å¸­å·äº†æœºå™¨å­¦ä¹ é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›æ˜¯å¦è¶³å¤Ÿåº”å¯¹å®é™…é‡è¦é—®é¢˜å‘¢ï¼Ÿä¸è¿‡å»ä¸»è¦é›†ä¸­åœ¨æ•°å­¦ã€ç¼–ç å’Œæ•°æ®å¤„ç†çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬å…³æ³¨ä¸€ä¸ªå—å†å²è¯­è¨€å­¦å¯å‘çš„å½’çº³æ¨ç†é—®é¢˜ï¼Œå°†å…¶å½¢å¼åŒ–ä¸ºé€šè¿‡ç¤ºä¾‹ç¼–ç¨‹ï¼ˆProgramming by Examplesï¼‰ã€‚ä¸ºäº†è§£å†³è®¸å¤šæ¨ç†åŸºå‡†æµ‹è¯•æ‰€é¢ä¸´çš„æ‰©å±•æ€§å’Œæ±¡æŸ“é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨è‡ªåŠ¨åŒ–çš„ç®¡é“ï¼Œç”¨äºåŠ¨æ€ç”Ÿæˆå…·æœ‰å¯æ§éš¾åº¦çš„åŸºå‡†æµ‹è¯•ã€‚ä½¿ç”¨æˆ‘ä»¬çš„ç®¡é“ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«è¿‘1kå®ä¾‹çš„æµ‹è¯•é›†ï¼Œå¯¹æ‰€æœ‰æœ€å…ˆè¿›çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹æ„æˆäº†æŒ‘æˆ˜ã€‚æœ€å¥½çš„æ¨¡å‹ï¼ˆClaude-3.7-Sonnetï¼‰é€šè¿‡ç‡ä»…ä¸º54%ï¼Œè¿™è¡¨æ˜LCoTå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†å²è¯­è¨€å­¦ä»¥åŠå…¶ä»–è®¸å¤šé¢†åŸŸä¸­æ™®éå­˜åœ¨çš„ä¸€ç±»æ¨ç†ä»ç„¶å›°éš¾é‡é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23126v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é•¿é“¾æ€ç»´ï¼ˆLCoTï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ä»¤äººç©ç›®ï¼Œä½†åœ¨å®é™…é‡è¦é—®é¢˜çš„æŠ½è±¡æ¨ç†èƒ½åŠ›æ˜¯å¦è¶³å¤Ÿé€šç”¨ä»å­˜åœ¨ç–‘é—®ã€‚ä¸åŒäºè¿‡å»ä¸»è¦é›†ä¸­åœ¨æ•°å­¦ã€ç¼–ç å’Œæ•°æ®æ•´ç†çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å…³æ³¨å†å²è¯­è¨€å­¦å¯å‘å¼çš„å½’çº³æ¨ç†é—®é¢˜ï¼Œå¹¶å°†å…¶åˆ¶å®šä¸ºâ€œé€šè¿‡ç¤ºä¾‹ç¼–ç¨‹â€ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨è‡ªåŠ¨ç®¡é“ï¼Œå¯ä»¥åŠ¨æ€ç”Ÿæˆæ­¤ä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ§åˆ¶å…¶éš¾åº¦ï¼Œä»¥è§£å†³å½“å‰è®¸å¤šæ¨ç†åŸºå‡†æµ‹è¯•çš„æ‰©å±•æ€§å’Œæ±¡æŸ“é—®é¢˜ã€‚ä½¿ç”¨æ­¤ç®¡é“ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«è¿‘1000ä¸ªå®ä¾‹çš„æµ‹è¯•é›†ï¼Œå¯¹äºæ‰€æœ‰æœ€å…ˆè¿›çš„æ¨ç†LLMéƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæœ€å¥½çš„æ¨¡å‹ï¼ˆClaude-3.7-Sonnetï¼‰é€šè¿‡ç‡ä»…ä¸º54%ï¼Œè¿™è¡¨æ˜LCoT LLMåœ¨å†å²è¯­è¨€å­¦ä»¥åŠå…¶ä»–è®¸å¤šé¢†åŸŸçš„é€šç”¨ç±»æ¨ç†ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LCoTå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨è§£å†³å®é™…é—®é¢˜æ—¶å…¶é€šç”¨æ€§å°šå¾…æ£€éªŒã€‚</li>
<li>ä¸ä»¥å¾€å…³æ³¨æ•°å­¦ã€ç¼–ç å’Œæ•°æ®æ•´ç†çš„ç ”ç©¶ä¸åŒï¼Œè¯¥æ–‡æœ¬èšç„¦å†å²è¯­è¨€å­¦å¯å‘å¼çš„å½’çº³æ¨ç†é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§å…¨æ–°çš„æ–¹æ³•â€”â€”â€œé€šè¿‡ç¤ºä¾‹ç¼–ç¨‹â€æ¥ç ”ç©¶å’Œæµ‹è¯•LLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªå…¨è‡ªåŠ¨ç®¡é“æ¥åŠ¨æ€ç”Ÿæˆå…·æœ‰å¯æ§éš¾åº¦çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥è§£å†³ç°æœ‰æ¨ç†åŸºå‡†æµ‹è¯•çš„æ‰©å±•æ€§å’Œæ±¡æŸ“é—®é¢˜ã€‚</li>
<li>è¯¥ç®¡é“ç”Ÿæˆçš„æµ‹è¯•é›†åŒ…å«è¿‘1000ä¸ªå®ä¾‹ï¼Œå¯¹ç°æœ‰LLMsæ„æˆæŒ‘æˆ˜ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆå¦‚Claude-3.7-Sonnetï¼‰åœ¨å†å²è¯­è¨€å­¦é¢†åŸŸçš„é€šç”¨ç±»æ¨ç†ä»»åŠ¡é€šè¿‡ç‡è¾ƒä½ï¼ˆä»…ä¸º54%ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7f968e8535ce09636b57411d4cfb3b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3c87b391a69e295c3a25bfd1aaffae2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Infi-MMR-Curriculum-based-Unlocking-Multimodal-Reasoning-via-Phased-Reinforcement-Learning-in-Multimodal-Small-Language-Models"><a href="#Infi-MMR-Curriculum-based-Unlocking-Multimodal-Reasoning-via-Phased-Reinforcement-Learning-in-Multimodal-Small-Language-Models" class="headerlink" title="Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased   Reinforcement Learning in Multimodal Small Language Models"></a>Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased   Reinforcement Learning in Multimodal Small Language Models</h2><p><strong>Authors:Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang</strong></p>
<p>Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the modelâ€™s logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini). </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œä¾‹å¦‚DeepSeek-R1ï¼Œå®ƒåˆ©ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ¥æ˜¾è‘—å¢å¼ºé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æˆå°±æ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é¢ä¸´ç€ä¸¥å³»æŒ‘æˆ˜ï¼Œå¯¹äºå¤šæ¨¡æ€å°å‹è¯­è¨€æ¨¡å‹ï¼ˆMSLMï¼‰æ¥è¯´ï¼Œè¿™äº›æŒ‘æˆ˜æ›´åŠ çªå‡ºï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸çš„åŸºç¡€æ¨ç†èƒ½åŠ›è¾ƒå¼±ï¼šï¼ˆ1ï¼‰é«˜è´¨é‡å¤šæ¨¡æ€æ¨ç†æ•°æ®é›†çš„ç¨€ç¼ºï¼Œï¼ˆ2ï¼‰ç”±äºé›†æˆè§†è§‰å¤„ç†å¯¼è‡´çš„æ¨ç†èƒ½åŠ›ä¸‹é™ï¼Œï¼ˆ3ï¼‰ç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ å¯èƒ½äº§ç”Ÿå¤æ‚è€Œé”™è¯¯çš„æ¨ç†è¿‡ç¨‹çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–æ¡†æ¶Infi-MMRï¼Œé€šè¿‡ä¸‰ä¸ªç²¾å¿ƒæ„å»ºçš„é˜¶æ®µç³»ç»Ÿåœ°è§£é”MSLMçš„æ¨ç†æ½œåŠ›ï¼Œå¹¶æå‡ºäº†æˆ‘ä»¬çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹Infi-MMR-3Bã€‚ç¬¬ä¸€é˜¶æ®µï¼ŒåŸºç¡€æ¨ç†æ¿€æ´»ï¼Œåˆ©ç”¨é«˜è´¨é‡æ–‡æœ¬æ¨ç†æ•°æ®é›†æ¥æ¿€æ´»å’ŒåŠ å¼ºæ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µï¼Œè·¨æ¨¡æ€æ¨ç†é€‚åº”ï¼Œä½¿ç”¨å¢å¼ºçš„å¤šåª’ä½“æ•°æ®æ¥ä¿ƒè¿›æ¨ç†æŠ€èƒ½å‘å¤šæ¨¡æ€ç¯å¢ƒçš„é€æ­¥è½¬ç§»ã€‚ç¬¬ä¸‰é˜¶æ®µï¼Œå¤šæ¨¡æ€æ¨ç†å¢å¼ºï¼Œé‡‡ç”¨ç²¾é€‰çš„æ— å­—å¹•å¤šåª’ä½“æ•°æ®æ¥ç¼“è§£è¯­è¨€åè§ï¼Œå¹¶ä¿ƒè¿›ç¨³å¥çš„è·¨æ¨¡æ€æ¨ç†ã€‚Infi-MMR-3Bä¸ä»…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è·¨æ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›ï¼ˆåœ¨MathVerseæµ‹è¯•é›†ä¸Šè¾¾åˆ°43.68%ï¼Œåœ¨MathVisionæµ‹è¯•é›†ä¸Šè¾¾åˆ°27.04%ï¼Œåœ¨OlympiadBenchä¸Šè¾¾åˆ°21.33%ï¼‰ï¼Œè€Œä¸”è¾¾åˆ°äº†é€šç”¨æ¨ç†èƒ½åŠ›ï¼ˆåœ¨MathVistaæµ‹è¯•é›†ä¸Šè¾¾åˆ°67.2%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå¦‚DeepSeek-R1åˆ©ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ¥æå‡é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æˆå°±æ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ—¶ï¼Œé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€å°å‹è¯­è¨€æ¨¡å‹ï¼ˆMSLMsï¼‰ä¸­æ›´ä¸ºçªå‡ºï¼Œå…¶åŸºç¡€æ¨ç†èƒ½åŠ›é€šå¸¸è¾ƒå¼±ã€‚æŒ‘æˆ˜åŒ…æ‹¬é«˜è´¨é‡å¤šæ¨¡æ€æ¨ç†æ•°æ®é›†çš„ç¨€ç¼ºæ€§ã€å› é›†æˆè§†è§‰å¤„ç†è€Œé™ä½çš„æ¨ç†èƒ½åŠ›ä»¥åŠç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ å¯èƒ½äº§ç”Ÿå¤æ‚ä¸”é”™è¯¯çš„æ¨ç†è¿‡ç¨‹çš„é£é™©ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†Infi-MMRæ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªç²¾å¿ƒæ„å»ºçš„é˜¶æ®µç³»ç»Ÿåœ°è§£é”MSLMsçš„æ¨ç†æ½œåŠ›ï¼Œå¹¶æå‡ºäº†å¤šæ¨¡æ€æ¨ç†æ¨¡å‹Infi-MMR-3Bã€‚è¯¥æ¨¡å‹ç»å†ä¸‰ä¸ªé˜¶æ®µï¼Œä¾æ¬¡æ¿€æ´»åŸºç¡€æ¨ç†èƒ½åŠ›ã€é€‚åº”è·¨æ¨¡æ€æ¨ç†ä»¥åŠå¢å¼ºå¤šæ¨¡æ€æ¨ç†ã€‚Infi-MMR-3Bä¸ä»…å…·æœ‰æœ€å…ˆè¿›çš„è·¨æ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œåœ¨MathVerseæµ‹è¯•minié›†ä¸Šè¾¾åˆ°äº†43.68%ï¼Œåœ¨æ•°å­¦è§†è§‰æµ‹è¯•ä¸Šè¾¾åˆ°äº†27.04%ï¼Œåœ¨OlympiadBenchä¸Šè¾¾åˆ°äº†21.33%ï¼Œè¿˜å…·æœ‰å‡ºè‰²çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œåœ¨MathVistaæµ‹è¯•minié›†ä¸Šè¾¾åˆ°äº†67.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å°†æˆæœæ‰©å±•åˆ°å¤šæ¨¡æ€é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡æ€å°å‹è¯­è¨€æ¨¡å‹ï¼ˆMSLMsï¼‰çš„åŸºç¡€æ¨ç†èƒ½åŠ›è¾ƒå¼±ï¼Œé¢ä¸´é«˜è´¨é‡å¤šæ¨¡æ€æ¨ç†æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>å¤šæ¨¡æ€æ¨ç†é¢ä¸´ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šæ•°æ®ç¨€ç¼ºã€è§†è§‰å¤„ç†å¯¼è‡´çš„æ¨ç†èƒ½åŠ›ä¸‹é™ä»¥åŠå¼ºåŒ–å­¦ä¹ çš„æ½œåœ¨é£é™©ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶Infi-MMRï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µé€æ­¥å¢å¼ºMSLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä¾§é‡äºæ¿€æ´»åŸºç¡€æ¨ç†èƒ½åŠ›ï¼Œåˆ©ç”¨é«˜è´¨é‡æ–‡æœ¬æ¨ç†æ•°æ®é›†ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€‚åº”è·¨æ¨¡æ€æ¨ç†ï¼Œåˆ©ç”¨å¸¦æœ‰æè¿°çš„å¤šåª’ä½“æ•°æ®ä¿ƒè¿›æŠ€èƒ½è¿ç§»ã€‚</li>
<li>ç¬¬ä¸‰é˜¶æ®µå¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨ç²¾é€‰çš„æ— æè¿°å¤šåª’ä½“æ•°æ®ä»¥å‡å°‘è¯­è¨€åè§å¹¶ä¿ƒè¿›ç¨³å¥çš„è·¨æ¨¡æ€æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4708c72b6b45bb7323865714b82ecd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-631ffdb5149f238b9186a5dde5d53f4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1b618682fd31369a000346471ea885d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="WorkForceAgent-R1-Incentivizing-Reasoning-Capability-in-LLM-based-Web-Agents-via-Reinforcement-Learning"><a href="#WorkForceAgent-R1-Incentivizing-Reasoning-Capability-in-LLM-based-Web-Agents-via-Reinforcement-Learning" class="headerlink" title="WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web   Agents via Reinforcement Learning"></a>WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web   Agents via Reinforcement Learning</h2><p><strong>Authors:Yuchen Zhuang, Di Jin, Jiaao Chen, Wenqi Shi, Hanrui Wang, Chao Zhang</strong></p>
<p>Large language models (LLMs)-empowered web agents enables automating complex, real-time web navigation tasks in enterprise environments. However, existing web agents relying on supervised fine-tuning (SFT) often struggle with generalization and robustness due to insufficient reasoning capabilities when handling the inherently dynamic nature of web interactions. In this study, we introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based R1-style reinforcement learning framework designed explicitly to enhance single-step reasoning and planning for business-oriented web navigation tasks. We employ a structured reward function that evaluates both adherence to output formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly learn robust intermediate reasoning without explicit annotations or extensive expert demonstrations. Extensive experiments on the WorkArena benchmark demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by 10.26-16.59%, achieving competitive performance relative to proprietary LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç½‘ç»œä»£ç†èƒ½å¤Ÿè‡ªåŠ¨å®Œæˆä¼ä¸šç¯å¢ƒä¸­çš„å¤æ‚ã€å®æ—¶ç½‘ç»œå¯¼èˆªä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¾èµ–äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ç½‘ç»œä»£ç†å¾€å¾€ç”±äºå¤„ç†ç½‘ç»œäº¤äº’çš„å†…åœ¨åŠ¨æ€æ€§æ—¶æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œåœ¨æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†WorkForceAgent-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„ç½‘ç»œä»£ç†ï¼Œé‡‡ç”¨åŸºäºè§„åˆ™çš„R1é£æ ¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œä¸“é—¨ä¸ºæé«˜é¢å‘å•†ä¸šç½‘ç»œå¯¼èˆªä»»åŠ¡çš„å•æ­¥æ¨ç†å’Œè§„åˆ’èƒ½åŠ›è€Œè®¾è®¡ã€‚æˆ‘ä»¬é‡‡ç”¨ç»“æ„åŒ–çš„å¥–åŠ±å‡½æ•°ï¼Œæ—¢è¯„ä¼°è¾“å‡ºæ ¼å¼çš„éµå¾ªæƒ…å†µï¼Œåˆè¯„ä¼°è¡ŒåŠ¨çš„æ­£ç¡®æ€§ï¼Œä½¿WorkForceAgent-R1èƒ½å¤Ÿéšæ€§å­¦ä¹ ç¨³å¥çš„ä¸­é—´æ¨ç†ï¼Œè€Œæ— éœ€æ˜ç¡®çš„æ³¨é‡Šæˆ–å¤§é‡çš„ä¸“å®¶æ¼”ç¤ºã€‚åœ¨WorkArenaåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒWorkForceAgent-R1æ˜¾è‘—ä¼˜äºSFTåŸºå‡†æµ‹è¯•ï¼Œé«˜å‡º10.26-16.59%ï¼Œåœ¨é¢å‘å·¥ä½œåœºæ‰€çš„ç½‘ç»œå¯¼èˆªä»»åŠ¡ä¸­ï¼Œå…¶æ€§èƒ½ä¸ä¸“æœ‰LLMä»£ç†ï¼ˆgpt-4oï¼‰ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22942v1">PDF</a> Work in Progress</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Webä»£ç†èƒ½å¤Ÿè‡ªåŠ¨åŒ–ä¼ä¸šç¯å¢ƒä¸­çš„å¤æ‚ã€å®æ—¶Webå¯¼èˆªä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ç°æœ‰Webä»£ç†åœ¨å¤„ç†Webäº¤äº’çš„å†…åœ¨åŠ¨æ€æ€§æ—¶ï¼Œç”±äºæ¨ç†èƒ½åŠ›çš„ä¸è¶³ï¼Œå¸¸å¸¸é¢ä¸´æ³›åŒ–å’Œç¨³å¥æ€§é—®é¢˜ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†WorkForceAgent-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„Webä»£ç†ï¼Œé‡‡ç”¨åŸºäºè§„åˆ™çš„R1é£æ ¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œä¸“ä¸ºå¢å¼ºé¢å‘å•†ä¸šçš„Webå¯¼èˆªä»»åŠ¡çš„å•æ­¥æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨ç»“æ„åŒ–çš„å¥–åŠ±å‡½æ•°æ¥è¯„ä¼°è¾“å‡ºæ ¼å¼çš„éµå¾ªæƒ…å†µå’Œè¡ŒåŠ¨çš„å‡†ç¡®æ€§ï¼Œä½¿WorkForceAgent-R1èƒ½å¤Ÿåœ¨æ— éœ€æ˜ç¡®æ ‡æ³¨æˆ–å¤§é‡ä¸“å®¶æ¼”ç¤ºçš„æƒ…å†µä¸‹ï¼Œéšå¼å­¦ä¹ ç¨³å¥çš„ä¸­é—´æ¨ç†ã€‚åœ¨WorkArenaåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒWorkForceAgent-R1è¾ƒSFTåŸºçº¿æœ‰æ˜¾è‘—æé«˜ï¼Œæ€§èƒ½æå‡å¹…åº¦ä¸º10.26%~16.59%ï¼Œå¹¶ä¸”åœ¨é¢å‘èŒåœºçš„Webå¯¼èˆªä»»åŠ¡ä¸­ï¼Œä¸ä¸“æœ‰LLMä»£ç†ï¼ˆå¦‚gpt-4oï¼‰ç›¸æ¯”è¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹èƒ½çš„Webä»£ç†å¯ä»¥è‡ªåŠ¨åŒ–å¤æ‚çš„å®æ—¶Webå¯¼èˆªä»»åŠ¡ã€‚</li>
<li>ç°æœ‰Webä»£ç†åœ¨æ³›åŒ–å’Œç¨³å¥æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¤„ç†Webäº¤äº’çš„åŠ¨æ€æ€§æ—¶ã€‚</li>
<li>WorkForceAgent-R1æ˜¯ä¸€ä¸ªåŸºäºLLMçš„Webä»£ç†ï¼Œé‡‡ç”¨è§„åˆ™å¼ºåŒ–å­¦ä¹ æ¡†æ¶æå‡å•†ä¸šWebå¯¼èˆªä»»åŠ¡çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚</li>
<li>WorkForceAgent-R1é€šè¿‡ç»“æ„åŒ–çš„å¥–åŠ±å‡½æ•°éšå¼å­¦ä¹ ç¨³å¥çš„ä¸­é—´æ¨ç†ï¼Œæ— éœ€æ˜ç¡®æ ‡æ³¨æˆ–å¤§é‡ä¸“å®¶æ¼”ç¤ºã€‚</li>
<li>WorkForceAgent-R1åœ¨WorkArenaåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¾ƒç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åŸºçº¿æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>WorkForceAgent-R1åœ¨é¢å‘èŒåœºçš„Webå¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä¸ä¸“æœ‰LLMä»£ç†ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc7becc69ae57e802b8bbdaf6986fe9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a9cf2b616183bf6d022f36e3ed73311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c82a2609a117583355a83b40ef2682.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9eb10996b851c0b9b16bc965a8cbae1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="cadrille-Multi-modal-CAD-Reconstruction-with-Online-Reinforcement-Learning"><a href="#cadrille-Multi-modal-CAD-Reconstruction-with-Online-Reinforcement-Learning" class="headerlink" title="cadrille: Multi-modal CAD Reconstruction with Online Reinforcement   Learning"></a>cadrille: Multi-modal CAD Reconstruction with Online Reinforcement   Learning</h2><p><strong>Authors:Maksim Kolodiazhnyi, Denis Tarasov, Dmitrii Zhemchuzhnikov, Alexander Nikulin, Ilya Zisman, Anna Vorontsova, Anton Konushin, Vladislav Kurenkov, Danila Rukhovich</strong></p>
<p>Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ï¼Œèƒ½å¤Ÿåˆ›å»ºç²¾ç¡®ä¸”å¯ç¼–è¾‘çš„3Dæ¨¡å‹ã€‚ä½¿ç”¨å„ç§ä¼ æ„Ÿå™¨æˆ–ç”¨æˆ·æä¾›çš„æ•°æ®ä½œä¸ºCADé‡å»ºçš„è¾“å…¥ï¼Œå¯ä»¥ä½¿è®¾è®¡åº”ç”¨ç¨‹åºçš„è®¿é—®æ›´åŠ æ°‘ä¸»åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨å•ä¸€çš„è¾“å…¥æ¨¡å¼ï¼Œå¦‚ç‚¹äº‘ã€å›¾åƒæˆ–æ–‡æœ¬ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬å€ŸåŠ©æœ€æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€CADé‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åŒæ—¶å¤„ç†ä¸‰ç§è¾“å…¥æ¨¡å¼ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒèŒƒå¼çš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆåœ¨å¤§è§„æ¨¡ç¨‹åºç”Ÿæˆæ•°æ®ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç„¶åé‡‡ç”¨åœ¨çº¿åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒï¼Œè¿™äº›åé¦ˆæ˜¯ç¨‹åºè·å–çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡æ¢ç´¢åœ¨CADä»»åŠ¡ä¸­ä½¿ç”¨LLMçš„RLå¾®è°ƒï¼Œè¯æ˜åœ¨çº¿RLç®—æ³•ï¼ˆå¦‚Group Relative Preference Optimizationï¼ˆGRPOï¼‰ï¼‰èƒœè¿‡ç¦»çº¿æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨DeepCADåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„SFTæ¨¡å‹åœ¨ä¸‰ç§è¾“å…¥æ¨¡å¼ä¸Šå‡ä¼˜äºç°æœ‰çš„å•æ¨¡æ€æ–¹æ³•ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç»è¿‡RLå¾®è°ƒåï¼Œcadrilleåœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜çš„æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22914v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—æœºè¾…åŠ©æ•™å­¦è®¾è®¡çš„å…³é”®æŠ€æœ¯åœ¨å·¥ç¨‹å’Œåˆ¶é€ é¢†åŸŸçš„åº”ç”¨åŠå…¶ä¼˜åŠ¿ï¼Œæœ¬æ–‡ä¸»è¦ä»‹ç»äº†é‡‡ç”¨å¤šç§ä¼ æ„Ÿå™¨æˆ–ç”¨æˆ·æä¾›çš„å¤šç§æ•°æ®ä½œä¸ºè®¡ç®—æœºè¾…åŠ©è®¾è®¡é‡å»ºçš„è¾“å…¥ï¼Œé€šè¿‡åˆ©ç”¨æœ€æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è®¡ç®—æœºè¾…åŠ©è®¾è®¡é‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åŒæ—¶å¤„ç†ä¸‰ç§è¾“å…¥æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç²¾ç»†è°ƒæ•´çš„æ–¹æ³•åœ¨CADä»»åŠ¡ä¸­çš„åº”ç”¨æ•ˆæœï¼Œç»“æœæ˜¾ç¤ºï¼Œä¸å…¶ä»–çº¿ä¸‹æ›¿ä»£æ–¹æ¡ˆç›¸æ¯”ï¼Œé›†å›¢ç›¸å¯¹åå¥½ä¼˜åŒ–ç­‰åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©æ•™å­¦è®¾è®¡åœ¨åˆ›å»ºç²¾ç¡®ä¸”å¯ç¼–è¾‘çš„ä¸‰ç»´æ¨¡å‹æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸ä»…ä¸“æ³¨äºå•ä¸€çš„è¾“å…¥æ¨¡å¼ï¼Œå¦‚ç‚¹äº‘ã€å›¾åƒæˆ–æ–‡æœ¬ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</li>
<li>åˆ©ç”¨æœ€æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è®¡ç®—æœºè¾…åŠ©è®¾è®¡é‡å»ºæ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ä¸‰ç§è¾“å…¥æ¨¡å¼ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç²¾ç»†è°ƒæ•´æ–¹æ³•ç”¨äºCADä»»åŠ¡ã€‚</li>
<li>åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•å¦‚é›†å›¢ç›¸å¯¹åå¥½ä¼˜åŒ–åœ¨CADä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨DeepCADåŸºå‡†æµ‹è¯•ä¸­ï¼Œç²¾ç»†è°ƒæ•´åçš„æ¨¡å‹åœ¨æ‰€æœ‰ä¸‰ç§è¾“å…¥æ¨¡å¼ä¸Šçš„æ€§èƒ½å‡ä¼˜äºç°æœ‰çš„å•æ¨¡æ€æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6ed245f01a08dbaf350fa40c20d291d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2c948de385ec04a2e23b34153e9e315.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-547dc86ba4c2c54237a277985093fcc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26eaafa4b55ab3e8c2172b7fc164265f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Pangu-Embedded-An-Efficient-Dual-system-LLM-Reasoner-with-Metacognition"><a href="#Pangu-Embedded-An-Efficient-Dual-system-LLM-Reasoner-with-Metacognition" class="headerlink" title="Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition"></a>Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition</h2><p><strong>Authors:Hanting Chen, Yasheng Wang, Kai Han, Dong Li, Lin Li, Zhenni Bi, Jinpeng Li, Haoyu Wang, Fei Mi, Mingjian Zhu, Bin Wang, Kaikai Song, Yifei Fu, Xu He, Yu Luo, Chong Zhu, Quan He, Xueyu Wu, Wei He, Hailin Hu, Yehui Tang, Dacheng Tao, Xinghao Chen, Yunhe Wang</strong></p>
<p>This work presents Pangu Embedded, an efficient Large Language Model (LLM) reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible fast and slow thinking capabilities. Pangu Embedded addresses the significant computational costs and inference latency challenges prevalent in existing reasoning-optimized LLMs. We propose a two-stage training framework for its construction. In Stage 1, the model is finetuned via an iterative distillation process, incorporating inter-iteration model merging to effectively aggregate complementary knowledge. This is followed by reinforcement learning on Ascend clusters, optimized by a latency-tolerant scheduler that combines stale synchronous parallelism with prioritized data queues. The RL process is guided by a Multi-source Adaptive Reward System (MARS), which generates dynamic, task-specific reward signals using deterministic metrics and lightweight LLM evaluators for mathematics, coding, and general problem-solving tasks. Stage 2 introduces a dual-system framework, endowing Pangu Embedded with a â€œfastâ€ mode for routine queries and a deeper â€œslowâ€ mode for complex inference. This framework offers both manual mode switching for user control and an automatic, complexity-aware mode selection mechanism that dynamically allocates computational resources to balance latency and reasoning depth. Experimental results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate that Pangu Embedded with 7B parameters, outperforms similar-size models like Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art reasoning quality within a single, unified model architecture, highlighting a promising direction for developing powerful yet practically deployable LLM reasoners. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Pangu Embeddedï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨Ascendç¥ç»ç½‘ç»œå¤„ç†å•å…ƒï¼ˆNPUï¼‰ä¸Šå¼€å‘çš„é«˜æ•ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å™¨ï¼Œå…·å¤‡çµæ´»çš„å¿«æ…¢æ€è€ƒèƒ½åŠ›ã€‚Pangu Embeddedè§£å†³äº†ç°æœ‰ä¼˜åŒ–æ¨ç†çš„LLMä¸­æ™®éå­˜åœ¨çš„è®¡ç®—æˆæœ¬é«˜æ˜‚å’Œæ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶æ¥æ„å»ºå®ƒã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡è¿­ä»£è’¸é¦è¿‡ç¨‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ç»“åˆè·¨è¿­ä»£æ¨¡å‹åˆå¹¶ï¼Œä»¥æœ‰æ•ˆåœ°èšåˆäº’è¡¥çŸ¥è¯†ã€‚éšåæ˜¯åœ¨Ascendé›†ç¾¤ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œç”±å»¶è¿Ÿå®¹å¿è°ƒåº¦å™¨è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥è°ƒåº¦å™¨ç»“åˆäº†é™ˆæ—§åŒæ­¥å¹¶è¡Œæ€§ä¸ä¼˜å…ˆçº§æ•°æ®é˜Ÿåˆ—ã€‚å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ç”±å¤šæºè‡ªé€‚åº”å¥–åŠ±ç³»ç»Ÿï¼ˆMARSï¼‰å¼•å¯¼ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨ç¡®å®šæ€§æŒ‡æ ‡å’Œè½»é‡çº§LLMè¯„ä¼°å™¨ç”Ÿæˆé’ˆå¯¹æ•°å­¦ã€ç¼–ç å’Œä¸€èˆ¬é—®é¢˜è§£å†³ä»»åŠ¡çš„åŠ¨æ€ã€ç‰¹å®šä»»åŠ¡å¥–åŠ±ä¿¡å·ã€‚ç¬¬äºŒé˜¶æ®µå¼•å…¥äº†ä¸€ä¸ªåŒç³»ç»Ÿæ¡†æ¶ï¼Œèµ‹äºˆPangu Embeddedä¸€ç§ç”¨äºå¸¸è§„æŸ¥è¯¢çš„â€œå¿«é€Ÿâ€æ¨¡å¼å’Œä¸€ç§ç”¨äºå¤æ‚æ¨ç†çš„æ›´æ·±å±‚æ¬¡â€œæ…¢é€Ÿâ€æ¨¡å¼ã€‚è¯¥æ¡†æ¶æä¾›äº†æ‰‹åŠ¨æ¨¡å¼åˆ‡æ¢ä¾›ç”¨æˆ·æ§åˆ¶å’Œä¸€ç§è‡ªåŠ¨ã€æ„ŸçŸ¥å¤æ‚æ€§çš„æ¨¡å¼é€‰æ‹©æœºåˆ¶ï¼Œä»¥åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºæ¥å¹³è¡¡å»¶è¿Ÿå’Œæ¨ç†æ·±åº¦ã€‚åœ¨AIME 2024ã€GPQAå’ŒLiveCodeBenchç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‹¥æœ‰70äº¿å‚æ•°çš„Pangu Embeddedè¶…è¶Šäº†ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ï¼Œå¦‚Qwen3-8Bå’ŒGLM4-9Bã€‚å®ƒåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¶æ„å†…å®ç°äº†å¿«é€Ÿå“åº”å’Œæœ€æ–°æ°´å¹³çš„æ¨ç†è´¨é‡ï¼Œä¸ºå¼€å‘å¼ºå¤§ä¸”å®é™…å¯éƒ¨ç½²çš„LLMæ¨ç†å™¨æŒ‡æ˜äº†æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22375v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>panguåµŒå…¥å¼æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„åŸºäºAscendç¥ç»å¤„ç†å•å…ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å™¨ï¼Œå…·å¤‡çµæ´»çš„å¿«æ…¢æ€è€ƒèƒ½åŠ›ã€‚å®ƒè§£å†³äº†ç°æœ‰æ¨ç†ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶æ¥æ„å»ºæ¨¡å‹ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡è¿­ä»£è’¸é¦è¿‡ç¨‹å¾®è°ƒæ¨¡å‹ï¼Œå¹¶ç»“åˆè·¨è¿­ä»£æ¨¡å‹åˆå¹¶æ¥æœ‰æ•ˆåœ°èšåˆäº’è¡¥çŸ¥è¯†ã€‚éšåæ˜¯åœ¨Ascendé›†ç¾¤ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œç”±ä¸€ä¸ªå»¶è¿Ÿå®¹å¿è°ƒåº¦å™¨ä¼˜åŒ–ï¼Œè¯¥è°ƒåº¦å™¨ç»“åˆäº†è¿‡æ—¶åŒæ­¥å¹¶è¡Œæ€§ä¸ä¼˜å…ˆæ•°æ®é˜Ÿåˆ—ã€‚å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ç”±å¤šæºè‡ªé€‚åº”å¥–åŠ±ç³»ç»Ÿå¼•å¯¼ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨ç¡®å®šæ€§æŒ‡æ ‡å’Œè½»é‡çº§çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å™¨ä¸ºæ•°å­¦ã€ç¼–ç å’Œä¸€èˆ¬é—®é¢˜è§£å†³ä»»åŠ¡ç”ŸæˆåŠ¨æ€ã€ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±ä¿¡å·ã€‚ç¬¬äºŒé˜¶æ®µå¼•å…¥äº†ä¸€ä¸ªåŒç³»ç»Ÿæ¡†æ¶ï¼Œèµ‹äºˆpanguåµŒå…¥å¼â€œå¿«é€Ÿâ€æ¨¡å¼ç”¨äºå¸¸è§„æŸ¥è¯¢å’Œæ›´æ·±çš„â€œæ…¢é€Ÿâ€æ¨¡å¼ç”¨äºå¤æ‚æ¨ç†ã€‚è¯¥æ¡†æ¶æä¾›äº†æ‰‹åŠ¨æ¨¡å¼åˆ‡æ¢ä¾›ç”¨æˆ·æ§åˆ¶å’Œè‡ªåŠ¨å¤æ‚åº¦æ„ŸçŸ¥æ¨¡å¼é€‰æ‹©æœºåˆ¶ï¼Œä»¥åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºæ¥å¹³è¡¡å»¶è¿Ÿå’Œæ¨ç†æ·±åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒpanguåµŒå…¥å¼åœ¨AIME 2024ã€GPQAå’ŒLiveCodeBenchç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å¿«é€Ÿå“åº”å’Œå‡ºè‰²çš„æ¨ç†è´¨é‡ï¼Œä¸ºå¼€å‘å¼ºå¤§ä¸”å¯å®é™…éƒ¨ç½²çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å™¨æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Pangu Embeddedæ˜¯ä¸€ä¸ªåŸºäºAscendç¥ç»å¤„ç†å•å…ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†å™¨ï¼Œå…·æœ‰å¿«æ…¢æ€è€ƒèƒ½åŠ›ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹çš„è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡è¿­ä»£è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ï¼›ç¬¬äºŒé˜¶æ®µå¼•å…¥åŒç³»ç»Ÿæ¡†æ¶ï¼Œå®ç°å¿«é€Ÿå’Œæ…¢é€Ÿæ¨ç†æ¨¡å¼ã€‚</li>
<li>å¤šæºè‡ªé€‚åº”å¥–åŠ±ç³»ç»ŸæŒ‡å¯¼å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œæ ¹æ®ä»»åŠ¡ç‰¹å®šéœ€æ±‚ç”ŸæˆåŠ¨æ€å¥–åŠ±ä¿¡å·ã€‚</li>
<li>Pangu Embeddedå…·å¤‡è‡ªåŠ¨æ¨¡å¼é€‰æ‹©æœºåˆ¶ï¼Œèƒ½æ ¹æ®ä»»åŠ¡çš„å¤æ‚åº¦åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPangu Embeddedåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬AIME 2024ã€GPQAå’ŒLiveCodeBenchç­‰ã€‚</li>
<li>Pangu Embeddedæä¾›äº†å¿«é€Ÿå“åº”å’Œå‡ºè‰²çš„æ¨ç†è´¨é‡ï¼Œåœ¨å•ä¸€æ¨¡å‹æ¶æ„å†…å®ç°äº†å…ˆè¿›çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ff2b2d5607f912c1e68beb8adc06b0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30939b7363124d3bec7cd4f40d2facee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dab72efea4331e9c4dc6c0b47b351c39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-818c7a3f1ee99c1fbb95e4a993773278.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d46872bec61903266b3c1a5d7cc4e383.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A2Seek-Towards-Reasoning-Centric-Benchmark-for-Aerial-Anomaly-Understanding"><a href="#A2Seek-Towards-Reasoning-Centric-Benchmark-for-Aerial-Anomaly-Understanding" class="headerlink" title="A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly   Understanding"></a>A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly   Understanding</h2><p><strong>Authors:Mengjingcheng Mo, Xinyang Tong, Jiaxu Leng, Mingpi Tan, Jiankang Zheng, Yiran Liu, Haosheng Chen, Ji Gan, Weisheng Li, Xinbo Gao</strong></p>
<p>While unmanned aerial vehicles (UAVs) offer wide-area, high-altitude coverage for anomaly detection, they face challenges such as dynamic viewpoints, scale variations, and complex scenes. Existing datasets and methods, mainly designed for fixed ground-level views, struggle to adapt to these conditions, leading to significant performance drops in drone-view scenarios. To bridge this gap, we introduce A2Seek (Aerial Anomaly Seek), a large-scale, reasoning-centric benchmark dataset for aerial anomaly understanding. This dataset covers various scenarios and environmental conditions, providing high-resolution real-world aerial videos with detailed annotations, including anomaly categories, frame-level timestamps, region-level bounding boxes, and natural language explanations for causal reasoning. Building on this dataset, we propose A2Seek-R1, a novel reasoning framework that generalizes R1-style strategies to aerial anomaly understanding, enabling a deeper understanding of â€œWhereâ€ anomalies occur and â€œWhyâ€ they happen in aerial frames. To this end, A2Seek-R1 first employs a graph-of-thought (GoT)-guided supervised fine-tuning approach to activate the modelâ€™s latent reasoning capabilities on A2Seek. Then, we introduce Aerial Group Relative Policy Optimization (A-GRPO) to design rule-based reward functions tailored to aerial scenarios. Furthermore, we propose a novel â€œseekingâ€ mechanism that simulates UAV flight behavior by directing the modelâ€™s attention to informative regions. Extensive experiments demonstrate that A2Seek-R1 achieves up to a 22.04% improvement in AP for prediction accuracy and a 13.9% gain in mIoU for anomaly localization, exhibiting strong generalization across complex environments and out-of-distribution scenarios. Our dataset and code will be released at <a target="_blank" rel="noopener" href="https://hayneyday.github.io/A2Seek/">https://hayneyday.github.io/A2Seek/</a>. </p>
<blockquote>
<p>æ— äººæœºï¼ˆUAVï¼‰ä¸ºå¼‚å¸¸æ£€æµ‹æä¾›äº†å¤§èŒƒå›´ã€é«˜æµ·æ‹”çš„è¦†ç›–ï¼Œä½†å®ƒä»¬é¢ä¸´ç€åŠ¨æ€è§†ç‚¹ã€å°ºåº¦å˜åŒ–å’Œå¤æ‚åœºæ™¯ç­‰æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦è®¾è®¡ç”¨äºå›ºå®šåœ°é¢è§†è§’ï¼Œéš¾ä»¥é€‚åº”è¿™äº›æ¡ä»¶ï¼Œå¯¼è‡´åœ¨æ— äººæœºè§†è§’åœºæ™¯ä¸­æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†A2Seekï¼ˆç©ºä¸­å¼‚å¸¸æœå¯»ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç©ºä¸­å¼‚å¸¸ç†è§£çš„å¤§è§„æ¨¡ã€ä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å„ç§åœºæ™¯å’Œç¯å¢ƒæ¡ä»¶ï¼Œæä¾›äº†å¸¦æœ‰è¯¦ç»†æ³¨é‡Šçš„é«˜åˆ†è¾¨ç‡ç°å®ç©ºä¸­è§†é¢‘ï¼ŒåŒ…æ‹¬å¼‚å¸¸ç±»åˆ«ã€å¸§çº§æ—¶é—´æˆ³ã€åŒºåŸŸçº§è¾¹ç•Œæ¡†ä»¥åŠå› æœæ¨ç†çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚åŸºäºè¿™ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†A2Seek-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¨ç†æ¡†æ¶ï¼Œå®ƒå°†R1é£æ ¼çš„ç­–ç•¥æ¨å¹¿åˆ°ç©ºä¸­å¼‚å¸¸ç†è§£ï¼Œèƒ½å¤Ÿæ›´æ·±å…¥åœ°ç†è§£ç©ºä¸­å¸§ä¸­å¼‚å¸¸å‘ç”Ÿâ€œåœ¨å“ªé‡Œâ€å’Œâ€œä¸ºä»€ä¹ˆâ€å‘ç”Ÿã€‚ä¸ºæ­¤ï¼ŒA2Seek-R1é¦–å…ˆé‡‡ç”¨å—æ€ç»´å›¾ï¼ˆGoTï¼‰å¼•å¯¼çš„ç›‘ç£å¾®è°ƒæ–¹æ³•æ¥æ¿€æ´»æ¨¡å‹åœ¨A2Seekä¸Šçš„æ½œåœ¨æ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ç©ºä¸­ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆA-GRPOï¼‰æ¥é’ˆå¯¹ç©ºä¸­åœºæ™¯è®¾è®¡åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„â€œæœç´¢â€æœºåˆ¶ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹å…³æ³¨ä¿¡æ¯åŒºåŸŸæ¥æ¨¡æ‹Ÿæ— äººæœºçš„é£è¡Œè¡Œä¸ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒA2Seek-R1åœ¨é¢„æµ‹ç²¾åº¦ä¸Šå®ç°äº†é«˜è¾¾22.04%çš„APæé«˜ï¼Œåœ¨å¼‚å¸¸å®šä½ä¸Šå®ç°äº†13.9%çš„mIoUå¢ç›Šï¼Œåœ¨å¤æ‚ç¯å¢ƒå’Œè¶…å‡ºåˆ†å¸ƒçš„åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://hayneyday.github.io/A2Seek/%E5%B8%A6%E6%B3%A8%E7%9C%8B%E5%AF%BC%E5%BC%BA%E5%BC%BA%E7%9A%84%E7%A5%BE">https://hayneyday.github.io/A2Seek/å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21962v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ— äººæœºåœ¨å¹¿åŸŸã€é«˜æµ·æ‹”çš„å¼‚å¸¸æ£€æµ‹ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä½†é¢ä¸´åŠ¨æ€è§†è§’ã€å°ºåº¦å˜åŒ–å’Œå¤æ‚åœºæ™¯ç­‰æŒ‘æˆ˜ã€‚ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦ä¸ºå›ºå®šåœ°é¢è§†è§’è®¾è®¡ï¼Œéš¾ä»¥é€‚åº”è¿™äº›æ¡ä»¶ï¼Œå¯¼è‡´åœ¨æ— äººæœºè§†è§’ä¸‹çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºA2Seekæ•°æ®é›†å’ŒA2Seek-R1æ¨ç†æ¡†æ¶ã€‚æ•°æ®é›†åŒ…å«å„ç§åœºæ™¯å’Œç¯å¢ƒæ¡ä»¶ä¸‹çš„é«˜åˆ†è¾¨ç‡æ— äººæœºè§†é¢‘å’Œè¯¦ç»†æ³¨è§£ï¼›æ¡†æ¶åˆ™é€šè¿‡å›¾æ€ç»´å¯¼å‘çš„ç²¾ç»†åŒ–è°ƒæ•´å’Œæ¨¡æ‹Ÿæ— äººæœºé£è¡Œè¡Œä¸ºçš„â€œå¯»æ±‚â€æœºåˆ¶æ¥å¢å¼ºæ— äººæœºå¯¹å¼‚å¸¸çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯ï¼ŒA2Seek-R1é¢„æµ‹ç²¾åº¦æå‡å¯è¾¾APæå‡è‡³çº¦é«˜è‡³ç™¾åˆ†ä¹‹äºŒåäºŒç‚¹é›¶å››ï¼Œå¼‚å¸¸å®šä½mIoUæé«˜ç™¾åˆ†ä¹‹åä¸‰ç‚¹ä¹ï¼Œå±•ç°å‡ºåœ¨å¤æ‚ç¯å¢ƒå’Œåˆ†å¸ƒå¤–åœºæ™¯ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚æ›´å¤šè¯¦æƒ…å°†åœ¨<a target="_blank" rel="noopener" href="https://hayneyday.github.io/A2Seek/">https://hayneyday.github.io/A2Seek/</a>å…¬å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ— äººæœºåœ¨é«˜æµ·æ‹”è¿›è¡Œå¹¿åŸŸå¼‚å¸¸æ£€æµ‹æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚åŠ¨æ€è§†è§’å’Œå¤æ‚åœºæ™¯ç­‰ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦ä¸ºå›ºå®šåœ°é¢è§†è§’è®¾è®¡ï¼Œéš¾ä»¥æ»¡è¶³æ— äººæœºè§†è§’çš„éœ€æ±‚ã€‚</li>
<li>A2Seekæ•°æ®é›†æä¾›é«˜åˆ†è¾¨ç‡çš„æ— äººæœºè§†é¢‘å’Œè¯¦ç»†æ³¨è§£ï¼Œç”¨äºè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>A2Seek-R1æ¡†æ¶é€šè¿‡å›¾æ€ç»´å¯¼å‘çš„ç²¾ç»†åŒ–è°ƒæ•´å’Œæ¨¡æ‹Ÿæ— äººæœºé£è¡Œè¡Œä¸ºçš„æœºåˆ¶æ¥æå‡å¯¹å¼‚å¸¸çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>A2Seek-R1é¢„æµ‹ç²¾åº¦æ˜¾è‘—æé«˜ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b41298194b07093af0e5f77d1335929.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-230c458a9b45f77fe4e9ec1e49f208f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d55f56525710949108ff4e6b659ffb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-937915e313a6e036e2c588a707cf9053.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-for-Out-of-Distribution-Reasoning-in-LLMs-An-Empirical-Study-on-Diagnosis-Related-Group-Coding"><a href="#Reinforcement-Learning-for-Out-of-Distribution-Reasoning-in-LLMs-An-Empirical-Study-on-Diagnosis-Related-Group-Coding" class="headerlink" title="Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An   Empirical Study on Diagnosis-Related Group Coding"></a>Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An   Empirical Study on Diagnosis-Related Group Coding</h2><p><strong>Authors:Hanyin Wang, Zhenbang Wu, Gururaj Kolar, Hariprasad Korsapati, Brian Bartlett, Bryan Hull, Jimeng Sun</strong></p>
<p>Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks. </p>
<blockquote>
<p>è¯Šæ–­ç›¸å…³ç»„ï¼ˆDRGï¼‰ä»£ç å¯¹äºåŒ»é™¢æŠ¥é”€å’Œè¿è¥è‡³å…³é‡è¦ï¼Œä½†éœ€è¦æŠ•å…¥å¤§é‡åŠ³åŠ¨åŠ›è¿›è¡Œåˆ†é…ã€‚ç”±äºä»»åŠ¡æ¶‰åŠç¦»ç¾¤ç‚¹ï¼ˆOODï¼‰ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨DRGç¼–ç æ–¹é¢è¡¨ç°å›°éš¾ï¼šé¢„è®­ç»ƒè¯­æ–™åº“å¾ˆå°‘åŒ…å«ç§æœ‰ä¸´åºŠæˆ–è®¡è´¹æ•°æ®ã€‚æˆ‘ä»¬å¼•å…¥äº†DRG-Sapphireï¼Œå®ƒä½¿ç”¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»ä¸´åºŠç¬”è®°ä¸­è¿›è¡Œè‡ªåŠ¨åŒ–DRGç¼–ç ã€‚DRG-SapphireåŸºäºQwen2.5-7Bæ„å»ºï¼Œå¹¶ä½¿ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒï¼Œå¼•å…¥äº†ä¸€ç³»åˆ—RLå¢å¼ºåŠŸèƒ½ï¼Œä»¥è§£å†³åœ¨å…ˆå‰æ•°å­¦ä»»åŠ¡ä¸­æœªè§åˆ°çš„ç‰¹å®šé¢†åŸŸæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨MIMIC-IVåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸ºDRGåˆ†é…ç”Ÿæˆäº†ç»åŒ»ç”ŸéªŒè¯çš„ç†ç”±ï¼Œå¤§å¤§æé«˜äº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜æ­ç¤ºäº†å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºçŸ¥è¯†å¯†é›†å‹ã€ç¦»ç¾¤ç‚¹ä»»åŠ¡çš„æ›´å¹¿æ³›æŒ‘æˆ˜ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½å¤§çº¦ä¸ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¾‹å­çš„æ•°é‡å¯¹æ•°å‘ˆçº¿æ€§å…³ç³»ï¼Œè¿™è¡¨æ˜å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ä»æ ¹æœ¬ä¸Šå—åˆ°åŸºç¡€æ¨¡å‹ä¸­ç¼–ç é¢†åŸŸçŸ¥è¯†çš„é™åˆ¶ã€‚å¯¹äºåƒDRGç¼–ç è¿™æ ·çš„ç¦»ç¾¤ç‚¹ä»»åŠ¡ï¼Œå®ç°å¼ºå¤§çš„å¼ºåŒ–å­¦ä¹ æ€§èƒ½éœ€è¦åœ¨å¼ºåŒ–å­¦ä¹ ä¹‹å‰è¿›è¡Œè¶³å¤Ÿçš„çŸ¥è¯†æ³¨å…¥ã€‚å› æ­¤ï¼Œå¯¹äºæ­¤ç±»ä»»åŠ¡ï¼Œæ‰©å¤§SFTå¯èƒ½æ¯”å•ç‹¬æ‰©å¤§RLæ›´æœ‰æ•ˆå’Œè®¡ç®—é«˜æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21908v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹DRG-Sapphireå’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–DRGç¼–ç ã€‚é’ˆå¯¹ä»ä¸´åºŠè®°å½•è¿›è¡ŒDRGç¼–ç çš„ç‰¹æ®ŠæŒ‘æˆ˜ï¼ŒDRG-Sapphireå¼•å…¥äº†ç³»åˆ—RLå¢å¼ºæŠ€æœ¯å¹¶å®ç°äº†MIMIC-IVåŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›å‡†ç¡®åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜ç”Ÿæˆäº†åŒ»ç”ŸéªŒè¯çš„DRGåˆ†é…æ¨ç†ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ã€‚ç ”ç©¶å‘ç°å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½éšç›‘ç£å¾®è°ƒæ¡ˆä¾‹æ•°é‡çš„å¯¹æ•°è€Œçº¿æ€§å¢é•¿ï¼Œæç¤ºå¯¹äºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼Œå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§å—é™äºåŸºç¡€æ¨¡å‹ä¸­çš„é¢†åŸŸçŸ¥è¯†ã€‚å› æ­¤ï¼Œå¯¹äºæ­¤ç±»ä»»åŠ¡ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¹‹å‰è¿›è¡Œè¶³å¤Ÿçš„çŸ¥è¯†çŒè¾“å¯èƒ½æ›´æœ‰æ•ˆä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DRGç¼–ç å¯¹åŒ»é™¢æŠ¥é”€å’Œè¿è¥è‡³å…³é‡è¦ï¼Œä½†åˆ†é…å·¥ä½œéœ€è¦å¤§é‡åŠ³åŠ¨åŠ›ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› é¢„è®­ç»ƒè¯­æ–™åº“ä¸åŒ…å«ç§äººä¸´åºŠæˆ–è®¡è´¹æ•°æ®ï¼Œéš¾ä»¥è¿›è¡ŒDRGç¼–ç ã€‚</li>
<li>DRG-Sapphireä½¿ç”¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè‡ªåŠ¨åŒ–DRGç¼–ç ï¼Œè§£å†³äº†ç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>DRG-Sapphireåœ¨MIMIC-IVåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®åº¦ï¼Œå¹¶ç”Ÿæˆäº†åŒ»ç”ŸéªŒè¯çš„DRGåˆ†é…æ¨ç†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ä¸ç›‘ç£å¾®è°ƒæ¡ˆä¾‹çš„æ•°é‡ç›¸å…³ã€‚å¯¹äºçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼Œå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§å—é™äºåŸºç¡€æ¨¡å‹ä¸­çš„é¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>å¯¹äºæ­¤ç±»ä»»åŠ¡ï¼Œå¼ºåŒ–å­¦ä¹ ä¹‹å‰çš„çŸ¥è¯†çŒè¾“éå¸¸é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b6034ef74b2d0eaccda108b377a37dc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93ceccaee5361c4d5d1c7c28ebe5a0d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c51d4a97c86fd59394eea90c065f9cce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25bd4859aa09f2c832d797fb2f48612a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="R1-Code-Interpreter-Training-LLMs-to-Reason-with-Code-via-Supervised-and-Reinforcement-Learning"><a href="#R1-Code-Interpreter-Training-LLMs-to-Reason-with-Code-via-Supervised-and-Reinforcement-Learning" class="headerlink" title="R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised   and Reinforcement Learning"></a>R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised   and Reinforcement Learning</h2><p><strong>Authors:Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, Chuchu Fan</strong></p>
<p>Despite advances in reasoning and planning of R1-like models, Large Language Models (LLMs) still struggle with tasks requiring precise computation, symbolic manipulation, optimization, and algorithmic reasoning, in which textual reasoning lacks the rigor of code execution. A key challenge is enabling LLMs to decide when to use textual reasoning versus code generation. While OpenAI trains models to invoke a Code Interpreter as needed, public research lacks guidance on aligning pre-trained LLMs to effectively leverage code and generalize across diverse tasks. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 diverse questions. We fine-tune Qwen-2.5 models (3B&#x2F;7B&#x2F;14B) using various SFT and RL strategies, investigating different answer formats, reasoning vs. non-reasoning models, cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs. Unlike prior RL work on narrow domains, we find that Code Interpreter training is significantly harder due to high task diversity and expensive code execution, highlighting the critical role of the SFT stage. Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0% to 64.1%, outperforming GPT-4o (text-only: 58.6%) and approaching GPT-4o with Code Interpreter (70.9%), with the emergent self-checking behavior via code generation. Datasets, Codes, and Models are available at <a target="_blank" rel="noopener" href="https://github.com/yongchao98/R1-Code-Interpreter">https://github.com/yongchao98/R1-Code-Interpreter</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/yongchao98">https://huggingface.co/yongchao98</a>. </p>
<blockquote>
<p>å°½ç®¡R1ç±»ä¼¼æ¨¡å‹çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›æœ‰æ‰€è¿›æ­¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€è¦ç²¾ç¡®è®¡ç®—ã€ç¬¦å·æ“ä½œã€ä¼˜åŒ–å’Œç®—æ³•æ¨ç†çš„ä»»åŠ¡æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬æ¨ç†ç¼ºä¹ä»£ç æ‰§è¡Œçš„ä¸¥è°¨æ€§ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ä½¿LLMèƒ½å¤Ÿå†³å®šä½•æ—¶ä½¿ç”¨æ–‡æœ¬æ¨ç†ä½•æ—¶ä½¿ç”¨ä»£ç ç”Ÿæˆã€‚è™½ç„¶OpenAIè®­ç»ƒæ¨¡å‹æŒ‰éœ€è°ƒç”¨ä»£ç è§£é‡Šå™¨ï¼Œä½†å…¬å…±ç ”ç©¶åœ¨å¦‚ä½•å¯¹é½é¢„è®­ç»ƒLLMä»¥æœ‰æ•ˆåˆ©ç”¨ä»£ç å¹¶åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­æ¨å¹¿æ–¹é¢ç¼ºä¹æŒ‡å¯¼ã€‚æˆ‘ä»¬æ¨å‡ºäº†R1-Code-Interpreterï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šå›åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„çº¯æ–‡æœ¬LLMçš„æ‰©å±•ï¼Œå¯è‡ªä¸»ç”Ÿæˆå¤šä¸ªä»£ç æŸ¥è¯¢æ¥è¿›è¡Œé€æ­¥æ¨ç†ã€‚æˆ‘ä»¬ç²¾é€‰äº†144ä¸ªæ¨ç†å’Œè§„åˆ’ä»»åŠ¡ï¼ˆ107ä¸ªç”¨äºè®­ç»ƒï¼Œ37ä¸ªç”¨äºæµ‹è¯•ï¼‰ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«è¶…è¿‡2.ç ”ç©¶ä½¿ç”¨å„ç§SFTå’ŒRLç­–ç•¥å¾®è°ƒäº†Qwen-2.ä¸åŒçš„å›ç­”æ ¼å¼ï¼Œæ¨ç†ä¸éæ¨ç†æ¨¡å‹ä¹‹é—´çš„å¯¹æ¯”ï¼Œå†·å¯åŠ¨ä¸çƒ­å¯åŠ¨çš„æ¯”è¾ƒï¼ŒGRPOä¸PPOçš„å¯¹æ¯”ä»¥åŠæœ‰ç è¾“å‡ºä¸æ— ç è¾“å‡ºçš„å¯¹æ¯”ã€‚ä¸ä¹‹å‰é’ˆå¯¹ç‹­çª„é¢†åŸŸçš„RLç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬å‘ç°ç”±äºä»»åŠ¡å¤šæ ·æ€§å’Œæ˜‚è´µçš„ä»£ç æ‰§è¡Œæˆæœ¬ï¼Œä»£ç è§£é‡Šå™¨è®­ç»ƒæ›´ä¸ºå›°éš¾é‡é‡ï¼Œè¿™ä¹Ÿçªæ˜¾äº†SFTé˜¶æ®µçš„å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹R1-CI-14Bå°†å¹³å‡å‡†ç¡®ç‡æå‡åˆ°äº†åœ¨æµ‹è¯•çš„37ä¸ªä»»åŠ¡çš„å¹³å‡å‡†ç¡®ç‡ä»ç™¾åˆ†ä¹‹æå‡åˆ°äº†ç™¾åˆ†ä¹‹ã€‚åœ¨æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹ä¸Šæˆ‘ä»¬å·²è¿›è¡Œäº†å…¬å¼€åˆ†äº«åœ¨ <a target="_blank" rel="noopener" href="https://github.com/yongchao98/R1-Code-Interpreter">https://github.com/yongchao98/R1-Code-Interpreter</a> å¹¶å·²ç»ç™»é™† Hugging faceå¹³å°çš„ç½‘å€ ã€‚æˆ‘ä»¬çš„æœ€ç»ˆæ¨¡å‹RCIçš„æ€§èƒ½è¶…è¿‡äº†GPTç³»åˆ—ã€‚åœ¨GPTçš„åªå¤„ç†æ–‡æœ¬çš„åŸºç¡€ä¸Šæå‡åˆ°ä¸ç»“åˆäº†ä»£ç è§£é‡Šå™¨çš„GPTæ€§èƒ½æ¥è¿‘çš„ç¨‹åº¦è¾¾åˆ°äº†ç™¾åˆ†æ•°çº§åˆ«çš„æå‡è€Œä¸”å‡ºç°äº†è‡ªæˆ‘æ£€æŸ¥è¡Œä¸ºçš„æ¶Œç°ã€‚æˆ‘ä»¬é€šè¿‡ä»£ç ç”ŸæˆéªŒè¯å¹¶å±•ç¤ºè¿™ä¸€è¡Œä¸ºã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/yongchao98/R1-Code-Interpreter">https://github.com/yongchao98/R1-Code-Interpreter</a> æ‰¾åˆ°å¹¶é€šè¿‡Hugging faceå¹³å°ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21668v1">PDF</a> 33 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>R1ç³»åˆ—æ¨¡å‹è™½ç„¶åœ¨æ¨ç†å’Œè§„åˆ’æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éœ€è¦ç²¾ç¡®è®¡ç®—ã€ç¬¦å·æ“ä½œã€ä¼˜åŒ–å’Œç®—æ³•æ¨ç†çš„ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚æ–‡æœ¬æ¨ç†ç¼ºä¹ä»£ç æ‰§è¡Œçš„ä¸¥è°¨æ€§ï¼Œå…³é”®æŒ‘æˆ˜åœ¨äºå¦‚ä½•è®©LLMså†³å®šä½•æ—¶ä½¿ç”¨æ–‡æœ¬æ¨ç†ä¸ä»£ç ç”Ÿæˆã€‚æœ¬æ–‡æå‡ºR1-Code-Interpreterï¼Œè¿™æ˜¯æ–‡æœ¬æ¨¡å‹çš„ä¸€ä¸ªæ‰©å±•ï¼Œé€šè¿‡å¤šè½®ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¯è‡ªä¸»ç”Ÿæˆä»£ç æŸ¥è¯¢è¿›è¡Œé€æ­¥æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç”±äºä»»åŠ¡å¤šæ ·æ€§å’Œæ˜‚è´µçš„ä»£ç æ‰§è¡Œï¼Œä»£ç è§£é‡Šå™¨è®­ç»ƒæ›´åŠ å›°éš¾ï¼Œå¼ºè°ƒSFTé˜¶æ®µçš„é‡è¦ä½œç”¨ã€‚æœ€ç»ˆæ¨¡å‹R1-CI-14Båœ¨æµ‹è¯•ä»»åŠ¡ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä»44.0%æé«˜åˆ°64.1%ï¼Œä¼˜äºGPT-4oçš„çº¯æ–‡æœ¬æ¨¡å‹ï¼ˆ58.6%ï¼‰ï¼Œå¹¶æ¥è¿‘GPT-4oçš„ä»£ç è§£é‡Šå™¨ç‰ˆæœ¬ï¼ˆ70.9%ï¼‰ï¼Œå±•ç°å‡ºé€šè¿‡ä»£ç ç”Ÿæˆè¿›è¡Œè‡ªæˆ‘æ£€æŸ¥çš„æ–°å…´è¡Œä¸ºã€‚ç›¸å…³æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å¯åœ¨æŒ‡å®šç½‘ç«™ä¸‹è½½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éœ€è¦ç²¾ç¡®è®¡ç®—ã€ç¬¦å·æ“ä½œç­‰ä»»åŠ¡ä¸Šä»æœ‰å›°éš¾ã€‚</li>
<li>æ–‡æœ¬æ¨ç†ç¼ºä¹ä»£ç æ‰§è¡Œçš„ä¸¥è°¨æ€§ï¼Œéœ€è¦å†³å®šä½•æ—¶ä½¿ç”¨æ–‡æœ¬æ¨ç†ä¸ä»£ç ç”Ÿæˆã€‚</li>
<li>R1-Code-Interpreteræ˜¯æ–‡æœ¬æ¨¡å‹çš„ä¸€ä¸ªæ‰©å±•ï¼Œèƒ½é€šè¿‡è‡ªä¸»ç”Ÿæˆä»£ç æŸ¥è¯¢è¿›è¡Œé€æ­¥æ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜ä»£ç è§£é‡Šå™¨è®­ç»ƒæ›´åŠ å›°éš¾ï¼Œå› ä¸ºä»»åŠ¡å¤šæ ·æ€§å’Œæ˜‚è´µçš„ä»£ç æ‰§è¡Œæˆæœ¬ã€‚</li>
<li>SFTé˜¶æ®µåœ¨è®­ç»ƒä¸­çš„é‡è¦æ€§è¢«å¼ºè°ƒã€‚</li>
<li>æœ€ç»ˆæ¨¡å‹R1-CI-14Båœ¨æµ‹è¯•ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—æé«˜ï¼Œæ˜¾ç¤ºå‡ºè‡ªæˆ‘æ£€æŸ¥çš„æ–°å…´è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01b44f99b06e9f029d1587c9820b4b84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-783e89e52981df668e99507447bbcf2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76ac387122ed8436ba1a0bec06147587.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8225f197b4ffcd5dfb9f87bac904bd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0648064e9d37d605de438de2ace70a12.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Accelerating-RL-for-LLM-Reasoning-with-Optimal-Advantage-Regression"><a href="#Accelerating-RL-for-LLM-Reasoning-with-Optimal-Advantage-Regression" class="headerlink" title="Accelerating RL for LLM Reasoning with Optimal Advantage Regression"></a>Accelerating RL for LLM Reasoning with Optimal Advantage Regression</h2><p><strong>Authors:KiantÃ© Brantley, Mingyu Chen, Zhaolin Gao, Jason D. Lee, Wen Sun, Wenhao Zhan, Xuezhou Zhang</strong></p>
<p>Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A$<em>-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V$</em>, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A$<em>-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\times$ and peak memory usage by over 30% compared to PPO, GRPO, and REBEL. Implementation of $A$</em>-PO can be found at <a target="_blank" rel="noopener" href="https://github.com/ZhaolinGao/A-PO">https://github.com/ZhaolinGao/A-PO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•å¸¸å¸¸é¢ä¸´è®¡ç®—å¼€é”€å¤§ã€å†…å­˜æ¶ˆè€—é«˜çš„å›°æ‰°ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ¯ä¸ªæç¤ºéœ€è¦å¤šä»£è®¡ç®—ï¼Œå¹¶ä¸”ä¾èµ–äºå½“å‰ç­–ç•¥çš„ä»·å€¼è¯„ä¼°ç½‘ç»œæˆ–ä¼˜åŠ¿ä¼°è®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†A*-POï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œå®ƒç›´æ¥é€¼è¿‘æœ€ä¼˜ä¼˜åŠ¿å‡½æ•°ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°è®­ç»ƒç”¨äºæ¨ç†ä»»åŠ¡çš„LLMã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨ä»å‚è€ƒç­–ç•¥ä¸­è¿›è¡Œçš„ç¦»çº¿é‡‡æ ·ä¼°è®¡æœ€ä¼˜å€¼å‡½æ•°V<em>ï¼Œä»è€Œæ— éœ€æ˜‚è´µçš„åœ¨çº¿ä»·å€¼è¯„ä¼°ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„æœ€å°äºŒä¹˜å›å½’æŸå¤±è¿›è¡Œç­–ç•¥å†…æ›´æ–°ï¼Œæ¯ä¸ªæç¤ºä»…éœ€ä¸€æ¬¡ç”Ÿæˆã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬å»ºç«‹äº†æ€§èƒ½ä¿è¯ï¼Œå¹¶è¯æ˜äº†åœ¨ä¸éœ€è¦å¤æ‚æ¢ç´¢ç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä¼˜åŒ–KLæ­£åˆ™åŒ–çš„RLç›®æ ‡ã€‚ä»å®è¯è§’åº¦çœ‹ï¼ŒA</em>-POåœ¨å¹¿æ³›çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œä¸PPOã€GRPOå’ŒREBELç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†é«˜è¾¾2å€ï¼Œå³°å€¼å†…å­˜ä½¿ç”¨ç‡é™ä½äº†è¶…è¿‡30%ã€‚A*-POçš„å®ç°å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhaolinGao/A-PO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZhaolinGao/A-POæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20686v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥æé«˜å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•å¸¸å¸¸é¢ä¸´é«˜è®¡ç®—å¼€é”€å’Œå†…å­˜æ¶ˆè€—çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µç­–ç•¥ä¼˜åŒ–æ¡†æ¶A<em>POï¼Œé€šè¿‡ç›´æ¥è¿‘ä¼¼æœ€ä¼˜ä¼˜åŠ¿å‡½æ•°ï¼Œå®ç°äº†LLMåœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„é«˜æ•ˆè®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç¦»çº¿é‡‡æ ·ä»å‚è€ƒç­–ç•¥ä¼°ç®—æœ€ä¼˜å€¼å‡½æ•°V</em>ï¼Œé¿å…äº†æ˜‚è´µçš„åœ¨çº¿å€¼ä¼°è®¡ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨ç®€å•çš„æœ€å°äºŒä¹˜å›å½’æŸå¤±è¿›è¡Œåœ¨çº¿ç­–ç•¥æ›´æ–°ï¼Œåªéœ€å•æ¬¡ç”Ÿæˆæç¤ºã€‚ç†è®ºæ–¹é¢ï¼Œæˆ‘ä»¬æä¾›äº†æ€§èƒ½ä¿è¯ï¼Œå¹¶è¯æ˜åœ¨ä¸éœ€è¦å¤æ‚æ¢ç´¢ç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä¼˜åŒ–KLæ­£åˆ™åŒ–çš„RLç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒA*POåœ¨å¹¿æ³›çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´æœ€å¤šå‡å°‘ä¸€åŠï¼Œå³°å€¼å†…å­˜ä½¿ç”¨ç‡ç›¸è¾ƒäºPPOã€GRPOå’ŒREBELç­‰ç®—æ³•é™ä½è¶…è¿‡30%ã€‚å®æ–½ä»£ç å·²å‘å¸ƒè‡³<a target="_blank" rel="noopener" href="https://github.com/ZhaolinGao/A-PO%E3%80%82">https://github.com/ZhaolinGao/A-POã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥æå‡å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>å½“å‰ç­–ç•¥ä¼˜åŒ–æ–¹æ³•é¢ä¸´é«˜è®¡ç®—å¼€é”€å’Œå†…å­˜æ¶ˆè€—çš„é—®é¢˜ã€‚</li>
<li>A*POæ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡è¿‘ä¼¼æœ€ä¼˜ä¼˜åŠ¿å‡½æ•°å®ç°LLMçš„é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç¦»çº¿é‡‡æ ·ä¼°ç®—æœ€ä¼˜å€¼å‡½æ•°ï¼Œé¿å…åœ¨çº¿å€¼ä¼°è®¡çš„æˆæœ¬ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æœ€å°äºŒä¹˜å›å½’æŸå¤±è¿›è¡Œåœ¨çº¿ç­–ç•¥æ›´æ–°ï¼Œå•æ¬¡ç”Ÿæˆæç¤ºã€‚</li>
<li>A*POåœ¨ç†è®ºä¸Šæœ‰æ€§èƒ½ä¿è¯ï¼Œå¹¶èƒ½åœ¨ä¸ä¾èµ–å¤æ‚æ¢ç´¢ç­–ç•¥çš„æƒ…å†µä¸‹ä¼˜åŒ–KLæ­£åˆ™åŒ–çš„RLç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b22b2a08bd71738889189f561311afe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e78c526f81d41e25e45dfe0f8f2901b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d56a78431458ca4c8f15a00143bcfa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec4c7519072a7174b4ddd85f7914fb49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b95f6ffe7b9d2c301977f01445da0dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7b2a8628769b7a741645bcb54276c54.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SeqPO-SiMT-Sequential-Policy-Optimization-for-Simultaneous-Machine-Translation"><a href="#SeqPO-SiMT-Sequential-Policy-Optimization-for-Simultaneous-Machine-Translation" class="headerlink" title="SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine   Translation"></a>SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine   Translation</h2><p><strong>Authors:Ting Xu, Zhichao Huang, Jiankai Sun, Shanbo Cheng, Wai Lam</strong></p>
<p>We present Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a new policy optimization framework that defines the simultaneous machine translation (SiMT) task as a sequential decision making problem, incorporating a tailored reward to enhance translation quality while reducing latency. In contrast to popular Reinforcement Learning from Human Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task. This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT process using a tailored reward. We conduct experiments on six datasets from diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that SeqPO-SiMT consistently achieves significantly higher translation quality with lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning (SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17 in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly rival the offline translation of high-performing LLMs, including Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†é’ˆå¯¹åŒæ­¥æœºå™¨ç¿»è¯‘ï¼ˆSimultaneous Machine Translationï¼ŒSiMTï¼‰çš„åºè´¯ç­–ç•¥ä¼˜åŒ–ï¼ˆSequential Policy Optimizationï¼ŒSeqPO-SiMTï¼‰æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†SiMTä»»åŠ¡å®šä¹‰ä¸ºåºè´¯å†³ç­–é—®é¢˜ï¼Œå¹¶ç»“åˆå®šåˆ¶å¥–åŠ±æ¥æé«˜ç¿»è¯‘è´¨é‡åŒæ—¶å‡å°‘å»¶è¿Ÿã€‚ä¸é€šå¸¸åº”ç”¨äºå•æ­¥ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆReinforcement Learning from Human Feedbackï¼ŒRLHFï¼‰æ–¹æ³•ï¼ˆå¦‚PPOå’ŒDPOï¼‰ç›¸æ¯”ï¼ŒSeqPO-SiMTæœ‰æ•ˆåœ°è§£å†³äº†å¤šæ­¥SiMTä»»åŠ¡ã€‚è¿™ä¸ªç›´è§‚æ¡†æ¶å…è®¸SiMTå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½¿ç”¨å®šåˆ¶å¥–åŠ±æ¥æ¨¡æ‹Ÿå’Œå®Œå–„SiMTè¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªä¸åŒé¢†åŸŸçš„Enåˆ°Zhå’ŒZhåˆ°Ençš„SiMTä»»åŠ¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯æ˜SeqPO-SiMTåœ¨ä¿æŒè¾ƒä½å»¶è¿Ÿçš„åŒæ—¶ï¼Œå§‹ç»ˆå®ç°äº†æ›´é«˜çš„ç¿»è¯‘è´¨é‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨COMETä¸­ï¼ŒSeqPO-SiMTç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-tuningï¼ŒSFTï¼‰æ¨¡å‹çš„æ€§èƒ½æé«˜äº†1.13ç‚¹ï¼ŒåŒæ—¶åœ¨NEWSTEST2021 Enåˆ°Zhæ•°æ®é›†ä¸Šå¹³å‡å»¶è¿Ÿé™ä½äº†6.17ã€‚å°½ç®¡SiMTçš„ä¸Šä¸‹æ–‡è¿œå°‘äºç¦»çº¿ç¿»è¯‘ï¼Œä½†SeqPO-SiMTåœ¨7B LLMä¸Šçš„è¡¨ç°ä»¤äººæƒŠè®¶åœ°ä¸é«˜æ€§èƒ½ç¦»çº¿ç¿»è¯‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸åŒ¹æ•Œï¼ŒåŒ…æ‹¬Qwen-2.5-7B-Instructå’ŒLLaMA-3-8B-Instructã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20622v1">PDF</a> Accepted by The 63rd Annual Meeting of the Association for   Computational Linguistics (ACL 2025)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹åŒæ­¥æœºå™¨ç¿»è¯‘ï¼ˆSiMTï¼‰ä»»åŠ¡çš„åºè´¯ç­–ç•¥ä¼˜åŒ–æ¡†æ¶SeqPO-SiMTã€‚è¯¥æ¡†æ¶å°†SiMTå®šä¹‰ä¸ºåºè´¯å†³ç­–é—®é¢˜ï¼Œåˆ©ç”¨å®šåˆ¶å¥–åŠ±æ¥æé«˜ç¿»è¯‘è´¨é‡å¹¶é™ä½å»¶è¿Ÿã€‚åœ¨å…­ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSeqPO-SiMTåœ¨ä¿æŒä½å»¶è¿Ÿçš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ç¿»è¯‘è´¨é‡ã€‚ç‰¹åˆ«æ˜¯åœ¨COMETå’ŒNEWSTESTæ•°æ®é›†ä¸Šï¼ŒSeqPO-SiMTç›¸è¾ƒäºç›‘ç£å¾®è°ƒæ¨¡å‹è¡¨ç°æ›´ä¸ºä¼˜è¶Šã€‚å°½ç®¡SiMTçš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¿œå°‘äºç¦»çº¿ç¿»è¯‘ï¼Œä½†SeqPO-SiMTåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°å´æ„å¤–åœ°ä¸ä¹‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SeqPO-SiMTæ˜¯ä¸€ä¸ªæ–°çš„ç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºåŒæ­¥æœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>å®ƒå°†SiMTå®šä¹‰ä¸ºåºè´¯å†³ç­–é—®é¢˜ï¼Œå¹¶ä½¿ç”¨äº†å®šåˆ¶å¥–åŠ±æ¥æé«˜ç¿»è¯‘è´¨é‡å’Œé™ä½å»¶è¿Ÿã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSeqPO-SiMTæ˜¾è‘—æé«˜äº†ç¿»è¯‘è´¨é‡å¹¶ä¿æŒäº†ä½å»¶è¿Ÿã€‚</li>
<li>SeqPO-SiMTåœ¨COMETè¯„åˆ†ä¸Šè¶…è¶Šäº†ç›‘ç£å¾®è°ƒæ¨¡å‹ã€‚</li>
<li>SeqPO-SiMTåœ¨NEWSTESTæ•°æ®é›†ä¸Šçš„å¹³å‡å»¶è¿Ÿæ—¶é—´æœ‰æ‰€å‡å°‘ã€‚</li>
<li>å°½ç®¡SiMTçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æœ‰é™ï¼Œä½†SeqPO-SiMTåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°ä¸ç¦»çº¿ç¿»è¯‘ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35e1c166e5b72da3f22fea2df0f42e8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eedf214e0a7b0d41816bf32486f5e2c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be7a6042ced20175c0662fe614532c02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57c6523934df553ccb17c0e344bffea2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a33349a69c300a89927d739553310dd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e53120cbd10a7063978851a039a9cc2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-for-Autonomous-Driving-A-Comprehensive-Survey-and-Future-Prospects"><a href="#Chain-of-Thought-for-Autonomous-Driving-A-Comprehensive-Survey-and-Future-Prospects" class="headerlink" title="Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and   Future Prospects"></a>Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and   Future Prospects</h2><p><strong>Authors:Yixin Cui, Haotian Lin, Shuo Yang, Yixiao Wang, Yanjun Huang, Hong Chen</strong></p>
<p>The rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities. Such proficiencies have been leveraged in autonomous driving systems, contributing to significant improvements in system performance. Models such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning, an advanced cognitive method that simulates human thinking processes, demonstrating remarkable reasoning capabilities in complex tasks. By structuring complex driving scenarios within a systematic reasoning framework, this approach has emerged as a prominent research focus in autonomous driving, substantially improving the systemâ€™s ability to handle challenging cases. This paper investigates how CoT methods improve the reasoning abilities of autonomous driving models. Based on a comprehensive literature review, we present a systematic analysis of the motivations, methodologies, challenges, and future research directions of CoT in autonomous driving. Furthermore, we propose the insight of combining CoT with self-learning to facilitate self-evolution in driving systems. To ensure the relevance and timeliness of this study, we have compiled a dynamic repository of literature and open-source projects, diligently updated to incorporate forefront developments. The repository is publicly available at <a target="_blank" rel="noopener" href="https://github.com/cuiyx1720/Awesome-CoT4AD">https://github.com/cuiyx1720/Awesome-CoT4AD</a>. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹å¿«é€Ÿè¿›åŒ–ï¼Œæå¤§åœ°æé«˜äº†å…¶è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚è¿™ç§èƒ½åŠ›åœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­å¾—åˆ°äº†åº”ç”¨ï¼Œä¸ºç³»ç»Ÿæ€§èƒ½å¸¦æ¥äº†æ˜¾è‘—æ”¹è¿›ã€‚è¯¸å¦‚OpenAI o1å’ŒDeepSeek-R1ç­‰æ¨¡å‹ï¼Œé‡‡ç”¨â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰æ¨ç†ï¼Œè¿™æ˜¯ä¸€ç§é«˜çº§è®¤çŸ¥æ–¹æ³•ï¼Œæ¨¡æ‹Ÿäººç±»æ€ç»´è¿‡ç¨‹ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸€ä¸ªç³»ç»Ÿçš„æ¨ç†æ¡†æ¶å†…æ„å»ºå¤æ‚çš„é©¾é©¶åœºæ™¯ï¼Œè¿™ç§æ–¹æ³•å·²æˆä¸ºè‡ªåŠ¨é©¾é©¶çš„ä¸€ä¸ªçªå‡ºç ”ç©¶é‡ç‚¹ï¼Œæå¤§åœ°æé«˜äº†ç³»ç»Ÿå¤„ç†å¤æ‚æƒ…å†µçš„èƒ½åŠ›ã€‚æœ¬æ–‡ç ”ç©¶äº†CoTæ–¹æ³•å¦‚ä½•æ”¹å–„è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åŸºäºå…¨é¢çš„æ–‡çŒ®ç»¼è¿°ï¼Œæˆ‘ä»¬å¯¹CoTåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åŠ¨æœºã€æ–¹æ³•ã€æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†ç³»ç»Ÿçš„åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å°†CoTä¸è‡ªæˆ‘å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥ä¿ƒè¿›é©¾é©¶ç³»ç»Ÿçš„è‡ªæˆ‘è¿›åŒ–ã€‚ä¸ºç¡®ä¿ç ”ç©¶çš„æ—¶æ•ˆæ€§å’ŒåŠæ—¶æ€§ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªåŠ¨æ€çš„æ–‡çŒ®å’Œå¼€æºé¡¹ç›®ä»“åº“ï¼Œå¹¶ä¸æ–­æ›´æ–°ä»¥çº³å…¥æœ€æ–°çš„å‘å±•ã€‚è¯¥ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cuiyx1720/Awesome-CoT4AD%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/cuiyx1720/Awesome-CoT4ADè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20223v1">PDF</a> 18 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¿«é€Ÿå‘å±•æ˜¾è‘—æå‡äº†å…¶è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†æ–¹æ³•ï¼Œå¦‚OpenAI o1å’ŒDeepSeek-R1ç­‰æ¨¡å‹ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­å±•ç°å‡ºå“è¶Šçš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†CoTæ–¹æ³•å¦‚ä½•æå‡è‡ªåŠ¨é©¾é©¶æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ–‡çŒ®ç»¼è¿°ï¼Œå¯¹CoTåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åŠ¨æœºã€æ–¹æ³•ã€æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚æ­¤å¤–ï¼Œç»“åˆè‡ªæˆ‘å­¦ä¹ æœºåˆ¶æå‡ºäº†è‡ªæˆ‘è¿›åŒ–çš„é©¾é©¶ç³»ç»Ÿçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œé€»è¾‘æ¨ç†æ–¹é¢æœ‰äº†æ˜¾è‘—æå‡ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•è¢«åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼Œæå‡äº†ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>CoTé€šè¿‡æ¨¡æ‹Ÿäººç±»æ€è€ƒè¿‡ç¨‹ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CoTæ–¹æ³•çš„åº”ç”¨ä½¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿèƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹å¤æ‚å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚</li>
<li>æœ¬æ–‡å¯¹CoTåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åŠ¨æœºã€æ–¹æ³•ã€æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚</li>
<li>ç»“åˆè‡ªæˆ‘å­¦ä¹ æœºåˆ¶ï¼Œæå‡ºäº†è‡ªæˆ‘è¿›åŒ–çš„é©¾é©¶ç³»ç»Ÿçš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6bf4d112e9d1ee81a0a7c5c86de5a43e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8351ccee770ea1336176452c940b27e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20dc47b61a003a41b36558ef6d97b223.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b5f094c1df1d8db9e108fa7112d47a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac7c405680891f6eefdda6f69e59a8b4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SynLogic-Synthesizing-Verifiable-Reasoning-Data-at-Scale-for-Learning-Logical-Reasoning-and-Beyond"><a href="#SynLogic-Synthesizing-Verifiable-Reasoning-Data-at-Scale-for-Learning-Logical-Reasoning-and-Beyond" class="headerlink" title="SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning   Logical Reasoning and Beyond"></a>SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning   Logical Reasoning and Beyond</h2><p><strong>Authors:Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He</strong></p>
<p>Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at <a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI/SynLogic">https://github.com/MiniMax-AI/SynLogic</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¦‚OpenAI-o1å’ŒDeepSeek R1ç­‰è¿›å±•è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚è™½ç„¶å¼€æºå¤åˆ¶å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œä½†å¼€å‘é€šç”¨æ¨ç†èƒ½åŠ›çš„æ–¹æ³•å’Œèµ„æºä»ç„¶æ¢ç´¢ä¸è¶³ã€‚è¿™ä¸€å·®è·éƒ¨åˆ†æ˜¯ç”±äºæ”¶é›†é€‚åˆå¼ºåŒ–å­¦ä¹ çš„å¤šæ ·åŒ–å’Œå¯éªŒè¯çš„æ¨ç†æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å‡è®¾é€»è¾‘æ¨ç†å¯¹äºå‘å±•é€šç”¨æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå› ä¸ºé€»è¾‘æ˜¯æ¨ç†çš„åŸºæœ¬æ„å»ºå—ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SynLogicï¼Œè¿™æ˜¯ä¸€ä¸ªæ•°æ®åˆæˆæ¡†æ¶å’Œæ•°æ®é›†ï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆå¤šæ ·åŒ–çš„é€»è¾‘æ¨ç†æ•°æ®ï¼Œæ¶µç›–35ç§ä¸åŒçš„é€»è¾‘æ¨ç†ä»»åŠ¡ã€‚SynLogicæ–¹æ³•èƒ½å¤Ÿæ§åˆ¶æ•°æ®å’Œéš¾åº¦å’Œæ•°é‡çš„åˆæˆã€‚é‡è¦çš„æ˜¯ï¼Œæ‰€æœ‰ä¾‹å­éƒ½å¯ä»¥é€šè¿‡ç®€å•çš„è§„åˆ™è¿›è¡ŒéªŒè¯ï¼Œå› æ­¤éå¸¸é€‚åˆå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†å¼ºåŒ–å­¦ä¹ åœ¨SynLogicæ•°æ®é›†ä¸Šè®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼ŒåŸºäº7Bå’Œ32Bæ¨¡å‹ã€‚SynLogicåœ¨å¼€æºæ•°æ®é›†ä¸­å¤„äºæœ€å…ˆè¿›çš„é€»è¾‘æ¨ç†æ€§èƒ½åœ°ä½ï¼Œåœ¨BBEHä¸Šè¶…è¶Šäº†DeepSeek-R1-Distill-Qwen-32B 6ä¸ªç‚¹ã€‚æ­¤å¤–ï¼Œå°†SynLogicæ•°æ®ä¸æ•°å­¦å’Œç¼–ç ä»»åŠ¡æ··åˆï¼Œæé«˜äº†è¿™äº›é¢†åŸŸçš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†æ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ··åˆè®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºDeepSeek-R1-Zero-Qwen-32Bã€‚è¿™äº›å‘ç°ä½¿SynLogicæˆä¸ºæ¨åŠ¨LLMæ›´å¹¿æ³›æ¨ç†èƒ½åŠ›çš„é‡è¦èµ„æºã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI/SynLogic">https://github.com/MiniMax-AI/SynLogic</a>å¼€æºäº†æ•°æ®åˆæˆç®¡é“å’ŒSynLogicæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19641v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SynLogicæ•°æ®åˆæˆæ¡†æ¶å’Œæ•°æ®é›†ï¼Œç”¨äºç”Ÿæˆæ¶µç›–35ç§ä¸åŒé€»è¾‘æ¨ç†ä»»åŠ¡çš„å¤šæ ·åŒ–æ•°æ®ã€‚è¯¥æ•°æ®é›†èƒ½å¤Ÿé€šè¿‡è°ƒæ•´éš¾åº¦å’Œæ•°é‡è¿›è¡Œå—æ§çš„æ•°æ®åˆæˆï¼Œæ‰€æœ‰ä¾‹å­éƒ½å¯ä»¥ç”±ç®€å•çš„è§„åˆ™è¿›è¡ŒéªŒè¯ï¼Œéå¸¸é€‚åˆç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ã€‚å®éªŒè¯æ˜ï¼Œåœ¨SynLogicæ•°æ®é›†ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒèƒ½æœ‰æ•ˆæå‡é€»è¾‘æ¨ç†æ€§èƒ½ï¼Œå¹¶è¶…è¶ŠDeepSeek-R1-Distill-Qwen-32Bæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå°†SynLogicæ•°æ®ä¸æ•°å­¦å’Œç¼–ç ä»»åŠ¡æ··åˆè®­ç»ƒï¼Œèƒ½æé«˜è¿™äº›é¢†åŸŸçš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶æ˜¾è‘—å¢å¼ºæ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynLogicæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå¤šæ ·åŒ–é€»è¾‘æ¨ç†æ•°æ®çš„æ•°æ®åˆæˆæ¡†æ¶å’Œæ•°æ®é›†ï¼Œæ¶µç›–35ç§ä¸åŒçš„é€»è¾‘æ¨ç†ä»»åŠ¡ã€‚</li>
<li>SynLogicæ–¹æ³•èƒ½å¤Ÿè¿›è¡Œå—æ§çš„æ•°æ®åˆæˆï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´éš¾åº¦å’Œæ•°é‡ã€‚</li>
<li>æ‰€æœ‰SynLogicç”Ÿæˆçš„ä¾‹å­éƒ½å¯ä»¥ç”±ç®€å•çš„è§„åˆ™è¿›è¡ŒéªŒè¯ï¼Œéå¸¸é€‚åˆç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨SynLogicæ•°æ®é›†ä¸Šçš„è®­ç»ƒèƒ½æœ‰æ•ˆæå‡é€»è¾‘æ¨ç†æ€§èƒ½ï¼Œè¶…è¶ŠæŸäº›ç°æœ‰æ¨¡å‹ã€‚</li>
<li>æ··åˆSynLogicæ•°æ®ä¸æ•°å­¦å’Œç¼–ç ä»»åŠ¡è®­ç»ƒï¼Œèƒ½æé«˜è¿™äº›é¢†åŸŸçš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶å¢å¼ºæ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SynLogicæ•°æ®é›†å¯¹æ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ›´å¹¿æ³›æ¨ç†èƒ½åŠ›å…·æœ‰ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ae1b7b3e50d2b64b7b82db2ac7d4f05d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae46e603d036b51c6aee01664b142f07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ee478aea4d7b9a46e198f0148d35776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d2d1aeb165ce18fca2291b8fbcf6cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce1ec7471c9c52fc4eb01fcfddb11526.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Surrogate-Signals-from-Format-and-Length-Reinforcement-Learning-for-Solving-Mathematical-Problems-without-Ground-Truth-Answers"><a href="#Surrogate-Signals-from-Format-and-Length-Reinforcement-Learning-for-Solving-Mathematical-Problems-without-Ground-Truth-Answers" class="headerlink" title="Surrogate Signals from Format and Length: Reinforcement Learning for   Solving Mathematical Problems without Ground Truth Answers"></a>Surrogate Signals from Format and Length: Reinforcement Learning for   Solving Mathematical Problems without Ground Truth Answers</h2><p><strong>Authors:Rihui Xin, Han Liu, Zecheng Wang, Yupeng Zhang, Dianbo Sui, Xiaolin Hu, Bingning Wang</strong></p>
<p>Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costly, and sometimes unfeasible. This research delves into the utilization of format and length as surrogate signals to train LLMs for mathematical problem-solving, bypassing the need for traditional ground truth answers.Our study shows that a reward function centered on format correctness alone can yield performance improvements comparable to the standard GRPO algorithm in early phases. Recognizing the limitations of format-only rewards in the later phases, we incorporate length-based rewards. The resulting GRPO approach, leveraging format-length surrogate signals, not only matches but surpasses the performance of the standard GRPO algorithm relying on ground truth answers in certain scenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through systematic exploration and experimentation, this research not only offers a practical solution for training LLMs to solve mathematical problems and reducing the dependence on extensive ground truth data collection, but also reveals the essence of why our label-free approach succeeds: base model is like an excellent student who has already mastered mathematical and logical reasoning skills, but performs poorly on the test paper, it simply needs to develop good answering habits to achieve outstanding results in exams , in other words, to unlock the capabilities it already possesses. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå¼ºåŒ–å­¦ä¹ åœ¨é€‚åº”ç‰¹å®šåº”ç”¨æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œä¸ºæ•°å­¦é—®é¢˜è§£å†³è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è·å¾—çœŸå®ç­”æ¡ˆé€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€æˆæœ¬é«˜æ˜‚ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¯è¡Œã€‚æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†åˆ©ç”¨æ ¼å¼å’Œé•¿åº¦ä½œä¸ºä»£ç†ä¿¡å·æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ•°å­¦é—®é¢˜è§£å†³ï¼Œä»è€Œç»•è¿‡å¯¹ä¼ ç»ŸçœŸå®ç­”æ¡ˆçš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»…å›´ç»•æ ¼å¼æ­£ç¡®æ€§è®¾è®¡çš„å¥–åŠ±å‡½æ•°å¯ä»¥åœ¨æ—©æœŸé˜¶æ®µäº§ç”Ÿä¸æ ‡å‡†GRPOç®—æ³•ç›¸å½“çš„æ€§èƒ½æ”¹è¿›ã€‚æ„è¯†åˆ°ä»…ä½¿ç”¨æ ¼å¼å¥–åŠ±åœ¨åæœŸé˜¶æ®µçš„å±€é™æ€§ï¼Œæˆ‘ä»¬ç»“åˆäº†åŸºäºé•¿åº¦çš„å¥–åŠ±ã€‚ç”±æ­¤äº§ç”Ÿçš„GRPOæ–¹æ³•ï¼Œåˆ©ç”¨æ ¼å¼é•¿åº¦ä»£ç†ä¿¡å·ï¼Œä¸ä»…ä¸æ ‡å‡†GRPOç®—æ³•åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œè€Œä¸”åœ¨AIME2024ä¸Šå®ç°äº†40.0%çš„å‡†ç¡®ç‡ï¼Œä½¿ç”¨7BåŸºç¡€æ¨¡å‹ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¢ç´¢å’Œå®éªŒï¼Œæœ¬ç ”ç©¶ä¸ä»…ä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è§£å†³æ•°å­¦é—®é¢˜å¹¶å‡å°‘å¯¹ä¼ ç»ŸçœŸå®ç­”æ¡ˆçš„ä¾èµ–æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œè¿˜æ­ç¤ºäº†æˆ‘ä»¬çš„æ— æ ‡ç­¾æ–¹æ³•æˆåŠŸçš„åŸå› ï¼šåŸºç¡€æ¨¡å‹å°±åƒä¸€ä¸ªå·²ç»æŒæ¡äº†æ•°å­¦å’Œé€»è¾‘æ¨ç†æŠ€èƒ½çš„å­¦ç”Ÿï¼Œä½†åœ¨è¯•å·ä¸Šè¡¨ç°ä¸ä½³ï¼Œå®ƒåªéœ€è¦å…»æˆè‰¯å¥½çš„ç­”é¢˜ä¹ æƒ¯å°±èƒ½åœ¨è€ƒè¯•ä¸­å–å¾—ä¼˜å¼‚æˆç»©ï¼Œæ¢å¥è¯è¯´ï¼Œå°±æ˜¯è¦è§£é”å®ƒå·²ç»æ‹¥æœ‰çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19439v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå¼ºåŒ–å­¦ä¹ åœ¨é€‚åº”ç‰¹å®šåº”ç”¨æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œä¸ºæ•°å­¦é—®é¢˜è§£å†³è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è·å–çœŸå®ç­”æ¡ˆé€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€æˆæœ¬é«˜æ˜‚ï¼Œæœ‰æ—¶ç”šè‡³ä¸å¯è¡Œã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨æ ¼å¼å’Œé•¿åº¦ä½œä¸ºæ›¿ä»£ä¿¡å·æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ•°å­¦é—®é¢˜è§£å†³ï¼Œä»è€Œç»•è¿‡å¯¹çœŸå®ç­”æ¡ˆçš„éœ€æ±‚ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»¥æ ¼å¼æ­£ç¡®æ€§ä¸ºä¸­å¿ƒçš„å¥–åŠ±å‡½æ•°å¯ä»¥åœ¨æ—©æœŸé˜¶æ®µäº§ç”Ÿä¸æ ‡å‡†GRPOç®—æ³•ç›¸å½“çš„æ€§èƒ½æ”¹è¿›ã€‚åœ¨åæœŸé˜¶æ®µï¼Œæˆ‘ä»¬ç»“åˆäº†åŸºäºé•¿åº¦çš„å¥–åŠ±ã€‚ç»“æœäº§ç”Ÿçš„GRPOæ–¹æ³•ï¼Œåˆ©ç”¨æ ¼å¼é•¿åº¦æ›¿ä»£ä¿¡å·ï¼Œä¸ä»…ä¸æ ‡å‡†GRPOç®—æ³•ç›¸å½“ï¼Œè€Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹æ›´èƒœä¸€ç­¹ï¼Œåœ¨AIME2024ä¸Šå®ç°40.0%çš„å‡†ç¡®ç‡ï¼Œä½¿ç”¨7BåŸºç¡€æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºè§£å†³æ•°å­¦é—®é¢˜çš„è¯­è¨€æ¨¡å‹è®­ç»ƒæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œé™ä½äº†å¯¹å¤§é‡çœŸå®æ•°æ®çš„ä¾èµ–ï¼Œè¿˜æ­ç¤ºäº†æ— æ ‡ç­¾æ–¹æ³•æˆåŠŸçš„åŸå› ï¼šåŸºç¡€æ¨¡å‹å°±åƒä¸€ä¸ªå·²ç»æŒæ¡äº†æ•°å­¦å’Œé€»è¾‘æ¨ç†æŠ€èƒ½ä½†éœ€è¦åœ¨è¯•å·ä¸Šè¡¨ç°è‰¯å¥½çš„å­¦ç”Ÿä¸€æ ·ï¼Œå®ƒéœ€è¦å…»æˆå¥½çš„ç­”é¢˜ä¹ æƒ¯æ¥å–å¾—ä¼˜å¼‚çš„æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå¼ºåŒ–å­¦ä¹ åœ¨ç‰¹å®šåº”ç”¨é€‚åº”ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ•°å­¦é—®é¢˜è§£å†³è·å–çœŸå®ç­”æ¡ˆå…·æœ‰æŒ‘æˆ˜æ€§ã€æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨æ ¼å¼å’Œé•¿åº¦ä½œä¸ºæ›¿ä»£ä¿¡å·æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ•°å­¦é—®é¢˜è§£å†³ã€‚</li>
<li>æ ¼å¼æ­£ç¡®æ€§çš„å¥–åŠ±å‡½æ•°åœ¨æ—©æœŸé˜¶æ®µå¯ä»¥äº§ç”Ÿæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>ç»“åˆåŸºäºé•¿åº¦çš„å¥–åŠ±ä»¥å¼¥è¡¥æ ¼å¼å•ä¸€å¥–åŠ±åœ¨åæœŸé˜¶æ®µçš„å±€é™æ€§ã€‚</li>
<li>åˆ©ç”¨æ ¼å¼é•¿åº¦æ›¿ä»£ä¿¡å·çš„GRPOæ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹ä¸æ ‡å‡†æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼Œè¾¾åˆ°AIME2024ä¸Šçš„é«˜å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ecaad05d2ba44fd912b1631524785b4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d20385106b0194dc68760dac405bbd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a733b3f96c1d6ea14ff0c6575233e84b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06032ef805e84cad7da1bcbb71174fd8.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VerIPO-Cultivating-Long-Reasoning-in-Video-LLMs-via-Verifier-Gudied-Iterative-Policy-Optimization"><a href="#VerIPO-Cultivating-Long-Reasoning-in-Video-LLMs-via-Verifier-Gudied-Iterative-Policy-Optimization" class="headerlink" title="VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied   Iterative Policy Optimization"></a>VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied   Iterative Policy Optimization</h2><p><strong>Authors:Yunxin Li, Xinyu Chen, Zitao Li, Zhenyu Liu, Longyue Wang, Wenhan Luo, Baotian Hu, Min Zhang</strong></p>
<p>Applying Reinforcement Learning (RL) to Video Large Language Models (Video-LLMs) shows significant promise for complex video reasoning. However, popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group Relative Policy Optimization (GRPO), are limited by data preparation bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the quality of long chain-of-thoughts (CoTs) and downstream performance.To address these limitations, we propose VerIPO, a Verifier-guided Iterative Policy Optimization method designed to gradually improve video LLMsâ€™ capacity for generating deep, long-term reasoning chains. The core component is Rollout-Aware Verifier, positioned between the GRPO and Direct Preference Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop. This verifier leverages small LLMs as a judge to assess the reasoning logic of rollouts, enabling the construction of high-quality contrastive data, including reflective and contextually consistent CoTs. These curated preference samples drive the efficient DPO stage (7x faster than GRPO), leading to marked improvements in reasoning chain quality, especially in terms of length and contextual consistency. This training loop benefits from GRPOâ€™s expansive search and DPOâ€™s targeted optimization. Experimental results demonstrate: 1) Significantly faster and more effective optimization compared to standard GRPO variants, yielding superior performance; 2) Our trained models exceed the direct inference of large-scale instruction-tuned Video-LLMs, producing long and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long reasoning models (e.g., Video-R1), highlighting its effectiveness and stability. </p>
<blockquote>
<p>å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰å¯¹äºå¤æ‚è§†é¢‘æ¨ç†æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæµè¡Œçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰æ–¹æ³•ï¼Œå¦‚åŸºäºç»“æœçš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå—åˆ°æ•°æ®å‡†å¤‡ç“¶é¢ˆï¼ˆä¾‹å¦‚å™ªå£°æˆ–æˆæœ¬é«˜ï¼‰çš„é™åˆ¶ï¼Œå¹¶ä¸”åœ¨é•¿æ€ç»´é“¾ï¼ˆCoTsï¼‰çš„è´¨é‡å’Œä¸‹æ¸¸æ€§èƒ½æ–¹é¢çš„æ”¹è¿›ä¸ç¨³å®šã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†VerIPOï¼Œè¿™æ˜¯ä¸€ç§éªŒè¯å™¨å¼•å¯¼è¿­ä»£ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨é€æ­¥æ”¹è¿›è§†é¢‘LLMç”Ÿæˆæ·±åº¦ã€é•¿æœŸæ¨ç†é“¾çš„èƒ½åŠ›ã€‚æ ¸å¿ƒç»„ä»¶æ˜¯Rollout-Aware Verifierï¼Œå®ƒä½äºGRPOå’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒé˜¶æ®µä¹‹é—´ï¼Œå½¢æˆGRPO-Verifier-DPOè®­ç»ƒå¾ªç¯ã€‚è¯¥éªŒè¯å™¨åˆ©ç”¨å°å‹LLMä½œä¸ºæ³•å®˜æ¥è¯„ä¼°æ»šåŠ¨æ¡çš„é€»è¾‘æ¨ç†ï¼Œä»è€Œæ„å»ºé«˜è´¨é‡å¯¹æ¯”æ•°æ®ï¼ŒåŒ…æ‹¬åå°„å’Œä¸Šä¸‹æ–‡ä¸€è‡´çš„CoTsã€‚è¿™äº›ç²¾é€‰çš„åå¥½æ ·æœ¬é©±åŠ¨é«˜æ•ˆçš„DPOé˜¶æ®µï¼ˆæ¯”GRPOå¿«7å€ï¼‰ï¼Œå¯¼è‡´æ¨ç†é“¾è´¨é‡æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿åº¦å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢ã€‚è¿™ç§è®­ç»ƒå¾ªç¯å—ç›ŠäºGRPOçš„å¹¿æ³›æœç´¢å’ŒDPOçš„é’ˆå¯¹æ€§ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼š1ï¼‰ä¸ä¼˜åŒ–è¿‡çš„GRPOå˜ä½“ç›¸æ¯”ï¼Œä¼˜åŒ–è¿‡ç¨‹æ›´å¿«ã€æ›´æœ‰æ•ˆï¼Œæ€§èƒ½æ›´ä¼˜ï¼›2ï¼‰æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šæ ·çš„è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šï¼Œèƒ½å¤Ÿäº§ç”Ÿé•¿è€Œä¸Šä¸‹æ–‡è¿è´¯çš„CoTsï¼Œè¶…è¶Šäº†å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´çš„è§†é¢‘LLMçš„ç›´æ¥æ¨ç†ï¼›3ï¼‰æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸€æ¬¡è¿­ä»£ä¸­çš„è¡¨ç°ä¼˜äºå¼ºå¤§çš„LMMsï¼ˆå¦‚Kimi-VLï¼‰å’Œé•¿æœŸæ¨ç†æ¨¡å‹ï¼ˆå¦‚Video-R1ï¼‰ï¼Œçªæ˜¾äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19000v1">PDF</a> 19 pages, 9 figures, Project Link:   <a target="_blank" rel="noopener" href="https://github.com/HITsz-TMG/VerIPO">https://github.com/HITsz-TMG/VerIPO</a></p>
<p><strong>Summary</strong><br>     é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰è¿›è¡Œè®­ç»ƒåœ¨å¤æ‚è§†é¢‘æ¨ç†æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼ºåŒ–ç²¾ç»†è°ƒæ•´ï¼ˆRFTï¼‰æ–¹æ³•ï¼Œå¦‚åŸºäºç»“æœçš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå—é™äºæ•°æ®å‡†å¤‡ç“¶é¢ˆï¼ˆå¦‚å™ªå£°æˆ–é«˜æˆæœ¬ï¼‰ï¼Œå¹¶ä¸”åœ¨é•¿æ€ç»´é“¾ï¼ˆCoTsï¼‰çš„è´¨é‡å’Œä¸‹æ¸¸æ€§èƒ½æå‡æ–¹é¢è¡¨ç°ä¸ç¨³å®šã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†VerIPOæ–¹æ³•ï¼Œé€šè¿‡éªŒè¯å™¨å¼•å¯¼è¿­ä»£ç­–ç•¥ä¼˜åŒ–ï¼Œé€æ­¥å¢å¼ºè§†é¢‘LLMç”Ÿæˆæ·±åº¦ã€é•¿æœŸæ¨ç†é“¾çš„èƒ½åŠ›ã€‚å…¶æ ¸å¿ƒç»„ä»¶æ˜¯Rollout-Aware Verifierï¼Œä½äºGRPOå’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒé˜¶æ®µä¹‹é—´ï¼Œå½¢æˆGRPO-Verifier-DPOè®­ç»ƒå¾ªç¯ã€‚è¯¥éªŒè¯å™¨åˆ©ç”¨å°å‹LLMä½œä¸ºåˆ¤æ–­è€…æ¥è¯„ä¼°æ»šåŠ¨ç»“æœçš„æ¨ç†é€»è¾‘ï¼Œèƒ½å¤Ÿæ„å»ºé«˜è´¨é‡å¯¹æ¯”æ•°æ®ï¼ŒåŒ…æ‹¬åæ€å’Œä¸Šä¸‹æ–‡ä¸€è‡´çš„CoTsã€‚è¿™äº›ç²¾é€‰çš„åå¥½æ ·æœ¬é©±åŠ¨é«˜æ•ˆçš„DPOé˜¶æ®µï¼ˆæ¯”GRPOå¿«7å€ï¼‰ï¼Œåœ¨æ¨ç†é“¾çš„è´¨é‡æ–¹é¢ï¼Œå°¤å…¶æ˜¯åœ¨é•¿åº¦å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¯¥è®­ç»ƒå¾ªç¯å—ç›ŠäºGRPOçš„å¹¿æ³›æœç´¢å’ŒDPOçš„ç›®æ ‡ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡†GRPOå˜ä½“ç›¸æ¯”ï¼Œä¼˜åŒ–æ›´å¿«ã€æ›´æœ‰æ•ˆï¼Œæ€§èƒ½ä¼˜è¶Šï¼›æˆ‘ä»¬çš„è®­ç»ƒæ¨¡å‹åœ¨å¤šæ ·åŒ–çš„è§†é¢‘æ¨ç†ä»»åŠ¡ä¸Šï¼Œèƒ½å¤Ÿäº§ç”Ÿé•¿è€Œä¸Šä¸‹æ–‡ä¸€è‡´çš„CoTsï¼Œè¶…è¿‡å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´Video-LLMçš„ç›´æ¥æ¨ç†ï¼›æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸€æ¬¡è¿­ä»£ä¸­çš„è¡¨ç°ä¼˜äºå¼ºå¤§çš„LMMså’Œé•¿æœŸæ¨ç†æ¨¡å‹ï¼Œå‡¸æ˜¾å…¶æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨å±•ç°å‡ºå¯¹å¤æ‚è§†é¢‘æ¨ç†çš„å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰å¼ºåŒ–ç²¾ç»†è°ƒæ•´æ–¹æ³•é¢ä¸´æ•°æ®å‡†å¤‡ç“¶é¢ˆåŠæ€§èƒ½ä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>VerIPOæ–¹æ³•é€šè¿‡éªŒè¯å™¨å¼•å¯¼è¿­ä»£ç­–ç•¥ä¼˜åŒ–ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™ã€‚</li>
<li>Rollout-Aware Verifieræ˜¯VerIPOçš„æ ¸å¿ƒï¼Œèƒ½æ„å»ºé«˜è´¨é‡å¯¹æ¯”æ•°æ®ã€‚</li>
<li>VerIPOé€šè¿‡æé«˜æ¨ç†é“¾è´¨é‡ï¼Œå°¤å…¶åœ¨é•¿åº¦å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ–¹é¢å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>VerIPOè®­ç»ƒå¾ªç¯ç»“åˆGRPOçš„å¹¿æ³›æœç´¢å’ŒDPOçš„ç›®æ ‡ä¼˜åŒ–ï¼Œå®ç°å¿«é€Ÿä¸”æœ‰æ•ˆçš„ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64431e3edf5b6b28bc61f2fed6617c2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36eebf326da1dbd8806a8c303e87aafc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4286ccc81001fa95b5deb0087ec34f1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f695335a894219145759ca2b32998725.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-01/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-01/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-01/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c5ece0295520d590c06c0e7c317c48ae.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-01  How Transformers Learn Regular Language Recognition A Theoretical Study   on Training Dynamics and Implicit Bias
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-31/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-62a8baf23f981129e20441ec31d906a6.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-31  Video Editing for Audio-Visual Dubbing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-31
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
