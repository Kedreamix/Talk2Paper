<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-01  How Transformers Learn Regular Language Recognition A Theoretical Study   on Training Dynamics and Implicit Bias">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c5ece0295520d590c06c0e7c317c48ae.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-01-æ›´æ–°"><a href="#2025-06-01-æ›´æ–°" class="headerlink" title="2025-06-01 æ›´æ–°"></a>2025-06-01 æ›´æ–°</h1><h2 id="How-Transformers-Learn-Regular-Language-Recognition-A-Theoretical-Study-on-Training-Dynamics-and-Implicit-Bias"><a href="#How-Transformers-Learn-Regular-Language-Recognition-A-Theoretical-Study-on-Training-Dynamics-and-Implicit-Bias" class="headerlink" title="How Transformers Learn Regular Language Recognition: A Theoretical Study   on Training Dynamics and Implicit Bias"></a>How Transformers Learn Regular Language Recognition: A Theoretical Study   on Training Dynamics and Implicit Bias</h2><p><strong>Authors:Ruiquan Huang, Yingbin Liang, Jing Yang</strong></p>
<p>Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as <code>even pairs&#39; and </code>parity checkâ€™, the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1&#x2F;t)$. Our experiments validate those theoretical results. </p>
<blockquote>
<p>è¯­è¨€è¯†åˆ«ä»»åŠ¡åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­æ‰®æ¼”ç€æ ¹æœ¬æ€§çš„è§’è‰²ï¼Œå¹¶ä¸”å·²å¹¿æ³›åº”ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚è¿™äº›ä»»åŠ¡åœ¨è§£é‡Šå˜æ¢å™¨çš„å·¥ä½œåŸç†æ–¹é¢ä¹Ÿèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¸¸è§„è¯­è¨€è¯†åˆ«ç±»åˆ«ä¸­çš„ä¸¤ä¸ªä»£è¡¨æ€§ä»»åŠ¡ï¼Œå³â€œå¶æ•°å¯¹â€å’Œâ€œå¥‡å¶æ ¡éªŒâ€ï¼Œè¿™ä¸¤ä¸ªä»»åŠ¡çš„ç›®æ ‡æ˜¯ç¡®å®šåœ¨ç»™å®šçš„åºåˆ—ä¸­æŸäº›å­åºåˆ—çš„å‡ºç°æ˜¯å¦ä¸ºå¶æ•°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¢ç´¢ç”±æ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚ç»„æˆçš„ä¸€å±‚å˜æ¢å™¨å¦‚ä½•å­¦ä¹ è§£å†³è¿™äº›ä»»åŠ¡ï¼Œé€šè¿‡ç†è®ºåˆ†æå…¶åœ¨æ¢¯åº¦ä¸‹é™ä¸‹çš„è®­ç»ƒåŠ¨æ€ã€‚è™½ç„¶â€œå¶æ•°å¯¹â€å¯ä»¥ç›´æ¥ç”±ä¸€å±‚è½¬æ¢å™¨è§£å†³ï¼Œä½†â€œå¥‡å¶æ ¡éªŒâ€éœ€è¦é€šè¿‡å°†æ€ç»´é“¾ï¼ˆCoTï¼‰é›†æˆåˆ°ä¸ºâ€œå¶æ•°å¯¹â€ä»»åŠ¡è®­ç»ƒè‰¯å¥½çš„è½¬æ¢å™¨çš„æ¨ç†é˜¶æ®µï¼Œæˆ–è€…é›†æˆåˆ°ä¸€å±‚è½¬æ¢å™¨çš„è®­ç»ƒä¸­æ¥è§£å†³ã€‚å¯¹äºè¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚çš„è”åˆè®­ç»ƒè¡¨ç°å‡ºä¸¤ä¸ªæ˜æ˜¾çš„é˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæ³¨æ„åŠ›å±‚è¿…é€Ÿå¢é•¿ï¼Œå°†æ•°æ®åºåˆ—æ˜ å°„æˆå¯åˆ†ç¦»å‘é‡ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ³¨æ„åŠ›å±‚å˜å¾—ç¨³å®šï¼Œè€Œçº¿æ€§å±‚çš„å¢é•¿å‘ˆå¯¹æ•°è¶‹åŠ¿ï¼Œå¹¶ä¸”æœå‘ä¸€ä¸ªæœ€å¤§é—´éš”è¶…å¹³é¢å‘å±•ï¼Œè¯¥è¶…å¹³é¢èƒ½å¤Ÿæ­£ç¡®åœ°å°†æ³¨æ„åŠ›å±‚çš„è¾“å‡ºåˆ†ä¸ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼ŒæŸå¤±ä»¥O(1&#x2F;t)çš„é€Ÿåº¦å‡å°‘ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†è¿™äº›ç†è®ºç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00926v3">PDF</a> accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¯­è¨€è¯†åˆ«ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹â€œå¶æ•°å¯¹â€å’Œâ€œå¥‡å¶æ ¡éªŒâ€è¿™ä¸¤ä¸ªä»£è¡¨æ€§ä»»åŠ¡çš„ç†è®ºåˆ†æã€‚æ–‡ç« èšç„¦äºå•å±‚å˜å‹å™¨æ¨¡å‹ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™åˆ†æå…¶è®­ç»ƒåŠ¨æ€ï¼Œæ¢è®¨æ¨¡å‹å¦‚ä½•å­¦ä¹ è§£å†³è¿™äº›ä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹äºâ€œå¶æ•°å¯¹â€ä»»åŠ¡ï¼Œå•å±‚å˜å‹å™¨å¯ç›´æ¥è§£å†³ï¼›è€Œå¯¹äºâ€œå¥‡å¶æ ¡éªŒâ€ä»»åŠ¡ï¼Œåˆ™éœ€ç»“åˆé“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚çš„è”åˆè®­ç»ƒå‘ˆç°ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ³¨æ„åŠ›å±‚è¿…é€Ÿå¢é•¿ï¼Œå°†æ•°æ®åºåˆ—æ˜ å°„ä¸ºå¯åˆ†å‘é‡ï¼›ç¬¬äºŒé˜¶æ®µæ³¨æ„åŠ›å±‚ç¨³å®šï¼Œçº¿æ€§å±‚å¯¹æ•°å¢é•¿å¹¶é€æ¸æ¥è¿‘æœ€å¤§é—´éš”è¶…å¹³é¢ï¼Œå°†æ³¨æ„åŠ›å±‚è¾“å‡ºåˆ†ä¸ºæ­£è´Ÿæ ·æœ¬ã€‚å®éªŒéªŒè¯äº†è¿™äº›ç†è®ºç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€è¯†åˆ«ä»»åŠ¡æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æ­ç¤ºå˜å‹å™¨å·¥ä½œæœºåˆ¶ã€‚</li>
<li>â€œå¶æ•°å¯¹â€å’Œâ€œå¥‡å¶æ ¡éªŒâ€æ˜¯è¯­è¨€è¯†åˆ«ä¸­çš„ä»£è¡¨æ€§ä»»åŠ¡ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹å¯¹åºåˆ—ä¸­å­åºåˆ—å‡ºç°æ¬¡æ•°çš„åˆ¤æ–­èƒ½åŠ›ã€‚</li>
<li>å•å±‚å˜å‹å™¨æ¨¡å‹é€šè¿‡æ¢¯åº¦ä¸‹é™å­¦ä¹ è§£å†³è¿™äº›ä»»åŠ¡æ—¶ï¼Œå±•ç°å‡ºä¸¤ä¸ªè®­ç»ƒé˜¶æ®µï¼šæ³¨æ„åŠ›å±‚çš„å¿«é€Ÿå¢é•¿å’Œçº¿æ€§å±‚çš„å¯¹æ•°å¢é•¿ã€‚</li>
<li>å¯¹äºâ€œå¶æ•°å¯¹â€ä»»åŠ¡ï¼Œå•å±‚å˜å‹å™¨å¯ç›´æ¥è§£å†³ï¼›è€Œå¯¹äºâ€œå¥‡å¶æ ¡éªŒâ€ä»»åŠ¡ï¼Œéœ€è¦å¼•å…¥é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¿›è¡Œæ¨ç†ã€‚</li>
<li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ³¨æ„åŠ›å±‚å°†æ•°æ®åºåˆ—æ˜ å°„ä¸ºå¯åˆ†å‘é‡ï¼Œè€Œçº¿æ€§å±‚åˆ™è´Ÿè´£æ ¹æ®æœ€å¤§é—´éš”åŸåˆ™æ­£ç¡®åŒºåˆ†æ­£è´Ÿæ ·æœ¬ã€‚</li>
<li>æ¨¡å‹æŸå¤±éšè®­ç»ƒæ—¶é—´é€’å‡ï¼Œéµå¾ªO(1&#x2F;t)çš„é€Ÿç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5351cd24f89ec5f4a109386cbf645c1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Temporal-Relation-Extraction-in-Clinical-Texts-A-Span-based-Graph-Transformer-Approach"><a href="#Temporal-Relation-Extraction-in-Clinical-Texts-A-Span-based-Graph-Transformer-Approach" class="headerlink" title="Temporal Relation Extraction in Clinical Texts: A Span-based Graph   Transformer Approach"></a>Temporal Relation Extraction in Clinical Texts: A Span-based Graph   Transformer Approach</h2><p><strong>Authors:Rochana Chaturvedi, Peyman Baghershahi, Sourav Medya, Barbara Di Eugenio</strong></p>
<p>Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the well-studied I2B2 2012 Temporal Relations Challenge corpus. This task is inherently challenging due to complex clinical language, long documents, and sparse annotations. We introduce GRAPHTREX, a novel method integrating span-based entity-relation extraction, clinical large pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT) to capture local and global dependencies. Our HGT component facilitates information propagation across the document through innovative global landmarks that bridge distant entities. Our method improves the state-of-the-art with 5.5% improvement in the tempeval $F_1$ score over the previous best and up to 8.9% improvement on long-range relations, which presents a formidable challenge. We further demonstrate generalizability by establishing a strong baseline on the E3C corpus. This work not only advances temporal information extraction but also lays the groundwork for improved diagnostic and prognostic models through enhanced temporal reasoning. </p>
<blockquote>
<p>ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–æ—¶é—´ä¿¡æ¯æ˜¯ä¸Šä¸‹æ–‡äº‹ä»¶åŒ–å’Œè·å–å¯æ“ä½œæ´å¯Ÿçš„å…³é”®ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚æˆ‘ä»¬åˆ©ç”¨ç»è¿‡æ·±å…¥ç ”ç©¶çš„I2B2 2012æ—¶é—´å…³ç³»æŒ‘æˆ˜èµ›è¯­æ–™åº“ï¼Œæ¥è§£å†³æå–ä¸´åºŠäº‹ä»¶åŠå…¶æ—¶é—´å…³ç³»çš„ä»»åŠ¡ã€‚ç”±äºå¤æ‚çš„ä¸´åºŠè¯­è¨€ã€é•¿æ–‡æ¡£å’Œç¨€ç–çš„æ³¨é‡Šï¼Œæ­¤ä»»åŠ¡æœ¬è´¨ä¸Šæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†GRAPHTREXï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œèåˆäº†åŸºäºèŒƒå›´çš„å®ä½“å…³ç³»æå–ã€ä¸´åºŠå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLPLMsï¼‰å’Œå¼‚è´¨å›¾è½¬æ¢å™¨ï¼ˆHGTï¼‰ï¼Œä»¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬çš„HGTç»„ä»¶é€šè¿‡åˆ›æ–°çš„å…¨å±€åœ°æ ‡ï¼Œåœ¨æ–‡æ¡£ä¸­å®ç°äº†è·¨æ–‡æ¡£çš„ä¿¡æ¯ä¼ æ’­ï¼Œè¿™äº›åœ°æ ‡æ¶èµ·äº†é¥è¿œå®ä½“ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨tempeval F1åˆ†æ•°ä¸Šæé«˜äº†5.5%ï¼Œåœ¨è¿œç¨‹å…³ç³»ä¸Šæœ€å¤šæé«˜äº†8.9%ï¼Œå¯¹å…ˆå‰çš„æœ€ä½³æ°´å¹³æ„æˆäº†å¼ºæœ‰åŠ›çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨E3Cè¯­æ–™åº“ä¸Šå»ºç«‹äº†å¼ºå¤§çš„åŸºçº¿æ¥è¿›ä¸€æ­¥è¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ¨åŠ¨äº†æ—¶é—´ä¿¡æ¯æå–çš„è¿›æ­¥ï¼Œè€Œä¸”é€šè¿‡å¢å¼ºæ—¶é—´æ¨ç†ä¸ºæ”¹è¿›è¯Šæ–­å’Œé¢„åæ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18085v2">PDF</a> Introducing a novel method for joint extraction of medical events and   temporal relations from free-text, leveraging clinical LPLMs and   Heterogeneous Graph Transformers, achieving a 5.5% improvement over the   previous state-of-the-art and up to 8.9% on long-range relations</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºI2B2 2012æ—¶åºå…³ç³»æŒ‘æˆ˜è¯­æ–™åº“ï¼Œåˆ©ç”¨spanå®ä½“å…³ç³»æå–ã€ä¸´åºŠå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¼‚è´¨å›¾è½¬æ¢å™¨ï¼ˆHGTï¼‰ç­‰æŠ€æœ¯ï¼Œå®ç°ä¸´åºŠäº‹ä»¶åŠå…¶æ—¶åºå…³ç³»çš„æå–ã€‚è¯¥æ–¹æ³•é€šè¿‡å…¨å±€åœ°æ ‡åˆ›æ–°æ€§åœ°å®ç°æ–‡æ¡£ä¸­çš„ä¿¡æ¯ä¼ æ’­ï¼Œå¹¶æœ‰æ•ˆæé«˜è¿œè¿‘å®ä½“å…³ç³»çš„æ•æ‰èƒ½åŠ›ã€‚æœ¬æ–‡æ–¹æ³•ä¸ä»…æå‡äº†æ—¶åºä¿¡æ¯æå–çš„æ€§èƒ½ï¼Œè¿˜ä¸ºè¯Šæ–­ä¸é¢„åæ¨¡å‹çš„æ”¹è¿›æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åˆ©ç”¨I2B2 2012è¯­æ–™åº“è¿›è¡Œä¸´åºŠäº‹ä»¶åŠå…¶æ—¶åºå…³ç³»æå–çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GRAPHTREXï¼Œé›†æˆäº†spanå®ä½“å…³ç³»æå–ã€å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¼‚è´¨å›¾è½¬æ¢å™¨ï¼ˆHGTï¼‰ã€‚</li>
<li>HGTç»„ä»¶é€šè¿‡å…¨å±€åœ°æ ‡å®ç°æ–‡æ¡£ä¸­çš„ä¿¡æ¯ä¼ æ’­ï¼Œæé«˜äº†æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¾èµ–å…³ç³»çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨tempeval $F_1$å¾—åˆ†ä¸Šè¾ƒä¹‹å‰æœ€ä½³æ–¹æ³•æé«˜äº†5.5%ï¼Œå¹¶åœ¨é•¿ç¨‹å…³ç³»ä¸Šæé«˜äº†8.9%ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…åœ¨æ—¶åºä¿¡æ¯æå–ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œè¿˜åœ¨è¯Šæ–­å’Œé¢„åæ¨¡å‹çš„æ”¹è¿›ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨E3Cè¯­æ–™åº“ä¸Šå»ºç«‹å¼ºå¤§çš„åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c99bc342a26ba051e8606d9d196d35b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-210edbe46cbbd1e3641e3b7d5662930d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33c77763bcc8c010355119bb166d4672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fdc08697505d55738748bc034c81b21.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HiDe-LLaVA-Hierarchical-Decoupling-for-Continual-Instruction-Tuning-of-Multimodal-Large-Language-Model"><a href="#HiDe-LLaVA-Hierarchical-Decoupling-for-Continual-Instruction-Tuning-of-Multimodal-Large-Language-Model" class="headerlink" title="HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of   Multimodal Large Language Model"></a>HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of   Multimodal Large Language Model</h2><p><strong>Authors:Haiyang Guo, Fanhu Zeng, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu</strong></p>
<p>Instruction tuning is widely used to improve a pre-trained Multimodal Large Language Model (MLLM) by training it on curated task-specific datasets, enabling better comprehension of human instructions. However, it is infeasible to collect all possible instruction datasets simultaneously in real-world scenarios. Thus, enabling MLLM with continual instruction tuning is essential for maintaining their adaptability. However, existing methods often trade off memory efficiency for performance gains, significantly compromising overall efficiency. In this paper, we propose a task-specific expansion and task-general fusion framework based on the variations in Centered Kernel Alignment (CKA) similarity across different model layers when trained on diverse datasets. Furthermore, we analyze the information leakage present in the existing benchmark and propose a new and more challenging benchmark to rationally evaluate the performance of different methods. Comprehensive experiments showcase a significant performance improvement of our method compared to existing state-of-the-art methods. Code and dataset are released at <a target="_blank" rel="noopener" href="https://github.com/Ghy0501/HiDe-LLaVA">https://github.com/Ghy0501/HiDe-LLaVA</a>. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒè¢«å¹¿æ³›ç”¨äºé€šè¿‡è®­ç»ƒåœ¨ç²¾é€‰çš„ä»»åŠ¡ç‰¹å®šæ•°æ®é›†ä¸Šæ¥æ”¹è¿›é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä»è€Œå®ç°æ›´å¥½çš„å¯¹äººç±»æŒ‡ä»¤çš„ç†è§£ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œçš„åœºæ™¯ä¸­ï¼ŒåŒæ—¶æ”¶é›†æ‰€æœ‰å¯èƒ½çš„æŒ‡ä»¤æ•°æ®é›†æ˜¯ä¸å¯èƒ½çš„ã€‚å› æ­¤ï¼Œä¸ºMLLMå¯ç”¨æŒç»­æŒ‡ä»¤å¾®è°ƒå¯¹äºä¿æŒå…¶é€‚åº”æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä»¥ç‰ºç‰²å†…å­˜æ•ˆç‡ä¸ºä»£ä»·æ¥æé«˜æ€§èƒ½ï¼Œä»è€Œä¸¥é‡å½±å“æ•´ä½“æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºè®­ç»ƒå¤šæ ·åŒ–æ•°æ®é›†æ—¶ä¸åŒæ¨¡å‹å±‚ä¸­å¿ƒå†…æ ¸å¯¹é½ï¼ˆCKAï¼‰ç›¸ä¼¼æ€§çš„å˜åŒ–çš„ä»»åŠ¡ç‰¹å®šæ‰©å±•å’Œä»»åŠ¡é€šç”¨èåˆæ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„ä¿¡æ¯æ³„éœ²é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•æ¥åˆç†è¯„ä¼°ä¸åŒæ–¹æ³•çš„æ€§èƒ½ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä»£ç å’Œæ•°æ®é›†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Ghy0501/HiDe-LLaVA%E4%B8%8A%E3%80%82">https://github.com/Ghy0501/HiDe-LLaVAä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12941v2">PDF</a> ACL 2025 (Main)</p>
<p><strong>æ€»ç»“</strong><br>è®­ç»ƒæ•°æ®å—é™çš„çœŸå®ä¸–ç•Œä¸­ï¼Œä¸æ–­å¾®è°ƒæŒ‡å¯¼æ¨¡å‹ç”¨äºå¢å¼ºå…¶å¯¹å„ç§ä»»åŠ¡çš„å­¦ä¹ æ˜¯å¿…è¦ä¹‹ä¸¾ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨è®°å¿†æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚æœ¬æ–‡åŸºäºä¸åŒæ•°æ®é›†è®­ç»ƒä¸‹æ¨¡å‹å„å±‚é—´çš„ä¸­å¿ƒæ ¸å¯¹é½ï¼ˆCKAï¼‰ç›¸ä¼¼åº¦å˜åŒ–ï¼Œæå‡ºç‰¹å®šä»»åŠ¡æ‰©å±•å’Œé€šç”¨ä»»åŠ¡èåˆæ¡†æ¶ï¼ŒåŒæ—¶åˆ†æç°æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„ä¿¡æ¯æ³„éœ²é—®é¢˜ï¼Œå¹¶æ¨å‡ºæ›´å…·æŒ‘æˆ˜æ€§çš„æ–°åŸºå‡†æµ‹è¯•ã€‚æœ¬æ–‡çš„æ–¹æ³•è¾ƒç°æœ‰çš„å°–ç«¯æŠ€æœ¯æ˜¾è‘—æé«˜æ€§èƒ½ã€‚GitHubä¸Šæœ‰å¯¹åº”çš„ä»£ç å’Œæ•°æ®é›†èµ„æºï¼š<a target="_blank" rel="noopener" href="https://github.com/Ghy0501/HiDe-LLaVA%E3%80%82%E8%AF%A5LLM%E6%80%A7%E8%83%BD%E6%8C%81%E7%BB%AD%E6%94%B9%E8%BF%9B%E6%8A%80%E6%9C%AF%E5%AF%B9%E5%AE%9E%E7%8E%B0%E6%99%BA%E8%83%BD%E5%8C%96%E5%8D%87%E7%BA%A7%E5%A4%A7%E6%9C%89%E8%A3%A8%E7%9B%8A%E3%80%82">https://github.com/Ghy0501/HiDe-LLaVAã€‚è¯¥LLMæ€§èƒ½æŒç»­æ”¹è¿›æŠ€æœ¯å¯¹å®ç°æ™ºèƒ½åŒ–å‡çº§å¤§æœ‰è£¨ç›Šã€‚</a></p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ul>
<li>è®­ç»ƒæ•°æ®å—é™çš„çœŸå®ä¸–ç•Œä¸­ï¼ŒæŒç»­å¾®è°ƒæŒ‡å¯¼æ¨¡å‹ä»¥å¢å¼ºå…¶å¯¹å„ç§ä»»åŠ¡çš„å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€ç‰ºç‰²è®°å¿†æ•ˆç‡æ¥æ¢å–æ€§èƒ½æå‡ï¼Œéš¾ä»¥è¾¾åˆ°ç†æƒ³æ•ˆæœã€‚</li>
<li>åŸºäºCKAç›¸ä¼¼åº¦å˜åŒ–æå‡ºç‰¹å®šä»»åŠ¡æ‰©å±•å’Œé€šç”¨ä»»åŠ¡èåˆæ¡†æ¶ï¼Œæœ‰æ•ˆæ”¹è¿›æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97ac3105965ad2029e1fcacffd3161cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61115fde72b7507075cb36733e11e93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14484e830d8e49529d3164b2421cbbae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9141ea9ab4b1c147373f6265aa995f7f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Length-Controlled-Margin-Based-Preference-Optimization-without-Reference-Model"><a href="#Length-Controlled-Margin-Based-Preference-Optimization-without-Reference-Model" class="headerlink" title="Length-Controlled Margin-Based Preference Optimization without Reference   Model"></a>Length-Controlled Margin-Based Preference Optimization without Reference   Model</h2><p><strong>Authors:Gengxu Li, Tingyu Xia, Yi Chang, Yuan Wu</strong></p>
<p>Direct Preference Optimization (DPO) is a widely adopted offline algorithm for preference-based reinforcement learning from human feedback (RLHF), designed to improve training simplicity and stability by redefining reward functions. However, DPO is hindered by several limitations, including length bias, memory inefficiency, and probability degradation. To address these challenges, we propose Length-Controlled Margin-Based Preference Optimization (LMPO), a more efficient and robust alternative. LMPO introduces a uniform reference model as an upper bound for the DPO loss, enabling a more accurate approximation of the original optimization objective. Additionally, an average log-probability optimization strategy is employed to minimize discrepancies between training and inference phases. A key innovation of LMPO lies in its Length-Controlled Margin-Based loss function, integrated within the Bradley-Terry framework. This loss function regulates response length while simultaneously widening the margin between preferred and rejected outputs. By doing so, it mitigates probability degradation for both accepted and discarded responses, addressing a significant limitation of existing methods. We evaluate LMPO against state-of-the-art preference optimization techniques on two open-ended large language models, Mistral and LLaMA3, across six conditional benchmarks. Our experimental results demonstrate that LMPO effectively controls response length, reduces probability degradation, and outperforms existing approaches. The code is available at <a target="_blank" rel="noopener" href="https://github.com/gengxuli/LMPO">https://github.com/gengxuli/LMPO</a>. </p>
<blockquote>
<p>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºåŸºäºäººç±»åé¦ˆçš„åå¥½å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„ç¦»çº¿ç®—æ³•ï¼Œé€šè¿‡é‡æ–°å®šä¹‰å¥–åŠ±å‡½æ•°æ¥æé«˜è®­ç»ƒç®€æ´æ€§å’Œç¨³å®šæ€§ã€‚ç„¶è€Œï¼ŒDPOå—åˆ°é•¿åº¦åè§ã€å†…å­˜æ•ˆç‡ä½ä¸‹å’Œæ¦‚ç‡é€€åŒ–ç­‰é™åˆ¶çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é•¿åº¦æ§åˆ¶è¾¹è·åŸºäºåå¥½ä¼˜åŒ–ï¼ˆLMPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ›´é«˜æ•ˆå’Œç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚LMPOå¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„å‚è€ƒæ¨¡å‹ä½œä¸ºDPOæŸå¤±çš„ä¸Šç•Œï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°é€¼è¿‘åŸå§‹ä¼˜åŒ–ç›®æ ‡ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨å¹³å‡å¯¹æ•°æ¦‚ç‡ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥å‡å°è®­ç»ƒå’Œæ¨ç†é˜¶æ®µä¹‹é—´çš„å·®å¼‚ã€‚LMPOçš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶é•¿åº¦æ§åˆ¶è¾¹è·åŸºç¡€ä¸Šçš„æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°ç»“åˆäº†Bradley-Terryæ¡†æ¶ã€‚è¯¥æŸå¤±å‡½æ•°æ—¢æ§åˆ¶å“åº”é•¿åº¦ï¼Œåˆæ‰©å¤§äº†é¦–é€‰å’Œæ‹’ç»è¾“å‡ºä¹‹é—´çš„è¾¹è·ã€‚é€šè¿‡è¿™æ ·åšï¼Œå®ƒå‡è½»äº†æ¥å—å’Œä¸¢å¼ƒå“åº”çš„æ¦‚ç‡é€€åŒ–é—®é¢˜ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„ä¸€ä¸ªé‡è¦é™åˆ¶ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¼€æ”¾çš„å¤§å‹è¯­è¨€æ¨¡å‹Mistralå’ŒLLaMA3ä¸Šï¼Œå¯¹LMPOä¸æœ€å…ˆè¿›çš„åå¥½ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œäº†è¯„ä¼°ï¼Œè·¨è¶Šå…­ä¸ªæ¡ä»¶åŸºå‡†ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLMPOæœ‰æ•ˆåœ°æ§åˆ¶äº†å“åº”é•¿åº¦ï¼Œå‡å°‘äº†æ¦‚ç‡é€€åŒ–ï¼Œå¹¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/gengxuli/LMPO%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/gengxuli/LMPOæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14643v2">PDF</a> 18 pages, 3 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Direct Preference Optimizationï¼ˆDPOï¼‰åœ¨åŸºäºäººç±»åé¦ˆçš„åå¥½å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†Length-Controlled Margin-Based Preference Optimizationï¼ˆLMPOï¼‰ä½œä¸ºæ›´æœ‰æ•ˆç‡ä¸”ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚LMPOå¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„å‚è€ƒæ¨¡å‹ä½œä¸ºDPOæŸå¤±çš„ä¸Šé™ï¼Œå¹¶é‡‡ç”¨å¹³å‡å¯¹æ•°æ¦‚ç‡ä¼˜åŒ–ç­–ç•¥æ¥ç¼©å°è®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„å·®å¼‚ã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºLength-Controlled Margin-BasedæŸå¤±å‡½æ•°ï¼Œè¯¥æŸå¤±å‡½æ•°èƒ½åœ¨Bradley-Terryæ¡†æ¶ä¸‹è°ƒèŠ‚å“åº”é•¿åº¦å¹¶æ‰©å¤§é¦–é€‰å’Œæ‹’ç»è¾“å‡ºä¹‹é—´çš„é—´éš”ï¼Œä»è€Œè§£å†³ç°æœ‰æ–¹æ³•çš„æ¦‚ç‡é€€åŒ–é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLMPOåœ¨æ§åˆ¶å“åº”é•¿åº¦ã€å‡å°‘æ¦‚ç‡é€€åŒ–æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨ä¸¤ä¸ªå¤§å‹å¼€æ”¾å¼è¯­è¨€æ¨¡å‹ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DPOæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„å¸¸ç”¨ç¦»çº¿ç®—æ³•ï¼Œä½†åœ¨å¤„ç†äººç±»åé¦ˆæ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LMPOä½œä¸ºä¸€ç§æ”¹è¿›æ–¹æ³•è¢«æå‡ºï¼Œè§£å†³äº†DPOå­˜åœ¨çš„é•¿åº¦åè§ã€å†…å­˜æ•ˆç‡ä½å’Œæ¦‚ç‡é€€åŒ–ç­‰é—®é¢˜ã€‚</li>
<li>LMPOé€šè¿‡å¼•å…¥ç»Ÿä¸€å‚è€ƒæ¨¡å‹å’Œå¹³å‡å¯¹æ•°æ¦‚ç‡ä¼˜åŒ–ç­–ç•¥æ¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>Length-Controlled Margin-BasedæŸå¤±å‡½æ•°æ˜¯LMPOçš„å…³é”®åˆ›æ–°ç‚¹ï¼Œå®ƒèƒ½è°ƒèŠ‚å“åº”é•¿åº¦å¹¶æ‰©å¤§é¦–é€‰å’Œæ‹’ç»è¾“å‡ºé—´çš„é—´éš”ã€‚</li>
<li>LMPOèƒ½æœ‰æ•ˆæ§åˆ¶å“åº”é•¿åº¦å¹¶å‡å°‘æ¦‚ç‡é€€åŒ–ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLMPOåœ¨å¤šä¸ªæ¡ä»¶åŸºå‡†æµ‹è¯•ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41a5ba839ad24072749e60b454260e66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e57b54ab3d4abc8f3e723836929216d1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GeLLMO-Generalizing-Large-Language-Models-for-Multi-property-Molecule-Optimization"><a href="#GeLLMO-Generalizing-Large-Language-Models-for-Multi-property-Molecule-Optimization" class="headerlink" title="GeLLMO: Generalizing Large Language Models for Multi-property Molecule   Optimization"></a>GeLLMO: Generalizing Large Language Models for Multi-property Molecule   Optimization</h2><p><strong>Authors:Vishal Dey, Xiao Hu, Xia Ning</strong></p>
<p>Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMsâ€™ potential for molecule optimization, we introduce MuMOInstruct, the first high-quality instruction-tuning dataset specifically focused on complex multi-property molecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLMOs, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that GeLLMOs consistently outperform state-of-the-art baselines. GeLLMOs also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of GeLLMOs as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. MuMOInstruct, models, and code are accessible through <a target="_blank" rel="noopener" href="https://github.com/ninglab/GeLLMO">https://github.com/ninglab/GeLLMO</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†å¤§å¤šæ•°åˆ†å­ä¼˜åŒ–çš„è®¡ç®—æ–¹æ³•ä»ç„¶å±€é™äºå•å±æ€§æˆ–åŒå±æ€§ä¼˜åŒ–ä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨æ‰©å±•åˆ°æ–°ä¼˜åŒ–ä»»åŠ¡æ—¶é¢ä¸´å¯æ‰©å±•æ€§å·®å’Œé€šç”¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–°å‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†æ˜¾è‘—çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å±•ç¤ºLLMåœ¨åˆ†å­ä¼˜åŒ–æ–¹é¢çš„æ½œåŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MuMOInstructï¼Œè¿™æ˜¯é¦–ä¸ªä¸“æ³¨äºå¤æ‚å¤šå±æ€§åˆ†å­ä¼˜åŒ–ä»»åŠ¡çš„é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚åˆ©ç”¨MuMOInstructï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—ç”¨äºåˆ†å­ä¼˜åŒ–çš„æŒ‡ä»¤è°ƒä¼˜LLMï¼Œç§°ä¸ºGeLLMOsã€‚åœ¨5ä¸ªé¢†åŸŸå†…çš„ä»»åŠ¡å’Œ5ä¸ªè·¨åŸŸä»»åŠ¡ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒGeLLMOså§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ã€‚GeLLMOsåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºå‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºåŠŸèƒ½å¼ºå¤§çš„å°é—­å¼LLMã€‚è¿™ç§å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›è¯æ˜äº†GeLLMOsä½œä¸ºåˆ†å­ä¼˜åŒ–åŸºç¡€æ¨¡å‹çš„å·¨å¤§æ½œåŠ›ï¼Œä»è€Œå¯ä»¥åœ¨æ— éœ€èµ„æºå¯†é›†å‹çš„é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹è§£å†³æ–°å‹ä¼˜åŒ–ä»»åŠ¡ã€‚å¯ä»¥é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ninglab/GeLLMO%E8%AE%BF%E9%97%AEMuMOInstruct%E3%80%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/ninglab/GeLLMOè®¿é—®MuMOInstructã€æ¨¡å‹å’Œä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13398v2">PDF</a> Accepted to ACL Main 2025. Vishal Dey and Xiao Hu contributed equally   to this paper</p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨åˆ†å­ä¼˜åŒ–é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ä¸ºåº”å¯¹å½“å‰åˆ†å­ä¼˜åŒ–è®¡ç®—æ–¹æ³•çš„å±€é™ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†MuMOInstructæ•°æ®é›†ï¼Œå¹¶åŸºäºæ­¤å¼€å‘äº†GeLLMOsç³»åˆ—æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨åŸŸå†…å’ŒåŸŸå¤–ä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œä½“ç°äº†å¼ºå¤§çš„é›¶å°„å‡»èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºåˆ†å­ä¼˜åŒ–åŸºç¡€æ¨¡å‹çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨åˆ†å­ä¼˜åŒ–é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰åˆ†å­ä¼˜åŒ–è®¡ç®—æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å•&#x2F;åŒå±æ€§ä¼˜åŒ–ä»»åŠ¡çš„çº¦æŸã€è¾ƒå·®çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MuMOInstructæ•°æ®é›†æ˜¯é¦–ä¸ªä¸“æ³¨äºå¤æ‚å¤šå±æ€§åˆ†å­ä¼˜åŒ–ä»»åŠ¡çš„é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚</li>
<li>GeLLMOsç³»åˆ—æ¨¡å‹åˆ©ç”¨MuMOInstructæ•°æ®é›†è¿›è¡Œå¼€å‘ï¼Œç”¨äºåˆ†å­ä¼˜åŒ–ã€‚</li>
<li>GeLLMOsåœ¨åŸŸå†…å’ŒåŸŸå¤–ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡è¶…è¶Šç°æœ‰åŸºçº¿ã€‚</li>
<li>GeLLMOså±•ç°å‡ºå¼ºå¤§çš„é›¶å°„å‡»æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœªè§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13398">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-377655278f015d543a662871b7eb36a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3294e87e2f86ecf8d19036d6ed0ecc83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abff64608effa7baa27e26b75ca91d65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b7a0e742ec8ce7db7d7ef1fb4a58370.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MUDDFormer-Breaking-Residual-Bottlenecks-in-Transformers-via-Multiway-Dynamic-Dense-Connections"><a href="#MUDDFormer-Breaking-Residual-Bottlenecks-in-Transformers-via-Multiway-Dynamic-Dense-Connections" class="headerlink" title="MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway   Dynamic Dense Connections"></a>MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway   Dynamic Dense Connections</h2><p><strong>Authors:Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan</strong></p>
<p>We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at <a target="_blank" rel="noopener" href="https://github.com/Caiyun-AI/MUDDFormer">https://github.com/Caiyun-AI/MUDDFormer</a> . </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å¤šå‘åŠ¨æ€å¯†é›†ï¼ˆMUDDï¼‰è¿æ¥ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥è§£å†³æ®‹å·®è¿æ¥çš„å±€é™æ€§ï¼Œå¹¶å¢å¼ºTransformerä¸­çš„è·¨å±‚ä¿¡æ¯æµåŠ¨ã€‚ä¸ç°æœ‰å…·æœ‰é™æ€å’Œå…±äº«è¿æ¥æƒé‡çš„å¯†é›†è¿æ¥æ–¹æ³•ä¸åŒï¼ŒMUDDæ ¹æ®æ¯ä¸ªåºåˆ—ä½ç½®å’ŒTransformerå—çš„æ¯ä¸ªç‹¬ç«‹è¾“å…¥æµï¼ˆæŸ¥è¯¢ã€é”®ã€å€¼æˆ–æ®‹å·®ï¼‰çš„éšè—çŠ¶æ€åŠ¨æ€ç”Ÿæˆè¿æ¥æƒé‡ã€‚MUDDè¿æ¥å¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ä»»ä½•Transformeræ¶æ„ä¸­ï¼Œä»¥åˆ›å»ºMUDDFormerã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMUDDFormeråœ¨å„ç§æ¨¡å‹æ¶æ„å’Œè§„æ¨¡ä¸Šçš„è¯­è¨€å»ºæ¨¡ä¸­æ˜¾è‘—ä¼˜äºTransformerï¼Œå®ç°äº†ä½¿ç”¨1.8å€è‡³2.4å€è®¡ç®—çš„Transformeræ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMUDDPythia-2.8Båœ¨é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä¸­ä¸Pythia-6.9Bç›¸åŒ¹æ•Œï¼Œç”šè‡³åœ¨äº”ä¸ªé•œå¤´è®¾ç½®ä¸­èƒ½å¤Ÿä¸Pythia-12Bç«äº‰ï¼ŒåŒæ—¶åªå¢åŠ äº†0.23%çš„å‚æ•°å’Œ0.4%çš„è®¡ç®—é‡ã€‚JAXå’ŒPyTorchçš„ä»£ç ä»¥åŠé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Caiyun-AI/MUDDFormer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Caiyun-AI/MUDDFormeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12170v2">PDF</a> Accepted to the 42nd International Conference on Machine Learning   (ICMLâ€™25)</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºMUltiway Dynamic Denseï¼ˆMUDDï¼‰è¿æ¥ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œè§£å†³äº†æ®‹å·®è¿æ¥çš„å±€é™æ€§ï¼Œå¢å¼ºäº†Transformerä¸­çš„è·¨å±‚ä¿¡æ¯æµåŠ¨ã€‚MUDDåŠ¨æ€ç”Ÿæˆè¿æ¥æƒé‡ï¼Œå–å†³äºæ¯ä¸ªåºåˆ—ä½ç½®å’ŒTransformerå—çš„æ¯ä¸ªç‹¬ç«‹è¾“å…¥æµï¼ˆæŸ¥è¯¢ã€é”®ã€å€¼æˆ–æ®‹å·®ï¼‰çš„éšè—çŠ¶æ€ã€‚MUDDè¿æ¥å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•Transformeræ¶æ„ä¸­ï¼Œåˆ›å»ºMUDDFormerã€‚å®éªŒè¡¨æ˜ï¼ŒMUDDFormeråœ¨å„ç§æ¨¡å‹æ¶æ„å’Œè§„æ¨¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºTransformerï¼Œåœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢è¾¾åˆ°äº†ä½¿ç”¨1.8X-2.4Xè®¡ç®—è®­ç»ƒçš„Transformerçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>MUDDè¿æ¥æ˜¯ä¸€ç§é’ˆå¯¹Transformerä¸­æ®‹å·®è¿æ¥å±€é™æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨å¢å¼ºè·¨å±‚ä¿¡æ¯æµåŠ¨ã€‚</p>
</li>
<li><p>MUDDåŠ¨æ€ç”Ÿæˆè¿æ¥æƒé‡ï¼ŒåŸºäºæ¯ä¸ªåºåˆ—ä½ç½®å’ŒTransformerå—å†…æ¯ä¸ªç‹¬ç«‹è¾“å…¥æµçš„éšè—çŠ¶æ€ã€‚</p>
</li>
<li><p>MUDDè¿æ¥å¯æ— ç¼é›†æˆåˆ°ä»»ä½•Transformeræ¶æ„ä¸­ï¼Œå½¢æˆMUDDFormerã€‚</p>
</li>
<li><p>MUDDFormeråœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸTransformerï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</p>
</li>
<li><p>MUDDPythia-2.8Bæ¨¡å‹åœ¨é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸Pythia-6.9Bç›¸åŒ¹é…ï¼Œå¹¶åœ¨äº”é•œå¤´è®¾ç½®ä¸­ç”šè‡³ä¸Pythia-12Bç›¸ç«äº‰ã€‚</p>
</li>
<li><p>MUDDè¿æ¥çš„å‚æ•°å¢åŠ å¾ˆå°‘ï¼ˆä»…0.23%ï¼‰ï¼Œè®¡ç®—é‡å¢åŠ ä¹Ÿå¾ˆæœ‰é™ï¼ˆä»…0.4%ï¼‰ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c051e65d74f132b4053a2cbb6565a9ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69aae94f5b8967228ccfc079bf87be2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70a53df2faf533d14c35579db724f2c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1853e8a87f5c123c116c6f3f1f75a7d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a5475acc7656dbb0fb28945faab04ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e6a314b68317cd081cbc8a50cb8e750.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation"><a href="#DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation" class="headerlink" title="DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation"></a>DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation</h2><p><strong>Authors:Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang</strong></p>
<p>Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness. </p>
<blockquote>
<p>è¿‘æœŸæœ‰å‡ é¡¹ç ”ç©¶å°è¯•ç»“åˆæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ï¼Œå°è¯•æ— ç¦»æ•£è¯­éŸ³ç¬¦å·ç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºï¼Œä½†å®ƒä»¬ç»å¸¸é¢ä¸´è®¡ç®—è´Ÿè½½è¿‡å¤§æˆ–ç»“æœä¸ç†æƒ³ç­‰æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ‰©æ•£è½¬æ¢å™¨è‡ªå›å½’å»ºæ¨¡ï¼ˆDiTARï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¯­è¨€æ¨¡å‹å’Œæ‰©æ•£è½¬æ¢å™¨çš„åŸºäºè¡¥ä¸çš„è‡ªå›å½’æ¡†æ¶ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†è‡ªå›å½’æ¨¡å‹å¯¹è¿ç»­ç¬¦å·çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚DiTARé‡‡ç”¨ä¸€ç§åˆ†è€Œæ²»ä¹‹çš„è¡¥ä¸ç”Ÿæˆç­–ç•¥ï¼Œè¯­è¨€æ¨¡å‹å¤„ç†èšåˆçš„è¡¥ä¸åµŒå…¥ï¼Œç„¶åæ‰©æ•£è½¬æ¢å™¨æ ¹æ®è¯­è¨€æ¨¡å‹çš„è¾“å‡ºç”Ÿæˆä¸‹ä¸€ä¸ªè¡¥ä¸ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºå°†æ¸©åº¦å®šä¹‰ä¸ºåœ¨åå‘æ‰©æ•£ODEä¸­å¼•å…¥å™ªå£°çš„æ—¶é—´ç‚¹ï¼Œä»¥å¹³è¡¡å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚åœ¨å¹¿æ³›çš„è§„æ¨¡åˆ†æä¸­ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜DiTARå…·æœ‰å“è¶Šçš„å¯æ‰©å±•æ€§ã€‚åœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä¸­ï¼ŒDiTARåœ¨ç¨³å¥æ€§ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œè‡ªç„¶æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03930v3">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Diffusion Transformer Autoregressive Modelingï¼ˆDiTARï¼‰æ–¹æ³•ï¼Œç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£è½¬æ¢å™¨ï¼Œç”¨äºåŸºäºè¿ç»­ä»¤ç‰Œçš„è‡ªåŠ¨å›å½’ç”Ÿæˆè¯­éŸ³è¡¨ç¤ºã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è‡ªåŠ¨å›å½’æ¨¡å‹å¯¹è¿ç»­ä»¤ç‰Œçš„æœ‰æ•ˆæ€§ï¼Œå¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚DiTARé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥è¿›è¡Œè¡¥ä¸ç”Ÿæˆï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹å¤„ç†èšåˆçš„è¡¥ä¸åµŒå…¥ï¼Œè€Œæ‰©æ•£è½¬æ¢å™¨åˆ™åŸºäºè¯­è¨€æ¨¡å‹çš„è¾“å‡ºç”Ÿæˆä¸‹ä¸€ä¸ªè¡¥ä¸ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å®šä¹‰æ¸©åº¦æ¥å¹³è¡¡å¤šæ ·æ€§å’Œç¡®å®šæ€§ï¼Œå³åœ¨åå‘æ‰©æ•£ODEè¿‡ç¨‹ä¸­å¼•å…¥å™ªå£°çš„æ—¶é—´ç‚¹ã€‚æ­¤å¤–ï¼ŒDiTARåœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä¸­å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ç¨³å¥æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè‡ªç„¶æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiTARæ–¹æ³•ç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£è½¬æ¢å™¨ï¼Œç”¨äºåŸºäºè¿ç»­ä»¤ç‰Œçš„è‡ªåŠ¨å›å½’ç”Ÿæˆè¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>DiTARé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹å¤„ç†è¡¥ä¸åµŒå…¥ï¼Œæ‰©æ•£è½¬æ¢å™¨åˆ™ç”Ÿæˆä¸‹ä¸€ä¸ªè¡¥ä¸ã€‚</li>
<li>é€šè¿‡å®šä¹‰æ¸©åº¦æ¥å¹³è¡¡æ¨ç†è¿‡ç¨‹ä¸­çš„å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚</li>
<li>DiTARåœ¨åå‘æ‰©æ•£ODEè¿‡ç¨‹ä¸­å¼•å…¥å™ªå£°çš„æ—¶é—´ç‚¹å®šä¹‰ä¸ºæ¸©åº¦ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å°è¯•ã€‚</li>
<li>DiTARå…·æœ‰å‡ºè‰²çš„å¯æ‰©å±•æ€§ï¼Œè¿™åœ¨å…¶å¹¿æ³›çš„è§„æ¨¡åˆ†æä¸­å¾—åˆ°è¯æ˜ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆæ–¹é¢ï¼ŒDiTARåœ¨ç¨³å¥æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè‡ªç„¶æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e3d0ad6896f09597a191813bcb58fd6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f924470bdfa0f5c21084830453ac89b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee180b77613f77e67b848abf6ac477b1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Permutation-Symmetry-of-Transformers-The-Role-of-Rotation-for-Model-Fusion"><a href="#Beyond-the-Permutation-Symmetry-of-Transformers-The-Role-of-Rotation-for-Model-Fusion" class="headerlink" title="Beyond the Permutation Symmetry of Transformers: The Role of Rotation   for Model Fusion"></a>Beyond the Permutation Symmetry of Transformers: The Role of Rotation   for Model Fusion</h2><p><strong>Authors:Binchi Zhang, Zaiyi Zheng, Zhengzhang Chen, Jundong Li</strong></p>
<p>Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on <a target="_blank" rel="noopener" href="https://github.com/zhengzaiyi/RotationSymmetry">https://github.com/zhengzaiyi/RotationSymmetry</a>. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰å‚æ•°ç©ºé—´ä¸­çš„å¯¹ç§°æ€§å·²ç»è¢«è¯æ˜å¯¹å„ç§æ·±åº¦å­¦ä¹ åº”ç”¨æœ‰ç›Šã€‚ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„ä¾‹å­æ˜¯å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰ä¸­çš„ç½®æ¢å¯¹ç§°æ€§ï¼Œå…¶ä¸­å¯¹æŸä¸€å±‚çš„æƒé‡çŸ©é˜µçš„è¡Œè¿›è¡Œç½®æ¢ï¼Œå¹¶å¯¹ç›¸é‚»å±‚åº”ç”¨åå‘ç½®æ¢ï¼Œå¯ä»¥å¾—åˆ°åŠŸèƒ½ç­‰æ•ˆçš„æ¨¡å‹ã€‚è™½ç„¶ç½®æ¢å¯¹ç§°æ€§å®Œå…¨è¡¨å¾äº†MLPsçš„ç­‰ä»·é›†ï¼Œä½†å…¶ç¦»æ•£æ€§è´¨é™åˆ¶äº†å…¶åœ¨å˜å‹å™¨æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—‹è½¬å¯¹ç§°æ€§ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å‚æ•°ç©ºé—´å¯¹ç§°æ€§ï¼Œé€šè¿‡æ—‹è½¬è‡ªæ³¨æ„åŠ›å±‚çš„å‚æ•°çŸ©é˜µæ¥æ¨å¹¿ç½®æ¢å¯¹ç§°æ€§ã€‚ä¸ç½®æ¢å¯¹ç§°æ€§ä¸åŒï¼Œæ—‹è½¬å¯¹ç§°æ“ä½œåœ¨ä¸€ä¸ªè¿ç»­åŸŸä¸­è¿›è¡Œï¼Œä»è€Œæ˜¾è‘—æ‰©å¤§äº†å˜å‹å™¨çš„ç­‰ä»·é›†ã€‚åŸºäºè¿™ä¸€å±æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç†è®ºä¸Šçš„æœ€ä¼˜å‚æ•°åŒ¹é…ç®—æ³•ï¼Œä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—æ¥å¢å¼ºæ¨¡å‹èåˆã€‚æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å˜å‹å™¨æ¨¡å‹åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†å’Œè§†è§‰ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºæ—‹è½¬å¯¹ç§°æ€§çš„åŒ¹é…ç®—æ³•æå¤§åœ°æé«˜äº†æ¨¡å‹èåˆçš„æ•ˆæœï¼Œå‡¸æ˜¾äº†å‚æ•°ç©ºé—´å¯¹ç§°æ€§åœ¨ä¿ƒè¿›æ¨¡å‹èåˆæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/zhengzaiyi/RotationSymmetry%E3%80%82">https://github.com/zhengzaiyi/RotationSymmetryã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00264v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong><br>æ·±å±‚ç¥ç»ç½‘ç»œå‚æ•°ç©ºé—´çš„å¯¹ç§°æ€§å¯¹äºæ·±åº¦å­¦ä¹ åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ¬æ–‡ä»‹ç»äº†æ—‹è½¬å¯¹ç§°æ€§è¿™ä¸€æ–°å‹å‚æ•°ç©ºé—´å¯¹ç§°æ€§å½¢å¼ï¼Œå°†å…¶åº”ç”¨äºå˜å‹å™¨ï¼Œå¹¶åŸºäºæ­¤æ€§è´¨æå‡ºäº†ä¸€ç§ç†è®ºä¸Šçš„æœ€ä½³å‚æ•°åŒ¹é…ç®—æ³•ï¼Œç”¨äºå¢å¼ºæ¨¡å‹èåˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•èƒ½æ˜¾è‘—æé«˜æ¨¡å‹èåˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹ç§°æ€§åœ¨æ·±å±‚ç¥ç»ç½‘ç»œå‚æ•°ç©ºé—´ä¸­å¯¹æ·±åº¦å­¦ä¹ åº”ç”¨æœ‰ç›Šã€‚</li>
<li>è®ºæ–‡æå‡ºäº†æ—‹è½¬å¯¹ç§°æ€§è¿™ä¸€æ–°å‹å‚æ•°ç©ºé—´å¯¹ç§°æ€§å½¢å¼ã€‚</li>
<li>æ—‹è½¬å¯¹ç§°æ€§èƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºå˜å‹å™¨ï¼Œæ˜¯å¯¹æ’åˆ—å¯¹ç§°æ€§çš„é‡è¦æ¨å¹¿ã€‚</li>
<li>åŸºäºæ—‹è½¬å¯¹ç§°æ€§ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ç†è®ºä¸Šçš„æœ€ä½³å‚æ•°åŒ¹é…ç®—æ³•ã€‚</li>
<li>è¯¥ç®—æ³•å¯ä½œä¸ºå³æ’å³ç”¨æ¨¡å—ï¼Œç”¨äºå¢å¼ºæ¨¡å‹èåˆèƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•èƒ½æ˜¾è‘—æé«˜æ¨¡å‹èåˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a13ca474c944056a7842d47a28493937.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b1e8daae8db8853b69e3aaeb130d6b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ff65f4fa09b4be5b34e280be45ee10c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d17d22e29f6bd90d35989f598131fcf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d71b01f790b7c2dd3e203529c8c4dab6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning"><a href="#QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning" class="headerlink" title="QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning"></a>QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning</h2><p><strong>Authors:Xinyang Tong, Pengxiang Ding, Yiguo Fan, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, Shangke Lyu</strong></p>
<p>This paper addresses the inherent inference latency challenges associated with deploying multimodal large language models (MLLM) in quadruped vision-language-action (QUAR-VLA) tasks. Our investigation reveals that conventional parameter reduction techniques ultimately impair the performance of the language foundation model during the action instruction tuning phase, making them unsuitable for this purpose. We introduce a novel latency-free quadruped MLLM model, dubbed QUART-Online, designed to enhance inference efficiency without degrading the performance of the language foundation model. By incorporating Action Chunk Discretization (ACD), we compress the original action representation space, mapping continuous action values onto a smaller set of discrete representative vectors while preserving critical information. Subsequently, we fine-tune the MLLM to integrate vision, language, and compressed actions into a unified semantic space. Experimental results demonstrate that QUART-Online operates in tandem with the existing MLLM system, achieving real-time inference in sync with the underlying controller frequency, significantly boosting the success rate across various tasks by 65%. Our project page is <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å››è¶³è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆQUAR-VLAï¼‰ä»»åŠ¡ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ‰€é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼Œä¼ ç»Ÿçš„å‚æ•°å‡å°‘æŠ€æœ¯æœ€ç»ˆä¼šæŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨è¡ŒåŠ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æ€§èƒ½ï¼Œä½¿å…¶ä¸é€‚åˆæ­¤ç›®çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹ï¼Œåä¸ºQUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸é™ä½è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥è¡ŒåŠ¨ç‰‡æ®µç¦»æ•£åŒ–ï¼ˆACDï¼‰ï¼Œæˆ‘ä»¬å‹ç¼©äº†åŸå§‹è¡ŒåŠ¨è¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­çš„è¡ŒåŠ¨å€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œä»¥å°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åçš„è¡ŒåŠ¨æ•´åˆåˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰çš„MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°ä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥çš„å®æ—¶æ¨ç†ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†65%ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://quart-online.github.io./">https://quart-online.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15576v5">PDF</a> Accepted to ICRA 2025; Github page: <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨åœ¨å››è¶³æœºå™¨äººè§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ—¶é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°åœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µï¼Œä¼ ç»Ÿå‚æ•°ç¼©å‡æŠ€æœ¯ä¼šæŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹æ€§èƒ½ï¼Œä¸é€‚åˆæ­¤åœºæ™¯ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹â€”â€”QUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡è€Œä¸æŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰ï¼Œå‹ç¼©åŸå§‹åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­åŠ¨ä½œå€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚ç„¶åå¾®è°ƒMLLMï¼Œå°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°äº†ä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥çš„å®æ—¶æ¨ç†ï¼Œåœ¨å„é¡¹ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†65%ã€‚é¡¹ç›®ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://quart-online.github.io/">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶è§£å†³äº†åœ¨å››è¶³æœºå™¨äººè§†è§‰è¯­è¨€åŠ¨ä½œä»»åŠ¡ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿçš„å‚æ•°ç¼©å‡æŠ€æœ¯åœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¯¹è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½æœ‰è´Ÿé¢å½±å“ã€‚</li>
<li>QUART-Onlineæ¨¡å‹æ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸æŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰ï¼Œå‹ç¼©åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œæ˜ å°„åˆ°ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šã€‚</li>
<li>QUART-Onlineé€šè¿‡å¾®è°ƒé›†æˆè§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œçš„MLLMï¼Œå½¢æˆç»Ÿä¸€è¯­ä¹‰ç©ºé—´ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒQUART-Onlineä¸ç°æœ‰ç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°å®æ—¶æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d55fbfd356475ab4d69e26b44e407f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d30d5b758629aee84aab79c5afdf7dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d516c516a55f5f18abacd645ad59b53b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b154fe6f439d338b4f41959c7d03f3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5ece0295520d590c06c0e7c317c48ae.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Core-Context-Aware-Transformers-for-Long-Context-Language-Modeling"><a href="#Core-Context-Aware-Transformers-for-Long-Context-Language-Modeling" class="headerlink" title="Core Context Aware Transformers for Long Context Language Modeling"></a>Core Context Aware Transformers for Long Context Language Modeling</h2><p><strong>Authors:Yaofo Chen, Zeng You, Shuhai Zhang, Haokun Li, Yirui Li, Yaowei Wang, Mingkui Tan</strong></p>
<p>Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤§é‡ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œè¿™ä¸»è¦å½’åŠŸäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶è¦æ±‚ä¸€ä¸ªæ ‡è®°ï¼ˆtokenï¼‰å°†æ‰€æœ‰å…ˆå‰çš„æ ‡è®°éƒ½ä½œä¸ºä¸Šä¸‹æ–‡æ¥è®¡ç®—æ³¨æ„åŠ›ã€‚ç„¶è€Œï¼Œå½“ä¸Šä¸‹æ–‡é•¿åº¦Lå˜å¾—éå¸¸å¤§ï¼ˆä¾‹å¦‚128Kï¼‰æ—¶ï¼Œä¸Šä¸‹æ–‡ä¸­æ½œåœ¨å†—ä½™ä¿¡æ¯çš„æ•°é‡å¾€å¾€ä¼šå¢åŠ ã€‚å†—ä½™çš„ä¸Šä¸‹æ–‡ä¸ä»…é˜»ç¢å»ºæ¨¡è¡¨ç¤ºæ€§èƒ½ï¼Œè¿˜ä¼šäº§ç”Ÿä¸å¿…è¦çš„è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„æ ¸å¿ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆCCAï¼‰æ³¨æ„åŠ›ï¼Œç”¨äºé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼š1ï¼‰å…¨å±€æ€§æ± åŒ–æ¨¡å—å¯¹è¾“å…¥æ ‡è®°è¿›è¡Œåˆ†ç»„ï¼Œå¹¶æ ¹æ®å®ƒä»¬çš„é‡è¦æ€§åŠ¨æ€åœ°å°†æ¯ä¸ªç»„å‹ç¼©æˆä¸€ä¸ªæ ¸å¿ƒæ ‡è®°ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨èšç„¦å’Œå¼ºåŒ–æ ¸å¿ƒä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å‡å°‘å†—ä½™ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„é•¿æœŸä¾èµ–å»ºæ¨¡ã€‚2ï¼‰å±€éƒ¨æ€§ä¿æŒæ¨¡å—ç»“åˆäº†ç›¸é‚»æ ‡è®°ï¼Œä»¥ä¿æŒå±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œä»¥å®ç°è¯¦ç»†çš„è¡¨ç¤ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„CCAæ³¨æ„åŠ›èƒ½å¤Ÿä»¥æœ€å°çš„å¾®è°ƒæˆæœ¬æ›¿æ¢ç°æœ‰LLMä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæœ€æ–°çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12465v2">PDF</a> Accepted for publication at ICML 2025</p>
<p><strong>Summary</strong>ï¼šåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å†—ä½™ä¿¡æ¯å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„æ ¸å¿ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆCCAï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬å…¨å±€æ„ŸçŸ¥æ± åŒ–æ¨¡å—å’Œå±€éƒ¨ä¿æŒæ¨¡å—ï¼Œæ—¨åœ¨é«˜æ•ˆå»ºæ¨¡é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é•¿æ–‡æœ¬å»ºæ¨¡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Transformer-based LLMsåœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬å†—ä½™ä¿¡æ¯å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æ ¸å¿ƒä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆCCAï¼‰æ³¨æ„åŠ›æœºåˆ¶åŒ…æ‹¬å…¨å±€æ„ŸçŸ¥æ± åŒ–æ¨¡å—å’Œå±€éƒ¨ä¿æŒæ¨¡å—ã€‚</li>
<li>å…¨å±€æ„ŸçŸ¥æ± åŒ–æ¨¡å—é€šè¿‡åŠ¨æ€å‹ç¼©è¾“å…¥æ ‡è®°æ¥å¼ºåŒ–æ ¸å¿ƒä¸Šä¸‹æ–‡ï¼Œå‡å°‘å†—ä½™ä¿¡æ¯ã€‚</li>
<li>å±€éƒ¨ä¿æŒæ¨¡å—ä¿ç•™é‚»è¿‘æ ‡è®°ä»¥è¿›è¡Œè¯¦ç»†çš„å±€éƒ¨ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚</li>
<li>CCA-Attentionèƒ½å¤Ÿæ›¿æ¢ç°æœ‰LLMä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œä¸”å¾®è°ƒæˆæœ¬ä½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é•¿æ–‡æœ¬å»ºæ¨¡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77ccd945462e4add5b164282cd7371b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40b602f47d28b43700efe26f86ca568c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87dad71f9789ae4f7588a8ec09e7663a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4f5884df1f8a0703532a02667888f00.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Visatronic-A-Multimodal-Decoder-Only-Model-for-Speech-Synthesis"><a href="#Visatronic-A-Multimodal-Decoder-Only-Model-for-Speech-Synthesis" class="headerlink" title="Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis"></a>Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis</h2><p><strong>Authors:Akshita Gupta, Tatiana Likhomanenko, Karren Dai Yang, Richard He Bai, Zakaria Aldeneh, Navdeep Jaitly</strong></p>
<p>The rapid progress of foundation models and large language models (LLMs) has fueled significantly improvement in the capabilities of machine learning systems that benefit from mutlimodal input data. However, existing multimodal models are predominantly built on top of pre-trained LLMs, which can limit accurate modeling of temporal dependencies across other modalities and thus limit the modelâ€™s ability to jointly process and leverage multimodal inputs. To specifically investigate the alignment of text, video, and speech modalities in LLM-style (decoder-only) models, we consider a simplified multimodal generation task, Video-Text to Speech (VTTS): speech generation conditioned on both its corresponding text and video of talking people. The ultimate goal is to generate speech that not only follows the text but also aligns temporally with the video and is consistent with the facial expressions. In this paper, we first introduce Visatronic, a unified multimodal decoder-only transformer model that adopts an LLM-style architecture to embed visual, textual, and speech inputs into a shared subspace, treating all modalities as temporally aligned token streams. Next, we carefully explore different token mixing strategies to understand the best way to propagate information from the steps where video and text conditioning is input to the steps where the audio is generated. We extensively evaluate Visatronic on the challenging VoxCeleb2 dataset and demonstrate zero-shot generalization to LRS3, where Visatronic, trained on VoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only on LRS3, which report a 21.4% WER. Additionally, we propose a new objective metric, TimeSync, specifically designed to measure phoneme-level temporal alignment between generated and reference speech, further ensuring synchronization quality. Demo: <a target="_blank" rel="noopener" href="https://apple.github.io/visatronic-demo/">https://apple.github.io/visatronic-demo/</a> </p>
<blockquote>
<p>éšç€åŸºç¡€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå—ç›Šäºæ­¤èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€è¾“å…¥æ•°æ®çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„èƒ½åŠ›å¾—åˆ°äº†æå¤§çš„æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹å¤§å¤šå»ºç«‹åœ¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åŸºç¡€ä¸Šï¼Œè¿™é™åˆ¶äº†å…¶ä»–æ¨¡æ€é—´æ—¶é—´ä¾èµ–æ€§çš„å‡†ç¡®å»ºæ¨¡ï¼Œä»è€Œä¹Ÿé™åˆ¶äº†æ¨¡å‹å¯¹å¤šæ¨¡æ€è¾“å…¥çš„è”åˆå¤„ç†å’Œåº”ç”¨èƒ½åŠ›ã€‚ä¸ºäº†ä¸“é—¨ç ”ç©¶æ–‡æœ¬ã€è§†é¢‘å’Œè¯­éŸ³æ¨¡æ€åœ¨LLMé£æ ¼ï¼ˆä»…è§£ç å™¨ï¼‰æ¨¡å‹ä¸­çš„å¯¹é½é—®é¢˜ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ä¸ªç®€åŒ–çš„å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡â€”â€”è§†é¢‘æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆVTTSï¼‰ï¼šè¯­éŸ³ç”Ÿæˆæ—¢ä¾èµ–äºç›¸åº”çš„æ–‡æœ¬ï¼Œåˆä¾èµ–äºäººä»¬çš„è°ˆè¯è§†é¢‘ã€‚æœ€ç»ˆç›®æ ‡æ˜¯ç”Ÿæˆä¸ä»…éµå¾ªæ–‡æœ¬å†…å®¹ï¼Œè€Œä¸”ä¸è§†é¢‘åœ¨æ—¶é—´ä¸Šæ˜ å°„å¯¹é½ã€ä¸é¢éƒ¨è¡¨æƒ…ä¸€è‡´çš„è¯­éŸ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†Visatronicï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ä»…è§£ç å™¨transformeræ¨¡å‹ï¼Œå®ƒé‡‡ç”¨LLMé£æ ¼çš„æ¶æ„å°†è§†è§‰ã€æ–‡æœ¬å’Œè¯­éŸ³è¾“å…¥åµŒå…¥åˆ°å…±äº«çš„å­ç©ºé—´ä¸­ï¼Œå°†æ‰€æœ‰æ¨¡æ€è§†ä¸ºæ—¶é—´å¯¹é½çš„ä»¤ç‰Œæµã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»”ç»†æ¢ç´¢äº†ä¸åŒçš„ä»¤ç‰Œæ··åˆç­–ç•¥ï¼Œä»¥äº†è§£ä»è§†é¢‘å’Œæ–‡æœ¬è°ƒèŠ‚æ­¥éª¤å‘ç”ŸæˆéŸ³é¢‘æ­¥éª¤ä¼ æ’­ä¿¡æ¯çš„æœ€ä½³æ–¹å¼ã€‚æˆ‘ä»¬å¯¹VoxCeleb2æ•°æ®é›†è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†Visatronicåœ¨LRS3ä¸Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚å…¶ä¸­ï¼ŒVisatronicåœ¨VoxCeleb2ä¸Šè®­ç»ƒåå®ç°äº†4.5%çš„WERï¼ˆè¯é”™è¯¯ç‡ï¼‰ï¼Œä¼˜äºä»…åœ¨LRS3ä¸Šè®­ç»ƒçš„å…ˆå‰æœ€ä½³æ–¹æ³•ï¼ˆæŠ¥å‘Šäº†21.4%çš„WERï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„å®¢è§‚åº¦é‡æ ‡å‡†TimeSyncï¼Œä¸“é—¨ç”¨äºæµ‹é‡ç”Ÿæˆè¯­éŸ³å’Œå‚è€ƒè¯­éŸ³ä¹‹é—´çš„éŸ³ç´ çº§æ—¶é—´å¯¹é½ï¼Œè¿›ä¸€æ­¥ç¡®ä¿åŒæ­¥è´¨é‡ã€‚æ¼”ç¤ºåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://apple.github.io/visatronic-demo/%E3%80%82">https://apple.github.io/visatronic-demo/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17690v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€åŸºç¡€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¤šæ¨¡æ€è¾“å…¥æ•°æ®åœ¨æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­çš„åº”ç”¨èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ¨¡å‹ä¸»è¦ä¾èµ–äºé¢„è®­ç»ƒçš„LLMï¼Œè¿™é™åˆ¶äº†è·¨å…¶ä»–æ¨¡æ€çš„æ—¶é—´ä¾èµ–æ€§çš„å‡†ç¡®å»ºæ¨¡ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹å¯¹å¤šæ¨¡æ€è¾“å…¥çš„è”åˆå¤„ç†ä¸åˆ©ç”¨èƒ½åŠ›ã€‚ä¸ºäº†ç ”ç©¶æ–‡æœ¬ã€è§†é¢‘å’Œè¯­éŸ³æ¨¡æ€åœ¨LLMé£æ ¼æ¨¡å‹ä¸­çš„å¯¹é½é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€åŒ–çš„å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡â€”â€”è§†é¢‘æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆVTTSï¼‰ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€è§£ç å™¨æ¨¡å‹Visatronicï¼Œè¯¥æ¨¡å‹é‡‡ç”¨LLMé£æ ¼æ¶æ„åµŒå…¥è§†è§‰ã€æ–‡æœ¬å’Œè¯­éŸ³è¾“å…¥åˆ°å…±äº«å­ç©ºé—´ï¼Œå¹¶å°†æ‰€æœ‰æ¨¡æ€è§†ä¸ºæ—¶é—´å¯¹é½çš„ä»¤ç‰Œæµã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VoxCeleb2æ•°æ®é›†ä¸Šè¯„ä¼°Visatronicï¼Œå…¶åœ¨è®­ç»ƒæ¡ä»¶ä¸‹å®ç°é›¶æ—¶å·®ã€è¶…ä¼˜è¡¨ç°ï¼Œå±•ç¤ºå‡ºè‰²çš„æ—¶é—´åŒæ­¥èƒ½åŠ›ã€‚åŒæ—¶æå‡ºä¸€ä¸ªæ–°çš„å®¢è§‚æŒ‡æ ‡TimeSyncï¼Œä¸“é—¨ç”¨äºæµ‹é‡ç”Ÿæˆè¯­éŸ³ä¸å‚è€ƒè¯­éŸ³ä¹‹é—´çš„éŸ³ç´ çº§æ—¶é—´å¯¹é½ç¨‹åº¦ï¼Œç¡®ä¿åŒæ­¥è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨¡å‹çš„è¿›æ­¥å—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§ç±»å‹çš„è¾“å…¥æ•°æ®ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶é¢ä¸´æ—¶é—´ä¾èµ–æ€§å»ºæ¨¡çš„æŒ‘æˆ˜ã€‚</li>
<li>è§†é¢‘æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆVTTSï¼‰ä»»åŠ¡æ—¨åœ¨ç ”ç©¶æ–‡æœ¬ã€è§†é¢‘å’Œè¯­éŸ³æ¨¡æ€çš„å¯¹é½é—®é¢˜ã€‚</li>
<li>Visatronicæ¨¡å‹é‡‡ç”¨LLMé£æ ¼æ¶æ„å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œå°†å…¶åµŒå…¥åˆ°å…±äº«å­ç©ºé—´ã€‚</li>
<li>Visatronicåœ¨VoxCeleb2æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé€šè¿‡é›¶æ—¶å·®æ³›åŒ–å®ç°ä¼˜è¶Šçš„æ—¶é—´åŒæ­¥èƒ½åŠ›ã€‚</li>
<li>æå‡ºæ–°çš„å®¢è§‚æŒ‡æ ‡TimeSyncï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆè¯­éŸ³ä¸å‚è€ƒè¯­éŸ³ä¹‹é—´çš„éŸ³ç´ çº§æ—¶é—´å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45a6c6dfa6b81fd005b779083e2a519a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbe22e9665ec8b0309dfaa8a708b8550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02c056bd02f973ea4e3070664b7e8576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a90e2243467d0f786c7463ce9e7a5d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acef3515d5a164a7d5b20ebe94e1f514.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4f4fb89cfe947b46d5aa0b772b4f3ea.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DRPruning-Efficient-Large-Language-Model-Pruning-through-Distributionally-Robust-Optimization"><a href="#DRPruning-Efficient-Large-Language-Model-Pruning-through-Distributionally-Robust-Optimization" class="headerlink" title="DRPruning: Efficient Large Language Model Pruning through   Distributionally Robust Optimization"></a>DRPruning: Efficient Large Language Model Pruning through   Distributionally Robust Optimization</h2><p><strong>Authors:Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jing Li, Min Zhang, Zhaopeng Tu</strong></p>
<p>Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains, leading to biased performance. To address this, we propose DRPruning, a method that dynamically adjusts the data distribution during training to restore balanced performance across heterogeneous and multi-tasking data. Experiments in monolingual and multilingual settings show that DRPruning surpasses similarly sized models in both pruning and continued pretraining over perplexity, downstream tasks, and instruction tuning. Further analysis demonstrates the robustness of DRPruning towards various domains and distribution shifts. Furthermore, DRPruning can determine optimal reference losses and data ratios automatically, suggesting potential for broader applications. Code and scripts are available at <a target="_blank" rel="noopener" href="https://github.com/hexuandeng/DRPruning">https://github.com/hexuandeng/DRPruning</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†é¢ä¸´ç€æ¨¡å‹å°ºå¯¸å¢å¤§å’Œè®¡ç®—æˆæœ¬å¢åŠ çš„æŒ‘æˆ˜ã€‚ç»“æ„åŒ–å‰ªæå¯ä»¥å‡å°‘æ¨¡å‹å¤§å°å¹¶åŠ é€Ÿæ¨ç†ï¼Œä½†å¾€å¾€ä¼šå¯¼è‡´ä¸åŒé¢†åŸŸçš„æ€§èƒ½ä¸å‡åŒ€ä¸‹é™ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½åè§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DRPruningæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ•°æ®åˆ†å¸ƒï¼Œä»¥æ¢å¤å¼‚æ„å¤šä»»åŠ¡æ•°æ®çš„å¹³è¡¡æ€§èƒ½ã€‚åœ¨å•è¯­å’Œå¤šè¯­ç¯å¢ƒä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒDRPruningåœ¨å‰ªæå’ŒæŒç»­é¢„è®­ç»ƒæ–¹é¢çš„å›°æƒ‘åº¦ã€ä¸‹æ¸¸ä»»åŠ¡å’ŒæŒ‡ä»¤è°ƒæ•´æ–¹é¢éƒ½è¶…è¶Šäº†ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒDRPruningå¯¹ä¸åŒé¢†åŸŸå’Œåˆ†å¸ƒå˜åŒ–å…·æœ‰ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒDRPruningè¿˜å¯ä»¥è‡ªåŠ¨ç¡®å®šæœ€ä½³å‚è€ƒæŸå¤±å’Œæ•°æ®æ¯”ç‡ï¼Œè¡¨æ˜å…¶æ›´å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚ç›¸å…³ä»£ç å’Œè„šæœ¬å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/hexuandeng/DRPruning%E3%80%82">https://github.com/hexuandeng/DRPruningã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14055v2">PDF</a> Accepted by ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>LLMé¢ä¸´æ¨¡å‹å°ºå¯¸å¢å¤§ä¸è®¡ç®—æˆæœ¬ä¸Šå‡çš„æŒ‘æˆ˜ã€‚ç»“æ„åŒ–å‰ªæè™½å¯å‡å°‘æ¨¡å‹å°ºå¯¸å¹¶åŠ é€Ÿæ¨ç†ï¼Œä½†å¸¸å¯¼è‡´è·¨é¢†åŸŸæ€§èƒ½ä¸å‡è¡¡çš„ä¸‹é™ã€‚æœ¬ç ”ç©¶æå‡ºDRPruningæ–¹æ³•ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ•°æ®åˆ†å¸ƒï¼Œä»¥æ¢å¤åœ¨å¼‚è´¨å¤šä»»åŠ¡æ•°æ®ä¸Šçš„å¹³è¡¡æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒDRPruningåœ¨å‰ªæå’ŒæŒç»­é¢„è®­ç»ƒæ–¹é¢çš„è¡¨ç°å‡è¶…è¶ŠåŒç±»æ¨¡å‹ï¼Œæé«˜äº†å›°æƒ‘åº¦ã€ä¸‹æ¸¸ä»»åŠ¡ä¸æŒ‡ä»¤å¾®è°ƒçš„æ•ˆæœã€‚æ­¤å¤–ï¼ŒDRPruningå¯è‡ªåŠ¨ç¡®å®šæœ€ä½³å‚è€ƒæŸå¤±å’Œæ•°æ®æ¯”ç‡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚ç›¸å…³ä»£ç å’Œè„šæœ¬å¯åœ¨hexuandeng&#x2F;DRPruningè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé¢ä¸´æ¨¡å‹å°ºå¯¸å¢å¤§å’Œè®¡ç®—æˆæœ¬ä¸Šå‡çš„éš¾é¢˜ã€‚</li>
<li>ç»“æ„åŒ–å‰ªæå¯å‡å°‘æ¨¡å‹å°ºå¯¸å¹¶åŠ é€Ÿæ¨ç†ï¼Œä½†å¯èƒ½å¯¼è‡´è·¨é¢†åŸŸæ€§èƒ½ä¸å‡è¡¡ã€‚</li>
<li>DRPruningæ–¹æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®åˆ†å¸ƒæ¥æ¢å¤è·¨é¢†åŸŸçš„å¹³è¡¡æ€§èƒ½ã€‚</li>
<li>DRPruningåœ¨å‰ªæå’ŒæŒç»­é¢„è®­ç»ƒæ–¹é¢çš„è¡¨ç°è¶…è¶ŠåŒç±»æ¨¡å‹ã€‚</li>
<li>DRPruningå¯æé«˜å›°æƒ‘åº¦ã€ä¸‹æ¸¸ä»»åŠ¡ä¸æŒ‡ä»¤å¾®è°ƒçš„æ•ˆæœã€‚</li>
<li>DRPruningå¯è‡ªåŠ¨ç¡®å®šæœ€ä½³å‚è€ƒæŸå¤±å’Œæ•°æ®æ¯”ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ace7566d64b3e152a1278dd0f02667a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86b4f90fe9360074fdadceea2690d027.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61ba9a81870a6d2f9b2a8dad66fc4463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f34313922f98ded87a7857e75908c02d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e021345d94d4ed5095bbb9cce3aba6fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f49af4c605e92036d35c924eb40c818a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee0e2bcfb4da3538951888acfb18665f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fd0efd456211b3e4cbed584818f73e4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Controllable-Context-Sensitivity-and-the-Knob-Behind-It"><a href="#Controllable-Context-Sensitivity-and-the-Knob-Behind-It" class="headerlink" title="Controllable Context Sensitivity and the Knob Behind It"></a>Controllable Context Sensitivity and the Knob Behind It</h2><p><strong>Authors:Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell</strong></p>
<p>When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context (Paris is in England) and a question (Where is Paris?); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either France or England). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a modelâ€™s performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior. </p>
<blockquote>
<p>åœ¨åšå‡ºé¢„æµ‹æ—¶ï¼Œè¯­è¨€æ¨¡å‹å¿…é¡»åœ¨ä¾èµ–ä¸Šä¸‹æ–‡å’Œå…ˆå‰çŸ¥è¯†ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚é€‰æ‹©æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„æ•æ„Ÿç¨‹åº¦æ˜¯ä¸€ä¸ªåŸºæœ¬åŠŸèƒ½ï¼Œå› ä¸ºè¿™ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨è¯¸å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œé—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯»æ‰¾ä¸€ä¸ªæ§åˆ¶è¿™ç§æ•æ„Ÿæ€§çš„æ—‹é’®ï¼Œä»¥ç¡®å®šè¯­è¨€æ¨¡å‹æ˜¯ä»ä¸Šä¸‹æ–‡è¿˜æ˜¯å…ˆå‰çŸ¥è¯†ä¸­å¾—å‡ºç­”æ¡ˆã€‚ä¸ºäº†æŒ‡å¯¼è¿™æ¬¡æœç´¢ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯æ§ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦çš„ä»»åŠ¡ã€‚åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå‘æ¨¡å‹æä¾›ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚â€œå·´é»åœ¨è‹±å›½â€ï¼‰å’Œé—®é¢˜ï¼ˆâ€œå·´é»åœ¨å“ªé‡Œï¼Ÿâ€ï¼‰ï¼›ç„¶åæŒ‡ç¤ºæ¨¡å‹ä½¿ç”¨å…¶å…ˆå‰çŸ¥è¯†æˆ–ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œå¹¶è¯„ä¼°å®ƒæ˜¯å¦èƒ½ä¸ºä¸¤ç§æ„å›¾ï¼ˆæ³•å›½æˆ–è‹±å›½ï¼‰ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆã€‚åœ¨æ­¤ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒåï¼ŒLlama-3.1ã€Mistral-v0.3å’ŒGemma-2çš„æŒ‡ä»¤è°ƒæ•´ç‰ˆæœ¬å¯ä»¥é«˜å‡†ç¡®ç‡ï¼ˆ85-95%ï¼‰åœ°è§£å†³æ­¤é—®é¢˜ã€‚åˆ†æè¿™äº›é«˜æ€§èƒ½æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§æ–°å‹çº¿æ€§æ—¶é—´ç®—æ³•æ¥ç¼©å°å¯¹ä¸Šä¸‹æ–‡æ•æ„Ÿåº¦é‡è¦çš„å±‚çº§ã€‚ç„¶åï¼Œåœ¨æ¯ä¸ªæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬åœ¨å•å±‚ä¸­è¯†åˆ«å‡ºä¸€ä¸ªä¸€ç»´å­ç©ºé—´ï¼Œè¯¥å­ç©ºé—´èƒ½å¤Ÿç¼–ç æ¨¡å‹æ˜¯éµå¾ªä¸Šä¸‹æ–‡è¿˜æ˜¯å…ˆå‰çŸ¥è¯†ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè™½ç„¶æˆ‘ä»¬åœ¨å·²å¾®è°ƒè¿‡çš„æ¨¡å‹ä¸­è¯†åˆ«äº†è¿™ä¸ªå­ç©ºé—´ï¼Œä½†æˆ‘ä»¬å‘ç°è¯¥å­ç©ºé—´ä¸ä»…åœ¨é‚£ä¸€æ¨¡å‹ä¸­æœ‰æ•ˆï¼Œè€Œä¸”åœ¨é‚£ä¸ªæ¨¡å‹å®¶æ—çš„éæŒ‡ä»¤å’ŒåŸºçº¿æ¨¡å‹ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°åœ¨è¯¥å­ç©ºé—´ä¸­ï¼Œæ¨¡å‹çš„è¡¨ç°ä¸å…¶åŒºåˆ†ä¸Šä¸‹æ–‡åŒæ„ç­”æ¡ˆå’Œå¿½ç•¥ä¸Šä¸‹æ–‡ç­”æ¡ˆçš„èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå•ä¸ªå­ç©ºé—´ä¿ƒè¿›äº†æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å’Œå…ˆå‰çŸ¥è¯†ä¹‹é—´çš„é€‰æ‹©ï¼Œæš—ç¤ºäº†ä¸€ä¸ªæ§åˆ¶æ­¤è¡Œä¸ºçš„ç®€å•åŸºæœ¬æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07404v3">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†è¯­è¨€æ¨¡å‹ä¸­æ§åˆ¶è¯­å¢ƒæ•æ„Ÿåº¦çš„æœºåˆ¶ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªä»»åŠ¡æ¥æŒ‡å¯¼å¯»æ‰¾æ§åˆ¶è¯­å¢ƒæ•æ„Ÿåº¦çš„æ—‹é’®ï¼Œå¹¶å‘ç°æŸäº›æ¨¡å‹å¯ä»¥åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚é€šè¿‡åœ¨è¿™äº›æ¨¡å‹ä¸­è¯†åˆ«å‡ºç‰¹å®šçš„å•å±‚ä¸­çš„ä¸€ç»´å­ç©ºé—´ï¼Œè¯¥å­ç©ºé—´èƒ½å†³å®šæ¨¡å‹æ˜¯éµå¾ªè¯­å¢ƒè¿˜æ˜¯å…ˆéªŒçŸ¥è¯†ã€‚è¿™ç§å­ç©ºé—´åœ¨ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹å’Œéç²¾ç»†è°ƒæ•´æ¨¡å‹ä¸­å‡æœ‰æ•ˆï¼Œæš—ç¤ºäº†æ¨¡å‹åœ¨äºŒè€…ä¹‹é—´çš„é€‰æ‹©æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¸å…¶åœ¨è¿™ä¸ªå­ç©ºé—´ä¸­åŒºåˆ†éµå¾ªè¯­å¢ƒå’Œå¿½ç•¥è¯­å¢ƒç­”æ¡ˆçš„èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>è¯­è¨€æ¨¡å‹éœ€è¦åœ¨ä¸Šä¸‹æ–‡å’Œå…ˆéªŒçŸ¥è¯†ä¹‹é—´åšå‡ºæƒè¡¡ï¼Œé€‰æ‹©å¯¹ä¸Šä¸‹æ–‡çš„æ•æ„Ÿåº¦æ˜¯ä¸€ä¸ªåŸºæœ¬åŠŸèƒ½ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªä»»åŠ¡æ¥å¯»æ‰¾æ§åˆ¶è¯­å¢ƒæ•æ„Ÿåº¦çš„æ—‹é’®ï¼Œè¯¥ä»»åŠ¡è¦æ±‚æ¨¡å‹åœ¨ç»™å®šé”™è¯¯è¯­å¢ƒçš„æƒ…å†µä¸‹æ­£ç¡®å›ç­”é—®é¢˜ã€‚</li>
<li>å‘ç°æŸäº›è¯­è¨€æ¨¡å‹åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ï¼Œå¹¶èƒ½é€šè¿‡åˆ†ææ¨¡å‹ä¸­çš„ç‰¹å®šå±‚æ¥è¯†åˆ«å…³é”®å­ç©ºé—´ã€‚</li>
<li>è¿™ä¸ªå­ç©ºé—´å†³å®šäº†æ¨¡å‹æ˜¯éµå¾ªä¸Šä¸‹æ–‡è¿˜æ˜¯å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åœ¨ä¸åŒçš„æ¨¡å‹ä¸­å‡æœ‰æ•ˆã€‚</li>
<li>æ¨¡å‹æ€§èƒ½ä¸å…¶åŒºåˆ†éµå¾ªè¯­å¢ƒå’Œå¿½ç•¥è¯­å¢ƒç­”æ¡ˆçš„èƒ½åŠ›ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†è¯­è¨€æ¨¡å‹åœ¨æƒè¡¡ä¸Šä¸‹æ–‡å’Œå…ˆéªŒçŸ¥è¯†æ—¶çš„åŸºæœ¬æœºåˆ¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-043096a901319531036443f9bafb94b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2687419b9cb79a95096dc67514c2eda3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SLiM-One-shot-Quantization-and-Sparsity-with-Low-rank-Approximation-for-LLM-Weight-Compression"><a href="#SLiM-One-shot-Quantization-and-Sparsity-with-Low-rank-Approximation-for-LLM-Weight-Compression" class="headerlink" title="SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for   LLM Weight Compression"></a>SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for   LLM Weight Compression</h2><p><strong>Authors:Mohammad Mozaffari, Amir Yazdanbakhsh, Maryam Mehri Dehnavi</strong></p>
<p>Conventional model compression techniques for LLMs address high memory consumption and slow inference challenges but typically require computationally expensive retraining to preserve accuracy. In contrast, one-shot compression methods eliminate retraining cost, but struggle to achieve accuracy comparable to dense models. This paper presents SLIM, a new one-shot compression framework that holistically integrates hardware-friendly quantization, sparsity, and low-rank approximation into a unified process. First, we formulate the quantization process using a probabilistic approach (SLIM-Quant) that enables us to apply uniform quantization. Then, we use an existing one-shot pruning method to apply semi-structured sparsity on top of the quantized weights. Finally, to compensate for the introduced aggregated quantization and sparsity error, we use a novel saliency function with unique invertible and additive features that enables us to mathematically compute the value of low-rank adapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4 sparsity with 4-bit weight quantization, outperforming prior methods. Models compressed with SLIM achieve up to 4.3x and 3.8x on Nvidia RTX3060 and A100 GPUs, respectively. Additionally, they achieve up to 0.23x end-to-end memory reduction in comparison to their dense counterparts. We also propose an optional PEFT recipe that further improves accuracy by up to 1.66% (LLaMA-2-13B) compared to SLIM without fine-tuning. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨¡å‹å‹ç¼©æŠ€æœ¯è§£å†³äº†é«˜å†…å­˜æ¶ˆè€—å’Œæ¨ç†é€Ÿåº¦æ…¢çš„æŒ‘æˆ˜ï¼Œä½†é€šå¸¸éœ€è¦è®¡ç®—æ˜‚è´µçš„é‡æ–°è®­ç»ƒæ¥ä¿æŒå‡†ç¡®æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸€æ¬¡æ€§å‹ç¼©æ–¹æ³•æ¶ˆé™¤äº†é‡æ–°è®­ç»ƒçš„æˆæœ¬ï¼Œä½†éš¾ä»¥å®ç°ä¸å¯†é›†æ¨¡å‹ç›¸å½“çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºäº†SLIMï¼Œä¸€ç§æ–°çš„ä¸€æ¬¡æ€§å‹ç¼©æ¡†æ¶ï¼Œå®ƒå…¨é¢åœ°å°†ç¡¬ä»¶å‹å¥½çš„é‡åŒ–ã€ç¨€ç–æ€§å’Œä½ç§©é€¼è¿‘æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„è¿‡ç¨‹ä¸­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æ¦‚ç‡æ–¹æ³•ï¼ˆSLIM-Quantï¼‰æ¥åˆ¶å®šé‡åŒ–è¿‡ç¨‹ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåº”ç”¨å‡åŒ€é‡åŒ–ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨é‡åŒ–æƒé‡ä¹‹ä¸Šä½¿ç”¨ç°æœ‰çš„ä¸€æ¬¡æ€§ä¿®å‰ªæ–¹æ³•ï¼Œåº”ç”¨åŠç»“æ„åŒ–ç¨€ç–æ€§ã€‚æœ€åï¼Œä¸ºäº†å¼¥è¡¥å¼•å…¥çš„èšåˆé‡åŒ–å’Œç¨€ç–æ€§è¯¯å·®ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§å…·æœ‰ç‹¬ç‰¹å¯é€†å’Œå¯åŠ ç‰¹æ€§çš„æ–°å‹æ˜¾è‘—æ€§å‡½æ•°ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿæ•°å­¦åœ°è®¡ç®—ä½ç§©é€‚é…å™¨çš„ä»·å€¼ã€‚SLIMåœ¨2:4çš„ç¨€ç–æ€§ä¸‹ï¼Œé€šè¿‡4ä½æƒé‡é‡åŒ–ï¼Œæé«˜äº†é«˜è¾¾5.66%ï¼ˆLLaMA-2-7Bï¼‰çš„æ¨¡å‹ç²¾åº¦ï¼Œä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚ä½¿ç”¨SLIMå‹ç¼©çš„æ¨¡å‹åœ¨Nvidia RTX3060å’ŒA100 GPUä¸Šåˆ†åˆ«å®ç°äº†é«˜è¾¾4.3å€å’Œ3.8å€çš„åŠ é€Ÿã€‚æ­¤å¤–ï¼Œä¸å®ƒä»¬çš„å¯†é›†å¯¹åº”æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒä»¬è¿˜å®ç°äº†é«˜è¾¾0.23å€çš„ç«¯åˆ°ç«¯å†…å­˜å‡å°‘ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¯é€‰çš„PEFTé…æ–¹ï¼Œä¸æ²¡æœ‰å¾®è°ƒè¿‡çš„SLIMç›¸æ¯”ï¼Œå®ƒè¿›ä¸€æ­¥æé«˜äº†é«˜è¾¾1.66%ï¼ˆLLaMA-2-13Bï¼‰çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09615v3">PDF</a> Published at Proceedings of the 42 nd International Conference on   Machine Learning (ICML 2025)</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSLIMçš„æ–°çš„ä¸€ç«™å¼å‹ç¼©æ¡†æ¶ï¼Œé›†æˆäº†ç¡¬ä»¶å‹å¥½çš„é‡åŒ–ã€ç¨€ç–æ€§å’Œä½ç§©é€¼è¿‘ã€‚é€šè¿‡æ¦‚ç‡æ–¹æ³•å®ç°å‡åŒ€é‡åŒ–ï¼Œåˆ©ç”¨ç°æœ‰çš„ä¸€ç«™å¼ä¿®å‰ªæ–¹æ³•å®ç°åŠç»“æ„åŒ–ç¨€ç–æ€§ï¼Œå¹¶ä½¿ç”¨æ–°çš„æ˜¾è‘—æ€§å‡½æ•°è®¡ç®—ä½ç§©é€‚é…å™¨çš„å€¼ä»¥è¡¥å¿é‡åŒ–ä¸ç¨€ç–æ€§è¯¯å·®ã€‚SLIMåœ¨ä¿æŒæ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®ç°äº†LLMçš„é«˜å‹ç¼©ç‡ï¼Œå¹¶åœ¨Nvidia RTX3060å’ŒA100 GPUsä¸Šå®ç°äº†é«˜åŠ é€Ÿæ¯”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLIMæ¡†æ¶å®ç°äº†LLMçš„ä¸€ç«™å¼å‹ç¼©ï¼Œé›†æˆäº†ç¡¬ä»¶å‹å¥½çš„é‡åŒ–ã€ç¨€ç–æ€§å’Œä½ç§©é€¼è¿‘ã€‚</li>
<li>é€šè¿‡æ¦‚ç‡æ–¹æ³•å®ç°å‡åŒ€é‡åŒ–ï¼Œé™ä½æ¨¡å‹å†…å­˜æ¶ˆè€—ã€‚</li>
<li>åˆ©ç”¨ç°æœ‰çš„ä¸€ç«™å¼ä¿®å‰ªæ–¹æ³•å®ç°åŠç»“æ„åŒ–ç¨€ç–æ€§ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹å‹ç¼©æ•ˆç‡ã€‚</li>
<li>é€šè¿‡æ˜¾è‘—æ€§å‡½æ•°è®¡ç®—ä½ç§©é€‚é…å™¨çš„å€¼ï¼Œä»¥è¡¥å¿é‡åŒ–ä¸ç¨€ç–æ€§è¯¯å·®ï¼Œä¿æŒæ¨¡å‹å‡†ç¡®æ€§ã€‚</li>
<li>SLIMæé«˜äº†æ¨¡å‹å‡†ç¡®æ€§ï¼Œå¹¶åœ¨ç‰¹å®šé…ç½®ä¸‹å®ç°äº†é«˜è¾¾5.66%çš„å‡†ç¡®åº¦æå‡ã€‚</li>
<li>å‹ç¼©åçš„æ¨¡å‹åœ¨Nvidia RTX3060å’ŒA100 GPUsä¸Šå®ç°äº†é«˜è¾¾4.3xå’Œ3.8xçš„åŠ é€Ÿæ¯”ã€‚</li>
<li>ç›¸æ¯”å¯†é›†æ¨¡å‹ï¼Œå‹ç¼©åçš„æ¨¡å‹å®ç°äº†é«˜è¾¾0.23xçš„ç«¯åˆ°ç«¯å†…å­˜å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d51f6d8ca544ef8f598e7918b0d5a55e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11fcffca373c3d0bd5671b894ab94265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a0e2f3d41f3dba5c7908bafe7bff971.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4f58b4f70a203b21f1653c383eae84b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Structure-Enhanced-Protein-Instruction-Tuning-Towards-General-Purpose-Protein-Understanding-with-LLMs"><a href="#Structure-Enhanced-Protein-Instruction-Tuning-Towards-General-Purpose-Protein-Understanding-with-LLMs" class="headerlink" title="Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose   Protein Understanding with LLMs"></a>Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose   Protein Understanding with LLMs</h2><p><strong>Authors:Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang</strong></p>
<p>Proteins, as essential biomolecules, play a central role in biological processes, including metabolic reactions and DNA replication. Accurate prediction of their properties and functions is crucial in biological applications. Recent development of protein language models (pLMs) with supervised fine tuning provides a promising solution to this problem. However, the fine-tuned model is tailored for particular downstream prediction task, and achieving general-purpose protein understanding remains a challenge. In this paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT) framework to bridge this gap. Our approach incorporates a novel structure-aware module into pLMs to enrich their structural knowledge, and subsequently integrates these enhanced pLMs with large language models (LLMs) to advance protein understanding. In this framework, we propose a novel instruction tuning pipeline. First, we warm up the enhanced pLMs using contrastive learning and structure denoising. Then, caption-based instructions are used to establish a basic understanding of proteins. Finally, we refine this understanding by employing a mixture of experts (MoEs) to capture more complex properties and functional information with the same number of activated parameters. Moreover, we construct the largest and most comprehensive protein instruction dataset to date, which allows us to train and evaluate the general-purpose protein understanding model. Extensive experiments on both open-ended generation and closed-set answer tasks demonstrate the superior performance of SEPIT over both closed-source general LLMs and open-source LLMs trained with protein knowledge. </p>
<blockquote>
<p>è›‹ç™½è´¨ä½œä¸ºé‡è¦çš„ç”Ÿç‰©åˆ†å­ï¼Œåœ¨åŒ…æ‹¬ä»£è°¢ååº”å’ŒDNAå¤åˆ¶ç­‰ç”Ÿç‰©è¿‡ç¨‹ä¸­æ‰®æ¼”ç€æ ¸å¿ƒè§’è‰²ã€‚å¯¹å…¶å±æ€§å’ŒåŠŸèƒ½çš„å‡†ç¡®é¢„æµ‹åœ¨ç”Ÿç‰©åº”ç”¨ä¸­æ˜¯è‡³å…³é‡è¦çš„ã€‚æœ€è¿‘é€šè¿‡ç›‘ç£å¾®è°ƒå¼€å‘çš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¾®è°ƒæ¨¡å‹æ˜¯é’ˆå¯¹ç‰¹å®šçš„ä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡å®šåˆ¶çš„ï¼Œå®ç°é€šç”¨è›‹ç™½è´¨ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç»“æ„å¢å¼ºè›‹ç™½è´¨æŒ‡ä»¤å¾®è°ƒï¼ˆSEPITï¼‰æ¡†æ¶æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ä¸€ä¸ªæ–°çš„ç»“æ„æ„ŸçŸ¥æ¨¡å—çº³å…¥pLMsä¸­ï¼Œä»¥ä¸°å¯Œå…¶ç»“æ„çŸ¥è¯†ï¼Œéšåå°†è¿™äº›å¢å¼ºçš„pLMsä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é›†æˆï¼Œä»¥æ¨è¿›è›‹ç™½è´¨ç†è§£ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡ä»¤å¾®è°ƒæµç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å’Œç»“æ„å»å™ªæ¥é¢„çƒ­å¢å¼ºçš„pLMsã€‚ç„¶åï¼ŒåŸºäºæè¿°çš„æŒ‡ä»¤ç”¨äºå»ºç«‹å¯¹è›‹ç™½è´¨çš„åŸºæœ¬ç†è§£ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ··åˆä¸“å®¶ç³»ç»Ÿï¼ˆMoEsï¼‰æ¥æ•æ‰æ›´å¤šå¤æ‚çš„å±æ€§å’ŒåŠŸèƒ½ä¿¡æ¯ï¼Œä»¥å®Œå–„è¿™ç§ç†è§£ï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„æ¿€æ´»å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§ã€æœ€å…¨é¢çš„è›‹ç™½è´¨æŒ‡ä»¤æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°é€šç”¨è›‹ç™½è´¨ç†è§£æ¨¡å‹ã€‚åœ¨å¼€æ”¾ç”Ÿæˆä»»åŠ¡å’Œå°é—­ç­”æ¡ˆä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSEPITåœ¨å°é—­æºé€šç”¨LLMså’Œç”¨è›‹ç™½è´¨çŸ¥è¯†è®­ç»ƒçš„å¼€æºLLMsä¸Šçš„è¡¨ç°å‡ä¼˜äºå‰è€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03553v3">PDF</a> Accepted by KDD2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Structure-Enhanced Protein Instruction Tuningï¼ˆSEPITï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å¼¥è¡¥è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰åœ¨é€šç”¨è›‹ç™½è´¨ç†è§£æ–¹é¢çš„ä¸è¶³ã€‚SEPITæ¡†æ¶ç»“åˆç»“æ„æ„ŸçŸ¥æ¨¡å—æ¥å¢å¼ºpLMsçš„ç»“æ„çŸ¥è¯†ï¼Œå¹¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é›†æˆï¼Œä»¥ä¿ƒè¿›è›‹ç™½è´¨ç†è§£ã€‚é€šè¿‡å¯¹æ¯”å®éªŒï¼Œè¯æ˜SEPITæ¡†æ¶åœ¨å¼€æ”¾å’Œå°é—­ç­”æ¡ˆä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºå°é—­å’Œå¼€æºçš„LLMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è›‹ç™½è´¨åœ¨ç”Ÿç‰©è¿‡ç¨‹ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼ŒåŒ…æ‹¬ä»£è°¢ååº”å’ŒDNAå¤åˆ¶ã€‚å¯¹å…¶å±æ€§å’ŒåŠŸèƒ½çš„å‡†ç¡®é¢„æµ‹åœ¨ç”Ÿç‰©åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰çš„è¿‘æœŸå‘å±•ä¸ºé¢„æµ‹è›‹ç™½è´¨å±æ€§æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ç°é€šç”¨è›‹ç™½è´¨ç†è§£ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>SEPITæ¡†æ¶ç»“åˆäº†ç»“æ„æ„ŸçŸ¥æ¨¡å—æ¥å¢å¼ºpLMsçš„ç»“æ„çŸ¥è¯†ï¼Œå¹¶æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œæ—¨åœ¨æé«˜è›‹ç™½è´¨ç†è§£ã€‚</li>
<li>SEPITé‡‡ç”¨ä¸€ç§æ–°çš„æŒ‡ä»¤è°ƒæ•´ç®¡é“ï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ å’Œç»“æ„å»å™ªæ¥é¢„çƒ­å¢å¼ºpLMsï¼Œå¹¶é€šè¿‡åŸºäºå­—å¹•çš„æŒ‡ä»¤å»ºç«‹åŸºæœ¬çš„è›‹ç™½è´¨ç†è§£ã€‚</li>
<li>MoEsï¼ˆæ··åˆä¸“å®¶ç³»ç»Ÿï¼‰ç”¨äºæ•æ‰æ›´å¤æ‚çš„å±æ€§å’ŒåŠŸèƒ½ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒæ¿€æ´»å‚æ•°æ•°é‡ä¸å˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªè¿„ä»Šä¸ºæ­¢æœ€å¤§å’Œæœ€å…¨é¢çš„è›‹ç™½è´¨æŒ‡ä»¤æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°é€šç”¨è›‹ç™½è´¨ç†è§£æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cca895d4f0df71d1831f73301c213fbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e95351837838a2da24a102c320dcd4e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54460238602334516742b5d6fc815a1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-025f1207f5188abab195a429f73d88ac.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Training-Nonlinear-Transformers-for-Chain-of-Thought-Inference-A-Theoretical-Generalization-Analysis"><a href="#Training-Nonlinear-Transformers-for-Chain-of-Thought-Inference-A-Theoretical-Generalization-Analysis" class="headerlink" title="Training Nonlinear Transformers for Chain-of-Thought Inference: A   Theoretical Generalization Analysis"></a>Training Nonlinear Transformers for Chain-of-Thought Inference: A   Theoretical Generalization Analysis</h2><p><strong>Authors:Hongkang Li, Songtao Lu, Pin-Yu Chen, Xiaodong Cui, Meng Wang</strong></p>
<p>Chain-of-Thought (CoT) is an efficient prompting method that enables the reasoning ability of large language models by augmenting the query using multiple examples with multiple intermediate steps. Despite the empirical success, the theoretical understanding of how to train a Transformer to achieve the CoT ability remains less explored. This is primarily due to the technical challenges involved in analyzing the nonconvex optimization on nonlinear attention models. To the best of our knowledge, this work provides the first theoretical study of training Transformers with nonlinear attention to obtain the CoT generalization capability so that the resulting model can inference on unseen tasks when the input is augmented by examples of the new task. We first quantify the required training samples and iterations to train a Transformer model towards CoT ability. We then prove the success of its CoT generalization on unseen tasks with distribution-shifted testing data. Moreover, we theoretically characterize the conditions for an accurate reasoning output by CoT even when the provided reasoning examples contain noises and are not always accurate. In contrast, in-context learning (ICL), which can be viewed as one-step CoT without intermediate steps, may fail to provide an accurate output when CoT does. These theoretical findings are justified through experiments. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„æç¤ºæ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¤šä¸ªç¤ºä¾‹å’Œå¤šä¸ªä¸­é—´æ­¥éª¤æ¥æ‰©å±•æŸ¥è¯¢ï¼Œä»è€Œæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡åœ¨å®è¯ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†å¦‚ä½•è®­ç»ƒå˜å‹å™¨ï¼ˆTransformerï¼‰ä»¥å®ç°CoTèƒ½åŠ›çš„ç†è®ºç ”ç©¶ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºåˆ†æéçº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ä¸Šçš„éå‡¸ä¼˜åŒ–æ‰€æ¶‰åŠçš„æŠ€æœ¯æŒ‘æˆ˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œé¦–æ¬¡å¯¹è®­ç»ƒå˜å‹å™¨ä¸è·å¾—CoTæ³›åŒ–èƒ½åŠ›çš„éçº¿æ€§æ³¨æ„åŠ›è¿›è¡Œäº†ç†è®ºç ”ç©¶ï¼Œä»¥ä¾¿åœ¨è¾“å…¥é€šè¿‡æ–°ä»»åŠ¡çš„ç¤ºä¾‹å¢å¼ºæ—¶ï¼Œæ¨¡å‹å¯ä»¥å¯¹æœªè§è¿‡çš„ä»»åŠ¡è¿›è¡Œæ¨æ–­ã€‚æˆ‘ä»¬é¦–å…ˆé‡åŒ–è®­ç»ƒå˜å‹å™¨æ¨¡å‹å®ç°CoTèƒ½åŠ›æ‰€éœ€çš„è®­ç»ƒæ ·æœ¬å’Œè¿­ä»£æ¬¡æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡åœ¨å…·æœ‰åˆ†å¸ƒåç§»çš„æµ‹è¯•æ•°æ®ä¸Šè¯æ˜å…¶CoTæ³›åŒ–çš„æˆåŠŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ä»¥ä¸‹æƒ…å†µè¿›è¡Œäº†ç†è®ºæè¿°ï¼šå³ä½¿æä¾›çš„æ¨ç†ç¤ºä¾‹åŒ…å«å™ªå£°å¹¶ä¸æ€»æ˜¯å‡†ç¡®ï¼ŒCoTä»èƒ½äº§ç”Ÿå‡†ç¡®çš„æ¨ç†è¾“å‡ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learningï¼ŒICLï¼‰å¯ä»¥çœ‹ä½œæ˜¯æ²¡æœ‰ä¸­é—´æ­¥éª¤çš„ä¸€æ­¥å¼CoTï¼Œå½“CoTå­˜åœ¨æ—¶ï¼Œå®ƒå¯èƒ½æ— æ³•æä¾›å‡†ç¡®çš„è¾“å‡ºã€‚è¿™äº›ç†è®ºå‘ç°é€šè¿‡å®éªŒå¾—åˆ°äº†è¯å®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02167v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ­¤æ–‡æœ¬è¯¦ç»†ä»‹ç»äº†Chain-of-Thoughtï¼ˆCoTï¼‰ä½œä¸ºä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºæ–¹æ³•ï¼Œé€šè¿‡å¤šä¸ªç¤ºä¾‹å’Œå¤šä¸ªä¸­é—´æ­¥éª¤å¢å¼ºæŸ¥è¯¢æ¥å®ç°æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡åœ¨å®è·µä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å…³äºå¦‚ä½•è®­ç»ƒTransformerä»¥å®ç°CoTèƒ½åŠ›çš„ç†è®ºç ”ç©¶ä»ç„¶è¾ƒå°‘ã€‚æœ¬æ–‡é¦–æ¬¡ç ”ç©¶äº†è®­ç»ƒå…·æœ‰éçº¿æ€§æ³¨æ„åŠ›çš„Transformerä»¥è·å–CoTæ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¢å¼ºæ–°ä»»åŠ¡è¾“å…¥åè¿›è¡Œæ¨ç†ã€‚æ–‡ç« é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯äº†CoTåœ¨å¤„ç†æœªè§ä»»åŠ¡ä»¥åŠé¢å¯¹åˆ†å¸ƒå˜åŒ–æµ‹è¯•æ•°æ®æ—¶çš„è¡¨ç°ï¼Œå¹¶è¿›ä¸€æ­¥æ¢è®¨äº†å½“æä¾›çš„æ¨ç†ç¤ºä¾‹åŒ…å«å™ªå£°å¹¶ä¸å®Œå…¨å‡†ç¡®æ—¶ï¼ŒCoTä»èƒ½å‡†ç¡®æ¨ç†çš„æ¡ä»¶ã€‚æ­¤å¤–ï¼Œä¸è¢«è§†ä¸ºæ— ä¸­é—´æ­¥éª¤çš„ä¸€æ­¥å¼CoTï¼ˆIn-context learningï¼ŒICLï¼‰ç›¸æ¯”ï¼ŒCoTåœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥æä¾›æ›´å‡†ç¡®çš„è¾“å‡ºã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒè¯å®äº†è¿™äº›ç†è®ºå‘ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Chain-of-Thought (CoT) æ˜¯ä¸€ç§é€šè¿‡å¤šä¸ªç¤ºä¾‹å’Œä¸­é—´æ­¥éª¤å¢å¼ºæŸ¥è¯¢æ¥å¯å‘å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæç¤ºæ–¹æ³•ã€‚</li>
<li>ç›®å‰å¯¹äºå¦‚ä½•è®­ç»ƒTransformerä»¥å®ç°CoTèƒ½åŠ›çš„ç†è®ºç ”ç©¶ç›¸å¯¹è¾ƒå°‘ï¼Œä¸»è¦ç”±äºæ¶‰åŠéçº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„éå‡¸ä¼˜åŒ–åˆ†ææŠ€æœ¯æŒ‘æˆ˜ã€‚</li>
<li>æ­¤ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†è®­ç»ƒå…·æœ‰éçº¿æ€§æ³¨æ„åŠ›çš„Transformerä»¥è·å–CoTæ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¢å¼ºæ–°ä»»åŠ¡è¾“å…¥åè¿›è¡Œæ¨ç†ã€‚</li>
<li>æ–‡ç« ç†è®ºåˆ†æäº†è®­ç»ƒæ ·æœ¬æ•°é‡å’Œè¿­ä»£æ¬¡æ•°çš„è¦æ±‚ï¼Œä»¥è®­ç»ƒå…·å¤‡CoTèƒ½åŠ›çš„Transformeræ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç†è®ºåˆ†æè¯å®äº†CoTåœ¨æœªè§ä»»åŠ¡ä¸Šçš„æ³›åŒ–æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åˆ†å¸ƒå˜åŒ–çš„æµ‹è¯•æ•°æ®æ—¶ã€‚</li>
<li>å½“æä¾›çš„æ¨ç†ç¤ºä¾‹åŒ…å«å™ªå£°å¹¶ä¸å®Œå…¨å‡†ç¡®æ—¶ï¼Œç†è®ºä¸Šæè¿°äº†CoTä»èƒ½å‡†ç¡®æ¨ç†çš„æ¡ä»¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b896d0970cd94b2002c572c0991ad0bd.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Understand-Symbolic-Graphics-Programs"><a href="#Can-Large-Language-Models-Understand-Symbolic-Graphics-Programs" class="headerlink" title="Can Large Language Models Understand Symbolic Graphics Programs?"></a>Can Large Language Models Understand Symbolic Graphics Programs?</h2><p><strong>Authors:Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard SchÃ¶lkopf</strong></p>
<p>Against the backdrop of enthusiasm for large language models (LLMs), there is a growing need to scientifically assess their capabilities and shortcomings. This is nontrivial in part because it is difficult to find tasks which the models have not encountered during training. Utilizing symbolic graphics programs, we propose a domain well-suited to test multiple spatial-semantic reasoning skills of LLMs. Popular in computer graphics, these programs procedurally generate visual data. While LLMs exhibit impressive skills in general program synthesis and analysis, symbolic graphics programs offer a new layer of evaluation: they allow us to test an LLMâ€™s ability to answer semantic questions about the images or 3D geometries without a vision encoder. To semantically understand the symbolic programs, LLMs would need to possess the ability to â€œimagineâ€ and reason how the corresponding graphics content would look with only the symbolic description of the local curvatures and strokes. We use this task to evaluate LLMs by creating a large benchmark for the semantic visual understanding of symbolic graphics programs, built procedurally with minimal human effort. Particular emphasis is placed on transformations of images that leave the image level semantics invariant while introducing significant changes to the underlying program. We evaluate commercial and open-source LLMs on our benchmark to assess their ability to reason about visual output of programs, finding that LLMs considered stronger at reasoning generally perform better. Lastly, we introduce a novel method to improve this ability â€“ Symbolic Instruction Tuning (SIT), in which the LLM is finetuned with pre-collected instruction data on symbolic graphics programs. Interestingly, we find that SIT not only improves LLMâ€™s understanding on symbolic programs, but it also improves general reasoning ability on various other benchmarks. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤‡å—å…³æ³¨çš„å¤§èƒŒæ™¯ä¸‹ï¼Œç§‘å­¦è¯„ä¼°å…¶èƒ½åŠ›å’ŒçŸ­æ¿çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚è¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯éå¹³å‡¡çš„ï¼Œå› ä¸ºå¾ˆéš¾æ‰¾åˆ°æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´æœªæ›¾é‡åˆ°è¿‡çš„ä»»åŠ¡ã€‚æˆ‘ä»¬åˆ©ç”¨ç¬¦å·å›¾å½¢ç¨‹åºï¼Œæå‡ºäº†ä¸€ä¸ªéå¸¸é€‚åˆæµ‹è¯•LLMå¤šæ–¹ç©ºé—´è¯­ä¹‰æ¨ç†èƒ½åŠ›çš„é¢†åŸŸã€‚è¿™äº›ç¨‹åºåœ¨ç”µè„‘åˆ¶å›¾é¢†åŸŸå¾ˆå—æ¬¢è¿ï¼Œå¯ä»¥ç¨‹åºæ€§åœ°ç”Ÿæˆè§†è§‰æ•°æ®ã€‚è™½ç„¶LLMåœ¨ä¸€èˆ¬ç¨‹åºåˆæˆå’Œåˆ†ææ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æŠ€èƒ½ï¼Œä½†ç¬¦å·å›¾å½¢ç¨‹åºæä¾›äº†ä¸€å±‚æ–°çš„è¯„ä¼°ï¼šå®ƒä»¬å…è®¸æˆ‘ä»¬æµ‹è¯•LLMå›ç­”å…³äºå›¾åƒæˆ–3Då‡ ä½•çš„è¯­ä¹‰é—®é¢˜çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€ä½¿ç”¨è§†è§‰ç¼–ç å™¨ã€‚ä¸ºäº†è¯­ä¹‰åœ°ç†è§£ç¬¦å·ç¨‹åºï¼ŒLLMéœ€è¦æ‹¥æœ‰ä»…å‡­ç¬¦å·æè¿°å±€éƒ¨æ›²ç‡å’Œç¬”è§¦æ¥â€œæƒ³è±¡â€å¹¶æ¨ç†ç›¸åº”å›¾å½¢å†…å®¹å¤–è§‚çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºç”¨äºç¬¦å·å›¾å½¢ç¨‹åºè¯­ä¹‰è§†è§‰ç†è§£çš„å¤§å‹åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMï¼Œè¯¥æµ‹è¯•é‡‡ç”¨ç¨‹åºæ„å»ºï¼Œå‡ ä¹ä¸éœ€è¦äººå·¥å‚ä¸ã€‚æˆ‘ä»¬ç‰¹åˆ«é‡è§†å›¾åƒè½¬æ¢ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡åœ¨ä¿æŒå›¾åƒçº§åˆ«è¯­ä¹‰ä¸å˜çš„åŒæ—¶ï¼Œä¸ºåº•å±‚ç¨‹åºå¼•å…¥äº†é‡å¤§å˜åŒ–ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å•†ç”¨å’Œå¼€æºçš„LLMï¼Œä»¥è¯„ä¼°å®ƒä»¬å¯¹ç¨‹åºè§†è§‰è¾“å‡ºçš„æ¨ç†èƒ½åŠ›ï¼Œå‘ç°è¢«è®¤ä¸ºåœ¨æ¨ç†æ–¹é¢æ›´å‡ºè‰²çš„LLMé€šå¸¸è¡¨ç°æ›´å¥½ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›è¿™ç§èƒ½åŠ›çš„æ–°æ–¹æ³•â€”â€”ç¬¦å·æŒ‡ä»¤å¾®è°ƒï¼ˆSITï¼‰ï¼Œå¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”é¢„å…ˆæ”¶é›†çš„ç¬¦å·å›¾å½¢ç¨‹åºçš„æŒ‡ä»¤æ•°æ®ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°SITä¸ä»…æé«˜äº†LLMå¯¹ç¬¦å·ç¨‹åºçš„ç†è§£èƒ½åŠ›ï¼Œè€Œä¸”è¿˜æé«˜äº†å…¶åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­çš„ä¸€èˆ¬æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08313v4">PDF</a> ICLR 2025 Spotlight (v4: 47 pages, 26 figures, project page:   <a target="_blank" rel="noopener" href="https://sgp-bench.github.io/">https://sgp-bench.github.io/</a>)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç§‘å­¦è¯„ä¼°é—®é¢˜ï¼Œæå‡ºåˆ©ç”¨ç¬¦å·å›¾å½¢ç¨‹åºé¢†åŸŸæµ‹è¯•LLMçš„ç©ºé—´è¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚ç¬¦å·å›¾å½¢ç¨‹åºèƒ½å¤Ÿç¨‹åºåŒ–ç”Ÿæˆè§†è§‰æ•°æ®ï¼Œä¸ºè¯„ä¼°LLMæä¾›äº†æ–°çš„å±‚é¢ã€‚LLMåœ¨è¯¥ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„ç¨‹åºåˆæˆå’Œåˆ†æèƒ½åŠ›ï¼Œéœ€è¦é€šè¿‡ç†è§£ç¬¦å·ç¨‹åºæ¥å›ç­”å…³äºå›¾åƒæˆ–3Då‡ ä½•çš„è¯­ä¹‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« åˆ›å»ºäº†ä¸€ä¸ªå¤§å‹åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMå¯¹ç¬¦å·å›¾å½¢ç¨‹åºçš„ç†è§£èƒ½åŠ›ï¼Œå¹¶å¼ºè°ƒå›¾åƒè¯­ä¹‰ä¸å˜è€Œåº•å±‚ç¨‹åºå‘ç”Ÿæ˜¾è‘—å˜åŒ–çš„è½¬æ¢ã€‚æ–‡ç« è¯„ä¼°äº†å•†ä¸šå’Œå¼€æºLLMåœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå¹¶æå‡ºä¸€ç§æ–°æ–¹æ³•â€”â€”ç¬¦å·æŒ‡ä»¤å¾®è°ƒï¼ˆSITï¼‰æ¥æé«˜LLMçš„ç†è§£èƒ½åŠ›ã€‚å®éªŒå‘ç°ï¼ŒSITä¸ä»…èƒ½æé«˜LLMå¯¹ç¬¦å·ç¨‹åºçš„ç†è§£ï¼Œè¿˜èƒ½æé«˜å…¶åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMçš„ç©ºé—´è¯­ä¹‰æ¨ç†èƒ½åŠ›éœ€è¦é€šè¿‡ç§‘å­¦è¯„ä¼°ï¼Œç¬¦å·å›¾å½¢ç¨‹åºé¢†åŸŸæ˜¯æµ‹è¯•æ­¤èƒ½åŠ›çš„åˆé€‚é€‰æ‹©ã€‚</li>
<li>ç¬¦å·å›¾å½¢ç¨‹åºèƒ½å¤Ÿç¨‹åºåŒ–ç”Ÿæˆè§†è§‰æ•°æ®ï¼Œä¸ºè¯„ä¼°LLMæä¾›äº†æ–°å±‚é¢ã€‚</li>
<li>LLMåœ¨ç¨‹åºåˆæˆå’Œåˆ†ææ–¹é¢è¡¨ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œéœ€è¦é€šè¿‡ç†è§£ç¬¦å·ç¨‹åºæ¥å›ç­”å…³äºå›¾åƒæˆ–3Då‡ ä½•çš„è¯­ä¹‰é—®é¢˜ã€‚</li>
<li>åˆ›å»ºäº†ä¸€ä¸ªå¤§å‹åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMå¯¹ç¬¦å·å›¾å½¢ç¨‹åºè¯­ä¹‰è§†è§‰ç†è§£çš„èƒ½åŠ›ã€‚</li>
<li>è¯„ä¼°äº†å•†ä¸šå’Œå¼€æºLLMåœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå‘ç°æ¨ç†èƒ½åŠ›è¾ƒå¼ºçš„LLMè¡¨ç°æ›´å¥½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•â€”â€”ç¬¦å·æŒ‡ä»¤å¾®è°ƒï¼ˆSITï¼‰ï¼Œä»¥æé«˜LLMå¯¹ç¬¦å·ç¨‹åºçš„ç†è§£åŠæ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f07d4051dd17a0c201da7227509f2a60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-683d9ce159b5a9451d604dd45ead4141.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7873ff9545f61f2c24dc4f7c1d05271.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc945c5db6e367ff9455f95a06278e73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4255be007f1df3fd623edb627c42b911.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e12a771a2af2be34b5140bcb79ca7cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ff1e9103bd9a95db46a2f19ab99ba51.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="OmniBal-Towards-Fast-Instruction-Tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance"><a href="#OmniBal-Towards-Fast-Instruction-Tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance" class="headerlink" title="OmniBal: Towards Fast Instruction-Tuning for Vision-Language Models via   Omniverse Computation Balance"></a>OmniBal: Towards Fast Instruction-Tuning for Vision-Language Models via   Omniverse Computation Balance</h2><p><strong>Authors:Yongqiang Yao, Jingru Tan, Feizhao Zhang, Jiahao Hu, Yazhe Niu, Xin Jin, Bo Li, Pengfei Liu, Ruihao Gong, Dahua Lin, Ningyi Xu</strong></p>
<p>Vision-language instruction-tuning models have recently achieved significant performance improvements. In this work, we discover that large-scale 3D parallel training on those models leads to an imbalanced computation load across different devices. The vision and language parts are inherently heterogeneous: their data distribution and model architecture differ significantly, which affects distributed training efficiency. To address this issue, we rebalance the computational load from data, model, and memory perspectives, achieving more balanced computation across devices. Specifically, for the data, instances are grouped into new balanced mini-batches within and across devices. A search-based method is employed for the model to achieve a more balanced partitioning. For memory optimization, we adaptively adjust the re-computation strategy for each partition to utilize the available memory fully. These three perspectives are not independent but are closely connected, forming an omniverse balanced training framework. Extensive experiments are conducted to validate the effectiveness of our method. Compared with the open-source training code of InternVL-Chat, training time is reduced greatly, achieving about 1.8$\times$ speed-up. Our methodâ€™s efficacy and generalizability are further validated across various models and datasets. Codes will be released at <a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal">https://github.com/ModelTC/OmniBal</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æœ€è¿‘å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°å¤§è§„æ¨¡3Då¹¶è¡Œè®­ç»ƒè¿™äº›æ¨¡å‹ä¼šå¯¼è‡´ä¸åŒè®¾å¤‡é—´è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡ã€‚è§†è§‰å’Œè¯­è¨€éƒ¨åˆ†æœ¬è´¨ä¸Šæ˜¯å¼‚æ„çš„ï¼šå®ƒä»¬çš„æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹æ¶æ„å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œä»è€Œå½±å“åˆ†å¸ƒå¼è®­ç»ƒæ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»æ•°æ®ã€æ¨¡å‹å’Œå†…å­˜ä¸‰ä¸ªæ–¹é¢é‡æ–°å¹³è¡¡è®¡ç®—è´Ÿè½½ï¼Œå®ç°è®¾å¤‡é—´æ›´å¹³è¡¡çš„è®¡ç®—ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ•°æ®ï¼Œå®ä¾‹ä¼šåœ¨è®¾å¤‡å’Œè·¨è®¾å¤‡ä¹‹é—´åˆ†ç»„å½¢æˆæ–°çš„å¹³è¡¡å°æ‰¹æ¬¡ã€‚é‡‡ç”¨åŸºäºæœç´¢çš„æ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œæ›´å¹³è¡¡çš„åˆ†åŒºã€‚ä¸ºäº†ä¼˜åŒ–å†…å­˜ï¼Œæˆ‘ä»¬è‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªåˆ†åŒºçš„é‡æ–°è®¡ç®—ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨å¯ç”¨å†…å­˜ã€‚è¿™ä¸‰ä¸ªæ–¹é¢ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œè€Œæ˜¯ç´§å¯†ç›¸å…³çš„ï¼Œå½¢æˆäº†ä¸€ä¸ªå…¨æ–¹ä½å¹³è¡¡çš„è®­ç»ƒæ¡†æ¶ã€‚è¿›è¡Œäº†å¤§é‡å®éªŒæ¥éªŒè¯æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸InternVL-Chatçš„å¼€æºè®­ç»ƒä»£ç ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ—¶é—´å¤§å¤§ç¼©çŸ­ï¼Œå®ç°äº†çº¦1.8å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§åœ¨å„ç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå¾—åˆ°äº†è¿›ä¸€æ­¥éªŒè¯ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/ModelTC/OmniBalå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20761v4">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡è§†è§‰è¯­è¨€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æœ¬ç ”ç©¶å‘ç°å¤§è§„æ¨¡ä¸‰ç»´å¹¶è¡Œè®­ç»ƒä¼šå¯¼è‡´ä¸åŒè®¾å¤‡é—´è®¡ç®—è´Ÿè½½ä¸å‡è¡¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»æ•°æ®ã€æ¨¡å‹å’Œå†…å­˜ä¸‰ä¸ªè§’åº¦é‡æ–°å¹³è¡¡è®¡ç®—è´Ÿè½½ï¼Œå®ç°äº†è®¾å¤‡é—´çš„å‡è¡¡è®¡ç®—ã€‚é€šè¿‡å®ä¾‹åˆ†ç»„ã€æ¨¡å‹æœç´¢å’Œå†…å­˜ä¼˜åŒ–ç­‰æ–¹æ³•ï¼Œå½¢æˆå…¨æ–¹ä½å¹³è¡¡çš„è®­ç»ƒæ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæé«˜è®­ç»ƒæ•ˆç‡ï¼Œä¸å…¬å¼€ä»£ç InternVL-Chatç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´å¤§å¹…ç¼©çŸ­ï¼Œå®ç°äº†çº¦1.8å€çš„åŠ é€Ÿæ•ˆæœï¼Œä¸”è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå…·æœ‰æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ã€‚ç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal%E4%B8%8A%E3%80%82">https://github.com/ModelTC/OmniBalä¸Šã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è§†è§‰è¯­è¨€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>ä¸‰ç»´å¹¶è¡Œè®­ç»ƒå¯¼è‡´ä¸åŒè®¾å¤‡é—´è®¡ç®—è´Ÿè½½ä¸å‡è¡¡ã€‚</li>
<li>æå‡ºä»æ•°æ®ã€æ¨¡å‹å’Œå†…å­˜ä¸‰ä¸ªè§’åº¦é‡æ–°å¹³è¡¡è®¡ç®—è´Ÿè½½çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å®ä¾‹åˆ†ç»„ã€æ¨¡å‹æœç´¢å’Œå†…å­˜ä¼˜åŒ–å½¢æˆå…¨æ–¹ä½å¹³è¡¡çš„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>ä¸å…¬å¼€ä»£ç ç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´å¤§å¹…ç¼©çŸ­ï¼Œå®ç°äº†çº¦1.8å€çš„åŠ é€Ÿæ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå…·æœ‰æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7e0bf113b767796f706dbcd55094388e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a45dd304e55736f9eefa248f1a9bf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-210f99f0226ef26deaa57aafa4e06601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-401fc3a4bbe25a7b26e6cfab69506370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fa473f471c98770a269b4e64f560637.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Can-Small-Language-Models-Learn-Unlearn-and-Retain-Noise-Patterns"><a href="#Can-Small-Language-Models-Learn-Unlearn-and-Retain-Noise-Patterns" class="headerlink" title="Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?"></a>Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?</h2><p><strong>Authors:Nicy Scaria, Silvester John Joseph Kennedy, Deepak Subramani</strong></p>
<p>With the growing need for efficient language models in resource-constrained environments, Small Language Models (SLMs) have emerged as compact and practical alternatives to Large Language Models (LLMs). While studies have explored noise handling in LLMs, little is known about how SLMs handle noise, a critical factor for their reliable real-world deployment. This study investigates the ability of SLMs with parameters between 1 and 3 billion to learn, retain, and subsequently eliminate different types of noise (word flip, character flip, transliteration, irrelevant content, and contradictory information). Four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma1.1 2B, and Phi2 2.7B) were instruction-tuned on noise-free data and tested with in-context examples to assess noise learning. Subsequently, noise patterns were introduced in instruction tuning to assess their adaptability. The results revealed differences in how models handle noise, with smaller models like Olmo quickly adapting to noise patterns. Phi2â€™s carefully curated, structured, and high-quality pretraining data enabled resistance to character level, transliteration, and counterfactual noise, while Gemma adapted successfully to transliteration noise through its multilingual pretraining. Subsequent clean data training effectively mitigated noise effects. These findings provide practical strategies for developing robust SLMs for real-world applications. </p>
<blockquote>
<p>éšç€èµ„æºå—é™ç¯å¢ƒä¸­å¯¹é«˜æ•ˆè¯­è¨€æ¨¡å‹çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç´§å‡‘å®ç”¨æ›¿ä»£å“åº”è¿è€Œç”Ÿã€‚è™½ç„¶å·²æœ‰ç ”ç©¶æ¢ç´¢äº†LLMä¸­çš„å™ªå£°å¤„ç†ï¼Œä½†å¯¹äºSLMå¦‚ä½•å¤„ç†å™ªå£°çŸ¥ä¹‹ç”šå°‘ï¼Œè¿™æ˜¯å…¶å¯é ç°å®ä¸–ç•Œéƒ¨ç½²çš„å…³é”®å› ç´ ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å‚æ•°åœ¨1åˆ°3äº¿ä¹‹é—´çš„SLMå­¦ä¹ ã€ä¿ç•™å’Œéšåæ¶ˆé™¤ä¸åŒç±»å‹å™ªå£°ï¼ˆå•è¯ç¿»è½¬ã€å­—ç¬¦ç¿»è½¬ã€éŸ³è¯‘ã€æ— å…³å†…å®¹å’ŒçŸ›ç›¾ä¿¡æ¯ï¼‰çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶é€‰æ‹©äº†å››ä¸ªé¢„è®­ç»ƒSLMï¼ˆOlmo 1Bã€Qwen1.5 1.8Bã€Gemma1.1 2Bå’ŒPhi2 2.7Bï¼‰ï¼Œå®ƒä»¬åœ¨æ— å™ªå£°æ•°æ®ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å®ä¾‹è¿›è¡Œæµ‹è¯•ï¼Œä»¥è¯„ä¼°å™ªå£°å­¦ä¹ èƒ½åŠ›ã€‚éšåï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒä¸­å¼•å…¥å™ªå£°æ¨¡å¼ä»¥è¯„ä¼°å…¶é€‚åº”æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå„æ¨¡å‹å¤„ç†å™ªå£°çš„æ–¹å¼å­˜åœ¨å·®å¼‚ï¼Œè¾ƒå°çš„æ¨¡å‹å¦‚Olmoèƒ½è¿…é€Ÿé€‚åº”å™ªå£°æ¨¡å¼ã€‚Phi2ç»è¿‡ç²¾å¿ƒç­–åˆ’ã€ç»“æ„åŒ–å’Œé«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®ï¼Œä½¿å…¶èƒ½å¤ŸæŠµæŠ—å­—ç¬¦çº§åˆ«ã€éŸ³è¯‘å’Œåå‘äº‹å®çš„å™ªå£°ï¼Œè€ŒGemmaåˆ™é€šè¿‡å…¶å¤šè¯­è¨€é¢„è®­ç»ƒæˆåŠŸé€‚åº”äº†éŸ³è¯‘å™ªå£°ã€‚éšåçš„æ¸…æ´æ•°æ®è®­ç»ƒæœ‰æ•ˆåœ°å‡è½»äº†å™ªå£°å½±å“ã€‚è¿™äº›å‘ç°ä¸ºå®ç°ç¨³å¥çš„SLMç”¨äºå®é™…åº”ç”¨æä¾›äº†å®ç”¨ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.00996v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨å¤„ç†å™ªéŸ³æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºèµ„æºå—é™ç¯å¢ƒä¸­çš„å®é™…éƒ¨ç½²è‡³å…³é‡è¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºä¸åŒSLMåœ¨å¤„ç†ä¸åŒå™ªéŸ³ç±»å‹ï¼ˆå¦‚è¯è¯­ç¿»è½¬ã€å­—ç¬¦ç¿»è½¬ã€éŸ³è¯‘ã€æ— å…³å†…å®¹å’ŒçŸ›ç›¾ä¿¡æ¯ï¼‰æ—¶å­˜åœ¨å·®å¼‚ã€‚é€šè¿‡å¼•å…¥å™ªå£°æ¨¡å¼å¯¹æ¨¡å‹è¿›è¡Œé€‚åº”æ€§è¯„ä¼°ï¼Œå‘ç°å°å‹æ¨¡å‹å¯ä»¥å¿«é€Ÿé€‚åº”å™ªå£°æ¨¡å¼ã€‚æŸäº›æ¨¡å‹ç”±äºå…¶é¢„è®­ç»ƒæ•°æ®çš„ç‰¹æ€§ï¼Œå¦‚Phi2ï¼Œå…·æœ‰æŠµæŠ—å­—ç¬¦çº§åˆ«ã€éŸ³è¯‘å’Œåå‘äº‹å®çš„å™ªå£°çš„èƒ½åŠ›ã€‚éšåè¿›è¡Œçš„æ¸…æ´æ•°æ®è®­ç»ƒæœ‰æ•ˆåœ°å‡è½»äº†å™ªå£°çš„å½±å“ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘ç”¨äºå®é™…åº”ç”¨çš„ç¨³å¥SLMæä¾›äº†å®ç”¨ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºç´§å‡‘ä¸”å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ»¡è¶³äº†èµ„æºå—é™ç¯å¢ƒä¸­å¯¹é«˜æ•ˆè¯­è¨€æ¨¡å‹çš„éœ€æ±‚ã€‚</li>
<li>ç›®å‰å¯¹äºSLMå¤„ç†å™ªéŸ³çš„èƒ½åŠ›çš„ç ”ç©¶å°šä¸è¶³ã€‚</li>
<li>æœ¬ç ”ç©¶è°ƒæŸ¥äº†SLMåœ¨å­¦ä¹ ã€ä¿ç•™å’Œæ¶ˆé™¤ä¸åŒç±»å‹å™ªéŸ³ï¼ˆå¦‚è¯è¯­ç¿»è½¬ã€å­—ç¬¦ç¿»è½¬ç­‰ï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ä¸åŒSLMåœ¨å¤„ç†å™ªéŸ³æ—¶å­˜åœ¨å·®å¼‚ï¼Œå°å‹æ¨¡å‹å¯ä»¥å¿«é€Ÿé€‚åº”å™ªå£°æ¨¡å¼ã€‚</li>
<li>æŸäº›æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®ç‰¹æ€§ä½¿å…¶å…·æœ‰æŠµæŠ—ç‰¹å®šç±»å‹å™ªå£°çš„èƒ½åŠ›ã€‚</li>
<li>æ¸…æ´æ•°æ®è®­ç»ƒå¯ä»¥æœ‰æ•ˆåœ°å‡è½»å™ªå£°å¯¹SLMçš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.00996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7027c1d9a0e95db955f1db80727cd3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51a33527cfc93f49d74f011f26d1292a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c0e82c44d21203383fcaebed8c25fcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e608b118b6f3f84a46cb89d0fa3a6291.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e5f22c3902e2f647156294961e7f850.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-01/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-01/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c41f9698f0b9d7fb6ec207616348e39e.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-01  LEAVS An LLM-based Labeler for Abdominal CT Supervision
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-01/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-937915e313a6e036e2c588a707cf9053.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-01  VideoReasonBench Can MLLMs Perform Vision-Centric Complex Video   Reasoning?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
