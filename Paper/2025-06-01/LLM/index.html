<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-06-01  How Transformers Learn Regular Language Recognition A Theoretical Study   on Training Dynamics and Implicit Bias">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c5ece0295520d590c06c0e7c317c48ae.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    78 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-01-更新"><a href="#2025-06-01-更新" class="headerlink" title="2025-06-01 更新"></a>2025-06-01 更新</h1><h2 id="How-Transformers-Learn-Regular-Language-Recognition-A-Theoretical-Study-on-Training-Dynamics-and-Implicit-Bias"><a href="#How-Transformers-Learn-Regular-Language-Recognition-A-Theoretical-Study-on-Training-Dynamics-and-Implicit-Bias" class="headerlink" title="How Transformers Learn Regular Language Recognition: A Theoretical Study   on Training Dynamics and Implicit Bias"></a>How Transformers Learn Regular Language Recognition: A Theoretical Study   on Training Dynamics and Implicit Bias</h2><p><strong>Authors:Ruiquan Huang, Yingbin Liang, Jing Yang</strong></p>
<p>Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as <code>even pairs&#39; and </code>parity check’, the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1&#x2F;t)$. Our experiments validate those theoretical results. </p>
<blockquote>
<p>语言识别任务在自然语言处理（NLP）中扮演着根本性的角色，并且已广泛应用于评估大型语言模型（LLM）的性能。这些任务在解释变换器的工作原理方面也起着至关重要的作用。在这项工作中，我们专注于常规语言识别类别中的两个代表性任务，即“偶数对”和“奇偶校验”，这两个任务的目标是确定在给定的序列中某些子序列的出现是否为偶数。我们的目标是探索由注意力层和线性层组成的一层变换器如何学习解决这些任务，通过理论分析其在梯度下降下的训练动态。虽然“偶数对”可以直接由一层转换器解决，但“奇偶校验”需要通过将思维链（CoT）集成到为“偶数对”任务训练良好的转换器的推理阶段，或者集成到一层转换器的训练中来解决。对于这两个问题，我们的分析表明，注意力层和线性层的联合训练表现出两个明显的阶段。在第一阶段，注意力层迅速增长，将数据序列映射成可分离向量。在第二阶段，注意力层变得稳定，而线性层的增长呈对数趋势，并且朝向一个最大间隔超平面发展，该超平面能够正确地将注意力层的输出分为正样本和负样本，损失以O(1&#x2F;t)的速度减少。我们的实验验证了这些理论结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00926v3">PDF</a> accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了自然语言处理中的语言识别任务，特别是针对“偶数对”和“奇偶校验”这两个代表性任务的理论分析。文章聚焦于单层变压器模型，通过梯度下降分析其训练动态，探讨模型如何学习解决这些任务。研究发现，对于“偶数对”任务，单层变压器可直接解决；而对于“奇偶校验”任务，则需结合链式思维（CoT）进行推理。此外，研究发现注意力层和线性层的联合训练呈现两个阶段：第一阶段注意力层迅速增长，将数据序列映射为可分向量；第二阶段注意力层稳定，线性层对数增长并逐渐接近最大间隔超平面，将注意力层输出分为正负样本。实验验证了这些理论结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言识别任务是自然语言处理中的重要基准，用于评估大型语言模型的性能，并揭示变压器工作机制。</li>
<li>“偶数对”和“奇偶校验”是语言识别中的代表性任务，用于测试模型对序列中子序列出现次数的判断能力。</li>
<li>单层变压器模型通过梯度下降学习解决这些任务时，展现出两个训练阶段：注意力层的快速增长和线性层的对数增长。</li>
<li>对于“偶数对”任务，单层变压器可直接解决；而对于“奇偶校验”任务，需要引入链式思维（CoT）进行推理。</li>
<li>在训练过程中，注意力层将数据序列映射为可分向量，而线性层则负责根据最大间隔原则正确区分正负样本。</li>
<li>模型损失随训练时间递减，遵循O(1&#x2F;t)的速率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00926">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e5351cd24f89ec5f4a109386cbf645c1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Temporal-Relation-Extraction-in-Clinical-Texts-A-Span-based-Graph-Transformer-Approach"><a href="#Temporal-Relation-Extraction-in-Clinical-Texts-A-Span-based-Graph-Transformer-Approach" class="headerlink" title="Temporal Relation Extraction in Clinical Texts: A Span-based Graph   Transformer Approach"></a>Temporal Relation Extraction in Clinical Texts: A Span-based Graph   Transformer Approach</h2><p><strong>Authors:Rochana Chaturvedi, Peyman Baghershahi, Sourav Medya, Barbara Di Eugenio</strong></p>
<p>Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the well-studied I2B2 2012 Temporal Relations Challenge corpus. This task is inherently challenging due to complex clinical language, long documents, and sparse annotations. We introduce GRAPHTREX, a novel method integrating span-based entity-relation extraction, clinical large pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT) to capture local and global dependencies. Our HGT component facilitates information propagation across the document through innovative global landmarks that bridge distant entities. Our method improves the state-of-the-art with 5.5% improvement in the tempeval $F_1$ score over the previous best and up to 8.9% improvement on long-range relations, which presents a formidable challenge. We further demonstrate generalizability by establishing a strong baseline on the E3C corpus. This work not only advances temporal information extraction but also lays the groundwork for improved diagnostic and prognostic models through enhanced temporal reasoning. </p>
<blockquote>
<p>从非结构化文本中提取时间信息是上下文事件化和获取可操作洞察的关键，特别是在医疗领域。我们利用经过深入研究的I2B2 2012时间关系挑战赛语料库，来解决提取临床事件及其时间关系的任务。由于复杂的临床语言、长文档和稀疏的注释，此任务本质上是具有挑战性的。我们引入了GRAPHTREX，这是一种新颖的方法，融合了基于范围的实体关系提取、临床大型预训练语言模型（LPLMs）和异质图转换器（HGT），以捕捉局部和全局依赖关系。我们的HGT组件通过创新的全局地标，在文档中实现了跨文档的信息传播，这些地标架起了遥远实体之间的桥梁。我们的方法在tempeval F1分数上提高了5.5%，在远程关系上最多提高了8.9%，对先前的最佳水平构成了强有力的挑战。我们还通过在E3C语料库上建立了强大的基线来进一步证明了其泛化能力。这项工作不仅推动了时间信息提取的进步，而且通过增强时间推理为改进诊断和预后模型奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18085v2">PDF</a> Introducing a novel method for joint extraction of medical events and   temporal relations from free-text, leveraging clinical LPLMs and   Heterogeneous Graph Transformers, achieving a 5.5% improvement over the   previous state-of-the-art and up to 8.9% on long-range relations</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于I2B2 2012时序关系挑战语料库，利用span实体关系提取、临床大型预训练语言模型和异质图转换器（HGT）等技术，实现临床事件及其时序关系的提取。该方法通过全局地标创新性地实现文档中的信息传播，并有效提高远近实体关系的捕捉能力。本文方法不仅提升了时序信息提取的性能，还为诊断与预后模型的改进提供了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了利用I2B2 2012语料库进行临床事件及其时序关系提取的重要性。</li>
<li>提出了一种新的方法GRAPHTREX，集成了span实体关系提取、大型预训练语言模型和异质图转换器（HGT）。</li>
<li>HGT组件通过全局地标实现文档中的信息传播，提高了捕捉局部和全局依赖关系的能力。</li>
<li>该方法在tempeval $F_1$得分上较之前最佳方法提高了5.5%，并在长程关系上提高了8.9%。</li>
<li>该方法不仅在时序信息提取上表现优秀，还在诊断和预后模型的改进中展现出潜力。</li>
<li>方法具有良好的泛化能力，能够在E3C语料库上建立强大的基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7c99bc342a26ba051e8606d9d196d35b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-210edbe46cbbd1e3641e3b7d5662930d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33c77763bcc8c010355119bb166d4672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fdc08697505d55738748bc034c81b21.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HiDe-LLaVA-Hierarchical-Decoupling-for-Continual-Instruction-Tuning-of-Multimodal-Large-Language-Model"><a href="#HiDe-LLaVA-Hierarchical-Decoupling-for-Continual-Instruction-Tuning-of-Multimodal-Large-Language-Model" class="headerlink" title="HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of   Multimodal Large Language Model"></a>HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of   Multimodal Large Language Model</h2><p><strong>Authors:Haiyang Guo, Fanhu Zeng, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu</strong></p>
<p>Instruction tuning is widely used to improve a pre-trained Multimodal Large Language Model (MLLM) by training it on curated task-specific datasets, enabling better comprehension of human instructions. However, it is infeasible to collect all possible instruction datasets simultaneously in real-world scenarios. Thus, enabling MLLM with continual instruction tuning is essential for maintaining their adaptability. However, existing methods often trade off memory efficiency for performance gains, significantly compromising overall efficiency. In this paper, we propose a task-specific expansion and task-general fusion framework based on the variations in Centered Kernel Alignment (CKA) similarity across different model layers when trained on diverse datasets. Furthermore, we analyze the information leakage present in the existing benchmark and propose a new and more challenging benchmark to rationally evaluate the performance of different methods. Comprehensive experiments showcase a significant performance improvement of our method compared to existing state-of-the-art methods. Code and dataset are released at <a target="_blank" rel="noopener" href="https://github.com/Ghy0501/HiDe-LLaVA">https://github.com/Ghy0501/HiDe-LLaVA</a>. </p>
<blockquote>
<p>指令微调被广泛用于通过训练在精选的任务特定数据集上来改进预训练的多模态大型语言模型（MLLM），从而实现更好的对人类指令的理解。然而，在现实世界的场景中，同时收集所有可能的指令数据集是不可能的。因此，为MLLM启用持续指令微调对于保持其适应性至关重要。然而，现有方法往往以牺牲内存效率为代价来提高性能，从而严重影响整体效率。在本文中，我们提出了一个基于训练多样化数据集时不同模型层中心内核对齐（CKA）相似性的变化的任务特定扩展和任务通用融合框架。此外，我们分析了现有基准测试中的信息泄露问题，并提出了一个新的更具挑战性的基准测试来合理评估不同方法的性能。综合实验表明，我们的方法与现有最先进的方法相比，性能得到了显著提升。代码和数据集发布在<a target="_blank" rel="noopener" href="https://github.com/Ghy0501/HiDe-LLaVA%E4%B8%8A%E3%80%82">https://github.com/Ghy0501/HiDe-LLaVA上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12941v2">PDF</a> ACL 2025 (Main)</p>
<p><strong>总结</strong><br>训练数据受限的真实世界中，不断微调指导模型用于增强其对各种任务的学习是必要之举。现有方法存在记忆效率和性能之间的权衡问题。本文基于不同数据集训练下模型各层间的中心核对齐（CKA）相似度变化，提出特定任务扩展和通用任务融合框架，同时分析现有基准测试中的信息泄露问题，并推出更具挑战性的新基准测试。本文的方法较现有的尖端技术显著提高性能。GitHub上有对应的代码和数据集资源：<a target="_blank" rel="noopener" href="https://github.com/Ghy0501/HiDe-LLaVA%E3%80%82%E8%AF%A5LLM%E6%80%A7%E8%83%BD%E6%8C%81%E7%BB%AD%E6%94%B9%E8%BF%9B%E6%8A%80%E6%9C%AF%E5%AF%B9%E5%AE%9E%E7%8E%B0%E6%99%BA%E8%83%BD%E5%8C%96%E5%8D%87%E7%BA%A7%E5%A4%A7%E6%9C%89%E8%A3%A8%E7%9B%8A%E3%80%82">https://github.com/Ghy0501/HiDe-LLaVA。该LLM性能持续改进技术对实现智能化升级大有裨益。</a></p>
<p><strong>要点摘要</strong></p>
<ul>
<li>训练数据受限的真实世界中，持续微调指导模型以增强其对各种任务的学习至关重要。</li>
<li>现有方法往往牺牲记忆效率来换取性能提升，难以达到理想效果。</li>
<li>基于CKA相似度变化提出特定任务扩展和通用任务融合框架，有效改进性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12941">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-97ac3105965ad2029e1fcacffd3161cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61115fde72b7507075cb36733e11e93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14484e830d8e49529d3164b2421cbbae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9141ea9ab4b1c147373f6265aa995f7f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Length-Controlled-Margin-Based-Preference-Optimization-without-Reference-Model"><a href="#Length-Controlled-Margin-Based-Preference-Optimization-without-Reference-Model" class="headerlink" title="Length-Controlled Margin-Based Preference Optimization without Reference   Model"></a>Length-Controlled Margin-Based Preference Optimization without Reference   Model</h2><p><strong>Authors:Gengxu Li, Tingyu Xia, Yi Chang, Yuan Wu</strong></p>
<p>Direct Preference Optimization (DPO) is a widely adopted offline algorithm for preference-based reinforcement learning from human feedback (RLHF), designed to improve training simplicity and stability by redefining reward functions. However, DPO is hindered by several limitations, including length bias, memory inefficiency, and probability degradation. To address these challenges, we propose Length-Controlled Margin-Based Preference Optimization (LMPO), a more efficient and robust alternative. LMPO introduces a uniform reference model as an upper bound for the DPO loss, enabling a more accurate approximation of the original optimization objective. Additionally, an average log-probability optimization strategy is employed to minimize discrepancies between training and inference phases. A key innovation of LMPO lies in its Length-Controlled Margin-Based loss function, integrated within the Bradley-Terry framework. This loss function regulates response length while simultaneously widening the margin between preferred and rejected outputs. By doing so, it mitigates probability degradation for both accepted and discarded responses, addressing a significant limitation of existing methods. We evaluate LMPO against state-of-the-art preference optimization techniques on two open-ended large language models, Mistral and LLaMA3, across six conditional benchmarks. Our experimental results demonstrate that LMPO effectively controls response length, reduces probability degradation, and outperforms existing approaches. The code is available at <a target="_blank" rel="noopener" href="https://github.com/gengxuli/LMPO">https://github.com/gengxuli/LMPO</a>. </p>
<blockquote>
<p>直接偏好优化（DPO）是一种广泛应用于基于人类反馈的偏好型强化学习（RLHF）的离线算法，通过重新定义奖励函数来提高训练简洁性和稳定性。然而，DPO受到长度偏见、内存效率低下和概率退化等限制的影响。为了解决这些挑战，我们提出了长度控制边距基于偏好优化（LMPO），这是一种更高效和稳健的替代方案。LMPO引入了一个统一的参考模型作为DPO损失的上界，能够更准确地逼近原始优化目标。此外，还采用平均对数概率优化策略，以减小训练和推理阶段之间的差异。LMPO的关键创新之处在于其长度控制边距基础上的损失函数，该函数结合了Bradley-Terry框架。该损失函数既控制响应长度，又扩大了首选和拒绝输出之间的边距。通过这样做，它减轻了接受和丢弃响应的概率退化问题，解决了现有方法的一个重要限制。我们在两个开放的大型语言模型Mistral和LLaMA3上，对LMPO与最先进的偏好优化技术进行了评估，跨越六个条件基准。我们的实验结果表明，LMPO有效地控制了响应长度，减少了概率退化，并优于现有方法。代码可在[<a target="_blank" rel="noopener" href="https://github.com/gengxuli/LMPO%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/gengxuli/LMPO找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14643v2">PDF</a> 18 pages, 3 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Direct Preference Optimization（DPO）在基于人类反馈的偏好强化学习（RLHF）中的局限性，并提出了Length-Controlled Margin-Based Preference Optimization（LMPO）作为更有效率且稳健的替代方案。LMPO引入了一个统一的参考模型作为DPO损失的上限，并采用平均对数概率优化策略来缩小训练和推理阶段的差异。其关键创新在于Length-Controlled Margin-Based损失函数，该损失函数能在Bradley-Terry框架下调节响应长度并扩大首选和拒绝输出之间的间隔，从而解决现有方法的概率退化问题。实验结果表明，LMPO在控制响应长度、减少概率退化方面表现优异，且在两个大型开放式语言模型上优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DPO是强化学习中的常用离线算法，但在处理人类反馈时存在局限性。</li>
<li>LMPO作为一种改进方法被提出，解决了DPO存在的长度偏见、内存效率低和概率退化等问题。</li>
<li>LMPO通过引入统一参考模型和平均对数概率优化策略来提高效率和准确性。</li>
<li>Length-Controlled Margin-Based损失函数是LMPO的关键创新点，它能调节响应长度并扩大首选和拒绝输出间的间隔。</li>
<li>LMPO能有效控制响应长度并减少概率退化。</li>
<li>实验结果显示，LMPO在多个条件基准测试下表现优异，并在大型语言模型上优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14643">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-41a5ba839ad24072749e60b454260e66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e57b54ab3d4abc8f3e723836929216d1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GeLLMO-Generalizing-Large-Language-Models-for-Multi-property-Molecule-Optimization"><a href="#GeLLMO-Generalizing-Large-Language-Models-for-Multi-property-Molecule-Optimization" class="headerlink" title="GeLLMO: Generalizing Large Language Models for Multi-property Molecule   Optimization"></a>GeLLMO: Generalizing Large Language Models for Multi-property Molecule   Optimization</h2><p><strong>Authors:Vishal Dey, Xiao Hu, Xia Ning</strong></p>
<p>Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMs’ potential for molecule optimization, we introduce MuMOInstruct, the first high-quality instruction-tuning dataset specifically focused on complex multi-property molecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLMOs, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that GeLLMOs consistently outperform state-of-the-art baselines. GeLLMOs also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of GeLLMOs as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. MuMOInstruct, models, and code are accessible through <a target="_blank" rel="noopener" href="https://github.com/ninglab/GeLLMO">https://github.com/ninglab/GeLLMO</a>. </p>
<blockquote>
<p>尽管最近有所进展，但大多数分子优化的计算方法仍然局限于单属性或双属性优化任务，并且在扩展到新优化任务时面临可扩展性差和通用性不足的问题。与此同时，大型语言模型（LLM）在新型任务上表现出了显著的跨域泛化能力。为了展示LLM在分子优化方面的潜力，我们推出了MuMOInstruct，这是首个专注于复杂多属性分子优化任务的高质量指令调整数据集。利用MuMOInstruct，我们开发了一系列用于分子优化的指令调优LLM，称为GeLLMOs。在5个领域内的任务和5个跨域任务上的全面评估表明，GeLLMOs始终优于最新基线。GeLLMOs在未见过的任务上也表现出出色的零样本泛化能力，显著优于功能强大的封闭式LLM。这种强大的泛化能力证明了GeLLMOs作为分子优化基础模型的巨大潜力，从而可以在无需资源密集型的重新训练的情况下解决新型优化任务。可以通过<a target="_blank" rel="noopener" href="https://github.com/ninglab/GeLLMO%E8%AE%BF%E9%97%AEMuMOInstruct%E3%80%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/ninglab/GeLLMO访问MuMOInstruct、模型和代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13398v2">PDF</a> Accepted to ACL Main 2025. Vishal Dey and Xiao Hu contributed equally   to this paper</p>
<p><strong>Summary</strong></p>
<p>LLMs在分子优化领域展现出巨大潜力。为应对当前分子优化计算方法的局限，研究团队引入了MuMOInstruct数据集，并基于此开发了GeLLMOs系列模型。该模型在域内和域外任务上均表现优异，体现了强大的零射击能力，展示了其作为分子优化基础模型的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在分子优化领域具有巨大潜力。</li>
<li>当前分子优化计算方法存在局限性，如单&#x2F;双属性优化任务的约束、较差的可扩展性和泛化能力。</li>
<li>MuMOInstruct数据集是首个专注于复杂多属性分子优化任务的高质量指令调整数据集。</li>
<li>GeLLMOs系列模型利用MuMOInstruct数据集进行开发，用于分子优化。</li>
<li>GeLLMOs在域内和域外任务上的表现均超越现有基线。</li>
<li>GeLLMOs展现出强大的零射击泛化能力，在未见任务上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13398">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-377655278f015d543a662871b7eb36a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3294e87e2f86ecf8d19036d6ed0ecc83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abff64608effa7baa27e26b75ca91d65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b7a0e742ec8ce7db7d7ef1fb4a58370.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MUDDFormer-Breaking-Residual-Bottlenecks-in-Transformers-via-Multiway-Dynamic-Dense-Connections"><a href="#MUDDFormer-Breaking-Residual-Bottlenecks-in-Transformers-via-Multiway-Dynamic-Dense-Connections" class="headerlink" title="MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway   Dynamic Dense Connections"></a>MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway   Dynamic Dense Connections</h2><p><strong>Authors:Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan</strong></p>
<p>We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at <a target="_blank" rel="noopener" href="https://github.com/Caiyun-AI/MUDDFormer">https://github.com/Caiyun-AI/MUDDFormer</a> . </p>
<blockquote>
<p>我们提出了多向动态密集（MUDD）连接，这是一种简单有效的方法，可以解决残差连接的局限性，并增强Transformer中的跨层信息流动。与现有具有静态和共享连接权重的密集连接方法不同，MUDD根据每个序列位置和Transformer块的每个独立输入流（查询、键、值或残差）的隐藏状态动态生成连接权重。MUDD连接可以无缝地集成到任何Transformer架构中，以创建MUDDFormer。大量实验表明，MUDDFormer在各种模型架构和规模上的语言建模中显著优于Transformer，实现了使用1.8倍至2.4倍计算的Transformer性能。值得注意的是，MUDDPythia-2.8B在预训练和下游任务中与Pythia-6.9B相匹敌，甚至在五个镜头设置中能够与Pythia-12B竞争，同时只增加了0.23%的参数和0.4%的计算量。JAX和PyTorch的代码以及预训练模型可在<a target="_blank" rel="noopener" href="https://github.com/Caiyun-AI/MUDDFormer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Caiyun-AI/MUDDFormer找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12170v2">PDF</a> Accepted to the 42nd International Conference on Machine Learning   (ICML’25)</p>
<p><strong>Summary</strong><br>     本文提出MUltiway Dynamic Dense（MUDD）连接，这是一种简单有效的方法，解决了残差连接的局限性，增强了Transformer中的跨层信息流动。MUDD动态生成连接权重，取决于每个序列位置和Transformer块的每个独立输入流（查询、键、值或残差）的隐藏状态。MUDD连接可以无缝集成到任何Transformer架构中，创建MUDDFormer。实验表明，MUDDFormer在各种模型架构和规模上的表现均优于Transformer，在语言建模方面达到了使用1.8X-2.4X计算训练的Transformer的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>MUDD连接是一种针对Transformer中残差连接局限性的解决方案，旨在增强跨层信息流动。</p>
</li>
<li><p>MUDD动态生成连接权重，基于每个序列位置和Transformer块内每个独立输入流的隐藏状态。</p>
</li>
<li><p>MUDD连接可无缝集成到任何Transformer架构中，形成MUDDFormer。</p>
</li>
<li><p>MUDDFormer在语言建模方面显著优于传统Transformer，且计算效率更高。</p>
</li>
<li><p>MUDDPythia-2.8B模型在预训练和下游任务中的性能与Pythia-6.9B相匹配，并在五镜头设置中甚至与Pythia-12B相竞争。</p>
</li>
<li><p>MUDD连接的参数增加很少（仅0.23%），计算量增加也很有限（仅0.4%）。</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12170">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c051e65d74f132b4053a2cbb6565a9ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69aae94f5b8967228ccfc079bf87be2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70a53df2faf533d14c35579db724f2c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1853e8a87f5c123c116c6f3f1f75a7d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a5475acc7656dbb0fb28945faab04ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e6a314b68317cd081cbc8a50cb8e750.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation"><a href="#DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation" class="headerlink" title="DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation"></a>DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation</h2><p><strong>Authors:Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang</strong></p>
<p>Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness. </p>
<blockquote>
<p>近期有几项研究尝试结合扩散模型和自回归模型，尝试无离散语音符号生成连续语音表示，但它们经常面临计算负载过大或结果不理想等挑战。在这项工作中，我们提出了“扩散转换器自回归建模（DiTAR）”，这是一种结合语言模型和扩散转换器的基于补丁的自回归框架。这种方法显著提高了自回归模型对连续符号的有效性，并降低了计算需求。DiTAR采用一种分而治之的补丁生成策略，语言模型处理聚合的补丁嵌入，然后扩散转换器根据语言模型的输出生成下一个补丁。对于推理，我们提出将温度定义为在反向扩散ODE中引入噪声的时间点，以平衡多样性和确定性。在广泛的规模分析中，我们还表明DiTAR具有卓越的可扩展性。在零样本语音生成中，DiTAR在稳健性、说话人相似度和自然性方面达到了最新技术水平的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03930v3">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了Diffusion Transformer Autoregressive Modeling（DiTAR）方法，结合了语言模型和扩散转换器，用于基于连续令牌的自动回归生成语音表示。该方法显著提高了自动回归模型对连续令牌的有效性，并降低了计算需求。DiTAR采用分而治之的策略进行补丁生成，利用语言模型处理聚合的补丁嵌入，而扩散转换器则基于语言模型的输出生成下一个补丁。在推理过程中，通过定义温度来平衡多样性和确定性，即在反向扩散ODE过程中引入噪声的时间点。此外，DiTAR在零样本语音生成中实现了出色的性能，在稳健性、说话人相似性和自然性方面达到了最新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiTAR方法结合了语言模型和扩散转换器，用于基于连续令牌的自动回归生成语音表示。</li>
<li>DiTAR采用分而治之的策略，利用语言模型处理补丁嵌入，扩散转换器则生成下一个补丁。</li>
<li>通过定义温度来平衡推理过程中的多样性和确定性。</li>
<li>DiTAR在反向扩散ODE过程中引入噪声的时间点定义为温度，这是一种新的尝试。</li>
<li>DiTAR具有出色的可扩展性，这在其广泛的规模分析中得到证明。</li>
<li>在零样本语音生成方面，DiTAR在稳健性、说话人相似性和自然性方面达到了最新性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e3d0ad6896f09597a191813bcb58fd6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f924470bdfa0f5c21084830453ac89b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee180b77613f77e67b848abf6ac477b1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Permutation-Symmetry-of-Transformers-The-Role-of-Rotation-for-Model-Fusion"><a href="#Beyond-the-Permutation-Symmetry-of-Transformers-The-Role-of-Rotation-for-Model-Fusion" class="headerlink" title="Beyond the Permutation Symmetry of Transformers: The Role of Rotation   for Model Fusion"></a>Beyond the Permutation Symmetry of Transformers: The Role of Rotation   for Model Fusion</h2><p><strong>Authors:Binchi Zhang, Zaiyi Zheng, Zhengzhang Chen, Jundong Li</strong></p>
<p>Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on <a target="_blank" rel="noopener" href="https://github.com/zhengzaiyi/RotationSymmetry">https://github.com/zhengzaiyi/RotationSymmetry</a>. </p>
<blockquote>
<p>深度神经网络（DNNs）参数空间中的对称性已经被证明对各种深度学习应用有益。一个众所周知的例子是多层感知器（MLPs）中的置换对称性，其中对某一层的权重矩阵的行进行置换，并对相邻层应用反向置换，可以得到功能等效的模型。虽然置换对称性完全表征了MLPs的等价集，但其离散性质限制了其在变压器模型中的应用。在本文中，我们引入了旋转对称性，这是一种新型参数空间对称性，通过旋转自注意力层的参数矩阵来推广置换对称性。与置换对称性不同，旋转对称操作在一个连续域中进行，从而显著扩大了变压器的等价集。基于这一属性，我们提出了一种理论上的最优参数匹配算法，作为一个即插即用的模块来增强模型融合。我们使用预训练的变压器模型在多种自然语言处理和视觉任务上评估了我们的方法。实验结果表明，我们的基于旋转对称性的匹配算法极大地提高了模型融合的效果，凸显了参数空间对称性在促进模型融合方面的潜力。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/zhengzaiyi/RotationSymmetry%E3%80%82">https://github.com/zhengzaiyi/RotationSymmetry。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00264v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong><br>深层神经网络参数空间的对称性对于深度学习应用具有重要意义。本文介绍了旋转对称性这一新型参数空间对称性形式，将其应用于变压器，并基于此性质提出了一种理论上的最佳参数匹配算法，用于增强模型融合能力。实验结果表明，该算法能显著提高模型融合性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对称性在深层神经网络参数空间中对深度学习应用有益。</li>
<li>论文提出了旋转对称性这一新型参数空间对称性形式。</li>
<li>旋转对称性能够广泛应用于变压器，是对排列对称性的重要推广。</li>
<li>基于旋转对称性，论文提出了一种理论上的最佳参数匹配算法。</li>
<li>该算法可作为即插即用模块，用于增强模型融合能力。</li>
<li>实验结果表明，该算法能显著提高模型融合性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00264">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a13ca474c944056a7842d47a28493937.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b1e8daae8db8853b69e3aaeb130d6b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ff65f4fa09b4be5b34e280be45ee10c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d17d22e29f6bd90d35989f598131fcf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d71b01f790b7c2dd3e203529c8c4dab6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning"><a href="#QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning" class="headerlink" title="QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning"></a>QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning</h2><p><strong>Authors:Xinyang Tong, Pengxiang Ding, Yiguo Fan, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, Shangke Lyu</strong></p>
<p>This paper addresses the inherent inference latency challenges associated with deploying multimodal large language models (MLLM) in quadruped vision-language-action (QUAR-VLA) tasks. Our investigation reveals that conventional parameter reduction techniques ultimately impair the performance of the language foundation model during the action instruction tuning phase, making them unsuitable for this purpose. We introduce a novel latency-free quadruped MLLM model, dubbed QUART-Online, designed to enhance inference efficiency without degrading the performance of the language foundation model. By incorporating Action Chunk Discretization (ACD), we compress the original action representation space, mapping continuous action values onto a smaller set of discrete representative vectors while preserving critical information. Subsequently, we fine-tune the MLLM to integrate vision, language, and compressed actions into a unified semantic space. Experimental results demonstrate that QUART-Online operates in tandem with the existing MLLM system, achieving real-time inference in sync with the underlying controller frequency, significantly boosting the success rate across various tasks by 65%. Our project page is <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a>. </p>
<blockquote>
<p>本文旨在解决在四足视觉-语言-行动（QUAR-VLA）任务中部署多模态大型语言模型（MLLM）所面临的固有推理延迟挑战。我们的调查发现，传统的参数减少技术最终会损害语言基础模型在行动指令调整阶段的性能，使其不适合此目的。我们引入了一种新型的无延迟四足MLLM模型，名为QUART-Online，旨在提高推理效率，同时不降低语言基础模型的性能。通过引入行动片段离散化（ACD），我们压缩了原始行动表示空间，将连续的行动值映射到一组较小的离散代表向量上，同时保留关键信息。随后，我们对MLLM进行微调，以将视觉、语言和压缩后的行动整合到统一的语义空间中。实验结果表明，QUART-Online与现有的MLLM系统协同工作，实现与底层控制器频率同步的实时推理，在各种任务中的成功率提高了65%。我们的项目页面是<a target="_blank" rel="noopener" href="https://quart-online.github.io./">https://quart-online.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15576v5">PDF</a> Accepted to ICRA 2025; Github page: <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a></p>
<p><strong>Summary</strong><br>     本文探讨在四足机器人视觉语言动作（QUAR-VLA）任务中部署多模态大型语言模型（MLLM）时面临的固有推理延迟挑战。研究发现在动作指令调整阶段，传统参数缩减技术会损害语言基础模型性能，不适合此场景。因此，本文提出了一种新型的无延迟四足MLLM模型——QUART-Online，旨在提高推理效率而不损害语言基础模型性能。通过引入动作块离散化（ACD），压缩原始动作表示空间，将连续动作值映射到一组较小的离散代表向量上，同时保留关键信息。然后微调MLLM，将视觉、语言和压缩动作集成到一个统一的语义空间中。实验结果表明，QUART-Online与现有MLLM系统协同工作，实现了与底层控制器频率同步的实时推理，在各项任务中的成功率提高了65%。项目网址：<a target="_blank" rel="noopener" href="https://quart-online.github.io/">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究解决了在四足机器人视觉语言动作任务中部署多模态大型语言模型的推理延迟问题。</li>
<li>传统的参数缩减技术在动作指令调整阶段对语言基础模型的性能有负面影响。</li>
<li>QUART-Online模型旨在提高推理效率，同时不损害语言基础模型的性能。</li>
<li>通过引入动作块离散化（ACD），压缩动作表示空间，映射到离散代表向量上。</li>
<li>QUART-Online通过微调集成视觉、语言和压缩动作的MLLM，形成统一语义空间。</li>
<li>实验结果显示，QUART-Online与现有系统协同工作，实现实时推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9d55fbfd356475ab4d69e26b44e407f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d30d5b758629aee84aab79c5afdf7dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d516c516a55f5f18abacd645ad59b53b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b154fe6f439d338b4f41959c7d03f3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5ece0295520d590c06c0e7c317c48ae.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Core-Context-Aware-Transformers-for-Long-Context-Language-Modeling"><a href="#Core-Context-Aware-Transformers-for-Long-Context-Language-Modeling" class="headerlink" title="Core Context Aware Transformers for Long Context Language Modeling"></a>Core Context Aware Transformers for Long Context Language Modeling</h2><p><strong>Authors:Yaofo Chen, Zeng You, Shuhai Zhang, Haokun Li, Yirui Li, Yaowei Wang, Mingkui Tan</strong></p>
<p>Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods. </p>
<blockquote>
<p>基于Transformer的大型语言模型（LLM）在大量任务中取得了显著的成功，这主要归功于自注意力机制。自注意力机制要求一个标记（token）将所有先前的标记都作为上下文来计算注意力。然而，当上下文长度L变得非常大（例如128K）时，上下文中潜在冗余信息的数量往往会增加。冗余的上下文不仅阻碍建模表示性能，还会产生不必要的计算和存储开销。在本文中，我们提出了一种即插即用的核心上下文感知（CCA）注意力，用于高效的长上下文建模，它包括两个互补模块：1）全局性池化模块对输入标记进行分组，并根据它们的重要性动态地将每个组压缩成一个核心标记。通过这种方式，我们的方法能够自动聚焦和强化核心上下文，同时在学习过程中减少冗余，从而实现有效的长期依赖建模。2）局部性保持模块结合了相邻标记，以保持局部上下文，以实现详细的表示。值得注意的是，我们的CCA注意力能够以最小的微调成本替换现有LLM中的自注意力模块。大量的实验结果证明，我们的方法在长上下文建模和计算效率方面都优于最新的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12465v2">PDF</a> Accepted for publication at ICML 2025</p>
<p><strong>Summary</strong>：基于Transformer的大型语言模型（LLM）在处理长文本时面临挑战，如冗余信息和计算效率问题。本文提出了一种即插即用的核心上下文感知（CCA）注意力机制，包括全局感知池化模块和局部保持模块，旨在高效建模长文本上下文。实验结果表明，该方法在长文本建模和计算效率方面优于现有方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Transformer-based LLMs在处理长文本时面临的挑战包括冗余信息和计算效率问题。</li>
<li>核心上下文感知（CCA）注意力机制包括全局感知池化模块和局部保持模块。</li>
<li>全局感知池化模块通过动态压缩输入标记来强化核心上下文，减少冗余信息。</li>
<li>局部保持模块保留邻近标记以进行详细的局部上下文表示。</li>
<li>CCA-Attention能够替换现有LLM中的自注意力模块，且微调成本低。</li>
<li>实验结果表明，该方法在长文本建模和计算效率方面表现出优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12465">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77ccd945462e4add5b164282cd7371b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40b602f47d28b43700efe26f86ca568c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87dad71f9789ae4f7588a8ec09e7663a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4f5884df1f8a0703532a02667888f00.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Visatronic-A-Multimodal-Decoder-Only-Model-for-Speech-Synthesis"><a href="#Visatronic-A-Multimodal-Decoder-Only-Model-for-Speech-Synthesis" class="headerlink" title="Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis"></a>Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis</h2><p><strong>Authors:Akshita Gupta, Tatiana Likhomanenko, Karren Dai Yang, Richard He Bai, Zakaria Aldeneh, Navdeep Jaitly</strong></p>
<p>The rapid progress of foundation models and large language models (LLMs) has fueled significantly improvement in the capabilities of machine learning systems that benefit from mutlimodal input data. However, existing multimodal models are predominantly built on top of pre-trained LLMs, which can limit accurate modeling of temporal dependencies across other modalities and thus limit the model’s ability to jointly process and leverage multimodal inputs. To specifically investigate the alignment of text, video, and speech modalities in LLM-style (decoder-only) models, we consider a simplified multimodal generation task, Video-Text to Speech (VTTS): speech generation conditioned on both its corresponding text and video of talking people. The ultimate goal is to generate speech that not only follows the text but also aligns temporally with the video and is consistent with the facial expressions. In this paper, we first introduce Visatronic, a unified multimodal decoder-only transformer model that adopts an LLM-style architecture to embed visual, textual, and speech inputs into a shared subspace, treating all modalities as temporally aligned token streams. Next, we carefully explore different token mixing strategies to understand the best way to propagate information from the steps where video and text conditioning is input to the steps where the audio is generated. We extensively evaluate Visatronic on the challenging VoxCeleb2 dataset and demonstrate zero-shot generalization to LRS3, where Visatronic, trained on VoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only on LRS3, which report a 21.4% WER. Additionally, we propose a new objective metric, TimeSync, specifically designed to measure phoneme-level temporal alignment between generated and reference speech, further ensuring synchronization quality. Demo: <a target="_blank" rel="noopener" href="https://apple.github.io/visatronic-demo/">https://apple.github.io/visatronic-demo/</a> </p>
<blockquote>
<p>随着基础模型和大语言模型（LLM）的快速发展，受益于此能够处理多模态输入数据的机器学习系统的能力得到了极大的提升。然而，现有的多模态模型大多建立在预训练的语言模型基础上，这限制了其他模态间时间依赖性的准确建模，从而也限制了模型对多模态输入的联合处理和应用能力。为了专门研究文本、视频和语音模态在LLM风格（仅解码器）模型中的对齐问题，我们考虑了一个简化的多模态生成任务——视频文本转语音（VTTS）：语音生成既依赖于相应的文本，又依赖于人们的谈话视频。最终目标是生成不仅遵循文本内容，而且与视频在时间上映射对齐、与面部表情一致的语音。在本文中，我们首先介绍了Visatronic，这是一个统一的多模态仅解码器transformer模型，它采用LLM风格的架构将视觉、文本和语音输入嵌入到共享的子空间中，将所有模态视为时间对齐的令牌流。接下来，我们仔细探索了不同的令牌混合策略，以了解从视频和文本调节步骤向生成音频步骤传播信息的最佳方式。我们对VoxCeleb2数据集进行了全面的评估，并展示了Visatronic在LRS3上的零样本泛化能力。其中，Visatronic在VoxCeleb2上训练后实现了4.5%的WER（词错误率），优于仅在LRS3上训练的先前最佳方法（报告了21.4%的WER）。此外，我们还提出了一种新的客观度量标准TimeSync，专门用于测量生成语音和参考语音之间的音素级时间对齐，进一步确保同步质量。演示地址：<a target="_blank" rel="noopener" href="https://apple.github.io/visatronic-demo/%E3%80%82">https://apple.github.io/visatronic-demo/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17690v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着基础模型与大型语言模型（LLM）的快速发展，多模态输入数据在机器学习系统中的应用能力得到了显著提升。然而，现有的多模态模型主要依赖于预训练的LLM，这限制了跨其他模态的时间依赖性的准确建模，从而限制了模型对多模态输入的联合处理与利用能力。为了研究文本、视频和语音模态在LLM风格模型中的对齐问题，本文提出了一种简化的多模态生成任务——视频文本转语音（VTTS）。本文介绍了一种统一的多模态解码器模型Visatronic，该模型采用LLM风格架构嵌入视觉、文本和语音输入到共享子空间，并将所有模态视为时间对齐的令牌流。在具有挑战性的VoxCeleb2数据集上评估Visatronic，其在训练条件下实现零时差、超优表现，展示出色的时间同步能力。同时提出一个新的客观指标TimeSync，专门用于测量生成语音与参考语音之间的音素级时间对齐程度，确保同步质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态模型的进步受益于大型语言模型（LLM）的发展，能够处理多种类型的输入数据。</li>
<li>现有模型在处理多模态输入时面临时间依赖性建模的挑战。</li>
<li>视频文本转语音（VTTS）任务旨在研究文本、视频和语音模态的对齐问题。</li>
<li>Visatronic模型采用LLM风格架构处理多模态输入，将其嵌入到共享子空间。</li>
<li>Visatronic在VoxCeleb2数据集上表现出卓越性能，通过零时差泛化实现优越的时间同步能力。</li>
<li>提出新的客观指标TimeSync，用于评估生成语音与参考语音之间的音素级时间对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-45a6c6dfa6b81fd005b779083e2a519a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbe22e9665ec8b0309dfaa8a708b8550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02c056bd02f973ea4e3070664b7e8576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a90e2243467d0f786c7463ce9e7a5d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acef3515d5a164a7d5b20ebe94e1f514.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4f4fb89cfe947b46d5aa0b772b4f3ea.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DRPruning-Efficient-Large-Language-Model-Pruning-through-Distributionally-Robust-Optimization"><a href="#DRPruning-Efficient-Large-Language-Model-Pruning-through-Distributionally-Robust-Optimization" class="headerlink" title="DRPruning: Efficient Large Language Model Pruning through   Distributionally Robust Optimization"></a>DRPruning: Efficient Large Language Model Pruning through   Distributionally Robust Optimization</h2><p><strong>Authors:Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jing Li, Min Zhang, Zhaopeng Tu</strong></p>
<p>Large language models (LLMs) deliver impressive results but face challenges from increasing model sizes and computational costs. Structured pruning reduces model size and speeds up inference but often causes uneven degradation across domains, leading to biased performance. To address this, we propose DRPruning, a method that dynamically adjusts the data distribution during training to restore balanced performance across heterogeneous and multi-tasking data. Experiments in monolingual and multilingual settings show that DRPruning surpasses similarly sized models in both pruning and continued pretraining over perplexity, downstream tasks, and instruction tuning. Further analysis demonstrates the robustness of DRPruning towards various domains and distribution shifts. Furthermore, DRPruning can determine optimal reference losses and data ratios automatically, suggesting potential for broader applications. Code and scripts are available at <a target="_blank" rel="noopener" href="https://github.com/hexuandeng/DRPruning">https://github.com/hexuandeng/DRPruning</a>. </p>
<blockquote>
<p>大型语言模型（LLM）虽然取得了令人印象深刻的结果，但面临着模型尺寸增大和计算成本增加的挑战。结构化剪枝可以减少模型大小并加速推理，但往往会导致不同领域的性能不均匀下降，从而导致性能偏见。为了解决这一问题，我们提出了DRPruning方法，该方法在训练过程中动态调整数据分布，以恢复异构多任务数据的平衡性能。在单语和多语环境下的实验表明，DRPruning在剪枝和持续预训练方面的困惑度、下游任务和指令调整方面都超越了类似规模的模型。进一步的分析表明，DRPruning对不同领域和分布变化具有稳健性。此外，DRPruning还可以自动确定最佳参考损失和数据比率，表明其更广泛的应用潜力。相关代码和脚本可访问<a target="_blank" rel="noopener" href="https://github.com/hexuandeng/DRPruning%E3%80%82">https://github.com/hexuandeng/DRPruning。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14055v2">PDF</a> Accepted by ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>LLM面临模型尺寸增大与计算成本上升的挑战。结构化剪枝虽可减少模型尺寸并加速推理，但常导致跨领域性能不均衡的下降。本研究提出DRPruning方法，在训练过程中动态调整数据分布，以恢复在异质多任务数据上的平衡性能。实验表明，DRPruning在剪枝和持续预训练方面的表现均超越同类模型，提高了困惑度、下游任务与指令微调的效果。此外，DRPruning可自动确定最佳参考损失和数据比率，具有广泛的应用潜力。相关代码和脚本可在hexuandeng&#x2F;DRPruning获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM面临模型尺寸增大和计算成本上升的难题。</li>
<li>结构化剪枝可减少模型尺寸并加速推理，但可能导致跨领域性能不均衡。</li>
<li>DRPruning方法通过动态调整训练过程中的数据分布来恢复跨领域的平衡性能。</li>
<li>DRPruning在剪枝和持续预训练方面的表现超越同类模型。</li>
<li>DRPruning可提高困惑度、下游任务与指令微调的效果。</li>
<li>DRPruning可自动确定最佳参考损失和数据比率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14055">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ace7566d64b3e152a1278dd0f02667a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86b4f90fe9360074fdadceea2690d027.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61ba9a81870a6d2f9b2a8dad66fc4463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f34313922f98ded87a7857e75908c02d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e021345d94d4ed5095bbb9cce3aba6fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f49af4c605e92036d35c924eb40c818a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee0e2bcfb4da3538951888acfb18665f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fd0efd456211b3e4cbed584818f73e4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Controllable-Context-Sensitivity-and-the-Knob-Behind-It"><a href="#Controllable-Context-Sensitivity-and-the-Knob-Behind-It" class="headerlink" title="Controllable Context Sensitivity and the Knob Behind It"></a>Controllable Context Sensitivity and the Knob Behind It</h2><p><strong>Authors:Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell</strong></p>
<p>When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context (Paris is in England) and a question (Where is Paris?); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either France or England). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a model’s performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior. </p>
<blockquote>
<p>在做出预测时，语言模型必须在依赖上下文和先前知识之间进行权衡。选择模型对上下文的敏感程度是一个基本功能，因为这使模型能够在诸如检索增强生成和问答等任务中表现出色。在本文中，我们寻找一个控制这种敏感性的旋钮，以确定语言模型是从上下文还是先前知识中得出答案。为了指导这次搜索，我们设计了一个可控上下文敏感度的任务。在此任务中，我们首先向模型提供上下文（例如“巴黎在英国”）和问题（“巴黎在哪里？”）；然后指示模型使用其先前知识或上下文知识，并评估它是否能为两种意图（法国或英国）生成正确答案。在此任务上进行微调后，Llama-3.1、Mistral-v0.3和Gemma-2的指令调整版本可以高准确率（85-95%）地解决此问题。分析这些高性能模型，我们使用一种新型线性时间算法来缩小对上下文敏感度重要的层级。然后，在每个模型中，我们在单层中识别出一个一维子空间，该子空间能够编码模型是遵循上下文还是先前知识。有趣的是，虽然我们在已微调过的模型中识别了这个子空间，但我们发现该子空间不仅在那一模型中有效，而且在那个模型家族的非指令和基线模型中也是如此。最后，我们发现在该子空间中，模型的表现与其区分上下文同意答案和忽略上下文答案的能力之间存在强烈的相关性。这些结果表明，单个子空间促进了模型在上下文和先前知识之间的选择，暗示了一个控制此行为的简单基本机制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07404v3">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>摘要</strong></p>
<p>本文探索了语言模型中控制语境敏感度的机制。文章提出了一个任务来指导寻找控制语境敏感度的旋钮，并发现某些模型可以在这个任务上表现出高准确率。通过在这些模型中识别出特定的单层中的一维子空间，该子空间能决定模型是遵循语境还是先验知识。这种子空间在精细调整的模型和非精细调整模型中均有效，暗示了模型在二者之间的选择机制。此外，模型的性能与其在这个子空间中区分遵循语境和忽略语境答案的能力之间存在强烈的相关性。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>语言模型需要在上下文和先验知识之间做出权衡，选择对上下文的敏感度是一个基本功能。</li>
<li>设计了一个任务来寻找控制语境敏感度的旋钮，该任务要求模型在给定错误语境的情况下正确回答问题。</li>
<li>发现某些语言模型在这个任务上表现出高准确率，并能通过分析模型中的特定层来识别关键子空间。</li>
<li>这个子空间决定了模型是遵循上下文还是先验知识，并在不同的模型中均有效。</li>
<li>模型性能与其区分遵循语境和忽略语境答案的能力之间存在强烈的相关性。这些结果揭示了语言模型在权衡上下文和先验知识时的基本机制。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07404">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-043096a901319531036443f9bafb94b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2687419b9cb79a95096dc67514c2eda3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SLiM-One-shot-Quantization-and-Sparsity-with-Low-rank-Approximation-for-LLM-Weight-Compression"><a href="#SLiM-One-shot-Quantization-and-Sparsity-with-Low-rank-Approximation-for-LLM-Weight-Compression" class="headerlink" title="SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for   LLM Weight Compression"></a>SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for   LLM Weight Compression</h2><p><strong>Authors:Mohammad Mozaffari, Amir Yazdanbakhsh, Maryam Mehri Dehnavi</strong></p>
<p>Conventional model compression techniques for LLMs address high memory consumption and slow inference challenges but typically require computationally expensive retraining to preserve accuracy. In contrast, one-shot compression methods eliminate retraining cost, but struggle to achieve accuracy comparable to dense models. This paper presents SLIM, a new one-shot compression framework that holistically integrates hardware-friendly quantization, sparsity, and low-rank approximation into a unified process. First, we formulate the quantization process using a probabilistic approach (SLIM-Quant) that enables us to apply uniform quantization. Then, we use an existing one-shot pruning method to apply semi-structured sparsity on top of the quantized weights. Finally, to compensate for the introduced aggregated quantization and sparsity error, we use a novel saliency function with unique invertible and additive features that enables us to mathematically compute the value of low-rank adapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4 sparsity with 4-bit weight quantization, outperforming prior methods. Models compressed with SLIM achieve up to 4.3x and 3.8x on Nvidia RTX3060 and A100 GPUs, respectively. Additionally, they achieve up to 0.23x end-to-end memory reduction in comparison to their dense counterparts. We also propose an optional PEFT recipe that further improves accuracy by up to 1.66% (LLaMA-2-13B) compared to SLIM without fine-tuning. </p>
<blockquote>
<p>传统的针对大型语言模型（LLMs）的模型压缩技术解决了高内存消耗和推理速度慢的挑战，但通常需要计算昂贵的重新训练来保持准确性。相比之下，一次性压缩方法消除了重新训练的成本，但难以实现与密集模型相当的准确性。本文提出了SLIM，一种新的一次性压缩框架，它全面地将硬件友好的量化、稀疏性和低秩逼近整合到一个统一的过程中。首先，我们使用概率方法（SLIM-Quant）来制定量化过程，这使我们能够应用均匀量化。然后，我们在量化权重之上使用现有的一次性修剪方法，应用半结构化稀疏性。最后，为了弥补引入的聚合量化和稀疏性误差，我们使用一种具有独特可逆和可加特性的新型显著性函数，它使我们能够数学地计算低秩适配器的价值。SLIM在2:4的稀疏性下，通过4位权重量化，提高了高达5.66%（LLaMA-2-7B）的模型精度，优于先前的方法。使用SLIM压缩的模型在Nvidia RTX3060和A100 GPU上分别实现了高达4.3倍和3.8倍的加速。此外，与它们的密集对应模型相比，它们还实现了高达0.23倍的端到端内存减少。我们还提出了一种可选的PEFT配方，与没有微调过的SLIM相比，它进一步提高了高达1.66%（LLaMA-2-13B）的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09615v3">PDF</a> Published at Proceedings of the 42 nd International Conference on   Machine Learning (ICML 2025)</p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种名为SLIM的新的一站式压缩框架，集成了硬件友好的量化、稀疏性和低秩逼近。通过概率方法实现均匀量化，利用现有的一站式修剪方法实现半结构化稀疏性，并使用新的显著性函数计算低秩适配器的值以补偿量化与稀疏性误差。SLIM在保持模型准确性的同时，实现了LLM的高压缩率，并在Nvidia RTX3060和A100 GPUs上实现了高加速比。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLIM框架实现了LLM的一站式压缩，集成了硬件友好的量化、稀疏性和低秩逼近。</li>
<li>通过概率方法实现均匀量化，降低模型内存消耗。</li>
<li>利用现有的一站式修剪方法实现半结构化稀疏性，进一步提高模型压缩效率。</li>
<li>通过显著性函数计算低秩适配器的值，以补偿量化与稀疏性误差，保持模型准确性。</li>
<li>SLIM提高了模型准确性，并在特定配置下实现了高达5.66%的准确度提升。</li>
<li>压缩后的模型在Nvidia RTX3060和A100 GPUs上实现了高达4.3x和3.8x的加速比。</li>
<li>相比密集模型，压缩后的模型实现了高达0.23x的端到端内存减少。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09615">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d51f6d8ca544ef8f598e7918b0d5a55e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11fcffca373c3d0bd5671b894ab94265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a0e2f3d41f3dba5c7908bafe7bff971.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4f58b4f70a203b21f1653c383eae84b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Structure-Enhanced-Protein-Instruction-Tuning-Towards-General-Purpose-Protein-Understanding-with-LLMs"><a href="#Structure-Enhanced-Protein-Instruction-Tuning-Towards-General-Purpose-Protein-Understanding-with-LLMs" class="headerlink" title="Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose   Protein Understanding with LLMs"></a>Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose   Protein Understanding with LLMs</h2><p><strong>Authors:Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang</strong></p>
<p>Proteins, as essential biomolecules, play a central role in biological processes, including metabolic reactions and DNA replication. Accurate prediction of their properties and functions is crucial in biological applications. Recent development of protein language models (pLMs) with supervised fine tuning provides a promising solution to this problem. However, the fine-tuned model is tailored for particular downstream prediction task, and achieving general-purpose protein understanding remains a challenge. In this paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT) framework to bridge this gap. Our approach incorporates a novel structure-aware module into pLMs to enrich their structural knowledge, and subsequently integrates these enhanced pLMs with large language models (LLMs) to advance protein understanding. In this framework, we propose a novel instruction tuning pipeline. First, we warm up the enhanced pLMs using contrastive learning and structure denoising. Then, caption-based instructions are used to establish a basic understanding of proteins. Finally, we refine this understanding by employing a mixture of experts (MoEs) to capture more complex properties and functional information with the same number of activated parameters. Moreover, we construct the largest and most comprehensive protein instruction dataset to date, which allows us to train and evaluate the general-purpose protein understanding model. Extensive experiments on both open-ended generation and closed-set answer tasks demonstrate the superior performance of SEPIT over both closed-source general LLMs and open-source LLMs trained with protein knowledge. </p>
<blockquote>
<p>蛋白质作为重要的生物分子，在包括代谢反应和DNA复制等生物过程中扮演着核心角色。对其属性和功能的准确预测在生物应用中是至关重要的。最近通过监督微调开发的蛋白质语言模型（pLMs）为解决这个问题提供了有前景的解决方案。然而，微调模型是针对特定的下游预测任务定制的，实现通用蛋白质理解仍然是一个挑战。在本文中，我们介绍了结构增强蛋白质指令微调（SEPIT）框架来弥补这一差距。我们的方法将一个新的结构感知模块纳入pLMs中，以丰富其结构知识，随后将这些增强的pLMs与大型语言模型（LLMs）集成，以推进蛋白质理解。在这个框架下，我们提出了一种新的指令微调流程。首先，我们使用对比学习和结构去噪来预热增强的pLMs。然后，基于描述的指令用于建立对蛋白质的基本理解。最后，我们通过使用混合专家系统（MoEs）来捕捉更多复杂的属性和功能信息，以完善这种理解，同时保持相同的激活参数数量。此外，我们构建了迄今为止最大、最全面的蛋白质指令数据集，用于训练和评估通用蛋白质理解模型。在开放生成任务和封闭答案任务上的大量实验表明，SEPIT在封闭源通用LLMs和用蛋白质知识训练的开源LLMs上的表现均优于前者。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03553v3">PDF</a> Accepted by KDD2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Structure-Enhanced Protein Instruction Tuning（SEPIT）框架，该框架旨在弥补蛋白质语言模型（pLMs）在通用蛋白质理解方面的不足。SEPIT框架结合结构感知模块来增强pLMs的结构知识，并与大型语言模型（LLMs）集成，以促进蛋白质理解。通过对比实验，证明SEPIT框架在开放和封闭答案任务上的表现均优于封闭和开源的LLMs。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>蛋白质在生物过程中起着关键作用，包括代谢反应和DNA复制。对其属性和功能的准确预测在生物应用中至关重要。</li>
<li>蛋白质语言模型（pLMs）的近期发展为预测蛋白质属性提供了有前途的解决方案，但实现通用蛋白质理解仍存在挑战。</li>
<li>SEPIT框架结合了结构感知模块来增强pLMs的结构知识，并整合到大型语言模型（LLMs）中，旨在提高蛋白质理解。</li>
<li>SEPIT采用一种新的指令调整管道，使用对比学习和结构去噪来预热增强pLMs，并通过基于字幕的指令建立基本的蛋白质理解。</li>
<li>MoEs（混合专家系统）用于捕捉更复杂的属性和功能信息，同时保持激活参数数量不变。</li>
<li>构建了一个迄今为止最大和最全面的蛋白质指令数据集，用于训练和评估通用蛋白质理解模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03553">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cca895d4f0df71d1831f73301c213fbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e95351837838a2da24a102c320dcd4e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54460238602334516742b5d6fc815a1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-025f1207f5188abab195a429f73d88ac.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Training-Nonlinear-Transformers-for-Chain-of-Thought-Inference-A-Theoretical-Generalization-Analysis"><a href="#Training-Nonlinear-Transformers-for-Chain-of-Thought-Inference-A-Theoretical-Generalization-Analysis" class="headerlink" title="Training Nonlinear Transformers for Chain-of-Thought Inference: A   Theoretical Generalization Analysis"></a>Training Nonlinear Transformers for Chain-of-Thought Inference: A   Theoretical Generalization Analysis</h2><p><strong>Authors:Hongkang Li, Songtao Lu, Pin-Yu Chen, Xiaodong Cui, Meng Wang</strong></p>
<p>Chain-of-Thought (CoT) is an efficient prompting method that enables the reasoning ability of large language models by augmenting the query using multiple examples with multiple intermediate steps. Despite the empirical success, the theoretical understanding of how to train a Transformer to achieve the CoT ability remains less explored. This is primarily due to the technical challenges involved in analyzing the nonconvex optimization on nonlinear attention models. To the best of our knowledge, this work provides the first theoretical study of training Transformers with nonlinear attention to obtain the CoT generalization capability so that the resulting model can inference on unseen tasks when the input is augmented by examples of the new task. We first quantify the required training samples and iterations to train a Transformer model towards CoT ability. We then prove the success of its CoT generalization on unseen tasks with distribution-shifted testing data. Moreover, we theoretically characterize the conditions for an accurate reasoning output by CoT even when the provided reasoning examples contain noises and are not always accurate. In contrast, in-context learning (ICL), which can be viewed as one-step CoT without intermediate steps, may fail to provide an accurate output when CoT does. These theoretical findings are justified through experiments. </p>
<blockquote>
<p>链式思维（Chain-of-Thought，CoT）是一种高效的提示方法，它通过利用多个示例和多个中间步骤来扩展查询，从而激发大型语言模型的推理能力。尽管在实证上取得了成功，但如何训练变压器（Transformer）以实现CoT能力的理论研究仍然有待探索。这主要是因为分析非线性注意力模型上的非凸优化所涉及的技术挑战。据我们所知，这项工作首次对训练变压器与获得CoT泛化能力的非线性注意力进行了理论研究，以便在输入通过新任务的示例增强时，模型可以对未见过的任务进行推断。我们首先量化训练变压器模型实现CoT能力所需的训练样本和迭代次数。然后，我们通过在具有分布偏移的测试数据上证明其CoT泛化的成功。此外，我们对以下情况进行了理论描述：即使提供的推理示例包含噪声并不总是准确，CoT仍能产生准确的推理输出。相比之下，上下文学习（In-Context Learning，ICL）可以看作是没有中间步骤的一步式CoT，当CoT存在时，它可能无法提供准确的输出。这些理论发现通过实验得到了证实。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02167v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>此文本详细介绍了Chain-of-Thought（CoT）作为一种大型语言模型的提示方法，通过多个示例和多个中间步骤增强查询来实现推理能力。尽管在实践中取得了成功，但关于如何训练Transformer以实现CoT能力的理论研究仍然较少。本文首次研究了训练具有非线性注意力的Transformer以获取CoT泛化能力的方法，使模型能够在增强新任务输入后进行推理。文章通过理论分析和实验验证了CoT在处理未见任务以及面对分布变化测试数据时的表现，并进一步探讨了当提供的推理示例包含噪声并不完全准确时，CoT仍能准确推理的条件。此外，与被视为无中间步骤的一步式CoT（In-context learning，ICL）相比，CoT在某些情况下可以提供更准确的输出。通过一系列实验证实了这些理论发现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Chain-of-Thought (CoT) 是一种通过多个示例和中间步骤增强查询来启发大型语言模型推理能力的有效提示方法。</li>
<li>目前对于如何训练Transformer以实现CoT能力的理论研究相对较少，主要由于涉及非线性注意力模型的非凸优化分析技术挑战。</li>
<li>此研究首次探讨了训练具有非线性注意力的Transformer以获取CoT泛化能力的方法，使其能够在增强新任务输入后进行推理。</li>
<li>文章理论分析了训练样本数量和迭代次数的要求，以训练具备CoT能力的Transformer模型。</li>
<li>通过理论分析证实了CoT在未见任务上的泛化成功，特别是在处理分布变化的测试数据时。</li>
<li>当提供的推理示例包含噪声并不完全准确时，理论上描述了CoT仍能准确推理的条件。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02167">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b896d0970cd94b2002c572c0991ad0bd.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Understand-Symbolic-Graphics-Programs"><a href="#Can-Large-Language-Models-Understand-Symbolic-Graphics-Programs" class="headerlink" title="Can Large Language Models Understand Symbolic Graphics Programs?"></a>Can Large Language Models Understand Symbolic Graphics Programs?</h2><p><strong>Authors:Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf</strong></p>
<p>Against the backdrop of enthusiasm for large language models (LLMs), there is a growing need to scientifically assess their capabilities and shortcomings. This is nontrivial in part because it is difficult to find tasks which the models have not encountered during training. Utilizing symbolic graphics programs, we propose a domain well-suited to test multiple spatial-semantic reasoning skills of LLMs. Popular in computer graphics, these programs procedurally generate visual data. While LLMs exhibit impressive skills in general program synthesis and analysis, symbolic graphics programs offer a new layer of evaluation: they allow us to test an LLM’s ability to answer semantic questions about the images or 3D geometries without a vision encoder. To semantically understand the symbolic programs, LLMs would need to possess the ability to “imagine” and reason how the corresponding graphics content would look with only the symbolic description of the local curvatures and strokes. We use this task to evaluate LLMs by creating a large benchmark for the semantic visual understanding of symbolic graphics programs, built procedurally with minimal human effort. Particular emphasis is placed on transformations of images that leave the image level semantics invariant while introducing significant changes to the underlying program. We evaluate commercial and open-source LLMs on our benchmark to assess their ability to reason about visual output of programs, finding that LLMs considered stronger at reasoning generally perform better. Lastly, we introduce a novel method to improve this ability – Symbolic Instruction Tuning (SIT), in which the LLM is finetuned with pre-collected instruction data on symbolic graphics programs. Interestingly, we find that SIT not only improves LLM’s understanding on symbolic programs, but it also improves general reasoning ability on various other benchmarks. </p>
<blockquote>
<p>在大型语言模型（LLM）备受关注的大背景下，科学评估其能力和短板的需求日益增加。这在一定程度上是非平凡的，因为很难找到模型在训练期间未曾遇到过的任务。我们利用符号图形程序，提出了一个非常适合测试LLM多方空间语义推理能力的领域。这些程序在电脑制图领域很受欢迎，可以程序性地生成视觉数据。虽然LLM在一般程序合成和分析方面表现出令人印象深刻的技能，但符号图形程序提供了一层新的评估：它们允许我们测试LLM回答关于图像或3D几何的语义问题的能力，而无需使用视觉编码器。为了语义地理解符号程序，LLM需要拥有仅凭符号描述局部曲率和笔触来“想象”并推理相应图形内容外观的能力。我们通过创建用于符号图形程序语义视觉理解的大型基准测试来评估LLM，该测试采用程序构建，几乎不需要人工参与。我们特别重视图像转换任务，该任务在保持图像级别语义不变的同时，为底层程序引入了重大变化。我们在基准测试上评估了商用和开源的LLM，以评估它们对程序视觉输出的推理能力，发现被认为在推理方面更出色的LLM通常表现更好。最后，我们引入了一种改进这种能力的新方法——符号指令微调（SIT），对LLM进行微调，使其适应预先收集的符号图形程序的指令数据。有趣的是，我们发现SIT不仅提高了LLM对符号程序的理解能力，而且还提高了其在其他基准测试中的一般推理能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08313v4">PDF</a> ICLR 2025 Spotlight (v4: 47 pages, 26 figures, project page:   <a target="_blank" rel="noopener" href="https://sgp-bench.github.io/">https://sgp-bench.github.io/</a>)</p>
<p><strong>摘要</strong></p>
<p>本文探讨了大型语言模型（LLM）的科学评估问题，提出利用符号图形程序领域测试LLM的空间语义推理能力。符号图形程序能够程序化生成视觉数据，为评估LLM提供了新的层面。LLM在该任务中展现出强大的程序合成和分析能力，需要通过理解符号程序来回答关于图像或3D几何的语义问题。为此，文章创建了一个大型基准测试，用于评估LLM对符号图形程序的理解能力，并强调图像语义不变而底层程序发生显著变化的转换。文章评估了商业和开源LLM在此基准测试上的表现，并提出一种新方法——符号指令微调（SIT）来提高LLM的理解能力。实验发现，SIT不仅能提高LLM对符号程序的理解，还能提高其在其他基准测试上的推理能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM的空间语义推理能力需要通过科学评估，符号图形程序领域是测试此能力的合适选择。</li>
<li>符号图形程序能够程序化生成视觉数据，为评估LLM提供了新层面。</li>
<li>LLM在程序合成和分析方面表现出强大能力，需要通过理解符号程序来回答关于图像或3D几何的语义问题。</li>
<li>创建了一个大型基准测试，用于评估LLM对符号图形程序语义视觉理解的能力。</li>
<li>评估了商业和开源LLM在基准测试上的表现，发现推理能力较强的LLM表现更好。</li>
<li>引入了一种新方法——符号指令微调（SIT），以提高LLM对符号程序的理解及推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08313">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f07d4051dd17a0c201da7227509f2a60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-683d9ce159b5a9451d604dd45ead4141.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7873ff9545f61f2c24dc4f7c1d05271.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc945c5db6e367ff9455f95a06278e73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4255be007f1df3fd623edb627c42b911.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e12a771a2af2be34b5140bcb79ca7cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ff1e9103bd9a95db46a2f19ab99ba51.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="OmniBal-Towards-Fast-Instruction-Tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance"><a href="#OmniBal-Towards-Fast-Instruction-Tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance" class="headerlink" title="OmniBal: Towards Fast Instruction-Tuning for Vision-Language Models via   Omniverse Computation Balance"></a>OmniBal: Towards Fast Instruction-Tuning for Vision-Language Models via   Omniverse Computation Balance</h2><p><strong>Authors:Yongqiang Yao, Jingru Tan, Feizhao Zhang, Jiahao Hu, Yazhe Niu, Xin Jin, Bo Li, Pengfei Liu, Ruihao Gong, Dahua Lin, Ningyi Xu</strong></p>
<p>Vision-language instruction-tuning models have recently achieved significant performance improvements. In this work, we discover that large-scale 3D parallel training on those models leads to an imbalanced computation load across different devices. The vision and language parts are inherently heterogeneous: their data distribution and model architecture differ significantly, which affects distributed training efficiency. To address this issue, we rebalance the computational load from data, model, and memory perspectives, achieving more balanced computation across devices. Specifically, for the data, instances are grouped into new balanced mini-batches within and across devices. A search-based method is employed for the model to achieve a more balanced partitioning. For memory optimization, we adaptively adjust the re-computation strategy for each partition to utilize the available memory fully. These three perspectives are not independent but are closely connected, forming an omniverse balanced training framework. Extensive experiments are conducted to validate the effectiveness of our method. Compared with the open-source training code of InternVL-Chat, training time is reduced greatly, achieving about 1.8$\times$ speed-up. Our method’s efficacy and generalizability are further validated across various models and datasets. Codes will be released at <a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal">https://github.com/ModelTC/OmniBal</a>. </p>
<blockquote>
<p>视觉语言指令调整模型最近取得了显著的性能改进。在这项工作中，我们发现大规模3D并行训练这些模型会导致不同设备间计算负载不平衡。视觉和语言部分本质上是异构的：它们的数据分布和模型架构存在很大差异，从而影响分布式训练效率。为了解决这一问题，我们从数据、模型和内存三个方面重新平衡计算负载，实现设备间更平衡的计算。具体来说，对于数据，实例会在设备和跨设备之间分组形成新的平衡小批次。采用基于搜索的方法对模型进行更平衡的分区。为了优化内存，我们自适应地调整每个分区的重新计算策略，以充分利用可用内存。这三个方面不是独立的，而是紧密相关的，形成了一个全方位平衡的训练框架。进行了大量实验来验证我们方法的有效性。与InternVL-Chat的开源训练代码相比，我们的训练时间大大缩短，实现了约1.8倍的速度提升。我们的方法的有效性和通用性在各种模型和数据集上得到了进一步验证。代码将在<a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/ModelTC/OmniBal发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20761v4">PDF</a> </p>
<p><strong>Summary</strong><br>大规模视觉语言指令微调模型在性能上取得了显著的提升。本研究发现大规模三维并行训练会导致不同设备间计算负载不均衡。为解决这一问题，我们从数据、模型和内存三个角度重新平衡计算负载，实现了设备间的均衡计算。通过实例分组、模型搜索和内存优化等方法，形成全方位平衡的训练框架。实验证明，该方法能有效提高训练效率，与公开代码InternVL-Chat相比，训练时间大幅缩短，实现了约1.8倍的加速效果，且该方法在不同模型和数据集上具有有效性和泛化性。相关代码将发布在<a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal%E4%B8%8A%E3%80%82">https://github.com/ModelTC/OmniBal上。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模视觉语言指令微调模型在性能上有显著提升。</li>
<li>三维并行训练导致不同设备间计算负载不均衡。</li>
<li>提出从数据、模型和内存三个角度重新平衡计算负载的方法。</li>
<li>通过实例分组、模型搜索和内存优化形成全方位平衡的训练框架。</li>
<li>与公开代码相比，训练时间大幅缩短，实现了约1.8倍的加速效果。</li>
<li>该方法在不同模型和数据集上具有有效性和泛化性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20761">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7e0bf113b767796f706dbcd55094388e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a45dd304e55736f9eefa248f1a9bf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-210f99f0226ef26deaa57aafa4e06601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-401fc3a4bbe25a7b26e6cfab69506370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fa473f471c98770a269b4e64f560637.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Can-Small-Language-Models-Learn-Unlearn-and-Retain-Noise-Patterns"><a href="#Can-Small-Language-Models-Learn-Unlearn-and-Retain-Noise-Patterns" class="headerlink" title="Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?"></a>Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?</h2><p><strong>Authors:Nicy Scaria, Silvester John Joseph Kennedy, Deepak Subramani</strong></p>
<p>With the growing need for efficient language models in resource-constrained environments, Small Language Models (SLMs) have emerged as compact and practical alternatives to Large Language Models (LLMs). While studies have explored noise handling in LLMs, little is known about how SLMs handle noise, a critical factor for their reliable real-world deployment. This study investigates the ability of SLMs with parameters between 1 and 3 billion to learn, retain, and subsequently eliminate different types of noise (word flip, character flip, transliteration, irrelevant content, and contradictory information). Four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma1.1 2B, and Phi2 2.7B) were instruction-tuned on noise-free data and tested with in-context examples to assess noise learning. Subsequently, noise patterns were introduced in instruction tuning to assess their adaptability. The results revealed differences in how models handle noise, with smaller models like Olmo quickly adapting to noise patterns. Phi2’s carefully curated, structured, and high-quality pretraining data enabled resistance to character level, transliteration, and counterfactual noise, while Gemma adapted successfully to transliteration noise through its multilingual pretraining. Subsequent clean data training effectively mitigated noise effects. These findings provide practical strategies for developing robust SLMs for real-world applications. </p>
<blockquote>
<p>随着资源受限环境中对高效语言模型的需求不断增长，小型语言模型（SLMs）作为大型语言模型（LLMs）的紧凑实用替代品应运而生。虽然已有研究探索了LLM中的噪声处理，但对于SLM如何处理噪声知之甚少，这是其可靠现实世界部署的关键因素。本研究旨在探讨参数在1到3亿之间的SLM学习、保留和随后消除不同类型噪声（单词翻转、字符翻转、音译、无关内容和矛盾信息）的能力。本研究选择了四个预训练SLM（Olmo 1B、Qwen1.5 1.8B、Gemma1.1 2B和Phi2 2.7B），它们在无噪声数据上进行指令微调，并通过上下文实例进行测试，以评估噪声学习能力。随后，在指令微调中引入噪声模式以评估其适应性。结果表明，各模型处理噪声的方式存在差异，较小的模型如Olmo能迅速适应噪声模式。Phi2经过精心策划、结构化和高质量的预训练数据，使其能够抵抗字符级别、音译和反向事实的噪声，而Gemma则通过其多语言预训练成功适应了音译噪声。随后的清洁数据训练有效地减轻了噪声影响。这些发现为实现稳健的SLM用于实际应用提供了实用策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.00996v3">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本研究探讨了小型语言模型（SLM）在处理噪音方面的能力，这对于资源受限环境中的实际部署至关重要。实验结果显示不同SLM在处理不同噪音类型（如词语翻转、字符翻转、音译、无关内容和矛盾信息）时存在差异。通过引入噪声模式对模型进行适应性评估，发现小型模型可以快速适应噪声模式。某些模型由于其预训练数据的特性，如Phi2，具有抵抗字符级别、音译和反向事实的噪声的能力。随后进行的清洁数据训练有效地减轻了噪声的影响。这些发现为开发用于实际应用的稳健SLM提供了实用策略。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>小型语言模型（SLM）作为紧凑且实用的替代方案，满足了资源受限环境中对高效语言模型的需求。</li>
<li>目前对于SLM处理噪音的能力的研究尚不足。</li>
<li>本研究调查了SLM在学习、保留和消除不同类型噪音（如词语翻转、字符翻转等）方面的能力。</li>
<li>不同SLM在处理噪音时存在差异，小型模型可以快速适应噪声模式。</li>
<li>某些模型的预训练数据特性使其具有抵抗特定类型噪声的能力。</li>
<li>清洁数据训练可以有效地减轻噪声对SLM的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.00996">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c7027c1d9a0e95db955f1db80727cd3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51a33527cfc93f49d74f011f26d1292a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c0e82c44d21203383fcaebed8c25fcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e608b118b6f3f84a46cb89d0fa3a6291.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e5f22c3902e2f647156294961e7f850.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-01/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-01/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c41f9698f0b9d7fb6ec207616348e39e.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-01  LEAVS An LLM-based Labeler for Abdominal CT Supervision
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-01/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-937915e313a6e036e2c588a707cf9053.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-01  VideoReasonBench Can MLLMs Perform Vision-Centric Complex Video   Reasoning?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
