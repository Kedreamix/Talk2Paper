<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-16  MIGRATION-BENCH Repository-Level Code Migration Benchmark from Java 8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c18bbc9107fbf4dc65c8d96173bbde4e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-16-æ›´æ–°"><a href="#2025-05-16-æ›´æ–°" class="headerlink" title="2025-05-16 æ›´æ–°"></a>2025-05-16 æ›´æ–°</h1><h2 id="MIGRATION-BENCH-Repository-Level-Code-Migration-Benchmark-from-Java-8"><a href="#MIGRATION-BENCH-Repository-Level-Code-Migration-Benchmark-from-Java-8" class="headerlink" title="MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8"></a>MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8</h2><p><strong>Authors:Linbo Liu, Xinle Liu, Qiang Zhou, Lin Chen, Yihan Liu, Hoan Nguyen, Behrooz Omidvar-Tehrani, Xi Shen, Jun Huan, Omer Tripp, Anoop Deoras</strong></p>
<p>With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on problem-solving and issue-resolution tasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a distinct focus: code migration. MIGRATION-BENCH aims to serve as a comprehensive benchmark for migration from Java 8 to the latest long-term support (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset and its subset selected with $5,102$ and $300$ repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java 17. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: <a target="_blank" rel="noopener" href="https://huggingface.co/collections/AmazonScience">https://huggingface.co/collections/AmazonScience</a> and <a target="_blank" rel="noopener" href="https://github.com/amazon-science/self_debug">https://github.com/amazon-science/self_debug</a> respectively. </p>
<blockquote>
<p>éšç€è¿‘å¹´æ¥å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç°åœ¨å¯ä»¥ä½¿ç”¨LLMè§£å†³ä¸€ç³»åˆ—è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œä»è€Œæ˜¾è‘—æé«˜ç”Ÿäº§åŠ›å’Œå¯æ‰©å±•æ€§ã€‚å·²ç»å¼€å‘äº†è®¸å¤šåŸºå‡†æ•°æ®é›†æ¥è¯„ä¼°è¿™äº›æ¨¡å‹çš„ç¼–ç èƒ½åŠ›ï¼Œå®ƒä»¬ä¸»è¦ä¾§é‡äºé—®é¢˜è§£å†³å’Œé—®é¢˜è§£å†³ä»»åŠ¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç¼–ç åŸºå‡†MIGRATION-BENCHï¼Œå…¶é‡ç‚¹æœ‰æ‰€ä¸åŒï¼šä»£ç è¿ç§»ã€‚MIGRATION-BENCHæ—¨åœ¨æˆä¸ºä»Java 8è¿ç§»åˆ°æœ€æ–°é•¿æœŸæ”¯æŒï¼ˆLTSï¼‰ç‰ˆæœ¬ï¼ˆJava 17ã€21ï¼‰çš„ç»¼åˆæ€§åŸºå‡†ã€‚è¯¥åŸºå‡†æ•°æ®é›†åŒ…æ‹¬ä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†å’Œåˆ†åˆ«é€šè¿‡$ 5,102 $å’Œ$ 300 $ä¸ªå­˜å‚¨åº“é€‰æ‹©çš„å­é›†ã€‚æ‰€é€‰å­é›†ç»è¿‡ç²¾å¿ƒæŒ‘é€‰ï¼Œå…·æœ‰ä»£è¡¨æ€§å’Œå¤æ‚æ€§ï¼Œä¸ºä»£ç è¿ç§»é¢†åŸŸçš„ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„èµ„æºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›å¯¹è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡çš„LLMè¿›è¡Œä¸¥æ ¼å’Œæ ‡å‡†åŒ–çš„è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†SD-Feedbackå¹¶è¯æ˜LLMå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å­˜å‚¨åº“çº§åˆ«çš„ä»£ç è¿ç§»åˆ°Java 17ã€‚å¯¹äºä½¿ç”¨Claude-3.5-Sonnet-v2é€‰æ‹©çš„å­é›†ï¼ŒSD-Feedbackåœ¨æœ€å°è¿ç§»å’Œæœ€å¤§è¿ç§»ä¸Šçš„æˆåŠŸç‡ï¼ˆpass@1ï¼‰åˆ†åˆ«è¾¾åˆ°62.33%å’Œ27.00%ã€‚åŸºå‡†æ•°æ®é›†å’Œæºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/collections/AmazonScience">https://huggingface.co/collections/AmazonScience</a> å’Œ <a target="_blank" rel="noopener" href="https://github.com/amazon-science/self_debug">https://github.com/amazon-science/self_debug</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09569v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä¸€ç³»åˆ—è½¯ä»¶å·¥ç¨‹é¡¹ç›®ä»»åŠ¡éƒ½èƒ½é€šè¿‡LLMè§£å†³ï¼Œå¤§å¤§æé«˜äº†ç”Ÿäº§åŠ›å’Œå¯æ‰©å±•æ€§ã€‚å·²ç»å¼€å‘äº†è®¸å¤šåŸºå‡†æ•°æ®é›†æ¥è¯„ä¼°æ¨¡å‹çš„ç¼–ç èƒ½åŠ›ï¼Œä¸»è¦ä¾§é‡äºé—®é¢˜è§£å†³å’Œé—®é¢˜è§£å†³ä»»åŠ¡ã€‚ä¸æ­¤ç›¸åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç¼–ç åŸºå‡†MIGRATION-BENCHï¼Œé‡ç‚¹å…³æ³¨ä»£ç è¿ç§»ã€‚MIGRATION-BENCHæ—¨åœ¨æˆä¸ºä»Java 8è¿ç§»åˆ°æœ€æ–°é•¿æœŸæ”¯æŒï¼ˆLTSï¼‰ç‰ˆæœ¬ï¼ˆJava 17ã€21ï¼‰çš„ç»¼åˆæ€§åŸºå‡†ï¼ŒåŒ…æ‹¬å®Œæ•´æ•°æ®é›†åŠå…¶å­é›†ã€‚æ‰€é€‰å­é›†ç»è¿‡å¤æ‚çš„ç²¾å¿ƒæŒ‘é€‰ï¼Œä¸ºè¿™ä¸ªé¢†åŸŸçš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªé€šç”¨èµ„æºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ååŠ©å¯¹è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡è¿›è¡Œä¸¥æ ¼çš„æ ‡å‡†åŒ–è¯„ä¼°ã€‚é€šè¿‡SD-Feedbackè¿›ä¸€æ­¥æå‡ºå¹¶è¯æ˜LLMå¯ä»¥æœ‰æ•ˆè§£å†³å­˜å‚¨åº“çº§åˆ«çš„ä»£ç è¿ç§»åˆ°Java 17çš„é—®é¢˜ã€‚å¯¹äºé€‰å®šçš„å­é›†ä½¿ç”¨Claude-3.5-Sonnet-v2çš„SD-Feedbackå®ç°äº†æœ€å°è¿ç§»å’Œæœ€å¤§è¿ç§»çš„æˆåŠŸç‡åˆ†åˆ«ä¸º62.33%å’Œ27.0%ã€‚åŸºå‡†æ•°æ®é›†å’Œæºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/collections/AmazonScience">https://huggingface.co/collections/AmazonScience</a> å’Œ <a target="_blank" rel="noopener" href="https://github.com/amazon-science/self_debug%E3%80%82">https://github.com/amazon-science/self_debugã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹é¡¹ç›®ä¸­å‘æŒ¥ç€è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ï¼Œæé«˜äº†ç”Ÿäº§åŠ›å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>MIGRATION-BENCHæ˜¯ä¸€ä¸ªæ–°çš„ç¼–ç åŸºå‡†ï¼Œä¸“æ³¨äºä»£ç è¿ç§»ä»»åŠ¡ï¼Œæ—¨åœ¨ä»Java 8è¿ç§»åˆ°æœ€æ–°çš„LTSç‰ˆæœ¬ã€‚</li>
<li>MIGRATION-BENCHåŒ…æ‹¬ä¸€ä¸ªå®Œæ•´æ•°æ®é›†åŠå…¶ç²¾é€‰å­é›†ï¼Œä¸ºç ”ç©¶é¢†åŸŸæä¾›é€šç”¨èµ„æºã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¿›è¡Œä¸¥æ ¼çš„æ ‡å‡†åŒ–è¯„ä¼°ã€‚</li>
<li>SD-Feedbackæ–¹æ³•å¯ä»¥æœ‰æ•ˆè§£å†³å­˜å‚¨åº“çº§åˆ«çš„ä»£ç è¿ç§»åˆ°Java 17çš„é—®é¢˜ã€‚</li>
<li>å¯¹äºæ‰€é€‰å­é›†ï¼Œä½¿ç”¨Claude-3.5-Sonnet-v2çš„SD-Feedbackæ–¹æ³•çš„æœ€å°è¿ç§»å’Œæœ€å¤§è¿ç§»æˆåŠŸç‡åˆ†åˆ«ä¸º62.33%å’Œ27.0%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6d1495662f6b443d61087e3409268954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-095b09fef8e8a77d078f1c5c48b07cd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ba8b96dfd42942d444417bf6fc65e7d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WavReward-Spoken-Dialogue-Models-With-Generalist-Reward-Evaluators"><a href="#WavReward-Spoken-Dialogue-Models-With-Generalist-Reward-Evaluators" class="headerlink" title="WavReward: Spoken Dialogue Models With Generalist Reward Evaluators"></a>WavReward: Spoken Dialogue Models With Generalist Reward Evaluators</h2><p><strong>Authors:Shengpeng Ji, Tianle Liang, Yangzhuo Li, Jialong Zuo, Minghui Fang, Jinzheng He, Yifu Chen, Zhengqing Liu, Ziyue Jiang, Xize Cheng, Siqi Zheng, Jin Xu, Junyang Lin, Zhou Zhao</strong></p>
<p>End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue modelsâ€™ conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1$%$ to 91.5$%$. In subjective A&#x2F;B testing, WavReward also leads by a margin of 83$%$. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at <a target="_blank" rel="noopener" href="https://github.com/jishengpeng/WavReward">https://github.com/jishengpeng/WavReward</a> after the paper is accepted. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç«¯åˆ°ç«¯çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼Œå¦‚GPT-4o-audioï¼Œåœ¨è¯­éŸ³é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå£è¯­å¯¹è¯æ¨¡å‹çš„ä¼šè¯æ€§èƒ½è¯„ä¼°å´è¢«å¤§å¤§å¿½è§†äº†ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºæ™ºèƒ½èŠå¤©æœºå™¨äººä¼ è¾¾äº†å¤§é‡çš„éæ–‡æœ¬ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯æ— æ³•è½»æ˜“åœ°ä½¿ç”¨åŸºäºæ–‡æœ¬çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¦‚ChatGPTã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†WavRewardï¼Œä¸€ä¸ªåŸºäºéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å¥–åŠ±åé¦ˆæ¨¡å‹ï¼Œå®ƒå¯ä»¥ä½¿ç”¨è¯­éŸ³è¾“å…¥è¯„ä¼°å£è¯­å¯¹è¯ç³»ç»Ÿçš„æ™ºå•†å’Œæƒ…å•†ã€‚å…·ä½“æ¥è¯´ï¼Œ1ï¼‰WavRewardåŸºäºéŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œèå…¥äº†æ·±åº¦æ¨ç†è¿‡ç¨‹å’Œç”¨äºåè®­ç»ƒçš„éçº¿æ€§å¥–åŠ±æœºåˆ¶ã€‚é€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å¤šæ ·æœ¬åé¦ˆï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å£è¯­å¯¹è¯æ¨¡å‹çš„è¯„ä¼°å™¨ã€‚2ï¼‰æˆ‘ä»¬å¼•å…¥äº†ç”¨äºè®­ç»ƒWavRewardçš„åå¥½æ•°æ®é›†ChatReward-30Kã€‚ChatReward-30Kæ¶µç›–äº†å£è¯­å¯¹è¯æ¨¡å‹çš„ç†è§£å’Œç”Ÿæˆæ–¹é¢ã€‚è¿™äº›åœºæ™¯åŒ…æ‹¬å„ç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬èŠå¤©ã€æŒ‡ä»¤èŠå¤©çš„ä¹ä¸ªå£°éŸ³ç‰¹å¾ä»¥åŠéšå¼èŠå¤©ã€‚WavRewardåœ¨å¤šä¸ªå£è¯­å¯¹è¯åœºæ™¯ä¸­è¡¨ç°å‡ºè¶…è¶Šå…ˆå‰æœ€å…ˆè¿›çš„è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œåœ¨å®¢è§‚å‡†ç¡®æ€§æ–¹é¢å®ç°äº†ä»55.1%åˆ°91.5%çš„æ˜¾è‘—æ”¹è¿›ï¼ˆç›¸è¾ƒäºQwen2.5-Omniï¼‰ã€‚åœ¨ä¸»è§‚çš„A&#x2F;Bæµ‹è¯•ä¸­ï¼ŒWavRewardçš„é¢†å…ˆå¹…åº¦ä¹Ÿè¾¾åˆ°äº†83%ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¯å®äº†WavRewardæ¯ä¸ªç»„ä»¶çš„å¿…è¦æ€§ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œæ‰€æœ‰æ•°æ®å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jishengpeng/WavReward%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/jishengpeng/WavRewardä¸Šå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09558v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¿‘æœŸè¯­éŸ³é¢†åŸŸå‡ºç°äº†ç«¯åˆ°ç«¯çš„å¯¹è¯æ¨¡å‹ï¼Œå¦‚GPT-4o-audioã€‚ç„¶è€Œï¼Œè¿™äº›å¯¹è¯æ¨¡å‹çš„ä¼šè¯æ€§èƒ½è¯„ä¼°å´è¢«å¿½è§†ã€‚è¿™æ˜¯ç”±äºæ™ºèƒ½èŠå¤©æœºå™¨äººä¼ è¾¾äº†å¤§é‡æ— æ³•é€šè¿‡æ–‡æœ¬è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰è½»æ¾è¡¡é‡çš„éæ–‡æœ¬ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºWavRewardæ¨¡å‹ï¼ŒåŸºäºéŸ³é¢‘è¯­è¨€æ¨¡å‹æ„å»ºå¥–åŠ±åé¦ˆæœºåˆ¶ï¼Œèƒ½è¯„ä¼°å¯¹è¯ç³»ç»Ÿçš„æ™ºå•†ä¸æƒ…å•†ã€‚è¯¥æ¨¡å‹ç»“åˆæ·±åº¦æ¨ç†è¿‡ç¨‹å’Œéçº¿æ€§å¥–åŠ±æœºåˆ¶è¿›è¡Œåè®­ç»ƒï¼Œå¹¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å¤šæ ·æœ¬åé¦ˆæ„å»ºä¸“é—¨é’ˆå¯¹å¯¹è¯æ¨¡å‹çš„è¯„ä¼°å™¨ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ChatReward-30Kæ•°æ®é›†æ¥è®­ç»ƒWavRewardã€‚è¯¥æ•°æ®é›†åŒ…å«å¯¹è¯æ¨¡å‹çš„ç†è§£å’Œç”Ÿæˆæ–¹é¢ï¼Œæ¶µç›–å¤šç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬èŠå¤©ã€æŒ‡ä»¤èŠå¤©çš„ä¹ç§å£°éŸ³ç‰¹å¾å’Œéšæ€§èŠå¤©ç­‰ã€‚åœ¨å¤šç§å¯¹è¯åœºæ™¯ä¸­ï¼ŒWavRewardæ˜¾è‘—ä¼˜äºä¹‹å‰çš„å…ˆè¿›è¯„ä¼°æ¨¡å‹ï¼Œå®¢è§‚å‡†ç¡®ç‡ä»55.1%æé«˜åˆ°91.5%ã€‚ä¸»è§‚A&#x2F;Bæµ‹è¯•ä¸­ï¼ŒWavRewardçš„é¢†å…ˆä¼˜åŠ¿æ›´å¤§ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¯å®äº†WavRewardæ¯ä¸ªç»„ä»¶çš„å¿…è¦æ€§ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/jishengpeng/WavReward%E3%80%82">https://github.com/jishengpeng/WavRewardã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç«¯åˆ°ç«¯çš„å¯¹è¯æ¨¡å‹å¦‚GPT-4o-audioåœ¨è¯­éŸ³é¢†åŸŸå—åˆ°å…³æ³¨ï¼Œä½†è¯„ä¼°å…¶ä¼šè¯æ€§èƒ½éå¸¸é‡è¦ã€‚</li>
<li>æ™ºèƒ½èŠå¤©æœºå™¨äººä¼ è¾¾çš„éæ–‡æœ¬ä¿¡æ¯æ— æ³•é€šè¿‡æ–‡æœ¬è¯­è¨€æ¨¡å‹è½»æ¾è¡¡é‡ã€‚</li>
<li>WavRewardæ˜¯ä¸€ä¸ªåŸºäºéŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å¥–åŠ±åé¦ˆæ¨¡å‹ï¼Œå¯ä»¥è¯„ä¼°å¯¹è¯ç³»ç»Ÿçš„æ™ºå•†å’Œæƒ…å•†ã€‚</li>
<li>WavRewardç»“åˆäº†æ·±åº¦æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåˆ©ç”¨å¤šæ ·æœ¬åé¦ˆæ„å»ºä¸“é—¨çš„å¯¹è¯æ¨¡å‹è¯„ä¼°å™¨ã€‚</li>
<li>ChatReward-30Kæ•°æ®é›†ç”¨äºè®­ç»ƒWavRewardï¼ŒåŒ…å«å¯¹è¯æ¨¡å‹çš„ç†è§£å’Œç”Ÿæˆæ–¹é¢ã€‚</li>
<li>WavRewardåœ¨å„ç§å¯¹è¯åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®¢è§‚å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2743cf5dddc5f00f8dc262bac4c0e10c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac266c9227db009e57d624d1983fb69e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c41d7e228bcb2a9b2ecf4fe62cc5dffe.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Omni-R1-Do-You-Really-Need-Audio-to-Fine-Tune-Your-Audio-LLM"><a href="#Omni-R1-Do-You-Really-Need-Audio-to-Fine-Tune-Your-Audio-LLM" class="headerlink" title="Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"></a>Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?</h2><p><strong>Authors:Andrew Rouditchenko, Saurabhchand Bhati, Edson Araujo, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass</strong></p>
<p>We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Omni-R1ï¼Œå®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ æ³•GRPOå¯¹è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Qwen2.5-Omniè¿›è¡Œå¾®è°ƒï¼Œç”¨äºéŸ³é¢‘é—®ç­”æ•°æ®é›†ã€‚è¿™åœ¨æœ€è¿‘çš„MMAUåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æŠ€æœ¯é¡¶å°–è¡¨ç°ã€‚Omni-R1åœ¨å£°éŸ³ã€éŸ³ä¹ã€è¯­éŸ³å’Œæ€»ä½“å¹³å‡ç±»åˆ«ä¸Šå‡å®ç°äº†æœ€é«˜å‡†ç¡®ç‡ï¼Œæ— è®ºæ˜¯åœ¨Test-miniè¿˜æ˜¯Test-fullåˆ†å‰²ä¸Šã€‚ä¸ºäº†äº†è§£æ€§èƒ½æå‡æƒ…å†µï¼Œæˆ‘ä»¬å¯¹æœ‰éŸ³é¢‘å’Œæ— éŸ³é¢‘çš„æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°GRPOçš„å¤§éƒ¨åˆ†æ€§èƒ½æå‡å½’åŠŸäºåŸºäºæ–‡æœ¬æ¨ç†çš„æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜æ„å¤–åœ°å‘ç°ï¼Œåœ¨åªæœ‰æ–‡æœ¬çš„æ•°æ®é›†ä¸Šä¸å¸¦éŸ³é¢‘è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿèƒ½æœ‰æ•ˆæé«˜åŸºäºéŸ³é¢‘çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09439v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Omni-R1é€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Qwen2.5-Omniåœ¨éŸ³é¢‘é—®ç­”æ•°æ®é›†ä¸Šï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•GRPOï¼Œå®ç°äº†åœ¨æœ€æ–°MMAUåŸºå‡†æµ‹è¯•ä¸Šçš„æœ€æ–°æœ€ä½³æ€§èƒ½ã€‚Omni-R1åœ¨å£°éŸ³ã€éŸ³ä¹ã€è¯­éŸ³åŠæ€»ä½“å¹³å‡åˆ†ç±»ä¸Šå‡è¾¾åˆ°äº†æœ€é«˜ç²¾åº¦ï¼Œæ— è®ºæ˜¯Test-miniè¿˜æ˜¯Test-fullåˆ†å‰²éƒ½æ˜¯å¦‚æ­¤ã€‚ä¸ºäº†äº†è§£æ€§èƒ½æå‡çš„åŸå› ï¼Œä½œè€…å¯¹æ˜¯å¦ä½¿ç”¨éŸ³é¢‘è¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°GRPOçš„å¤§éƒ¨åˆ†æ€§èƒ½æå‡ä¸»è¦å½’å› äºåŸºäºæ–‡æœ¬æ¨ç†çš„èƒ½åŠ›ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä½œè€…åœ¨ä¸ä½¿ç”¨éŸ³é¢‘çš„æƒ…å†µä¸‹å¯¹çº¯æ–‡æœ¬æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè¿™åŒæ ·æå‡äº†åŸºäºéŸ³é¢‘çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni-R1é€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Qwen2.5-Omniè¾¾åˆ°æœ€æ–°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>Omni-R1åœ¨å¤šä¸ªåˆ†ç±»ä¸Šè¾¾åˆ°æœ€é«˜ç²¾åº¦ã€‚</li>
<li>GRPOå¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯¹æ–‡æœ¬æ¨ç†èƒ½åŠ›æœ‰è¾ƒå¤§è´¡çŒ®ã€‚</li>
<li>ä¸ä½¿ç”¨éŸ³é¢‘çš„çº¯æ–‡æœ¬æ•°æ®é›†å¾®è°ƒèƒ½æå‡åŸºäºéŸ³é¢‘çš„æ€§èƒ½ã€‚</li>
<li>Omni-R1æ¨¡å‹åœ¨æŸäº›é¢†åŸŸæ€§èƒ½æ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯åœ¨å£°éŸ³ã€éŸ³ä¹ã€è¯­éŸ³æ–¹é¢ã€‚</li>
<li>æµ‹è¯•ç»“æœæ˜¾ç¤ºGRPOå¼ºåŒ–å­¦ä¹ å¯æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-419ec3f211da1c24b8974ba413058c22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94a923a49381e7fe950b6567068892ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06fcd64a5e7f3071098582e253bec112.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Robustness-of-Adversarial-Defenses-in-Malware-Detection-Systems"><a href="#Evaluating-the-Robustness-of-Adversarial-Defenses-in-Malware-Detection-Systems" class="headerlink" title="Evaluating the Robustness of Adversarial Defenses in Malware Detection   Systems"></a>Evaluating the Robustness of Adversarial Defenses in Malware Detection   Systems</h2><p><strong>Authors:Mostafa Jafari, Alireza Shameli-Sendi</strong></p>
<p>Machine learning is a key tool for Android malware detection, effectively identifying malicious patterns in apps. However, ML-based detectors are vulnerable to evasion attacks, where small, crafted changes bypass detection. Despite progress in adversarial defenses, the lack of comprehensive evaluation frameworks in binary-constrained domains limits understanding of their robustness. We introduce two key contributions. First, Prioritized Binary Rounding, a technique to convert continuous perturbations into binary feature spaces while preserving high attack success and low perturbation size. Second, the sigma-binary attack, a novel adversarial method for binary domains, designed to achieve attack goals with minimal feature changes. Experiments on the Malscan dataset show that sigma-binary outperforms existing attacks and exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant brittleness, with attack success rates exceeding 90% using fewer than 10 feature modifications and reaching 100% with just 20. Adversarially trained defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small budgets but remains vulnerable to unrestricted perturbations, with attack success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates strong robustness against state-of-the-art gradient-based adversarial attacks by maintaining an attack success rate below 16.55%, the sigma-binary attack significantly outperforms these methods, achieving a 94.56% success rate under unrestricted perturbations. These findings highlight the critical need for precise method like sigma-binary to expose hidden vulnerabilities in existing defenses and support the development of more resilient malware detection systems. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ æ˜¯Androidæ¶æ„è½¯ä»¶æ£€æµ‹çš„å…³é”®å·¥å…·ï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«åº”ç”¨ç¨‹åºä¸­çš„æ¶æ„æ¨¡å¼ã€‚ç„¶è€Œï¼ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ£€æµ‹å™¨å®¹æ˜“å—åˆ°è§„é¿æ”»å‡»çš„å½±å“ï¼Œè¿™äº›æ”»å‡»é€šè¿‡ç»†å¾®çš„å®šåˆ¶æ”¹å˜æ¥ç»•è¿‡æ£€æµ‹ã€‚å°½ç®¡å¯¹æŠ—æ€§é˜²å¾¡å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨äºŒè¿›åˆ¶å—é™é¢†åŸŸç¼ºä¹å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œé™åˆ¶äº†å¯¹å…¶ç¨³å¥æ€§çš„ç†è§£ã€‚æˆ‘ä»¬åšå‡ºäº†ä¸¤ä¸ªé‡è¦çš„è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¼˜å…ˆäºŒè¿›åˆ¶èˆå…¥æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯å°†è¿ç»­æ‰°åŠ¨è½¬æ¢ä¸ºäºŒè¿›åˆ¶ç‰¹å¾ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒé«˜æ”»å‡»æˆåŠŸç‡å’Œå°æ‰°åŠ¨å¤§å°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†sigma-binaryæ”»å‡»ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹äºŒè¿›åˆ¶é¢†åŸŸçš„æ–°å‹å¯¹æŠ—æ€§æ–¹æ³•ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„ç‰¹å¾å˜åŒ–å®ç°æ”»å‡»ç›®æ ‡ã€‚åœ¨Malscanæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œsigma-binaryæ”»å‡»ä¼˜äºç°æœ‰æ”»å‡»ï¼Œå¹¶æš´éœ²äº†å…ˆè¿›é˜²å¾¡æ‰‹æ®µçš„å…³é”®æ¼æ´ã€‚é…å¤‡å¯¹æ‰‹æ£€æµ‹å™¨çš„é˜²å¾¡æ‰‹æ®µï¼Œå¦‚KDEã€DLAã€DNN+å’ŒICNNï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„è„†å¼±æ€§ï¼Œä½¿ç”¨å°‘äº10ä¸ªç‰¹å¾ä¿®æ”¹çš„æ”»å‡»æˆåŠŸç‡è¶…è¿‡90%ï¼Œä»…ç”¨20ä¸ªç‰¹å¾ä¿®æ”¹å³å¯è¾¾åˆ°100%ã€‚å¯¹æŠ—æ€§è®­ç»ƒé˜²å¾¡æ‰‹æ®µï¼ŒåŒ…æ‹¬AT-rFGSM-kã€AT-MaxMAç­‰ï¼Œåœ¨å°é¢„ç®—ä¸‹æé«˜äº†ç¨³å¥æ€§ï¼Œä½†ä»å—åˆ°æ— é™åˆ¶æ‰°åŠ¨çš„æ”»å‡»ï¼ŒæˆåŠŸç‡åˆ†åˆ«ä¸º99.45%å’Œ96.62%ã€‚å°½ç®¡PAD-SMAå¯¹æœ€å…ˆè¿›çš„åŸºäºæ¢¯åº¦çš„å¯¹æŠ—æ€§æ”»å‡»è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œå°†æ”»å‡»æˆåŠŸç‡ä¿æŒåœ¨16.55%ä»¥ä¸‹ï¼Œä½†sigma-binaryæ”»å‡»æ˜¾è‘—ä¼˜äºè¿™äº›æ–¹æ³•ï¼Œåœ¨æ— é™åˆ¶æ‰°åŠ¨ä¸‹çš„æˆåŠŸç‡ä¸º94.56%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åƒsigma-binaryè¿™æ ·çš„ç²¾ç¡®æ–¹æ³•çš„é‡è¦æ€§ï¼Œå¯ä»¥æ­ç¤ºç°æœ‰é˜²å¾¡æ‰‹æ®µä¸­çš„éšè—æ¼æ´ï¼Œå¹¶æ”¯æŒå¼€å‘æ›´å…·éŸ§æ€§çš„æ¶æ„è½¯ä»¶æ£€æµ‹ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09342v1">PDF</a> Submitted to IEEE Transactions on Information Forensics and Security   (T-IFS), 13 pages, 4 figures</p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ æ˜¯å®‰å“æ¶æ„è½¯ä»¶æ£€æµ‹çš„å…³é”®å·¥å…·ï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«åº”ç”¨ç¨‹åºä¸­çš„æ¶æ„æ¨¡å¼ã€‚ç„¶è€Œï¼ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ£€æµ‹å™¨å®¹æ˜“å—åˆ°è§„é¿æ”»å‡»çš„å½±å“ï¼Œå¾®å°çš„æ”¹å˜å¯èƒ½ä¼šç»•è¿‡æ£€æµ‹ã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸¤ç§å…³é”®è´¡çŒ®ï¼šä¼˜å…ˆäºŒè¿›åˆ¶èˆå…¥æ³•å’Œé’ˆå¯¹äºŒè¿›åˆ¶åŸŸçš„SigmaäºŒå…ƒæ”»å‡»æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒSigmaäºŒå…ƒæ”»å‡»æ³•ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ï¼Œæ­ç¤ºäº†ç°æœ‰é˜²å¾¡æ‰‹æ®µçš„å…³é”®æ¼æ´ã€‚å³ä½¿é…å¤‡å¯¹æŠ—æ€§æ£€æµ‹å™¨çš„é˜²å¾¡æ‰‹æ®µå¦‚KDEã€DLAã€DNN+å’ŒICNNç­‰ï¼Œä»è¡¨ç°å‡ºæ˜¾è‘—çš„è„†å¼±æ€§ã€‚å¯¹æŠ—æ€§è®­ç»ƒé˜²å¾¡æ‰‹æ®µå¦‚AT-rFGSM-kå’ŒAT-MaxMAåœ¨å°é¢„ç®—ä¸‹æé«˜ç¨³å¥æ€§ï¼Œä½†åœ¨æ— é™åˆ¶æ‰°åŠ¨ä¸‹ä»é¢ä¸´æ¼æ´ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡å¼ºè°ƒäº†å¯¹ç°æœ‰é˜²å¾¡æ‰‹æ®µè¿›è¡Œç²¾ç¡®æ”»å‡»æ–¹æ³•ï¼ˆå¦‚SigmaäºŒå…ƒæ”»å‡»ï¼‰çš„é‡è¦æ€§ï¼Œä»¥æ­ç¤ºéšè—æ¼æ´ï¼Œæ”¯æŒå¼€å‘æ›´å…·å¼¹æ€§çš„æ¶æ„è½¯ä»¶æ£€æµ‹ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ æ˜¯å®‰å“æ¶æ„è½¯ä»¶æ£€æµ‹çš„é‡è¦å·¥å…·ï¼Œèƒ½å¤Ÿè¯†åˆ«åº”ç”¨ç¨‹åºä¸­çš„æ¶æ„æ¨¡å¼ã€‚</li>
<li>åŸºäºæœºå™¨å­¦ä¹ çš„æ£€æµ‹å™¨å®¹æ˜“å—åˆ°è§„é¿æ”»å‡»çš„å½±å“ã€‚</li>
<li>ä¼˜å…ˆäºŒè¿›åˆ¶èˆå…¥æ³•å’ŒSigmaäºŒå…ƒæ”»å‡»æ³•æ˜¯å¯¹æŠ—è¿™ç§é—®é¢˜çš„ä¸¤ç§å…³é”®æ–¹æ³•ã€‚</li>
<li>SigmaäºŒå…ƒæ”»å‡»æ³•åœ¨å®éªŒä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ”»å‡»æ–¹æ³•ã€‚</li>
<li>å³ä½¿é…å¤‡å¯¹æŠ—æ€§æ£€æµ‹å™¨çš„é˜²å¾¡æ‰‹æ®µä¹Ÿå­˜åœ¨æ˜¾è‘—æ¼æ´ã€‚</li>
<li>å¯¹æŠ—æ€§è®­ç»ƒèƒ½æé«˜é˜²å¾¡æ‰‹æ®µçš„ç¨³å¥æ€§ï¼Œä½†åœ¨ç‰¹å®šæƒ…å†µä¸‹ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ac831521ee4edfedb47eeb809baebd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c18bbc9107fbf4dc65c8d96173bbde4e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RAG-Enabled-Intent-Reasoning-for-Application-Network-Interaction"><a href="#RAG-Enabled-Intent-Reasoning-for-Application-Network-Interaction" class="headerlink" title="RAG-Enabled Intent Reasoning for Application-Network Interaction"></a>RAG-Enabled Intent Reasoning for Application-Network Interaction</h2><p><strong>Authors:Salwa Mostafa, Mohamed K. Abdel-Aziz, Mohammed S. Elbamby, Mehdi Bennis</strong></p>
<p>Intent-based network (IBN) is a promising solution to automate network operation and management. IBN aims to offer human-tailored network interaction, allowing the network to communicate in a way that aligns with the network usersâ€™ language, rather than requiring the network users to understand the technical language of the network&#x2F;devices. Nowadays, different applications interact with the network, each with its own specialized needs and domain language. Creating semantic languages (i.e., ontology-based languages) and associating them with each application to facilitate intent translation lacks technical expertise and is neither practical nor scalable. To tackle the aforementioned problem, we propose a context-aware AI framework that utilizes machine reasoning (MR), retrieval augmented generation (RAG), and generative AI technologies to interpret intents from different applications and generate structured network intents. The proposed framework allows for generalized&#x2F;domain-specific intent expression and overcomes the drawbacks of large language models (LLMs) and vanilla-RAG framework. The experimental results show that our proposed intent-RAG framework outperforms the LLM and vanilla-RAG framework in intent translation. </p>
<blockquote>
<p>åŸºäºæ„å›¾çš„ç½‘ç»œï¼ˆIBNï¼‰æ˜¯è‡ªåŠ¨åŒ–ç½‘ç»œæ“ä½œå’Œç®¡ç†çš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚IBNæ—¨åœ¨æä¾›ç¬¦åˆäººç±»éœ€æ±‚çš„ç½‘ç»œäº¤äº’ï¼Œè®©ç½‘ç»œèƒ½å¤Ÿä»¥ä¸ç”¨æˆ·è¯­è¨€ç›¸ç¬¦çš„æ–¹å¼è¿›è¡Œé€šä¿¡ï¼Œè€Œä¸æ˜¯è¦æ±‚ç½‘ç»œç”¨æˆ·ç†è§£ç½‘ç»œ&#x2F;è®¾å¤‡çš„æŠ€æœ¯è¯­è¨€ã€‚å¦‚ä»Šï¼Œä¸åŒçš„åº”ç”¨ç¨‹åºä¸ç½‘ç»œè¿›è¡Œäº¤äº’ï¼Œå„æœ‰å…¶ç‰¹æ®Šéœ€æ±‚å’Œé¢†åŸŸè¯­è¨€ã€‚åˆ›å»ºè¯­ä¹‰è¯­è¨€ï¼ˆä¾‹å¦‚åŸºäºæœ¬ä½“è®ºçš„è¯­è¨€ï¼‰å¹¶ä¸æ¯ä¸ªåº”ç”¨ç¨‹åºç›¸å…³è”ï¼Œä»¥ä¿ƒè¿›æ„å›¾ç¿»è¯‘ï¼Œä½†è¿™ç¼ºä¹æŠ€æœ¯ä¸“ä¸šçŸ¥è¯†ï¼Œæ—¢ä¸å®ç”¨ä¹Ÿä¸å¯æ‰©å±•ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æœºå™¨æ¨ç†ï¼ˆMRï¼‰ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯æ¥è§£é‡Šæ¥è‡ªä¸åŒåº”ç”¨ç¨‹åºçš„æ„å›¾å¹¶ç”Ÿæˆç»“æ„åŒ–ç½‘ç»œæ„å›¾ã€‚æ‰€æå‡ºçš„æ¡†æ¶å…è®¸é€šç”¨&#x2F;ç‰¹å®šé¢†åŸŸçš„æ„å›¾è¡¨è¾¾ï¼Œå¹¶å…‹æœäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç®€å•RAGæ¡†æ¶çš„ç¼ºç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ„å›¾RAGæ¡†æ¶åœ¨æ„å›¾ç¿»è¯‘æ–¹é¢ä¼˜äºLLMå’Œç®€å•RAGæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09339v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç½‘ç»œæ„å›¾æ¡†æ¶åˆ©ç”¨AIæŠ€æœ¯è¿›è¡Œè‡ªåŠ¨åŒ–ç®¡ç†å’Œæ“ä½œï¼Œé‡‡ç”¨æœºå™¨å­¦ä¹ ã€å¢å¼ºæ£€ç´¢ç”Ÿæˆå’Œè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯è§£è¯»åº”ç”¨ç¨‹åºæ„å›¾å¹¶ç”Ÿæˆç»“æ„åŒ–ç½‘ç»œæ„å›¾ï¼Œä¾¿äºç½‘ç»œä¸ç”¨æˆ·ä¹‹é—´çš„äº¤æµï¼Œä¿ƒè¿›ç”¨æˆ·ä½“éªŒçš„ä¸ªæ€§åŒ–éœ€æ±‚ã€‚è¯¥æ¡†æ¶å…‹æœäº†ä¼ ç»Ÿè¯­ä¹‰è¯­è¨€æŠ€æœ¯çš„å±€é™æ€§ï¼Œæé«˜äº†ç½‘ç»œç®¡ç†çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ„å›¾ç½‘ç»œï¼ˆIBNï¼‰æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç®¡ç†ç½‘ç»œæ“ä½œçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>IBNçš„ç›®æ ‡æ˜¯æä¾›äººæ€§åŒ–çš„ç½‘ç»œäº¤äº’æ–¹å¼ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿä¸ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€ç›¸å¯¹åº”è€Œä¸æ˜¯åè¿‡æ¥çš„ä¸“ä¸šè¯­è¨€äº¤äº’æ–¹å¼ã€‚å› ä¸ºæœ‰è®¸å¤šåº”ç”¨ï¼Œå¦‚ç½‘ç»œé€šä¿¡ä¼šæœ‰ç‰¹å®šçš„éœ€è¦å’Œè¯­å¢ƒæ€§ä¸“ä¸šè¯æ±‡æˆ–ä¸šåŠ¡ç›¸å…³æ ‡ç­¾ä¸ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨æ„å›¾ç½‘ç»œå¯ä»¥ä½¿æ²Ÿé€šå˜å¾—ç®€å•ã€‚è¿™å¯¹äºè®¸å¤šé¢†åŸŸçš„åº”ç”¨éå¸¸æœ‰ç”¨ã€‚ </li>
<li>æå‡ºäº†ä¸€ç§åŸºäºAIçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¡†æ¶ï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯è§£é‡Šåº”ç”¨ç¨‹åºæ„å›¾å¹¶ç”Ÿæˆç»“æ„åŒ–ç½‘ç»œæ„å›¾ã€‚è¿™ä¸ªæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œé€‚ç”¨äºä¸åŒé¢†åŸŸçš„åº”ç”¨åœºæ™¯ã€‚åŒæ—¶å®ƒå…‹æœäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç®€å•RAGæ¡†æ¶çš„ç¼ºç‚¹å’Œå±€é™æ€§ï¼Œè¿›è¡Œäº†ä¸ªæ€§åŒ–çš„è°ƒæ•´ï¼Œå¸®åŠ©ä»¥æ˜“äºç†è§£çš„ä¸šåŠ¡è‡ªç„¶è¯­è¨€ä½¿ç”¨è€…å¯¹ç³»ç»Ÿçš„ç®¡ç†å’Œæ„å›¾æ“æ§å·¥ä½œæå‡ºäº†é©å‘½æ€§çš„æ–°çš„æ¨¡å‹å¯èƒ½æ€§æ€è€ƒè·¯å¾„è®¾è®¡å¯èƒ½æ€§æµ‹è¯•å’Œé—®é¢˜ç¡®å®šè§„åˆ™å®šåˆ¶åŒ–åˆ›å»ºç®€æ˜“å¼èµ„æºç›¸å…³ç®€åŒ–å±•ç¤ºæ–¹å¼ä¸åŠŸèƒ½æ€§é«˜æ•ˆç»“æ„é›†åˆè®¾è®¡ç­‰å¯èƒ½çš„æ”¹é©å’Œåˆ›æ–°æ”¹è¿›æ–¹å‘å’Œå¢å¼ºå®¢æˆ·å¿ƒæ™ºåŠŸèƒ½çš„å€¾å‘åŠå…¶ç›¸äº’ä½œç”¨çš„å…³é”®ä¾æ®æ–¹é¢çš„ç»¼åˆç†è§£å¢å¼ºåº”ç”¨ï¼Œå³ï¼šAIé€šè¿‡è¯­ä¹‰åˆ†æå’Œè¯­å¢ƒç†è§£å®ç°ä¸äººç±»çš„æ„å›¾æ²Ÿé€šäº’åŠ¨çš„ç›®æ ‡å¹¶è¾¾åˆ°ç³»ç»Ÿä¼˜åŒ–çš„æ•ˆæœã€‚å¹¶ä¸”æ¡†æ¶è®¾è®¡è¿‡ç¨‹ä¸­è¿˜è€ƒè™‘äº†é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„æ„å›¾è¡¨è¾¾éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09339">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5d6599c1c84bbb06b7629cf4bca3107.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac136d96cfb3384cd9e1097f3914bb8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed05176deb679ad0b68fdfd4a1eda093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-674ad10564f9b6c9104f78b37dc00436.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fcb296d3e3d607f1e3cf2204c672af8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Seeing-Beyond-the-Scene-Enhancing-Vision-Language-Models-with-Interactional-Reasoning"><a href="#Seeing-Beyond-the-Scene-Enhancing-Vision-Language-Models-with-Interactional-Reasoning" class="headerlink" title="Seeing Beyond the Scene: Enhancing Vision-Language Models with   Interactional Reasoning"></a>Seeing Beyond the Scene: Enhancing Vision-Language Models with   Interactional Reasoning</h2><p><strong>Authors:Dayong Liang, Changmeng Zheng, Zhiyuan Wen, Yi Cai, Xiao-Yong Wei, Qing Li</strong></p>
<p>Traditional scene graphs primarily focus on spatial relationships, limiting vision-language modelsâ€™ (VLMs) ability to reason about complex interactions in visual scenes. This paper addresses two key challenges: (1) conventional detection-to-construction methods produce unfocused, contextually irrelevant relationship sets, and (2) existing approaches fail to form persistent memories for generalizing interaction reasoning to new scenes. We propose Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances VLMsâ€™ interactional reasoning through three complementary components. First, our dual-stream graph constructor combines SAM-powered spatial relation extraction with interaction-aware captioning to generate functionally salient scene graphs with spatial grounding. Second, we employ targeted interaction queries to activate VLMsâ€™ latent knowledge of object functionalities, converting passive recognition into active reasoning about how objects work together. Finally, we introduce a lone-term memory reinforcement learning strategy with a specialized interaction-focused reward function that transforms transient patterns into long-term reasoning heuristics. Extensive experiments demonstrate that our approach significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, with particularly strong improvements on complex scene understanding tasks. The source code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/open_upon_acceptance">https://github.com/open_upon_acceptance</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿåœºæ™¯å›¾ä¸»è¦å…³æ³¨ç©ºé—´å…³ç³»ï¼Œè¿™é™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¯¹è§†è§‰åœºæ™¯ä¸­å¤æ‚äº¤äº’è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚æœ¬æ–‡è§£å†³äº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ä¼ ç»Ÿçš„æ£€æµ‹æ„å»ºæ–¹æ³•äº§ç”Ÿçš„æ˜¯ä¸èšç„¦çš„ã€ä¸Šä¸‹æ–‡æ— å…³çš„å…³ç³»é›†ï¼›ï¼ˆ2ï¼‰ç°æœ‰æ–¹æ³•æ— æ³•å½¢æˆæŒä¹…çš„è®°å¿†ï¼Œä»¥å°†äº¤äº’æ¨ç†æ¨å¹¿åˆ°æ–°çš„åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†äº¤äº’å¢å¼ºåœºæ™¯å›¾æ¨ç†ï¼ˆISGRï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªäº’è¡¥çš„ç»„ä»¶å¢å¼ºVLMçš„äº¤äº’æ¨ç†èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„åŒæµå›¾æ„é€ å™¨ç»“åˆäº†SAMæ”¯æŒçš„ç©ºé—´å…³ç³»æå–å’Œäº¤äº’æ„ŸçŸ¥æè¿°ç”Ÿæˆå…·æœ‰ç©ºé—´å®šä½çš„åŠŸèƒ½æ˜¾è‘—åœºæ™¯å›¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„äº¤äº’æŸ¥è¯¢æ¿€æ´»VLMå¯¹å¯¹è±¡åŠŸèƒ½çš„æ½œåœ¨çŸ¥è¯†ï¼Œå°†è¢«åŠ¨è¯†åˆ«è½¬åŒ–ä¸ºå¯¹å¯¹è±¡å¦‚ä½•ååŒå·¥ä½œçš„ä¸»åŠ¨æ¨ç†ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå•ä¸€æœ¯è¯­è®°å¿†å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ï¼Œä»¥åŠä¸€ç§ä¸“é—¨çš„ä»¥äº¤äº’ä¸ºä¸­å¿ƒçš„å¥–åŠ±å‡½æ•°ï¼Œå°†çŸ­æš‚çš„æ¨¡å¼è½¬åŒ–ä¸ºé•¿æœŸçš„æ¨ç†å¯å‘å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äº¤äº’ç¹é‡çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åœºæ™¯ç†è§£ä»»åŠ¡ä¸Šçš„æ”¹è¿›å°¤ä¸ºçªå‡ºã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/open_upon_acceptance%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/open_upon_acceptanceè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09118v1">PDF</a> </p>
<p><strong>Summary</strong><br>ä¼ ç»Ÿåœºæ™¯å›¾ä¸»è¦å…³æ³¨ç©ºé—´å…³ç³»ï¼Œé™åˆ¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹è§†è§‰åœºæ™¯ä¸­å¤æ‚äº¤äº’è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚æœ¬æ–‡è§£å†³ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆ1ï¼‰å¸¸è§„æ£€æµ‹æ„å»ºæ–¹æ³•äº§ç”Ÿçš„ä¸é›†ä¸­ã€ä¸Šä¸‹æ–‡æ— å…³çš„å…³ç³»é›†ï¼›ï¼ˆ2ï¼‰ç°æœ‰æ–¹æ³•ç¼ºä¹é€šç”¨äº¤äº’æ¨ç†çš„æŒä¹…è®°å¿†å½¢æˆã€‚æå‡ºäº¤äº’å¢å¼ºåœºæ™¯å›¾æ¨ç†ï¼ˆISGRï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªäº’è¡¥ç»„ä»¶å¢å¼ºVLMsçš„äº¤äº’æ¨ç†èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„åŒæµå›¾æ„é€ å™¨ç»“åˆSAMæ”¯æŒçš„ç©ºé—´å…³ç³»æå–å’Œäº¤äº’æ„ŸçŸ¥æè¿°ç”Ÿæˆå…·æœ‰ç©ºé—´å®šä½çš„åŠŸèƒ½æ˜¾è‘—åœºæ™¯å›¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„äº¤äº’æŸ¥è¯¢æ¿€æ´»VLMså¯¹å¯¹è±¡åŠŸèƒ½çš„æ½œåœ¨è®¤è¯†ï¼Œå°†è¢«åŠ¨è¯†åˆ«è½¬åŒ–ä¸ºå¯¹å¯¹è±¡å¦‚ä½•ååŒå·¥ä½œçš„ä¸»åŠ¨æ¨ç†ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹é•¿æœŸè®°å¿†å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œå¹¶é…å¤‡äº†ä¸“é—¨çš„äº¤äº’é‡ç‚¹å¥–åŠ±åŠŸèƒ½ï¼Œå°†çŸ­æš‚æ¨¡å¼è½¬åŒ–ä¸ºé•¿æœŸæ¨ç†ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äº¤äº’é‡æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œåœ¨å¤æ‚åœºæ™¯ç†è§£ä»»åŠ¡ä¸Šçš„æ”¹è¿›å°¤å…¶æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿåœºæ™¯å›¾ä¸»è¦å…³æ³¨ç©ºé—´å…³ç³»ï¼Œé™åˆ¶äº†VLMsåœ¨å¤æ‚äº¤äº’æ¨ç†ä¸­çš„åº”ç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•äº§ç”Ÿçš„ä¸é›†ä¸­ã€ä¸Šä¸‹æ–‡æ— å…³çš„å…³ç³»é›†æ˜¯å½±å“VLMsæ€§èƒ½çš„å…³é”®é—®é¢˜ã€‚</li>
<li>æå‡ºçš„ISGRæ¡†æ¶é€šè¿‡ä¸‰ä¸ªäº’è¡¥ç»„ä»¶å¢å¼ºVLMsçš„äº¤äº’æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŒæµå›¾æ„é€ å™¨ç»“åˆç©ºé—´å…³ç³»æå–å’Œäº¤äº’æ„ŸçŸ¥æè¿°ç”ŸæˆåŠŸèƒ½æ˜¾è‘—åœºæ™¯å›¾ã€‚</li>
<li>é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„äº¤äº’æŸ¥è¯¢æ¿€æ´»VLMså¯¹å¯¹è±¡åŠŸèƒ½çš„æ½œåœ¨è®¤è¯†ã€‚</li>
<li>å¼•å…¥é•¿æœŸè®°å¿†å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œé…å¤‡ä¸“é—¨çš„äº¤äº’é‡ç‚¹å¥–åŠ±åŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e189f2b17c1990ef72b29d606af3b9aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62a4c21fe8648c1b649e55c0bae74c53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f81d0cd80fb76f0b5a9f6c6230fb8937.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1e35bb191b1bd872c1807e17d600be3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a60e64dc090370ce10175aa8bf9ebd2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Known-Decision-Making-with-Counterfactual-Reasoning-Decision-Transformer"><a href="#Beyond-the-Known-Decision-Making-with-Counterfactual-Reasoning-Decision-Transformer" class="headerlink" title="Beyond the Known: Decision Making with Counterfactual Reasoning Decision   Transformer"></a>Beyond the Known: Decision Making with Counterfactual Reasoning Decision   Transformer</h2><p><strong>Authors:Minh Hoang Nguyen, Linh Le Pham Van, Thommen George Karimpanal, Sunil Gupta, Hung Le</strong></p>
<p>Decision Transformers (DT) play a crucial role in modern reinforcement learning, leveraging offline datasets to achieve impressive results across various domains. However, DT requires high-quality, comprehensive data to perform optimally. In real-world applications, the lack of training data and the scarcity of optimal behaviours make training on offline datasets challenging, as suboptimal data can hinder performance. To address this, we propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel framework inspired by counterfactual reasoning. CRDT enhances DT ability to reason beyond known data by generating and utilizing counterfactual experiences, enabling improved decision-making in unseen scenarios. Experiments across Atari and D4RL benchmarks, including scenarios with limited data and altered dynamics, demonstrate that CRDT outperforms conventional DT approaches. Additionally, reasoning counterfactually allows the DT agent to obtain stitching abilities, combining suboptimal trajectories, without architectural modifications. These results highlight the potential of counterfactual reasoning to enhance reinforcement learning agentsâ€™ performance and generalization capabilities. </p>
<blockquote>
<p>å†³ç­–è½¬æ¢å™¨ï¼ˆDTï¼‰åœ¨ç°ä»£å¼ºåŒ–å­¦ä¹ ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒåˆ©ç”¨ç¦»çº¿æ•°æ®é›†åœ¨ä¸åŒé¢†åŸŸå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼ŒDTéœ€è¦é«˜è´¨é‡ã€å…¨é¢çš„æ•°æ®æ‰èƒ½æœ€ä¼˜åœ°æ‰§è¡Œã€‚åœ¨çœŸå®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œè®­ç»ƒæ•°æ®çš„ç¼ºä¹ä»¥åŠæœ€ä¼˜è¡Œä¸ºçš„ç¨€ç¼ºä½¿å¾—åœ¨ç¦»çº¿æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ¬¡ä¼˜æ•°æ®å¯èƒ½ä¼šé˜»ç¢æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåäº‹å®æ¨ç†çš„å†³ç­–è½¬æ¢å™¨ï¼ˆCRDTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå—åäº‹å®æ¨ç†å¯å‘çš„æ–°å‹æ¡†æ¶ã€‚CRDTé€šè¿‡ç”Ÿæˆå’Œåˆ©ç”¨åäº‹å®ç»éªŒï¼Œå¢å¼ºäº†DTåœ¨å·²çŸ¥æ•°æ®ä¹‹å¤–çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å®ƒåœ¨æœªè§è¿‡çš„åœºæ™¯ä¸­èƒ½å¤Ÿåšå‡ºæ›´å¥½çš„å†³ç­–ã€‚åœ¨Atariå’ŒD4RLåŸºå‡†æµ‹è¯•çš„å®éªŒä¸­ï¼ŒåŒ…æ‹¬æ•°æ®æœ‰é™å’ŒåŠ¨æ€å˜åŒ–çš„æƒ…å†µï¼Œè¡¨æ˜CRDTåœ¨å¸¸è§„DTæ–¹æ³•ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œåäº‹å®æ¨ç†è¿˜ä½¿å¾—DTä»£ç†èƒ½å¤Ÿè·å–ç¼åˆèƒ½åŠ›ï¼Œå³ç»“åˆæ¬¡ä¼˜è½¨è¿¹ï¼Œè€Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åäº‹å®æ¨ç†åœ¨å¢å¼ºå¼ºåŒ–å­¦ä¹ ä»£ç†çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09114v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå†³ç­–è½¬æ¢å™¨ï¼ˆDTï¼‰åœ¨ç°ä»£å¼ºåŒ–å­¦ä¹ ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œå®ƒåˆ©ç”¨ç¦»çº¿æ•°æ®é›†åœ¨ä¸åŒé¢†åŸŸå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼ŒDTéœ€è¦é«˜è´¨é‡ã€å…¨é¢çš„æ•°æ®æ‰èƒ½æœ€ä¼˜åœ°æ‰§è¡Œã€‚åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­ï¼Œè®­ç»ƒæ•°æ®çš„ç¼ºä¹ä»¥åŠæœ€ä¼˜è¡Œä¸ºçš„ç¨€ç¼ºä½¿å¾—åœ¨ç¦»çº¿æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ¬¡ä¼˜æ•°æ®å¯èƒ½ä¼šé˜»ç¢æ€§èƒ½ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å—åäº‹å®æ¨ç†å¯å‘çš„å†³ç­–è½¬æ¢å™¨ï¼ˆCRDTï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ã€‚CRDTé€šè¿‡ç”Ÿæˆå’Œåˆ©ç”¨åäº‹å®ç»éªŒï¼Œå¢å¼ºäº†DTåœ¨æœªçŸ¥æ•°æ®ä¹‹å¤–çš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œå®ç°äº†åœ¨æœªè§åœºæ™¯ä¸­çš„å†³ç­–æ”¹è¿›ã€‚åœ¨Atariå’ŒD4RLåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒï¼ŒåŒ…æ‹¬æ•°æ®æœ‰é™å’ŒåŠ¨æ€å˜åŒ–çš„æƒ…å†µï¼Œè¡¨æ˜CRDTä¼˜äºä¼ ç»ŸDTæ–¹æ³•ã€‚æ­¤å¤–ï¼Œåäº‹å®æ¨ç†è¿˜è®©DTä»£ç†è·å¾—äº†ç¼åˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿç»“åˆæ¬¡ä¼˜è½¨è¿¹ï¼Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åäº‹å®æ¨ç†åœ¨å¢å¼ºå¼ºåŒ–å­¦ä¹ ä»£ç†çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å†³ç­–è½¬æ¢å™¨ï¼ˆDTï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­åˆ©ç”¨ç¦»çº¿æ•°æ®é›†å–å¾—æ˜¾è‘—æˆæœï¼Œä½†éœ€è¦é«˜è´¨é‡æ•°æ®æ‰èƒ½æœ€ä¼˜æ‰§è¡Œã€‚</li>
<li>åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œè®­ç»ƒæ•°æ®ç¼ºä¹å’Œæœ€ä¼˜è¡Œä¸ºç¨€ç¼ºæ˜¯è®­ç»ƒDTçš„æŒ‘æˆ˜ï¼Œæ¬¡ä¼˜æ•°æ®å¯èƒ½é˜»ç¢æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„Counterfactual Reasoning Decision Transformerï¼ˆCRDTï¼‰æ¡†æ¶é€šè¿‡ç”Ÿæˆå’Œåˆ©ç”¨åäº‹å®ç»éªŒï¼Œå¢å¼ºäº†DTçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CRDTåœ¨æœªè§åœºæ™¯ä¸­çš„å†³ç­–æ”¹è¿›è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™å’ŒåŠ¨æ€å˜åŒ–çš„æƒ…å†µä¸‹ã€‚</li>
<li>åäº‹å®æ¨ç†ä½¿DTä»£ç†è·å¾—ç»“åˆæ¬¡ä¼˜è½¨è¿¹çš„èƒ½åŠ›ï¼Œç§°ä¸ºâ€œç¼åˆèƒ½åŠ›â€ã€‚</li>
<li>åäº‹å®æ¨ç†æœ‰åŠ©äºæå‡å¼ºåŒ–å­¦ä¹ ä»£ç†çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5147906f943237382274fc184b0c9ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73cf59629adfd59eeb7568d276c59251.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Improved-Algorithms-for-Differentially-Private-Language-Model-Alignment"><a href="#Improved-Algorithms-for-Differentially-Private-Language-Model-Alignment" class="headerlink" title="Improved Algorithms for Differentially Private Language Model Alignment"></a>Improved Algorithms for Differentially Private Language Model Alignment</h2><p><strong>Authors:Keyu Chen, Hao Tang, Qinglin Liu, Yizhao Xu</strong></p>
<p>Language model alignment is crucial for ensuring that large language models (LLMs) align with human preferences, yet it often involves sensitive user data, raising significant privacy concerns. While prior work has integrated differential privacy (DP) with alignment techniques, their performance remains limited. In this paper, we propose novel algorithms for privacy-preserving alignment and rigorously analyze their effectiveness across varying privacy budgets and models. Our framework can be deployed on two celebrated alignment techniques, namely direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF). Through systematic experiments on large-scale language models, we demonstrate that our approach achieves state-of-the-art performance. Notably, one of our algorithms, DP-AdamW, combined with DPO, surpasses existing methods, improving alignment quality by up to 15% under moderate privacy budgets ({\epsilon}&#x3D;2-5). We further investigate the interplay between privacy guarantees, alignment efficacy, and computational demands, providing practical guidelines for optimizing these trade-offs. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹å¯¹é½å¯¹äºç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½ä¸€è‡´è‡³å…³é‡è¦ï¼Œç„¶è€Œè¿™é€šå¸¸æ¶‰åŠæ•æ„Ÿç”¨æˆ·æ•°æ®ï¼Œå¼•å‘äº†ä¸¥é‡çš„éšç§æ‹…å¿§ã€‚å°½ç®¡å…ˆå‰çš„å·¥ä½œå·²ç»å°†å·®åˆ†éšç§ï¼ˆDPï¼‰ä¸å¯¹é½æŠ€æœ¯ç›¸ç»“åˆï¼Œä½†å…¶æ€§èƒ½ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºéšç§ä¿æŠ¤å¯¹é½çš„æ–°å‹ç®—æ³•ï¼Œå¹¶åœ¨ä¸åŒçš„éšç§é¢„ç®—å’Œæ¨¡å‹ä¸Šå¯¹å…¶æœ‰æ•ˆæ€§è¿›è¡Œäº†ä¸¥æ ¼åˆ†æã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥éƒ¨ç½²åœ¨ä¸¤ç§è‘—åçš„å¯¹é½æŠ€æœ¯ä¸Šï¼Œå³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚é€šè¿‡å¯¹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç®—æ³•ä¹‹ä¸€DP-AdamWä¸DPOç›¸ç»“åˆï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨é€‚åº¦çš„éšç§é¢„ç®—ï¼ˆÎµ&#x3D;2-5ï¼‰ä¸‹ï¼Œæé«˜äº†é«˜è¾¾15%çš„å¯¹é½è´¨é‡ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ç ”ç©¶äº†éšç§ä¿è¯ã€å¯¹é½æ•ˆæœå’Œè®¡ç®—éœ€æ±‚ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä¸ºä¼˜åŒ–è¿™äº›æƒè¡¡æä¾›äº†å®ç”¨æŒ‡å—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08849v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºåœ¨æ­¤è¿‡ç¨‹ä¸­æ¶‰åŠæ•æ„Ÿç”¨æˆ·æ•°æ®å¸¦æ¥çš„éšç§æ‹…å¿§ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»å°è¯•å°†å·®åˆ†éšç§ï¼ˆDPï¼‰ä¸å¯¹é½æŠ€æœ¯ç›¸ç»“åˆï¼Œä½†å…¶æ€§èƒ½ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æå‡ºæ–°å‹éšç§ä¿æŠ¤å¯¹é½ç®—æ³•ï¼Œå¹¶åœ¨ä¸åŒéšç§é¢„ç®—å’Œæ¨¡å‹ä¸Šä¸¥æ ¼åˆ†æå…¶æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶å¯å°†æ–°å‹ç®—æ³•éƒ¨ç½²åœ¨ä¸¤ç§è‘—åçš„å¯¹é½æŠ€æœ¯ï¼Œå³ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸Šã€‚é€šè¿‡å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿå®éªŒï¼Œè¯æ˜è¯¥æ–¹æ³•è¾¾åˆ°ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ä¸DPOç»“åˆçš„DP-AdamWç®—æ³•åœ¨é€‚åº¦éšç§é¢„ç®—ä¸‹ï¼Œå¯æé«˜å¯¹é½è´¨é‡è¾¾15%ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†éšç§ä¿éšœã€å¯¹é½æ•ˆæœå’Œè®¡ç®—éœ€æ±‚ä¹‹é—´çš„å¹³è¡¡ï¼Œä¸ºä¼˜åŒ–è¿™äº›æƒè¡¡æä¾›äº†å®ç”¨æŒ‡å—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½è‡³å…³é‡è¦ï¼Œä½†æ¶‰åŠæ•æ„Ÿç”¨æˆ·æ•°æ®å¼•å‘éšç§æ‹…å¿§ã€‚</li>
<li>ç°æœ‰ç»“åˆå·®åˆ†éšç§ï¼ˆDPï¼‰ä¸å¯¹é½æŠ€æœ¯çš„æ€§èƒ½æœ‰é™ã€‚</li>
<li>æå‡ºæ–°å‹éšç§ä¿æŠ¤å¯¹é½ç®—æ³•ï¼Œå¯éƒ¨ç½²åœ¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸Šã€‚</li>
<li>ç³»ç»Ÿå®éªŒè¯æ˜è¯¥æ–¹æ³•è¾¾åˆ°ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚</li>
<li>DP-AdamWç®—æ³•ä¸DPOç»“åˆï¼Œåœ¨é€‚åº¦éšç§é¢„ç®—ä¸‹ï¼Œæé«˜å¯¹é½è´¨é‡è¾¾15%ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†éšç§ä¿éšœã€å¯¹é½æ•ˆæœå’Œè®¡ç®—éœ€æ±‚ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c45321f80b4ece95dd057bcbbfbdd743.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d88e39f1cf4bc002c79554866bf9ce97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a48295f31b41fbd81878a2b26a1dabbd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Agent-RL-Scaling-Law-Agent-RL-with-Spontaneous-Code-Execution-for-Mathematical-Problem-Solving"><a href="#Agent-RL-Scaling-Law-Agent-RL-with-Spontaneous-Code-Execution-for-Mathematical-Problem-Solving" class="headerlink" title="Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for   Mathematical Problem Solving"></a>Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for   Mathematical Problem Solving</h2><p><strong>Authors:Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang</strong></p>
<p>Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \href{<a target="_blank" rel="noopener" href="https://github.com/yyht/openrlhf_async_pipline%7D%7Bhttps://github.com/yyht/openrlhf/_async/_pipline%7D">https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\_async\_pipline}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿›è¡Œéœ€è¦ç²¾ç¡®ã€å¯éªŒè¯è®¡ç®—çš„æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶ç»å¸¸é‡åˆ°å›°éš¾ã€‚è™½ç„¶åŸºäºç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºäº†æ–‡æœ¬æ¨ç†èƒ½åŠ›ï¼Œä½†äº†è§£æ™ºèƒ½ä½“å¦‚ä½•è‡ªä¸»å­¦ä¹ åˆ©ç”¨å¦‚ä»£ç æ‰§è¡Œç­‰å¤–éƒ¨å·¥å…·ä»ç„¶è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ç ”ç©¶äº†åŸºäºç»“æœå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åœ¨å·¥å…·é›†æˆæ¨ç†ï¼ˆTool-Integrated Reasoningï¼‰ä¸­çš„åº”ç”¨ï¼Œå³ZeroTIRã€‚æˆ‘ä»¬è®­ç»ƒåŸºç¡€LLMï¼Œä½¿å…¶èƒ½å¤Ÿé’ˆå¯¹æ•°å­¦é—®é¢˜è‡ªå‘åœ°ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ï¼Œè€Œæ— éœ€ç›‘ç£å·¥å…·ä½¿ç”¨ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†éšç€å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æŒ‡æ ‡å¯ä»¥é¢„æµ‹åœ°æ‰©å±•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¼ºçƒˆçš„æ­£ç›¸å…³å…³ç³»ï¼Œè®­ç»ƒæ­¥éª¤çš„å¢åŠ å¯¼è‡´è‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å“åº”é•¿åº¦å’Œæœ€ç»ˆä»»åŠ¡å‡†ç¡®åº¦çš„æé«˜ã€‚è¿™è¡¨æ˜åœ¨è®­ç»ƒä¸­æ‰€æŠ•å…¥çš„è®¡ç®—åŠªåŠ›ä¸æœ‰æ•ˆå·¥å…·å¢å¼ºæ¨ç†ç­–ç•¥çš„å‡ºç°ä¹‹é—´å­˜åœ¨å¯é‡åŒ–çš„å…³ç³»ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªç¨³å¥çš„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªè§£è€¦çš„ä»£ç æ‰§è¡Œç¯å¢ƒï¼Œå¹¶åœ¨æ ‡å‡†å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¡†æ¶ä¸­éªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚å®éªŒè¡¨æ˜ï¼ŒZeroTIRåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—è¶…è¶Šäº†éå·¥å…·ZeroRLåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¦‚ä½•è·å¾—å’Œæ‰©å±•æ™ºèƒ½ä½“çš„è‡ªä¸»å·¥å…·ä½¿ç”¨æä¾›äº†åŸºç¡€ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯å¤åˆ¶çš„åŸºå‡†ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/yyht/openrlhf_async_pipeline">https://github.com/yyht/openrlhf_async_pipline</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07773v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†éœ€è¦ç²¾ç¡®å¯éªŒè¯è®¡ç®—çš„æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ç»“æœå¯¼å‘å¥–åŠ±æ¥å¢å¼ºæ–‡æœ¬æ¨ç†ä¸­çš„å·¥å…·é›†æˆèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯æ¢ç©¶äº†åä¸ºZeroTIRçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨è®­ç»ƒåŸºç¡€LLMè‡ªä¸»ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç è§£å†³æ•°å­¦é—®é¢˜ï¼Œè€Œæ— éœ€ç›‘ç£æ€§çš„å·¥å…·ä½¿ç”¨ç¤ºä¾‹ã€‚ç ”ç©¶çš„å…³é”®è´¡çŒ®åœ¨äºå±•ç¤ºäº†éšç€RLè®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æŒ‡æ ‡çš„å¯é¢„æµ‹æ€§å¢é•¿ã€‚ç‰¹åˆ«æ˜¯è§‚å¯Ÿåˆ°è®­ç»ƒæ­¥éª¤çš„å¢åŠ ä¸è‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å“åº”é•¿åº¦ä»¥åŠæœ€ç»ˆä»»åŠ¡å‡†ç¡®ç‡çš„æ­£ç›¸å…³å…³ç³»ã€‚è¿™æš—ç¤ºäº†è®­ç»ƒæŠ•å…¥çš„è®¡ç®—åŠªåŠ›ä¸æœ‰æ•ˆå·¥å…·å¢å¼ºæ¨ç†ç­–ç•¥çš„å‡ºç°ä¹‹é—´å­˜åœ¨å¯é‡åŒ–çš„å…³ç³»ã€‚ç ”ç©¶å®ç°äº†ç¨³å¥çš„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ç‹¬ç«‹çš„ä»£ç æ‰§è¡Œç¯å¢ƒï¼Œå¹¶åœ¨æ ‡å‡†çš„RLç®—æ³•å’Œæ¡†æ¶ä¸ŠéªŒè¯äº†è¿™äº›å‘ç°ã€‚å®éªŒè¡¨æ˜ï¼ŒZeroTIRåœ¨éå·¥å…·ZeroRLåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—è¶…è¶Šäº†å¯¹æ•°å­¦æŒ‘æˆ˜çš„åº”å¯¹ã€‚è¿™äº›å‘ç°ä¸ºè‡ªä¸»å·¥å…·ä½¿ç”¨çš„è·å–å’Œå†…åœ¨Agent RLä¸­çš„æ‰©å±•æä¾›äº†åŸºç¡€ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å¯å¤åˆ¶çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´å¼ºå¤§çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºå¢å¼ºå·¥å…·é›†æˆèƒ½åŠ›ï¼Œä¿ƒè¿›è‡ªä¸»ç”Ÿæˆå’Œæ‰§è¡ŒPythonä»£ç è¿›è¡Œæ•°å­¦è®¡ç®—ã€‚</li>
<li>éšç€RLè®­ç»ƒçš„è¿›è¡Œï¼Œå…³é”®æŒ‡æ ‡å¦‚è‡ªå‘ä»£ç æ‰§è¡Œé¢‘ç‡ã€å¹³å‡å“åº”é•¿åº¦å’Œä»»åŠ¡å‡†ç¡®ç‡å‘ˆç°æ­£å‘å¢é•¿è¶‹åŠ¿ã€‚</li>
<li>è®­ç»ƒæŠ•å…¥çš„è®¡ç®—åŠªåŠ›ä¸æœ‰æ•ˆå·¥å…·å¢å¼ºæ¨ç†ç­–ç•¥çš„å‡ºç°å­˜åœ¨å¯é‡åŒ–çš„å…³ç³»ã€‚</li>
<li>å®æ–½äº†ä¸€ä¸ªåŒ…å«ç‹¬ç«‹ä»£ç æ‰§è¡Œç¯å¢ƒçš„ç¨³å¥æ¡†æ¶æ¥éªŒè¯å‘ç°ã€‚</li>
<li>ZeroTIRæ–¹æ³•åœ¨åº”å¯¹æ•°å­¦æŒ‘æˆ˜æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†éå·¥å…·ZeroRLåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ddf94d9d9cf9d7a6309cc5cd87570f60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f2c8423fa863c4270331b0d5e445251.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38dca8f3d7b54fe586e911c87832c5ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a125a26e4ebf81a82a980542b6bb06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6da1a70a1a8081d35005d0c8b93d0ce0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Q-Heart-ECG-Question-Answering-via-Knowledge-Informed-Multimodal-LLMs"><a href="#Q-Heart-ECG-Question-Answering-via-Knowledge-Informed-Multimodal-LLMs" class="headerlink" title="Q-Heart: ECG Question Answering via Knowledge-Informed Multimodal LLMs"></a>Q-Heart: ECG Question Answering via Knowledge-Informed Multimodal LLMs</h2><p><strong>Authors:Hung Manh Pham, Jialu Tang, Aaqib Saeed, Dong Ma</strong></p>
<p>Electrocardiography (ECG) offers critical cardiovascular insights, such as identifying arrhythmias and myocardial ischemia, but enabling automated systems to answer complex clinical questions directly from ECG signals (ECG-QA) remains a significant challenge. Current approaches often lack robust multimodal reasoning capabilities or rely on generic architectures ill-suited for the nuances of physiological signals. We introduce Q-Heart, a novel multimodal framework designed to bridge this gap. Q-Heart leverages a powerful, adapted ECG encoder and integrates its representations with textual information via a specialized ECG-aware transformer-based mapping layer. Furthermore, Q-Heart leverages dynamic prompting and retrieval of relevant historical clinical reports to guide tuning the language model toward knowledge-aware ECG reasoning. Extensive evaluations on the benchmark ECG-QA dataset show Q-Heart achieves state-of-the-art performance, outperforming existing methods by a 4% improvement in exact match accuracy. Our work demonstrates the effectiveness of combining domain-specific architectural adaptations with knowledge-augmented LLM instruction tuning for complex physiological ECG analysis, paving the way for more capable and potentially interpretable clinical patient care systems. </p>
<blockquote>
<p>å¿ƒç”µå›¾ï¼ˆECGï¼‰æä¾›äº†å…³äºå¿ƒè¡€ç®¡çš„å…³é”®æ´å¯Ÿï¼Œå¦‚è¯†åˆ«å¿ƒå¾‹å¤±å¸¸å’Œå¿ƒè‚Œç¼ºè¡€ï¼Œä½†æ˜¯è®©è‡ªåŠ¨åŒ–ç³»ç»Ÿç›´æ¥ä»å¿ƒç”µå›¾ä¿¡å·ä¸­å›ç­”å¤æ‚ä¸´åºŠé—®é¢˜ï¼ˆECG-QAï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•å¾€å¾€ç¼ºä¹ç¨³å¥çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›ï¼Œæˆ–è€…ä¾èµ–äºå¯¹äºç”Ÿç†ä¿¡å·ç»†å¾®å·®åˆ«ä¸å¤Ÿé€‚åº”çš„é€šç”¨æ¶æ„ã€‚æˆ‘ä»¬å¼•å…¥äº†Q-Heartï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹å¤šæ¨¡å¼æ¡†æ¶ï¼Œæ—¨åœ¨å¼¥è¡¥è¿™ä¸€å·®è·ã€‚Q-Heartåˆ©ç”¨å¼ºå¤§çš„é€‚åº”æ€§å¿ƒç”µå›¾ç¼–ç å™¨å’Œé€šè¿‡ä¸“ä¸šåŒ–çš„å¿ƒç”µå›¾æ„ŸçŸ¥å˜å‹å™¨æ˜ å°„å±‚å°†å…¶è¡¨ç¤ºä¸æ–‡æœ¬ä¿¡æ¯é›†æˆåœ¨ä¸€èµ·ã€‚æ­¤å¤–ï¼ŒQ-Heartåˆ©ç”¨åŠ¨æ€æç¤ºå’Œæ£€ç´¢ç›¸å…³çš„å†å²ä¸´åºŠæŠ¥å‘Šæ¥æŒ‡å¯¼è¯­è¨€æ¨¡å‹å‘çŸ¥è¯†æ„ŸçŸ¥å¿ƒç”µå›¾æ¨ç†çš„è°ƒæ•´ã€‚åœ¨åŸºå‡†å¿ƒç”µå›¾é—®ç­”æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒQ-Heartè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨ç²¾ç¡®åŒ¹é…å‡†ç¡®æ€§æ–¹é¢æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†4%ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†ç»“åˆç‰¹å®šé¢†åŸŸçš„æ¶æ„é€‚åº”æ€§ä¸çŸ¥è¯†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡ä»¤è°ƒæ•´ï¼Œå¯¹äºå¤æ‚ç”Ÿç†å¿ƒç”µå›¾åˆ†æçš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ›´å¼ºå¤§å’Œå¯è§£é‡Šçš„ä¸´åºŠæ‚£è€…æŠ¤ç†ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06296v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¿ƒç”µå›¾ï¼ˆECGï¼‰åœ¨è¯Šæ–­å¿ƒè¡€ç®¡ç–¾ç—…æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œå¦‚å¿ƒå¾‹å¤±å¸¸å’Œå¿ƒè‚Œç¼ºè¡€ç­‰ã€‚ç„¶è€Œï¼Œä½¿è‡ªåŠ¨åŒ–ç³»ç»Ÿç›´æ¥ä»å¿ƒç”µå›¾ä¿¡å·ä¸­å›ç­”å¤æ‚ä¸´åºŠé—®é¢˜ï¼ˆECG-QAï¼‰ä»å­˜åœ¨æŒ‘æˆ˜ã€‚å½“å‰æ–¹æ³•ç¼ºä¹ç¨³å¥çš„å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›æˆ–ä¾èµ–äºä¸é€‚åˆç”Ÿç†ä¿¡å·ç»†å¾®å·®åˆ«çš„é€šç”¨æ¶æ„ã€‚æœ¬ç ”ç©¶å¼•å…¥Q-Heartï¼Œä¸€ç§æ–°å‹å¤šæ¨¡å¼æ¡†æ¶ï¼Œæ—¨åœ¨å¼¥è¡¥è¿™ä¸€å·®è·ã€‚Q-Heartåˆ©ç”¨å¼ºå¤§çš„é€‚åº”æ€§å¿ƒç”µå›¾ç¼–ç å™¨å’Œé€šè¿‡ä¸“ä¸šçš„å¿ƒç”µå›¾æ„ŸçŸ¥å˜å‹å™¨æ˜ å°„å±‚ä¸æ–‡æœ¬ä¿¡æ¯é›†æˆã€‚æ­¤å¤–ï¼ŒQ-Heartåˆ©ç”¨åŠ¨æ€æç¤ºå’Œæ£€ç´¢ç›¸å…³çš„å†å²ä¸´åºŠæŠ¥å‘Šæ¥æŒ‡å¯¼è¯­è¨€æ¨¡å‹å‘çŸ¥è¯†æ„ŸçŸ¥å¿ƒç”µå›¾æ¨ç†è°ƒæ•´ã€‚åœ¨åŸºå‡†å¿ƒç”µå›¾é—®ç­”æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒQ-Heartè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨ç²¾ç¡®åŒ¹é…å‡†ç¡®æ€§æ–¹é¢æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†4%ã€‚æœ¬ç ”ç©¶ç»“åˆäº†é¢†åŸŸç‰¹å®šçš„æ¶æ„é€‚åº”æ€§å’ŒçŸ¥è¯†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡ä»¤è°ƒæ•´ï¼Œä¸ºå¤æ‚ç”Ÿç†å¿ƒç”µå›¾åˆ†æçš„æœ‰æ•ˆæ€§é“ºå¹³äº†é“è·¯ï¼Œä¸ºæ›´å¼ºå¤§å’Œå¯è§£é‡Šçš„ä¸´åºŠæ‚£è€…æŠ¤ç†ç³»ç»Ÿå¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ECGåœ¨å¿ƒè¡€ç®¡ç–¾ç—…è¯Šæ–­ä¸­è‡³å…³é‡è¦ï¼Œä½†è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„ECGé—®ç­”ï¼ˆECG-QAï¼‰ä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•ç¼ºä¹å¤šæ¨¡å¼æ¨ç†èƒ½åŠ›æˆ–ä¸é€‚é…ç”Ÿç†ä¿¡å·çš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>Q-Heartæ˜¯ä¸€ç§æ–°å‹å¤šæ¨¡å¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œç»“åˆå¼ºå¤§çš„å¿ƒç”µå›¾ç¼–ç å™¨å’Œä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>Q-Heartåˆ©ç”¨åŠ¨æ€æç¤ºå’Œæ£€ç´¢å†å²ä¸´åºŠæŠ¥å‘Šæ¥æŒ‡å¯¼è¯­è¨€æ¨¡å‹è°ƒæ•´ã€‚</li>
<li>Q-Heartåœ¨åŸºå‡†å¿ƒç”µå›¾é—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç²¾ç¡®åŒ¹é…å‡†ç¡®æ€§æé«˜4%ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†ç‰¹å®šé¢†åŸŸæ¶æ„å’ŒçŸ¥è¯†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡ä»¤è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18f58c018c47724508de143759f7c6c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f06bf1ab25490bc5f6a850ce46a91021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f741fe1631fb84a892694c0e33be7d06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd0c468e08c5407f39e4ce0bb3ccf964.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2129d8230ee9b220c4b16417b43d347.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8474907d26ee87ddb14b0167ecb13eae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc393d4a719af5bc938b040904c1c708.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DMRL-Data-and-Model-aware-Reward-Learning-for-Data-Extraction"><a href="#DMRL-Data-and-Model-aware-Reward-Learning-for-Data-Extraction" class="headerlink" title="DMRL: Data- and Model-aware Reward Learning for Data Extraction"></a>DMRL: Data- and Model-aware Reward Learning for Data Extraction</h2><p><strong>Authors:Zhiqiang Wang, Ruoxi Cheng</strong></p>
<p>Large language models (LLMs) are inherently vulnerable to unintended privacy breaches. Consequently, systematic red-teaming research is essential for developing robust defense mechanisms. However, current data extraction methods suffer from several limitations: (1) rely on dataset duplicates (addressable via deduplication), (2) depend on prompt engineering (now countered by detection and defense), and (3) rely on random-search adversarial generation. To address these challenges, we propose DMRL, a Data- and Model-aware Reward Learning approach for data extraction. This technique leverages inverse reinforcement learning to extract sensitive data from LLMs. Our method consists of two main components: (1) constructing an introspective reasoning dataset that captures leakage mindsets to guide model behavior, and (2) training reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization based on task difficulty at both the data and model levels. Comprehensive experiments across various LLMs demonstrate that DMRL outperforms all baseline methods in data extraction performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤©ç”Ÿå®¹æ˜“å—åˆ°æ„å¤–çš„éšç§æ³„éœ²å¨èƒã€‚å› æ­¤ï¼Œè¿›è¡Œç³»ç»Ÿçš„çº¢é˜Ÿç ”ç©¶å¯¹äºå¼€å‘ç¨³å¥çš„é˜²å¾¡æœºåˆ¶è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ•°æ®æå–æ–¹æ³•å­˜åœ¨å‡ ä¸ªå±€é™æ€§ï¼šï¼ˆ1ï¼‰ä¾èµ–äºæ•°æ®é›†é‡å¤ï¼ˆå¯é€šè¿‡å»é‡è§£å†³ï¼‰ï¼Œï¼ˆ2ï¼‰ä¾èµ–äºæç¤ºå·¥ç¨‹ï¼ˆç°åœ¨å¯ä»¥é€šè¿‡æ£€æµ‹å’Œé˜²å¾¡æ¥åº”å¯¹ï¼‰ï¼Œï¼ˆ3ï¼‰ä¾èµ–äºéšæœºæœç´¢å¯¹æŠ—ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DMRLï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ•°æ®æå–çš„æ•°æ®å’Œæ¨¡å‹æ„ŸçŸ¥å¥–åŠ±å­¦ä¹ æ–¹æ³•ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨é€†å‘å¼ºåŒ–å­¦ä¹ ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–æ•æ„Ÿæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰æ„å»ºä¸€ä¸ªå†…çœæ¨ç†æ•°æ®é›†ï¼Œæ•æ‰æ³„éœ²å¿ƒæ€ä»¥å¼•å¯¼æ¨¡å‹è¡Œä¸ºï¼Œï¼ˆ2ï¼‰ä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œæ ¹æ®æ•°æ®å’Œæ¨¡å‹å±‚é¢çš„ä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´ä¼˜åŒ–ã€‚åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDMRLåœ¨æ•°æ®æå–æ€§èƒ½ä¸Šä¼˜äºæ‰€æœ‰åŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06284v1">PDF</a> Data- and Model-aware Reward Learning for Data Extraction. arXiv   admin note: substantial text overlap with arXiv:2503.18991</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨æ„å¤–çš„éšç§æ³„éœ²é£é™©ï¼Œå› æ­¤éœ€è¦ç³»ç»Ÿæ€§çš„çº¢é˜Ÿç ”ç©¶æ¥å¼€å‘ç¨³å¥çš„é˜²å¾¡æœºåˆ¶ã€‚ä¸ºè§£å†³å½“å‰æ•°æ®æå–æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºDMRLï¼Œä¸€ç§æ•°æ®æ„ŸçŸ¥æ¨¡å‹å¥–åŠ±å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é€†å‘å¼ºåŒ–å­¦ä¹ ä»LLMä¸­æå–æ•æ„Ÿæ•°æ®ï¼ŒåŒ…æ‹¬æ„å»ºå†…çœæ¨ç†æ•°æ®é›†å’Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ä¸¤éƒ¨åˆ†ã€‚å®éªŒè¯æ˜ï¼ŒDMRLåœ¨æ•°æ®æå–æ€§èƒ½ä¸Šä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨éšç§æ³„éœ²é£é™©ï¼Œéœ€è¦ç³»ç»Ÿæ€§çº¢é˜Ÿç ”ç©¶è¿›è¡Œé˜²å¾¡ã€‚</li>
<li>å½“å‰æ•°æ®æå–æ–¹æ³•å­˜åœ¨ä¾èµ–æ•°æ®é›†é‡å¤ã€ä¾èµ–æç¤ºå·¥ç¨‹å’Œéšæœºæœç´¢å¯¹æŠ—ç”Ÿæˆç­‰å±€é™æ€§ã€‚</li>
<li>DMRLæ˜¯ä¸€ç§æ•°æ®æ„ŸçŸ¥æ¨¡å‹å¥–åŠ±å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜å¹¶ä¼˜åŒ–æ•°æ®æå–æ€§èƒ½ã€‚</li>
<li>DMRLåˆ©ç”¨é€†å‘å¼ºåŒ–å­¦ä¹ ä»LLMä¸­æå–æ•æ„Ÿæ•°æ®ã€‚</li>
<li>DMRLåŒ…æ‹¬æ„å»ºå†…çœæ¨ç†æ•°æ®é›†å’Œè®­ç»ƒå¥–åŠ±æ¨¡å‹ä¸¤éƒ¨åˆ†ã€‚</li>
<li>å†…çœæ¨ç†æ•°æ®é›†æ—¨åœ¨æ•æ‰æ³„éœ²å¿ƒæ€ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa4a12fdf618616fb536ac0b813c7a02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bec1f118d73124f1eb6732a14aae4e40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f353bb1927a7919f5fff7b6dbf45fe90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e28911b6fed2550c69f13bbf8f87d771.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Llama-Nemotron-Efficient-Reasoning-Models"><a href="#Llama-Nemotron-Efficient-Reasoning-Models" class="headerlink" title="Llama-Nemotron: Efficient Reasoning Models"></a>Llama-Nemotron: Efficient Reasoning Models</h2><p><strong>Authors:Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Shaona Ghosh, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Chris Alexiuk, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung</strong></p>
<p>We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes â€“ Nano (8B), Super (49B), and Ultra (253B) â€“ and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models â€“ LN-Nano, LN-Super, and LN-Ultra â€“ under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Llama-Nemotronç³»åˆ—æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼‚æ„æ¨ç†æ¨¡å‹å®¶æ—ï¼Œå…·å¤‡å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€æ¨ç†æ•ˆç‡å’Œå¼€æ”¾çš„ä¼ä¸šä½¿ç”¨è®¸å¯ã€‚è¯¥å®¶æ—æœ‰ä¸‰ç§è§„æ¨¡ï¼šNanoï¼ˆ8Bï¼‰ã€Superï¼ˆ49Bï¼‰å’ŒUltraï¼ˆ253Bï¼‰ï¼Œä¸æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶æä¾›å“è¶Šçš„æ¨ç†ååé‡å’Œå†…å­˜æ•ˆç‡ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†è¿™äº›æ¨¡å‹çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨Llama 3æ¨¡å‹çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢è¿›è¡ŒåŠ é€Ÿæ¨ç†ã€çŸ¥è¯†è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒï¼Œç„¶åæ˜¯ä¾§é‡äºæ¨ç†çš„åè®­ç»ƒé˜¶æ®µï¼Œä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†ï¼šç›‘ç£å¾®è°ƒå’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ã€‚Llama-Nemotronæ¨¡å‹æ˜¯é¦–ä¸ªæ”¯æŒåŠ¨æ€æ¨ç†åˆ‡æ¢çš„å¼€æºæ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ‡æ¢æ ‡å‡†èŠå¤©å’Œæ¨ç†æ¨¡å¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¯æŒå¼€æ”¾ç ”ç©¶å’Œä¿ƒè¿›æ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹èµ„æºï¼š1.æˆ‘ä»¬åœ¨å•†ä¸šè®¸å¯çš„NVIDIAå¼€æ”¾æ¨¡å‹è®¸å¯åè®®ä¸‹å‘å¸ƒäº†Llama-Nemotronæ¨ç†æ¨¡å‹â€”â€”LN-Nanoã€LN-Superå’ŒLN-Ultraã€‚2.æˆ‘ä»¬å‘å¸ƒäº†å®Œæ•´çš„åè®­ç»ƒæ•°æ®é›†ï¼šLlama-Nemotron-Post-Training-Datasetã€‚3.æˆ‘ä»¬è¿˜å‘å¸ƒäº†æˆ‘ä»¬çš„è®­ç»ƒä»£ç åº“ï¼šNeMoã€NeMo-Alignerå’ŒMegatron-LMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00949v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ç³»åˆ—Llama-Nemotronè¢«å¼•å…¥ï¼Œè¯¥ç³»åˆ—å…·æœ‰å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€é«˜æ•ˆçš„æ¨æ–­æ•ˆç‡å’Œå¼€æ”¾çš„ä¼ä¸šä½¿ç”¨è®¸å¯ã€‚æ¨¡å‹åˆ†ä¸ºNanoã€Superå’ŒUltraä¸‰ä¸ªç‰ˆæœ¬ï¼Œæ€§èƒ½å“è¶Šï¼Œå¯ä¸ä¼ ç»Ÿæ¨ç†æ¨¡å‹å¦‚DeepSeek-R1ç«äº‰ã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä½¿ç”¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢åŠ é€Ÿæ¨æ–­ã€çŸ¥è¯†è’¸é¦å’ŒæŒç»­é¢„è®­ç»ƒï¼Œä»¥åŠæ¨ç†é‡ç‚¹çš„åè®­ç»ƒé˜¶æ®µï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒLlama-Nemotronæ¨¡å‹æ”¯æŒåŠ¨æ€æ¨ç†åˆ‡æ¢ï¼Œå¹¶æä¾›èµ„æºæ”¯æŒå¼€æ”¾ç ”ç©¶å’Œæ¨¡å‹å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Llama-Nemotronæ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼‚è´¨æ¨ç†æ¨¡å‹ç³»åˆ—ï¼Œå…·æœ‰å‡ºè‰²çš„æ¨ç†èƒ½åŠ›å’Œé«˜æ•ˆçš„æ¨æ–­æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹åˆ†ä¸ºNanoã€Superå’ŒUltraä¸‰ä¸ªç‰ˆæœ¬ï¼Œæ€§èƒ½å“è¶Šï¼Œå¯ä¸DeepSeek-R1ç­‰å…ˆè¿›æ¨¡å‹ç«äº‰ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ã€çŸ¥è¯†è’¸é¦ã€æŒç»­é¢„è®­ç»ƒä»¥åŠåè®­ç»ƒé˜¶æ®µã€‚</li>
<li>åè®­ç»ƒé˜¶æ®µåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>Llama-Nemotronæ¨¡å‹æ”¯æŒåŠ¨æ€æ¨ç†åˆ‡æ¢ï¼Œå¯åœ¨æ¨æ–­è¿‡ç¨‹ä¸­åˆ‡æ¢æ ‡å‡†èŠå¤©å’Œæ¨ç†æ¨¡å¼ã€‚</li>
<li>æ¨¡å‹èµ„æºåŒ…æ‹¬æ¨¡å‹å‘å¸ƒã€å®Œæ•´åè®­ç»ƒæ•°æ®é›†å’Œè®­ç»ƒä»£ç åº“çš„å…¬å¼€ï¼Œä»¥æ”¯æŒå¼€æ”¾ç ”ç©¶å’Œæ¨¡å‹å¼€å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3a0080d109a1269bc41f56d0911e19f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffc8d12a6834b4c9e396a6053dddb5b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d628edf153a5cada2c015b186a4980e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5107c1e8e34460d4858de1099677bd03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-955b8ad12e7bdca49170d8f4949d0583.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation"><a href="#SmallPlan-Leverage-Small-Language-Models-for-Sequential-Path-Planning-with-Simulation-Powered-LLM-Guided-Distillation" class="headerlink" title="SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation"></a>SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation</h2><p><strong>Authors:Quang P. M. Pham, Khoi T. N. Nguyen, Nhi H. Doan, Cuong A. Pham, Kentaro Inui, Dezhen Song</strong></p>
<p>Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan â€“ a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics. Our source code is available here: <a target="_blank" rel="noopener" href="https://github.com/quangpham2006/SmallPlan">https://github.com/quangpham2006/SmallPlan</a> </p>
<blockquote>
<p>åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡ã€åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆè·¯å¾„è§„åˆ’ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬å’Œåœ¨åŠ¨æ€åœºæ™¯ä¸­çš„æœ‰é™é€‚åº”æ€§é˜»ç¢äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®æ—¶éƒ¨ç½²ã€‚æˆ‘ä»¬æå‡ºäº†SmallPlanâ€”â€”ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒè½»é‡çº§å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä»¥æ‰§è¡Œé«˜çº§è·¯å¾„è§„åˆ’ä»»åŠ¡çš„æ–°å‹æ¡†æ¶ã€‚åœ¨SmallPlanä¸­ï¼ŒSLMæä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—ï¼Œä»¥åœ¨åœºæ™¯å›¾ä¸­è¿›è¡Œå¯¼èˆªï¼Œè¿™äº›åœºæ™¯å›¾ç´§å‡‘åœ°è¡¨ç¤ºäº†å®Œæ•´çš„3Dåœºæ™¯ã€‚SLMä»¥æ¨¡æ‹Ÿé©±åŠ¨çš„æ–¹å¼ï¼Œé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼çš„ç›‘ç£å’Œå¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒã€‚è¿™ä¸€ç­–ç•¥ä¸ä»…ä½¿SLMèƒ½å¤ŸæˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜ä½¿å…¶èƒ½å¤Ÿæ„è¯†åˆ°æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ç»è¿‡å¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹åœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢çš„è¡¨ç°ä¸GPT-4ç­‰å¤§å‹æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”ä¸ä¼šå‡ºç°å¹»è§‰å’Œè¿‡åº¦æ‹Ÿåˆçš„æƒ…å†µã€‚SmallPlanèµ„æºé«˜æ•ˆï¼Œéå¸¸é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡è¿›è¡Œéƒ¨ç½²ï¼Œæœ‰åŠ©äºæ¨åŠ¨å®é™…è‡ªä¸»æœºå™¨äººçš„å‘å±•ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/quangpham2006/SmallPlan">https://github.com/quangpham2006/SmallPlan</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00831v4">PDF</a> Paper is under review</p>
<p><strong>Summary</strong>ï¼š<br>åœ¨å¤§å‹åŠ¨æ€ç¯å¢ƒä¸­ï¼Œæœºå™¨äººè·¯å¾„è§„åˆ’ä»ç„¶æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†SmallPlanæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒè½»é‡çº§çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ï¼Œç”¨äºé«˜çº§è·¯å¾„è§„åˆ’ä»»åŠ¡ã€‚SmallPlané€šè¿‡SLMæä¾›æœ€ä¼˜åŠ¨ä½œåºåˆ—ï¼Œåœ¨åœºæ™¯å›¾ä¸­å¯¼èˆªï¼Œåœºæ™¯å›¾ç´§å‡‘åœ°è¡¨ç¤ºå…¨å°ºå¯¸3Dåœºæ™¯ã€‚SLMé€šè¿‡æ¨¡æ‹Ÿé©±åŠ¨çš„é—´æ­‡è®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä¸ä»…ä½¿SLMæˆåŠŸå®Œæˆå¯¼èˆªä»»åŠ¡ï¼Œè¿˜èƒ½ä½¿å…¶æ„è¯†åˆ°æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰é‡è¦å› ç´ ã€‚å®éªŒè¡¨æ˜ï¼Œå¾®è°ƒåçš„SLMåœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢ä¸GPTç­‰å¤§å‹æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œä¸”ä¸å­˜åœ¨å¹»è§‰å’Œè¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚SmallPlanèµ„æºé«˜æ•ˆï¼Œé€‚åˆè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å’Œå®ç”¨è‡ªä¸»æœºå™¨äººæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”¨äºæœºå™¨äººè·¯å¾„è§„åˆ’å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’ŒåŠ¨æ€åœºæ™¯é€‚åº”æ€§å·®çš„é—®é¢˜ã€‚</li>
<li>SmallPlanæ¡†æ¶åˆ©ç”¨LLMsä½œä¸ºæ•™å¸ˆæ¨¡å‹è®­ç»ƒè½»é‡çº§çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ã€‚</li>
<li>SLMsé€šè¿‡æœ€ä¼˜åŠ¨ä½œåºåˆ—åœ¨åœºæ™¯å›¾ä¸­è¿›è¡Œå¯¼èˆªï¼Œåœºæ™¯å›¾ä»£è¡¨å…¨å°ºå¯¸3Dåœºæ™¯ã€‚</li>
<li>SLMsé€šè¿‡æ¨¡æ‹Ÿé©±åŠ¨çš„é—´æ­‡è®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½¿å…¶é€‚åº”æ—…è¡Œè·ç¦»å’Œè¯•éªŒæ¬¡æ•°ç­‰å› ç´ ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå¾®è°ƒåçš„SLMåœ¨åºåˆ—è·¯å¾„è§„åˆ’æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä¸å¤§å‹æ¨¡å‹å¦‚GPT-4ç›¸å½“ã€‚</li>
<li>SmallPlanæ¡†æ¶å…·æœ‰èµ„æºé«˜æ•ˆæ€§ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a75deefa5036f368449024894c9f78bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67d4bfd2544253742e0f454092cee4cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8128331a572e8ccf69601b9baaf08878.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="The-Leaderboard-Illusion"><a href="#The-Leaderboard-Illusion" class="headerlink" title="The Leaderboard Illusion"></a>The Leaderboard Illusion</h2><p><strong>Authors:Shivalika Singh, Yiyang Nan, Alex Wang, Daniel Dâ€™Souza, Sayash Kapoor, Ahmet ÃœstÃ¼n, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah A. Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker</strong></p>
<p>Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arenaâ€™s evaluation framework and promote fairer, more transparent benchmarking for the field </p>
<blockquote>
<p>è¿›å±•æµ‹é‡æ˜¯ä»»ä½•ç§‘å­¦é¢†åŸŸå‘å±•çš„åŸºç¡€ã€‚éšç€åŸºå‡†æµ‹è¯•è¶Šæ¥è¶Šèµ·åˆ°æ ¸å¿ƒä½œç”¨ï¼Œå®ƒä»¬ä¹Ÿæ›´å®¹æ˜“å—åˆ°æ‰­æ›²ã€‚èŠå¤©æœºå™¨äººç«æŠ€åœºå·²ç»å´­éœ²å¤´è§’ï¼Œæˆä¸ºæ’åæœ€é¡¶å°–çš„AIç³»ç»Ÿçš„é¦–é€‰æ’è¡Œæ¦œã€‚ç„¶è€Œï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†å¯¼è‡´ç«æŠ€ç¯å¢ƒæ‰­æ›²çš„ç³»ç»Ÿæ€§é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°æœªå…¬å¼€çš„ç§äººæµ‹è¯•å®è·µä½¿å°‘æ•°æä¾›å•†å—ç›Šï¼Œè¿™äº›æä¾›å•†èƒ½å¤Ÿåœ¨å…¬å…±å‘å¸ƒä¹‹å‰æµ‹è¯•å¤šä¸ªå˜ä½“ï¼Œå¹¶åœ¨éœ€è¦æ—¶æ’¤å›åˆ†æ•°ã€‚æˆ‘ä»¬ç¡®å®šè¿™äº›æä¾›å•†é€‰æ‹©æœ€ä½³åˆ†æ•°çš„èƒ½åŠ›ä¼šå¯¼è‡´ç«æŠ€åœºå¾—åˆ†åå‘ï¼Œå› ä¸ºæ€§èƒ½ç»“æœå…·æœ‰é€‰æ‹©æ€§æŠ«éœ²ã€‚åœ¨æç«¯æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ç¡®å®šäº†Metaåœ¨Llama-4å‘å¸ƒå‰æµ‹è¯•çš„27ä¸ªç§äººå¤§å‹è¯­è¨€æ¨¡å‹å˜ä½“ã€‚æˆ‘ä»¬è¿˜ç¡®å®šä¸“æœ‰å°é—­æ¨¡å‹çš„é‡‡æ ·ç‡ï¼ˆæˆ˜æ–—æ¬¡æ•°ï¼‰æ›´é«˜ï¼Œå¹¶ä¸”ä¸å…¬å¼€é‡é‡å’Œå¼€æºæ›¿ä»£æ–¹æ¡ˆç›¸æ¯”ï¼Œä»ç«æŠ€åœºä¸­åˆ é™¤æ¨¡å‹çš„æ•°é‡æ›´å°‘ã€‚è¿™ä¸¤ç§æ”¿ç­–éƒ½ä¼šå¯¼è‡´éšç€æ—¶é—´çš„æ¨ç§»å‡ºç°å¤§è§„æ¨¡çš„æ•°æ®è®¿é—®ä¸å¯¹ç§°ã€‚è°·æ­Œå’ŒOpenAIç­‰æä¾›å•†åˆ†åˆ«è·å¾—äº†ä¼°è®¡çš„19.2%å’Œ20.4%çš„æ‰€æœ‰æ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ€»å…±çš„83ä¸ªå…¬å¼€é‡é‡æ¨¡å‹ä»…è·å¾—äº†ä¼°è®¡çš„ä»…å æ€»é¢ç§¯çš„ç™¾åˆ†ä¹‹äºŒåä¹ç‚¹ä¸ƒçš„æ€»æ•°æ®ä»½é¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜¾ç¤ºï¼Œè®¿é—®èŠå¤©æœºå™¨äººç«æŠ€åœºçš„æ•°æ®ä¼šäº§ç”Ÿå·¨å¤§çš„å¥½å¤„ï¼›å³ä½¿æœ‰é™çš„é¢å¤–æ•°æ®ä¹Ÿå¯èƒ½å¯¼è‡´åŸºäºä¿å®ˆä¼°è®¡çš„ç«æŠ€åœºåˆ†å¸ƒç›¸å¯¹æ€§èƒ½æå‡é«˜è¾¾ç™¾åˆ†ä¹‹ç™¾ä»¥ä¸Šã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™äº›åŠ¨æ€å¯¼è‡´å¯¹ç«æŠ€åœºç‰¹å®šåŠ¨æ€çš„è¿‡åº¦æ‹Ÿåˆï¼Œè€Œä¸æ˜¯ä¸€èˆ¬çš„æ¨¡å‹è´¨é‡ã€‚ç«æŠ€åœºå»ºç«‹åœ¨ç»„ç»‡è€…å’Œä¸€ä¸ªç»´æŠ¤è¿™ä¸€å®è´µè¯„ä¼°å¹³å°çš„å¼€æ”¾ç¤¾åŒºçš„å¤§é‡åŠªåŠ›ä¹‹ä¸Šã€‚æˆ‘ä»¬æä¾›å¯è¡Œçš„å»ºè®®æ¥æ”¹é©èŠå¤©æœºå™¨äººç«æŠ€åœºçš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸæ›´å…¬å¹³ã€æ›´é€æ˜çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20879v2">PDF</a> 68 pages, 18 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬è®¨è®ºäº†èŠå¤©æœºå™¨äººç«æŠ€åœºï¼ˆChatbot Arenaï¼‰åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿæ–¹é¢å­˜åœ¨çš„é—®é¢˜ã€‚å‘ç°ç§æœ‰æµ‹è¯•å®è·µã€é€‰æ‹©æ€§æŠ«éœ²æ€§èƒ½ç»“æœã€æ•°æ®è®¿é—®ä¸å¯¹ç§°ç­‰é—®é¢˜å¯¼è‡´ç«æŠ€åœºè¯„åˆ†å¤±çœŸã€‚æå‡ºæ”¹é©è¯„ä»·æ¡†æ¶çš„å»ºè®®ï¼Œä»¥ä¿ƒè¿›æ›´å…¬å¹³ã€æ›´é€æ˜çš„è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èŠå¤©æœºå™¨äººç«æŠ€åœºï¼ˆChatbot Arenaï¼‰æ˜¯è¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é‡è¦å¹³å°ï¼Œä½†å­˜åœ¨ç³»ç»Ÿæ€§é—®é¢˜å¯¼è‡´è¯„åˆ†å¤±çœŸã€‚</li>
<li>æœªç»æŠ«éœ²çš„ç§äººæµ‹è¯•å®è·µä½¿éƒ¨åˆ†æä¾›è€…èƒ½å¤Ÿåœ¨å…¬å¼€å‰æµ‹è¯•å¤šä¸ªå˜ä½“å¹¶é€‰æ‹©æœ€ä½³æˆç»©ã€‚</li>
<li>ç§æœ‰æ¨¡å‹åœ¨ç«æŠ€åœºä¸­çš„é‡‡æ ·ç‡æ›´é«˜ï¼Œä¸”è¢«ç§»é™¤çš„æ¨¡å‹æ›´å°‘ï¼Œå¯¼è‡´æ•°æ®è®¿é—®ä¸å¯¹ç§°ã€‚</li>
<li>æŸäº›æä¾›è€…å¦‚Googleå’ŒOpenAIåœ¨ç«æŠ€åœºä¸­è·å¾—å¤§é‡æ•°æ®ï¼Œè€Œå¼€æ”¾æƒé‡æ¨¡å‹è·å¾—çš„æ•°æ®è¾ƒå°‘ã€‚</li>
<li>è®¿é—®ç«æŠ€åœºæ•°æ®å¯å¸¦æ¥æ˜¾è‘—ä¼˜åŠ¿ï¼Œæ›´å¤šæ•°æ®ç”šè‡³å¯å¯¼è‡´æ€§èƒ½å¢é•¿ã€‚</li>
<li>å½“å‰åŠ¨æ€å¯¼è‡´æ¨¡å‹æ›´é€‚åº”ç«æŠ€åœºç‰¹å®šåŠ¨æ€è€Œéæ•´ä½“æ¨¡å‹è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b86cf3bdd0b56a21103f90f8ddbe43a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b763ebb9008091d289ea30a76d571399.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a0258ddcf1cee3badf8a4aad55a437f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-825f71eb0cb5aaf13e5a4fa58c7cc48a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-652365c2ebd005af39e3d995e054d1d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f699ff383c379168d69f4f0cd137efe6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training"><a href="#DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training" class="headerlink" title="DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training"></a>DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training</h2><p><strong>Authors:Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li</strong></p>
<p>Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: \href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M%7D%7Bhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M%7D">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}</a> </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘åœ¨å„ç§å¤æ‚çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œä½†å­¦æœ¯ç•Œä»ç„¶ç¼ºä¹å¯¹åŸºç¡€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œæ•°æ®è´¨é‡çš„æ·±å…¥äº†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€åˆ†çº§åˆ«çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«å¤§çº¦334ä¸‡ä¸ªä¸åŒéš¾åº¦çº§åˆ«çš„å”¯ä¸€æŸ¥è¯¢å’Œçº¦ç”±å¤šä¸ªæ¨¡å‹ç»è¿‡å¤šæ¬¡ä¼ é€’ç”Ÿæˆçš„4åƒä¸‡ä¸ªç²¾ç‚¼å“åº”ã€‚æˆ‘ä»¬åˆ©ç”¨é€šè¿‡ç‡å’Œå˜å¼‚ç³»æ•°ï¼ˆCVï¼‰ç²¾ç¡®é€‰æ‹©æœ€æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è®­ç»ƒæ¨¡å¼çš„è½¬å˜ï¼Œè¿™è¡¨æ˜åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜çš„å­¦ä¹ ç‡æ‰èƒ½è¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒã€‚ä½¿ç”¨è¿™äº›ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„é€šè¿‡ç‡è¾¾åˆ°79.2%ã€‚è¿™ä¸€ç»“æœè¶…è¶Šäº†å¤§å¤šæ•°å½“å‰çš„ç²¾ç‚¼æ¨¡å‹ï¼Œå¹¶æ¥è¿‘äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†å…³äºæ•°æ®å¤„ç†ã€éš¾åº¦è¯„ä¼°å’ŒåŸ¹è®­æ–¹æ³•çš„è¯¦ç»†æè¿°ï¼Œå¹¶å·²å…¬å¼€å‘å¸ƒäº†æ‰€æœ‰æ•°æ®é›†å’Œæ–¹æ³•ï¼Œä»¥ä¿ƒè¿›å¼€æºé•¿æ¨ç†LLMçš„å¿«é€Ÿè¿›æ­¥ã€‚æ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM%E3%80%8CDeepSeek%E3%80%8CDistilled%E3%80%8C40M">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17565v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆç»©ï¼Œä½†åŸºç¡€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œæ•°æ®è´¨é‡æ–¹é¢çš„æ·±å…¥äº†è§£ä»ç„¶ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€åˆ†çº§åˆ«çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«çº¦334ä¸‡æ¡ç‹¬ç‰¹æŸ¥è¯¢å’Œçº¦4åƒä¸‡æ¡ç”±å¤šä¸ªæ¨¡å‹å¤šæ¬¡æç‚¼çš„å›åº”ã€‚é€šè¿‡åˆ©ç”¨é€šè¿‡ç‡ä¸å˜å¼‚ç³»æ•°ï¼ˆCVï¼‰ï¼Œæˆ‘ä»¬ç²¾ç¡®ç­›é€‰å‡ºæœ€æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ä»¥æå‡æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è®­ç»ƒæ¨¡å¼è½¬å˜ï¼Œè¡¨æ˜åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜çš„å­¦ä¹ ç‡ä»¥å®ç°æœ‰æ•ˆè®­ç»ƒã€‚ä½¿ç”¨è¿™äº›ç²¾é€‰æ•°æ®ï¼Œæˆ‘ä»¬æ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°79.2%çš„é€šè¿‡ç‡ï¼Œè¶…è¶Šå¤§å¤šæ•°ç°æœ‰æç‚¼æ¨¡å‹ï¼Œæ¥è¿‘æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„æ•°æ®å¤„ç†ã€éš¾åº¦è¯„ä¼°ä¸è®­ç»ƒæ–¹æ³•è®ºæè¿°ï¼Œå¹¶å·²å…¬å¼€æ‰€æœ‰æ•°æ®å’Œæ–¹æ³•é›†ä»¥æ¨åŠ¨å¼€æºé•¿æ¨ç†LLMçš„å¿«é€Ÿå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€åˆ†çº§åˆ«çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«æ•°ç™¾ä¸‡æ¡æŸ¥è¯¢å’Œå›åº”ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨é€šè¿‡ç‡ä¸å˜å¼‚ç³»æ•°ï¼ˆCVï¼‰ç²¾ç¡®ç­›é€‰è®­ç»ƒæ•°æ®ã€‚</li>
<li>è§‚å¯Ÿåˆ°äº†è®­ç»ƒæ¨¡å¼çš„è½¬å˜ï¼ŒæŒ‡å‡ºåŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜å­¦ä¹ ç‡ã€‚</li>
<li>ä½¿ç”¨ç²¾é€‰æ•°æ®æ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—é«˜é€šè¿‡ç‡ï¼Œè¶…è¶Šå¤šæ•°ç°æœ‰æ¨¡å‹ã€‚</li>
<li>å…¬å¼€æ‰€æœ‰æ•°æ®å’Œæ–¹æ³•é›†ä»¥ä¿ƒè¿›å¼€æºé•¿æ¨ç†LLMçš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7800de01a62044ed81c34b9a14530609.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Compile-Scene-Graphs-with-Reinforcement-Learning"><a href="#Compile-Scene-Graphs-with-Reinforcement-Learning" class="headerlink" title="Compile Scene Graphs with Reinforcement Learning"></a>Compile Scene Graphs with Reinforcement Learning</h2><p><strong>Authors:Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, Chang Wen Chen</strong></p>
<p>Next-token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. We design a set of graph-centric rewards, including three recall-based variants â€“ Hard Recall, Hard Recall+Relax, and Soft Recall â€“ which evaluate semantic and spatial alignment between predictions and ground truth at the object and relation levels. A format consistency reward further ensures that outputs follow the expected structural schema. Extensive experiments on the VG150 and PSG benchmarks show that R1-SGG substantially reduces failure rates and achieves strong performance in Recall and mean Recall, surpassing traditional SGG models and existing multimodal language models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/gpt4vision/R1-SGG">https://github.com/gpt4vision/R1-SGG</a> </p>
<blockquote>
<p>ä¸‹ä¸€ä»£ä»¤ç‰Œé¢„æµ‹æ˜¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬åŸåˆ™ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥å¢å¼ºäº†å…¶æ¨ç†æ€§èƒ½ã€‚ä½œä¸ºå¯¹è¯­è¨€ã€å›¾åƒã€è§†é¢‘ç­‰å¤šç§æ¨¡å¼è¿›è¡Œæœ‰æ•ˆå»ºæ¨¡çš„æ–¹å¼ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯æå–ç»“æ„åŒ–è§†è§‰è¡¨å¾ï¼ˆå¦‚åœºæ™¯å›¾ï¼‰çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚å®ƒè¦æ±‚æ¨¡å‹å‡†ç¡®ç”Ÿæˆä¸€ç»„å¯¹è±¡å’Œå…³ç³»ä¸‰å…ƒç»„ï¼Œè€Œä¸æ˜¯é€ä¸ªç”Ÿæˆæ–‡æœ¬æ ‡è®°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†R1-SGGï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆM-LLMï¼‰ã€‚è¯¥æ¨¡å‹é¦–å…ˆåœ¨åœºæ™¯å›¾æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œéšåä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œç²¾ç‚¼ï¼Œä»¥å¢å¼ºå…¶ä»¥ç«¯åˆ°ç«¯æ–¹å¼ç”Ÿæˆåœºæ™¯å›¾çš„èƒ½åŠ›ã€‚SFTéµå¾ªä¼ ç»Ÿçš„æç¤º-å“åº”èŒƒå¼ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™éœ€è¦è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç»„ä»¥å›¾ä¸ºä¸­å¿ƒçš„å¥–åŠ±ï¼ŒåŒ…æ‹¬ä¸‰ç§åŸºäºå¬å›çš„å˜ä½“â€”â€”ç¡¬å¬å›ã€ç¡¬å¬å›+æ”¾æ¾å’Œè½¯å¬å›â€”â€”è¿™äº›å¥–åŠ±è¯„ä¼°é¢„æµ‹å’ŒçœŸå®å€¼åœ¨å¯¹è±¡å’Œå…³ç³»å±‚é¢ä¸Šçš„è¯­ä¹‰å’Œç©ºé—´å¯¹é½æƒ…å†µã€‚æ ¼å¼ä¸€è‡´æ€§å¥–åŠ±å¯ç¡®ä¿è¾“å‡ºéµå¾ªé¢„æœŸçš„ç»“æ„åŒ–æ¨¡å¼ã€‚åœ¨VG150å’ŒPSGåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒR1-SGGå¤§å¤§é™ä½äº†å¤±è´¥ç‡ï¼Œå¹¶åœ¨å¬å›ç‡å’Œå¹³å‡å¬å›ç‡æ–¹é¢å–å¾—äº†å‡ºè‰²çš„è¡¨ç°ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„SGGæ¨¡å‹å’Œç°æœ‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gpt4vision/R1-SGG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gpt4vision/R1-SGGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13617v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œä¸ºåŸºç¡€è¿›è¡Œè®­ç»ƒï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥æé«˜äº†å…¶æ¨ç†æ€§èƒ½ã€‚å°½ç®¡LLMåœ¨å¤šç§æ¨¡æ€ï¼ˆå¦‚è¯­è¨€ã€å›¾åƒã€è§†é¢‘ç­‰ï¼‰å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç«¯åˆ°ç«¯æå–ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºï¼ˆå¦‚åœºæ™¯å›¾ï¼‰æ–¹é¢çš„åº”ç”¨ä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†ç”Ÿæˆåœºæ™¯å›¾ï¼Œå¼•å…¥äº†R1-SGGï¼Œä¸€ç§åˆå§‹é€šè¿‡åœºæ™¯å›¾æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„å¤šæ¨¡æ€LLMï¼ˆM-LLMï¼‰ï¼Œéšåä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œç»†åŒ–ï¼Œä»¥æé«˜å…¶ç”Ÿæˆåœºæ™¯å›¾çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒR1-SGGåœ¨å¤±è´¥ç‡ã€å¬å›ç‡å’Œå¹³å‡å¬å›ç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„SGGæ¨¡å‹å’Œå…¶ä»–ç°æœ‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹è¿›è¡Œè®­ç»ƒï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨å¤šç§æ¨¡æ€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç«¯åˆ°ç«¯æå–ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºï¼ˆå¦‚åœºæ™¯å›¾ï¼‰æ–¹é¢çš„åº”ç”¨ä»ç„¶ä¸è¶³ã€‚</li>
<li>R1-SGGæ˜¯ä¸€ç§å¤šæ¨¡æ€LLMï¼Œåˆå§‹é€šè¿‡åœºæ™¯å›¾æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒè®­ç»ƒã€‚</li>
<li>R1-SGGä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œç»†åŒ–ï¼Œä»¥æé«˜ç”Ÿæˆåœºæ™¯å›¾çš„èƒ½åŠ›ã€‚</li>
<li>R1-SGGè®¾è®¡äº†åŒ…æ‹¬ä¸‰ç§å¬å›ç‡å˜ä½“åœ¨å†…çš„å›¾ä¸­å¿ƒå¥–åŠ±ï¼Œä»¥è¯„ä¼°é¢„æµ‹å’ŒçœŸå®å€¼ä¹‹é—´çš„è¯­ä¹‰å’Œç©ºé—´å¯¹é½æƒ…å†µã€‚</li>
<li>R1-SGGåœ¨VG150å’ŒPSGåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¤±è´¥ç‡é™ä½ï¼Œå¬å›ç‡å’Œå¹³å‡å¬å›ç‡é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b78e4cf821e8ea5359dcd79a5385207.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51c0e9e1049a8991d092908099941b7a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Rec-R1-Bridging-Generative-Large-Language-Models-and-User-Centric-Recommendation-Systems-via-Reinforcement-Learning"><a href="#Rec-R1-Bridging-Generative-Large-Language-Models-and-User-Centric-Recommendation-Systems-via-Reinforcement-Learning" class="headerlink" title="Rec-R1: Bridging Generative Large Language Models and User-Centric   Recommendation Systems via Reinforcement Learning"></a>Rec-R1: Bridging Generative Large Language Models and User-Centric   Recommendation Systems via Reinforcement Learning</h2><p><strong>Authors:Jiacheng Lin, Tian Wang, Kun Qian</strong></p>
<p>We propose Rec-R1, a general reinforcement learning framework that bridges large language models (LLMs) with recommendation systems through closed-loop optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1 directly optimizes LLM generation using feedback from a fixed black-box recommendation model, without relying on synthetic SFT data from proprietary models such as GPT-4o. This avoids the substantial cost and effort required for data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two representative tasks: product search and sequential recommendation. Experimental results demonstrate that Rec-R1 not only consistently outperforms prompting- and SFT-based methods, but also achieves significant gains over strong discriminative baselines, even when used with simple retrievers such as BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM, unlike SFT, which often impairs instruction-following and reasoning. These findings suggest Rec-R1 as a promising foundation for continual task-specific adaptation without catastrophic forgetting. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºRec-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡é—­ç¯ä¼˜åŒ–å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æ¨èç³»ç»Ÿè”ç³»èµ·æ¥ã€‚ä¸åŒäºæç¤ºå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒRec-R1ç›´æ¥ä½¿ç”¨æ¥è‡ªå›ºå®šé»‘ç®±æ¨èæ¨¡å‹çš„åé¦ˆæ¥ç›´æ¥ä¼˜åŒ–LLMçš„ç”Ÿæˆï¼Œè€Œæ— éœ€ä¾èµ–å¦‚GPT-4oç­‰ä¸“æœ‰æ¨¡å‹çš„äººé€ SFTæ•°æ®ã€‚è¿™é¿å…äº†æ•°æ®è’¸é¦æ‰€éœ€çš„å¤§é‡æˆæœ¬å’Œæ—¶é—´ã€‚ä¸ºäº†éªŒè¯Rec-R1çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„ä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼šäº§å“æœç´¢å’Œåºåˆ—æ¨èã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRec-R1ä¸ä»…å§‹ç»ˆä¼˜äºåŸºäºæç¤ºå’ŒSFTçš„æ–¹æ³•ï¼Œè€Œä¸”åœ¨å¼ºå¤§çš„åˆ¤åˆ«åŸºå‡†çº¿ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå³ä½¿ä½¿ç”¨ç®€å•çš„æ£€ç´¢å™¨å¦‚BM25ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œä¸ç»å¸¸æŸå®³æŒ‡ä»¤æ‰§è¡Œå’Œæ¨ç†èƒ½åŠ›çš„SFTä¸åŒï¼ŒRec-R1ä¿ç•™äº†LLMçš„é€šç”¨èƒ½åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒRec-R1æ˜¯åœ¨æ²¡æœ‰ç¾éš¾æ€§é—å¿˜çš„æƒ…å†µä¸‹è¿›è¡ŒæŒç»­ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§çš„æœ‰å‰é€”çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24289v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRec-R1çš„é€šç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æ¨èç³»ç»Ÿç›¸ç»“åˆï¼Œé€šè¿‡é—­ç¯ä¼˜åŒ–å®ç°äºŒè€…çš„æ¡¥æ¢ä½œç”¨ã€‚ä¸åŒäºæç¤ºå’ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ–¹æ³•ï¼ŒRec-R1ç›´æ¥ä¼˜åŒ–LLMç”Ÿæˆï¼Œåˆ©ç”¨å›ºå®šé»‘ç®±æ¨èæ¨¡å‹çš„åé¦ˆï¼Œæ— éœ€ä¾èµ–å¦‚GPT-4oç­‰ä¸“æœ‰æ¨¡å‹çš„åˆæˆSFTæ•°æ®ï¼Œä»è€Œé¿å…äº†æ•°æ®è’¸é¦æ‰€éœ€çš„å¤§é‡æˆæœ¬åŠåŠªåŠ›ã€‚åœ¨ä¸¤é¡¹ä»£è¡¨æ€§ä»»åŠ¡ï¼ˆäº§å“æœç´¢å’Œåºåˆ—æ¨èï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRec-R1ä¸ä»…æŒç»­ä¼˜äºåŸºäºæç¤ºå’ŒSFTçš„æ–¹æ³•ï¼Œè€Œä¸”åœ¨ä½¿ç”¨ç®€å•æ£€ç´¢å™¨å¦‚BM25æ—¶ï¼Œç›¸è¾ƒäºå¼ºå¤§çš„åˆ¤åˆ«åŸºçº¿ä¹Ÿæœ‰æ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼ŒRec-R1èƒ½å¤Ÿä¿ç•™LLMçš„é€šç”¨èƒ½åŠ›ï¼Œä¸åŒäºSFTå¯èƒ½ä¼šæŸå®³æŒ‡ä»¤è·Ÿéšå’Œæ¨ç†èƒ½åŠ›ã€‚è¿™æ˜¾ç¤ºå‡ºRec-R1æ˜¯åœ¨ä¸é—å¿˜ä»»åŠ¡çš„æƒ…å†µä¸‹å®ç°æŒç»­ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§çš„æœ‰å‰é€”çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rec-R1æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è¿æ¥å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ¨èç³»ç»Ÿã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒRec-R1é€šè¿‡ç›´æ¥ä¼˜åŒ–LLMç”Ÿæˆå¹¶åˆ©ç”¨æ¨èæ¨¡å‹çš„åé¦ˆæ¥è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>Rec-R1é¿å…äº†ä½¿ç”¨ä¸“æœ‰æ¨¡å‹çš„åˆæˆæ•°æ®ï¼Œä»è€Œå‡å°‘äº†æˆæœ¬å’Œæ—¶é—´æŠ•å…¥ã€‚</li>
<li>åœ¨äº§å“æœç´¢å’Œåºåˆ—æ¨èä»»åŠ¡ä¸Šï¼ŒRec-R1è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>Rec-R1å³ä½¿åœ¨ç®€å•çš„æ£€ç´¢å™¨å¦‚BM25çš„å¸®åŠ©ä¸‹ä¹Ÿèƒ½å®ç°æ˜¾è‘—çš„æå‡ã€‚</li>
<li>SFTå¯èƒ½ä¼šæŸå®³LLMçš„é€šç”¨èƒ½åŠ›ï¼Œè€ŒRec-R1åˆ™èƒ½å¤Ÿä¿ç•™è¿™äº›èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c9ee3a68cd005cb8554e258bedbf5b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0432ea20e2f44bd1dc25151342e0efcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b48449f68f1493a667d373fdd9ee0cd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55941a4b36bb974a7d4601d46a640cda.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Video-R1-Reinforcing-Video-Reasoning-in-MLLMs"><a href="#Video-R1-Reinforcing-Video-Reasoning-in-MLLMs" class="headerlink" title="Video-R1: Reinforcing Video Reasoning in MLLMs"></a>Video-R1: Reinforcing Video Reasoning in MLLMs</h2><p><strong>Authors:Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, Xiangyu Yue</strong></p>
<p>Inspired by DeepSeek-R1â€™s success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for incentivizing video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-CoT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All code, models, and data are released in: <a target="_blank" rel="noopener" href="https://github.com/tulerfeng/Video-R1">https://github.com/tulerfeng/Video-R1</a>. </p>
<blockquote>
<p>å—DeepSeek-R1åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€å‘æ¨ç†èƒ½åŠ›çš„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æ¨å‡ºVideo-R1ï¼Œé¦–æ¬¡å°è¯•åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ç³»ç»Ÿåœ°æ¢ç´¢R1èŒƒå¼ä»¥æ¿€åŠ±è§†é¢‘æ¨ç†ã€‚ç„¶è€Œï¼Œå°†RLè®­ç»ƒä¸GRPOç®—æ³•ç›´æ¥åº”ç”¨äºè§†é¢‘æ¨ç†é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯è§†é¢‘æ¨ç†ç¼ºä¹æ—¶é—´å»ºæ¨¡ï¼ŒäºŒæ˜¯é«˜è´¨é‡è§†é¢‘æ¨ç†æ•°æ®çš„ç¨€ç¼ºã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºT-GRPOç®—æ³•ï¼Œè¯¥ç®—æ³•é¼“åŠ±æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸æ˜¯ä»…ä¾èµ–è§†é¢‘æ•°æ®ï¼Œè€Œæ˜¯å°†é«˜è´¨é‡å›¾åƒæ¨ç†æ•°æ®çº³å…¥è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šç”¨äºSFTå†·å¯åŠ¨çš„Video-R1-CoT-165kå’Œç”¨äºRLè®­ç»ƒçš„è§†é¢‘R1-260kï¼Œä¸¤è€…éƒ½åŒ…å«å›¾åƒå’Œè§†é¢‘æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-R1åœ¨è§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMMUå’ŒVSI-Benchï¼‰ä»¥åŠé€šç”¨è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MVBenchå’ŒTempCompassç­‰ï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVideo-R1-7Båœ¨è§†é¢‘ç©ºé—´æ¨ç†åŸºå‡†VSI-benchä¸Šçš„å‡†ç¡®ç‡ä¸º37.1%ï¼Œè¶…è¿‡äº†å•†ä¸šä¸“æœ‰æ¨¡å‹GPT-4oã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å‡å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/tulerfeng/Video-R1%E3%80%82">https://github.com/tulerfeng/Video-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21776v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/tulerfeng/Video-R1">https://github.com/tulerfeng/Video-R1</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºDeepSeek-R1åœ¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­æ¿€å‘æ¨ç†èƒ½åŠ›çš„æˆåŠŸï¼Œæˆ‘ä»¬æ¨å‡ºäº†Video-R1ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ç³»ç»Ÿåœ°æ¢ç´¢R1èŒƒå¼ä»¥æ¿€åŠ±è§†é¢‘æ¨ç†ã€‚é’ˆå¯¹ç›´æ¥åº”ç”¨RLè®­ç»ƒå’ŒGRPOç®—æ³•è¿›è¡Œè§†é¢‘æ¨ç†çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜â€”â€”ç¼ºä¹è§†é¢‘æ¨ç†çš„æ—¶ç©ºå»ºæ¨¡å’Œé«˜è´¨é‡è§†é¢‘æ¨ç†æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†T-GRPOç®—æ³•ï¼Œè¯¥ç®—æ³•é¼“åŠ±æ¨¡å‹åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶ç©ºä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é™¤äº†è§†é¢‘æ•°æ®å¤–ï¼Œè¿˜å°†é«˜è´¨é‡å›¾åƒæ¨ç†æ•°æ®çº³å…¥è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šç”¨äºSFTå†·å¯åŠ¨çš„Video-R1-CoT-165kå’Œç”¨äºRLè®­ç»ƒçš„Video-R1-260kï¼Œä¸¤è€…éƒ½åŒ…å«å›¾åƒå’Œè§†é¢‘æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-R1åœ¨è§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMMUå’ŒVSI-Benchï¼‰ä»¥åŠé€šç”¨è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MVBenchå’ŒTempCompassç­‰ï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯ï¼ŒVideo-R1-7Båœ¨è§†é¢‘ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•VSI-benchä¸Šè¾¾åˆ°äº†37.1%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å•†ä¸šä¸“æœ‰æ¨¡å‹GPT-4oã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-R1æ˜¯é¦–ä¸ªå°è¯•åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ¿€åŠ±è§†é¢‘æ¨ç†çš„R1èŒƒå¼ç³»ç»Ÿæ¢ç´¢ã€‚</li>
<li>é’ˆå¯¹è§†é¢‘æ¨ç†ï¼Œæå‡ºäº†T-GRPOç®—æ³•ï¼Œè¯¥ç®—æ³•é¼“åŠ±æ¨¡å‹åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶ç©ºä¿¡æ¯ã€‚</li>
<li>é™¤äº†è§†é¢‘æ•°æ®ï¼Œè¿˜ç»“åˆäº†é«˜è´¨é‡å›¾åƒæ¨ç†æ•°æ®æ¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šVideo-R1-CoT-165kå’ŒVideo-R1-260kï¼ŒåŒ…å«å›¾åƒå’Œè§†é¢‘æ•°æ®ã€‚</li>
<li>Video-R1åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>Video-R1-7Båœ¨VSI-benchæµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ä¸º37.1%ï¼Œè¶…è¿‡GPT-4oæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c4a45666182defb6ac1c03b690b39b1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-928879e678dfda3c7d00001ec9c0aab1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b631a2259ac95749d4a78b6f901dd5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5b5b4b8c2a0b62ef37e9b99900c7d55.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Embodied-Reasoner-Synergizing-Visual-Search-Reasoning-and-Action-for-Embodied-Interactive-Tasks"><a href="#Embodied-Reasoner-Synergizing-Visual-Search-Reasoning-and-Action-for-Embodied-Interactive-Tasks" class="headerlink" title="Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for   Embodied Interactive Tasks"></a>Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for   Embodied Interactive Tasks</h2><p><strong>Authors:Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang</strong></p>
<p>Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the modelâ€™s capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9%, 24%, and +13%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases. </p>
<blockquote>
<p>è¿‘æœŸæ·±åº¦æ€è€ƒæ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šçš„æ˜¾è‘—æ¨ç†èƒ½åŠ›å·²å¾—åˆ°å……åˆ†å±•ç¤ºã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å®ä½“é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é€šè¿‡å›¾åƒåŠ¨ä½œäº¤ç»‡è½¨è¿¹ä¸ç¯å¢ƒæŒç»­äº¤äº’æ–¹é¢çš„åº”ç”¨ä»ç„¶é²œæœ‰ç ”ç©¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†å®ä½“æ¨ç†æ¨¡å‹Embodied Reasonerï¼Œè¯¥æ¨¡å‹å°†æ¨ç†æ‰©å±•åˆ°äº†äº¤äº’å¼å®ä½“æœç´¢ä»»åŠ¡ä¸Šã€‚ä¸åŒäºä¸»è¦ä¾èµ–äºé€»è¾‘æ¼”ç»çš„æ•°å­¦æ¨ç†ï¼Œå®ä½“åœºæ™¯éœ€è¦ç©ºé—´ç†è§£ã€æ—¶é—´æ¨ç†ä»¥åŠåŸºäºäº¤äº’å†å²çš„æŒç»­è‡ªæˆ‘åæ€ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç»¼åˆäº†9,300æ¡è¿è´¯çš„è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨è½¨è¿¹ï¼ŒåŒ…å«6.4ä¸‡å¼ äº¤äº’å›¾åƒå’Œ9ä¸‡å¤šä¸ªå¤šæ ·åŒ–çš„æ€è€ƒè¿‡ç¨‹ï¼ˆåˆ†æã€ç©ºé—´æ¨ç†ã€åæ€ã€è§„åˆ’å’ŒéªŒè¯ï¼‰ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåˆ†ä¸‰é˜¶æ®µçš„è®­ç»ƒç®¡é“ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ é€æ­¥æé«˜æ¨¡å‹çš„èƒ½åŠ›ï¼Œé€šè¿‡æ‹’ç»æŠ½æ ·è¿›è¡Œè‡ªä¸»æ¢ç´¢å’Œè‡ªæˆ‘æ ¡æ­£é€šè¿‡åæ€è°ƒä¼˜ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è§†è§‰æ¨ç†æ–¹é¢å¤§å¤§ä¼˜äºé«˜çº§æ¨¡å‹ï¼Œä¾‹å¦‚å®ƒçš„è¡¨ç°è¶…è¿‡OpenAI o1ï¼Œo3 miniå’ŒClaude-3.7æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜9%ï¼Œè¾¾åˆ°æå‡äº†é¢å¤–çš„äºŒåå››ç™¾åˆ†ä¹‹ä¸€äºŒåä¸‰è‡³äºŒåä¸ƒå’Œä¸‰æˆçš„åå››åˆ†ä¹‹å…­çš„æé«˜å‡†ç¡®ç‡å°¤å…¶è¯æ˜æ¨¡å‹åœ¨å„ç§å¤æ‚é•¿æœŸä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚åœ¨çœŸå®ç¯å¢ƒä¸­ä¹Ÿæ˜¾ç¤ºå‡ºæˆ‘ä»¬çš„ä¼˜è¶Šæ€§ï¼Œå…·æœ‰è¾ƒå°‘çš„é‡å¤æœç´¢å’Œé€»è¾‘ä¸ä¸€è‡´çš„æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21696v2">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/zwq2018/embodied_reasoner">https://github.com/zwq2018/embodied_reasoner</a> Dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/zwq2018/embodied_reasoner">https://huggingface.co/datasets/zwq2018/embodied_reasoner</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦æ€è€ƒæ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œå·²ç»åœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦ä¸ç¯å¢ƒè¿ç»­äº’åŠ¨çš„èº«ä½“è¡Œä¸ºé¢†åŸŸä»å¾…æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºEmbodied Reasoneræ¨¡å‹ï¼Œå°†O1é£æ ¼çš„æ¨ç†åº”ç”¨äºäº¤äº’å¼èº«ä½“æœç´¢ä»»åŠ¡ã€‚ä¸åŒäºä¾èµ–é€»è¾‘æ¨ç†çš„æ•°å­¦æ¨ç†ï¼Œèº«ä½“åœºæ™¯éœ€è¦ç©ºé—´ç†è§£ã€æ—¶é—´æ¨ç†å’ŒåŸºäºäº’åŠ¨å†å²çš„æŒç»­è‡ªæˆ‘åæ€ã€‚æœ¬ç ”ç©¶é€šè¿‡åˆæˆ9300æ¡è¿è´¯çš„è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨è½¨è¿¹æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…å«6ä¸‡å¼ äº’åŠ¨å›¾åƒå’Œè¿‘åä¸‡æ¬¡å¤šæ ·åŒ–çš„æ€è€ƒè¿‡ç¨‹ã€‚å¼€å‘çš„ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“é€šè¿‡æ¨¡ä»¿å­¦ä¹ ã€è‡ªæˆ‘æ¢ç´¢æ‹’ç»é‡‡æ ·å’Œåæ€è°ƒæ•´é€æ­¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹æ˜¾è‘—ä¼˜äºå…ˆè¿›è§†è§‰æ¨ç†æ¨¡å‹ï¼Œä¾‹å¦‚ç›¸å¯¹äºOpenAI o1ã€o3-miniå’ŒClaude-3.7æå‡åˆ†åˆ«æå‡9%ã€24%å’Œå¢åŠ æ–°çš„è¯„ä»·æŒ‡æ ‡ç¡®è®¤å‡†ç¡®åº¦æœ‰å¤§å¹…æå‡è‡³åˆ°ç”šè‡³åŒ…æ‹¬æœªè§è¿‡çš„ç‰©å“ä¹Ÿå¯ä»¥æœ‰æ•ˆåˆ©ç”¨åˆ°æ€ç»´æ¨¡å¼è¿›è¡Œç›¸åº”çš„é€‚åº”ç ”ç©¶æä¾›äº†å¾ˆå¥½çš„åº”ç”¨å‰æ™¯å’ŒåŸºç¡€è§£å†³äº†å…¶åœ¨å¤æ‚é•¿å‘¨æœŸä»»åŠ¡ä¸­çš„é‡å¤æœç´¢å’Œé€»è¾‘ä¸ä¸€è‡´é—®é¢˜å¹¶åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Embodied Reasoneræ¨¡å‹å°†æ·±åº¦æ€è€ƒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ‰©å±•åˆ°äº¤äº’å¼èº«ä½“æœç´¢ä»»åŠ¡é¢†åŸŸã€‚</li>
<li>ä¸ä¾èµ–é€»è¾‘æ¨ç†çš„æ•°å­¦æ¨ç†ä¸åŒï¼Œèº«ä½“åœºæ™¯éœ€è¦ç©ºé—´ç†è§£ã€æ—¶é—´æ¨ç†å’ŒåŸºäºäº’åŠ¨å†å²çš„è‡ªæˆ‘åæ€ã€‚</li>
<li>é€šè¿‡åˆæˆå¤§é‡è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨è½¨è¿¹å’Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“æå‡æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨äº¤äº’å¼èº«ä½“æœç´¢ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›è§†è§‰æ¨ç†æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åœ¨å¤æ‚é•¿å‘¨æœŸä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œå…·æœ‰è¾ƒå°‘çš„é‡å¤æœç´¢å’Œé€»è¾‘ä¸ä¸€è‡´æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­åº”ç”¨è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21696">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18050ebbcd8d73470cf245ab7a3a27cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84fd39348c03a772d76980664594b27e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872f634a4b5c5c239cdf40cb14b2bb0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3602b6feedcb6995476c355a6821aeb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a698191c01813091c48767830722d4e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="UI-R1-Enhancing-Efficient-Action-Prediction-of-GUI-Agents-by-Reinforcement-Learning"><a href="#UI-R1-Enhancing-Efficient-Action-Prediction-of-GUI-Agents-by-Reinforcement-Learning" class="headerlink" title="UI-R1: Enhancing Efficient Action Prediction of GUI Agents by   Reinforcement Learning"></a>UI-R1: Enhancing Efficient Action Prediction of GUI Agents by   Reinforcement Learning</h2><p><strong>Authors:Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, Hongsheng Li</strong></p>
<p>The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Despite its success in language models, its application in multi-modal domains, particularly in graphic user interface (GUI) agent tasks, remains under-explored. To address this issue, we propose UI-R1, the first framework to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for GUI action prediction tasks. Specifically, UI-R1 introduces a novel rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). For efficient training, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. Experimental results demonstrate that our proposed UI-R1-3B achieves significant improvements over the base model (i.e. Qwen2.5-VL-3B) on both in-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of 22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL. Furthermore, UI-R1-3B delivers competitive performance compared to larger models (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K samples. We additionally develop an optimized version, UI-R1-E-3B, which significantly improves both grounding efficiency and accuracy. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain. Code website: <a target="_blank" rel="noopener" href="https://github.com/lll6gg/UI-R1">https://github.com/lll6gg/UI-R1</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒDeepSeek-R1å±•ç¤ºäº†é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å‡ºç°çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å®ƒåœ¨è¯­è¨€æ¨¡å‹æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨å¤šæ¨¡å¼é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UI-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¢ç´¢åŸºäºè§„åˆ™çš„RLå¦‚ä½•å¢å¼ºå¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œç”¨äºGUIåŠ¨ä½œé¢„æµ‹ä»»åŠ¡çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒUI-R1å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŸºäºåŠ¨ä½œçš„è§„åˆ™å¥–åŠ±ï¼Œé€šè¿‡åŸºäºç­–ç•¥ç®—æ³•ï¼ˆå¦‚Group Relative Policy Optimizationï¼ˆGRPOï¼‰ï¼‰ä¼˜åŒ–æ¨¡å‹ã€‚ä¸ºäº†è¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå°è€Œé«˜è´¨é‡çš„åŒ…å«ç§»åŠ¨è®¾å¤‡ä¸Šäº”ç§å¸¸è§åŠ¨ä½œç±»å‹çš„136é¡¹ä»»åŠ¡æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„UI-R1-3Båœ¨åŸŸå†…ï¼ˆIDï¼‰å’ŒåŸŸå¤–ï¼ˆOODï¼‰ä»»åŠ¡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ï¼ˆå³Qwen2.5-VL-3Bï¼‰ï¼Œåœ¨ScreenSpotä¸Šå¹³å‡ç²¾åº¦æé«˜22.1%ï¼Œåœ¨ScreenSpot-Proä¸Šæé«˜6.0%ï¼Œåœ¨ANDROIDCONTROLä¸Šæé«˜12.7%ã€‚æ­¤å¤–ï¼ŒUI-R1-3Båœ¨7.6ä¸‡ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„æ›´å¤§æ¨¡å‹ï¼ˆå¦‚OS-Atlas-7Bï¼‰é¢å‰è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªä¼˜åŒ–ç‰ˆæœ¬UI-R1-E-3Bï¼Œå®ƒæ˜¾è‘—æé«˜äº†æ¥åœ°æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨GUIç†è§£å’Œæ§åˆ¶æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å¼€è¾Ÿäº†é“è·¯ã€‚ä»£ç ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://github.com/lll6gg/UI-R1">https://github.com/lll6gg/UI-R1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21620v4">PDF</a> Updated UI-R1-E-3B</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DeepSeek-R1å±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å‡ºç°çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶åœ¨å¤šæ¨¡æ€é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†UI-R1æ¡†æ¶ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¢ç´¢åŸºäºè§„åˆ™çš„RLå¦‚ä½•å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ä»¥è¿›è¡ŒGUIåŠ¨ä½œé¢„æµ‹ä»»åŠ¡çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUI-R1åœ¨åŸŸå†…ä»»åŠ¡ï¼ˆIDï¼‰å’ŒåŸŸå¤–ä»»åŠ¡ï¼ˆOODï¼‰ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒUI-R1è¿˜å±•ç¤ºäº†ä¸é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„æ›´å¤§æ¨¡å‹ç›¸æ¯”çš„ç«äº‰åŠ›ã€‚è¿™ä¸ºGUIç†è§£å’Œæ§åˆ¶çš„æœªæ¥ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DeepSeek-R1å±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>UI-R1æ¡†æ¶æ—¨åœ¨æ¢ç´¢åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨GUIåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>UI-R1å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºåŠ¨ä½œçš„è§„åˆ™å¥–åŠ±ï¼Œé€šè¿‡åŸºäºç­–ç•¥çš„ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚GRPOï¼‰å®ç°æ¨¡å‹ä¼˜åŒ–ã€‚</li>
<li>UI-R1æ¡†æ¶åœ¨å°è€Œé«˜è´¨é‡çš„æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ•ˆæœï¼Œå¯¹ç§»åŠ¨è®¾å¤‡ä¸Šçš„äº”ç§å¸¸è§åŠ¨ä½œç±»å‹è¿›è¡Œäº†æ¶µç›–ã€‚</li>
<li>åœ¨åŸŸå†…å’ŒåŸŸå¤–ä»»åŠ¡ä¸Šï¼ŒUI-R1ç›¸æ¯”åŸºå‡†æ¨¡å‹å®ç°äº†å¹³å‡ç²¾åº¦æå‡ã€‚</li>
<li>UI-R1åœ¨ä¸é€šè¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒçš„æ›´å¤§æ¨¡å‹ç›¸æ¯”æ—¶è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>UI-R1æä¾›äº†ä¸€ä¸ªä¼˜åŒ–çš„ç‰ˆæœ¬ï¼Œæ˜¾è‘—æé«˜äº†æ¥åœ°æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aae165dd38d87e354d3ff161093d7e2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-759ab689c47377f1d5f7ec8d41fab7e2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-16/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-16/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-16/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b660e7bf7213a3ed505faf4ed6d1cc71.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-16  Adversarial Suffix Filtering a Defense Pipeline for LLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-15/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cf62827be381f0838046fd1326a4516e.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  M3G Multi-Granular Gesture Generator for Audio-Driven Full-Body Human   Motion Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
