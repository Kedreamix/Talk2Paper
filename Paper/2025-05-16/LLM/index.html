<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-05-16  Adversarial Suffix Filtering a Defense Pipeline for LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b660e7bf7213a3ed505faf4ed6d1cc71.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-16-更新"><a href="#2025-05-16-更新" class="headerlink" title="2025-05-16 更新"></a>2025-05-16 更新</h1><h2 id="Adversarial-Suffix-Filtering-a-Defense-Pipeline-for-LLMs"><a href="#Adversarial-Suffix-Filtering-a-Defense-Pipeline-for-LLMs" class="headerlink" title="Adversarial Suffix Filtering: a Defense Pipeline for LLMs"></a>Adversarial Suffix Filtering: a Defense Pipeline for LLMs</h2><p><strong>Authors:David Khachaturov, Robert Mullins</strong></p>
<p>Large Language Models (LLMs) are increasingly embedded in autonomous systems and public-facing environments, yet they remain susceptible to jailbreak vulnerabilities that may undermine their security and trustworthiness. Adversarial suffixes are considered to be the current state-of-the-art jailbreak, consistently outperforming simpler methods and frequently succeeding even in black-box settings. Existing defenses rely on access to the internal architecture of models limiting diverse deployment, increase memory and computation footprints dramatically, or can be bypassed with simple prompt engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$ (ASF), a lightweight novel model-agnostic defensive pipeline designed to protect LLMs against adversarial suffix attacks. ASF functions as an input preprocessor and sanitizer that detects and filters adversarially crafted suffixes in prompts, effectively neutralizing malicious injections. We demonstrate that ASF provides comprehensive defense capabilities across both black-box and white-box attack settings, reducing the attack efficacy of state-of-the-art adversarial suffix generation methods to below 4%, while only minimally affecting the target model’s capabilities in non-adversarial scenarios. </p>
<blockquote>
<p>大型语言模型（LLM）越来越多地被嵌入到自主系统和面向公众的环境中，然而它们仍然容易受到漏洞攻击，这些攻击可能会破坏其安全性和可信度。对抗性后缀被认为是目前最先进的攻击方式，持续优于更简单的方法，即使在黑箱环境中也经常取得成功。现有的防御手段依赖于对模型内部架构的访问，限制了其多样化部署，且极大地增加了内存和计算占用足迹，或者可以通过简单的提示工程方法绕过。我们引入了<strong>对抗性后缀过滤（ASF）</strong>，这是一种轻量级的新型模型无关防御管道，旨在保护LLM免受对抗性后缀攻击。ASF充当输入预处理器和净化器，可检测和过滤提示中的对抗性后缀，有效中和恶意注入。我们证明，ASF在黑白盒攻击环境中提供了全面的防御能力，将最先进的对抗性后缀生成方法的攻击效率降低至低于4%，同时仅对目标模型在非对抗性场景中的能力产生微小影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09602v1">PDF</a> </p>
<p><strong>Summary</strong><br>大规模语言模型（LLMs）在自主系统和面向公众的环境中应用越来越广泛，但它们仍然容易受到漏洞攻击，可能影响其安全性和可信度。对抗性后缀被认为是当前最先进的攻击方式，现有的防御手段存在依赖模型内部架构、增加内存和计算成本或可通过简单提示工程方法绕过等问题。本文介绍了一种新型的模型无关防御管道——对抗性后缀过滤器（ASF），旨在保护LLMs免受对抗性后缀攻击。ASF作为输入预处理器和净化器，能够检测和过滤提示中的对抗性后缀，有效中和恶意注入。实验表明，ASF在黑白盒攻击场景下提供全面的防御能力，将最先进对抗性后缀生成方法的攻击效果降低至4%以下，同时几乎不影响模型在非对抗场景下的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在自主系统和公共环境中广泛应用，但存在安全性和信任度问题。</li>
<li>对抗性后缀是当前最先进的攻击方式，能够绕过现有防御手段。</li>
<li>对抗性后缀过滤器（ASF）是一种新型的模型无关防御管道，旨在保护LLMs免受对抗性后缀攻击。</li>
<li>ASF作为输入预处理器和净化器，能有效检测和过滤提示中的对抗性后缀。</li>
<li>实验表明，ASF在黑白盒攻击场景下提供全面的防御能力，大幅降低攻击效果。</li>
<li>ASF对模型性能的影响极小，几乎不影响其在非对抗场景下的能力。<br>7.ASF的引入为LLMs的安全性和稳健性提供了新的防护手段。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09602">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-abdde6c41d84ee23f3f2fd6c4ac901b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cd2a542ddd5ceaf097d3a4f9674a38a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="How-Hungry-is-AI-Benchmarking-Energy-Water-and-Carbon-Footprint-of-LLM-Inference"><a href="#How-Hungry-is-AI-Benchmarking-Energy-Water-and-Carbon-Footprint-of-LLM-Inference" class="headerlink" title="How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of   LLM Inference"></a>How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of   LLM Inference</h2><p><strong>Authors:Nidhal Jegham, Marwen Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi</strong></p>
<p>As large language models (LLMs) spread across industries, understanding their environmental footprint at the inference level is no longer optional; it is essential. However, most existing studies exclude proprietary models, overlook infrastructural variability and overhead, or focus solely on training, even as inference increasingly dominates AI’s environmental impact. To bridge this gap, this paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models as deployed in commercial data centers. Our framework combines public API performance data with region-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries&#x2F;day results in substantial annual environmental impacts. These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset. These findings illustrate a growing paradox: although individual queries are efficient, their global scale drives disproportionate resource consumption. Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of LLM deployments, laying a foundation for future environmental accountability in AI development and sustainability standards. </p>
<blockquote>
<p>随着大型语言模型（LLM）在各行业的普及，对其推理级别的环境足迹的了解已经不再可有可无，而是变得至关重要。然而，大多数现有研究都排除了专有模型，忽略了基础设施的变性和开销，或者只关注训练方面，即使推理越来越多地主导人工智能的环境影响。为了弥补这一空白，本文介绍了一个新型的基础设施感知基准测试框架，该框架可以量化部署在商用数据中心中的30款最前沿LLM推理的环境足迹。我们的框架结合了公开API性能数据、地区特定的环境乘数以及硬件配置统计推断。此外，我们还利用交叉效率数据包络分析（DEA）对模型按照相对于环境成本的表现进行排名。我们的结果表明，O3和DeepSeek-R1是最能耗的模型，长提示的能耗超过33瓦时，是GPT-4.1 nano能耗的70倍以上，而Claude-3.7 Sonnet在生态效率方面排名最高。虽然单个短GPT-4o查询消耗0.43瓦时，但每天扩展到7亿次查询会产生巨大的年度环境影响。其中包括与3.5万个美国家庭相当的电力使用、淡水蒸发量相当于满足120万人的年度饮水需求以及碳排放量需要芝加哥规模的森林来中和。这些发现揭示了一个日益增长的悖论：虽然单个查询效率很高，但其在全球范围内的推广导致了资源消耗的极度不平衡。本研究提供了一种标准化、实证的基准测试方法来评估LLM部署的可持续性，为未来人工智能发展和可持续性标准中的环境问责提供了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09598v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的推理阶段环境足迹评估至关重要，但现有研究存在诸多不足。本研究引入了一种新型的基础设施感知基准测试框架，旨在量化商业数据中心部署的30款先进LLM模型推理阶段的环保足迹。研究结合公开API性能数据、地区特定环境乘数与硬件配置统计推断，并利用交叉效率数据包络分析（DEA）对模型进行性能与环保成本的排名。结果显示某些模型如o3和DeepSeek-R1能耗极高，而Claude-3.7 Sonnet在生态效率方面表现最佳。个人查询虽然能效较高，但在大规模应用下会产生显著的环境影响，包括电力消耗、淡水蒸发和碳排放等问题。本研究为评估LLM部署的可持续性提供了标准化、实证的方法论。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的推理阶段环境足迹评估具有重要性。</li>
<li>现有研究在LLM环境足迹评估方面存在不足，如忽略专有模型、基础设施可变性和开销等。</li>
<li>本研究引入了一种新型的基础设施感知基准测试框架，用于量化商业数据中心部署的LLM模型推理阶段的环保足迹。</li>
<li>框架结合了公开API性能数据、地区特定环境乘数和硬件配置统计推断。</li>
<li>通过交叉效率数据包络分析（DEA），发现某些模型如o3和DeepSeek-R1能耗极高，而Claude-3.7 Sonnet在生态效率方面表现最佳。</li>
<li>个人查询虽然能效较高，但在大规模应用下会产生显著的环境影响，包括电力消耗、淡水蒸发和碳排放等问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09598">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f4f82f61d8dd033c9f47798735e8303.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d31dc7f1b994f8e67c97d9eb6be9eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c518d97911ab20f7122fa7f9827bb0fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92093279f26667e6ab129c3f91605247.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d1b1a14097e89b0fa6438dee7c4e18c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="WorldView-Bench-A-Benchmark-for-Evaluating-Global-Cultural-Perspectives-in-Large-Language-Models"><a href="#WorldView-Bench-A-Benchmark-for-Evaluating-Global-Cultural-Perspectives-in-Large-Language-Models" class="headerlink" title="WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives   in Large Language Models"></a>WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives   in Large Language Models</h2><p><strong>Authors:Abdullah Mushtaq, Imran Taj, Rafay Naeem, Ibrahim Ghaznavi, Junaid Qadir</strong></p>
<p>Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems. </p>
<blockquote>
<p>大型语言模型（LLM）主要接受训练并通过对齐方式，加强了西方中心主义的认知论和社会文化规范，导致文化同质化，并限制了它们反映全球文明多样性的能力。现有的基准测试框架未能充分捕捉这种偏见，因为它们依赖于僵化、封闭形式的评估，忽视了文化包容性复杂性。为了解决这个问题，我们引入了WorldView-Bench基准测试，旨在评估大型语言模型（LLM）的全球文化包容性（GCI）。我们的方法基于Senturk等人提出的多元世界观，区分了强化文化同质化的单一模型（Uniplex模型）和整合多元视角的多元模型（Multiplex模型）。WorldView-Bench通过自由形式的生成评估来衡量文化极化以及排斥替代观点的情况，而不是通过传统的分类基准测试。我们通过两种干预策略来实现应用多元化：（1）情境实施多元大型语言模型（LLM），该系统提示嵌入多元化原则；（2）多主体系统（MAS）实施多元化的大型语言模型，其中代表不同文化观点的大型语言模型主体协同生成响应。我们的结果证明了多主体系统实施的大型语言模型在观点分布得分（PDS）熵方面从基线水平的13%显著增加至94%，同时转向积极情绪（占67.7%）和文化平衡增强。这些发现突显了多元意识人工智能评估在缓解大型语言模型中的文化偏见方面的潜力，为更包容和伦理对齐的人工智能系统铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09595v1">PDF</a> Preprint. Submitted to the Journal of Artificial Intelligence   Research (JAIR) on April 29, 2025</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）的训练与对齐方式主要强化了西方中心的知识论和社会文化规范，导致文化同质化，并限制了其反映全球文明多样性的能力。现有的评估框架未能充分捕捉这种偏见，它们依赖于僵化、封闭形式的评估，忽视了文化包容性的复杂性。为解决这一问题，我们提出了WorldView-Bench，一个用于评估LLM全球文化包容性（GCI）的基准测试，通过分析其容纳不同世界观的能力来实现。我们的方法基于Senturk等人提出的多元世界观，区分了强化文化同质化的单一模型（Uniplex）和整合多元视角的多元模型（Multiplex）。WorldView-Bench通过自由形式生成评估，而不是传统的分类基准测试，来衡量文化两极化以及排斥替代观点的情况。我们通过两种干预策略实施应用多元性：（1）情境实施的多元LLM，其中系统提示嵌入多元性原则；（2）多智能体系统（MAS）实施的多元LLM，其中代表不同文化视角的多个LLM智能体协同生成响应。我们的结果表明，与基线相比，多智能体系统实施的多元LLM的透视分布得分（PDS）熵从13%显着增加到94%，同时向积极情绪（67.7%）转变，并增强了文化平衡性。这些发现突显了多元意识人工智能评估在缓解LLM中的文化偏见方面的潜力，为构建更具包容性和道德对齐的人工智能系统铺平了道路。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）主要反映和强化西方中心的知识论和社会文化规范，导致文化同质化。</li>
<li>现有评估框架无法充分捕捉LLM中的文化偏见，因为它们主要依赖封闭和僵化的评估方法。</li>
<li>WorldView-Bench是一个新的基准测试，旨在评估LLM的全球文化包容性（GCI），通过衡量其容纳不同世界观的能力来实现。</li>
<li>引入多元世界观概念，区分单一模型（Uniplex）和多元模型（Multiplex）。</li>
<li>WorldView-Bench采用自由形式生成评估来捕捉LLM中的文化极化现象和替代观点排斥情况。</li>
<li>通过情境实施和多智能体系统（MAS）实施两种策略来应用多元性，显著提高了透视分布得分（PDS）熵，同时促进了积极情绪和文化平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09595">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-015340ca5ac1dfab355a7f35717b5378.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49731c932989794fe6dbb55db791a66f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MIGRATION-BENCH-Repository-Level-Code-Migration-Benchmark-from-Java-8"><a href="#MIGRATION-BENCH-Repository-Level-Code-Migration-Benchmark-from-Java-8" class="headerlink" title="MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8"></a>MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8</h2><p><strong>Authors:Linbo Liu, Xinle Liu, Qiang Zhou, Lin Chen, Yihan Liu, Hoan Nguyen, Behrooz Omidvar-Tehrani, Xi Shen, Jun Huan, Omer Tripp, Anoop Deoras</strong></p>
<p>With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on problem-solving and issue-resolution tasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a distinct focus: code migration. MIGRATION-BENCH aims to serve as a comprehensive benchmark for migration from Java 8 to the latest long-term support (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset and its subset selected with $5,102$ and $300$ repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java 17. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: <a target="_blank" rel="noopener" href="https://huggingface.co/collections/AmazonScience">https://huggingface.co/collections/AmazonScience</a> and <a target="_blank" rel="noopener" href="https://github.com/amazon-science/self_debug">https://github.com/amazon-science/self_debug</a> respectively. </p>
<blockquote>
<p>随着近年来功能强大的大型语言模型（LLM）的快速发展，现在可以使用LLM解决广泛的软件工程任务，从而极大地提高了生产力和可扩展性。已经开发了许多基准数据集来评估这些模型的编码能力，它们主要侧重于问题解决和问题解决任务。相比之下，我们引入了一个新的编码基准MIGRATION-BENCH，它具有独特的重点：代码迁移。MIGRATION-BENCH旨在成为从Java 8迁移到最新长期支持（LTS）版本（Java 17、21）的全面基准。MIGRATION-BENCH包括完整数据集和分别使用$5,102$和$300$个存储库选择的子集。所选子集是经过复杂性和难度挑选出来的，为代码迁移领域的研究提供了多功能资源支持。此外，我们还提供了一个全面的评估框架，以促进对这一具有挑战性的任务的LLM进行严格和标准化的评估。我们进一步提出SD-Feedback，并证明LLM可以有效地处理存储库级别的代码迁移到Java 17。对于使用Claude-3.5-Sonnet-v2选择的子集，SD-Feedback在最小迁移和最大迁移方面的成功率分别为62.33%和27.00%（pass@1）。基准数据集和源代码可在：<a target="_blank" rel="noopener" href="https://huggingface.co/collections/AmazonScience">https://huggingface.co/collections/AmazonScience</a> 和 <a target="_blank" rel="noopener" href="https://github.com/amazon-science/self_debug">https://github.com/amazon-science/self_debug</a> 获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09569v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>随着近年来强大的大型语言模型（LLM）的快速发展，一系列软件工程任务都可以利用LLM来解决，大大提高了生产力和可扩展性。主要聚焦于问题解决和问题解决任务的评估编码能力的基准数据集已经开发出来，与此相反，我们介绍了一个新的编码基准MIGRATION-BENCH，其独特之处在于代码迁移。MIGRATION-BENCH旨在成为从Java 8迁移到最新的长期支持（LTS）版本（Java 17、21）的迁移的综合基准。它包括一个完整的数据集和分别选择包含5,102和300个存储库的两个子集来展示代码的复杂性和难度，从而作为该领域研究的资源宝库。我们还提供了一个全面的评估框架，以促进对这一具有挑战性的任务的LLM进行严谨和标准化的评估。我们进一步提出SD-Feedback并证明LLM可以有效地处理存储库级别的代码迁移到Java 17。对于选定的子集使用Claude-3.5-Sonnet-v版反馈实现62.33％和最大迁移反馈成功率为百分之二十七点零零一。基准数据集和源代码可在：<a target="_blank" rel="noopener" href="https://huggingface.co/collections/AmazonScience">huggingface.co&#x2F;collections&#x2F;AmazonScience</a>和GitHub的亚马逊科学&#x2F;自我调试页面找到。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）的快速发展为软件工程任务的解决提供了强大的工具，增强了生产力和可扩展性。</li>
<li>MIGRATION-BENCH作为一种新的编码基准，专注于代码迁移，旨在支持从Java 8迁移到最新LTS版本的研究。</li>
<li>MIGRATION-BENCH包括一个完整的数据集和其子集，展示了代码的复杂性和难度。</li>
<li>提供了一个全面的评估框架，对LLM在代码迁移任务上的性能进行严谨和标准化的评估。</li>
<li>提出SD-Feedback方法并证明其在处理存储库级别的代码迁移中的有效性。</li>
<li>对于选定的子集，使用Claude-3.5-Sonnet-v版反馈实现了较高的成功迁移率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09569">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6d1495662f6b443d61087e3409268954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-095b09fef8e8a77d078f1c5c48b07cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ba8b96dfd42942d444417bf6fc65e7d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="BLIP3-o-A-Family-of-Fully-Open-Unified-Multimodal-Models-Architecture-Training-and-Dataset"><a href="#BLIP3-o-A-Family-of-Fully-Open-Unified-Multimodal-Models-Architecture-Training-and-Dataset" class="headerlink" title="BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,   Training and Dataset"></a>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,   Training and Dataset</h2><p><strong>Authors:Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, Ran Xu</strong></p>
<p>Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets. </p>
<blockquote>
<p>在多模态模型领域的最新研究中，统一图像理解和生成技术越来越受到关注。尽管图像理解的设计选择已经得到了广泛的研究，但在统一框架中同时包含图像生成的模型架构和训练策略仍被较少探索。受自回归和扩散模型在高质生成和可扩展性方面的强大潜力的驱动，我们对它们在统一多模态设置中的应用进行了全面研究，重点关注图像表示、建模目标和训练策略。基于这些研究，我们引入了一种新方法，使用扩散变压器生成语义丰富的CLIP图像特征，与传统的基于VAE的表示形成对比。这种设计不仅提高了训练效率，还改善了生成质量。此外，我们证明了统一模型的顺序预训练策略——首先在图像理解上进行训练，然后是在图像生成上进行训练——具有实用价值，能够在保持图像理解能力的同时，发展强大的图像生成能力。最后，我们通过使用涵盖各种场景、物体、人类动作等的多样化标题提示GPT-4o，精心创建了一个高质量用于图像生成的指令调整数据集BLIP3o-60k。基于我们创新的模型设计、训练方案和数据集，我们开发了一系列最先进的统一多模态模型BLIP3-o。BLIP3-o在大多数流行的涵盖图像理解和生成任务的基准测试中实现了卓越的性能。为了促进未来的研究，我们全面开源了我们的模型，包括代码、模型权重、训练脚本以及预训练和指令调整数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09568v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了将图像理解和生成结合在一同处理的潜力与挑战。文中提出了一种新颖方法，通过采用扩散转换器生成富含语义的CLIP图像特征来提高训练和生成的效率与质量。研究结果显示出自动回归模型和扩散模型在高质量生成中的优势，为此开展了对它们在统一的多模态场景中的应用的综合研究。为配合这种新型设计，同时设计了预先训练的次序策略和特定的图像生成数据集BLIP3o-60k。最终，基于这些创新设计、训练方法和数据集，发展出了具备超前性能的统一多模态模型BLIP3-o系列。为了推进后续研究，开源了其模型代码、权重和训练脚本等关键资料。通过模型和相关的资源来促进图像理解和生成领域的未来发展。此举极大地提高了不同基准测试的性能，包括图像理解和生成任务。为了方便未来研究，我们完全开源了我们的模型，包括代码、模型权重、训练脚本以及预训练和指令调整数据集。这将为相关领域的发展提供重要推动力。总的来说，该文章是探索多模态模型统一框架的开创性研究，标志着向这一领域的巨大进展迈出了一步。不仅强调性能的提高和创新的设计方案，还为研究人员提供了一个广泛开放的开源资源库来推动未来的研究和发展。这将有助于促进该领域的进一步发展和创新。文中提出了一种新颖的方法，旨在实现图像理解和生成的统一处理框架，展示了其巨大的潜力和优势。通过对相关模型的优化和改进以及数据集的建设和创新实践策略的采用等方式推动该领域的发展，最终将实现更高效和准确的图像处理应用。<strong>Key Takeaways</strong>:</p>
<ul>
<li>文章主要探讨将图像理解与生成结合的统一模型设计及其应用挑战与进展。指出传统设计中虽然对于图像理解部分有深入研究，但统一的框架结合图像生成的模型和训练策略仍然被忽视的问题。引入了一种新颖方法作为突破口来提高模型的性能与质量，并进行对比分析；分析了传统的模型和生成技术在这个领域中存在的问题与不足之处等短板现象并进行总结和展望解决办法以及对不同解决策略的探讨提出了一个新的解决策略进行了创新尝试和分析同时构建了数据集以便验证和提升算法效果为解决这些短板提供一种新的解决思路和实验数据支持来解决这个问题并进一步探讨这个领域的未来发展前景进而展开深入的探索研究介绍了其在改进图像处理领域的杰出贡献进行了评价</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09568">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33aa03fbae588858ae133c1cf4caa04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1c59a7596173a94015358ec00fb8b6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37d252eca6eae72c28ee6460d613aa7e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PT-MoE-An-Efficient-Finetuning-Framework-for-Integrating-Mixture-of-Experts-into-Prompt-Tuning"><a href="#PT-MoE-An-Efficient-Finetuning-Framework-for-Integrating-Mixture-of-Experts-into-Prompt-Tuning" class="headerlink" title="PT-MoE: An Efficient Finetuning Framework for Integrating   Mixture-of-Experts into Prompt Tuning"></a>PT-MoE: An Efficient Finetuning Framework for Integrating   Mixture-of-Experts into Prompt Tuning</h2><p><strong>Authors:Zongqian Li, Yixuan Su, Nigel Collier</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models, yet existing approaches exhibit counter-intuitive phenomena: integrating router into prompt tuning (PT) increases training efficiency yet does not improve performance universally; parameter reduction through matrix decomposition can improve performance in specific domains. Motivated by these observations and the modular nature of PT, we propose PT-MoE, a novel framework that integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets demonstrate that PT-MoE achieves state-of-the-art performance in both question answering (QA) and mathematical problem solving tasks, improving F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA. Our analysis reveals that while PT methods generally excel in QA tasks and LoRA-based methods in math datasets, the integration of matrix decomposition and MoE in PT-MoE yields complementary benefits: decomposition enables efficient parameter sharing across experts while MoE provides dynamic adaptation, collectively enabling PT-MoE to demonstrate cross-task consistency and generalization abilities. These findings, along with ablation studies on routing mechanisms and architectural components, provide insights for future PEFT methods. </p>
<blockquote>
<p>参数高效的微调（PEFT）方法在大规模语言模型适应方面显示出巨大潜力。然而，现有方法展现出了一些反直觉现象：将路由器集成到提示调整（PT）中会提高训练效率，但并不普遍地改善性能；通过矩阵分解进行参数缩减可以在特定领域提高性能。受这些观察结果和PT的模块化性质的启发，我们提出了PT-MoE，这是一个集成了矩阵分解与专家混合（MoE）路由的高效PT新型框架。在11个数据集上的结果表明，PT-MoE在问答和数学问题解答任务中达到了最新技术水平，在问答任务中相对于PT提高了1.49点的F1分数，相对于LoRA提高了2.13点；在数学准确性上相对于PT提高了10.75点，相对于LoRA提高了0.44点，同时使用的参数比LoRA减少了25%。我们的分析表明，虽然PT方法在问答任务中通常表现出色，而基于LoRA的方法在数学数据集上表现较好，但PT-MoE中矩阵分解和MoE的集成带来了互补优势：分解能够在专家之间实现有效的参数共享，而MoE提供了动态适应性，共同使PT-MoE表现出跨任务的连贯性和泛化能力。这些发现以及对路由机制和架构组件的消融研究，为未来的PEFT方法提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09519v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>参数效率微调（PEFT）方法在大规模语言模型适应中显示出潜力，但现有方法存在反直觉现象。本文提出PT-MoE框架，结合矩阵分解和混合专家（MoE）路由，实现高效参数调谐。在17个数据集上的结果表明，PT-MoE在问答和数学问题解决任务中达到最新水平，同时在问答任务中比PT高出1.49点，比LoRA高出2.13点；在数学准确性方面，比PT高出10.75点，比LoRA高出0.44点，同时使用的参数比LoRA少25%。分析表明PT-MoE具有跨任务一致性和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>参数效率微调（PEFT）在适应大型语言模型中显示出潜力。</li>
<li>现有PEFT方法存在反直觉现象，需要通过改进提高性能。</li>
<li>提出PT-MoE框架，结合矩阵分解和混合专家（MoE）路由实现高效参数调谐。</li>
<li>PT-MoE在问答和数学问题解决任务中表现最佳，优于现有方法。</li>
<li>矩阵分解使参数共享更有效，而MoE提供动态适应性。</li>
<li>PT-MoE具有跨任务一致性和泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09519">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d0f3cdb140a401c50614c3b3d72dc07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59859291fd73f83e3ff1359cec180ec5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f92fbb9d758146aec4df6a0f618fc30d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdfbec01b548bd36004ba03ca3449986.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3f728014075fa5b370d693e4621c396.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Layered-Unlearning-for-Adversarial-Relearning"><a href="#Layered-Unlearning-for-Adversarial-Relearning" class="headerlink" title="Layered Unlearning for Adversarial Relearning"></a>Layered Unlearning for Adversarial Relearning</h2><p><strong>Authors:Timothy Qian, Vinith Suriyakumar, Ashia Wilson, Dylan Hadfield-Menell</strong></p>
<p>Our goal is to understand how post-training methods, such as fine-tuning, alignment, and unlearning, modify language model behavior and representations. We are particularly interested in the brittle nature of these modifications that makes them easy to bypass through prompt engineering or relearning. Recent results suggest that post-training induces shallow context-dependent &#96;&#96;circuits’’ that suppress specific response patterns. This could be one explanation for the brittleness of post-training. To test this hypothesis, we design an unlearning algorithm, Layered Unlearning (LU), that creates distinct inhibitory mechanisms for a growing subset of the data. By unlearning the first $i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU limits the ability of relearning on a subset of data to recover the full dataset. We evaluate LU through a combination of synthetic and large language model (LLM) experiments. We find that LU improves robustness to adversarial relearning for several different unlearning methods. Our results contribute to the state-of-the-art of machine unlearning and provide insight into the effect of post-training updates. </p>
<blockquote>
<p>我们的目标是理解如何在训练后的方法，如微调、对齐和遗忘等，如何改变语言模型的行为和表示。我们特别对这些修改带来的脆弱性感兴趣，这种脆弱性很容易通过提示工程或再学习来绕过。近期的研究结果表明，训练后会在特定环境下引发浅层的“电路”，这些电路抑制特定的响应模式。这可能是训练后脆弱性的一个解释。为了验证这一假设，我们设计了一种遗忘算法，即分层遗忘（LU），该算法为数据的一个不断增长子集创建了独特的抑制机制。在k阶段的第i个阶段，通过遗忘前i个折叠的数据而保留剩余的k-i个数据，LU限制了数据子集上重新学习的能力以恢复整个数据集。我们通过合成实验和大型语言模型（LLM）实验的结合来评估LU。我们发现LU提高了不同遗忘方法对于对抗性重新学习的稳健性。我们的研究为该领域的发展作出了贡献，并对训练后的更新所带来的影响有了进一步的了解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09500v1">PDF</a> 37 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了如何通过微调、对齐和遗忘等后训练方法来改变语言模型的行为和表示。研究重点关注这些修改方式的脆弱性，它们容易受到提示工程或再学习的绕过。近期研究表明，后训练会引发浅层上下文依赖的“电路”，抑制特定响应模式，这可能是其脆弱性的原因之一。为了验证这一假设，设计了一种遗忘算法——分层遗忘（LU），为不断增长的数据子集创建了独特的抑制机制。通过在第i个阶段的k-i数据遗忘过程中学习前i个折叠，LU限制了部分数据再学习的能力。实验评估表明，LU提高了对不同遗忘方法的对抗再学习的稳健性，为机器遗忘领域的研究提供了新视角和贡献。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究探讨了如何通过微调、对齐和遗忘等后训练方法来改变语言模型的行为和表示。</li>
<li>这些后训练修改方式的脆弱性受到关注，容易受到提示工程或再学习的绕过。</li>
<li>后训练可能引发浅层上下文依赖的“电路”，抑制特定响应模式，这是其脆弱性的原因之一。</li>
<li>分层遗忘（LU）算法被设计来应对这一问题，通过创建对数据子集的独特抑制机制来提高稳健性。</li>
<li>LU算法通过分阶段遗忘和再学习过程来限制再学习的能力。</li>
<li>实验评估表明，LU对提高对不同遗忘方法的对抗再学习的稳健性有积极效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09500">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c8fa37342bdb3d7de6a4327f47342eca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-488743345b68f7a7b03b6308dba8f005.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d60ddf0493315798f87086fa9f6c413.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dc37dc781d736891d06831898028f75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69087f5f3090083fc2f9907d9e04418.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49b1c97ae2226469a43304264c7c6a07.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Omni-R1-Do-You-Really-Need-Audio-to-Fine-Tune-Your-Audio-LLM"><a href="#Omni-R1-Do-You-Really-Need-Audio-to-Fine-Tune-Your-Audio-LLM" class="headerlink" title="Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"></a>Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?</h2><p><strong>Authors:Andrew Rouditchenko, Saurabhchand Bhati, Edson Araujo, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass</strong></p>
<p>We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance. </p>
<blockquote>
<p>我们提出了Omni-R1，它通过强化学习的方法GRPO对近期的多模态大型语言模型Qwen2.5-Omni进行微调，该模型在一个音频问答数据集上进行训练。这实现了在最新MMAU基准测试上的最新国家先进技术表现。Omni-R1在声音、音乐、语音和总体平均类别上均实现了最高精度，无论是在Test-mini还是Test-full分割上。为了了解性能提升情况，我们对带有和不带音频的模型进行了测试，发现GRPO的大部分性能提升可归功于基于文本的更优推理。我们还意外地发现，在只有文本的数据集上不使用音频进行微调，也能有效提高基于音频的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09439v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Omni-R1通过在一个音频问答数据集上微调多模态大型语言模型Qwen2.5-Omni，并使用强化学习算法GRPO，实现了在最新MMAU基准测试上的最新国家技术表现。Omni-R1在声音、音乐、语音和总体平均类别上均达到了最高准确率，无论是在Test-mini还是Test-full分割上。为了了解性能提升的原因，我们测试了带有和不带音频的模型，发现GRPO的大部分性能提升归因于基于文本推理的改进。我们还意外地发现，在文本数据集上微调而不使用音频也能有效提高音频性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni-R1通过微调多模态大型语言模型Qwen2.5-Omni并应用强化学习算法GRPO，达到了最新MMAU基准测试的最高准确率。</li>
<li>Omni-R1在声音、音乐、语音和总体平均类别上表现优异。</li>
<li>性能提升部分归因于基于文本推理的改进。</li>
<li>即使在不带音频的情况下进行微调，也能有效提高模型在音频任务上的性能。</li>
<li>研究展示了多模态模型在音频问答数据集上的潜力。</li>
<li>强化学习算法GRPO在模型性能优化中发挥了重要作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-419ec3f211da1c24b8974ba413058c22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a923a49381e7fe950b6567068892ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06fcd64a5e7f3071098582e253bec112.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CXMArena-Unified-Dataset-to-benchmark-performance-in-realistic-CXM-Scenarios"><a href="#CXMArena-Unified-Dataset-to-benchmark-performance-in-realistic-CXM-Scenarios" class="headerlink" title="CXMArena: Unified Dataset to benchmark performance in realistic CXM   Scenarios"></a>CXMArena: Unified Dataset to benchmark performance in realistic CXM   Scenarios</h2><p><strong>Authors:Raghav Garg, Kapil Sharma, Karan Gupta</strong></p>
<p>Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand’s CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmark’s difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques. </p>
<blockquote>
<p>大型语言模型（LLM）在客户体验管理（CXM）方面，特别是在呼叫中心运营方面，具有巨大的革命性潜力。然而，由于在隐私担忧导致的数据稀缺和当前基准测试的局限性，评估它们在复杂运营环境中的实际效用受到了阻碍。现有的基准测试通常缺乏现实性，未能融入深层知识库（KB）整合、现实世界噪音或超越会话流畅度的关键运营任务。为了弥补这一差距，我们推出了CXMArena，这是一个专门为评估运营CXM环境中的AI而设计的新型大规模合成基准数据集。鉴于呼叫中心功能的多样性，我们已经开发了一个可扩展的LLM驱动管道，模拟品牌CXM实体作为我们数据集的基础，如包括产品规格、问题分类和呼叫中心对话的知识文章。这些实体紧密地代表了现实世界的分布，因为注入了可控的噪声（由领域专家提供信息）并通过严格的自动化验证。在此基础上，我们发布了CXMArena，它提供了针对五个重要运营任务的专用基准测试：知识库细化、意图预测、代理质量遵守、文章搜索以及多回合RAG与集成工具。我们的基准实验突显了基准测试的困难：即使是最先进的嵌入和生成模型在文章搜索方面也只能达到68%的准确率，而标准嵌入方法在为知识库细化时得到的F1分数很低，为0.3，这突显了当前模型面临的挑战，需要采用复杂的管道和解决方案来超越传统技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09436v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在客户体验管理（CXM）领域具有巨大潜力，特别是在呼叫中心运营方面。然而，由于数据隐私担忧导致的数据稀缺性和现有基准测试的局限性，评估其在复杂操作环境中的实用性面临挑战。为解决这一问题，我们推出了CXMArena，一个专为评估操作型CXM中的AI而设计的大型合成基准数据集。该数据集通过模拟品牌CXM实体（如产品规格、问题分类和呼叫中心对话）的管道，体现了接触中心的多种特征。我们在此基础上发布了CXMArena，提供五个重要操作任务的专门基准测试：知识库优化、意图预测、代理质量遵守、文章搜索和多回合对话与集成工具。基准实验表明，即使是最先进的嵌入和生成模型，在文章搜索任务上的准确率也只有68%，而标准嵌入方法在知识库优化任务上的F1分数较低，这突显了当前模型面临的挑战，需要复杂的管道和解决方案来应对。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在CXM领域具有潜力，特别是在呼叫中心运营方面。</li>
<li>数据稀缺和现有基准测试的局限性限制了LLM在实际操作环境中的评估。</li>
<li>CXMArena是一个专为评估操作型CXM中的AI而设计的大型合成基准数据集。</li>
<li>CXMArena模拟了品牌CXM实体的管道，包括产品规格、问题分类和呼叫中心对话等。</li>
<li>CXMArena提供了五个重要操作任务的基准测试，包括知识库优化、意图预测等。</li>
<li>基准实验表明，当前模型在CXMArena上的表现存在挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09436">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0ffae04fc8453e3a2e659acef5d970bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59a37b70f30870479ceea7e7a2b36bf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b13a15c89c86afd5502d52f6ecb3aea4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df029c8b2203cccbf2482ac6f0aecc0d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Endo-CLIP-Progressive-Self-Supervised-Pre-training-on-Raw-Colonoscopy-Records"><a href="#Endo-CLIP-Progressive-Self-Supervised-Pre-training-on-Raw-Colonoscopy-Records" class="headerlink" title="Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy   Records"></a>Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy   Records</h2><p><strong>Authors:Yili He, Yan Zhu, Peiyao Fu, Ruijie Yang, Tianyi Chen, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang</strong></p>
<p>Pre-training on image-text colonoscopy records offers substantial potential for improving endoscopic image analysis, but faces challenges including non-informative background images, complex medical terminology, and ambiguous multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised framework that enhances Contrastive Language-Image Pre-training (CLIP) for this domain. Endo-CLIP’s three-stage framework–cleansing, attunement, and unification–addresses these challenges by (1) removing background frames, (2) leveraging large language models to extract clinical attributes for fine-grained contrastive learning, and (3) employing patient-level cross-attention to resolve multi-polyp ambiguities. Extensive experiments demonstrate that Endo-CLIP significantly outperforms state-of-the-art pre-training methods in zero-shot and few-shot polyp detection and classification, paving the way for more accurate and clinically relevant endoscopic analysis. </p>
<blockquote>
<p>对图像文本结肠镜检查记录的预训练在改进内镜图像分析方面具有巨大潜力，但面临着非信息性背景图像、复杂的医学术语和模糊的多病灶描述等挑战。我们引入了Endo-CLIP，这是一种新型的自监督框架，能够增强这一领域的对比语言图像预训练（CLIP）。Endo-CLIP的三阶段框架——净化、调整和统一，通过（1）去除背景帧，（2）利用大型语言模型提取用于精细对比学习的临床特征，（3）采用患者级交叉注意力来解决多息肉模糊问题，来解决这些挑战。大量实验表明，Endo-CLIP在零样本和少样本息肉检测和分类方面显著优于最新的预训练方法，为更准确和临床相关的内镜分析铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09435v1">PDF</a> Early accepted to MICCAI 2025</p>
<p><strong>Summary</strong>：预训练图像文本结肠镜检查记录对改进内镜图像分析具有巨大潜力，但面临非信息背景图像、复杂医学术语和模糊的多病灶描述等挑战。为此，我们引入了Endo-CLIP，一种新型的自监督框架，用于增强此领域的对比语言图像预训练（CLIP）。Endo-CLIP的三阶段框架包括净化、调整和统一，通过（1）去除背景帧，（2）利用大型语言模型提取临床属性进行精细对比学习，（3）采用患者级交叉注意力来解决多息肉模糊性。大量实验表明，Endo-CLIP在零样本和少样本息肉检测和分类方面显著优于最新的预训练方法，为更准确和临床相关的内镜分析铺平了道路。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>预训练图像文本结肠镜检查记录在改进内镜图像分析方面具有巨大潜力。</li>
<li>Endo-CLIP是一个新型自监督框架，旨在增强对比语言图像预训练（CLIP）在该领域的表现。</li>
<li>Endo-CLIP通过去除背景帧、利用大型语言模型提取临床属性进行精细对比学习以及采用患者级交叉注意力来解决挑战。</li>
<li>Endo-CLIP显著优于最新的预训练方法，在零样本和少样本息肉检测和分类方面具有优越性能。</li>
<li>Endo-CLIP的三阶段框架包括净化、调整和统一，确保了其有效性和准确性。</li>
<li>该研究为更准确和临床相关的内镜分析铺平了道路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09435">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-134bbf35634b0242eb5fcf5142640093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c040039c4d6653b8ed5abd15ba70502.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b660e7bf7213a3ed505faf4ed6d1cc71.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SafePath-Conformal-Prediction-for-Safe-LLM-Based-Autonomous-Navigation"><a href="#SafePath-Conformal-Prediction-for-Safe-LLM-Based-Autonomous-Navigation" class="headerlink" title="SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation"></a>SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation</h2><p><strong>Authors:Achref Doula, Max Mühläuser, Alejandro Sanchez Guinea</strong></p>
<p>Large Language Models (LLMs) show growing promise in autonomous driving by reasoning over complex traffic scenarios to generate path plans. However, their tendencies toward overconfidence, and hallucinations raise critical safety concerns. We introduce SafePath, a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. SafePath operates in three stages. In the first stage, we use an LLM that generates a set of diverse candidate paths, exploring possible trajectories based on agent behaviors and environmental cues. In the second stage, SafePath filters out high-risk trajectories while guaranteeing that at least one safe option is included with a user-defined probability, through a multiple-choice question-answering formulation that integrates conformal prediction. In the final stage, our approach selects the path with the lowest expected collision risk when uncertainty is low or delegates control to a human when uncertainty is high. We theoretically prove that SafePath guarantees a safe trajectory with a user-defined probability, and we show how its human delegation rate can be tuned to balance autonomy and safety. Extensive experiments on nuScenes and Highway-env show that SafePath reduces planning uncertainty by 77% and collision rates by up to 70%, demonstrating effectiveness in making LLM-driven path planning more safer. </p>
<blockquote>
<p>大型语言模型（LLM）在自动驾驶路径规划方面显示出越来越大的潜力，它们可以通过复杂的交通场景进行推理来生成路径计划。然而，它们过于自信和幻觉的倾向引发了关键的安全问题。我们引入了SafePath，这是一个模块化框架，利用形式化预测来增强基于LLM的路径规划的安全保障。SafePath分为三个阶段运行。在第一阶段，我们使用LLM生成一组多样化的候选路径，根据代理行为和环境线索探索可能的轨迹。在第二阶段，SafePath过滤掉高风险轨迹，同时通过多重选择问答的形式整合形式化预测，保证至少有一个安全选项被包括在内，且概率由用户定义。在最后一个阶段，当不确定性较低时，我们的方法会选择具有最低预期碰撞风险的路径；当不确定性较高时，则会将控制权交给人类。我们从理论上证明了SafePath能保证以用户定义的概率选择安全轨迹，并且展示了如何调整其人类代理率以平衡自主性和安全性。在nuScenes和Highway-env上的广泛实验表明，SafePath将规划不确定性降低了77%，碰撞率降低了高达70%，证明了其在使LLM驱动的路径规划更加安全方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09427v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在自动驾驶路径规划中具有巨大潜力，通过复杂的交通场景进行推理。然而，其过度自信与幻觉问题引发关键安全顾虑。为此，我们推出SafePath框架，该框架以模块化的形式增强LLM路径规划，并利用形式化安全保证技术——顺应预测技术进行优化。SafePath包含三个阶段：首先使用LLM生成多样化候选路径集；然后通过集成顺应预测技术的选择题问答模式筛选出高风险轨迹并保障至少一个安全选项的存在，其存在概率为用户自定义值；最后在不确定性较低时选择碰撞风险最低的路径执行或在高不确定性时移交控制权给人类。SafePath理论证明能保障用户自定义概率下的安全轨迹，并且可以通过调整人类代理率来平衡自主性与安全性。在nuScenes和Highway-env上的实验表明，SafePath将规划不确定性降低了77%，碰撞率最高可降低70%，有效提升了LLM驱动路径规划的安全性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在自动驾驶路径规划中展现出巨大潜力，能处理复杂的交通场景推理。</li>
<li>LLM的过度自信与幻觉问题引发安全顾虑。</li>
<li>SafePath框架通过模块化设计增强LLM路径规划，并引入形式化安全保证技术——顺应预测技术。</li>
<li>SafePath包含三个阶段：生成候选路径、筛选高风险轨迹并保障至少一个安全选项的存在、选择碰撞风险最低的路径或在不确定性高时移交控制权。</li>
<li>SafePath可保障用户自定义概率下的安全轨迹，并通过调整人类代理率平衡自主性与安全性。</li>
<li>实验显示SafePath能显著降低规划不确定性和碰撞率，有效提升LLM驱动路径规划的安全性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09427">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ceb2ba9a017666c47262cfa6727645ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e614b5ab27e5eda99db8a4cc5787d62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2dcd4884fec3e6cfcba7f5161d43556.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FaceShield-Explainable-Face-Anti-Spoofing-with-Multimodal-Large-Language-Models"><a href="#FaceShield-Explainable-Face-Anti-Spoofing-with-Multimodal-Large-Language-Models" class="headerlink" title="FaceShield: Explainable Face Anti-Spoofing with Multimodal Large   Language Models"></a>FaceShield: Explainable Face Anti-Spoofing with Multimodal Large   Language Models</h2><p><strong>Authors:Hongyang Wang, Yichen Shi, Zhuofu Tao, Yuhao Gao, Liepiao Zhang, Xun Lin, Jun Feng, Xiaochen Yuan, Zitong Yu, Xiaochun Cao</strong></p>
<p>Face anti-spoofing (FAS) is crucial for protecting facial recognition systems from presentation attacks. Previous methods approached this task as a classification problem, lacking interpretability and reasoning behind the predicted results. Recently, multimodal large language models (MLLMs) have shown strong capabilities in perception, reasoning, and decision-making in visual tasks. However, there is currently no universal and comprehensive MLLM and dataset specifically designed for FAS task. To address this gap, we propose FaceShield, a MLLM for FAS, along with the corresponding pre-training and supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K. FaceShield is capable of determining the authenticity of faces, identifying types of spoofing attacks, providing reasoning for its judgments, and detecting attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that incorporates both the original image and auxiliary information based on prior knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to random mask vision tokens, thereby improving the model’s generalization ability. We conducted extensive experiments on three benchmark datasets, demonstrating that FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks, i.e., coarse-grained classification, fine-grained classification, reasoning, and attack localization. Our instruction datasets, protocols, and codes will be released soon. </p>
<blockquote>
<p>人脸识别反欺骗（Face Anti-Spoofing，FAS）对于保护人脸识别系统免受伪造攻击至关重要。以往的方法将此任务视为分类问题，缺乏预测结果的可解释性和推理。最近，多模态大型语言模型（Multimodal Large Language Models，MLLMs）在视觉任务的感知、推理和决策方面表现出强大的能力。然而，目前尚没有针对FAS任务的通用且全面的MLLM及其专门设计的数据集。为了弥补这一空白，我们提出了FaceShield，这是一个用于FAS的MLLM，以及相应的预训练和监督微调（Supervised Fine-tuning，SFT）数据集FaceShield-pre10K和FaceShield-sft45K。FaceShield能够确定人脸的真实性，识别欺骗攻击的类型，为其判断提供理由，并检测攻击区域。具体来说，我们采用了基于先验知识的欺骗感知视觉感知（Spoof-Aware Vision Perception，SAVP），它结合了原始图像和辅助信息。然后，我们使用提示引导视觉令牌掩码（Prompt-Guided Vision Token Masking，PVTM）策略随机掩蔽视觉令牌，从而提高模型的泛化能力。我们在三个基准数据集上进行了大量实验，结果表明FaceShield在四个FAS任务上显著优于之前的深度学习模型和普通MLLMs，包括粗粒度分类、细粒度分类、推理和攻击定位。我们的指令数据集、协议和代码将很快发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09415v1">PDF</a> </p>
<p><strong>Summary</strong><br>     为提高面部识别系统的防欺诈能力，推出FaceShield——一种用于面部抗欺骗（FAS）的多模态大型语言模型。通过引入伪造感知视觉感知和提示引导视觉令牌遮蔽策略，FaceShield能在基准测试中显著超越现有模型，表现出卓越性能。现已推出预训练和监督微调数据集FaceShield-pre10K和FaceShield-sft45K。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）被用于面部抗欺骗（FAS）任务以增强面部识别系统的安全性。</li>
<li>当前缺乏针对FAS任务的通用和全面的MLLM及数据集。</li>
<li>提出FaceShield MLLM，配合预训练和监督微调数据集FaceShield-pre10K和FaceShield-sft45K，以用于FAS任务。</li>
<li>FaceShield具有判定面部真实性、识别欺骗攻击类型、为判断提供理由和检测攻击区域的能力。</li>
<li>利用伪造感知视觉感知（SAVP）技术结合原始图像和基于先验知识的辅助信息进行处理。</li>
<li>采用提示引导视觉令牌遮蔽（PVTM）策略随机遮蔽视觉令牌，提高模型的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09415">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-55185f3fdc9879becd930dd8fb965e70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8539d4c62cd72c3806d6ad0824439954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7404df70b25b27dfa327cb1521a6b50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7edcd9cd1280b4a3f6653bb9c6276e5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0749d3860911e2a67f1886cb053c46f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b302d7ee81c120d15d8c0389e6a0e04a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3e8889cd64e76f523db601e22f4a098.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Qwen3-Technical-Report"><a href="#Qwen3-Technical-Report" class="headerlink" title="Qwen3 Technical Report"></a>Qwen3 Technical Report</h2><p><strong>Authors:An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu</strong></p>
<p>In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models–such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)–and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0. </p>
<blockquote>
<p>在这项工作中，我们推出了Qwen3，它是Qwen模型家族的最新版本。Qwen3包含一系列大型语言模型（LLM），旨在提高性能、效率和多语言能力。Qwen3系列包括密集模型和混合专家（MoE）架构的模型，参数规模从6千亿到235千亿不等。Qwen3的关键创新在于将思考模式（用于复杂、多步骤推理）和非思考模式（用于快速、基于上下文的响应）集成到一个统一框架中。这消除了需要在不同的模型之间切换的需要，比如优化聊天模型（例如GPT-4o）和专用推理模型（例如QwQ-32B），并可根据用户查询或聊天模板实现动态模式切换。同时，Qwen3引入了一种思考预算机制，允许用户在推理过程中自适应地分配计算资源，从而根据任务复杂性平衡延迟和性能。此外，通过利用旗舰模型的知识，我们在构建小型模型时显著减少了所需的计算资源，同时确保了其高度竞争的性能。经验评估表明，Qwen3在多种基准测试中取得了最新结果，包括代码生成、数学推理、代理任务等，与更大的MoE模型和专有模型竞争。与前代产品Qwen2.5相比，Qwen3将多语言支持从29种扩展到119种语言和方言，通过改进跨语言理解和生成能力，增强了全球可及性。为便于复制和社区驱动的研究与开发，所有Qwen3模型均在Apache 2.0下公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09388v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>本文介绍了Qwen3，Qwen模型系列的最新版本。Qwen3包括一系列大型语言模型（LLM），旨在提高性能、效率和多语言能力。Qwen3系列包括密集模型和混合专家（MoE）架构的模型，参数规模从6千亿到235千亿不等。主要创新在于将思考模式（用于复杂的多步骤推理）和非思考模式（用于快速、基于上下文的响应）统一到框架中。同时引入思考预算机制，平衡推理的延迟和性能。通过旗舰模型的知识，减少构建小型模型的计算资源需求。Qwen3在多语言支持方面从29种语言和方言扩展到119种，提高了跨语言理解和生成能力。所有Qwen3模型均公开可用，以推动可复制性和社区驱动的研究和开发。

**Key Takeaways**
 
1. Qwen3是Qwen模型系列的最新版本的大型语言模型（LLM）。
2. Qwen3系列包括密集和混合专家（MoE）架构的模型，参数规模有所不同。
3. Qwen3集成了思考模式和非思考模式，实现动态模式切换。
4. Qwen3引入思考预算机制，根据任务复杂性平衡延迟和性能。
5. 利用旗舰模型知识减少小型模型的计算资源需求，保持高度竞争力。
6. Qwen3在多语言支持方面显著扩展，增强了全球访问能力。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09388">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-646a5867a5afc20a04fa44e0cea5effa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e848cea44c853eac82e4f86bf15d0793.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e8ca319b439a92e91a2cfe4b0f3e228.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5e21ab9d37d00217bbec6adc26a6c28.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MAKE-Multi-Aspect-Knowledge-Enhanced-Vision-Language-Pretraining-for-Zero-shot-Dermatological-Assessment"><a href="#MAKE-Multi-Aspect-Knowledge-Enhanced-Vision-Language-Pretraining-for-Zero-shot-Dermatological-Assessment" class="headerlink" title="MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for   Zero-shot Dermatological Assessment"></a>MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for   Zero-shot Dermatological Assessment</h2><p><strong>Authors:Siyuan Yan, Xieji Li, Ming Hu, Yiwen Jiang, Zhen Yu, Zongyuan Ge</strong></p>
<p>Dermatological diagnosis represents a complex multimodal challenge that requires integrating visual features with specialized clinical knowledge. While vision-language pretraining (VLP) has advanced medical AI, its effectiveness in dermatology is limited by text length constraints and the lack of structured texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced vision-language pretraining framework for zero-shot dermatological tasks. Recognizing that comprehensive dermatological descriptions require multiple knowledge aspects that exceed standard text constraints, our framework introduces: (1) a multi-aspect contrastive learning strategy that decomposes clinical narratives into knowledge-enhanced sub-texts through large language models, (2) a fine-grained alignment mechanism that connects subcaptions with diagnostically relevant image features, and (3) a diagnosis-guided weighting scheme that adaptively prioritizes different sub-captions based on clinical significance prior. Through pretraining on 403,563 dermatological image-text pairs collected from education resources, MAKE significantly outperforms state-of-the-art VLP models on eight datasets across zero-shot skin disease classification, concept annotation, and cross-modal retrieval tasks. Our code will be made publicly available at https: &#x2F;&#x2F;github.com&#x2F;SiyuanYan1&#x2F;MAKE. </p>
<blockquote>
<p>皮肤科诊断是一项复杂的跨学科挑战，需要整合视觉特征与专业的临床知识。虽然视觉语言预训练（VLP）已经推动了医疗人工智能的发展，但其在皮肤科领域的应用受限于文本长度和缺乏结构化文本。在本文中，我们介绍了MAKE，这是一个用于零样本皮肤科任务的全方位知识增强视觉语言预训练框架。我们认识到全面的皮肤科描述需要涉及多个知识方面，超出了标准文本限制，因此我们的框架引入了以下要素：（1）一种多视角对比学习策略，通过大型语言模型将临床叙事分解为知识增强的子文本；（2）一种精细对齐机制，将子标题与诊断相关的图像特征联系起来；（3）一种以诊断为导向的加权方案，根据临床重要性先验自适应地优先处理不同的子标题。通过对来自教育资源的403,563张皮肤科图像文本对进行预训练，MAKE在零样本皮肤病分类、概念标注和跨模态检索任务上的八个数据集上显著优于最新的VLP模型。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/SiyuanYan1/MAKE%E4%B8%8A%E5%85%AC%E5%BC%BA%E6%8F%A0%E4%BB%A5%E9%BB%BE%E7%BB%BE%E5%AD%A6%E6%9C%AF%E5%B7%A5%E4%BD%BF%E7%9D%A1%E7%9C%BC%E7%9A%84%E5%B7%A5%E4%BD%BF%E7%BF%BB">https://github.com/SiyuanYan1/MAKE上公开提供。</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09372v1">PDF</a> MICCAI2025 early acceptance; First two authors contribute equally</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为MAKE的多方面知识增强视觉语言预训练框架，用于零样本皮肤病学任务。该框架解决了传统视觉语言预训练在皮肤病学应用上的局限性，包括文本长度约束和缺乏结构化文本的问题。通过引入多方面对比学习策略、精细对齐机制和诊断引导加权方案，MAKE在零样本皮肤疾病分类、概念标注和跨模态检索任务上显著优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>皮肤病学诊断是一个复杂的跨模态挑战，需要整合视觉特征和临床专业知识。</li>
<li>视觉语言预训练（VLP）在医疗AI中的应用已有所进展，但在皮肤病学领域仍面临文本长度和结构化文本的局限性。</li>
<li>MAKE框架引入多方面知识增强子文本分解策略，通过大型语言模型将临床叙述分解为多个知识方面。</li>
<li>MAKE使用精细对齐机制将子标题与诊断相关的图像特征相连。</li>
<li>诊断引导加权方案能够自适应地根据临床重要性优先排序不同的子标题。</li>
<li>在教育资源收集的403,563张皮肤病学图像文本对上进行的预训练使MAKE显著优于其他先进的VLP模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09372">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d50a7088c56521db9866b1f0b7f2452f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-053b1d505cdab9a48db06c4b860d128b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4fe558bfae17ab207940c442e420f5f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RAG-Enabled-Intent-Reasoning-for-Application-Network-Interaction"><a href="#RAG-Enabled-Intent-Reasoning-for-Application-Network-Interaction" class="headerlink" title="RAG-Enabled Intent Reasoning for Application-Network Interaction"></a>RAG-Enabled Intent Reasoning for Application-Network Interaction</h2><p><strong>Authors:Salwa Mostafa, Mohamed K. Abdel-Aziz, Mohammed S. Elbamby, Mehdi Bennis</strong></p>
<p>Intent-based network (IBN) is a promising solution to automate network operation and management. IBN aims to offer human-tailored network interaction, allowing the network to communicate in a way that aligns with the network users’ language, rather than requiring the network users to understand the technical language of the network&#x2F;devices. Nowadays, different applications interact with the network, each with its own specialized needs and domain language. Creating semantic languages (i.e., ontology-based languages) and associating them with each application to facilitate intent translation lacks technical expertise and is neither practical nor scalable. To tackle the aforementioned problem, we propose a context-aware AI framework that utilizes machine reasoning (MR), retrieval augmented generation (RAG), and generative AI technologies to interpret intents from different applications and generate structured network intents. The proposed framework allows for generalized&#x2F;domain-specific intent expression and overcomes the drawbacks of large language models (LLMs) and vanilla-RAG framework. The experimental results show that our proposed intent-RAG framework outperforms the LLM and vanilla-RAG framework in intent translation. </p>
<blockquote>
<p>基于意图的网络（IBN）是自动化网络操作和管理的一种有前途的解决方案。IBN旨在提供符合人类需求的网络交互，让网络能够以与用户语言相符的方式进行沟通，而无需网络用户理解网络&#x2F;设备的技术语言。如今，不同的应用程序与网络进行交互，各有自己的特殊需求和领域语言。创建语义语言（例如基于本体论的语言）并与每个应用程序关联，以促进意图翻译，但这缺乏技术专业知识，既不实际也不可扩展。为了解决上述问题，我们提出了一种利用机器推理（MR）、检索增强生成（RAG）和生成式AI技术的上下文感知AI框架，以解释不同应用程序的意图并生成结构化网络意图。所提出的框架允许通用&#x2F;特定领域的意图表达，并克服了大型语言模型（LLM）和常规RAG框架的缺点。实验结果表明，我们提出的基于意图的RAG框架在意图翻译方面优于LLM和常规RAG框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09339v1">PDF</a> </p>
<p><strong>Summary</strong>：基于意图的网络（IBN）通过利用先进的AI技术，如机器推理、检索增强生成和生成式AI技术，实现了网络操作的自动化管理，并提供了人性化的网络交互体验。针对不同应用的需求和领域语言，IBN能够解析并生成结构化网络意图，提高了意图翻译的效率和准确性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>IBN致力于实现网络操作的自动化和管理，并注重人性化的网络交互体验。</li>
<li>IBN通过使用先进的AI技术解析和生成结构化网络意图，满足不同应用的需求和领域语言。</li>
<li>创建一个基于语义的语言（如基于本体论的语言）与每个应用程序相关联以促进意图翻译是一个不切实际且不可扩展的方法。</li>
<li>与大型语言模型（LLMs）和普通的RAG框架相比，提出的基于意图的RAG框架在意图翻译方面表现出更好的性能。</li>
<li>IBN的目标是让网络以与用户语言相符的方式与用户进行交互，而不是要求用户理解网络&#x2F;设备的专业技术语言。</li>
<li>基于意图的网络交互提高了用户体验并简化了网络操作和管理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09339">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a5d6599c1c84bbb06b7629cf4bca3107.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac136d96cfb3384cd9e1097f3914bb8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed05176deb679ad0b68fdfd4a1eda093.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-674ad10564f9b6c9104f78b37dc00436.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fcb296d3e3d607f1e3cf2204c672af8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Activation-Steering-in-Neural-Theorem-Provers"><a href="#Activation-Steering-in-Neural-Theorem-Provers" class="headerlink" title="Activation Steering in Neural Theorem Provers"></a>Activation Steering in Neural Theorem Provers</h2><p><strong>Authors:Shashank Kirtania</strong></p>
<p>Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments. </p>
<blockquote>
<p>大型语言模型（LLM）在使用像Lean这样的证明助手来证明定理方面显示出潜力。然而，目前最先进的语言模型在预测证明中的下一步时面临困难，这使得实践者使用不同的采样技术来提高LLM的能力。我们发现LLM能够预测正确的策略，但在一组候选策略中恰当地对其进行排名时面临挑战，这影响了整体的选择过程。为了克服这一障碍，我们使用激活引导来指导LLM的响应，以提高推理时的生成能力。我们的结果表明，激活引导提供了一种有前景的轻量级替代方法，特别是对于那些资源受限的环境来说特别有价值，可专门用于增强LLM中的定理证明能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15507v3">PDF</a> incorrect explanation for a concept, need to revise and update!</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在利用证明助手如Lean进行定理证明方面展现出潜力。然而，当前最先进的语言模型在预测证明中的下一步时遇到困难，促使实践者使用不同的采样技术来提升LLMs的能力。观察到LLM能够预测正确的策略，但在一组候选策略中恰当地排序面临挑战，影响整体选择过程。为克服这一难题，我们采用激活引导来指导LLMs的响应，改善推理时的生成。研究结果表明，激活引导为增强LLMs在定理证明方面的能力提供了一种有前途的轻量级替代方案，特别是在资源受限的环境中尤为宝贵。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在定理证明方面展现出潜力，可使用证明助手如Lean。</li>
<li>当前LLMs在预测证明中的下一步时遇到困难，需要采用采样技术提升能力。</li>
<li>LLM能够预测正确的策略，但排序候选策略时存在挑战。</li>
<li>激活引导是一种有前途的方法，可指导LLMs的响应，改善推理时的生成。</li>
<li>激活引导为增强LLMs在定理证明方面的能力提供了轻量级替代方案。</li>
<li>此方法特别在资源受限的环境中显示出其价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15507">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-716c5c8ed71490a9d8a964db1016f178.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-474238b33b66b42214e695ba254a866d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4581620a3f13882decd1445f44e686e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc0a4152e5c8e5a82a0d93eeb46fddb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1410590d77eae4be9cd5a1d827d3be35.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="PSPO-An-Effective-Process-supervised-Policy-Optimization-for-Reasoning-Alignment"><a href="#PSPO-An-Effective-Process-supervised-Policy-Optimization-for-Reasoning-Alignment" class="headerlink" title="PSPO*: An Effective Process-supervised Policy Optimization for Reasoning   Alignment"></a>PSPO*: An Effective Process-supervised Policy Optimization for Reasoning   Alignment</h2><p><strong>Authors:Jiawei Li, Xinyue Liang, Junlong Zhang, Yizhe Yang, Chong Feng, Yang Gao</strong></p>
<p>Process supervision enhances the performance of large language models in reasoning tasks by providing feedback at each step of chain-of-thought reasoning. However, due to the lack of effective process supervision methods, even advanced large language models are prone to logical errors and redundant reasoning. We claim that the effectiveness of process supervision significantly depends on both the accuracy and the length of reasoning chains. Moreover, we identify that these factors exhibit a nonlinear relationship with the overall reward score of the reasoning process. Inspired by these insights, we propose a novel process supervision paradigm, PSPO*, which systematically outlines the workflow from reward model training to policy optimization, and highlights the importance of nonlinear rewards in process supervision. Based on PSPO*, we develop the PSPO-WRS, which considers the number of reasoning steps in determining reward scores and utilizes an adjusted Weibull distribution for nonlinear reward shaping. Experimental results on six mathematical reasoning datasets demonstrate that PSPO-WRS consistently outperforms current mainstream models. </p>
<blockquote>
<p>过程监督通过提供思维链推理每一步的反馈，提高了大型语言模型在推理任务中的性能。然而，由于缺乏有效的过程监督方法，即使是先进的大型语言模型也容易出现逻辑错误和冗余推理。我们声称，过程监督的有效性在很大程度上取决于推理链的准确性和长度。此外，我们认定这些因素与推理过程的总体奖励得分呈现出非线性关系。基于这些见解，我们提出了一种新型的过程监督范式PSPO<em>，它系统地概述了从奖励模型训练到策略优化的工作流程，并强调了过程监督中非线性奖励的重要性。基于PSPO</em>，我们开发了PSPO-WRS，它在确定奖励分数时考虑了推理步骤的数量，并利用调整后的威布尔分布进行非线性奖励塑造。在六个数学推理数据集上的实验结果表明，PSPO-WRS持续优于当前主流模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11681v3">PDF</a> Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/DIRECT-BIT/PSPO">https://github.com/DIRECT-BIT/PSPO</a></p>
<p><strong>Summary</strong></p>
<p>过程监督通过在大语言模型的推理任务每一步提供反馈来提升其性能。然而由于缺乏有效的过程监督方法，先进的语言模型也容易出现逻辑错误和冗余推理。本文提出过程监督的有效性很大程度上取决于推理链的准确性和长度，并且与推理过程的总体奖励得分呈非线性关系。因此，本文提出了一种新的过程监督范式——PSPO*，并基于此开发出了PSPO-WRS模型，考虑推理步骤的数量来决定奖励分数并利用修正的威布尔分布进行非线性奖励塑造。实验结果表明，在六个数学推理数据集上，PSPO-WRS始终优于当前的主流模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>过程监督对于提升大语言模型的推理任务性能有重要作用。</li>
<li>缺乏有效的过程监督方法可能导致语言模型出现逻辑错误和冗余推理。</li>
<li>过程监督的有效性取决于推理链的准确性和长度。</li>
<li>推理链的这些因素与整体奖励得分之间存在非线性关系。</li>
<li>提出了一种新的过程监督范式——PSPO*，并介绍了其工作流程。</li>
<li>基于PSPO*，开发出了PSPO-WRS模型，该模型考虑推理步骤数量来决定奖励分数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a8b189c4f3e603bbf0c985ebf44837f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-98729f8e6a7b9c270ccf22f72bb58fd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ed7227be5a5248c0d9148533afe66e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8038aac90d251c9e70912ea1beb8f4de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-527b059e0c19d5344b63ad884d9bfac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a6ed85b44a0f629a25edf743c9520ce.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Construction-and-Application-of-Materials-Knowledge-Graph-in-Multidisciplinary-Materials-Science-via-Large-Language-Model"><a href="#Construction-and-Application-of-Materials-Knowledge-Graph-in-Multidisciplinary-Materials-Science-via-Large-Language-Model" class="headerlink" title="Construction and Application of Materials Knowledge Graph in   Multidisciplinary Materials Science via Large Language Model"></a>Construction and Application of Materials Knowledge Graph in   Multidisciplinary Materials Science via Large Language Model</h2><p><strong>Authors:Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Bram Hoex, Haofen Wang, Tong Xie, Wenjie Zhang</strong></p>
<p>Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges to the efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques integrated with large language models to extract and systematically organize a decade’s worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs. </p>
<blockquote>
<p>材料科学的知识广泛分布在广泛的科学文献中，对新材料的有效发现和整合构成了重大挑战。传统方法往往依赖于昂贵且耗时的实验方法，进一步加剧了快速创新的复杂性。为了解决这些挑战，人工智能与材料科学的融合开辟了加速发现过程的途径，尽管它还要求精确标注、数据提取和信息可追溯性。为了解决这些问题，本文介绍了材料知识图谱（MKG），它利用先进的自然语言处理技术，结合大型语言模型，提取并系统地整理了过去十年高质量的研究内容，形成结构化三元组，包含162,605个节点和731,772条边。MKG将信息分类为全面的标签，如名称、公式和应用，围绕精心设计的本体进行结构化设计，从而提高了数据的可用性和集成度。通过实施基于网络的算法，MKG不仅有助于高效链接预测，而且显著减少对传统实验方法的依赖。这种结构化方法不仅简化了材料研究，而且为更复杂的科学知识图谱奠定了基石。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.03080v4">PDF</a> 14 pages, 7 figures, 3 tables; Accepted by 38th Conference on Neural   Information Processing Systems (NeurIPS 2024)</p>
<p><strong>Summary</strong>：材料科学知识广泛分散在大量的科学文献中，给新材料的发现和整合带来了巨大挑战。传统方法往往依赖于昂贵且耗时的实验方法，进一步加剧了快速创新的复杂性。针对这些挑战，人工智能与材料科学的融合为加速发现过程打开了途径，但同时也需要精确标注、数据提取和信息的可追溯性。为解决这些问题，本文引入了材料知识图谱（MKG），它利用先进的自然语言处理技术与大型语言模型集成，提取并系统地组织了过去十年高质量的研究内容，形成结构化的三元组，包含162,605个节点和731,772条边。MKG通过精心设计的本体，将信息分类为名称、公式、应用等综合标签，提高了数据的可用性和整合性。通过实施网络算法，MKG不仅促进了有效的链接预测，而且大大减少了对传统实验方法的依赖。这种结构化方法不仅简化了材料研究，而且为更复杂的科学知识图谱奠定了基础。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>材料科学知识广泛分散，高效发现和整合新材料面临挑战。</li>
<li>传统实验方法既耗时又昂贵，限制快速创新。</li>
<li>人工智能与材料科学的结合为加速材料发现提供了新的途径。</li>
<li>材料知识图谱（MKG）利用自然语言处理和大型语言模型技术，系统地组织和提取材料科学领域的高质量研究。</li>
<li>MKG通过结构化的三元组形式存储信息，包含大量的节点和边，形成网络结构。</li>
<li>MKG通过本体将信息分类，提高数据的可用性和整合性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.03080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-98e9ea2d6df314830f4da943c23602a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dbdc10d03794283bb77d98bcace321d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0609a8fc0ddec16d4d2f9a60636f3d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4617fb9fe046410e3a5bf77db0d3f6d4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Detecting-Multimedia-Generated-by-Large-AI-Models-A-Survey"><a href="#Detecting-Multimedia-Generated-by-Large-AI-Models-A-Survey" class="headerlink" title="Detecting Multimedia Generated by Large AI Models: A Survey"></a>Detecting Multimedia Generated by Large AI Models: A Survey</h2><p><strong>Authors:Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu</strong></p>
<p>The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) and beyond detection (adding attributes like generalizability, robustness, and interpretability to detectors). Additionally, we have presented a brief overview of generation mechanisms, public datasets, online detection tools, and evaluation metrics to provide a valuable resource for researchers and practitioners in this field. Most importantly, we offer a focused analysis from a social media perspective to highlight their broader societal impact. Furthermore, we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our aim for this survey is to fill an academic gap and contribute to global AI security efforts, helping to ensure the integrity of information in the digital realm. The project link is <a target="_blank" rel="noopener" href="https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey">https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey</a>. </p>
<blockquote>
<p>大型人工智能模型（尤其是扩散模型和大语言模型）的快速发展标志着一个新时代的来临，AI生成的多媒体内容越来越融入日常生活的各个方面。尽管这些技术在许多领域都有益，但它们产生的多媒体内容也存在显著的风险，包括潜在滥用、社会动荡和伦理问题。因此，检测由大型人工智能模型生成的多媒体内容变得至关重要，相关研究也显著增多。尽管如此，关于检测LAIM生成的多媒体的系统性综述仍存在明显差距。为了填补这一空白，我们提供了第一篇全面涵盖检测由大型人工智能模型创建的多媒体（如文本、图像、视频、音频和多模态内容）的现有研究的综述。我们提出了一种新的检测方法论分类，按媒体模态分类，并从两个角度进行对齐：纯检测（旨在提高检测性能）和超越检测（向检测器添加通用性、稳健性和可解释性等属性）。此外，我们还概述了生成机制、公开数据集、在线检测工具和评估指标，为这一领域的研究人员和实践者提供了有价值的资源。最重要的是，我们从社交媒体的角度进行了深入分析，以强调它们对社会更广泛的影响。此外，我们还确定了当前检测面临的挑战，并提出了未来研究的方向，解决检测由大型人工智能模型生成的多媒体方面尚未探索、正在出现和新出现的问题。我们希望通过这项调查填补学术空白，为全球人工智能安全努力做出贡献，帮助确保数字领域的信息完整性。<a target="_blank" rel="noopener" href="https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey">项目链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.00045v5">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大型人工智能模型（LAIMs）的快速发展，特别是扩散模型和大语言模型的兴起，标志着一个新时代的到来。AI生成的多媒体内容日益融入日常生活的各个方面，虽然为众多领域带来了益处，但也存在潜在的误用、社会扰乱和伦理问题。因此，检测LAIM生成的多媒体内容至关重要，相关研究也迅速增加。然而，针对检测LAIM生成多媒体的系统性综述仍存在显著差距。为此，我们提供了第一篇全面综述，涵盖了检测LAIM生成的多媒体（如文本、图像、视频、音频和多模态内容）的现有研究。我们引入了一种新的检测方法的分类法，按媒体模态分类，并从纯粹检测（旨在提高检测性能）和超越检测（向检测器添加通用性、稳健性和可解释性等属性）两个角度进行介绍。此外，我们还概述了生成机制、公共数据集、在线检测工具和评估指标，为研究人员和实践者提供有价值的资源。最重要的是，我们从社交媒体的角度进行了深入分析，以突出其更广泛的社会影响。我们还确定了当前检测的挑战，并提出了未来研究的方向，以解决在检测LAIM生成的多媒体方面尚未探索、正在进行和新兴的问题。本综述旨在填补学术空白，为全球AI安全做出贡献，确保数字领域的资讯完整性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型人工智能模型（LAIMs）的发展推动了AI生成多媒体内容的普及，涉及多种媒体类型。</li>
<li>AI生成的多媒体在日常生活中的广泛应用带来了潜在的误用、社会扰乱和伦理问题。</li>
<li>检测LAIM生成的多媒体内容变得至关重要，相关研究正在迅速增加。</li>
<li>当前关于检测LAIM生成多媒体的系统性综述存在差距，本文旨在填补这一空白。</li>
<li>介绍了检测LAIM生成多媒体的现有研究、生成机制、公共数据集和在线检测工具。</li>
<li>从社交媒体角度分析了LAIM生成多媒体的广泛社会影响。</li>
<li>确定了当前检测的挑战，并提出了未来研究的方向，以解决新兴问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.00045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bcf4c64f2c042a2623e4bf31954c857b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4449c26a66fa7efc16665b06f1085c45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67d77441225e003f639f72d551a397c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55ce1bca568eabaeb44b6b66e4bb1a9e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-16/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-16/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-16/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1f2c8423fa863c4270331b0d5e445251.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-05-16  Streaming Multi-agent Pathfinding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c18bbc9107fbf4dc65c8d96173bbde4e.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-16  MIGRATION-BENCH Repository-Level Code Migration Benchmark from Java 8
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
