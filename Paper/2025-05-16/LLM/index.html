<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-16  Adversarial Suffix Filtering a Defense Pipeline for LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b660e7bf7213a3ed505faf4ed6d1cc71.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-16-æ›´æ–°"><a href="#2025-05-16-æ›´æ–°" class="headerlink" title="2025-05-16 æ›´æ–°"></a>2025-05-16 æ›´æ–°</h1><h2 id="Adversarial-Suffix-Filtering-a-Defense-Pipeline-for-LLMs"><a href="#Adversarial-Suffix-Filtering-a-Defense-Pipeline-for-LLMs" class="headerlink" title="Adversarial Suffix Filtering: a Defense Pipeline for LLMs"></a>Adversarial Suffix Filtering: a Defense Pipeline for LLMs</h2><p><strong>Authors:David Khachaturov, Robert Mullins</strong></p>
<p>Large Language Models (LLMs) are increasingly embedded in autonomous systems and public-facing environments, yet they remain susceptible to jailbreak vulnerabilities that may undermine their security and trustworthiness. Adversarial suffixes are considered to be the current state-of-the-art jailbreak, consistently outperforming simpler methods and frequently succeeding even in black-box settings. Existing defenses rely on access to the internal architecture of models limiting diverse deployment, increase memory and computation footprints dramatically, or can be bypassed with simple prompt engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$ (ASF), a lightweight novel model-agnostic defensive pipeline designed to protect LLMs against adversarial suffix attacks. ASF functions as an input preprocessor and sanitizer that detects and filters adversarially crafted suffixes in prompts, effectively neutralizing malicious injections. We demonstrate that ASF provides comprehensive defense capabilities across both black-box and white-box attack settings, reducing the attack efficacy of state-of-the-art adversarial suffix generation methods to below 4%, while only minimally affecting the target modelâ€™s capabilities in non-adversarial scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«åµŒå…¥åˆ°è‡ªä¸»ç³»ç»Ÿå’Œé¢å‘å…¬ä¼—çš„ç¯å¢ƒä¸­ï¼Œç„¶è€Œå®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°æ¼æ´æ”»å‡»ï¼Œè¿™äº›æ”»å‡»å¯èƒ½ä¼šç ´åå…¶å®‰å…¨æ€§å’Œå¯ä¿¡åº¦ã€‚å¯¹æŠ—æ€§åç¼€è¢«è®¤ä¸ºæ˜¯ç›®å‰æœ€å…ˆè¿›çš„æ”»å‡»æ–¹å¼ï¼ŒæŒç»­ä¼˜äºæ›´ç®€å•çš„æ–¹æ³•ï¼Œå³ä½¿åœ¨é»‘ç®±ç¯å¢ƒä¸­ä¹Ÿç»å¸¸å–å¾—æˆåŠŸã€‚ç°æœ‰çš„é˜²å¾¡æ‰‹æ®µä¾èµ–äºå¯¹æ¨¡å‹å†…éƒ¨æ¶æ„çš„è®¿é—®ï¼Œé™åˆ¶äº†å…¶å¤šæ ·åŒ–éƒ¨ç½²ï¼Œä¸”æå¤§åœ°å¢åŠ äº†å†…å­˜å’Œè®¡ç®—å ç”¨è¶³è¿¹ï¼Œæˆ–è€…å¯ä»¥é€šè¿‡ç®€å•çš„æç¤ºå·¥ç¨‹æ–¹æ³•ç»•è¿‡ã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>å¯¹æŠ—æ€§åç¼€è¿‡æ»¤ï¼ˆASFï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„æ–°å‹æ¨¡å‹æ— å…³é˜²å¾¡ç®¡é“ï¼Œæ—¨åœ¨ä¿æŠ¤LLMå…å—å¯¹æŠ—æ€§åç¼€æ”»å‡»ã€‚ASFå……å½“è¾“å…¥é¢„å¤„ç†å™¨å’Œå‡€åŒ–å™¨ï¼Œå¯æ£€æµ‹å’Œè¿‡æ»¤æç¤ºä¸­çš„å¯¹æŠ—æ€§åç¼€ï¼Œæœ‰æ•ˆä¸­å’Œæ¶æ„æ³¨å…¥ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒASFåœ¨é»‘ç™½ç›’æ”»å‡»ç¯å¢ƒä¸­æä¾›äº†å…¨é¢çš„é˜²å¾¡èƒ½åŠ›ï¼Œå°†æœ€å…ˆè¿›çš„å¯¹æŠ—æ€§åç¼€ç”Ÿæˆæ–¹æ³•çš„æ”»å‡»æ•ˆç‡é™ä½è‡³ä½äº4%ï¼ŒåŒæ—¶ä»…å¯¹ç›®æ ‡æ¨¡å‹åœ¨éå¯¹æŠ—æ€§åœºæ™¯ä¸­çš„èƒ½åŠ›äº§ç”Ÿå¾®å°å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09602v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªä¸»ç³»ç»Ÿå’Œé¢å‘å…¬ä¼—çš„ç¯å¢ƒä¸­åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°æ¼æ´æ”»å‡»ï¼Œå¯èƒ½å½±å“å…¶å®‰å…¨æ€§å’Œå¯ä¿¡åº¦ã€‚å¯¹æŠ—æ€§åç¼€è¢«è®¤ä¸ºæ˜¯å½“å‰æœ€å…ˆè¿›çš„æ”»å‡»æ–¹å¼ï¼Œç°æœ‰çš„é˜²å¾¡æ‰‹æ®µå­˜åœ¨ä¾èµ–æ¨¡å‹å†…éƒ¨æ¶æ„ã€å¢åŠ å†…å­˜å’Œè®¡ç®—æˆæœ¬æˆ–å¯é€šè¿‡ç®€å•æç¤ºå·¥ç¨‹æ–¹æ³•ç»•è¿‡ç­‰é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ¨¡å‹æ— å…³é˜²å¾¡ç®¡é“â€”â€”å¯¹æŠ—æ€§åç¼€è¿‡æ»¤å™¨ï¼ˆASFï¼‰ï¼Œæ—¨åœ¨ä¿æŠ¤LLMså…å—å¯¹æŠ—æ€§åç¼€æ”»å‡»ã€‚ASFä½œä¸ºè¾“å…¥é¢„å¤„ç†å™¨å’Œå‡€åŒ–å™¨ï¼Œèƒ½å¤Ÿæ£€æµ‹å’Œè¿‡æ»¤æç¤ºä¸­çš„å¯¹æŠ—æ€§åç¼€ï¼Œæœ‰æ•ˆä¸­å’Œæ¶æ„æ³¨å…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒASFåœ¨é»‘ç™½ç›’æ”»å‡»åœºæ™¯ä¸‹æä¾›å…¨é¢çš„é˜²å¾¡èƒ½åŠ›ï¼Œå°†æœ€å…ˆè¿›å¯¹æŠ—æ€§åç¼€ç”Ÿæˆæ–¹æ³•çš„æ”»å‡»æ•ˆæœé™ä½è‡³4%ä»¥ä¸‹ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“æ¨¡å‹åœ¨éå¯¹æŠ—åœºæ™¯ä¸‹çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è‡ªä¸»ç³»ç»Ÿå’Œå…¬å…±ç¯å¢ƒä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨å®‰å…¨æ€§å’Œä¿¡ä»»åº¦é—®é¢˜ã€‚</li>
<li>å¯¹æŠ—æ€§åç¼€æ˜¯å½“å‰æœ€å…ˆè¿›çš„æ”»å‡»æ–¹å¼ï¼Œèƒ½å¤Ÿç»•è¿‡ç°æœ‰é˜²å¾¡æ‰‹æ®µã€‚</li>
<li>å¯¹æŠ—æ€§åç¼€è¿‡æ»¤å™¨ï¼ˆASFï¼‰æ˜¯ä¸€ç§æ–°å‹çš„æ¨¡å‹æ— å…³é˜²å¾¡ç®¡é“ï¼Œæ—¨åœ¨ä¿æŠ¤LLMså…å—å¯¹æŠ—æ€§åç¼€æ”»å‡»ã€‚</li>
<li>ASFä½œä¸ºè¾“å…¥é¢„å¤„ç†å™¨å’Œå‡€åŒ–å™¨ï¼Œèƒ½æœ‰æ•ˆæ£€æµ‹å’Œè¿‡æ»¤æç¤ºä¸­çš„å¯¹æŠ—æ€§åç¼€ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒASFåœ¨é»‘ç™½ç›’æ”»å‡»åœºæ™¯ä¸‹æä¾›å…¨é¢çš„é˜²å¾¡èƒ½åŠ›ï¼Œå¤§å¹…é™ä½æ”»å‡»æ•ˆæœã€‚</li>
<li>ASFå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“æå°ï¼Œå‡ ä¹ä¸å½±å“å…¶åœ¨éå¯¹æŠ—åœºæ™¯ä¸‹çš„èƒ½åŠ›ã€‚<br>7.ASFçš„å¼•å…¥ä¸ºLLMsçš„å®‰å…¨æ€§å’Œç¨³å¥æ€§æä¾›äº†æ–°çš„é˜²æŠ¤æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-abdde6c41d84ee23f3f2fd6c4ac901b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cd2a542ddd5ceaf097d3a4f9674a38a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="How-Hungry-is-AI-Benchmarking-Energy-Water-and-Carbon-Footprint-of-LLM-Inference"><a href="#How-Hungry-is-AI-Benchmarking-Energy-Water-and-Carbon-Footprint-of-LLM-Inference" class="headerlink" title="How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of   LLM Inference"></a>How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of   LLM Inference</h2><p><strong>Authors:Nidhal Jegham, Marwen Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi</strong></p>
<p>As large language models (LLMs) spread across industries, understanding their environmental footprint at the inference level is no longer optional; it is essential. However, most existing studies exclude proprietary models, overlook infrastructural variability and overhead, or focus solely on training, even as inference increasingly dominates AIâ€™s environmental impact. To bridge this gap, this paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models as deployed in commercial data centers. Our framework combines public API performance data with region-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries&#x2F;day results in substantial annual environmental impacts. These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset. These findings illustrate a growing paradox: although individual queries are efficient, their global scale drives disproportionate resource consumption. Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of LLM deployments, laying a foundation for future environmental accountability in AI development and sustainability standards. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„è¡Œä¸šçš„æ™®åŠï¼Œå¯¹å…¶æ¨ç†çº§åˆ«çš„ç¯å¢ƒè¶³è¿¹çš„äº†è§£å·²ç»ä¸å†å¯æœ‰å¯æ— ï¼Œè€Œæ˜¯å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶éƒ½æ’é™¤äº†ä¸“æœ‰æ¨¡å‹ï¼Œå¿½ç•¥äº†åŸºç¡€è®¾æ–½çš„å˜æ€§å’Œå¼€é”€ï¼Œæˆ–è€…åªå…³æ³¨è®­ç»ƒæ–¹é¢ï¼Œå³ä½¿æ¨ç†è¶Šæ¥è¶Šå¤šåœ°ä¸»å¯¼äººå·¥æ™ºèƒ½çš„ç¯å¢ƒå½±å“ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹çš„åŸºç¡€è®¾æ–½æ„ŸçŸ¥åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥é‡åŒ–éƒ¨ç½²åœ¨å•†ç”¨æ•°æ®ä¸­å¿ƒä¸­çš„30æ¬¾æœ€å‰æ²¿LLMæ¨ç†çš„ç¯å¢ƒè¶³è¿¹ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†å…¬å¼€APIæ€§èƒ½æ•°æ®ã€åœ°åŒºç‰¹å®šçš„ç¯å¢ƒä¹˜æ•°ä»¥åŠç¡¬ä»¶é…ç½®ç»Ÿè®¡æ¨æ–­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨äº¤å‰æ•ˆç‡æ•°æ®åŒ…ç»œåˆ†æï¼ˆDEAï¼‰å¯¹æ¨¡å‹æŒ‰ç…§ç›¸å¯¹äºç¯å¢ƒæˆæœ¬çš„è¡¨ç°è¿›è¡Œæ’åã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒO3å’ŒDeepSeek-R1æ˜¯æœ€èƒ½è€—çš„æ¨¡å‹ï¼Œé•¿æç¤ºçš„èƒ½è€—è¶…è¿‡33ç“¦æ—¶ï¼Œæ˜¯GPT-4.1 nanoèƒ½è€—çš„70å€ä»¥ä¸Šï¼Œè€ŒClaude-3.7 Sonnetåœ¨ç”Ÿæ€æ•ˆç‡æ–¹é¢æ’åæœ€é«˜ã€‚è™½ç„¶å•ä¸ªçŸ­GPT-4oæŸ¥è¯¢æ¶ˆè€—0.43ç“¦æ—¶ï¼Œä½†æ¯å¤©æ‰©å±•åˆ°7äº¿æ¬¡æŸ¥è¯¢ä¼šäº§ç”Ÿå·¨å¤§çš„å¹´åº¦ç¯å¢ƒå½±å“ã€‚å…¶ä¸­åŒ…æ‹¬ä¸3.5ä¸‡ä¸ªç¾å›½å®¶åº­ç›¸å½“çš„ç”µåŠ›ä½¿ç”¨ã€æ·¡æ°´è’¸å‘é‡ç›¸å½“äºæ»¡è¶³120ä¸‡äººçš„å¹´åº¦é¥®æ°´éœ€æ±‚ä»¥åŠç¢³æ’æ”¾é‡éœ€è¦èŠåŠ å“¥è§„æ¨¡çš„æ£®æ—æ¥ä¸­å’Œã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ä¸€ä¸ªæ—¥ç›Šå¢é•¿çš„æ‚–è®ºï¼šè™½ç„¶å•ä¸ªæŸ¥è¯¢æ•ˆç‡å¾ˆé«˜ï¼Œä½†å…¶åœ¨å…¨çƒèŒƒå›´å†…çš„æ¨å¹¿å¯¼è‡´äº†èµ„æºæ¶ˆè€—çš„æåº¦ä¸å¹³è¡¡ã€‚æœ¬ç ”ç©¶æä¾›äº†ä¸€ç§æ ‡å‡†åŒ–ã€å®è¯çš„åŸºå‡†æµ‹è¯•æ–¹æ³•æ¥è¯„ä¼°LLMéƒ¨ç½²çš„å¯æŒç»­æ€§ï¼Œä¸ºæœªæ¥äººå·¥æ™ºèƒ½å‘å±•å’Œå¯æŒç»­æ€§æ ‡å‡†ä¸­çš„ç¯å¢ƒé—®è´£æä¾›äº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09598v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†é˜¶æ®µç¯å¢ƒè¶³è¿¹è¯„ä¼°è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰ç ”ç©¶å­˜åœ¨è¯¸å¤šä¸è¶³ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºç¡€è®¾æ–½æ„ŸçŸ¥åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ–å•†ä¸šæ•°æ®ä¸­å¿ƒéƒ¨ç½²çš„30æ¬¾å…ˆè¿›LLMæ¨¡å‹æ¨ç†é˜¶æ®µçš„ç¯ä¿è¶³è¿¹ã€‚ç ”ç©¶ç»“åˆå…¬å¼€APIæ€§èƒ½æ•°æ®ã€åœ°åŒºç‰¹å®šç¯å¢ƒä¹˜æ•°ä¸ç¡¬ä»¶é…ç½®ç»Ÿè®¡æ¨æ–­ï¼Œå¹¶åˆ©ç”¨äº¤å‰æ•ˆç‡æ•°æ®åŒ…ç»œåˆ†æï¼ˆDEAï¼‰å¯¹æ¨¡å‹è¿›è¡Œæ€§èƒ½ä¸ç¯ä¿æˆæœ¬çš„æ’åã€‚ç»“æœæ˜¾ç¤ºæŸäº›æ¨¡å‹å¦‚o3å’ŒDeepSeek-R1èƒ½è€—æé«˜ï¼Œè€ŒClaude-3.7 Sonnetåœ¨ç”Ÿæ€æ•ˆç‡æ–¹é¢è¡¨ç°æœ€ä½³ã€‚ä¸ªäººæŸ¥è¯¢è™½ç„¶èƒ½æ•ˆè¾ƒé«˜ï¼Œä½†åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸‹ä¼šäº§ç”Ÿæ˜¾è‘—çš„ç¯å¢ƒå½±å“ï¼ŒåŒ…æ‹¬ç”µåŠ›æ¶ˆè€—ã€æ·¡æ°´è’¸å‘å’Œç¢³æ’æ”¾ç­‰é—®é¢˜ã€‚æœ¬ç ”ç©¶ä¸ºè¯„ä¼°LLMéƒ¨ç½²çš„å¯æŒç»­æ€§æä¾›äº†æ ‡å‡†åŒ–ã€å®è¯çš„æ–¹æ³•è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†é˜¶æ®µç¯å¢ƒè¶³è¿¹è¯„ä¼°å…·æœ‰é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨LLMç¯å¢ƒè¶³è¿¹è¯„ä¼°æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¦‚å¿½ç•¥ä¸“æœ‰æ¨¡å‹ã€åŸºç¡€è®¾æ–½å¯å˜æ€§å’Œå¼€é”€ç­‰ã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºç¡€è®¾æ–½æ„ŸçŸ¥åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºé‡åŒ–å•†ä¸šæ•°æ®ä¸­å¿ƒéƒ¨ç½²çš„LLMæ¨¡å‹æ¨ç†é˜¶æ®µçš„ç¯ä¿è¶³è¿¹ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†å…¬å¼€APIæ€§èƒ½æ•°æ®ã€åœ°åŒºç‰¹å®šç¯å¢ƒä¹˜æ•°å’Œç¡¬ä»¶é…ç½®ç»Ÿè®¡æ¨æ–­ã€‚</li>
<li>é€šè¿‡äº¤å‰æ•ˆç‡æ•°æ®åŒ…ç»œåˆ†æï¼ˆDEAï¼‰ï¼Œå‘ç°æŸäº›æ¨¡å‹å¦‚o3å’ŒDeepSeek-R1èƒ½è€—æé«˜ï¼Œè€ŒClaude-3.7 Sonnetåœ¨ç”Ÿæ€æ•ˆç‡æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>ä¸ªäººæŸ¥è¯¢è™½ç„¶èƒ½æ•ˆè¾ƒé«˜ï¼Œä½†åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸‹ä¼šäº§ç”Ÿæ˜¾è‘—çš„ç¯å¢ƒå½±å“ï¼ŒåŒ…æ‹¬ç”µåŠ›æ¶ˆè€—ã€æ·¡æ°´è’¸å‘å’Œç¢³æ’æ”¾ç­‰é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f4f82f61d8dd033c9f47798735e8303.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d31dc7f1b994f8e67c97d9eb6be9eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c518d97911ab20f7122fa7f9827bb0fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92093279f26667e6ab129c3f91605247.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d1b1a14097e89b0fa6438dee7c4e18c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="WorldView-Bench-A-Benchmark-for-Evaluating-Global-Cultural-Perspectives-in-Large-Language-Models"><a href="#WorldView-Bench-A-Benchmark-for-Evaluating-Global-Cultural-Perspectives-in-Large-Language-Models" class="headerlink" title="WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives   in Large Language Models"></a>WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives   in Large Language Models</h2><p><strong>Authors:Abdullah Mushtaq, Imran Taj, Rafay Naeem, Ibrahim Ghaznavi, Junaid Qadir</strong></p>
<p>Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦æ¥å—è®­ç»ƒå¹¶é€šè¿‡å¯¹é½æ–¹å¼ï¼ŒåŠ å¼ºäº†è¥¿æ–¹ä¸­å¿ƒä¸»ä¹‰çš„è®¤çŸ¥è®ºå’Œç¤¾ä¼šæ–‡åŒ–è§„èŒƒï¼Œå¯¼è‡´æ–‡åŒ–åŒè´¨åŒ–ï¼Œå¹¶é™åˆ¶äº†å®ƒä»¬åæ˜ å…¨çƒæ–‡æ˜å¤šæ ·æ€§çš„èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•æ¡†æ¶æœªèƒ½å……åˆ†æ•æ‰è¿™ç§åè§ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºåƒµåŒ–ã€å°é—­å½¢å¼çš„è¯„ä¼°ï¼Œå¿½è§†äº†æ–‡åŒ–åŒ…å®¹æ€§å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†WorldView-BenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…¨çƒæ–‡åŒ–åŒ…å®¹æ€§ï¼ˆGCIï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºSenturkç­‰äººæå‡ºçš„å¤šå…ƒä¸–ç•Œè§‚ï¼ŒåŒºåˆ†äº†å¼ºåŒ–æ–‡åŒ–åŒè´¨åŒ–çš„å•ä¸€æ¨¡å‹ï¼ˆUniplexæ¨¡å‹ï¼‰å’Œæ•´åˆå¤šå…ƒè§†è§’çš„å¤šå…ƒæ¨¡å‹ï¼ˆMultiplexæ¨¡å‹ï¼‰ã€‚WorldView-Benché€šè¿‡è‡ªç”±å½¢å¼çš„ç”Ÿæˆè¯„ä¼°æ¥è¡¡é‡æ–‡åŒ–æåŒ–ä»¥åŠæ’æ–¥æ›¿ä»£è§‚ç‚¹çš„æƒ…å†µï¼Œè€Œä¸æ˜¯é€šè¿‡ä¼ ç»Ÿçš„åˆ†ç±»åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ç§å¹²é¢„ç­–ç•¥æ¥å®ç°åº”ç”¨å¤šå…ƒåŒ–ï¼šï¼ˆ1ï¼‰æƒ…å¢ƒå®æ–½å¤šå…ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¯¥ç³»ç»Ÿæç¤ºåµŒå…¥å¤šå…ƒåŒ–åŸåˆ™ï¼›ï¼ˆ2ï¼‰å¤šä¸»ä½“ç³»ç»Ÿï¼ˆMASï¼‰å®æ–½å¤šå…ƒåŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­ä»£è¡¨ä¸åŒæ–‡åŒ–è§‚ç‚¹çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸»ä½“ååŒç”Ÿæˆå“åº”ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†å¤šä¸»ä½“ç³»ç»Ÿå®æ–½çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§‚ç‚¹åˆ†å¸ƒå¾—åˆ†ï¼ˆPDSï¼‰ç†µæ–¹é¢ä»åŸºçº¿æ°´å¹³çš„13%æ˜¾è‘—å¢åŠ è‡³94%ï¼ŒåŒæ—¶è½¬å‘ç§¯ææƒ…ç»ªï¼ˆå 67.7%ï¼‰å’Œæ–‡åŒ–å¹³è¡¡å¢å¼ºã€‚è¿™äº›å‘ç°çªæ˜¾äº†å¤šå…ƒæ„è¯†äººå·¥æ™ºèƒ½è¯„ä¼°åœ¨ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–åè§æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæ›´åŒ…å®¹å’Œä¼¦ç†å¯¹é½çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09595v1">PDF</a> Preprint. Submitted to the Journal of Artificial Intelligence   Research (JAIR) on April 29, 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒä¸å¯¹é½æ–¹å¼ä¸»è¦å¼ºåŒ–äº†è¥¿æ–¹ä¸­å¿ƒçš„çŸ¥è¯†è®ºå’Œç¤¾ä¼šæ–‡åŒ–è§„èŒƒï¼Œå¯¼è‡´æ–‡åŒ–åŒè´¨åŒ–ï¼Œå¹¶é™åˆ¶äº†å…¶åæ˜ å…¨çƒæ–‡æ˜å¤šæ ·æ€§çš„èƒ½åŠ›ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶æœªèƒ½å……åˆ†æ•æ‰è¿™ç§åè§ï¼Œå®ƒä»¬ä¾èµ–äºåƒµåŒ–ã€å°é—­å½¢å¼çš„è¯„ä¼°ï¼Œå¿½è§†äº†æ–‡åŒ–åŒ…å®¹æ€§çš„å¤æ‚æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WorldView-Benchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°LLMå…¨çƒæ–‡åŒ–åŒ…å®¹æ€§ï¼ˆGCIï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡åˆ†æå…¶å®¹çº³ä¸åŒä¸–ç•Œè§‚çš„èƒ½åŠ›æ¥å®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºSenturkç­‰äººæå‡ºçš„å¤šå…ƒä¸–ç•Œè§‚ï¼ŒåŒºåˆ†äº†å¼ºåŒ–æ–‡åŒ–åŒè´¨åŒ–çš„å•ä¸€æ¨¡å‹ï¼ˆUniplexï¼‰å’Œæ•´åˆå¤šå…ƒè§†è§’çš„å¤šå…ƒæ¨¡å‹ï¼ˆMultiplexï¼‰ã€‚WorldView-Benché€šè¿‡è‡ªç”±å½¢å¼ç”Ÿæˆè¯„ä¼°ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„åˆ†ç±»åŸºå‡†æµ‹è¯•ï¼Œæ¥è¡¡é‡æ–‡åŒ–ä¸¤æåŒ–ä»¥åŠæ’æ–¥æ›¿ä»£è§‚ç‚¹çš„æƒ…å†µã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ç§å¹²é¢„ç­–ç•¥å®æ–½åº”ç”¨å¤šå…ƒæ€§ï¼šï¼ˆ1ï¼‰æƒ…å¢ƒå®æ–½çš„å¤šå…ƒLLMï¼Œå…¶ä¸­ç³»ç»Ÿæç¤ºåµŒå…¥å¤šå…ƒæ€§åŸåˆ™ï¼›ï¼ˆ2ï¼‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å®æ–½çš„å¤šå…ƒLLMï¼Œå…¶ä¸­ä»£è¡¨ä¸åŒæ–‡åŒ–è§†è§’çš„å¤šä¸ªLLMæ™ºèƒ½ä½“ååŒç”Ÿæˆå“åº”ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå®æ–½çš„å¤šå…ƒLLMçš„é€è§†åˆ†å¸ƒå¾—åˆ†ï¼ˆPDSï¼‰ç†µä»13%æ˜¾ç€å¢åŠ åˆ°94%ï¼ŒåŒæ—¶å‘ç§¯ææƒ…ç»ªï¼ˆ67.7%ï¼‰è½¬å˜ï¼Œå¹¶å¢å¼ºäº†æ–‡åŒ–å¹³è¡¡æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å¤šå…ƒæ„è¯†äººå·¥æ™ºèƒ½è¯„ä¼°åœ¨ç¼“è§£LLMä¸­çš„æ–‡åŒ–åè§æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæ„å»ºæ›´å…·åŒ…å®¹æ€§å’Œé“å¾·å¯¹é½çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»è¦åæ˜ å’Œå¼ºåŒ–è¥¿æ–¹ä¸­å¿ƒçš„çŸ¥è¯†è®ºå’Œç¤¾ä¼šæ–‡åŒ–è§„èŒƒï¼Œå¯¼è‡´æ–‡åŒ–åŒè´¨åŒ–ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ¡†æ¶æ— æ³•å……åˆ†æ•æ‰LLMä¸­çš„æ–‡åŒ–åè§ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦ä¾èµ–å°é—­å’ŒåƒµåŒ–çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>WorldView-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„å…¨çƒæ–‡åŒ–åŒ…å®¹æ€§ï¼ˆGCIï¼‰ï¼Œé€šè¿‡è¡¡é‡å…¶å®¹çº³ä¸åŒä¸–ç•Œè§‚çš„èƒ½åŠ›æ¥å®ç°ã€‚</li>
<li>å¼•å…¥å¤šå…ƒä¸–ç•Œè§‚æ¦‚å¿µï¼ŒåŒºåˆ†å•ä¸€æ¨¡å‹ï¼ˆUniplexï¼‰å’Œå¤šå…ƒæ¨¡å‹ï¼ˆMultiplexï¼‰ã€‚</li>
<li>WorldView-Benché‡‡ç”¨è‡ªç”±å½¢å¼ç”Ÿæˆè¯„ä¼°æ¥æ•æ‰LLMä¸­çš„æ–‡åŒ–æåŒ–ç°è±¡å’Œæ›¿ä»£è§‚ç‚¹æ’æ–¥æƒ…å†µã€‚</li>
<li>é€šè¿‡æƒ…å¢ƒå®æ–½å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å®æ–½ä¸¤ç§ç­–ç•¥æ¥åº”ç”¨å¤šå…ƒæ€§ï¼Œæ˜¾è‘—æé«˜äº†é€è§†åˆ†å¸ƒå¾—åˆ†ï¼ˆPDSï¼‰ç†µï¼ŒåŒæ—¶ä¿ƒè¿›äº†ç§¯ææƒ…ç»ªå’Œæ–‡åŒ–å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-015340ca5ac1dfab355a7f35717b5378.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49731c932989794fe6dbb55db791a66f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MIGRATION-BENCH-Repository-Level-Code-Migration-Benchmark-from-Java-8"><a href="#MIGRATION-BENCH-Repository-Level-Code-Migration-Benchmark-from-Java-8" class="headerlink" title="MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8"></a>MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8</h2><p><strong>Authors:Linbo Liu, Xinle Liu, Qiang Zhou, Lin Chen, Yihan Liu, Hoan Nguyen, Behrooz Omidvar-Tehrani, Xi Shen, Jun Huan, Omer Tripp, Anoop Deoras</strong></p>
<p>With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on problem-solving and issue-resolution tasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a distinct focus: code migration. MIGRATION-BENCH aims to serve as a comprehensive benchmark for migration from Java 8 to the latest long-term support (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset and its subset selected with $5,102$ and $300$ repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java 17. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: <a target="_blank" rel="noopener" href="https://huggingface.co/collections/AmazonScience">https://huggingface.co/collections/AmazonScience</a> and <a target="_blank" rel="noopener" href="https://github.com/amazon-science/self_debug">https://github.com/amazon-science/self_debug</a> respectively. </p>
<blockquote>
<p>éšç€è¿‘å¹´æ¥åŠŸèƒ½å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç°åœ¨å¯ä»¥ä½¿ç”¨LLMè§£å†³å¹¿æ³›çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œä»è€Œæå¤§åœ°æé«˜äº†ç”Ÿäº§åŠ›å’Œå¯æ‰©å±•æ€§ã€‚å·²ç»å¼€å‘äº†è®¸å¤šåŸºå‡†æ•°æ®é›†æ¥è¯„ä¼°è¿™äº›æ¨¡å‹çš„ç¼–ç èƒ½åŠ›ï¼Œå®ƒä»¬ä¸»è¦ä¾§é‡äºé—®é¢˜è§£å†³å’Œé—®é¢˜è§£å†³ä»»åŠ¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç¼–ç åŸºå‡†MIGRATION-BENCHï¼Œå®ƒå…·æœ‰ç‹¬ç‰¹çš„é‡ç‚¹ï¼šä»£ç è¿ç§»ã€‚MIGRATION-BENCHæ—¨åœ¨æˆä¸ºä»Java 8è¿ç§»åˆ°æœ€æ–°é•¿æœŸæ”¯æŒï¼ˆLTSï¼‰ç‰ˆæœ¬ï¼ˆJava 17ã€21ï¼‰çš„å…¨é¢åŸºå‡†ã€‚MIGRATION-BENCHåŒ…æ‹¬å®Œæ•´æ•°æ®é›†å’Œåˆ†åˆ«ä½¿ç”¨$5,102$å’Œ$300$ä¸ªå­˜å‚¨åº“é€‰æ‹©çš„å­é›†ã€‚æ‰€é€‰å­é›†æ˜¯ç»è¿‡å¤æ‚æ€§å’Œéš¾åº¦æŒ‘é€‰å‡ºæ¥çš„ï¼Œä¸ºä»£ç è¿ç§»é¢†åŸŸçš„ç ”ç©¶æä¾›äº†å¤šåŠŸèƒ½èµ„æºæ”¯æŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›å¯¹è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡çš„LLMè¿›è¡Œä¸¥æ ¼å’Œæ ‡å‡†åŒ–çš„è¯„ä¼°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºSD-Feedbackï¼Œå¹¶è¯æ˜LLMå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å­˜å‚¨åº“çº§åˆ«çš„ä»£ç è¿ç§»åˆ°Java 17ã€‚å¯¹äºä½¿ç”¨Claude-3.5-Sonnet-v2é€‰æ‹©çš„å­é›†ï¼ŒSD-Feedbackåœ¨æœ€å°è¿ç§»å’Œæœ€å¤§è¿ç§»æ–¹é¢çš„æˆåŠŸç‡åˆ†åˆ«ä¸º62.33%å’Œ27.00%ï¼ˆpass@1ï¼‰ã€‚åŸºå‡†æ•°æ®é›†å’Œæºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/collections/AmazonScience">https://huggingface.co/collections/AmazonScience</a> å’Œ <a target="_blank" rel="noopener" href="https://github.com/amazon-science/self_debug">https://github.com/amazon-science/self_debug</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09569v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€è¿‘å¹´æ¥å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä¸€ç³»åˆ—è½¯ä»¶å·¥ç¨‹ä»»åŠ¡éƒ½å¯ä»¥åˆ©ç”¨LLMæ¥è§£å†³ï¼Œå¤§å¤§æé«˜äº†ç”Ÿäº§åŠ›å’Œå¯æ‰©å±•æ€§ã€‚ä¸»è¦èšç„¦äºé—®é¢˜è§£å†³å’Œé—®é¢˜è§£å†³ä»»åŠ¡çš„è¯„ä¼°ç¼–ç èƒ½åŠ›çš„åŸºå‡†æ•°æ®é›†å·²ç»å¼€å‘å‡ºæ¥ï¼Œä¸æ­¤ç›¸åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªæ–°çš„ç¼–ç åŸºå‡†MIGRATION-BENCHï¼Œå…¶ç‹¬ç‰¹ä¹‹å¤„åœ¨äºä»£ç è¿ç§»ã€‚MIGRATION-BENCHæ—¨åœ¨æˆä¸ºä»Java 8è¿ç§»åˆ°æœ€æ–°çš„é•¿æœŸæ”¯æŒï¼ˆLTSï¼‰ç‰ˆæœ¬ï¼ˆJava 17ã€21ï¼‰çš„è¿ç§»çš„ç»¼åˆåŸºå‡†ã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†å’Œåˆ†åˆ«é€‰æ‹©åŒ…å«5,102å’Œ300ä¸ªå­˜å‚¨åº“çš„ä¸¤ä¸ªå­é›†æ¥å±•ç¤ºä»£ç çš„å¤æ‚æ€§å’Œéš¾åº¦ï¼Œä»è€Œä½œä¸ºè¯¥é¢†åŸŸç ”ç©¶çš„èµ„æºå®åº“ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›å¯¹è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡çš„LLMè¿›è¡Œä¸¥è°¨å’Œæ ‡å‡†åŒ–çš„è¯„ä¼°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºSD-Feedbackå¹¶è¯æ˜LLMå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å­˜å‚¨åº“çº§åˆ«çš„ä»£ç è¿ç§»åˆ°Java 17ã€‚å¯¹äºé€‰å®šçš„å­é›†ä½¿ç”¨Claude-3.5-Sonnet-vç‰ˆåé¦ˆå®ç°62.33ï¼…å’Œæœ€å¤§è¿ç§»åé¦ˆæˆåŠŸç‡ä¸ºç™¾åˆ†ä¹‹äºŒåä¸ƒç‚¹é›¶é›¶ä¸€ã€‚åŸºå‡†æ•°æ®é›†å’Œæºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/collections/AmazonScience">huggingface.co&#x2F;collections&#x2F;AmazonScience</a>å’ŒGitHubçš„äºšé©¬é€Šç§‘å­¦&#x2F;è‡ªæˆ‘è°ƒè¯•é¡µé¢æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºè½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„è§£å†³æä¾›äº†å¼ºå¤§çš„å·¥å…·ï¼Œå¢å¼ºäº†ç”Ÿäº§åŠ›å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>MIGRATION-BENCHä½œä¸ºä¸€ç§æ–°çš„ç¼–ç åŸºå‡†ï¼Œä¸“æ³¨äºä»£ç è¿ç§»ï¼Œæ—¨åœ¨æ”¯æŒä»Java 8è¿ç§»åˆ°æœ€æ–°LTSç‰ˆæœ¬çš„ç ”ç©¶ã€‚</li>
<li>MIGRATION-BENCHåŒ…æ‹¬ä¸€ä¸ªå®Œæ•´çš„æ•°æ®é›†å’Œå…¶å­é›†ï¼Œå±•ç¤ºäº†ä»£ç çš„å¤æ‚æ€§å’Œéš¾åº¦ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œå¯¹LLMåœ¨ä»£ç è¿ç§»ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¿›è¡Œä¸¥è°¨å’Œæ ‡å‡†åŒ–çš„è¯„ä¼°ã€‚</li>
<li>æå‡ºSD-Feedbackæ–¹æ³•å¹¶è¯æ˜å…¶åœ¨å¤„ç†å­˜å‚¨åº“çº§åˆ«çš„ä»£ç è¿ç§»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¯¹äºé€‰å®šçš„å­é›†ï¼Œä½¿ç”¨Claude-3.5-Sonnet-vç‰ˆåé¦ˆå®ç°äº†è¾ƒé«˜çš„æˆåŠŸè¿ç§»ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d1495662f6b443d61087e3409268954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-095b09fef8e8a77d078f1c5c48b07cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ba8b96dfd42942d444417bf6fc65e7d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="BLIP3-o-A-Family-of-Fully-Open-Unified-Multimodal-Models-Architecture-Training-and-Dataset"><a href="#BLIP3-o-A-Family-of-Fully-Open-Unified-Multimodal-Models-Architecture-Training-and-Dataset" class="headerlink" title="BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,   Training and Dataset"></a>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,   Training and Dataset</h2><p><strong>Authors:Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, Ran Xu</strong></p>
<p>Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€æ¨¡å‹é¢†åŸŸçš„æœ€æ–°ç ”ç©¶ä¸­ï¼Œç»Ÿä¸€å›¾åƒç†è§£å’Œç”ŸæˆæŠ€æœ¯è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚å°½ç®¡å›¾åƒç†è§£çš„è®¾è®¡é€‰æ‹©å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­åŒæ—¶åŒ…å«å›¾åƒç”Ÿæˆçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚å—è‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨ç”Ÿæˆå’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å¼ºå¤§æ½œåŠ›çš„é©±åŠ¨ï¼Œæˆ‘ä»¬å¯¹å®ƒä»¬åœ¨ç»Ÿä¸€å¤šæ¨¡æ€è®¾ç½®ä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨å›¾åƒè¡¨ç¤ºã€å»ºæ¨¡ç›®æ ‡å’Œè®­ç»ƒç­–ç•¥ã€‚åŸºäºè¿™äº›ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿ç”¨æ‰©æ•£å˜å‹å™¨ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„CLIPå›¾åƒç‰¹å¾ï¼Œä¸ä¼ ç»Ÿçš„åŸºäºVAEçš„è¡¨ç¤ºå½¢æˆå¯¹æ¯”ã€‚è¿™ç§è®¾è®¡ä¸ä»…æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œè¿˜æ”¹å–„äº†ç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†ç»Ÿä¸€æ¨¡å‹çš„é¡ºåºé¢„è®­ç»ƒç­–ç•¥â€”â€”é¦–å…ˆåœ¨å›¾åƒç†è§£ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶åæ˜¯åœ¨å›¾åƒç”Ÿæˆä¸Šè¿›è¡Œè®­ç»ƒâ€”â€”å…·æœ‰å®ç”¨ä»·å€¼ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå›¾åƒç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œå‘å±•å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ¶µç›–å„ç§åœºæ™¯ã€ç‰©ä½“ã€äººç±»åŠ¨ä½œç­‰çš„å¤šæ ·åŒ–æ ‡é¢˜æç¤ºGPT-4oï¼Œç²¾å¿ƒåˆ›å»ºäº†ä¸€ä¸ªé«˜è´¨é‡ç”¨äºå›¾åƒç”Ÿæˆçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†BLIP3o-60kã€‚åŸºäºæˆ‘ä»¬åˆ›æ–°çš„æ¨¡å‹è®¾è®¡ã€è®­ç»ƒæ–¹æ¡ˆå’Œæ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹BLIP3-oã€‚BLIP3-oåœ¨å¤§å¤šæ•°æµè¡Œçš„æ¶µç›–å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å…¨é¢å¼€æºäº†æˆ‘ä»¬çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ä»£ç ã€æ¨¡å‹æƒé‡ã€è®­ç»ƒè„šæœ¬ä»¥åŠé¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09568v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†å›¾åƒç†è§£å’Œç”Ÿæˆç»“åˆåœ¨ä¸€åŒå¤„ç†çš„æ½œåŠ›ä¸æŒ‘æˆ˜ã€‚æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°é¢–æ–¹æ³•ï¼Œé€šè¿‡é‡‡ç”¨æ‰©æ•£è½¬æ¢å™¨ç”Ÿæˆå¯Œå«è¯­ä¹‰çš„CLIPå›¾åƒç‰¹å¾æ¥æé«˜è®­ç»ƒå’Œç”Ÿæˆçš„æ•ˆç‡ä¸è´¨é‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºå‡ºè‡ªåŠ¨å›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹åœ¨é«˜è´¨é‡ç”Ÿæˆä¸­çš„ä¼˜åŠ¿ï¼Œä¸ºæ­¤å¼€å±•äº†å¯¹å®ƒä»¬åœ¨ç»Ÿä¸€çš„å¤šæ¨¡æ€åœºæ™¯ä¸­çš„åº”ç”¨çš„ç»¼åˆç ”ç©¶ã€‚ä¸ºé…åˆè¿™ç§æ–°å‹è®¾è®¡ï¼ŒåŒæ—¶è®¾è®¡äº†é¢„å…ˆè®­ç»ƒçš„æ¬¡åºç­–ç•¥å’Œç‰¹å®šçš„å›¾åƒç”Ÿæˆæ•°æ®é›†BLIP3o-60kã€‚æœ€ç»ˆï¼ŒåŸºäºè¿™äº›åˆ›æ–°è®¾è®¡ã€è®­ç»ƒæ–¹æ³•å’Œæ•°æ®é›†ï¼Œå‘å±•å‡ºäº†å…·å¤‡è¶…å‰æ€§èƒ½çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹BLIP3-oç³»åˆ—ã€‚ä¸ºäº†æ¨è¿›åç»­ç ”ç©¶ï¼Œå¼€æºäº†å…¶æ¨¡å‹ä»£ç ã€æƒé‡å’Œè®­ç»ƒè„šæœ¬ç­‰å…³é”®èµ„æ–™ã€‚é€šè¿‡æ¨¡å‹å’Œç›¸å…³çš„èµ„æºæ¥ä¿ƒè¿›å›¾åƒç†è§£å’Œç”Ÿæˆé¢†åŸŸçš„æœªæ¥å‘å±•ã€‚æ­¤ä¸¾æå¤§åœ°æé«˜äº†ä¸åŒåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬å®Œå…¨å¼€æºäº†æˆ‘ä»¬çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ä»£ç ã€æ¨¡å‹æƒé‡ã€è®­ç»ƒè„šæœ¬ä»¥åŠé¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚è¿™å°†ä¸ºç›¸å…³é¢†åŸŸçš„å‘å±•æä¾›é‡è¦æ¨åŠ¨åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ–‡ç« æ˜¯æ¢ç´¢å¤šæ¨¡æ€æ¨¡å‹ç»Ÿä¸€æ¡†æ¶çš„å¼€åˆ›æ€§ç ”ç©¶ï¼Œæ ‡å¿—ç€å‘è¿™ä¸€é¢†åŸŸçš„å·¨å¤§è¿›å±•è¿ˆå‡ºäº†ä¸€æ­¥ã€‚ä¸ä»…å¼ºè°ƒæ€§èƒ½çš„æé«˜å’Œåˆ›æ–°çš„è®¾è®¡æ–¹æ¡ˆï¼Œè¿˜ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¹¿æ³›å¼€æ”¾çš„å¼€æºèµ„æºåº“æ¥æ¨åŠ¨æœªæ¥çš„ç ”ç©¶å’Œå‘å±•ã€‚è¿™å°†æœ‰åŠ©äºä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•å’Œåˆ›æ–°ã€‚æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°å›¾åƒç†è§£å’Œç”Ÿæˆçš„ç»Ÿä¸€å¤„ç†æ¡†æ¶ï¼Œå±•ç¤ºäº†å…¶å·¨å¤§çš„æ½œåŠ›å’Œä¼˜åŠ¿ã€‚é€šè¿‡å¯¹ç›¸å…³æ¨¡å‹çš„ä¼˜åŒ–å’Œæ”¹è¿›ä»¥åŠæ•°æ®é›†çš„å»ºè®¾å’Œåˆ›æ–°å®è·µç­–ç•¥çš„é‡‡ç”¨ç­‰æ–¹å¼æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ï¼Œæœ€ç»ˆå°†å®ç°æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„å›¾åƒå¤„ç†åº”ç”¨ã€‚<strong>Key Takeaways</strong>:</p>
<ul>
<li>æ–‡ç« ä¸»è¦æ¢è®¨å°†å›¾åƒç†è§£ä¸ç”Ÿæˆç»“åˆçš„ç»Ÿä¸€æ¨¡å‹è®¾è®¡åŠå…¶åº”ç”¨æŒ‘æˆ˜ä¸è¿›å±•ã€‚æŒ‡å‡ºä¼ ç»Ÿè®¾è®¡ä¸­è™½ç„¶å¯¹äºå›¾åƒç†è§£éƒ¨åˆ†æœ‰æ·±å…¥ç ”ç©¶ï¼Œä½†ç»Ÿä¸€çš„æ¡†æ¶ç»“åˆå›¾åƒç”Ÿæˆçš„æ¨¡å‹å’Œè®­ç»ƒç­–ç•¥ä»ç„¶è¢«å¿½è§†çš„é—®é¢˜ã€‚å¼•å…¥äº†ä¸€ç§æ–°é¢–æ–¹æ³•ä½œä¸ºçªç ´å£æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ä¸è´¨é‡ï¼Œå¹¶è¿›è¡Œå¯¹æ¯”åˆ†æï¼›åˆ†æäº†ä¼ ç»Ÿçš„æ¨¡å‹å’Œç”ŸæˆæŠ€æœ¯åœ¨è¿™ä¸ªé¢†åŸŸä¸­å­˜åœ¨çš„é—®é¢˜ä¸ä¸è¶³ä¹‹å¤„ç­‰çŸ­æ¿ç°è±¡å¹¶è¿›è¡Œæ€»ç»“å’Œå±•æœ›è§£å†³åŠæ³•ä»¥åŠå¯¹ä¸åŒè§£å†³ç­–ç•¥çš„æ¢è®¨æå‡ºäº†ä¸€ä¸ªæ–°çš„è§£å†³ç­–ç•¥è¿›è¡Œäº†åˆ›æ–°å°è¯•å’Œåˆ†æåŒæ—¶æ„å»ºäº†æ•°æ®é›†ä»¥ä¾¿éªŒè¯å’Œæå‡ç®—æ³•æ•ˆæœä¸ºè§£å†³è¿™äº›çŸ­æ¿æä¾›ä¸€ç§æ–°çš„è§£å†³æ€è·¯å’Œå®éªŒæ•°æ®æ”¯æŒæ¥è§£å†³è¿™ä¸ªé—®é¢˜å¹¶è¿›ä¸€æ­¥æ¢è®¨è¿™ä¸ªé¢†åŸŸçš„æœªæ¥å‘å±•å‰æ™¯è¿›è€Œå±•å¼€æ·±å…¥çš„æ¢ç´¢ç ”ç©¶ä»‹ç»äº†å…¶åœ¨æ”¹è¿›å›¾åƒå¤„ç†é¢†åŸŸçš„æ°å‡ºè´¡çŒ®è¿›è¡Œäº†è¯„ä»·</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33aa03fbae588858ae133c1cf4caa04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1c59a7596173a94015358ec00fb8b6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37d252eca6eae72c28ee6460d613aa7e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PT-MoE-An-Efficient-Finetuning-Framework-for-Integrating-Mixture-of-Experts-into-Prompt-Tuning"><a href="#PT-MoE-An-Efficient-Finetuning-Framework-for-Integrating-Mixture-of-Experts-into-Prompt-Tuning" class="headerlink" title="PT-MoE: An Efficient Finetuning Framework for Integrating   Mixture-of-Experts into Prompt Tuning"></a>PT-MoE: An Efficient Finetuning Framework for Integrating   Mixture-of-Experts into Prompt Tuning</h2><p><strong>Authors:Zongqian Li, Yixuan Su, Nigel Collier</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models, yet existing approaches exhibit counter-intuitive phenomena: integrating router into prompt tuning (PT) increases training efficiency yet does not improve performance universally; parameter reduction through matrix decomposition can improve performance in specific domains. Motivated by these observations and the modular nature of PT, we propose PT-MoE, a novel framework that integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets demonstrate that PT-MoE achieves state-of-the-art performance in both question answering (QA) and mathematical problem solving tasks, improving F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA. Our analysis reveals that while PT methods generally excel in QA tasks and LoRA-based methods in math datasets, the integration of matrix decomposition and MoE in PT-MoE yields complementary benefits: decomposition enables efficient parameter sharing across experts while MoE provides dynamic adaptation, collectively enabling PT-MoE to demonstrate cross-task consistency and generalization abilities. These findings, along with ablation studies on routing mechanisms and architectural components, provide insights for future PEFT methods. </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é€‚åº”æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å±•ç°å‡ºäº†ä¸€äº›åç›´è§‰ç°è±¡ï¼šå°†è·¯ç”±å™¨é›†æˆåˆ°æç¤ºè°ƒæ•´ï¼ˆPTï¼‰ä¸­ä¼šæé«˜è®­ç»ƒæ•ˆç‡ï¼Œä½†å¹¶ä¸æ™®éåœ°æ”¹å–„æ€§èƒ½ï¼›é€šè¿‡çŸ©é˜µåˆ†è§£è¿›è¡Œå‚æ•°ç¼©å‡å¯ä»¥åœ¨ç‰¹å®šé¢†åŸŸæé«˜æ€§èƒ½ã€‚å—è¿™äº›è§‚å¯Ÿç»“æœå’ŒPTçš„æ¨¡å—åŒ–æ€§è´¨çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†PT-MoEï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆäº†çŸ©é˜µåˆ†è§£ä¸ä¸“å®¶æ··åˆï¼ˆMoEï¼‰è·¯ç”±çš„é«˜æ•ˆPTæ–°å‹æ¡†æ¶ã€‚åœ¨11ä¸ªæ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒPT-MoEåœ¨é—®ç­”å’Œæ•°å­¦é—®é¢˜è§£ç­”ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨é—®ç­”ä»»åŠ¡ä¸­ç›¸å¯¹äºPTæé«˜äº†1.49ç‚¹çš„F1åˆ†æ•°ï¼Œç›¸å¯¹äºLoRAæé«˜äº†2.13ç‚¹ï¼›åœ¨æ•°å­¦å‡†ç¡®æ€§ä¸Šç›¸å¯¹äºPTæé«˜äº†10.75ç‚¹ï¼Œç›¸å¯¹äºLoRAæé«˜äº†0.44ç‚¹ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°æ¯”LoRAå‡å°‘äº†25%ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè™½ç„¶PTæ–¹æ³•åœ¨é—®ç­”ä»»åŠ¡ä¸­é€šå¸¸è¡¨ç°å‡ºè‰²ï¼Œè€ŒåŸºäºLoRAçš„æ–¹æ³•åœ¨æ•°å­¦æ•°æ®é›†ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œä½†PT-MoEä¸­çŸ©é˜µåˆ†è§£å’ŒMoEçš„é›†æˆå¸¦æ¥äº†äº’è¡¥ä¼˜åŠ¿ï¼šåˆ†è§£èƒ½å¤Ÿåœ¨ä¸“å®¶ä¹‹é—´å®ç°æœ‰æ•ˆçš„å‚æ•°å…±äº«ï¼Œè€ŒMoEæä¾›äº†åŠ¨æ€é€‚åº”æ€§ï¼Œå…±åŒä½¿PT-MoEè¡¨ç°å‡ºè·¨ä»»åŠ¡çš„è¿è´¯æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä»¥åŠå¯¹è·¯ç”±æœºåˆ¶å’Œæ¶æ„ç»„ä»¶çš„æ¶ˆèç ”ç©¶ï¼Œä¸ºæœªæ¥çš„PEFTæ–¹æ³•æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09519v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é€‚åº”ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨åç›´è§‰ç°è±¡ã€‚æœ¬æ–‡æå‡ºPT-MoEæ¡†æ¶ï¼Œç»“åˆçŸ©é˜µåˆ†è§£å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰è·¯ç”±ï¼Œå®ç°é«˜æ•ˆå‚æ•°è°ƒè°ã€‚åœ¨17ä¸ªæ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒPT-MoEåœ¨é—®ç­”å’Œæ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶åœ¨é—®ç­”ä»»åŠ¡ä¸­æ¯”PTé«˜å‡º1.49ç‚¹ï¼Œæ¯”LoRAé«˜å‡º2.13ç‚¹ï¼›åœ¨æ•°å­¦å‡†ç¡®æ€§æ–¹é¢ï¼Œæ¯”PTé«˜å‡º10.75ç‚¹ï¼Œæ¯”LoRAé«˜å‡º0.44ç‚¹ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°æ¯”LoRAå°‘25%ã€‚åˆ†æè¡¨æ˜PT-MoEå…·æœ‰è·¨ä»»åŠ¡ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰åœ¨é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>ç°æœ‰PEFTæ–¹æ³•å­˜åœ¨åç›´è§‰ç°è±¡ï¼Œéœ€è¦é€šè¿‡æ”¹è¿›æé«˜æ€§èƒ½ã€‚</li>
<li>æå‡ºPT-MoEæ¡†æ¶ï¼Œç»“åˆçŸ©é˜µåˆ†è§£å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰è·¯ç”±å®ç°é«˜æ•ˆå‚æ•°è°ƒè°ã€‚</li>
<li>PT-MoEåœ¨é—®ç­”å’Œæ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>çŸ©é˜µåˆ†è§£ä½¿å‚æ•°å…±äº«æ›´æœ‰æ•ˆï¼Œè€ŒMoEæä¾›åŠ¨æ€é€‚åº”æ€§ã€‚</li>
<li>PT-MoEå…·æœ‰è·¨ä»»åŠ¡ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d0f3cdb140a401c50614c3b3d72dc07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59859291fd73f83e3ff1359cec180ec5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f92fbb9d758146aec4df6a0f618fc30d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdfbec01b548bd36004ba03ca3449986.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3f728014075fa5b370d693e4621c396.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Layered-Unlearning-for-Adversarial-Relearning"><a href="#Layered-Unlearning-for-Adversarial-Relearning" class="headerlink" title="Layered Unlearning for Adversarial Relearning"></a>Layered Unlearning for Adversarial Relearning</h2><p><strong>Authors:Timothy Qian, Vinith Suriyakumar, Ashia Wilson, Dylan Hadfield-Menell</strong></p>
<p>Our goal is to understand how post-training methods, such as fine-tuning, alignment, and unlearning, modify language model behavior and representations. We are particularly interested in the brittle nature of these modifications that makes them easy to bypass through prompt engineering or relearning. Recent results suggest that post-training induces shallow context-dependent &#96;&#96;circuitsâ€™â€™ that suppress specific response patterns. This could be one explanation for the brittleness of post-training. To test this hypothesis, we design an unlearning algorithm, Layered Unlearning (LU), that creates distinct inhibitory mechanisms for a growing subset of the data. By unlearning the first $i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU limits the ability of relearning on a subset of data to recover the full dataset. We evaluate LU through a combination of synthetic and large language model (LLM) experiments. We find that LU improves robustness to adversarial relearning for several different unlearning methods. Our results contribute to the state-of-the-art of machine unlearning and provide insight into the effect of post-training updates. </p>
<blockquote>
<p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç†è§£å¦‚ä½•åœ¨è®­ç»ƒåçš„æ–¹æ³•ï¼Œå¦‚å¾®è°ƒã€å¯¹é½å’Œé—å¿˜ç­‰ï¼Œå¦‚ä½•æ”¹å˜è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºå’Œè¡¨ç¤ºã€‚æˆ‘ä»¬ç‰¹åˆ«å¯¹è¿™äº›ä¿®æ”¹å¸¦æ¥çš„è„†å¼±æ€§æ„Ÿå…´è¶£ï¼Œè¿™ç§è„†å¼±æ€§å¾ˆå®¹æ˜“é€šè¿‡æç¤ºå·¥ç¨‹æˆ–å†å­¦ä¹ æ¥ç»•è¿‡ã€‚è¿‘æœŸçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®­ç»ƒåä¼šåœ¨ç‰¹å®šç¯å¢ƒä¸‹å¼•å‘æµ…å±‚çš„â€œç”µè·¯â€ï¼Œè¿™äº›ç”µè·¯æŠ‘åˆ¶ç‰¹å®šçš„å“åº”æ¨¡å¼ã€‚è¿™å¯èƒ½æ˜¯è®­ç»ƒåè„†å¼±æ€§çš„ä¸€ä¸ªè§£é‡Šã€‚ä¸ºäº†éªŒè¯è¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é—å¿˜ç®—æ³•ï¼Œå³åˆ†å±‚é—å¿˜ï¼ˆLUï¼‰ï¼Œè¯¥ç®—æ³•ä¸ºæ•°æ®çš„ä¸€ä¸ªä¸æ–­å¢é•¿å­é›†åˆ›å»ºäº†ç‹¬ç‰¹çš„æŠ‘åˆ¶æœºåˆ¶ã€‚åœ¨ké˜¶æ®µçš„ç¬¬iä¸ªé˜¶æ®µï¼Œé€šè¿‡é—å¿˜å‰iä¸ªæŠ˜å çš„æ•°æ®è€Œä¿ç•™å‰©ä½™çš„k-iä¸ªæ•°æ®ï¼ŒLUé™åˆ¶äº†æ•°æ®å­é›†ä¸Šé‡æ–°å­¦ä¹ çš„èƒ½åŠ›ä»¥æ¢å¤æ•´ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬é€šè¿‡åˆæˆå®éªŒå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®éªŒçš„ç»“åˆæ¥è¯„ä¼°LUã€‚æˆ‘ä»¬å‘ç°LUæé«˜äº†ä¸åŒé—å¿˜æ–¹æ³•å¯¹äºå¯¹æŠ—æ€§é‡æ–°å­¦ä¹ çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè¯¥é¢†åŸŸçš„å‘å±•ä½œå‡ºäº†è´¡çŒ®ï¼Œå¹¶å¯¹è®­ç»ƒåçš„æ›´æ–°æ‰€å¸¦æ¥çš„å½±å“æœ‰äº†è¿›ä¸€æ­¥çš„äº†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09500v1">PDF</a> 37 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¾®è°ƒã€å¯¹é½å’Œé—å¿˜ç­‰åè®­ç»ƒæ–¹æ³•æ¥æ”¹å˜è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºå’Œè¡¨ç¤ºã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨è¿™äº›ä¿®æ”¹æ–¹å¼çš„è„†å¼±æ€§ï¼Œå®ƒä»¬å®¹æ˜“å—åˆ°æç¤ºå·¥ç¨‹æˆ–å†å­¦ä¹ çš„ç»•è¿‡ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œåè®­ç»ƒä¼šå¼•å‘æµ…å±‚ä¸Šä¸‹æ–‡ä¾èµ–çš„â€œç”µè·¯â€ï¼ŒæŠ‘åˆ¶ç‰¹å®šå“åº”æ¨¡å¼ï¼Œè¿™å¯èƒ½æ˜¯å…¶è„†å¼±æ€§çš„åŸå› ä¹‹ä¸€ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€å‡è®¾ï¼Œè®¾è®¡äº†ä¸€ç§é—å¿˜ç®—æ³•â€”â€”åˆ†å±‚é—å¿˜ï¼ˆLUï¼‰ï¼Œä¸ºä¸æ–­å¢é•¿çš„æ•°æ®å­é›†åˆ›å»ºäº†ç‹¬ç‰¹çš„æŠ‘åˆ¶æœºåˆ¶ã€‚é€šè¿‡åœ¨ç¬¬iä¸ªé˜¶æ®µçš„k-iæ•°æ®é—å¿˜è¿‡ç¨‹ä¸­å­¦ä¹ å‰iä¸ªæŠ˜å ï¼ŒLUé™åˆ¶äº†éƒ¨åˆ†æ•°æ®å†å­¦ä¹ çš„èƒ½åŠ›ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLUæé«˜äº†å¯¹ä¸åŒé—å¿˜æ–¹æ³•çš„å¯¹æŠ—å†å­¦ä¹ çš„ç¨³å¥æ€§ï¼Œä¸ºæœºå™¨é—å¿˜é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°è§†è§’å’Œè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¾®è°ƒã€å¯¹é½å’Œé—å¿˜ç­‰åè®­ç»ƒæ–¹æ³•æ¥æ”¹å˜è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºå’Œè¡¨ç¤ºã€‚</li>
<li>è¿™äº›åè®­ç»ƒä¿®æ”¹æ–¹å¼çš„è„†å¼±æ€§å—åˆ°å…³æ³¨ï¼Œå®¹æ˜“å—åˆ°æç¤ºå·¥ç¨‹æˆ–å†å­¦ä¹ çš„ç»•è¿‡ã€‚</li>
<li>åè®­ç»ƒå¯èƒ½å¼•å‘æµ…å±‚ä¸Šä¸‹æ–‡ä¾èµ–çš„â€œç”µè·¯â€ï¼ŒæŠ‘åˆ¶ç‰¹å®šå“åº”æ¨¡å¼ï¼Œè¿™æ˜¯å…¶è„†å¼±æ€§çš„åŸå› ä¹‹ä¸€ã€‚</li>
<li>åˆ†å±‚é—å¿˜ï¼ˆLUï¼‰ç®—æ³•è¢«è®¾è®¡æ¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åˆ›å»ºå¯¹æ•°æ®å­é›†çš„ç‹¬ç‰¹æŠ‘åˆ¶æœºåˆ¶æ¥æé«˜ç¨³å¥æ€§ã€‚</li>
<li>LUç®—æ³•é€šè¿‡åˆ†é˜¶æ®µé—å¿˜å’Œå†å­¦ä¹ è¿‡ç¨‹æ¥é™åˆ¶å†å­¦ä¹ çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLUå¯¹æé«˜å¯¹ä¸åŒé—å¿˜æ–¹æ³•çš„å¯¹æŠ—å†å­¦ä¹ çš„ç¨³å¥æ€§æœ‰ç§¯ææ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c8fa37342bdb3d7de6a4327f47342eca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-488743345b68f7a7b03b6308dba8f005.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d60ddf0493315798f87086fa9f6c413.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dc37dc781d736891d06831898028f75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69087f5f3090083fc2f9907d9e04418.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49b1c97ae2226469a43304264c7c6a07.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Omni-R1-Do-You-Really-Need-Audio-to-Fine-Tune-Your-Audio-LLM"><a href="#Omni-R1-Do-You-Really-Need-Audio-to-Fine-Tune-Your-Audio-LLM" class="headerlink" title="Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?"></a>Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?</h2><p><strong>Authors:Andrew Rouditchenko, Saurabhchand Bhati, Edson Araujo, Samuel Thomas, Hilde Kuehne, Rogerio Feris, James Glass</strong></p>
<p>We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Omni-R1ï¼Œå®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•GRPOå¯¹è¿‘æœŸçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Qwen2.5-Omniè¿›è¡Œå¾®è°ƒï¼Œè¯¥æ¨¡å‹åœ¨ä¸€ä¸ªéŸ³é¢‘é—®ç­”æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™å®ç°äº†åœ¨æœ€æ–°MMAUåŸºå‡†æµ‹è¯•ä¸Šçš„æœ€æ–°å›½å®¶å…ˆè¿›æŠ€æœ¯è¡¨ç°ã€‚Omni-R1åœ¨å£°éŸ³ã€éŸ³ä¹ã€è¯­éŸ³å’Œæ€»ä½“å¹³å‡ç±»åˆ«ä¸Šå‡å®ç°äº†æœ€é«˜ç²¾åº¦ï¼Œæ— è®ºæ˜¯åœ¨Test-miniè¿˜æ˜¯Test-fullåˆ†å‰²ä¸Šã€‚ä¸ºäº†äº†è§£æ€§èƒ½æå‡æƒ…å†µï¼Œæˆ‘ä»¬å¯¹å¸¦æœ‰å’Œä¸å¸¦éŸ³é¢‘çš„æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°GRPOçš„å¤§éƒ¨åˆ†æ€§èƒ½æå‡å¯å½’åŠŸäºåŸºäºæ–‡æœ¬çš„æ›´ä¼˜æ¨ç†ã€‚æˆ‘ä»¬è¿˜æ„å¤–åœ°å‘ç°ï¼Œåœ¨åªæœ‰æ–‡æœ¬çš„æ•°æ®é›†ä¸Šä¸ä½¿ç”¨éŸ³é¢‘è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿèƒ½æœ‰æ•ˆæé«˜åŸºäºéŸ³é¢‘çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09439v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Omni-R1é€šè¿‡åœ¨ä¸€ä¸ªéŸ³é¢‘é—®ç­”æ•°æ®é›†ä¸Šå¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Qwen2.5-Omniï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•GRPOï¼Œå®ç°äº†åœ¨æœ€æ–°MMAUåŸºå‡†æµ‹è¯•ä¸Šçš„æœ€æ–°å›½å®¶æŠ€æœ¯è¡¨ç°ã€‚Omni-R1åœ¨å£°éŸ³ã€éŸ³ä¹ã€è¯­éŸ³å’Œæ€»ä½“å¹³å‡ç±»åˆ«ä¸Šå‡è¾¾åˆ°äº†æœ€é«˜å‡†ç¡®ç‡ï¼Œæ— è®ºæ˜¯åœ¨Test-miniè¿˜æ˜¯Test-fullåˆ†å‰²ä¸Šã€‚ä¸ºäº†äº†è§£æ€§èƒ½æå‡çš„åŸå› ï¼Œæˆ‘ä»¬æµ‹è¯•äº†å¸¦æœ‰å’Œä¸å¸¦éŸ³é¢‘çš„æ¨¡å‹ï¼Œå‘ç°GRPOçš„å¤§éƒ¨åˆ†æ€§èƒ½æå‡å½’å› äºåŸºäºæ–‡æœ¬æ¨ç†çš„æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜æ„å¤–åœ°å‘ç°ï¼Œåœ¨æ–‡æœ¬æ•°æ®é›†ä¸Šå¾®è°ƒè€Œä¸ä½¿ç”¨éŸ³é¢‘ä¹Ÿèƒ½æœ‰æ•ˆæé«˜éŸ³é¢‘æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni-R1é€šè¿‡å¾®è°ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Qwen2.5-Omniå¹¶åº”ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•GRPOï¼Œè¾¾åˆ°äº†æœ€æ–°MMAUåŸºå‡†æµ‹è¯•çš„æœ€é«˜å‡†ç¡®ç‡ã€‚</li>
<li>Omni-R1åœ¨å£°éŸ³ã€éŸ³ä¹ã€è¯­éŸ³å’Œæ€»ä½“å¹³å‡ç±»åˆ«ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ€§èƒ½æå‡éƒ¨åˆ†å½’å› äºåŸºäºæ–‡æœ¬æ¨ç†çš„æ”¹è¿›ã€‚</li>
<li>å³ä½¿åœ¨ä¸å¸¦éŸ³é¢‘çš„æƒ…å†µä¸‹è¿›è¡Œå¾®è°ƒï¼Œä¹Ÿèƒ½æœ‰æ•ˆæé«˜æ¨¡å‹åœ¨éŸ³é¢‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€æ¨¡å‹åœ¨éŸ³é¢‘é—®ç­”æ•°æ®é›†ä¸Šçš„æ½œåŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç®—æ³•GRPOåœ¨æ¨¡å‹æ€§èƒ½ä¼˜åŒ–ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-419ec3f211da1c24b8974ba413058c22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a923a49381e7fe950b6567068892ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06fcd64a5e7f3071098582e253bec112.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CXMArena-Unified-Dataset-to-benchmark-performance-in-realistic-CXM-Scenarios"><a href="#CXMArena-Unified-Dataset-to-benchmark-performance-in-realistic-CXM-Scenarios" class="headerlink" title="CXMArena: Unified Dataset to benchmark performance in realistic CXM   Scenarios"></a>CXMArena: Unified Dataset to benchmark performance in realistic CXM   Scenarios</h2><p><strong>Authors:Raghav Garg, Kapil Sharma, Karan Gupta</strong></p>
<p>Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brandâ€™s CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmarkâ€™s difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®¢æˆ·ä½“éªŒç®¡ç†ï¼ˆCXMï¼‰æ–¹é¢ï¼Œç‰¹åˆ«æ˜¯åœ¨å‘¼å«ä¸­å¿ƒè¿è¥æ–¹é¢ï¼Œå…·æœ‰å·¨å¤§çš„é©å‘½æ€§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºåœ¨éšç§æ‹…å¿§å¯¼è‡´çš„æ•°æ®ç¨€ç¼ºå’Œå½“å‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼Œè¯„ä¼°å®ƒä»¬åœ¨å¤æ‚è¿è¥ç¯å¢ƒä¸­çš„å®é™…æ•ˆç”¨å—åˆ°äº†é˜»ç¢ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ç¼ºä¹ç°å®æ€§ï¼Œæœªèƒ½èå…¥æ·±å±‚çŸ¥è¯†åº“ï¼ˆKBï¼‰æ•´åˆã€ç°å®ä¸–ç•Œå™ªéŸ³æˆ–è¶…è¶Šä¼šè¯æµç•…åº¦çš„å…³é”®è¿è¥ä»»åŠ¡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CXMArenaï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°è¿è¥CXMç¯å¢ƒä¸­çš„AIè€Œè®¾è®¡çš„æ–°å‹å¤§è§„æ¨¡åˆæˆåŸºå‡†æ•°æ®é›†ã€‚é‰´äºå‘¼å«ä¸­å¿ƒåŠŸèƒ½çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬å·²ç»å¼€å‘äº†ä¸€ä¸ªå¯æ‰©å±•çš„LLMé©±åŠ¨ç®¡é“ï¼Œæ¨¡æ‹Ÿå“ç‰ŒCXMå®ä½“ä½œä¸ºæˆ‘ä»¬æ•°æ®é›†çš„åŸºç¡€ï¼Œå¦‚åŒ…æ‹¬äº§å“è§„æ ¼ã€é—®é¢˜åˆ†ç±»å’Œå‘¼å«ä¸­å¿ƒå¯¹è¯çš„çŸ¥è¯†æ–‡ç« ã€‚è¿™äº›å®ä½“ç´§å¯†åœ°ä»£è¡¨äº†ç°å®ä¸–ç•Œçš„åˆ†å¸ƒï¼Œå› ä¸ºæ³¨å…¥äº†å¯æ§çš„å™ªå£°ï¼ˆç”±é¢†åŸŸä¸“å®¶æä¾›ä¿¡æ¯ï¼‰å¹¶é€šè¿‡ä¸¥æ ¼çš„è‡ªåŠ¨åŒ–éªŒè¯ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å‘å¸ƒäº†CXMArenaï¼Œå®ƒæä¾›äº†é’ˆå¯¹äº”ä¸ªé‡è¦è¿è¥ä»»åŠ¡çš„ä¸“ç”¨åŸºå‡†æµ‹è¯•ï¼šçŸ¥è¯†åº“ç»†åŒ–ã€æ„å›¾é¢„æµ‹ã€ä»£ç†è´¨é‡éµå®ˆã€æ–‡ç« æœç´¢ä»¥åŠå¤šå›åˆRAGä¸é›†æˆå·¥å…·ã€‚æˆ‘ä»¬çš„åŸºå‡†å®éªŒçªæ˜¾äº†åŸºå‡†æµ‹è¯•çš„å›°éš¾ï¼šå³ä½¿æ˜¯æœ€å…ˆè¿›çš„åµŒå…¥å’Œç”Ÿæˆæ¨¡å‹åœ¨æ–‡ç« æœç´¢æ–¹é¢ä¹Ÿåªèƒ½è¾¾åˆ°68%çš„å‡†ç¡®ç‡ï¼Œè€Œæ ‡å‡†åµŒå…¥æ–¹æ³•åœ¨ä¸ºçŸ¥è¯†åº“ç»†åŒ–æ—¶å¾—åˆ°çš„F1åˆ†æ•°å¾ˆä½ï¼Œä¸º0.3ï¼Œè¿™çªæ˜¾äº†å½“å‰æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œéœ€è¦é‡‡ç”¨å¤æ‚çš„ç®¡é“å’Œè§£å†³æ–¹æ¡ˆæ¥è¶…è¶Šä¼ ç»ŸæŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09436v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®¢æˆ·ä½“éªŒç®¡ç†ï¼ˆCXMï¼‰é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‘¼å«ä¸­å¿ƒè¿è¥æ–¹é¢ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®éšç§æ‹…å¿§å¯¼è‡´çš„æ•°æ®ç¨€ç¼ºæ€§å’Œç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼Œè¯„ä¼°å…¶åœ¨å¤æ‚æ“ä½œç¯å¢ƒä¸­çš„å®ç”¨æ€§é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CXMArenaï¼Œä¸€ä¸ªä¸“ä¸ºè¯„ä¼°æ“ä½œå‹CXMä¸­çš„AIè€Œè®¾è®¡çš„å¤§å‹åˆæˆåŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é€šè¿‡æ¨¡æ‹Ÿå“ç‰ŒCXMå®ä½“ï¼ˆå¦‚äº§å“è§„æ ¼ã€é—®é¢˜åˆ†ç±»å’Œå‘¼å«ä¸­å¿ƒå¯¹è¯ï¼‰çš„ç®¡é“ï¼Œä½“ç°äº†æ¥è§¦ä¸­å¿ƒçš„å¤šç§ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨æ­¤åŸºç¡€ä¸Šå‘å¸ƒäº†CXMArenaï¼Œæä¾›äº”ä¸ªé‡è¦æ“ä½œä»»åŠ¡çš„ä¸“é—¨åŸºå‡†æµ‹è¯•ï¼šçŸ¥è¯†åº“ä¼˜åŒ–ã€æ„å›¾é¢„æµ‹ã€ä»£ç†è´¨é‡éµå®ˆã€æ–‡ç« æœç´¢å’Œå¤šå›åˆå¯¹è¯ä¸é›†æˆå·¥å…·ã€‚åŸºå‡†å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„åµŒå…¥å’Œç”Ÿæˆæ¨¡å‹ï¼Œåœ¨æ–‡ç« æœç´¢ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰68%ï¼Œè€Œæ ‡å‡†åµŒå…¥æ–¹æ³•åœ¨çŸ¥è¯†åº“ä¼˜åŒ–ä»»åŠ¡ä¸Šçš„F1åˆ†æ•°è¾ƒä½ï¼Œè¿™çªæ˜¾äº†å½“å‰æ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¤æ‚çš„ç®¡é“å’Œè§£å†³æ–¹æ¡ˆæ¥åº”å¯¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨CXMé¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‘¼å«ä¸­å¿ƒè¿è¥æ–¹é¢ã€‚</li>
<li>æ•°æ®ç¨€ç¼ºå’Œç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§é™åˆ¶äº†LLMåœ¨å®é™…æ“ä½œç¯å¢ƒä¸­çš„è¯„ä¼°ã€‚</li>
<li>CXMArenaæ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°æ“ä½œå‹CXMä¸­çš„AIè€Œè®¾è®¡çš„å¤§å‹åˆæˆåŸºå‡†æ•°æ®é›†ã€‚</li>
<li>CXMArenaæ¨¡æ‹Ÿäº†å“ç‰ŒCXMå®ä½“çš„ç®¡é“ï¼ŒåŒ…æ‹¬äº§å“è§„æ ¼ã€é—®é¢˜åˆ†ç±»å’Œå‘¼å«ä¸­å¿ƒå¯¹è¯ç­‰ã€‚</li>
<li>CXMArenaæä¾›äº†äº”ä¸ªé‡è¦æ“ä½œä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬çŸ¥è¯†åº“ä¼˜åŒ–ã€æ„å›¾é¢„æµ‹ç­‰ã€‚</li>
<li>åŸºå‡†å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨CXMArenaä¸Šçš„è¡¨ç°å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ffae04fc8453e3a2e659acef5d970bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59a37b70f30870479ceea7e7a2b36bf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b13a15c89c86afd5502d52f6ecb3aea4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df029c8b2203cccbf2482ac6f0aecc0d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Endo-CLIP-Progressive-Self-Supervised-Pre-training-on-Raw-Colonoscopy-Records"><a href="#Endo-CLIP-Progressive-Self-Supervised-Pre-training-on-Raw-Colonoscopy-Records" class="headerlink" title="Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy   Records"></a>Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy   Records</h2><p><strong>Authors:Yili He, Yan Zhu, Peiyao Fu, Ruijie Yang, Tianyi Chen, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang</strong></p>
<p>Pre-training on image-text colonoscopy records offers substantial potential for improving endoscopic image analysis, but faces challenges including non-informative background images, complex medical terminology, and ambiguous multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised framework that enhances Contrastive Language-Image Pre-training (CLIP) for this domain. Endo-CLIPâ€™s three-stage frameworkâ€“cleansing, attunement, and unificationâ€“addresses these challenges by (1) removing background frames, (2) leveraging large language models to extract clinical attributes for fine-grained contrastive learning, and (3) employing patient-level cross-attention to resolve multi-polyp ambiguities. Extensive experiments demonstrate that Endo-CLIP significantly outperforms state-of-the-art pre-training methods in zero-shot and few-shot polyp detection and classification, paving the way for more accurate and clinically relevant endoscopic analysis. </p>
<blockquote>
<p>å¯¹å›¾åƒæ–‡æœ¬ç»“è‚ é•œæ£€æŸ¥è®°å½•çš„é¢„è®­ç»ƒåœ¨æ”¹è¿›å†…é•œå›¾åƒåˆ†ææ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†é¢ä¸´ç€éä¿¡æ¯æ€§èƒŒæ™¯å›¾åƒã€å¤æ‚çš„åŒ»å­¦æœ¯è¯­å’Œæ¨¡ç³Šçš„å¤šç—…ç¶æè¿°ç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†Endo-CLIPï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªç›‘ç£æ¡†æ¶ï¼Œèƒ½å¤Ÿå¢å¼ºè¿™ä¸€é¢†åŸŸçš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ã€‚Endo-CLIPçš„ä¸‰é˜¶æ®µæ¡†æ¶â€”â€”å‡€åŒ–ã€è°ƒæ•´å’Œç»Ÿä¸€ï¼Œé€šè¿‡ï¼ˆ1ï¼‰å»é™¤èƒŒæ™¯å¸§ï¼Œï¼ˆ2ï¼‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–ç”¨äºç²¾ç»†å¯¹æ¯”å­¦ä¹ çš„ä¸´åºŠç‰¹å¾ï¼Œï¼ˆ3ï¼‰é‡‡ç”¨æ‚£è€…çº§äº¤å‰æ³¨æ„åŠ›æ¥è§£å†³å¤šæ¯è‚‰æ¨¡ç³Šé—®é¢˜ï¼Œæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEndo-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ¯è‚‰æ£€æµ‹å’Œåˆ†ç±»æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œä¸ºæ›´å‡†ç¡®å’Œä¸´åºŠç›¸å…³çš„å†…é•œåˆ†æé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09435v1">PDF</a> Early accepted to MICCAI 2025</p>
<p><strong>Summary</strong>ï¼šé¢„è®­ç»ƒå›¾åƒæ–‡æœ¬ç»“è‚ é•œæ£€æŸ¥è®°å½•å¯¹æ”¹è¿›å†…é•œå›¾åƒåˆ†æå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†é¢ä¸´éä¿¡æ¯èƒŒæ™¯å›¾åƒã€å¤æ‚åŒ»å­¦æœ¯è¯­å’Œæ¨¡ç³Šçš„å¤šç—…ç¶æè¿°ç­‰æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Endo-CLIPï¼Œä¸€ç§æ–°å‹çš„è‡ªç›‘ç£æ¡†æ¶ï¼Œç”¨äºå¢å¼ºæ­¤é¢†åŸŸçš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ã€‚Endo-CLIPçš„ä¸‰é˜¶æ®µæ¡†æ¶åŒ…æ‹¬å‡€åŒ–ã€è°ƒæ•´å’Œç»Ÿä¸€ï¼Œé€šè¿‡ï¼ˆ1ï¼‰å»é™¤èƒŒæ™¯å¸§ï¼Œï¼ˆ2ï¼‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–ä¸´åºŠå±æ€§è¿›è¡Œç²¾ç»†å¯¹æ¯”å­¦ä¹ ï¼Œï¼ˆ3ï¼‰é‡‡ç”¨æ‚£è€…çº§äº¤å‰æ³¨æ„åŠ›æ¥è§£å†³å¤šæ¯è‚‰æ¨¡ç³Šæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEndo-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ¯è‚‰æ£€æµ‹å’Œåˆ†ç±»æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œä¸ºæ›´å‡†ç¡®å’Œä¸´åºŠç›¸å…³çš„å†…é•œåˆ†æé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é¢„è®­ç»ƒå›¾åƒæ–‡æœ¬ç»“è‚ é•œæ£€æŸ¥è®°å½•åœ¨æ”¹è¿›å†…é•œå›¾åƒåˆ†ææ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>Endo-CLIPæ˜¯ä¸€ä¸ªæ–°å‹è‡ªç›‘ç£æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨è¯¥é¢†åŸŸçš„è¡¨ç°ã€‚</li>
<li>Endo-CLIPé€šè¿‡å»é™¤èƒŒæ™¯å¸§ã€åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå–ä¸´åºŠå±æ€§è¿›è¡Œç²¾ç»†å¯¹æ¯”å­¦ä¹ ä»¥åŠé‡‡ç”¨æ‚£è€…çº§äº¤å‰æ³¨æ„åŠ›æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>Endo-CLIPæ˜¾è‘—ä¼˜äºæœ€æ–°çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ¯è‚‰æ£€æµ‹å’Œåˆ†ç±»æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>Endo-CLIPçš„ä¸‰é˜¶æ®µæ¡†æ¶åŒ…æ‹¬å‡€åŒ–ã€è°ƒæ•´å’Œç»Ÿä¸€ï¼Œç¡®ä¿äº†å…¶æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ›´å‡†ç¡®å’Œä¸´åºŠç›¸å…³çš„å†…é•œåˆ†æé“ºå¹³äº†é“è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-134bbf35634b0242eb5fcf5142640093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c040039c4d6653b8ed5abd15ba70502.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b660e7bf7213a3ed505faf4ed6d1cc71.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SafePath-Conformal-Prediction-for-Safe-LLM-Based-Autonomous-Navigation"><a href="#SafePath-Conformal-Prediction-for-Safe-LLM-Based-Autonomous-Navigation" class="headerlink" title="SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation"></a>SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation</h2><p><strong>Authors:Achref Doula, Max MÃ¼hlÃ¤user, Alejandro Sanchez Guinea</strong></p>
<p>Large Language Models (LLMs) show growing promise in autonomous driving by reasoning over complex traffic scenarios to generate path plans. However, their tendencies toward overconfidence, and hallucinations raise critical safety concerns. We introduce SafePath, a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. SafePath operates in three stages. In the first stage, we use an LLM that generates a set of diverse candidate paths, exploring possible trajectories based on agent behaviors and environmental cues. In the second stage, SafePath filters out high-risk trajectories while guaranteeing that at least one safe option is included with a user-defined probability, through a multiple-choice question-answering formulation that integrates conformal prediction. In the final stage, our approach selects the path with the lowest expected collision risk when uncertainty is low or delegates control to a human when uncertainty is high. We theoretically prove that SafePath guarantees a safe trajectory with a user-defined probability, and we show how its human delegation rate can be tuned to balance autonomy and safety. Extensive experiments on nuScenes and Highway-env show that SafePath reduces planning uncertainty by 77% and collision rates by up to 70%, demonstrating effectiveness in making LLM-driven path planning more safer. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨é©¾é©¶è·¯å¾„è§„åˆ’æ–¹é¢æ˜¾ç¤ºå‡ºè¶Šæ¥è¶Šå¤§çš„æ½œåŠ›ï¼Œå®ƒä»¬å¯ä»¥é€šè¿‡å¤æ‚çš„äº¤é€šåœºæ™¯è¿›è¡Œæ¨ç†æ¥ç”Ÿæˆè·¯å¾„è®¡åˆ’ã€‚ç„¶è€Œï¼Œå®ƒä»¬è¿‡äºè‡ªä¿¡å’Œå¹»è§‰çš„å€¾å‘å¼•å‘äº†å…³é”®çš„å®‰å…¨é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†SafePathï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å½¢å¼åŒ–é¢„æµ‹æ¥å¢å¼ºåŸºäºLLMçš„è·¯å¾„è§„åˆ’çš„å®‰å…¨ä¿éšœã€‚SafePathåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µè¿è¡Œã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨LLMç”Ÿæˆä¸€ç»„å¤šæ ·åŒ–çš„å€™é€‰è·¯å¾„ï¼Œæ ¹æ®ä»£ç†è¡Œä¸ºå’Œç¯å¢ƒçº¿ç´¢æ¢ç´¢å¯èƒ½çš„è½¨è¿¹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒSafePathè¿‡æ»¤æ‰é«˜é£é™©è½¨è¿¹ï¼ŒåŒæ—¶é€šè¿‡å¤šé‡é€‰æ‹©é—®ç­”çš„å½¢å¼æ•´åˆå½¢å¼åŒ–é¢„æµ‹ï¼Œä¿è¯è‡³å°‘æœ‰ä¸€ä¸ªå®‰å…¨é€‰é¡¹è¢«åŒ…æ‹¬åœ¨å†…ï¼Œä¸”æ¦‚ç‡ç”±ç”¨æˆ·å®šä¹‰ã€‚åœ¨æœ€åä¸€ä¸ªé˜¶æ®µï¼Œå½“ä¸ç¡®å®šæ€§è¾ƒä½æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼šé€‰æ‹©å…·æœ‰æœ€ä½é¢„æœŸç¢°æ’é£é™©çš„è·¯å¾„ï¼›å½“ä¸ç¡®å®šæ€§è¾ƒé«˜æ—¶ï¼Œåˆ™ä¼šå°†æ§åˆ¶æƒäº¤ç»™äººç±»ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†SafePathèƒ½ä¿è¯ä»¥ç”¨æˆ·å®šä¹‰çš„æ¦‚ç‡é€‰æ‹©å®‰å…¨è½¨è¿¹ï¼Œå¹¶ä¸”å±•ç¤ºäº†å¦‚ä½•è°ƒæ•´å…¶äººç±»ä»£ç†ç‡ä»¥å¹³è¡¡è‡ªä¸»æ€§å’Œå®‰å…¨æ€§ã€‚åœ¨nuSceneså’ŒHighway-envä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSafePathå°†è§„åˆ’ä¸ç¡®å®šæ€§é™ä½äº†77%ï¼Œç¢°æ’ç‡é™ä½äº†é«˜è¾¾70%ï¼Œè¯æ˜äº†å…¶åœ¨ä½¿LLMé©±åŠ¨çš„è·¯å¾„è§„åˆ’æ›´åŠ å®‰å…¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09427v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨é©¾é©¶è·¯å¾„è§„åˆ’ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡å¤æ‚çš„äº¤é€šåœºæ™¯è¿›è¡Œæ¨ç†ã€‚ç„¶è€Œï¼Œå…¶è¿‡åº¦è‡ªä¿¡ä¸å¹»è§‰é—®é¢˜å¼•å‘å…³é”®å®‰å…¨é¡¾è™‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºSafePathæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥æ¨¡å—åŒ–çš„å½¢å¼å¢å¼ºLLMè·¯å¾„è§„åˆ’ï¼Œå¹¶åˆ©ç”¨å½¢å¼åŒ–å®‰å…¨ä¿è¯æŠ€æœ¯â€”â€”é¡ºåº”é¢„æµ‹æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ã€‚SafePathåŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆä½¿ç”¨LLMç”Ÿæˆå¤šæ ·åŒ–å€™é€‰è·¯å¾„é›†ï¼›ç„¶åé€šè¿‡é›†æˆé¡ºåº”é¢„æµ‹æŠ€æœ¯çš„é€‰æ‹©é¢˜é—®ç­”æ¨¡å¼ç­›é€‰å‡ºé«˜é£é™©è½¨è¿¹å¹¶ä¿éšœè‡³å°‘ä¸€ä¸ªå®‰å…¨é€‰é¡¹çš„å­˜åœ¨ï¼Œå…¶å­˜åœ¨æ¦‚ç‡ä¸ºç”¨æˆ·è‡ªå®šä¹‰å€¼ï¼›æœ€ååœ¨ä¸ç¡®å®šæ€§è¾ƒä½æ—¶é€‰æ‹©ç¢°æ’é£é™©æœ€ä½çš„è·¯å¾„æ‰§è¡Œæˆ–åœ¨é«˜ä¸ç¡®å®šæ€§æ—¶ç§»äº¤æ§åˆ¶æƒç»™äººç±»ã€‚SafePathç†è®ºè¯æ˜èƒ½ä¿éšœç”¨æˆ·è‡ªå®šä¹‰æ¦‚ç‡ä¸‹çš„å®‰å…¨è½¨è¿¹ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡è°ƒæ•´äººç±»ä»£ç†ç‡æ¥å¹³è¡¡è‡ªä¸»æ€§ä¸å®‰å…¨æ€§ã€‚åœ¨nuSceneså’ŒHighway-envä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSafePathå°†è§„åˆ’ä¸ç¡®å®šæ€§é™ä½äº†77%ï¼Œç¢°æ’ç‡æœ€é«˜å¯é™ä½70%ï¼Œæœ‰æ•ˆæå‡äº†LLMé©±åŠ¨è·¯å¾„è§„åˆ’çš„å®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªåŠ¨é©¾é©¶è·¯å¾„è§„åˆ’ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤„ç†å¤æ‚çš„äº¤é€šåœºæ™¯æ¨ç†ã€‚</li>
<li>LLMçš„è¿‡åº¦è‡ªä¿¡ä¸å¹»è§‰é—®é¢˜å¼•å‘å®‰å…¨é¡¾è™‘ã€‚</li>
<li>SafePathæ¡†æ¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡å¢å¼ºLLMè·¯å¾„è§„åˆ’ï¼Œå¹¶å¼•å…¥å½¢å¼åŒ–å®‰å…¨ä¿è¯æŠ€æœ¯â€”â€”é¡ºåº”é¢„æµ‹æŠ€æœ¯ã€‚</li>
<li>SafePathåŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šç”Ÿæˆå€™é€‰è·¯å¾„ã€ç­›é€‰é«˜é£é™©è½¨è¿¹å¹¶ä¿éšœè‡³å°‘ä¸€ä¸ªå®‰å…¨é€‰é¡¹çš„å­˜åœ¨ã€é€‰æ‹©ç¢°æ’é£é™©æœ€ä½çš„è·¯å¾„æˆ–åœ¨ä¸ç¡®å®šæ€§é«˜æ—¶ç§»äº¤æ§åˆ¶æƒã€‚</li>
<li>SafePathå¯ä¿éšœç”¨æˆ·è‡ªå®šä¹‰æ¦‚ç‡ä¸‹çš„å®‰å…¨è½¨è¿¹ï¼Œå¹¶é€šè¿‡è°ƒæ•´äººç±»ä»£ç†ç‡å¹³è¡¡è‡ªä¸»æ€§ä¸å®‰å…¨æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºSafePathèƒ½æ˜¾è‘—é™ä½è§„åˆ’ä¸ç¡®å®šæ€§å’Œç¢°æ’ç‡ï¼Œæœ‰æ•ˆæå‡LLMé©±åŠ¨è·¯å¾„è§„åˆ’çš„å®‰å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ceb2ba9a017666c47262cfa6727645ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e614b5ab27e5eda99db8a4cc5787d62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2dcd4884fec3e6cfcba7f5161d43556.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FaceShield-Explainable-Face-Anti-Spoofing-with-Multimodal-Large-Language-Models"><a href="#FaceShield-Explainable-Face-Anti-Spoofing-with-Multimodal-Large-Language-Models" class="headerlink" title="FaceShield: Explainable Face Anti-Spoofing with Multimodal Large   Language Models"></a>FaceShield: Explainable Face Anti-Spoofing with Multimodal Large   Language Models</h2><p><strong>Authors:Hongyang Wang, Yichen Shi, Zhuofu Tao, Yuhao Gao, Liepiao Zhang, Xun Lin, Jun Feng, Xiaochen Yuan, Zitong Yu, Xiaochun Cao</strong></p>
<p>Face anti-spoofing (FAS) is crucial for protecting facial recognition systems from presentation attacks. Previous methods approached this task as a classification problem, lacking interpretability and reasoning behind the predicted results. Recently, multimodal large language models (MLLMs) have shown strong capabilities in perception, reasoning, and decision-making in visual tasks. However, there is currently no universal and comprehensive MLLM and dataset specifically designed for FAS task. To address this gap, we propose FaceShield, a MLLM for FAS, along with the corresponding pre-training and supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K. FaceShield is capable of determining the authenticity of faces, identifying types of spoofing attacks, providing reasoning for its judgments, and detecting attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that incorporates both the original image and auxiliary information based on prior knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to random mask vision tokens, thereby improving the modelâ€™s generalization ability. We conducted extensive experiments on three benchmark datasets, demonstrating that FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks, i.e., coarse-grained classification, fine-grained classification, reasoning, and attack localization. Our instruction datasets, protocols, and codes will be released soon. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«åæ¬ºéª—ï¼ˆFace Anti-Spoofingï¼ŒFASï¼‰å¯¹äºä¿æŠ¤äººè„¸è¯†åˆ«ç³»ç»Ÿå…å—ä¼ªé€ æ”»å‡»è‡³å…³é‡è¦ã€‚ä»¥å¾€çš„æ–¹æ³•å°†æ­¤ä»»åŠ¡è§†ä¸ºåˆ†ç±»é—®é¢˜ï¼Œç¼ºä¹é¢„æµ‹ç»“æœçš„å¯è§£é‡Šæ€§å’Œæ¨ç†ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Modelsï¼ŒMLLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡çš„æ„ŸçŸ¥ã€æ¨ç†å’Œå†³ç­–æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰å°šæ²¡æœ‰é’ˆå¯¹FASä»»åŠ¡çš„é€šç”¨ä¸”å…¨é¢çš„MLLMåŠå…¶ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†FaceShieldï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºFASçš„MLLMï¼Œä»¥åŠç›¸åº”çš„é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-tuningï¼ŒSFTï¼‰æ•°æ®é›†FaceShield-pre10Kå’ŒFaceShield-sft45Kã€‚FaceShieldèƒ½å¤Ÿç¡®å®šäººè„¸çš„çœŸå®æ€§ï¼Œè¯†åˆ«æ¬ºéª—æ”»å‡»çš„ç±»å‹ï¼Œä¸ºå…¶åˆ¤æ–­æä¾›ç†ç”±ï¼Œå¹¶æ£€æµ‹æ”»å‡»åŒºåŸŸã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºå…ˆéªŒçŸ¥è¯†çš„æ¬ºéª—æ„ŸçŸ¥è§†è§‰æ„ŸçŸ¥ï¼ˆSpoof-Aware Vision Perceptionï¼ŒSAVPï¼‰ï¼Œå®ƒç»“åˆäº†åŸå§‹å›¾åƒå’Œè¾…åŠ©ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æç¤ºå¼•å¯¼è§†è§‰ä»¤ç‰Œæ©ç ï¼ˆPrompt-Guided Vision Token Maskingï¼ŒPVTMï¼‰ç­–ç•¥éšæœºæ©è”½è§†è§‰ä»¤ç‰Œï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜FaceShieldåœ¨å››ä¸ªFASä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œæ™®é€šMLLMsï¼ŒåŒ…æ‹¬ç²—ç²’åº¦åˆ†ç±»ã€ç»†ç²’åº¦åˆ†ç±»ã€æ¨ç†å’Œæ”»å‡»å®šä½ã€‚æˆ‘ä»¬çš„æŒ‡ä»¤æ•°æ®é›†ã€åè®®å’Œä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09415v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¸ºæé«˜é¢éƒ¨è¯†åˆ«ç³»ç»Ÿçš„é˜²æ¬ºè¯ˆèƒ½åŠ›ï¼Œæ¨å‡ºFaceShieldâ€”â€”ä¸€ç§ç”¨äºé¢éƒ¨æŠ—æ¬ºéª—ï¼ˆFASï¼‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥ä¼ªé€ æ„ŸçŸ¥è§†è§‰æ„ŸçŸ¥å’Œæç¤ºå¼•å¯¼è§†è§‰ä»¤ç‰Œé®è”½ç­–ç•¥ï¼ŒFaceShieldèƒ½åœ¨åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šç°æœ‰æ¨¡å‹ï¼Œè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç°å·²æ¨å‡ºé¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®é›†FaceShield-pre10Kå’ŒFaceShield-sft45Kã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¢«ç”¨äºé¢éƒ¨æŠ—æ¬ºéª—ï¼ˆFASï¼‰ä»»åŠ¡ä»¥å¢å¼ºé¢éƒ¨è¯†åˆ«ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚</li>
<li>å½“å‰ç¼ºä¹é’ˆå¯¹FASä»»åŠ¡çš„é€šç”¨å’Œå…¨é¢çš„MLLMåŠæ•°æ®é›†ã€‚</li>
<li>æå‡ºFaceShield MLLMï¼Œé…åˆé¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®é›†FaceShield-pre10Kå’ŒFaceShield-sft45Kï¼Œä»¥ç”¨äºFASä»»åŠ¡ã€‚</li>
<li>FaceShieldå…·æœ‰åˆ¤å®šé¢éƒ¨çœŸå®æ€§ã€è¯†åˆ«æ¬ºéª—æ”»å‡»ç±»å‹ã€ä¸ºåˆ¤æ–­æä¾›ç†ç”±å’Œæ£€æµ‹æ”»å‡»åŒºåŸŸçš„èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨ä¼ªé€ æ„ŸçŸ¥è§†è§‰æ„ŸçŸ¥ï¼ˆSAVPï¼‰æŠ€æœ¯ç»“åˆåŸå§‹å›¾åƒå’ŒåŸºäºå…ˆéªŒçŸ¥è¯†çš„è¾…åŠ©ä¿¡æ¯è¿›è¡Œå¤„ç†ã€‚</li>
<li>é‡‡ç”¨æç¤ºå¼•å¯¼è§†è§‰ä»¤ç‰Œé®è”½ï¼ˆPVTMï¼‰ç­–ç•¥éšæœºé®è”½è§†è§‰ä»¤ç‰Œï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-55185f3fdc9879becd930dd8fb965e70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8539d4c62cd72c3806d6ad0824439954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7404df70b25b27dfa327cb1521a6b50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7edcd9cd1280b4a3f6653bb9c6276e5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0749d3860911e2a67f1886cb053c46f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b302d7ee81c120d15d8c0389e6a0e04a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3e8889cd64e76f523db601e22f4a098.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Qwen3-Technical-Report"><a href="#Qwen3-Technical-Report" class="headerlink" title="Qwen3 Technical Report"></a>Qwen3 Technical Report</h2><p><strong>Authors:An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu</strong></p>
<p>In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different modelsâ€“such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)â€“and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Qwen3ï¼Œå®ƒæ˜¯Qwenæ¨¡å‹å®¶æ—çš„æœ€æ–°ç‰ˆæœ¬ã€‚Qwen3åŒ…å«ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½ã€æ•ˆç‡å’Œå¤šè¯­è¨€èƒ½åŠ›ã€‚Qwen3ç³»åˆ—åŒ…æ‹¬å¯†é›†æ¨¡å‹å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„çš„æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡ä»6åƒäº¿åˆ°235åƒäº¿ä¸ç­‰ã€‚Qwen3çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ€è€ƒæ¨¡å¼ï¼ˆç”¨äºå¤æ‚ã€å¤šæ­¥éª¤æ¨ç†ï¼‰å’Œéæ€è€ƒæ¨¡å¼ï¼ˆç”¨äºå¿«é€Ÿã€åŸºäºä¸Šä¸‹æ–‡çš„å“åº”ï¼‰é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚è¿™æ¶ˆé™¤äº†éœ€è¦åœ¨ä¸åŒçš„æ¨¡å‹ä¹‹é—´åˆ‡æ¢çš„éœ€è¦ï¼Œæ¯”å¦‚ä¼˜åŒ–èŠå¤©æ¨¡å‹ï¼ˆä¾‹å¦‚GPT-4oï¼‰å’Œä¸“ç”¨æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚QwQ-32Bï¼‰ï¼Œå¹¶å¯æ ¹æ®ç”¨æˆ·æŸ¥è¯¢æˆ–èŠå¤©æ¨¡æ¿å®ç°åŠ¨æ€æ¨¡å¼åˆ‡æ¢ã€‚åŒæ—¶ï¼ŒQwen3å¼•å…¥äº†ä¸€ç§æ€è€ƒé¢„ç®—æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·åœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œæ ¹æ®ä»»åŠ¡å¤æ‚æ€§å¹³è¡¡å»¶è¿Ÿå’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨æ——èˆ°æ¨¡å‹çš„çŸ¥è¯†ï¼Œæˆ‘ä»¬åœ¨æ„å»ºå°å‹æ¨¡å‹æ—¶æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„è®¡ç®—èµ„æºï¼ŒåŒæ—¶ç¡®ä¿äº†å…¶é«˜åº¦ç«äº‰çš„æ€§èƒ½ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒQwen3åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°ç»“æœï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ã€ä»£ç†ä»»åŠ¡ç­‰ï¼Œä¸æ›´å¤§çš„MoEæ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹ç«äº‰ã€‚ä¸å‰ä»£äº§å“Qwen2.5ç›¸æ¯”ï¼ŒQwen3å°†å¤šè¯­è¨€æ”¯æŒä»29ç§æ‰©å±•åˆ°119ç§è¯­è¨€å’Œæ–¹è¨€ï¼Œé€šè¿‡æ”¹è¿›è·¨è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¢å¼ºäº†å…¨çƒå¯åŠæ€§ã€‚ä¸ºä¾¿äºå¤åˆ¶å’Œç¤¾åŒºé©±åŠ¨çš„ç ”ç©¶ä¸å¼€å‘ï¼Œæ‰€æœ‰Qwen3æ¨¡å‹å‡åœ¨Apache 2.0ä¸‹å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09388v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ä»‹ç»äº†Qwen3ï¼ŒQwenæ¨¡å‹ç³»åˆ—çš„æœ€æ–°ç‰ˆæœ¬ã€‚Qwen3åŒ…æ‹¬ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½ã€æ•ˆç‡å’Œå¤šè¯­è¨€èƒ½åŠ›ã€‚Qwen3ç³»åˆ—åŒ…æ‹¬å¯†é›†æ¨¡å‹å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„çš„æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡ä»6åƒäº¿åˆ°235åƒäº¿ä¸ç­‰ã€‚ä¸»è¦åˆ›æ–°åœ¨äºå°†æ€è€ƒæ¨¡å¼ï¼ˆç”¨äºå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ï¼‰å’Œéæ€è€ƒæ¨¡å¼ï¼ˆç”¨äºå¿«é€Ÿã€åŸºäºä¸Šä¸‹æ–‡çš„å“åº”ï¼‰ç»Ÿä¸€åˆ°æ¡†æ¶ä¸­ã€‚åŒæ—¶å¼•å…¥æ€è€ƒé¢„ç®—æœºåˆ¶ï¼Œå¹³è¡¡æ¨ç†çš„å»¶è¿Ÿå’Œæ€§èƒ½ã€‚é€šè¿‡æ——èˆ°æ¨¡å‹çš„çŸ¥è¯†ï¼Œå‡å°‘æ„å»ºå°å‹æ¨¡å‹çš„è®¡ç®—èµ„æºéœ€æ±‚ã€‚Qwen3åœ¨å¤šè¯­è¨€æ”¯æŒæ–¹é¢ä»29ç§è¯­è¨€å’Œæ–¹è¨€æ‰©å±•åˆ°119ç§ï¼Œæé«˜äº†è·¨è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚æ‰€æœ‰Qwen3æ¨¡å‹å‡å…¬å¼€å¯ç”¨ï¼Œä»¥æ¨åŠ¨å¯å¤åˆ¶æ€§å’Œç¤¾åŒºé©±åŠ¨çš„ç ”ç©¶å’Œå¼€å‘ã€‚

**Key Takeaways**
 
1. Qwen3æ˜¯Qwenæ¨¡å‹ç³»åˆ—çš„æœ€æ–°ç‰ˆæœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚
2. Qwen3ç³»åˆ—åŒ…æ‹¬å¯†é›†å’Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„çš„æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡æœ‰æ‰€ä¸åŒã€‚
3. Qwen3é›†æˆäº†æ€è€ƒæ¨¡å¼å’Œéæ€è€ƒæ¨¡å¼ï¼Œå®ç°åŠ¨æ€æ¨¡å¼åˆ‡æ¢ã€‚
4. Qwen3å¼•å…¥æ€è€ƒé¢„ç®—æœºåˆ¶ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚æ€§å¹³è¡¡å»¶è¿Ÿå’Œæ€§èƒ½ã€‚
5. åˆ©ç”¨æ——èˆ°æ¨¡å‹çŸ¥è¯†å‡å°‘å°å‹æ¨¡å‹çš„è®¡ç®—èµ„æºéœ€æ±‚ï¼Œä¿æŒé«˜åº¦ç«äº‰åŠ›ã€‚
6. Qwen3åœ¨å¤šè¯­è¨€æ”¯æŒæ–¹é¢æ˜¾è‘—æ‰©å±•ï¼Œå¢å¼ºäº†å…¨çƒè®¿é—®èƒ½åŠ›ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09388">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-646a5867a5afc20a04fa44e0cea5effa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e848cea44c853eac82e4f86bf15d0793.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e8ca319b439a92e91a2cfe4b0f3e228.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5e21ab9d37d00217bbec6adc26a6c28.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MAKE-Multi-Aspect-Knowledge-Enhanced-Vision-Language-Pretraining-for-Zero-shot-Dermatological-Assessment"><a href="#MAKE-Multi-Aspect-Knowledge-Enhanced-Vision-Language-Pretraining-for-Zero-shot-Dermatological-Assessment" class="headerlink" title="MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for   Zero-shot Dermatological Assessment"></a>MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for   Zero-shot Dermatological Assessment</h2><p><strong>Authors:Siyuan Yan, Xieji Li, Ming Hu, Yiwen Jiang, Zhen Yu, Zongyuan Ge</strong></p>
<p>Dermatological diagnosis represents a complex multimodal challenge that requires integrating visual features with specialized clinical knowledge. While vision-language pretraining (VLP) has advanced medical AI, its effectiveness in dermatology is limited by text length constraints and the lack of structured texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced vision-language pretraining framework for zero-shot dermatological tasks. Recognizing that comprehensive dermatological descriptions require multiple knowledge aspects that exceed standard text constraints, our framework introduces: (1) a multi-aspect contrastive learning strategy that decomposes clinical narratives into knowledge-enhanced sub-texts through large language models, (2) a fine-grained alignment mechanism that connects subcaptions with diagnostically relevant image features, and (3) a diagnosis-guided weighting scheme that adaptively prioritizes different sub-captions based on clinical significance prior. Through pretraining on 403,563 dermatological image-text pairs collected from education resources, MAKE significantly outperforms state-of-the-art VLP models on eight datasets across zero-shot skin disease classification, concept annotation, and cross-modal retrieval tasks. Our code will be made publicly available at https: &#x2F;&#x2F;github.com&#x2F;SiyuanYan1&#x2F;MAKE. </p>
<blockquote>
<p>çš®è‚¤ç§‘è¯Šæ–­æ˜¯ä¸€é¡¹å¤æ‚çš„è·¨å­¦ç§‘æŒ‘æˆ˜ï¼Œéœ€è¦æ•´åˆè§†è§‰ç‰¹å¾ä¸ä¸“ä¸šçš„ä¸´åºŠçŸ¥è¯†ã€‚è™½ç„¶è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰å·²ç»æ¨åŠ¨äº†åŒ»ç–—äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œä½†å…¶åœ¨çš®è‚¤ç§‘é¢†åŸŸçš„åº”ç”¨å—é™äºæ–‡æœ¬é•¿åº¦å’Œç¼ºä¹ç»“æ„åŒ–æ–‡æœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MAKEï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬çš®è‚¤ç§‘ä»»åŠ¡çš„å…¨æ–¹ä½çŸ¥è¯†å¢å¼ºè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬è®¤è¯†åˆ°å…¨é¢çš„çš®è‚¤ç§‘æè¿°éœ€è¦æ¶‰åŠå¤šä¸ªçŸ¥è¯†æ–¹é¢ï¼Œè¶…å‡ºäº†æ ‡å‡†æ–‡æœ¬é™åˆ¶ï¼Œå› æ­¤æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä»¥ä¸‹è¦ç´ ï¼šï¼ˆ1ï¼‰ä¸€ç§å¤šè§†è§’å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å°†ä¸´åºŠå™äº‹åˆ†è§£ä¸ºçŸ¥è¯†å¢å¼ºçš„å­æ–‡æœ¬ï¼›ï¼ˆ2ï¼‰ä¸€ç§ç²¾ç»†å¯¹é½æœºåˆ¶ï¼Œå°†å­æ ‡é¢˜ä¸è¯Šæ–­ç›¸å…³çš„å›¾åƒç‰¹å¾è”ç³»èµ·æ¥ï¼›ï¼ˆ3ï¼‰ä¸€ç§ä»¥è¯Šæ–­ä¸ºå¯¼å‘çš„åŠ æƒæ–¹æ¡ˆï¼Œæ ¹æ®ä¸´åºŠé‡è¦æ€§å…ˆéªŒè‡ªé€‚åº”åœ°ä¼˜å…ˆå¤„ç†ä¸åŒçš„å­æ ‡é¢˜ã€‚é€šè¿‡å¯¹æ¥è‡ªæ•™è‚²èµ„æºçš„403,563å¼ çš®è‚¤ç§‘å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼ŒMAKEåœ¨é›¶æ ·æœ¬çš®è‚¤ç—…åˆ†ç±»ã€æ¦‚å¿µæ ‡æ³¨å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šçš„å…«ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°çš„VLPæ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/SiyuanYan1/MAKE%E4%B8%8A%E5%85%AC%E5%BC%BA%E6%8F%A0%E4%BB%A5%E9%BB%BE%E7%BB%BE%E5%AD%A6%E6%9C%AF%E5%B7%A5%E4%BD%BF%E7%9D%A1%E7%9C%BC%E7%9A%84%E5%B7%A5%E4%BD%BF%E7%BF%BB">https://github.com/SiyuanYan1/MAKEä¸Šå…¬å¼€æä¾›ã€‚</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09372v1">PDF</a> MICCAI2025 early acceptance; First two authors contribute equally</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMAKEçš„å¤šæ–¹é¢çŸ¥è¯†å¢å¼ºè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºé›¶æ ·æœ¬çš®è‚¤ç—…å­¦ä»»åŠ¡ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»Ÿè§†è§‰è¯­è¨€é¢„è®­ç»ƒåœ¨çš®è‚¤ç—…å­¦åº”ç”¨ä¸Šçš„å±€é™æ€§ï¼ŒåŒ…æ‹¬æ–‡æœ¬é•¿åº¦çº¦æŸå’Œç¼ºä¹ç»“æ„åŒ–æ–‡æœ¬çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å¤šæ–¹é¢å¯¹æ¯”å­¦ä¹ ç­–ç•¥ã€ç²¾ç»†å¯¹é½æœºåˆ¶å’Œè¯Šæ–­å¼•å¯¼åŠ æƒæ–¹æ¡ˆï¼ŒMAKEåœ¨é›¶æ ·æœ¬çš®è‚¤ç–¾ç—…åˆ†ç±»ã€æ¦‚å¿µæ ‡æ³¨å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çš®è‚¤ç—…å­¦è¯Šæ–­æ˜¯ä¸€ä¸ªå¤æ‚çš„è·¨æ¨¡æ€æŒ‘æˆ˜ï¼Œéœ€è¦æ•´åˆè§†è§‰ç‰¹å¾å’Œä¸´åºŠä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰åœ¨åŒ»ç–—AIä¸­çš„åº”ç”¨å·²æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨çš®è‚¤ç—…å­¦é¢†åŸŸä»é¢ä¸´æ–‡æœ¬é•¿åº¦å’Œç»“æ„åŒ–æ–‡æœ¬çš„å±€é™æ€§ã€‚</li>
<li>MAKEæ¡†æ¶å¼•å…¥å¤šæ–¹é¢çŸ¥è¯†å¢å¼ºå­æ–‡æœ¬åˆ†è§£ç­–ç•¥ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å°†ä¸´åºŠå™è¿°åˆ†è§£ä¸ºå¤šä¸ªçŸ¥è¯†æ–¹é¢ã€‚</li>
<li>MAKEä½¿ç”¨ç²¾ç»†å¯¹é½æœºåˆ¶å°†å­æ ‡é¢˜ä¸è¯Šæ–­ç›¸å…³çš„å›¾åƒç‰¹å¾ç›¸è¿ã€‚</li>
<li>è¯Šæ–­å¼•å¯¼åŠ æƒæ–¹æ¡ˆèƒ½å¤Ÿè‡ªé€‚åº”åœ°æ ¹æ®ä¸´åºŠé‡è¦æ€§ä¼˜å…ˆæ’åºä¸åŒçš„å­æ ‡é¢˜ã€‚</li>
<li>åœ¨æ•™è‚²èµ„æºæ”¶é›†çš„403,563å¼ çš®è‚¤ç—…å­¦å›¾åƒæ–‡æœ¬å¯¹ä¸Šè¿›è¡Œçš„é¢„è®­ç»ƒä½¿MAKEæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›çš„VLPæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d50a7088c56521db9866b1f0b7f2452f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-053b1d505cdab9a48db06c4b860d128b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4fe558bfae17ab207940c442e420f5f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RAG-Enabled-Intent-Reasoning-for-Application-Network-Interaction"><a href="#RAG-Enabled-Intent-Reasoning-for-Application-Network-Interaction" class="headerlink" title="RAG-Enabled Intent Reasoning for Application-Network Interaction"></a>RAG-Enabled Intent Reasoning for Application-Network Interaction</h2><p><strong>Authors:Salwa Mostafa, Mohamed K. Abdel-Aziz, Mohammed S. Elbamby, Mehdi Bennis</strong></p>
<p>Intent-based network (IBN) is a promising solution to automate network operation and management. IBN aims to offer human-tailored network interaction, allowing the network to communicate in a way that aligns with the network usersâ€™ language, rather than requiring the network users to understand the technical language of the network&#x2F;devices. Nowadays, different applications interact with the network, each with its own specialized needs and domain language. Creating semantic languages (i.e., ontology-based languages) and associating them with each application to facilitate intent translation lacks technical expertise and is neither practical nor scalable. To tackle the aforementioned problem, we propose a context-aware AI framework that utilizes machine reasoning (MR), retrieval augmented generation (RAG), and generative AI technologies to interpret intents from different applications and generate structured network intents. The proposed framework allows for generalized&#x2F;domain-specific intent expression and overcomes the drawbacks of large language models (LLMs) and vanilla-RAG framework. The experimental results show that our proposed intent-RAG framework outperforms the LLM and vanilla-RAG framework in intent translation. </p>
<blockquote>
<p>åŸºäºæ„å›¾çš„ç½‘ç»œï¼ˆIBNï¼‰æ˜¯è‡ªåŠ¨åŒ–ç½‘ç»œæ“ä½œå’Œç®¡ç†çš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚IBNæ—¨åœ¨æä¾›ç¬¦åˆäººç±»éœ€æ±‚çš„ç½‘ç»œäº¤äº’ï¼Œè®©ç½‘ç»œèƒ½å¤Ÿä»¥ä¸ç”¨æˆ·è¯­è¨€ç›¸ç¬¦çš„æ–¹å¼è¿›è¡Œæ²Ÿé€šï¼Œè€Œæ— éœ€ç½‘ç»œç”¨æˆ·ç†è§£ç½‘ç»œ&#x2F;è®¾å¤‡çš„æŠ€æœ¯è¯­è¨€ã€‚å¦‚ä»Šï¼Œä¸åŒçš„åº”ç”¨ç¨‹åºä¸ç½‘ç»œè¿›è¡Œäº¤äº’ï¼Œå„æœ‰è‡ªå·±çš„ç‰¹æ®Šéœ€æ±‚å’Œé¢†åŸŸè¯­è¨€ã€‚åˆ›å»ºè¯­ä¹‰è¯­è¨€ï¼ˆä¾‹å¦‚åŸºäºæœ¬ä½“è®ºçš„è¯­è¨€ï¼‰å¹¶ä¸æ¯ä¸ªåº”ç”¨ç¨‹åºå…³è”ï¼Œä»¥ä¿ƒè¿›æ„å›¾ç¿»è¯‘ï¼Œä½†è¿™ç¼ºä¹æŠ€æœ¯ä¸“ä¸šçŸ¥è¯†ï¼Œæ—¢ä¸å®é™…ä¹Ÿä¸å¯æ‰©å±•ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æœºå™¨æ¨ç†ï¼ˆMRï¼‰ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œç”Ÿæˆå¼AIæŠ€æœ¯çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥AIæ¡†æ¶ï¼Œä»¥è§£é‡Šä¸åŒåº”ç”¨ç¨‹åºçš„æ„å›¾å¹¶ç”Ÿæˆç»“æ„åŒ–ç½‘ç»œæ„å›¾ã€‚æ‰€æå‡ºçš„æ¡†æ¶å…è®¸é€šç”¨&#x2F;ç‰¹å®šé¢†åŸŸçš„æ„å›¾è¡¨è¾¾ï¼Œå¹¶å…‹æœäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¸¸è§„RAGæ¡†æ¶çš„ç¼ºç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„åŸºäºæ„å›¾çš„RAGæ¡†æ¶åœ¨æ„å›¾ç¿»è¯‘æ–¹é¢ä¼˜äºLLMå’Œå¸¸è§„RAGæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09339v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºæ„å›¾çš„ç½‘ç»œï¼ˆIBNï¼‰é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„AIæŠ€æœ¯ï¼Œå¦‚æœºå™¨æ¨ç†ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œç”Ÿæˆå¼AIæŠ€æœ¯ï¼Œå®ç°äº†ç½‘ç»œæ“ä½œçš„è‡ªåŠ¨åŒ–ç®¡ç†ï¼Œå¹¶æä¾›äº†äººæ€§åŒ–çš„ç½‘ç»œäº¤äº’ä½“éªŒã€‚é’ˆå¯¹ä¸åŒåº”ç”¨çš„éœ€æ±‚å’Œé¢†åŸŸè¯­è¨€ï¼ŒIBNèƒ½å¤Ÿè§£æå¹¶ç”Ÿæˆç»“æ„åŒ–ç½‘ç»œæ„å›¾ï¼Œæé«˜äº†æ„å›¾ç¿»è¯‘çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>IBNè‡´åŠ›äºå®ç°ç½‘ç»œæ“ä½œçš„è‡ªåŠ¨åŒ–å’Œç®¡ç†ï¼Œå¹¶æ³¨é‡äººæ€§åŒ–çš„ç½‘ç»œäº¤äº’ä½“éªŒã€‚</li>
<li>IBNé€šè¿‡ä½¿ç”¨å…ˆè¿›çš„AIæŠ€æœ¯è§£æå’Œç”Ÿæˆç»“æ„åŒ–ç½‘ç»œæ„å›¾ï¼Œæ»¡è¶³ä¸åŒåº”ç”¨çš„éœ€æ±‚å’Œé¢†åŸŸè¯­è¨€ã€‚</li>
<li>åˆ›å»ºä¸€ä¸ªåŸºäºè¯­ä¹‰çš„è¯­è¨€ï¼ˆå¦‚åŸºäºæœ¬ä½“è®ºçš„è¯­è¨€ï¼‰ä¸æ¯ä¸ªåº”ç”¨ç¨‹åºç›¸å…³è”ä»¥ä¿ƒè¿›æ„å›¾ç¿»è¯‘æ˜¯ä¸€ä¸ªä¸åˆ‡å®é™…ä¸”ä¸å¯æ‰©å±•çš„æ–¹æ³•ã€‚</li>
<li>ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ™®é€šçš„RAGæ¡†æ¶ç›¸æ¯”ï¼Œæå‡ºçš„åŸºäºæ„å›¾çš„RAGæ¡†æ¶åœ¨æ„å›¾ç¿»è¯‘æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>IBNçš„ç›®æ ‡æ˜¯è®©ç½‘ç»œä»¥ä¸ç”¨æˆ·è¯­è¨€ç›¸ç¬¦çš„æ–¹å¼ä¸ç”¨æˆ·è¿›è¡Œäº¤äº’ï¼Œè€Œä¸æ˜¯è¦æ±‚ç”¨æˆ·ç†è§£ç½‘ç»œ&#x2F;è®¾å¤‡çš„ä¸“ä¸šæŠ€æœ¯è¯­è¨€ã€‚</li>
<li>åŸºäºæ„å›¾çš„ç½‘ç»œäº¤äº’æé«˜äº†ç”¨æˆ·ä½“éªŒå¹¶ç®€åŒ–äº†ç½‘ç»œæ“ä½œå’Œç®¡ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09339">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5d6599c1c84bbb06b7629cf4bca3107.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac136d96cfb3384cd9e1097f3914bb8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed05176deb679ad0b68fdfd4a1eda093.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-674ad10564f9b6c9104f78b37dc00436.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fcb296d3e3d607f1e3cf2204c672af8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Activation-Steering-in-Neural-Theorem-Provers"><a href="#Activation-Steering-in-Neural-Theorem-Provers" class="headerlink" title="Activation Steering in Neural Theorem Provers"></a>Activation Steering in Neural Theorem Provers</h2><p><strong>Authors:Shashank Kirtania</strong></p>
<p>Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½¿ç”¨åƒLeanè¿™æ ·çš„è¯æ˜åŠ©æ‰‹æ¥è¯æ˜å®šç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹è¯æ˜ä¸­çš„ä¸‹ä¸€æ­¥æ—¶é¢ä¸´å›°éš¾ï¼Œè¿™ä½¿å¾—å®è·µè€…ä½¿ç”¨ä¸åŒçš„é‡‡æ ·æŠ€æœ¯æ¥æé«˜LLMçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°LLMèƒ½å¤Ÿé¢„æµ‹æ­£ç¡®çš„ç­–ç•¥ï¼Œä½†åœ¨ä¸€ç»„å€™é€‰ç­–ç•¥ä¸­æ°å½“åœ°å¯¹å…¶è¿›è¡Œæ’åæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™å½±å“äº†æ•´ä½“çš„é€‰æ‹©è¿‡ç¨‹ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éšœç¢ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¿€æ´»å¼•å¯¼æ¥æŒ‡å¯¼LLMçš„å“åº”ï¼Œä»¥æé«˜æ¨ç†æ—¶çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¿€æ´»å¼•å¯¼æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è½»é‡çº§æ›¿ä»£æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›èµ„æºå—é™çš„ç¯å¢ƒæ¥è¯´ç‰¹åˆ«æœ‰ä»·å€¼ï¼Œå¯ä¸“é—¨ç”¨äºå¢å¼ºLLMä¸­çš„å®šç†è¯æ˜èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15507v3">PDF</a> incorrect explanation for a concept, need to revise and update!</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ©ç”¨è¯æ˜åŠ©æ‰‹å¦‚Leanè¿›è¡Œå®šç†è¯æ˜æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹è¯æ˜ä¸­çš„ä¸‹ä¸€æ­¥æ—¶é‡åˆ°å›°éš¾ï¼Œä¿ƒä½¿å®è·µè€…ä½¿ç”¨ä¸åŒçš„é‡‡æ ·æŠ€æœ¯æ¥æå‡LLMsçš„èƒ½åŠ›ã€‚è§‚å¯Ÿåˆ°LLMèƒ½å¤Ÿé¢„æµ‹æ­£ç¡®çš„ç­–ç•¥ï¼Œä½†åœ¨ä¸€ç»„å€™é€‰ç­–ç•¥ä¸­æ°å½“åœ°æ’åºé¢ä¸´æŒ‘æˆ˜ï¼Œå½±å“æ•´ä½“é€‰æ‹©è¿‡ç¨‹ã€‚ä¸ºå…‹æœè¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨æ¿€æ´»å¼•å¯¼æ¥æŒ‡å¯¼LLMsçš„å“åº”ï¼Œæ”¹å–„æ¨ç†æ—¶çš„ç”Ÿæˆã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¿€æ´»å¼•å¯¼ä¸ºå¢å¼ºLLMsåœ¨å®šç†è¯æ˜æ–¹é¢çš„èƒ½åŠ›æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å°¤ä¸ºå®è´µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å®šç†è¯æ˜æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œå¯ä½¿ç”¨è¯æ˜åŠ©æ‰‹å¦‚Leanã€‚</li>
<li>å½“å‰LLMsåœ¨é¢„æµ‹è¯æ˜ä¸­çš„ä¸‹ä¸€æ­¥æ—¶é‡åˆ°å›°éš¾ï¼Œéœ€è¦é‡‡ç”¨é‡‡æ ·æŠ€æœ¯æå‡èƒ½åŠ›ã€‚</li>
<li>LLMèƒ½å¤Ÿé¢„æµ‹æ­£ç¡®çš„ç­–ç•¥ï¼Œä½†æ’åºå€™é€‰ç­–ç•¥æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ¿€æ´»å¼•å¯¼æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯æŒ‡å¯¼LLMsçš„å“åº”ï¼Œæ”¹å–„æ¨ç†æ—¶çš„ç”Ÿæˆã€‚</li>
<li>æ¿€æ´»å¼•å¯¼ä¸ºå¢å¼ºLLMsåœ¨å®šç†è¯æ˜æ–¹é¢çš„èƒ½åŠ›æä¾›äº†è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>æ­¤æ–¹æ³•ç‰¹åˆ«åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ˜¾ç¤ºå‡ºå…¶ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-716c5c8ed71490a9d8a964db1016f178.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-474238b33b66b42214e695ba254a866d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4581620a3f13882decd1445f44e686e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc0a4152e5c8e5a82a0d93eeb46fddb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1410590d77eae4be9cd5a1d827d3be35.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="PSPO-An-Effective-Process-supervised-Policy-Optimization-for-Reasoning-Alignment"><a href="#PSPO-An-Effective-Process-supervised-Policy-Optimization-for-Reasoning-Alignment" class="headerlink" title="PSPO*: An Effective Process-supervised Policy Optimization for Reasoning   Alignment"></a>PSPO*: An Effective Process-supervised Policy Optimization for Reasoning   Alignment</h2><p><strong>Authors:Jiawei Li, Xinyue Liang, Junlong Zhang, Yizhe Yang, Chong Feng, Yang Gao</strong></p>
<p>Process supervision enhances the performance of large language models in reasoning tasks by providing feedback at each step of chain-of-thought reasoning. However, due to the lack of effective process supervision methods, even advanced large language models are prone to logical errors and redundant reasoning. We claim that the effectiveness of process supervision significantly depends on both the accuracy and the length of reasoning chains. Moreover, we identify that these factors exhibit a nonlinear relationship with the overall reward score of the reasoning process. Inspired by these insights, we propose a novel process supervision paradigm, PSPO*, which systematically outlines the workflow from reward model training to policy optimization, and highlights the importance of nonlinear rewards in process supervision. Based on PSPO*, we develop the PSPO-WRS, which considers the number of reasoning steps in determining reward scores and utilizes an adjusted Weibull distribution for nonlinear reward shaping. Experimental results on six mathematical reasoning datasets demonstrate that PSPO-WRS consistently outperforms current mainstream models. </p>
<blockquote>
<p>è¿‡ç¨‹ç›‘ç£é€šè¿‡æä¾›æ€ç»´é“¾æ¨ç†æ¯ä¸€æ­¥çš„åé¦ˆï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æœ‰æ•ˆçš„è¿‡ç¨‹ç›‘ç£æ–¹æ³•ï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿå®¹æ˜“å‡ºç°é€»è¾‘é”™è¯¯å’Œå†—ä½™æ¨ç†ã€‚æˆ‘ä»¬å£°ç§°ï¼Œè¿‡ç¨‹ç›‘ç£çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ¨ç†é“¾çš„å‡†ç¡®æ€§å’Œé•¿åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¤å®šè¿™äº›å› ç´ ä¸æ¨ç†è¿‡ç¨‹çš„æ€»ä½“å¥–åŠ±å¾—åˆ†å‘ˆç°å‡ºéçº¿æ€§å…³ç³»ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è¿‡ç¨‹ç›‘ç£èŒƒå¼PSPO<em>ï¼Œå®ƒç³»ç»Ÿåœ°æ¦‚è¿°äº†ä»å¥–åŠ±æ¨¡å‹è®­ç»ƒåˆ°ç­–ç•¥ä¼˜åŒ–çš„å·¥ä½œæµç¨‹ï¼Œå¹¶å¼ºè°ƒäº†è¿‡ç¨‹ç›‘ç£ä¸­éçº¿æ€§å¥–åŠ±çš„é‡è¦æ€§ã€‚åŸºäºPSPO</em>ï¼Œæˆ‘ä»¬å¼€å‘äº†PSPO-WRSï¼Œå®ƒåœ¨ç¡®å®šå¥–åŠ±åˆ†æ•°æ—¶è€ƒè™‘äº†æ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œå¹¶åˆ©ç”¨è°ƒæ•´åçš„å¨å¸ƒå°”åˆ†å¸ƒè¿›è¡Œéçº¿æ€§å¥–åŠ±å¡‘é€ ã€‚åœ¨å…­ä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPSPO-WRSæŒç»­ä¼˜äºå½“å‰ä¸»æµæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11681v3">PDF</a> Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/DIRECT-BIT/PSPO">https://github.com/DIRECT-BIT/PSPO</a></p>
<p><strong>Summary</strong></p>
<p>è¿‡ç¨‹ç›‘ç£é€šè¿‡åœ¨å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ä»»åŠ¡æ¯ä¸€æ­¥æä¾›åé¦ˆæ¥æå‡å…¶æ€§èƒ½ã€‚ç„¶è€Œç”±äºç¼ºä¹æœ‰æ•ˆçš„è¿‡ç¨‹ç›‘ç£æ–¹æ³•ï¼Œå…ˆè¿›çš„è¯­è¨€æ¨¡å‹ä¹Ÿå®¹æ˜“å‡ºç°é€»è¾‘é”™è¯¯å’Œå†—ä½™æ¨ç†ã€‚æœ¬æ–‡æå‡ºè¿‡ç¨‹ç›‘ç£çš„æœ‰æ•ˆæ€§å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ¨ç†é“¾çš„å‡†ç¡®æ€§å’Œé•¿åº¦ï¼Œå¹¶ä¸”ä¸æ¨ç†è¿‡ç¨‹çš„æ€»ä½“å¥–åŠ±å¾—åˆ†å‘ˆéçº¿æ€§å…³ç³»ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿‡ç¨‹ç›‘ç£èŒƒå¼â€”â€”PSPO*ï¼Œå¹¶åŸºäºæ­¤å¼€å‘å‡ºäº†PSPO-WRSæ¨¡å‹ï¼Œè€ƒè™‘æ¨ç†æ­¥éª¤çš„æ•°é‡æ¥å†³å®šå¥–åŠ±åˆ†æ•°å¹¶åˆ©ç”¨ä¿®æ­£çš„å¨å¸ƒå°”åˆ†å¸ƒè¿›è¡Œéçº¿æ€§å¥–åŠ±å¡‘é€ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…­ä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼ŒPSPO-WRSå§‹ç»ˆä¼˜äºå½“å‰çš„ä¸»æµæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹ç›‘ç£å¯¹äºæå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†ä»»åŠ¡æ€§èƒ½æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>ç¼ºä¹æœ‰æ•ˆçš„è¿‡ç¨‹ç›‘ç£æ–¹æ³•å¯èƒ½å¯¼è‡´è¯­è¨€æ¨¡å‹å‡ºç°é€»è¾‘é”™è¯¯å’Œå†—ä½™æ¨ç†ã€‚</li>
<li>è¿‡ç¨‹ç›‘ç£çš„æœ‰æ•ˆæ€§å–å†³äºæ¨ç†é“¾çš„å‡†ç¡®æ€§å’Œé•¿åº¦ã€‚</li>
<li>æ¨ç†é“¾çš„è¿™äº›å› ç´ ä¸æ•´ä½“å¥–åŠ±å¾—åˆ†ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¿‡ç¨‹ç›‘ç£èŒƒå¼â€”â€”PSPO*ï¼Œå¹¶ä»‹ç»äº†å…¶å·¥ä½œæµç¨‹ã€‚</li>
<li>åŸºäºPSPO*ï¼Œå¼€å‘å‡ºäº†PSPO-WRSæ¨¡å‹ï¼Œè¯¥æ¨¡å‹è€ƒè™‘æ¨ç†æ­¥éª¤æ•°é‡æ¥å†³å®šå¥–åŠ±åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a8b189c4f3e603bbf0c985ebf44837f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-98729f8e6a7b9c270ccf22f72bb58fd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ed7227be5a5248c0d9148533afe66e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8038aac90d251c9e70912ea1beb8f4de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-527b059e0c19d5344b63ad884d9bfac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a6ed85b44a0f629a25edf743c9520ce.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Construction-and-Application-of-Materials-Knowledge-Graph-in-Multidisciplinary-Materials-Science-via-Large-Language-Model"><a href="#Construction-and-Application-of-Materials-Knowledge-Graph-in-Multidisciplinary-Materials-Science-via-Large-Language-Model" class="headerlink" title="Construction and Application of Materials Knowledge Graph in   Multidisciplinary Materials Science via Large Language Model"></a>Construction and Application of Materials Knowledge Graph in   Multidisciplinary Materials Science via Large Language Model</h2><p><strong>Authors:Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Bram Hoex, Haofen Wang, Tong Xie, Wenjie Zhang</strong></p>
<p>Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges to the efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques integrated with large language models to extract and systematically organize a decadeâ€™s worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs. </p>
<blockquote>
<p>ææ–™ç§‘å­¦çš„çŸ¥è¯†å¹¿æ³›åˆ†å¸ƒåœ¨å¹¿æ³›çš„ç§‘å­¦æ–‡çŒ®ä¸­ï¼Œå¯¹æ–°ææ–™çš„æœ‰æ•ˆå‘ç°å’Œæ•´åˆæ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€ä¾èµ–äºæ˜‚è´µä¸”è€—æ—¶çš„å®éªŒæ–¹æ³•ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†å¿«é€Ÿåˆ›æ–°çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œäººå·¥æ™ºèƒ½ä¸ææ–™ç§‘å­¦çš„èåˆå¼€è¾Ÿäº†åŠ é€Ÿå‘ç°è¿‡ç¨‹çš„é€”å¾„ï¼Œå°½ç®¡å®ƒè¿˜è¦æ±‚ç²¾ç¡®æ ‡æ³¨ã€æ•°æ®æå–å’Œä¿¡æ¯å¯è¿½æº¯æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†ææ–™çŸ¥è¯†å›¾è°±ï¼ˆMKGï¼‰ï¼Œå®ƒåˆ©ç”¨å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå–å¹¶ç³»ç»Ÿåœ°æ•´ç†äº†è¿‡å»åå¹´é«˜è´¨é‡çš„ç ”ç©¶å†…å®¹ï¼Œå½¢æˆç»“æ„åŒ–ä¸‰å…ƒç»„ï¼ŒåŒ…å«162,605ä¸ªèŠ‚ç‚¹å’Œ731,772æ¡è¾¹ã€‚MKGå°†ä¿¡æ¯åˆ†ç±»ä¸ºå…¨é¢çš„æ ‡ç­¾ï¼Œå¦‚åç§°ã€å…¬å¼å’Œåº”ç”¨ï¼Œå›´ç»•ç²¾å¿ƒè®¾è®¡çš„æœ¬ä½“è¿›è¡Œç»“æ„åŒ–è®¾è®¡ï¼Œä»è€Œæé«˜äº†æ•°æ®çš„å¯ç”¨æ€§å’Œé›†æˆåº¦ã€‚é€šè¿‡å®æ–½åŸºäºç½‘ç»œçš„ç®—æ³•ï¼ŒMKGä¸ä»…æœ‰åŠ©äºé«˜æ•ˆé“¾æ¥é¢„æµ‹ï¼Œè€Œä¸”æ˜¾è‘—å‡å°‘å¯¹ä¼ ç»Ÿå®éªŒæ–¹æ³•çš„ä¾èµ–ã€‚è¿™ç§ç»“æ„åŒ–æ–¹æ³•ä¸ä»…ç®€åŒ–äº†ææ–™ç ”ç©¶ï¼Œè€Œä¸”ä¸ºæ›´å¤æ‚çš„ç§‘å­¦çŸ¥è¯†å›¾è°±å¥ å®šäº†åŸºçŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.03080v4">PDF</a> 14 pages, 7 figures, 3 tables; Accepted by 38th Conference on Neural   Information Processing Systems (NeurIPS 2024)</p>
<p><strong>Summary</strong>ï¼šææ–™ç§‘å­¦çŸ¥è¯†å¹¿æ³›åˆ†æ•£åœ¨å¤§é‡çš„ç§‘å­¦æ–‡çŒ®ä¸­ï¼Œç»™æ–°ææ–™çš„å‘ç°å’Œæ•´åˆå¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€ä¾èµ–äºæ˜‚è´µä¸”è€—æ—¶çš„å®éªŒæ–¹æ³•ï¼Œè¿›ä¸€æ­¥åŠ å‰§äº†å¿«é€Ÿåˆ›æ–°çš„å¤æ‚æ€§ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œäººå·¥æ™ºèƒ½ä¸ææ–™ç§‘å­¦çš„èåˆä¸ºåŠ é€Ÿå‘ç°è¿‡ç¨‹æ‰“å¼€äº†é€”å¾„ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦ç²¾ç¡®æ ‡æ³¨ã€æ•°æ®æå–å’Œä¿¡æ¯çš„å¯è¿½æº¯æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ææ–™çŸ¥è¯†å›¾è°±ï¼ˆMKGï¼‰ï¼Œå®ƒåˆ©ç”¨å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ä¸å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆï¼Œæå–å¹¶ç³»ç»Ÿåœ°ç»„ç»‡äº†è¿‡å»åå¹´é«˜è´¨é‡çš„ç ”ç©¶å†…å®¹ï¼Œå½¢æˆç»“æ„åŒ–çš„ä¸‰å…ƒç»„ï¼ŒåŒ…å«162,605ä¸ªèŠ‚ç‚¹å’Œ731,772æ¡è¾¹ã€‚MKGé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æœ¬ä½“ï¼Œå°†ä¿¡æ¯åˆ†ç±»ä¸ºåç§°ã€å…¬å¼ã€åº”ç”¨ç­‰ç»¼åˆæ ‡ç­¾ï¼Œæé«˜äº†æ•°æ®çš„å¯ç”¨æ€§å’Œæ•´åˆæ€§ã€‚é€šè¿‡å®æ–½ç½‘ç»œç®—æ³•ï¼ŒMKGä¸ä»…ä¿ƒè¿›äº†æœ‰æ•ˆçš„é“¾æ¥é¢„æµ‹ï¼Œè€Œä¸”å¤§å¤§å‡å°‘äº†å¯¹ä¼ ç»Ÿå®éªŒæ–¹æ³•çš„ä¾èµ–ã€‚è¿™ç§ç»“æ„åŒ–æ–¹æ³•ä¸ä»…ç®€åŒ–äº†ææ–™ç ”ç©¶ï¼Œè€Œä¸”ä¸ºæ›´å¤æ‚çš„ç§‘å­¦çŸ¥è¯†å›¾è°±å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ææ–™ç§‘å­¦çŸ¥è¯†å¹¿æ³›åˆ†æ•£ï¼Œé«˜æ•ˆå‘ç°å’Œæ•´åˆæ–°ææ–™é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå®éªŒæ–¹æ³•æ—¢è€—æ—¶åˆæ˜‚è´µï¼Œé™åˆ¶å¿«é€Ÿåˆ›æ–°ã€‚</li>
<li>äººå·¥æ™ºèƒ½ä¸ææ–™ç§‘å­¦çš„ç»“åˆä¸ºåŠ é€Ÿææ–™å‘ç°æä¾›äº†æ–°çš„é€”å¾„ã€‚</li>
<li>ææ–™çŸ¥è¯†å›¾è°±ï¼ˆMKGï¼‰åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œç³»ç»Ÿåœ°ç»„ç»‡å’Œæå–ææ–™ç§‘å­¦é¢†åŸŸçš„é«˜è´¨é‡ç ”ç©¶ã€‚</li>
<li>MKGé€šè¿‡ç»“æ„åŒ–çš„ä¸‰å…ƒç»„å½¢å¼å­˜å‚¨ä¿¡æ¯ï¼ŒåŒ…å«å¤§é‡çš„èŠ‚ç‚¹å’Œè¾¹ï¼Œå½¢æˆç½‘ç»œç»“æ„ã€‚</li>
<li>MKGé€šè¿‡æœ¬ä½“å°†ä¿¡æ¯åˆ†ç±»ï¼Œæé«˜æ•°æ®çš„å¯ç”¨æ€§å’Œæ•´åˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.03080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-98e9ea2d6df314830f4da943c23602a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dbdc10d03794283bb77d98bcace321d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0609a8fc0ddec16d4d2f9a60636f3d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4617fb9fe046410e3a5bf77db0d3f6d4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Detecting-Multimedia-Generated-by-Large-AI-Models-A-Survey"><a href="#Detecting-Multimedia-Generated-by-Large-AI-Models-A-Survey" class="headerlink" title="Detecting Multimedia Generated by Large AI Models: A Survey"></a>Detecting Multimedia Generated by Large AI Models: A Survey</h2><p><strong>Authors:Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu</strong></p>
<p>The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) and beyond detection (adding attributes like generalizability, robustness, and interpretability to detectors). Additionally, we have presented a brief overview of generation mechanisms, public datasets, online detection tools, and evaluation metrics to provide a valuable resource for researchers and practitioners in this field. Most importantly, we offer a focused analysis from a social media perspective to highlight their broader societal impact. Furthermore, we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our aim for this survey is to fill an academic gap and contribute to global AI security efforts, helping to ensure the integrity of information in the digital realm. The project link is <a target="_blank" rel="noopener" href="https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey">https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey</a>. </p>
<blockquote>
<p>å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼‰çš„å¿«é€Ÿå‘å±•æ ‡å¿—ç€ä¸€ä¸ªæ–°æ—¶ä»£çš„æ¥ä¸´ï¼ŒAIç”Ÿæˆçš„å¤šåª’ä½“å†…å®¹è¶Šæ¥è¶Šèå…¥æ—¥å¸¸ç”Ÿæ´»çš„å„ä¸ªæ–¹é¢ã€‚å°½ç®¡è¿™äº›æŠ€æœ¯åœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰ç›Šï¼Œä½†å®ƒä»¬äº§ç”Ÿçš„å¤šåª’ä½“å†…å®¹ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„é£é™©ï¼ŒåŒ…æ‹¬æ½œåœ¨æ»¥ç”¨ã€ç¤¾ä¼šåŠ¨è¡å’Œä¼¦ç†é—®é¢˜ã€‚å› æ­¤ï¼Œæ£€æµ‹ç”±å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿæˆçš„å¤šåª’ä½“å†…å®¹å˜å¾—è‡³å…³é‡è¦ï¼Œç›¸å…³ç ”ç©¶ä¹Ÿæ˜¾è‘—å¢å¤šã€‚å°½ç®¡å¦‚æ­¤ï¼Œå…³äºæ£€æµ‹LAIMç”Ÿæˆçš„å¤šåª’ä½“çš„ç³»ç»Ÿæ€§ç»¼è¿°ä»å­˜åœ¨æ˜æ˜¾å·®è·ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æä¾›äº†ç¬¬ä¸€ç¯‡å…¨é¢æ¶µç›–æ£€æµ‹ç”±å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹åˆ›å»ºçš„å¤šåª’ä½“ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œå¤šæ¨¡æ€å†…å®¹ï¼‰çš„ç°æœ‰ç ”ç©¶çš„ç»¼è¿°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ£€æµ‹æ–¹æ³•è®ºåˆ†ç±»ï¼ŒæŒ‰åª’ä½“æ¨¡æ€åˆ†ç±»ï¼Œå¹¶ä»ä¸¤ä¸ªè§’åº¦è¿›è¡Œå¯¹é½ï¼šçº¯æ£€æµ‹ï¼ˆæ—¨åœ¨æé«˜æ£€æµ‹æ€§èƒ½ï¼‰å’Œè¶…è¶Šæ£€æµ‹ï¼ˆå‘æ£€æµ‹å™¨æ·»åŠ é€šç”¨æ€§ã€ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ç­‰å±æ€§ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¦‚è¿°äº†ç”Ÿæˆæœºåˆ¶ã€å…¬å¼€æ•°æ®é›†ã€åœ¨çº¿æ£€æµ‹å·¥å…·å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ä»ç¤¾äº¤åª’ä½“çš„è§’åº¦è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä»¥å¼ºè°ƒå®ƒä»¬å¯¹ç¤¾ä¼šæ›´å¹¿æ³›çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®å®šäº†å½“å‰æ£€æµ‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ï¼Œè§£å†³æ£€æµ‹ç”±å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿæˆçš„å¤šåª’ä½“æ–¹é¢å°šæœªæ¢ç´¢ã€æ­£åœ¨å‡ºç°å’Œæ–°å‡ºç°çš„é—®é¢˜ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™é¡¹è°ƒæŸ¥å¡«è¡¥å­¦æœ¯ç©ºç™½ï¼Œä¸ºå…¨çƒäººå·¥æ™ºèƒ½å®‰å…¨åŠªåŠ›åšå‡ºè´¡çŒ®ï¼Œå¸®åŠ©ç¡®ä¿æ•°å­—é¢†åŸŸçš„ä¿¡æ¯å®Œæ•´æ€§ã€‚<a target="_blank" rel="noopener" href="https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey">é¡¹ç›®é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.00045v5">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆLAIMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œæ ‡å¿—ç€ä¸€ä¸ªæ–°æ—¶ä»£çš„åˆ°æ¥ã€‚AIç”Ÿæˆçš„å¤šåª’ä½“å†…å®¹æ—¥ç›Šèå…¥æ—¥å¸¸ç”Ÿæ´»çš„å„ä¸ªæ–¹é¢ï¼Œè™½ç„¶ä¸ºä¼—å¤šé¢†åŸŸå¸¦æ¥äº†ç›Šå¤„ï¼Œä½†ä¹Ÿå­˜åœ¨æ½œåœ¨çš„è¯¯ç”¨ã€ç¤¾ä¼šæ‰°ä¹±å’Œä¼¦ç†é—®é¢˜ã€‚å› æ­¤ï¼Œæ£€æµ‹LAIMç”Ÿæˆçš„å¤šåª’ä½“å†…å®¹è‡³å…³é‡è¦ï¼Œç›¸å…³ç ”ç©¶ä¹Ÿè¿…é€Ÿå¢åŠ ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ£€æµ‹LAIMç”Ÿæˆå¤šåª’ä½“çš„ç³»ç»Ÿæ€§ç»¼è¿°ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æä¾›äº†ç¬¬ä¸€ç¯‡å…¨é¢ç»¼è¿°ï¼Œæ¶µç›–äº†æ£€æµ‹LAIMç”Ÿæˆçš„å¤šåª’ä½“ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œå¤šæ¨¡æ€å†…å®¹ï¼‰çš„ç°æœ‰ç ”ç©¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ£€æµ‹æ–¹æ³•çš„åˆ†ç±»æ³•ï¼ŒæŒ‰åª’ä½“æ¨¡æ€åˆ†ç±»ï¼Œå¹¶ä»çº¯ç²¹æ£€æµ‹ï¼ˆæ—¨åœ¨æé«˜æ£€æµ‹æ€§èƒ½ï¼‰å’Œè¶…è¶Šæ£€æµ‹ï¼ˆå‘æ£€æµ‹å™¨æ·»åŠ é€šç”¨æ€§ã€ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ç­‰å±æ€§ï¼‰ä¸¤ä¸ªè§’åº¦è¿›è¡Œä»‹ç»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¦‚è¿°äº†ç”Ÿæˆæœºåˆ¶ã€å…¬å…±æ•°æ®é›†ã€åœ¨çº¿æ£€æµ‹å·¥å…·å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æœ‰ä»·å€¼çš„èµ„æºã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ä»ç¤¾äº¤åª’ä½“çš„è§’åº¦è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä»¥çªå‡ºå…¶æ›´å¹¿æ³›çš„ç¤¾ä¼šå½±å“ã€‚æˆ‘ä»¬è¿˜ç¡®å®šäº†å½“å‰æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ï¼Œä»¥è§£å†³åœ¨æ£€æµ‹LAIMç”Ÿæˆçš„å¤šåª’ä½“æ–¹é¢å°šæœªæ¢ç´¢ã€æ­£åœ¨è¿›è¡Œå’Œæ–°å…´çš„é—®é¢˜ã€‚æœ¬ç»¼è¿°æ—¨åœ¨å¡«è¡¥å­¦æœ¯ç©ºç™½ï¼Œä¸ºå…¨çƒAIå®‰å…¨åšå‡ºè´¡çŒ®ï¼Œç¡®ä¿æ•°å­—é¢†åŸŸçš„èµ„è®¯å®Œæ•´æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆLAIMsï¼‰çš„å‘å±•æ¨åŠ¨äº†AIç”Ÿæˆå¤šåª’ä½“å†…å®¹çš„æ™®åŠï¼Œæ¶‰åŠå¤šç§åª’ä½“ç±»å‹ã€‚</li>
<li>AIç”Ÿæˆçš„å¤šåª’ä½“åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¹¿æ³›åº”ç”¨å¸¦æ¥äº†æ½œåœ¨çš„è¯¯ç”¨ã€ç¤¾ä¼šæ‰°ä¹±å’Œä¼¦ç†é—®é¢˜ã€‚</li>
<li>æ£€æµ‹LAIMç”Ÿæˆçš„å¤šåª’ä½“å†…å®¹å˜å¾—è‡³å…³é‡è¦ï¼Œç›¸å…³ç ”ç©¶æ­£åœ¨è¿…é€Ÿå¢åŠ ã€‚</li>
<li>å½“å‰å…³äºæ£€æµ‹LAIMç”Ÿæˆå¤šåª’ä½“çš„ç³»ç»Ÿæ€§ç»¼è¿°å­˜åœ¨å·®è·ï¼Œæœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚</li>
<li>ä»‹ç»äº†æ£€æµ‹LAIMç”Ÿæˆå¤šåª’ä½“çš„ç°æœ‰ç ”ç©¶ã€ç”Ÿæˆæœºåˆ¶ã€å…¬å…±æ•°æ®é›†å’Œåœ¨çº¿æ£€æµ‹å·¥å…·ã€‚</li>
<li>ä»ç¤¾äº¤åª’ä½“è§’åº¦åˆ†æäº†LAIMç”Ÿæˆå¤šåª’ä½“çš„å¹¿æ³›ç¤¾ä¼šå½±å“ã€‚</li>
<li>ç¡®å®šäº†å½“å‰æ£€æµ‹çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ï¼Œä»¥è§£å†³æ–°å…´é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.00045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bcf4c64f2c042a2623e4bf31954c857b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4449c26a66fa7efc16665b06f1085c45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67d77441225e003f639f72d551a397c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55ce1bca568eabaeb44b6b66e4bb1a9e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-16/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-16/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-16/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1f2c8423fa863c4270331b0d5e445251.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-16  Streaming Multi-agent Pathfinding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c18bbc9107fbf4dc65c8d96173bbde4e.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-16  MIGRATION-BENCH Repository-Level Code Migration Benchmark from Java 8
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
