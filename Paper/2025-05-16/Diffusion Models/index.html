<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-16  LightLab Controlling Light Sources in Images with Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-243254f4484c2472f04d1cfb478cff57.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-16-更新"><a href="#2025-05-16-更新" class="headerlink" title="2025-05-16 更新"></a>2025-05-16 更新</h1><h2 id="LightLab-Controlling-Light-Sources-in-Images-with-Diffusion-Models"><a href="#LightLab-Controlling-Light-Sources-in-Images-with-Diffusion-Models" class="headerlink" title="LightLab: Controlling Light Sources in Images with Diffusion Models"></a>LightLab: Controlling Light Sources in Images with Diffusion Models</h2><p><strong>Authors:Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, Yedid Hoshen</strong></p>
<p>We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference. </p>
<blockquote>
<p>我们提出了一种简单而有效的基于扩散的方法，用于对图像中的光源进行精细的、参数化的控制。现有的重照明方法要么依赖于多个输入视图在推理时进行逆向渲染，要么无法提供对光线变化的明确控制。我们的方法通过微调一组真实的原始照片对扩散模型，辅以大规模合成渲染的图像，以激发其用于重照明的逼真度先验。我们利用光线的线性合成图像对，展示目标光源或环境照明的受控光线变化。使用此数据和适当的微调方案，我们训练了一个模型，可以对光照强度进行精确控制，并实现明确的色彩控制。最后，我们展示了我们的方法如何实现引人注目的光线编辑结果，并基于用户偏好优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09608v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://nadmag.github.io/LightLab/">https://nadmag.github.io/LightLab/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于扩散的简单而有效的方法，用于对图像中的光源进行精细控制。现有重照明方法要么依赖于多个输入视图进行逆向渲染，要么无法提供对光照变化的明确控制。本文通过微调扩散模型，实现了对光源的精细控制，并展示了令人信服的光编辑结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于扩散的方法，用于图像中的精细光源控制。</li>
<li>现有重照明方法存在逆向渲染和缺乏明确光照控制的问题。</li>
<li>通过微调扩散模型，结合真实照片和合成渲染图像数据，训练模型以实现精确照明变化。</li>
<li>该方法可控制光源的光强度和颜色。</li>
<li>实现了令人信服的光编辑结果。</li>
<li>该方法在用户偏好评估中优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09608">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d85699b12e1aef28040e90d3a1ac50cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ec76ab35e41d9d35151a8e5c79a823d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8719a739ddc63428203dce190b4ad4a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-038db8ae6fa4a8b8030b46d517cf6483.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c871c353072962e245cd3fa1e1845d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5c933fd99e41df869f053640ef0f70c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Don’t-Forget-your-Inverse-DDIM-for-Image-Editing"><a href="#Don’t-Forget-your-Inverse-DDIM-for-Image-Editing" class="headerlink" title="Don’t Forget your Inverse DDIM for Image Editing"></a>Don’t Forget your Inverse DDIM for Image Editing</h2><p><strong>Authors:Guillermo Gomez-Trenado, Pablo Mesejo, Oscar Cordón, Stéphane Lathuilière</strong></p>
<p>The field of text-to-image generation has undergone significant advancements with the introduction of diffusion models. Nevertheless, the challenge of editing real images persists, as most methods are either computationally intensive or produce poor reconstructions. This paper introduces SAGE (Self-Attention Guidance for image Editing) - a novel technique leveraging pre-trained diffusion models for image editing. SAGE builds upon the DDIM algorithm and incorporates a novel guidance mechanism utilizing the self-attention layers of the diffusion U-Net. This mechanism computes a reconstruction objective based on attention maps generated during the inverse DDIM process, enabling efficient reconstruction of unedited regions without the need to precisely reconstruct the entire input image. Thus, SAGE directly addresses the key challenges in image editing. The superiority of SAGE over other methods is demonstrated through quantitative and qualitative evaluations and confirmed by a statistically validated comprehensive user study, in which all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE ranks as the top-performing method in seven out of 10 quantitative analyses and secures second and third places in the remaining three. </p>
<blockquote>
<p>随着扩散模型（diffusion models）的引入，文本到图像生成领域已经取得了重大进展。然而，编辑真实图像的挑战仍然存在，因为大多数方法要么计算量大，要么产生的重建效果差。本文介绍了一种利用预训练的扩散模型进行图像编辑的新型技术——SAGE（Self-Attention Guidance for image Editing）。SAGE基于DDIM算法构建，并融入了一种新型指导机制，该机制利用扩散U-Net的自注意力层。这种机制基于反向DDIM过程中生成的注意力图计算重建目标，无需精确重建整个输入图像，即可有效地重建未编辑区域。因此，SAGE直接解决了图像编辑中的关键挑战。SAGE的优越性通过定量和定性评估以及经过统计验证的综合用户研究得到了证明，在调查的47名用户中，所有用户都认为SAGE优于其他方法。此外，在10项定量分析中有7项排名第一，其余三项分别获得第二和第三名。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09571v1">PDF</a> 12 pages, 12 figures, code available at   <a target="_blank" rel="noopener" href="https://guillermogotre.github.io/sage/">https://guillermogotre.github.io/sage/</a></p>
<p><strong>Summary</strong></p>
<p>文本到图像生成领域在扩散模型的引入下取得了显著进展。然而，编辑真实图像的挑战仍然存在，因为大多数方法要么计算量大，要么产生的重建效果差。本文介绍了SAGE（用于图像编辑的自我注意引导），这是一种利用预训练的扩散模型的新技术。SAGE建立在DDIM算法的基础上，并引入了一种新的引导机制，利用扩散U-Net的自注意层。该机制基于反向DDIM过程中生成的注意力图计算重建目标，能够高效重建未编辑区域，无需精确重建整个输入图像。因此，SAGE直接解决了图像编辑中的关键挑战。SAGE的优越性通过定量和定性评估以及经过统计验证的综合用户研究得到了证明，47名受访用户都更倾向于使用SAGE而非其他方法。此外，在10项定量分析中，SAGE在7项中排名第一，在其余三项中分别获得第二和第三名。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本到图像生成领域取得了显著进展。</li>
<li>现有图像编辑方法面临计算量大或重建效果差的挑战。</li>
<li>SAGE技术利用预训练的扩散模型进行图像编辑。</li>
<li>SAGE建立在DDIM算法之上，并引入自我注意引导机制。</li>
<li>SAGE通过注意力图计算重建目标，实现高效区域重建。</li>
<li>SAGE在多项定量分析和用户研究中表现出优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09571">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6b58f1ba7d833c9eebad9d87823b77a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73588b5c3e46ce46cb5c0883a78a1c74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eb19656c06ef0b55a9d87f95bb5e7e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0416441bb6c4663c48c079930078712.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6072a163994a9843bae4e02580eb7ce9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0e95aae346c6c41a859965f5afd4d34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f54f52cddd6fef4b60e0297e90cf72d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf8c1672d65f226127d7263199df6a50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-243254f4484c2472f04d1cfb478cff57.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BLIP3-o-A-Family-of-Fully-Open-Unified-Multimodal-Models-Architecture-Training-and-Dataset"><a href="#BLIP3-o-A-Family-of-Fully-Open-Unified-Multimodal-Models-Architecture-Training-and-Dataset" class="headerlink" title="BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,   Training and Dataset"></a>BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,   Training and Dataset</h2><p><strong>Authors:Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, Ran Xu</strong></p>
<p>Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets. </p>
<blockquote>
<p>在多模态模型领域的最新研究中，统一图像理解和生成已经引起了越来越多的关注。虽然图像理解的设计选择已经得到了广泛的研究，但在统一框架中进行图像生成的模型架构和训练策略仍然鲜有研究。受自回归和扩散模型在高质量和可扩展性生成方面的强大潜力的驱动，我们对它们在统一多模态设置中的使用进行了全面研究，重点研究图像表示、建模目标和训练策略。基于这些研究，我们介绍了一种新方法，该方法采用扩散变压器生成语义丰富的CLIP图像特征，这与传统的基于VAE的表示形成对比。这种设计不仅提高了训练效率，还提高了生成质量。此外，我们证明了统一模型的顺序预训练策略（首先在图像理解上进行训练，随后在图像生成上进行训练）具有实用价值，能够在保持图像理解能力的同时，发展强大的图像生成能力。最后，我们通过使用涵盖各种场景、物体、人类动作等的多样化描述来精心创建高质量指令调整数据集BLIP3o-60k，为GPT-4o提供提示来进行图像生成的训练。基于我们创新的模型设计、训练配方和数据集，我们开发了一系列最先进的统一多模态模型BLIP3-o。BLIP3-o在涵盖图像理解和生成任务的大多数流行基准测试中实现了卓越的性能。为了促进未来的研究，我们全面开源我们的模型，包括代码、模型权重、训练脚本以及预训练和指令调整数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09568v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了多模态模型中的图像理解和生成技术，对采用扩散模型和自回归模型的统一框架进行了深入研究，并介绍了新的图像特征生成方法。通过采用扩散变压器生成语义丰富的CLIP图像特征，提高了训练效率和生成质量。此外，文章还提出了一种序贯预训练策略，并开发了一系列最先进的统一多模态模型BLIP3-o，该模型在图像理解和生成任务上的表现均达到或超越了现有水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究关注多模态模型中图像理解和生成的结合。</li>
<li>探讨了采用扩散模型和自回归模型的统一框架。</li>
<li>引入了一种新的图像特征生成方法，使用扩散变压器生成语义丰富的CLIP图像特征。</li>
<li>提出了序贯预训练策略，先训练图像理解，再训练图像生成，以提高模型的实用性能。</li>
<li>开发了一系列先进的统一多模态模型BLIP3-o。</li>
<li>BLIP3-o模型在图像理解和生成任务上的表现均达到或超越了现有水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09568">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33aa03fbae588858ae133c1cf4caa04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1c59a7596173a94015358ec00fb8b6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37d252eca6eae72c28ee6460d613aa7e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Marigold-Affordable-Adaptation-of-Diffusion-Based-Image-Generators-for-Image-Analysis"><a href="#Marigold-Affordable-Adaptation-of-Diffusion-Based-Image-Generators-for-Image-Analysis" class="headerlink" title="Marigold: Affordable Adaptation of Diffusion-Based Image Generators for   Image Analysis"></a>Marigold: Affordable Adaptation of Diffusion-Based Image Generators for   Image Analysis</h2><p><strong>Authors:Bingxin Ke, Kevin Qu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, Konrad Schindler</strong></p>
<p>The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models’ ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model’s architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: <a target="_blank" rel="noopener" href="https://marigoldcomputervision.github.io/">https://marigoldcomputervision.github.io</a> </p>
<blockquote>
<p>过去十年，深度学习在计算机视觉领域的成功主要依赖于大规模有标注数据集和强大的预训练模型。在数据稀缺的情况下，这些预训练模型的质量对于有效的迁移学习至关重要。图像分类和自监督学习一直是预训练CNN和基于transformer架构的传统主要方法。最近，文本到图像生成模型的兴起，特别是那些在潜在空间使用去噪扩散的模型，引入了一类新的基础模型，这些模型在大型有字幕图像数据集上进行训练。这些模型生成未见内容的现实图像的能力表明它们对视觉世界有深入理解。在这项工作中，我们提出了Marigold，这是一系列条件生成模型和微调协议，它从预训练的潜在扩散模型（如Stable Diffusion）中提取知识，并适应密集图像分析任务，包括单眼深度估计、表面法线预测和内在分解。Marigold只需对预训练的潜在扩散模型的架构进行最小修改，使用小型合成数据集在单个GPU上进行几天的训练，并展示了零样本迁移的卓越泛化能力。项目页面：<a target="_blank" rel="noopener" href="https://marigoldcomputervision.github.io/">https://marigoldcomputervision.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09358v1">PDF</a> Journal extension of our CVPR 2024 paper, featuring new tasks,   improved efficiency, high-resolution capabilities, and enhanced accessibility</p>
<p><strong>Summary</strong><br>     深度学习在计算机视觉领域的成功，得益于大规模标注数据集和强大的预训练模型。在数据稀缺的情况下，预训练模型的质量对迁移学习至关重要。近期文本到图像生成模型，特别是潜在空间去噪扩散模型的兴起，引入了一种新的基础模型，该模型在大型带字幕的图像数据集上进行训练。本文介绍Marigold，这是一系列条件生成模型和微调协议，它从预训练的潜在扩散模型如Stable Diffusion中提取知识，并适应密集图像分析任务，包括单眼深度估计、表面法线预测和内在分解。Marigold对预训练的潜在扩散模型的架构进行了最小的修改，使用小型合成数据集在单个GPU上进行几天的训练，并展示了零样本迁移学习的最新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在计算机视觉领域的成功依赖于大规模标注数据集和强大的预训练模型。</li>
<li>在数据稀缺的环境中，预训练模型的质量对迁移学习至关重要。</li>
<li>文本到图像生成模型的兴起，特别是潜在空间去噪扩散模型，为计算机视觉领域引入了新的基础模型。</li>
<li>Marigold是一种条件生成模型和微调协议，能够从预训练的潜在扩散模型中提取知识并适应密集图像分析任务。</li>
<li>Marigold具有最小的架构修改需求，使用小型合成数据集在单个GPU上进行训练。</li>
<li>Marigold展示了零样本迁移学习的最新水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09358">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9d3de22ae0cbf338a3345997508b1b2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b611e75ea40d8505a66ea37f1d329e4b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64ca6acc61ce14531551d6ca4a3f2400.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generating-time-consistent-dynamics-with-discriminator-guided-image-diffusion-models"><a href="#Generating-time-consistent-dynamics-with-discriminator-guided-image-diffusion-models" class="headerlink" title="Generating time-consistent dynamics with discriminator-guided image   diffusion models"></a>Generating time-consistent dynamics with discriminator-guided image   diffusion models</h2><p><strong>Authors:Philipp Hess, Maximilian Gelbrecht, Christof Schötz, Michael Aich, Yu Huang, Shangshang Yang, Niklas Boers</strong></p>
<p>Realistic temporal dynamics are crucial for many video generation, processing and modelling applications, e.g. in computational fluid dynamics, weather prediction, or long-term climate simulations. Video diffusion models (VDMs) are the current state-of-the-art method for generating highly realistic dynamics. However, training VDMs from scratch can be challenging and requires large computational resources, limiting their wider application. Here, we propose a time-consistency discriminator that enables pretrained image diffusion models to generate realistic spatiotemporal dynamics. The discriminator guides the sampling inference process and does not require extensions or finetuning of the image diffusion model. We compare our approach against a VDM trained from scratch on an idealized turbulence simulation and a real-world global precipitation dataset. Our approach performs equally well in terms of temporal consistency, shows improved uncertainty calibration and lower biases compared to the VDM, and achieves stable centennial-scale climate simulations at daily time steps. </p>
<blockquote>
<p>现实的时间动态对于许多视频生成、处理和建模应用至关重要，例如在计算流体动力学、天气预报或长期气候模拟中。视频扩散模型（VDM）是目前生成高度真实动态的最先进方法。然而，从头训练VDM可能具有挑战性，并需要巨大的计算资源，这限制了其更广泛的应用。在这里，我们提出了一种时间一致性鉴别器，它可以让预训练的图像扩散模型生成逼真的时空动态。鉴别器引导采样推理过程，并不需要对图像扩散模型进行扩展或微调。我们将我们的方法与在理想化的湍流模拟和真实世界全球降水数据集上从头训练的VDM进行比较。我们的方法在时间一致性方面表现同样出色，与VDM相比，显示出改进的不确定性校准和更低的偏差，并在每日时间步长下实现稳定世纪尺度的气候模拟。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09089v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了视频扩散模型（VDMs）在生成高度逼真的动态视频中的重要性及其面临的挑战。为应对这些挑战，本文提出了一种时间一致性鉴别器，该鉴别器可使预训练的图像扩散模型生成逼真的时空动态。该鉴别器能够引导采样推理过程，且无需扩展或微调图像扩散模型。通过对比实验，本文方法在时间一致性方面表现优异，不确定性校准和偏差改进明显，相较于VDM更加稳定地实现了按日常时间步长进行世纪规模的气候模拟。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频扩散模型（VDMs）是生成高度逼真的动态视频的最新技术。</li>
<li>训练VDMs存在挑战，需要大量的计算资源。</li>
<li>提出了一种时间一致性鉴别器，使预训练的图像扩散模型能够生成逼真的时空动态。</li>
<li>时间一致性鉴别器能引导采样推理过程，无需扩展或微调图像扩散模型。</li>
<li>对比实验表明，该方法在时间一致性方面表现优秀。</li>
<li>与VDM相比，该方法在不确定性校准和偏差改进方面有明显优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09089">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e4743307d965ceb69341a67000149d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-174bdc11e2dde549c06d6dfa638c5717.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bced45e780c9ea73d429561736dec284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-156db5c1eb49e129709b6438e64a9ff9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e5e6e1e0267299efa5c597c25afd1fc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Detecting-Multimedia-Generated-by-Large-AI-Models-A-Survey"><a href="#Detecting-Multimedia-Generated-by-Large-AI-Models-A-Survey" class="headerlink" title="Detecting Multimedia Generated by Large AI Models: A Survey"></a>Detecting Multimedia Generated by Large AI Models: A Survey</h2><p><strong>Authors:Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu</strong></p>
<p>The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) and beyond detection (adding attributes like generalizability, robustness, and interpretability to detectors). Additionally, we have presented a brief overview of generation mechanisms, public datasets, online detection tools, and evaluation metrics to provide a valuable resource for researchers and practitioners in this field. Most importantly, we offer a focused analysis from a social media perspective to highlight their broader societal impact. Furthermore, we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our aim for this survey is to fill an academic gap and contribute to global AI security efforts, helping to ensure the integrity of information in the digital realm. The project link is <a target="_blank" rel="noopener" href="https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey">https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey</a>. </p>
<blockquote>
<p>大型人工智能模型（LAIMs）的快速发展，尤其是扩散模型和大语言模型，标志着一个人工智能生成多媒体越来越融入日常生活的时代来临。尽管这些技术在许多领域都带来了好处，但它们也带来了显著的风险，包括潜在误用、社会混乱和伦理问题。因此，检测由LAIM生成的多媒体已经成为至关重要的事情，相关的研究也出现了显著增长。尽管如此，专门针对检测LAIM生成的多媒体的系统性综述仍存在明显差距。为了解决这个问题，我们提供了第一篇全面涵盖现有检测LAIM生成多媒体（如文本、图像、视频、音频和多模态内容）的研究综述。具体来说，我们引入了一种新的检测方法的分类法，按媒体模态分类，并从两个角度进行对齐：纯检测（旨在提高检测性能）和超越检测（向检测器添加通用性、鲁棒性和可解释性等属性）。此外，我们还简要概述了生成机制、公共数据集、在线检测工具和评估指标，为这一领域的研究人员和实践者提供了有价值的资源。最重要的是，我们从社交媒体的角度进行了重点分析，以强调它们对社会的更广泛影响。此外，我们还确定了当前检测面临的挑战，并提出了未来研究的方向，解决在检测由LAIM生成的多媒体方面尚未探索、正在出现和新兴的问题。我们撰写此综述的目的是填补学术空白，为全球人工智能安全努力做出贡献，帮助确保数字领域的信息完整性。项目链接是<a target="_blank" rel="noopener" href="https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey%E3%80%82">https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.00045v5">PDF</a> </p>
<p><strong>摘要</strong><br>扩散模型等大型AI模型（LAIMs）的快速发展为日常生活带来机遇和挑战，这些AI生成的多媒体内容在许多领域有着广泛应用。因此，检测和识别LAIM生成的多媒体内容变得至关重要。本文首次全面综述了关于检测LAIM生成的多媒体内容的研究，包括文本、图像、视频、音频和多模态内容等。文章提出了检测方法的全新分类法，按媒体类型分类，并从纯粹检测（旨在提高检测性能）和超越检测（增加通用性、鲁棒性和可解释性）两个角度进行阐述。此外，文章还概述了生成机制、公开数据集、在线检测工具和评估指标等。更重要的是，文章从社交媒体的角度分析了LAIM的广泛社会影响，并指出了当前面临的挑战和未来研究方向。本文旨在填补学术空白，为AI安全做出贡献，确保数字领域的信息完整性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型AI模型（LAIMs）尤其是扩散模型的快速发展推动了AI生成多媒体内容的普及，广泛应用于日常各方面。</li>
<li>AI生成的多媒体内容面临着潜在的误用、社会混乱和伦理问题等重大风险。</li>
<li>检测LAIM生成的多媒体内容至关重要，但现有的系统综述存在显著缺口。</li>
<li>本文首次全面综述了检测LAIM生成的多媒体内容的研究，涵盖了各种媒体类型。</li>
<li>提出了检测方法的全新分类法，从纯粹检测和超越检测两个角度阐述。</li>
<li>文章还分析了LAIM的社会影响，并指出了当前面临的挑战和未来研究方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.00045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bcf4c64f2c042a2623e4bf31954c857b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4449c26a66fa7efc16665b06f1085c45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67d77441225e003f639f72d551a397c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55ce1bca568eabaeb44b6b66e4bb1a9e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-16/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-16/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8f5d2e549b25260ab6ee45b2743d90b4.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-16  Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using   Implicit Neural Representations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-16/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-63bb0d89c33caa0190081dcec8f8b30d.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-16  Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
