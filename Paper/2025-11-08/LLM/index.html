<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-08  Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought   Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e2171d528cad808762f7cbec2fab90fe')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-08-æ›´æ–°"><a href="#2025-11-08-æ›´æ–°" class="headerlink" title="2025-11-08 æ›´æ–°"></a>2025-11-08 æ›´æ–°</h1><h2 id="Logit-Entropy-Adaptive-Stopping-Heuristic-for-Efficient-Chain-of-Thought-Reasoning"><a href="#Logit-Entropy-Adaptive-Stopping-Heuristic-for-Efficient-Chain-of-Thought-Reasoning" class="headerlink" title="Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought   Reasoning"></a>Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought   Reasoning</h2><p><strong>Authors:Mohammad Atif Quamar, Mohammad Areeb</strong></p>
<p>Chain-of-Thought (CoT) prompting is a key technique for enabling complex reasoning in large language models. However, generating full, fixed-length rationales is computationally wasteful, inflating both token usage and latency. We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free decoding algorithm that adaptively halts rationale generation. LEASH monitors two intrinsic signals: the slope of token-level entropy and the improvement in the top-logit margin. It terminates the generation once both signals plateau, indicating the model has reached a stable reasoning state. Across four instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces average token generation by 30â€“35% and latency by 27%, while incurring a 10 p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no additional training or supervision, offering a simple and efficient alternative to CoT decoding. </p>
<blockquote>
<p>Chain-of-Thought (CoT)æç¤ºæ˜¯å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¤æ‚æ¨ç†çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”Ÿæˆå®Œæ•´ã€å›ºå®šé•¿åº¦çš„ç†ç”±åœ¨è®¡ç®—ä¸Šæ˜¯æµªè´¹çš„ï¼Œå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨é‡å’Œå»¶è¿Ÿã€‚æˆ‘ä»¬å¼•å…¥äº†LEASHï¼šLogit-Entropyè‡ªé€‚åº”åœæ­¢å¯å‘å¼ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç ç®—æ³•ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°ä¸­æ­¢ç†ç”±ç”Ÿæˆã€‚LEASHç›‘æ§ä¸¤ä¸ªå†…åœ¨ä¿¡å·ï¼šä»¤ç‰Œçº§ç†µçš„æ–œç‡å’Œé¡¶çº§logitè¾¹é™…çš„æ”¹å–„ã€‚ä¸€æ—¦è¿™ä¸¤ä¸ªä¿¡å·è¾¾åˆ°å¹³ç¨³çŠ¶æ€ï¼Œå°±ç»ˆæ­¢ç”Ÿæˆï¼Œè¿™è¡¨æ˜æ¨¡å‹å·²ç»è¾¾åˆ°äº†ç¨³å®šçš„æ¨ç†çŠ¶æ€ã€‚åœ¨GSM8Kå’ŒAQuA-RATåŸºå‡†æµ‹è¯•ä¸Šï¼Œå¯¹å››ä¸ªæŒ‡ä»¤è°ƒæ•´è¿‡çš„æ¨¡å‹ä½¿ç”¨LEASHï¼Œå¹³å‡ä»¤ç‰Œç”Ÿæˆé‡å‡å°‘äº†30-35%ï¼Œå»¶è¿Ÿå‡å°‘äº†27%ï¼Œç›¸å¯¹äºCoTï¼Œç²¾åº¦ä¸‹é™äº†10ä¸ªç™¾åˆ†ç‚¹ã€‚LEASHä¸æ¨¡å‹æ— å…³ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–ç›‘ç£ï¼Œä¸ºCoTè§£ç æä¾›äº†ç®€å•é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04654v1">PDF</a> Presented at the 1st Workshop on Efficient Reasoning (NeurIPS 2025)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ˜¯æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤æ‚æ¨ç†çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”Ÿæˆå®Œæ•´ã€å›ºå®šé•¿åº¦çš„ç†ç”±åœ¨è®¡ç®—ä¸Šæ˜¯æµªè´¹çš„ï¼Œå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨é‡å’Œå»¶è¿Ÿã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç ç®—æ³•LEASHï¼ˆLogit-Entropyè‡ªé€‚åº”åœæ­¢å¯å‘å¼ç®—æ³•ï¼‰ï¼Œè¯¥ç®—æ³•å¯è‡ªé€‚åº”åœ°ç»ˆæ­¢ç†ç”±ç”Ÿæˆã€‚LEASHç›‘æ§ä¸¤ä¸ªå†…åœ¨ä¿¡å·ï¼šä»¤ç‰Œçº§åˆ«çš„ç†µæ–œç‡å’Œtop-logitè¾¹é™…æ”¹å–„ã€‚å½“è¿™ä¸¤ä¸ªä¿¡å·è¶‹äºå¹³ç¨³æ—¶ï¼Œå®ƒä¼šç»ˆæ­¢ç”Ÿæˆï¼Œè¡¨æ˜æ¨¡å‹å·²åˆ°è¾¾ç¨³å®šçš„æ¨ç†çŠ¶æ€ã€‚åœ¨GSM8Kå’ŒAQuA-RATåŸºå‡†æµ‹è¯•çš„å››ä¸ªæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸­ï¼ŒLEASHå°†å¹³å‡ä»¤ç‰Œç”Ÿæˆé‡å‡å°‘äº†30-35%ï¼Œå»¶è¿Ÿå‡å°‘äº†27%ï¼Œè€Œç›¸å¯¹äºCoTçš„å‡†ç¡®ç‡ä¸‹é™äº†10ä¸ªç™¾åˆ†ç‚¹ã€‚LEASHå…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–ç›‘ç£ï¼Œä¸ºCoTè§£ç æä¾›äº†ç®€å•é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ˜¯æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†çš„å…³é”®ã€‚</li>
<li>ç”Ÿæˆå®Œæ•´å›ºå®šé•¿åº¦çš„ç†ç”±åœ¨è®¡ç®—ä¸Šæˆæœ¬è¾ƒé«˜ã€‚</li>
<li>LEASHæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç ç®—æ³•ï¼Œå¯è‡ªé€‚åº”ç»ˆæ­¢ç†ç”±ç”Ÿæˆã€‚</li>
<li>LEASHé€šè¿‡ç›‘æ§ä¸¤ä¸ªå†…åœ¨ä¿¡å·ï¼šä»¤ç‰Œçº§åˆ«çš„ç†µæ–œç‡å’Œtop-logitè¾¹é™…æ”¹å–„æ¥å·¥ä½œã€‚</li>
<li>LEASHåœ¨å¤šä¸ªæ¨¡å‹ä¸­å‡å°‘äº†ä»¤ç‰Œç”Ÿæˆå’Œå»¶è¿Ÿã€‚</li>
<li>ç›¸å¯¹äºCoTï¼ŒLEASHçš„å‡†ç¡®ç‡æœ‰æ‰€ä¸‹é™ï¼Œä½†æä¾›äº†ä¸€ç§ç®€å•é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74f315d1ea44a0a21d7f9bb4fc75158f" align="middle">
<img src="https://picx.zhimg.com/v2-718cb559503d928414ae8009d808d62c" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="When-retrieval-outperforms-generation-Dense-evidence-retrieval-for-scalable-fake-news-detection"><a href="#When-retrieval-outperforms-generation-Dense-evidence-retrieval-for-scalable-fake-news-detection" class="headerlink" title="When retrieval outperforms generation: Dense evidence retrieval for   scalable fake news detection"></a>When retrieval outperforms generation: Dense evidence retrieval for   scalable fake news detection</h2><p><strong>Authors:Alamgir Munir Qazi, John P. McCrae, Jamal Abdul Nasir</strong></p>
<p>The proliferation of misinformation necessitates robust yet computationally efficient fact verification systems. While current state-of-the-art approaches leverage Large Language Models (LLMs) for generating explanatory rationales, these methods face significant computational barriers and hallucination risks in real-world deployments. We present DeReC (Dense Retrieval Classification), a lightweight framework that demonstrates how general-purpose text embeddings can effectively replace autoregressive LLM-based approaches in fact verification tasks. By combining dense retrieval with specialized classification, our system achieves better accuracy while being significantly more efficient. DeReC outperforms explanation-generating LLMs in efficiency, reducing runtime by 95% on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92% on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds), showcasing its effectiveness across varying dataset sizes. On the RAWFC dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art method L-Defense (61.20%). Our results demonstrate that carefully engineered retrieval-based systems can match or exceed LLM performance in specialized tasks while being significantly more practical for real-world deployment. </p>
<blockquote>
<p>è™šå‡ä¿¡æ¯çš„æ³›æ»¥è¿«åˆ‡éœ€è¦å¼ºå¤§ä¸”è®¡ç®—æ•ˆç‡é«˜çš„äº‹å®æ ¸æŸ¥ç³»ç»Ÿã€‚è™½ç„¶å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè§£é‡Šæ€§ç†ç”±ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­é¢ä¸´ç€é‡å¤§çš„è®¡ç®—éšœç¢å’Œå¹»è§†é£é™©ã€‚æˆ‘ä»¬æå‡ºäº†DeReCï¼ˆå¯†é›†æ£€ç´¢åˆ†ç±»ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ–‡æœ¬åµŒå…¥çš„è½»é‡çº§æ¡†æ¶ï¼Œå®ƒå±•ç¤ºäº†å¦‚ä½•åœ¨äº‹å®æ ¸æŸ¥ä»»åŠ¡ä¸­æœ‰æ•ˆåœ°æ›¿ä»£åŸºäºè‡ªå›å½’çš„LLMæ–¹æ³•ã€‚é€šè¿‡å¯†é›†æ£€ç´¢ä¸ä¸“é¡¹åˆ†ç±»ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å®ç°æ›´é«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ•ˆç‡ä¹Ÿæ˜¾è‘—æé«˜ã€‚DeReCåœ¨æ•ˆç‡ä¸Šè¶…è¶Šäº†ç”Ÿæˆè§£é‡Šæ€§ç†ç”±çš„LLMï¼Œåœ¨RAWFCä¸Šè¿è¡Œæ—¶ç¼©çŸ­äº†95%ï¼ˆä»4å°æ—¶36åˆ†é’Ÿå‡å°‘åˆ°45åˆ†é’Ÿï¼‰å’Œåœ¨LIAR-RAWä¸Šç¼©çŸ­äº†92%ï¼ˆä»çº¦ä¸‰å°æ—¶å‡å°‘åˆ°çº¦ä¸¤å°æ—¶ï¼‰ï¼Œå±•ç°äº†å…¶åœ¨ä¸åŒæ•°æ®é›†å¤§å°ä¸Šçš„æœ‰æ•ˆæ€§ã€‚åœ¨RAWFCæ•°æ®é›†ä¸Šï¼ŒDeReCçš„F1åˆ†æ•°è¾¾åˆ°65.58%ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„L-Defenseæ–¹æ³•ï¼ˆä»…å¾—åˆ†äºæ ¹æ®æˆ‘æ‰€è·å–çš„åŸæ–‡è®°å½•ï¼šä¸ä½¿ç”¨è¯­è¨€çš„å¤„ç†é¢„æµ‹é˜²å¾¡æ£€æµ‹æ—¶å¾—åˆ†61.20%ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„åŸºäºæ£€ç´¢çš„ç³»ç»Ÿå¯ä»¥åœ¨ç‰¹å®šä»»åŠ¡ä¸­ä¸LLMæ€§èƒ½ç›¸åŒ¹é…ç”šè‡³è¶…è¶Šï¼ŒåŒæ—¶åœ¨å®é™…éƒ¨ç½²ä¸­æ›´åŠ å®ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04643v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>éšç€å‡ä¿¡æ¯çš„æ³›æ»¥ï¼Œéœ€è¦å¼€å‘æ—¢ç¨³å¥åˆè®¡ç®—æ•ˆç‡é«˜çš„äº‹å®æ ¸æŸ¥ç³»ç»Ÿã€‚å½“å‰é«˜çº§æ–¹æ³•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè§£é‡Šç†ç”±ï¼Œä½†åœ¨ç°å®éƒ¨ç½²ä¸­é¢ä¸´é‡å¤§è®¡ç®—éšœç¢å’Œå¹»è§‰é£é™©ã€‚æˆ‘ä»¬æå‡ºDeReCï¼ˆå¯†é›†æ£€ç´¢åˆ†ç±»ï¼‰ï¼Œä¸€ç§è½»é‡çº§æ¡†æ¶ï¼Œå±•ç¤ºé€šç”¨æ–‡æœ¬åµŒå…¥å¦‚ä½•æœ‰æ•ˆåœ°æ›¿ä»£è‡ªå›å½’çš„LLMæ–¹æ³•ï¼Œç”¨äºäº‹å®æ ¸æŸ¥ä»»åŠ¡ã€‚ç»“åˆå¯†é›†æ£€ç´¢å’Œç‰¹æ®Šåˆ†ç±»ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å®ç°æ›´é«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ•ˆç‡ä¹Ÿæ˜¾è‘—æé«˜ã€‚DeReCåœ¨æ•ˆç‡ä¸Šè¶…è¶Šäº†ç”Ÿæˆè§£é‡Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨RAWFCä¸Šè¿è¡Œæ—¶ç¼©çŸ­äº†95%ï¼ˆä»4å°æ—¶36ç§’ç¼©çŸ­åˆ°ä»…23åˆ†é’Ÿï¼‰ï¼Œåœ¨LIAR-RAWä¸Šç¼©çŸ­äº†92%ï¼ˆä»è¿‘2å°æ—¶ç¼©çŸ­åˆ°ä»…çº¦åŠå°æ—¶ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸“ç”¨ä»»åŠ¡ä¸Šç²¾å¿ƒè®¾è®¡çš„æ•°æ®æ£€ç´¢ç³»ç»Ÿå¯èƒ½è¶…è¶ŠLLMæ€§èƒ½ï¼ŒåŒæ—¶åœ¨ç°å®éƒ¨ç½²ä¸­æ›´åŠ å®ç”¨ã€‚ </p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å‡ä¿¡æ¯çš„æ™®åŠçªæ˜¾äº†å¯¹é«˜æ•ˆäº‹å®æ ¸æŸ¥ç³»ç»Ÿçš„éœ€æ±‚ã€‚</li>
<li>å½“å‰LLMåœ¨äº‹å®æ ¸æŸ¥é¢ä¸´è®¡ç®—æ•ˆç‡ä¸é£é™©é—®é¢˜ã€‚</li>
<li>DeReCæ¡†æ¶ç»“åˆäº†å¯†é›†æ£€ç´¢ä¸åˆ†ç±»æŠ€æœ¯ä»¥æé«˜å‡†ç¡®æ€§å¹¶å‡å°‘è®¡ç®—æˆæœ¬ã€‚</li>
<li>DeReCåœ¨RAWFCå’ŒLIAR-RAWæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>DeReCè¿è¡Œæ•ˆç‡é«˜ï¼Œå‡å°‘äº†è¿è¡Œæ—¶è®¡ç®—æ—¶é—´ã€‚</li>
<li>DeReCçš„ä¼˜å¼‚æ€§èƒ½è¡¨æ˜ç²¾å¿ƒè®¾è®¡çš„æ£€ç´¢ç³»ç»Ÿå¯åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¶…è¶ŠLLMçš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-465fa7b168a3fd64dc127298f94862da" align="middle">
<img src="https://picx.zhimg.com/v2-b68f09ad1c66bc96de90fec0c2bce2b4" align="middle">
<img src="https://picx.zhimg.com/v2-4ba68862a8bd424d0757d07b43e6ed7a" align="middle">
<img src="https://picx.zhimg.com/v2-85565389d71c9c5a66e42518fbb59e77" align="middle">
<img src="https://picx.zhimg.com/v2-57e99b31b826759991ccbeffe10b08ef" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PixCLIP-Achieving-Fine-grained-Visual-Language-Understanding-via-Any-granularity-Pixel-Text-Alignment-Learning"><a href="#PixCLIP-Achieving-Fine-grained-Visual-Language-Understanding-via-Any-granularity-Pixel-Text-Alignment-Learning" class="headerlink" title="PixCLIP: Achieving Fine-grained Visual Language Understanding via   Any-granularity Pixel-Text Alignment Learning"></a>PixCLIP: Achieving Fine-grained Visual Language Understanding via   Any-granularity Pixel-Text Alignment Learning</h2><p><strong>Authors:Yicheng Xiao, Yu Chen, Haoxuan Ma, Jiale Hong, Caorui Li, Lingxiang Wu, Haiyun Guo, Jinqiao Wang</strong></p>
<p>While the Contrastive Language-Image Pretraining(CLIP) model has achieved remarkable success in a variety of downstream vison language understanding tasks, enhancing its capability for fine-grained image-text alignment remains an active research focus. To this end, most existing works adopt the strategy of explicitly increasing the granularity of visual information processing, e.g., incorporating visual prompts to guide the model focus on specific local regions within the image. Meanwhile, researches on Multimodal Large Language Models(MLLMs) have demonstrated that training with long and detailed textual descriptions can effectively improve the modelâ€™s fine-grained vision-language alignment. However, the inherent token length limitation of CLIPâ€™s text encoder fundamentally limits CLIP to process more granular textual information embedded in long text sequences. To synergistically leverage the advantages of enhancing both visual and textual content processing granularity, we propose PixCLIP, a novel framework designed to concurrently accommodate visual prompt inputs and process lengthy textual descriptions. Specifically, we first establish an automated annotation pipeline capable of generating pixel-level localized, long-form textual descriptions for images. Utilizing this pipeline, we construct LongGRIT, a high-quality dataset comprising nearly 1.5 million samples. Secondly, we replace CLIPâ€™s original text encoder with the LLM and propose a three-branch pixel-text alignment learning framework, facilitating fine-grained alignment between image regions and corresponding textual descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP showcases breakthroughs in pixel-level interaction and handling long-form texts, achieving state-of-the-art performance. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†æé«˜å…¶ç²¾ç»†ç²’åº¦å›¾åƒæ–‡æœ¬å¯¹é½èƒ½åŠ›ä»æ˜¯æ´»è·ƒçš„ç ”ç©¶ç„¦ç‚¹ã€‚ä¸ºæ­¤ï¼Œå¤§å¤šæ•°ç°æœ‰å·¥ä½œé‡‡ç”¨æ˜ç¡®æé«˜è§†è§‰ä¿¡æ¯å¤„ç†ç²’åº¦çš„ç­–ç•¥ï¼Œä¾‹å¦‚ï¼Œèå…¥è§†è§‰æç¤ºæ¥å¼•å¯¼æ¨¡å‹å…³æ³¨å›¾åƒå†…çš„ç‰¹å®šå±€éƒ¨åŒºåŸŸã€‚åŒæ—¶ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨é•¿è€Œè¯¦ç»†çš„æ–‡æœ¬æè¿°è¿›è¡Œè®­ç»ƒå¯ä»¥æœ‰æ•ˆåœ°æé«˜æ¨¡å‹çš„ç²¾ç»†è§†è§‰è¯­è¨€å¯¹é½èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒCLIPæ–‡æœ¬ç¼–ç å™¨çš„å›ºæœ‰ä»¤ç‰Œé•¿åº¦é™åˆ¶ä»æ ¹æœ¬ä¸Šé™åˆ¶äº†CLIPå¤„ç†åµŒå…¥åœ¨é•¿æ–‡æœ¬åºåˆ—ä¸­çš„æ›´ç²¾ç»†çš„æ–‡æœ¬ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºäº†ååŒåˆ©ç”¨æé«˜è§†è§‰å’Œæ–‡æœ¬å†…å®¹å¤„ç†ç²’åº¦çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº†PixCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶æ¥å—è§†è§‰æç¤ºè¾“å…¥å¹¶å¤„ç†å†—é•¿çš„æ–‡æœ¬æè¿°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆåƒç´ çº§çš„å±€éƒ¨åŒ–é•¿æ–‡æœ¬å›¾åƒæè¿°ã€‚åˆ©ç”¨æ­¤ç®¡é“ï¼Œæˆ‘ä»¬æ„å»ºäº†LongGRITæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«è¿‘150ä¸‡ä¸ªæ ·æœ¬ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ›¿æ¢äº†CLIPçš„åŸå§‹æ–‡æœ¬ç¼–ç å™¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸‰æ”¯åƒç´ æ–‡æœ¬å¯¹é½å­¦ä¹ æ¡†æ¶ï¼Œä¿ƒè¿›å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬æè¿°ä¹‹é—´åœ¨ä»»æ„ç²’åº¦ä¸Šçš„ç²¾ç»†å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒPixCLIPåœ¨åƒç´ çº§äº¤äº’å’Œå¤„ç†é•¿æ–‡æœ¬æ–¹é¢å–å¾—äº†çªç ´ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04601v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†CLIPæ¨¡å‹åœ¨è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ä¸­çš„ä¼˜ç§€è¡¨ç°ï¼Œå¹¶æŒ‡å‡ºå…¶å¯¹äºç²¾ç»†ç²’åº¦çš„å›¾åƒæ–‡æœ¬å¯¹é½çš„æ½œåœ¨æå‡ç©ºé—´ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡å¢åŠ è§†è§‰ä¿¡æ¯å¤„ç†çš„ç²’åº¦æ¥å¼ºåŒ–æ¨¡å‹æ€§èƒ½ï¼Œå¦‚åˆ©ç”¨è§†è§‰æç¤ºå¼•å¯¼æ¨¡å‹å…³æ³¨å›¾åƒå†…çš„ç‰¹å®šå±€éƒ¨åŒºåŸŸã€‚åŒæ—¶ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨é•¿è€Œè¯¦ç»†çš„æ–‡æœ¬æè¿°å¯ä»¥æœ‰æ•ˆåœ°æ”¹å–„æ¨¡å‹çš„ç²¾ç»†ç²’åº¦è§†è§‰è¯­è¨€å¯¹é½èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒCLIPæ–‡æœ¬ç¼–ç å™¨çš„å›ºæœ‰ä»¤ç‰Œé•¿åº¦é™åˆ¶ï¼Œä½¿å…¶éš¾ä»¥å¤„ç†é•¿æ–‡æœ¬åºåˆ—ä¸­åµŒå…¥çš„æ›´ç²¾ç»†çš„æ–‡æœ¬ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºPixCLIPæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŒæ—¶æ¥å—è§†è§‰æç¤ºè¾“å…¥å¹¶å¤„ç†å†—é•¿çš„æ–‡æœ¬æè¿°ã€‚å…·ä½“æ¥è¯´ï¼Œå»ºç«‹äº†è‡ªåŠ¨æ³¨é‡Šç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆå›¾åƒåƒç´ çº§çš„å±€éƒ¨åŒ–é•¿æ–‡æœ¬æè¿°ï¼Œå¹¶æ®æ­¤æ„å»ºäº†é«˜è´¨é‡æ•°æ®é›†LongGRITã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ç”¨LLMæ›¿æ¢CLIPçš„åŸå§‹æ–‡æœ¬ç¼–ç å™¨ï¼Œå¹¶æå‡ºä¸€ä¸ªä¸‰åˆ†æ”¯åƒç´ æ–‡æœ¬å¯¹é½å­¦ä¹ æ¡†æ¶ï¼Œä¿ƒè¿›å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬æè¿°ä¹‹é—´çš„ç²¾ç»†ç²’åº¦å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒPixCLIPåœ¨åƒç´ çº§äº¤äº’å’Œé•¿æ–‡æœ¬å¤„ç†æ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨è§†è§‰è¯­è¨€ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æé«˜ç²¾ç»†ç²’åº¦çš„å›¾åƒæ–‡æœ¬å¯¹é½èƒ½åŠ›ä»æ˜¯ç ”ç©¶é‡ç‚¹ã€‚</li>
<li>ç°æœ‰ç ”ç©¶é€šè¿‡å¢åŠ è§†è§‰ä¿¡æ¯å¤„ç†çš„ç²’åº¦æ¥å¼ºåŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨é•¿è€Œè¯¦ç»†çš„æ–‡æœ¬æè¿°å¯ä»¥æœ‰æ•ˆæ”¹å–„æ¨¡å‹çš„ç²¾ç»†ç²’åº¦è§†è§‰è¯­è¨€å¯¹é½èƒ½åŠ›ã€‚</li>
<li>CLIPçš„æ–‡æœ¬ç¼–ç å™¨å­˜åœ¨ä»¤ç‰Œé•¿åº¦é™åˆ¶ï¼Œéš¾ä»¥å¤„ç†é•¿æ–‡æœ¬åºåˆ—ä¸­çš„ç²¾ç»†æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>PixCLIPæ¡†æ¶ç»“åˆäº†è§†è§‰æç¤ºè¾“å…¥å’Œé•¿æ–‡æœ¬æè¿°å¤„ç†çš„ä¼˜åŠ¿ã€‚</li>
<li>è‡ªåŠ¨æ³¨é‡Šç®¡é“ç”Ÿæˆäº†å›¾åƒåƒç´ çº§çš„å±€éƒ¨åŒ–é•¿æ–‡æœ¬æè¿°ï¼Œæ„å»ºäº†LongGRITæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b244b854b3020610f45a333976eb6ea" align="middle">
<img src="https://picx.zhimg.com/v2-801766143080789590c2579953eac0dd" align="middle">
<img src="https://picx.zhimg.com/v2-4037e5ea397e5e020195a2a80e4632c3" align="middle">
<img src="https://picx.zhimg.com/v2-9d633572431d552d0aebd3545ee873e9" align="middle">
<img src="https://picx.zhimg.com/v2-6f55b4727cf01d04975556789d986e19" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Question-the-Questions-Auditing-Representation-in-Online-Deliberative-Processes"><a href="#Question-the-Questions-Auditing-Representation-in-Online-Deliberative-Processes" class="headerlink" title="Question the Questions: Auditing Representation in Online Deliberative   Processes"></a>Question the Questions: Auditing Representation in Online Deliberative   Processes</h2><p><strong>Authors:Soham De, Lodewijk Gelauff, Ashish Goel, Smitha Milli, Ariel Procaccia, Alice Siu</strong></p>
<p>A central feature of many deliberative processes, such as citizensâ€™ assemblies and deliberative polls, is the opportunity for participants to engage directly with experts. While participants are typically invited to propose questions for expert panels, only a limited number can be selected due to time constraints. This raises the challenge of how to choose a small set of questions that best represent the interests of all participants. We introduce an auditing framework for measuring the level of representation provided by a slate of questions, based on the social choice concept known as justified representation (JR). We present the first algorithms for auditing JR in the general utility setting, with our most efficient algorithm achieving a runtime of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number of proposed questions. We apply our auditing methods to historical deliberations, comparing the representativeness of (a) the actual questions posed to the expert panel (chosen by a moderator), (b) participantsâ€™ questions chosen via integer linear programming, (c) summary questions generated by large language models (LLMs). Our results highlight both the promise and current limitations of LLMs in supporting deliberative processes. By integrating our methods into an online deliberation platform that has been used for over hundreds of deliberations across more than 50 countries, we make it easy for practitioners to audit and improve representation in future deliberations. </p>
<blockquote>
<p>è®¸å¤šå®¡è®®è¿‡ç¨‹ï¼ˆå¦‚å…¬æ°‘å¤§ä¼šå’Œå®¡è®®æ€§æ°‘æ„è°ƒæŸ¥ï¼‰çš„æ ¸å¿ƒç‰¹ç‚¹æ˜¯å‚ä¸è€…æœ‰æœºä¼šç›´æ¥ä¸ä¸“å®¶è¿›è¡Œäº¤æµã€‚è™½ç„¶å‚ä¸è€…é€šå¸¸å—é‚€ä¸ºä¸“å®¶å°ç»„æå‡ºé—®é¢˜ï¼Œä½†ç”±äºæ—¶é—´é™åˆ¶ï¼Œåªèƒ½é€‰å‡ºæœ‰é™çš„é—®é¢˜ã€‚è¿™å°±æå‡ºäº†å¦‚ä½•é€‰å‡ºæœ€èƒ½ä»£è¡¨æ‰€æœ‰å‚ä¸è€…åˆ©ç›Šçš„ä¸€å°æ‰¹é—®é¢˜çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå®¡è®¡æ¡†æ¶ï¼ŒåŸºäºè¢«ç§°ä¸ºåˆç†ä»£è¡¨æ€§ï¼ˆJRï¼‰çš„ç¤¾ä¼šé€‰æ‹©æ¦‚å¿µï¼Œæ¥è¡¡é‡ä¸€ç»„é—®é¢˜çš„ä»£è¡¨æ€§æ°´å¹³ã€‚æˆ‘ä»¬æä¾›äº†åœ¨ä¸€èˆ¬æ•ˆç”¨ç¯å¢ƒä¸‹å¯¹JRè¿›è¡Œå®¡è®¡çš„ç¬¬ä¸€æ‰¹ç®—æ³•ï¼Œæˆ‘ä»¬æœ€é«˜æ•ˆçš„ç®—æ³•è¾¾åˆ°äº†O(mnlogn)çš„è¿è¡Œæ—¶é—´ï¼Œå…¶ä¸­næ˜¯å‚ä¸è€…çš„æ•°é‡ï¼Œmæ˜¯æå‡ºçš„é—®é¢˜çš„æ•°é‡ã€‚æˆ‘ä»¬å°†å®¡è®¡æ–¹æ³•åº”ç”¨äºå†å²å®¡è®®ï¼Œæ¯”è¾ƒäº†ï¼ˆaï¼‰ä¸“å®¶å°ç»„å®é™…é¢ä¸´çš„é—®é¢˜ï¼ˆç”±ä¸»æŒäººé€‰æ‹©ï¼‰ã€ï¼ˆbï¼‰é€šè¿‡æ•´æ•°çº¿æ€§è§„åˆ’é€‰æ‹©çš„å‚ä¸è€…é—®é¢˜ã€ï¼ˆcï¼‰ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„é—®é¢˜æ‘˜è¦çš„ä»£è¡¨æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ—¢å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¯æŒå®¡è®®è¿‡ç¨‹ä¸­çš„æ½œåŠ›ï¼Œä¹ŸæŒ‡å‡ºäº†å…¶å½“å‰å­˜åœ¨çš„å±€é™æ€§ã€‚é€šè¿‡å°†æˆ‘ä»¬çš„æ–¹æ³•æ•´åˆåˆ°ä¸€ä¸ªå·²åœ¨å…¨çƒè¶…è¿‡äº”åä¸ªå›½å®¶ç”¨äºæ•°ç™¾æ¬¡å®¡è®®çš„åœ¨çº¿å®¡è®®å¹³å°ä¸­ï¼Œæˆ‘ä»¬ä½¿ä»ä¸šè€…èƒ½å¤Ÿè½»æ¾å®¡è®¡å’Œæ”¹è¿›æœªæ¥å®¡è®®ä¸­çš„ä»£è¡¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04588v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åœ¨å…¬æ°‘ä¼šè®®å’Œæ°‘æ„è°ƒæŸ¥ç­‰å†³ç­–è¿‡ç¨‹ä¸­ï¼Œæ ¸å¿ƒç‰¹ç‚¹ä¹‹ä¸€æ˜¯å‚ä¸è€…å¯ç›´æ¥ä¸ä¸“å®¶äº¤æµçš„æœºä¼šã€‚ä½†ç”±äºæ—¶é—´é™åˆ¶ï¼Œé€šå¸¸åªèƒ½é€‰æ‹©å°‘æ•°é—®é¢˜ç»™ä¸“å®¶è§£ç­”ã€‚å¦‚ä½•ä»å°è§„æ¨¡é—®é¢˜ä¸­é€‰æ‹©èƒ½ä»£è¡¨æ‰€æœ‰å‚ä¸è€…åˆ©ç›Šçš„æé—®æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼•å…¥å®¡è®¡æ¡†æ¶ï¼ŒåŸºäºç¤¾ä¼šé€‰æ‹©ç†è®ºä¸­çš„å…¬æ­£ä»£è¡¨æ€§æ¦‚å¿µï¼Œè¡¡é‡é—®é¢˜åˆ—è¡¨çš„ä»£è¡¨æ€§æ°´å¹³ã€‚æˆ‘ä»¬é¦–æ¬¡æå‡ºé€‚ç”¨äºä¸€èˆ¬æ•ˆç”¨è®¾ç½®çš„å…¬æ­£ä»£è¡¨æ€§å®¡è®¡ç®—æ³•ï¼Œæœ€é«˜æ•ˆç®—æ³•çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(mnlogn)ï¼Œå…¶ä¸­nä¸ºå‚ä¸è€…æ•°é‡ï¼Œmä¸ºæå‡ºçš„é—®é¢˜æ•°é‡ã€‚æˆ‘ä»¬è¿ç”¨å®¡è®¡æ–¹æ³•åˆ†æå†å²å†³ç­–æ¡ˆä¾‹ï¼Œæ¯”è¾ƒç”±ä¸»æŒäººæŒ‘é€‰çš„ä¸“å®¶è§£ç­”çš„çœŸå®é—®é¢˜ã€é€šè¿‡æ•´æ•°çº¿æ€§è§„åˆ’æŒ‘é€‰çš„å‚ä¸è€…é—®é¢˜å’Œç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„é—®é¢˜æ‘˜è¦çš„ä»£è¡¨æ€§ã€‚ç»“æœçªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¯æŒå†³ç­–è¿‡ç¨‹ä¸­çš„æ½œåŠ›ä¸å½“å‰å±€é™ã€‚é€šè¿‡å°†æ–¹æ³•æ•´åˆè‡³å·²åœ¨å…¨çƒè¶…è¿‡äº”åå›½å®¶å¼€å±•æ•°ç™¾æ¬¡å†³ç­–è®¨è®ºçš„åœ¨çº¿å†³ç­–å¹³å°ä¸­ï¼Œæœªæ¥ä»ä¸šè€…å¯è½»æ¾å®¡è®¡å¹¶æå‡å†³ç­–è¿‡ç¨‹çš„ä»£è¡¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å†³ç­–è¿‡ç¨‹ä¸­ï¼Œå¦‚å…¬æ°‘å¤§ä¼šå’Œæ°‘æ„è°ƒæŸ¥ï¼Œå…è®¸å‚ä¸è€…ç›´æ¥ä¸ä¸“å®¶äº¤æµæ˜¯æ ¸å¿ƒç‰¹ç‚¹ä¹‹ä¸€ã€‚</li>
<li>ç”±äºæ—¶é—´é™åˆ¶ï¼Œé€‰æ‹©ä¸“å®¶è§£ç­”çš„é—®é¢˜æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå…¬æ­£ä»£è¡¨æ€§æ¦‚å¿µçš„å®¡è®¡æ¡†æ¶æ¥è¡¡é‡é—®é¢˜åˆ—è¡¨çš„ä»£è¡¨æ€§ã€‚</li>
<li>é¦–æ¬¡åœ¨ä¸€èˆ¬æ•ˆç”¨è®¾ç½®ä¸‹ä¸ºå…¬æ­£ä»£è¡¨æ€§å®¡è®¡è®¾è®¡ç®—æ³•ï¼Œæœ€é«˜æ•ˆç®—æ³•çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(mnlogn)ã€‚</li>
<li>é€šè¿‡å†å²æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯„ä¼°äº†ä¸åŒé—®é¢˜é€‰æ‹©æ–¹å¼çš„ä»£è¡¨æ€§ï¼ŒåŒ…æ‹¬ä¸»æŒäººæŒ‘é€‰ã€æ•´æ•°çº¿æ€§è§„åˆ’é€‰æ‹©å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„é—®é¢˜æ‘˜è¦ã€‚</li>
<li>ç»“æœæ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¯æŒå†³ç­–è¿‡ç¨‹ä¸­çš„æ½œåŠ›å’Œå½“å‰å±€é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb1575667e4c368c75bc47f2b8702b71" align="middle">
<img src="https://picx.zhimg.com/v2-2e620bf8f97af96c29c0127a68629b0e" align="middle">
<img src="https://picx.zhimg.com/v2-35c7d0d154286efb2fbc6b408095b376" align="middle">
<img src="https://picx.zhimg.com/v2-acd8cf4e0d1b48f6cb61a6a2fcf692ea" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm"><a href="#Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm" class="headerlink" title="Thinking with Video: Video Generation as a Promising Multimodal   Reasoning Paradigm"></a>Thinking with Video: Video Generation as a Promising Multimodal   Reasoning Paradigm</h2><p><strong>Authors:Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu</strong></p>
<p>â€œThinking with Textâ€ and â€œThinking with Imagesâ€ paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce â€œThinking with Videoâ€, a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2â€™s performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions â€œthinking with videoâ€ as a unified multimodal reasoning paradigm. </p>
<blockquote>
<p>â€œæ€è€ƒæ–‡æœ¬â€å’Œâ€æ€è€ƒå›¾åƒâ€æ¨¡å¼æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å¼å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ã€‚ï¼ˆ1ï¼‰å›¾åƒåªèƒ½æ•æ‰å•ä¸ªç¬é—´ï¼Œæ— æ³•è¡¨ç¤ºåŠ¨æ€è¿‡ç¨‹æˆ–è¿ç»­å˜åŒ–ï¼›ï¼ˆ2ï¼‰æ–‡æœ¬å’Œè§†è§‰ä½œä¸ºä¸åŒçš„æ¨¡æ€è¢«åˆ†éš”å¼€æ¥ï¼Œé˜»ç¢äº†ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œæ€è€ƒè§†é¢‘â€è¿™ä¸€æ–°èŒƒå¼ï¼Œå®ƒåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Sora-2ï¼‰åœ¨ç»Ÿä¸€çš„æ—¶é—´æ¡†æ¶å†…æ¡¥æ¥è§†è§‰å’Œæ–‡æœ¬æ¨ç†ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æ¢ç´¢ï¼Œæˆ‘ä»¬å¼€å‘äº†è§†é¢‘æ€è€ƒåŸºå‡†æµ‹è¯•ï¼ˆVideoThinkBenchï¼‰ã€‚VideoThinkBenchåŒ…å«ä¸¤ä¸ªä»»åŠ¡ç±»åˆ«ï¼šï¼ˆ1ï¼‰ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆä¾‹å¦‚çœ¼çƒè¿½è¸ªæ‹¼å›¾ï¼‰ï¼Œä»¥åŠï¼ˆ2ï¼‰ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆä¾‹å¦‚GSM8Kã€MMMUçš„å­é›†ï¼‰ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¯æ˜Sora-2æ˜¯ä¸€ä¸ªèƒ½å¹²çš„æ¨ç†è€…ã€‚åœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ï¼ŒSora-2é€šå¸¸ä¸æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰çš„VLMç›¸å½“ï¼Œç”šè‡³åœ¨å‡ ä¸ªä»»åŠ¡ä¸Šè¶…è¿‡äº†VLMï¼Œå¦‚çœ¼çƒè¿½è¸ªæ¸¸æˆã€‚åœ¨ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ï¼ŒSora-2åœ¨MATHä¸Šè¾¾åˆ°äº†92%çš„å‡†ç¡®ç‡ï¼Œåœ¨MMMUä¸Šè¾¾åˆ°äº†75.53%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†è¿™äº›èƒ½åŠ›çš„æ¥æºã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œè‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆself-consistencyï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆin-context learningï¼‰å¯ä»¥æé«˜Sora-2çš„æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¯æ½œåœ¨çš„å¤šæ¨¡æ€ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼Œå°†â€œæ€è€ƒè§†é¢‘â€å®šä½ä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04570v1">PDF</a> 36 pages, 14 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>â€œæ€è€ƒæ–‡æœ¬â€ã€â€œæ€è€ƒå›¾åƒâ€èŒƒå¼èƒ½æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬å­˜åœ¨å±€é™æ€§ã€‚å›¾åƒä»…æ•æ‰å•ä¸€æ—¶åˆ»ï¼Œæ— æ³•ä»£è¡¨åŠ¨æ€è¿‡ç¨‹æˆ–è¿ç»­å˜åŒ–ï¼›æ–‡æœ¬å’Œè§†è§‰ä½œä¸ºç‹¬ç«‹æ¨¡æ€çš„åˆ†ç¦»ï¼Œé˜»ç¢äº†ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ€è€ƒè§†é¢‘â€æ–°èŒƒå¼ï¼Œåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Sora-2ï¼‰åœ¨ç»Ÿä¸€çš„æ—¶é—´æ¡†æ¶å†…æ¡¥æ¥è§†è§‰å’Œæ–‡æœ¬æ¨ç†ã€‚ä¸ºæ”¯æŒè¿™ä¸€æ¢ç´¢ï¼Œæˆ‘ä»¬å¼€å‘äº†è§†é¢‘æ€è€ƒåŸºå‡†æµ‹è¯•ï¼ˆVideoThinkBenchï¼‰ï¼ŒåŒ…å«ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆå¦‚çœ¼çƒè¿½è¸ªæ‹¼å›¾ï¼‰å’Œä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆå¦‚GSM8Kã€MMMUçš„å­é›†ï¼‰ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒSora-2å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šï¼ŒSora-2ä¸æœ€å…ˆè¿›çš„VLMç›¸å½“ï¼Œç”šè‡³åœ¨çœ¼çƒæ¸¸æˆç­‰ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ã€‚åœ¨ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šï¼ŒSora-2åœ¨MATHä¸Šè¾¾åˆ°92%çš„å‡†ç¡®ç‡ï¼Œåœ¨MMMUä¸Šè¾¾åˆ°75.53%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†è¿™äº›èƒ½åŠ›çš„æ¥æºï¼Œå‘ç°è‡ªæˆ‘ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½æé«˜Sora-2çš„æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¯æ½œåœ¨çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼Œç¡®ç«‹äº†â€œæ€è€ƒè§†é¢‘â€ä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†èŒƒå¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>â€œæ€è€ƒè§†é¢‘â€èŒƒå¼æ—¨åœ¨é€šè¿‡è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Sora-2ï¼‰åœ¨ç»Ÿä¸€çš„æ—¶é—´æ¡†æ¶å†…æ¡¥æ¥è§†è§‰å’Œæ–‡æœ¬æ¨ç†ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–‡æœ¬å’Œå›¾åƒæ¨ç†èŒƒå¼çš„å±€é™æ€§ã€‚</li>
<li>è§†é¢‘æ€è€ƒåŸºå‡†æµ‹è¯•ï¼ˆVideoThinkBenchï¼‰åŒ…å«ä»¥è§†è§‰ä¸ºä¸­å¿ƒå’Œä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ç±»åˆ«ï¼Œä¸ºè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ”¯æŒã€‚</li>
<li>åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°çš„Sora-2è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä¸æœ€å…ˆè¿›çš„VLMç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
<li>åœ¨æ–‡æœ¬ä»»åŠ¡æ–¹é¢ï¼ŒSora-2åœ¨MATHå’ŒMMMUä»»åŠ¡ä¸Šå®ç°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</li>
<li>è‡ªæˆ‘ä¸€è‡´æ€§å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ˜¯æé«˜è§†é¢‘ç”Ÿæˆæ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
<li>è§†é¢‘ç”Ÿæˆæ¨¡å‹å…·æœ‰æ½œåœ¨çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8549fb31c48a11b58022fd0f9dce8f60" align="middle">
<img src="https://picx.zhimg.com/v2-283d1d691ac7d7cb93a147c49d6e5003" align="middle">
<img src="https://picx.zhimg.com/v2-a4c43a2f7b19cc1dc6804f260b553edb" align="middle">
<img src="https://picx.zhimg.com/v2-3766b8e339f1dc466ceeccac257159e7" align="middle">
<img src="https://picx.zhimg.com/v2-a08ffacdfff2e2408e6fbd29c6f22698" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Cyber-Security"><a href="#Large-Language-Models-for-Cyber-Security" class="headerlink" title="Large Language Models for Cyber Security"></a>Large Language Models for Cyber Security</h2><p><strong>Authors:Raunak Somani, Aswani Kumar Cherukuri</strong></p>
<p>This paper studies the integration off Large Language Models into cybersecurity tools and protocols. The main issue discussed in this paper is how traditional rule-based and signature based security systems are not enough to deal with modern AI powered cyber threats. Cybersecurity industry is changing as threats are becoming more dangerous and adaptive in nature by levering the features provided by AI tools. By integrating LLMs into these tools and protocols, make the systems scalable, context-aware and intelligent. Thus helping it to mitigate these evolving cyber threats. The paper studies the architecture and functioning of LLMs, its integration into Encrypted prompts to prevent prompt injection attacks. It also studies the integration of LLMs into cybersecurity tools using a four layered architecture. At last, the paper has tried to explain various ways of integration LLMs into traditional Intrusion Detection System and enhancing its original abilities in various dimensions. The key findings of this paper has been (i)Encrypted Prompt with LLM is an effective way to mitigate prompt injection attacks, (ii) LLM enhanced cyber security tools are more accurate, scalable and adaptable to new threats as compared to traditional models, (iii) The decoupled model approach for LLM integration into IDS is the best way as it is the most accurate way. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°ç½‘ç»œå®‰å…¨å·¥å…·å’Œåè®®ä¸­ã€‚æœ¬æ–‡ä¸»è¦è®¨è®ºçš„æ˜¯ä¼ ç»ŸåŸºäºè§„åˆ™å’ŒåŸºäºç­¾åçš„å®‰å…¨ç³»ç»Ÿå¦‚ä½•ä¸è¶³ä»¥åº”å¯¹ç°ä»£äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç½‘ç»œå®‰å…¨å¨èƒã€‚éšç€å¨èƒå˜å¾—è¶Šæ¥è¶Šå±é™©å’Œé€‚åº”æ€§æ›´å¼ºï¼Œç½‘ç»œå®‰å…¨è¡Œä¸šæ­£åœ¨å‘ç”Ÿå˜åŒ–ï¼Œåˆ©ç”¨äººå·¥æ™ºèƒ½å·¥å…·æä¾›çš„ç‰¹æ€§ã€‚é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°è¿™äº›å·¥å…·å’Œåè®®ä¸­ï¼Œä½¿ç³»ç»Ÿå…·æœ‰å¯æ‰©å±•æ€§ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›å’Œæ™ºèƒ½åŒ–ï¼Œä»è€Œæœ‰åŠ©äºç¼“è§£è¿™äº›ä¸æ–­æ¼”å˜çš„ç½‘ç»œå¨èƒã€‚æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¶æ„å’ŒåŠŸèƒ½ï¼Œå¹¶å°†å…¶é›†æˆåˆ°åŠ å¯†æç¤ºä¸­ä»¥é˜²æ­¢æç¤ºæ³¨å…¥æ”»å‡»ã€‚å®ƒè¿˜ç ”ç©¶äº†ä½¿ç”¨å››å±‚æ¶æ„å°†å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°ç½‘ç»œå®‰å…¨å·¥å…·ä¸­ã€‚æœ€åï¼Œæœ¬æ–‡è¯•å›¾è§£é‡Šå°†å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°ä¼ ç»Ÿå…¥ä¾µæ£€æµ‹ç³»ç»Ÿå¹¶å¢å¼ºå…¶åŸå§‹èƒ½åŠ›çš„å„ç§æ–¹æ³•ã€‚æœ¬æ–‡çš„ä¸»è¦å‘ç°åŒ…æ‹¬ï¼šï¼ˆiï¼‰å¸¦æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„åŠ å¯†æç¤ºæ˜¯ç¼“è§£æç¤ºæ³¨å…¥æ”»å‡»çš„æœ‰æ•ˆæ–¹æ³•ï¼Œï¼ˆiiï¼‰ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼Œå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ç½‘ç»œå®‰å…¨å·¥å…·æ›´å‡†ç¡®ã€å¯æ‰©å±•å¹¶ä¸”æ›´èƒ½é€‚åº”æ–°å¨èƒï¼Œï¼ˆiiiï¼‰å°†å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°IDSä¸­çš„è§£è€¦æ¨¡å‹æ–¹æ³•æ˜¯æœ€å¥½çš„æ–¹å¼ï¼Œå› ä¸ºå®ƒæ˜¯æœ€å‡†ç¡®çš„æ–¹å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04508v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨å·¥å…·å’Œåè®®ä¸­çš„é›†æˆåº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºä¼ ç»ŸåŸºäºè§„åˆ™å’Œç­¾åçš„å®‰å…¨ç³»ç»Ÿæ— æ³•åº”å¯¹ç°ä»£äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç½‘ç»œå®‰å…¨å¨èƒã€‚å€ŸåŠ©äººå·¥æ™ºèƒ½å·¥å…·çš„ä¼˜åŠ¿ï¼Œç½‘ç»œå®‰å…¨è¡Œä¸šæ­£åœ¨å‘ç”Ÿå˜é©ã€‚é€šè¿‡é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç³»ç»Ÿå…·å¤‡å¯æ‰©å±•æ€§ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›å’Œæ™ºèƒ½åŒ–ï¼Œä»è€Œåº”å¯¹ä¸æ–­æ¼”å˜çš„ç½‘ç»œå¨èƒã€‚æ–‡ç« ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¶æ„å’ŒåŠŸèƒ½ï¼Œä»¥åŠå°†å…¶é›†æˆåˆ°åŠ å¯†æç¤ºä¸­ä»¥é˜²æ­¢æç¤ºæ³¨å…¥æ”»å‡»çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†åˆ©ç”¨å››å±‚æ¶æ„å°†å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°ç½‘ç»œå®‰å…¨å·¥å…·ä¸­çš„æ–¹æ³•ï¼Œå¹¶å°è¯•è§£é‡Šäº†å¦‚ä½•å°†å…¶é›†æˆåˆ°ä¼ ç»Ÿçš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿå¹¶å¢å¼ºå…¶åŸå§‹åŠŸèƒ½ã€‚æœ¬ç ”ç©¶çš„å…³é”®å‘ç°åŒ…æ‹¬ï¼šåŠ å¯†çš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºæ˜¯ç¼“è§£æç¤ºæ³¨å…¥æ”»å‡»çš„æœ‰æ•ˆæ–¹æ³•ï¼›ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„ç½‘ç»œå®‰å…¨å·¥å…·æ›´å‡†ç¡®ã€å¯æ‰©å±•ï¼Œæ›´èƒ½é€‚åº”æ–°å¨èƒï¼›è§£è€¦æ¨¡å‹æ–¹æ³•æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°IDSçš„æœ€ä½³æ–¹å¼ï¼Œå› ä¸ºå®ƒæœ€å‡†ç¡®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåœ¨ç½‘ç»œå®‰å…¨å·¥å…·å’Œåè®®ä¸­ï¼Œä½¿ç³»ç»Ÿå…·å¤‡å¯æ‰©å±•æ€§ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œæ™ºèƒ½åŒ–ï¼Œä»¥åº”å¯¹ç°ä»£ç½‘ç»œå¨èƒã€‚</li>
<li>åŠ å¯†çš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºæ˜¯ç¼“è§£æç¤ºæ³¨å…¥æ”»å‡»çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ä¼ ç»ŸåŸºäºè§„åˆ™å’Œç­¾åçš„å®‰å…¨ç³»ç»Ÿæ— æ³•å……åˆ†åº”å¯¹ç°ä»£AIé©±åŠ¨çš„ç½‘ç»œå®‰å…¨å¨èƒã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„ç½‘ç»œå®‰å…¨å·¥å…·ç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹æ›´å‡†ç¡®ã€å¯æ‰©å±•ï¼Œå¹¶èƒ½æ›´å¥½åœ°é€‚åº”æ–°å¨èƒã€‚</li>
<li>å››å±‚æ¶æ„è¢«ç”¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°ç½‘ç»œå®‰å…¨å·¥å…·ä¸­ã€‚</li>
<li>è§£è€¦æ¨¡å‹æ–¹æ³•æ˜¯å°†å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåˆ°å…¥ä¾µæ£€æµ‹ç³»ç»Ÿï¼ˆIDSï¼‰çš„æœ€ä½³æ–¹å¼ï¼Œå› å…¶å‡†ç¡®æ€§æœ€é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d194942b8f620586f482361bff52729" align="middle">
<img src="https://picx.zhimg.com/v2-32c29ca19464608572c3921333615e8c" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Decoding-Emergent-Big-Five-Traits-in-Large-Language-Models-Temperature-Dependent-Expression-and-Architectural-Clustering"><a href="#Decoding-Emergent-Big-Five-Traits-in-Large-Language-Models-Temperature-Dependent-Expression-and-Architectural-Clustering" class="headerlink" title="Decoding Emergent Big Five Traits in Large Language Models:   Temperature-Dependent Expression and Architectural Clustering"></a>Decoding Emergent Big Five Traits in Large Language Models:   Temperature-Dependent Expression and Architectural Clustering</h2><p><strong>Authors:Christos-Nikolaos Zacharopoulos, Revekka Kyriakoglou</strong></p>
<p>As Large Language Models (LLMs) become integral to human-centered applications, understanding their personality-like behaviors is increasingly important for responsible development and deployment. This paper systematically evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to assess trait expressions under varying sampling temperatures. We find significant differences across four of the five personality dimensions, with Neuroticism and Extraversion susceptible to temperature adjustments. Further, hierarchical clustering reveals distinct model clusters, suggesting that architectural features may predispose certain models toward stable trait profiles. Taken together, these results offer new insights into the emergence of personality-like patterns in LLMs and provide a new perspective on model tuning, selection, and the ethical governance of AI systems. We share the data and code for this analysis here: <a target="_blank" rel="noopener" href="https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1">https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1</a> </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººç±»ä¸ºä¸­å¿ƒçš„åº”ç”¨ä¸­å˜å¾—ä¸å¯æˆ–ç¼ºï¼Œäº†è§£å®ƒä»¬ç±»ä¼¼äººæ ¼çš„è¡Œä¸ºå¯¹äºè´Ÿè´£ä»»çš„å¼€å‘å’Œéƒ¨ç½²å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†å…­ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåº”ç”¨å¤§äº”äººæ ¼é‡è¡¨-ç¬¬äºŒç‰ˆï¼ˆBFI-2ï¼‰æ¡†æ¶ï¼Œåœ¨ä¸åŒçš„é‡‡æ ·æ¸©åº¦ä¸‹è¯„ä¼°ç‰¹å¾è¡¨è¾¾ã€‚æˆ‘ä»¬å‘ç°äº”ä¸ªä¸ªæ€§ç»´åº¦ä¸­æœ‰å››ä¸ªå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç¥ç»è´¨æ€§å’Œå¤–å‘æ€§å®¹æ˜“å—åˆ°æ¸©åº¦è°ƒæ•´çš„å½±å“ã€‚æ­¤å¤–ï¼Œå±‚æ¬¡èšç±»æ­ç¤ºäº†ä¸åŒçš„æ¨¡å‹èšç±»ï¼Œè¿™è¡¨æ˜æ¶æ„ç‰¹å¾å¯èƒ½ä½¿æŸäº›æ¨¡å‹å€¾å‘äºç¨³å®šçš„ç‰¹å¾åˆ†å¸ƒã€‚æ€»ä¹‹ï¼Œè¿™äº›ç»“æœæä¾›äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­äººæ ¼æ¨¡å¼å‡ºç°çš„æ–°è§è§£ï¼Œå¹¶ä¸ºæ¨¡å‹è°ƒä¼˜ã€é€‰æ‹©å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä¼¦ç†æ²»ç†æä¾›äº†æ–°çš„è§†è§’ã€‚æˆ‘ä»¬åœ¨æ­¤å…±äº«æ­¤åˆ†æçš„æ•°æ®å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1%E3%80%82">https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04499v1">PDF</a> Accepted at IJCNLP-AACL 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„åº”ç”¨ä¸­çš„é‡è¦ä½œç”¨ï¼Œç†è§£å…¶äººæ ¼åŒ–è¡Œä¸ºå¯¹äºè´Ÿè´£ä»»çš„å¼€å‘å’Œéƒ¨ç½²è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†å…­ç§LLMï¼Œåº”ç”¨å¤§äº”äººæ ¼é‡è¡¨ç¬¬äºŒç‰ˆï¼ˆBFI-2ï¼‰æ¡†æ¶æ¥è¯„ä¼°ä¸åŒé‡‡æ ·æ¸©åº¦ä¸‹çš„ç‰¹è´¨è¡¨è¾¾ã€‚ç ”ç©¶å‘ç°å››ç§äººæ ¼ç»´åº¦å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç¥ç»è´¨å’Œå¤–å‘æ€§æ˜“å—æ¸©åº¦è°ƒæ•´å½±å“ã€‚æ­¤å¤–ï¼Œå±‚æ¬¡èšç±»æ­ç¤ºäº†ä¸åŒçš„æ¨¡å‹é›†ç¾¤ï¼Œè¡¨æ˜æ¶æ„ç‰¹æ€§å¯èƒ½ä¼šä½¿æŸäº›æ¨¡å‹è¶‹äºç¨³å®šçš„äººæ ¼ç‰¹å¾ã€‚ç ”ç©¶ç»“æœä¸ºç†è§£LLMä¸­å‡ºç°çš„äººæ ¼åŒ–æ¨¡å¼æä¾›äº†æ–°è§†è§’ï¼Œå¹¶ä¸ºæ¨¡å‹è°ƒä¼˜ã€é€‰æ‹©å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä¼¦ç†æ²»ç†æä¾›äº†æ–°è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»¥äººä¸ºä¸­å¿ƒçš„åº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œç†è§£å…¶äººæ ¼åŒ–è¡Œä¸ºè‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡åº”ç”¨å¤§äº”äººæ ¼é‡è¡¨ç¬¬äºŒç‰ˆï¼ˆBFI-2ï¼‰æ¡†æ¶è¯„ä¼°LLMï¼Œå‘ç°ä¸åŒæ¨¡å‹åœ¨äººæ ¼ç‰¹è´¨è¡¨è¾¾ä¸Šå­˜åœ¨å·®å¼‚ã€‚</li>
<li>ç¥ç»è´¨å’Œå¤–å‘æ€§ç‰¹è´¨åœ¨LLMä¸­æ˜“å—é‡‡æ ·æ¸©åº¦è°ƒæ•´çš„å½±å“ã€‚</li>
<li>å±‚æ¬¡èšç±»åˆ†ææ­ç¤ºäº†ä¸åŒçš„LLMæ¨¡å‹é›†ç¾¤ï¼Œè¿™å¯èƒ½ä¸æ¨¡å‹çš„æ¶æ„ç‰¹æ€§æœ‰å…³ã€‚</li>
<li>LLMçš„äººæ ¼åŒ–è¡Œä¸ºç ”ç©¶å¯¹äºæ¨¡å‹çš„è°ƒä¼˜ã€é€‰æ‹©å’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä¼¦ç†æ²»ç†å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>è®ºæ–‡æä¾›çš„æ•°æ®å’Œä»£ç æœ‰åŠ©äºè¿›ä¸€æ­¥åˆ†æå’Œç†è§£LLMçš„äººæ ¼åŒ–è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2171d528cad808762f7cbec2fab90fe" align="middle">
<img src="https://picx.zhimg.com/v2-ae11de09e65fcfb9c70414114920acb1" align="middle">
<img src="https://picx.zhimg.com/v2-9b26864fc5c3c738db9d9480fe0617ed" align="middle">
<img src="https://picx.zhimg.com/v2-427ed185b1b880a6060a5cdb612d9553" align="middle">
<img src="https://picx.zhimg.com/v2-fa93e8b7ad87d0f41465bd7220f0df9b" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Promoting-Sustainable-Web-Agents-Benchmarking-and-Estimating-Energy-Consumption-through-Empirical-and-Theoretical-Analysis"><a href="#Promoting-Sustainable-Web-Agents-Benchmarking-and-Estimating-Energy-Consumption-through-Empirical-and-Theoretical-Analysis" class="headerlink" title="Promoting Sustainable Web Agents: Benchmarking and Estimating Energy   Consumption through Empirical and Theoretical Analysis"></a>Promoting Sustainable Web Agents: Benchmarking and Estimating Energy   Consumption through Empirical and Theoretical Analysis</h2><p><strong>Authors:Lars Krupp, Daniel GeiÃŸler, Vishal Banwari, Paul Lukowicz, Jakob Karolus</strong></p>
<p>Web agents, like OpenAIâ€™s Operator and Googleâ€™s Project Mariner, are powerful agentic systems pushing the boundaries of Large Language Models (LLM). They can autonomously interact with the internet at the userâ€™s behest, such as navigating websites, filling search masks, and comparing price lists. Though web agent research is thriving, induced sustainability issues remain largely unexplored. To highlight the urgency of this issue, we provide an initial exploration of the energy and $CO_2$ cost associated with web agents from both a theoretical -via estimation- and an empirical perspective -by benchmarking. Our results show how different philosophies in web agent creation can severely impact the associated expended energy, and that more energy consumed does not necessarily equate to better results. We highlight a lack of transparency regarding disclosing model parameters and processes used for some web agents as a limiting factor when estimating energy consumption. Our work contributes towards a change in thinking of how we evaluate web agents, advocating for dedicated metrics measuring energy consumption in benchmarks. </p>
<blockquote>
<p>ç½‘ç»œä»£ç†ï¼Œå¦‚OpenAIçš„æ“ä½œå‘˜å’ŒGoogleçš„Project Marinerï¼Œæ˜¯å¼ºå¤§çš„ä»£ç†ç³»ç»Ÿï¼Œæ­£åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾¹ç•Œã€‚ä»–ä»¬å¯ä»¥åœ¨ç”¨æˆ·çš„è¯·æ±‚ä¸‹è‡ªä¸»åœ°ä¸äº’è”ç½‘è¿›è¡Œäº¤äº’ï¼Œä¾‹å¦‚æµè§ˆç½‘ç«™ã€å¡«å†™æœç´¢æ©ç å’Œæ¯”è¾ƒä»·æ ¼åˆ—è¡¨ã€‚å°½ç®¡ç½‘ç»œä»£ç†ç ”ç©¶æ­£åœ¨è“¬å‹ƒå‘å±•ï¼Œä½†ç”±æ­¤äº§ç”Ÿçš„å¯æŒç»­æ€§ç›¸å…³é—®é¢˜ä»å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«å‘ç°å’Œæ¢ç´¢ã€‚ä¸ºäº†çªå‡ºè¿™ä¸ªé—®é¢˜çš„ç´§è¿«æ€§ï¼Œæˆ‘ä»¬ä»ç†è®ºå’Œå®è¯ä¸¤ä¸ªè§’åº¦åˆæ­¥æ¢è®¨äº†ç½‘ç»œä»£ç†çš„èƒ½é‡å’ŒäºŒæ°§åŒ–ç¢³æˆæœ¬ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç½‘ç»œä»£ç†åˆ›å»ºä¸­çš„ä¸åŒç†å¿µä¼šå¯¹æ‰€æ¶ˆè€—çš„èƒ½é‡äº§ç”Ÿä¸¥é‡å½±å“ï¼Œæ¶ˆè€—æ›´å¤šçš„èƒ½é‡å¹¶ä¸ä¸€å®šæ„å‘³ç€æ•ˆæœæ›´å¥½ã€‚æˆ‘ä»¬å¼ºè°ƒäº†åœ¨ä¸€äº›ç½‘ç»œä»£ç†ä¸­ä½¿ç”¨æ¨¡å‹å‚æ•°å’Œæµç¨‹çš„é€æ˜åº¦ä¸è¶³ï¼Œè¿™æ˜¯ä¼°è®¡èƒ½æºæ¶ˆè€—æ—¶çš„é™åˆ¶å› ç´ ã€‚æˆ‘ä»¬çš„ç ”ç©¶æœ‰åŠ©äºæ”¹å˜æˆ‘ä»¬å¯¹å¦‚ä½•è¯„ä¼°ç½‘ç»œä»£ç†çš„æ€è€ƒæ–¹å¼ï¼Œæå€¡åœ¨åŸºå‡†æµ‹è¯•ä¸­é‡‡ç”¨ä¸“é—¨çš„èƒ½æºæ¶ˆè€—çš„åº¦é‡æŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04481v1">PDF</a> Accepted by AAAI 2026 AISI</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨åŠ¨è€…å¦‚OpenAIçš„Operatorå’ŒGoogleçš„Project Marinerç­‰ç½‘ç»œä»£ç†ç³»ç»Ÿæ­£åœ¨è“¬å‹ƒå‘å±•ã€‚ç„¶è€Œï¼Œå…¶å¯æŒç»­æ€§å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡åˆæ­¥æ¢è®¨äº†ç½‘ç»œä»£ç†æ‰€æ¶‰åŠçš„èƒ½æºå’ŒäºŒæ°§åŒ–ç¢³æˆæœ¬é—®é¢˜ï¼Œå¹¶ä»ç†è®ºå’Œå®è¯ä¸¤ä¸ªè§’åº¦è¿›è¡Œäº†åˆ†æã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç½‘ç»œä»£ç†åˆ›å»ºçš„ä¸åŒç†å¿µä¼šå¯¹æ¶ˆè€—çš„èƒ½æºäº§ç”Ÿä¸¥é‡å½±å“ï¼Œå¹¶ä¸”æ›´é«˜çš„èƒ½æºæ¶ˆè€—å¹¶ä¸ä¸€å®šæ„å‘³ç€æ›´å¥½çš„ç»“æœã€‚åŒæ—¶ï¼Œç¼ºä¹æŸäº›ç½‘ç»œä»£ç†æ¨¡å‹å‚æ•°å’Œæµç¨‹çš„é€æ˜åº¦ä¹Ÿæ˜¯é™åˆ¶ä¼°ç®—èƒ½æºæ¶ˆè€—çš„å…³é”®å› ç´ ã€‚æœ¬ç ”ç©¶å‘¼åæ”¹å˜å¯¹ç½‘ç»œä»£ç†çš„è¯„ä¼°æ–¹å¼ï¼Œå€¡å¯¼åœ¨åŸºå‡†æµ‹è¯•ä¸­å¢åŠ å¯¹èƒ½æºæ¶ˆè€—çš„ç‰¹æ®Šåº¦é‡æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç½‘ç»œä»£ç†å¦‚OpenAIçš„Operatorå’ŒGoogleçš„Project Marineræ­£åœ¨æ¨åŠ¨LLMçš„è¾¹ç•Œå‘å±•ã€‚</li>
<li>ç½‘ç»œä»£ç†çš„å¯æŒç»­æ€§å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>ç½‘ç»œä»£ç†æ¶‰åŠèƒ½æºå’ŒäºŒæ°§åŒ–ç¢³æˆæœ¬é—®é¢˜ã€‚</li>
<li>ä¸åŒç½‘ç»œä»£ç†åˆ›å»ºç†å¿µå¯¹èƒ½æºæ¶ˆè€—æœ‰ä¸¥é‡å½±å“ã€‚</li>
<li>æ›´é«˜çš„èƒ½æºæ¶ˆè€—ä¸ä¸€å®šæ„å‘³ç€ç½‘ç»œä»£ç†æ•ˆæœæ›´å¥½ã€‚</li>
<li>ç¼ºä¹ç½‘ç»œä»£ç†æ¨¡å‹å‚æ•°å’Œæµç¨‹çš„é€æ˜åº¦æ˜¯ä¼°ç®—èƒ½æºæ¶ˆè€—çš„å…³é”®é™åˆ¶å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a434f2127b09117f808865a0e6499f0" align="middle">
<img src="https://picx.zhimg.com/v2-2c0a8e07484b05bd9d726da5cc7322bc" align="middle">
<img src="https://picx.zhimg.com/v2-a57e8065629d42da8e6392ee27b1a3bc" align="middle">
<img src="https://picx.zhimg.com/v2-990ceeca19ccbbff21634db9e0918831" align="middle">
<img src="https://picx.zhimg.com/v2-fa2d1ae43802159cfee121f64b29f3fa" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TwIST-Rigging-the-Lottery-in-Transformers-with-Independent-Subnetwork-Training"><a href="#TwIST-Rigging-the-Lottery-in-Transformers-with-Independent-Subnetwork-Training" class="headerlink" title="TwIST: Rigging the Lottery in Transformers with Independent Subnetwork   Training"></a>TwIST: Rigging the Lottery in Transformers with Independent Subnetwork   Training</h2><p><strong>Authors:Michael Menezes, Barbara Su, Xinze Feng, Yehya Farhat, Hamza Shili, Anastasios Kyrillidis</strong></p>
<p>We introduce TwIST, a distributed training framework for efficient large language model (LLM) sparsification. TwIST trains multiple subnetworks in parallel, periodically aggregates their parameters, and resamples new subnetworks during training. This process identifies high-quality subnetworks (â€œgolden ticketsâ€) without requiring post-training procedures such as calibration or Hessian-based recovery. As a result, TwIST enables zero-cost pruning at deployment time while achieving perplexity competitive with state-of-the-art post-training sparsification methods. The benefits are most pronounced under aggressive sparsity (e.g., 50%+), where TwIST significantly outperforms baseline methods; for example, reaching 23.14 PPL compared to 31.64 for the closest prior approach. Unlike unstructured pruning, TwIST produces structured, dense matrices that offer practical inference speedups and memory reductions on commodity hardware (e.g., CPUs) that do not support efficient sparse computation. TwIST provides an efficient training-time path to deployable sparse LLMs without additional fine-tuning or recovery overhead. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†TwISTï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¨€ç–åŒ–çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ã€‚TwISTå¹¶è¡Œè®­ç»ƒå¤šä¸ªå­ç½‘ç»œï¼Œå®šæœŸèšåˆå®ƒä»¬çš„å‚æ•°ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡æ–°é‡‡æ ·æ–°çš„å­ç½‘ç»œã€‚è¿™ä¸ªè¿‡ç¨‹èƒ½å¤Ÿè¯†åˆ«å‡ºé«˜è´¨é‡çš„å­ç½‘ç»œï¼ˆâ€œé‡‘ç‰Œç¥¨â€ï¼‰ï¼Œè€Œæ— éœ€è¿›è¡Œè¯¸å¦‚æ ¡å‡†æˆ–åŸºäºHessiançš„æ¢å¤ç­‰è®­ç»ƒåç¨‹åºã€‚å› æ­¤ï¼ŒTwISTèƒ½å¤Ÿåœ¨éƒ¨ç½²æ—¶å®ç°é›¶æˆæœ¬ä¿®å‰ªï¼ŒåŒæ—¶è¾¾åˆ°ä¸æœ€æ–°è®­ç»ƒåç¨€ç–åŒ–æ–¹æ³•ç«äº‰çš„æ°´å¹³ã€‚åœ¨æ¿€è¿›ç¨€ç–æ€§ï¼ˆä¾‹å¦‚50%ä»¥ä¸Šï¼‰çš„æƒ…å†µä¸‹ï¼ŒTwISTçš„ä¼˜åŠ¿æœ€ä¸ºæ˜¾è‘—ï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼›ä¾‹å¦‚ï¼Œè¾¾åˆ°23.14 PPLï¼Œè€Œæœ€æ¥è¿‘çš„å…ˆå‰æ–¹æ³•ä¸º31.64ã€‚ä¸åŒäºéç»“æ„åŒ–ä¿®å‰ªï¼ŒTwISTäº§ç”Ÿçš„æ˜¯ç»“æ„åŒ–ã€å¯†é›†çŸ©é˜µï¼Œåœ¨å•†å“ç¡¬ä»¶ï¼ˆå¦‚ä¸æ”¯æŒé«˜æ•ˆç¨€ç–è®¡ç®—çš„CPUï¼‰ä¸Šæä¾›å®é™…çš„æ¨ç†é€Ÿåº¦æå‡å’Œå†…å­˜å‡å°‘ã€‚TwISTæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒæ—¶é—´è·¯å¾„ï¼Œå¯ä»¥éƒ¨ç½²ç¨€ç–LLMï¼Œè€Œæ— éœ€é¢å¤–çš„å¾®è°ƒæˆ–æ¢å¤å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TwISTï¼Œä¸€ä¸ªç”¨äºé«˜æ•ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¨€ç–åŒ–çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ã€‚TwISTèƒ½å¤Ÿå¹¶è¡Œè®­ç»ƒå¤šä¸ªå­ç½‘ç»œï¼Œå®šæœŸèšåˆå‚æ•°ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡æ–°é‡‡æ ·æ–°çš„å­ç½‘ç»œã€‚è¯¥è¿‡ç¨‹èƒ½å¤Ÿè¯†åˆ«é«˜è´¨é‡çš„å­ç½‘ç»œï¼ˆå³â€œä¼˜è´¨ç¥¨è¯â€ï¼‰ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒåç¨‹åºï¼Œå¦‚æ ¡å‡†æˆ–åŸºäºHessiançš„æ¢å¤ã€‚å› æ­¤ï¼ŒTwISTå®ç°äº†éƒ¨ç½²æ—¶çš„é›¶æˆæœ¬ä¿®å‰ªï¼ŒåŒæ—¶è¾¾åˆ°äº†ä¸æœ€æ–°è®­ç»ƒåç¨€ç–åŒ–æ–¹æ³•ç«äº‰çš„è¡¨ç°ã€‚åœ¨æ¿€çƒˆçš„ç¨€ç–æ€§ä¸‹ï¼ˆä¾‹å¦‚50%ä»¥ä¸Šï¼‰ï¼ŒTwISTçš„ä¼˜åŠ¿æœ€ä¸ºæ˜¾è‘—ï¼Œä¾‹å¦‚è¾¾åˆ°23.14çš„å›°æƒ‘åº¦ï¼Œè€Œæœ€æ¥è¿‘çš„å…ˆå‰æ–¹æ³•åˆ™ä¸º31.64ã€‚ä¸åŒäºéç»“æ„åŒ–ä¿®å‰ªï¼ŒTwISTèƒ½å¤Ÿäº§ç”Ÿç»“æ„åŒ–çš„å¯†é›†çŸ©é˜µï¼Œåœ¨å®é™…ç¡¬ä»¶ï¼ˆå¦‚CPUï¼‰ä¸Šè¿›è¡Œæ¨ç†åŠ é€Ÿå’Œå†…å­˜ç¼©å‡ï¼Œè¿™äº›ç¡¬ä»¶ä¸æ”¯æŒé«˜æ•ˆçš„ç¨€ç–è®¡ç®—ã€‚TwISTä¸ºè®­ç»ƒæœŸé—´çš„å¯éƒ¨ç½²ç¨€ç–LLMæä¾›äº†ä¸€æ¡é«˜æ•ˆè·¯å¾„ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒæˆ–æ¢å¤å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TwISTæ˜¯ä¸€ä¸ªç”¨äºLLMç¨€ç–åŒ–çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯†åˆ«é«˜è´¨é‡çš„å­ç½‘ç»œã€‚</li>
<li>TwISTé€šè¿‡å¹¶è¡Œè®­ç»ƒå¤šä¸ªå­ç½‘ç»œï¼Œå®šæœŸèšåˆå‚æ•°ï¼Œå¹¶é‡æ–°é‡‡æ ·æ–°çš„å­ç½‘ç»œæ¥å·¥ä½œã€‚</li>
<li>TwISTå®ç°äº†éƒ¨ç½²æ—¶çš„é›¶æˆæœ¬ä¿®å‰ªã€‚</li>
<li>TwISTåœ¨æ¿€çƒˆçš„ç¨€ç–æ€§ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¾‹å¦‚è¾¾åˆ°è¾ƒä½çš„å›°æƒ‘åº¦ã€‚</li>
<li>TwISTä¸åŒäºéç»“æ„åŒ–ä¿®å‰ªï¼Œèƒ½å¤Ÿäº§ç”Ÿç»“æ„åŒ–çš„å¯†é›†çŸ©é˜µã€‚</li>
<li>TwISTé€‚ç”¨äºå•†å“ç¡¬ä»¶ï¼Œå¦‚CPUï¼Œèƒ½å¤Ÿå®ç°æ¨ç†åŠ é€Ÿå’Œå†…å­˜ç¼©å‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94866bd092105a40c6f0b0fbf8f5de29" align="middle">
<img src="https://picx.zhimg.com/v2-96e8dcbf170cd37203b545bc289576b2" align="middle">
<img src="https://picx.zhimg.com/v2-3fd76c8a6c14585324c0b9251da1bb07" align="middle">
<img src="https://picx.zhimg.com/v2-da15195b92cefe4cd42b6b5153e936d5" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Secure-Code-Generation-at-Scale-with-Reflexion"><a href="#Secure-Code-Generation-at-Scale-with-Reflexion" class="headerlink" title="Secure Code Generation at Scale with Reflexion"></a>Secure Code Generation at Scale with Reflexion</h2><p><strong>Authors:Arup Datta, Ahmed Aljohani, Hyunsook Do</strong></p>
<p>Large language models (LLMs) are now widely used to draft and refactor code, but code that works is not necessarily secure. We evaluate secure code generation using the Instruct Prime, which eliminated compliance-required prompts and cue contamination, and evaluate five instruction-tuned code LLMs using a zero-shot baseline and a three-round reflexion prompting approach. Security is measured using the Insecure Code Detector (ICD), and results are reported by measuring Repair, Regression, and NetGain metrics, considering the programming language and CWE family. Our findings show that insecurity remains common at the first round: roughly 25-33% of programs are insecure at a zero-shot baseline (t0 ). Weak cryptography&#x2F;config-dependent bugs are the hardest to avoid while templated ones like XSS, code injection, and hard-coded secrets are handled more reliably. Python yields the highest secure rates; C and C# are the lowest, with Java, JS, PHP, and C++ in the middle. Reflexion prompting improves security for all models, improving average accuracy from 70.74% at t0 to 79.43% at t3 , with the largest gains in the first round followed by diminishing returns. The trends with Repair, Regression, and NetGain metrics show that applying one to two rounds produces most of the benefits. A replication package is available at <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.17065846">https://doi.org/10.5281/zenodo.17065846</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç°åœ¨å¹¿æ³›ç”¨äºç¼–å†™å’Œé‡æ„ä»£ç ï¼Œä½†èƒ½å¤Ÿè¿è¡Œçš„ä»£ç å¹¶ä¸ä¸€å®šå®‰å…¨ã€‚æˆ‘ä»¬ä½¿ç”¨Instruct Primeå¯¹å®‰å…¨ä»£ç ç”Ÿæˆè¿›è¡Œè¯„ä¼°ï¼Œè¯¥å·¥å…·æ¶ˆé™¤äº†åˆè§„æ€§æ‰€éœ€çš„æç¤ºå’Œçº¿ç´¢æ±¡æŸ“ï¼Œå¹¶é‡‡ç”¨é›¶åŸºå‡†çº¿å’Œä¸‰è½®åæ€æç¤ºæ–¹æ³•å¯¹äº”ä¸ªæŒ‡ä»¤è°ƒæ•´å‹ä»£ç LLMè¿›è¡Œè¯„ä¼°ã€‚å®‰å…¨æ€§é€šè¿‡ä¸å®‰å…¨ä»£ç æ£€æµ‹å™¨ï¼ˆICDï¼‰æ¥è¡¡é‡ï¼Œç»“æœé€šè¿‡è¡¡é‡ä¿®å¤ã€å›å½’å’ŒNetGainæŒ‡æ ‡æ¥æŠ¥å‘Šï¼ŒåŒæ—¶è€ƒè™‘ç¼–ç¨‹è¯­è¨€å’ŒCWEå®¶æ—ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œåœ¨ç¬¬ä¸€è½®ä¸­ï¼Œä¸å®‰å…¨çš„æƒ…å†µä»ç„¶å¾ˆå¸¸è§ï¼šå¤§çº¦25-33%çš„ç¨‹åºåœ¨é›¶åŸºå‡†ï¼ˆt0ï¼‰æ—¶æ˜¯ä¸å®‰å…¨çš„ã€‚é¿å…å¼±åŠ å¯†&#x2F;é…ç½®ä¾èµ–çš„æ¼æ´æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„ï¼Œè€Œæ¨¡æ¿åŒ–çš„æ¼æ´å¦‚è·¨ç«™è„šæœ¬æ”»å‡»ã€ä»£ç æ³¨å…¥å’Œç¡¬ç¼–ç å¯†é’¥åˆ™å¤„ç†å¾—æ›´å¯é ã€‚Pythonçš„å®‰å…¨ç‡æœ€é«˜ï¼›Cå’ŒC#çš„å®‰å…¨ç‡æœ€ä½ï¼ŒJavaã€JSã€PHPå’ŒC++å¤„äºä¸­é—´æ°´å¹³ã€‚åæ€æç¤ºå¯ä»¥æé«˜æ‰€æœ‰æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œä»åˆå§‹çš„70.74%æé«˜åˆ°ç¬¬ä¸‰è½®æ—¶çš„79.43%ï¼Œåœ¨ç¬¬ä¸€è½®ä¸­è·å¾—çš„æ”¶ç›Šæœ€å¤§ï¼Œéšåæ”¶ç›Šé€’å‡ã€‚åº”ç”¨ä¸€åˆ°ä¸¤è½®çš„è¶‹åŠ¿æ˜¾ç¤ºå‡ºä¿®å¤ã€å›å½’å’ŒNetGainæŒ‡æ ‡çš„å¤§éƒ¨åˆ†æ•ˆç›Šã€‚å¤åˆ¶åŒ…å¯åœ¨<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.17065846%E6%89%BE%E5%88%B0%E3%80%82">https://doi.org/10.5281/zenodo.17065846æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03898v1">PDF</a> Accepted for publication at the 2nd IEEE International Conference on   AI-powered Software (AIware 2025)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸ç”¨äºç¼–å†™å’Œé‡æ„ä»£ç ï¼Œä½†å¯è¿è¡Œçš„ä»£ç ä¸ä¸€å®šå®‰å…¨ã€‚æœ¬ç ”ç©¶ä½¿ç”¨Instruct Primeè¯„ä¼°å®‰å…¨ä»£ç ç”Ÿæˆï¼Œé€šè¿‡æ¶ˆé™¤åˆè§„æç¤ºå’Œçº¿ç´¢æ±¡æŸ“ï¼Œå¹¶é‡‡ç”¨é›¶åŸºå‡†çº¿å’Œä¸‰è½®åæ€æç¤ºæ³•å¯¹äº”ä¸ªæŒ‡ä»¤è°ƒæ•´å‹ä»£ç LLMè¿›è¡Œè¯„ä¼°ã€‚å®‰å…¨æ€§é€šè¿‡ä¸å®‰å…¨ä»£ç æ£€æµ‹å™¨ï¼ˆICDï¼‰è¡¡é‡ï¼Œç»“æœé€šè¿‡ä¿®å¤ã€å›å½’å’Œå‡€æ”¶ç›ŠæŒ‡æ ‡æŠ¥å‘Šï¼ŒåŒæ—¶è€ƒè™‘ç¼–ç¨‹è¯­è¨€å’ŒCWEå®¶æ—ã€‚ç ”ç©¶å‘ç°ï¼Œç¬¬ä¸€è½®æ—¶ä»å­˜åœ¨æ™®éçš„ä¸å®‰å…¨æ€§ï¼šå¤§çº¦25-33%çš„ç¨‹åºåœ¨é›¶åŸºå‡†çº¿ï¼ˆt0ï¼‰æ—¶å­˜åœ¨ä¸å®‰å…¨æ€§ã€‚é¿å…å¼±åŠ å¯†&#x2F;é…ç½®ä¾èµ–çš„bugæœ€ä¸ºå›°éš¾ï¼Œè€Œæ¨¡æ¿åŒ–çš„å¦‚è·¨ç«™è„šæœ¬æ”»å‡»ã€ä»£ç æ³¨å…¥å’Œç¡¬ç¼–ç ç§˜å¯†ç­‰åˆ™æ›´ä¸ºå¯é åœ°å¤„ç†ã€‚Pythonçš„å®‰å…¨ç‡æœ€é«˜ï¼›Cå’ŒC#æœ€ä½ï¼ŒJavaã€JSã€PHPå’ŒC++åœ¨ä¸­é—´ã€‚åæ€æç¤ºæé«˜äº†æ‰€æœ‰æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œä»t0æ—¶çš„å¹³å‡å‡†ç¡®ç‡70.74%æé«˜åˆ°t3æ—¶çš„79.43%ï¼Œé¦–è½®æ”¶ç›Šæœ€å¤§ï¼Œéšåæ”¶ç›Šé€’å‡ã€‚ä¿®å¤ã€å›å½’å’Œå‡€æ”¶ç›ŠæŒ‡æ ‡çš„è¶‹åŠ¿æ˜¾ç¤ºï¼Œåº”ç”¨ä¸€è‡³ä¸¤è½®ä¼šäº§ç”Ÿå¤§éƒ¨åˆ†æ•ˆç›Šã€‚ç›¸å…³ç ”ç©¶å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.17065846%E8%8E%B7%E5%8F%96%E5%A4%9D%E5%88%B6%E5%8C%85%E3%80%82">https://doi.org/10.5281/zenodo.17065846è·å–å¤åˆ¶åŒ…ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMå¸¸ç”¨äºä»£ç ç”Ÿæˆï¼Œä½†ç”Ÿæˆçš„ä»£ç å¯èƒ½å­˜åœ¨å®‰å…¨éšæ‚£ã€‚</li>
<li>Instruct Primeèƒ½æœ‰æ•ˆè¯„ä¼°å®‰å…¨ä»£ç ç”Ÿæˆï¼Œæ¶ˆé™¤ä¸å¿…è¦çš„æç¤ºå’Œçº¿ç´¢æ±¡æŸ“ã€‚</li>
<li>é€šè¿‡ä¸å®‰å…¨ä»£ç æ£€æµ‹å™¨ï¼ˆICDï¼‰è¡¡é‡å®‰å…¨æ€§ï¼Œæ¶‰åŠç¼–ç¨‹è¯­è¨€å’ŒCWEå®¶æ—ã€‚</li>
<li>åˆå§‹é˜¶æ®µå­˜åœ¨è¾ƒé«˜æ¯”ä¾‹çš„ä¸å®‰å…¨ä»£ç ï¼ˆçº¦25-33%ï¼‰ã€‚</li>
<li>é¿å…æŸäº›ç±»å‹çš„bugï¼ˆå¦‚å¼±åŠ å¯†&#x2F;é…ç½®ä¾èµ–ï¼‰ç›¸å¯¹å›°éš¾ã€‚</li>
<li>Pythonçš„å®‰å…¨ä»£ç ç”Ÿæˆè¡¨ç°æœ€ä½³ï¼Œè€ŒCå’ŒC#çš„è¡¨ç°è¾ƒå·®ã€‚å…¶ä»–è¯­è¨€å¦‚Javaã€JSã€PHPå’ŒC++è¡¨ç°å±…ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c13d67c667af6ea2586bc8d3632a9a27" align="middle">
<img src="https://picx.zhimg.com/v2-a07326ce9cd3b3a6c3eec1391c1a667b" align="middle">
<img src="https://picx.zhimg.com/v2-c29e588013b9d9ffb78030ec9747fb65" align="middle">
<img src="https://picx.zhimg.com/v2-02f154908603f2870a00bd607a3fd776" align="middle">
<img src="https://picx.zhimg.com/v2-75a2913ce6c0a60234b558f421a33154" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="How-Different-Tokenization-Algorithms-Impact-LLMs-and-Transformer-Models-for-Binary-Code-Analysis"><a href="#How-Different-Tokenization-Algorithms-Impact-LLMs-and-Transformer-Models-for-Binary-Code-Analysis" class="headerlink" title="How Different Tokenization Algorithms Impact LLMs and Transformer Models   for Binary Code Analysis"></a>How Different Tokenization Algorithms Impact LLMs and Transformer Models   for Binary Code Analysis</h2><p><strong>Authors:Ahmed Mostafa, Raisul Arefin Nahid, Samuel Mulder</strong></p>
<p>Tokenization is fundamental in assembly code analysis, impacting intrinsic characteristics like vocabulary size, semantic coverage, and extrinsic performance in downstream tasks. Despite its significance, tokenization in the context of assembly code remains an underexplored area. This study aims to address this gap by evaluating the intrinsic properties of Natural Language Processing (NLP) tokenization models and parameter choices, such as vocabulary size. We explore preprocessing customization options and pre-tokenization rules tailored to the unique characteristics of assembly code. Additionally, we assess their impact on downstream tasks like function signature prediction â€“ a critical problem in binary code analysis.   To this end, we conduct a thorough study on various tokenization models, systematically analyzing their efficiency in encoding assembly instructions and capturing semantic nuances. Through intrinsic evaluations, we compare tokenizers based on tokenization efficiency, vocabulary compression, and representational fidelity for assembly code. Using state-of-the-art pre-trained models such as the decoder-only Large Language Model (LLM) Llama 3.2, the encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate the effectiveness of these tokenizers across multiple performance metrics. Preliminary findings indicate that tokenizer choice significantly influences downstream performance, with intrinsic metrics providing partial but incomplete predictability of extrinsic evaluation outcomes. These results reveal complex trade-offs between intrinsic tokenizer properties and their utility in practical assembly code tasks. Ultimately, this study provides valuable insights into optimizing tokenization models for low-level code analysis, contributing to the robustness and scalability of Natural Language Model (NLM)-based binary analysis workflows. </p>
<blockquote>
<p>ä»¤ç‰ŒåŒ–åœ¨æ±‡ç¼–ä»£ç åˆ†æä¸­è‡³å…³é‡è¦ï¼Œå®ƒå½±å“è¯æ±‡å¤§å°ã€è¯­ä¹‰è¦†ç›–ç‡å’Œä¸‹æ¸¸ä»»åŠ¡çš„å¤–åœ¨æ€§èƒ½ç­‰å†…åœ¨ç‰¹å¾ã€‚å°½ç®¡å…¶æ„ä¹‰é‡å¤§ï¼Œä½†åœ¨æ±‡ç¼–ä»£ç çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œä»¤ç‰ŒåŒ–ä»ç„¶æ˜¯ä¸€ä¸ªå°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡è¯„ä¼°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»¤ç‰ŒåŒ–æ¨¡å‹çš„å†…åœ¨å±æ€§ä»¥åŠå‚æ•°é€‰æ‹©ï¼ˆå¦‚è¯æ±‡å¤§å°ï¼‰æ¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬æ¢ç´¢äº†é’ˆå¯¹æ±‡ç¼–ä»£ç ç‹¬ç‰¹ç‰¹å¾çš„é¢„å¤„ç†å®šåˆ¶é€‰é¡¹å’Œé¢„ä»¤ç‰ŒåŒ–è§„åˆ™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†å®ƒä»¬å¯¹å‡½æ•°ç­¾åé¢„æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“â€”â€”è¿™æ˜¯äºŒè¿›åˆ¶ä»£ç åˆ†æä¸­çš„ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹å„ç§ä»¤ç‰ŒåŒ–æ¨¡å‹è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œç³»ç»Ÿåˆ†æå®ƒä»¬åœ¨ç¼–ç æ±‡ç¼–æŒ‡ä»¤å’Œæ•æ‰è¯­ä¹‰ç»†å¾®å·®åˆ«æ–¹é¢çš„æ•ˆç‡ã€‚é€šè¿‡å†…åœ¨è¯„ä¼°ï¼Œæˆ‘ä»¬åŸºäºä»¤ç‰ŒåŒ–æ•ˆç‡ã€è¯æ±‡å‹ç¼©å’Œæ±‡ç¼–ä»£ç çš„ä»£è¡¨æ€§ä¿çœŸåº¦æ¥æ¯”è¾ƒä»¤ç‰ŒåŒ–å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚åªè§£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰Llama 3.2ã€åªç¼–ç çš„å˜å‹å™¨BERTå’Œç¼–ç å™¨è§£ç å™¨æ¨¡å‹BARTï¼Œè¯„ä¼°è¿™äº›ä»¤ç‰ŒåŒ–å™¨åœ¨å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚åˆæ­¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»¤ç‰ŒåŒ–å™¨çš„é€‰æ‹©å¯¹ä¸‹æ¸¸æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œå†…åœ¨æŒ‡æ ‡æä¾›äº†å¯¹å¤–éƒ¨è¯„ä¼°ç»“æœçš„éƒ¨åˆ†ä½†ä¸å®Œå…¨çš„å¯é¢„æµ‹æ€§ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†å†…åœ¨ä»¤ç‰ŒåŒ–å™¨ç‰¹æ€§ä¸å…¶åœ¨å®é™…æ±‡ç¼–ä»£ç ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ä¹‹é—´çš„å¤æ‚æƒè¡¡ã€‚æœ€ç»ˆï¼Œæœ¬ç ”ç©¶ä¸ºä¼˜åŒ–ç”¨äºä½çº§ä»£ç åˆ†æçš„ä»¤ç‰ŒåŒ–æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œä¸ºåŸºäºè‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆNLMï¼‰çš„äºŒè¿›åˆ¶åˆ†æå·¥ä½œæµç¨‹çš„ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03825v1">PDF</a> Publication Notice. This paper was published in the BAR 2025 Workshop   (with NDSS 2025) and is for research and educational use. Copyright   \c{opyright} 2025 Internet Society. All rights reserved. Personal&#x2F;classroom   reproduction is permitted with this notice and full paper citation. All other   uses, including commercial, require prior written permission from the   Internet Society</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä»¤ç‰ŒåŒ–åœ¨æ±‡ç¼–ä»£ç åˆ†æä¸­çš„é‡è¦æ€§ï¼Œå¹¶æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»¤ç‰ŒåŒ–æ¨¡å‹çš„å†…åœ¨ç‰¹æ€§åŠå…¶å‚æ•°é€‰æ‹©å¯¹æ±‡ç¼–ä»£ç åˆ†æçš„å½±å“ã€‚æ–‡ç« é€šè¿‡è¯„ä¼°å¤šç§ä»¤ç‰ŒåŒ–æ¨¡å‹åœ¨ç¼–ç æ±‡ç¼–æŒ‡ä»¤å’Œæ•æ‰è¯­ä¹‰ç»†å¾®å·®åˆ«æ–¹é¢çš„æ•ˆç‡ï¼Œè¿›è¡Œäº†ä¸€é¡¹æ·±å…¥ç ”ç©¶ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†ä¸åŒä»¤ç‰ŒåŒ–æ¨¡å‹åœ¨å‡½æ•°ç­¾åé¢„æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚åˆæ­¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»¤ç‰ŒåŒ–æ¨¡å‹çš„é€‰æ‹©å¯¹ä¸‹æ¸¸æ€§èƒ½å…·æœ‰æ˜¾è‘—å½±å“ã€‚æœ€ç»ˆï¼Œè¯¥ç ”ç©¶ä¸ºä¼˜åŒ–ç”¨äºä½çº§åˆ«ä»£ç åˆ†æçš„ä»¤ç‰ŒåŒ–æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œæœ‰åŠ©äºå¢å¼ºè‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆNLMï¼‰åœ¨äºŒè¿›åˆ¶åˆ†æå·¥ä½œæµç¨‹ä¸­çš„ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»¤ç‰ŒåŒ–åœ¨æ±‡ç¼–ä»£ç åˆ†æä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå½±å“è¯æ±‡å¤§å°ã€è¯­ä¹‰è¦†ç›–ç­‰å†…åœ¨ç‰¹æ€§å’Œä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>NLPä»¤ç‰ŒåŒ–æ¨¡å‹çš„å†…åœ¨ç‰¹æ€§å’Œå‚æ•°é€‰æ‹©æ˜¯ç ”ç©¶çš„é‡ç‚¹ï¼Œå¦‚è¯æ±‡å¤§å°ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šç§ä»¤ç‰ŒåŒ–æ¨¡å‹åœ¨ç¼–ç æ±‡ç¼–æŒ‡ä»¤å’Œæ•æ‰è¯­ä¹‰ç»†å¾®å·®åˆ«æ–¹é¢çš„æ•ˆç‡ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰Llama 3.2ç­‰ï¼Œè¯„ä»·äº†ä»¤ç‰ŒåŒ–æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åˆæ­¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»¤ç‰ŒåŒ–æ¨¡å‹çš„é€‰æ‹©å¯¹ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>å†…åœ¨è¯„ä¼°æŒ‡æ ‡åªèƒ½éƒ¨åˆ†é¢„æµ‹å¤–åœ¨è¯„ä¼°ç»“æœï¼Œå­˜åœ¨å¤æ‚çš„æƒè¡¡å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b15369fb4b2b9fd71501b34e542d8704" align="middle">
<img src="https://picx.zhimg.com/v2-e65e9d156613173454c68dcec473e491" align="middle">
<img src="https://picx.zhimg.com/v2-d55590c2cdb801b905633c5f9b342171" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SurgViVQA-Temporally-Grounded-Video-Question-Answering-for-Surgical-Scene-Understanding"><a href="#SurgViVQA-Temporally-Grounded-Video-Question-Answering-for-Surgical-Scene-Understanding" class="headerlink" title="SurgViVQA: Temporally-Grounded Video Question Answering for Surgical   Scene Understanding"></a>SurgViVQA: Temporally-Grounded Video Question Answering for Surgical   Scene Understanding</h2><p><strong>Authors:Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque</strong></p>
<p>Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Videoâ€“Text Encoder to fuse video and question features, capturing temporal cues such as motion and toolâ€“tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11% on REAL-Colon-VQA and +9% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at <a target="_blank" rel="noopener" href="https://github.com/madratak/SurgViVQA">https://github.com/madratak/SurgViVQA</a>. </p>
<blockquote>
<p>æ‰‹æœ¯é¢†åŸŸçš„è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰æ—¨åœ¨é€šè¿‡ä½¿AIæ¨¡å‹èƒ½å¤Ÿæ¨ç†å‡ºæ—¶é—´ä¸Šè¿è´¯çš„äº‹ä»¶ï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„å¸§ï¼Œä»è€Œæé«˜æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„ç†è§£ã€‚å½“å‰çš„æ–¹æ³•ä»…é™äºé™æ€å›¾åƒç‰¹å¾ï¼Œè€Œå¯ç”¨çš„æ•°æ®é›†é€šå¸¸ç¼ºä¹æ—¶é—´æ³¨é‡Šï¼Œå¿½ç•¥äº†å¯¹å‡†ç¡®ç¨‹åºè§£é‡Šè‡³å…³é‡è¦çš„åŠ¨æ€ã€‚æˆ‘ä»¬æå‡ºäº†SurgViVQAï¼Œä¸€ä¸ªæ‰‹æœ¯è§†é¢‘é—®ç­”æ¨¡å‹ï¼Œå®ƒå°†è§†è§‰æ¨ç†ä»é™æ€å›¾åƒæ‰©å±•åˆ°åŠ¨æ€æ‰‹æœ¯åœºæ™¯ã€‚å®ƒä½¿ç”¨é®ç½©è§†é¢‘æ–‡æœ¬ç¼–ç å™¨æ¥èåˆè§†é¢‘å’Œé—®é¢˜çš„ç‰¹å¾ï¼Œæ•æ‰è¯¸å¦‚è¿åŠ¨å’Œå·¥å…·ç»„ç»‡äº¤äº’ä¹‹ç±»çš„ä¸´æ—¶çº¿ç´¢ï¼Œç„¶åç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†è¿™äº›çº¿ç´¢è§£ç æˆè¿è´¯çš„ç­”æ¡ˆã€‚ä¸ºäº†è¯„ä¼°å…¶æ€§èƒ½ï¼Œæˆ‘ä»¬æ•´ç†å‡ºäº†REAL-Colon-VQAæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ç»„ç»“è‚ é•œè§†é¢‘æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸è¿åŠ¨ç›¸å…³çš„é—®é¢˜å’Œè¯Šæ–­å±æ€§ï¼Œä»¥åŠé‡æ–°è¡¨è¿°æˆ–è¯­ä¹‰æ›´æ”¹çš„æ¨¡æ¿å¤–é—®é¢˜ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨REAL-Colon-VQAå’Œå…¬å…±EndoVis18-VQAæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒSurgViVQAåœ¨åŸºäºå›¾åƒçš„VQAåŸºå‡†æ¨¡å‹ä¸Šè¡¨ç°æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®è¯å‡†ç¡®æ€§æ–¹é¢ï¼Œåœ¨REAL-Colon-VQAä¸Šæ¯”PitVQAé«˜å‡º+11%ï¼Œåœ¨EndoVis18-VQAä¸Šé«˜å‡º+9%ã€‚å¯¹é—®é¢˜è¿›è¡Œçš„æ‰°åŠ¨ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å…¶å¯¹é—®é¢˜è¡¨è¿°çš„æ”¹è¿›å’Œç¨³å¥æ€§ã€‚SurgViVQAå’ŒREAL-Colon-VQAæ•°æ®é›†ä¸ºæ‰‹æœ¯è§†é¢‘é—®ç­”ä¸­çš„æ—¶é—´æ„ŸçŸ¥ç†è§£æä¾›äº†æ¡†æ¶ï¼Œä½¿AIæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£é‡ŠåŠ¨æ€ç¨‹åºä¸Šä¸‹æ–‡ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/madratak/SurgViVQA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/madratak/SurgViVQAä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03325v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ‰‹æœ¯é¢†åŸŸçš„è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰æ—¨åœ¨é€šè¿‡ä½¿AIæ¨¡å‹èƒ½å¤Ÿæ¨ç†å‡ºæ—¶é—´ä¸Šè¿è´¯çš„äº‹ä»¶è€Œéå­¤ç«‹çš„å¸§ï¼Œä»è€Œæå‡æœ¯ä¸­ç†è§£ã€‚å½“å‰çš„æ–¹æ³•ä»…é™äºé™æ€å›¾åƒç‰¹å¾ï¼Œè€Œå¯ç”¨çš„æ•°æ®é›†å¾€å¾€ç¼ºä¹æ—¶é—´æ³¨é‡Šï¼Œå¿½ç•¥äº†å¯¹å‡†ç¡®ç¨‹åºè§£è¯»è‡³å…³é‡è¦çš„åŠ¨æ€è¦ç´ ã€‚æˆ‘ä»¬æå‡ºäº†SurgViVQAï¼Œä¸€ç§æ‰‹æœ¯è§†é¢‘é—®ç­”æ¨¡å‹ï¼Œå®ƒå°†è§†è§‰æ¨ç†ä»é™æ€å›¾åƒæ‰©å±•åˆ°åŠ¨æ€æ‰‹æœ¯åœºæ™¯ã€‚å®ƒä½¿ç”¨æ©ç è§†é¢‘-æ–‡æœ¬ç¼–ç å™¨èåˆè§†é¢‘å’Œé—®é¢˜ç‰¹å¾ï¼Œæ•æ‰å¦‚è¿åŠ¨å’Œå·¥å…·-ç»„ç»‡äº¤äº’ç­‰æ—¶é—´çº¿ç´¢ï¼Œç„¶åç”±å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£ç æˆè¿è´¯çš„ç­”æ¡ˆã€‚ä¸ºäº†è¯„ä¼°å…¶æ€§èƒ½ï¼Œæˆ‘ä»¬ç²¾é€‰äº†REAL-Colon-VQAæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¿åŠ¨ç›¸å…³é—®é¢˜ã€è¯Šæ–­å±æ€§å’Œé‡æ–°è¡¨è¿°æˆ–è¯­ä¹‰æ”¹å˜çš„å‡ºæ¨¡æ¿é—®é¢˜ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨REAL-Colon-VQAå’Œå…¬å…±EndoVis18-VQAæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒSurgViVQAä¼˜äºç°æœ‰çš„åŸºäºå›¾åƒçš„VQAåŸºå‡†æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³é”®è¯å‡†ç¡®ç‡æ–¹é¢ï¼Œç›¸è¾ƒäºPitVQAåœ¨REAL-Colon-VQAä¸Šæé«˜äº†+11%ï¼Œåœ¨EndoVis18-VQAä¸Šæé«˜äº†+9%ã€‚å¯¹é—®é¢˜çš„æ‰°åŠ¨ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å…¶æ”¹è¿›çš„ä¸€èˆ¬æ€§å’Œå¯¹é—®é¢˜è¡¨è¿°å˜åŒ–çš„ç¨³å¥æ€§ã€‚SurgViVQAå’ŒREAL-Colon-VQAæ•°æ®é›†ä¸ºæ‰‹æœ¯è§†é¢‘é—®ç­”ä¸­çš„æ—¶é—´æ„ŸçŸ¥ç†è§£æä¾›äº†æ¡†æ¶ï¼Œä½¿AIæ¨¡å‹æ›´æœ‰æ•ˆåœ°è§£é‡ŠåŠ¨æ€ç¨‹åºä¸Šä¸‹æ–‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰åœ¨æ‰‹æœ¯é¢†åŸŸçš„é‡è¦æ€§åœ¨äºæå‡AIæ¨¡å‹å¯¹æœ¯ä¸­æƒ…å†µçš„äº†è§£ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨ç†åœ¨æ—¶é—´ä¸Šæœ‰è¿è´¯çš„äº‹ä»¶ã€‚</li>
<li>å½“å‰æ‰‹æœ¯é¢†åŸŸçš„VideoQAæ¨¡å‹ä¸»è¦å±€é™äºé™æ€å›¾åƒç‰¹å¾ï¼Œç¼ºä¹æ•æ‰åŠ¨æ€æ‰‹æœ¯åœºæ™¯çš„è¦ç´ ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ‰‹æœ¯VideoQAæ¨¡å‹â€”â€”SurgViVQAï¼Œèƒ½å¤Ÿæ‰©å±•è§†è§‰æ¨ç†è‡³åŠ¨æ€æ‰‹æœ¯åœºæ™¯ã€‚</li>
<li>SurgViVQAä½¿ç”¨Masked Video-Text Encoderèåˆè§†é¢‘å’Œé—®é¢˜ç‰¹å¾ï¼Œæ•æ‰æ—¶é—´çº¿ç´¢å¦‚è¿åŠ¨å’Œå·¥å…·ä¸ç»„ç»‡çš„äº¤äº’ã€‚</li>
<li>å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£ç è§†é¢‘å’Œé—®é¢˜çš„èåˆç‰¹å¾ï¼Œç”Ÿæˆè¿è´¯çš„ç­”æ¡ˆã€‚</li>
<li>æ¨å‡ºæ–°çš„æ•°æ®é›†REAL-Colon-VQAç”¨äºè¯„ä¼°SurgViVQAæ€§èƒ½ï¼ŒåŒ…å«è¿åŠ¨ç›¸å…³é—®é¢˜ã€è¯Šæ–­å±æ€§å’Œé‡æ–°è¡¨è¿°çš„é—®é¢˜ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºSurgViVQAåœ¨å…³é”®è¯å‡†ç¡®ç‡ä¸Šä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œå¹¶å…·æœ‰è¾ƒå¥½çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-397ba56d83d35d0f8a6c089dcf138554" align="middle">
<img src="https://picx.zhimg.com/v2-15e5eac438d6c52fcd246c0dc94d8b68" align="middle">
<img src="https://picx.zhimg.com/v2-30b491b146ee6944c2ddecd7f914c555" align="middle">
<img src="https://picx.zhimg.com/v2-baffec2e7c2b47ff3365ec32e2b5e97e" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SnapStream-Efficient-Long-Sequence-Decoding-on-Dataflow-Accelerators"><a href="#SnapStream-Efficient-Long-Sequence-Decoding-on-Dataflow-Accelerators" class="headerlink" title="SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"></a>SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators</h2><p><strong>Authors:Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar</strong></p>
<p>The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching. </p>
<blockquote>
<p>éšç€æ”¯æŒè¶…è¿‡ç™¾äº¿å‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè¶…è¿‡åä¸‡ä¸Šä¸‹æ–‡é•¿åº¦çš„æ”¯æŒè¶Šæ¥è¶Šå¤šï¼Œå¯¹èŠ¯ç‰‡å†…å­˜çš„éœ€æ±‚ä¹Ÿåœ¨ä¸æ–­å¢åŠ ï¼Œä»¥æ”¯æŒå¤§è§„æ¨¡çš„KVç¼“å­˜ã€‚StreamingLLMå’ŒSnapKVç­‰æŠ€æœ¯å±•ç¤ºäº†å¦‚ä½•åœ¨ä¿æŒæ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶æ§åˆ¶KVç¼“å­˜å¤§å°ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯åœ¨å·¥ä¸šéƒ¨ç½²ä¸­å¹¶ä¸å¸¸ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨vLLMæˆ–SGLangç­‰æ¡†æ¶çš„æƒ…å†µä¸‹ã€‚åŸå› æœ‰ä¸¤æ–¹é¢ï¼šä¸€æ–¹é¢ï¼Œè¿™äº›æ¡†æ¶é‡‡ç”¨çš„é™æ€å›¾å’Œè¿ç»­æ‰¹å¤„ç†æ–¹æ³•ä½¿å¾—éš¾ä»¥å¯¹æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ç®—æ³•è¿›è¡Œä¿®æ”¹ï¼›å¦ä¸€æ–¹é¢ï¼Œè¿™äº›æŠ€æœ¯å¯¹ç°ä»£æŒ‡ä»¤éµå¾ªå’Œæ¨ç†æ¨¡å‹çš„å‡†ç¡®æ€§å½±å“å°šä¸æ¸…æ¥šï¼Œè¿™ä½¿å¾—å®ç°è¿™äº›æŠ€æœ¯çš„å¿…è¦æ€§å˜å¾—æ¨¡ç³Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†è¿™äº›å‡†ç¡®æ€§å½±å“åœ¨Llama-3.1-8B-Instructå’ŒDeepSeek-R1ä¸Šçš„è¡¨ç°ï¼Œå¹¶å¼€å‘äº†SnapStreamï¼Œè¿™æ˜¯ä¸€ç§å¯å¤§è§„æ¨¡éƒ¨ç½²çš„KVç¼“å­˜å‹ç¼©æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­å±•ç¤ºäº†SnapStreamåœ¨DeepSeek-671Bçš„16è·¯å¼ é‡å¹¶è¡Œéƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè¯¥ç¯å¢ƒè¿è¡Œåœ¨SambaNova SN40LåŠ é€Ÿå™¨ä¸Šï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¸º128kï¼Œæ¯ç§’å¯å¤„ç†é«˜è¾¾1832ä¸ªä»¤ç‰Œã€‚SnapStreamåœ¨LongBench-v2ã€AIME24å’ŒLiveCodeBenchä¸Šçš„å‡†ç¡®ç‡ç•¥æœ‰ä¸‹é™ï¼Œä½†åœ¨èŠ¯ç‰‡å†…å­˜ä½¿ç”¨ä¸Šæé«˜äº†å››å€ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨å…·æœ‰é™æ€å›¾å’Œè¿ç»­æ‰¹å¤„ç†çš„ç”Ÿäº§æ¨ç†ç³»ç»Ÿä¸­éƒ¨ç½²ç¨€ç–KVæ³¨æ„åŠ›æŠ€æœ¯çš„å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03092v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°è¶…è¿‡100Bï¼Œå¯¹èŠ¯ç‰‡å†…å­˜éœ€æ±‚å¢å¤§ã€‚StreamingLLMå’ŒSnapKVç­‰æŠ€æœ¯å¯æ§åˆ¶KVç¼“å­˜å¤§å°åŒæ—¶ä¿æŒæ¨¡å‹ç²¾åº¦ã€‚ç„¶è€Œï¼Œåœ¨vLLMæˆ–SGLangç­‰æ¡†æ¶ä¸­ï¼Œç”±äºé™æ€å›¾å’Œè¿ç»­æ‰¹å¤„ç†æ–¹æ³•çš„é‡‡ç”¨ï¼Œè¿™äº›æŠ€æœ¯å¹¶ä¸å¸¸ç”¨ã€‚æœ¬æ–‡æ¢ç´¢äº†è¿™äº›æŠ€æœ¯å¯¹Llama-3.1-8B-Instructå’ŒDeepSeek-R1å‡†ç¡®æ€§çš„å½±å“ï¼Œå¹¶å¼€å‘äº†SnapStreamæ–¹æ³•ï¼Œå¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ã€‚åœ¨SambaNova SN40LåŠ é€Ÿå™¨ä¸Šè¿è¡ŒDeepSeek-671Bæ—¶ï¼ŒSnapStreamå®ç°äº†å¯¹èŠ¯ç‰‡å†…å­˜ä½¿ç”¨çš„å››å€æ”¹è¿›ï¼Œå¹¶åœ¨LongBench-v2ã€AIME24å’ŒLiveCodeBenchä¸Šå¼•å…¥äº†æœ€å°çš„ç²¾åº¦æŸå¤±ã€‚è¿™æ˜¯é¦–ä¸ªåœ¨å…·æœ‰é™æ€å›¾å’Œè¿ç»­æ‰¹å¤„ç†çš„ç”Ÿäº§æ¨æ–­ç³»ç»Ÿä¸­éƒ¨ç½²ç¨€ç–KVæ³¨æ„åŠ›æŠ€æœ¯çš„å®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°å¢å¤šå¯¼è‡´å¯¹èŠ¯ç‰‡å†…å­˜éœ€æ±‚å¢åŠ ã€‚</li>
<li>StreamingLLMå’ŒSnapKVç­‰æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹ç²¾åº¦çš„åŒæ—¶æ§åˆ¶KVç¼“å­˜å¤§å°ã€‚</li>
<li>ç°æœ‰æ¡†æ¶å¦‚vLLMå’ŒSGLangå¾ˆå°‘ä½¿ç”¨è¿™äº›æŠ€æœ¯ï¼Œä¸»è¦åŸå› æ˜¯é™æ€å›¾å’Œè¿ç»­æ‰¹å¤„ç†æ–¹æ³•çš„é‡‡ç”¨ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†è¿™äº›æŠ€æœ¯å¯¹ç‰¹å®šæ¨¡å‹çš„å‡†ç¡®æ€§å½±å“ã€‚</li>
<li>å¼€å‘äº†SnapStreamæ–¹æ³•ï¼Œå¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œå®ç°å¯¹èŠ¯ç‰‡å†…å­˜ä½¿ç”¨çš„æ”¹è¿›ã€‚</li>
<li>SnapStreamåœ¨ç‰¹å®šæ¨¡å‹ä¸Šçš„å®é™…æ•ˆæœæ˜¯å®ç°äº†å¯¹èŠ¯ç‰‡å†…å­˜ä½¿ç”¨çš„å››å€æ”¹è¿›ï¼Œå¹¶å¼•å…¥äº†æœ€å°çš„ç²¾åº¦æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d73f710e29ca05753c65dd7c21e81c82" align="middle">
<img src="https://picx.zhimg.com/v2-981a3587aef050c4c8dad3d5d4fb9eeb" align="middle">
<img src="https://picx.zhimg.com/v2-c9bd64dc131fec4d49d7f96f0bce599d" align="middle">
<img src="https://picx.zhimg.com/v2-f55f17e198552c38e6aac98dc1b774bd" align="middle">
<img src="https://picx.zhimg.com/v2-8e7dd0ffa5a09ad06890e0c9733a1a40" align="middle">
<img src="https://picx.zhimg.com/v2-332d607fb21faf2e407977b9a4890e89" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="OceanAI-A-Conversational-Platform-for-Accurate-Transparent-Near-Real-Time-Oceanographic-Insights"><a href="#OceanAI-A-Conversational-Platform-for-Accurate-Transparent-Near-Real-Time-Oceanographic-Insights" class="headerlink" title="OceanAI: A Conversational Platform for Accurate, Transparent,   Near-Real-Time Oceanographic Insights"></a>OceanAI: A Conversational Platform for Accurate, Transparent,   Near-Real-Time Oceanographic Insights</h2><p><strong>Authors:Bowen Chen, Jayesh Gajbhar, Gregory Dusek, Rob Redmon, Patrick Hogan, Paul Liu, DelWayne Bohnenstiehl, Dongkuan Xu, Ruoying He</strong></p>
<p>Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified â€œhallucinationsâ€ undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as â€œWhat was Boston Harborâ€™s highest water level in 2024?â€ triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at <a target="_blank" rel="noopener" href="https://oceanai.ai4ocean.xyz/">https://oceanai.ai4ocean.xyz</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ç§‘å­¦é¢†åŸŸï¼Œç„¶è€Œé€šç”¨çš„å¯¹è¯å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿé€šå¸¸ä¼šç”Ÿæˆæœªç»è¯å®çš„â€œå¹»è§‰â€ï¼Œç ´åç§‘å­¦çš„ä¸¥è°¨æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†OceanAIï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹è¯å¹³å°ï¼Œå®ƒå°†å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªç„¶è¯­è¨€æµç•…æ€§ä¸å®æ—¶ã€å‚æ•°åŒ–çš„è®¿é—®å›½å®¶æµ·æ´‹å’Œå¤§æ°”ç®¡ç†å±€ï¼ˆNOAAï¼‰æƒå¨æµ·æ´‹å­¦æ•°æ®æµç›¸ç»“åˆã€‚æ¯ä¸ªæŸ¥è¯¢ï¼Œå¦‚â€œæ³¢å£«é¡¿æ¸¯å£åœ¨2024å¹´çš„æœ€é«˜æ°´ä½æ˜¯å¤šå°‘ï¼Ÿâ€éƒ½ä¼šè§¦å‘å®æ—¶APIè°ƒç”¨ï¼Œè¿™äº›è°ƒç”¨ä¼šè¯†åˆ«ã€è§£æå’Œç»¼åˆç›¸å…³çš„æ•°æ®é›†ï¼Œä»¥å¯å†ç”Ÿçš„è‡ªç„¶è¯­è¨€å“åº”å’Œæ•°æ®å¯è§†åŒ–å½¢å¼å‘ˆç°ã€‚åœ¨ä¸ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„AIèŠå¤©ç•Œé¢äº§å“çš„ç›²å¯¹æ¯”æµ‹è¯•ä¸­ï¼Œåªæœ‰OceanAIèƒ½å¤Ÿç”Ÿæˆå¸¦æœ‰åŸå§‹æ•°æ®å¼•ç”¨çš„NOAAæ¥æºå€¼ï¼›å…¶ä»–äº§å“è¦ä¹ˆæ‹’ç»å›ç­”ï¼Œè¦ä¹ˆæä¾›æœªç»æ”¯æŒçš„ç»“æœã€‚OceanAIè®¾è®¡ç”¨äºå¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿè¿æ¥åˆ°å¤šä¸ªNOAAæ•°æ®äº§å“å’Œå˜é‡ï¼Œæ”¯æŒæµ·æ´‹å±å®³é¢„æŠ¥ã€ç”Ÿæ€ç³»ç»Ÿè¯„ä¼°å’Œæ°´è´¨ç›‘æµ‹ç­‰åº”ç”¨ã€‚é€šè¿‡åŸºäºè¾“å‡ºå’Œå¯éªŒè¯çš„è§‚å¯Ÿï¼ŒOceanAIä¿ƒè¿›äº†é€æ˜åº¦ã€å¯é‡å¤æ€§å’Œä¿¡ä»»åº¦ï¼Œä¸ºæµ·æ´‹é¢†åŸŸçš„äººå·¥æ™ºèƒ½å†³ç­–æ”¯æŒæä¾›äº†å¯æ‰©å±•çš„æ¡†æ¶ã€‚å…¬å…±æ¼”ç¤ºç‰ˆæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://oceanai.ai4ocean.xyzè®¿é—®./">https://oceanai.ai4ocean.xyzè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01019v2">PDF</a> A related presentation will be given at the AGU(American Geophysical   Union) and AMS(American Meteorological Society) Annual Meetings</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ç§‘å­¦é¢†åŸŸï¼Œä½†é€šç”¨çš„å¯¹è¯å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿå¸¸å¸¸äº§ç”Ÿæœªç»è¯å®çš„â€œå¹»è§‰â€ï¼Œç ´åç§‘å­¦çš„ä¸¥è°¨æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†OceanAIï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹è¯å¹³å°ï¼Œå®ƒå°†å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªç„¶è¯­è¨€æµç•…æ€§ä¸å®æ—¶ã€å‚æ•°åŒ–çš„ç¾å›½å›½å®¶æµ·æ´‹å’Œå¤§æ°”ç®¡ç†å±€ï¼ˆNOAAï¼‰æµ·æ´‹å­¦æ•°æ®æµè®¿é—®ç›¸ç»“åˆã€‚ä¾‹å¦‚ï¼Œâ€œæ³¢å£«é¡¿æ¸¯å£åœ¨2024å¹´çš„æœ€é«˜æ°´ä½æ˜¯å¤šå°‘ï¼Ÿâ€è¿™æ ·çš„æŸ¥è¯¢ä¼šè§¦å‘å®æ—¶APIè°ƒç”¨ï¼Œè¯†åˆ«ã€è§£æå’Œç»¼åˆç›¸å…³æ•°æ®é›†ï¼Œä»¥å¯é‡ç°çš„è‡ªç„¶è¯­è¨€å›åº”å’Œæ•°æ®å¯è§†åŒ–å½¢å¼æä¾›ä¿¡æ¯ã€‚ä¸å…¶ä»–å¹¿æ³›ä½¿ç”¨çš„AIèŠå¤©ç•Œé¢äº§å“ç›¸æ¯”ï¼Œåªæœ‰OceanAIèƒ½æä¾›NOAAæ¥æºçš„å¸¦æœ‰åŸå§‹æ•°æ®å‚è€ƒçš„å€¼ï¼›å…¶ä»–äº§å“è¦ä¹ˆä¸å›ç­”ï¼Œè¦ä¹ˆæä¾›æ— æ”¯æŒçš„ç»“æœã€‚OceanAIè®¾è®¡ç”¨äºå¯æ‰©å±•æ€§ï¼Œå¯è¿æ¥åˆ°å¤šä¸ªNOAAæ•°æ®äº§å“å’Œå˜é‡ï¼Œæ”¯æŒæµ·æ´‹å±å®³é¢„æŠ¥ã€ç”Ÿæ€ç³»ç»Ÿè¯„ä¼°å’Œæ°´è´¨ç›‘æµ‹ç­‰åº”ç”¨ã€‚é€šè¿‡åŸºäºè§‚å¯Ÿå’Œå¯è§‚å¯Ÿæ•°æ®çš„è¾“å‡ºï¼ŒOceanAIæé«˜äº†é€æ˜åº¦ã€å¯é‡å¤æ€§å’Œä¿¡ä»»åº¦ï¼Œä¸ºæµ·æ´‹ä¸­çš„AIå†³ç­–æ”¯æŒæä¾›äº†å¯æ‰©å±•çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OceanAIæ˜¯ä¸€ä¸ªç»“åˆäº†è‡ªç„¶è¯­è¨€å¤„ç†å’Œå®æ—¶æµ·æ´‹å­¦æ•°æ®æµçš„å¯¹è¯å¹³å°ã€‚</li>
<li>å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æµç•…æ€§ï¼Œæä¾›å¯¹NOAAæƒå¨æµ·æ´‹å­¦æ•°æ®çš„å‚æ•°åŒ–å®æ—¶è®¿é—®ã€‚</li>
<li>OceanAIèƒ½é’ˆå¯¹ç‰¹å®šæŸ¥è¯¢å¦‚â€œæŸåœ°çš„æœ€é«˜æ°´ä½â€ç­‰ï¼Œæä¾›å¯å†ç”Ÿçš„è‡ªç„¶è¯­è¨€å›åº”å’Œæ•°æ®å¯è§†åŒ–ã€‚</li>
<li>ä¸å…¶ä»–AIèŠå¤©ç•Œé¢äº§å“ç›¸æ¯”ï¼ŒOceanAIæä¾›çš„ç­”æ¡ˆæ˜¯åŸºäºNOAAåŸå§‹æ•°æ®çš„ï¼Œå…·æœ‰æ•°æ®å‚è€ƒã€‚</li>
<li>è¯¥å¹³å°å…·æœ‰å¯æ‰©å±•æ€§ï¼Œèƒ½è¿æ¥åˆ°å¤šç§NOAAæ•°æ®äº§å“å’Œå˜é‡ã€‚</li>
<li>OceanAIåº”ç”¨äºæµ·æ´‹å±å®³é¢„æŠ¥ã€ç”Ÿæ€ç³»ç»Ÿè¯„ä¼°å’Œæ°´è´¨ç›‘æµ‹ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75b48734faf6bed8f1b796930d78a2bf" align="middle">
<img src="https://picx.zhimg.com/v2-68211f505d648af34fd81030df009a41" align="middle">
<img src="https://picx.zhimg.com/v2-36f152b83427444820aa110ccdccf45a" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Homogeneous-Keys-Heterogeneous-Values-Exploiting-Local-KV-Cache-Asymmetry-for-Long-Context-LLMs"><a href="#Homogeneous-Keys-Heterogeneous-Values-Exploiting-Local-KV-Cache-Asymmetry-for-Long-Context-LLMs" class="headerlink" title="Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache   Asymmetry for Long-Context LLMs"></a>Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache   Asymmetry for Long-Context LLMs</h2><p><strong>Authors:Wanyun Cui, Mingwei Xu</strong></p>
<p>Recent advances in Large Language Models (LLMs) have highlighted the critical importance of extending context length, yet the quadratic complexity of attention mechanisms poses significant challenges for efficient long-context modeling. KV cache compression has emerged as a key approach to address this challenge. Through extensive empirical analysis, we reveal a fundamental yet previously overlooked asymmetry in KV caches: while adjacent keys receive similar attention weights ({\it local homogeneity}), adjacent values demonstrate distinct {\it heterogeneous} distributions. This key-value asymmetry reveals a critical limitation in existing compression methods that treat keys and values uniformly. To address the limitation, we propose a training-free compression framework (AsymKV) that combines homogeneity-based key merging with a mathematically proven lossless value compression. Extensive experiments demonstrate that AsymKV consistently outperforms existing long-context methods across various tasks and base models. For example, on LLaMA3.1-8B, AsymKV achieves an average score of 43.95 on LongBench, surpassing SOTA methods like H$_2$O (38.89) by a large margin.Our code can be found in this link:<a target="_blank" rel="noopener" href="https://github.com/the-scale-lab/Asymkv">https://github.com/the-scale-lab/Asymkv</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å‡¸æ˜¾äº†æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦çš„é‡è¦æ€§ï¼Œç„¶è€Œï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§ä¸ºé«˜æ•ˆé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚KVç¼“å­˜å‹ç¼©å·²æˆä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„å…³é”®æ–¹æ³•ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°äº†KVç¼“å­˜ä¸­ä¸€ä¸ªåŸºæœ¬ä¸”ä¹‹å‰è¢«å¿½è§†çš„ä¸å¯¹ç§°æ€§ï¼šè™½ç„¶ç›¸é‚»çš„é”®ä¼šè·å¾—ç›¸ä¼¼çš„æ³¨æ„åŠ›æƒé‡ï¼ˆå±€éƒ¨åŒè´¨æ€§ï¼‰ï¼Œä½†ç›¸é‚»çš„å€¼å´è¡¨ç°å‡ºä¸åŒçš„å¼‚è´¨æ€§åˆ†å¸ƒã€‚è¿™ç§é”®å€¼ä¸å¯¹ç§°æ€§æ­ç¤ºäº†ç°æœ‰å‹ç¼©æ–¹æ³•åœ¨ç»Ÿä¸€å¤„ç†é”®å’Œå€¼æ—¶å­˜åœ¨çš„å…³é”®å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å‹ç¼©æ¡†æ¶ï¼ˆAsymKVï¼‰ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºäºåŒè´¨æ€§é”®åˆå¹¶å’Œç»è¿‡æ•°å­¦è¯æ˜çš„æ— æŸå€¼å‹ç¼©ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§ä»»åŠ¡å’ŒåŸºå‡†æ¨¡å‹ä¸Šï¼ŒAsymKVå§‹ç»ˆä¼˜äºç°æœ‰çš„é•¿ä¸Šä¸‹æ–‡æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨LLaMA3.1-8Bä¸Šï¼ŒAsymKVåœ¨LongBenchä¸Šçš„å¹³å‡å¾—åˆ†ä¸º43.95ï¼Œå¤§å¹…åº¦è¶…è¶Šäº†H_2Oï¼ˆ38.89ï¼‰ç­‰æœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨è¿™ä¸ªé“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/the-scale-lab/Asymkv">https://github.com/the-scale-lab/Asymkv</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05410v2">PDF</a> 14 pages,7 figures;Accepted by NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>LLMä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•ä¸­çš„å…³é”®æŒ‘æˆ˜æ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§ã€‚è¿‘æœŸç ”ç©¶é€šè¿‡KVç¼“å­˜å‹ç¼©æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæ­ç¤ºå‡ºKVç¼“å­˜ä¸­çš„é”®å’Œå€¼åˆ†å¸ƒä¸å¯¹ç§°ï¼šé”®çš„åˆ†å¸ƒå‘ˆç°å±€éƒ¨åŒè´¨æ€§ï¼Œè€Œå€¼çš„åˆ†å¸ƒåˆ™æ˜¯å¼‚æ„çš„ã€‚è¿™ä¸ºç»Ÿä¸€å¤„ç†é”®å€¼çš„æ–¹æ³•å¸¦æ¥é™åˆ¶ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„å‹ç¼©æ¡†æ¶ï¼ˆAsymKVï¼‰ï¼Œç»“åˆåŸºäºåŒè´¨çš„é”®åˆå¹¶ä¸æ— æŸå€¼å‹ç¼©æŠ€æœ¯ã€‚å®éªŒè¯æ˜ï¼ŒAsymKVåœ¨ä¸åŒä»»åŠ¡å’ŒåŸºå‡†æ¨¡å‹ä¸Šå‡ä¼˜äºç°æœ‰é•¿ä¸Šä¸‹æ–‡æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨LLaMA3.1-8Bä¸Šï¼ŒAsymKVåœ¨LongBenchä¸Šçš„å¹³å‡å¾—åˆ†ä¸º43.95ï¼Œå¤§å¹…è¶…è¶ŠH$_2$Oç­‰ç°æœ‰æœ€ä½³æ–¹æ³•ï¼ˆ38.89ï¼‰ã€‚ä»£ç å¯åœ¨æ­¤é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/the-scale-lab/Asymkv">https://github.com/the-scale-lab/Asymkv</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦æ—¶é¢ä¸´æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§æŒ‘æˆ˜ã€‚</li>
<li>KVç¼“å­˜å‹ç¼©æ˜¯åº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„å…³é”®æ–¹æ³•ã€‚</li>
<li>KVç¼“å­˜ä¸­å­˜åœ¨é”®å’Œå€¼åˆ†å¸ƒä¸å¯¹ç§°çš„é—®é¢˜ï¼Œå³é”®çš„åˆ†å¸ƒå‘ˆç°å±€éƒ¨åŒè´¨æ€§ï¼Œè€Œå€¼çš„åˆ†å¸ƒæ˜¯å¼‚æ„çš„ã€‚</li>
<li>ç°æœ‰å‹ç¼©æ–¹æ³•åœ¨å¤„ç†è¿™ç§é”®å€¼ä¸å¯¹ç§°æ—¶å­˜åœ¨é™åˆ¶ã€‚</li>
<li>æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„å‹ç¼©æ¡†æ¶ï¼ˆAsymKVï¼‰ï¼Œç»“åˆé”®çš„åŒè´¨åˆå¹¶å’Œå€¼çš„æ— æŸå‹ç¼©æŠ€æœ¯ã€‚</li>
<li>AsymKVåœ¨ä¸åŒä»»åŠ¡å’ŒåŸºå‡†æ¨¡å‹ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰é•¿ä¸Šä¸‹æ–‡æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f5d8f58a7bec8a395dcd15c06998b25" align="middle">
<img src="https://picx.zhimg.com/v2-27f4aa76bfc542231c8043cc41bd8774" align="middle">
<img src="https://picx.zhimg.com/v2-2a49e9bdddf24d55d06475497ea07d45" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="How-do-Transformers-Learn-Implicit-Reasoning"><a href="#How-do-Transformers-Learn-Implicit-Reasoning" class="headerlink" title="How do Transformers Learn Implicit Reasoning?"></a>How do Transformers Learn Implicit Reasoning?</h2><p><strong>Authors:Jiaran Ye, Zijun Yao, Zhidian Huang, Liangming Pan, Jinxin Liu, Yushi Bai, Amy Xin, Weichuan Liu, Xiaoyin Che, Lei Hou, Juanzi Li</strong></p>
<p>Recent work suggests that large language models (LLMs) can perform multi-hop reasoning implicitly â€“ producing correct answers without explicitly verbalizing intermediate steps â€“ but the underlying mechanisms remain poorly understood. In this paper, we study how such implicit reasoning emerges by training transformers from scratch in a controlled symbolic environment. Our analysis reveals a three-stage developmental trajectory: early memorization, followed by in-distribution generalization, and eventually cross-distribution generalization. We find that training with atomic triples is not necessary but accelerates learning, and that second-hop generalization relies on query-level exposure to specific compositional structures. To interpret these behaviors, we introduce two diagnostic tools: cross-query semantic patching, which identifies semantically reusable intermediate representations, and a cosine-based representational lens, which reveals that successful reasoning correlates with the cosine-base clustering in hidden space. This clustering phenomenon in turn provides a coherent explanation for the behavioral dynamics observed across training, linking representational structure to reasoning capability. These findings provide new insights into the interpretability of implicit multi-hop reasoning in LLMs, helping to clarify how complex reasoning processes unfold internally and offering pathways to enhance the transparency of such models. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥éšå¼åœ°è¿›è¡Œå¤šè·³æ¨ç†â€”â€”åœ¨ä¸éœ€è¦æ˜ç¡®è¡¨è¿°ä¸­é—´æ­¥éª¤çš„æƒ…å†µä¸‹ç»™å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶ä»é²œä¸ºäººçŸ¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨ä¸€ä¸ªå—æ§çš„ç¬¦å·ç¯å¢ƒä¸­ä»å¤´è®­ç»ƒè½¬æ¢å™¨æ¥ç ”ç©¶è¿™ç§éšå¼æ¨ç†æ˜¯å¦‚ä½•å‡ºç°çš„ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„å‘å±•è½¨è¿¹ï¼šæ—©æœŸçš„è®°å¿†ï¼Œç„¶åæ˜¯å†…éƒ¨åˆ†å¸ƒæ¨å¹¿ï¼Œæœ€åæ˜¯è·¨åˆ†å¸ƒæ¨å¹¿ã€‚æˆ‘ä»¬å‘ç°ä½¿ç”¨åŸå­ä¸‰å…ƒç»„è¿›è¡Œè®­ç»ƒå¹¶ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†å¯ä»¥åŠ é€Ÿå­¦ä¹ ï¼Œè€Œç¬¬äºŒè·³æ¨å¹¿ä¾èµ–äºå¯¹ç‰¹å®šç»„åˆç»“æ„çš„æŸ¥è¯¢çº§åˆ«æš´éœ²ã€‚ä¸ºäº†è§£é‡Šè¿™äº›è¡Œä¸ºï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§è¯Šæ–­å·¥å…·ï¼šè·¨æŸ¥è¯¢è¯­ä¹‰ä¿®è¡¥ï¼Œç”¨äºè¯†åˆ«å¯é‡å¤ä½¿ç”¨çš„ä¸­é—´è¡¨ç¤ºï¼›åŸºäºä½™å¼¦å€¼çš„è¡¨ç¤ºé€é•œï¼Œæ­ç¤ºäº†æˆåŠŸçš„æ¨ç†ä¸éšè—ç©ºé—´ä¸­çš„ä½™å¼¦åŸºç¡€èšç±»ä¹‹é—´çš„ç›¸å…³æ€§ã€‚è¿™ç§èšç±»ç°è±¡è¿›è€Œä¸ºæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„è¡Œä¸ºåŠ¨æ€æä¾›äº†è¿è´¯çš„è§£é‡Šï¼Œå°†è¡¨ç¤ºç»“æ„ä¸æ¨ç†èƒ½åŠ›è”ç³»èµ·æ¥ã€‚è¿™äº›å‘ç°ä¸ºæˆ‘ä»¬æä¾›äº†å…³äºLLMä¸­éšå¼å¤šè·³æ¨ç†è§£é‡Šæ€§çš„æ–°è§è§£ï¼Œæœ‰åŠ©äºé˜æ˜å¤æ‚çš„æ¨ç†è¿‡ç¨‹å¦‚ä½•åœ¨å†…éƒ¨å±•å¼€ï¼Œå¹¶æä¾›äº†æé«˜æ­¤ç±»æ¨¡å‹é€æ˜åº¦çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23653v2">PDF</a> Accepted as Spotlight at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿéšå¼è¿›è¡Œå¤šè·³æ¨ç†ï¼Œå³äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆè€Œæ— éœ€æ˜ç¡®è¡¨è¿°ä¸­é—´æ­¥éª¤ã€‚æœ¬ç ”ç©¶é€šè¿‡åœ¨æœ‰æ§åˆ¶ç¬¦å·çš„ç¯å¢ƒä¸­ä»å¤´è®­ç»ƒå˜æ¢å™¨ï¼Œæ¢è®¨äº†è¿™ç§éšå¼æ¨ç†æ˜¯å¦‚ä½•å‡ºç°çš„ã€‚åˆ†ææ­ç¤ºäº†ä¸‰ä¸ªå‘å±•é˜¶æ®µï¼šæ—©æœŸè®°å¿†ã€å†…éƒ¨åˆ†å¸ƒæ³›åŒ–å’Œè·¨åˆ†å¸ƒæ³›åŒ–ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨åŸå­ä¸‰å…ƒç»„è¿›è¡Œè®­ç»ƒå¹¶éå¿…éœ€ï¼Œä½†å¯ä»¥åŠ é€Ÿå­¦ä¹ ï¼Œè€Œç¬¬äºŒè·³æ³›åŒ–ä¾èµ–äºå¯¹ç‰¹å®šç»„åˆç»“æ„çš„æŸ¥è¯¢çº§æš´éœ²ã€‚ä¸ºäº†è§£é‡Šè¿™äº›è¡Œä¸ºï¼Œç ”ç©¶å¼•å…¥äº†ä¸¤ç§è¯Šæ–­å·¥å…·ï¼šè·¨æŸ¥è¯¢è¯­ä¹‰è¡¥ä¸å’ŒåŸºäºä½™å¼¦çš„ä»£è¡¨é€é•œã€‚è¿™äº›å‘ç°æ­ç¤ºäº†éšå¼å¤šè·³æ¨ç†åœ¨LLMä¸­çš„å¯è§£é‡Šæ€§ï¼Œæœ‰åŠ©äºé˜æ˜å¤æ‚çš„æ¨ç†è¿‡ç¨‹å¦‚ä½•åœ¨å†…éƒ¨å±•å¼€ï¼Œå¹¶ä¸ºæé«˜æ­¤ç±»æ¨¡å‹çš„é€æ˜åº¦æä¾›äº†é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡éšå¼å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€æ˜ç¡®è¡¨è¿°ä¸­é—´æ­¥éª¤å°±èƒ½äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆã€‚</li>
<li>éšå¼æ¨ç†çš„å‡ºç°ä¸åœ¨æ§åˆ¶ç¬¦å·ç¯å¢ƒä¸­è®­ç»ƒå˜æ¢å™¨æœ‰å…³ï¼Œç»å†äº†æ—©æœŸè®°å¿†ã€å†…éƒ¨åˆ†å¸ƒæ³›åŒ–å’Œè·¨åˆ†å¸ƒæ³›åŒ–ä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>ä½¿ç”¨åŸå­ä¸‰å…ƒç»„è¿›è¡Œè®­ç»ƒè™½ç„¶ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†å¯ä»¥åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>ç¬¬äºŒè·³æ³›åŒ–ä¾èµ–äºå¯¹ç‰¹å®šç»„åˆç»“æ„çš„æŸ¥è¯¢çº§æš´éœ²ã€‚</li>
<li>å¼•å…¥çš„è·¨æŸ¥è¯¢è¯­ä¹‰è¡¥ä¸å’ŒåŸºäºä½™å¼¦çš„ä»£è¡¨é€é•œä¸¤ç§è¯Šæ–­å·¥å…·æœ‰åŠ©äºè§£é‡ŠLLMçš„éšå¼æ¨ç†è¡Œä¸ºã€‚</li>
<li>æˆåŠŸæ¨ç†ä¸éšè—ç©ºé—´ä¸­çš„ä½™å¼¦åŸºç¡€èšç±»ç°è±¡ç›¸å…³è”ï¼Œè¿™ä¸ºè§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡Œä¸ºåŠ¨æ€æä¾›äº†è¿è´¯çš„è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61b7f9477ccaebf624bacb42232968ed" align="middle">
<img src="https://picx.zhimg.com/v2-5891f20e9a4d738b86427a0e5c0959ef" align="middle">
<img src="https://picx.zhimg.com/v2-5a828fe003041f5b18d91873da1056b1" align="middle">
<img src="https://picx.zhimg.com/v2-c5bbbc0f1e2e4d91b2d7f8c0fe801e1a" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Exact-Expressive-Power-of-Transformers-with-Padding"><a href="#Exact-Expressive-Power-of-Transformers-with-Padding" class="headerlink" title="Exact Expressive Power of Transformers with Padding"></a>Exact Expressive Power of Transformers with Padding</h2><p><strong>Authors:William Merrill, Ashish Sabharwal</strong></p>
<p>Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding. Are there more efficient alternatives to expand a transformerâ€™s expressive power without adding parameters? We consider transformers with padding tokens as a form of parallelizable test-time compute. We show that averaging-hard-attention, masked-pre-norm transformers with polynomial padding recognize precisely the class $\mathsf{FO}$-uniform $\mathsf{TC}^0$ of extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was known, proving a matching lower bound had been elusive. Further, our novel analysis reveals the precise expanded power of padded transformers when coupled with another form of inference-time compute, namely dynamically increasing depth via looping. Our core technical contribution is to show how padding helps bring the notions of complete problems and reductions, which have been a cornerstone of classical complexity theory, to the formal study of transformers. Armed with this new tool, we prove that padded transformers with $O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class $\mathsf{FO}$-uniform $\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and looping together systematically expand transformersâ€™ expressive power: with polylogarithmic looping, polynomially padded transformers recognize precisely the class $\mathsf{FO}$-uniform $\mathsf{NC}$, the best that could be expected without losing parallelism (unless $\mathsf{NC} &#x3D; \mathsf{P}$). Our results thus motivate further exploration of padding and looping as parallelizable alternatives to chain of thought for test-time compute. </p>
<blockquote>
<p>æ€æƒ³é“¾æ˜¯åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜è®¡ç®—èƒ½åŠ›çš„è‡ªç„¶æ¨ç†æ—¶é—´æ–¹æ³•ï¼Œä½†æˆæœ¬æ˜¯é¡ºåºè§£ç ã€‚æœ‰æ²¡æœ‰æ›´æœ‰æ•ˆçš„æ–¹æ³•æ¥æ‰©å¤§Transformerçš„è¡¨è¾¾åŠ›è€Œä¸å¢åŠ å‚æ•°ï¼Ÿæˆ‘ä»¬å°†å¸¦æœ‰å¡«å……æ ‡è®°çš„Transformerè§†ä¸ºä¸€ç§å¯å¹¶è¡ŒåŒ–çš„æµ‹è¯•æ—¶é—´è®¡ç®—å½¢å¼ã€‚æˆ‘ä»¬å±•ç¤ºäº†å…·æœ‰å¤šé¡¹å¼å¡«å……çš„å¹³å‡ç¡¬æ³¨æ„åŠ›ã€æ©ç é¢„èŒƒæ•°Transformerèƒ½å¤Ÿç²¾ç¡®è¯†åˆ«å‡ºæå¯å¹¶è¡ŒåŒ–é—®é¢˜çš„ç±»$\mathsf{FO}$-uniform $\mathsf{TC}^0$ã€‚è™½ç„¶å·²çŸ¥$\mathsf{TC}^0$çš„ä¸Šç•Œï¼Œä½†è¯æ˜åŒ¹é…çš„ä¸‹ç•Œä¸€ç›´éš¾ä»¥æ‰æ‘¸ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–°å‹åˆ†ææ­ç¤ºäº†å¡«å……Transformerä¸å¦ä¸€ç§æ¨ç†æ—¶é—´è®¡ç®—ç›¸ç»“åˆæ—¶çš„ç²¾ç¡®æ‰©å±•èƒ½åŠ›ï¼Œå³é€šè¿‡å¾ªç¯åŠ¨æ€å¢åŠ æ·±åº¦ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®åœ¨äºå±•ç¤ºå¡«å……å¦‚ä½•å¸®åŠ©å¸¦æ¥å®Œå…¨é—®é¢˜å’Œå½’çº¦çš„æ¦‚å¿µï¼Œè¿™æ˜¯ç»å…¸å¤æ‚æ€§ç†è®ºçš„æ ¸å¿ƒï¼Œå¹¶å°†å…¶åº”ç”¨äºTransformerçš„æ­£å¼ç ”ç©¶ã€‚å€ŸåŠ©è¿™ä¸ªæ–°å·¥å…·ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨è¾“å…¥é•¿åº¦ä¸ºnçš„æƒ…å†µä¸‹ï¼Œå¸¦æœ‰$O(\log^d n)$å¾ªç¯çš„å¡«å……Transformerç²¾ç¡®åœ°è¯†åˆ«äº†ä¸­ç­‰å¹¶è¡ŒåŒ–é—®é¢˜çš„ç±»åˆ«$\mathsf{FO}$-uniform $\mathsf{TC}^d$ã€‚å› æ­¤ï¼Œå¡«å……å’Œå¾ªç¯ä¸€èµ·ç³»ç»Ÿåœ°æ‰©å¤§äº†Transformerçš„è¡¨è¾¾åŠ›ï¼šä½¿ç”¨å¤šé¡¹å¼å¡«å……å’Œå…·æœ‰å¯¹æ•°å¾ªç¯çš„Transformerç²¾ç¡®åœ°è¯†åˆ«å‡º$\mathsf{FO}$-uniform $\mathsf{NC}$ç±»çš„é—®é¢˜ï¼Œè¿™æ˜¯åœ¨ä¿æŒå¹¶è¡Œæ€§æ—¶æ‰€èƒ½è¾¾åˆ°çš„æœ€ä½³æ•ˆæœï¼ˆé™¤é$\mathsf{NC}&#x3D;\mathsf{P}$ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¿›ä¸€æ­¥æ¿€å‘äº†å¡«å……å’Œå¾ªç¯ä½œä¸ºå¹¶è¡ŒåŒ–æ€æƒ³é“¾çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºæµ‹è¯•æ—¶é—´è®¡ç®—çš„ç ”ç©¶å’Œæ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18948v2">PDF</a> Neurips 2025</p>
<p><strong>æ‘˜è¦</strong><br>è¯¥æ–‡æœ¬æ¢è®¨äº†åœ¨æ— éœ€å¢åŠ å‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¡«å……æ ‡è®°ï¼ˆpadding tokensï¼‰å’Œå¾ªç¯æœºåˆ¶æé«˜transformerå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¡ç®—æ•ˆç‡çš„å¯èƒ½æ€§ã€‚æ–‡ä¸­ä»‹ç»äº†é€šè¿‡å¹³å‡ç¡¬æ³¨æ„åŠ›æ³•ï¼Œé‡‡ç”¨å¸¦æœ‰å¤šé¡¹å¼å¡«å……çš„æ©ç é¢„æ ‡å‡†åŒ–transformerï¼Œèƒ½å¤Ÿç²¾ç¡®è¯†åˆ«å‡ºæå¯å¹¶è¡ŒåŒ–é—®é¢˜ç±»$\text{FO}$-uniform $\text{TC}^0$ã€‚æ­¤å¤–ï¼Œç»“åˆå¦ä¸€ç§æ¨ç†æ—¶é—´è®¡ç®—æ–¹å¼â€”â€”åŠ¨æ€å¢åŠ æ·±åº¦å¾ªç¯ï¼Œæ­ç¤ºäº†å¡«å……å¯¹transformeræ€§èƒ½çš„ç²¾ç¡®æ‰©å±•ä½œç”¨ã€‚æ–‡ç« çš„æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®åœ¨äºå±•ç¤ºäº†å¡«å……å¦‚ä½•å¸®åŠ©å¼•å…¥ç»å…¸å¤æ‚æ€§ç†è®ºä¸­çš„å®Œæ•´é—®é¢˜å’Œç®€åŒ–æ¦‚å¿µï¼Œå¯¹transformerè¿›è¡Œæ­£å¼ç ”ç©¶ã€‚é€šè¿‡ä½¿ç”¨å¡«å……å’Œå¾ªç¯ä½œä¸ºå¹¶è¡ŒåŒ–çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½œè€…è¯æ˜äº†å¡«å……å˜å‹å™¨é€šè¿‡åœ¨å¯¹æ•°å¾ªç¯ä¸Šå·¥ä½œç²¾ç¡®è¯†åˆ«å¯å¹¶è¡ŒåŒ–é—®é¢˜ç±»$\text{FO}$-uniform $\text{TC}^d$ã€‚å› æ­¤ï¼Œå¡«å……å’Œå¾ªç¯ä¸€èµ·ç³»ç»Ÿåœ°æ‰©å±•äº†å˜å‹å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿™äº›ç»“æœè¿›ä¸€æ­¥é¼“åŠ±äº†å¯¹å¡«å……å’Œå¾ªç¯ä½œä¸ºæµ‹è¯•æ—¶é—´è®¡ç®—çš„å¹¶è¡Œæ›¿ä»£æ–¹æ³•çš„æ¢ç´¢ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºå¡«å……æ ‡è®°çš„è‡ªç„¶æ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŸºäºtransformerçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨å¹³å‡ç¡¬æ³¨æ„åŠ›æ³•å’Œæ©ç é¢„æ ‡å‡†åŒ–transformeræŠ€æœ¯ï¼Œèƒ½å¤Ÿç²¾ç¡®è¯†åˆ«å‡ºæå¯å¹¶è¡ŒåŒ–é—®é¢˜ç±»$\text{FO}$-uniform $\text{TC}^0$ã€‚</li>
<li>å¡«å……æŠ€æœ¯ç»“åˆäº†åŠ¨æ€æ·±åº¦å¾ªç¯æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æ­ç¤ºäº†å…¶å¯¹transformeræ€§èƒ½æ‰©å±•çš„ç²¾ç¡®ä½œç”¨ã€‚</li>
<li>æ–‡ç« å±•ç¤ºäº†å¡«å……å¦‚ä½•å¼•å…¥ç»å…¸å¤æ‚æ€§ç†è®ºä¸­çš„å®Œæ•´é—®é¢˜å’Œç®€åŒ–æ¦‚å¿µï¼Œä¸ºtransformerç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å¡«å……å’Œå¾ªç¯æŠ€æœ¯ï¼Œè¯æ˜äº†å¡«å……å˜å‹å™¨èƒ½å¤Ÿç²¾ç¡®è¯†åˆ«å¯å¹¶è¡ŒåŒ–é—®é¢˜ç±»$\text{FO}$-uniform $\text{TC}^d$ã€‚</li>
<li>å¡«å……å’Œå¾ªç¯ç»“åˆæ˜¾è‘—æé«˜äº†transformerçš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¯ä½œä¸ºæµ‹è¯•æ—¶é—´è®¡ç®—çš„ä¸€ç§æœ‰æ•ˆå¹¶è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7d021b51ff2edabdeeb97e447edd905" align="middle">
<img src="https://picx.zhimg.com/v2-1ec597e92f2948f1af79129d249a01ae" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Little-Depth-Goes-a-Long-Way-The-Expressive-Power-of-Log-Depth-Transformers"><a href="#A-Little-Depth-Goes-a-Long-Way-The-Expressive-Power-of-Log-Depth-Transformers" class="headerlink" title="A Little Depth Goes a Long Way: The Expressive Power of Log-Depth   Transformers"></a>A Little Depth Goes a Long Way: The Expressive Power of Log-Depth   Transformers</h2><p><strong>Authors:William Merrill, Ashish Sabharwal</strong></p>
<p>Recent theoretical results show transformers cannot express sequential reasoning problems over long inputs, intuitively because their computational depth is bounded. However, prior work treats the depth as a constant, leaving it unclear to what degree bounded depth may suffice for solving problems over short inputs, or how increasing the transformerâ€™s depth affects its expressive power. We address these questions by analyzing transformers whose depth can grow minimally with context length $n$. We show even highly uniform transformers with depth $\Theta(\log n)$ can express two important problems: recognizing regular languages, which captures state tracking abilities and was known to be expressible only by an unconventional, non-uniform model of transformers, and graph connectivity, which underlies multi-step reasoning. Notably, both of these problems cannot be expressed by fixed-depth transformers under standard complexity conjectures, demonstrating the expressivity benefit of growing depth. Moreover, our theory quantitatively predicts how depth must grow with input length to express these problems, showing that depth scaling is more efficient than scaling width or chain-of-thought steps. Empirically, our detailed experiments designed to bridge the expressivity vs. learnability gap reveal that our theoretical depth requirements for regular language recognition closely match the practical depth requirements for successfully training transformers. Thus, our results clarify how depth affects a transformerâ€™s reasoning capabilities, and provide practical guidance for effective depth selection for sequential reasoning. </p>
<blockquote>
<p>æœ€è¿‘çš„ç†è®ºç»“æœæ˜¾ç¤ºï¼Œå˜å‹å™¨æ— æ³•å¯¹é•¿è¾“å…¥è¿›è¡Œé¡ºåºæ¨ç†é—®é¢˜çš„è¡¨è¾¾ï¼Œç›´è§‰ä¸Šæ˜¯å› ä¸ºå…¶è®¡ç®—æ·±åº¦æ˜¯æœ‰é™çš„ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶å°†æ·±åº¦è§†ä¸ºå¸¸æ•°ï¼Œå°šä¸æ˜ç¡®æœ‰é™çš„æ·±åº¦åœ¨è§£å†³çŸ­è¾“å…¥é—®é¢˜ä¸Šèƒ½èµ·å¤šå¤§çš„ä½œç”¨ï¼Œæˆ–è€…å¢åŠ å˜å‹å™¨çš„æ·±åº¦ä¼šå¦‚ä½•å½±å“å…¶è¡¨è¾¾èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡åˆ†ææ·±åº¦å¯ä»¥éšä¸Šä¸‹æ–‡é•¿åº¦næœ€å°ç¨‹åº¦å¢é•¿çš„å˜å‹å™¨æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†å³ä½¿æ˜¯é«˜åº¦å‡åŒ€çš„å˜å‹å™¨ï¼Œå…¶æ·±åº¦ä¸ºÎ˜ï¼ˆlog nï¼‰ä¹Ÿèƒ½è¡¨è¾¾ä¸¤ä¸ªé‡è¦é—®é¢˜ï¼šè¯†åˆ«æ­£åˆ™è¯­è¨€ï¼Œè¿™æ•æ‰äº†çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ï¼Œå·²çŸ¥åªæœ‰éä¼ ç»Ÿçš„éå‡åŒ€å˜å‹å™¨æ¨¡å‹æ‰èƒ½è¡¨è¾¾ï¼›ä»¥åŠå›¾è¿é€šæ€§ï¼Œè¿™æ˜¯å¤šæ­¥æ¨ç†çš„åŸºç¡€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ ¹æ®æ ‡å‡†å¤æ‚æ€§çŒœæƒ³ï¼Œè¿™ä¸¤ä¸ªé—®é¢˜éƒ½ä¸èƒ½ç”±å›ºå®šæ·±åº¦çš„å˜å‹å™¨æ¥è¡¨è¾¾ï¼Œè¿™è¯æ˜äº†å¢é•¿æ·±åº¦åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç†è®ºå®šé‡é¢„æµ‹äº†æ·±åº¦å¿…é¡»å¦‚ä½•éšè¾“å…¥é•¿åº¦çš„å¢é•¿è€Œå¢é•¿æ‰èƒ½è¡¨è¾¾è¿™äº›é—®é¢˜ï¼Œè¡¨æ˜æ·±åº¦ç¼©æ”¾æ¯”å®½åº¦ç¼©æ”¾æˆ–æ€ç»´é“¾æ­¥éª¤æ›´ä¸ºé«˜æ•ˆã€‚ä»å®è¯ä¸Šçœ‹ï¼Œæˆ‘ä»¬è®¾è®¡çš„è¯¦ç»†å®éªŒæ—¨åœ¨å¼¥åˆè¡¨è¾¾ä¸å¯å­¦ä¹ æ€§ä¹‹é—´çš„å·®è·ï¼Œå¹¶æ­ç¤ºå‡ºç†è®ºä¸Šçš„æ­£åˆ™è¯­è¨€è¯†åˆ«æ‰€éœ€çš„æ·±åº¦ä¸å®é™…æˆåŠŸè®­ç»ƒå˜å‹å™¨æ‰€éœ€çš„æ·±åº¦éå¸¸æ¥è¿‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç»“æœæ˜ç¡®äº†æ·±åº¦å¦‚ä½•å½±å“å˜å‹å™¨çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸ºæœ‰æ•ˆçš„æ·±åº¦é€‰æ‹©æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03961v3">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>     è¿‘æœŸç†è®ºç»“æœæ˜¾ç¤ºï¼Œå˜å‹å™¨åœ¨å¤„ç†é•¿è¾“å…¥åºåˆ—çš„æ¨ç†é—®é¢˜æ—¶å­˜åœ¨å±€é™ï¼Œå…¶è®¡ç®—æ·±åº¦æ˜¯å—é™çš„ã€‚å…ˆå‰çš„ç ”ç©¶å°†æ·±åº¦è§†ä¸ºå¸¸æ•°ï¼Œå°šä¸æ¸…æ¥šæœ‰é™æ·±åº¦å¯¹çŸ­è¾“å…¥çš„è§£å†³é—®é¢˜ç¨‹åº¦å¦‚ä½•ï¼Œä»¥åŠå¢åŠ å˜å‹å™¨çš„æ·±åº¦ä¼šå¦‚ä½•å½±å“å…¶è¡¨è¾¾èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡åˆ†ææ·±åº¦éšä¸Šä¸‹æ–‡é•¿åº¦nå¢é•¿çš„æœ€å°‘å˜å‹å™¨æ¥è§£ç­”è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿å…·æœ‰é«˜åº¦ç»Ÿä¸€çš„æ·±åº¦ä¸ºÎ¸ï¼ˆlog nï¼‰çš„å˜å‹å™¨ï¼Œä¹Ÿèƒ½è¡¨è¾¾ä¸¤ä¸ªé‡è¦é—®é¢˜ï¼šè¯†åˆ«è§„åˆ™è¯­è¨€å’Œå›¾å½¢è¿æ¥ã€‚è¿™äº›é—®é¢˜æ­¤å‰è¢«è®¤ä¸ºåªèƒ½ç”±éä¼ ç»Ÿçš„ã€éå¸¸è§„çš„å˜å‹å™¨æ¨¡å‹æ¥è¡¨è¾¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ ¹æ®æ ‡å‡†å¤æ‚æ€§çŒœæƒ³ï¼Œè¿™äº›é—®é¢˜æ— æ³•ç”±å›ºå®šæ·±åº¦çš„å˜å‹å™¨æ¥è¡¨è¾¾ï¼Œè¿™æ˜¾ç¤ºäº†å¢é•¿æ·±åº¦å¯¹æé«˜è¡¨è¾¾èƒ½åŠ›çš„å¥½å¤„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç†è®ºè¿˜å®šé‡é¢„æµ‹äº†è¡¨è¾¾è¿™äº›é—®é¢˜æ‰€éœ€çš„æ·±åº¦ä¸è¾“å…¥é•¿åº¦çš„å¢é•¿å…³ç³»ï¼Œè¡¨æ˜æ·±åº¦ç¼©æ”¾æ¯”å®½åº¦ç¼©æ”¾æˆ–æ€è€ƒæ­¥éª¤æ›´æœ‰æ•ˆç‡ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°ç†è®ºä¸Šçš„æ·±åº¦è¦æ±‚å¯¹è§„åˆ™è¯­è¨€è¯†åˆ«çš„è¦æ±‚ä¸å®é™…æˆåŠŸè®­ç»ƒå˜å‹å™¨çš„æ·±åº¦è¦æ±‚éå¸¸å»åˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜ç¡®äº†æ·±åº¦å¯¹å˜å‹å™¨æ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œå¹¶ä¸ºæœ‰æ•ˆåœ°é€‰æ‹©æ·±åº¦è¿›è¡Œé¡ºåºæ¨ç†æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜å‹å™¨åœ¨å¤„ç†é•¿è¾“å…¥åºåˆ—çš„æ¨ç†é—®é¢˜æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå…¶è®¡ç®—æ·±åº¦æ˜¯å—é™çš„ã€‚</li>
<li>å…ˆå‰ç ”ç©¶å°†å˜å‹å™¨çš„æ·±åº¦è§†ä¸ºå¸¸æ•°ï¼Œç¼ºä¹å¯¹çŸ­è¾“å…¥å’Œæ·±åº¦å¢åŠ å¯¹å…¶è¡¨è¾¾èƒ½åŠ›å½±å“çš„ç ”ç©¶ã€‚</li>
<li>é€šè¿‡åˆ†ææ·±åº¦éšä¸Šä¸‹æ–‡é•¿åº¦å¢é•¿çš„å˜å‹å™¨ï¼Œå‘ç°å³ä½¿æ˜¯è¾ƒæµ…çš„å˜å‹å™¨ä¹Ÿèƒ½è¡¨è¾¾è¯†åˆ«è§„åˆ™è¯­è¨€å’Œå›¾å½¢è¿æ¥ç­‰é‡è¦é—®é¢˜ã€‚</li>
<li>è¿™äº›é—®é¢˜æ— æ³•ç”±å›ºå®šæ·±åº¦çš„å˜å‹å™¨è¡¨è¾¾ï¼Œå¢åŠ æ·±åº¦æœ‰åŠ©äºæé«˜è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>ç†è®ºé¢„æµ‹äº†è¡¨è¾¾ç‰¹å®šé—®é¢˜æ‰€éœ€çš„æ·±åº¦ä¸è¾“å…¥é•¿åº¦çš„å¢é•¿å…³ç³»ï¼Œè¡¨æ˜æ·±åº¦ç¼©æ”¾æ¯”å®½åº¦æˆ–æ€è€ƒæ­¥éª¤æ›´ä¸ºé«˜æ•ˆã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºç†è®ºä¸Šçš„æ·±åº¦è¦æ±‚å¯¹å®é™…è®­ç»ƒå˜å‹å™¨çš„æ·±åº¦é€‰æ‹©å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f6df3e2e6007f6e2f383405c02037b7" align="middle">
<img src="https://picx.zhimg.com/v2-b10f108c77f96306c9a75bd9f778eb9f" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="IndicSentEval-How-Effectively-do-Multilingual-Transformer-Models-encode-Linguistic-Properties-for-Indic-Languages"><a href="#IndicSentEval-How-Effectively-do-Multilingual-Transformer-Models-encode-Linguistic-Properties-for-Indic-Languages" class="headerlink" title="IndicSentEval: How Effectively do Multilingual Transformer Models encode   Linguistic Properties for Indic Languages?"></a>IndicSentEval: How Effectively do Multilingual Transformer Models encode   Linguistic Properties for Indic Languages?</h2><p><strong>Authors:Akhilesh Aravapalli, Mounika Marreddy, Radhika Mamidi, Manish Gupta, Subba Reddy Oota</strong></p>
<p>Transformer-based models have revolutionized the field of natural language processing. To understand why they perform so well and to assess their reliability, several studies have focused on questions such as: Which linguistic properties are encoded by these models, and to what extent? How robust are these models in encoding linguistic properties when faced with perturbations in the input text? However, these studies have mainly focused on BERT and the English language. In this paper, we investigate similar questions regarding encoding capability and robustness for 8 linguistic properties across 13 different perturbations in 6 Indic languages, using 9 multilingual Transformer models (7 universal and 2 Indic-specific). To conduct this study, we introduce a novel multilingual benchmark dataset, IndicSentEval, containing approximately $\sim$47K sentences. Surprisingly, our probing analysis of surface, syntactic, and semantic properties reveals that while almost all multilingual models demonstrate consistent encoding performance for English, they show mixed results for Indic languages. As expected, Indic-specific multilingual models capture linguistic properties in Indic languages better than universal models. Intriguingly, universal models broadly exhibit better robustness compared to Indic-specific models, particularly under perturbations such as dropping both nouns and verbs, dropping only verbs, or keeping only nouns. Overall, this study provides valuable insights into probing and perturbation-specific strengths and weaknesses of popular multilingual Transformer-based models for different Indic languages. We make our code and dataset publicly available [<a target="_blank" rel="noopener" href="https://github.com/aforakhilesh/IndicBertology]">https://github.com/aforakhilesh/IndicBertology]</a>. </p>
<blockquote>
<p>Transformeræ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£å®ƒä»¬ä¸ºä½•è¡¨ç°å¦‚æ­¤å‡ºè‰²å¹¶è¯„ä¼°å…¶å¯é æ€§ï¼Œå¤šé¡¹ç ”ç©¶ä¸“æ³¨äºä»¥ä¸‹é—®é¢˜ï¼šè¿™äº›æ¨¡å‹ç¼–ç äº†å“ªäº›è¯­è¨€ç‰¹æ€§ï¼Œä»¥åŠåˆ°äº†ä»€ä¹ˆç¨‹åº¦ï¼Ÿå½“é¢å¯¹è¾“å…¥æ–‡æœ¬çš„æ‰°åŠ¨æ—¶ï¼Œè¿™äº›æ¨¡å‹åœ¨ç¼–ç è¯­è¨€ç‰¹æ€§æ–¹é¢çš„ç¨³å¥æ€§å¦‚ä½•ï¼Ÿç„¶è€Œï¼Œè¿™äº›ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨BERTå’Œè‹±è¯­ä¸Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹9ç§å¤šè¯­è¨€Transformeræ¨¡å‹ï¼ˆ7ç§é€šç”¨æ¨¡å‹å’Œ2ç§å°åº¦è¯­ç‰¹å®šæ¨¡å‹ï¼‰ï¼Œç ”ç©¶å…³äºç¼–ç èƒ½åŠ›å’Œç¨³å¥æ€§çš„ç›¸å…³é—®é¢˜ï¼Œæ¶‰åŠ6ç§å°åº¦è¯­è¨€çš„8ç§è¯­è¨€ç‰¹æ€§ä»¥åŠ13ç§ä¸åŒçš„æ‰°åŠ¨ã€‚ä¸ºäº†è¿›è¡Œè¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€åŸºå‡†æ•°æ®é›†IndicSentEvalï¼ŒåŒ…å«çº¦47Kä¸ªå¥å­ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å¯¹è¡¨å±‚ã€å¥æ³•å±‚å’Œè¯­ä¹‰ç‰¹æ€§çš„æ¢æŸ¥åˆ†æè¡¨æ˜ï¼Œå‡ ä¹æ‰€æœ‰å¤šè¯­è¨€æ¨¡å‹åœ¨è‹±è¯­ä¸Šçš„ç¼–ç æ€§èƒ½éƒ½è¡¨ç°ä¸€è‡´ï¼Œä½†åœ¨å°åº¦è¯­ä¸Šçš„è¡¨ç°å´å–œå¿§å‚åŠã€‚æ­£å¦‚é¢„æœŸé‚£æ ·ï¼Œé’ˆå¯¹å°åº¦è¯­çš„ç‰¹å®šå°åº¦è¯­å¤šè¯­è¨€æ¨¡å‹åœ¨æ•æ‰å°åº¦è¯­çš„è¯­è¨€ç‰¹æ€§æ–¹é¢æ¯”é€šç”¨æ¨¡å‹è¡¨ç°å¾—æ›´å¥½ã€‚æœ‰è¶£çš„æ˜¯ï¼Œé€šç”¨æ¨¡å‹åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¡¨ç°å‡ºæ¯”å°åº¦è¯­ç‰¹å®šæ¨¡å‹æ›´å¥½çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ é™¤åè¯å’ŒåŠ¨è¯ã€ä»…åˆ é™¤åŠ¨è¯æˆ–ä»…ä¿ç•™åè¯ç­‰æ‰°åŠ¨æƒ…å†µä¸‹ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶ä¸ºä¸åŒå°åº¦è¯­æµè¡Œçš„å¤šè¯­è¨€Transformeræ¨¡å‹åœ¨æ¢æŸ¥å’Œç‰¹å®šæ‰°åŠ¨æ–¹é¢çš„ä¼˜ç¼ºç‚¹æä¾›äº†å®è´µçš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€å¯ç”¨ï¼š[<a target="_blank" rel="noopener" href="https://github.com/aforakhilesh/IndicBertology]%E3%80%82">https://github.com/aforakhilesh/IndicBertology]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02611v2">PDF</a> 25 pages, 11 figures, Accepted at IJCNLP-AACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šè¯­è¨€Transformeræ¨¡å‹åœ¨ç¼–ç å°åº¦è¯­è¨€ï¼ˆIndic languagesï¼‰æ–¹é¢çš„æ€§èƒ½ä¸ç¨³å¥æ€§ã€‚ç ”ç©¶æ¶‰åŠ9ä¸ªå¤šè¯­è¨€Transformeræ¨¡å‹ï¼Œé’ˆå¯¹6ç§å°åº¦è¯­è¨€çš„8ç§è¯­è¨€ç‰¹æ€§è¿›è¡Œç¼–ç èƒ½åŠ›å’Œç¨³å¥æ€§çš„è¯„ä¼°ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„å¤šè¯­è¨€åŸºå‡†æ•°æ®é›†IndicSentEvalï¼ŒåŒ…å«çº¦47Kä¸ªå¥å­ï¼Œç ”ç©¶å‘ç°åœ¨è‹±è¯­ä¸­æ‰€æœ‰å¤šè¯­è¨€æ¨¡å‹çš„ç¼–ç æ€§èƒ½ä¸€è‡´ï¼Œä½†åœ¨å°åº¦è¯­è¨€ä¸­ç»“æœå„å¼‚ã€‚å°½ç®¡å°åº¦ç‰¹å®šå¤šè¯­è¨€æ¨¡å‹åœ¨å°åº¦è¯­è¨€ä¸­çš„è¯­è¨€å­¦ç‰¹æ€§æ•æ‰å¾—æ›´å¥½ï¼Œä½†é€šç”¨æ¨¡å‹åœ¨æŸäº›æ‰°åŠ¨æƒ…å†µä¸‹å±•ç°å‡ºæ›´å¥½çš„ç¨³å¥æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶ä¸ºä¸åŒå°åº¦è¯­è¨€çš„å¤šè¯­è¨€Transformeræ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­è¨€Transformeræ¨¡å‹åœ¨ç¼–ç å°åº¦è¯­è¨€æ–¹é¢å­˜åœ¨æ€§èƒ½å·®å¼‚ã€‚</li>
<li>é’ˆå¯¹6ç§å°åº¦è¯­è¨€çš„8ç§è¯­è¨€ç‰¹æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>å¼•å…¥æ–°çš„å¤šè¯­è¨€åŸºå‡†æ•°æ®é›†IndicSentEvalï¼ŒåŒ…å«çº¦47Kä¸ªå¥å­ã€‚</li>
<li>åœ¨è‹±è¯­ä¸­ï¼Œæ‰€æœ‰å¤šè¯­è¨€æ¨¡å‹çš„ç¼–ç æ€§èƒ½ä¸€è‡´ï¼›ä½†åœ¨å°åº¦è¯­è¨€ä¸­ï¼Œç»“æœå„å¼‚ã€‚</li>
<li>å°åº¦ç‰¹å®šå¤šè¯­è¨€æ¨¡å‹åœ¨å°åº¦è¯­è¨€ä¸­çš„è¯­è¨€å­¦ç‰¹æ€§æ•æ‰å¾—æ›´å¥½ã€‚</li>
<li>é€šç”¨æ¨¡å‹åœ¨æŸäº›æ‰°åŠ¨æƒ…å†µä¸‹å±•ç°å‡ºæ›´å¥½çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5aa91d2f19761dc670f5bb18b578a717" align="middle">
<img src="https://picx.zhimg.com/v2-b6ebcf9a68f33b1710bf8f5836360790" align="middle">
<img src="https://picx.zhimg.com/v2-b8ccfda7d72a6cff2c56d51c90541dde" align="middle">
<img src="https://picx.zhimg.com/v2-e120f2d72e4d4a53589217b0679deea6" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LLM-Targeted-Underperformance-Disproportionately-Impacts-Vulnerable-Users"><a href="#LLM-Targeted-Underperformance-Disproportionately-Impacts-Vulnerable-Users" class="headerlink" title="LLM Targeted Underperformance Disproportionately Impacts Vulnerable   Users"></a>LLM Targeted Underperformance Disproportionately Impacts Vulnerable   Users</h2><p><strong>Authors:Elinor Poole-Dayan, Deb Roy, Jad Kabbara</strong></p>
<p>While state-of-the-art large language models (LLMs) have shown impressive performance on many tasks, there has been extensive research on undesirable model behavior such as hallucinations and bias. In this work, we investigate how the quality of LLM responses changes in terms of information accuracy, truthfulness, and refusals depending on three user traits: English proficiency, education level, and country of origin. We present extensive experimentation on three state-of-the-art LLMs and two different datasets targeting truthfulness and factuality. Our findings suggest that undesirable behaviors in state-of-the-art LLMs occur disproportionately more for users with lower English proficiency, of lower education status, and originating from outside the US, rendering these models unreliable sources of information towards their most vulnerable users. </p>
<blockquote>
<p>è™½ç„¶æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å…³äºå…¶ä¸å¸Œæœ›å‡ºç°çš„è¡Œä¸ºï¼ˆå¦‚è™šæ„å’Œåè§ï¼‰çš„ç ”ç©¶å·²éå¸¸å¹¿æ³›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨è‹±è¯­ç†Ÿç»ƒç¨‹åº¦ã€æ•™è‚²æ°´å¹³å’ŒåŸç±å›½ä¸‰ä¸ªç”¨æˆ·ç‰¹å¾æ–¹é¢ï¼ŒLLMå“åº”çš„ä¿¡æ¯å‡†ç¡®æ€§ã€çœŸå®æ€§å’Œæ‹’ç»æ–¹é¢çš„è´¨é‡å¦‚ä½•å˜åŒ–ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸¤ä¸ªé’ˆå¯¹çœŸå®æ€§å’Œäº‹å®æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ç”¨æˆ·è‹±è¯­æ°´å¹³è¾ƒä½ã€å—æ•™è‚²ç¨‹åº¦è¾ƒä½ä»¥åŠéç¾å›½ç±çš„ç”¨æˆ·ç¾¤ä½“ä¸­ï¼Œæœ€å…ˆè¿›çš„LLMæ›´æœ‰å¯èƒ½å‡ºç°ä¸è‰¯è¡Œä¸ºï¼Œå¯¼è‡´è¿™äº›æ¨¡å‹å¯¹å…¶æœ€è„†å¼±çš„ç”¨æˆ·ç¾¤ä½“ä¸å¯é ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17737v2">PDF</a> Paper accepted at AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”¨æˆ·ç‰¹è´¨ï¼ˆè‹±è¯­ç†Ÿç»ƒç¨‹åº¦ã€æ•™è‚²æ°´å¹³å’ŒåŸç±å›½ï¼‰å½±å“ä¸‹çš„å›åº”è´¨é‡å˜åŒ–ï¼ŒåŒ…æ‹¬ä¿¡æ¯å‡†ç¡®æ€§ã€çœŸå®æ€§å’Œæ‹’ç»å›ç­”çš„æƒ…å†µã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¯¹äºè‹±è¯­æ°´å¹³è¾ƒä½ã€æ•™è‚²ç¨‹åº¦è¾ƒä½å’Œéç¾å›½ç”¨æˆ·ï¼Œå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºæ›´å¤šçš„ä¸è‰¯è¡Œä¸ºï¼Œå¦‚å¹»æƒ³å’Œåè§ï¼Œä½¿å¾—è¿™äº›æ¨¡å‹å¯¹äºæœ€è„†å¼±çš„ç”¨æˆ·ç¾¤è€Œè¨€å¹¶ä¸å¯é çš„èµ„è®¯æ¥æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”¨æˆ·ç‰¹è´¨å½±å“ä¸‹çš„å›åº”è´¨é‡æœ‰æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>è‹±è¯­ç†Ÿç»ƒç¨‹åº¦ã€æ•™è‚²æ°´å¹³å’ŒåŸç±å›½æ˜¯å½±å“LLMå›åº”çš„å…³é”®å› ç´ ã€‚</li>
<li>LLMåœ¨å›åº”ä½è‹±è¯­ç†Ÿç»ƒç¨‹åº¦çš„ç”¨æˆ·æ—¶æ˜“å‡ºç°ä¸è‰¯è¡Œä¸ºï¼Œå¦‚å¹»æƒ³å’Œåè§ã€‚</li>
<li>LLMå¯¹äºä½æ•™è‚²æ°´å¹³ç”¨æˆ·çš„ä¿¡æ¯å‡†ç¡®æ€§å’ŒçœŸå®æ€§è¡¨ç°è¾ƒå·®ã€‚</li>
<li>éç¾å›½ç”¨æˆ·åœ¨ä½¿ç”¨LLMæ—¶å¯èƒ½é¢ä¸´æ›´é«˜çš„ä¸è‰¯è¡Œä¸ºé£é™©ã€‚</li>
<li>LLMå¯¹ç‰¹å®šç”¨æˆ·ç¾¤ä½“çš„ä¸è‰¯è¡Œä¸ºå¯èƒ½å¯¼è‡´è¿™äº›æ¨¡å‹æˆä¸ºä¸å¯é çš„ä¿¡æ¯æ¥æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d956e03247062fdfacdddf340156f43a" align="middle">
<img src="https://picx.zhimg.com/v2-a66ea8aed45c72b1aa7804b480226854" align="middle">
<img src="https://picx.zhimg.com/v2-92013dd1a476a5b62d3e1667803ebb38" align="middle">
<img src="https://picx.zhimg.com/v2-357dcc14af46796e1842404725fa5627" align="middle">
<img src="https://picx.zhimg.com/v2-677a87ec83768b6fbae502e840a6174d" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-08/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-08/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-58d1c34ca10d2f042f862869865032e6" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-08  Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest   Path Problems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-31abecb445a5bf16bc7ed845390e313b" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-08  SIMS-V Simulated Instruction-Tuning for Spatial Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
