<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-08  SIMS-V Simulated Instruction-Tuning for Spatial Video Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-31abecb445a5bf16bc7ed845390e313b')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-13
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-08-æ›´æ–°"><a href="#2025-11-08-æ›´æ–°" class="headerlink" title="2025-11-08 æ›´æ–°"></a>2025-11-08 æ›´æ–°</h1><h2 id="SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding"><a href="#SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding" class="headerlink" title="SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding"></a>SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</h2><p><strong>Authors:Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie</strong></p>
<p>Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V â€“ a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks. </p>
<blockquote>
<p>å°½ç®¡å¤šåª’ä½“è¯­è¨€æ¨¡å‹åœ¨é«˜é˜¶è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨æ—¶ç©ºæ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚å½“å‰çš„ç©ºé—´è®­ç»ƒæ–¹å¼ä¾èµ–äºçœŸå®ä¸–ç•Œè§†é¢‘æ•°æ®ï¼Œä½†è·å–å¸¦æœ‰ç²¾ç¡®ç©ºé—´æ³¨é‡Šçš„å¤šæ ·åŒ–é•œå¤´ä»ç„¶æ˜¯ä¸€ä¸ªç“¶é¢ˆã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬æ¨å‡ºäº†SIMS-Vâ€”â€”ä¸€ä¸ªç³»ç»ŸåŒ–çš„æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨3Dæ¨¡æ‹Ÿå™¨çš„ç‰¹æƒä¿¡æ¯ï¼Œä¸ºå¤šåª’ä½“è¯­è¨€æ¨¡å‹åˆ›å»ºç©ºé—´ä¸°å¯Œçš„è§†é¢‘è®­ç»ƒæ•°æ®ã€‚ä½¿ç”¨æ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬é€šè¿‡ç³»ç»Ÿçš„é—®é¢˜ç±»å‹ã€æ··åˆå’Œè§„æ¨¡çš„å‰¥ç¦»ï¼Œç ”ç©¶å“ªäº›æ¨¡æ‹Ÿæ•°æ®çš„å±æ€§èƒ½æœ‰æ•ˆå®ç°ç°å®ä¸–ç•Œè¿ç§»ã€‚æˆ‘ä»¬ç¡®å®šäº†æœ€æœ‰æ•ˆçš„æœ€å°é—®é¢˜é›†ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªç±»åˆ«ï¼šåº¦é‡æµ‹é‡ã€è§†è§’ä¾èµ–æ¨ç†å’Œæ—¶é—´è·Ÿè¸ªï¼Œå°½ç®¡ä½¿ç”¨çš„é—®é¢˜ç±»å‹è¾ƒå°‘ï¼Œä½†åœ¨å‘å±•å¯è½¬ç§»çš„ç©ºé—´æ™ºèƒ½æ–¹é¢å´è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶è¶…è¶Šäº†å…¨é¢è¦†ç›–çš„æ•ˆæœã€‚è¿™äº›è§è§£ä½¿è®­ç»ƒæ›´åŠ é«˜æ•ˆï¼šæˆ‘ä»¬çš„ä»…å¾®è°ƒäº†2.5ä¸‡ä¸ªæ¨¡æ‹Ÿä¾‹å­çš„7Bå‚æ•°è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¶…è¶Šäº†æ›´å¤§çš„72BåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨ä¸¥æ ¼çš„ç°å®ä¸–ç•Œç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸ä¸“æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¿æŒå¯¹ä¸€èˆ¬è§†é¢‘ç†è§£æ€§èƒ½çš„åŒæ—¶ï¼Œåœ¨å®ä½“å’Œç°å®ä¸–ç•Œç©ºé—´ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04668v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ellisbrown.github.io/sims-v">https://ellisbrown.github.io/sims-v</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´æ¨ç†æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨3Dæ¨¡æ‹Ÿå™¨ç”Ÿæˆä¸°å¯Œç©ºé—´è§†é¢‘è®­ç»ƒæ•°æ®çš„æ–°æ¡†æ¶SIMS-Vã€‚é€šè¿‡å¯¹æ¨¡æ‹Ÿæ•°æ®çš„ç³»ç»Ÿæ€§åˆ†æï¼Œç ”ç©¶ç¡®å®šäº†ä¸‰ç§æœ€æœ‰æ•ˆçš„æ¨¡æ‹Ÿæ•°æ®é—®é¢˜ç±»å‹ï¼ŒåŒ…æ‹¬åº¦é‡æµ‹é‡ã€è§†è§’ä¾èµ–æ¨ç†å’Œæ—¶é—´è·Ÿè¸ªã€‚è¿™äº›å‘ç°ä½¿å¾—é«˜æ•ˆè®­ç»ƒæˆä¸ºå¯èƒ½ï¼šä½¿ç”¨å°‘é‡æ¨¡æ‹Ÿæ•°æ®å¯¹å°å‹è§†é¢‘LLMè¿›è¡Œå¾®è°ƒå³å¯å®ç°æ€§èƒ½æå‡ï¼Œä¸”åœ¨ä¸¥æ ¼çš„ç°å®ä¸–ç•Œç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„é€šç”¨æ€§èƒ½å¾—ä»¥ä¿ç•™å¹¶å¤§å¹…åº¦æå‡äº†å¤æ‚ä»»åŠ¡çš„è¡¨ç°ã€‚æ€»ä½“æ¥è¯´ï¼Œé€šè¿‡é«˜æ•ˆçš„æ¨¡æ‹Ÿè®­ç»ƒæ–¹å¼æ¨åŠ¨ç©ºé—´æ¨ç†æŠ€æœ¯çš„é£è·ƒè¿›æ­¥ã€‚è¿™é¡¹æ–°æŠ€æœ¯å¯èƒ½ä¼šæ¨è¿›è§†è§‰å¤„ç†çš„ç ”ç©¶æ–¹å‘å¹¶æä¾›æ½œåœ¨çš„å¤§è§„æ¨¡è®­ç»ƒæ–¹æ³•çš„å‘å±•ï¼ŒåŒæ—¶ä¹Ÿæœ‰åŠ©äºæå‡æœºå™¨åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³é”®è§è§£çš„ç®€è¦æ¦‚è¿°ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f997e25cdcb23056cd5bf98dd8a4b69" align="middle">
<img src="https://picx.zhimg.com/v2-9b7908e7b4d530ddbfe34bf8369432fd" align="middle">
<img src="https://picx.zhimg.com/v2-52f42f1419938fa54544bd5f64346a29" align="middle">
<img src="https://picx.zhimg.com/v2-89e36d3c85f9093a0206fc45a67b863a" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Real-to-Sim-Robot-Policy-Evaluation-with-Gaussian-Splatting-Simulation-of-Soft-Body-Interactions"><a href="#Real-to-Sim-Robot-Policy-Evaluation-with-Gaussian-Splatting-Simulation-of-Soft-Body-Interactions" class="headerlink" title="Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation   of Soft-Body Interactions"></a>Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation   of Soft-Body Interactions</h2><p><strong>Authors:Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song, Guangyan Cai, Zhuo Xu, Xiaochen Hu, Changxi Zheng, Yunzhu Li</strong></p>
<p>Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: <a target="_blank" rel="noopener" href="https://real2sim-eval.github.io/">https://real2sim-eval.github.io/</a> </p>
<blockquote>
<p>æœºå™¨äººæ“ä½œç­–ç•¥æ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œä½†åœ¨ç°å®ä¸–ç•Œä¸­ç›´æ¥è¯„ä¼°å®ƒä»¬ä»ç„¶æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ä¸”éš¾ä»¥é‡ç°ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ¶‰åŠå¯å˜å½¢ç‰©ä½“çš„ä»»åŠ¡ã€‚ä»¿çœŸæä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œç³»ç»Ÿçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰ä»¿çœŸå™¨å¾€å¾€æ— æ³•æ•æ‰è½¯ä½“äº¤äº’çš„è€¦åˆè§†è§‰å’Œç‰©ç†å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»ç°å®åˆ°ä»¿çœŸçš„ç­–ç•¥è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç°å®ä¸–ç•Œçš„è§†é¢‘æ„å»ºè½¯ä½“æ•°å­—å­ªç”Ÿä½“ï¼Œå¹¶ä½¿ç”¨ä¸‰ç»´é«˜æ–¯å–·ç»˜æŠ€æœ¯ä»¥é«˜åº¦é€¼çœŸçš„ä¿çœŸåº¦æ¸²æŸ“æœºå™¨äººã€ç‰©ä½“å’Œç¯å¢ƒã€‚æˆ‘ä»¬åœ¨ä»£è¡¨æ€§çš„å¯å˜å½¢æ“ä½œä»»åŠ¡ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ¯›ç»’ç©å…·åŒ…è£…ã€çº¿è·¯å¸ƒç½®å’ŒTå—æ¨åŠ¨ï¼Œè¯æ˜æ¨¡æ‹Ÿè¿è¡Œç»“æœä¸çœŸå®ä¸–ç•Œæ‰§è¡Œæ€§èƒ½é«˜åº¦ç›¸å…³ï¼Œå¹¶æ­ç¤ºäº†å­¦ä¹ ç­–ç•¥çš„å…³é”®è¡Œä¸ºæ¨¡å¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†åŸºäºç‰©ç†çš„é‡å»ºä¸é«˜è´¨æ„Ÿçš„æ¸²æŸ“ç›¸ç»“åˆï¼Œå¯ä»¥å®ç°æœºå™¨äººæ“ä½œç­–ç•¥çš„å¯é‡å¤æ€§ã€å¯æ‰©å±•æ€§å’Œå‡†ç¡®çš„è¯„ä¼°ã€‚ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://real2sim-eval.github.io/">https://real2sim-eval.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04665v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://real2sim-eval.github.io/">https://real2sim-eval.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>ç°å®ä¸–ç•Œä¸­æœºå™¨äººæ“ä½œç­–ç•¥çš„ç›´æ¥è¯„ä¼°æˆæœ¬é«˜æ˜‚ã€è€—æ—¶é•¿ä¸”éš¾ä»¥é‡ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¯å˜å½¢ç‰©ä½“çš„ä»»åŠ¡ä¸­ã€‚ä»¿çœŸæä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œç³»ç»Ÿçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰ä»¿çœŸå™¨å¾€å¾€æ— æ³•æ•æ‰è½¯ä½“äº¤äº’çš„å¤æ‚è§†è§‰å’Œç‰©ç†ç‰¹æ€§ã€‚æˆ‘ä»¬æå‡ºä¸€ç§ä»ç°å®åˆ°ä»¿çœŸçš„ç­–ç•¥è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºè½¯ä½“æ•°å­—åŒèƒèƒæ¨¡æ‹ŸçœŸå®ä¸–ç•Œè§†é¢‘ä¸­çš„æœºå™¨äººã€ç‰©ä½“å’Œç¯å¢ƒï¼Œä½¿ç”¨ä¸‰ç»´é«˜æ–¯ç»˜å›¾å®ç°é€¼çœŸæ¸²æŸ“ã€‚åœ¨å…¸å‹çš„å¯å˜å½¢æ“ä½œä»»åŠ¡ä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¡«å……æ¯›ç»’ç©å…·ã€çº¿è·¯é“ºè®¾å’ŒTå—æ¨åŠ¨ç­‰ä»»åŠ¡ï¼Œè¯æ˜æ¨¡æ‹Ÿè¿è¡Œç»“æœä¸çœŸå®ä¸–ç•Œæ‰§è¡Œæ€§èƒ½é«˜åº¦ç›¸å…³ï¼Œå¹¶æ­ç¤ºäº†å­¦ä¹ ç­–ç•¥çš„å…³é”®è¡Œä¸ºæ¨¡å¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“åˆç‰©ç†ä¿¡æ¯é‡å»ºå’Œé«˜å“è´¨æ¸²æŸ“å¯å®ç°æœºå™¨äººæ“ä½œç­–ç•¥çš„å¯é‡ç°ã€å¯æ‰©å±•å’Œå‡†ç¡®è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°å®ä¸–ç•Œä¸­æœºå™¨äººæ“ä½œç­–ç•¥è¯„ä¼°å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ¶‰åŠå¯å˜å½¢ç‰©ä½“çš„ä»»åŠ¡ï¼Œè€Œä»¿çœŸæä¾›äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰ä»¿çœŸå™¨å¾€å¾€æ— æ³•å‡†ç¡®æ¨¡æ‹Ÿè½¯ä½“äº¤äº’çš„å¤æ‚è§†è§‰å’Œç‰©ç†ç‰¹æ€§ã€‚</li>
<li>æå‡ºä¸€ç§ä»ç°å®åˆ°ä»¿çœŸçš„ç­–ç•¥è¯„ä¼°æ¡†æ¶ï¼Œæ„å»ºè½¯ä½“æ•°å­—åŒèƒèƒæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„æœºå™¨äººã€ç‰©ä½“å’Œç¯å¢ƒã€‚</li>
<li>ä½¿ç”¨ä¸‰ç»´é«˜æ–¯ç»˜å›¾å®ç°é€¼çœŸæ¸²æŸ“ï¼Œæé«˜æ¨¡æ‹Ÿçš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚</li>
<li>åœ¨å¤šç§å…¸å‹çš„å¯å˜å½¢æ“ä½œä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬æ¯›ç»’ç©å…·å¡«å……ã€çº¿è·¯é“ºè®¾å’ŒTå—æ¨åŠ¨ç­‰ä»»åŠ¡ã€‚</li>
<li>æ¨¡æ‹Ÿè¿è¡Œç»“æœä¸çœŸå®ä¸–ç•Œæ‰§è¡Œæ€§èƒ½é«˜åº¦ç›¸å…³ï¼Œèƒ½å¤Ÿå‡†ç¡®åæ˜ ç­–ç•¥çš„ä¼˜åŠ£å’Œè¡Œä¸ºæ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-440d6cb3346a8e804ef63850d4c6dca5" align="middle">
<img src="https://picx.zhimg.com/v2-36ae69ddb760ba3032be16165b4a1722" align="middle">
<img src="https://picx.zhimg.com/v2-c66ba73e9c32995f665724a5b113ad08" align="middle">
<img src="https://picx.zhimg.com/v2-3747636e16a25f378e5f856a4d6d72f7" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Logit-Entropy-Adaptive-Stopping-Heuristic-for-Efficient-Chain-of-Thought-Reasoning"><a href="#Logit-Entropy-Adaptive-Stopping-Heuristic-for-Efficient-Chain-of-Thought-Reasoning" class="headerlink" title="Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought   Reasoning"></a>Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought   Reasoning</h2><p><strong>Authors:Mohammad Atif Quamar, Mohammad Areeb</strong></p>
<p>Chain-of-Thought (CoT) prompting is a key technique for enabling complex reasoning in large language models. However, generating full, fixed-length rationales is computationally wasteful, inflating both token usage and latency. We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free decoding algorithm that adaptively halts rationale generation. LEASH monitors two intrinsic signals: the slope of token-level entropy and the improvement in the top-logit margin. It terminates the generation once both signals plateau, indicating the model has reached a stable reasoning state. Across four instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces average token generation by 30â€“35% and latency by 27%, while incurring a 10 p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no additional training or supervision, offering a simple and efficient alternative to CoT decoding. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æç¤ºæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°å¤æ‚æ¨ç†çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”Ÿæˆå®Œæ•´ã€å›ºå®šé•¿åº¦çš„ç†ç”±åœ¨è®¡ç®—ä¸Šæ˜¯æµªè´¹çš„ï¼Œå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨é‡å’Œå»¶è¿Ÿã€‚æˆ‘ä»¬å¼•å…¥äº†LEASHï¼šæ— éœ€è®­ç»ƒçš„è§£ç ç®—æ³•â€”â€”å¯¹æ•°ç†µè‡ªé€‚åº”åœæ­¢å¯å‘å¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è‡ªé€‚åº”åœ°ç»ˆæ­¢ç†ç”±ç”Ÿæˆã€‚LEASHç›‘æ§ä¸¤ä¸ªå†…åœ¨ä¿¡å·ï¼šä»¤ç‰Œçº§åˆ«çš„ç†µæ–œç‡å’Œé¡¶éƒ¨å¯¹æ•°è¾¹ç•Œçš„æ”¹å–„æƒ…å†µã€‚ä¸€æ—¦è¿™ä¸¤ä¸ªä¿¡å·è¾¾åˆ°å¹³ç¨³çŠ¶æ€ï¼Œå³è¡¨ç¤ºæ¨¡å‹å·²ç»åˆ°è¾¾ç¨³å®šçš„æ¨ç†çŠ¶æ€ï¼Œæ­¤æ—¶ä¾¿ä¼šç»ˆæ­¢ç”Ÿæˆã€‚åœ¨GSM8Kå’ŒAQuA-RATåŸºå‡†æµ‹è¯•çš„å››ä¸ªæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸­ï¼ŒLEASHå°†å¹³å‡ä»¤ç‰Œç”Ÿæˆé‡å‡å°‘äº†30-35%ï¼Œå»¶è¿Ÿå‡å°‘äº†27%ï¼Œç›¸å¯¹äºCoTï¼Œå‡†ç¡®ç‡ä¸‹é™äº†10ä¸ªç™¾åˆ†ç‚¹ã€‚LEASHå…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–ç›‘ç£ï¼Œä¸ºCoTè§£ç æä¾›äº†ç®€å•é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04654v1">PDF</a> Presented at the 1st Workshop on Efficient Reasoning (NeurIPS 2025)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æç¤ºæ˜¯æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”Ÿæˆå®Œæ•´ã€å›ºå®šé•¿åº¦çš„æ¨ç†ç†ç”±åœ¨è®¡ç®—ä¸Šæ˜¯æµªè´¹çš„ï¼Œå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨é‡å’Œå»¶è¿Ÿã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†LEASHï¼šä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç ç®—æ³•ï¼Œå¯è‡ªé€‚åº”åœ°ç»ˆæ­¢ç†ç”±ç”Ÿæˆã€‚LEASHç›‘æ§ä¸¤ä¸ªå†…åœ¨ä¿¡å·ï¼šä»¤ç‰Œçº§åˆ«ç†µçš„æ–œç‡å’Œé¡¶éƒ¨å¯¹æ•°è¾¹ç•Œçš„æ”¹å–„ã€‚ä¸€æ—¦è¿™ä¸¤ä¸ªä¿¡å·è¾¾åˆ°ç¨³å®šçŠ¶æ€ï¼Œå³è¡¨ç¤ºæ¨¡å‹å·²åˆ°è¾¾ç¨³å®šçš„æ¨ç†çŠ¶æ€ï¼Œç®—æ³•å°±ä¼šç»ˆæ­¢ç”Ÿæˆã€‚åœ¨GSM8Kå’ŒAQuA-RATåŸºå‡†æµ‹è¯•çš„å››é¡¹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸­ï¼ŒLEASHå°†å¹³å‡ä»¤ç‰Œç”Ÿæˆé‡å‡å°‘äº†30-35%ï¼Œå»¶è¿Ÿå‡å°‘äº†27%ï¼Œç›¸å¯¹äºCoTï¼Œå‡†ç¡®ç‡ä¸‹é™äº†10ä¸ªç™¾åˆ†ç‚¹ã€‚LEASHå…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–ç›‘ç£ï¼Œä¸ºCoTè§£ç æä¾›äº†ç®€å•é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ˜¯æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†çš„å…³é”®ã€‚</li>
<li>ç”Ÿæˆå®Œæ•´ã€å›ºå®šé•¿åº¦çš„æ¨ç†ç†ç”±åœ¨è®¡ç®—ä¸Šæ˜¯æµªè´¹çš„ï¼Œä¼šå¢åŠ ä»¤ç‰Œä½¿ç”¨é‡å’Œå»¶è¿Ÿã€‚</li>
<li>LEASHæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§£ç ç®—æ³•ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°ç»ˆæ­¢ç†ç”±ç”Ÿæˆã€‚</li>
<li>LEASHé€šè¿‡ç›‘æ§ä¸¤ä¸ªå†…åœ¨ä¿¡å·ï¼šä»¤ç‰Œçº§åˆ«ç†µçš„æ–œç‡å’Œé¡¶éƒ¨å¯¹æ•°è¾¹ç•Œçš„æ”¹å–„ï¼Œæ¥åˆ¤æ–­ä½•æ—¶ç»ˆæ­¢ç”Ÿæˆã€‚</li>
<li>åœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLEASHèƒ½æœ‰æ•ˆå‡å°‘ä»¤ç‰Œç”Ÿæˆé‡å’Œå»¶è¿Ÿã€‚</li>
<li>ç›¸å¯¹äºCoTï¼ŒLEASHçš„å‡†ç¡®ç‡æœ‰æ‰€ä¸‹é™ï¼Œä½†æä¾›äº†ä¸€ç§ç®€å•é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74f315d1ea44a0a21d7f9bb4fc75158f" align="middle">
<img src="https://picx.zhimg.com/v2-718cb559503d928414ae8009d808d62c" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm"><a href="#Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm" class="headerlink" title="Thinking with Video: Video Generation as a Promising Multimodal   Reasoning Paradigm"></a>Thinking with Video: Video Generation as a Promising Multimodal   Reasoning Paradigm</h2><p><strong>Authors:Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu</strong></p>
<p>â€œThinking with Textâ€ and â€œThinking with Imagesâ€ paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce â€œThinking with Videoâ€, a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2â€™s performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions â€œthinking with videoâ€ as a unified multimodal reasoning paradigm. </p>
<blockquote>
<p>â€œæ–‡å­—æ€è€ƒâ€å’Œâ€å›¾åƒæ€è€ƒâ€èŒƒå¼æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›èŒƒå¼å­˜åœ¨å›ºæœ‰å±€é™æ€§ã€‚ï¼ˆ1ï¼‰å›¾åƒåªèƒ½æ•æ‰ç¬é—´ï¼Œæ— æ³•è¡¨ç¤ºåŠ¨æ€è¿‡ç¨‹æˆ–è¿ç»­å˜åŒ–ï¼›ï¼ˆ2ï¼‰æ–‡å­—å’Œè§†è§‰ä½œä¸ºä¸åŒçš„æ¨¡æ€è¢«åˆ†éš”å¼€æ¥ï¼Œé˜»ç¢äº†ç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œè§†é¢‘æ€è€ƒâ€è¿™ä¸€æ–°èŒƒå¼ï¼Œå®ƒåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Sora-2ï¼‰åœ¨ç»Ÿä¸€çš„æ—¶é—´æ¡†æ¶å†…æ¡¥æ¥è§†è§‰å’Œæ–‡æœ¬æ¨ç†ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æ¢ç´¢ï¼Œæˆ‘ä»¬å¼€å‘äº†è§†é¢‘æ€è€ƒåŸºå‡†æµ‹è¯•ï¼ˆVideoThinkBenchï¼‰ã€‚VideoThinkBenchåŒ…å«ä¸¤ä¸ªä»»åŠ¡ç±»åˆ«ï¼šï¼ˆ1ï¼‰ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œçœ¼çƒæµ‹éªŒï¼‰ï¼Œä»¥åŠï¼ˆ2ï¼‰ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼ŒGSM8Kçš„å­é›†ï¼ŒMMMUï¼‰ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¯æ˜Sora-2æ˜¯ä¸€ä¸ªèƒ½èƒœä»»çš„æ¨ç†è€…ã€‚åœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ï¼ŒSora-2é€šå¸¸ä¸æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼ˆSOTAï¼‰VLMç›¸å½“ï¼Œç”šè‡³åœ¨å‡ ä¸ªä»»åŠ¡ä¸Šè¶…è¿‡äº†VLMï¼Œå¦‚çœ¼çƒæ¸¸æˆã€‚åœ¨ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ï¼ŒSora-2åœ¨MATHä¸Šè¾¾åˆ°92%çš„å‡†ç¡®ç‡ï¼Œåœ¨MMMUä¸Šè¾¾åˆ°75.53%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç³»ç»Ÿåœ°åˆ†æäº†è¿™äº›èƒ½åŠ›çš„æ¥æºã€‚æˆ‘ä»¬è¿˜å‘ç°è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆself-consistencyï¼‰å’Œä¸Šä¸‹æ–‡å†…å­¦ä¹ ï¼ˆin-context learningï¼‰å¯ä»¥æé«˜Sora-2çš„æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹æ˜¯æ½œåœ¨çš„ä¸€ä½“åŒ–å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼Œâ€è§†é¢‘æ€è€ƒâ€æ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04570v1">PDF</a> 36 pages, 14 figures</p>
<p><strong>Summary</strong><br>åœ¨æ–‡æœ¬ä¸­ï¼Œä»‹ç»äº†â€œThinking with Videoâ€è¿™ä¸€æ–°çš„èŒƒå¼ï¼Œè¯¥èŒƒå¼åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Sora-2ï¼‰æ¥æ¡¥æ¥è§†è§‰å’Œæ–‡æœ¬æ¨ç†çš„ç»Ÿä¸€æ—¶é—´æ¡†æ¶ã€‚ä¸ºæ”¯æŒè¿™ä¸€æ¢ç´¢ï¼Œå¼€å‘äº†Video Thinking Benchmarkï¼ˆVideoThinkBenchï¼‰ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤ç±»ä»»åŠ¡ï¼šä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡å’Œä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒSora-2ä½œä¸ºæ¨ç†å™¨çš„èƒ½åŠ›å¾—åˆ°äº†éªŒè¯ï¼Œåœ¨æŸäº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ç”šè‡³è¶…è¿‡äº†å½“å‰æœ€ä½³çš„VLMsã€‚æ€»ä½“è€Œè¨€ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹å…·æœ‰æˆä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹çš„æ½œåŠ›ï¼Œè€Œâ€œThinking with Videoâ€åˆ™å®šä½ä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨ç†èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>â€œThinking with Videoâ€èŒƒå¼é€šè¿‡åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚Sora-2ï¼Œæ¥èåˆè§†è§‰å’Œæ–‡æœ¬æ¨ç†ï¼Œå…‹æœäº†è¿‡å»â€œThinking with Textâ€å’Œâ€œThinking with Imagesâ€èŒƒå¼çš„å±€é™æ€§ã€‚</li>
<li>VideoThinkBenchåŒ…å«ä¸¤ç±»ä»»åŠ¡ï¼šä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆå¦‚Eyeballing Puzzlesï¼‰å’Œä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ˆå¦‚GSM8Kã€MMMUçš„å­é›†ï¼‰ï¼Œä¸ºè¯„ä¼°æ¨¡å‹æä¾›äº†å…¨é¢çš„æµ‹è¯•ç¯å¢ƒã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºSora-2åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æœ€ä½³VLMsç›¸å½“æˆ–æ›´ä¼˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šã€‚</li>
<li>åœ¨ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šï¼ŒSora-2è¾¾åˆ°äº†ä¸€å®šçš„å‡†ç¡®æ€§æ°´å¹³ï¼Œè¿™æ˜¾ç¤ºäº†å…¶å¼ºå¤§çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¯¹Sora-2çš„æ€§èƒ½è¿›è¡Œç³»ç»Ÿæ€§åˆ†æï¼Œå‘ç°è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆself-consistencyï¼‰å’Œä¸Šä¸‹æ–‡å†…å­¦ä¹ ï¼ˆin-context learningï¼‰æ˜¯æé«˜å…¶æ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
<li>è§†é¢‘ç”Ÿæˆæ¨¡å‹å…·æœ‰æˆä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ¨¡å‹çš„æ½œåŠ›ï¼Œè¿™è¡¨æ˜åœ¨å¤šåª’ä½“å†…å®¹ç†è§£å’Œç”Ÿæˆæ–¹é¢æœ‰ç€å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8549fb31c48a11b58022fd0f9dce8f60" align="middle">
<img src="https://picx.zhimg.com/v2-283d1d691ac7d7cb93a147c49d6e5003" align="middle">
<img src="https://picx.zhimg.com/v2-a4c43a2f7b19cc1dc6804f260b553edb" align="middle">
<img src="https://picx.zhimg.com/v2-3766b8e339f1dc466ceeccac257159e7" align="middle">
<img src="https://picx.zhimg.com/v2-a08ffacdfff2e2408e6fbd29c6f22698" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="THEval-Evaluation-Framework-for-Talking-Head-Video-Generation"><a href="#THEval-Evaluation-Framework-for-Talking-Head-Video-Generation" class="headerlink" title="THEval. Evaluation Framework for Talking Head Video Generation"></a>THEval. Evaluation Framework for Talking Head Video Generation</h2><p><strong>Authors:Nabyl Quignon, Baptiste Chopin, Yaohui Wang, Antitza Dantcheva</strong></p>
<p>Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field. </p>
<blockquote>
<p>è§†é¢‘ç”Ÿæˆå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç”Ÿæˆçš„è§†é¢‘è¶Šæ¥è¶Šé€¼çœŸã€‚ç„¶è€Œï¼Œç”Ÿæˆçš„è¿…é€Ÿå‘å±•è¶…å‡ºäº†å……è¶³è¯„ä¼°æŒ‡æ ‡çš„åˆ¶å®šé€Ÿåº¦ã€‚ç›®å‰ï¼Œå¤´éƒ¨è¯´è¯ç”Ÿæˆçš„è¯„ä»·ä¸»è¦ä¾èµ–äºæœ‰é™çš„è¯„ä»·æŒ‡æ ‡ï¼ŒåŒ…æ‹¬é€šç”¨è§†é¢‘è´¨é‡ã€å”‡åŒæ­¥ä»¥åŠç”¨æˆ·ç ”ç©¶ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä»·æ¡†æ¶ï¼ŒåŒ…å«ä¸ä¸‰ä¸ªç»´åº¦ç›¸å…³çš„8ä¸ªæŒ‡æ ‡ï¼šï¼ˆiï¼‰è´¨é‡ï¼Œï¼ˆiiï¼‰è‡ªç„¶åº¦ï¼Œï¼ˆiiiï¼‰åŒæ­¥æ€§ã€‚åœ¨é€‰æ‹©æŒ‡æ ‡æ—¶ï¼Œæˆ‘ä»¬æ³¨é‡æ•ˆç‡ä»¥åŠä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚åŸºäºæ­¤è€ƒè™‘ï¼Œæˆ‘ä»¬ç®€åŒ–äº†å¯¹å¤´éƒ¨ã€å˜´å·´å’Œçœ‰æ¯›çš„ç²¾ç»†åŠ¨æ€åˆ†æä»¥åŠé¢éƒ¨è´¨é‡ã€‚æˆ‘ä»¬åœ¨ç”±17ç§æœ€æ–°æ¨¡å‹ç”Ÿæˆçš„85000ä¸ªè§†é¢‘ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè™½ç„¶è®¸å¤šç®—æ³•åœ¨å”‡åŒæ­¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç”Ÿæˆè¡¨ç°åŠ›å’Œæ— ç‘•ç–µçš„ç»†èŠ‚æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›è§†é¢‘æ˜¯åŸºäºæˆ‘ä»¬æ•´ç†çš„ä¸€ä¸ªæ–°å‹çœŸå®æ•°æ®é›†ç”Ÿæˆçš„ï¼Œæ—¨åœ¨å‡è½»è®­ç»ƒæ•°æ®çš„åè§ã€‚æˆ‘ä»¬æå‡ºçš„åŸºå‡†æ¡†æ¶æ—¨åœ¨è¯„ä¼°ç”Ÿæˆæ–¹æ³•çš„æ”¹è¿›ã€‚åŸå§‹ä»£ç ã€æ•°æ®é›†å’Œæ’è¡Œæ¦œå°†å…¬å¼€å‘å¸ƒå¹¶å®šæœŸæ›´æ–°æ–°çš„æ–¹æ³•ï¼Œä»¥åæ˜ è¯¥é¢†åŸŸçš„è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04520v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç”Ÿæˆçš„è§†é¢‘è¶Šæ¥è¶Šé€¼çœŸã€‚ç„¶è€Œï¼Œè¯„ä¼°æŒ‡æ ‡çš„å‘å±•è·Ÿä¸ä¸Šç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿè¿›æ­¥ã€‚é’ˆå¯¹è¯´è¯äººå¤´éƒ¨ç”Ÿæˆè§†é¢‘çš„è¯„ä¼°ï¼Œç›®å‰ä¸»è¦ä¾èµ–äºæœ‰é™çš„æŒ‡æ ‡ï¼Œå¦‚è§†é¢‘è´¨é‡ã€å”‡åŒæ­¥å’Œç”¨æˆ·ç ”ç©¶ç­‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«8ä¸ªä¸ä¸‰ä¸ªç»´åº¦ç›¸å…³çš„æŒ‡æ ‡ï¼šè´¨é‡ã€è‡ªç„¶åº¦å’ŒåŒæ­¥æ€§ã€‚åœ¨é€‰æ‹©æŒ‡æ ‡æ—¶ï¼Œæˆ‘ä»¬æ³¨é‡æ•ˆç‡å’Œå¯¹äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚åŸºäºè¿™äº›è€ƒè™‘ï¼Œæˆ‘ä»¬ç®€åŒ–äº†å¯¹å¤´éƒ¨ã€å˜´å·´å’Œçœ‰æ¯›çš„ç²¾ç»†åŠ¨æ€åˆ†æï¼Œä»¥åŠé¢éƒ¨è´¨é‡ã€‚æˆ‘ä»¬å¯¹ç”±æœ€æ–°é¡¶çº§æ¨¡å‹ç”Ÿæˆçš„85000ä¸ªè§†é¢‘è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå‘ç°å°½ç®¡è®¸å¤šç®—æ³•åœ¨å”‡åŒæ­¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç”Ÿæˆè¡¨ç°åŠ›å’Œæ— ç‘•ç–µç»†èŠ‚æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›è§†é¢‘æ˜¯åŸºäºæˆ‘ä»¬ç²¾å¿ƒæ•´ç†çš„çœŸå®æ•°æ®é›†ç”Ÿæˆçš„ï¼Œæ—¨åœ¨å‡å°‘è®­ç»ƒæ•°æ®çš„åè§ã€‚æˆ‘ä»¬æå‡ºçš„åŸºå‡†æ¡†æ¶æ—¨åœ¨è¯„ä¼°ç”Ÿæˆæ–¹æ³•çš„æ”¹è¿›æƒ…å†µã€‚åŸå§‹ä»£ç ã€æ•°æ®é›†å’Œæ’è¡Œæ¦œå°†å®šæœŸå…¬å¼€å‘å¸ƒå’Œæ›´æ–°ï¼Œä»¥åæ˜ è¯¥é¢†åŸŸçš„è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç”Ÿæˆè§†é¢‘è¶Šæ¥è¶Šé€¼çœŸã€‚</li>
<li>å½“å‰è¯„ä¼°è¯´è¯äººå¤´éƒ¨ç”Ÿæˆè§†é¢‘çš„æŒ‡æ ‡æœ‰é™ï¼Œä¸»è¦ä¾§é‡äºè§†é¢‘è´¨é‡ã€å”‡åŒæ­¥å’Œç”¨æˆ·ç ”ç©¶ã€‚</li>
<li>æå‡ºæ–°çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«8ä¸ªæŒ‡æ ‡ï¼Œæ¶‰åŠè´¨é‡ã€è‡ªç„¶åº¦å’ŒåŒæ­¥æ€§ä¸‰ä¸ªç»´åº¦ã€‚</li>
<li>åœ¨é€‰æ‹©è¯„ä¼°æŒ‡æ ‡æ—¶ï¼Œå¼ºè°ƒæ•ˆç‡å’Œå¯¹äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç°æœ‰ç®—æ³•åœ¨å”‡åŒæ­¥æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç”Ÿæˆè¡¨ç°åŠ›å’Œæ— ç‘•ç–µç»†èŠ‚æ–¹é¢ä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨çœŸå®æ•°æ®é›†ç”Ÿæˆè§†é¢‘ï¼Œä»¥å‡å°‘è®­ç»ƒæ•°æ®çš„åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83efaa7c8315836890254b0cca42540e" align="middle">
<img src="https://picx.zhimg.com/v2-0ec34a24e484e5230672a711d01539b0" align="middle">
<img src="https://picx.zhimg.com/v2-7852cef2e0dbacd0b30030bf7a2efb90" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="V-Thinker-Interactive-Thinking-with-Images"><a href="#V-Thinker-Interactive-Thinking-with-Images" class="headerlink" title="V-Thinker: Interactive Thinking with Images"></a>V-Thinker: Interactive Thinking with Images</h2><p><strong>Authors:Runqi Qiao, Qiuna Tan, Minghan Yang, Guanting Dong, Peiqing Yang, Shiqiang Lang, Enhui Wan, Xiaowan Wang, Yida Xu, Lan Yang, Chong Sun, Chen Li, Honggang Zhang</strong></p>
<p>Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising â€œThinking with Imagesâ€ paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications. </p>
<blockquote>
<p>èµ‹èƒ½å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä»¥æ·±åº¦æ•´åˆå›¾åƒäº¤äº’ä¸é•¿å‘¨æœŸæ¨ç†èƒ½åŠ›ï¼Œä¸€ç›´æ˜¯è¯¥é¢†åŸŸçš„é•¿æœŸæŒ‘æˆ˜ã€‚æœ€è¿‘ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ¨ç†æŠ€æœ¯çš„è¿›æ­¥æ¢ç´¢äº†ä¸ºLMMçš„â€œå›¾æ–‡ç»“åˆæ€è€ƒâ€èŒƒå¼ï¼Œæ ‡å¿—ç€ä»å›¾åƒè¾…åŠ©æ¨ç†åˆ°å›¾åƒäº¤äº’æ€è€ƒçš„è½¬å˜ã€‚è™½ç„¶è¿™ä¸€é‡Œç¨‹ç¢‘å¼çš„è¿›å±•ä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨äºç²¾ç»†çš„å›¾åƒåŒºåŸŸï¼Œä½†å—é™äºæœ‰é™çš„è§†è§‰å·¥å…·ç©ºé—´å’Œç‰¹å®šçš„ä»»åŠ¡å·¥ä½œæµç¨‹è®¾è®¡ï¼Œè¿›å±•ä»ç„¶å—åˆ°é™åˆ¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†V-Thinkerï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„å¤šæ¨¡æ€æ¨ç†åŠ©æ‰‹ï¼Œå®ƒé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ ä½¿ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„äº¤äº’æ€è€ƒæˆä¸ºå¯èƒ½ã€‚V-ThinkeråŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰æ•°æ®è¿›åŒ–é£è½®ï¼Œå®ƒè‡ªåŠ¨åˆæˆã€å‘å±•å’ŒéªŒè¯è·¨ä¸‰ä¸ªç»´åº¦çš„äº¤äº’å¼æ¨ç†æ•°æ®é›†â€”â€”å¤šæ ·æ€§ã€è´¨é‡å’Œéš¾åº¦ï¼›ï¼ˆ2ï¼‰è§†è§‰æ¸è¿›è®­ç»ƒè¯¾ç¨‹ï¼Œé¦–å…ˆé€šè¿‡ç‚¹çº§ç›‘ç£å¯¹é½æ„ŸçŸ¥ï¼Œç„¶åé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ•´åˆäº¤äº’å¼æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†é’ˆå¯¹è§†è§‰ä¸ºä¸­å¿ƒäº¤äº’å¼æ¨ç†ä»»åŠ¡çš„ä¸“å®¶éªŒè¯åŸºå‡†VTBenchã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ä¸€èˆ¬è¿˜æ˜¯äº¤äº’å¼æ¨ç†åœºæ™¯ä¸­ï¼ŒV-Thinkeréƒ½æŒç»­è¶…è¶Šå¼ºå¤§çš„LMMåŸºå‡†æ¨¡å‹ï¼Œä¸ºæ¨è¿›å›¾åƒäº¤äº’æ¨ç†åº”ç”¨æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04460v1">PDF</a> Working in progress</p>
<p><strong>Summary</strong><br>åœ¨èµ‹èƒ½å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ–¹é¢ï¼Œé€šè¿‡æ•´åˆå›¾åƒäº¤äº’ä¸é•¿æœŸæ¨ç†èƒ½åŠ›ä»å­˜åœ¨æŒ‘æˆ˜ã€‚è¿‘æœŸåœ¨è§†è§‰ä¸ºä¸­å¿ƒï¼ˆVision-centricï¼‰çš„æ¨ç†ç ”ç©¶ä¸­ï¼Œå‡ºç°äº†ä¸ºLMMsçš„â€œå›¾æ–‡æ€ç»´â€èŒƒå¼ã€‚æ­¤çªç ´è™½ç„¶èƒ½ä½¿æ¨¡å‹å…³æ³¨äºå›¾åƒç»†èŠ‚åŒºåŸŸï¼Œä½†ä»å—é™äºæœ‰é™çš„è§†è§‰å·¥å…·ç©ºé—´å’Œä»»åŠ¡ç‰¹å®šçš„å·¥ä½œæµç¨‹è®¾è®¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºV-Thinkerï¼Œä¸€ç§é€šç”¨å¤šæ¨¡æ€æ¨ç†åŠ©æ‰‹ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ å®ç°äº¤äº’å¼ã€è§†è§‰ä¸ºä¸­å¿ƒçš„æ€è€ƒã€‚å…¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆä¸€ï¼‰æ•°æ®è¿›åŒ–é£è½®ï¼ˆData Evolution Flywheelï¼‰ï¼Œå®ƒèƒ½è‡ªåŠ¨åˆæˆã€æ¼”å˜å’ŒéªŒè¯è·¨è¶Šä¸‰ä¸ªç»´åº¦çš„äº¤äº’å¼æ¨ç†æ•°æ®é›†ï¼›ï¼ˆäºŒï¼‰è§†è§‰æ¸è¿›è®­ç»ƒè¯¾ç¨‹ï¼ˆVisual Progressive Training Curriculumï¼‰ï¼Œå®ƒé¦–å…ˆé€šè¿‡ç‚¹å¯¹ç‚¹ç›‘ç£å¯¹é½æ„ŸçŸ¥ï¼Œç„¶åé€šè¿‡ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ•´åˆäº¤äº’å¼æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†VTBenchä¸“å®¶éªŒè¯åŸºå‡†æµ‹è¯•ï¼Œé’ˆå¯¹è§†è§‰ä¸ºä¸­å¿ƒäº¤äº’å¼æ¨ç†ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼ŒV-Thinkeråœ¨ä¸€èˆ¬å’Œäº¤äº’å¼æ¨ç†åœºæ™¯ä¸­å‡ä¼˜äºå¼ºå¤§çš„LMMåŸºçº¿æ¨¡å‹ï¼Œä¸ºæ¨è¿›å›¾åƒäº¤äº’æ¨ç†åº”ç”¨æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ•´åˆå›¾åƒäº¤äº’ä¸é•¿æœŸæ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>â€œå›¾æ–‡æ€ç»´â€èŒƒå¼æ˜¯LMMsçš„ä¸€ä¸ªçªç ´ï¼Œä½¿æ¨¡å‹å…³æ³¨å›¾åƒç»†èŠ‚åŒºåŸŸã€‚</li>
<li>å½“å‰æ–¹æ³•å—é™äºæœ‰é™çš„è§†è§‰å·¥å…·ç©ºé—´å’Œä»»åŠ¡ç‰¹å®šçš„å·¥ä½œæµç¨‹è®¾è®¡ã€‚</li>
<li>V-Thinkeræ˜¯ä¸€ä¸ªé€šç”¨å¤šæ¨¡æ€æ¨ç†åŠ©æ‰‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°äº¤äº’å¼ã€è§†è§‰ä¸ºä¸­å¿ƒçš„æ€è€ƒã€‚</li>
<li>V-ThinkeråŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šæ•°æ®è¿›åŒ–é£è½®å’Œè§†è§‰æ¸è¿›è®­ç»ƒè¯¾ç¨‹ã€‚</li>
<li>VTBenchæ˜¯ä¸€ä¸ªä¸“å®¶éªŒè¯åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†è§‰ä¸ºä¸­å¿ƒäº¤äº’å¼æ¨ç†ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒV-Thinkeråœ¨ä¸€èˆ¬å’Œäº¤äº’å¼æ¨ç†åœºæ™¯ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b701d9b68b84628744ae864e0d67677c" align="middle">
<img src="https://picx.zhimg.com/v2-4c1fe0d6d8c82097fc6e372dd33b337c" align="middle">
<img src="https://picx.zhimg.com/v2-641944859c8c9006b64a34abf4620a9b" align="middle">
<img src="https://picx.zhimg.com/v2-e7200569c0a0ab90cf537e3693d07162" align="middle">
<img src="https://picx.zhimg.com/v2-ffe3f065bba19f5eb47a07c9a6888057" align="middle">
<img src="https://picx.zhimg.com/v2-1e8de77570c24c13e85cd4095acbdaae" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Monitor-Generate-Verify-MGV-Formalising-Metacognitive-Theory-for-Language-Model-Reasoning"><a href="#Monitor-Generate-Verify-MGV-Formalising-Metacognitive-Theory-for-Language-Model-Reasoning" class="headerlink" title="Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for   Language Model Reasoning"></a>Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for   Language Model Reasoning</h2><p><strong>Authors:Nick Oh, Fernand Gobet</strong></p>
<p>Test-time reasoning architectures such as those following the Generate-Verify paradigm â€“ where a model iteratively refines or verifies its own generated outputs â€“ prioritise generation and verification but exclude the monitoring processes that determine when and how reasoning should begin. This omission may contribute to the prefix dominance trap, in which models commit early to suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy loss. We address this architectural gap by formalising Flavellâ€™s and Nelson and Narensâ€™ metacognitive theories into computational specifications, proposing the Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify paradigm by adding explicit monitoring that captures metacognitive experiences (from difficulty assessments to confidence judgements) before generation begins and refines future monitoring through verification feedback. Though we present no empirical validation, this work provides the first systematic computational translation of foundational metacognitive theories, offering a principled vocabulary for understanding reasoning system failures and suggesting specific architectural interventions for future test-time reasoning designs. </p>
<blockquote>
<p>åœ¨éµå¾ªç”Ÿæˆ-éªŒè¯èŒƒå¼çš„æµ‹è¯•æ—¶é—´æ¨ç†æ¶æ„ä¸­ï¼Œæ¨¡å‹ä¼šè¿­ä»£åœ°å®Œå–„æˆ–éªŒè¯è‡ªèº«ç”Ÿæˆçš„è¾“å‡ºï¼Œè¿™ç§æ¶æ„è™½ç„¶é‡è§†ç”Ÿæˆå’ŒéªŒè¯ï¼Œä½†å¿½ç•¥äº†ç¡®å®šä½•æ—¶ä»¥åŠå¦‚ä½•å¼€å§‹æ¨ç†çš„ç›‘æ§è¿‡ç¨‹ã€‚è¿™ç§é—æ¼å¯èƒ½å¯¼è‡´å‰ç¼€ä¸»å¯¼é™·é˜±ï¼Œå³æ¨¡å‹è¿‡æ—©åœ°é™·å…¥éæœ€ä¼˜æ¨ç†è·¯å¾„ï¼Œå¹¶ä¸”å¾ˆå°‘æ¢å¤ï¼Œé€ æˆå¤§çº¦20%çš„å‡†ç¡®ç‡æŸå¤±ã€‚æˆ‘ä»¬é€šè¿‡å°†å¼—æ‹‰ç»´å°”ä»¥åŠçº³å°”é€Šå’Œçº³ä¼¦æ–¯çš„å…ƒè®¤çŸ¥ç†è®ºå½¢å¼åŒ–ä¸ºè®¡ç®—è§„èŒƒæ¥è§£å†³è¿™ä¸€æ¶æ„ç¼ºé™·ï¼Œæå‡ºäº†ç›‘æ§-ç”Ÿæˆ-éªŒè¯ï¼ˆMGVï¼‰æ¡†æ¶ã€‚MGVé€šè¿‡æ·»åŠ æ˜ç¡®çš„ç›‘æ§æ¥æ‰©å±•ç”Ÿæˆ-éªŒè¯èŒƒå¼ï¼Œåœ¨ç”Ÿæˆå¼€å§‹ä¹‹å‰æ•è·å…ƒè®¤çŸ¥ä½“éªŒï¼ˆä»éš¾åº¦è¯„ä¼°åˆ°ä¿¡å¿ƒåˆ¤æ–­ï¼‰ï¼Œå¹¶é€šè¿‡éªŒè¯åé¦ˆæ¥å®Œå–„æœªæ¥çš„ç›‘æ§ã€‚å°½ç®¡æˆ‘ä»¬æ²¡æœ‰æä¾›å®è¯éªŒè¯ï¼Œä½†è¿™é¡¹å·¥ä½œæä¾›äº†åŸºç¡€å…ƒè®¤çŸ¥ç†è®ºçš„é¦–æ¬¡ç³»ç»Ÿæ€§è®¡ç®—ç¿»è¯‘ï¼Œä¸ºç†è§£æ¨ç†ç³»ç»Ÿå¤±è´¥æä¾›äº†åŸåˆ™æ€§çš„è¯æ±‡ï¼Œå¹¶ä¸ºæœªæ¥çš„æµ‹è¯•æ—¶é—´æ¨ç†è®¾è®¡æå‡ºäº†å…·ä½“çš„æ¶æ„å¹²é¢„æªæ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04341v1">PDF</a> To-be presented at the Workshop on the Foundations of Reasoning in   Language Models at NeurIPS 2025 (non-archival)</p>
<p><strong>Summary</strong>ï¼šç”ŸæˆéªŒè¯èŒƒå¼ä¸‹çš„æµ‹è¯•æ—¶æ¨ç†æ¶æ„å…è®¸æ¨¡å‹è¿­ä»£åœ°å®Œå–„æˆ–éªŒè¯å…¶ç”Ÿæˆçš„è¾“å‡ºï¼Œä½†å¿½ç•¥äº†ç›‘æµ‹è¿‡ç¨‹ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹é™·å…¥æ—©æœŸä¸ç†æƒ³çš„æ¨ç†è·¯å¾„ï¼Œé€ æˆå¤§çº¦20%çš„å‡†ç¡®ç‡æŸå¤±ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å€Ÿé‰´å¼—æ‹‰ç»´å°”ã€çº³å°”é€Šå’Œå†…ä¼¦æ–¯çš„å…ƒè®¤çŸ¥ç†è®ºï¼Œæå‡ºäº†ç›‘æ§ç”ŸæˆéªŒè¯ï¼ˆMGVï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ‰©å±•äº†ç”ŸæˆéªŒè¯èŒƒå¼ï¼Œé€šè¿‡æ˜ç¡®çš„ç›‘æµ‹æ•æ‰å…ƒè®¤çŸ¥ä½“éªŒï¼ˆå¦‚éš¾åº¦è¯„ä¼°å’Œä¿¡å¿ƒåˆ¤æ–­ï¼‰ï¼Œåœ¨ç”Ÿæˆä¹‹å‰è¿›è¡Œç›‘æµ‹ï¼Œå¹¶é€šè¿‡éªŒè¯åé¦ˆä¼˜åŒ–æœªæ¥çš„ç›‘æµ‹ã€‚å°½ç®¡æ²¡æœ‰å®è¯éªŒè¯ï¼Œä½†è¿™é¡¹å·¥ä½œä¸ºå…ƒè®¤çŸ¥ç†è®ºæä¾›äº†ç³»ç»Ÿçš„è®¡ç®—ç¿»è¯‘ï¼Œæœ‰åŠ©äºç†è§£æ¨ç†ç³»ç»Ÿçš„å¤±è´¥å¹¶æä¾›æœªæ¥çš„å¹²é¢„æªæ–½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æµ‹è¯•æ—¶æ¨ç†æ¶æ„å¦‚ç”ŸæˆéªŒè¯èŒƒå¼å­˜åœ¨ç¼ºé™·ï¼Œå¯èƒ½å¿½ç•¥ç›‘æµ‹è¿‡ç¨‹å¯¼è‡´æ¨¡å‹é™·å…¥æ—©æœŸä¸ç†æƒ³çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>æ¨¡å‹å¿½è§†ç›‘æµ‹è¿‡ç¨‹å¯èƒ½å¯¼è‡´å‡†ç¡®ç‡æŸå¤±çº¦20%ã€‚</li>
<li>MGVæ¡†æ¶æ‰©å±•äº†ç”ŸæˆéªŒè¯èŒƒå¼ï¼Œé€šè¿‡æ˜ç¡®çš„ç›‘æµ‹æ•æ‰å…ƒè®¤çŸ¥ä½“éªŒã€‚</li>
<li>MGVæ¡†æ¶åœ¨ç”Ÿæˆä¹‹å‰è¿›è¡Œç›‘æµ‹ï¼Œå¹¶é€šè¿‡éªŒè¯åé¦ˆä¼˜åŒ–æœªæ¥çš„ç›‘æµ‹ã€‚</li>
<li>æœ¬æ–‡å€Ÿé‰´äº†å¼—æ‹‰ç»´å°”ã€çº³å°”é€Šå’Œå†…ä¼¦æ–¯çš„å…ƒè®¤çŸ¥ç†è®ºï¼Œä¸ºè®¡ç®—ç¿»è¯‘æä¾›äº†åŸºç¡€ã€‚</li>
<li>è¯¥å·¥ä½œæœ‰åŠ©äºç†è§£æ¨ç†ç³»ç»Ÿçš„å¤±è´¥åŸå› ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04341">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f0e5a175c5adcea86a38feb22a070cf" align="middle">
<img src="https://picx.zhimg.com/v2-fc29688c672edacc603f7e734aebb64a" align="middle">
<img src="https://picx.zhimg.com/v2-d17c6ffa241dfd21d8ab4886e8f3c967" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MacroNav-Multi-Task-Context-Representation-Learning-Enables-Efficient-Navigation-in-Unknown-Environments"><a href="#MacroNav-Multi-Task-Context-Representation-Learning-Enables-Efficient-Navigation-in-Unknown-Environments" class="headerlink" title="MacroNav: Multi-Task Context Representation Learning Enables Efficient   Navigation in Unknown Environments"></a>MacroNav: Multi-Task Context Representation Learning Enables Efficient   Navigation in Unknown Environments</h2><p><strong>Authors:Kuankuan Sima, Longbin Tang, Haozhe Ma, Lin Zhao</strong></p>
<p>Autonomous navigation in unknown environments requires compact yet expressive spatial understanding under partial observability to support high-level decision making. Existing approaches struggle to balance rich contextual representation with navigation efficiency. We present MacroNav, a learning-based navigation framework featuring two key components: (1) a lightweight context encoder trained via multi-task self-supervised learning to capture multi-scale, navigation-centric spatial representations; and (2) a reinforcement learning policy that seamlessly integrates these representations with graph-based reasoning for efficient action selection. Extensive experiments demonstrate the context encoderâ€™s efficient and robust environmental understanding. Real-world deployments further validate MacroNavâ€™s effectiveness, yielding significant gains over state-of-the-art navigation methods in both Success Rate (SR) and Success weighted by Path Length (SPL), while maintaining low computational cost. Code will be released upon acceptance. </p>
<blockquote>
<p>åœ¨æœªçŸ¥ç¯å¢ƒä¸­è¿›è¡Œè‡ªä¸»å¯¼èˆªï¼Œéœ€è¦åœ¨éƒ¨åˆ†å¯è§‚å¯Ÿæ€§çš„æƒ…å†µä¸‹ï¼Œå…·å¤‡ç´§å‡‘è€Œå¯Œæœ‰è¡¨ç°åŠ›çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œä»¥æ”¯æŒé«˜çº§å†³ç­–åˆ¶å®šã€‚ç°æœ‰æ–¹æ³•å¾ˆéš¾åœ¨ä¸°å¯Œçš„ä¸Šä¸‹æ–‡è¡¨ç¤ºå’Œå¯¼èˆªæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æˆ‘ä»¬æå‡ºäº†MacroNavï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå­¦ä¹ çš„å¯¼èˆªæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰é€šè¿‡å¤šä»»åŠ¡è‡ªæˆ‘ç›‘ç£å­¦ä¹ è®­ç»ƒçš„ä¸€ä¸ªè½»é‡çº§ä¸Šä¸‹æ–‡ç¼–ç å™¨ï¼Œç”¨äºæ•è·å¤šå°ºåº¦ã€ä»¥å¯¼èˆªä¸ºä¸­å¿ƒçš„ç©ºé—´è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰ä¸€ç§å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œæ— ç¼é›†æˆè¿™äº›è¡¨ç¤ºä¸åŸºäºå›¾çš„æ¨ç†ï¼Œä»¥å®ç°é«˜æ•ˆçš„è¡ŒåŠ¨é€‰æ‹©ã€‚å¤§é‡å®éªŒè¯æ˜äº†ä¸Šä¸‹æ–‡ç¼–ç å™¨å¯¹ç¯å¢ƒçš„ç†è§£å’Œé²æ£’æ€§ã€‚åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­è¿›ä¸€æ­¥éªŒè¯äº†MacroNavçš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨æˆåŠŸç‡å’Œè·¯å¾„é•¿åº¦åŠ æƒæˆåŠŸç‡æ–¹é¢å‡ä¼˜äºæœ€æ–°å¯¼èˆªæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚ä»£ç åœ¨éªŒæ”¶é€šè¿‡åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04320v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„å¯¼èˆªæ¡†æ¶MacroNavï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ä¸ªè½»é‡çº§ä¸Šä¸‹æ–‡ç¼–ç å™¨ï¼Œé€šè¿‡å¤šä»»åŠ¡è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ•è·å¤šå°ºåº¦ã€ä»¥å¯¼èˆªä¸ºä¸­å¿ƒçš„ç©ºé—´è¡¨ç¤ºï¼›ä»¥åŠä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œæ— ç¼é›†æˆè¿™äº›è¡¨ç¤ºä¸åŸºäºå›¾çš„æ¨ç†ï¼Œä»¥è¿›è¡Œé«˜æ•ˆçš„åŠ¨ä½œé€‰æ‹©ã€‚å®éªŒè¡¨æ˜ï¼Œä¸Šä¸‹æ–‡ç¼–ç å™¨èƒ½å¤Ÿé«˜æ•ˆä¸”ç¨³å¥åœ°ç†è§£ç¯å¢ƒã€‚åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­ï¼ŒMacroNavç›¸å¯¹äºæœ€æ–°å¯¼èˆªæ–¹æ³•åœ¨æˆåŠŸç‡ï¼ˆSRï¼‰å’Œè·¯å¾„é•¿åº¦åŠ æƒæˆåŠŸç‡ï¼ˆSPLï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MacroNavæ˜¯ä¸€ä¸ªåŸºäºå­¦ä¹ çš„å¯¼èˆªæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„è‡ªä¸»å¯¼èˆªé—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ä¸ªè½»é‡çº§ä¸Šä¸‹æ–‡ç¼–ç å™¨å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç¼–ç å™¨é€šè¿‡å¤šä»»åŠ¡è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ•è·å¤šå°ºåº¦ã€ä»¥å¯¼èˆªä¸ºä¸­å¿ƒçš„ç©ºé—´è¡¨ç¤ºã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç­–ç•¥é›†æˆäº†ç©ºé—´è¡¨ç¤ºä¸åŸºäºå›¾çš„æ¨ç†ï¼Œä»¥å®ç°é«˜æ•ˆåŠ¨ä½œé€‰æ‹©ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒMacroNavåœ¨ç¯å¢ƒç†è§£æ–¹é¢è¡¨ç°å‡ºé«˜æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­ï¼ŒMacroNavåœ¨æˆåŠŸç‡ï¼ˆSRï¼‰å’Œè·¯å¾„é•¿åº¦åŠ æƒæˆåŠŸç‡ï¼ˆSPLï¼‰æ–¹é¢è¶…è¶Šäº†ç°æœ‰å¯¼èˆªæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e7d547e6c24c4be42c47342ff268b85" align="middle">
<img src="https://picx.zhimg.com/v2-e44abd157823c5a2fc4fed89a748dd59" align="middle">
<img src="https://picx.zhimg.com/v2-0280c34f5759ef38012e07b743f72dfc" align="middle">
<img src="https://picx.zhimg.com/v2-de2a6ecf5953ea48ef09b3a55a0d3e66" align="middle">
<img src="https://picx.zhimg.com/v2-31abecb445a5bf16bc7ed845390e313b" align="middle">
<img src="https://picx.zhimg.com/v2-f30a4046ae6f9525b5ba6d09fa9f7ab1" align="middle">
<img src="https://picx.zhimg.com/v2-a05e83631ca60bbff74be2b1c659dc71" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents"><a href="#GUI-360-A-Comprehensive-Dataset-and-Benchmark-for-Computer-Using-Agents" class="headerlink" title="GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents"></a>GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents</h2><p><strong>Authors:Jian Mu, Chaoyun Zhang, Chiming Ni, Lu Wang, Bo Qiao, Kartik Mathur, Qianhui Wu, Yuhang Xie, Xiaojun Ma, Mengyu Zhou, Si Qin, Liqun Li, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</strong></p>
<p>We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and benchmark suite designed to advance computer-using agents (CUAs). CUAs present unique challenges and is constrained by three persistent gaps: a scarcity of real-world CUA tasks, the lack of automated collection-and-annotation pipelines for multi-modal trajectories, and the absence of a unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction.   GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated pipeline for query sourcing, environment-template construction, task instantiation, batched execution, and LLM-driven quality filtering. The released corpus contains over 1.2M executed action steps across thousands of trajectories in popular Windows office applications, and includes full-resolution screenshots, accessibility metadata when available, instantiated goals, intermediate reasoning traces, and both successful and failed action trajectories. The dataset supports three canonical tasks, GUI grounding, screen parsing, and action prediction, and a hybrid GUI+API action space that reflects modern agent designs. Benchmarking state-of-the-art visionâ€“language models on GUI-360$^\circ$ reveals substantial out-of-the-box shortcomings in grounding and action prediction; supervised fine-tuning and reinforcement learning yield significant gains but do not close the gap to human-level reliability. We release GUI-360$^\circ$ and accompanying code to facilitate reproducible research and accelerate progress on robust desktop CUAs.   The full dataset has been made public on <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/vyokky/GUI-360">https://huggingface.co/datasets/vyokky/GUI-360</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†GUI-360Â°ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ç»¼åˆæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨æ¨åŠ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰çš„å‘å±•ã€‚è®¡ç®—æœºä½¿ç”¨ä»£ç†é¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå¹¶å—åˆ°ä¸‰ä¸ªæŒç»­å­˜åœ¨çš„å·®è·çš„é™åˆ¶ï¼šç¼ºä¹çœŸå®ä¸–ç•Œçš„CUAä»»åŠ¡ã€ç¼ºä¹ç”¨äºå¤šæ¨¡å¼è½¨è¿¹çš„è‡ªåŠ¨åŒ–æ”¶é›†å’Œæ³¨é‡Šç®¡é“ï¼Œä»¥åŠç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†ï¼Œå¯ä»¥è”åˆè¯„ä¼°GUIå®šä½ã€å±å¹•è§£æå’ŒåŠ¨ä½œé¢„æµ‹ã€‚GUI-360Â°é€šè¿‡å¢å¼ºçš„LLMå’Œé«˜åº¦è‡ªåŠ¨åŒ–çš„ç®¡é“æ¥è§£å†³è¿™äº›å·®è·ï¼Œç”¨äºæŸ¥è¯¢æºã€ç¯å¢ƒæ¨¡æ¿æ„å»ºã€ä»»åŠ¡å®ä¾‹åŒ–ã€æ‰¹å¤„ç†æ‰§è¡Œå’ŒLLMé©±åŠ¨çš„è´¨é‡è¿‡æ»¤ã€‚å‘å¸ƒçš„æ•°æ®é›†åŒ…å«æµè¡Œçš„WindowsåŠå…¬è½¯ä»¶ä¸­æ•°åƒæ¡è½¨è¿¹çš„è¶…è¿‡120ä¸‡ä¸ªæ‰§è¡ŒåŠ¨ä½œæ­¥éª¤ï¼ŒåŒ…æ‹¬å…¨åˆ†è¾¨ç‡æˆªå›¾ã€å¯ç”¨çš„è®¿é—®å…ƒæ•°æ®ã€å®ä¾‹åŒ–ç›®æ ‡ã€ä¸­é—´æ¨ç†è½¨è¿¹ä»¥åŠæˆåŠŸå’Œå¤±è´¥çš„åŠ¨ä½œè½¨è¿¹ã€‚è¯¥æ•°æ®é›†æ”¯æŒGUIå®šä½ã€å±å¹•è§£æå’ŒåŠ¨ä½œé¢„æµ‹ä¸‰ä¸ªå…¸å‹ä»»åŠ¡ï¼Œä»¥åŠåæ˜ ç°ä»£ä»£ç†è®¾è®¡çš„æ··åˆGUI+APIåŠ¨ä½œç©ºé—´ã€‚åœ¨GUI-360Â°æ•°æ®é›†ä¸Šè¯„ä¼°æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºåœ¨å®šä½å’ŒåŠ¨ä½œé¢„æµ‹æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼›æœ‰ç›‘ç£çš„å¾®è°ƒå¼ºåŒ–å­¦ä¹ äº§ç”Ÿäº†å·¨å¤§æ”¶ç›Šï¼Œä½†ä»æœªå¼¥äººç±»æ°´å¹³çš„å¯é æ€§å·®è·ã€‚æˆ‘ä»¬å‘å¸ƒGUI-360Â°å’Œä¼´éšçš„ä»£ç ï¼Œä»¥ä¿ƒè¿›å¯é‡å¤çš„ç ”ç©¶ï¼Œå¹¶åŠ é€Ÿç¨³å¥æ¡Œé¢CUAçš„è¿›å±•ã€‚å®Œæ•´çš„æ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/vyokky/GUI-360%E5%85%AC%E5%BC%80%E3%80%82">https://huggingface.co/datasets/vyokky/GUI-360å…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04307v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GUI-360Â°æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å…¨é¢çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨æ¨åŠ¨è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰çš„å‘å±•ã€‚å®ƒè§£å†³äº†ä¸‰ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ï¼šçœŸå®ä¸–ç•ŒCUAä»»åŠ¡çš„ç¨€ç¼ºã€ç¼ºä¹å¤šæ¨¡æ€è½¨è¿¹çš„è‡ªåŠ¨åŒ–æ”¶é›†ä¸æ³¨é‡Šç®¡é“ä»¥åŠç¼ºä¹è”åˆè¯„ä¼°GUIå®šä½ã€å±å¹•è§£æå’ŒåŠ¨ä½œé¢„æµ‹çš„åŸºå‡†æµ‹è¯•ã€‚GUI-360Â°ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ç®¡é“æ¥æŸ¥è¯¢æºã€æ„å»ºç¯å¢ƒæ¨¡æ¿ã€ä»»åŠ¡å®ä¾‹åŒ–ã€æ‰¹é‡æ‰§è¡Œå’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è´¨é‡è¿‡æ»¤ã€‚å‘å¸ƒçš„æ•°æ®é›†åŒ…å«è¶…è¿‡120ä¸‡ä¸ªåœ¨æµè¡Œçš„WindowsåŠå…¬è½¯ä»¶ä¸­çš„æ‰§è¡ŒåŠ¨ä½œæ­¥éª¤å’Œæ•°åƒæ¡è½¨è¿¹ï¼Œå¹¶æ”¯æŒGUIå®šä½ã€å±å¹•è§£æå’ŒåŠ¨ä½œé¢„æµ‹ä¸‰ä¸ªå…¸å‹ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GUI-360Â°æ˜¯ä¸€ä¸ªé’ˆå¯¹è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰çš„å¤§å‹æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚</li>
<li>GUI-360Â°è§£å†³äº†ä¸‰ä¸ªæŒ‘æˆ˜ï¼šçœŸå®ä¸–ç•ŒCUAä»»åŠ¡çš„ç¨€ç¼ºæ€§ã€ç¼ºä¹è‡ªåŠ¨åŒ–æ”¶é›†ä¸æ³¨é‡Šç®¡é“ä»¥åŠç¼ºä¹è”åˆè¯„ä¼°å¤šä¸ªæ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¶…è¿‡120ä¸‡ä¸ªæ‰§è¡ŒåŠ¨ä½œæ­¥éª¤å’Œæ•°åƒæ¡è½¨è¿¹ï¼Œæ¶µç›–GUIå®šä½ã€å±å¹•è§£æå’ŒåŠ¨ä½œé¢„æµ‹ä¸‰ä¸ªä»»åŠ¡ã€‚</li>
<li>æ•°æ®é›†é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œæ•°æ®æ”¶é›†å’Œå¤„ç†ã€‚</li>
<li>ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨GUI-360Â°ä¸Šçš„è¡¨ç°å­˜åœ¨æ˜¾è‘—çŸ­æ¿ï¼Œç»è¿‡ç›‘ç£å¾®è°ƒåæ€§èƒ½æœ‰æ‰€æå‡ä½†ä»æœªè¾¾åˆ°äººç±»æ°´å¹³å¯é æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-633da2d5900d20056e1d0909fca741dd" align="middle">
<img src="https://picx.zhimg.com/v2-084dab512a87956f73efe9bde5bf58a8" align="middle">
<img src="https://picx.zhimg.com/v2-d59f35673334f696a6f909f07edc7ddc" align="middle">
<img src="https://picx.zhimg.com/v2-ee40b9d9883af677d748fcc47d3f0a25" align="middle">
<img src="https://picx.zhimg.com/v2-6694e675cf6103206b0abf886ef5f9cf" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SSPO-Subsentence-level-Policy-Optimization"><a href="#SSPO-Subsentence-level-Policy-Optimization" class="headerlink" title="SSPO: Subsentence-level Policy Optimization"></a>SSPO: Subsentence-level Policy Optimization</h2><p><strong>Authors:Kun Yang, Zikang chen, Yanmeng Wang, Zhigen Li</strong></p>
<p>As a significant part of post-training of the Large Language Models (LLMs), Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMsâ€™ reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative Policy Optimization) and GSPO (Group Sequence Policy Optimization), are observed to suffer from unstable policy updates and low usage of sampling data, respectively. The importance ratio of GRPO is calculated at the token level, which focuses more on optimizing a single token. This will be easily affected by outliers, leading to model training collapse. GSPO proposed the calculation of the response level importance ratio, which solves the problem of high variance and training noise accumulation in the calculation of the GRPO importance ratio. However, since all the response tokens share a common importance ratio, extreme values can easily raise or lower the overall mean, leading to the entire response being mistakenly discarded, resulting in a decrease in the utilization of sampled data. This paper introduces SSPO, which applies sentence-level importance ratio, taking the balance between GRPO and GSPO. SSPO not only avoids training collapse and high variance, but also prevents the whole response tokens from being abandoned by the clipping mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily adjust the clipping bounds, encouraging high-entropy tokens to explore and narrow the clipping range of low-entropy tokens. In particular, SSPO achieves an average score of 46.57 across five datasets, surpassing GRPO (43.01) and GSPO (44.42), and wins state-of-the-art performance on three datasets. These results highlight SSPOâ€™s effectiveness in leveraging generated data by taking the essence of GSPO but rejecting its shortcomings. </p>
<blockquote>
<p>ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè®­ç»ƒçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æå¤§åœ°æé«˜äº†LLMçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€äº›RLVRç®—æ³•ï¼Œå¦‚GRPOï¼ˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰å’ŒGSPOï¼ˆç¾¤ä½“åºåˆ—ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œå­˜åœ¨ä¸ç¨³å®šç­–ç•¥æ›´æ–°å’Œé‡‡æ ·æ•°æ®åˆ©ç”¨ç‡ä½çš„é—®é¢˜ã€‚GRPOçš„é‡è¦æ€§æ¯”ç‡æ˜¯åœ¨ä»¤ç‰Œçº§åˆ«è®¡ç®—çš„ï¼Œæ›´ä¾§é‡äºä¼˜åŒ–å•ä¸ªä»¤ç‰Œã€‚è¿™å¾ˆå®¹æ˜“å—åˆ°å¼‚å¸¸å€¼çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹è®­ç»ƒå´©æºƒã€‚GSPOæå‡ºäº†å“åº”çº§åˆ«é‡è¦æ€§æ¯”ç‡çš„è®¡ç®—ï¼Œè§£å†³äº†GRPOé‡è¦æ€§æ¯”ç‡è®¡ç®—ä¸­æ–¹å·®é«˜å’Œè®­ç»ƒå™ªå£°ç§¯ç´¯çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºæ‰€æœ‰å“åº”ä»¤ç‰Œå…±äº«ç›¸åŒçš„é‡è¦æ€§æ¯”ç‡ï¼Œæç«¯å€¼å¾ˆå®¹æ˜“æé«˜æˆ–é™ä½æ•´ä½“å¹³å‡å€¼ï¼Œå¯¼è‡´æ•´ä¸ªå“åº”è¢«é”™è¯¯åœ°ä¸¢å¼ƒï¼Œå¯¼è‡´é‡‡æ ·æ•°æ®åˆ©ç”¨ç‡é™ä½ã€‚æœ¬æ–‡ä»‹ç»äº†SSPOï¼Œå®ƒåº”ç”¨å¥å­çº§åˆ«çš„é‡è¦æ€§æ¯”ç‡ï¼Œåœ¨GRPOå’ŒGSPOä¹‹é—´å–å¾—å¹³è¡¡ã€‚SSPOä¸ä»…é¿å…äº†è®­ç»ƒå´©æºƒå’Œ highæ–¹å·®ï¼Œè¿˜é˜²æ­¢äº†æ•´ä¸ªå“åº”ä»¤ç‰Œè¢«è£å‰ªæœºåˆ¶ä¸¢å¼ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å¥å­ç†µåº”ç”¨äºPPO-CLIPï¼Œä»¥ç¨³å®šè°ƒæ•´è£å‰ªè¾¹ç•Œï¼Œé¼“åŠ±é«˜ç†µä»¤ç‰Œè¿›è¡Œæ¢ç´¢å¹¶ç¼©å°ä½ç†µä»¤ç‰Œçš„è£å‰ªèŒƒå›´ã€‚ç‰¹åˆ«æ˜¯ï¼ŒSSPOåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡å¾—åˆ†ä¸º46.57ï¼Œè¶…è¿‡äº†GRPOï¼ˆ43.01ï¼‰å’ŒGSPOï¼ˆ44.42ï¼‰ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è¿™äº›ç»“æœçªæ˜¾äº†SSPOåœ¨åˆ©ç”¨ç”Ÿæˆæ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå®ƒå–GSPOçš„ç²¾åè€Œèˆå…¶çŸ­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04256v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å¯¹äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æœ‰å¾ˆå¤§å¸®åŠ©ã€‚ç„¶è€Œï¼ŒGRPOå’ŒGSPOç­‰RLVRç®—æ³•å­˜åœ¨ä¸ç¨³å®šç­–ç•¥æ›´æ–°å’Œæ•°æ®é‡‡æ ·åˆ©ç”¨ç‡ä½çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºçš„SSPOç®—æ³•åœ¨GRPOå’ŒGSPOä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œé€šè¿‡åº”ç”¨å¥å­çº§åˆ«çš„é‡è¦æ€§æ¯”ç‡ï¼Œé¿å…äº†è®­ç»ƒå´©æºƒã€é«˜æ–¹å·®ç­‰é—®é¢˜ï¼Œå¹¶é˜²æ­¢æ•´ä¸ªå“åº”ä»¤ç‰Œè¢«ä¸¢å¼ƒã€‚æ­¤å¤–ï¼ŒSSPOè¿˜é‡‡ç”¨å¥å­ç†µæ¥ç¨³å®šè°ƒæ•´è£å‰ªè¾¹ç•Œï¼Œé¼“åŠ±é«˜ç†µä»¤ç‰Œè¿›è¡Œæ¢ç´¢å¹¶ç¼©å°ä½ç†µä»¤ç‰Œçš„è£å‰ªèŒƒå›´ã€‚SSPOåœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡å¾—åˆ†ä¸º46.57ï¼Œè¶…è¿‡äº†GRPOï¼ˆ43.01ï¼‰å’ŒGSPOï¼ˆ44.42ï¼‰ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RLVRåœ¨LLMçš„æ¨ç†èƒ½åŠ›æå‡ä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
<li>GRPOå’ŒGSPOåœ¨ç­–ç•¥æ›´æ–°å’Œæ•°æ®é‡‡æ ·åˆ©ç”¨ç‡æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚</li>
<li>SSPOç®—æ³•åœ¨GRPOå’ŒGSPOä¹‹é—´æ‰¾åˆ°äº†å¹³è¡¡ï¼Œé€šè¿‡å¥å­çº§åˆ«çš„é‡è¦æ€§æ¯”ç‡é¿å…äº†è®­ç»ƒå´©æºƒå’Œé«˜æ–¹å·®ç­‰é—®é¢˜ã€‚</li>
<li>SSPOé‡‡ç”¨äº†å¥å­ç†µæ¥ç¨³å®šè°ƒæ•´è£å‰ªè¾¹ç•Œï¼Œä»¥æé«˜æ•°æ®åˆ©ç”¨ç‡ã€‚</li>
<li>SSPOåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†GRPOå’ŒGSPOï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>SSPOèƒ½å¤Ÿåˆ©ç”¨ç”Ÿæˆæ•°æ®ï¼Œå¹¶æ‘’å¼ƒäº†GSPOçš„ç¼ºç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f88802d4507b7dcec56838d5dc41cce0" align="middle">
<img src="https://picx.zhimg.com/v2-b83946952af9eb0cf1c09b6b11a93072" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Plan-of-Knowledge-Retrieval-Augmented-Large-Language-Models-for-Temporal-Knowledge-Graph-Question-Answering"><a href="#Plan-of-Knowledge-Retrieval-Augmented-Large-Language-Models-for-Temporal-Knowledge-Graph-Question-Answering" class="headerlink" title="Plan of Knowledge: Retrieval-Augmented Large Language Models for   Temporal Knowledge Graph Question Answering"></a>Plan of Knowledge: Retrieval-Augmented Large Language Models for   Temporal Knowledge Graph Question Answering</h2><p><strong>Authors:Xinying Qian, Ying Zhang, Yu Zhao, Baohang Zhou, Xuhui Sui, Xiaojie Yuan</strong></p>
<p>Temporal Knowledge Graph Question Answering (TKGQA) aims to answer time-sensitive questions by leveraging factual information from Temporal Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG embeddings or graph neural networks to inject temporal knowledge, they fail to fully understand the complex semantic information of time constraints. Recently, Large Language Models (LLMs) have shown remarkable progress, benefiting from their strong semantic understanding and reasoning generalization capabilities. However, their temporal reasoning ability remains limited. LLMs frequently suffer from hallucination and a lack of knowledge. To address these limitations, we propose the Plan of Knowledge framework with a contrastive temporal retriever, which is named PoK. Specifically, the proposed Plan of Knowledge module decomposes a complex temporal question into a sequence of sub-objectives from the pre-defined tools, serving as intermediate guidance for reasoning exploration. In parallel, we construct a Temporal Knowledge Store (TKS) with a contrastive retrieval framework, enabling the model to selectively retrieve semantically and temporally aligned facts from TKGs. By combining structured planning with temporal knowledge retrieval, PoK effectively enhances the interpretability and factual consistency of temporal reasoning. Extensive experiments on four benchmark TKGQA datasets demonstrate that PoK significantly improves the retrieval precision and reasoning accuracy of LLMs, surpassing the performance of the state-of-the-art TKGQA methods by 56.0% at most. </p>
<blockquote>
<p>æ—¶åºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆTKGQAï¼‰æ—¨åœ¨åˆ©ç”¨æ—¶åºçŸ¥è¯†å›¾è°±ï¼ˆTKGï¼‰ä¸­çš„äº‹å®ä¿¡æ¯æ¥å›ç­”æ—¶é—´æ•æ„Ÿçš„é—®é¢˜ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»é‡‡ç”¨é¢„è®­ç»ƒçš„æ—¶åºçŸ¥è¯†å›¾è°±åµŒå…¥æˆ–å›¾ç¥ç»ç½‘ç»œæ¥æ³¨å…¥æ—¶åºçŸ¥è¯†ï¼Œä½†å®ƒä»¬æœªèƒ½å®Œå…¨ç†è§£æ—¶é—´çº¦æŸçš„å¤æ‚è¯­ä¹‰ä¿¡æ¯ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶å¼ºå¤§çš„è¯­ä¹‰ç†è§£å’Œæ¨ç†æ³›åŒ–èƒ½åŠ›ï¼Œå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ—¶åºæ¨ç†èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ç»å¸¸å­˜åœ¨å¹»è§‰å’Œç¼ºä¹çŸ¥è¯†çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰å¯¹æ¯”æ—¶åºæ£€ç´¢å™¨çš„çŸ¥è¯†è®¡åˆ’æ¡†æ¶ï¼Œåä¸ºPoKã€‚å…·ä½“æ¥è¯´ï¼Œæ‰€æå‡ºçš„çŸ¥è¯†è®¡åˆ’æ¨¡å—å°†å¤æ‚çš„æ—¶åºé—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¥è‡ªé¢„å®šä¹‰å·¥å…·çš„å­ç›®æ ‡ï¼Œä½œä¸ºæ¨ç†æ¢ç´¢çš„ä¸­é—´æŒ‡å¯¼ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯¹æ¯”æ£€ç´¢æ¡†æ¶ä¸‹çš„æ—¶åºçŸ¥è¯†å­˜å‚¨ï¼ˆTKSï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»æ—¶åºçŸ¥è¯†å›¾è°±ä¸­é€‰æ‹©æ€§åœ°æ£€ç´¢è¯­ä¹‰å’Œæ—¶åºå¯¹é½çš„äº‹å®ã€‚é€šè¿‡ç»“åˆç»“æ„åŒ–è§„åˆ’ä¸æ—¶åºçŸ¥è¯†æ£€ç´¢ï¼ŒPoKæœ‰æ•ˆåœ°æé«˜äº†æ—¶åºæ¨ç†çš„å¯è§£é‡Šæ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚åœ¨å››ä¸ªåŸºå‡†TKGQAæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPoKæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ£€ç´¢ç²¾åº¦å’Œæ¨ç†å‡†ç¡®æ€§ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›TKGQAæ–¹æ³•çš„æ€§èƒ½ï¼Œæœ€å¤šæé«˜äº†56.0%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04072v1">PDF</a> Submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Temporal Knowledge Graph Question Answeringï¼ˆTKGQAï¼‰çš„ç›®æ ‡æ˜¯åˆ©ç”¨æ—¶åºçŸ¥è¯†å›¾è°±ï¼ˆTKGsï¼‰ä¸­çš„äº‹å®ä¿¡æ¯æ¥å›ç­”æ—¶é—´æ•æ„Ÿçš„é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡å·²æœ‰ç ”ç©¶é‡‡ç”¨é¢„è®­ç»ƒTKGåµŒå…¥å’Œå›¾ç¥ç»ç½‘ç»œæ¥æ³¨å…¥æ—¶åºçŸ¥è¯†ï¼Œä½†å®ƒä»¬æœªèƒ½å®Œå…¨ç†è§£æ—¶é—´çº¦æŸçš„å¤æ‚è¯­ä¹‰ä¿¡æ¯ã€‚æ–‡ç« æå‡ºä¸€ä¸ªåä¸ºPoKçš„çŸ¥è¯†è®¡åˆ’æ¡†æ¶ï¼Œç»“åˆå¯¹æ¯”æ—¶åºæ£€ç´¢å™¨ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¶åºæ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚PoKé€šè¿‡å°†å¤æ‚çš„æ—¶åºé—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—å­ç›®æ ‡ï¼Œå¹¶æä¾›é¢„å®šä¹‰å·¥å…·çš„ä¸­é—´æŒ‡å¯¼æ¥è¿›è¡Œæ¨ç†æ¢ç´¢ã€‚åŒæ—¶ï¼Œæ„å»ºä¸€ä¸ªå¯¹æ¯”çŸ¥è¯†å­˜å‚¨åº“ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‰æ‹©æ€§åœ°ä»TKGsä¸­æ£€ç´¢è¯­ä¹‰å’Œæ—¶åºå¯¹é½çš„äº‹å®ã€‚é€šè¿‡ç»“åˆç»“æ„åŒ–è§„åˆ’ä¸æ—¶åºçŸ¥è¯†æ£€ç´¢ï¼ŒPoKæœ‰æ•ˆæé«˜äº†æ—¶åºæ¨ç†çš„å¯è§£é‡Šæ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒPoKæ˜¾è‘—æé«˜äº†LLMsçš„æ£€ç´¢ç²¾åº¦å’Œæ¨ç†å‡†ç¡®æ€§ï¼Œåœ¨å››ä¸ªåŸºå‡†TKGQAæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†æœ€å…ˆè¿›çš„TKGQAæ–¹æ³•ï¼Œæœ€å¤šæé«˜äº†56.0%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TKGQAæ—¨åœ¨åˆ©ç”¨TKGsä¸­çš„äº‹å®ä¿¡æ¯å›ç­”æ—¶é—´æ•æ„Ÿé—®é¢˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶æœªèƒ½å……åˆ†ç†è§£æ—¶é—´çº¦æŸçš„å¤æ‚è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>PoKæ¡†æ¶é€šè¿‡åˆ†è§£å¤æ‚æ—¶åºé—®é¢˜ä¸ºä¸€ç³»åˆ—å­ç›®æ ‡å¹¶æä¾›ä¸­é—´æŒ‡å¯¼è¿›è¡Œæ¨ç†æ¢ç´¢ã€‚</li>
<li>PoKç»“åˆç»“æ„åŒ–è§„åˆ’ä¸å¯¹æ¯”çŸ¥è¯†å­˜å‚¨åº“è¿›è¡Œæ—¶åºçŸ¥è¯†æ£€ç´¢ã€‚</li>
<li>PoKæé«˜äº†LLMsåœ¨æ—¶åºæ¨ç†æ–¹é¢çš„å¯è§£é‡Šæ€§å’Œäº‹å®ä¸€è‡´æ€§ã€‚</li>
<li>PoKåœ¨å››ä¸ªåŸºå‡†TKGQAæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ff785809d607df4007c6f386fde48a9" align="middle">
<img src="https://picx.zhimg.com/v2-1782c52d8d94795116b240f8dcb98323" align="middle">
<img src="https://picx.zhimg.com/v2-6c40b0f521a3535b2928e47a36f1c768" align="middle">
<img src="https://picx.zhimg.com/v2-adf34c4163a2db93f12d07bc4eaeb5e0" align="middle">
<img src="https://picx.zhimg.com/v2-80b064ca1cb104df9b068185cc8dcb60" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Benchmarking-and-Studying-the-LLM-based-Agent-System-in-End-to-End-Software-Development"><a href="#Benchmarking-and-Studying-the-LLM-based-Agent-System-in-End-to-End-Software-Development" class="headerlink" title="Benchmarking and Studying the LLM-based Agent System in End-to-End   Software Development"></a>Benchmarking and Studying the LLM-based Agent System in End-to-End   Software Development</h2><p><strong>Authors:Zhengran Zeng, Yixin Li, Rui Xie, Wei Ye, Shikun Zhang</strong></p>
<p>The development of LLM-based autonomous agents for end-to-end software development represents a significant paradigm shift in software engineering. However, the scientific evaluation of these systems is hampered by significant challenges, including overly simplistic benchmarks and the difficulty of conducting fair comparisons between different agent architectures due to confounding implementation variables. To address these limitations, we first construct a challenging and dynamically curated E2EDevBench to simulate realistic development scenarios. Second, we propose a hybrid evaluation framework that combines test-case-based functional assessment with fine-grained, LLM-based requirement verification. Using this framework, we conduct a controlled empirical study on three representative agent architectures implemented upon a unified foundation to isolate the impact of workflow design. Our findings reveal that state-of-the-art agents can fulfill approximately 50% of requirements on \bench{}, but their success is critically dependent on the architectural strategy for task decomposition and collaboration. Furthermore, our analysis indicates that the primary bottleneck is the omission of requirements and inadequate self-verification. This work provides the community with a more realistic benchmark, a comprehensive evaluation framework, and crucial insights into the current capabilities and core challenges of software development agents, guiding future research toward enhancing requirement comprehension and planning. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªä¸»ä»£ç†è½¯ä»¶åœ¨ç«¯åˆ°ç«¯è½¯ä»¶å¼€å‘ä¸­çš„åº”ç”¨ï¼Œä»£è¡¨äº†è½¯ä»¶å·¥ç¨‹ä¸­çš„é‡å¤§èŒƒå¼è½¬å˜ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿçš„ç§‘å­¦è¯„ä¼°é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡äºç®€å•çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠç”±äºæ··æ·†çš„å®æ–½å˜é‡å¯¼è‡´çš„ä¸åŒä»£ç†æ¶æ„ä¹‹é—´çš„å…¬å¹³æ¯”è¾ƒå›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’ŒåŠ¨æ€ç­–åˆ’çš„E2EDevBenchï¼Œä»¥æ¨¡æ‹Ÿç°å®å¼€å‘åœºæ™¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºäºæµ‹è¯•ç”¨ä¾‹çš„åŠŸèƒ½è¯„ä¼°ä¸åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç²¾ç»†è¦æ±‚éªŒè¯ã€‚ä½¿ç”¨è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹ä¸‰ç§åŸºäºç»Ÿä¸€åŸºç¡€çš„ä»£è¡¨æ€§ä»£ç†æ¶æ„è¿›è¡Œäº†æ§åˆ¶å®è¯ç ”ç©¶ï¼Œä»¥éš”ç¦»å·¥ä½œæµç¨‹è®¾è®¡çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæœ€å…ˆè¿›çš„ä»£ç†å¯ä»¥åœ¨Benchä¸Šå®Œæˆå¤§çº¦50%çš„è¦æ±‚ï¼Œä½†å®ƒä»¬çš„æˆåŠŸä¸¥é‡ä¾èµ–äºä»»åŠ¡åˆ†è§£å’Œåä½œçš„æ¶æ„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸»è¦ç“¶é¢ˆåœ¨äºé—æ¼è¦æ±‚å’Œè‡ªæˆ‘éªŒè¯ä¸è¶³ã€‚è¿™é¡¹å·¥ä½œä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªæ›´ç°å®çš„åŸºå‡†æµ‹è¯•ã€ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥åŠå¯¹å½“å‰è½¯ä»¶å¼€å‘ä»£ç†çš„èƒ½åŠ›å’Œæ ¸å¿ƒæŒ‘æˆ˜çš„å…³é”®è§è§£ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æé«˜éœ€æ±‚ç†è§£å’Œè§„åˆ’çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04064v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åŸºäºå¤§æ¨¡å‹çš„è‡ªä¸»ä»£ç†è½¯ä»¶åœ¨ç«¯åˆ°ç«¯è½¯ä»¶å¼€å‘ä¸­çš„å‘å±•åŠå…¶è¯„ä¼°é—®é¢˜ã€‚ä¸ºè§£å†³å½“å‰è¯„ä¼°æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è¿‡äºç®€å•çš„åŸºå‡†æµ‹è¯•å’Œéš¾ä»¥åœ¨ä¸åŒä»£ç†æ¶æ„ä¹‹é—´è¿›è¡Œå…¬å¹³æ¯”è¾ƒï¼Œç ”ç©¶å›¢é˜Ÿå»ºç«‹äº†åŠ¨æ€æ¨¡æ‹ŸçœŸå®å¼€å‘åœºæ™¯çš„E2EDevBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œå¹¶æå‡ºäº†ç»“åˆæµ‹è¯•ç”¨ä¾‹åŠŸèƒ½è¯„ä¼°ä¸ç²¾ç»†çš„å¤§æ¨¡å‹éœ€æ±‚éªŒè¯çš„æ··åˆè¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡å¯¹ä¸‰ç§ä»£è¡¨æ€§ä»£ç†æ¶æ„çš„å®è¯ç ”ç©¶å‘ç°ï¼Œå½“å‰å…ˆè¿›ä»£ç†åªèƒ½æ»¡è¶³çº¦ä¸€åŠçš„éœ€æ±‚ï¼Œå…¶æˆåŠŸå–å†³äºä»»åŠ¡åˆ†è§£å’Œåä½œçš„æ¶æ„ç­–ç•¥ã€‚ç“¶é¢ˆåœ¨äºè¦æ±‚é—æ¼å’Œç¼ºä¹è‡ªæˆ‘éªŒè¯ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªæ›´ç°å®çš„åŸºå‡†æµ‹è¯•ã€å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥åŠå¯¹è½¯ä»¶å¼€å‘ä»£ç†å½“å‰èƒ½åŠ›å’Œæ ¸å¿ƒæŒ‘æˆ˜çš„é‡è¦è§è§£ï¼Œä¸ºæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå¤§æ¨¡å‹çš„è‡ªä¸»ä»£ç†è½¯ä»¶åœ¨ç«¯åˆ°ç«¯è½¯ä»¶å¼€å‘ä¸­ä»£è¡¨äº†æ˜¾è‘—çš„èŒƒå¼è½¬å˜ã€‚</li>
<li>å½“å‰è¯„ä¼°è¿™äº›ç³»ç»Ÿæ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡äºç®€å•çš„åŸºå‡†æµ‹è¯•å’Œéš¾ä»¥æ¯”è¾ƒä¸åŒçš„ä»£ç†æ¶æ„ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå»ºç«‹äº†E2EDevBenchåŸºå‡†æµ‹è¯•å¹³å°æ¥æ¨¡æ‹ŸçœŸå®å¼€å‘åœºæ™¯ã€‚</li>
<li>æå‡ºäº†ç»“åˆæµ‹è¯•ç”¨ä¾‹åŠŸèƒ½è¯„ä¼°ä¸ç²¾ç»†çš„å¤§æ¨¡å‹éœ€æ±‚éªŒè¯çš„æ··åˆè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰å…ˆè¿›ä»£ç†åªèƒ½æ»¡è¶³çº¦ä¸€åŠçš„éœ€æ±‚ã€‚</li>
<li>ä»£ç†çš„æˆåŠŸå–å†³äºä»»åŠ¡åˆ†è§£å’Œåä½œçš„æ¶æ„ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-698de4602ebcd337798fc15cb796f36e" align="middle">
<img src="https://picx.zhimg.com/v2-f6ba8f5b4571d8901615f6628b774fc5" align="middle">
<img src="https://picx.zhimg.com/v2-5b57f74379d14013c256fbf02bbfd2c4" align="middle">
<img src="https://picx.zhimg.com/v2-7b19fcfa3f325e2f4a6a76a3ad5fa5d0" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="To-See-or-To-Read-User-Behavior-Reasoning-in-Multimodal-LLMs"><a href="#To-See-or-To-Read-User-Behavior-Reasoning-in-Multimodal-LLMs" class="headerlink" title="To See or To Read: User Behavior Reasoning in Multimodal LLMs"></a>To See or To Read: User Behavior Reasoning in Multimodal LLMs</h2><p><strong>Authors:Tianning Dong, Luyi Ma, Varun Vasudevan, Jason Cho, Sushant Kumar, Kannan Achan</strong></p>
<p>Multimodal Large Language Models (MLLMs) are reshaping how modern agentic systems reason over sequential user-behavior data. However, whether textual or image representations of user behavior data are more effective for maximizing MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a systematic benchmarking framework for assessing modality trade-offs in user-behavior reasoning across six MLLMs by representing transaction data as (1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a real-world purchase-sequence dataset, we find that when data is represented as images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared with an equivalent textual representation without any additional computational cost. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åœ¨æ”¹å˜ç°ä»£ä»£ç†ç³»ç»Ÿå¯¹åºåˆ—ç”¨æˆ·è¡Œä¸ºæ•°æ®çš„æ¨ç†æ–¹å¼ã€‚ç„¶è€Œï¼Œæ— è®ºæ˜¯æ–‡æœ¬è¿˜æ˜¯å›¾åƒè¡¨ç¤ºçš„ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œå“ªç§è¡¨ç¤ºæ–¹å¼æ›´èƒ½æœ‰æ•ˆåœ°æœ€å¤§åŒ–MLLMçš„æ€§èƒ½ï¼Œè¿™ä¸€ç‚¹å°šæœªå¾—åˆ°å……åˆ†çš„æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†<code>BehaviorLens</code>ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°åœ¨å…­ä¸ªMLLMä¸­è¿›è¡Œç”¨æˆ·è¡Œä¸ºæ¨ç†æ—¶çš„æ¨¡æ€æƒè¡¡ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ç§æ–¹å¼è¡¨ç¤ºäº¤æ˜“æ•°æ®ï¼šï¼ˆ1ï¼‰ä¸€æ®µæ–‡æœ¬ï¼Œï¼ˆ2ï¼‰æ•£ç‚¹å›¾ï¼Œï¼ˆ3ï¼‰æµç¨‹å›¾ã€‚é€šè¿‡ä½¿ç”¨ç°å®ä¸–ç•Œçš„è´­ä¹°åºåˆ—æ•°æ®é›†ï¼Œæˆ‘ä»¬å‘ç°å½“æ•°æ®ä»¥å›¾åƒå½¢å¼è¡¨ç¤ºæ—¶ï¼Œä¸ç­‰æ•ˆçš„æ–‡æœ¬è¡¨ç¤ºç›¸æ¯”ï¼ŒMLLMçš„ä¸‹æ¬¡è´­ä¹°é¢„æµ‹ç²¾åº¦æé«˜äº†87.5%ï¼Œä¸”æ²¡æœ‰ä»»ä½•é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03845v1">PDF</a> Accepted by the 39th Conference on Neural Information Processing   Systems (NeurIPS 2025) Workshop: Efficient Reasoning</p>
<p><strong>Summary</strong>ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åœ¨æ”¹å˜ç°ä»£ä»£ç†ç³»ç»Ÿå¯¹ç”¨æˆ·è¡Œä¸ºæ•°æ®çš„åºåˆ—åŒ–æ¨ç†æ–¹å¼ã€‚ç„¶è€Œï¼Œå…³äºç”¨æˆ·è¡Œä¸ºæ•°æ®çš„æ–‡æœ¬è¡¨ç¤ºå’Œå›¾åƒè¡¨ç¤ºåœ¨æœ€å¤§åŒ–MLLMæ€§èƒ½æ–¹é¢å“ªä¸ªæ›´æœ‰æ•ˆå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸º<code>BehaviorLens</code>çš„ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡å¯¹äº¤æ˜“æ•°æ®ä»¥æ–‡æœ¬æ®µè½ã€æ•£ç‚¹å›¾å’Œæµç¨‹å›¾ä¸‰ç§æ–¹å¼è¿›è¡Œå‘ˆç°ï¼Œè¯„ä¼°ç”¨æˆ·è¡Œä¸ºæ¨ç†ä¸­çš„æ¨¡æ€æƒè¡¡é—®é¢˜ã€‚ä½¿ç”¨çœŸå®è´­ä¹°åºåˆ—æ•°æ®é›†å‘ç°ï¼Œå°†æ•°æ®ä»¥å›¾åƒå½¢å¼å‘ˆç°ï¼ŒMLLMsçš„ä¸‹æ¬¡è´­ä¹°é¢„æµ‹å‡†ç¡®åº¦æé«˜äº†87.5%ï¼Œä¸”æ— éœ€å¢åŠ è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åœ¨æ”¹å˜å¯¹ç”¨æˆ·è¡Œä¸ºæ•°æ®çš„æ¨ç†æ–¹å¼ã€‚</li>
<li><code>BehaviorLens</code>æ¡†æ¶ç”¨äºè¯„ä¼°ä¸åŒå‘ˆç°æ–¹å¼ï¼ˆæ–‡æœ¬ã€å›¾åƒï¼‰å¯¹MLLMæ€§èƒ½çš„å½±å“ã€‚</li>
<li>åœ¨ç”¨æˆ·è¡Œä¸ºæ•°æ®ä¸­ï¼Œå›¾åƒè¡¨ç¤ºç›¸æ¯”æ–‡æœ¬è¡¨ç¤ºèƒ½æ˜¾è‘—æé«˜MLLMçš„é¢„æµ‹å‡†ç¡®åº¦ã€‚</li>
<li>é‡‡ç”¨å›¾åƒè¡¨ç¤ºç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œé¢„æµ‹å‡†ç¡®åº¦æé«˜87.5%ä¸”æ— éœ€å¢åŠ è®¡ç®—æˆæœ¬ã€‚</li>
<li>çœŸå®è´­ä¹°åºåˆ—æ•°æ®é›†åœ¨ç ”ç©¶ä¸­èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚</li>
<li>æ•£ç‚¹å›¾å’Œæµç¨‹å›¾ä½œä¸ºæ•°æ®å‘ˆç°æ–¹å¼ä¹Ÿè¢«ç ”ç©¶å¹¶è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03845">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08dd65be48ba34e00b71d54a25ad2929" align="middle">
<img src="https://picx.zhimg.com/v2-6bc381d0715fbbd12abf4564c2ea6b4c" align="middle">
<img src="https://picx.zhimg.com/v2-32d6b998b8b229d4938b560f429dfc8f" align="middle">
<img src="https://picx.zhimg.com/v2-1a7ce7b9a58e68790999cca7800cc65d" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Scaling-Agent-Learning-via-Experience-Synthesis"><a href="#Scaling-Agent-Learning-via-Experience-Synthesis" class="headerlink" title="Scaling Agent Learning via Experience Synthesis"></a>Scaling Agent Learning via Experience Synthesis</h2><p><strong>Authors:Zhaorun Chen, Zhuokai Zhao, Kai Zhang, Bo Liu, Qi Qi, Yifan Wu, Tarun Kalluri, Sara Cao, Yuanhao Xiong, Haibo Tong, Huaxiu Yao, Hengduo Li, Jiacheng Zhu, Xian Li, Dawn Song, Bo Li, Jason Weston, Dat Huynh</strong></p>
<p>While reinforcement learning (RL) can empower large language model (LLM) agents by enabling self-improvement through interaction, its practical adoption remains challenging due to costly rollouts, limited task diversity, unreliable reward signals, and infrastructure complexity, all of which obstruct the collection of scalable experience data. To address these challenges, we introduce DreamGym, the first unified framework designed to synthesize diverse experiences with scalability in mind to enable effective online RL training for autonomous agents. Rather than relying on expensive real-environment rollouts, DreamGym distills environment dynamics into a reasoning-based experience model that derives consistent state transitions and feedback signals through step-by-step reasoning, enabling scalable agent rollout collection for RL. To improve the stability and quality of transitions, DreamGym leverages an experience replay buffer initialized with offline real-world data and continuously enriched with fresh interactions to actively support agent training. To improve knowledge acquisition, DreamGym adaptively generates new tasks that challenge the current agent policy, enabling more effective online curriculum learning. Experiments across diverse environments and agent backbones demonstrate that DreamGym substantially improves RL training, both in fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in RL-ready but costly settings, it matches GRPO and PPO performance using only synthetic interactions. When transferring a policy trained purely on synthetic experiences to real-environment RL, DreamGym yields significant additional performance gains while requiring far fewer real-world interactions, providing a scalable warm-start strategy for general-purpose RL. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥é€šè¿‡äº¤äº’ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°è‡ªæˆ‘æå‡ï¼Œä»è€Œå¢å¼ºå…¶èƒ½åŠ›ï¼Œä½†å…¶åœ¨å®è·µä¸­çš„é‡‡çº³ä»é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æˆæœ¬é«˜æ˜‚çš„æ»šåŠ¨æ“ä½œã€ä»»åŠ¡å¤šæ ·æ€§æœ‰é™ã€å¥–åŠ±ä¿¡å·ä¸å¯é ä»¥åŠåŸºç¡€è®¾æ–½å¤æ‚æ€§ç­‰ï¼Œè¿™äº›éƒ½é˜»ç¢äº†å¯æ‰©å±•ç»éªŒæ•°æ®çš„æ”¶é›†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DreamGymï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆæˆå¤šæ ·åŒ–çš„ç»éªŒå¹¶è€ƒè™‘å¯æ‰©å±•æ€§ï¼Œä»¥å®ç°å¯¹è‡ªä¸»ä»£ç†è¿›è¡Œæœ‰æ•ˆåœ¨çº¿RLè®­ç»ƒã€‚DreamGymä¸ä¾èµ–äºæ˜‚è´µçš„çœŸå®ç¯å¢ƒæ»šåŠ¨æ“ä½œï¼Œè€Œæ˜¯å°†ç¯å¢ƒåŠ¨æ€æ€§æç‚¼æˆåŸºäºæ¨ç†çš„ç»éªŒæ¨¡å‹ï¼Œé€šè¿‡é€æ­¥æ¨ç†æ¥å¾—å‡ºä¸€è‡´çš„çŠ¶æ€è½¬æ¢å’Œåé¦ˆä¿¡å·ï¼Œä»è€Œå®ç°å¯æ‰©å±•çš„ä»£ç†æ»šåŠ¨æ”¶é›†ç”¨äºRLã€‚ä¸ºäº†æé«˜è½¬æ¢çš„ç¨³å®šæ€§å’Œè´¨é‡ï¼ŒDreamGymåˆ©ç”¨åˆå§‹åŒ–ä¸ºç¦»çº¿çœŸå®ä¸–ç•Œæ•°æ®çš„ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œå¹¶ä¸æ–­ä¸°å¯Œæ–°é²œäº¤äº’ä»¥ç§¯ææ”¯æŒä»£ç†è®­ç»ƒã€‚ä¸ºäº†æé«˜çŸ¥è¯†è·å–èƒ½åŠ›ï¼ŒDreamGymè‡ªé€‚åº”ç”Ÿæˆæ–°ä»»åŠ¡ä»¥æŒ‘æˆ˜å½“å‰ä»£ç†ç­–ç•¥ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„åœ¨çº¿è¯¾ç¨‹å­¦ä¹ ã€‚åœ¨å¤šç§ç¯å¢ƒå’Œä»£ç†éª¨æ¶ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDreamGymåœ¨å®Œå…¨åˆæˆè®¾ç½®å’Œæ¨¡æ‹Ÿåˆ°çœŸå®è½¬ç§»åœºæ™¯ä¸­å‡æ˜¾è‘—æ”¹è¿›äº†RLè®­ç»ƒã€‚åœ¨éRLå‡†å¤‡ä»»åŠ¡ï¼ˆå¦‚WebArenaï¼‰ä¸Šï¼ŒDreamGymçš„åŸºçº¿æ€§èƒ½ä¼˜äºæ‰€æœ‰åŸºçº¿30%ä»¥ä¸Šã€‚åœ¨RLå°±ç»ªä½†æˆæœ¬é«˜æ˜‚çš„ç¯å¢ƒä¸­ï¼Œå®ƒä»…ä½¿ç”¨åˆæˆäº¤äº’å³å¯ä¸GRPOå’ŒPPOæ€§èƒ½ç›¸åŒ¹é…ã€‚å½“å°†ä»…åŸºäºåˆæˆç»éªŒè®­ç»ƒçš„ç­–ç•¥è½¬ç§»åˆ°çœŸå®ç¯å¢ƒçš„RLæ—¶ï¼ŒDreamGymåœ¨éœ€è¦æ›´å°‘çš„çœŸå®ä¸–ç•Œäº¤äº’çš„æƒ…å†µä¸‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºé€šç”¨RLæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„é¢„çƒ­å¯åŠ¨ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03773v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡äº¤äº’èµ‹èƒ½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†äººè‡ªæˆ‘æå‡ï¼Œä½†åœ¨å®è·µä¸­é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒDreamGymç»Ÿä¸€æ¡†æ¶è¢«å¼•å…¥ç”¨äºåˆæˆå¤šæ ·åŒ–çš„ç»éªŒå¹¶å®ç°å¯æ‰©å±•æ€§ï¼Œä»¥å®ç°å¯¹è‡ªä¸»ä»£ç†çš„æœ‰æ•ˆåœ¨çº¿RLè®­ç»ƒã€‚DreamGymé€šè¿‡åŸºäºæ¨ç†çš„ç»éªŒæ¨¡å‹å°†ç¯å¢ƒåŠ¨æ€ç®€åŒ–ä¸ºæ­¥éª¤å¼æ¨ç†å’Œåé¦ˆä¿¡å·ï¼Œæ— éœ€ä¾èµ–æ˜‚è´µçš„å®é™…ç¯å¢ƒæ¼”ç»ƒï¼Œä»è€Œå®ç°ä»£ç†çš„æ‰©å±•æ€§æ»šåŠ¨æ”¶é›†ã€‚é€šè¿‡åˆ©ç”¨åˆå§‹åŒ–çš„ç¦»çº¿ç°å®ä¸–ç•Œæ•°æ®å’ŒæŒç»­ä¸°å¯Œçš„äº¤äº’æ¥æ”¯æŒä»£ç†è®­ç»ƒï¼ŒDreamGymæ”¹å–„äº†è¿‡æ¸¡çš„ç¨³å®šæ€§å’Œè´¨é‡ã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡è‡ªé€‚åº”ç”Ÿæˆæ–°ä»»åŠ¡ä¿ƒè¿›çŸ¥è¯†è·å–ï¼ŒæŒ‘æˆ˜å½“å‰ä»£ç†ç­–ç•¥ä»¥å®ç°æ›´æœ‰æ•ˆçš„åœ¨çº¿è¯¾ç¨‹å­¦ä¹ ã€‚åœ¨è·¨ç¯å¢ƒå’Œä¸åŒä»£ç†ä¸»ä½“çš„å®éªŒä¸­è¯æ˜ï¼ŒDreamGymå¤§å¹…æé«˜äº†RLè®­ç»ƒæ•ˆæœã€‚åœ¨åˆæˆç¯å¢ƒä¸­ä»¥åŠä»¿çœŸåˆ°ç°å®è¿ç§»åœºæ™¯ä¸­ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹ï¼ŒDreamGymå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å³ä½¿åœ¨éRLå‡†å¤‡ä»»åŠ¡ä¸Šï¼Œå…¶æ€§èƒ½ä¹Ÿè¶…è¿‡äº†æ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼›åœ¨RLå°±ç»ªä½†æˆæœ¬é«˜æ˜‚çš„ç¯å¢ƒä¸­ï¼Œä»…å‡­åˆæˆäº¤äº’ä¾¿åŒ¹é…äº†GRPOå’ŒPPOçš„æ€§èƒ½ã€‚å½“å°†ä»…é€šè¿‡åˆæˆç»éªŒè®­ç»ƒçš„ç­–ç•¥è½¬ç§»åˆ°å®é™…ç¯å¢ƒRLæ—¶ï¼ŒDreamGymå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½å¢ç›Šï¼ŒåŒæ—¶å¤§å¹…å‡å°‘äº†å®é™…äº¤äº’çš„éœ€æ±‚ï¼Œä¸ºé€šç”¨RLæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„é¢„çƒ­å¯åŠ¨ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡äº¤äº’èµ‹èƒ½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†äººè‡ªæˆ‘æå‡ï¼Œä½†ä»é¢ä¸´å®è·µæŒ‘æˆ˜ã€‚</li>
<li>DreamGymæ˜¯é¦–ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨åˆæˆå¤šæ ·åŒ–çš„ç»éªŒå¹¶å®ç°å¯æ‰©å±•æ€§ï¼Œä»¥æ”¯æŒè‡ªä¸»ä»£ç†çš„æœ‰æ•ˆåœ¨çº¿RLè®­ç»ƒã€‚</li>
<li>DreamGymé€šè¿‡åŸºäºæ¨ç†çš„ç»éªŒæ¨¡å‹ç®€åŒ–ç¯å¢ƒåŠ¨æ€ï¼Œé¿å…äº†æ˜‚è´µçš„å®é™…ç¯å¢ƒæ¼”ç»ƒæˆæœ¬ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨ç¦»çº¿ç°å®ä¸–ç•Œæ•°æ®å’ŒæŒç»­ä¸°å¯Œçš„äº¤äº’æ•°æ®æ¥æ”¹å–„è¿‡æ¸¡ç¨³å®šæ€§å’Œè´¨é‡ã€‚</li>
<li>DreamGymè‡ªé€‚åº”ç”Ÿæˆæ–°ä»»åŠ¡ä»¥æŒ‘æˆ˜å½“å‰ä»£ç†ç­–ç•¥ï¼Œä»è€Œä¿ƒè¿›çŸ¥è¯†è·å–å’Œåœ¨çº¿è¯¾ç¨‹å­¦ä¹ ã€‚</li>
<li>åœ¨è·¨ç¯å¢ƒå’Œä¸åŒä¸»ä½“å®éªŒä¸­è¯æ˜ï¼ŒDreamGymæ˜¾è‘—æé«˜äº†RLè®­ç»ƒæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-423b9d223f0138e8261ee118e548d33d" align="middle">
<img src="https://picx.zhimg.com/v2-a31f7aa0bc8f50b659a2e29cb97b4c9b" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CareMedEval-dataset-Evaluating-Critical-Appraisal-and-Reasoning-in-the-Biomedical-Field"><a href="#CareMedEval-dataset-Evaluating-Critical-Appraisal-and-Reasoning-in-the-Biomedical-Field" class="headerlink" title="CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the   Biomedical Field"></a>CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the   Biomedical Field</h2><p><strong>Authors:Doria Bonzi, Alexandre Guiggi, FrÃ©dÃ©ric BÃ©chet, Carlos Ramisch, Benoit Favre</strong></p>
<p>Critical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal. </p>
<blockquote>
<p>ç§‘å­¦æ–‡çŒ®çš„æ‰¹åˆ¤æ€§è¯„ä»·æ˜¯ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬æŠ€èƒ½ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­æä¾›äº†æœ‰å‰æ™¯çš„æ”¯æŒï¼Œä½†å®ƒä»¬çš„å¯é æ€§ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸“ä¸šé¢†åŸŸçš„æ‰¹åˆ¤æ€§æ¨ç†æ–¹é¢ã€‚æˆ‘ä»¬ä»‹ç»äº†CareMedEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŸåˆ›æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨ç”Ÿç‰©åŒ»å­¦æ‰¹åˆ¤æ€§è¯„ä»·å’Œæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†æ¥æºäºæ³•å›½åŒ»å­¦ç”Ÿå®é™…è€ƒè¯•ä¸­çš„é—®é¢˜ï¼ŒåŒ…å«åŸºäº37ç¯‡ç§‘å­¦æ–‡ç« çš„534ä¸ªé—®é¢˜ã€‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒCareMedEvalæ˜ç¡®è¯„ä¼°åŸºäºç§‘å­¦è®ºæ–‡çš„æ‰¹åˆ¤æ€§é˜…è¯»å’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šç§ä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹å¯¹æœ€æ–°é€šç”¨LLMå’Œç”Ÿç‰©åŒ»å­¦ä¸“ä¸šLLMçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè¯¥ä»»åŠ¡çš„éš¾åº¦å¾ˆå¤§ï¼šå¼€æ”¾å’Œå•†ä¸šæ¨¡å‹æœªèƒ½è¶…è¿‡å‡†ç¡®åŒ¹é…ç‡0.5ï¼Œå°½ç®¡ç”Ÿæˆä¸­é—´æ¨ç†æ ‡è®°å¯ä»¥æ˜¾è‘—æ”¹å–„ç»“æœã€‚ç„¶è€Œï¼Œæ¨¡å‹åœ¨å…³äºç ”ç©¶å±€é™æ€§å’Œç»Ÿè®¡åˆ†æçš„é—®é¢˜ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚CareMedEvalä¸ºåŸºäºæƒ…å¢ƒçš„æ¨ç†æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œæš´éœ²äº†å½“å‰LLMçš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥å¼€å‘è‡ªåŠ¨åŒ–æ”¯æŒæ‰¹åˆ¤æ€§è¯„ä»·é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03441v2">PDF</a> Preprint submitted to LREC 2026 (under review) To access the dataset,   see <a target="_blank" rel="noopener" href="https://github.com/bonzid/CareMedEval">https://github.com/bonzid/CareMedEval</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CareMedEvalæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®æ‰¹åˆ¤æ€§è¯„ä»·å’Œæ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»æ³•å›½åŒ»å­¦ç”ŸçœŸå®è€ƒè¯•ä¸­çš„534ä¸ªé—®é¢˜ï¼ŒåŸºäº37ç¯‡ç§‘å­¦æ–‡ç« æ„å»ºè€Œæˆã€‚è¯¥æ•°æ®é›†è¯„ä¼°äº†åŸºäºç§‘å­¦è®ºæ–‡çš„æ‰¹åˆ¤æ€§é˜…è¯»å’Œæ¨ç†èƒ½åŠ›ã€‚å¯¹æœ€æ–°çš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œç”Ÿç‰©åŒ»å­¦ä¸“ä¸šåŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè¿™ä¸€ä»»åŠ¡æå…·æŒ‘æˆ˜æ€§ï¼Œç°æœ‰çš„æ¨¡å‹ç²¾ç¡®åŒ¹é…ç‡ä¸è¶…è¿‡0.5ã€‚è™½ç„¶ç”Ÿæˆä¸­é—´æ¨ç†ç¬¦å·ä¼šæ˜¾è‘—æ”¹å–„ç»“æœï¼Œä½†åœ¨ç ”ç©¶å±€é™æ€§å’Œç»Ÿè®¡åˆ†æé—®é¢˜ä¸Šï¼Œæ¨¡å‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†ä¸ºæœªæ¥çš„è‡ªåŠ¨æ”¯æŒæä¾›äº†æœºä¼šï¼Œæœ‰æœ›å…‹æœå¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œä¿ƒè¿›å…¶åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CareMedEvalæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®æ‰¹åˆ¤æ€§è¯„ä»·å’Œæ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½çš„æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä»çœŸå®è€ƒè¯•ä¸­æå–çš„534ä¸ªé—®é¢˜ï¼Œè¿™äº›é—®é¢˜åŸºäºç§‘å­¦æ–‡ç« è®¾è®¡è€Œæˆã€‚</li>
<li>è¯¥æ•°æ®é›†æ³¨é‡è¯„ä¼°åŸºäºç§‘å­¦è®ºæ–‡çš„æ‰¹åˆ¤æ€§é˜…è¯»å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿™ä¸€ä»»åŠ¡ä¸Šè¡¨ç°æœ‰é™ï¼Œç²¾ç¡®åŒ¹é…ç‡ä¸è¶…è¿‡0.5ã€‚</li>
<li>ç”Ÿæˆä¸­é—´æ¨ç†ç¬¦å·å¯ä»¥æ˜¾è‘—æ”¹å–„ç»“æœï¼Œä½†åœ¨ç ”ç©¶å±€é™æ€§å’Œç»Ÿè®¡åˆ†ææ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0172aa9a7202cbf8b38ebb36a774460" align="middle">
<img src="https://picx.zhimg.com/v2-3b5da8cc338fc458640f52f3d344b306" align="middle">
<img src="https://picx.zhimg.com/v2-ae3139686eb14ce100621736f4934a1b" align="middle">
<img src="https://picx.zhimg.com/v2-ae3c8fe95259567b71a88e177d122466" align="middle">
<img src="https://picx.zhimg.com/v2-755a23bfb5bc1fa0ed51494488478add" align="middle">
<img src="https://picx.zhimg.com/v2-dfb9d5a861ee149ce0c8c2276c087f9a" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Decomposition-Enhanced-Training-for-Post-Hoc-Attributions-In-Language-Models"><a href="#Decomposition-Enhanced-Training-for-Post-Hoc-Attributions-In-Language-Models" class="headerlink" title="Decomposition-Enhanced Training for Post-Hoc Attributions In Language   Models"></a>Decomposition-Enhanced Training for Post-Hoc Attributions In Language   Models</h2><p><strong>Authors:Sriram Balasubramanian, Samyadeep Basu, Koustava Goswami, Ryan Rossi, Varun Manjunatha, Roshan Santhosh, Ruiyi Zhang, Soheil Feizi, Nedim Lipka</strong></p>
<p>Large language models (LLMs) are increasingly used for long-document question answering, where reliable attribution to sources is critical for trust. Existing post-hoc attribution methods work well for extractive QA but struggle in multi-hop, abstractive, and semi-extractive settings, where answers synthesize information across passages. To address these challenges, we argue that post-hoc attribution can be reframed as a reasoning problem, where answers are decomposed into constituent units, each tied to specific context. We first show that prompting models to generate such decompositions alongside attributions improves performance. Building on this, we introduce DecompTune, a post-training method that teaches models to produce answer decompositions as intermediate reasoning steps. We curate a diverse dataset of complex QA tasks, annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and 14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards. Across extensive experiments and ablations, DecompTune substantially improves attribution quality, outperforming prior methods and matching or exceeding state-of-the-art frontier models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°ç”¨äºé•¿æ–‡æ¡£é—®ç­”ï¼Œå…¶ä¸­å¯é çš„æ¥æºå½’å±å¯¹äºä¿¡ä»»è‡³å…³é‡è¦ã€‚ç°æœ‰çš„äº‹åå½’å› æ–¹æ³•åœ¨æå–å¼é—®ç­”ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šè·³ã€æŠ½è±¡å’ŒåŠæå–è®¾ç½®ä¸­è¡¨ç°è¾ƒå·®ï¼Œè¿™äº›è®¾ç½®ä¸­çš„ç­”æ¡ˆä¼šç»¼åˆå„æ®µè½çš„ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¤ä¸ºäº‹åå½’å› å¯ä»¥é‡æ–°æ„å»ºä¸ºæ¨ç†é—®é¢˜ï¼Œç­”æ¡ˆè¢«åˆ†è§£ä¸ºä¸ç‰¹å®šä¸Šä¸‹æ–‡ç›¸å…³è”çš„ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜ï¼Œæç¤ºæ¨¡å‹ç”Ÿæˆè¿™ç§åˆ†è§£å¹¶é™„å¸¦å½’å› å¯ä»¥æé«˜æ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†DecompTuneï¼Œè¿™æ˜¯ä¸€ç§åè®­ç»ƒæ–¹æ³•ï¼Œå¯ä»¥æ•™å¯¼æ¨¡å‹å°†ç­”æ¡ˆåˆ†è§£ä½œä¸ºä¸­é—´æ¨ç†æ­¥éª¤æ¥äº§ç”Ÿç­”æ¡ˆã€‚æˆ‘ä»¬æ•´ç†äº†ä¸€å¥—å¤æ‚çš„é—®ç­”ä»»åŠ¡æ•°æ®é›†ï¼Œç”±å¼ºå¤§çš„LLMè¿›è¡Œåˆ†è§£æ³¨é‡Šï¼Œå¹¶ä½¿ç”¨å¸¦æœ‰ç‰¹å®šä»»åŠ¡å¥–åŠ±çš„ä¸¤é˜¶æ®µSFT+GRPOç®¡é“å¯¹Qwen-2.5ï¼ˆ7Bå’Œ14Bï¼‰è¿›è¡Œåè®­ç»ƒã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼ŒDecompTuneå¤§å¹…æé«˜äº†å½’å› è´¨é‡ï¼Œä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œå¹¶åŒ¹é…æˆ–è¶…è¿‡æœ€æ–°å‰æ²¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25766v3">PDF</a> Post-hoc attribution</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿æ–‡æ¡£é—®ç­”æ—¶è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå¯é çš„æ¥æºå½’å±å¯¹äºä¿¡ä»»è‡³å…³é‡è¦ã€‚ç°æœ‰çš„äº‹åå½’å› æ–¹æ³•åœ¨å¤„ç†æå–å‹é—®ç­”æ—¶æ•ˆæœå¾ˆå¥½ï¼Œä½†åœ¨å¤šè·³ã€æŠ½è±¡å’ŒåŠæå–åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†äº‹åå½’å› é‡æ–°å®šä½ä¸ºæ¨ç†é—®é¢˜ï¼Œç­”æ¡ˆè¢«åˆ†è§£ä¸ºä¸ç‰¹å®šä¸Šä¸‹æ–‡ç›¸å…³è”çš„å„ä¸ªç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬é¦–å…ˆå±•ç¤ºé€šè¿‡æç¤ºæ¨¡å‹ç”Ÿæˆæ­¤ç±»åˆ†è§£ä¸å½’å› ï¼Œå¯ä»¥æé«˜æ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†DecompTuneï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒåæ–¹æ³•ï¼Œç”¨äºæ•™å¯¼æ¨¡å‹å°†ç­”æ¡ˆåˆ†è§£ä½œä¸ºä¸­é—´æ¨ç†æ­¥éª¤äº§ç”Ÿã€‚æˆ‘ä»¬ä½¿ç”¨å¼ºå¤§çš„LLMå¯¹å¤æ‚çš„é—®ç­”ä»»åŠ¡è¿›è¡Œåˆ†è§£æ ‡æ³¨ï¼Œå¹¶åˆ¶ä½œäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ã€‚é€šè¿‡å¯¹Qwen-2.5ï¼ˆ7Bå’Œ14Bï¼‰è¿›è¡Œåè®­ç»ƒï¼Œä½¿ç”¨å¸¦æœ‰ç‰¹å®šä»»åŠ¡å¥–åŠ±çš„ä¸¤é˜¶æ®µSFT+GRPOç®¡é“ï¼ŒDecompTuneå¤§å¹…æé«˜äº†å½’å› è´¨é‡ï¼Œä¼˜äºä¹‹å‰çš„æ–¹æ³•å¹¶è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ä¸šç•Œå‰æ²¿æ¨¡å‹çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿æ–‡æ¡£é—®ç­”æ—¶é¢ä¸´å¯é å½’å› çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰äº‹åå½’å› æ–¹æ³•åœ¨æå–å‹é—®ç­”åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šè·³ã€æŠ½è±¡å’ŒåŠæå–åœºæ™¯ä¸­å­˜åœ¨é—®é¢˜ã€‚</li>
<li>å°†äº‹åå½’å› é‡æ–°å®šä½ä¸ºæ¨ç†é—®é¢˜ï¼Œç­”æ¡ˆè¢«åˆ†è§£ä¸ºä¸ç‰¹å®šä¸Šä¸‹æ–‡ç›¸å…³è”çš„å„ä¸ªç»„æˆéƒ¨åˆ†ã€‚</li>
<li>é€šè¿‡æç¤ºæ¨¡å‹ç”Ÿæˆç­”æ¡ˆåˆ†è§£å¯ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>DecompTuneæ˜¯ä¸€ç§è®­ç»ƒåæ–¹æ³•ï¼Œç”¨äºæ•™å¯¼æ¨¡å‹å°†ç­”æ¡ˆåˆ†è§£ä½œä¸ºä¸­é—´æ¨ç†æ­¥éª¤äº§ç”Ÿã€‚</li>
<li>ä½¿ç”¨å¤šæ ·åŒ–æ•°æ®é›†è¿›è¡Œåè®­ç»ƒï¼ŒåŒ…æ‹¬ç”±å¼ºå¤§LLMå¯¹å¤æ‚é—®ç­”ä»»åŠ¡çš„åˆ†è§£æ ‡æ³¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f06e035ecaef5f4b2ba2d643c463b539" align="middle">
<img src="https://picx.zhimg.com/v2-55e128fb1ec1626eb74c951904e53a8a" align="middle">
<img src="https://picx.zhimg.com/v2-f4399ff55380180bd39a15de943db7fd" align="middle">
<img src="https://picx.zhimg.com/v2-30d52593c3f8151d871644d3c78e3ed2" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Lookahead-Tree-Based-Rollouts-for-Enhanced-Trajectory-Level-Exploration-in-Reinforcement-Learning-with-Verifiable-Rewards"><a href="#Lookahead-Tree-Based-Rollouts-for-Enhanced-Trajectory-Level-Exploration-in-Reinforcement-Learning-with-Verifiable-Rewards" class="headerlink" title="Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration   in Reinforcement Learning with Verifiable Rewards"></a>Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration   in Reinforcement Learning with Verifiable Rewards</h2><p><strong>Authors:Shangyu Xing, Siyuan Wang, Chenyuan Yang, Xinyu Dai, Xiang Ren</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR), particularly with algorithms like Group Relative Policy Optimization (GRPO), has proven highly effective in enhancing the reasoning capabilities of large language models. However, a critical bottleneck in current pipelines lies in the limited diversity of sampled trajectories during group rollouts. Homogeneous trajectories and their associated rewards would diminish the return signals for policy updates, thereby hindering effective policy learning. This lack of diversity stems primarily from token-level stochastic sampling, where local variations are likely to collapse into near-identical reasoning paths. To address this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a novel rollout strategy designed to explicitly promotes trajectory-level diversity by enforcing branching into different candidate tokens likely to yield distinct continuations. Specifically, LATR iteratively operates in three stages: (1) branching at high-uncertainty generation steps, (2) performing lookahead simulation for each new branch, and (3) pruning branches that exhibits prolonged similarity during simulation. Compared with stochastic Sampling, LATR accelerates policy learning by 131% on average and improves final pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy Optimization (DAPO) algorithms across different reasoning tasks. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/starreeze/latr">https://github.com/starreeze/latr</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ç‰¹åˆ«æ˜¯ä¸é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰ç®—æ³•ç›¸ç»“åˆï¼Œåœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºé«˜åº¦æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå½“å‰ç®¡é“ä¸­çš„ä¸€ä¸ªå…³é”®ç“¶é¢ˆåœ¨äºé›†å›¢æ»šåŠ¨è¿‡ç¨‹ä¸­é‡‡æ ·è½¨è¿¹çš„å¤šæ ·æ€§æœ‰é™ã€‚åŒè´¨çš„è½¨è¿¹åŠå…¶ç›¸å…³çš„å¥–åŠ±ä¼šé™ä½ç­–ç•¥æ›´æ–°çš„å›æŠ¥ä¿¡å·ï¼Œä»è€Œé˜»ç¢æœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚è¿™ç§å¤šæ ·æ€§çš„ç¼ºä¹ä¸»è¦æºäºä»¤ç‰Œçº§çš„éšæœºé‡‡æ ·ï¼Œå±€éƒ¨å˜åŒ–å¾ˆå¯èƒ½é™·å…¥å‡ ä¹ç›¸åŒçš„æ¨ç†è·¯å¾„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå‰ç»æ ‘çš„æ»šåŠ¨ï¼ˆLATRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ»šåŠ¨ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåˆ¶åˆ†æ”¯åˆ°å¯èƒ½äº§ç”Ÿä¸åŒå»¶ç»­çš„ä¸åŒå€™é€‰ä»¤ç‰Œæ¥æ˜¾å¼ä¿ƒè¿›è½¨è¿¹çº§çš„å¤šæ ·æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒLATRä»¥ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œæ“ä½œï¼šï¼ˆ1ï¼‰åœ¨é«˜ä¸ç¡®å®šæ€§ç”Ÿæˆæ­¥éª¤è¿›è¡Œåˆ†æ”¯ï¼Œï¼ˆ2ï¼‰å¯¹æ¯ä¸ªæ–°åˆ†æ”¯è¿›è¡Œå‰ç»æ¨¡æ‹Ÿï¼Œï¼ˆ3ï¼‰å‰ªé™¤æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­æŒç»­æ—¶é—´è¾ƒé•¿çš„ç›¸ä¼¼åˆ†æ”¯ã€‚ä¸éšæœºé‡‡æ ·ç›¸æ¯”ï¼ŒLATRåœ¨GRPOå’ŒåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ç®—æ³•ä¸Šå¹³å‡åŠ é€Ÿç­–ç•¥å­¦ä¹ 131%ï¼Œå¹¶åœ¨ä¸åŒçš„æ¨ç†ä»»åŠ¡ä¸­æé«˜äº†æœ€ç»ˆé€šè¿‡ç‡ä¸º1çš„æ€§èƒ½4.2%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/starreeze/latr%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/starreeze/latrå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24302v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ç»“åˆï¼Œå¦‚é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰ç®—æ³•ï¼Œèƒ½æœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰ç®¡é“çš„å…³é”®ç“¶é¢ˆåœ¨äºç¾¤ä½“æ»šåŠ¨è¿‡ç¨‹ä¸­é‡‡æ ·è½¨è¿¹çš„å¤šæ ·æ€§æœ‰é™ã€‚åŒè´¨çš„è½¨è¿¹åŠå…¶ç›¸å…³å¥–åŠ±ä¼šé™ä½ç­–ç•¥æ›´æ–°çš„å›æŠ¥ä¿¡å·ï¼Œé˜»ç¢æœ‰æ•ˆçš„ç­–ç•¥å­¦ä¹ ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå‰ç»æ ‘çš„æ»šåŠ¨ï¼ˆLATRï¼‰ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåˆ¶åˆ†æ”¯åˆ°ä¸åŒçš„å€™é€‰ä»¤ç‰Œæ¥æ˜ç¡®ä¿ƒè¿›è½¨è¿¹çº§åˆ«çš„å¤šæ ·æ€§ï¼Œä»è€Œäº§ç”Ÿä¸åŒçš„å»¶ç»­ã€‚LATRé€šè¿‡ä¸‰ä¸ªé˜¶æ®µæ“ä½œï¼šä¸€æ˜¯åœ¨é«˜ä¸ç¡®å®šæ€§ç”Ÿæˆæ­¥éª¤è¿›è¡Œåˆ†æ”¯ï¼›äºŒæ˜¯å¯¹æ¯ä¸ªæ–°åˆ†æ”¯è¿›è¡Œå‰ç»æ¨¡æ‹Ÿï¼›ä¸‰æ˜¯åˆ é™¤æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­é•¿æ—¶é—´ç›¸ä¼¼çš„åˆ†æ”¯ã€‚ç›¸è¾ƒäºéšæœºé‡‡æ ·ï¼ŒLATRå¹³å‡åŠ é€Ÿç­–ç•¥å­¦ä¹ 131%ï¼Œæœ€ç»ˆé€šè¿‡ä¸€æ¬¡æ‰§è¡ŒæˆåŠŸç‡çš„æ€§èƒ½æé«˜äº†4.2%ï¼Œæ— è®ºæ˜¯åœ¨GRPOè¿˜æ˜¯åŠ¨æ€é‡‡æ ·æ”¿ç­–ä¼˜åŒ–ï¼ˆDAPOï¼‰ç®—æ³•ä¸­ï¼Œå‡åº”ç”¨äºä¸åŒçš„æ¨ç†ä»»åŠ¡ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²åœ¨å…¬å¼€å¹³å°å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRä¸GRPOç­‰ç®—æ³•å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰ç®¡é“é¢ä¸´çš„å…³é”®ç“¶é¢ˆåœ¨äºé‡‡æ ·è½¨è¿¹å¤šæ ·æ€§æœ‰é™ã€‚</li>
<li>åŒè´¨è½¨è¿¹å’Œç›¸å…³å¥–åŠ±ä¼šé™ä½ç­–ç•¥æ›´æ–°çš„å›æŠ¥ä¿¡å·ã€‚</li>
<li>LATRç­–ç•¥æ—¨åœ¨é€šè¿‡å¼ºåˆ¶åˆ†æ”¯åˆ°ä¸åŒçš„å€™é€‰ä»¤ç‰Œæ¥æå‡è½¨è¿¹çº§åˆ«çš„å¤šæ ·æ€§ã€‚</li>
<li>LATRé€šè¿‡ä¸‰ä¸ªé˜¶æ®µæ“ä½œï¼šåˆ†æ”¯ã€å‰ç»æ¨¡æ‹Ÿå’Œåˆ é™¤é•¿æ—¶é—´ç›¸ä¼¼çš„åˆ†æ”¯ã€‚</li>
<li>LATRç›¸è¾ƒäºéšæœºé‡‡æ ·èƒ½æ˜¾è‘—åŠ é€Ÿç­–ç•¥å­¦ä¹ å¹¶æé«˜æ‰§è¡ŒæˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cc5344fc39b2312b7a4ed1fb293eaa4" align="middle">
<img src="https://picx.zhimg.com/v2-0aff562f8b19b309f110286bc90c3371" align="middle">
<img src="https://picx.zhimg.com/v2-690aad9c363742b4c3a16b6393241230" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="PaTaRM-Bridging-Pairwise-and-Pointwise-Signals-via-Preference-Aware-Task-Adaptive-Reward-Modeling"><a href="#PaTaRM-Bridging-Pairwise-and-Pointwise-Signals-via-Preference-Aware-Task-Adaptive-Reward-Modeling" class="headerlink" title="PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware   Task-Adaptive Reward Modeling"></a>PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware   Task-Adaptive Reward Modeling</h2><p><strong>Authors:Ai Jian, Jingqing Ruan, Xing Ma, Dailin Li, QianLin Zhou, Ke Zeng, Xunliang Cai</strong></p>
<p>Reward models (RMs) are central to reinforcement learning from human feedback (RLHF), providing the critical supervision signals that align large language models (LLMs) with human preferences. While generative reward models (GRMs) offer greater interpretability than traditional scalar RMs, current training paradigms remain limited. Pair-wise methods rely on binary good-versus-bad labels, which cause mismatches for point-wise inference and necessitate complex pairing strategies for effective application in RLHF. On the other hand, point-wise methods require more elaborate absolute labeling with rubric-driven criteria, resulting in poor adaptability and high annotation costs. In this work, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a unified framework that integrates a preference-aware reward (PAR) mechanism with dynamic rubric adaptation. PaTaRM leverages relative preference information from pairwise data to construct robust point-wise training signals, eliminating the need for explicit point-wise labels. Simultaneously, it employs a task-adaptive rubric system that flexibly generates evaluation criteria for both global task consistency and instance-specific fine-grained reasoning. This design enables efficient, generalizable, and interpretable reward modeling for RLHF. Extensive experiments show that PaTaRM achieves an average relative improvement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B models. Furthermore, PaTaRM boosts downstream RLHF performance, with an average improvement of 13.6% across IFEval and InFoBench benchmarks, confirming its effectiveness and robustness. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/JaneEyre0530/PaTaRM">https://github.com/JaneEyre0530/PaTaRM</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„æ ¸å¿ƒï¼Œæä¾›ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¬¦åˆäººç±»åå¥½çš„å…³é”®ç›‘ç£ä¿¡å·ã€‚è™½ç„¶ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰æä¾›äº†æ¯”ä¼ ç»Ÿæ ‡é‡RMæ›´å¤§çš„å¯è§£é‡Šæ€§ï¼Œä½†å½“å‰çš„è®­ç»ƒæ¨¡å¼ä»ç„¶æœ‰é™ã€‚é…å¯¹æ–¹æ³•ä¾èµ–äºäºŒå…ƒçš„å¥½ä¸åçš„æ ‡ç­¾ï¼Œè¿™ä¼šå¯¼è‡´ç‚¹å¯¹ç‚¹æ¨ç†çš„ä¸åŒ¹é…ï¼Œå¹¶éœ€è¦å¤æ‚çš„é…å¯¹ç­–ç•¥æ‰èƒ½åœ¨RLHFä¸­æœ‰æ•ˆåº”ç”¨ã€‚å¦ä¸€æ–¹é¢ï¼Œç‚¹å¯¹ç‚¹æ–¹æ³•éœ€è¦æ›´è¯¦ç»†çš„ç»å¯¹æ ‡ç­¾å’ŒåŸºäºæ ‡å‡†çš„æ ‡å‡†ï¼Œå¯¼è‡´é€‚åº”æ€§å·®å’Œæ ‡æ³¨æˆæœ¬é«˜æ˜‚ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åå¥½æ„ŸçŸ¥ä»»åŠ¡è‡ªé€‚åº”å¥–åŠ±æ¨¡å‹ï¼ˆPaTaRMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†åå¥½æ„ŸçŸ¥å¥–åŠ±ï¼ˆPARï¼‰æœºåˆ¶ä¸åŠ¨æ€æ ‡å‡†è‡ªé€‚åº”ç›¸ç»“åˆã€‚PaTaRMåˆ©ç”¨é…å¯¹æ•°æ®ä¸­çš„ç›¸å¯¹åå¥½ä¿¡æ¯æ¥æ„å»ºç¨³å¥çš„ç‚¹å¯¹ç‚¹è®­ç»ƒä¿¡å·ï¼Œæ— éœ€æ˜ç¡®çš„ç‚¹å¯¹ç‚¹æ ‡ç­¾ã€‚åŒæ—¶ï¼Œå®ƒé‡‡ç”¨ä»»åŠ¡è‡ªé€‚åº”æ ‡å‡†ç³»ç»Ÿï¼Œçµæ´»ç”Ÿæˆå…¨å±€ä»»åŠ¡ä¸€è‡´æ€§å’Œå®ä¾‹ç‰¹å®šç²¾ç»†æ¨ç†çš„è¯„ä»·æ ‡å‡†ã€‚è¿™ç§è®¾è®¡å®ç°äº†é«˜æ•ˆã€é€šç”¨å’Œå¯è§£é‡Šçš„å¥–åŠ±æ¨¡å‹ï¼Œé€‚ç”¨äºRLHFã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPaTaRMåœ¨RewardBenchå’ŒRMBenchçš„Qwen3-8Bå’ŒQwen3-14Bæ¨¡å‹ä¸Šå¹³å‡ç›¸å¯¹æé«˜äº†4.7%ã€‚æ­¤å¤–ï¼ŒPaTaRMæé«˜äº†ä¸‹æ¸¸RLHFçš„æ€§èƒ½ï¼Œåœ¨IFEvalå’ŒInFoBenchåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æé«˜äº†13.6%ï¼Œè¯å®äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JaneEyre0530/PaTaRM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/JaneEyre0530/PaTaRMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.24235v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„é‡è¦æ€§ï¼Œå®ƒæä¾›ä¸äººç±»åå¥½å¯¹é½çš„å…³é”®ç›‘ç£ä¿¡å·ã€‚é’ˆå¯¹å½“å‰å¥–åŠ±æ¨¡å‹çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§èåˆåå¥½æ„ŸçŸ¥å¥–åŠ±ï¼ˆPARï¼‰å’Œä»»åŠ¡é€‚åº”æ€§å¥–åŠ±æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶â€”â€”PaTaRMã€‚è¯¥æ¡†æ¶åˆ©ç”¨é…å¯¹æ•°æ®çš„ç›¸å¯¹åå¥½ä¿¡æ¯æ„å»ºç¨³å¥çš„ç‚¹æ€è®­ç»ƒä¿¡å·ï¼Œæ— éœ€æ˜¾å¼ç‚¹æ€æ ‡ç­¾ã€‚åŒæ—¶ï¼Œå®ƒé‡‡ç”¨ä»»åŠ¡é€‚åº”æ€§è¯„ä»·ä½“ç³»ï¼Œæ—¢èƒ½ä¿è¯å…¨å±€ä»»åŠ¡ä¸€è‡´æ€§åˆèƒ½å®ç°å®ä¾‹ç‰¹å®šç²¾ç»†æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒPaTaRMåœ¨å¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°äº†å¹³å‡ç›¸å¯¹æ”¹è¿›4.7%ï¼Œæé«˜äº†ä¸‹æ¸¸RLHFæ€§èƒ½çš„å¹³å‡æ”¹è¿›ç‡ä¸º13.6%ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­æ‰®æ¼”æ ¸å¿ƒè§’è‰²ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›ä¸äººç±»åå¥½å¯¹é½çš„å…³é”®ç›‘ç£ä¿¡å·ã€‚</li>
<li>å½“å‰å¥–åŠ±æ¨¡å‹å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ–°æ–¹æ³•æ¥è§£å†³é…å¯¹å’Œç‚¹æ€æ ‡æ³¨çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶â€”â€”PaTaRMï¼Œèåˆäº†åå¥½æ„ŸçŸ¥å¥–åŠ±ï¼ˆPARï¼‰å’Œä»»åŠ¡é€‚åº”æ€§å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>PaTaRMåˆ©ç”¨ç›¸å¯¹åå¥½ä¿¡æ¯æ„å»ºç¨³å¥çš„ç‚¹æ€è®­ç»ƒä¿¡å·ï¼Œæ— éœ€å¤æ‚çš„é…å¯¹ç­–ç•¥æˆ–æ˜‚è´µçš„ç‚¹æ€æ ‡æ³¨ã€‚</li>
<li>PaTaRMé‡‡ç”¨ä»»åŠ¡é€‚åº”æ€§è¯„ä»·ä½“ç³»ï¼Œå®ç°å…¨å±€ä»»åŠ¡ä¸€è‡´æ€§å’Œå®ä¾‹ç‰¹å®šç²¾ç»†æ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒPaTaRMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>PaTaRMçš„ä»£ç å·²å…¬å¼€ï¼Œä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.24235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa6e535cd914e79118c7269f3f3d9e9a" align="middle">
<img src="https://picx.zhimg.com/v2-762159f6da0195aa44b0e6ca70021081" align="middle">
<img src="https://picx.zhimg.com/v2-e5789889c4ac6b4356c7000e4eee39ed" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Evaluating-Long-Term-Memory-for-Long-Context-Question-Answering"><a href="#Evaluating-Long-Term-Memory-for-Long-Context-Question-Answering" class="headerlink" title="Evaluating Long-Term Memory for Long-Context Question Answering"></a>Evaluating Long-Term Memory for Long-Context Question Answering</h2><p><strong>Authors:Alessandra Terranova, BjÃ¶rn Ross, Alexandra Birch</strong></p>
<p>In order for large language models to achieve true conversational continuity and benefit from experiential learning, they need memory. While research has focused on the development of complex memory systems, it remains unclear which types of memory are most effective for long-context conversational tasks. We present a systematic evaluation of memory-augmented methods using LoCoMo, a benchmark of synthetic long-context dialogues annotated for question-answering tasks that require diverse reasoning strategies. We analyse full-context prompting, semantic memory through retrieval-augmented generation and agentic memory, episodic memory through in-context learning, and procedural memory through prompt optimization. Our findings show that memory-augmented approaches reduce token usage by over 90% while maintaining competitive accuracy. Memory architecture complexity should scale with model capability, with small foundation models benefitting most from RAG, and strong instruction-tuned reasoning model gaining from episodic learning through reflections and more complex agentic semantic memory. In particular, episodic memory can help LLMs recognise the limits of their own knowledge. </p>
<blockquote>
<p>ä¸ºäº†è®©å¤§å‹è¯­è¨€æ¨¡å‹å®ç°çœŸæ­£çš„å¯¹è¯è¿è´¯æ€§å¹¶ä»ä¸­å—ç›Šä½“éªŒå­¦ä¹ ï¼Œå®ƒä»¬éœ€è¦è®°å¿†ã€‚è™½ç„¶ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤æ‚è®°å¿†ç³»ç»Ÿçš„å‘å±•ä¸Šï¼Œä½†å°šä¸æ¸…æ¥šå“ªç§ç±»å‹çš„è®°å¿†å¯¹äºé•¿è¯­å¢ƒå¯¹è¯ä»»åŠ¡æœ€ä¸ºæœ‰æ•ˆã€‚æˆ‘ä»¬ä½¿ç”¨LoCoMoï¼ˆä¸€ä¸ªä¸ºé—®ç­”ä»»åŠ¡æ ‡æ³¨çš„åˆæˆé•¿è¯­å¢ƒå¯¹è¯çš„åŸºå‡†æµ‹è¯•ï¼‰å¯¹å¢å¼ºè®°å¿†æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å¤šç§æ¨ç†ç­–ç•¥ã€‚æˆ‘ä»¬åˆ†æäº†å…¨è¯­å¢ƒæç¤ºã€é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œè‡ªä¸»è®°å¿†çš„è¯­ä¹‰è®°å¿†ã€é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ çš„æƒ…æ™¯è®°å¿†ä»¥åŠé€šè¿‡æç¤ºä¼˜åŒ–ç¨‹åºè®°å¿†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¢å¼ºè®°å¿†çš„æ–¹æ³•å¯ä»¥å°†ä»¤ç‰Œä½¿ç”¨é‡å‡å°‘90%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›å‡†ç¡®åº¦ã€‚è®°å¿†æ¶æ„çš„å¤æ‚æ€§åº”éšç€æ¨¡å‹èƒ½åŠ›çš„æ‰©å±•è€Œæ‰©å±•ï¼Œå°å‹åŸºç¡€æ¨¡å‹ä»RAGä¸­è·ç›Šæœ€å¤§ï¼Œè€Œå¼ºå¤§çš„æŒ‡ä»¤è°ƒæ•´æ¨ç†æ¨¡å‹åˆ™ä»é€šè¿‡åæ€å’Œæ›´å¤æ‚çš„è‡ªä¸»è¯­ä¹‰è®°å¿†ä¸­è·ç›Šã€‚ç‰¹åˆ«æ˜¯æƒ…æ™¯è®°å¿†å¯ä»¥å¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹è¯†åˆ«è‡ªèº«çŸ¥è¯†çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23730v1">PDF</a> 14 pages including appendix, 3 figures. Submitted to October ARR and   to Metacognition in Generative AI EurIPS workshop (under review for both)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹è¦å®ç°çœŸæ­£çš„å¯¹è¯è¿ç»­æ€§å’Œä»ç»éªŒå­¦ä¹ ä¸­å—ç›Šï¼Œéœ€è¦è®°å¿†åŠ›çš„æ”¯æŒã€‚ç ”ç©¶å·²å…³æ³¨å¤æ‚è®°å¿†ç³»ç»Ÿçš„å‘å±•ï¼Œä½†å¯¹äºé•¿è¯­å¢ƒå¯¹è¯ä»»åŠ¡ï¼Œå°šä¸æ¸…æ¥šå“ªç§ç±»å‹çš„è®°å¿†æœ€ä¸ºæœ‰æ•ˆã€‚æœ¬æ–‡é€šè¿‡LoCoMoåŸºå‡†æµ‹è¯•ï¼Œå¯¹è®°å¿†å¢å¼ºæ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼ŒåŒ…æ‹¬å…¨è¯­å¢ƒæç¤ºã€é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆå®ç°çš„è¯­ä¹‰è®°å¿†å’Œè‡ªä¸»è®°å¿†ã€é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°çš„æƒ…æ™¯è®°å¿†ä»¥åŠé€šè¿‡æç¤ºä¼˜åŒ–å®ç°çš„ç¨‹åºæ€§è®°å¿†ã€‚ç ”ç©¶å‘ç°ï¼Œè®°å¿†å¢å¼ºæ–¹æ³•å¯åœ¨å‡å°‘ä»¤ç‰Œä½¿ç”¨çš„åŒæ—¶ä¿æŒç«äº‰åŠ›ï¼›è®°å¿†æ¶æ„çš„å¤æ‚æ€§åº”éšæ¨¡å‹èƒ½åŠ›çš„å¢å¼ºè€Œæ‰©å±•ï¼Œå°å‹åŸºç¡€æ¨¡å‹æœ€å—ç›ŠäºRAGï¼Œè€Œå¼ºå¤§çš„æŒ‡ä»¤è°ƒä¼˜æ¨ç†æ¨¡å‹åˆ™å—ç›Šäºé€šè¿‡åæ€å’Œæ›´å¤æ‚çš„è‡ªä¸»è¯­ä¹‰è®°å¿†çš„æƒ…æ™¯å­¦ä¹ ã€‚ç‰¹åˆ«æ˜¯æƒ…æ™¯è®°å¿†æœ‰åŠ©äºLLMè¯†åˆ«è‡ªèº«çŸ¥è¯†çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è¦å®ç°çœŸæ­£çš„å¯¹è¯è¿ç»­æ€§éœ€è¦ä»ç»éªŒä¸­å­¦ä¹ å¹¶é…å¤‡è®°å¿†åŠ›æ”¯æŒã€‚</li>
<li>ç›®å‰å¯¹äºé•¿è¯­å¢ƒå¯¹è¯ä»»åŠ¡ä¸­å“ªç§ç±»å‹çš„è®°å¿†æœ€ä¸ºæœ‰æ•ˆå°šä¸æ¸…æ¥šã€‚</li>
<li>LoCoMoåŸºå‡†æµ‹è¯•è¢«ç”¨äºç³»ç»Ÿè¯„ä¼°è®°å¿†å¢å¼ºæ–¹æ³•ã€‚</li>
<li>è®°å¿†å¢å¼ºæ–¹æ³•èƒ½åœ¨å‡å°‘ä»¤ç‰Œä½¿ç”¨çš„åŒæ—¶ä¿æŒç«äº‰åŠ›ã€‚</li>
<li>è®°å¿†æ¶æ„çš„å¤æ‚æ€§éšç€æ¨¡å‹èƒ½åŠ›çš„æå‡è€Œæ‰©å±•ã€‚</li>
<li>å°å‹åŸºç¡€æ¨¡å‹ä¸»è¦å—ç›ŠäºRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ï¼Œè€Œå¼ºå¤§çš„æŒ‡ä»¤è°ƒä¼˜æ¨ç†æ¨¡å‹åˆ™æ›´ä¾èµ–äºæƒ…æ™¯å­¦ä¹ å’Œå¤æ‚çš„è‡ªä¸»è¯­ä¹‰è®°å¿†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b0c3b753171f50ddaecdbcec38b0a2e" align="middle">
<img src="https://picx.zhimg.com/v2-66c3cdbb9aeca9c0b8ee50a3966f8d0f" align="middle">
<img src="https://picx.zhimg.com/v2-ce751aefa29e65aba3a3ef0d997633a5" align="middle">
<img src="https://picx.zhimg.com/v2-30dc72255170cb436b71f0cf12a4a0b2" align="middle">
<img src="https://picx.zhimg.com/v2-62d1b1d1b7bd0f0892320db619ce4441" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VOLD-Reasoning-Transfer-from-LLMs-to-Vision-Language-Models-via-On-Policy-Distillation"><a href="#VOLD-Reasoning-Transfer-from-LLMs-to-Vision-Language-Models-via-On-Policy-Distillation" class="headerlink" title="VOLD: Reasoning Transfer from LLMs to Vision-Language Models via   On-Policy Distillation"></a>VOLD: Reasoning Transfer from LLMs to Vision-Language Models via   On-Policy Distillation</h2><p><strong>Authors:Walid Bousselham, Hilde Kuehne, Cordelia Schmid</strong></p>
<p>Training vision-language models (VLMs) for complex reasoning remains a challenging task, i.a. due to the scarcity of high-quality image-text reasoning data. Conversely, text-based reasoning resources are abundant and scalable, but it is still an open question how to leveraging them for VLM reasoning. To address this problem, we propose VOLD, a framework to transfer reasoning capabilities from text-only teacher models to VLM student models. To this end, VOLD combines reinforcement learning via Group Relative Policy Optimization (GRPO) with on-policy distillation, which allows the student reasoning traces to be guided by the teacher model, resulting in a significant gain over using GRPO alone. We further show that a cold-start alignment is essential for an effective transfer during the online training phase in this scenario and that without sufficient distributional alignment between teacher and student, on-policy distillation fails to provide meaningful guidance. We evaluate VOLD across diverse benchmarks including MMMU-Pro, MathVision, MathVista, and LogicVista, showing that VOLD outperforms the baseline model significantly and improves over the state of the art by a margin. Our ablation shows the importance of a cold-start alignment via SFT for on-policy distillation with a text-only teacher. </p>
<blockquote>
<p>è®­ç»ƒç”¨äºå¤æ‚æ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯å› ä¸ºé«˜è´¨é‡çš„å›¾æ–‡æ¨ç†æ•°æ®ç¨€ç¼ºã€‚ç›¸åï¼ŒåŸºäºæ–‡æœ¬æ¨ç†çš„èµ„æºä¸°å¯Œä¸”å¯æ‰©å±•ã€‚ç„¶è€Œï¼Œå¦‚ä½•åˆ©ç”¨è¿™äº›èµ„æºç”¨äºVLMæ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VOLDæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å°†ä»…åŸºäºæ–‡æœ¬çš„æ•™å¸ˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°VLMå­¦ç”Ÿæ¨¡å‹ã€‚ä¸ºæ­¤ï¼ŒVOLDç»“åˆäº†é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ä¸åŸºäºç­–ç•¥çš„è’¸é¦ï¼Œè¿™å…è®¸å­¦ç”Ÿæ¨ç†è½¨è¿¹å—åˆ°æ•™å¸ˆæ¨¡å‹çš„æŒ‡å¯¼ï¼Œä¸ä»…ä½¿ç”¨GRPOç›¸æ¯”ï¼Œè¿™å¸¦æ¥äº†æ˜¾ç€çš„æ”¶ç›Šã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œåœ¨æ­¤åœºæ™¯ä¸­ï¼Œåœ¨çº¿è®­ç»ƒé˜¶æ®µæœ‰æ•ˆçš„è½¬ç§»è¿‡ç¨‹ä¸­å†·å¯åŠ¨å¯¹é½è‡³å…³é‡è¦ï¼Œå¹¶ä¸”åœ¨æ•™å¸ˆå’Œå­¦ç”Ÿä¹‹é—´ç¼ºä¹è¶³å¤Ÿçš„åˆ†å¸ƒå¯¹é½ä¼šå¯¼è‡´åŸºäºç­–ç•¥çš„è’¸é¦æ— æ³•æä¾›æœ‰æ„ä¹‰çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬MMMU-Proã€MathVisionã€MathVistaå’ŒLogicVistaç­‰å¤šç§åŸºå‡†æµ‹è¯•ä¸Šå¯¹VOLDè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜VOLDæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå¹¶ä»¥ä¸€å®šå¹…åº¦æé«˜äº†ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶æ˜¾ç¤ºäº†ä½¿ç”¨ä»…åŸºäºæ–‡æœ¬çš„æ•™å¸ˆè¿›è¡Œç­–ç•¥è’¸é¦æ—¶ï¼Œé€šè¿‡SFTè¿›è¡Œå†·å¯åŠ¨å¯¹é½çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.23497v2">PDF</a> <a target="_blank" rel="noopener" href="http://www.walidbousselham.com/VOLD/">www.walidbousselham.com/VOLD/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåä¸ºVOLDçš„æ¡†æ¶ï¼Œç”¨äºä»æ–‡æœ¬æ¨¡å‹å‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è½¬ç§»æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆåŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ä¸åœ¨çº¿ç­–ç•¥è’¸é¦ï¼Œä½¿VLMå­¦ç”Ÿåœ¨æ•™å¸ˆçš„å¼•å¯¼ä¸‹è¿›è¡Œæ¨ç†ï¼Œç›¸è¾ƒäºä»…ä½¿ç”¨GRPOæœ‰æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¿˜æ˜¾ç¤ºï¼Œåœ¨åœ¨çº¿è®­ç»ƒé˜¶æ®µè¿›è¡Œæœ‰æ•ˆçš„å†·å¯åŠ¨å¯¹é½è‡³å…³é‡è¦ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVOLDæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå¹¶æé«˜äº†ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VOLDæ¡†æ¶æ—¨åœ¨ä»æ–‡æœ¬æ¨¡å‹å‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è½¬ç§»æ¨ç†èƒ½åŠ›ã€‚</li>
<li>VOLDç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä¸åœ¨çº¿ç­–ç•¥è’¸é¦ï¼Œä½¿VLMå­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿåœ¨æ•™å¸ˆçš„å¼•å¯¼ä¸‹è¿›è¡Œæ¨ç†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å®ç°ã€‚</li>
<li>åœ¨åœ¨çº¿è®­ç»ƒé˜¶æ®µï¼Œæœ‰æ•ˆçš„å†·å¯åŠ¨å¯¹é½å¯¹äºè½¬ç§»å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>VOLDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¼˜äºåŸºå‡†æ¨¡å‹å¹¶æå‡ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>å¯¹é½ç­–ç•¥åœ¨åœ¨çº¿ç­–ç•¥è’¸é¦ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.23497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-377d17e2f9ef81c5687b8f6704a70b37" align="middle">
<img src="https://picx.zhimg.com/v2-3baccfb3d3fb1f2f6f2cef49dee5270a" align="middle">
<img src="https://picx.zhimg.com/v2-f73c9061a0a64b74b9a1c3a68ee389d4" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-08/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-08/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-08/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e2171d528cad808762f7cbec2fab90fe" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-08  Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought   Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-07/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-84e599f1945d1c2a0e6d71e1947520ee" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-07  ENDF/B-VIII.1 Updated Nuclear Reaction Data Library for Science and   Applications
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32298.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
