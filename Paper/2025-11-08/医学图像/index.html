<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-08  MedSapiens Taking a Pose to Rethink Medical Imaging Landmark Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-36ed3e9170f8d3900fd0ca13e1c9a04d')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-08-æ›´æ–°"><a href="#2025-11-08-æ›´æ–°" class="headerlink" title="2025-11-08 æ›´æ–°"></a>2025-11-08 æ›´æ–°</h1><h2 id="MedSapiens-Taking-a-Pose-to-Rethink-Medical-Imaging-Landmark-Detection"><a href="#MedSapiens-Taking-a-Pose-to-Rethink-Medical-Imaging-Landmark-Detection" class="headerlink" title="MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection"></a>MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</h2><p><strong>Authors:Marawan Elbatel, Anbang Wang, Keyuan Liu, Kaouther Mouheb, Enrique Almar-Munoz, Lizhuo Lin, Yanqi Yang, Karim Lekadir, Xiaomeng Li</strong></p>
<p>This paper does not introduce a novel architecture; instead, it revisits a fundamental yet overlooked baseline: adapting human-centric foundation models for anatomical landmark detection in medical imaging. While landmark detection has traditionally relied on domain-specific models, the emergence of large-scale pre-trained vision models presents new opportunities. In this study, we investigate the adaptation of Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through multi-dataset pretraining, establishing a new state of the art across multiple datasets. Our proposed model, MedSapiens, demonstrates that human-centric foundation models, inherently optimized for spatial pose localization, provide strong priors for anatomical landmark detection, yet this potential has remained largely untapped. We benchmark MedSapiens against existing state-of-the-art models, achieving up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in the average success detection rate (SDR). To further assess MedSapiens adaptability to novel downstream tasks with few annotations, we evaluate its performance in limited-data settings, achieving 2.69% improvement over the few-shot state of the art in SDR. Code and model weights are available at <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/MedSapiens">https://github.com/xmed-lab/MedSapiens</a> . </p>
<blockquote>
<p>æœ¬æ–‡å¹¶æœªä»‹ç»æ–°å‹æ¶æ„ï¼Œè€Œæ˜¯é‡æ–°å…³æ³¨ä¸€ä¸ªåŸºç¡€å´è¢«å¿½è§†çš„åŸºçº¿ï¼šé€‚åº”ä»¥äººç±»ä¸ºä¸­å¿ƒçš„åŸºçº¿æ¨¡å‹ï¼Œç”¨äºåŒ»å­¦æˆåƒä¸­çš„è§£å‰–åœ°æ ‡æ£€æµ‹ã€‚è™½ç„¶åœ°æ ‡æ£€æµ‹ä¼ ç»Ÿä¸Šä¾èµ–äºç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ï¼Œä½†å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„å‡ºç°å¸¦æ¥äº†æ–°çš„æœºé‡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†äººç±»ä¸ºä¸­å¿ƒçš„åŸºç¡€æ¨¡å‹Sapiensåœ¨åŒ»å­¦æˆåƒä¸­çš„å¤šæ•°æ®é›†é¢„è®­ç»ƒé€‚åº”æ€§ï¼Œè¯¥æ¨¡å‹æ˜¯ä¸ºå§¿æ€ä¼°è®¡è€Œè®¾è®¡çš„ï¼Œé€šè¿‡å¤šæ•°æ®é›†é¢„è®­ç»ƒï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚æˆ‘ä»¬æå‡ºçš„æ¨¡å‹MedSapiensè¡¨æ˜ï¼Œä»¥äººç±»ä¸ºä¸­å¿ƒã€é’ˆå¯¹ç©ºé—´å§¿æ€å®šä½è¿›è¡Œä¼˜åŒ–çš„åŸºç¡€æ¨¡å‹ä¸ºè§£å‰–åœ°æ ‡æ£€æµ‹æä¾›äº†å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œä½†è¿™ç§æ½œåŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«å¼€å‘ã€‚æˆ‘ä»¬å°†MedSapiensä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œåœ¨å¹³å‡æˆåŠŸæ£€æµ‹ç‡ï¼ˆSDRï¼‰ä¸Šï¼Œä¸ä¸€èˆ¬æ¨¡å‹ç›¸æ¯”æé«˜äº†5.26%ï¼Œä¸ä¸“ä¸šæ¨¡å‹ç›¸æ¯”æé«˜äº†é«˜è¾¾21.81%ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°MedSapienså¯¹å…·æœ‰å°‘é‡æ³¨é‡Šçš„æ–°ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§ï¼Œæˆ‘ä»¬åœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸­è¯„ä¼°äº†å…¶æ€§èƒ½ï¼Œåœ¨SDRä¸Šå®ç°äº†å¯¹ç°æœ‰æŠ€æœ¯çš„2.69%çš„æå‡ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/MedSapiens%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xmed-lab/MedSapiensæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04255v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä¸å¼•å…¥æ–°å‹æ¶æ„ï¼Œè€Œæ˜¯é‡æ–°å…³æ³¨ä¸€ä¸ªåŸºç¡€å´è¢«å¿½è§†çš„é¢†åŸŸï¼šé€‚åº”ä»¥äººä¸ºä¸­å¿ƒçš„åŸºç¡€æ¨¡å‹ï¼Œç”¨äºåŒ»å­¦æˆåƒä¸­çš„è§£å‰–åœ°æ ‡æ£€æµ‹ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†Sapiensæ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„é€‚åº”æƒ…å†µï¼Œé€šè¿‡å¤šæ•°æ®é›†é¢„è®­ç»ƒï¼Œå»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚æ‰€æå‡ºçš„MedSapiensæ¨¡å‹è¯æ˜ï¼Œä»¥äººä¸ºä¸­å¿ƒçš„åŸºç¡€æ¨¡å‹å¯¹ç©ºé—´å§¿åŠ¿å®šä½è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¸ºè§£å‰–åœ°æ ‡æ£€æµ‹æä¾›äº†å¼ºæœ‰åŠ›çš„å…ˆéªŒçŸ¥è¯†ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼ŒMedSapiensåœ¨å¹³å‡æˆåŠŸæ£€æµ‹ç‡ï¼ˆSDRï¼‰ä¸Šæé«˜äº†æœ€å¤š5.26%å’Œé«˜è¾¾21.81%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å°‘é‡æ ‡æ³¨çš„æ–°ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶é‡æ–°å…³æ³¨ä»¥äººä¸ºä¸­å¿ƒçš„åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯ç”¨äºè§£å‰–åœ°æ ‡æ£€æµ‹ã€‚</li>
<li>MedSapiensæ¨¡å‹æ˜¯é€šè¿‡å¤šæ•°æ®é›†é¢„è®­ç»ƒè€Œå»ºç«‹çš„ï¼Œå±•ç¤ºäº†ä¸€ç§æ–°çš„åŸºå‡†æ–¹æ³•ã€‚</li>
<li>MedSapiensåˆ©ç”¨äººä½“å§¿åŠ¿ä¼°è®¡è¿›è¡Œä¼˜åŒ–ï¼Œä¸ºè§£å‰–åœ°æ ‡æ£€æµ‹æä¾›å¼ºæœ‰åŠ›çš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒMedSapiensåœ¨å¹³å‡æˆåŠŸæ£€æµ‹ç‡ï¼ˆSDRï¼‰ä¸Šæœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>MedSapiensåœ¨æ•°æ®æ ‡æ³¨æœ‰é™çš„æƒ…å†µä¸‹å±•ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒè§†è§‰æ¨¡å‹çš„æœºä¼šï¼Œå°¤å…¶æ˜¯å°†å…¶é€‚åº”äºåŒ»å­¦æˆåƒé¢†åŸŸçš„æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-184209a160163402eb00bf77ef92f167" align="middle">
<img src="https://picx.zhimg.com/v2-a6db88a608faf74fa3294275aa0c7132" align="middle">
<img src="https://picx.zhimg.com/v2-a4314a759c12512efbd56a31c06de4ef" align="middle">
<img src="https://picx.zhimg.com/v2-aeb17a3968b95c43ced1ef220603e752" align="middle">
<img src="https://picx.zhimg.com/v2-4064f9066fbf07e7b8cf45c69dc89659" align="middle">
<img src="https://picx.zhimg.com/v2-a872e2188accf7a9667d8873d84aa7a1" align="middle">
<img src="https://picx.zhimg.com/v2-2d69409c3f689d11107cfd4ba94eabaf" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hadronic-Processes-in-Advection-Dominated-Accretion-Flow-as-the-Origin-of-TeV-Excesses-in-BL-Lac-Objects"><a href="#Hadronic-Processes-in-Advection-Dominated-Accretion-Flow-as-the-Origin-of-TeV-Excesses-in-BL-Lac-Objects" class="headerlink" title="Hadronic Processes in Advection-Dominated Accretion Flow as the Origin   of TeV Excesses in BL Lac Objects"></a>Hadronic Processes in Advection-Dominated Accretion Flow as the Origin   of TeV Excesses in BL Lac Objects</h2><p><strong>Authors:Ji-Shun Lian, Ze-Rui Wang, Jin Zhang</strong></p>
<p>The spectral energy distributions (SEDs) of certain BL Lac objects (BL Lacs) exhibit an additional hard $\gamma$-ray component in the TeV energy range that surpasses the predictions of the one-zone leptonic jet model. The origin of this excess emission remains unclear. In this study, we selected five BL Lacs whose SEDs display a very hard intrinsic spectrum in the TeV band and successfully reproduced their broadband SEDs using a two-zone lepto-hadronic model. Within this framework, the emission observed in the optical, X-ray, GeV $\gamma$-ray, and sub-TeV $\gamma$-ray bands is modeled using the synchrotron and synchrotron self-Compton radiation processes of the relativistic electrons in the jets. Meanwhile, the TeV excess is attributed to $\gamma$-ray emission resulting from the photomeson ($p\gamma$) process via $\pi^0$ decay occurring within advection-dominated accretion flows (ADAFs). This scenario requires a hard proton spectrum with a spectral index of $p \sim 1.6-1.7$ and a cutoff energy ranging from 30 to 90 TeV, as well as a relatively large ADAF radius. Such hard proton spectra suggest that the dominant acceleration mechanisms are likely magnetic reconnection and&#x2F;or stochastic acceleration processes within ADAFs. Additionally, the emission from the cascaded electrons results in a bump in the keVâ€“MeV band; however, it is overwhelmed by the jet emission. Although the hadronuclear ($pp$) process cannot be entirely ruled out, it would necessitate an even harder proton spectrum and a higher cutoff energy compared to the $p\gamma$ process, making it a less favorable explanation for the observed TeV excess. </p>
<blockquote>
<p>æŸäº›BL Lacå¤©ä½“ï¼ˆBL Lacsï¼‰çš„è°±èƒ½é‡åˆ†å¸ƒï¼ˆSEDsï¼‰åœ¨TeVèƒ½é‡èŒƒå›´å†…è¡¨ç°å‡ºé¢å¤–çš„ç¡¬Î³å°„çº¿æˆåˆ†ï¼Œè¿™è¶…å‡ºäº†å•åŒºleptonic jetæ¨¡å‹çš„é¢„æµ‹ã€‚è¿™ç§è¿‡é‡å‘å°„çš„èµ·æºä»ç„¶ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©äº†äº”ä¸ªSEDsåœ¨TeVæ³¢æ®µè¡¨ç°å‡ºéå¸¸ç¡¬çš„å›ºæœ‰è°±çš„BL Lacï¼Œå¹¶æˆåŠŸåœ°ä½¿ç”¨ä¸¤åŒºlepto-hadronicæ¨¡å‹å†ç°äº†å®ƒä»¬çš„å®½å¸¦SEDsã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œé€šè¿‡jetä¸­çš„ç›¸å¯¹è®ºæ€§ç”µå­çš„åŒæ­¥åŠ é€Ÿå’ŒåŒæ­¥è‡ªåº·æ™®é¡¿è¾å°„è¿‡ç¨‹ï¼Œå¯¹å…‰å­¦ã€Xå°„çº¿ã€ä¼½é©¬å°„çº¿ä»¥åŠäºšTeVä¼½é©¬å°„çº¿æ³¢æ®µè§‚æµ‹åˆ°çš„å‘å°„è¿›è¡Œäº†å»ºæ¨¡ã€‚åŒæ—¶ï¼Œå°†TeVè¿‡å‰©å½’å› äºå…‰ä»‹å­ï¼ˆpÎ³ï¼‰è¿‡ç¨‹çš„ä¼½é©¬å°„çº¿å‘å°„ï¼Œè¿™æ˜¯é€šè¿‡advection-dominated accretion flowsï¼ˆADAFsï¼‰å†…çš„Ï€Â°è¡°å˜å‘ç”Ÿçš„ã€‚è¿™ç§æƒ…å†µéœ€è¦å…·æœ‰è°±æŒ‡æ•°pï½1.6-1.7å’Œæˆªæ­¢èƒ½é‡èŒƒå›´åœ¨30åˆ°90 TeVä¹‹é—´çš„ç¡¬è´¨å­è°±ï¼Œä»¥åŠç›¸å¯¹è¾ƒå¤§çš„ADAFåŠå¾„ã€‚è¿™ç§ç¡¬è´¨å­è°±è¡¨æ˜ï¼Œä¸»è¦çš„åŠ é€Ÿæœºåˆ¶å¯èƒ½æ˜¯ADAFsä¸­çš„ç£é‡è”å’Œ&#x2F;æˆ–éšæœºåŠ é€Ÿè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæ¥è‡ªçº§è”ç”µå­çš„å‘å°„åœ¨keV-MeVæ³¢æ®µäº§ç”Ÿäº†ä¸€ä¸ªå‡¸èµ·ï¼Œä½†å®ƒè¢«jetå‘å°„æ‰€æ·¹æ²¡ã€‚è™½ç„¶æ ¸å­ï¼ˆppï¼‰è¿‡ç¨‹ä¸èƒ½å®Œå…¨æ’é™¤ï¼Œä½†å®ƒéœ€è¦æ›´ç¡¬çš„è´¨å­è°±å’Œæ›´é«˜çš„æˆªæ­¢èƒ½é‡ä¸pÎ³è¿‡ç¨‹ç›¸æ¯”ï¼Œè¿™ä½¿å¾—å®ƒæˆä¸ºè§£é‡Šè§‚å¯Ÿåˆ°çš„TeVè¿‡å‰©çš„æ›´ä¸åˆé€‚çš„é€‰é¡¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04202v1">PDF</a> 13 pages, 6 Figures, 1 Table. Accepted for publication in ApJ</p>
<p><strong>Summary</strong><br>    æŸäº›BL Lacå¤©ä½“ï¼ˆBL Lacsï¼‰çš„è°±èƒ½é‡åˆ†å¸ƒï¼ˆSEDsï¼‰åœ¨TeVèƒ½é‡èŒƒå›´å†…è¡¨ç°å‡ºè¶…å‡ºå•åŒºè±æ™®é¡¿å–·å°„æ¨¡å‹çš„é¢„æµ‹ä¹‹å¤–çš„ç¡¬Î³å°„çº¿æˆåˆ†ã€‚æœ¬ç ”ç©¶é€‰ç”¨äº”ä¸ªSEDsåœ¨TeVæ³¢æ®µå…·æœ‰éå¸¸ç¡¬æœ¬åº•è°±çš„BL Lacsï¼Œå¹¶ä½¿ç”¨ä¸¤åŒºè±æ™®æ‰˜å“ˆå¾·æ¨¡å‹æˆåŠŸå¤ç°äº†å…¶å®½å¸¦SEDsã€‚å…¶ä¸­ï¼Œå…‰å­¦ã€Xå°„çº¿ã€GeV Î³å°„çº¿å’ŒäºšTeV Î³å°„çº¿æ³¢æ®µå†…çš„å‘å°„è¢«å»ºæ¨¡ä¸ºå–·å°„ä¸­çš„ç›¸å¯¹è®ºæ€§ç”µå­çš„åŒæ­¥è¾å°„å’ŒåŒæ­¥è‡ªåº·æ™®é¡¿è¾å°„è¿‡ç¨‹ã€‚è€ŒTeVè¿‡å‰©åˆ™è¢«å½’å› äºå…‰ä»‹å­ï¼ˆpÎ³ï¼‰è¿‡ç¨‹ä¸­Ï€^0è¡°å˜äº§ç”Ÿçš„Î³å°„çº¿å‘å°„ï¼Œè¯¥è¿‡ç¨‹å‘ç”Ÿåœ¨ADAFsä¸­ã€‚è¿™ä¸€æƒ…æ™¯éœ€è¦å…·æœ‰è°±æŒ‡æ•°p~1.6-1.7å’Œæˆªæ–­èƒ½é‡åœ¨30åˆ°90TeVä¹‹é—´çš„ç¡¬è´¨å­è°±ï¼Œä»¥åŠç›¸å¯¹è¾ƒå¤§çš„ADAFåŠå¾„ã€‚æ­¤å¤–ï¼Œçº§è”ç”µå­çš„å‘å°„ä¼šåœ¨keV-MeVæ³¢æ®µäº§ç”Ÿä¸€ä¸ªå‡¸èµ·ï¼Œä½†è¢«å–·å°„å‘å°„æ‰€æ©ç›–ã€‚å°½ç®¡æ ¸å­ï¼ˆppï¼‰è¿‡ç¨‹ä¸èƒ½å®Œå…¨æ’é™¤ï¼Œä½†å®ƒéœ€è¦æ›´ç¡¬çš„è´¨å­è°±å’Œæ›´é«˜çš„æˆªæ–­èƒ½é‡ï¼Œå› æ­¤ç›¸æ¯”pÎ³è¿‡ç¨‹ï¼Œå®ƒå¯¹äºè§‚å¯Ÿåˆ°çš„TeVè¿‡å‰©çš„è§£é‡Šä¸å¤ªæœ‰åˆ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BL Lacå¤©ä½“çš„è°±èƒ½é‡åˆ†å¸ƒè¡¨ç°å‡ºTeVèƒ½é‡èŒƒå›´å†…çš„ç¡¬Î³å°„çº¿æˆåˆ†ï¼Œè¶…å‡ºå•åŒºè±æ™®é¡¿å–·å°„æ¨¡å‹çš„é¢„æµ‹ã€‚</li>
<li>ä¸¤åŒºè±æ™®æ‰˜å“ˆå¾·æ¨¡å‹æˆåŠŸå¤ç°äº†å…·æœ‰ç¡¬æœ¬åº•è°±çš„BL Lacsçš„å®½å¸¦SEDsã€‚</li>
<li>å…‰å­¦ã€Xå°„çº¿ã€GeV Î³å°„çº¿å’ŒäºšTeV Î³å°„çº¿æ³¢æ®µçš„å‘å°„å¯é€šè¿‡åŒæ­¥è¾å°„å’ŒåŒæ­¥è‡ªåº·æ™®é¡¿è¾å°„è¿‡ç¨‹å»ºæ¨¡ã€‚</li>
<li>TeVè¿‡å‰©å½’å› äºADAFsä¸­çš„å…‰ä»‹å­ï¼ˆpÎ³ï¼‰è¿‡ç¨‹äº§ç”Ÿçš„Î³å°„çº¿å‘å°„ã€‚</li>
<li>æ­¤æƒ…æ™¯éœ€è¦å…·æœ‰ç‰¹å®šè°±æŒ‡æ•°å’Œæˆªæ–­èƒ½é‡çš„ç¡¬è´¨å­è°±ï¼Œä»¥åŠè¾ƒå¤§çš„ADAFåŠå¾„ã€‚</li>
<li>çº§è”ç”µå­åœ¨keV-MeVæ³¢æ®µçš„å‘å°„è¡¨ç°ä¸ºå‡¸èµ·ï¼Œä½†è¢«å–·å°„å‘å°„æ‰€æ©ç›–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-087d8554382849097c6f1e0c295597e2" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Covariance-Descriptors-Meet-General-Vision-Encoders-Riemannian-Deep-Learning-for-Medical-Image-Classification"><a href="#Covariance-Descriptors-Meet-General-Vision-Encoders-Riemannian-Deep-Learning-for-Medical-Image-Classification" class="headerlink" title="Covariance Descriptors Meet General Vision Encoders: Riemannian Deep   Learning for Medical Image Classification"></a>Covariance Descriptors Meet General Vision Encoders: Riemannian Deep   Learning for Medical Image Classification</h2><p><strong>Authors:Josef Mayr, Anna Reithmeir, Maxime Di Folco, Julia A. Schnabel</strong></p>
<p>Covariance descriptors capture second-order statistics of image features. They have shown strong performance in general computer vision tasks, but remain underexplored in medical imaging. We investigate their effectiveness for both conventional and learning-based medical image classification, with a particular focus on SPDNet, a classification network specifically designed for symmetric positive definite (SPD) matrices. We propose constructing covariance descriptors from features extracted by pre-trained general vision encoders (GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and MedSAM - are evaluated across eleven binary and multi-class datasets from the MedMNSIT benchmark. Our results show that covariance descriptors derived from GVE features consistently outperform those derived from handcrafted features. Moreover, SPDNet yields superior performance to state-of-the-art methods when combined with DINOv2 features. Our findings highlight the potential of combining covariance descriptors with powerful pretrained vision encoders for medical image analysis. </p>
<blockquote>
<p>åæ–¹å·®æè¿°ç¬¦èƒ½å¤Ÿæ•æ‰å›¾åƒç‰¹å¾çš„äºŒé˜¶ç»Ÿè®¡ä¿¡æ¯ã€‚å®ƒä»¬åœ¨ä¸€èˆ¬çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨åŒ»å­¦å½±åƒä¸­ä»ç„¶è¢«è¾ƒå°‘ç ”ç©¶ã€‚æˆ‘ä»¬ç ”ç©¶äº†å®ƒä»¬åœ¨ä¼ ç»Ÿå’ŒåŸºäºå­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯å…³æ³¨ä¸“ä¸ºå¯¹ç§°æ­£å®šï¼ˆSPDï¼‰çŸ©é˜µè®¾è®¡çš„åˆ†ç±»ç½‘ç»œSPDNetã€‚æˆ‘ä»¬æå‡ºä»é¢„è®­ç»ƒçš„é€šç”¨è§†è§‰ç¼–ç å™¨ï¼ˆGVEï¼‰æå–çš„ç‰¹å¾æ„å»ºåæ–¹å·®æè¿°ç¬¦ï¼Œå¹¶ä¸æ‰‹å·¥æè¿°ç¬¦è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨æ¥è‡ªMedMNSITåŸºå‡†çš„11ä¸ªäºŒè¿›ä½å’Œå¤šç±»åˆ«æ•°æ®é›†ä¸­è¯„ä¼°äº†DINOv2å’ŒMedSAMä¸¤ç§GVEã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä»GVEç‰¹å¾æ´¾ç”Ÿçš„åæ–¹å·®æè¿°ç¬¦å§‹ç»ˆä¼˜äºä»æ‰‹å·¥ç‰¹å¾æ´¾ç”Ÿçš„æè¿°ç¬¦ã€‚æ­¤å¤–ï¼Œå½“ä¸DINOv2ç‰¹å¾ç»“åˆæ—¶ï¼ŒSPDNetçš„æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†å°†åæ–¹å·®æè¿°ç¬¦ä¸å¼ºå¤§çš„é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ç›¸ç»“åˆåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04190v1">PDF</a> Preprint. Submitted to the IEEE International Symposium on Biomedical   Imaging (ISBI) 2026</p>
<p><strong>Summary</strong><br>     åæ–¹å·®æè¿°ç¬¦èƒ½æ•æ‰å›¾åƒç‰¹å¾çš„ç¬¬äºŒé˜¶ç»Ÿè®¡ä¿¡æ¯ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åŒ»å­¦æˆåƒä¸­å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬ç ”ç©¶æ¢è®¨å…¶åœ¨ä¼ ç»Ÿå’ŒåŸºäºå­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯ä¸ºå¯¹ç§°æ­£å®šçŸ©é˜µè®¾è®¡çš„SPDNetã€‚ç ”ç©¶æå‡ºæ„å»ºæ¥æºäºé¢„è®­ç»ƒé€šç”¨è§†è§‰ç¼–ç å™¨ç‰¹å¾çš„åæ–¹å·®æè¿°ç¬¦ï¼Œå¹¶ä¸æ‰‹å·¥æè¿°ç¬¦è¿›è¡Œæ¯”è¾ƒã€‚åœ¨MedMNSITåŸºå‡†çš„å¤šä¸ªäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»æ•°æ®é›†ä¸Šè¯„ä¼°äº†DINOv2å’ŒMedSAMä¸¤ç§ç¼–ç å™¨ï¼Œç»“æœæ˜¾ç¤ºæ¥æºäºç¼–ç å™¨ç‰¹å¾çš„åæ–¹å·®æè¿°ç¬¦è¡¨ç°ä¼˜äºæ‰‹å·¥ç‰¹å¾ï¼Œç»“åˆSPDNetå’ŒDINOv2ç‰¹å¾èƒ½è¾¾åˆ°ä¸šç•Œå‰æ²¿æ°´å¹³ã€‚è¿™å‡¸æ˜¾äº†å°†åæ–¹å·®æè¿°ç¬¦ä¸å¼ºå¤§é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ç»“åˆåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åæ–¹å·®æè¿°ç¬¦åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åŒ»å­¦æˆåƒä¸­åº”ç”¨è¾ƒå°‘ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†åæ–¹å·®æè¿°ç¬¦åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿå’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•ã€‚</li>
<li>SPDNetæ˜¯å¯¹å¯¹ç§°æ­£å®šçŸ©é˜µè®¾è®¡çš„åˆ†ç±»ç½‘ç»œï¼Œç ”ç©¶ä¸­å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>æå‡ºåˆ©ç”¨é¢„è®­ç»ƒçš„é€šç”¨è§†è§‰ç¼–ç å™¨ï¼ˆGVEï¼‰æ„å»ºåæ–¹å·®æè¿°ç¬¦ã€‚</li>
<li>ä¸æ‰‹å·¥æè¿°ç¬¦ç›¸æ¯”ï¼Œæ¥æºäºé¢„è®­ç»ƒé€šç”¨è§†è§‰ç¼–ç å™¨çš„åæ–¹å·®æè¿°ç¬¦è¡¨ç°æ›´ä½³ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè¯„ä¼°äº†DINOv2å’ŒMedSAMä¸¤ç§ç¼–ç å™¨ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b802f8031acdff12033229c0246fe264" align="middle">
<img src="https://picx.zhimg.com/v2-3c462703f97bfc51702061229de0a556" align="middle">
<img src="https://picx.zhimg.com/v2-849199be0e9831c44f0b250b4a44aac6" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Systematic-Evaluation-of-Preprocessing-Techniques-for-Accurate-Image-Registration-in-Digital-Pathology"><a href="#Systematic-Evaluation-of-Preprocessing-Techniques-for-Accurate-Image-Registration-in-Digital-Pathology" class="headerlink" title="Systematic Evaluation of Preprocessing Techniques for Accurate Image   Registration in Digital Pathology"></a>Systematic Evaluation of Preprocessing Techniques for Accurate Image   Registration in Digital Pathology</h2><p><strong>Authors:Fatemehzahra Darzi, Rodrigo Escobar Diaz Guerrero, Thomas Bocklitz</strong></p>
<p>Image registration refers to the process of spatially aligning two or more images by mapping them into a common coordinate system, so that corresponding anatomical or tissue structures are matched across images. In digital pathology, registration enables direct comparison and integration of information from different stains or imaging modalities, sup-porting applications such as biomarker analysis and tissue reconstruction. Accurate registration of images from different modalities is an essential step in digital pathology. In this study, we investigated how various color transformation techniques affect image registration between hematoxylin and eosin (H&amp;E) stained images and non-linear multimodal images. We used a dataset of 20 tissue sample pairs, with each pair undergoing several preprocessing steps, including different color transformation (CycleGAN, Macenko, Reinhard, Vahadane), inversion, contrast adjustment, intensity normalization, and denoising. All images were registered using the VALIS registration method, which first applies rigid registration and then performs non-rigid registration in two steps on both low and high-resolution images. Registration performance was evaluated using the relative Target Registration Error (rTRE). We reported the median of median rTRE values (MMrTRE) and the average of median rTRE values (AMrTRE) for each method. In addition, we performed a custom point-based evaluation using ten manually selected key points. Registration was done separately for two scenarios, using either the original or inverted multimodal images. In both scenarios, CycleGAN color transformation achieved the lowest registration errors, while the other methods showed higher errors. These findings show that applying color transformation before registration improves alignment between images from different modalities and supports more reliable analysis in digital pathology. </p>
<blockquote>
<p>å›¾åƒé…å‡†æ˜¯æŒ‡é€šè¿‡æ˜ å°„åˆ°åŒä¸€åæ ‡ç³»ï¼Œå°†ä¸¤ä¸ªæˆ–å¤šä¸ªå›¾åƒåœ¨ç©ºé—´ä¸Šè¿›è¡Œå¯¹é½çš„è¿‡ç¨‹ï¼Œä»è€Œä½¿ç›¸åº”çš„è§£å‰–æˆ–ç»„ç»‡ç»“æ„åœ¨å›¾åƒä¹‹é—´åŒ¹é…ã€‚åœ¨æ•°å­—ç—…ç†å­¦ä¸­ï¼Œé…å‡†èƒ½å¤Ÿç›´æ¥æ¯”è¾ƒå¹¶æ•´åˆæ¥è‡ªä¸åŒæŸ“è‰²æˆ–æˆåƒæ¨¡å¼çš„ä¿¡æ¯ï¼Œæ”¯æŒç”Ÿç‰©æ ‡å¿—ç‰©åˆ†æå’Œç»„ç»‡é‡å»ºç­‰åº”ç”¨ã€‚ä¸åŒæ¨¡å¼çš„å›¾åƒå‡†ç¡®é…å‡†æ˜¯æ•°å­—ç—…ç†å­¦ä¸­çš„åŸºæœ¬æ­¥éª¤ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å„ç§é¢œè‰²è½¬æ¢æŠ€æœ¯å¦‚ä½•å½±å“è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²å›¾åƒä¸éçº¿æ€§å¤šæ¨¡æ€å›¾åƒä¹‹é—´çš„å›¾åƒé…å‡†ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«20ä¸ªç»„ç»‡æ ·æœ¬å¯¹çš„æ•°æ®é›†ï¼Œæ¯ä¸ªæ ·æœ¬å¯¹éƒ½è¦ç»å†å‡ ä¸ªé¢„å¤„ç†æ­¥éª¤ï¼ŒåŒ…æ‹¬ä¸åŒçš„é¢œè‰²è½¬æ¢ï¼ˆCycleGANã€Macenkoã€Reinhardã€Vahadaneï¼‰ã€åè½¬ã€å¯¹æ¯”åº¦è°ƒæ•´ã€å¼ºåº¦å½’ä¸€åŒ–å’Œå»å™ªã€‚æ‰€æœ‰å›¾åƒå‡ä½¿ç”¨VALISé…å‡†æ–¹æ³•è¿›è¡Œé…å‡†ï¼Œè¯¥æ–¹æ³•é¦–å…ˆåº”ç”¨åˆšæ€§é…å‡†ï¼Œç„¶ååœ¨é«˜ä½åˆ†è¾¨ç‡å›¾åƒä¸Šåˆ†ä¸ºä¸¤æ­¥æ‰§è¡Œéåˆšæ€§é…å‡†ã€‚ä½¿ç”¨ç›¸å¯¹ç›®æ ‡é…å‡†è¯¯å·®ï¼ˆrTREï¼‰æ¥è¯„ä¼°é…å‡†æ€§èƒ½ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†æ¯ç§æ–¹æ³•çš„å¹³å‡ä¸­ä½rTREå€¼ï¼ˆMMrTREï¼‰å’Œä¸­ä½æ•°çš„ä¸­ä½rTREå€¼ï¼ˆAMrTREï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†åŸºäºåä¸ªæ‰‹åŠ¨é€‰æ‹©çš„å…³é”®ç‚¹çš„è‡ªå®šä¹‰ç‚¹è¯„ä¼°æ³•ã€‚é…å‡†æ˜¯é’ˆå¯¹ä¸¤ç§æƒ…æ™¯åˆ†åˆ«è¿›è¡Œçš„ï¼Œä½¿ç”¨åŸå§‹æˆ–åè½¬çš„å¤šæ¨¡æ€å›¾åƒã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼ŒCycleGANé¢œè‰²è½¬æ¢éƒ½å®ç°äº†æœ€ä½çš„é…å‡†è¯¯å·®ï¼Œè€Œå…¶ä»–æ–¹æ³•æ˜¾ç¤ºå‡ºæ›´é«˜çš„è¯¯å·®ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåœ¨é…å‡†ä¹‹å‰åº”ç”¨é¢œè‰²è½¬æ¢å¯æ”¹å–„ä¸åŒæ¨¡æ€å›¾åƒä¹‹é—´çš„å¯¹é½ï¼Œå¹¶æ”¯æŒæ•°å­—ç—…ç†å­¦ä¸­è¿›è¡Œæ›´å¯é çš„åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04171v1">PDF</a> 14 pages, 7 Figures</p>
<p><strong>Summary</strong><br>    å›¾åƒé…å‡†æ˜¯é€šè¿‡å°†ä¸¤å¹…æˆ–å¤šå¹…å›¾åƒæ˜ å°„åˆ°åŒä¸€åæ ‡ç³»ä¸­ï¼Œå®ç°ç©ºé—´å¯¹é½çš„è¿‡ç¨‹ï¼Œä»è€Œä½¿ä¸åŒå›¾åƒä¸­çš„ç›¸åº”è§£å‰–æˆ–ç»„ç»‡ç»“æ„ç›¸åŒ¹é…ã€‚åœ¨æ•°å­—ç—…ç†å­¦ä¸­ï¼Œé…å‡†èƒ½å¤Ÿç›´æ¥æ¯”è¾ƒå’Œæ•´åˆä¸åŒæŸ“è‰²æˆ–æˆåƒæ–¹å¼çš„ä¿¡æ¯ï¼Œæ”¯æŒç”Ÿç‰©æ ‡å¿—ç‰©åˆ†æå’Œç»„ç»‡é‡å»ºç­‰åº”ç”¨ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¸åŒçš„è‰²å½©è½¬æ¢æŠ€æœ¯å¦‚ä½•å½±å“è‹æœ¨ç²¾å’Œä¼Šçº¢æŸ“è‰²å›¾åƒä¸éçº¿æ€§å¤šæ¨¡æ€å›¾åƒä¹‹é—´çš„å›¾åƒé…å‡†ã€‚ä½¿ç”¨åŒ…å«20ä¸ªç»„ç»‡æ ·æœ¬å¯¹çš„æ•°æ®åº“ï¼Œç»è¿‡å¤šç§é¢„å¤„ç†æ­¥éª¤å’Œè‰²å½©è½¬æ¢æŠ€æœ¯ï¼Œæ‰€æœ‰å›¾åƒå‡ä½¿ç”¨VALISé…å‡†æ–¹æ³•è¿›è¡Œé…å‡†ã€‚æ€§èƒ½è¯„ä¼°é‡‡ç”¨ç›®æ ‡é…å‡†è¯¯å·®ç›¸å¯¹å€¼ï¼ˆrTREï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼ŒCycleGANè‰²å½©è½¬æ¢åœ¨é…å‡†è¯¯å·®æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€Œå…¶ä»–æ–¹æ³•è¯¯å·®è¾ƒé«˜ã€‚è¿™è¡¨æ˜åœ¨é…å‡†ä¹‹å‰åº”ç”¨è‰²å½©è½¬æ¢å¯ä»¥æ”¹è¿›ä¸åŒæ¨¡æ€å›¾åƒä¹‹é—´çš„å¯¹é½ï¼Œå¹¶æ”¯æŒæ›´å¯é çš„åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒé…å‡†æ˜¯æ•°å­—ç—…ç†å­¦ä¸­è‡³å…³é‡è¦çš„æ­¥éª¤ï¼Œå®ƒå…è®¸ä¸åŒæŸ“è‰²æˆ–æˆåƒæ–¹å¼çš„ä¿¡æ¯è¿›è¡Œæ¯”è¾ƒå’Œæ•´åˆã€‚</li>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†è‰²å½©è½¬æ¢æŠ€æœ¯å¯¹å›¾åƒé…å‡†çš„å½±å“ã€‚</li>
<li>ä½¿ç”¨åŒ…å«20ä¸ªç»„ç»‡æ ·æœ¬å¯¹çš„æ•°æ®åº“è¿›è¡Œå®éªŒã€‚</li>
<li>æ‰€æœ‰çš„å›¾åƒéƒ½ä½¿ç”¨VALISé…å‡†æ–¹æ³•è¿›è¡Œé…å‡†ï¼Œè¯¥æ–¹æ³•é¦–å…ˆåº”ç”¨åˆšæ€§é…å‡†ï¼Œç„¶ååœ¨é«˜ä½åˆ†è¾¨ç‡å›¾åƒä¸Šè¿›è¡Œä¸¤æ­¥éåˆšæ€§é…å‡†ã€‚</li>
<li>æ€§èƒ½è¯„ä¼°é‡‡ç”¨ç›¸å¯¹ç›®æ ‡æ³¨å†Œè¯¯å·®ï¼ˆrTREï¼‰ã€‚</li>
<li>CycleGANè‰²å½©è½¬æ¢åœ¨é…å‡†è¿‡ç¨‹ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1c94a4fa401732ffe92634b51602eab" align="middle">
<img src="https://picx.zhimg.com/v2-83f9fe387a42438e3814dc397227cd97" align="middle">
<img src="https://picx.zhimg.com/v2-9b361514b715be6afafb56532c7619f0" align="middle">
<img src="https://picx.zhimg.com/v2-b7a41546094bbda49d0115122dad3f4f" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="When-Swin-Transformer-Meets-KANs-An-Improved-Transformer-Architecture-for-Medical-Image-Segmentation"><a href="#When-Swin-Transformer-Meets-KANs-An-Improved-Transformer-Architecture-for-Medical-Image-Segmentation" class="headerlink" title="When Swin Transformer Meets KANs: An Improved Transformer Architecture   for Medical Image Segmentation"></a>When Swin Transformer Meets KANs: An Improved Transformer Architecture   for Medical Image Segmentation</h2><p><strong>Authors:Nishchal Sapkota, Haoyan Shi, Yejia Zhang, Xianshi Ma, Bofang Zheng, Danny Z. Chen</strong></p>
<p>Medical image segmentation is critical for accurate diagnostics and treatment planning, but remains challenging due to complex anatomical structures and limited annotated training data. CNN-based segmentation methods excel at local feature extraction, but struggle with modeling long-range dependencies. Transformers, on the other hand, capture global context more effectively, but are inherently data-hungry and computationally expensive. In this work, we introduce UKAST, a U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By leveraging rational base functions and Group Rational KANs (GR-KANs) from the Kolmogorov-Arnold Transformer (KAT), our architecture addresses the inefficiencies of vanilla spline-based KANs, yielding a more expressive and data-efficient framework with reduced FLOPs and only a very small increase in parameter count compared to SwinUNETR. UKAST achieves state-of-the-art performance on four diverse 2D and 3D medical image segmentation benchmarks, consistently surpassing both CNN- and Transformer-based baselines. Notably, it attains superior accuracy in data-scarce settings, alleviating the data-hungry limitations of standard Vision Transformers. These results show the potential of KAN-enhanced Transformers to advance data-efficient medical image segmentation. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/nsapkota417/UKAST">https://github.com/nsapkota417/UKAST</a> </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºå‡†ç¡®çš„è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆè‡³å…³é‡è¦ï¼Œä½†ç”±äºå¤æ‚çš„è§£å‰–ç»“æ„å’Œæœ‰é™çš„æ ‡æ³¨è®­ç»ƒæ•°æ®ï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åŸºäºCNNçš„åˆ†å‰²æ–¹æ³•åœ¨å±€éƒ¨ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»æ–¹é¢å´é‡åˆ°å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼Œå˜å‹å™¨æ›´æœ‰æ•ˆåœ°æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†æœ¬è´¨ä¸Šéœ€è¦å¤§é‡çš„æ•°æ®å’Œæ˜‚è´µçš„è®¡ç®—èµ„æºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†UKASTï¼Œè¿™æ˜¯ä¸€ç§ç±»ä¼¼äºU-Netçš„æ¶æ„ï¼Œå®ƒå°†åŸºäºæœ‰ç†å‡½æ•°çš„Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰é›†æˆåˆ°Swin Transformerç¼–ç å™¨ã€‚é€šè¿‡åˆ©ç”¨Kolmogorov-Arnold Transformerï¼ˆKATï¼‰ä¸­çš„æœ‰ç†åŸºå‡½æ•°å’Œç»„æœ‰ç†KANsï¼ˆGR-KANsï¼‰ï¼Œæˆ‘ä»¬çš„æ¶æ„è§£å†³äº†æ™®é€šæ ·æ¡æ’å€¼åŸºKANsçš„æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œäº§ç”Ÿäº†ä¸€ä¸ªæ›´å…·è¡¨ç°åŠ›å’Œæ•°æ®é«˜æ•ˆçš„æ¡†æ¶ï¼Œä¸SwinUNETRç›¸æ¯”ï¼Œæµ®ç‚¹è¿ç®—å‡å°‘ï¼Œå‚æ•°è®¡æ•°ä»…ç•¥æœ‰å¢åŠ ã€‚UKASTåœ¨å››ä¸ªä¸åŒçš„2Då’Œ3DåŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸€è‡´è¶…è¶Šäº†åŸºäºCNNå’ŒTransformerçš„åŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸­è¾¾åˆ°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œç¼“è§£äº†æ ‡å‡†è§†è§‰å˜å‹å™¨å¯¹æ•°æ®çš„éœ€æ±‚é™åˆ¶ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†å¢å¼ºå‹KAN Transformeråœ¨æ•°æ®é«˜æ•ˆåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://github.com/nsapkota417/UKAST]%EF%BC%88%E4%B8%AD%E6%96%87%E9%93%BE%E6%8E%A5%E8%AF%B7%E8%87%AA%E8%A1%8C%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%AF%B9%E5%BA%94%E4%B8%AD%E6%96%87%E7%BD%91%E7%AB%99%EF%BC%89">https://github.com/nsapkota417/UKAST]ï¼ˆä¸­æ–‡é“¾æ¥è¯·è‡ªè¡Œè½¬æ¢ä¸ºå¯¹åº”ä¸­æ–‡ç½‘ç«™ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04084v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„é‡è¦æ€§å’ŒæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚çš„è§£å‰–ç»“æ„å’Œæœ‰é™çš„æ ‡æ³¨è®­ç»ƒæ•°æ®ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„U-Netæ¶æ„UKASTï¼Œå®ƒç»“åˆäº†Kolmogorov-Arnold Networksï¼ˆKANsï¼‰å’ŒSwin Transformerç¼–ç å™¨ã€‚UKASTå…·æœ‰æ›´å¥½çš„è¡¨è¾¾èƒ½åŠ›å’Œæ•°æ®æ•ˆç‡ï¼Œé™ä½äº†è®¡ç®—é‡å¹¶å¢åŠ äº†è¾ƒå°‘çš„å‚æ•°ã€‚å®ƒåœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸‹è¡¨ç°å‡ºä¼˜è¶Šçš„å‡†ç¡®æ€§ã€‚è¿™ä¸ºæ¨åŠ¨æ•°æ®é«˜æ•ˆåŒ»å­¦å›¾åƒåˆ†å‰²çš„å‘å±•æä¾›äº†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹å‡†ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´å¤æ‚è§£å‰–ç»“æ„å’Œæœ‰é™è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>CNNå’ŒTransformeråœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å„æœ‰ä¼˜åŠ¿ä¸ä¸è¶³ã€‚CNNæ“…é•¿å±€éƒ¨ç‰¹å¾æå–ï¼Œä½†éš¾ä»¥å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼›è€ŒTransformerèƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†æ•°æ®éœ€æ±‚é‡å¤§ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>UKASTæ˜¯ä¸€ä¸ªæ–°å‹çš„U-Netæ¶æ„ï¼Œç»“åˆäº†Kolmogorov-Arnold Networksï¼ˆKANsï¼‰å’ŒSwin Transformerç¼–ç å™¨ã€‚</li>
<li>UKASTé€šè¿‡åˆ©ç”¨ç†æ€§åŸºç¡€å‡½æ•°å’ŒGroup Rational KANsï¼ˆGR-KANsï¼‰ï¼Œè§£å†³äº†å¸¸è§„spline-based KANsçš„æ•ˆç‡é—®é¢˜ã€‚</li>
<li>UKASTåœ¨å››ä¸ªä¸åŒçš„åŒ»å­¦å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸‹è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>UKASTä»£ç å·²å…¬å¼€ï¼Œä¸ºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨æä¾›äº†ä¾¿åˆ©ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dedcd24a39bb49dee41d337393e66f94" align="middle">
<img src="https://picx.zhimg.com/v2-0136c3155a1ee7ee4d8152ebb998a48f" align="middle">
<img src="https://picx.zhimg.com/v2-ace62025e26b7f444723718a4bc22705" align="middle">
<img src="https://picx.zhimg.com/v2-c7a1475396396d615c542596488d4374" align="middle">
<img src="https://picx.zhimg.com/v2-f85730bfb70b3b74cd4e10f3961e5ef3" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Adversarial-and-Score-Based-CT-Denoising-CycleGAN-vs-Noise2Score"><a href="#Adversarial-and-Score-Based-CT-Denoising-CycleGAN-vs-Noise2Score" class="headerlink" title="Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score"></a>Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score</h2><p><strong>Authors:Abu Hanif Muhammad Syarubany</strong></p>
<p>We study CT image denoising in the unpaired and self-supervised regimes by evaluating two strong, training-data-efficient paradigms: a CycleGAN-based residual translator and a Noise2Score (N2S) score-matching denoiser. Under a common evaluation protocol, a configuration sweep identifies a simple standard U-Net backbone within CycleGAN (lambda_cycle &#x3D; 30, lambda_iden &#x3D; 2, ngf &#x3D; ndf &#x3D; 64) as the most reliable setting; we then train it to convergence with a longer schedule. The selected CycleGAN improves the noisy input from 34.66 dB &#x2F; 0.9234 SSIM to 38.913 dB &#x2F; 0.971 SSIM and attains an estimated score of 1.9441 and an unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly behind in absolute PSNR &#x2F; SSIM, achieves large gains over very noisy inputs, highlighting its utility when clean pairs are unavailable. Overall, CycleGAN offers the strongest final image quality, whereas Noise2Score provides a robust pair-free alternative with competitive performance. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score">https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†åœ¨æ— é…å¯¹å’Œè‡ªç›‘ç£ç¯å¢ƒä¸‹çš„CTå›¾åƒå»å™ªï¼Œé€šè¿‡è¯„ä¼°ä¸¤ç§å¼ºå¤§ä¸”è®­ç»ƒæ•°æ®é«˜æ•ˆçš„èŒƒå¼ï¼šåŸºäºCycleGANçš„æ®‹å·®ç¿»è¯‘å™¨å’ŒNoise2Scoreï¼ˆN2Sï¼‰å¾—åˆ†åŒ¹é…å»å™ªå™¨ã€‚åœ¨ä¸€ä¸ªå¸¸è§çš„è¯„ä¼°åè®®ä¸‹ï¼Œé…ç½®æ‰«æç¡®å®šäº†CycleGANä¸­ä¸€ä¸ªç®€å•çš„æ ‡å‡†U-Netä¸»å¹²ï¼ˆlambda_cycle &#x3D; 30ï¼Œlambda_iden &#x3D; 2ï¼Œngf &#x3D; ndf &#x3D; 64ï¼‰æ˜¯æœ€å¯é çš„è®¾ç½®ï¼›ç„¶åæˆ‘ä»¬ä½¿ç”¨æ›´é•¿çš„è®¡åˆ’å°†å…¶è®­ç»ƒåˆ°æ”¶æ•›ã€‚æ‰€é€‰çš„CycleGANå°†å™ªå£°è¾“å…¥ä»34.66 dB &#x2F; 0.9234 SSIMæé«˜åˆ°38.913 dB &#x2F; 0.971 SSIMï¼Œä¼°è®¡å¾—åˆ†ä¸º1.9441ï¼Œæœªè§é›†ï¼ˆKaggleæ’è¡Œæ¦œï¼‰å¾—åˆ†ä¸º1.9343ã€‚Noise2Scoreè™½ç„¶åœ¨ç»å¯¹çš„PSNR &#x2F; SSIMä¸Šç•¥é€Šä¸€ç­¹ï¼Œä½†åœ¨å¤„ç†éå¸¸å˜ˆæ‚çš„è¾“å…¥æ—¶å–å¾—äº†å¾ˆå¤§çš„æ”¶ç›Šï¼Œè¿™çªå‡ºäº†å½“æ²¡æœ‰å¹²å‡€çš„é…å¯¹æ—¶å®ƒçš„å®ç”¨æ€§ã€‚æ€»çš„æ¥è¯´ï¼ŒCycleGANæä¾›äº†æœ€å¼ºçš„æœ€ç»ˆå›¾åƒè´¨é‡ï¼Œè€ŒNoise2Scoreåˆ™æä¾›äº†ä¸€ç§æ— éœ€é…å¯¹çš„ç¨³å¥æ›¿ä»£æ–¹æ¡ˆï¼Œè¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Scoreæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04083v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºCycleGANçš„æ®‹å·®ç¿»è¯‘å™¨å’ŒNoise2Scoreï¼ˆN2Sï¼‰å¾—åˆ†åŒ¹é…å»å™ªå™¨åœ¨æ— é…å¯¹å’Œè‡ªæˆ‘ç›‘ç£ä¸‹å¯¹CTå›¾åƒè¿›è¡Œå»å™ªç ”ç©¶ã€‚é€šè¿‡å¯¹ä¸¤ç§é«˜æ•ˆã€è®­ç»ƒæ•°æ®æ•ˆç‡é«˜çš„èŒƒå¼è¿›è¡Œè¯„ä¼°ï¼Œç¡®å®šCycleGANçš„æœ€å¯é é…ç½®ä¸ºç®€å•çš„æ ‡å‡†U-Netéª¨å¹²ç½‘ï¼Œå¹¶å°†å…¶è®­ç»ƒè‡³æ”¶æ•›ã€‚CycleGANæé«˜äº†å™ªå£°è¾“å…¥çš„å›¾åƒè´¨é‡ï¼Œè€ŒNoise2Scoreåœ¨ç»å¯¹PSNR&#x2F;SSIMä¸Šç•¥é€Šä¸€ç­¹ï¼Œä½†åœ¨éå¸¸å˜ˆæ‚çš„è¾“å…¥ä¸Šå–å¾—äº†å¾ˆå¤§è¿›æ­¥ã€‚æ€»ä½“è€Œè¨€ï¼ŒCycleGANæä¾›äº†æœ€å¼ºçš„æœ€ç»ˆå›¾åƒè´¨é‡ï¼Œè€ŒNoise2Scoreæä¾›äº†å…·æœ‰ç«äº‰åŠ›çš„æ— é…å¯¹æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†æ— é…å¯¹å’Œè‡ªæˆ‘ç›‘ç£ä¸‹çš„CTå›¾åƒå»å™ªã€‚</li>
<li>é€šè¿‡è¯„ä¼°ä¸¤ç§è®­ç»ƒæ•°æ®æ•ˆç‡é«˜çš„èŒƒå¼ï¼šåŸºäºCycleGANçš„æ®‹å·®ç¿»è¯‘å™¨å’ŒNoise2Scoreå¾—åˆ†åŒ¹é…å»å™ªå™¨ã€‚</li>
<li>CycleGANçš„æœ€å¯é é…ç½®ä¸ºå…·æœ‰ç®€å•æ ‡å‡†U-Netéª¨å¹²ç½‘ï¼Œå¹¶é€šè¿‡å»¶é•¿è®­ç»ƒæ—¶é—´è¾¾åˆ°æ”¶æ•›ã€‚</li>
<li>CycleGANæ˜¾è‘—æé«˜äº†å™ªå£°è¾“å…¥çš„å›¾åƒè´¨é‡ã€‚</li>
<li>Noise2Scoreåœ¨å˜ˆæ‚è¾“å…¥ä¸Šå–å¾—äº†è¿›æ­¥ï¼Œå°½ç®¡åœ¨ç»å¯¹PSNR&#x2F;SSIMä¸Šç•¥é€ŠäºCycleGANã€‚</li>
<li>CycleGANæœ€ç»ˆå›¾åƒè´¨é‡æœ€å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60a3fda5567fc55c43cac9f4539ed919" align="middle">
<img src="https://picx.zhimg.com/v2-0a65e993a94e06b1ec1489579197e1a0" align="middle">
<img src="https://picx.zhimg.com/v2-40d89437e30139313cf99062429d1ea7" align="middle">
<img src="https://picx.zhimg.com/v2-8540d60f9094d7ee16eef1d0b0461ee4" align="middle">
<img src="https://picx.zhimg.com/v2-05d5d1d4ebb0aab4f105e28ac8f7c898" align="middle">
<img src="https://picx.zhimg.com/v2-cce364903db5fdfe34b63363d428fd5c" align="middle">
<img src="https://picx.zhimg.com/v2-8f4e28747a3ed3a3ab1c7e5105ca3646" align="middle">
<img src="https://picx.zhimg.com/v2-b53771e07f9b4665c85c6db667303a27" align="middle">
<img src="https://picx.zhimg.com/v2-219ecfef3f13fa562248b12d88a1331f" align="middle">
<img src="https://picx.zhimg.com/v2-2d8520eaeb9a6ef41e1a9d989a4d67e9" align="middle">
<img src="https://picx.zhimg.com/v2-863d74107523bf39a5fa6377c694e641" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Improving-the-Performance-of-Radiology-Report-De-identification-with-Large-Scale-Training-and-Benchmarking-Against-Cloud-Vendor-Methods"><a href="#Improving-the-Performance-of-Radiology-Report-De-identification-with-Large-Scale-Training-and-Benchmarking-Against-Cloud-Vendor-Methods" class="headerlink" title="Improving the Performance of Radiology Report De-identification with   Large-Scale Training and Benchmarking Against Cloud Vendor Methods"></a>Improving the Performance of Radiology Report De-identification with   Large-Scale Training and Benchmarking Against Cloud Vendor Methods</h2><p><strong>Authors:Eva Prakash, Maayane Attias, Pierre Chambon, Justin Xu, Steven Truong, Jean-Benoit Delbrouck, Tessa Cook, Curtis Langlotz</strong></p>
<p>Objective: To enhance automated de-identification of radiology reports by scaling transformer-based models through extensive training datasets and benchmarking performance against commercial cloud vendor systems for protected health information (PHI) detection. Materials and Methods: In this retrospective study, we built upon a state-of-the-art, transformer-based, PHI de-identification pipeline by fine-tuning on two large annotated radiology corpora from Stanford University, encompassing chest X-ray, chest CT, abdomen&#x2F;pelvis CT, and brain MR reports and introducing an additional PHI category (AGE) into the architecture. Model performance was evaluated on test sets from Stanford and the University of Pennsylvania (Penn) for token-level PHI detection. We further assessed (1) the stability of synthetic PHI generation using a â€œhide-in-plain-sightâ€ method and (2) performance against commercial systems. Precision, recall, and F1 scores were computed across all PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining the previous state-of-the-art model performance. Synthetic PHI evaluation showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50 independently de-identified Penn datasets. Our model outperformed all vendor systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754). Discussion: Large-scale, multimodal training improved cross-institutional generalization and robustness. Synthetic PHI generation preserved data utility while ensuring privacy. Conclusion: A transformer-based de-identification model trained on diverse radiology datasets outperforms prior academic and commercial systems in PHI detection and establishes a new benchmark for secure clinical text processing. </p>
<blockquote>
<p>ç›®æ ‡ï¼šé€šè¿‡å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†æ‰©å±•åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œå¹¶ä¸å•†ä¸šäº‘ä¾›åº”å•†ç³»ç»Ÿçš„ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰æ£€æµ‹æ€§èƒ½è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»è€Œæé«˜æ”¾å°„å­¦æŠ¥å‘Šè‡ªåŠ¨åŒ–å»æ ‡è¯†åŒ–çš„èƒ½åŠ›ã€‚ææ–™ä¸æ–¹æ³•ï¼šåœ¨è¿™é¡¹å›é¡¾æ€§ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹åœ¨æœ€æ–°ã€åŸºäºå˜å‹å™¨çš„PHIå»æ ‡è¯†åŒ–ç®¡é“ä¸Šï¼Œé€šè¿‡ä¸¤ä¸ªæ¥è‡ªæ–¯å¦ç¦å¤§å­¦çš„æ ‡æ³¨æ”¾å°„å­¦è¯­æ–™åº“è¿›è¡Œå¾®è°ƒï¼Œæ¶µç›–èƒ¸éƒ¨Xå°„çº¿ã€èƒ¸éƒ¨CTã€è…¹éƒ¨&#x2F;éª¨ç›†CTå’Œå¤§è„‘MRæŠ¥å‘Šï¼Œå¹¶å°†é¢å¤–çš„PHIç±»åˆ«ï¼ˆå¹´é¾„ï¼‰å¼•å…¥æ¶æ„ä¸­ã€‚æˆ‘ä»¬åœ¨æ–¯å¦ç¦å’Œå®¾å¤•æ³•å°¼äºšå¤§å­¦ï¼ˆå®¾å¤§ï¼‰çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†æ¨¡å‹åœ¨ä»¤ç‰Œçº§PHIæ£€æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯„ä¼°äº†ï¼ˆ1ï¼‰â€œéšè—äºæ™®é€šè§†é‡ä¸­â€æ–¹æ³•çš„åˆæˆPHIç”Ÿæˆçš„ç¨³å®šæ€§ï¼Œä»¥åŠï¼ˆ2ï¼‰â€œä¸å•†ä¸šç³»ç»Ÿçš„æ€§èƒ½å¯¹æ¯”â€ã€‚è®¡ç®—äº†æ‰€æœ‰PHIç±»åˆ«çš„ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚ç»“æœï¼šæˆ‘ä»¬çš„æ¨¡å‹åœ¨å®¾å¤§æ•°æ®é›†ä¸Šçš„æ€»ä½“F1åˆ†æ•°ä¸º0.973ï¼Œæ–¯å¦ç¦æ•°æ®é›†ä¸Šä¸º0.996ï¼Œä¼˜äºæˆ–ä¿æŒäº†å…ˆå‰çš„æœ€æ–°æ¨¡å‹æ€§èƒ½ã€‚åˆæˆPHIè¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨50ä¸ªç‹¬ç«‹å»æ ‡è¯†åŒ–çš„å®¾å¤§æ•°æ®é›†ä¸­ï¼Œæ£€æµ‹èƒ½åŠ›ä¸€è‡´ï¼ˆæ€»ä½“F1ï¼š0.959 [0.958-0.960]ï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åˆæˆå®¾å¤§æŠ¥å‘Šä¸Šä¼˜äºæ‰€æœ‰ä¾›åº”å•†ç³»ç»Ÿï¼ˆæ€»ä½“F1ï¼š0.960 vs. 0.632-0.754ï¼‰ã€‚è®¨è®ºï¼šå¤§è§„æ¨¡ã€å¤šæ¨¡å¼è®­ç»ƒæé«˜äº†è·¨æœºæ„æ¨å¹¿å’Œç¨³å¥æ€§ã€‚åˆæˆPHIç”Ÿæˆåœ¨ä¿ç•™æ•°æ®å®ç”¨æ€§çš„åŒæ—¶ç¡®ä¿äº†éšç§ã€‚ç»“è®ºï¼šåŸºäºå˜å‹å™¨çš„å»æ ‡è¯†åŒ–æ¨¡å‹ï¼Œç»è¿‡å¤šæ ·åŒ–çš„æ”¾å°„å­¦æ•°æ®é›†è®­ç»ƒï¼Œåœ¨PHIæ£€æµ‹æ–¹é¢ä¼˜äºå…ˆå‰çš„å­¦æœ¯å’Œå•†ä¸šç³»ç»Ÿï¼Œä¸ºå®‰å…¨ä¸´åºŠæ–‡æœ¬å¤„ç†å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04079v1">PDF</a> In submission to JAMIA</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œé€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ”¾å°„å­¦æŠ¥å‘Šè‡ªåŠ¨å»æ ‡è¯†åŒ–çš„æ€§èƒ½ï¼Œå¹¶ä¸å•†ä¸šäº‘ä¾›åº”å•†ç³»ç»Ÿçš„ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰æ£€æµ‹æ€§èƒ½è¿›è¡Œäº†æ¯”è¾ƒã€‚è¯¥æ¨¡å‹åœ¨æ–¯å¦ç¦å’Œå®¾å¤•æ³•å°¼äºšå¤§å­¦æ•°æ®é›†ä¸Šçš„PHIæ£€æµ‹F1å¾—åˆ†è¾ƒé«˜ï¼Œè¶…è¿‡æˆ–ç»´æŒäº†å…ˆå‰æœ€æ–°æ¨¡å‹çš„è¡¨ç°ã€‚åˆæˆPHIè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ç‹¬ç«‹å»æ ‡è¯†çš„å®¾å¤•æ³•å°¼äºšæ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„å¯æ£€æµ‹æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨åˆæˆå®¾å¤•æ³•å°¼äºšæŠ¥å‘Šä¸Šä¼˜äºæ‰€æœ‰ä¾›åº”å•†ç³»ç»Ÿã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤§è§„æ¨¡å¤šæ¨¡å¼è®­ç»ƒæé«˜äº†è·¨æœºæ„æ¨å¹¿å’Œç¨³å¥æ€§ï¼ŒåˆæˆPHIç”Ÿæˆç¡®ä¿äº†æ•°æ®çš„éšç§åŒæ—¶ä¿ç•™å…¶ä½¿ç”¨æ€§ã€‚æœ¬ç ”ç©¶å»ºç«‹çš„åŸºäºå˜å‹å™¨çš„å»æ ‡è¯†åŒ–æ¨¡å‹åœ¨PHIæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºå®‰å…¨ä¸´åºŠæ–‡æœ¬å¤„ç†å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨åŸºäºå˜å‹å™¨çš„æ¨¡å‹è¿›è¡Œæ”¾å°„å­¦æŠ¥å‘Šè‡ªåŠ¨å»æ ‡è¯†åŒ–ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨æ–¯å¦ç¦å’Œå®¾å¤•æ³•å°¼äºšå¤§å­¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†PHIæ£€æµ‹çš„è¯„ä¼°ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åˆæˆPHIè¯„ä¼°æ˜¾ç¤ºäº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>æ¨¡å‹åœ¨è·¨æœºæ„æ¨å¹¿å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>åˆæˆPHIç”ŸæˆæŠ€æœ¯ç¡®ä¿äº†æ•°æ®çš„éšç§æ€§å’Œä½¿ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c40dcf416b9b8572aa389705591ecaac" align="middle">
<img src="https://picx.zhimg.com/v2-08dc0d1c25f522d3efd2635079cf5be9" align="middle">
<img src="https://picx.zhimg.com/v2-24d9b51051e818e9f6ab7d6a4406e891" align="middle">
<img src="https://picx.zhimg.com/v2-b77d1e19411d79af5a8884fbb5cd3277" align="middle">
<img src="https://picx.zhimg.com/v2-391f30177d6d75bfc0e755a881895822" align="middle">
<img src="https://picx.zhimg.com/v2-f4b34b765155b7d09d178431cd529caf" align="middle">
<img src="https://picx.zhimg.com/v2-e6677dd46adf4430670606508a181d32" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MedDChest-A-Content-Aware-Multimodal-Foundational-Vision-Model-for-Thoracic-Imaging"><a href="#MedDChest-A-Content-Aware-Multimodal-Foundational-Vision-Model-for-Thoracic-Imaging" class="headerlink" title="MedDChest: A Content-Aware Multimodal Foundational Vision Model for   Thoracic Imaging"></a>MedDChest: A Content-Aware Multimodal Foundational Vision Model for   Thoracic Imaging</h2><p><strong>Authors:Mahmoud Soliman, Islam Osman, Mohamed S. Shehata, Rasika Rajapakshe</strong></p>
<p>The performance of vision models in medical imaging is often hindered by the prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain natural images. To address this fundamental domain gap, we propose MedDChest, a new foundational Vision Transformer (ViT) model optimized specifically for thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated, multimodal dataset of over 1.2 million images, encompassing different modalities including Chest X-ray and Computed Tomography (CT) compiled from 10 public sources. A core technical contribution of our work is Guided Random Resized Crops, a novel content-aware data augmentation strategy that biases sampling towards anatomically relevant regions, overcoming the inefficiency of standard cropping techniques on medical scans. We validate our modelâ€™s effectiveness by fine-tuning it on a diverse set of downstream diagnostic tasks. Comprehensive experiments empirically demonstrate that MedDChest significantly outperforms strong, publicly available ImageNet-pretrained models. By establishing the superiority of large-scale, in-domain pre-training combined with domain-specific data augmentation, MedDChest provides a powerful and robust feature extractor that serves as a significantly better starting point for a wide array of thoracic diagnostic tasks. The model weights will be made publicly available to foster future research and applications. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å½±åƒä¸­ï¼Œè§†è§‰æ¨¡å‹çš„æ€§èƒ½å¸¸å¸¸å—åˆ°ä½¿ç”¨é¢„è®­ç»ƒçš„åŸŸå¤–è‡ªç„¶å›¾åƒå¾®è°ƒä¸»å¯¼æ¨¡å¼çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€åŸºæœ¬åŸŸå·®è·é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MedDChestï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹èƒ¸éƒ¨å½±åƒä¼˜åŒ–è®¾è®¡çš„å…¨æ–°åŸºç¡€è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬ä»å¤§é‡ç²¾é€‰çš„å¤šæ¨¡å¼æ•°æ®é›†ä¸­ä»å¤´å¼€å§‹é¢„è®­ç»ƒMedDChestï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡120ä¸‡å¼ å›¾åƒï¼Œæ¶µç›–ä¸åŒæ¨¡å¼ï¼ŒåŒ…æ‹¬æ¥è‡ª10ä¸ªå…¬å…±æºçš„èƒ¸éƒ¨Xå°„çº¿å’Œè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ã€‚æˆ‘ä»¬å·¥ä½œçš„æ ¸å¿ƒæŠ€æœ¯è´¡çŒ®æ˜¯å¼•å¯¼éšæœºè°ƒæ•´è£å‰ªï¼ˆGuided Random Resized Cropsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å†…å®¹æ„ŸçŸ¥æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå®ƒåå‘äºè§£å‰–ç›¸å…³åŒºåŸŸçš„é‡‡æ ·ï¼Œå…‹æœäº†æ ‡å‡†è£å‰ªæŠ€æœ¯åœ¨åŒ»å­¦æ‰«æä¸Šçš„ä½æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸€ç³»åˆ—ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹æ¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚ç»¼åˆå®éªŒç»éªŒè¡¨æ˜ï¼ŒMedDChestæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„å…¬å¼€ImageNeté¢„è®­ç»ƒæ¨¡å‹ã€‚é€šè¿‡å»ºç«‹å¤§è§„æ¨¡é¢†åŸŸé¢„è®­ç»ƒä¸ç‰¹å®šé¢†åŸŸæ•°æ®å¢å¼ºç›¸ç»“åˆçš„ä¼˜åŠ¿ï¼ŒMedDChestæä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”ç¨³å¥çš„ç‰¹å¾æå–å™¨ï¼Œå¯ä½œä¸ºå¤šç§èƒ¸éƒ¨è¯Šæ–­ä»»åŠ¡çš„ä¸€ä¸ªæ›´å¥½çš„èµ·ç‚¹ã€‚æ¨¡å‹çš„æƒé‡å°†å…¬å¼€æä¾›ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ä¸åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04016v1">PDF</a> 10 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>ç»è¿‡ç ”ç©¶å‘ç°ï¼Œé’ˆå¯¹åŒ»ç–—å½±åƒçš„é¢„è®­ç»ƒæ¨¡å‹åœ¨åº”ç”¨äºåŒ»å­¦æˆåƒæ—¶å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é’ˆå¯¹èƒ¸éƒ¨å½±åƒçš„Vision Transformeræ¨¡å‹â€”â€”MedDChestã€‚è¯¥æ¨¡å‹åœ¨å¤§é‡ç²¾é€‰çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«è¶…è¿‡1.2ç™¾ä¸‡å¼ æ¶µç›–ä¸åŒæ¨¡æ€ï¼ˆå¦‚Xå…‰èƒ¸ç‰‡ä¸è®¡ç®—æœºæ–­å±‚æ‰«æï¼‰çš„å›¾åƒã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„å†…å®¹æ„ŸçŸ¥æ•°æ®å¢å¼ºç­–ç•¥â€”â€”å¯¼å‘éšæœºå°ºå¯¸è£å‰ªï¼ˆGuided Random Resized Cropsï¼‰ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°åœ¨åŒ»å­¦æ‰«æå›¾åƒä¸Šé‡‡æ ·è§£å‰–ç›¸å…³åŒºåŸŸã€‚é€šè¿‡å®éªŒéªŒè¯ï¼ŒMedDCheståœ¨å¤šç§ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºå…¬å¼€çš„ImageNeté¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™è¡¨æ˜å¤§è§„æ¨¡é¢†åŸŸå†…é¢„è®­ç»ƒç»“åˆé¢†åŸŸç‰¹å®šæ•°æ®å¢å¼ºç­–ç•¥èƒ½æä¾›æ›´å¼ºå¤§ã€æ›´ç¨³å¥çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œä¸ºå¹¿æ³›çš„èƒ¸éƒ¨è¯Šæ–­ä»»åŠ¡æä¾›æ›´ä¼˜ç§€çš„èµ·ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedDChestæ˜¯ä¸€ç§é’ˆå¯¹èƒ¸éƒ¨å½±åƒä¼˜åŒ–çš„æ–°Vision Transformeræ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨è¶…è¿‡1.2ç™¾ä¸‡å¼ å›¾åƒçš„å¤§è§„æ¨¡ã€ç²¾é€‰ã€å¤šæ¨¡æ€æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>æå‡ºäº†å¯¼å‘éšæœºå°ºå¯¸è£å‰ªï¼ˆGuided Random Resized Cropsï¼‰è¿™ä¸€æ–°å‹å†…å®¹æ„ŸçŸ¥æ•°æ®å¢å¼ºç­–ç•¥ã€‚</li>
<li>MedDCheståœ¨å¤šç§ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºImageNeté¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>å¤§è§„æ¨¡é¢†åŸŸå†…é¢„è®­ç»ƒç»“åˆé¢†åŸŸç‰¹å®šæ•°æ®å¢å¼ºç­–ç•¥èƒ½æå‡ç‰¹å¾æå–èƒ½åŠ›ã€‚</li>
<li>MedDChestæ¨¡å‹ä¸ºå¹¿æ³›çš„èƒ¸éƒ¨è¯Šæ–­ä»»åŠ¡æä¾›æ›´ä¼˜ç§€çš„èµ·ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04016">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61f0b4d5366470019f7342fc492e4cfe" align="middle">
<img src="https://picx.zhimg.com/v2-d8f60215ea89f6aad1393b5debcb0d67" align="middle">
<img src="https://picx.zhimg.com/v2-e61bfd91140fcc40cec09c96a13ec878" align="middle">
<img src="https://picx.zhimg.com/v2-db633a9ab807bf30caeef52f36fe5fc0" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CORE-A-Cell-Level-Coarse-to-Fine-Image-Registration-Engine-for-Multi-stain-Image-Alignment"><a href="#CORE-A-Cell-Level-Coarse-to-Fine-Image-Registration-Engine-for-Multi-stain-Image-Alignment" class="headerlink" title="CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for   Multi-stain Image Alignment"></a>CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for   Multi-stain Image Alignment</h2><p><strong>Authors:Esha Sadia Nasir, Behnaz Elhaminia, Mark Eastwood, Catherine King, Owen Cain, Lorraine Harper, Paul Moss, Dimitrios Chanouzas, David Snead, Nasir Rajpoot, Adam Shephard, Shan E Ahmed Raza</strong></p>
<p>Accurate and efficient registration of whole slide images (WSIs) is essential for high-resolution, nuclei-level analysis in multi-stained tissue slides. We propose a novel coarse-to-fine framework CORE for accurate nuclei-level registration across diverse multimodal whole-slide image (WSI) datasets. The coarse registration stage leverages prompt-based tissue mask extraction to effectively filter out artefacts and non-tissue regions, followed by global alignment using tissue morphology and ac- celerated dense feature matching with a pre-trained feature extractor. From the coarsely aligned slides, nuclei centroids are detected and subjected to fine-grained rigid registration using a custom, shape-aware point-set registration model. Finally, non-rigid alignment at the cellular level is achieved by estimating a non-linear dis- placement field using Coherent Point Drift (CPD). Our approach benefits from automatically generated nuclei that enhance the accuracy of deformable registra- tion and ensure precise nuclei-level correspondence across modalities. The pro- posed model is evaluated on three publicly available WSI registration datasets, and two private datasets. We show that CORE outperforms current state-of-the-art methods in terms of generalisability, precision, and robustness in bright-field and immunofluorescence microscopy WSIs </p>
<blockquote>
<p>åœ¨å¤šé‡æŸ“è‰²ç»„ç»‡åˆ‡ç‰‡ä¸­è¿›è¡Œé«˜åˆ†è¾¨ç‡ã€ç»†èƒæ ¸çº§åˆ«çš„åˆ†ææ—¶ï¼Œå¯¹æ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œå‡†ç¡®é«˜æ•ˆçš„æ³¨å†Œè‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç”±ç²—åˆ°ç»†çš„æ³¨å†Œæ¡†æ¶COREï¼Œç”¨äºåœ¨å¤šç§æ¨¡å¼çš„æ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰æ•°æ®é›†ä¸­è¿›è¡Œå‡†ç¡®çš„ç»†èƒæ ¸çº§åˆ«æ³¨å†Œã€‚ç²—æ³¨å†Œé˜¶æ®µåˆ©ç”¨åŸºäºæç¤ºçš„ç»„ç»‡æ©è†œæå–æŠ€æœ¯ï¼Œæœ‰æ•ˆåœ°è¿‡æ»¤æ‰ä¼ªå½±å’Œéç»„ç»‡åŒºåŸŸï¼Œéšååˆ©ç”¨ç»„ç»‡å½¢æ€è¿›è¡Œå…¨å±€å¯¹é½ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨åŠ é€Ÿå¯†é›†ç‰¹å¾åŒ¹é…ã€‚ä»ç²—ç•¥å¯¹é½çš„å¹»ç¯ç‰‡ä¸­æ£€æµ‹ç»†èƒæ ¸è´¨å¿ƒï¼Œå¹¶ä½¿ç”¨è‡ªå®šä¹‰çš„ã€å…·æœ‰å½¢çŠ¶æ„ŸçŸ¥çš„ç‚¹é›†æ³¨å†Œæ¨¡å‹è¿›è¡Œç²¾ç»†çš„åˆšæ€§æ³¨å†Œã€‚æœ€åï¼Œé€šè¿‡ä¼°è®¡éçº¿æ€§ä½ç§»åœºæ¥å®ç°ç»†èƒå±‚é¢çš„éåˆšæ€§å¯¹é½ï¼Œä½¿ç”¨ååŒç‚¹æ¼‚ç§»ï¼ˆCPDï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—ç›Šäºè‡ªåŠ¨ç”Ÿæˆçš„ç»†èƒæ ¸ï¼Œæé«˜äº†å¯å˜å½¢æ³¨å†Œçš„å‡†ç¡®æ€§ï¼Œå¹¶ç¡®ä¿è·¨æ¨¡æ€çš„ç²¾ç¡®ç»†èƒæ ¸çº§åˆ«å¯¹åº”ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨ä¸‰ä¸ªå…¬å¼€çš„WSIæ³¨å†Œæ•°æ®é›†å’Œä¸¤ä¸ªç§æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒCOREåœ¨æ˜åœºå’Œå…ç–«è§å…‰æ˜¾å¾®é•œWSIçš„é€šç”¨æ€§ã€ç²¾åº¦å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03826v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹çš„ç²—åˆ°ç»†æ¡†æ¶COREè¢«æå‡ºæ¥å®ç°å¤šæ¨¡æ€å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„ç»†èƒæ ¸çº§åˆ«ç²¾ç¡®é…å‡†ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ç²—é…å‡†é˜¶æ®µå’Œç²¾ç»†é…å‡†é˜¶æ®µã€‚ç²—é…å‡†é˜¶æ®µåŸºäºæç¤ºè¿›è¡Œç»„ç»‡æ©è†œæå–ï¼Œæœ‰æ•ˆè¿‡æ»¤å‡ºä¼ªå½±å’Œéç»„ç»‡åŒºåŸŸï¼Œç„¶ååˆ©ç”¨ç»„ç»‡å½¢æ€å’Œé¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨è¿›è¡Œå…¨å±€å¯¹é½å’ŒåŠ é€Ÿå¯†é›†ç‰¹å¾åŒ¹é…ã€‚ç²¾ç»†é…å‡†é˜¶æ®µåˆ™åŸºäºæ£€æµ‹åˆ°çš„ç»†èƒæ ¸è´¨å¿ƒï¼Œåˆ©ç”¨è‡ªå®šä¹‰çš„å½¢çŠ¶æ„ŸçŸ¥ç‚¹é›†é…å‡†æ¨¡å‹è¿›è¡Œç²¾ç»†é…å‡†ï¼Œå¹¶é€šè¿‡ä¼°è®¡éçº¿æ€§ä½ç§»åœºå®ç°ç»†èƒçº§åˆ«çš„éåˆšæ€§å¯¹é½ã€‚è¯¥æ–¹æ³•è‡ªåŠ¨ç”Ÿæˆçš„ç»†èƒæ ¸æé«˜äº†å¯å˜å½¢é…å‡†çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¤šç§å…¬å¼€å’Œç§æœ‰æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶ä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€ç²¾ç¡®åº¦å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æ–°å‹çš„ç²—åˆ°ç»†æ¡†æ¶COREï¼Œç”¨äºå¤šæ¨¡æ€å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„ç»†èƒæ ¸çº§åˆ«ç²¾ç¡®é…å‡†ã€‚</li>
<li>ç²—é…å‡†é˜¶æ®µé€šè¿‡æç¤ºè¿›è¡Œç»„ç»‡æ©è†œæå–ï¼Œæœ‰æ•ˆè¿‡æ»¤ä¼ªå½±å’Œéç»„ç»‡åŒºåŸŸã€‚</li>
<li>åˆ©ç”¨ç»„ç»‡å½¢æ€å’Œé¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨è¿›è¡Œå…¨å±€å¯¹é½å’ŒåŠ é€Ÿå¯†é›†ç‰¹å¾åŒ¹é…ã€‚</li>
<li>ç²¾ç»†é…å‡†é˜¶æ®µåŸºäºæ£€æµ‹åˆ°çš„ç»†èƒæ ¸è´¨å¿ƒè¿›è¡Œç‚¹é›†é…å‡†ã€‚</li>
<li>é€šè¿‡ä¼°è®¡éçº¿æ€§ä½ç§»åœºå®ç°ç»†èƒçº§åˆ«çš„éåˆšæ€§å¯¹é½ã€‚</li>
<li>è‡ªåŠ¨ç”Ÿæˆçš„ç»†èƒæ ¸æé«˜äº†å¯å˜å½¢é…å‡†çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bc71e581cf50e98bb0c502039da8c16" align="middle">
<img src="https://picx.zhimg.com/v2-cc4a96a5ef88260415c9ba9f4818215f" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Possibility-of-ferro-octupolar-order-in-Ba-2-CaOsO-6-assessed-by-X-ray-magnetic-dichroism-measurements"><a href="#Possibility-of-ferro-octupolar-order-in-Ba-2-CaOsO-6-assessed-by-X-ray-magnetic-dichroism-measurements" class="headerlink" title="Possibility of ferro-octupolar order in Ba$_2$CaOsO$_6$ assessed by   X-ray magnetic dichroism measurements"></a>Possibility of ferro-octupolar order in Ba$_2$CaOsO$_6$ assessed by   X-ray magnetic dichroism measurements</h2><p><strong>Authors:Goro Shibata, Naomi Kawamura, Jun Okamoto, Arata Tanaka, Hiroaki Hayashi, Kazunari Yamaura, Hsiao-Yu Huang, Amol Singh, Chien-Te Chen, Di-Jing Huang, Sergey V. Streltsov, Atsushi Fujimori</strong></p>
<p>Localized $5d^2$ electrons in a cubic crystal field possess multipoles such as electric quadrupoles and magnetic octupoles. We studied the cubic double perovskite Ba$_2$CaOsO$_6$ containing the Os$^{6+}$ ($5d^2$) ions, which exhibits a phase transition to a <code>hidden order&#39; below $T^* \sim$ 50 K, by X-ray absorption spectroscopy (XAS) and X-ray magnetic circular dichroism (XMCD) at the Os $L_&#123;2,3&#125;$ edge. The cubic ligand-field splitting between the $t_&#123;2g&#125;$ and $e_g$ levels of Os $5d$ was deduced by XAS to be $\sim$4 eV. The temperature dependence of the XMCD spectra was consistent with a $\sim$18 meV residual cubic splitting of the lowest $J_&#123;\rm eff&#125; =$ 2 multiplet state into the non-Kramers $E_g$ doublet ground state and the $T_&#123;2g&#125;$ triplet excited state. Ligand-field (LF) multiplet calculation under fictitious strong magnetic fields indicated that the exchange interaction between nearest-neighbor octupoles should be as strong as $\sim$1.5 meV if a ferro-octupole order is stabilized in the </code>hidden-orderedâ€™ state, consistent with the exchange interaction of $\sim$1 meV previously predicted theoretically using model and density functional theory calculations. </p>
<blockquote>
<p>å…·æœ‰ç«‹æ–¹æ™¶ä½“åœºçš„å±€éƒ¨$5d^2$ç”µå­å…·æœ‰å¤šé‡æï¼Œä¾‹å¦‚ç”µå››æå’Œç£å…«æã€‚æˆ‘ä»¬ç ”ç©¶äº†å…·æœ‰Os$^{6+}$ï¼ˆ$5d^2$ï¼‰ç¦»å­çš„ç«‹æ–¹åŒé’™é’›çŸ¿Ba$<em>2$CaOsO$<em>6$ï¼Œå®ƒåœ¨ä½äº$T^* \sim 50 K$æ—¶ä¼šå‘ç”Ÿåˆ°â€œéšè—é¡ºåºâ€çš„ç›¸å˜ã€‚æˆ‘ä»¬é€šè¿‡Os $L</em>{2,3}$è¾¹ç¼˜çš„Xå°„çº¿å¸æ”¶å…‰è°±ï¼ˆXASï¼‰å’ŒXå°„çº¿ç£åœ†äºŒè‰²æ€§ï¼ˆXMCDï¼‰è¿›è¡Œäº†ç ”ç©¶ã€‚é€šè¿‡XASæ¨æ–­å‡ºOs $5d$çš„$t</em>{2g}$å’Œ$e_g$èƒ½çº§ä¹‹é—´çš„ç«‹æ–¹é…ä½“åœºåˆ†è£‚çº¦ä¸º$4 eV$ã€‚XMCDå…‰è°±çš„æ¸©åº¦ä¾èµ–æ€§è¡¨æ˜æœ€ä½çš„å¤šé‡æ€çŠ¶æ€$J_{\rm eff} &#x3D; 2$å­˜åœ¨çº¦$18 meV$çš„æ®‹ä½™ç«‹æ–¹åˆ†è£‚ï¼Œè¿™ä½¿å…¶æˆä¸ºéKramersåŸºæ€$E_g$åŒæ€å’Œæ¿€å‘æ€çš„$T_{2g}$ä¸‰é‡æ€ã€‚åœ¨å‡è®¾çš„å¼ºç£åœºä¸‹çš„é…ä½“åœºå¤šé‡æ€è®¡ç®—è¡¨æ˜ï¼Œå¦‚æœåœ¨â€œéšè—æœ‰åºâ€çŠ¶æ€ä¸‹ç¨³å®šäº†é“ç£å…«æé¡ºåºï¼Œåˆ™æœ€è¿‘é‚»å…«æä¹‹é—´çš„äº¤æ¢ç›¸äº’ä½œç”¨åº”è¯¥è¾¾åˆ°$\sim 1.5 meV$ï¼Œè¿™ä¸ä¹‹å‰ä½¿ç”¨æ¨¡å‹å’Œå¯†åº¦æ³›å‡½ç†è®ºè®¡ç®—çš„ç†è®ºé¢„æµ‹å€¼$\sim 1 meV$çš„äº¤æ¢ç›¸äº’ä½œç”¨ä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.00448v2">PDF</a> 6 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç«‹æ–¹åŒé’™é’›çŸ¿Ba$_2$CaOsO$_6$ä¸­çš„Os$^{6+}$ï¼ˆ$5d^2$ï¼‰ç¦»å­ï¼Œå‘ç°å…¶åœ¨ä½æ¸©ä¸‹å­˜åœ¨ä¸€ç§â€œéšè—åºâ€ã€‚é€šè¿‡Xå°„çº¿å¸æ”¶å…‰è°±ï¼ˆXASï¼‰å’ŒXå°„çº¿ç£åœ†äºŒè‰²æ€§ï¼ˆXMCDï¼‰ç ”ç©¶ï¼Œå‘ç°è¯¥ç¦»å­åœ¨ç«‹æ–¹æ™¶ä½“åœºä¸­å…·æœ‰å¤šé‡æï¼Œå¦‚ç”µå››æå’Œç£å…«æã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨éšè—åºçŠ¶æ€ä¸‹å¯èƒ½å­˜åœ¨é“ç£æ€§å…«æåºï¼Œä¸”äº¤æ¢ç›¸äº’ä½œç”¨é¢„è®¡ç›¸å½“å¼ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Os$^{6+}$ç¦»å­åœ¨ç«‹æ–¹åŒé’™é’›çŸ¿Ba$_2$CaOsO$_6$ä¸­çš„ç”µå­ç»“æ„è¡¨ç°å‡ºå¤šé‡æç‰¹æ€§ã€‚</li>
<li>é€šè¿‡XASç ”ç©¶ç¡®å®šäº†Os $5d$çš„$t_{2g}$å’Œ$e_g$èƒ½çº§ä¹‹é—´çš„ç«‹æ–¹é…ä½“åœºåˆ†è£‚çº¦ä¸º4 eVã€‚</li>
<li>XMCDè°±çš„æ¸©åº¦ä¾èµ–æ€§è¡¨æ˜å­˜åœ¨çº¦18 meVçš„æ®‹ä½™ç«‹æ–¹åˆ†è£‚ã€‚</li>
<li>ç ”ç©¶ç»“æœæ”¯æŒäº†æœ€ä½$J_{\rm eff} &#x3D;$ 2å¤šé‡æ€çš„éKramers $E_g$åŸºæ€å’Œ$T_{2g}$æ¿€å‘æ€çš„å­˜åœ¨ã€‚</li>
<li>åœ¨å‡æƒ³çš„å¼ºç£åœºä¸‹è¿›è¡Œçš„é…ä½“åœºå¤šé‡æ€è®¡ç®—è¡¨æ˜ï¼Œå¦‚æœâ€œéšè—åºâ€çŠ¶æ€ä¸‹ç¨³å®šäº†é“ç£æ€§å…«æåºï¼Œåˆ™è¿‘é‚»å…«æä¹‹é—´çš„äº¤æ¢ç›¸äº’ä½œç”¨å¯èƒ½è¾¾åˆ°çº¦1.5 meVã€‚</li>
<li>è¯¥ç»“æœä¸ä¹‹å‰ä½¿ç”¨æ¨¡å‹å’Œå¯†åº¦æ³›å‡½ç†è®ºè®¡ç®—å¾—åˆ°çš„çº¦1 meVçš„äº¤æ¢ç›¸äº’ä½œç”¨ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.00448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5578da1225f0a670aa7ce8fe8250f9a5" align="middle">
<img src="https://picx.zhimg.com/v2-1913bdabda72d8eb5ba6797c8bd8ff66" align="middle">
<img src="https://picx.zhimg.com/v2-31bef018c0c16abcb7dfbb8529dcc339" align="middle">
<img src="https://picx.zhimg.com/v2-9a8738ad750e34f133373025eff189e3" align="middle">
<img src="https://picx.zhimg.com/v2-ea7975577fe5326a6be08027e83b5c85" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Cross-modal-Causal-Intervention-for-Alzheimerâ€™s-Disease-Prediction"><a href="#Cross-modal-Causal-Intervention-for-Alzheimerâ€™s-Disease-Prediction" class="headerlink" title="Cross-modal Causal Intervention for Alzheimerâ€™s Disease Prediction"></a>Cross-modal Causal Intervention for Alzheimerâ€™s Disease Prediction</h2><p><strong>Authors:Yutao Jin, Haowen Xiao, Junyong Zhai, Yuxiao Li, Jielei Chu, Fengmao Lv, Yuxiao Li</strong></p>
<p>Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimerâ€™s Disease (AD), where early identification and intervention can effectively slow the progression to dementia. However, diagnosing AD remains a significant challenge in neurology due to the confounders caused mainly by the selection bias of multi-modal data and the complex relationships between variables. To address these issues, we propose a novel visual-language causality-inspired framework named Cross-modal Causal Intervention with Mediator for Alzheimerâ€™s Disease Diagnosis (MediAD) for diagnostic assistance. Our MediAD employs Large Language Models (LLMs) to summarize clinical data under strict templates, therefore enriching textual inputs. The MediAD model utilizes Magnetic Resonance Imaging (MRI), clinical data, and textual data enriched by LLMs to classify participants into Cognitively Normal (CN), MCI, and AD categories. Because of the presence of confounders, such as cerebral vascular lesions and age-related biomarkers, non-causal models are likely to capture spurious input-output correlations, generating less reliable results. Our framework implicitly mitigates the effect of both observable and unobservable confounders through a unified causal intervention method. Experimental results demonstrate the outstanding performance of our method in distinguishing CN&#x2F;MCI&#x2F;AD cases, outperforming other methods in most evaluation metrics. The study showcases the potential of integrating causal reasoning with multi-modal learning for neurological disease diagnosis. </p>
<blockquote>
<p>è½»åº¦è®¤çŸ¥éšœç¢ï¼ˆMCIï¼‰æ˜¯é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„å‰é©±é˜¶æ®µï¼Œæ—©æœŸè¯†åˆ«å’Œå¹²é¢„å¯ä»¥æœ‰æ•ˆå‡ç¼“å‘ç—´å‘†çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºå¤šç§æ¨¡å¼æ•°æ®çš„é€‰æ‹©åè§å’Œå˜é‡ä¹‹é—´çš„å¤æ‚å…³ç³»å¯¼è‡´çš„æ··æ·†å› ç´ ï¼ŒADçš„è¯Šæ–­åœ¨ç¥ç»å­¦é¢†åŸŸä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€å› æœçµæ„Ÿæ¡†æ¶ï¼Œåä¸ºâ€œç”¨äºé˜¿å°”èŒ¨æµ·é»˜æ°ç—‡è¯Šæ–­çš„è·¨æ¨¡æ€å› æœå¹²é¢„ä¸­ä»‹â€ï¼ˆMediADï¼‰ï¼Œç”¨äºè¾…åŠ©è¯Šæ–­ã€‚æˆ‘ä»¬çš„MediADé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ä¸¥æ ¼æ¨¡æ¿ä¸‹çš„ä¸´åºŠæ•°æ®è¿›è¡Œæ€»ç»“ï¼Œä»è€Œä¸°å¯Œæ–‡æœ¬è¾“å…¥ã€‚MediADæ¨¡å‹åˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€ä¸´åºŠæ•°æ®å’ŒLLMä¸°å¯Œçš„æ–‡æœ¬æ•°æ®ï¼Œå°†å‚ä¸è€…åˆ†ç±»ä¸ºè®¤çŸ¥æ­£å¸¸ï¼ˆCNï¼‰ã€MCIå’ŒADç±»åˆ«ã€‚ç”±äºå­˜åœ¨è„‘è¡€ç®¡ç—…å˜å’Œå¹´é¾„ç›¸å…³ç”Ÿç‰©æ ‡å¿—ç‰©ç­‰æ··æ‚å› ç´ ï¼Œéå› æœæ¨¡å‹å¯èƒ½ä¼šæ•æ‰åˆ°è™šå‡çš„è¾“å…¥-è¾“å‡ºç›¸å…³æ€§ï¼Œä»è€Œäº§ç”Ÿä¸å¤ªå¯é çš„ç»“æœã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ç»Ÿä¸€çš„å› æœå¹²é¢„æ–¹æ³•éšå«åœ°å‡è½»äº†å¯è§‚æµ‹å’Œä¸å¯è§‚æµ‹æ··æ‚å› ç´ çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒºåˆ†CN&#x2F;MCI&#x2F;ADç—…ä¾‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å¤§å¤šæ•°è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å°†å› æœæ¨ç†ä¸å¤šæ¨¡å¼å­¦ä¹ æ•´åˆç”¨äºç¥ç»ç³»ç»Ÿç–¾ç—…è¯Šæ–­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13956v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰è¯Šæ–­çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€å› æœç†è®ºçš„æ–°å‹æ¡†æ¶â€”â€”Cross-modal Causal Intervention with Mediatorï¼ˆMediADï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä¸´åºŠæ•°æ®è¿›è¡Œæ€»ç»“å¹¶ä¸°å¯Œæ–‡æœ¬è¾“å…¥ï¼Œåˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€ä¸´åºŠæ•°æ®å’Œæ–‡æœ¬æ•°æ®å°†å‚ä¸è€…åˆ†ä¸ºè®¤çŸ¥æ­£å¸¸ï¼ˆCNï¼‰ã€è½»åº¦è®¤çŸ¥éšœç¢ï¼ˆMCIï¼‰å’ŒADä¸‰ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒºåˆ†CN&#x2F;MCI&#x2F;ADç—…ä¾‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å¤§å¤šæ•°è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å°†å› æœæ¨ç†ä¸å¤šæ¨¡æ€å­¦ä¹ ç›¸ç»“åˆåœ¨ç¥ç»ç³»ç»Ÿç–¾ç—…è¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½»åº¦è®¤çŸ¥éšœç¢ï¼ˆMCIï¼‰æ˜¯é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„å…ˆå…†é˜¶æ®µï¼Œæ—©æœŸè¯†åˆ«å’Œå¹²é¢„å¯æœ‰æ•ˆå‡ç¼“å‘ç—´å‘†çš„è¿›å±•ã€‚</li>
<li>ADè¯Šæ–­åœ¨ç¥ç»å­¦ä¸­ä»å…·æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå¤šæ¨¡æ€æ•°æ®çš„é€‰æ‹©åè§å’Œå˜é‡é—´å¤æ‚å…³ç³»å¯¼è‡´çš„æ··æ·†å› ç´ ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è§†è§‰è¯­è¨€å› æœçµæ„Ÿæ¡†æ¶â€”â€”MediADï¼Œç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¯Šæ–­ã€‚</li>
<li>MediADæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä¸´åºŠæ•°æ®è¿›è¡Œæ€»ç»“å¹¶ä¸°å¯Œæ–‡æœ¬è¾“å…¥ã€‚</li>
<li>MediADä½¿ç”¨MRIã€ä¸´åºŠæ•°æ®å’Œæ–‡æœ¬æ•°æ®å¯¹å‚ä¸è€…è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>å› æœæ¨ç†æ–¹æ³•èƒ½æœ‰æ•ˆå‡è½»å¯è§‚æµ‹å’Œä¸å¯è§‚æµ‹æ··æ·†å› ç´ çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e646c0ce6d99aa1a0d1d69db5960ebd9" align="middle">
<img src="https://picx.zhimg.com/v2-1f531a894d14f6a3d5aba0b91fd98733" align="middle">
<img src="https://picx.zhimg.com/v2-3e730e6e1e8db2c049bd08945dbd3bbe" align="middle">
<img src="https://picx.zhimg.com/v2-68fffe9ba70e026573f40fe8bc0f2023" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DIsoN-Decentralized-Isolation-Networks-for-Out-of-Distribution-Detection-in-Medical-Imaging"><a href="#DIsoN-Decentralized-Isolation-Networks-for-Out-of-Distribution-Detection-in-Medical-Imaging" class="headerlink" title="DIsoN: Decentralized Isolation Networks for Out-of-Distribution   Detection in Medical Imaging"></a>DIsoN: Decentralized Isolation Networks for Out-of-Distribution   Detection in Medical Imaging</h2><p><strong>Authors:Felix Wagner, Pramit Saha, Harry Anthony, J. Alison Noble, Konstantinos Kamnitsas</strong></p>
<p>Safe deployment of machine learning (ML) models in safety-critical domains such as medical imaging requires detecting inputs with characteristics not seen during training, known as out-of-distribution (OOD) detection, to prevent unreliable predictions. Effective OOD detection after deployment could benefit from access to the training data, enabling direct comparison between test samples and the training data distribution to identify differences. State-of-the-art OOD detection methods, however, either discard the training data after deployment or assume that test samples and training data are centrally stored together, an assumption that rarely holds in real-world settings. This is because shipping the training data with the deployed model is usually impossible due to the size of training databases, as well as proprietary or privacy constraints. We introduce the Isolation Network, an OOD detection framework that quantifies the difficulty of separating a target test sample from the training data by solving a binary classification task. We then propose Decentralized Isolation Networks (DIsoN), which enables the comparison of training and test data when data-sharing is impossible, by exchanging only model parameters between the remote computational nodes of training and deployment. We further extend DIsoN with class-conditioning, comparing a target sample solely with training data of its predicted class. We evaluate DIsoN on four medical imaging datasets (dermatology, chest X-ray, breast ultrasound, histopathology) across 12 OOD detection tasks. DIsoN performs favorably against existing methods while respecting data-privacy. This decentralized OOD detection framework opens the way for a new type of service that ML developers could provide along with their models: providing remote, secure utilization of their training data for OOD detection services. Code: <a target="_blank" rel="noopener" href="https://github.com/FelixWag/DIsoN">https://github.com/FelixWag/DIsoN</a> </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒç­‰å®‰å…¨å…³é”®é¢†åŸŸä¸­å®‰å…¨éƒ¨ç½²æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹éœ€è¦æ£€æµ‹è®­ç»ƒæœŸé—´æœªè§ç‰¹å¾è¾“å…¥ï¼Œè¿™ç§°ä¸ºç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹ï¼Œä»¥é˜²æ­¢ä¸å¯é é¢„æµ‹ã€‚éƒ¨ç½²åæœ‰æ•ˆçš„OODæ£€æµ‹å¯ä»¥å—ç›Šäºè®¿é—®è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨æµ‹è¯•æ ·æœ¬å’Œè®­ç»ƒæ•°æ®åˆ†å¸ƒä¹‹é—´è¿›è¡Œæ¯”è¾ƒä»¥è¯†åˆ«å·®å¼‚ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•è¦ä¹ˆåœ¨éƒ¨ç½²åä¸¢å¼ƒè®­ç»ƒæ•°æ®ï¼Œè¦ä¹ˆå‡è®¾æµ‹è¯•æ ·æœ¬å’Œè®­ç»ƒæ•°æ®é›†ä¸­å­˜å‚¨åœ¨ä¸€èµ·ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„è®¾ç½®å‡ ä¹å¾ˆå°‘å¦‚æ­¤ã€‚è¿™æ˜¯å› ä¸ºç”±äºè®­ç»ƒæ•°æ®åº“çš„å¤§å°ä»¥åŠä¸“æœ‰æˆ–éšç§çº¦æŸï¼Œé€šå¸¸ä¸å¯èƒ½å°†è®­ç»ƒæ•°æ®ä¸éƒ¨ç½²çš„æ¨¡å‹ä¸€èµ·ä¼ è¾“ã€‚æˆ‘ä»¬å¼•å…¥äº†éš”ç¦»ç½‘ç»œï¼Œè¿™æ˜¯ä¸€ç§OODæ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡è§£å†³äºŒåˆ†ç±»ä»»åŠ¡æ¥é‡åŒ–å°†ç›®æ ‡æµ‹è¯•æ ·æœ¬ä»è®­ç»ƒæ•°æ®ä¸­åˆ†ç¦»çš„éš¾åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†æ•£å¼éš”ç¦»ç½‘ç»œï¼ˆDIsoNï¼‰ï¼Œé€šè¿‡åœ¨è®­ç»ƒå’Œéƒ¨ç½²çš„è¿œç¨‹è®¡ç®—èŠ‚ç‚¹ä¹‹é—´ä»…äº¤æ¢æ¨¡å‹å‚æ•°ï¼Œä»è€Œèƒ½å¤Ÿåœ¨ä¸å¯èƒ½è¿›è¡Œæ•°æ®å…±äº«çš„æƒ…å†µä¸‹æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†DIsoNæ‰©å±•åˆ°ç±»æ¡ä»¶ï¼Œä»…å°†ç›®æ ‡æ ·æœ¬ä¸å…¶é¢„æµ‹ç±»çš„è®­ç»ƒæ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨å››ä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ï¼ˆçš®è‚¤ç§‘ã€èƒ¸éƒ¨Xå°„çº¿ã€ä¹³è…ºè¶…å£°ã€ç»„ç»‡ç—…ç†å­¦ï¼‰ä¸Šå¯¹DIsoNè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†1Exx OODæ£€æµ‹ä»»åŠ¡ã€‚DIsoNè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•åŒæ—¶å°Šé‡æ•°æ®éšç§ã€‚è¿™ç§åˆ†æ•£å¼çš„OODæ£€æµ‹æ¡†æ¶ä¸ºMLå¼€å‘äººå‘˜æä¾›äº†ä¸€ç§æ–°å‹æœåŠ¡çš„æ–¹å¼ï¼šåœ¨æä¾›æ¨¡å‹çš„åŒæ—¶æä¾›è¿œç¨‹ã€å®‰å…¨çš„åˆ©ç”¨ä»–ä»¬çš„è®­ç»ƒæ•°æ®è¿›è¡ŒOODæ£€æµ‹æœåŠ¡ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/FelixWag/DIsoN">https://github.com/FelixWag/DIsoN</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09024v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºåŒ»ç–—å›¾åƒç­‰é¢†åŸŸçš„æœºå™¨å­¦ä¹ æ¨¡å‹å®‰å…¨éƒ¨ç½²çš„æ–¹æ³•ã€‚é’ˆå¯¹éƒ¨ç½²åçš„æ¨¡å‹è¾“å…¥ï¼Œé€šè¿‡éš”ç¦»ç½‘ç»œï¼ˆIsolation Networkï¼‰è¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼Œä»¥è¯†åˆ«å‡ºä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸åŒçš„æµ‹è¯•æ ·æœ¬ã€‚åŒæ—¶ï¼Œæå‡ºäº†åˆ†æ•£éš”ç¦»ç½‘ç»œï¼ˆDIsoNï¼‰ï¼Œèƒ½å¤Ÿåœ¨æ— æ³•å…±äº«æ•°æ®çš„æƒ…å†µä¸‹æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚è¯¥æ–¹æ³•åœ¨å››ä¸ªåŒ»ç–—å½±åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œèƒ½å¤Ÿå®ç°å¯¹æ–°ç±»å‹çš„æœ‰æ•ˆæœåŠ¡ã€‚åœ¨éƒ¨ç½²æ¨¡å‹çš„åŒæ—¶ï¼Œèƒ½å¤Ÿè¿œç¨‹å®‰å…¨åˆ©ç”¨è®­ç»ƒæ•°æ®è¿›è¡Œå¼‚å¸¸æ£€æµ‹æœåŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aede40033566bdf653b18d34a9cf5556" align="middle">
<img src="https://picx.zhimg.com/v2-9abb7fbccf12e3e86d9cf12907dc0fce" align="middle">
<img src="https://picx.zhimg.com/v2-81375a39c01f7d469b04d5d40a10a611" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CPathAgent-An-Agent-based-Foundation-Model-for-Interpretable-High-Resolution-Pathology-Image-Analysis-Mimicking-Pathologistsâ€™-Diagnostic-Logic"><a href="#CPathAgent-An-Agent-based-Foundation-Model-for-Interpretable-High-Resolution-Pathology-Image-Analysis-Mimicking-Pathologistsâ€™-Diagnostic-Logic" class="headerlink" title="CPathAgent: An Agent-based Foundation Model for Interpretable   High-Resolution Pathology Image Analysis Mimicking Pathologistsâ€™ Diagnostic   Logic"></a>CPathAgent: An Agent-based Foundation Model for Interpretable   High-Resolution Pathology Image Analysis Mimicking Pathologistsâ€™ Diagnostic   Logic</h2><p><strong>Authors:Yuxuan Sun, Yixuan Si, Chenglu Zhu, Kai Zhang, Zhongyi Shui, Bowen Ding, Tao Lin, Lin Yang</strong></p>
<p>Recent advances in computational pathology have led to the emergence of numerous foundation models. These models typically rely on general-purpose encoders with multi-instance learning for whole slide image (WSI) classification or apply multimodal approaches to generate reports directly from images. However, these models cannot emulate the diagnostic approach of pathologists, who systematically examine slides at low magnification to obtain an overview before progressively zooming in on suspicious regions to formulate comprehensive diagnoses. Instead, existing models directly output final diagnoses without revealing the underlying reasoning process. To address this gap, we introduce CPathAgent, an innovative agent-based approach that mimics pathologistsâ€™ diagnostic workflow by autonomously navigating across WSI based on observed visual features, thereby generating substantially more transparent and interpretable diagnostic summaries. To achieve this, we develop a multi-stage training strategy that unifies patch-level, region-level, and WSI-level capabilities within a single model, which is essential for replicating how pathologists understand and reason across diverse image scales. Additionally, we construct PathMMU-HR2, the first expert-validated benchmark for large region analysis. This represents a critical intermediate scale between patches and whole slides, reflecting a key clinical reality where pathologists typically examine several key large regions rather than entire slides at once. Extensive experiments demonstrate that CPathAgent consistently outperforms existing approaches across benchmarks at three different image scales, validating the effectiveness of our agent-based diagnostic approach and highlighting a promising direction for computational pathology. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„è¿›æ­¥å‚¬ç”Ÿäº†ä¼—å¤šåŸºç¡€æ¨¡å‹çš„å‡ºç°ã€‚è¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå…·æœ‰å¤šå®ä¾‹å­¦ä¹ åŠŸèƒ½çš„é€šç”¨ç¼–ç å™¨ï¼Œç”¨äºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»ï¼Œæˆ–è€…é‡‡ç”¨å¤šæ¨¡æ€æ–¹æ³•ç›´æ¥ä»å›¾åƒç”ŸæˆæŠ¥å‘Šã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ— æ³•æ¨¡æ‹Ÿç—…ç†åŒ»ç”Ÿçš„è¯Šæ–­è¿‡ç¨‹ï¼Œç—…ç†åŒ»ç”Ÿä¼šåœ¨ä½å€é•œä¸‹ç³»ç»Ÿåœ°æ£€æŸ¥å¹»ç¯ç‰‡ä»¥è·å¾—æ¦‚è§ˆï¼Œç„¶åé€æ­¥æ”¾å¤§åˆ°å¯ç–‘åŒºåŸŸä»¥å½¢æˆå…¨é¢çš„è¯Šæ–­ã€‚ç›¸åï¼Œç°æœ‰æ¨¡å‹ç›´æ¥è¾“å‡ºæœ€ç»ˆè¯Šæ–­ç»“æœï¼Œè€Œæ²¡æœ‰æ­ç¤ºæ½œåœ¨çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CPathAgentï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆ›æ–°çš„ä»£ç†æ–¹æ³•ï¼Œé€šè¿‡åŸºäºè§‚å¯Ÿåˆ°çš„è§†è§‰ç‰¹å¾åœ¨WSIä¸­è‡ªä¸»å¯¼èˆªï¼Œä»è€Œç”Ÿæˆæ›´åŠ é€æ˜å’Œå¯è§£é‡Šçš„è¯Šæ–­æ‘˜è¦ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå°†è¡¥ä¸çº§åˆ«ã€åŒºåŸŸçº§åˆ«å’ŒWSIçº§åˆ«çš„èƒ½åŠ›ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œè¿™å¯¹äºå¤åˆ¶ç—…ç†åŒ»ç”Ÿå¦‚ä½•åœ¨ä¸åŒå›¾åƒå°ºåº¦ä¸Šç†è§£å’Œæ¨ç†æ˜¯è‡³å…³é‡è¦çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†PathMMU-HR2ï¼Œè¿™æ˜¯ç”¨äºå¤§åŒºåŸŸåˆ†æçš„é¦–ä¸ªä¸“å®¶éªŒè¯çš„åŸºå‡†æµ‹è¯•ã€‚è¿™ä»£è¡¨äº†è¡¥ä¸å’Œæ•´ä¸ªå¹»ç¯ç‰‡ä¹‹é—´çš„ä¸­é—´è§„æ¨¡çš„å…³é”®è¿‡æ¸¡ï¼Œåæ˜ äº†ä¸´åºŠä¸Šçš„å®é™…æƒ…å†µï¼Œå³ç—…ç†åŒ»ç”Ÿé€šå¸¸ä¸€æ¬¡æ£€æŸ¥å‡ ä¸ªå…³é”®çš„å¤§åŒºåŸŸè€Œä¸æ˜¯æ•´ä¸ªå¹»ç¯ç‰‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCPathAgentåœ¨ä¸‰ä¸ªä¸åŒå›¾åƒå°ºåº¦çš„åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„åŸºäºä»£ç†çš„è¯Šæ–­æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æŒ‡å‡ºäº†è®¡ç®—ç—…ç†å­¦çš„ä¸€ä¸ªæœ‰å‰é€”çš„å‘å±•æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20510v2">PDF</a> 52 pages, 34 figures</p>
<p><strong>Summary</strong><br>     è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„æœ€æ–°è¿›å±•å‚¬ç”Ÿäº†ä¼—å¤šåŸºç¡€æ¨¡å‹çš„å‡ºç°ã€‚è¿™äº›æ¨¡å‹é€šå¸¸é‡‡ç”¨é€šç”¨ç¼–ç å™¨ä¸å¤šå®ä¾‹å­¦ä¹ æŠ€æœ¯ï¼Œç”¨äºå…¨åˆ‡ç‰‡å›¾åƒåˆ†ç±»ï¼Œæˆ–é‡‡ç”¨å¤šæ¨¡æ€æ–¹æ³•ç›´æ¥ä»å›¾åƒç”ŸæˆæŠ¥å‘Šã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ— æ³•æ¨¡æ‹Ÿç—…ç†åŒ»ç”Ÿçš„è¯Šæ–­è¿‡ç¨‹ï¼Œå³ç³»ç»Ÿæ€§åœ°ä½å€è§‚å¯Ÿåˆ‡ç‰‡ä»¥è·å¾—æ¦‚è§ˆï¼Œå†é€æ­¥æ”¾å¤§åˆ°å¯ç–‘åŒºåŸŸä»¥åšå‡ºå…¨é¢è¯Šæ–­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CPathAgentï¼Œè¿™æ˜¯ä¸€ç§æ¨¡ä»¿ç—…ç†åŒ»ç”Ÿè¯Šæ–­å·¥ä½œæµçš„åŸºäºä»£ç†çš„æ–¹æ³•ï¼Œå¯è‡ªä¸»åœ¨å…¨åˆ‡ç‰‡å›¾åƒä¸­è¿›è¡Œå¯¼èˆªï¼Œæ ¹æ®è§‚å¯Ÿåˆ°çš„è§†è§‰ç‰¹å¾ç”Ÿæˆæ›´åŠ é€æ˜å’Œå¯è§£é‡Šçš„è¯Šæ–­æ‘˜è¦ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå°†è¡¥ä¸çº§åˆ«ã€åŒºåŸŸçº§åˆ«å’Œå…¨åˆ‡ç‰‡çº§åˆ«çš„èƒ½åŠ›åœ¨ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ç»Ÿä¸€èµ·æ¥ï¼Œè¿™å¯¹äºå¤åˆ¶ç—…ç†åŒ»ç”Ÿå¦‚ä½•åœ¨ä¸åŒå›¾åƒå°ºåº¦ä¸Šè¿›è¡Œç†è§£å’Œæ¨ç†è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†PathMMU-HR2ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡ä¸“å®¶éªŒè¯çš„å¤§å‹åŒºåŸŸåˆ†æçš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒåæ˜ äº†ç—…ç†åŒ»ç”Ÿé€šå¸¸ä¸€æ¬¡æŸ¥çœ‹çš„å‡ ä¸ªå…³é”®å¤§å‹åŒºåŸŸï¼Œè€Œä¸æ˜¯æ•´ä¸ªåˆ‡ç‰‡ï¼Œè¿™ä»£è¡¨äº†ä¸´åºŠå®é™…æƒ…å†µä¸­çš„å…³é”®ä¸­é—´å°ºåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒCPathAgentåœ¨ä¸‰ä¸ªä¸åŒå›¾åƒå°ºåº¦çš„åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„åŸºäºä»£ç†çš„è¯Šæ–­æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æŒ‡å‡ºäº†è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—ç—…ç†å­¦é¢†åŸŸå‡ºç°å¤šä¸ªåŸºç¡€æ¨¡å‹ï¼Œé€šå¸¸é‡‡ç”¨é€šç”¨ç¼–ç å™¨ä¸å¤šå®ä¾‹å­¦ä¹ æˆ–å¤šæ¨¡æ€æ–¹æ³•è¿›è¡Œå›¾åƒåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆã€‚</li>
<li>å½“å‰æ¨¡å‹æ— æ³•æ¨¡æ‹Ÿç—…ç†åŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹ï¼Œç¼ºä¹é€æ˜åº¦ä¸å¯è§£é‡Šæ€§ã€‚</li>
<li>CPathAgentè¢«å¼•å…¥ä»¥æ¨¡ä»¿ç—…ç†åŒ»ç”Ÿçš„è¯Šæ–­æµç¨‹ï¼Œé€šè¿‡è‡ªä¸»å¯¼èˆªå…¨åˆ‡ç‰‡å›¾åƒå¹¶åŸºäºè§†è§‰ç‰¹å¾ç”Ÿæˆè¯Šæ–­æ‘˜è¦ã€‚</li>
<li>CPathAgenté‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥æ¥ç»Ÿä¸€ä¸åŒå›¾åƒå°ºåº¦ä¸‹çš„èƒ½åŠ›ã€‚</li>
<li>PathMMU-HR2æ˜¯é¦–ä¸ªé’ˆå¯¹å¤§å‹åŒºåŸŸåˆ†æçš„ä¸“å®¶éªŒè¯åŸºå‡†æµ‹è¯•ï¼Œåæ˜ ç—…ç†åŒ»ç”Ÿå®é™…å·¥ä½œä¸­çš„å…³é”®ä¸­é—´å°ºåº¦ã€‚</li>
<li>CPathAgentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4b483196a1c72fc2d30443ecf4727c2" align="middle">
<img src="https://picx.zhimg.com/v2-063159e6fccfb370f2ef1f5d90eb0d75" align="middle">
<img src="https://picx.zhimg.com/v2-e3b60478222b069494773d5298ed374c" align="middle">
<img src="https://picx.zhimg.com/v2-8391cd0ebb4ed2d37219b9b211c6cfe2" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="NMCSE-Noise-Robust-Multi-Modal-Coupling-Signal-Estimation-Method-via-Optimal-Transport-for-Cardiovascular-Disease-Detection"><a href="#NMCSE-Noise-Robust-Multi-Modal-Coupling-Signal-Estimation-Method-via-Optimal-Transport-for-Cardiovascular-Disease-Detection" class="headerlink" title="NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via   Optimal Transport for Cardiovascular Disease Detection"></a>NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via   Optimal Transport for Cardiovascular Disease Detection</h2><p><strong>Authors:Peihong Zhang, Zhixin Li, Rui Sang, Yuxuan Liu, Yiqiang Cai, Yizhou Tan, Shengchen Li</strong></p>
<p>The coupling signal refers to a latent physiological signal that characterizes the transformation from cardiac electrical excitation, captured by the electrocardiogram (ECG), to mechanical contraction, recorded by the phonocardiogram (PCG). By encoding the temporal and functional interplay between electrophysiological and hemodynamic events, it serves as an intrinsic link between modalities and offers a unified representation of cardiac function, with strong potential to enhance multi-modal cardiovascular disease (CVD) detection. However, existing coupling signal estimation methods remain highly vulnerable to noise, particularly in real-world clinical and physiological settings, which undermines their robustness and limits practical value. In this study, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates coupling signal estimation as a distribution matching problem solved via optimal transport. By jointly aligning amplitude and timing, NMCSE avoids noise amplification and enables stable signal estimation. When integrated into a Temporal-Spatial Feature Extraction (TSFE) network, the estimated coupling signal effectively enhances multi-modal fusion for more accurate CVD detection. To evaluate robustness under real-world conditions, we design two complementary experiments targeting distinct sources of noise. The first uses the PhysioNet 2016 dataset with simulated hospital noise to assess the resilience of NMCSE to clinical interference. The second leverages the EPHNOGRAM dataset with motion-induced physiological noise to evaluate intra-state estimation stability across activity levels. Experimental results show that NMCSE consistently outperforms existing methods under both clinical and physiological noise, highlighting it as a noise-robust estimation approach that enables reliable multi-modal cardiac detection in real-world conditions. </p>
<blockquote>
<p>è€¦åˆä¿¡å·æŒ‡çš„æ˜¯ä¸€ç§æ½œåœ¨çš„ç”Ÿç†ä¿¡å·ï¼Œå®ƒæè¿°äº†å¿ƒç”µå›¾ï¼ˆECGï¼‰æ•æ‰çš„å¿ƒè„ç”µå…´å¥‹åˆ°ç”±å¿ƒéŸ³å›¾ï¼ˆPCGï¼‰è®°å½•çš„æœºæ¢°æ”¶ç¼©çš„è½¬æ¢è¿‡ç¨‹ã€‚é€šè¿‡ç¼–ç ç”µç”Ÿç†å’Œè¡€æµåŠ¨åŠ›å­¦äº‹ä»¶ä¹‹é—´çš„æ—¶é—´æ€§å’ŒåŠŸèƒ½æ€§ç›¸äº’ä½œç”¨ï¼Œå®ƒæˆä¸ºä¸åŒæ¨¡æ€ä¹‹é—´çš„å›ºæœ‰è”ç³»ï¼Œå¹¶ä¸ºå¿ƒè„åŠŸèƒ½æä¾›äº†ç»Ÿä¸€çš„è¡¨ç¤ºï¼Œå…·æœ‰å¢å¼ºå¤šæ¨¡æ€å¿ƒè¡€ç®¡ç–¾ç—…ï¼ˆCVDï¼‰æ£€æµ‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è€¦åˆä¿¡å·ä¼°è®¡æ–¹æ³•ä»ç„¶é«˜åº¦å®¹æ˜“å—åˆ°å™ªå£°çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•Œçš„ä¸´åºŠå’Œç”Ÿç†ç¯å¢ƒä¸­ï¼Œè¿™å‰Šå¼±äº†å®ƒä»¬çš„ç¨³å¥æ€§å¹¶é™åˆ¶äº†å®ç”¨ä»·å€¼ã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å™ªå£°é²æ£’å¤šæ¨¡æ€è€¦åˆä¿¡å·ä¼°è®¡ï¼ˆNMCSEï¼‰ï¼Œå®ƒå°†è€¦åˆä¿¡å·ä¼°è®¡é‡æ–°å®šä¹‰ä¸ºé€šè¿‡æœ€ä¼˜ä¼ è¾“è§£å†³çš„åˆ†å¸ƒåŒ¹é…é—®é¢˜ã€‚é€šè¿‡è”åˆå¯¹é½æŒ¯å¹…å’Œæ—¶é—´ï¼ŒNMCSEé¿å…äº†å™ªå£°æ”¾å¤§ï¼Œå¹¶å®ç°äº†ç¨³å®šçš„ä¿¡å·ä¼°è®¡ã€‚å½“æ•´åˆåˆ°æ—¶ç©ºç‰¹å¾æå–ï¼ˆTSFEï¼‰ç½‘ç»œæ—¶ï¼Œä¼°è®¡çš„è€¦åˆä¿¡å·æœ‰æ•ˆåœ°å¢å¼ºäº†å¤šæ¨¡æ€èåˆï¼Œä»è€Œå®ç°äº†æ›´å‡†ç¡®çš„CVDæ£€æµ‹ã€‚ä¸ºäº†è¯„ä¼°åœ¨ç°å®æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªäº’è¡¥çš„å®éªŒï¼Œé’ˆå¯¹ä¸åŒçš„å™ªå£°æ¥æºã€‚ç¬¬ä¸€ä¸ªå®éªŒä½¿ç”¨PhysioNet 2016æ•°æ®é›†å’Œæ¨¡æ‹Ÿçš„åŒ»é™¢å™ªå£°æ¥è¯„ä¼°NMCSEå¯¹ä¸´åºŠå¹²æ‰°çš„æŠ—æ€§ã€‚ç¬¬äºŒä¸ªå®éªŒåˆ©ç”¨EPHNOGRAMæ•°æ®é›†å’Œè¿åŠ¨å¼•èµ·çš„ç”Ÿç†å™ªå£°æ¥è¯„ä¼°ä¸åŒæ´»åŠ¨æ°´å¹³ä¸‹çš„çŠ¶æ€å†…ä¼°è®¡ç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNMCSEåœ¨ä¸´åºŠå’Œç”Ÿç†å™ªå£°ä¸‹å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå‡¸æ˜¾å‡ºå®ƒæ˜¯ä¸€ç§å™ªå£°é²æ£’çš„ä¼°è®¡æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ç°å®æ¡ä»¶ä¸‹å®ç°å¯é çš„å¤šæ¨¡æ€å¿ƒè„æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18174v3">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºä¸€ç§å™ªå£°é²æ£’çš„å¤šæ¨¡æ€è€¦åˆä¿¡å·ä¼°è®¡æ–¹æ³•ï¼ˆNMCSEï¼‰ï¼Œç”¨äºåœ¨å¿ƒç”µå›¾å’Œå¿ƒéŸ³å›¾ä¹‹é—´å»ºç«‹è”ç³»ã€‚é€šè¿‡å°†æŒ¯å¹…å’Œæ—¶åºè”åˆå¯¹é½ï¼Œé¿å…å™ªå£°æ”¾å¤§å¹¶å®ç°ç¨³å®šä¿¡å·ä¼°è®¡ã€‚è¯¥ç ”ç©¶åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹æµ‹è¯•äº†è¯¥æ–¹æ³•çš„é²æ£’æ€§ï¼Œæ˜¾ç¤ºå…¶åœ¨å™ªå£°æ¡ä»¶ä¸‹è¡¨ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚è¿™ä¸ºå¤šæ¨¡æ€å¿ƒè¡€ç®¡ç–¾ç—…çš„å‡†ç¡®æ£€æµ‹æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è€¦åˆä¿¡å·æ˜¯å¿ƒç”µå›¾å’Œå¿ƒéŸ³å›¾ä¹‹é—´è¡¨å¾å¿ƒè„ç”µå…´å¥‹è½¬åŒ–ä¸ºæœºæ¢°æ”¶ç¼©çš„æ½œåœ¨ç”Ÿç†ä¿¡å·ã€‚</li>
<li>è€¦åˆä¿¡å·æ˜¯ä¸åŒæ¨¡æ€ä¹‹é—´çš„å›ºæœ‰è”ç³»ï¼Œä¸ºå¿ƒè„åŠŸèƒ½æä¾›ç»Ÿä¸€è¡¨ç¤ºï¼Œæœ‰åŠ©äºå¤šæ¨¡æ€å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹ã€‚</li>
<li>ç°æœ‰è€¦åˆä¿¡å·ä¼°è®¡æ–¹æ³•æ˜“å—å™ªå£°å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•Œçš„ä¸´åºŠå’Œç”Ÿç†ç¯å¢ƒä¸­ã€‚</li>
<li>ç ”ç©¶æå‡ºå™ªå£°é²æ£’çš„å¤šæ¨¡æ€è€¦åˆä¿¡å·ä¼°è®¡æ–¹æ³•ï¼ˆNMCSEï¼‰ï¼Œæ—¨åœ¨è§£å†³è€¦åˆä¿¡å·çš„ä¼°è®¡é—®é¢˜ã€‚</li>
<li>NMCSEé€šè¿‡å°†æŒ¯å¹…å’Œæ—¶åºè”åˆå¯¹é½ï¼Œé¿å…å™ªå£°æ”¾å¤§å¹¶å®ç°ç¨³å®šä¿¡å·ä¼°è®¡ã€‚</li>
<li>é›†æˆåˆ°æ—¶ç©ºç‰¹å¾æå–ç½‘ç»œåï¼Œä¼°è®¡çš„è€¦åˆä¿¡å·å¯æé«˜å¤šæ¨¡æ€èåˆï¼Œä¸ºæ›´å‡†ç¡®çš„å¿ƒè¡€ç®¡ç–¾ç—…æ£€æµ‹æä¾›æœ‰æ•ˆæ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5fb2cfec6787bca715558fefdc6014e" align="middle">
<img src="https://picx.zhimg.com/v2-644bfb584253343c3f0e9c47b14a6c68" align="middle">
<img src="https://picx.zhimg.com/v2-1a5f2874722196cbcdadf09e86d49282" align="middle">
<img src="https://picx.zhimg.com/v2-ee83e79b60b926d4f255f873e893fba5" align="middle">
<img src="https://picx.zhimg.com/v2-2a62197a0852809c4465a1179789b499" align="middle">
<img src="https://picx.zhimg.com/v2-9968d0be4ab721f322b8484b90e64a0c" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Multimodal-Cancer-Modeling-in-the-Age-of-Foundation-Model-Embeddings"><a href="#Multimodal-Cancer-Modeling-in-the-Age-of-Foundation-Model-Embeddings" class="headerlink" title="Multimodal Cancer Modeling in the Age of Foundation Model Embeddings"></a>Multimodal Cancer Modeling in the Age of Foundation Model Embeddings</h2><p><strong>Authors:Steven Song, Morgan Borjigin-Wang, Irene Madejski, Robert L. Grossman</strong></p>
<p>The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference dataset in cancer through its harmonized genomics, clinical, and imaging data. Numerous prior studies have developed bespoke deep learning models over TCGA for tasks such as cancer survival prediction. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive feature embeddings agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the ability to train classical machine learning models over multimodal, zero-shot FM embeddings of cancer data. We demonstrate the ease and additive effect of multimodal fusion, outperforming unimodal models. Further, we show the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we propose an embedding-centric approach to multimodal cancer modeling. </p>
<blockquote>
<p>ç™Œç—‡åŸºå› ç»„å›¾è°±ï¼ˆTCGAï¼‰é€šè¿‡å…¶ç»Ÿä¸€çš„åŸºå› ç»„å­¦ã€ä¸´åºŠå’Œæˆåƒæ•°æ®ï¼Œä¸ºç™Œç—‡ç ”ç©¶å¸¦æ¥äº†æ–°çš„å‘ç°ï¼Œå¹¶ä½œä¸ºå¤§è§„æ¨¡å‚è€ƒæ•°æ®é›†å‘æŒ¥ä½œç”¨ã€‚è®¸å¤šæ—©æœŸç ”ç©¶å·²ç»åœ¨TCGAä¸Šå¼€å‘ç”¨äºç™Œç—‡ç”Ÿå­˜é¢„æµ‹ç­‰ä»»åŠ¡çš„æ·±åº¦å®šåˆ¶æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç”Ÿç‰©åŒ»å­¦æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªç°ä»£èŒƒå¼æ˜¯å¼€å‘åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰ï¼Œä»¥æ´¾ç”Ÿå‡ºç‹¬ç«‹äºç‰¹å®šå»ºæ¨¡ä»»åŠ¡çš„ç‰¹å¾åµŒå…¥ã€‚ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬å°¤å…¶æ˜¯FMsçš„å¼€å‘å’Œåº”ç”¨åœ¨ä¸æ–­å¢åŠ ã€‚è™½ç„¶TCGAåŒ…å«äº†ä½œä¸ºç—…ç†æŠ¥å‘Šçš„æ–‡æœ¬æ•°æ®ï¼Œä½†è¿™äº›æ•°æ®åœ¨å†å²ä¸Šè¢«åˆ©ç”¨ä¸è¶³ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨å¤šæ¨¡æ€é›¶å°„æµFMåµŒå…¥çš„ç™Œç—‡æ•°æ®ä¸Šè®­ç»ƒç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜äº†å¤šæ¨¡æ€èåˆçš„ç®€å•æ€§å’Œé™„åŠ æ•ˆæœï¼Œå…¶æ€§èƒ½ä¼˜äºå•æ¨¡æ€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŒ…å«ç—…ç†æŠ¥å‘Šæ–‡æœ¬çš„å¥½å¤„ï¼Œå¹¶ä¸¥æ ¼è¯„ä¼°äº†åŸºäºæ¨¡å‹çš„æ–‡æœ¬æ‘˜è¦å’Œè™šæ„åŒ–çš„å½±å“ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥åµŒå…¥ä¸ºä¸­å¿ƒçš„ç™Œç—‡å¤šæ¨¡æ€å»ºæ¨¡æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07683v3">PDF</a> camera ready version for ML4H 2025</p>
<p><strong>Summary</strong></p>
<p>TCGAæ•°æ®åº“é€šè¿‡å…¶ç»Ÿä¸€çš„åŸºå› ç»„å­¦ã€ä¸´åºŠå’Œæˆåƒæ•°æ®ä¸ºç™Œç—‡ç ”ç©¶æä¾›äº†æ–°çš„å‘ç°ï¼Œå¹¶ä½œä¸ºå¤§è§„æ¨¡å‚è€ƒæ•°æ®é›†æœåŠ¡äºç™Œç—‡ç ”ç©¶ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤šæ¨¡æ€æ•°æ®ä¸‹ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹åµŒå…¥ç™Œç—‡æ•°æ®è®­ç»ƒç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶éªŒè¯äº†å¤šæ¨¡æ€èåˆçš„ä¼˜åŠ¿ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†ç—…ç†æŠ¥å‘Šæ–‡æœ¬çš„ä½œç”¨ï¼Œå¹¶é€šè¿‡æ¨¡å‹è¯„ä¼°äº†æ–‡æœ¬æ‘˜è¦å’Œè™šæ„çš„å½±å“ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡æå‡ºäº†ä»¥åµŒå…¥ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€ç™Œç—‡å»ºæ¨¡æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TCGAæ•°æ®åº“ä½œä¸ºç™Œç—‡ç ”ç©¶çš„å¤§è§„æ¨¡å‚è€ƒæ•°æ®é›†ï¼Œé€šè¿‡å…¶ç»Ÿä¸€çš„åŸºå› ç»„å­¦ã€ä¸´åºŠå’Œæˆåƒæ•°æ®ä¿ƒè¿›äº†ç™Œç—‡ç ”ç©¶çš„æ–°å‘ç°ã€‚</li>
<li>ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°æ®ä¸‹çš„è®­ç»ƒèƒ½åŠ›å¾—åˆ°äº†ç ”ç©¶éªŒè¯ã€‚</li>
<li>å¤šæ¨¡æ€èåˆå…·æœ‰ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç—…ç†æŠ¥å‘Šæ–‡æœ¬åœ¨ç™Œç—‡å»ºæ¨¡ä¸­çš„ä½œç”¨å¾—åˆ°äº†é‡è§†å’Œæ¢è®¨ã€‚</li>
<li>é€šè¿‡æ¨¡å‹è¯„ä¼°ï¼ŒéªŒè¯äº†æ–‡æœ¬æ‘˜è¦å’Œè™šæ„å¯¹æ¨¡å‹çš„å½±å“ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»¥åµŒå…¥ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€ç™Œç—‡å»ºæ¨¡æ–¹æ³•ï¼Œæ—¨åœ¨å……åˆ†åˆ©ç”¨å„ç§æ•°æ®æºçš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-164c2ebef3e2da4975a875e9edd0d7de" align="middle">
<img src="https://picx.zhimg.com/v2-3087aefaaaf1b95d8fcbb1805e76a9b4" align="middle">
<img src="https://picx.zhimg.com/v2-64ff2e74d9f2ddc632a9f4544b4db1ad" align="middle">
<img src="https://picx.zhimg.com/v2-a95fab82d01ff9cb1b7e8b733fde1099" align="middle">
<img src="https://picx.zhimg.com/v2-5855228188dcbdbd292b1b11f97778f3" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Accelerating-Volumetric-Medical-Image-Annotation-via-Short-Long-Memory-SAM-2"><a href="#Accelerating-Volumetric-Medical-Image-Annotation-via-Short-Long-Memory-SAM-2" class="headerlink" title="Accelerating Volumetric Medical Image Annotation via Short-Long Memory   SAM 2"></a>Accelerating Volumetric Medical Image Annotation via Short-Long Memory   SAM 2</h2><p><strong>Authors:Yuwen Chen, Zafer Yildiz, Qihang Li, Yaqian Chen, Haoyu Dong, Hanxue Gu, Nicholas Konz, Maciej A. Mazurowski</strong></p>
<p>Manual annotation of volumetric medical images, such as magnetic resonance imaging (MRI) and computed tomography (CT), is a labor-intensive and time-consuming process. Recent advancements in foundation models for video object segmentation, such as Segment Anything Model 2 (SAM 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. However, the performance of SAM 2 in this context varies. Our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. To address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. We evaluate SLM-SAM 2 on four public datasets covering organs, bones, and muscles across MRI, CT, and ultrasound videos. We show that the proposed method markedly outperforms the default SAM 2, achieving an average Dice Similarity Coefficient improvement of 0.14 and 0.10 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger resistance to over-propagation, reducing the time required to correct propagated masks by 60.575% per volume compared to SAM 2, making a notable step toward more accurate automated annotation of medical images for segmentation model development. </p>
<blockquote>
<p>å¯¹è¯¸å¦‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’Œè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç­‰ä½“ç§¯åŒ»å­¦å›¾åƒè¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†å‹å’Œè€—æ—¶é•¿çš„è¿‡ç¨‹ã€‚æœ€è¿‘çš„è§†é¢‘å¯¹è±¡åˆ†å‰²åŸºç¡€æ¨¡å‹çš„è¿›å±•ï¼Œå¦‚Segment Anything Model 2ï¼ˆSAM 2ï¼‰ï¼Œé€šè¿‡æ‰‹åŠ¨æ ‡æ³¨ä¸€ä¸ªæˆ–å¤šä¸ªåˆ‡ç‰‡å¹¶åœ¨æ•´ä¸ªä½“ç§¯ä¸Šä¼ æ’­ç›®æ ‡è’™ç‰ˆï¼Œä¸ºåŠ é€Ÿæ ‡æ³¨è¿‡ç¨‹æä¾›äº†æ½œåœ¨çš„æœºä¼šã€‚ç„¶è€Œï¼ŒSAM 2åœ¨æ­¤èƒŒæ™¯ä¸‹çš„è¡¨ç°æœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¾èµ–å•ä¸€å†…å­˜é“¶è¡Œå’Œæ³¨æ„åŠ›æ¨¡å—å®¹æ˜“å‡ºç°è¯¯å·®ä¼ æ’­ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡å‡ºç°åœ¨å‰ä¸€åˆ‡ç‰‡ä¸­ä½†å½“å‰åˆ‡ç‰‡ä¸­æ²¡æœ‰å‡ºç°çš„è¾¹ç•ŒåŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Short-Long Memory SAM 2ï¼ˆSLM-SAM 2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œèåˆäº†çŸ­æœŸå’Œé•¿æœŸå†…å­˜é“¶è¡Œä»¥åŠç‹¬ç«‹çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬MRIã€CTå’Œè¶…å£°è§†é¢‘çš„å™¨å®˜ã€éª¨éª¼å’Œè‚Œè‚‰æ–¹é¢çš„å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°äº†SLM-SAM 2ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åˆå§‹é€‚åº”æ—¶å¯ç”¨5ä¸ªå·å’Œ1ä¸ªå·çš„æƒ…å†µä¸‹ï¼Œåˆ†åˆ«å®ç°äº†å¹³å‡Diceç›¸ä¼¼ç³»æ•°æé«˜0.14å’Œ0.10çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚SLM-SAM 2è¿˜è¡¨ç°å‡ºæ›´å¼ºçš„æŠ—è¿‡åº¦ä¼ æ’­æ€§ï¼Œä¸SAM 2ç›¸æ¯”ï¼Œæ¯ä¸ªä½“ç§¯æ ¡æ­£ä¼ æ’­è’™ç‰ˆæ‰€éœ€çš„æ—¶é—´å‡å°‘äº†60.575%ï¼Œè¿™åœ¨æœç€æ›´å‡†ç¡®è‡ªåŠ¨æ ‡æ³¨åŒ»å­¦å›¾åƒä»¥è¿›è¡Œåˆ†å‰²æ¨¡å‹å¼€å‘æ–¹é¢è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01854v2">PDF</a> Accepted for publication in IEEE Transactions on Medical Imaging   (IEEE TMI)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦å›¾åƒæ‰‹åŠ¨æ ‡æ³¨çš„ç¹çå’Œè€—æ—¶é—®é¢˜ï¼Œå¹¶æ¢è®¨äº†ä½¿ç”¨Segment Anything Model 2ï¼ˆSAM 2ï¼‰è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œå®éªŒä¸­å‘ç°äº†SAM 2åœ¨è¾¹ç•ŒåŒºåŸŸå®¹æ˜“å‡ºç°è¯¯å·®ä¼ æ’­çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Short-Long Memory SAM 2ï¼ˆSLM-SAM 2ï¼‰ï¼Œé€šè¿‡å¼•å…¥çŸ­æœŸå’Œé•¿æœŸè®°å¿†åº“ä»¥åŠç‹¬ç«‹çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚åœ¨MRIã€CTå’Œè¶…å£°è§†é¢‘ç­‰å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSLM-SAM 2åœ¨åˆå§‹é€‚åº”æ—¶è¾ƒSAM 2æœ‰æ˜æ˜¾æå‡ï¼ŒDiceç›¸ä¼¼ç³»æ•°å¹³å‡æé«˜äº†0.14å’Œ0.10ã€‚æ­¤å¤–ï¼ŒSLM-SAM 2è¿˜å…·æœ‰è¾ƒå¼ºçš„æŠ—è¿‡åº¦ä¼ æ’­èƒ½åŠ›ï¼Œå‡å°‘äº†ä¿®æ­£ä¼ æ’­æ©è†œæ‰€éœ€çš„æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹åŠ¨æ ‡æ³¨åŒ»å­¦å›¾åƒæ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†ä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚</li>
<li>Segment Anything Model 2 (SAM 2) å¯ç”¨äºåŠ é€Ÿæ ‡æ³¨è¿‡ç¨‹ã€‚</li>
<li>SAM 2åœ¨è¾¹ç•ŒåŒºåŸŸå­˜åœ¨è¯¯å·®ä¼ æ’­é—®é¢˜ã€‚</li>
<li>Short-Long Memory SAM 2 (SLM-SAM 2) é€šè¿‡ç»“åˆçŸ­æœŸå’Œé•¿æœŸè®°å¿†åº“åŠç‹¬ç«‹æ³¨æ„åŠ›æ¨¡å—ï¼Œæé«˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²ç²¾åº¦ã€‚</li>
<li>SLM-SAM 2åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºSAM 2ï¼ŒDiceç›¸ä¼¼ç³»æ•°æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>SLM-SAM 2å…·æœ‰è¾ƒå¼ºçš„æŠ—è¿‡åº¦ä¼ æ’­èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e95fc6551f711c54ed7d9ca2de161102" align="middle">
<img src="https://picx.zhimg.com/v2-0fdf14a41f68d8921ca08cc12a852911" align="middle">
<img src="https://picx.zhimg.com/v2-b3222c16930beb4c1ff7fac1d7534b11" align="middle">
<img src="https://picx.zhimg.com/v2-ec335c13a882c83de8ce565d93997e28" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="RadZero-Similarity-Based-Cross-Attention-for-Explainable-Vision-Language-Alignment-in-Chest-X-ray-with-Zero-Shot-Multi-Task-Capability"><a href="#RadZero-Similarity-Based-Cross-Attention-for-Explainable-Vision-Language-Alignment-in-Chest-X-ray-with-Zero-Shot-Multi-Task-Capability" class="headerlink" title="RadZero: Similarity-Based Cross-Attention for Explainable   Vision-Language Alignment in Chest X-ray with Zero-Shot Multi-Task Capability"></a>RadZero: Similarity-Based Cross-Attention for Explainable   Vision-Language Alignment in Chest X-ray with Zero-Shot Multi-Task Capability</h2><p><strong>Authors:Jonggwon Park, Byungmu Yoon, Soobum Kim, Kyoyun Choi</strong></p>
<p>Recent advancements in multimodal models have significantly improved vision-language (VL) alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning and offer limited interpretability through attention probability visualizations. To address these challenges, we introduce $\textbf{RadZero}$, a novel framework for VL alignment in chest X-ray with zero-shot multi-task capability. A key component of our approach is $\textbf{VL-CABS}$ ($\textbf{V}$ision-$\textbf{L}$anguage $\textbf{C}$ross-$\textbf{A}$ttention $\textbf{B}$ased on $\textbf{S}$imilarity), which aligns text embeddings with local image features for interpretable, fine-grained VL reasoning. RadZero leverages large language models to extract concise semantic sentences from radiology reports and employs multi-positive contrastive training to effectively capture relationships between images and multiple relevant textual descriptions. It uses a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, VL-CABS enables zero-shot inference with similarity probability for classification, and pixel-level VL similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, VL similarity map analysis highlights the potential of VL-CABS for improving explainability in VL alignment. Additionally, qualitative evaluation demonstrates RadZeroâ€™s capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging. Code is available at $\href{<a target="_blank" rel="noopener" href="https://github.com/deepnoid-ai/RadZero%7D%7Bhttps://github.com/deepnoid-ai/RadZero%7D$">https://github.com/deepnoid-ai/RadZero}{https://github.com/deepnoid-ai/RadZero}$</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€æ¨¡å‹çš„è¿›å±•åœ¨æ”¾å°„å­¦ä¸­çš„è§†è§‰è¯­è¨€ï¼ˆVLï¼‰å¯¹é½æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆè¿ç”¨å¤æ‚çš„æ”¾å°„å­¦æŠ¥å‘Šè¿›è¡Œå­¦ä¹ ï¼Œå¹¶ä¸”é€šè¿‡æ³¨æ„åŠ›æ¦‚ç‡å¯è§†åŒ–æä¾›çš„è§£é‡Šæ€§æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<strong>RadZero</strong>ï¼Œä¸€ä¸ªå…·æœ‰é›¶æ ·æœ¬å¤šä»»åŠ¡èƒ½åŠ›çš„èƒ¸éƒ¨Xå…‰è§†è§‰è¯­è¨€å¯¹é½æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®ç»„ä»¶æ˜¯<strong>VL-CABS</strong>ï¼ˆåŸºäºç›¸ä¼¼æ€§çš„è§†è§‰è¯­è¨€è·¨æ³¨æ„åŠ›ï¼‰ï¼Œå®ƒå°†æ–‡æœ¬åµŒå…¥ä¸å±€éƒ¨å›¾åƒç‰¹å¾å¯¹é½ï¼Œä»¥å®ç°å¯è§£é‡Šã€ç²¾ç»†çš„VLæ¨ç†ã€‚RadZeroåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä»æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–ç®€æ´çš„è¯­ä¹‰å¥å­ï¼Œå¹¶é‡‡ç”¨å¤šé˜³æ€§å¯¹æ¯”è®­ç»ƒæ¥æœ‰æ•ˆæ•æ‰å›¾åƒä¸å¤šä¸ªç›¸å…³æ–‡æœ¬æè¿°ä¹‹é—´çš„å…³ç³»ã€‚å®ƒä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä»¥åŠé¢å¤–çš„å¯è®­ç»ƒTransformerå±‚ï¼Œä»¥å®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ã€‚é€šè¿‡è®¡ç®—æ–‡æœ¬åµŒå…¥å’Œå±€éƒ¨å›¾åƒè¡¥ä¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ŒVL-CABSå®ç°äº†é›¶æ ·æœ¬æ¨æ–­ï¼Œå…·æœ‰åˆ†ç±»çš„ç›¸ä¼¼æ€§æ¦‚ç‡ï¼Œä»¥åŠç”¨äºæ¥åœ°å’Œåˆ†å‰²çš„åƒç´ çº§VLç›¸ä¼¼æ€§åœ°å›¾ã€‚åœ¨å…¬å…±èƒ¸éƒ¨Xå…‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRadZeroåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€æ¥åœ°å’Œåˆ†å‰²æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒVLç›¸ä¼¼æ€§åœ°å›¾åˆ†æçªå‡ºäº†VL-CABSåœ¨æé«˜VLå¯¹é½çš„è§£é‡Šæ€§æ–¹é¢çš„æ½œåŠ›ã€‚å®šæ€§è¯„ä¼°ä¹Ÿè¯æ˜äº†RadZeroåœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨åŒ»å­¦å½±åƒä¸­çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨[<a target="_blank" rel="noopener" href="https://github.com/deepnoid-ai/RadZero]%E3%80%82">https://github.com/deepnoid-ai/RadZero]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07416v3">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRadZeroçš„æ–°æ¡†æ¶ï¼Œç”¨äºèƒ¸éƒ¨Xå…‰ç‰‡çš„è§†è§‰è¯­è¨€å¯¹é½ã€‚å®ƒå€ŸåŠ©å¤šæ¨¡æ€æ¨¡å‹çš„æŠ€æœ¯è¿›æ­¥ï¼Œæå‡äº†ç†è§£å’Œè§£é‡Šæ”¾å°„å­¦æŠ¥å‘Šçš„èƒ½åŠ›ã€‚RadZeroé‡‡ç”¨VL-CABSæŠ€æœ¯å¯¹é½æ–‡æœ¬åµŒå…¥å’Œå±€éƒ¨å›¾åƒç‰¹å¾ï¼Œå¹¶å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç²¾ç»†åŒ–æ¨ç†ã€‚æ­¤å¤–ï¼ŒRadZeroé‡‡ç”¨å¤šé˜³æ€§å¯¹æ¯”è®­ç»ƒæ¥æ•æ‰å›¾åƒå’Œå¤šä¸ªç›¸å…³æ–‡æœ¬æè¿°ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶é€šè¿‡é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œå¯è®­ç»ƒçš„Transformerå±‚å®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRadZeroåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å®šä½ã€åˆ†å‰²ç­‰æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RadZeroæ˜¯ä¸€ä¸ªç”¨äºèƒ¸éƒ¨Xå…‰ç‰‡è§†è§‰è¯­è¨€å¯¹é½çš„æ–°æ¡†æ¶ï¼Œå…·æœ‰é›¶æ ·æœ¬å¤šä»»åŠ¡èƒ½åŠ›ã€‚</li>
<li>RadZeroé€šè¿‡VL-CABSæŠ€æœ¯å®ç°æ–‡æœ¬åµŒå…¥å’Œå±€éƒ¨å›¾åƒç‰¹å¾çš„å¯¹é½ï¼Œæé«˜äº†ç†è§£å’Œè§£é‡Šæ”¾å°„å­¦æŠ¥å‘Šçš„èƒ½åŠ›ã€‚</li>
<li>RadZeroå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç²¾ç»†åŒ–æ¨ç†ï¼Œå¹¶é‡‡ç”¨å¤šé˜³æ€§å¯¹æ¯”è®­ç»ƒæ¥æ•æ‰å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>RadZeroä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’Œå¯è®­ç»ƒçš„Transformerå±‚è¿›è¡Œé«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒå¤„ç†ã€‚</li>
<li>RadZeroé€šè¿‡è®¡ç®—æ–‡æœ¬åµŒå…¥å’Œå±€éƒ¨å›¾åƒè¡¥ä¸ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå®ç°äº†é›¶æ ·æœ¬æ¨æ–­å’Œåƒç´ çº§è§†è§‰è¯­è¨€ç›¸ä¼¼æ€§åœ°å›¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRadZeroåœ¨é›¶æ ·æœ¬åˆ†ç±»ã€å®šä½ã€åˆ†å‰²ç­‰æ–¹é¢å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œä¸”å…¶è§†è§‰è¯­è¨€ç›¸ä¼¼æ€§åœ°å›¾åˆ†ææœ‰åŠ©äºæé«˜è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb93b79bdb2eaf025c52f46f671206b8" align="middle">
<img src="https://picx.zhimg.com/v2-b7dd6e664c43d5101560c795c58c01bd" align="middle">
<img src="https://picx.zhimg.com/v2-336ff77ba1321cc58863bcf02dd06279" align="middle">
<img src="https://picx.zhimg.com/v2-9a89d7c8802784bf6a240c2db9ae9cef" align="middle">
<img src="https://picx.zhimg.com/v2-756aab9a67229690587c65a2567edcd8" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Augmented-Reality-based-Guidance-with-Deformable-Registration-in-Head-and-Neck-Tumor-Resection"><a href="#Augmented-Reality-based-Guidance-with-Deformable-Registration-in-Head-and-Neck-Tumor-Resection" class="headerlink" title="Augmented Reality-based Guidance with Deformable Registration in Head   and Neck Tumor Resection"></a>Augmented Reality-based Guidance with Deformable Registration in Head   and Neck Tumor Resection</h2><p><strong>Authors:Qingyun Yang, Fangjie Li, Jiayi Xu, Zixuan Liu, Sindhura Sridhar, Whitney Jin, Jennifer Du, Jon Heiselman, Michael Miga, Michael Topf, Jie Ying Wu</strong></p>
<p>Head and neck squamous cell carcinoma (HNSCC) has one of the highest rates of recurrence cases among solid malignancies. Recurrence rates can be reduced by improving positive margins localization. Frozen section analysis (FSA) of resected specimens is the gold standard for intraoperative margin assessment. However, because of the complex 3D anatomy and the significant shrinkage of resected specimens, accurate margin relocation from specimen back onto the resection site based on FSA results remains challenging. We propose a novel deformable registration framework that uses both the pre-resection upper surface and the post-resection site of the specimen to incorporate thickness information into the registration process. The proposed method significantly improves target registration error (TRE), demonstrating enhanced adaptability to thicker specimens. In tongue specimens, the proposed framework improved TRE by up to 33% as compared to prior deformable registration. Notably, tongue specimens exhibit complex 3D anatomies and hold the highest clinical significance compared to other head and neck specimens from the buccal and skin. We analyzed distinct deformation behaviors in different specimens, highlighting the need for tailored deformation strategies. To further aid intraoperative visualization, we also integrated this framework with an augmented reality-based auto-alignment system. The combined system can accurately and automatically overlay the deformed 3D specimen mesh with positive margin annotation onto the resection site. With a pilot study of the AR guided framework involving two surgeons, the integrated system improved the surgeonsâ€™ average target relocation error from 9.8 cm to 4.8 cm. </p>
<blockquote>
<p>å¤´é¢ˆéƒ¨é³çŠ¶ç»†èƒç™Œï¼ˆHNSCCï¼‰åœ¨å®ä½“æ¶æ€§è‚¿ç˜¤ä¸­å¤å‘ç‡è¾ƒé«˜ã€‚é€šè¿‡æ”¹å–„é˜³æ€§è¾¹ç¼˜å®šä½å¯ä»¥é™ä½å¤å‘ç‡ã€‚å†°å†»åˆ‡ç‰‡åˆ†æï¼ˆFSAï¼‰æ˜¯åˆ‡é™¤æ ‡æœ¬çš„æœ¯ä¸­è¾¹ç¼˜è¯„ä¼°çš„é‡‘æ ‡å‡†ã€‚ç„¶è€Œï¼Œç”±äºå¤æ‚çš„3Dè§£å‰–ç»“æ„å’Œåˆ‡é™¤æ ‡æœ¬çš„æ˜¾è‘—æ”¶ç¼©ï¼Œæ ¹æ®FSAç»“æœå‡†ç¡®åœ°å°†è¾¹ç¼˜é‡æ–°å®šä½åˆ°åˆ‡é™¤éƒ¨ä½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯å˜å½¢æ³¨å†Œæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨åˆ‡é™¤å‰çš„ä¸Šè¡¨é¢å’Œåˆ‡é™¤åçš„æ ‡æœ¬éƒ¨ä½ï¼Œå°†åšåº¦ä¿¡æ¯çº³å…¥æ³¨å†Œè¿‡ç¨‹ä¸­ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ç›®æ ‡æ³¨å†Œè¯¯å·®ï¼ˆTREï¼‰ï¼Œå¹¶è¡¨ç°å‡ºå¯¹è¾ƒåšæ ‡æœ¬çš„é€‚åº”æ€§å¢å¼ºã€‚åœ¨èˆŒæ ‡æœ¬ä¸­ï¼Œä¸ä¹‹å‰çš„å¯å˜å½¢æ³¨å†Œç›¸æ¯”ï¼Œæ‰€æå‡ºæ¡†æ¶çš„TREæé«˜äº†é«˜è¾¾33%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒèˆŒæ ‡æœ¬å…·æœ‰å¤æ‚çš„3Dè§£å‰–ç»“æ„ï¼Œç›¸è¾ƒäºæ¥è‡ªé¢Šéƒ¨å’Œçš®è‚¤çš„å…¶å®ƒå¤´é¢ˆéƒ¨æ ‡æœ¬ï¼Œå…¶ä¸´åºŠæ„ä¹‰æœ€é«˜ã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒæ ‡æœ¬ä¸­ä¸åŒçš„å˜å½¢è¡Œä¸ºï¼Œå¼ºè°ƒäº†éœ€è¦å®šåˆ¶å˜å½¢ç­–ç•¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¾…åŠ©æœ¯ä¸­å¯è§†åŒ–ï¼Œæˆ‘ä»¬è¿˜å°†æ­¤æ¡†æ¶ä¸åŸºäºå¢å¼ºç°å®çš„è‡ªåŠ¨å¯¹é½ç³»ç»Ÿè¿›è¡Œäº†é›†æˆã€‚è¯¥ç»¼åˆç³»ç»Ÿå¯ä»¥å‡†ç¡®ã€è‡ªåŠ¨åœ°å°†å˜å½¢çš„3Dæ ‡æœ¬ç½‘æ ¼ä¸é˜³æ€§è¾¹ç¼˜æ³¨é‡Šå åŠ åˆ°åˆ‡é™¤éƒ¨ä½ã€‚é€šè¿‡æ¶‰åŠä¸¤åå¤–ç§‘åŒ»ç”Ÿçš„ARå¼•å¯¼æ¡†æ¶è¯•ç‚¹ç ”ç©¶ï¼Œé›†æˆç³»ç»Ÿä½¿åŒ»ç”Ÿçš„ç›®æ ‡å¹³å‡é‡æ–°å®šä½è¯¯å·®ä»9.8å˜ç±³å‡å°‘åˆ°4.8å˜ç±³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08802v2">PDF</a> Accepted at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å˜å½¢æ³¨å†Œæ¡†æ¶ï¼Œåˆ©ç”¨æœ¯å‰å’Œæœ¯åçš„è¡¨é¢ä¿¡æ¯ï¼Œç»“åˆåšåº¦ä¿¡æ¯ï¼Œå¯¹æ‰‹æœ¯åˆ‡é™¤æ ‡æœ¬è¿›è¡Œå‡†ç¡®çš„è¾¹ç¼˜å®šä½ã€‚è¯¥æ¡†æ¶åœ¨å¤´é¢ˆéƒ¨é³çŠ¶ç»†èƒç™Œçš„æ‰‹æœ¯ä¸­æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å…·æœ‰å¤æ‚ä¸‰ç»´ç»“æ„çš„èˆŒæ ‡æœ¬æ—¶ã€‚ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ¡†æ¶èƒ½æé«˜ç›®æ ‡æ³¨å†Œè¯¯å·®çš„å‡†ç¡®åº¦ï¼Œå¹¶é›†æˆå¢å¼ºç°å®æŠ€æœ¯ï¼Œå®ç°è‡ªåŠ¨å¯¹é½ï¼Œè¾…åŠ©æœ¯ä¸­å¯è§†åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤´é¢ˆéƒ¨é³çŠ¶ç»†èƒç™Œå¤å‘ç‡é«˜ï¼Œæ”¹å–„é˜³æ€§è¾¹ç¼˜å®šä½èƒ½é™ä½å¤å‘ç‡ã€‚</li>
<li>å†°å†»åˆ‡ç‰‡åˆ†ææ˜¯æœ¯ä¸­è¾¹ç¼˜è¯„ä¼°çš„é‡‘æ ‡å‡†ï¼Œä½†æ ‡æœ¬çš„3Då¤æ‚ç»“æ„å’Œæ”¶ç¼©æ€§ä½¿å‡†ç¡®è¾¹ç¼˜å®šä½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ–°å‹å˜å½¢æ³¨å†Œæ¡†æ¶ç»“åˆæœ¯å‰å’Œæœ¯åçš„è¡¨é¢ä¿¡æ¯ï¼Œè€ƒè™‘åšåº¦å› ç´ ï¼Œæ˜¾è‘—æé«˜ç›®æ ‡æ³¨å†Œè¯¯å·®çš„å‡†ç¡®åº¦ã€‚</li>
<li>åœ¨èˆŒæ ‡æœ¬ä¸Šï¼Œæ–°æ¡†æ¶æ¯”ä¼ ç»Ÿæ–¹æ³•èƒ½æé«˜ç›®æ ‡æ³¨å†Œè¯¯å·®çš„å‡†ç¡®åº¦è¾¾33%ã€‚</li>
<li>èˆŒæ ‡æœ¬å…·æœ‰å¤æ‚çš„3Dç»“æ„ï¼Œåœ¨ä¸´åºŠä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>éœ€è¦é’ˆå¯¹ä¸åŒæ ‡æœ¬çš„å˜å½¢è¡Œä¸ºåˆ¶å®šå®šåˆ¶åŒ–çš„å˜å½¢ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-499a9d02bc57303818aa96773b25126d" align="middle">
<img src="https://picx.zhimg.com/v2-389a1f05e25ff36431b3418c60af8474" align="middle">
<img src="https://picx.zhimg.com/v2-f780695547017fd62eabf020b7a41fab" align="middle">
<img src="https://picx.zhimg.com/v2-9b4b63c7395a1f6497131682b1096528" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Dual-Input-Dynamic-Convolution-for-Positron-Range-Correction-in-PET-Image-Reconstruction"><a href="#Dual-Input-Dynamic-Convolution-for-Positron-Range-Correction-in-PET-Image-Reconstruction" class="headerlink" title="Dual-Input Dynamic Convolution for Positron Range Correction in PET   Image Reconstruction"></a>Dual-Input Dynamic Convolution for Positron Range Correction in PET   Image Reconstruction</h2><p><strong>Authors:Youness Mellak, Alexandre Bousse, Thibaut Merlin, Ã‰lise Ã‰mond, Mikko Hakulinen, Dimitris Visvikis</strong></p>
<p>Positron range (PR) blurring degrades positron emission tomography (PET) image resolution, particularly for high-energy emitters like gallium-68 (68Ga). We introduce Dual-input Dynamic Convolution (DDConv), a novel computationally efficient approach trained with voxel-specific PR point spread functions (PSFs) from Monte Carlo (MC) simulations and designed to be utilized within an iterative reconstruction algorithm to perform PR correction (PRC). By dynamically inferring local blurring kernels through a trained convolutional neural network (CNN), DDConv captures complex tissue interfaces more accurately than prior methods. Crucially, it also computes the transpose of the PR operator, ensuring consistency within iterative PET reconstruction. Comparisons with a state-of-the-art, tissue-dependent correction confirm the advantages of DDConv in recovering higher-resolution details in heterogeneous regions, including bone-soft tissue and lung-soft tissue boundaries. Experiments across digital phantoms and MC-simulated data show that DDConv offers near-MC accuracy, and outperforms the state-of-the-art technique, namely spatially-variant and tissue-dependent (SVTD), especially in areas with complex material interfaces. Results from physical phantom experiments further confirmed DDConvâ€™s robustness and practical applicability: while both DDConv and SVTD performed similarly in homogeneous soft-tissue regions, DDConv provided more accurate activity recovery and sharper delineation at heterogeneous lung-soft tissue interfaces. </p>
<blockquote>
<p>æ­£ç”µå­èŒƒå›´ï¼ˆPRï¼‰æ¨¡ç³Šé™ä½äº†æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒçš„åˆ†è¾¨ç‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåƒé•“-68ï¼ˆ68Gaï¼‰è¿™æ ·çš„é«˜èƒ½å‘å°„ä½“ã€‚æˆ‘ä»¬å¼•å…¥äº†åŒè¾“å…¥åŠ¨æ€å·ç§¯ï¼ˆDDConvï¼‰è¿™ä¸€æ–°å‹è®¡ç®—æ•ˆç‡é«˜çš„æ–¹æ³•ï¼Œå®ƒé‡‡ç”¨è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰æ¨¡æ‹Ÿçš„ç‰¹å®šäºä½“ç´ çš„PRç‚¹æ‰©æ•£å‡½æ•°ï¼ˆPSFsï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶è¢«è®¾è®¡ç”¨äºè¿­ä»£é‡å»ºç®—æ³•ä¸­æ‰§è¡ŒPRæ ¡æ­£ï¼ˆPRCï¼‰ã€‚DDConvé€šè¿‡è®­ç»ƒæœ‰ç´ çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åŠ¨æ€æ¨æ–­å±€éƒ¨æ¨¡ç³Šæ ¸ï¼Œèƒ½æ›´å‡†ç¡®åœ°æ•æ‰å¤æ‚çš„ç»„ç»‡ç•Œé¢ç›¸æ¯”äºä¹‹å‰çš„æ–¹æ³•ã€‚å…³é”®çš„æ˜¯ï¼Œå®ƒè¿˜è®¡ç®—PRç®—å­çš„è½¬ç½®ï¼Œç¡®ä¿è¿­ä»£PETé‡å»ºä¸­çš„ä¸€è‡´æ€§ã€‚ä¸æœ€å…ˆè¿›çš„ç»„ç»‡ç›¸å…³æ ¡æ­£æ–¹æ³•çš„æ¯”è¾ƒï¼Œè¯å®äº†DDConvåœ¨æ¢å¤å¼‚è´¨åŒºåŸŸçš„é«˜åˆ†è¾¨ç‡ç»†èŠ‚æ–¹é¢çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬éª¨-è½¯ç»„ç»‡ä»¥åŠè‚º-è½¯ç»„ç»‡è¾¹ç•Œã€‚åœ¨æ•°å­—å¹»å½±å’ŒMCæ¨¡æ‹Ÿæ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDDConvæ¥è¿‘MCçš„ç²¾åº¦ï¼Œå¹¶ä¸”ä¼˜äºæœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œå³ç©ºé—´å¯å˜å’Œç»„ç»‡ä¾èµ–æ€§ï¼ˆSVTDï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¤æ‚ææ–™ç•Œé¢çš„åŒºåŸŸã€‚ç‰©ç†å¹»å½±å®éªŒçš„ç»“æœè¿›ä¸€æ­¥è¯å®äº†DDConvçš„ç¨³å¥æ€§å’Œå®ç”¨æ€§ï¼šè™½ç„¶DDConvå’ŒSVTDåœ¨å‡åŒ€çš„è½¯ç»„ç»‡åŒºåŸŸè¡¨ç°ç›¸ä¼¼ï¼Œä½†DDConvåœ¨å¼‚è´¨è‚º-è½¯ç»„ç»‡ç•Œé¢æä¾›äº†æ›´å‡†ç¡®çš„æ´»æ€§æ¢å¤å’Œæ›´æ¸…æ™°çš„è½®å»“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00587v2">PDF</a> 11 pages, 10 figures, 2 tables</p>
<p><strong>Summary</strong><br>     æ­£ç”µå­èŒƒå›´ï¼ˆPRï¼‰æ¨¡ç³Šä¼šé™ä½æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒçš„åˆ†è¾¨ç‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé«˜èƒ½é‡å‘å°„ä½“å¦‚é•“-68ï¼ˆ68Gaï¼‰ã€‚ç ”ç©¶å¼•å…¥äº†åŒè¾“å…¥åŠ¨æ€å·ç§¯ï¼ˆDDConvï¼‰è¿™ä¸€æ–°å‹è®¡ç®—æ•ˆç‡é«˜çš„æ–¹æ³•ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰æ¨¡æ‹Ÿçš„åƒç´ çº§PRç‚¹æ‰©æ•£å‡½æ•°ï¼ˆPSFsï¼‰è¿›è¡Œè®­ç»ƒï¼Œç”¨äºè¿­ä»£é‡å»ºç®—æ³•ä¸­çš„PRæ ¡æ­£ï¼ˆPRCï¼‰ã€‚DDConvé€šè¿‡è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œåŠ¨æ€æ¨æ–­å±€éƒ¨æ¨¡ç³Šæ ¸ï¼Œæ›´å‡†ç¡®åœ°æ•æ‰å¤æ‚ç»„ç»‡ç•Œé¢ã€‚åŒæ—¶ï¼Œå®ƒè®¡ç®—PRç®—å­çš„è½¬ç½®ï¼Œç¡®ä¿è¿­ä»£PETé‡å»ºè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚ä¸æœ€æ–°å…ˆè¿›æŠ€æœ¯çš„ç»„ç»‡ä¾èµ–æ€§æ ¡æ­£ç›¸æ¯”ï¼ŒDDConvåœ¨æ¢å¤å¼‚è´¨åŒºåŸŸçš„è¾ƒé«˜åˆ†è¾¨ç‡ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼ŒåŒ…æ‹¬éª¨-è½¯ç»„ç»‡ã€è‚º-è½¯ç»„ç»‡è¾¹ç•Œç­‰ã€‚æ•°å­—å’Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ•°æ®æ˜¾ç¤ºï¼ŒDDConvæ¥è¿‘è’™ç‰¹å¡æ´›ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¤æ‚ææ–™ç•Œé¢çš„åŒºåŸŸï¼Œä¼˜äºç©ºé—´å¯å˜å’Œç»„ç»‡ä¾èµ–æ€§ï¼ˆSVTDï¼‰æŠ€æœ¯ã€‚ç‰©ç†å¹»å½±å®éªŒçš„ç»“æœè¿›ä¸€æ­¥è¯å®äº†DDConvçš„ç¨³å¥æ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Positron range (PR) blurringæ˜¯å½±å“æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒåˆ†è¾¨ç‡çš„ä¸€ä¸ªé‡è¦é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨é«˜èƒ½é‡å‘å°„ä½“æ—¶ã€‚</li>
<li>åŒè¾“å…¥åŠ¨æ€å·ç§¯ï¼ˆDDConvï¼‰æ˜¯ä¸€ç§æ–°å‹çš„PRæ ¡æ­£æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œæ¥åŠ¨æ€æ¨æ–­å±€éƒ¨æ¨¡ç³Šæ ¸ï¼Œä»¥æ”¹å–„PETå›¾åƒçš„åˆ†è¾¨ç‡ã€‚</li>
<li>DDConvèƒ½å¤Ÿæ›´å‡†ç¡®åœ°æ•æ‰å¤æ‚ç»„ç»‡ç•Œé¢ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>DDConvé€šè¿‡è®¡ç®—PRç®—å­çš„è½¬ç½®ï¼Œç¡®ä¿è¿­ä»£PETé‡å»ºè¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚</li>
<li>DDConvåœ¨æ¢å¤å¼‚è´¨åŒºåŸŸçš„è¾ƒé«˜åˆ†è¾¨ç‡ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨éª¨-è½¯ç»„ç»‡ã€è‚º-è½¯ç»„ç»‡è¾¹ç•Œç­‰åŒºåŸŸã€‚</li>
<li>DDConvåœ¨æ•°å­—å’Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ•°æ®ä¸­è¡¨ç°å‡ºè¿‘MCç²¾åº¦ï¼Œä¼˜äºç°æœ‰çš„ç©ºé—´å¯å˜å’Œç»„ç»‡ä¾èµ–æ€§ï¼ˆSVTDï¼‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55b73786af2b34b746a864f0d6a0ea27" align="middle">
<img src="https://picx.zhimg.com/v2-c8d1b7546372a4515dd453b827c6ba98" align="middle">
<img src="https://picx.zhimg.com/v2-ee1aa50b81f3b858c0e8597fb11d9ce2" align="middle">
<img src="https://picx.zhimg.com/v2-db2970bb6a8e2519b82cc8df1af86513" align="middle">
<img src="https://picx.zhimg.com/v2-ef1eb9555448897e89dea8eee2d16f13" align="middle">
<img src="https://picx.zhimg.com/v2-d093334147a2d53187f8eead3f2bb6e0" align="middle">
<img src="https://picx.zhimg.com/v2-5978d3c01bec299854453dc3cb4e816b" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="BEN-Using-Confidence-Guided-Matting-for-Dichotomous-Image-Segmentation"><a href="#BEN-Using-Confidence-Guided-Matting-for-Dichotomous-Image-Segmentation" class="headerlink" title="BEN: Using Confidence-Guided Matting for Dichotomous Image Segmentation"></a>BEN: Using Confidence-Guided Matting for Dichotomous Image Segmentation</h2><p><strong>Authors:Maxwell Meyer, Jack Spruyt</strong></p>
<p>Current approaches to dichotomous image segmentation (DIS) treat image matting and object segmentation as fundamentally different tasks. As improvements in image segmentation become increasingly challenging to achieve, combining image matting and grayscale segmentation techniques offers promising new directions for architectural innovation. Inspired by the possibility of aligning these two model tasks, we propose a new architectural approach for DIS called Confidence-Guided Matting (CGM). We created the first CGM model called Background Erase Network (BEN). BEN consists of two components: BEN Base for initial segmentation and BEN Refiner for confidence-based refinement. Our approach achieves substantial improvements over current state-of-the-art methods on the DIS5K validation dataset, demonstrating that matting-based refinement can significantly enhance segmentation quality. This work introduces a new paradigm for integrating matting and segmentation techniques, improving fine-grained object boundary prediction in computer vision. </p>
<blockquote>
<p>å½“å‰äºŒåˆ†å›¾åƒåˆ†å‰²ï¼ˆDISï¼‰çš„æ–¹æ³•å°†å›¾åƒæŠ å›¾å’Œå¯¹è±¡åˆ†å‰²è§†ä¸ºæ ¹æœ¬ä¸åŒçš„ä»»åŠ¡ã€‚éšç€å›¾åƒåˆ†å‰²çš„æ”¹è¿›è¶Šæ¥è¶Šéš¾ä»¥å®ç°ï¼Œç»“åˆå›¾åƒæŠ å›¾å’Œç°åº¦åˆ†å‰²æŠ€æœ¯ä¸ºæ¶æ„åˆ›æ–°æä¾›äº†æœ‰å‰æ™¯çš„æ–°æ–¹å‘ã€‚å—å¯¹é½è¿™ä¸¤ä¸ªæ¨¡å‹ä»»åŠ¡çš„å¯èƒ½æ€§çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”¨äºDISçš„æ¶æ„æ–¹æ³•ï¼Œç§°ä¸ºç½®ä¿¡åº¦å¼•å¯¼æŠ å›¾ï¼ˆCGMï¼‰ã€‚æˆ‘ä»¬åˆ›å»ºäº†ç¬¬ä¸€ä¸ªCGMæ¨¡å‹ï¼Œç§°ä¸ºèƒŒæ™¯æ“¦é™¤ç½‘ç»œï¼ˆBENï¼‰ã€‚BENç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šç”¨äºåˆå§‹åˆ†å‰²çš„BEN Baseå’ŒåŸºäºç½®ä¿¡åº¦è¿›è¡Œç²¾ç»†è°ƒæ•´çš„BEN Refinerã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨DIS5KéªŒè¯æ•°æ®é›†ä¸Šå®ç°äº†å¯¹å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯çš„å®è´¨æ€§æ”¹è¿›ï¼Œè¯æ˜äº†åŸºäºæŠ å›¾çš„ç»†åŒ–å¯ä»¥æ˜¾ç€æé«˜åˆ†å‰²è´¨é‡ã€‚è¿™é¡¹å·¥ä½œä¸ºæ•´åˆæŠ å›¾å’Œåˆ†å‰²æŠ€æœ¯å¼•å…¥äº†æ–°æ¨¡å¼ï¼Œæé«˜äº†è®¡ç®—æœºè§†è§‰ä¸­ç²¾ç»†å¯¹è±¡è¾¹ç•Œé¢„æµ‹çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06230v2">PDF</a> 6 pages, 2 figures, 3 tables, and 1 algorithms</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†å›¾åƒç£¨å…‰ä¸ç°åº¦åˆ†å‰²æŠ€æœ¯ç»“åˆï¼Œä¸ºäºŒå€¼å›¾åƒåˆ†å‰²ï¼ˆDISï¼‰æå‡ºä¸€ç§æ–°çš„æ¶æ„æ–¹æ³•â€”â€”ä¿¡å¿ƒå¼•å¯¼ç£¨å…‰ï¼ˆCGMï¼‰ã€‚è¯¥æ–‡ç« æå‡ºäº†é¦–ä¸ªåŸºäºä¿¡å¿ƒå¼•å¯¼ç£¨å…‰çš„æ¨¡å‹ï¼Œåä¸ºèƒŒæ™¯æ“¦é™¤ç½‘ç»œï¼ˆBENï¼‰ï¼Œç”±åˆå§‹åˆ†å‰²çš„BENåŸºç¡€éƒ¨åˆ†å’ŒåŸºäºä¿¡å¿ƒè¿›è¡Œç²¾ç»†åŒ–çš„BENç»†åŒ–å™¨æ„æˆã€‚æ­¤æ–¹æ³•åœ¨DIS5KéªŒè¯æ•°æ®é›†ä¸Šç›¸è¾ƒäºç°æœ‰å‰æ²¿æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æå‡ï¼Œè¯æ˜äº†åŸºäºç£¨å…‰çš„ç²¾ç»†åŒ–èƒ½æ˜¾è‘—æé«˜åˆ†å‰²è´¨é‡ï¼Œä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„ç£¨å…‰å’Œåˆ†å‰²æŠ€æœ¯é›†æˆæä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰äºŒå€¼å›¾åƒåˆ†å‰²ï¼ˆDISï¼‰æ–¹æ³•å°†å›¾åƒç£¨å…‰å’Œå¯¹è±¡åˆ†å‰²è§†ä¸ºæ ¹æœ¬ä¸åŒçš„ä»»åŠ¡ã€‚</li>
<li>ç»“åˆå›¾åƒç£¨å…‰å’Œç°åº¦åˆ†å‰²æŠ€æœ¯ä¸ºDISæä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>æå‡ºäº†ä¿¡å¿ƒå¼•å¯¼ç£¨å…‰ï¼ˆCGMï¼‰çš„æ–°æ¶æ„æ–¹æ³•ã€‚</li>
<li>é¦–ä¸ªåŸºäºä¿¡å¿ƒå¼•å¯¼ç£¨å…‰çš„æ¨¡å‹â€”â€”èƒŒæ™¯æ“¦é™¤ç½‘ç»œï¼ˆBENï¼‰ã€‚</li>
<li>BENåŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼šç”¨äºåˆå§‹åˆ†å‰²çš„BENåŸºç¡€å’Œç”¨äºä¿¡å¿ƒåŸºäºçš„ç²¾ç»†åŒ–çš„BENç»†åŒ–å™¨ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨DIS5KéªŒè¯æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ï¼Œè¯æ˜äº†ç£¨å…‰æŠ€æœ¯å¯¹æé«˜åˆ†å‰²è´¨é‡çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-765570297ba7e7a624c307325d46709f" align="middle">
<img src="https://picx.zhimg.com/v2-3222d97d8afdbe48a56e218606c53d49" align="middle">
<img src="https://picx.zhimg.com/v2-0cec0c11d250265bd835fe4c2861aaef" align="middle">
<img src="https://picx.zhimg.com/v2-0b2a0c82f328231eb17c1121b70d6d4a" align="middle">
<img src="https://picx.zhimg.com/v2-bc32a50d9516bdc0fc166fe0368a1022" align="middle">
<img src="https://picx.zhimg.com/v2-36ed3e9170f8d3900fd0ca13e1c9a04d" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-08/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7e7b9e8fad7ed3377854c06ba59c1450" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-08  MrMARTIAN A Multi-resolution Mass Reconstruction Algorithm Combining   Free-form and Analytic Components
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-08/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-63c9c75abb903f8d83eed3b4ed41486d" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-08  Proto-LeakNet Towards Signal-Leak Aware Attribution in Synthetic Human   Face Imagery
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
