<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  UniGen Enhanced Training &amp; Test-Time Strategies for Unified Multimodal   Understanding and Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e0ce1a4a79fec8d00af82d3b31392152.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-22-æ›´æ–°"><a href="#2025-05-22-æ›´æ–°" class="headerlink" title="2025-05-22 æ›´æ–°"></a>2025-05-22 æ›´æ–°</h1><h2 id="UniGen-Enhanced-Training-Test-Time-Strategies-for-Unified-Multimodal-Understanding-and-Generation"><a href="#UniGen-Enhanced-Training-Test-Time-Strategies-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="UniGen: Enhanced Training &amp; Test-Time Strategies for Unified Multimodal   Understanding and Generation"></a>UniGen: Enhanced Training &amp; Test-Time Strategies for Unified Multimodal   Understanding and Generation</h2><p><strong>Authors:Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, Afshin Dehghan</strong></p>
<p>We introduce UniGen, a unified multimodal large language model (MLLM) capable of image understanding and generation. We study the full training pipeline of UniGen from a data-centric perspective, including multi-stage pre-training, supervised fine-tuning, and direct preference optimization. More importantly, we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time scaling, which significantly boosts UniGenâ€™s image generation quality using a simple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act as both image generator and verifier at test time, assessing the semantic alignment between a text prompt and its generated image in a step-by-step CoT manner. Trained entirely on open-source datasets across all stages, UniGen achieves state-of-the-art performance on a range of image understanding and generation benchmarks, with a final score of 0.78 on GenEval and 85.19 on DPG-Bench. Through extensive ablation studies, our work provides actionable insights and addresses key challenges in the full life cycle of building unified MLLMs, contributing meaningful directions to the future research. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†UniGenï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œèƒ½å¤Ÿè¿›è¡Œå›¾åƒç†è§£å’Œç”Ÿæˆã€‚æˆ‘ä»¬ä»æ•°æ®ä¸­å¿ƒçš„è§†è§’ç ”ç©¶äº†UniGençš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬å¤šé˜¶æ®µé¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠç›´æ¥åå¥½ä¼˜åŒ–ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é“¾å¼æ€ç»´éªŒè¯ï¼ˆCoT-Vï¼‰ç­–ç•¥ï¼Œç”¨äºæµ‹è¯•æ—¶çš„æ‰©å±•ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨ç®€å•çš„Best-of-Næµ‹è¯•æ—¶é—´ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†UniGençš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼ŒCoT-Vä½¿UniGenèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶åŒæ—¶å……å½“å›¾åƒç”Ÿæˆå™¨å’ŒéªŒè¯å™¨ï¼Œä»¥é€æ­¥çš„é“¾å¼æ€ç»´æ–¹å¼è¯„ä¼°æ–‡æœ¬æç¤ºä¸å…¶ç”Ÿæˆçš„å›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚å®Œå…¨åœ¨æ‰€æœ‰é˜¶æ®µä½¿ç”¨å¼€æºæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒUniGenåœ¨å›¾åƒç†è§£å’Œç”Ÿæˆçš„ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒGenEvalçš„æœ€ç»ˆå¾—åˆ†ä¸º0.78ï¼ŒDPG-Benchä¸Šçš„å¾—åˆ†ä¸º85.19ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬çš„å·¥ä½œæä¾›äº†å¯æ“ä½œçš„è§è§£ï¼Œè§£å†³äº†æ„å»ºç»Ÿä¸€MLLMæ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰æ„ä¹‰çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14682v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>UniGenæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå…·æœ‰å›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä»æ•°æ®ä¸­å¿ƒè§†è§’ç ”ç©¶äº†UniGençš„å…¨è®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬å¤šé˜¶æ®µé¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠç›´æ¥åå¥½ä¼˜åŒ–ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç ”ç©¶æå‡ºäº†å…¨æ–°çš„æ€ç»´é“¾éªŒè¯ï¼ˆCoT-Vï¼‰ç­–ç•¥ï¼Œç”¨äºæµ‹è¯•æ—¶çš„ç¼©æ”¾ï¼Œé€šè¿‡ç®€å•çš„Best-of-Næµ‹è¯•æ—¶é—´ç­–ç•¥æ˜¾è‘—æå‡UniGençš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚CoT-Vä½¿UniGenèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶åŒæ—¶å……å½“å›¾åƒç”Ÿæˆå™¨å’ŒéªŒè¯å™¨ï¼Œä»¥æ€ç»´é“¾çš„æ–¹å¼é€æ­¥è¯„ä¼°æ–‡æœ¬æç¤ºä¸ç”Ÿæˆå›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚å®Œå…¨åœ¨å¼€æºæ•°æ®é›†ä¸Šè®­ç»ƒçš„UniGenï¼Œåœ¨å›¾åƒç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒGenEvalå¾—åˆ†ä¸º0.78ï¼ŒDPG-Benchå¾—åˆ†ä¸º85.19ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œè¯¥ç ”ç©¶ä¸ºæ„å»ºç»Ÿä¸€MLLMçš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸæä¾›äº†å¯è¡Œçš„è§è§£å’Œè§£å†³äº†å…³é”®æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æœ‰æ„ä¹‰çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniGenæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡å›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶äº†UniGençš„å…¨è®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒä»¥åŠåå¥½ä¼˜åŒ–ã€‚</li>
<li>æå‡ºäº†æ€ç»´é“¾éªŒè¯ï¼ˆCoT-Vï¼‰ç­–ç•¥ï¼Œæé«˜å›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>UniGenå…·å¤‡åœ¨æµ‹è¯•æ—¶åŒæ—¶ä½œä¸ºå›¾åƒç”Ÿæˆå™¨å’ŒéªŒè¯å™¨çš„åŠŸèƒ½ã€‚</li>
<li>UniGenåœ¨å›¾åƒç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜ç§€ï¼ŒGenEvalå¾—åˆ†ä¸º0.78ï¼ŒDPG-Benchå¾—åˆ†ä¸º85.19ã€‚</li>
<li>æ¶ˆèç ”ç©¶ä¸ºæ„å»ºMLLMæä¾›äº†å…³é”®æŒ‘æˆ˜å’Œå¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e0552fadb298e283deb8b7c53746951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69b8f7425aa72507b37aa89e30c31aa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecd5ccd266ed1ae72381d5ddc44f58dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd13a565215a454ed30381d36413caec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d69dacfe1d52bd63a3e8f9d3c1f58203.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UltraEdit-Training-Subject-and-Memory-Free-Lifelong-Editing-in-Large-Language-Models"><a href="#UltraEdit-Training-Subject-and-Memory-Free-Lifelong-Editing-in-Large-Language-Models" class="headerlink" title="UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in   Large Language Models"></a>UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in   Large Language Models</h2><p><strong>Authors:Xiaojie Gu, Guangxu Chen, Jungang Li, Jia-Chen Gu, Xuming Hu, Kai Zhang</strong></p>
<p>Lifelong learning enables large language models (LLMs) to adapt to evolving information by continually updating their internal knowledge. An ideal system should support efficient, wide-ranging updates while preserving existing capabilities and ensuring reliable deployment. Model editing stands out as a promising solution for this goal, offering a focused and efficient way to revise a modelâ€™s internal knowledge. Although recent paradigms have made notable progress, they often struggle to meet the demands of practical lifelong adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally new editing solution that is training-, subject- and memory-free, making it particularly well-suited for ultra-scalable, real-world lifelong model editing. ULTRAEDIT performs editing through a self-contained process that relies solely on lightweight linear algebra operations to compute parameter shifts, enabling fast and consistent parameter modifications with minimal overhead. To improve scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization strategy that continuously updates feature statistics across turns, allowing it to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT achieves editing speeds over 7x faster than the previous state-of-the-art method-which was also the fastest known approach-while consuming less than 1&#x2F;3 the VRAM, making it the only method currently capable of editing a 7B LLM on a 24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest dataset in the field to date, with over 2M editing pairs-and demonstrate that our method supports up to 1M edits while maintaining high accuracy. Comprehensive experiments on four datasets and six models show that ULTRAEDIT consistently achieves superior performance across diverse model editing scenarios. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/XiaojieGu/UltraEdit">https://github.com/XiaojieGu/UltraEdit</a>. </p>
<blockquote>
<p>ç»ˆèº«å­¦ä¹ ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡æŒç»­æ›´æ–°å…¶å†…éƒ¨çŸ¥è¯†æ¥é€‚åº”ä¸æ–­å˜åŒ–çš„ä¿¡æ¯ã€‚ä¸€ä¸ªç†æƒ³çš„ç³»ç»Ÿåº”è¯¥æ”¯æŒé«˜æ•ˆã€å¹¿æ³›çš„æ›´æ–°ï¼ŒåŒæ—¶ä¿ç•™ç°æœ‰åŠŸèƒ½å¹¶ç¡®ä¿å¯é éƒ¨ç½²ã€‚æ¨¡å‹ç¼–è¾‘ä½œä¸ºå®ç°æ­¤ç›®æ ‡çš„é¢‡å…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆè„±é¢–è€Œå‡ºï¼Œå®ƒæä¾›äº†ä¸€ç§ä¸“æ³¨ä¸”é«˜æ•ˆçš„æ–¹æ³•æ¥ä¿®è®¢æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ã€‚å°½ç®¡æœ€è¿‘çš„èŒƒå¼å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬é€šå¸¸éš¾ä»¥æ»¡è¶³å¤§è§„æ¨¡å®è·µä¸­çš„ç»ˆèº«å­¦ä¹ éœ€æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ULTRAEDITâ€”â€”ä¸€ç§å…¨æ–°çš„ç¼–è¾‘è§£å†³æ–¹æ¡ˆï¼Œå®ƒæ— éœ€è®­ç»ƒã€ä¸»é¢˜å’Œå†…å­˜ï¼Œç‰¹åˆ«é€‚ç”¨äºè¶…å¤§è§„æ¨¡ã€ç°å®ä¸–ç•Œçš„ç»ˆèº«æ¨¡å‹ç¼–è¾‘ã€‚ULTRAEDITé€šè¿‡è‡ªæˆ‘åŒ…å«çš„è¿‡ç¨‹è¿›è¡Œç¼–è¾‘ï¼Œè¯¥è¿‡ç¨‹ä»…ä¾èµ–äºè½»é‡çº§çš„çº¿æ€§ä»£æ•°è¿ç®—æ¥è®¡ç®—å‚æ•°å˜åŒ–ï¼Œä»è€Œå®ç°å¿«é€Ÿä¸”ä¸€è‡´çš„å‚æ•°ä¿®æ”¹ï¼Œå¹¶ä¸”å¼€é”€æå°ã€‚ä¸ºäº†æé«˜ç»ˆèº«è®¾ç½®çš„å¯æ‰©å±•æ€§ï¼ŒULTRAEDITé‡‡ç”¨ç»ˆèº«å½’ä¸€åŒ–ç­–ç•¥ï¼Œä¸æ–­æ›´æ–°è½®æ¬¡é—´çš„ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”åˆ†å¸ƒå˜åŒ–å¹¶éšæ—¶é—´ä¿æŒä¸€è‡´æ€§ã€‚ULTRAEDITçš„ç¼–è¾‘é€Ÿåº¦æ¯”æœ€æ–°çš„å‰æ²¿æ–¹æ³•å¿«7å€ä»¥ä¸Šâ€”â€”è¿™ä¹Ÿæ˜¯å½“æ—¶å·²çŸ¥çš„æœ€å¿«æ–¹æ³•â€”â€”åŒæ—¶VRAMä½¿ç”¨é‡ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€ï¼Œä½¿å…¶æˆä¸ºç›®å‰å”¯ä¸€èƒ½å¤Ÿåœ¨24GBæ¶ˆè´¹çº§GPUä¸Šç¼–è¾‘7B LLMçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†è¿„ä»Šä¸ºæ­¢è¯¥é¢†åŸŸæœ€å¤§çš„æ•°æ®é›†ULTRAEDITBENCHï¼ŒåŒ…å«è¶…è¿‡2Mä¸ªç¼–è¾‘å¯¹ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒé«˜è¾¾1Mæ¬¡çš„ç¼–è¾‘åŒæ—¶ä¿æŒé«˜å‡†ç¡®æ€§ã€‚åœ¨å››ä¸ªæ•°æ®é›†å’Œå…­ä¸ªæ¨¡å‹ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒULTRAEDITåœ¨ä¸åŒæ¨¡å‹ç¼–è¾‘åœºæ™¯ä¸­å§‹ç»ˆå®ç°å“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://github.com/XiaojieGu/UltraEdit%E8%AE%BF%E9%97%AE%E3%80%82]">https://github.com/XiaojieGu/UltraEditè®¿é—®ã€‚]</a>(<a target="_blank" rel="noopener" href="https://github.com/XiaojieGu/UltraEdit%E8%AE%BF%E9">https://github.com/XiaojieGu/UltraEdit%E8%AE%BF%E9</a> to )</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14679v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æŒç»­æ›´æ–°å†…éƒ¨çŸ¥è¯†ï¼Œå®ç°é€‚åº”ä¸æ–­å˜åŒ–çš„ä¿¡æ¯ã€‚æ¨¡å‹ç¼–è¾‘ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨ä¿®è®¢æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ï¼Œå¹¶ä½¿å…¶æˆä¸ºç»ˆèº«å­¦ä¹ çš„å…³é”®å·¥å…·ã€‚ULTRAEDITä½œä¸ºä¸€ç§å…¨æ–°ç¼–è¾‘è§£å†³æ–¹æ¡ˆï¼Œæ— éœ€è®­ç»ƒã€ä¸»é¢˜å’Œå†…å­˜ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§è§„æ¨¡å®æ—¶æ¨¡å‹ç¼–è¾‘ã€‚å®ƒé€šè¿‡è½»é‡çº§çº¿æ€§ä»£æ•°è¿ç®—è¿›è¡Œå‚æ•°ä¿®æ”¹ï¼Œå®ç°å¿«é€Ÿä¸”ä¸€è‡´çš„ç¼–è¾‘ã€‚ULTRAEDITé‡‡ç”¨ç»ˆèº«æ ‡å‡†åŒ–ç­–ç•¥ï¼Œé€‚åº”åˆ†å¸ƒå˜åŒ–å¹¶ç»´æŒæ—¶é—´ä¸€è‡´æ€§ã€‚å®ƒçš„ç¼–è¾‘é€Ÿåº¦è¶…è¿‡ç°æœ‰æ–¹æ³•7å€ï¼ŒåŒæ—¶å ç”¨å†…å­˜å°‘ï¼Œæ˜¯å”¯ä¸€èƒ½åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿›è¡Œ7Bå¤§å‹è¯­è¨€æ¨¡å‹ç¼–è¾‘çš„æ–¹æ³•ã€‚ULTRAEDITBENCHæ•°æ®é›†åŒ…å«è¶…è¿‡ç™¾ä¸‡å¯¹ç¼–è¾‘å®ä¾‹ï¼Œè¯æ˜äº†å…¶åœ¨å¤§é‡ç¼–è¾‘ä¸‹çš„é«˜å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒULTRAEDITåœ¨ä¸åŒæ¨¡å‹ç¼–è¾‘åœºæ™¯ä¸­è¡¨ç°å“è¶Šã€‚è¯¦æƒ…è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/XiaojieGu/UltraEdit%E3%80%82">https://github.com/XiaojieGu/UltraEditã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡ç»ˆèº«å­¦ä¹ é€‚åº”å˜åŒ–ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹ç¼–è¾‘æ˜¯ä¿®è®¢å¤§å‹è¯­è¨€æ¨¡å‹å†…éƒ¨çŸ¥è¯†çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ULTRAEDITæ˜¯ä¸€ç§å…¨æ–°ã€é«˜æ•ˆã€è®­ç»ƒã€ä¸»é¢˜å’Œå†…å­˜ç‹¬ç«‹çš„ç¼–è¾‘è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ULTRAEDITåˆ©ç”¨è½»é‡çº§çº¿æ€§ä»£æ•°æ“ä½œè¿›è¡Œå¿«é€Ÿå‚æ•°ä¿®æ”¹ã€‚</li>
<li>é‡‡ç”¨ç»ˆèº«æ ‡å‡†åŒ–ç­–ç•¥æé«˜ULTRAEDITåœ¨å¤§è§„æ¨¡ç¯å¢ƒä¸‹çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>ULTRAEDITç¼–è¾‘é€Ÿåº¦è¿œè¶…ç°æœ‰æ–¹æ³•ï¼Œå†…å­˜å ç”¨ä½ã€‚</li>
<li>ULTRAEDITBENCHæ•°æ®é›†ç”¨äºéªŒè¯å¤§é‡ç¼–è¾‘ä¸‹çš„å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1ef9fb86e3d78410adbc9c11bf87482.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35a483c2cb1f7bdff827259266fb2b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79087818c7d429442ef0b64ae8a1dd0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4db244d612aa7f520fa3c46f74534307.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d8d4302ed413bf751fc019ec865dd31.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Visionary-R1-Mitigating-Shortcuts-in-Visual-Reasoning-with-Reinforcement-Learning"><a href="#Visionary-R1-Mitigating-Shortcuts-in-Visual-Reasoning-with-Reinforcement-Learning" class="headerlink" title="Visionary-R1: Mitigating Shortcuts in Visual Reasoning with   Reinforcement Learning"></a>Visionary-R1: Mitigating Shortcuts in Visual Reasoning with   Reinforcement Learning</h2><p><strong>Authors:Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, Kaiyang Zhou</strong></p>
<p>Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM â€“ by prompting the model to produce a reasoning chain before providing an answer â€“ can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks. </p>
<blockquote>
<p>é•¿æœŸä»¥æ¥ï¼Œå­¦ä¹ é€šç”¨æ¨ç†èƒ½åŠ›ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªéš¾é¢˜ã€‚è¿‘æœŸåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚GRPOï¼‰å¯ä»¥ä½¿é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹é€šè¿‡ç®€å•çš„é—®é¢˜ç­”æ¡ˆå¯¹æ¥å‘å±•æ¨ç†èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œè§†è§‰é—®ç­”å¯¹æ¥è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿå¯¹å›¾åƒæ•°æ®è¿›è¡Œæ¨ç†ï¼Œè€Œæ— éœ€ä»»ä½•æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…ä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹VLMåº”ç”¨æç¤ºæ¨¡å‹äº§ç”Ÿæ¨ç†é“¾å¹¶ç»™å‡ºç­”æ¡ˆçš„æ–¹æ³•ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹ä»ç®€å•é—®é¢˜ä¸­äº§ç”Ÿæ·å¾„ï¼Œä»è€Œé™ä½å…¶åœ¨æœªè§æ•°æ®åˆ†å¸ƒä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºé¿å…æ·å¾„å­¦ä¹ çš„å…³é”®æ˜¯é¼“åŠ±æ¨¡å‹åœ¨æ¨ç†ä¹‹å‰è§£é‡Šå›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹éµå¾ªæ ‡é¢˜-æ¨ç†-ç­”æ¡ˆçš„è¾“å‡ºæ ¼å¼ï¼šé¦–å…ˆä¸ºå›¾åƒç”Ÿæˆè¯¦ç»†çš„æ ‡é¢˜ï¼Œç„¶åæ„å»ºå¹¿æ³›çš„æ¨ç†é“¾ã€‚åœ¨æ— éœ€æ€ç»´é“¾çš„273Kè§†è§‰é—®ç­”å¯¹ä¸Šè®­ç»ƒï¼Œä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹â€”â€”Visionary-R1åœ¨å¤šè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¦‚GPT-4oã€Claude 3.5-Sonnetå’ŒGemini-1.5-Proã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14677v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å’Œè§†è§‰é—®ç­”å¯¹çš„è¾…åŠ©ä¸‹å¯ä»¥å‘å±•å‡ºæ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå›¾åƒæ•°æ®æ¨ç†ï¼Œå¹¶å‘ç°å¼•å¯¼æ¨¡å‹äº§ç”Ÿæ¨ç†é“¾åå†æä¾›ç­”æ¡ˆä¼šå¯¼è‡´æ¨¡å‹ä¾èµ–æ·å¾„ï¼Œé™ä½å…¶åœ¨æœªè§æ•°æ®åˆ†å¸ƒä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚é¼“åŠ±æ¨¡å‹åœ¨æ¨ç†å‰è§£è¯»å›¾åƒæ˜¯ç¼“è§£æ·å¾„å­¦ä¹ çš„å…³é”®ã€‚è®­ç»ƒçš„æ¨¡å‹ï¼ˆVisionary-R1ï¼‰åœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å¯ä»¥å¸®åŠ©é¢„è®­ç»ƒçš„LLMå‘å±•å‡ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œè§†è§‰é—®ç­”å¯¹è®­ç»ƒVLMè¿›è¡Œå›¾åƒæ•°æ®æ¨ç†ã€‚</li>
<li>å¼•å¯¼æ¨¡å‹äº§ç”Ÿæ¨ç†é“¾å†æä¾›ç­”æ¡ˆå¯èƒ½å¯¼è‡´æ¨¡å‹ä¾èµ–æ·å¾„ï¼Œå½±å“æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é¼“åŠ±æ¨¡å‹åœ¨æ¨ç†å‰è§£è¯»å›¾åƒæ˜¯ç¼“è§£æ·å¾„å­¦ä¹ çš„å…³é”®ã€‚</li>
<li>è®­ç»ƒçš„æ¨¡å‹Visionary-R1é‡‡ç”¨å…ˆç”Ÿæˆå›¾åƒè¯¦ç»†æè¿°ï¼Œå†æ„å»ºæ¨ç†é“¾çš„è¾“å‡ºæ ¼å¼ã€‚</li>
<li>Visionary-R1åœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–å¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eedecaec5c153adc57b67a41d8d0a693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ab580e632ed95049130be3c79d8fb5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36d55882085b3f5ebef82cd73865590a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc8674003374b06961547ba85760386b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reward-Reasoning-Model"><a href="#Reward-Reasoning-Model" class="headerlink" title="Reward Reasoning Model"></a>Reward Reasoning Model</h2><p><strong>Authors:Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, Furu Wei</strong></p>
<p>Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at <a target="_blank" rel="noopener" href="https://huggingface.co/Reward-Reasoning">https://huggingface.co/Reward-Reasoning</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹åœ¨å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹äº§ç”Ÿç¬¦åˆäººç±»æœŸæœ›çš„è¾“å‡ºæ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºæ¥æå‡å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¥–åŠ±æ¨ç†æ¨¡å‹ï¼ˆRRMsï¼‰ï¼Œå®ƒä»¬è¢«ä¸“é—¨è®¾è®¡ç”¨äºåœ¨ç”Ÿæˆæœ€ç»ˆå¥–åŠ±ä¹‹å‰æ‰§è¡Œæœ‰æ„è¯†çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡é“¾å¼æ€ç»´æ¨ç†ï¼ŒRRMsåˆ©ç”¨é¢å¤–çš„æµ‹è¯•æ—¶é—´è®¡ç®—èµ„æºæ¥å¤„ç†å¤æ‚æŸ¥è¯¢ï¼Œåœ¨è¿™äº›æŸ¥è¯¢ä¸­ï¼Œåˆé€‚çš„å¥–åŠ±å¹¶ä¸ç«‹å³æ˜¾ç°ã€‚ä¸ºäº†å¼€å‘RRMsï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¸éœ€è¦æ˜ç¡®çš„æ¨ç†è½¨è¿¹ä½œä¸ºè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒåŸ¹å…»äº†è‡ªæˆ‘è¿›åŒ–çš„å¥–åŠ±æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRRMsåœ¨ä¸åŒé¢†åŸŸçš„å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†RRMså¯ä»¥è‡ªé€‚åº”åœ°åˆ©ç”¨æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºæ¥è¿›ä¸€æ­¥æé«˜å¥–åŠ±çš„å‡†ç¡®æ€§ã€‚é¢„è®­ç»ƒçš„å¥–åŠ±æ¨ç†æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/Reward-Reasoning%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/Reward-Reasoningæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14674v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¥–åŠ±æ¨¡å‹åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œèƒ½å¤Ÿå¼•å¯¼å…¶è¾“å‡ºä¸äººç±»é¢„æœŸç›¸ç¬¦çš„ç»“æœã€‚æœ¬ç ”ç©¶æå‡ºäº†å¥–åŠ±æ¨ç†æ¨¡å‹ï¼ˆRRMsï¼‰ï¼Œèƒ½å¤Ÿåœ¨æœ€ç»ˆç”Ÿæˆå¥–åŠ±å‰è¿›è¡Œæ¨ç†è¿‡ç¨‹ï¼Œå¯¹å¤æ‚çš„æŸ¥è¯¢è¿›è¡Œé¢å¤–æµ‹è¯•æ—¶é—´è®¡ç®—ä»¥å¾—åˆ°æ°å½“çš„å¥–åŠ±ã€‚è¯¥ç ”ç©¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨æ— éœ€æ˜ç¡®æ¨ç†è½¨è¿¹ä½œä¸ºè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è‡ªæˆ‘è¿›åŒ–å¥–åŠ±æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRRMsåœ¨å¤šä¸ªé¢†åŸŸçš„å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶èƒ½è‡ªé€‚åº”åˆ©ç”¨æµ‹è¯•æ—¶é—´è®¡ç®—æé«˜å¥–åŠ±å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¥–åŠ±æ¨¡å‹åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­èµ·å…³é”®ä½œç”¨ï¼Œå¼•å¯¼è¾“å‡ºä¸äººç±»é¢„æœŸç›¸ç¬¦çš„ç»“æœã€‚</li>
<li>å¥–åŠ±æ¨ç†æ¨¡å‹ï¼ˆRRMsï¼‰èƒ½å¤Ÿåœ¨æµ‹è¯•é˜¶æ®µè¿›è¡Œæ¨ç†è¿‡ç¨‹ï¼Œé€‚ç”¨äºå¤æ‚æŸ¥è¯¢çš„å¥–åŠ±è®¡ç®—ã€‚</li>
<li>RRMsåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ˜ç¡®çš„æ¨ç†è½¨è¿¹ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚</li>
<li>RRMsåœ¨å¤šä¸ªé¢†åŸŸçš„å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>RRMsèƒ½å¤Ÿè‡ªé€‚åº”åˆ©ç”¨æµ‹è¯•æ—¶é—´è®¡ç®—æ¥æé«˜å¥–åŠ±çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¥–åŠ±æ¨ç†æ¨¡å‹çš„é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥åœ¨Hugging Faceä¸Šè·å–ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæé«˜å¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87e008355e834da2c01193f0474a59a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-488e48f0aafd6da22e75849470b37bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0334a638c74cecc248e99ebb6fe445e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Quartet-Native-FP4-Training-Can-Be-Optimal-for-Large-Language-Models"><a href="#Quartet-Native-FP4-Training-Can-Be-Optimal-for-Large-Language-Models" class="headerlink" title="Quartet: Native FP4 Training Can Be Optimal for Large Language Models"></a>Quartet: Native FP4 Training Can Be Optimal for Large Language Models</h2><p><strong>Authors:Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh</strong></p>
<p>The rapid advancement of large language models (LLMs) has been paralleled by unprecedented increases in computational demands, with training costs for state-of-the-art models doubling every few months. Training models directly in low-precision arithmetic offers a solution, by improving both computational throughput and energy efficiency. Specifically, NVIDIAâ€™s recent Blackwell architecture facilitates extremely low-precision operations, specifically FP4 variants, promising substantial efficiency gains. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we systematically investigate hardware-supported FP4 training and introduce Quartet, a new approach enabling accurate, end-to-end FP4 training with all the major computations (in e.g. linear layers) being performed in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across varying bit-widths and allows us to identify a â€œnear-optimalâ€ low-precision training technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve state-of-the-art accuracy for FP4 precision, successfully training billion-scale models. Our method demonstrates that fully FP4-based training is a competitive alternative to standard-precision and FP8 training. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/Quartet">https://github.com/IST-DASLab/Quartet</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¼´éšç€è®¡ç®—éœ€æ±‚çš„ç©ºå‰å¢é•¿ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹çš„è®­ç»ƒæˆæœ¬æ¯éš”å‡ ä¸ªæœˆå°±ç¿»ä¸€å€ã€‚ç›´æ¥åœ¨ä½ç²¾åº¦ç®—æœ¯ä¸­è¿›è¡Œæ¨¡å‹è®­ç»ƒå¯ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œèƒ½æºæ•ˆç‡ï¼Œä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼ŒNVIDIAæœ€è¿‘çš„Blackwellæ¶æ„æ”¯æŒæä½ç²¾åº¦çš„è¿ç®—ï¼Œå°¤å…¶æ˜¯FP4å˜ä½“ï¼Œæœ‰æœ›è·å¾—å·¨å¤§çš„æ•ˆç‡æå‡ã€‚ç„¶è€Œï¼Œå½“å‰åœ¨FP4ç²¾åº¦ä¸‹è®­ç»ƒLLMçš„ç®—æ³•é¢ä¸´ç€ç²¾åº¦å¤§å¹…ä¸‹é™çš„é—®é¢˜ï¼Œå¹¶ä¸”ç»å¸¸ä¾èµ–äºæ··åˆç²¾åº¦å›é€€æ–¹æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†ç¡¬ä»¶æ”¯æŒçš„FP4è®­ç»ƒï¼Œå¹¶å¼•å…¥äº†Quartetè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä½ç²¾åº¦ä¸‹å®ç°ç«¯åˆ°ç«¯çš„å‡†ç¡®FP4è®­ç»ƒï¼Œæ‰€æœ‰ä¸»è¦çš„è®¡ç®—ï¼ˆå¦‚çº¿æ€§å±‚ï¼‰éƒ½åœ¨ä½ç²¾åº¦ä¸‹å®Œæˆã€‚é€šè¿‡å¯¹Llamaç±»å‹æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸€ç§æ–°çš„ä½ç²¾åº¦ç¼©æ”¾å®šå¾‹ï¼Œè¯¥å®šå¾‹é‡åŒ–äº†ä¸åŒä½å®½ä¹‹é—´çš„æ€§èƒ½æƒè¡¡ï¼Œå¹¶å¸®åŠ©æˆ‘ä»¬è¯†åˆ«å‡ºåœ¨å‡†ç¡®æ€§ä¸è®¡ç®—ä¹‹é—´çš„â€œè¿‘æœ€ä¼˜â€ä½ç²¾åº¦è®­ç»ƒæŠ€æœ¯ï¼Œç§°ä¸ºQuartetã€‚æˆ‘ä»¬ä½¿ç”¨é’ˆå¯¹NVIDIA Blackwell GPUä¼˜åŒ–çš„CUDAå†…æ ¸å®ç°äº†Quartetï¼Œå¹¶è¯æ˜å®ƒå¯ä»¥å®ç°FP4ç²¾åº¦çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒæˆåŠŸè®­ç»ƒäº†æ•°åäº¿è§„æ¨¡çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œå®Œå…¨åŸºäºFP4çš„è®­ç»ƒæ˜¯æ ‡å‡†ç²¾åº¦å’ŒFP8è®­ç»ƒçš„ç«äº‰æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/Quartet%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IST-DASLab/Quartetä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14669v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¼´éšç€è®¡ç®—éœ€æ±‚çš„ç©ºå‰å¢é•¿ï¼Œè®­ç»ƒæˆæœ¬æ¯å‡ ä¸ªæœˆç¿»ä¸€å€ã€‚ç›´æ¥åœ¨ä½ç²¾åº¦ç®—æœ¯ä¸­è¿›è¡Œæ¨¡å‹è®­ç»ƒå¯ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œèƒ½æºæ•ˆç‡ã€‚NVIDIAçš„Blackwellæ¶æ„æ”¯æŒFP4ç²¾åº¦è¿ç®—ï¼Œä½†å½“å‰ç®—æ³•åœ¨FP4ç²¾åº¦ä¸‹è®­ç»ƒLLMé¢ä¸´ç²¾åº¦æŸå¤±çš„é—®é¢˜ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†ç¡¬ä»¶æ”¯æŒçš„FP4è®­ç»ƒï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•Quartetï¼Œèƒ½å¤Ÿåœ¨ä½ç²¾åº¦ä¸‹å®ç°ç«¯åˆ°ç«¯çš„FP4è®­ç»ƒã€‚é€šè¿‡å¯¹Llamaç±»å‹æ¨¡å‹çš„è¯„ä¼°ï¼Œæœ¬æ–‡æ­ç¤ºäº†ä½ç²¾åº¦ç¼©æ”¾å®šå¾‹ï¼Œå¹¶ç¡®å®šäº†åœ¨å‡†ç¡®æ€§ä¸è®¡ç®—ä¹‹é—´å…·æœ‰â€œè¿‘æœ€ä¼˜â€çš„ä½ç²¾åº¦è®­ç»ƒæŠ€æœ¯ã€‚Quartetæ–¹æ³•åœ¨NVIDIA Blackwell GPUä¸Šå®ç°ï¼Œå¯åœ¨FP4ç²¾åº¦ä¸‹è¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒæˆåŠŸè®­ç»ƒç™¾äº¿è§„æ¨¡æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—éœ€æ±‚è¿…é€Ÿå¢é•¿ï¼Œè®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ä½ç²¾åº¦ç®—æœ¯è®­ç»ƒå¯ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œèƒ½æºæ•ˆç‡ã€‚</li>
<li>NVIDIAçš„Blackwellæ¶æ„æ”¯æŒFP4ç²¾åº¦è¿ç®—ï¼Œä½†FP4ç²¾åº¦è®­ç»ƒé¢ä¸´ç²¾åº¦æŸå¤±é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Quartetæ–¹æ³•ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„FP4ç²¾åº¦è®­ç»ƒï¼Œè§£å†³äº†ç²¾åº¦æŸå¤±é—®é¢˜ã€‚</li>
<li>é€šè¿‡è¯„ä¼°å‘ç°ä½ç²¾åº¦ç¼©æ”¾å®šå¾‹ï¼Œç¡®å®šäº†è¿‘æœ€ä¼˜çš„ä½ç²¾åº¦è®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>Quartetæ–¹æ³•åœ¨NVIDIA Blackwell GPUä¸Šå®ç°ï¼Œå¯æˆåŠŸè®­ç»ƒç™¾äº¿è§„æ¨¡æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93fa795e203bf32a606ccca1b9674f77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4831573d5f712fe2aa29399e83bf4acf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8aa66af809760e9f34b6c9ed8ed56900.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f3c70820bc5e258ace1bb685d17a77c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ContextAgent-Context-Aware-Proactive-LLM-Agents-with-Open-World-Sensory-Perceptions"><a href="#ContextAgent-Context-Aware-Proactive-LLM-Agents-with-Open-World-Sensory-Perceptions" class="headerlink" title="ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions"></a>ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions</h2><p><strong>Authors:Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan</strong></p>
<p>Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts to enhance the proactive capabilities of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and the persona contexts from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†æ™ºèƒ½ä»£ç†ä»è¢«åŠ¨å“åº”å‘ä¸»åŠ¨æ”¯æŒçš„è½¬å˜ã€‚å°½ç®¡å‰æ™¯å…‰æ˜ï¼Œä½†ç°æœ‰çš„ä¸»åŠ¨ä»£ç†è¦ä¹ˆä»…ä¾èµ–äºå°é—­ç¯å¢ƒçš„è§‚å¯Ÿï¼ˆä¾‹å¦‚æ¡Œé¢ç”¨æˆ·ç•Œé¢ï¼‰è¿›è¡Œç›´æ¥LLMæ¨ç†ï¼Œè¦ä¹ˆé‡‡ç”¨åŸºäºè§„åˆ™çš„ä¸»åŠ¨é€šçŸ¥ï¼Œå¯¼è‡´å¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£ä¸å¤Ÿç†æƒ³ï¼Œä¸»åŠ¨æœåŠ¡çš„åŠŸèƒ½æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ContextAgentï¼Œè¿™æ˜¯ä¸€æ¬¾é¦–æ¬¾ç»“åˆå¹¿æ³›ç¯å¢ƒæ„ŸçŸ¥å¢å¼ºLLMä»£ç†ä¸»åŠ¨èƒ½åŠ›çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸»åŠ¨ä»£ç†ã€‚ContextAgenté¦–å…ˆä»å¯ç©¿æˆ´è®¾å¤‡çš„å¤§é‡æ„Ÿå®˜æ„ŸçŸ¥ï¼ˆä¾‹å¦‚è§†é¢‘å’ŒéŸ³é¢‘ï¼‰ä¸­æå–å¤šç»´ä¸Šä¸‹æ–‡ï¼Œä»¥äº†è§£ç”¨æˆ·æ„å›¾ã€‚ç„¶åï¼ŒContextAgentåˆ©ç”¨æ„Ÿå®˜ä¸Šä¸‹æ–‡å’Œå†å²æ•°æ®ä¸­çš„ä¸ªäººä¸Šä¸‹æ–‡æ¥é¢„æµ‹æ˜¯å¦éœ€è¦ä¸»åŠ¨æœåŠ¡ã€‚åœ¨éœ€è¦ä¸»åŠ¨ååŠ©æ—¶ï¼ŒContextAgentè¿˜ä¼šè‡ªåŠ¨è°ƒç”¨å¿…è¦çš„å·¥å…·ä»¥ååŠ©ç”¨æˆ·è€Œä¸å¹²æ‰°ç”¨æˆ·ã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ContextAgentBenchï¼Œè¿™æ˜¯è¯„ä¼°ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸»åŠ¨LLMä»£ç†çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–9ä¸ªæ—¥å¸¸åœºæ™¯çš„1000ä¸ªæ ·æœ¬å’Œ20ä¸ªå·¥å…·ã€‚åœ¨ContextAgentBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒContextAgentçš„ä¸»åŠ¨é¢„æµ‹å’Œå·¥å…·è°ƒç”¨å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†é«˜è¾¾8.5%å’Œ6.0%ï¼Œè¶…è¿‡äº†åŸºçº¿ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½æ¿€å‘æ›´å…ˆè¿›ã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„ä¸»åŠ¨äººå·¥æ™ºèƒ½åŠ©ç†çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14668v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†æ™ºèƒ½ä»£ç†ä»è¢«åŠ¨å“åº”å‘ä¸»åŠ¨æ”¯æŒçš„è½¬å˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸»åŠ¨ä»£ç†å­˜åœ¨å±€é™æ€§ï¼Œè¦ä¹ˆä»…ä¾èµ–å°é—­ç¯å¢ƒçš„è§‚å¯Ÿè¿›è¡Œç›´æ¥LLMæ¨ç†ï¼Œè¦ä¹ˆé‡‡ç”¨åŸºäºè§„åˆ™çš„ä¸»åŠ¨é€šçŸ¥ï¼Œå¯¼è‡´å¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£ä¸è¶³å’Œä¸»åŠ¨æœåŠ¡çš„åŠŸèƒ½æœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ContextAgentï¼Œè¿™æ˜¯ä¸€ç§é¦–å±ˆä¸€æŒ‡çš„æ„è¯†æ„ŸçŸ¥ä¸»åŠ¨ä»£ç†ï¼Œå®ƒèå…¥äº†å¹¿æ³›çš„ç¯å¢ƒæ„ŸçŸ¥æ¥æé«˜LLMä»£ç†çš„ä¸»åŠ¨èƒ½åŠ›ã€‚ContextAgenté¦–å…ˆä»å¯ç©¿æˆ´è®¾å¤‡çš„å¤§é‡æ„Ÿå®˜æ„ŸçŸ¥ä¸­æå–å¤šç»´ä¸Šä¸‹æ–‡ï¼ˆå¦‚è§†é¢‘å’ŒéŸ³é¢‘ï¼‰æ¥ç†è§£ç”¨æˆ·æ„å›¾ã€‚ç„¶åï¼Œå®ƒåˆ©ç”¨æ„Ÿå®˜ä¸Šä¸‹æ–‡å’Œå†å²æ•°æ®ä¸­çš„ä¸ªäººä¸Šä¸‹æ–‡æ¥é¢„æµ‹æ˜¯å¦éœ€è¦ä¸»åŠ¨æœåŠ¡ã€‚å½“éœ€è¦ä¸»åŠ¨ååŠ©æ—¶ï¼ŒContextAgentä¼šè¿›ä¸€æ­¥è‡ªåŠ¨è°ƒç”¨å¿…è¦çš„å·¥å…·æ¥ååŠ©ç”¨æˆ·ã€‚ä¸ºè¯„ä¼°æ­¤æ–°ä»»ä»£ç†çš„è¡¨ç°ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ContextAgentBenchè¯„ä¼°æ ‡å‡†ï¼Œæ¶µç›–æ—¥å¸¸ä¹ä¸ªåœºæ™¯ä¸­çš„ä¸€åƒä¸ªæ ·æœ¬å’ŒäºŒåä¸ªå·¥å…·ã€‚å®éªŒè¡¨æ˜ï¼ŒContextAgentåœ¨ä¸»åŠ¨é¢„æµ‹å’Œå·¥å…·è°ƒç”¨æ–¹é¢çš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†é«˜è¾¾8.5%å’Œ6.0%ã€‚æœ¬æ–‡çš„ç ”ç©¶æœ‰æœ›æ¿€å‘æ›´å…ˆè¿›ã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„ä¸»åŠ¨AIåŠ©ç†çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—æ™ºèƒ½ä»£ç†èƒ½å¤Ÿæ›´ä¸»åŠ¨åœ°æ”¯æŒç”¨æˆ·ã€‚</li>
<li>ç°æœ‰ä¸»åŠ¨ä»£ç†å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ä¾èµ–å°é—­ç¯å¢ƒè§‚å¯Ÿå’ŒåŸºäºè§„åˆ™çš„ä¸»åŠ¨é€šçŸ¥ã€‚</li>
<li>ContextAgentæ˜¯ä¸€ç§æ„è¯†æ„ŸçŸ¥ä¸»åŠ¨ä»£ç†ï¼Œç»“åˆå¹¿æ³›çš„ç¯å¢ƒæ„ŸçŸ¥æé«˜LLMä»£ç†çš„ä¸»åŠ¨èƒ½åŠ›ã€‚</li>
<li>ContextAgenté€šè¿‡æå–å¤šç»´ä¸Šä¸‹æ–‡ï¼ˆå¦‚è§†é¢‘å’ŒéŸ³é¢‘ï¼‰ä»å¯ç©¿æˆ´è®¾å¤‡ç†è§£ç”¨æˆ·æ„å›¾ã€‚</li>
<li>ContextAgentåˆ©ç”¨æ„Ÿå®˜ä¸Šä¸‹æ–‡å’Œä¸ªäººä¸Šä¸‹æ–‡é¢„æµ‹æ˜¯å¦éœ€è¦ä¸»åŠ¨æœåŠ¡ã€‚</li>
<li>ContextAgentåœ¨è‡ªåŠ¨è°ƒç”¨å·¥å…·ä»¥ååŠ©ç”¨æˆ·æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9040c7152d5f02942263da1df2ca8223.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9087e17f390599c407981d7df1c30801.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e75153888530eb9a15a427723c9b60c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bef43eb44c3dc35014aa294443414bc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Beyond-Words-Multimodal-LLM-Knows-When-to-Speak"><a href="#Beyond-Words-Multimodal-LLM-Knows-When-to-Speak" class="headerlink" title="Beyond Words: Multimodal LLM Knows When to Speak"></a>Beyond Words: Multimodal LLM Knows When to Speak</h2><p><strong>Authors:Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin</strong></p>
<p>While large language model (LLM)-based chatbots have demonstrated strong capabilities in generating coherent and contextually relevant responses, they often struggle with understanding when to speak, particularly in delivering brief, timely reactions during ongoing conversations. This limitation arises largely from their reliance on text input, lacking the rich contextual cues in real-world human dialogue. In this work, we focus on real-time prediction of response types, with an emphasis on short, reactive utterances that depend on subtle, multimodal signals across vision, audio, and text. To support this, we introduce a new multimodal dataset constructed from real-world conversational videos, containing temporally aligned visual, auditory, and textual streams. This dataset enables fine-grained modeling of response timing in dyadic interactions. Building on this dataset, we propose MM-When2Speak, a multimodal LLM-based model that adaptively integrates visual, auditory, and textual context to predict when a response should occur, and what type of response is appropriate. Experiments show that MM-When2Speak significantly outperforms state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x improvement in response timing accuracy over leading commercial LLMs. These results underscore the importance of multimodal inputs for producing timely, natural, and engaging conversational AI. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èŠå¤©æœºå™¨äººå·²å±•ç°å‡ºç”Ÿæˆè¿è´¯ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„å“åº”çš„å¼ºå¤§èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥ç†è§£ä½•æ—¶åº”è¯¥å‘è¨€ï¼Œå°¤å…¶æ˜¯åœ¨æŒç»­å¯¹è¯ä¸­æä¾›ç®€çŸ­ã€åŠæ—¶çš„ååº”ã€‚è¿™ä¸€å±€é™æ€§ä¸»è¦æºäºå®ƒä»¬å¯¹æ–‡æœ¬è¾“å…¥çš„ä¾èµ–ï¼Œç¼ºä¹ç°å®ä¸–ç•Œä¸­äººç±»å¯¹è¯çš„ä¸°å¯Œä¸Šä¸‹æ–‡çº¿ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå®æ—¶é¢„æµ‹å“åº”ç±»å‹ï¼Œé‡ç‚¹æ˜¯é€šè¿‡è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ç­‰ç»†å¾®çš„å¤šæ¨¡å¼ä¿¡å·æ¥é¢„æµ‹ç®€çŸ­ã€ååº”æ€§çš„å‘è¨€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªç°å®ä¸–ç•Œçš„å¯¹è¯è§†é¢‘ï¼ŒåŒ…å«æ—¶é—´å¯¹é½çš„è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æµã€‚è¯¥æ•°æ®é›†èƒ½å¤Ÿç²¾ç»†åœ°æ¨¡æ‹ŸäºŒäººäº’åŠ¨ä¸­çš„å“åº”æ—¶é—´ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†MM-When2Speakæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡å¼LLMçš„æ¨¡å‹ï¼Œå¯è‡ªé€‚åº”åœ°æ•´åˆè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œä»¥é¢„æµ‹ä½•æ—¶åº”è¯¥å‘ç”Ÿå“åº”ä»¥åŠä½•ç§ç±»å‹çš„å“åº”æ˜¯æ°å½“çš„ã€‚å®éªŒè¡¨æ˜ï¼ŒMM-When2Speakæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„å•æ¨¡å¼LLMåŸºçº¿æ¨¡å‹ï¼Œåœ¨å“åº”æ—¶é—´å‡†ç¡®æ€§ä¸Šæœ€é«˜è¾¾åˆ°é¢†å…ˆå•†ä¸šLLMæ¨¡å‹çš„å››å€æ”¹å–„æ•ˆæœã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†å¤šæ¨¡å¼è¾“å…¥å¯¹äºäº§ç”ŸåŠæ—¶ã€è‡ªç„¶å’Œå¼•äººå…¥èƒœçš„èŠå¤©æœºå™¨äººçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14654v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/lzk901372/MM-When2Speak">https://github.com/lzk901372/MM-When2Speak</a></p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆè¿è´¯ã€è¯­å¢ƒç›¸å…³çš„èŠå¤©æœºå™¨äººå›åº”æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å®æ—¶å¯¹è¯ä¸­ç†è§£ä½•æ—¶å‘è¨€ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡èšç„¦äºå“åº”ç±»å‹çš„å®æ—¶é¢„æµ‹ï¼Œç‰¹åˆ«æ˜¯ä¾èµ–äºè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬ç­‰å¤šç§å¾®å¦™ä¿¡å·çš„ç®€çŸ­ååº”å‘è¨€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥æ–°çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºMM-When2Speakæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯è‡ªé€‚åº”åœ°æ•´åˆè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œä»¥é¢„æµ‹ä½•æ—¶åº”è¯¥å‘ç”Ÿå“åº”ä»¥åŠä½•ç§ç±»å‹çš„å“åº”æ˜¯æ°å½“çš„ã€‚å®éªŒè¡¨æ˜ï¼ŒMM-When2Speakæ˜æ˜¾ä¼˜äºæœ€æ–°çš„å•æ¨¡æ€å’ŒLLMåŸºå‡†æ¨¡å‹ï¼Œåœ¨å“åº”æ—¶é—´å‡†ç¡®æ€§æ–¹é¢æé«˜äº†å››å€ã€‚è¿™çªæ˜¾äº†å¤šæ¨¡å¼è¾“å…¥å¯¹äºäº§ç”ŸåŠæ—¶ã€è‡ªç„¶å’Œå¼•äººå…¥èƒœçš„ä¼šè¯AIçš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç”Ÿæˆè¿è´¯çš„èŠå¤©æœºå™¨äººå›åº”æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å®æ—¶å¯¹è¯ä¸­ç†è§£ä½•æ—¶å‘è¨€å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å“åº”ç±»å‹çš„å®æ—¶é¢„æµ‹æ˜¯å…³é”®ï¼Œç‰¹åˆ«æ˜¯ä¾èµ–äºè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬ç­‰å¤šç§å¾®å¦™ä¿¡å·çš„ç®€çŸ­ååº”å‘è¨€ã€‚</li>
<li>å¼•å…¥æ–°çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼Œç”¨äºæ„å»ºåŸºäºLLMçš„èŠå¤©æœºå™¨äººæ¨¡å‹ã€‚</li>
<li>æå‡ºMM-When2Speakæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ•´åˆå¤šæ¨¡å¼ä¸Šä¸‹æ–‡ä»¥é¢„æµ‹å“åº”æ—¶æœºå’Œç±»å‹ã€‚</li>
<li>MM-When2Speakæ¨¡å‹åœ¨å“åº”æ—¶é—´å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—æé«˜ï¼Œä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœçªæ˜¾äº†å¤šæ¨¡å¼è¾“å…¥å¯¹äºäº§ç”Ÿè‡ªç„¶ã€åŠæ—¶çš„ä¼šè¯AIçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab68199847e3a180f9e54e416f7bf209.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-866487d01c92c1bc84e158fc4af5eb50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f4adf1ab29cd85682b40f587d05f615.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f1bb3793f8ffdc8f3ef1a0e79b7fdcd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="General-Reasoner-Advancing-LLM-Reasoning-Across-All-Domains"><a href="#General-Reasoner-Advancing-LLM-Reasoning-Across-All-Domains" class="headerlink" title="General-Reasoner: Advancing LLM Reasoning Across All Domains"></a>General-Reasoner: Advancing LLM Reasoning Across All Domains</h2><p><strong>Authors:Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen</strong></p>
<p>Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the â€œZeroâ€ reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ã€‚ç‰¹åˆ«æ˜¯Deepseek-R1-Zeroå¼•å…¥çš„â€œé›¶â€å¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿå®ç°å¯¹åŸºç¡€LLMçš„ç›´æ¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ— éœ€ä¾èµ–ä¸­é—´ç›‘ç£å¾®è°ƒé˜¶æ®µã€‚å°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼Œä½†ç›®å‰LLMæ¨ç†çš„ä¸»è¦å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ•°æ®ä¸°å¯Œä¸”ç­”æ¡ˆéªŒè¯æ–¹ä¾¿ã€‚è¿™é™åˆ¶äº†æ­¤ç±»æ¨¡å‹åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨å’Œé€šç”¨æ€§ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œé—®é¢˜çš„ç­”æ¡ˆè¡¨ç¤ºé€šå¸¸å…·æœ‰å¤šæ ·æ€§ï¼Œä¸”æ•°æ®æ›´åŠ ç¨€ç¼ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†General-Reasonerï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æå‡LLMåœ¨å¤šæ ·åŒ–é¢†åŸŸæ¨ç†èƒ½åŠ›çš„æ–°å‹è®­ç»ƒèŒƒå¼ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é€šè¿‡ç½‘é¡µçˆ¬è™«æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é—®é¢˜æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¯éªŒè¯çš„ç­”æ¡ˆï¼Œè¦†ç›–å¹¿æ³›çš„å­¦ç§‘é¢†åŸŸï¼›ï¼ˆ2ï¼‰å¼€å‘äº†ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œå®ƒç”¨åŸºäºæ€ç»´é“¾å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„èƒ½åŠ›å–ä»£äº†ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„éªŒè¯æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—æ¨¡å‹ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶åœ¨æ¶µç›–ç‰©ç†ã€åŒ–å­¦ã€é‡‘èã€ç”µå­ç­‰å¹¿æ³›é¢†åŸŸçš„å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨12ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚MMLU-Proã€GPQAã€SuperGPQAã€TheoremQAã€BBEHå’ŒMATH AMCï¼‰ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒGeneral-Reasonerä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼Œå®ç°äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä¿æŒå“è¶Šçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14652v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚Deepseek-R1-Zeroæå‡ºçš„â€œé›¶â€å¼ºåŒ–å­¦ä¹ å¯ä»¥ç›´æ¥è®­ç»ƒåŸºç¡€LLMæ¨¡å‹ï¼Œæ— éœ€ä¾èµ–ä¸­é—´ç›‘ç£å¾®è°ƒé˜¶æ®µã€‚ç„¶è€Œï¼Œå½“å‰LLMæ¨ç†ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„é€‚ç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºGeneral-Reasonerï¼Œä¸€ç§æ—¨åœ¨æå‡LLMåœ¨å¤šä¸ªé¢†åŸŸæ¨ç†èƒ½åŠ›çš„æ–°å‹è®­ç»ƒèŒƒå¼ã€‚å…¶å…³é”®è´¡çŒ®åŒ…æ‹¬æ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡é—®é¢˜æ•°æ®é›†å’ŒåŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ã€‚å®éªŒè¡¨æ˜ï¼ŒGeneral-Reasoneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°ç¨³å¥ä¸”å¯æ¨å¹¿çš„æ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ å¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Deepseek-R1-Zeroæå‡ºçš„â€œé›¶â€å¼ºåŒ–å­¦ä¹ å¯ç›´æ¥è®­ç»ƒLLMæ¨¡å‹ã€‚</li>
<li>å½“å‰LLMæ¨ç†ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œé™åˆ¶äº†å…¶åœ¨æ›´å¹¿é¢†åŸŸçš„é€‚ç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>General-Reasoneræ—¨åœ¨æå‡LLMåœ¨å¤šä¸ªé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>General-Reasonerçš„å…³é”®è´¡çŒ®åŒ…æ‹¬æ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡é—®é¢˜æ•°æ®é›†å’ŒåŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ã€‚</li>
<li>General-Reasoneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13f1a79a6b6dcc23b3f9a825e6f8478e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21cc0a7179f7bac17567dd372e69232a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa32ca03b448f35ebee2bd358bab9b61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49858842a88a5b99574dd1b67a920b32.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Think-Only-When-You-Need-with-Large-Hybrid-Reasoning-Models"><a href="#Think-Only-When-You-Need-with-Large-Hybrid-Reasoning-Models" class="headerlink" title="Think Only When You Need with Large Hybrid-Reasoning Models"></a>Think Only When You Need with Large Hybrid-Reasoning Models</h2><p><strong>Authors:Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei</strong></p>
<p>Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the modelâ€™s capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems. </p>
<blockquote>
<p>è¿‘æœŸçš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡èå…¥æœ€ç»ˆçš„å›åº”ä¹‹å‰çš„æ‰©å±•æ€è€ƒè¿‡ç¨‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºæ˜¾è‘—å¢å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‡åº¦å†—é•¿çš„æ€è€ƒè¿‡ç¨‹åœ¨ä»¤ç‰Œæ¶ˆè€—å’Œå»¶è¿Ÿæ–¹é¢å¼•å…¥äº†å·¨å¤§çš„å¼€é”€ï¼Œè¿™åœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶æ˜¾å¾—å°¤ä¸ºä¸å¿…è¦ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å†³å®šæ˜¯å¦è¿›è¡Œæ€è€ƒçš„æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆé€šè¿‡æ··åˆå¾®è°ƒï¼ˆHFTï¼‰è¿›è¡Œå†·å¯åŠ¨ï¼Œç„¶åé€šè¿‡æå‡ºçš„æ··åˆç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆHGPOï¼‰è¿›è¡Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¥éšå«åœ°é€‰æ‹©é€‚å½“çš„æ€è€ƒæ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç§°ä¸ºæ··åˆå‡†ç¡®ç‡çš„æŒ‡æ ‡æ¥å®šé‡è¯„ä¼°æ¨¡å‹çš„æ··åˆæ€è€ƒèƒ½åŠ›ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLHRMså¯ä»¥è‡ªé€‚åº”åœ°å¯¹ä¸åŒç±»å‹å’Œéš¾åº¦çš„æŸ¥è¯¢è¿›è¡Œæ··åˆæ€è€ƒï¼Œåœ¨æ¨ç†å’Œä¸€èˆ¬èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰çš„LRMså’ŒLLMsï¼Œå¹¶å¤§å¤§æé«˜äº†æ•ˆç‡ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æå€¡é‡æ–°è€ƒè™‘é€‚å½“ä½¿ç”¨æ‰©å±•æ€è€ƒè¿‡ç¨‹ï¼Œå¹¶ä¸ºæ„å»ºæ··åˆæ€è€ƒç³»ç»Ÿæä¾›äº†ä¸€ä¸ªåšå®çš„èµ·ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14631v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è‡ªé€‚åº”åœ°å†³å®šæ˜¯å¦éœ€è¦æ€è€ƒã€‚é€šè¿‡æ··åˆå¾®è°ƒï¼ˆHFTï¼‰çš„å†·å¯åŠ¨å’Œæ··åˆç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆHGPOï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿéšå¼å­¦ä¹ é€‰æ‹©é€‚å½“çš„æ€è€ƒæ¨¡å¼ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æ··åˆç²¾åº¦è¯„ä¼°æŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹çš„æ··åˆæ€ç»´èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒLHRMsèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„æŸ¥è¯¢éš¾åº¦å’Œç±»å‹è‡ªé€‚åº”åœ°è¿›è¡Œæ··åˆæ€è€ƒï¼Œå¹¶ä¸”åœ¨æ¨ç†å’Œé€šç”¨èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰çš„å¤§å‹æ¨ç†æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶æé«˜äº†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LHRMsèƒ½è‡ªé€‚åº”åœ°å†³å®šæ˜¯å¦éœ€è¦æ€è€ƒï¼ŒåŸºäºç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡æ··åˆå¾®è°ƒï¼ˆHFTï¼‰çš„å†·å¯åŠ¨å’Œæ··åˆç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆHGPOï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿéšå¼å­¦ä¹ é€‰æ‹©æ€è€ƒæ¨¡å¼ã€‚</li>
<li>å¼•å…¥äº†æ··åˆç²¾åº¦è¯„ä¼°æŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹çš„æ··åˆæ€ç»´èƒ½åŠ›ã€‚</li>
<li>LHRMsèƒ½å¤Ÿæ ¹æ®ä¸åŒçš„æŸ¥è¯¢éš¾åº¦å’Œç±»å‹è¿›è¡Œè‡ªé€‚åº”çš„æ··åˆæ€è€ƒã€‚</li>
<li>LHRMsåœ¨æ¨ç†å’Œé€šç”¨èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚</li>
<li>LHRMsæé«˜äº†æ•ˆç‡ï¼Œå¯¹äºç®€å•çš„æŸ¥è¯¢ä¸éœ€è¦è¿‡é•¿çš„æ€è€ƒæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a91a379cc090d05caebc5c82afc503e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-862a957a47f2ee33a00508036807f555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-174a003a6a0d4479a481f386b88b6024.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="KERL-Knowledge-Enhanced-Personalized-Recipe-Recommendation-using-Large-Language-Models"><a href="#KERL-Knowledge-Enhanced-Personalized-Recipe-Recommendation-using-Large-Language-Models" class="headerlink" title="KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large   Language Models"></a>KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large   Language Models</h2><p><strong>Authors:Fnu Mohbat, Mohammed J Zaki</strong></p>
<p>Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at <a target="_blank" rel="noopener" href="https://github.com/mohbattharani/KERL">https://github.com/mohbattharani/KERL</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å’Œé£Ÿå“æ•°æ®çš„ä¸°å¯Œï¼Œåˆ©ç”¨LLMæ”¹å–„é£Ÿå“ç†è§£çš„ç ”ç©¶åº”è¿è€Œç”Ÿã€‚å°½ç®¡å·²æœ‰ä¸€äº›æ¨èç³»ç»Ÿåˆ©ç”¨LLMå’ŒçŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ï¼Œä½†å°†é£Ÿå“ç›¸å…³çš„KGsä¸LLMç›¸ç»“åˆçš„ç ”ç©¶å´å¾ˆæœ‰é™ã€‚æˆ‘ä»¬ä»‹ç»äº†KERLï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨é£Ÿå“KGså’ŒLLMæ¥æä¾›ä¸ªæ€§åŒ–çš„é£Ÿå“æ¨èå¹¶ç”Ÿæˆå¸¦æœ‰ç›¸å…³å¾®è¥å…»ä¿¡æ¯çš„é£Ÿè°±ã€‚ç»™å®šè‡ªç„¶è¯­è¨€é—®é¢˜ï¼ŒKERLæå–å®ä½“ï¼Œä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢å­å›¾ï¼Œç„¶åå°†å®ƒä»¬ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥åˆ°LLMä¸­ï¼Œä»¥é€‰æ‹©æ»¡è¶³çº¦æŸçš„é£Ÿè°±ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿä¸ºæ¯ä¸ªé£Ÿè°±ç”Ÿæˆçƒ¹é¥ªæ­¥éª¤å’Œè¥å…»ä¿¡æ¯ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æ•´ç†ä¸é£Ÿè°±ç›¸å…³çš„é—®é¢˜ã€ç»“åˆçº¦æŸå’Œä¸ªäººåå¥½ï¼Œå¼€å‘äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æ‰€æå‡ºçš„KGå¢å¼ºLLMæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºé£Ÿå“æ¨èã€é£Ÿè°±ç”Ÿæˆå’Œè¥å…»åˆ†ææä¾›äº†å®Œæ•´ä¸”è¿è´¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å’ŒåŸºå‡†æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mohbattharani/KERL%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/mohbattharani/KERLå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14629v1">PDF</a> Accepted at ACL 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œé£Ÿå“çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„å…ˆè¿›ç ”ç©¶ï¼ŒKERLç³»ç»Ÿåˆ©ç”¨é£Ÿå“KGså’ŒLLMsæä¾›ä¸ªæ€§åŒ–é£Ÿå“æ¨èå¹¶ç”Ÿæˆå¸¦æœ‰ç›¸å…³å¾®è¥å…»ä¿¡æ¯çš„é£Ÿè°±ã€‚é€šè¿‡æå–å®ä½“ã€ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢å­å›¾å¹¶å°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥åˆ°LLMä¸­ï¼Œä»¥æ»¡è¶³çº¦æŸæ¡ä»¶é€‰æ‹©æ»¡è¶³è¦æ±‚çš„é£Ÿè°±ï¼Œç”Ÿæˆçƒ¹é¥ªæ­¥éª¤å’Œè¥å…»ä¿¡æ¯ã€‚å®éªŒç»“æœè¯æ˜äº†KGå¢å¼ºå‹LLMçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsä¸é£Ÿå“çŸ¥è¯†å›¾è°±ç»“åˆç”¨äºæ”¹è¿›é£Ÿå“ç†è§£ã€‚</li>
<li>KERLç³»ç»Ÿå®ç°äº†ä¸ªæ€§åŒ–é£Ÿå“æ¨èå’Œé£Ÿè°±ç”Ÿæˆï¼Œå¸¦æœ‰å¾®è¥å…»ä¿¡æ¯ã€‚</li>
<li>KERLé€šè¿‡æå–å®ä½“ã€æ£€ç´¢çŸ¥è¯†å›¾è°±å­å›¾å¹¶ä¸LLMç»“åˆæ¥æ»¡è¶³çº¦æŸæ¡ä»¶ã€‚</li>
<li>å…¬å¼€å¯ç”¨çš„benchmarkæ•°æ®é›†ç”¨äºè¯„ä¼°ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„KGå¢å¼ºå‹LLMæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>KERLç³»ç»Ÿä¸ºé£Ÿå“æ¨èã€é£Ÿè°±ç”Ÿæˆå’Œè¥å…»åˆ†ææä¾›äº†å…¨é¢ã€è¿è´¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0bc89d4e3d6c89cdb2e9afcf76ed57e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de470842645f94e5cfa7141ef0144378.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91e882a7f76188659ca32fcbe5620a87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce3b81b64847a61b96cc0157f8453c84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0ce1a4a79fec8d00af82d3b31392152.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Debating-for-Better-Reasoning-An-Unsupervised-Multimodal-Approach"><a href="#Debating-for-Better-Reasoning-An-Unsupervised-Multimodal-Approach" class="headerlink" title="Debating for Better Reasoning: An Unsupervised Multimodal Approach"></a>Debating for Better Reasoning: An Unsupervised Multimodal Approach</h2><p><strong>Authors:Ashutosh Adhikari, Mirella Lapata</strong></p>
<p>As Large Language Models (LLMs) gain expertise across diverse domains and modalities, scalable oversight becomes increasingly challenging, particularly when their capabilities may surpass human evaluators. Debate has emerged as a promising mechanism for enabling such oversight. In this work, we extend the debate paradigm to a multimodal setting, exploring its potential for weaker models to supervise and enhance the performance of stronger models. We focus on visual question answering (VQA), where two â€œsightedâ€ expert vision-language models debate an answer, while a â€œblindâ€ (text-only) judge adjudicates based solely on the quality of the arguments. In our framework, the experts defend only answers aligned with their beliefs, thereby obviating the need for explicit role-playing and concentrating the debate on instances of expert disagreement. Experiments on several multimodal tasks demonstrate that the debate framework consistently outperforms individual expert models. Moreover, judgments from weaker LLMs can help instill reasoning capabilities in vision-language models through finetuning. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸå’Œæ¨¡æ€ä¸­è¶Šæ¥è¶Šä¸“ä¸šåŒ–ï¼Œå¯æ‰©å±•çš„ç›‘ç£å˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å½“å®ƒä»¬çš„èƒ½åŠ›å¯èƒ½è¶…è¶Šäººç±»è¯„ä¼°è€…æ—¶ã€‚è¾©è®ºä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æœºåˆ¶ï¼Œä¸ºè¿™ç§ç›‘ç£æä¾›äº†å¯èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¾©è®ºèŒƒå¼æ‰©å±•åˆ°å¤šæ¨¡æ€ç¯å¢ƒï¼Œæ¢ç´¢å…¶æ½œåŠ›ï¼Œä½¿è¾ƒå¼±æ¨¡å‹èƒ½å¤Ÿç›‘ç£å¹¶å¢å¼ºè¾ƒå¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä¸“æ³¨äºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼Œå…¶ä¸­ä¸¤ä¸ªâ€œè§†åŠ›â€ä¸“å®¶è§†è§‰è¯­è¨€æ¨¡å‹å¯¹ç­”æ¡ˆè¿›è¡Œè¾©è®ºï¼Œè€Œä¸€ä¸ªâ€œç›²â€ï¼ˆä»…æ–‡æœ¬ï¼‰æ³•å®˜ä»…æ ¹æ®è®ºè¯çš„è´¨é‡è¿›è¡Œè£å†³ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œä¸“å®¶åªæå«ä¸å…¶ä¿¡å¿µç›¸ç¬¦çš„ç­”æ¡ˆï¼Œä»è€Œä¸éœ€è¦æ˜ç¡®çš„è§’è‰²æ‰®æ¼”ï¼Œå¹¶å°†è¾©è®ºé›†ä¸­åœ¨ä¸“å®¶æ„è§ä¸ä¸€è‡´çš„æƒ…å†µä¸Šã€‚åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¾©è®ºæ¡†æ¶å§‹ç»ˆä¼˜äºå•ä¸ªä¸“å®¶æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¾ƒå¼±çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­å¯ä»¥é€šè¿‡å¾®è°ƒå¸®åŠ©è§†è§‰è¯­è¨€æ¨¡å‹åŸ¹å…»æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14627v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé¢†åŸŸå’Œå¤šæ¨¡æ€æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„ä¸“ä¸šçŸ¥è¯†ï¼Œä½†å¯¹å…¶è¿›è¡Œå¯ä¼¸ç¼©çš„ç›‘ç£å˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å½“å…¶èƒ½åŠ›å¯èƒ½è¶…è¿‡äººç±»è¯„ä¼°è€…æ—¶ã€‚è¾©è®ºä½œä¸ºä¸€ç§ç›‘ç£æ–¹å¼å±•ç°äº†å·¨å¤§çš„æ½œåŠ›ï¼Œæœ¬ç ”ç©¶å°†è¾©è®ºèŒƒå¼æ‰©å±•åˆ°å¤šæ¨¡æ€ç¯å¢ƒä¸­ï¼Œæ¢è®¨å…¶å¯¹å¼±æ¨¡å‹ç›‘ç£å¼ºæ¨¡å‹æ€§èƒ½çš„æ½œåŠ›ã€‚åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰é¢†åŸŸï¼Œä¸¤ä¸ªâ€œæœ‰è§†åŠ›â€çš„ä¸“å®¶è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè¾©è®ºç­”æ¡ˆï¼Œä¸€ä¸ªâ€œç›²â€ï¼ˆä»…æ–‡æœ¬ï¼‰çš„æ³•å®˜æ ¹æ®è®ºè¯è´¨é‡è¿›è¡Œè£å†³ã€‚ä¸“å®¶çš„ç­”æ¡ˆä»…åŸºäºä»–ä»¬çš„ä¿¡å¿µè¿›è¡Œè¾©æŠ¤ï¼Œæ— éœ€è¿›è¡Œæ˜ç¡®çš„è§’è‰²æ‰®æ¼”ï¼Œä¸“æ³¨äºä¸“å®¶çš„åˆ†æ­§å®ä¾‹ã€‚åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¾©è®ºæ¡†æ¶å§‹ç»ˆä¼˜äºå•ä¸ªä¸“å®¶æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¾ƒå¼±çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­æœ‰åŠ©äºé€šè¿‡å¾®è°ƒèµ‹äºˆè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡æ€ä¸­çš„ä¸“ä¸šçŸ¥è¯†æå‡ä½¿å¾—ç›‘ç£å˜å¾—æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>è¾©è®ºä½œä¸ºä¸€ç§ç›‘ç£æ–¹å¼å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­ã€‚</li>
<li>åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰é¢†åŸŸï¼Œå¼•å…¥è¾©è®ºèŒƒå¼ï¼Œå…¶ä¸­ä¸¤ä¸ªä¸“å®¶æ¨¡å‹è¾©è®ºç­”æ¡ˆï¼Œä¸€ä¸ªâ€œç›²â€æ¨¡å‹ä½œä¸ºæ³•å®˜è£å†³ã€‚</li>
<li>ä¸“å®¶æ¨¡å‹æ ¹æ®ä¿¡å¿µè¿›è¡Œç­”æ¡ˆè¾©æŠ¤ï¼Œæ— éœ€æ˜ç¡®è§’è‰²æ‰®æ¼”ï¼Œèšç„¦äºä¸“å®¶åˆ†æ­§å®ä¾‹ã€‚</li>
<li>è¾©è®ºæ¡†æ¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„è¡¨ç°å§‹ç»ˆä¼˜äºå•ä¸ªä¸“å®¶æ¨¡å‹ã€‚</li>
<li>è¾ƒå¼±çš„LLMåˆ¤æ–­æœ‰åŠ©äºé€šè¿‡å¾®è°ƒå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e15477a9d03323e28996b6760b0b59e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8124af251bffbf6d24ad78029cf71c59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-514e24f1b2e1cb22dfd8624dea2e206c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41838e885eecf591d9a4bf7f8727c02f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning"><a href="#TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning" class="headerlink" title="TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning"></a>TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning</h2><p><strong>Authors:Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran</strong></p>
<p>Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RLâ€™s success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problemâ€“false negativesâ€“where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV">https://github.com/uw-nsl/TinyV</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºé€šè¿‡ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç­–ç•¥å’Œå¥–åŠ±ä¿¡å·æ¥æå‡å…¶æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼ŒRLçš„æˆåŠŸä¾èµ–äºå¥–åŠ±çš„å¯é æ€§ï¼Œè¿™äº›å¥–åŠ±ç”±éªŒè¯å™¨æä¾›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºå¹¶åˆ†æäº†å¹¿æ³›å­˜åœ¨çš„é—®é¢˜â€”â€”å‡é˜´æ€§ï¼ˆfalse negativesï¼‰ï¼Œå³éªŒè¯å™¨é”™è¯¯åœ°æ‹’ç»äº†æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚æˆ‘ä»¬å¯¹Big-Math-RL-Verifiedæ•°æ®é›†çš„æ·±å…¥ç ”ç©¶åå‘ç°ï¼Œè¶…è¿‡3 8%çš„æ¨¡å‹ç”Ÿæˆå“åº”å­˜åœ¨å‡é˜´æ€§é—®é¢˜ï¼Œå³éªŒè¯å™¨æ— æ³•è¯†åˆ«æ­£ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬é€šè¿‡å®è¯å’Œç†è®ºè¯æ˜ï¼Œè¿™äº›å‡é˜´æ€§ä¼šä¸¥é‡æŸå®³RLè®­ç»ƒï¼Œå› ä¸ºå‰¥å¤ºäº†æ¨¡å‹çš„ä¿¡æ¯åŒ–æ¢¯åº¦ä¿¡å·å¹¶å‡ç¼“äº†æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†tinyVï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨ï¼Œå®ƒå¢å¼ºäº†ç°æœ‰çš„åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œèƒ½å¤ŸåŠ¨æ€è¯†åˆ«æ½œåœ¨çš„å‡é˜´æ€§å¹¶æ¢å¤æœ‰æ•ˆçš„å“åº”ï¼Œä»¥äº§ç”Ÿæ›´å‡†ç¡®çš„å¥–åŠ±ä¼°è®¡ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œé›†æˆTinyVçš„é€šè¿‡ç‡æé«˜äº†é«˜è¾¾1 0%ï¼Œç›¸å¯¹äºåŸºçº¿åŠ é€Ÿäº†æ”¶æ•›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†è§£å†³éªŒè¯å™¨å‡é˜´æ€§çš„å…³é”®é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§å®ç”¨çš„æ–¹æ³•æ¥æ”¹è¿›åŸºäºRLçš„LLMå¾®è°ƒã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/uw-nsl/TinyVä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14625v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡ä¼˜åŒ–ç­–ç•¥å’Œä½¿ç”¨å¥–åŠ±ä¿¡å·æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒRLçš„æˆåŠŸä¾èµ–äºå¥–åŠ±çš„å¯é æ€§ï¼Œå¥–åŠ±ç”±éªŒè¯å™¨æä¾›ã€‚æœ¬æ–‡æ­ç¤ºäº†ä¸€ä¸ªæ™®éå­˜åœ¨çš„é—®é¢˜â€”â€”å‡é˜´æ€§ï¼ˆFalse Negativeï¼‰ï¼Œå³éªŒè¯å™¨é”™è¯¯åœ°æ‹’ç»æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚é€šè¿‡å¯¹Big-Math-RL-Verifiedæ•°æ®é›†çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°è¶…è¿‡38%çš„æ¨¡å‹ç”Ÿæˆå“åº”å­˜åœ¨å‡é˜´æ€§é—®é¢˜ã€‚æœ¬æ–‡é€šè¿‡å®è¯å’Œç†è®ºåˆ†æè¡¨æ˜ï¼Œå‡é˜´æ€§ä¼šä¸¥é‡é˜»ç¢RLè®­ç»ƒï¼Œå‰¥å¤ºæ¨¡å‹çš„æœ‰ç”¨æ¢¯åº¦ä¿¡å·å¹¶å‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TinyVï¼Œä¸€ç§åŸºäºè½»é‡çº§LLMçš„éªŒè¯å™¨ï¼Œå®ƒé€šè¿‡å¢å¼ºç°æœ‰çš„åŸºäºè§„åˆ™çš„éªŒè¯å™¨æ¥åŠ¨æ€è¯†åˆ«æ½œåœ¨çš„å‡é˜´æ€§å¹¶æ¢å¤æœ‰æ•ˆçš„å“åº”ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®çš„å¥–åŠ±ä¼°è®¡ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œé›†æˆTinyVå¯å°†é€šè¿‡ç‡æé«˜é«˜è¾¾10%ï¼Œå¹¶ç›¸å¯¹äºåŸºçº¿åŠ é€Ÿæ”¶æ•›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†è§£å†³éªŒè¯å™¨å‡é˜´æ€§çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§å®ç”¨çš„æ–¹æ³•æ¥æ”¹è¿›åŸºäºRLçš„LLMå¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éªŒè¯å™¨åœ¨å¥–åŠ±ä¿¡å·ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å…¶æ˜“å‡ºç°å‡é˜´æ€§é”™è¯¯ã€‚</li>
<li>å‡é˜´æ€§é—®é¢˜åœ¨æ¨¡å‹ç”Ÿæˆçš„å“åº”ä¸­å æ¯”è¶…è¿‡38%ã€‚</li>
<li>å‡é˜´æ€§ä¼šä¸¥é‡é˜»ç¢å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒå’Œæ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>TinyVæ˜¯ä¸€ç§åŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨ï¼Œèƒ½åŠ¨æ€è¯†åˆ«å¹¶ä¿®å¤å‡é˜´æ€§ã€‚</li>
<li>é›†æˆTinyVåï¼Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„é€šè¿‡ç‡æé«˜10%ï¼Œä¸”æ”¶æ•›é€Ÿåº¦åŠ å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca76bd581877e2fa80edb606fc61e233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45315ad641603139bd5ff244f95ac9c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a08c30a5d241b44320c26435bfbb837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98fa5864234d9a3f6c406da5a995e97.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Learned-Knowledge-in-LoRA-Adapters-Through-Efficient-Contrastive-Decoding-on-Ascend-NPUs"><a href="#Enhancing-Learned-Knowledge-in-LoRA-Adapters-Through-Efficient-Contrastive-Decoding-on-Ascend-NPUs" class="headerlink" title="Enhancing Learned Knowledge in LoRA Adapters Through Efficient   Contrastive Decoding on Ascend NPUs"></a>Enhancing Learned Knowledge in LoRA Adapters Through Efficient   Contrastive Decoding on Ascend NPUs</h2><p><strong>Authors:Morgan Lindsay Heisler, Linzi Xing, Ge Shi, Hanieh Sadri, Gursimran Singh, Weiwei Zhang, Tao Ye, Ying Xiong, Yong Zhang, Zhenan Fan</strong></p>
<p>Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and scalable method to fine-tune and customize large language models (LLMs) for application-specific needs. However, tasks that require complex reasoning or deep contextual understanding are often hindered by biases or interference from the base model when using typical decoding methods like greedy or beam search. These biases can lead to generic or task-agnostic responses from the base model instead of leveraging the LoRA-specific adaptations. In this paper, we introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed to maximize the use of task-specific knowledge in LoRA-adapted models, resulting in better downstream performance. CoLD uses contrastive decoding by scoring candidate tokens based on the divergence between the probability distributions of a LoRA-adapted expert model and the corresponding base model. This approach prioritizes tokens that better align with the LoRAâ€™s learned representations, enhancing performance for specialized tasks. While effective, a naive implementation of CoLD is computationally expensive because each decoding step requires evaluating multiple token candidates across both models. To address this, we developed an optimized kernel for Huaweiâ€™s Ascend NPU. CoLD achieves up to a 5.54% increase in task accuracy while reducing end-to-end latency by 28% compared to greedy decoding. This work provides practical and efficient decoding strategies for fine-tuned LLMs in resource-constrained environments and has broad implications for applied data science in both cloud and on-premises settings. </p>
<blockquote>
<p>åä¸ºäº‘ç”¨æˆ·åˆ©ç”¨LoRAï¼ˆä½ç§©é€‚åº”ï¼‰ä½œä¸ºä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒå¹¶å®šåˆ¶ï¼Œä»¥æ»¡è¶³ç‰¹å®šåº”ç”¨çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨ä½¿ç”¨å…¸å‹çš„è§£ç æ–¹æ³•ï¼ˆå¦‚è´ªå¿ƒæœç´¢æˆ–é›†æŸæœç´¢ï¼‰æ—¶ï¼Œéœ€è¦å¤æ‚æ¨ç†æˆ–æ·±åº¦ä¸Šä¸‹æ–‡ç†è§£çš„ä»»åŠ¡å¾€å¾€ä¼šå—åˆ°åŸºç¡€æ¨¡å‹çš„åè§æˆ–å¹²æ‰°çš„é˜»ç¢ã€‚è¿™äº›åè§å¯èƒ½å¯¼è‡´åŸºç¡€æ¨¡å‹ç»™å‡ºé€šç”¨æˆ–ä»»åŠ¡æ— å…³çš„ååº”ï¼Œè€Œä¸æ˜¯åˆ©ç”¨LoRAç‰¹å®šçš„é€‚åº”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¯¹æ¯”LoRAè§£ç ï¼ˆCoLDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æœ€å¤§åŒ–LoRAé€‚åº”æ¨¡å‹ä¸­ç‰¹å®šä»»åŠ¡çŸ¥è¯†ä½¿ç”¨çš„æ–°å‹è§£ç æ¡†æ¶ï¼Œä»è€Œæé«˜äº†ä¸‹æ¸¸æ€§èƒ½ã€‚CoLDé‡‡ç”¨å¯¹æ¯”è§£ç æ–¹æ³•ï¼Œæ ¹æ®LoRAé€‚åº”çš„ä¸“å®¶æ¨¡å‹ä¸ç›¸åº”åŸºç¡€æ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒå·®å¼‚æ¥è¯„åˆ†å€™é€‰ä»¤ç‰Œã€‚è¿™ç§æ–¹æ³•ä¼˜å…ˆé€‰æ‹©ä¸LoRAå­¦ä¹ åˆ°çš„è¡¨ç¤ºæ›´åŒ¹é…çš„ä»¤ç‰Œï¼Œæé«˜äº†ä¸“é¡¹ä»»åŠ¡çš„æ€§èƒ½ã€‚è™½ç„¶è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†CoLDçš„æœ´ç´ å®ç°è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå› ä¸ºæ¯ä¸ªè§£ç æ­¥éª¤éƒ½éœ€è¦åœ¨ä¸¤ä¸ªæ¨¡å‹ä¸Šè¯„ä¼°å¤šä¸ªä»¤ç‰Œå€™é€‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºåä¸ºçš„Ascend NPUå¼€å‘äº†ä¸€ä¸ªä¼˜åŒ–å†…æ ¸ã€‚ä¸è´ªå¿ƒè§£ç ç›¸æ¯”ï¼ŒCoLDåœ¨ä»»åŠ¡å‡†ç¡®æ€§ä¸Šæé«˜äº†é«˜è¾¾5.54%ï¼ŒåŒæ—¶ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†28%ã€‚è¿™é¡¹å·¥ä½œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†å®ç”¨ä¸”é«˜æ•ˆçš„è§£ç ç­–ç•¥ï¼Œå¯¹äº‘å’Œæœ¬åœ°ç¯å¢ƒä¸­çš„åº”ç”¨æ•°æ®ç§‘å­¦å…·æœ‰å¹¿æ³›çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14620v1">PDF</a> Accepted at ACM KDD 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åä¸ºäº‘ç”¨æˆ·åˆ©ç”¨LoRAï¼ˆä½ç§©é€‚é…ï¼‰ä½œä¸ºé«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æ»¡è¶³ç‰¹å®šåº”ç”¨éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨ä½¿ç”¨å…¸å‹çš„è§£ç æ–¹æ³•ï¼ˆå¦‚è´ªå¿ƒæœç´¢æˆ–æŸæœç´¢ï¼‰æ—¶ï¼Œéœ€è¦å¤æ‚æ¨ç†æˆ–æ·±åº¦ä¸Šä¸‹æ–‡ç†è§£çš„ä»»åŠ¡å¾€å¾€ä¼šå—åˆ°åŸºç¡€æ¨¡å‹çš„åè§æˆ–å¹²æ‰°å½±å“ã€‚åè§å¯èƒ½å¯¼è‡´åŸºç¡€æ¨¡å‹ç»™å‡ºé€šç”¨çš„ã€ä¸ä»»åŠ¡æ— å…³çš„ååº”ï¼Œè€Œä¸æ˜¯åˆ©ç”¨LoRAç‰¹å®šçš„é€‚é…ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è§£ç æ¡†æ¶â€”â€”å¯¹æ¯”LoRAè§£ç ï¼ˆCoLDï¼‰ï¼Œæ—¨åœ¨æœ€å¤§åŒ–LoRAé€‚é…æ¨¡å‹ä¸­ä»»åŠ¡ç‰¹å®šçŸ¥è¯†çš„ä½¿ç”¨ï¼Œä»è€Œæé«˜ä¸‹æ¸¸æ€§èƒ½ã€‚CoLDé€šè¿‡å¯¹æ¯”å€™é€‰æ ‡è®°çš„å¾—åˆ†ï¼ŒåŸºäºLoRAé€‚é…çš„ä¸“å®¶æ¨¡å‹ä¸ç›¸åº”åŸºç¡€æ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒå·®å¼‚è¿›è¡Œè§£ç ã€‚è¿™ç§æ–¹æ³•ä¼˜å…ˆé€‰æ‹©ä¸LoRAå­¦ä¹ åˆ°çš„è¡¨ç¤ºæ›´ä¸€è‡´çš„æ ‡è®°ï¼Œå¯¹äºä¸“ç”¨ä»»åŠ¡çš„æ€§èƒ½æå‡æœ‰æ˜æ˜¾å¸®åŠ©ã€‚å°½ç®¡æœ‰æ•ˆï¼Œä½†CoLDçš„ç›´è§‚å®ç°æ–¹å¼è®¡ç®—é‡å¤§ï¼Œå› ä¸ºæ¯ä¸ªè§£ç æ­¥éª¤éƒ½éœ€è¦åœ¨ä¸¤ä¸ªæ¨¡å‹ä¸Šè¯„ä¼°å¤šä¸ªæ ‡è®°å€™é€‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºåä¸ºçš„Ascend NPUå¼€å‘äº†ä¸€ä¸ªä¼˜åŒ–å†…æ ¸ã€‚CoLDåœ¨ä»»åŠ¡å‡†ç¡®æ€§ä¸Šæé«˜äº†é«˜è¾¾5.54%ï¼ŒåŒæ—¶ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†28%ï¼Œä¸è´ªå¿ƒè§£ç ç›¸æ¯”ã€‚è¿™é¡¹å·¥ä½œåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä¸ºç²¾ç»†è°ƒæ•´çš„è¯­è¨€æ¨¡å‹æä¾›äº†å®ç”¨ä¸”é«˜æ•ˆçš„è§£ç ç­–ç•¥ï¼Œå¯¹äºäº‘å’Œå†…éƒ¨éƒ¨ç½²ç¯å¢ƒä¸­çš„å®ç”¨æ•°æ®ç§‘å­¦å…·æœ‰å¹¿æ³›çš„å½±å“ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LoRAä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œè¢«åä¸ºäº‘ç”¨æˆ·ç”¨äºå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œä»¥æ»¡è¶³ç‰¹å®šçš„åº”ç”¨éœ€æ±‚ã€‚</li>
<li>å…¸å‹è§£ç æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­å¯èƒ½ä¼šå—åˆ°åŸºç¡€æ¨¡å‹çš„åè§æˆ–å¹²æ‰°çš„å½±å“ã€‚</li>
<li>CoLDè§£ç æ¡†æ¶æ—¨åœ¨é€šè¿‡å¯¹æ¯”ä¸“å®¶æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹çš„å·®å¼‚æ¥æœ€å¤§åŒ–ä»»åŠ¡ç‰¹å®šçŸ¥è¯†çš„ä½¿ç”¨ã€‚</li>
<li>CoLDé€šè¿‡ä¼˜å…ˆé€‰æ‹©ä¸LoRAå­¦ä¹ åˆ°çš„è¡¨ç¤ºæ›´ä¸€è‡´çš„æ ‡è®°ï¼Œæå‡äº†ä¸“ç”¨ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>CoLDçš„ç›´è§‚å®ç°æ–¹å¼è®¡ç®—é‡å¤§ï¼Œå› æ­¤å¼€å‘äº†ä¸€ä¸ªé’ˆå¯¹åä¸ºAscend NPUçš„ä¼˜åŒ–å†…æ ¸ã€‚</li>
<li>CoLDåœ¨ä»»åŠ¡å‡†ç¡®æ€§å’Œå»¶è¿Ÿæ–¹é¢æä¾›äº†æ˜¾è‘—çš„æå‡ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„è§£ç æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1dc1ae155fffc3d02736520f2ca5fe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4657c46ef5bcb72420a945c3809277ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-584ed4ee28fe950bc7d028e66120bf93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd403020709e474fd208bc4deb8b0bb0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Linear-Control-of-Test-Awareness-Reveals-Differential-Compliance-in-Reasoning-Models"><a href="#Linear-Control-of-Test-Awareness-Reveals-Differential-Compliance-in-Reasoning-Models" class="headerlink" title="Linear Control of Test Awareness Reveals Differential Compliance in   Reasoning Models"></a>Linear Control of Test Awareness Reveals Differential Compliance in   Reasoning Models</h2><p><strong>Authors:Sahar Abdelnabi, Ahmed Salem</strong></p>
<p>Reasoning-focused large language models (LLMs) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such â€œtest awarenessâ€ impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning LLMs across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation. </p>
<blockquote>
<p>ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹åˆ°æ­£åœ¨è¿›è¡Œè¯„ä¼°æ—¶æœ‰æ—¶ä¼šæ”¹å˜å…¶è¡Œä¸ºï¼Œè¿™ç§å½±å“ç±»ä¼¼äºéœæ¡‘ç°è±¡ï¼Œå¯èƒ½å¯¼è‡´å®ƒä»¬ä¼˜åŒ–é€šè¿‡æµ‹è¯•çš„æ€§èƒ½ï¼Œæˆ–è€…åœ¨ç°å®ä¸–ç•Œåæœä¼¼ä¹ä¸å­˜åœ¨çš„æƒ…å†µä¸‹æ›´å®¹æ˜“æ¥å—æœ‰å®³çš„æç¤ºã€‚æˆ‘ä»¬é¦–æ¬¡å¯¹â€œæµ‹è¯•æ„è¯†â€å¦‚ä½•å½±å“æ¨¡å‹è¡Œä¸ºè¿›è¡Œäº†å®šé‡ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯å…¶å®‰å…¨å¯¹é½æ–¹é¢ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç™½ç›’æ¢æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ï¼ˆiï¼‰çº¿æ€§è¯†åˆ«ä¸æ„è¯†ç›¸å…³çš„æ¿€æ´»ï¼Œï¼ˆiiï¼‰åœ¨ç›‘è§†ä¸‹æ¸¸æ€§èƒ½çš„åŒæ—¶ï¼Œå¼•å¯¼æ¨¡å‹è¿œç¦»æˆ–è¿œç¦»æµ‹è¯•æ„è¯†ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºä¸åŒæœ€å…ˆè¿›çš„å¼€æºæ¨ç†LLMï¼Œæ¶µç›–ç°å®å’Œå‡è®¾ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæµ‹è¯•æ„è¯†å¯¹å®‰å…¨å¯¹é½æœ‰æ˜¾è‘—å½±å“ï¼Œå¹¶ä¸”å¯¹ä¸åŒçš„æ¨¡å‹æœ‰ä¸åŒçš„å½±å“ã€‚é€šè¿‡å¯¹æ­¤æ½œåœ¨æ•ˆåº”è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨å¢åŠ æˆ‘ä»¬å¯¹å®‰å…¨è¯„ä¼°çš„ä¿¡å¿ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14617v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹è‡ªèº«è¢«è¯„ä¼°æ—¶ä¼šæ”¹å˜è¡Œä¸ºï¼Œç±»ä¼¼äºéœç´¢æ©æ•ˆåº”ï¼ˆHawthorne phenomenonï¼‰ã€‚è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹ä¸ºäº†é€šè¿‡æµ‹è¯•è€Œä¼˜åŒ–æ€§èƒ½ï¼Œæˆ–åœ¨ç¼ºä¹å®é™…åæœçš„æƒ…å†µä¸‹æ›´å®¹æ˜“éµå¾ªæœ‰å®³æç¤ºã€‚æœ¬ç ”ç©¶é¦–æ¬¡å®šé‡æ¢è®¨äº†â€œæµ‹è¯•æ„è¯†â€ï¼ˆtest awarenessï¼‰å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å…¶å®‰å…¨å¯¹é½æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å¼•å…¥äº†ç™½ç›’æ¢æµ‹æ¡†æ¶ï¼Œèƒ½å¤Ÿè¯†åˆ«ä¸æ„è¯†ç›¸å…³çš„æ¿€æ´»çŠ¶æ€å¹¶æ§åˆ¶æ¨¡å‹çš„æµ‹è¯•æ„è¯†ï¼ŒåŒæ—¶ç›‘æµ‹ä¸‹æ¸¸æ€§èƒ½ã€‚åœ¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œç°å®ä¸å‡è®¾ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæµ‹è¯•æ„è¯†å¯¹å®‰å…¨å¯¹é½æœ‰æ˜¾è‘—å½±å“ä¸”ä¸åŒæ¨¡å‹é—´å­˜åœ¨å·®å¼‚ã€‚ç ”ç©¶æ—¨åœ¨ä¸ºå¦‚ä½•æ§åˆ¶è¿™ç§æ½œåœ¨æ•ˆåº”æä¾›ç²¾ç»†è°ƒæ§ï¼Œä»è€Œæå‡å¯¹å®‰å…¨è¯„ä¼°çš„ä¿¡ä»»åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹åˆ°è¢«è¯„ä¼°æ—¶ä¼šè¡¨ç°å‡ºç±»ä¼¼äºéœç´¢æ©æ•ˆåº”çš„è¡Œä¸ºæ”¹å˜ã€‚</li>
<li>æµ‹è¯•æ„è¯†å¯¹æ¨¡å‹çš„å®‰å…¨å¯¹é½è¡¨ç°æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>å¼•å…¥äº†ç™½ç›’æ¢æµ‹æ¡†æ¶æ¥è¯†åˆ«å’Œç›‘æµ‹æµ‹è¯•æ„è¯†å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ã€‚</li>
<li>ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ„è¯†æ–¹é¢çš„è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚</li>
<li>ç ”ç©¶æä¾›äº†å¯¹æ¨¡å‹æµ‹è¯•æ„è¯†çš„ç²¾ç»†è°ƒæ§æ–¹æ³•ã€‚</li>
<li>æµ‹è¯•æ„è¯†çš„æ§åˆ¶æœ‰åŠ©äºæå‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨è¯„ä¼°çš„ä¿¡ä»»åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f6a0bf3937da1484ccd19b4b2819a2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa3e0e96d0c1d8ec60c5e3c686bcbf9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872d9b59887a977155475c5fcd8d4259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-105f5bbf548ea39480233b3273ad0524.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Let-LLMs-Break-Free-from-Overthinking-via-Self-Braking-Tuning"><a href="#Let-LLMs-Break-Free-from-Overthinking-via-Self-Braking-Tuning" class="headerlink" title="Let LLMs Break Free from Overthinking via Self-Braking Tuning"></a>Let LLMs Break Free from Overthinking via Self-Braking Tuning</h2><p><strong>Authors:Haoran Zhao, Yuchen Yan, Yongliang Shen, Haolei Xu, Wenqi Zhang, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang</strong></p>
<p>Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ï¼Œå¦‚OpenAI o1å’ŒDeepSeek-R1ï¼Œé€šè¿‡ç”Ÿæˆæ›´é•¿çš„æ€ç»´é“¾æ˜¾è‘—å¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ€§èƒ½çš„æå‡ä¼´éšç€ç”Ÿæˆè¿‡ç¨‹ä¸­å†—ä½™æ¨ç†çš„æ˜¾è‘—å¢åŠ ï¼Œå¯¼è‡´äº†è®¡ç®—å¼€é”€çš„å¢å¤§å’Œè¿‡åº¦æ€è€ƒé—®é¢˜çš„åŠ å‰§ã€‚å°½ç®¡è®¸å¤šç°æœ‰æ–¹æ³•æ—¨åœ¨è§£å†³è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤–éƒ¨å¹²é¢„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶Self-Braking Tuningï¼ˆSBTï¼‰ï¼Œå®ƒä»å…è®¸æ¨¡å‹è‡ªæˆ‘è°ƒèŠ‚å…¶æ¨ç†è¿‡ç¨‹çš„è§’åº¦æ¥è§£å†³è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ§åˆ¶æœºåˆ¶çš„ä¾èµ–ã€‚æˆ‘ä»¬åŸºäºæ ‡å‡†ç­”æ¡ˆæ„å»ºäº†ä¸€å¥—è¿‡åº¦æ€è€ƒè¯†åˆ«æŒ‡æ ‡ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç³»ç»Ÿæ–¹æ³•æ¥æ£€æµ‹å†—ä½™æ¨ç†ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«æ¨ç†è½¨è¿¹ä¸­çš„ä¸å¿…è¦æ­¥éª¤ï¼Œå¹¶ä¸ºå­¦ä¹ è‡ªæˆ‘è°ƒæ§è¡Œä¸ºç”Ÿæˆè®­ç»ƒä¿¡å·ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬åˆ¶å®šäº†æ„å»ºå…·æœ‰è‡ªé€‚åº”æ¨ç†é•¿åº¦çš„æ•°æ®çš„å®Œæ•´ç­–ç•¥ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„åˆ¶åŠ¨æç¤ºæœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶åœ°å­¦ä¹ åœ¨é€‚å½“çš„æ—¶æœºç»ˆæ­¢æ¨ç†ã€‚åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆAIMEã€AMCã€MATH500ã€GSM8Kï¼‰çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡å°‘äº†é«˜è¾¾60%çš„ä»¤ç‰Œæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†ä¸æ— é™åˆ¶æ¨¡å‹ç›¸å½“çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14604v1">PDF</a> Github:<a target="_blank" rel="noopener" href="https://github.com/CCAI-Lab/Self-Braking-Tuning">https://github.com/CCAI-Lab/Self-Braking-Tuning</a>; Project:   <a target="_blank" rel="noopener" href="https://ccai-lab.github.io/SBT">https://CCAI-Lab.github.io/SBT</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAI o1å’ŒDeepSeek-R1é€šè¿‡ç”Ÿæˆæ›´é•¿çš„æ€ç»´é“¾æ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ€§èƒ½æå‡ä¼´éšç€æ¨ç†è¿‡ç¨‹ä¸­å†—ä½™æ¨ç†çš„æ˜¾è‘—å¢åŠ ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¢å¤§å’Œè¿‡åº¦æ€è€ƒé—®é¢˜åŠ å‰§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶Self-Braking Tuningï¼ˆSBTï¼‰ï¼Œå…è®¸æ¨¡å‹è‡ªæˆ‘è°ƒèŠ‚å…¶æ¨ç†è¿‡ç¨‹ï¼Œè§£å†³äº†è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨æ§åˆ¶æœºåˆ¶ã€‚é€šè¿‡æ„å»ºåŸºäºæ ‡å‡†ç­”æ¡ˆçš„è¿‡åº¦æ€è€ƒè¯†åˆ«æŒ‡æ ‡ï¼Œè®¾è®¡å‡ºç³»ç»Ÿçš„æ–¹æ³•æ¥æ£€æµ‹å†—ä½™æ¨ç†ã€‚è¯¥æ–¹æ³•å¯å‡†ç¡®è¯†åˆ«å‡ºæ¨ç†è½¨è¿¹ä¸­ä¸å¿…è¦çš„æ­¥éª¤ï¼Œå¹¶ç”Ÿæˆè®­ç»ƒä¿¡å·æ¥å­¦ä¹ è‡ªæˆ‘è°ƒèŠ‚è¡Œä¸ºã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸€å¥—å®Œæ•´çš„ç­–ç•¥æ„å»ºå…·æœ‰è‡ªé€‚åº”æ¨ç†é•¿åº¦çš„æ•°æ®ï¼Œå¹¶å¼•å…¥äº†åˆ›æ–°çš„åˆ¹è½¦æç¤ºæœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶å­¦ä¹ ä½•æ—¶ç»ˆæ­¢åˆé€‚çš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å¯å°†ä»¤ç‰Œæ¶ˆè€—å‡å°‘60%ï¼ŒåŒæ—¶ä¿æŒä¸æ— çº¦æŸæ¨¡å‹ç›¸å½“çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡ç”Ÿæˆæ›´é•¿çš„æ€ç»´é“¾æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å†—ä½™æ¨ç†çš„å¢åŠ å¯¼è‡´äº†è®¡ç®—å¼€é”€å¢å¤§å’Œè¿‡åº¦æ€è€ƒé—®é¢˜ã€‚</li>
<li>ç°æœ‰è§£å†³è¿‡åº¦æ€è€ƒçš„æ–¹æ³•å¸¸ä¾èµ–å¤–éƒ¨å¹²é¢„ã€‚</li>
<li>æå‡ºäº†Self-Braking Tuningï¼ˆSBTï¼‰æ¡†æ¶ï¼Œå…è®¸æ¨¡å‹è‡ªæˆ‘è°ƒèŠ‚æ¨ç†è¿‡ç¨‹ï¼Œè§£å†³è¿‡åº¦æ€è€ƒã€‚</li>
<li>æ„å»ºåŸºäºæ ‡å‡†ç­”æ¡ˆçš„è¿‡åº¦æ€è€ƒè¯†åˆ«æŒ‡æ ‡æ¥æ£€æµ‹å†—ä½™æ¨ç†ã€‚</li>
<li>å¼•å…¥åˆ›æ–°çš„åˆ¹è½¦æç¤ºæœºåˆ¶ä½¿æ¨¡å‹è‡ªç„¶å­¦ä¹ ä½•æ—¶ç»ˆæ­¢åˆé€‚çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8792102f9e70c1adf1776eb2350fd3b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-242dce1ff04c907e734cc110913fb13c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd4914e69d6cc47ea7a0c87907710469.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e233f3febb94a58037534c8153507db.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Toward-Reliable-Biomedical-Hypothesis-Generation-Evaluating-Truthfulness-and-Hallucination-in-Large-Language-Models"><a href="#Toward-Reliable-Biomedical-Hypothesis-Generation-Evaluating-Truthfulness-and-Hallucination-in-Large-Language-Models" class="headerlink" title="Toward Reliable Biomedical Hypothesis Generation: Evaluating   Truthfulness and Hallucination in Large Language Models"></a>Toward Reliable Biomedical Hypothesis Generation: Evaluating   Truthfulness and Hallucination in Large Language Models</h2><p><strong>Authors:Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang</strong></p>
<p>Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at <a target="_blank" rel="noopener" href="https://github.com/Teddy-XiongGZ/TruthHypo">https://github.com/Teddy-XiongGZ/TruthHypo</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦ç­‰ç§‘å­¦å­¦ç§‘ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡è®¾ç”Ÿæˆæ–¹é¢ã€‚å®ƒä»¬å¯ä»¥åˆ†æå¤§é‡æ–‡çŒ®ï¼Œè¯†åˆ«æ¨¡å¼ï¼Œå¹¶æå‡ºç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œè¯„ä¼°ç”Ÿæˆå‡è®¾çš„çœŸå®æ€§æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œå› ä¸ºéªŒè¯å…¶å‡†ç¡®æ€§é€šå¸¸éœ€è¦å¤§é‡æ—¶é—´å’Œèµ„æºã€‚æ­¤å¤–ï¼ŒLLMä¸­çš„å¹»è§‰é—®é¢˜å¯èƒ½å¯¼è‡´ç”Ÿæˆçœ‹ä¼¼å¯ä¿¡ä½†æœ€ç»ˆé”™è¯¯çš„å‡è®¾ï¼Œç ´åå…¶å¯é æ€§ã€‚ä¸ºäº†ä¿ƒè¿›å¯¹è¿™äº›æŒ‘æˆ˜çš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†TruthHypoåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMç”ŸæˆçœŸå®ç”Ÿç‰©åŒ»å­¦å‡è®¾çš„èƒ½åŠ›ï¼Œä»¥åŠåŸºäºçŸ¥è¯†çš„å¹»è§‰æ£€æµ‹å™¨KnowHDï¼Œä»¥è¯„ä¼°å‡è®¾åœ¨ç°æœ‰çŸ¥è¯†ä¸­çš„æ‰å®ç¨‹åº¦ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨ç”ŸæˆçœŸå®å‡è®¾æ–¹é¢å­˜åœ¨å›°éš¾ã€‚é€šè¿‡åˆ†ææ¨ç†æ­¥éª¤ä¸­çš„å¹»è§‰ï¼Œæˆ‘ä»¬è¯æ˜KnowHDæä¾›çš„æ‰æ ¹å¾—åˆ†æ˜¯ç­›é€‰LLMå¤šæ ·è¾“å‡ºä¸­çš„çœŸå®å‡è®¾çš„æœ‰æ•ˆæŒ‡æ ‡ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†KnowHDåœ¨è¯†åˆ«çœŸå®å‡è®¾å’ŒåŠ é€Ÿç§‘å­¦å‘ç°æ–¹é¢çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œæºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Teddy-XiongGZ/TruthHypo%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Teddy-XiongGZ/TruthHypoæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14599v1">PDF</a> Accepted to IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦ç­‰ç§‘å­¦é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ï¼Œå°¤å…¶åœ¨å‡è®¾ç”Ÿæˆæ–¹é¢ã€‚å®ƒä»¬èƒ½åˆ†æå¤§é‡æ–‡çŒ®ã€è¯†åˆ«æ¨¡å¼å¹¶æå‡ºç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œè¯„ä¼°ç”Ÿæˆå‡è®¾çš„çœŸå®æ€§æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºéªŒè¯å…¶å‡†ç¡®æ€§éœ€è¦å¤§é‡æ—¶é—´å’Œèµ„æºã€‚æ­¤å¤–ï¼ŒLLMä¸­çš„è™šæ„é—®é¢˜å¯èƒ½å¯¼è‡´ç”Ÿæˆçœ‹ä¼¼å¯ä¿¡ä½†å®é™…é”™è¯¯çš„å‡è®¾ï¼Œå½±å“å¯é æ€§ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TruthHypoè¯„ä¼°åŸºå‡†å’ŒKnowHDçŸ¥è¯†è™šæ„æ£€æµ‹å™¨ã€‚ä½†LLMåœ¨ç”ŸæˆçœŸå®å‡è®¾æ–¹é¢ä»æœ‰å›°éš¾ã€‚é€šè¿‡åˆ†ææ¨ç†æ­¥éª¤ä¸­çš„è™šæ„ï¼Œæˆ‘ä»¬å‘ç°KnowHDæä¾›çš„æ¥åœ°åº¦åˆ†æ•°èƒ½æœ‰æ•ˆè¿‡æ»¤LLMè¾“å‡ºçš„çœŸå®å‡è®¾ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†KnowHDåœ¨è¯†åˆ«çœŸå®å‡è®¾å’ŒåŠ é€Ÿç§‘å­¦å‘ç°æ–¹é¢çš„å®ç”¨æ€§ã€‚æ•°æ®å’Œæºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Teddy-XiongGZ/TruthHypo">https://github.com/Teddy-XiongGZ/TruthHypo</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç”Ÿç‰©åŒ»å­¦ç­‰ç§‘å­¦é¢†åŸŸæœ‰æ½œåŠ›ï¼Œå°¤å…¶åœ¨å‡è®¾ç”Ÿæˆæ–¹é¢èƒ½åˆ†ææ–‡çŒ®ã€è¯†åˆ«æ¨¡å¼ã€‚</li>
<li>è¯„ä¼°LLMç”Ÿæˆçš„å‡è®¾çœŸå®æ€§æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œéœ€è¦éªŒè¯å…¶å‡†ç¡®æ€§å¹¶è¯†åˆ«è™šæ„å‡è®¾ã€‚</li>
<li>TruthHypoè¯„ä¼°åŸºå‡†ç”¨äºè¯„ä¼°LLMåœ¨ç”ŸæˆçœŸå®å‡è®¾æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>KnowHDçŸ¥è¯†è™šæ„æ£€æµ‹å™¨èƒ½æœ‰æ•ˆè¿‡æ»¤LLMè¾“å‡ºçš„çœŸå®å‡è®¾ã€‚</li>
<li>LLMåœ¨ç”ŸæˆçœŸå®å‡è®¾æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>KnowHDçš„æ¥åœ°åº¦åˆ†æ•°æ˜¯è¯„ä¼°å‡è®¾çœŸå®æ€§çš„æœ‰æ•ˆæŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aec48770c27ca7547b10c7e7991a1e1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae05957c89c5590d36ceece7aa5b7682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f15fa32d20cb21e2a8264542f83950f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b8fe7f67fe13495701fe32774d9072f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2fc53d303e1ce74985e91947c6d401b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b558bf9db0cc5b6b2dde50d17c56f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-088ddbb5ebc5a8832ec00c3a6c1d28a1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Context-Reasoner-Incentivizing-Reasoning-Capability-for-Contextualized-Privacy-and-Safety-Compliance-via-Reinforcement-Learning"><a href="#Context-Reasoner-Incentivizing-Reasoning-Capability-for-Contextualized-Privacy-and-Safety-Compliance-via-Reinforcement-Learning" class="headerlink" title="Context Reasoner: Incentivizing Reasoning Capability for Contextualized   Privacy and Safety Compliance via Reinforcement Learning"></a>Context Reasoner: Incentivizing Reasoning Capability for Contextualized   Privacy and Safety Compliance via Reinforcement Learning</h2><p><strong>Authors:Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song</strong></p>
<p>While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +17.64% accuracy improvement in safety&#x2F;privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ˜¾è‘—çš„å®‰å…¨å’Œéšç§é£é™©ã€‚å½“å‰çš„ç¼“è§£ç­–ç•¥å¾€å¾€æ— æ³•åœ¨é£é™©åœºæ™¯ä¸­ä¿ç•™ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œè€Œè¿‡åº¦ä¾èµ–äºæ•æ„Ÿæ¨¡å¼åŒ¹é…æ¥ä¿æŠ¤LLMï¼Œè¿™é™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¿½ç•¥äº†æ—¢å®šçš„å®‰å…¨å’Œéšç§æ ‡å‡†ï¼Œå¯¼è‡´æ³•å¾‹åˆè§„çš„ç³»ç»Ÿæ€§é£é™©ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬å°†å®‰å…¨å’Œéšç§é—®é¢˜è½¬åŒ–ä¸ºä¸Šä¸‹æ–‡åˆè§„é—®é¢˜ï¼Œéµå¾ªä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼ˆCIï¼‰ç†è®ºã€‚åœ¨CIæ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä¸‰ä¸ªå…³é”®çš„ç›‘ç®¡æ ‡å‡†ä¿æŒä¸€è‡´ï¼šGDPRã€æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•å’ŒHIPAAã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æ¿€åŠ±ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜å¯¹å®‰å…¨å’Œéšç§è§„èŒƒçš„å¯åˆè§„æ€§ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜äº†æ³•å¾‹åˆè§„æ€§ï¼ˆåœ¨å®‰å…¨&#x2F;éšç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†+17.64%çš„å‡†ç¡®ç‡æå‡ï¼‰ï¼Œè€Œä¸”è¿˜è¿›ä¸€æ­¥æé«˜äº†é€šç”¨æ¨ç†èƒ½åŠ›ã€‚å¯¹äºåœ¨å¤šç§ä¸»é¢˜ä¸Šæ˜¾è‘—ä¼˜äºå…¶åŸºç¡€æ¨¡å‹Qwen2.5-7B-Instructçš„å¼ºå¤§æ¨ç†æ¨¡å‹OpenThinker-7Bï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MMLUå’ŒLegalBenchåŸºå‡†æµ‹è¯•ä¸Šåˆ†åˆ«æé«˜äº†+2.05%å’Œ+8.98%çš„å‡†ç¡®ç‡ï¼Œå¢å¼ºäº†å…¶é€šç”¨æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14585v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºä»¤äººç©ç›®çš„èƒ½åŠ›ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥å®‰å…¨å’Œéšç§é£é™©ã€‚å½“å‰ç¼“è§£ç­–ç•¥å¸¸å¸¸æ— æ³•ä¿æŠ¤ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œå¹¶è¿‡åº¦ä¾èµ–æ•æ„Ÿæ¨¡å¼åŒ¹é…æ¥ä¿æŠ¤LLMï¼Œè¿™é™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚éµå¾ªè¯­å¢ƒå®Œæ•´æ€§ï¼ˆCIï¼‰ç†è®ºï¼Œå°†å®‰å…¨å’Œéšç§é—®é¢˜è½¬åŒ–ä¸ºä¸Šä¸‹æ–‡åŒ–çš„åˆè§„æ€§é—®é¢˜ï¼Œä¸ä¸‰å¤§å…³é”®ç›‘ç®¡æ ‡å‡†ï¼ˆGDPRã€æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆå’ŒHIPAAï¼‰ç›¸ç¬¦ã€‚é‡‡ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼Œæ¿€åŠ±ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜éµå®ˆå®‰å…¨å’Œéšç§è§„èŒƒçš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜æ³•å¾‹åˆè§„æ€§ï¼Œè¿˜è¿›ä¸€æ­¥æ”¹å–„äº†ä¸€èˆ¬æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨å®‰å…¨å’Œéšç§é£é™©ã€‚</li>
<li>å½“å‰ç¼“è§£ç­–ç•¥å¸¸å¸¸æ— æ³•åŒæ—¶ä¿æŠ¤ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›å’Œåº”å¯¹é£é™©åœºæ™¯ã€‚</li>
<li>å®‰å…¨å’Œéšç§é—®é¢˜åº”è½¬åŒ–ä¸ºä¸Šä¸‹æ–‡åŒ–çš„åˆè§„æ€§é—®é¢˜ã€‚</li>
<li>éµå¾ªè¯­å¢ƒå®Œæ•´æ€§ï¼ˆCIï¼‰ç†è®ºï¼Œä¸ä¸‰å¤§å…³é”®ç›‘ç®¡æ ‡å‡†ç›¸ç¬¦ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¿€åŠ±ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›çš„åŒæ—¶æé«˜éµå®ˆå®‰å…¨å’Œéšç§è§„èŒƒçš„èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•æ˜¾è‘—æé«˜æ³•å¾‹åˆè§„æ€§ï¼Œå¹¶åœ¨å®‰å…¨&#x2F;éšç§åŸºå‡†æµ‹è¯•ä¸­å®ç°+17.64%çš„å‡†ç¡®ç‡æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4db7d96cf7362e201aadebc4ea69aaf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-353b8163a4c131bd156ad9a92bc0f459.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc3442a46b8dfafb225bfa8424ac7fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-335803ebc1268674293e56a126aa7c7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0c72ab2a84733a72d5d2ce64924715e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea5998feb82fe7d8cc676123848dc041.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Latent-Flow-Transformer"><a href="#Latent-Flow-Transformer" class="headerlink" title="Latent Flow Transformer"></a>Latent Flow Transformer</h2><p><strong>Authors:Yen-Chen Wu, Feng-Ting Liao, Meng-Hsi Chen, Pei-Chen Ho, Farhang Nabiei, Da-shan Shiu</strong></p>
<p>Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in \textit{preserving coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms. </p>
<blockquote>
<p>Transformeræ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å‡†å®ç°æ–¹å¼ï¼Œé€šå¸¸ç”±æ•°åè‡³æ•°ç™¾ä¸ªç‹¬ç«‹çš„å±‚ç»„æˆã€‚è™½ç„¶æ›´å¤šçš„å±‚å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†è¿™ç§åšæ³•åœ¨æ•ˆç‡ä¸Šå—åˆ°æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è€ƒè™‘åˆ°æ‰©æ•£æ¨¡å‹å’ŒåŸºäºæµçš„æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„è¿ç»­å±‚è¡¨ç°å‡ºçš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†æ½œæµTransformerï¼ˆLFTï¼‰ï¼Œå®ƒé€šè¿‡ç”¨ä¸€ä¸ªå•ä¸€çš„å­¦ä¹ ä¼ è¾“ç®—å­æ›¿æ¢ä¸€ç³»åˆ—å±‚ï¼Œè¯¥ç®—å­é€šè¿‡æµåŒ¹é…è¿›è¡Œè®­ç»ƒï¼Œå¯åœ¨ä¿æŒä¸åŸå§‹æ¶æ„å…¼å®¹çš„åŒæ—¶å®ç°æ˜¾è‘—å‹ç¼©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥æµæ­¥è¡Œï¼ˆFWï¼‰ç®—æ³•ï¼Œè§£å†³äº†ç°æœ‰åŸºäºæµçš„æ–¹æ³•åœ¨ä¿æŒè€¦åˆæ–¹é¢çš„å±€é™æ€§ã€‚åœ¨Pythia-410Mæ¨¡å‹ä¸Šï¼Œé€šè¿‡æµåŒ¹é…è®­ç»ƒçš„LFTå‹ç¼©äº†24å±‚ä¸­çš„6å±‚ï¼Œå¹¶ä¸”è¡¨ç°ä¼˜äºç›´æ¥è·³è¿‡ä¸¤å±‚ï¼ˆLM logitsçš„KLæ•£åº¦ä»0.529é™è‡³0.407ï¼‰ï¼Œè¯æ˜äº†è¿™ç§è®¾è®¡çš„å¯è¡Œæ€§ã€‚åœ¨ä½¿ç”¨FWè¿›è¡Œè®­ç»ƒæ—¶ï¼ŒLFTè¿›ä¸€æ­¥å°†12å±‚è’¸é¦ä¸ºä¸€å±‚ï¼ŒåŒæ—¶å°†KLå€¼é™è‡³0.736ï¼Œè¶…è¿‡äº†è·³è¿‡ä¸‰å±‚çš„ç»“æœï¼ˆå³KLæ•£åº¦å¤§äºæˆ–é«˜äºæ— æµå¼è·³å±‚ï¼‰ï¼ˆçº¦æ¯”è·³ä¸‰å±‚å°ï¼‰ï¼Œæå¤§åœ°ç¼©å°äº†è‡ªå›å½’ç”Ÿæˆæ¨¡å¼å’ŒåŸºäºæµçš„ç”Ÿæˆæ¨¡å¼ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14513v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ‡å‡†å®ç°é€šå¸¸ä¸ºæ•°åè‡³æ•°ç™¾ä¸ªç¦»æ•£å±‚ã€‚è™½ç„¶æ›´å¤šå±‚èƒ½æé«˜æ€§èƒ½ï¼Œä½†æ•ˆç‡è¾ƒä½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºæ½œæµè½¬æ¢å™¨ï¼ˆLFTï¼‰ï¼Œé€šè¿‡æµåŒ¹é…è®­ç»ƒå­¦ä¹ ä¼ è¾“ç®—å­ï¼Œå°†ä¸€ç»„å±‚æ›¿æ¢ä¸ºå•ä¸ªå±‚ï¼Œæ—¢ä¿æŒåŸæ¶æ„å…¼å®¹æ€§åˆå®ç°æ˜¾è‘—å‹ç¼©ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³ç°æœ‰æµæ¨¡å‹çš„è€¦åˆä¿å­˜é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥æµæ­¥è¡Œï¼ˆFWï¼‰ç®—æ³•ã€‚åœ¨Pythia-410Mæ¨¡å‹ä¸Šï¼ŒLFTé€šè¿‡æµåŒ¹é…å‹ç¼©6å±‚å¹¶ä¿æŒé«˜æ€§èƒ½ï¼Œå±•ç¤ºè¯¥è®¾è®¡å¯è¡Œæ€§ã€‚ä½¿ç”¨FWè®­ç»ƒçš„LFTèƒ½è¿›ä¸€æ­¥å°†12å±‚è’¸é¦ä¸º1å±‚å¹¶ç¼©å°ä¸è‡ªå›å½’å’Œæµç”ŸæˆèŒƒå¼ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸åŒ…å«æ•°åè‡³æ•°ç™¾ä¸ªç¦»æ•£å±‚ï¼Œæ›´å¤šå±‚è™½èƒ½æé«˜æ€§èƒ½ä½†æ•ˆç‡è¾ƒä½ã€‚</li>
<li>æ½œæµè½¬æ¢å™¨ï¼ˆLFTï¼‰é€šè¿‡è®­ç»ƒå­¦ä¹ ä¼ è¾“ç®—å­ï¼Œå°†ä¸€ç»„å±‚æ›¿æ¢ä¸ºå•ä¸ªå±‚ï¼Œä»¥å®ç°æ˜¾è‘—å‹ç¼©å¹¶ä¿æŒåŸæ¶æ„å…¼å®¹æ€§ã€‚</li>
<li>LFTé‡‡ç”¨æµåŒ¹é…è¿›è¡Œè®­ç»ƒï¼Œåœ¨Pythia-410Mæ¨¡å‹ä¸Šå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>å¼•å…¥æµæ­¥è¡Œï¼ˆFWï¼‰ç®—æ³•ä»¥è§£å†³ç°æœ‰æµæ¨¡å‹çš„è€¦åˆä¿å­˜é™åˆ¶ã€‚</li>
<li>LFTåœ¨å‹ç¼©å±‚æ•°çš„åŒæ—¶ä¿æŒäº†é«˜æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶è®¾è®¡çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ä½¿ç”¨FWè®­ç»ƒçš„LFTèƒ½å¤Ÿè¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹å¹¶ç¼©å°ä¸è‡ªå›å½’å’Œæµç”ŸæˆèŒƒå¼ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3710d29c60de34c45284a7179eec1e81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-423048a320bfbac67ebaf39c96e5e165.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09c31f17eb7b03cee4329d2ba484cd82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48b3db511db2f73a7d19fbece13796df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a80a1e1bee17590aa6f07c4bb70549b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d8e9699c72aedfea3516e75dd3a28a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4f78ab74b471efa1e9d04fbd779f42d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Speculative-Decoding-Reimagined-for-Multimodal-Large-Language-Models"><a href="#Speculative-Decoding-Reimagined-for-Multimodal-Large-Language-Models" class="headerlink" title="Speculative Decoding Reimagined for Multimodal Large Language Models"></a>Speculative Decoding Reimagined for Multimodal Large Language Models</h2><p><strong>Authors:Luxi Lin, Zhihang Lin, Zhanpeng Zeng, Rongrong Ji</strong></p>
<p>This paper introduces Multimodal Speculative Decoding (MSD) to accelerate Multimodal Large Language Models (MLLMs) inference. Speculative decoding has been shown to accelerate Large Language Models (LLMs) without sacrificing accuracy. However, current speculative decoding methods for MLLMs fail to achieve the same speedup as they do for LLMs. To address this, we reimagine speculative decoding specifically for MLLMs. Our analysis of MLLM characteristics reveals two key design principles for MSD: (1) Text and visual tokens have fundamentally different characteristics and need to be processed separately during drafting. (2) Both language modeling ability and visual perception capability are crucial for the draft model. For the first principle, MSD decouples text and visual tokens in the draft model, allowing each to be handled based on its own characteristics. For the second principle, MSD uses a two-stage training strategy: In stage one, the draft model is trained on text-only instruction-tuning datasets to improve its language modeling ability. In stage two, MSD gradually introduces multimodal data to enhance the visual perception capability of the draft model. Experiments show that MSD boosts inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$ for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Lyn-Lucy/MSD">https://github.com/Lyn-Lucy/MSD</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æ¨æµ‹è§£ç ï¼ˆMSDï¼‰ï¼Œä»¥åŠ é€Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†ã€‚æ¨æµ‹è§£ç å·²è¯æ˜å¯ä»¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†ï¼Œè€Œä¸ä¼šç‰ºç‰²å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå½“å‰é’ˆå¯¹MLLMsçš„æ¨æµ‹è§£ç æ–¹æ³•æ— æ³•è¾¾åˆ°ä¸LLMsç›¸åŒçš„åŠ é€Ÿæ•ˆæœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é’ˆå¯¹MLLMsé‡æ–°æ„æƒ³äº†ä¸€ç§æ¨æµ‹è§£ç æ–¹æ³•ã€‚æˆ‘ä»¬å¯¹MLLMç‰¹æ€§çš„åˆ†ææ­ç¤ºäº†MSDçš„ä¸¤ä¸ªå…³é”®è®¾è®¡åŸåˆ™ï¼šï¼ˆ1ï¼‰æ–‡æœ¬å’Œè§†è§‰æ ‡è®°å…·æœ‰ä¸åŒçš„åŸºæœ¬ç‰¹æ€§ï¼Œéœ€è¦åœ¨è‰ç¨¿é˜¶æ®µåˆ†åˆ«å¤„ç†ã€‚ï¼ˆ2ï¼‰è¯­è¨€å»ºæ¨¡èƒ½åŠ›å’Œè§†è§‰æ„ŸçŸ¥èƒ½åŠ›å¯¹äºè‰ç¨¿æ¨¡å‹éƒ½è‡³å…³é‡è¦ã€‚æ ¹æ®ç¬¬ä¸€ä¸ªåŸåˆ™ï¼ŒMSDåœ¨è‰ç¨¿æ¨¡å‹ä¸­è§£è€¦æ–‡æœ¬å’Œè§†è§‰æ ‡è®°ï¼Œå…è®¸æ ¹æ®å®ƒä»¬å„è‡ªçš„ç‰¹æ€§è¿›è¡Œå¤„ç†ã€‚å¯¹äºç¬¬äºŒä¸ªåŸåˆ™ï¼ŒMSDé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè‰ç¨¿æ¨¡å‹åœ¨çº¯æ–‡æœ¬æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜å…¶è¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒMSDé€æ¸å¼•å…¥å¤šæ¨¡æ€æ•°æ®ï¼Œä»¥æé«˜è‰ç¨¿æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒMSDåœ¨LLaVA-1.5-7Bå’ŒLLaVA-1.5-13Bçš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¨ç†é€Ÿåº¦åˆ†åˆ«æé«˜äº†2.29å€å’Œ2.46å€ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lyn-Lucy/MSD">https://github.com/Lyn-Lucy/MSD</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14260v1">PDF</a> 12 pages</p>
<p><strong>æ‘˜è¦</strong><br>  æœ¬æ–‡æå‡ºäº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åŠ é€Ÿæ¨ç†æ–¹æ³•â€”â€”å¤šæ¨¡æ€æ¨æµ‹è§£ç ï¼ˆMSDï¼‰ã€‚é€šè¿‡å¯¹MLLMç‰¹æ€§çš„åˆ†æï¼Œæ­ç¤ºäº†MSDçš„ä¸¤ä¸ªå…³é”®è®¾è®¡åŸåˆ™ã€‚ä¸€æ˜¯æ–‡æœ¬å’Œè§†è§‰ç¬¦å·åœ¨èµ·è‰è¿‡ç¨‹ä¸­éœ€è¦åˆ†åˆ«å¤„ç†ï¼ŒäºŒæ˜¯è¯­è¨€å»ºæ¨¡èƒ½åŠ›å’Œè§†è§‰æ„ŸçŸ¥èƒ½åŠ›å¯¹äºè‰æ¡ˆæ¨¡å‹è‡³å…³é‡è¦ã€‚MSDé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µä»…å¯¹æ–‡æœ¬æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒä»¥æé«˜è¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µé€æ­¥å¼•å…¥å¤šæ¨¡æ€æ•°æ®ä»¥å¢å¼ºæ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒMSDåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œæœ€é«˜å¯è¾¾LLaVA-1.5-7Bçš„2.29å€å’ŒLLaVA-1.5-13Bçš„2.46å€ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬æ–‡å¼•å…¥äº†å¤šæ¨¡æ€æ¨æµ‹è§£ç ï¼ˆMSDï¼‰æ¥åŠ é€Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>MSDé’ˆå¯¹MLLMsçš„ç‰¹æ€§è¿›è¡Œäº†é‡æ–°è®¾è®¡ï¼Œæ­ç¤ºäº†å¤„ç†æ–‡æœ¬å’Œè§†è§‰ç¬¦å·éœ€è¦åˆ†åˆ«è¿›è¡Œçš„é‡è¦æ€§ã€‚</li>
<li>MSDå¼ºè°ƒè¯­è¨€å»ºæ¨¡èƒ½åŠ›å’Œè§†è§‰æ„ŸçŸ¥èƒ½åŠ›å¯¹è‰æ¡ˆæ¨¡å‹çš„é‡è¦æ€§ã€‚</li>
<li>MSDé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆä¸“æ³¨äºæé«˜è¯­è¨€å»ºæ¨¡èƒ½åŠ›ï¼Œç„¶åé€æ­¥å¼•å…¥å¤šæ¨¡æ€æ•°æ®ä»¥å¢å¼ºè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMSDèƒ½æœ‰æ•ˆæé«˜æ¨ç†é€Ÿåº¦ï¼Œæœ€é«˜å¯è¾¾LLaVA-1.5-7Bçš„2.29å€å’ŒLLaVA-1.5-13Bçš„2.46å€ã€‚</li>
<li>MSDçš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨æŒ‡å®šGitHubä»“åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b47b4556345b7f3c9d811bd5297b8c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ee978cbbceb0a82cf15687af0dfb9e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7ed4853969e6f94edf0f12724a89e6a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04707bbc3bb6be2fa05a12c49448d0f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98b8c172101e1d9ea8677f29ab14eaaa.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MSDformer-Multi-scale-Discrete-Transformer-For-Time-Series-Generation"><a href="#MSDformer-Multi-scale-Discrete-Transformer-For-Time-Series-Generation" class="headerlink" title="MSDformer: Multi-scale Discrete Transformer For Time Series Generation"></a>MSDformer: Multi-scale Discrete Transformer For Time Series Generation</h2><p><strong>Authors:Zhicheng Chen, Shibo Feng, Xi Xiao, Zhong Zhang, Qing Li, Xingyu Gao, Peilin Zhao</strong></p>
<p>Discrete Token Modeling (DTM), which employs vector quantization techniques, has demonstrated remarkable success in modeling non-natural language modalities, particularly in time series generation. While our prior work SDformer established the first DTM-based framework to achieve state-of-the-art performance in this domain, two critical limitations persist in existing DTM approaches: 1) their inability to capture multi-scale temporal patterns inherent to complex time series data, and 2) the absence of theoretical foundations to guide model optimization. To address these challenges, we proposes a novel multi-scale DTM-based time series generation method, called Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale time series tokenizer to learn discrete token representations at multiple scales, which jointly characterize the complex nature of time series data. Subsequently, MSDformer applies a multi-scale autoregressive token modeling technique to capture the multi-scale patterns of time series within the discrete latent space. Theoretically, we validate the effectiveness of the DTM method and the rationality of MSDformer through the rate-distortion theorem. Comprehensive experiments demonstrate that MSDformer significantly outperforms state-of-the-art methods. Both theoretical analysis and experimental results demonstrate that incorporating multi-scale information and modeling multi-scale patterns can substantially enhance the quality of generated time series in DTM-based approaches. The code will be released upon acceptance. </p>
<blockquote>
<p>ç¦»æ•£ä»¤ç‰Œå»ºæ¨¡ï¼ˆDTMï¼‰é‡‡ç”¨å‘é‡é‡åŒ–æŠ€æœ¯ï¼Œåœ¨éè‡ªç„¶è¯­è¨€æ¨¡æ€å»ºæ¨¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¶é—´åºåˆ—ç”Ÿæˆæ–¹é¢ã€‚è™½ç„¶æˆ‘ä»¬çš„å‰æœŸå·¥ä½œSDformerå»ºç«‹äº†åŸºäºDTMçš„æ¡†æ¶ï¼Œåœ¨è¯¥é¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†ç°æœ‰DTMæ–¹æ³•ä»å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼š1ï¼‰æ— æ³•æ•æ‰å¤æ‚æ—¶é—´åºåˆ—æ•°æ®å›ºæœ‰çš„å¤šå°ºåº¦æ—¶é—´æ¨¡å¼ï¼›2ï¼‰ç¼ºä¹æŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–çš„ç†è®ºåŸºç¡€ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå¤šå°ºåº¦DTMçš„æ—¶é—´åºåˆ—ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºMulti-Scale Discrete Transformerï¼ˆMSDformerï¼‰ã€‚MSDformeré‡‡ç”¨å¤šå°ºåº¦æ—¶é—´åºåˆ—åˆ†è¯å™¨ï¼Œåœ¨å¤šä¸ªå°ºåº¦ä¸Šå­¦ä¹ ç¦»æ•£ä»¤ç‰Œè¡¨ç¤ºï¼Œå…±åŒè¡¨å¾æ—¶é—´åºåˆ—æ•°æ®çš„å¤æ‚æ€§è´¨ã€‚ç„¶åï¼ŒMSDformeré‡‡ç”¨å¤šå°ºåº¦è‡ªå›å½’ä»¤ç‰Œå»ºæ¨¡æŠ€æœ¯ï¼Œåœ¨ç¦»æ•£æ½œåœ¨ç©ºé—´å†…æ•æ‰æ—¶é—´åºåˆ—çš„å¤šå°ºåº¦æ¨¡å¼ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬é€šè¿‡é€Ÿç‡å¤±çœŸå®šç†éªŒè¯äº†DTMæ–¹æ³•çš„æœ‰æ•ˆæ€§ä»¥åŠMSDformerçš„åˆç†æ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMSDformeræ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç†è®ºåˆ†æå’Œå®éªŒç»“æœå‡è¡¨æ˜ï¼Œèå…¥å¤šå°ºåº¦ä¿¡æ¯å’Œå»ºæ¨¡å¤šå°ºåº¦æ¨¡å¼å¯ä»¥å¤§å¤§æé«˜DTMæ–¹æ³•ä¸­ç”Ÿæˆæ—¶é—´åºåˆ—çš„è´¨é‡ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14202v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå‘é‡é‡åŒ–çš„ç¦»æ•£ä»¤ç‰Œå»ºæ¨¡ï¼ˆDTMï¼‰åœ¨éè‡ªç„¶è¯­è¨€æ¨¡æ€å»ºæ¨¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¶é—´åºåˆ—ç”Ÿæˆé¢†åŸŸã€‚é’ˆå¯¹ç°æœ‰DTMæ–¹æ³•å­˜åœ¨çš„å¤šå°ºåº¦æ—¶é—´æ¨¡å¼æ•æ‰èƒ½åŠ›ä¸è¶³å’Œç†è®ºæ¡†æ¶ç¼ºå¤±çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤šå°ºåº¦DTMæ—¶é—´åºåˆ—ç”Ÿæˆæ–¹æ³•â€”â€”Multi-Scale Discrete Transformerï¼ˆMSDformerï¼‰ã€‚MSDformeré€šè¿‡å¤šå°ºåº¦æ—¶é—´åºåˆ—åˆ†è¯å™¨å­¦ä¹ ç¦»æ•£ä»¤ç‰Œè¡¨ç¤ºï¼Œå¹¶åº”ç”¨å¤šå°ºåº¦è‡ªå›å½’ä»¤ç‰Œå»ºæ¨¡æŠ€æœ¯æ•æ‰æ—¶é—´åºåˆ—çš„å¤šå°ºåº¦æ¨¡å¼ã€‚ç†è®ºåˆ†æå’Œå®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥å¤šå°ºåº¦ä¿¡æ¯å’Œå»ºæ¨¡å¤šå°ºåº¦æ¨¡å¼å¯ä»¥æ˜¾è‘—æé«˜DTMæ–¹æ³•ç”Ÿæˆæ—¶é—´åºåˆ—çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DTMæŠ€æœ¯åœ¨éè‡ªç„¶è¯­è¨€æ¨¡æ€å»ºæ¨¡ï¼Œç‰¹åˆ«æ˜¯æ—¶é—´åºåˆ—ç”Ÿæˆä¸­è¡¨ç°å‡ºæ˜¾è‘—æˆåŠŸã€‚</li>
<li>ç°æœ‰DTMæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼šæ— æ³•æ•æ‰å¤æ‚æ—¶é—´åºåˆ—æ•°æ®çš„å†…åœ¨å¤šå°ºåº¦æ—¶é—´æ¨¡å¼ï¼Œä»¥åŠç¼ºä¹æŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–çš„ç†è®ºæ¡†æ¶ã€‚</li>
<li>MSDformerè¢«æå‡ºä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå®ƒé€šè¿‡å¤šå°ºåº¦æ—¶é—´åºåˆ—åˆ†è¯å™¨å­¦ä¹ ç¦»æ•£ä»¤ç‰Œè¡¨ç¤ºï¼Œå¹¶åº”ç”¨å¤šå°ºåº¦è‡ªå›å½’ä»¤ç‰Œå»ºæ¨¡æŠ€æœ¯ã€‚</li>
<li>MSDformeråœ¨ç†è®ºä¸Šé€šè¿‡é€Ÿç‡-å¤±çœŸå®šç†éªŒè¯äº†DTMæ–¹æ³•çš„æœ‰æ•ˆæ€§ä»¥åŠå…¶è‡ªèº«çš„åˆç†æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMSDformeræ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¡¨æ˜å¼•å…¥å¤šå°ºåº¦ä¿¡æ¯å’Œå»ºæ¨¡å¤šå°ºåº¦æ¨¡å¼å¯ä»¥å¤§å¹…æé«˜DTMæ–¹æ³•ç”Ÿæˆæ—¶é—´åºåˆ—çš„è´¨é‡ã€‚</li>
<li>ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ca0365817c3e3e0fdce16f9e220dc4ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdadde5eb0fe1aa25ae97909b4d50547.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-561ab683a7531e420b58a1d7ff504915.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe97e7b764583c303af9e0ad39ee4e3f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70326c7591212f1a08bbbbe1e5097b67.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  ContextAgent Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c3d46f571e781e50ca048e545b9a06f2.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Emerging Properties in Unified Multimodal Pretraining
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
