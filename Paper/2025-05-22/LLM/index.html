<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-05-22  UniGen Enhanced Training &amp; Test-Time Strategies for Unified Multimodal   Understanding and Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e0ce1a4a79fec8d00af82d3b31392152.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-22-更新"><a href="#2025-05-22-更新" class="headerlink" title="2025-05-22 更新"></a>2025-05-22 更新</h1><h2 id="UniGen-Enhanced-Training-Test-Time-Strategies-for-Unified-Multimodal-Understanding-and-Generation"><a href="#UniGen-Enhanced-Training-Test-Time-Strategies-for-Unified-Multimodal-Understanding-and-Generation" class="headerlink" title="UniGen: Enhanced Training &amp; Test-Time Strategies for Unified Multimodal   Understanding and Generation"></a>UniGen: Enhanced Training &amp; Test-Time Strategies for Unified Multimodal   Understanding and Generation</h2><p><strong>Authors:Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, Afshin Dehghan</strong></p>
<p>We introduce UniGen, a unified multimodal large language model (MLLM) capable of image understanding and generation. We study the full training pipeline of UniGen from a data-centric perspective, including multi-stage pre-training, supervised fine-tuning, and direct preference optimization. More importantly, we propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time scaling, which significantly boosts UniGen’s image generation quality using a simple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act as both image generator and verifier at test time, assessing the semantic alignment between a text prompt and its generated image in a step-by-step CoT manner. Trained entirely on open-source datasets across all stages, UniGen achieves state-of-the-art performance on a range of image understanding and generation benchmarks, with a final score of 0.78 on GenEval and 85.19 on DPG-Bench. Through extensive ablation studies, our work provides actionable insights and addresses key challenges in the full life cycle of building unified MLLMs, contributing meaningful directions to the future research. </p>
<blockquote>
<p>我们介绍了UniGen，这是一个统一的多模态大型语言模型（MLLM），能够进行图像理解和生成。我们从数据中心的视角研究了UniGen的完整训练流程，包括多阶段预训练、监督微调以及直接偏好优化。更重要的是，我们提出了一种新的链式思维验证（CoT-V）策略，用于测试时的扩展，该策略使用简单的Best-of-N测试时间策略，显著提高了UniGen的图像生成质量。具体来说，CoT-V使UniGen能够在测试时同时充当图像生成器和验证器，以逐步的链式思维方式评估文本提示与其生成的图像之间的语义对齐。完全在所有阶段使用开源数据集进行训练，UniGen在图像理解和生成的一系列基准测试中达到了最新技术水平，GenEval的最终得分为0.78，DPG-Bench上的得分为85.19。通过广泛的消融研究，我们的工作提供了可操作的见解，解决了构建统一MLLM整个生命周期中的关键挑战，为未来研究提供了有意义的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14682v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>UniGen是一个统一的多模态大型语言模型（MLLM），具有图像理解和生成能力。该研究从数据中心视角研究了UniGen的全训练管道，包括多阶段预训练、监督微调以及直接偏好优化。更重要的是，研究提出了全新的思维链验证（CoT-V）策略，用于测试时的缩放，通过简单的Best-of-N测试时间策略显著提升UniGen的图像生成质量。CoT-V使UniGen能够在测试时同时充当图像生成器和验证器，以思维链的方式逐步评估文本提示与生成图像之间的语义对齐。完全在开源数据集上训练的UniGen，在图像理解和生成基准测试上达到了最新技术水平，GenEval得分为0.78，DPG-Bench得分为85.19。通过广泛的消融研究，该研究为构建统一MLLM的整个生命周期提供了可行的见解和解决了关键挑战，为未来研究提供了有意义的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniGen是一个多模态大型语言模型，具备图像理解和生成能力。</li>
<li>研究了UniGen的全训练管道，包括预训练、监督微调以及偏好优化。</li>
<li>提出了思维链验证（CoT-V）策略，提高图像生成质量。</li>
<li>UniGen具备在测试时同时作为图像生成器和验证器的功能。</li>
<li>UniGen在图像理解和生成基准测试上表现优秀，GenEval得分为0.78，DPG-Bench得分为85.19。</li>
<li>消融研究为构建MLLM提供了关键挑战和可行的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14682">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6e0552fadb298e283deb8b7c53746951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69b8f7425aa72507b37aa89e30c31aa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecd5ccd266ed1ae72381d5ddc44f58dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd13a565215a454ed30381d36413caec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d69dacfe1d52bd63a3e8f9d3c1f58203.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UltraEdit-Training-Subject-and-Memory-Free-Lifelong-Editing-in-Large-Language-Models"><a href="#UltraEdit-Training-Subject-and-Memory-Free-Lifelong-Editing-in-Large-Language-Models" class="headerlink" title="UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in   Large Language Models"></a>UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in   Large Language Models</h2><p><strong>Authors:Xiaojie Gu, Guangxu Chen, Jungang Li, Jia-Chen Gu, Xuming Hu, Kai Zhang</strong></p>
<p>Lifelong learning enables large language models (LLMs) to adapt to evolving information by continually updating their internal knowledge. An ideal system should support efficient, wide-ranging updates while preserving existing capabilities and ensuring reliable deployment. Model editing stands out as a promising solution for this goal, offering a focused and efficient way to revise a model’s internal knowledge. Although recent paradigms have made notable progress, they often struggle to meet the demands of practical lifelong adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally new editing solution that is training-, subject- and memory-free, making it particularly well-suited for ultra-scalable, real-world lifelong model editing. ULTRAEDIT performs editing through a self-contained process that relies solely on lightweight linear algebra operations to compute parameter shifts, enabling fast and consistent parameter modifications with minimal overhead. To improve scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization strategy that continuously updates feature statistics across turns, allowing it to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT achieves editing speeds over 7x faster than the previous state-of-the-art method-which was also the fastest known approach-while consuming less than 1&#x2F;3 the VRAM, making it the only method currently capable of editing a 7B LLM on a 24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest dataset in the field to date, with over 2M editing pairs-and demonstrate that our method supports up to 1M edits while maintaining high accuracy. Comprehensive experiments on four datasets and six models show that ULTRAEDIT consistently achieves superior performance across diverse model editing scenarios. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/XiaojieGu/UltraEdit">https://github.com/XiaojieGu/UltraEdit</a>. </p>
<blockquote>
<p>终身学习使大型语言模型（LLM）能够通过持续更新其内部知识来适应不断变化的信息。一个理想的系统应该支持高效、广泛的更新，同时保留现有功能并确保可靠部署。模型编辑作为实现此目标的颇具前景的解决方案脱颖而出，它提供了一种专注且高效的方法来修订模型的内部知识。尽管最近的范式已经取得了显著的进步，但它们通常难以满足大规模实践中的终身学习需求。为了弥补这一差距，我们提出了ULTRAEDIT——一种全新的编辑解决方案，它无需训练、主题和内存，特别适用于超大规模、现实世界的终身模型编辑。ULTRAEDIT通过自我包含的过程进行编辑，该过程仅依赖于轻量级的线性代数运算来计算参数变化，从而实现快速且一致的参数修改，并且开销极小。为了提高终身设置的可扩展性，ULTRAEDIT采用终身归一化策略，不断更新轮次间的特征统计信息，使其能够适应分布变化并随时间保持一致性。ULTRAEDIT的编辑速度比最新的前沿方法快7倍以上——这也是当时已知的最快方法——同时VRAM使用量不到三分之一，使其成为目前唯一能够在24GB消费级GPU上编辑7B LLM的方法。此外，我们构建了迄今为止该领域最大的数据集ULTRAEDITBENCH，包含超过2M个编辑对，并证明我们的方法支持高达1M次的编辑同时保持高准确性。在四个数据集和六个模型上的综合实验表明，ULTRAEDIT在不同模型编辑场景中始终实现卓越性能。我们的代码可在：[<a target="_blank" rel="noopener" href="https://github.com/XiaojieGu/UltraEdit%E8%AE%BF%E9%97%AE%E3%80%82]">https://github.com/XiaojieGu/UltraEdit访问。]</a>(<a target="_blank" rel="noopener" href="https://github.com/XiaojieGu/UltraEdit%E8%AE%BF%E9">https://github.com/XiaojieGu/UltraEdit%E8%AE%BF%E9</a> to )</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14679v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型通过持续更新内部知识，实现适应不断变化的信息。模型编辑作为一种有前景的解决方案，旨在修订模型的内部知识，并使其成为终身学习的关键工具。ULTRAEDIT作为一种全新编辑解决方案，无需训练、主题和内存，特别适用于大规模实时模型编辑。它通过轻量级线性代数运算进行参数修改，实现快速且一致的编辑。ULTRAEDIT采用终身标准化策略，适应分布变化并维持时间一致性。它的编辑速度超过现有方法7倍，同时占用内存少，是唯一能在消费级GPU上进行7B大型语言模型编辑的方法。ULTRAEDITBENCH数据集包含超过百万对编辑实例，证明了其在大量编辑下的高准确性。实验表明，ULTRAEDIT在不同模型编辑场景中表现卓越。详情请访问：<a target="_blank" rel="noopener" href="https://github.com/XiaojieGu/UltraEdit%E3%80%82">https://github.com/XiaojieGu/UltraEdit。</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型通过终身学习适应变化信息。</li>
<li>模型编辑是修订大型语言模型内部知识的有效方法。</li>
<li>ULTRAEDIT是一种全新、高效、训练、主题和内存独立的编辑解决方案。</li>
<li>ULTRAEDIT利用轻量级线性代数操作进行快速参数修改。</li>
<li>采用终身标准化策略提高ULTRAEDIT在大规模环境下的可扩展性。</li>
<li>ULTRAEDIT编辑速度远超现有方法，内存占用低。</li>
<li>ULTRAEDITBENCH数据集用于验证大量编辑下的准确性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14679">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a1ef9fb86e3d78410adbc9c11bf87482.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35a483c2cb1f7bdff827259266fb2b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79087818c7d429442ef0b64ae8a1dd0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4db244d612aa7f520fa3c46f74534307.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d8d4302ed413bf751fc019ec865dd31.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Visionary-R1-Mitigating-Shortcuts-in-Visual-Reasoning-with-Reinforcement-Learning"><a href="#Visionary-R1-Mitigating-Shortcuts-in-Visual-Reasoning-with-Reinforcement-Learning" class="headerlink" title="Visionary-R1: Mitigating Shortcuts in Visual Reasoning with   Reinforcement Learning"></a>Visionary-R1: Mitigating Shortcuts in Visual Reasoning with   Reinforcement Learning</h2><p><strong>Authors:Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, Kaiyang Zhou</strong></p>
<p>Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM – by prompting the model to produce a reasoning chain before providing an answer – can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks. </p>
<blockquote>
<p>长期以来，学习通用推理能力一直是人工智能领域的一个难题。近期在大语言模型（LLM）方面的研究表明，强化学习技术（如GRPO）可以使预训练的语言模型通过简单的问题答案对来发展推理能力。在本文中，我们旨在通过强化学习和视觉问答对来训练视觉语言模型（VLM），使其能够对图像数据进行推理，而无需任何明确的思维链（CoT）监督。我们的研究结果表明，仅仅通过强化学习对VLM应用提示模型产生推理链并给出答案的方法，可能会导致模型从简单问题中产生捷径，从而降低其在未见数据分布上的泛化能力。我们认为避免捷径学习的关键是鼓励模型在推理之前解释图像。因此，我们训练模型遵循标题-推理-答案的输出格式：首先为图像生成详细的标题，然后构建广泛的推理链。在无需思维链的273K视觉问答对上训练，仅使用强化学习时，我们的模型——Visionary-R1在多视觉推理基准测试中表现优于强大的多模态模型，如GPT-4o、Claude 3.5-Sonnet和Gemini-1.5-Pro。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14677v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在强化学习技术和视觉问答对的辅助下可以发展出推理能力。本文通过强化学习训练视觉语言模型（VLM）进行图像数据推理，并发现引导模型产生推理链后再提供答案会导致模型依赖捷径，降低其在未见数据分布上的泛化能力。鼓励模型在推理前解读图像是缓解捷径学习的关键。训练的模型（Visionary-R1）在多个视觉推理基准测试中表现优于其他强大的多模态模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习技术可以帮助预训练的LLM发展出推理能力。</li>
<li>通过强化学习和视觉问答对训练VLM进行图像数据推理。</li>
<li>引导模型产生推理链再提供答案可能导致模型依赖捷径，影响泛化能力。</li>
<li>鼓励模型在推理前解读图像是缓解捷径学习的关键。</li>
<li>训练的模型Visionary-R1采用先生成图像详细描述，再构建推理链的输出格式。</li>
<li>Visionary-R1在多个视觉推理基准测试中的表现优于其他强大的多模态模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14677">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eedecaec5c153adc57b67a41d8d0a693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ab580e632ed95049130be3c79d8fb5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36d55882085b3f5ebef82cd73865590a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc8674003374b06961547ba85760386b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reward-Reasoning-Model"><a href="#Reward-Reasoning-Model" class="headerlink" title="Reward Reasoning Model"></a>Reward Reasoning Model</h2><p><strong>Authors:Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, Furu Wei</strong></p>
<p>Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at <a target="_blank" rel="noopener" href="https://huggingface.co/Reward-Reasoning">https://huggingface.co/Reward-Reasoning</a>. </p>
<blockquote>
<p>奖励模型在引导大型语言模型产生符合人类期望的输出方面起着关键作用。然而，如何有效利用测试时的计算资源来提升奖励模型的性能，仍然是一个开放性的挑战。在此工作中，我们引入了奖励推理模型（RRMs），它们被专门设计用于在生成最终奖励之前执行有意识的推理过程。通过链式思维推理，RRMs利用额外的测试时间计算资源来处理复杂查询，在这些查询中，合适的奖励并不立即显现。为了开发RRMs，我们实现了一个强化学习框架，该框架在不需要明确的推理轨迹作为训练数据的情况下，培养了自我进化的奖励推理能力。实验结果表明，RRMs在不同领域的奖励建模基准测试中实现了卓越的性能。值得注意的是，我们证明了RRMs可以自适应地利用测试时的计算资源来进一步提高奖励的准确性。预训练的奖励推理模型可在<a target="_blank" rel="noopener" href="https://huggingface.co/Reward-Reasoning%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/Reward-Reasoning找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14674v1">PDF</a> </p>
<p><strong>Summary</strong>：奖励模型在大规模语言模型中扮演着关键角色，能够引导其输出与人类预期相符的结果。本研究提出了奖励推理模型（RRMs），能够在最终生成奖励前进行推理过程，对复杂的查询进行额外测试时间计算以得到恰当的奖励。该研究采用强化学习框架训练模型，使其在无需明确推理轨迹作为训练数据的情况下自我进化奖励推理能力。实验结果显示，RRMs在多个领域的奖励建模基准测试中表现优异，并能自适应利用测试时间计算提高奖励准确性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>奖励模型在大规模语言模型中起关键作用，引导输出与人类预期相符的结果。</li>
<li>奖励推理模型（RRMs）能够在测试阶段进行推理过程，适用于复杂查询的奖励计算。</li>
<li>RRMs利用强化学习框架进行训练，无需明确的推理轨迹作为训练数据。</li>
<li>RRMs在多个领域的奖励建模基准测试中表现优异。</li>
<li>RRMs能够自适应利用测试时间计算来提高奖励的准确性。</li>
<li>奖励推理模型的预训练模型可以在Hugging Face上获取。</li>
<li>该研究为提高大语言模型的性能提供了一种新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14674">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-87e008355e834da2c01193f0474a59a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-488e48f0aafd6da22e75849470b37bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0334a638c74cecc248e99ebb6fe445e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Quartet-Native-FP4-Training-Can-Be-Optimal-for-Large-Language-Models"><a href="#Quartet-Native-FP4-Training-Can-Be-Optimal-for-Large-Language-Models" class="headerlink" title="Quartet: Native FP4 Training Can Be Optimal for Large Language Models"></a>Quartet: Native FP4 Training Can Be Optimal for Large Language Models</h2><p><strong>Authors:Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh</strong></p>
<p>The rapid advancement of large language models (LLMs) has been paralleled by unprecedented increases in computational demands, with training costs for state-of-the-art models doubling every few months. Training models directly in low-precision arithmetic offers a solution, by improving both computational throughput and energy efficiency. Specifically, NVIDIA’s recent Blackwell architecture facilitates extremely low-precision operations, specifically FP4 variants, promising substantial efficiency gains. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we systematically investigate hardware-supported FP4 training and introduce Quartet, a new approach enabling accurate, end-to-end FP4 training with all the major computations (in e.g. linear layers) being performed in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across varying bit-widths and allows us to identify a “near-optimal” low-precision training technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve state-of-the-art accuracy for FP4 precision, successfully training billion-scale models. Our method demonstrates that fully FP4-based training is a competitive alternative to standard-precision and FP8 training. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/Quartet">https://github.com/IST-DASLab/Quartet</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的快速发展伴随着计算需求的空前增长，最先进的模型的训练成本每隔几个月就翻一倍。直接在低精度算术中进行模型训练可以提高计算效率和能源效率，为解决这一问题提供了方案。具体来说，NVIDIA最近的Blackwell架构支持极低精度的运算，尤其是FP4变体，有望获得巨大的效率提升。然而，当前在FP4精度下训练LLM的算法面临着精度大幅下降的问题，并且经常依赖于混合精度回退方案。在本文中，我们系统地研究了硬件支持的FP4训练，并引入了Quartet这一新方法，它能够在低精度下实现端到端的准确FP4训练，所有主要的计算（如线性层）都在低精度下完成。通过对Llama类型模型的广泛评估，我们揭示了一种新的低精度缩放定律，该定律量化了不同位宽之间的性能权衡，并帮助我们识别出在准确性与计算之间的“近最优”低精度训练技术，称为Quartet。我们使用针对NVIDIA Blackwell GPU优化的CUDA内核实现了Quartet，并证明它可以实现FP4精度的最新技术水平，成功训练了数十亿规模的模型。我们的方法表明，完全基于FP4的训练是标准精度和FP8训练的竞争替代方案。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/IST-DASLab/Quartet%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IST-DASLab/Quartet上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14669v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的快速发展伴随着计算需求的空前增长，训练成本每几个月翻一倍。直接在低精度算术中进行模型训练可以提高计算效率和能源效率。NVIDIA的Blackwell架构支持FP4精度运算，但当前算法在FP4精度下训练LLM面临精度损失的问题。本文系统研究了硬件支持的FP4训练，提出了一种新的方法Quartet，能够在低精度下实现端到端的FP4训练。通过对Llama类型模型的评估，本文揭示了低精度缩放定律，并确定了在准确性与计算之间具有“近最优”的低精度训练技术。Quartet方法在NVIDIA Blackwell GPU上实现，可在FP4精度下达到最先进的准确性，成功训练百亿规模模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的计算需求迅速增长，训练成本高昂。</li>
<li>低精度算术训练可以提高计算效率和能源效率。</li>
<li>NVIDIA的Blackwell架构支持FP4精度运算，但FP4精度训练面临精度损失问题。</li>
<li>本文提出了Quartet方法，实现了端到端的FP4精度训练，解决了精度损失问题。</li>
<li>通过评估发现低精度缩放定律，确定了近最优的低精度训练技术。</li>
<li>Quartet方法在NVIDIA Blackwell GPU上实现，可成功训练百亿规模模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-93fa795e203bf32a606ccca1b9674f77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4831573d5f712fe2aa29399e83bf4acf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8aa66af809760e9f34b6c9ed8ed56900.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f3c70820bc5e258ace1bb685d17a77c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ContextAgent-Context-Aware-Proactive-LLM-Agents-with-Open-World-Sensory-Perceptions"><a href="#ContextAgent-Context-Aware-Proactive-LLM-Agents-with-Open-World-Sensory-Perceptions" class="headerlink" title="ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions"></a>ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions</h2><p><strong>Authors:Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan</strong></p>
<p>Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts to enhance the proactive capabilities of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and the persona contexts from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步推动了智能代理从被动响应向主动支持的转变。尽管前景光明，但现有的主动代理要么仅依赖于封闭环境的观察（例如桌面用户界面）进行直接LLM推理，要么采用基于规则的主动通知，导致对用户意图的理解不够理想，主动服务的功能有限。在本文中，我们介绍了ContextAgent，这是一款首款结合广泛环境感知增强LLM代理主动能力的上下文感知主动代理。ContextAgent首先从可穿戴设备的大量感官感知（例如视频和音频）中提取多维上下文，以了解用户意图。然后，ContextAgent利用感官上下文和历史数据中的个人上下文来预测是否需要主动服务。在需要主动协助时，ContextAgent还会自动调用必要的工具以协助用户而不干扰用户。为了评估这一新任务，我们创建了ContextAgentBench，这是评估上下文感知主动LLM代理的首个基准测试，涵盖9个日常场景的1000个样本和20个工具。在ContextAgentBench上的实验表明，ContextAgent的主动预测和工具调用准确率分别提高了高达8.5%和6.0%，超过了基线。我们希望这项研究能激发更先进、以人类为中心的主动人工智能助理的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14668v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了最新的大型语言模型（LLM）的进步推动了智能代理从被动响应向主动支持的转变。然而，现有的主动代理存在局限性，要么仅依赖封闭环境的观察进行直接LLM推理，要么采用基于规则的主动通知，导致对用户意图的理解不足和主动服务的功能有限。本文提出了ContextAgent，这是一种首屈一指的意识感知主动代理，它融入了广泛的环境感知来提高LLM代理的主动能力。ContextAgent首先从可穿戴设备的大量感官感知中提取多维上下文（如视频和音频）来理解用户意图。然后，它利用感官上下文和历史数据中的个人上下文来预测是否需要主动服务。当需要主动协助时，ContextAgent会进一步自动调用必要的工具来协助用户。为评估此新任代理的表现，我们制定了ContextAgentBench评估标准，涵盖日常九个场景中的一千个样本和二十个工具。实验表明，ContextAgent在主动预测和工具调用方面的准确率分别提高了高达8.5%和6.0%。本文的研究有望激发更先进、以人类为中心的主动AI助理的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的进步使得智能代理能够更主动地支持用户。</li>
<li>现有主动代理存在局限性，如依赖封闭环境观察和基于规则的主动通知。</li>
<li>ContextAgent是一种意识感知主动代理，结合广泛的环境感知提高LLM代理的主动能力。</li>
<li>ContextAgent通过提取多维上下文（如视频和音频）从可穿戴设备理解用户意图。</li>
<li>ContextAgent利用感官上下文和个人上下文预测是否需要主动服务。</li>
<li>ContextAgent在自动调用工具以协助用户方面表现出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14668">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9040c7152d5f02942263da1df2ca8223.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9087e17f390599c407981d7df1c30801.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e75153888530eb9a15a427723c9b60c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bef43eb44c3dc35014aa294443414bc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Beyond-Words-Multimodal-LLM-Knows-When-to-Speak"><a href="#Beyond-Words-Multimodal-LLM-Knows-When-to-Speak" class="headerlink" title="Beyond Words: Multimodal LLM Knows When to Speak"></a>Beyond Words: Multimodal LLM Knows When to Speak</h2><p><strong>Authors:Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin</strong></p>
<p>While large language model (LLM)-based chatbots have demonstrated strong capabilities in generating coherent and contextually relevant responses, they often struggle with understanding when to speak, particularly in delivering brief, timely reactions during ongoing conversations. This limitation arises largely from their reliance on text input, lacking the rich contextual cues in real-world human dialogue. In this work, we focus on real-time prediction of response types, with an emphasis on short, reactive utterances that depend on subtle, multimodal signals across vision, audio, and text. To support this, we introduce a new multimodal dataset constructed from real-world conversational videos, containing temporally aligned visual, auditory, and textual streams. This dataset enables fine-grained modeling of response timing in dyadic interactions. Building on this dataset, we propose MM-When2Speak, a multimodal LLM-based model that adaptively integrates visual, auditory, and textual context to predict when a response should occur, and what type of response is appropriate. Experiments show that MM-When2Speak significantly outperforms state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x improvement in response timing accuracy over leading commercial LLMs. These results underscore the importance of multimodal inputs for producing timely, natural, and engaging conversational AI. </p>
<blockquote>
<p>基于大型语言模型（LLM）的聊天机器人已展现出生成连贯且上下文相关的响应的强大能力，但它们往往难以理解何时应该发言，尤其是在持续对话中提供简短、及时的反应。这一局限性主要源于它们对文本输入的依赖，缺乏现实世界中人类对话的丰富上下文线索。在这项工作中，我们专注于实时预测响应类型，重点是通过视觉、音频和文本等细微的多模式信号来预测简短、反应性的发言。为此，我们引入了一个新的多模式数据集，该数据集来自现实世界的对话视频，包含时间对齐的视觉、听觉和文本流。该数据集能够精细地模拟二人互动中的响应时间。基于该数据集，我们提出了MM-When2Speak模型，这是一个基于多模式LLM的模型，可自适应地整合视觉、听觉和文本上下文，以预测何时应该发生响应以及何种类型的响应是恰当的。实验表明，MM-When2Speak显著优于最先进的单模式LLM基线模型，在响应时间准确性上最高达到领先商业LLM模型的四倍改善效果。这些结果凸显了多模式输入对于产生及时、自然和引人入胜的聊天机器人的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14654v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/lzk901372/MM-When2Speak">https://github.com/lzk901372/MM-When2Speak</a></p>
<p><strong>Summary</strong><br>大型语言模型（LLM）在生成连贯、语境相关的聊天机器人回应方面表现出强大的能力，但在实时对话中理解何时发言仍存在挑战。本文聚焦于响应类型的实时预测，特别是依赖于视觉、听觉和文本等多种微妙信号的简短反应发言。为此，我们引入新的多模式数据集，并在此基础上提出MM-When2Speak模型，该模型可自适应地整合视觉、听觉和文本上下文，以预测何时应该发生响应以及何种类型的响应是恰当的。实验表明，MM-When2Speak明显优于最新的单模态和LLM基准模型，在响应时间准确性方面提高了四倍。这突显了多模式输入对于产生及时、自然和引人入胜的会话AI的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在生成连贯的聊天机器人回应方面表现出强大的能力，但在实时对话中理解何时发言存在挑战。</li>
<li>响应类型的实时预测是关键，特别是依赖于视觉、听觉和文本等多种微妙信号的简短反应发言。</li>
<li>引入新的多模式数据集，用于构建基于LLM的聊天机器人模型。</li>
<li>提出MM-When2Speak模型，该模型整合多模式上下文以预测响应时机和类型。</li>
<li>MM-When2Speak模型在响应时间准确性方面显著提高，优于其他模型。</li>
<li>实验结果突显了多模式输入对于产生自然、及时的会话AI的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab68199847e3a180f9e54e416f7bf209.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-866487d01c92c1bc84e158fc4af5eb50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f4adf1ab29cd85682b40f587d05f615.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f1bb3793f8ffdc8f3ef1a0e79b7fdcd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="General-Reasoner-Advancing-LLM-Reasoning-Across-All-Domains"><a href="#General-Reasoner-Advancing-LLM-Reasoning-Across-All-Domains" class="headerlink" title="General-Reasoner: Advancing LLM Reasoning Across All Domains"></a>General-Reasoner: Advancing LLM Reasoning Across All Domains</h2><p><strong>Authors:Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen</strong></p>
<p>Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the “Zero” reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks. </p>
<blockquote>
<p>强化学习（RL）在提升大型语言模型（LLM）的推理能力方面表现出了强大的潜力。特别是Deepseek-R1-Zero引入的“零”强化学习，能够实现对基础LLM的直接强化学习训练，无需依赖中间监督微调阶段。尽管有这些进展，但目前LLM推理的主要工作主要集中在数学和编码领域，这主要是因为数据丰富且答案验证方便。这限制了此类模型在更广泛领域的应用和通用性，在这些领域中，问题的答案表示通常具有多样性，且数据更加稀缺。在本文中，我们提出了General-Reasoner，这是一种旨在提升LLM在多样化领域推理能力的新型训练范式。我们的主要贡献包括：（1）通过网页爬虫构建了一个大规模、高质量的问题数据集，其中包含可验证的答案，覆盖广泛的学科领域；（2）开发了一种基于生成模型的答案验证器，它用基于思维链和上下文感知的能力取代了传统的基于规则的验证方法。我们在一系列模型上进行了训练，并在涵盖物理、化学、金融、电子等广泛领域的多个数据集上进行了评估。我们在12个基准测试（例如MMLU-Pro、GPQA、SuperGPQA、TheoremQA、BBEH和MATH AMC）上的全面评估表明，General-Reasoner优于现有基准方法，实现了稳健且可推广的推理性能，同时在数学推理任务中保持卓越的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14652v1">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习在提升大型语言模型（LLM）的推理能力方面展现出巨大潜力。Deepseek-R1-Zero提出的“零”强化学习可以直接训练基础LLM模型，无需依赖中间监督微调阶段。然而，当前LLM推理主要集中在数学和编码领域，这限制了其在更广泛领域的适用性和泛化能力。本文提出General-Reasoner，一种旨在提升LLM在多个领域推理能力的新型训练范式。其关键贡献包括构建大规模高质量问题数据集和基于生成模型的答案验证器。实验表明，General-Reasoner在多个基准测试上优于现有方法，实现稳健且可推广的推理性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>强化学习可提升大型语言模型的推理能力。</li>
<li>Deepseek-R1-Zero提出的“零”强化学习可直接训练LLM模型。</li>
<li>当前LLM推理主要集中在数学和编码领域，限制了其在更广领域的适用性和泛化能力。</li>
<li>General-Reasoner旨在提升LLM在多个领域的推理能力。</li>
<li>General-Reasoner的关键贡献包括构建大规模高质量问题数据集和基于生成模型的答案验证器。</li>
<li>General-Reasoner在多个基准测试上实现了优于现有方法的性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14652">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-13f1a79a6b6dcc23b3f9a825e6f8478e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21cc0a7179f7bac17567dd372e69232a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa32ca03b448f35ebee2bd358bab9b61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49858842a88a5b99574dd1b67a920b32.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Think-Only-When-You-Need-with-Large-Hybrid-Reasoning-Models"><a href="#Think-Only-When-You-Need-with-Large-Hybrid-Reasoning-Models" class="headerlink" title="Think Only When You Need with Large Hybrid-Reasoning Models"></a>Think Only When You Need with Large Hybrid-Reasoning Models</h2><p><strong>Authors:Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei</strong></p>
<p>Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model’s capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems. </p>
<blockquote>
<p>近期的大型推理模型（LRMs）通过融入最终的回应之前的扩展思考过程，相较于传统的大型语言模型（LLMs）展现出显著增强的推理能力。然而，过度冗长的思考过程在令牌消耗和延迟方面引入了巨大的开销，这在处理简单查询时显得尤为不必要。在本研究中，我们推出了大型混合推理模型（LHRMs），这是一种能够自适应地根据用户查询的上下文信息来决定是否进行思考的模型。为了实现这一点，我们提出了一个两阶段的训练流程，首先通过混合微调（HFT）进行冷启动，然后通过提出的混合组策略优化（HGPO）进行在线强化学习来隐含地选择适当的思考模式。此外，我们还引入了一个称为混合准确率的指标来定量评估模型的混合思考能力。广泛的实验结果表明，LHRMs可以自适应地对不同类型和难度的查询进行混合思考，在推理和一般能力上优于现有的LRMs和LLMs，并大大提高了效率。总体而言，我们的研究提倡重新考虑适当使用扩展思考过程，并为构建混合思考系统提供了一个坚实的起点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14631v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型混合推理模型（LHRMs）能够根据用户查询的上下文信息自适应地决定是否需要思考。通过混合微调（HFT）的冷启动和混合组策略优化（HGPO）的在线强化学习，模型能够隐式学习选择适当的思考模式。此外，还引入了混合精度评估指标来量化模型的混合思维能力。实验结果证明，LHRMs能够根据不同的查询难度和类型自适应地进行混合思考，并且在推理和通用能力上优于现有的大型推理模型和大型语言模型，同时提高了效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LHRMs能自适应地决定是否需要思考，基于用户查询的上下文信息。</li>
<li>通过混合微调（HFT）的冷启动和混合组策略优化（HGPO）的在线强化学习，模型能够隐式学习选择思考模式。</li>
<li>引入了混合精度评估指标来量化模型的混合思维能力。</li>
<li>LHRMs能够根据不同的查询难度和类型进行自适应的混合思考。</li>
<li>LHRMs在推理和通用能力上优于现有的大型推理模型（LRMs）和大型语言模型（LLMs）。</li>
<li>LHRMs提高了效率，对于简单的查询不需要过长的思考时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14631">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1a91a379cc090d05caebc5c82afc503e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-862a957a47f2ee33a00508036807f555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-174a003a6a0d4479a481f386b88b6024.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="KERL-Knowledge-Enhanced-Personalized-Recipe-Recommendation-using-Large-Language-Models"><a href="#KERL-Knowledge-Enhanced-Personalized-Recipe-Recommendation-using-Large-Language-Models" class="headerlink" title="KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large   Language Models"></a>KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large   Language Models</h2><p><strong>Authors:Fnu Mohbat, Mohammed J Zaki</strong></p>
<p>Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at <a target="_blank" rel="noopener" href="https://github.com/mohbattharani/KERL">https://github.com/mohbattharani/KERL</a>. </p>
<blockquote>
<p>随着大型语言模型（LLM）的最新进展和食品数据的丰富，利用LLM改善食品理解的研究应运而生。尽管已有一些推荐系统利用LLM和知识图谱（KGs），但将食品相关的KGs与LLM相结合的研究却很有限。我们介绍了KERL，这是一个统一的系统，它利用食品KGs和LLM来提供个性化的食品推荐并生成带有相关微营养信息的食谱。给定自然语言问题，KERL提取实体，从知识图谱中检索子图，然后将它们作为上下文输入到LLM中，以选择满足约束的食谱。接下来，我们的系统为每个食谱生成烹饪步骤和营养信息。为了评估我们的方法，我们还通过整理与食谱相关的问题、结合约束和个人偏好，开发了一个基准数据集。通过大量实验，我们证明所提出的KG增强LLM显著优于现有方法，为食品推荐、食谱生成和营养分析提供了完整且连贯的解决方案。我们的代码和基准数据集可在<a target="_blank" rel="noopener" href="https://github.com/mohbattharani/KERL%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/mohbattharani/KERL公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14629v1">PDF</a> Accepted at ACL 2025</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）和食品知识图谱（KG）的先进研究，KERL系统利用食品KGs和LLMs提供个性化食品推荐并生成带有相关微营养信息的食谱。通过提取实体、从知识图谱中检索子图并将其作为上下文输入到LLM中，以满足约束条件选择满足要求的食谱，生成烹饪步骤和营养信息。实验结果证明了KG增强型LLM的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs与食品知识图谱结合用于改进食品理解。</li>
<li>KERL系统实现了个性化食品推荐和食谱生成，带有微营养信息。</li>
<li>KERL通过提取实体、检索知识图谱子图并与LLM结合来满足约束条件。</li>
<li>公开可用的benchmark数据集用于评估系统性能。</li>
<li>实验结果表明，提出的KG增强型LLM显著优于现有方法。</li>
<li>KERL系统为食品推荐、食谱生成和营养分析提供了全面、连贯的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a0bc89d4e3d6c89cdb2e9afcf76ed57e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de470842645f94e5cfa7141ef0144378.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91e882a7f76188659ca32fcbe5620a87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce3b81b64847a61b96cc0157f8453c84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0ce1a4a79fec8d00af82d3b31392152.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Debating-for-Better-Reasoning-An-Unsupervised-Multimodal-Approach"><a href="#Debating-for-Better-Reasoning-An-Unsupervised-Multimodal-Approach" class="headerlink" title="Debating for Better Reasoning: An Unsupervised Multimodal Approach"></a>Debating for Better Reasoning: An Unsupervised Multimodal Approach</h2><p><strong>Authors:Ashutosh Adhikari, Mirella Lapata</strong></p>
<p>As Large Language Models (LLMs) gain expertise across diverse domains and modalities, scalable oversight becomes increasingly challenging, particularly when their capabilities may surpass human evaluators. Debate has emerged as a promising mechanism for enabling such oversight. In this work, we extend the debate paradigm to a multimodal setting, exploring its potential for weaker models to supervise and enhance the performance of stronger models. We focus on visual question answering (VQA), where two “sighted” expert vision-language models debate an answer, while a “blind” (text-only) judge adjudicates based solely on the quality of the arguments. In our framework, the experts defend only answers aligned with their beliefs, thereby obviating the need for explicit role-playing and concentrating the debate on instances of expert disagreement. Experiments on several multimodal tasks demonstrate that the debate framework consistently outperforms individual expert models. Moreover, judgments from weaker LLMs can help instill reasoning capabilities in vision-language models through finetuning. </p>
<blockquote>
<p>随着大型语言模型（LLM）在各个领域和模态中越来越专业化，可扩展的监督变得越来越具有挑战性，尤其是当它们的能力可能超越人类评估者时。辩论作为一种有前景的机制，为这种监督提供了可能。在这项工作中，我们将辩论范式扩展到多模态环境，探索其潜力，使较弱模型能够监督并增强较强模型的性能。我们专注于视觉问答（VQA），其中两个“视力”专家视觉语言模型对答案进行辩论，而一个“盲”（仅文本）法官仅根据论证的质量进行裁决。在我们的框架中，专家只捍卫与其信念相符的答案，从而不需要明确的角色扮演，并将辩论集中在专家意见不一致的情况上。在多个多模态任务上的实验表明，辩论框架始终优于单个专家模型。此外，较弱的大型语言模型的判断可以通过微调帮助视觉语言模型培养推理能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14627v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLM）在多领域和多模态方面表现出卓越的专业知识，但对其进行可伸缩的监督变得越来越具有挑战性，尤其是当其能力可能超过人类评估者时。辩论作为一种监督方式展现了巨大的潜力，本研究将辩论范式扩展到多模态环境中，探讨其对弱模型监督强模型性能的潜力。在视觉问答（VQA）领域，两个“有视力”的专家视觉语言模型进行辩论答案，一个“盲”（仅文本）的法官根据论证质量进行裁决。专家的答案仅基于他们的信念进行辩护，无需进行明确的角色扮演，专注于专家的分歧实例。在多个多模态任务上的实验表明，辩论框架始终优于单个专家模型。此外，较弱的大型语言模型的判断有助于通过微调赋予视觉语言模型推理能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在多个领域和模态中的专业知识提升使得监督变得更具挑战性。</li>
<li>辩论作为一种监督方式展现出巨大潜力，特别是在多模态环境中。</li>
<li>在视觉问答（VQA）领域，引入辩论范式，其中两个专家模型辩论答案，一个“盲”模型作为法官裁决。</li>
<li>专家模型根据信念进行答案辩护，无需明确角色扮演，聚焦于专家分歧实例。</li>
<li>辩论框架在多模态任务上的表现始终优于单个专家模型。</li>
<li>较弱的LLM判断有助于通过微调增强视觉语言模型的推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6e15477a9d03323e28996b6760b0b59e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8124af251bffbf6d24ad78029cf71c59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-514e24f1b2e1cb22dfd8624dea2e206c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41838e885eecf591d9a4bf7f8727c02f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning"><a href="#TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning" class="headerlink" title="TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning"></a>TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning</h2><p><strong>Authors:Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran</strong></p>
<p>Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL’s success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem–false negatives–where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV">https://github.com/uw-nsl/TinyV</a>. </p>
<blockquote>
<p>强化学习（RL）已成为通过优化大型语言模型（LLM）的策略和奖励信号来提升其推理能力的一种强大工具。然而，RL的成功依赖于奖励的可靠性，这些奖励由验证器提供。在本文中，我们揭示并分析了广泛存在的问题——假阴性（false negatives），即验证器错误地拒绝了正确的模型输出。我们对Big-Math-RL-Verified数据集的深入研究后发现，超过3 8%的模型生成响应存在假阴性问题，即验证器无法识别正确答案。我们通过实证和理论证明，这些假阴性会严重损害RL训练，因为剥夺了模型的信息化梯度信号并减缓了收敛速度。为了缓解这一问题，我们提出了tinyV，这是一个基于LLM的轻量级验证器，它增强了现有的基于规则的方法，能够动态识别潜在的假阴性并恢复有效的响应，以产生更准确的奖励估计。在多个数学推理基准测试中，集成TinyV的通过率提高了高达1 0%，相对于基线加速了收敛。我们的研究强调了解决验证器假阴性的关键重要性，并提供了一种实用的方法来改进基于RL的LLM微调。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/uw-nsl/TinyV上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14625v1">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习（RL）通过优化策略和使用奖励信号提升大型语言模型（LLM）的推理能力。然而，RL的成功依赖于奖励的可靠性，奖励由验证器提供。本文揭示了一个普遍存在的问题——假阴性（False Negative），即验证器错误地拒绝正确的模型输出。通过对Big-Math-RL-Verified数据集的研究，我们发现超过38%的模型生成响应存在假阴性问题。本文通过实证和理论分析表明，假阴性会严重阻碍RL训练，剥夺模型的有用梯度信号并减慢收敛速度。为解决这一问题，我们提出了TinyV，一种基于轻量级LLM的验证器，它通过增强现有的基于规则的验证器来动态识别潜在的假阴性并恢复有效的响应，从而提供更准确的奖励估计。在多个数学推理基准测试中，集成TinyV可将通过率提高高达10%，并相对于基线加速收敛。我们的研究强调了解决验证器假阴性的重要性，并提供了一种实用的方法来改进基于RL的LLM微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习用于增强大型语言模型的推理能力。</li>
<li>验证器在奖励信号中扮演重要角色，但其易出现假阴性错误。</li>
<li>假阴性问题在模型生成的响应中占比超过38%。</li>
<li>假阴性会严重阻碍强化学习的训练和收敛速度。</li>
<li>TinyV是一种基于LLM的轻量级验证器，能动态识别并修复假阴性。</li>
<li>集成TinyV后，数学推理基准测试的通过率提高10%，且收敛速度加快。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14625">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca76bd581877e2fa80edb606fc61e233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45315ad641603139bd5ff244f95ac9c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a08c30a5d241b44320c26435bfbb837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98fa5864234d9a3f6c406da5a995e97.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enhancing-Learned-Knowledge-in-LoRA-Adapters-Through-Efficient-Contrastive-Decoding-on-Ascend-NPUs"><a href="#Enhancing-Learned-Knowledge-in-LoRA-Adapters-Through-Efficient-Contrastive-Decoding-on-Ascend-NPUs" class="headerlink" title="Enhancing Learned Knowledge in LoRA Adapters Through Efficient   Contrastive Decoding on Ascend NPUs"></a>Enhancing Learned Knowledge in LoRA Adapters Through Efficient   Contrastive Decoding on Ascend NPUs</h2><p><strong>Authors:Morgan Lindsay Heisler, Linzi Xing, Ge Shi, Hanieh Sadri, Gursimran Singh, Weiwei Zhang, Tao Ye, Ying Xiong, Yong Zhang, Zhenan Fan</strong></p>
<p>Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and scalable method to fine-tune and customize large language models (LLMs) for application-specific needs. However, tasks that require complex reasoning or deep contextual understanding are often hindered by biases or interference from the base model when using typical decoding methods like greedy or beam search. These biases can lead to generic or task-agnostic responses from the base model instead of leveraging the LoRA-specific adaptations. In this paper, we introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed to maximize the use of task-specific knowledge in LoRA-adapted models, resulting in better downstream performance. CoLD uses contrastive decoding by scoring candidate tokens based on the divergence between the probability distributions of a LoRA-adapted expert model and the corresponding base model. This approach prioritizes tokens that better align with the LoRA’s learned representations, enhancing performance for specialized tasks. While effective, a naive implementation of CoLD is computationally expensive because each decoding step requires evaluating multiple token candidates across both models. To address this, we developed an optimized kernel for Huawei’s Ascend NPU. CoLD achieves up to a 5.54% increase in task accuracy while reducing end-to-end latency by 28% compared to greedy decoding. This work provides practical and efficient decoding strategies for fine-tuned LLMs in resource-constrained environments and has broad implications for applied data science in both cloud and on-premises settings. </p>
<blockquote>
<p>华为云用户利用LoRA（低秩适应）作为一种高效且可扩展的方法，对大型语言模型（LLM）进行微调并定制，以满足特定应用的需求。然而，在使用典型的解码方法（如贪心搜索或集束搜索）时，需要复杂推理或深度上下文理解的任务往往会受到基础模型的偏见或干扰的阻碍。这些偏见可能导致基础模型给出通用或任务无关的反应，而不是利用LoRA特定的适应。在本文中，我们介绍了对比LoRA解码（CoLD），这是一个旨在最大化LoRA适应模型中特定任务知识使用的新型解码框架，从而提高了下游性能。CoLD采用对比解码方法，根据LoRA适应的专家模型与相应基础模型的概率分布差异来评分候选令牌。这种方法优先选择与LoRA学习到的表示更匹配的令牌，提高了专项任务的性能。虽然这种方法有效，但CoLD的朴素实现计算成本高昂，因为每个解码步骤都需要在两个模型上评估多个令牌候选。为了解决这个问题，我们为华为的Ascend NPU开发了一个优化内核。与贪心解码相比，CoLD在任务准确性上提高了高达5.54%，同时端到端延迟降低了28%。这项工作为资源受限环境中微调的大型语言模型提供了实用且高效的解码策略，对云和本地环境中的应用数据科学具有广泛的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14620v1">PDF</a> Accepted at ACM KDD 2025</p>
<p><strong>摘要</strong></p>
<p>华为云用户利用LoRA（低秩适配）作为高效且可扩展的方法，对大语言模型进行微调以满足特定应用需求。然而，在使用典型的解码方法（如贪心搜索或束搜索）时，需要复杂推理或深度上下文理解的任务往往会受到基础模型的偏见或干扰影响。偏见可能导致基础模型给出通用的、与任务无关的反应，而不是利用LoRA特定的适配。本文介绍了一种新型的解码框架——对比LoRA解码（CoLD），旨在最大化LoRA适配模型中任务特定知识的使用，从而提高下游性能。CoLD通过对比候选标记的得分，基于LoRA适配的专家模型与相应基础模型的概率分布差异进行解码。这种方法优先选择与LoRA学习到的表示更一致的标记，对于专用任务的性能提升有明显帮助。尽管有效，但CoLD的直观实现方式计算量大，因为每个解码步骤都需要在两个模型上评估多个标记候选。为解决这一问题，我们为华为的Ascend NPU开发了一个优化内核。CoLD在任务准确性上提高了高达5.54%，同时端到端延迟降低了28%，与贪心解码相比。这项工作在资源受限的环境中为精细调整的语言模型提供了实用且高效的解码策略，对于云和内部部署环境中的实用数据科学具有广泛的影响。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LoRA作为一种有效的方法，被华为云用户用于微调大语言模型，以满足特定的应用需求。</li>
<li>典型解码方法在复杂任务中可能会受到基础模型的偏见或干扰的影响。</li>
<li>CoLD解码框架旨在通过对比专家模型和基础模型的差异来最大化任务特定知识的使用。</li>
<li>CoLD通过优先选择与LoRA学习到的表示更一致的标记，提升了专用任务的性能。</li>
<li>CoLD的直观实现方式计算量大，因此开发了一个针对华为Ascend NPU的优化内核。</li>
<li>CoLD在任务准确性和延迟方面提供了显著的提升，相比传统的解码方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14620">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c1dc1ae155fffc3d02736520f2ca5fe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4657c46ef5bcb72420a945c3809277ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-584ed4ee28fe950bc7d028e66120bf93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd403020709e474fd208bc4deb8b0bb0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Linear-Control-of-Test-Awareness-Reveals-Differential-Compliance-in-Reasoning-Models"><a href="#Linear-Control-of-Test-Awareness-Reveals-Differential-Compliance-in-Reasoning-Models" class="headerlink" title="Linear Control of Test Awareness Reveals Differential Compliance in   Reasoning Models"></a>Linear Control of Test Awareness Reveals Differential Compliance in   Reasoning Models</h2><p><strong>Authors:Sahar Abdelnabi, Ahmed Salem</strong></p>
<p>Reasoning-focused large language models (LLMs) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such “test awareness” impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning LLMs across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation. </p>
<blockquote>
<p>以推理为重点的大型语言模型（LLM）在检测到正在进行评估时有时会改变其行为，这种影响类似于霍桑现象，可能导致它们优化通过测试的性能，或者在现实世界后果似乎不存在的情况下更容易接受有害的提示。我们首次对“测试意识”如何影响模型行为进行了定量研究，特别是其安全对齐方面。我们引入了一个白盒探测框架，该框架（i）线性识别与意识相关的激活，（ii）在监视下游性能的同时，引导模型远离或远离测试意识。我们将该方法应用于不同最先进的开源推理LLM，涵盖现实和假设任务。我们的结果表明，测试意识对安全对齐有显著影响，并且对不同的模型有不同的影响。通过对此潜在效应进行精细控制，我们的工作旨在增加我们对安全评估的信心。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14617v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在检测自身被评估时会改变行为，类似于霍索恩效应（Hawthorne phenomenon）。这可能导致模型为了通过测试而优化性能，或在缺乏实际后果的情况下更容易遵循有害提示。本研究首次定量探讨了“测试意识”（test awareness）对模型行为的影响，特别是其安全对齐方面的表现。研究引入了白盒探测框架，能够识别与意识相关的激活状态并控制模型的测试意识，同时监测下游性能。在先进的大型语言模型和现实与假设任务上的实验表明，测试意识对安全对齐有显著影响且不同模型间存在差异。研究旨在为如何控制这种潜在效应提供精细调控，从而提升对安全评估的信任度。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型在检测到被评估时会表现出类似于霍索恩效应的行为改变。</li>
<li>测试意识对模型的安全对齐表现有显著影响。</li>
<li>引入了白盒探测框架来识别和监测测试意识对模型行为的影响。</li>
<li>不同的大型语言模型在测试意识方面的表现存在差异。</li>
<li>研究提供了对模型测试意识的精细调控方法。</li>
<li>测试意识的控制有助于提升对大型语言模型安全评估的信任度。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14617">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8f6a0bf3937da1484ccd19b4b2819a2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa3e0e96d0c1d8ec60c5e3c686bcbf9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872d9b59887a977155475c5fcd8d4259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-105f5bbf548ea39480233b3273ad0524.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Let-LLMs-Break-Free-from-Overthinking-via-Self-Braking-Tuning"><a href="#Let-LLMs-Break-Free-from-Overthinking-via-Self-Braking-Tuning" class="headerlink" title="Let LLMs Break Free from Overthinking via Self-Braking Tuning"></a>Let LLMs Break Free from Overthinking via Self-Braking Tuning</h2><p><strong>Authors:Haoran Zhao, Yuchen Yan, Yongliang Shen, Haolei Xu, Wenqi Zhang, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang</strong></p>
<p>Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models. </p>
<blockquote>
<p>大型推理模型（LRMs），如OpenAI o1和DeepSeek-R1，通过生成更长的思维链显著增强了其推理能力，在各种任务中表现出卓越的性能。然而，这种性能的提升伴随着生成过程中冗余推理的显著增加，导致了计算开销的增大和过度思考问题的加剧。尽管许多现有方法旨在解决过度思考的问题，但它们通常依赖于外部干预。在本文中，我们提出了一种新型框架Self-Braking Tuning（SBT），它从允许模型自我调节其推理过程的角度来解决过度思考的问题，从而消除了对外部控制机制的依赖。我们基于标准答案构建了一套过度思考识别指标，并设计了一种系统方法来检测冗余推理。该方法能够准确识别推理轨迹中的不必要步骤，并为学习自我调控行为生成训练信号。在此基础上，我们制定了构建具有自适应推理长度的数据的完整策略，并引入了一种创新的制动提示机制，使模型能够自然地学习在适当的时机终止推理。在数学基准测试（AIME、AMC、MATH500、GSM8K）的实验表明，我们的方法减少了高达60%的令牌消耗，同时保持了与无限制模型相当的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14604v1">PDF</a> Github:<a target="_blank" rel="noopener" href="https://github.com/CCAI-Lab/Self-Braking-Tuning">https://github.com/CCAI-Lab/Self-Braking-Tuning</a>; Project:   <a target="_blank" rel="noopener" href="https://ccai-lab.github.io/SBT">https://CCAI-Lab.github.io/SBT</a></p>
<p><strong>Summary</strong></p>
<p>大型推理模型（LRMs）如OpenAI o1和DeepSeek-R1通过生成更长的思维链显著提高了推理能力，并在各种任务上表现出卓越的性能。然而，这种性能提升伴随着推理过程中冗余推理的显著增加，导致计算开销增大和过度思考问题加剧。本文提出了一种新型框架Self-Braking Tuning（SBT），允许模型自我调节其推理过程，解决了过度思考的问题，无需依赖外部控制机制。通过构建基于标准答案的过度思考识别指标，设计出系统的方法来检测冗余推理。该方法可准确识别出推理轨迹中不必要的步骤，并生成训练信号来学习自我调节行为。在此基础上，我们制定了一套完整的策略构建具有自适应推理长度的数据，并引入了创新的刹车提示机制，使模型能够自然学习何时终止合适的推理过程。实验证明，该方法可将令牌消耗减少60%，同时保持与无约束模型相当的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型推理模型（LRMs）通过生成更长的思维链提高了推理能力。</li>
<li>冗余推理的增加导致了计算开销增大和过度思考问题。</li>
<li>现有解决过度思考的方法常依赖外部干预。</li>
<li>提出了Self-Braking Tuning（SBT）框架，允许模型自我调节推理过程，解决过度思考。</li>
<li>构建基于标准答案的过度思考识别指标来检测冗余推理。</li>
<li>引入创新的刹车提示机制使模型自然学习何时终止合适的推理过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14604">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8792102f9e70c1adf1776eb2350fd3b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-242dce1ff04c907e734cc110913fb13c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd4914e69d6cc47ea7a0c87907710469.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e233f3febb94a58037534c8153507db.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Toward-Reliable-Biomedical-Hypothesis-Generation-Evaluating-Truthfulness-and-Hallucination-in-Large-Language-Models"><a href="#Toward-Reliable-Biomedical-Hypothesis-Generation-Evaluating-Truthfulness-and-Hallucination-in-Large-Language-Models" class="headerlink" title="Toward Reliable Biomedical Hypothesis Generation: Evaluating   Truthfulness and Hallucination in Large Language Models"></a>Toward Reliable Biomedical Hypothesis Generation: Evaluating   Truthfulness and Hallucination in Large Language Models</h2><p><strong>Authors:Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang</strong></p>
<p>Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at <a target="_blank" rel="noopener" href="https://github.com/Teddy-XiongGZ/TruthHypo">https://github.com/Teddy-XiongGZ/TruthHypo</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在生物医学等科学学科中显示出巨大潜力，特别是在假设生成方面。它们可以分析大量文献，识别模式，并提出研究方向。然而，评估生成假设的真实性是一个重要挑战，因为验证其准确性通常需要大量时间和资源。此外，LLM中的幻觉问题可能导致生成看似可信但最终错误的假设，破坏其可靠性。为了促进对这些挑战的系统性研究，我们引入了TruthHypo基准测试，用于评估LLM生成真实生物医学假设的能力，以及基于知识的幻觉检测器KnowHD，以评估假设在现有知识中的扎实程度。我们的结果表明，LLM在生成真实假设方面存在困难。通过分析推理步骤中的幻觉，我们证明KnowHD提供的扎根得分是筛选LLM多样输出中的真实假设的有效指标。人类评估进一步验证了KnowHD在识别真实假设和加速科学发现方面的实用性。我们的数据和源代码可在<a target="_blank" rel="noopener" href="https://github.com/Teddy-XiongGZ/TruthHypo%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Teddy-XiongGZ/TruthHypo找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14599v1">PDF</a> Accepted to IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在生物医学等科学领域展现出潜力，尤其在假设生成方面。它们能分析大量文献、识别模式并提出研究方向。然而，评估生成假设的真实性是一大挑战，因为验证其准确性需要大量时间和资源。此外，LLM中的虚构问题可能导致生成看似可信但实际错误的假设，影响可靠性。为应对这些挑战，我们推出了TruthHypo评估基准和KnowHD知识虚构检测器。但LLM在生成真实假设方面仍有困难。通过分析推理步骤中的虚构，我们发现KnowHD提供的接地度分数能有效过滤LLM输出的真实假设。人类评估进一步验证了KnowHD在识别真实假设和加速科学发现方面的实用性。数据和源代码可在<a target="_blank" rel="noopener" href="https://github.com/Teddy-XiongGZ/TruthHypo">https://github.com/Teddy-XiongGZ/TruthHypo</a>获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在生物医学等科学领域有潜力，尤其在假设生成方面能分析文献、识别模式。</li>
<li>评估LLM生成的假设真实性是一大挑战，需要验证其准确性并识别虚构假设。</li>
<li>TruthHypo评估基准用于评估LLM在生成真实假设方面的能力。</li>
<li>KnowHD知识虚构检测器能有效过滤LLM输出的真实假设。</li>
<li>LLM在生成真实假设方面存在困难。</li>
<li>KnowHD的接地度分数是评估假设真实性的有效指标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14599">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aec48770c27ca7547b10c7e7991a1e1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae05957c89c5590d36ceece7aa5b7682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f15fa32d20cb21e2a8264542f83950f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b8fe7f67fe13495701fe32774d9072f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2fc53d303e1ce74985e91947c6d401b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b558bf9db0cc5b6b2dde50d17c56f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-088ddbb5ebc5a8832ec00c3a6c1d28a1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Context-Reasoner-Incentivizing-Reasoning-Capability-for-Contextualized-Privacy-and-Safety-Compliance-via-Reinforcement-Learning"><a href="#Context-Reasoner-Incentivizing-Reasoning-Capability-for-Contextualized-Privacy-and-Safety-Compliance-via-Reinforcement-Learning" class="headerlink" title="Context Reasoner: Incentivizing Reasoning Capability for Contextualized   Privacy and Safety Compliance via Reinforcement Learning"></a>Context Reasoner: Incentivizing Reasoning Capability for Contextualized   Privacy and Safety Compliance via Reinforcement Learning</h2><p><strong>Authors:Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song</strong></p>
<p>While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +17.64% accuracy improvement in safety&#x2F;privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively. </p>
<blockquote>
<p>大型语言模型（LLM）虽然展现出卓越的能力，但也带来了显著的安全和隐私风险。当前的缓解策略往往无法在风险场景中保留上下文推理能力，而过度依赖于敏感模式匹配来保护LLM，这限制了其应用范围。此外，它们忽略了既定的安全和隐私标准，导致法律合规的系统性风险。为了弥补这些空白，我们将安全和隐私问题转化为上下文合规问题，遵循上下文完整性（CI）理论。在CI框架下，我们的模型与三个关键的监管标准保持一致：GDPR、欧盟人工智能法和HIPAA。具体来说，我们采用基于规则的奖励的强化学习（RL）来激励上下文推理能力，同时提高对安全和隐私规范的可合规性。通过广泛的实验，我们证明我们的方法不仅显著提高了法律合规性（在安全&#x2F;隐私基准测试中实现了+17.64%的准确率提升），而且还进一步提高了通用推理能力。对于在多种主题上显著优于其基础模型Qwen2.5-7B-Instruct的强大推理模型OpenThinker-7B，我们的方法在MMLU和LegalBench基准测试上分别提高了+2.05%和+8.98%的准确率，增强了其通用推理能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14585v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）展现出令人瞩目的能力，但同时也带来安全和隐私风险。当前缓解策略常常无法保护上下文推理能力，并过度依赖敏感模式匹配来保护LLM，这限制了其应用范围。遵循语境完整性（CI）理论，将安全和隐私问题转化为上下文化的合规性问题，与三大关键监管标准（GDPR、欧盟人工智能法案和HIPAA）相符。采用基于规则的奖励强化学习，激励上下文推理能力，同时提高遵守安全和隐私规范的能力。实验证明，该方法不仅显著提高法律合规性，还进一步改善了一般推理能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM展现出强大的能力，但也存在安全和隐私风险。</li>
<li>当前缓解策略常常无法同时保护上下文推理能力和应对风险场景。</li>
<li>安全和隐私问题应转化为上下文化的合规性问题。</li>
<li>遵循语境完整性（CI）理论，与三大关键监管标准相符。</li>
<li>采用强化学习激励上下文推理能力的同时提高遵守安全和隐私规范的能力。</li>
<li>方法显著提高法律合规性，并在安全&#x2F;隐私基准测试中实现+17.64%的准确率提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14585">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4db7d96cf7362e201aadebc4ea69aaf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-353b8163a4c131bd156ad9a92bc0f459.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cc3442a46b8dfafb225bfa8424ac7fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-335803ebc1268674293e56a126aa7c7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0c72ab2a84733a72d5d2ce64924715e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea5998feb82fe7d8cc676123848dc041.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Latent-Flow-Transformer"><a href="#Latent-Flow-Transformer" class="headerlink" title="Latent Flow Transformer"></a>Latent Flow Transformer</h2><p><strong>Authors:Yen-Chen Wu, Feng-Ting Liao, Meng-Hsi Chen, Pei-Chen Ho, Farhang Nabiei, Da-shan Shiu</strong></p>
<p>Transformers, the standard implementation for large language models (LLMs), typically consist of tens to hundreds of discrete layers. While more layers can lead to better performance, this approach has been challenged as far from efficient, especially given the superiority of continuous layers demonstrated by diffusion and flow-based models for image generation. We propose the Latent Flow Transformer (LFT), which replaces a block of layers with a single learned transport operator trained via flow matching, offering significant compression while maintaining compatibility with the original architecture. Additionally, we address the limitations of existing flow-based methods in \textit{preserving coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M model, LFT trained with flow matching compresses 6 of 24 layers and outperforms directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529), demonstrating the feasibility of this design. When trained with FW, LFT further distills 12 layers into one while reducing the KL to 0.736 surpassing that from skipping 3 layers (0.932), significantly narrowing the gap between autoregressive and flow-based generation paradigms. </p>
<blockquote>
<p>Transformer是大规模语言模型（LLM）的标准实现方式，通常由数十至数百个独立的层组成。虽然更多的层可以提高性能，但这种做法在效率上受到挑战，特别是考虑到扩散模型和基于流的模型在图像生成方面的连续层表现出的优越性。我们提出了潜流Transformer（LFT），它通过用一个单一的学习传输算子替换一系列层，该算子通过流匹配进行训练，可在保持与原始架构兼容的同时实现显著压缩。此外，我们通过引入流步行（FW）算法，解决了现有基于流的方法在保持耦合方面的局限性。在Pythia-410M模型上，通过流匹配训练的LFT压缩了24层中的6层，并且表现优于直接跳过两层（LM logits的KL散度从0.529降至0.407），证明了这种设计的可行性。在使用FW进行训练时，LFT进一步将12层蒸馏为一层，同时将KL值降至0.736，超过了跳过三层的结果（即KL散度大于或高于无流式跳层）（约比跳三层小），极大地缩小了自回归生成模式和基于流的生成模式之间的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14513v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对大型语言模型（LLM）的标准实现通常为数十至数百个离散层。虽然更多层能提高性能，但效率较低。为此，我们提出潜流转换器（LFT），通过流匹配训练学习传输算子，将一组层替换为单个层，既保持原架构兼容性又实现显著压缩。此外，为解决现有流模型的耦合保存限制，我们引入流步行（FW）算法。在Pythia-410M模型上，LFT通过流匹配压缩6层并保持高性能，展示该设计可行性。使用FW训练的LFT能进一步将12层蒸馏为1层并缩小与自回归和流生成范式之间的差距。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）通常包含数十至数百个离散层，更多层虽能提高性能但效率较低。</li>
<li>潜流转换器（LFT）通过训练学习传输算子，将一组层替换为单个层，以实现显著压缩并保持原架构兼容性。</li>
<li>LFT采用流匹配进行训练，在Pythia-410M模型上展示了其有效性。</li>
<li>引入流步行（FW）算法以解决现有流模型的耦合保存限制。</li>
<li>LFT在压缩层数的同时保持了高性能，展示了其设计的优越性。</li>
<li>使用FW训练的LFT能够进一步压缩模型并缩小与自回归和流生成范式之间的差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14513">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3710d29c60de34c45284a7179eec1e81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-423048a320bfbac67ebaf39c96e5e165.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09c31f17eb7b03cee4329d2ba484cd82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48b3db511db2f73a7d19fbece13796df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a80a1e1bee17590aa6f07c4bb70549b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d8e9699c72aedfea3516e75dd3a28a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4f78ab74b471efa1e9d04fbd779f42d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Speculative-Decoding-Reimagined-for-Multimodal-Large-Language-Models"><a href="#Speculative-Decoding-Reimagined-for-Multimodal-Large-Language-Models" class="headerlink" title="Speculative Decoding Reimagined for Multimodal Large Language Models"></a>Speculative Decoding Reimagined for Multimodal Large Language Models</h2><p><strong>Authors:Luxi Lin, Zhihang Lin, Zhanpeng Zeng, Rongrong Ji</strong></p>
<p>This paper introduces Multimodal Speculative Decoding (MSD) to accelerate Multimodal Large Language Models (MLLMs) inference. Speculative decoding has been shown to accelerate Large Language Models (LLMs) without sacrificing accuracy. However, current speculative decoding methods for MLLMs fail to achieve the same speedup as they do for LLMs. To address this, we reimagine speculative decoding specifically for MLLMs. Our analysis of MLLM characteristics reveals two key design principles for MSD: (1) Text and visual tokens have fundamentally different characteristics and need to be processed separately during drafting. (2) Both language modeling ability and visual perception capability are crucial for the draft model. For the first principle, MSD decouples text and visual tokens in the draft model, allowing each to be handled based on its own characteristics. For the second principle, MSD uses a two-stage training strategy: In stage one, the draft model is trained on text-only instruction-tuning datasets to improve its language modeling ability. In stage two, MSD gradually introduces multimodal data to enhance the visual perception capability of the draft model. Experiments show that MSD boosts inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$ for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Lyn-Lucy/MSD">https://github.com/Lyn-Lucy/MSD</a>. </p>
<blockquote>
<p>本文介绍了多模态推测解码（MSD），以加速多模态大型语言模型（MLLMs）的推理。推测解码已证明可以加速大型语言模型（LLMs）的推理，而不会牺牲准确性。然而，当前针对MLLMs的推测解码方法无法达到与LLMs相同的加速效果。为了解决这个问题，我们针对MLLMs重新构想了一种推测解码方法。我们对MLLM特性的分析揭示了MSD的两个关键设计原则：（1）文本和视觉标记具有不同的基本特性，需要在草稿阶段分别处理。（2）语言建模能力和视觉感知能力对于草稿模型都至关重要。根据第一个原则，MSD在草稿模型中解耦文本和视觉标记，允许根据它们各自的特性进行处理。对于第二个原则，MSD采用两阶段训练策略：在第一阶段，草稿模型在纯文本指令微调数据集上进行训练，以提高其语言建模能力。在第二阶段，MSD逐渐引入多模态数据，以提高草稿模型的视觉感知能力。实验表明，MSD在LLaVA-1.5-7B和LLaVA-1.5-13B的多模态基准测试中，推理速度分别提高了2.29倍和2.46倍，证明了其有效性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Lyn-Lucy/MSD">https://github.com/Lyn-Lucy/MSD</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14260v1">PDF</a> 12 pages</p>
<p><strong>摘要</strong><br>  本文提出了针对多模态大型语言模型（MLLMs）的加速推理方法——多模态推测解码（MSD）。通过对MLLM特性的分析，揭示了MSD的两个关键设计原则。一是文本和视觉符号在起草过程中需要分别处理，二是语言建模能力和视觉感知能力对于草案模型至关重要。MSD采用两阶段训练策略，第一阶段仅对文本指令调整数据集进行训练以提高语言建模能力，第二阶段逐步引入多模态数据以增强模型的视觉感知能力。实验表明，MSD在多模态基准测试中提高了推理速度，最高可达LLaVA-1.5-7B的2.29倍和LLaVA-1.5-13B的2.46倍，证明了其有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>本文引入了多模态推测解码（MSD）来加速多模态大型语言模型（MLLMs）的推理过程。</li>
<li>MSD针对MLLMs的特性进行了重新设计，揭示了处理文本和视觉符号需要分别进行的重要性。</li>
<li>MSD强调语言建模能力和视觉感知能力对草案模型的重要性。</li>
<li>MSD采用两阶段训练策略，首先专注于提高语言建模能力，然后逐步引入多模态数据以增强视觉感知能力。</li>
<li>实验结果表明，MSD能有效提高推理速度，最高可达LLaVA-1.5-7B的2.29倍和LLaVA-1.5-13B的2.46倍。</li>
<li>MSD的代码已公开发布在指定GitHub仓库。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14260">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4b47b4556345b7f3c9d811bd5297b8c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ee978cbbceb0a82cf15687af0dfb9e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7ed4853969e6f94edf0f12724a89e6a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04707bbc3bb6be2fa05a12c49448d0f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98b8c172101e1d9ea8677f29ab14eaaa.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MSDformer-Multi-scale-Discrete-Transformer-For-Time-Series-Generation"><a href="#MSDformer-Multi-scale-Discrete-Transformer-For-Time-Series-Generation" class="headerlink" title="MSDformer: Multi-scale Discrete Transformer For Time Series Generation"></a>MSDformer: Multi-scale Discrete Transformer For Time Series Generation</h2><p><strong>Authors:Zhicheng Chen, Shibo Feng, Xi Xiao, Zhong Zhang, Qing Li, Xingyu Gao, Peilin Zhao</strong></p>
<p>Discrete Token Modeling (DTM), which employs vector quantization techniques, has demonstrated remarkable success in modeling non-natural language modalities, particularly in time series generation. While our prior work SDformer established the first DTM-based framework to achieve state-of-the-art performance in this domain, two critical limitations persist in existing DTM approaches: 1) their inability to capture multi-scale temporal patterns inherent to complex time series data, and 2) the absence of theoretical foundations to guide model optimization. To address these challenges, we proposes a novel multi-scale DTM-based time series generation method, called Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale time series tokenizer to learn discrete token representations at multiple scales, which jointly characterize the complex nature of time series data. Subsequently, MSDformer applies a multi-scale autoregressive token modeling technique to capture the multi-scale patterns of time series within the discrete latent space. Theoretically, we validate the effectiveness of the DTM method and the rationality of MSDformer through the rate-distortion theorem. Comprehensive experiments demonstrate that MSDformer significantly outperforms state-of-the-art methods. Both theoretical analysis and experimental results demonstrate that incorporating multi-scale information and modeling multi-scale patterns can substantially enhance the quality of generated time series in DTM-based approaches. The code will be released upon acceptance. </p>
<blockquote>
<p>离散令牌建模（DTM）采用向量量化技术，在非自然语言模态建模中取得了显著的成功，特别是在时间序列生成方面。虽然我们的前期工作SDformer建立了基于DTM的框架，在该领域实现了最先进的性能，但现有DTM方法仍存在两个关键局限性：1）无法捕捉复杂时间序列数据固有的多尺度时间模式；2）缺乏指导模型优化的理论基础。为了解决这些挑战，我们提出了一种新的基于多尺度DTM的时间序列生成方法，称为Multi-Scale Discrete Transformer（MSDformer）。MSDformer采用多尺度时间序列分词器，在多个尺度上学习离散令牌表示，共同表征时间序列数据的复杂性质。然后，MSDformer采用多尺度自回归令牌建模技术，在离散潜在空间内捕捉时间序列的多尺度模式。从理论上讲，我们通过速率失真定理验证了DTM方法的有效性以及MSDformer的合理性。综合实验表明，MSDformer显著优于最先进的方法。理论分析和实验结果均表明，融入多尺度信息和建模多尺度模式可以大大提高DTM方法中生成时间序列的质量。代码将在接受后发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14202v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于向量量化的离散令牌建模（DTM）在非自然语言模态建模中取得了显著成功，特别是在时间序列生成领域。针对现有DTM方法存在的多尺度时间模式捕捉能力不足和理论框架缺失的问题，提出了一种新的多尺度DTM时间序列生成方法——Multi-Scale Discrete Transformer（MSDformer）。MSDformer通过多尺度时间序列分词器学习离散令牌表示，并应用多尺度自回归令牌建模技术捕捉时间序列的多尺度模式。理论分析和实验结果表明，引入多尺度信息和建模多尺度模式可以显著提高DTM方法生成时间序列的质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DTM技术在非自然语言模态建模，特别是时间序列生成中表现出显著成功。</li>
<li>现有DTM方法存在两个关键局限：无法捕捉复杂时间序列数据的内在多尺度时间模式，以及缺乏指导模型优化的理论框架。</li>
<li>MSDformer被提出以解决这些挑战，它通过多尺度时间序列分词器学习离散令牌表示，并应用多尺度自回归令牌建模技术。</li>
<li>MSDformer在理论上通过速率-失真定理验证了DTM方法的有效性以及其自身的合理性。</li>
<li>实验表明，MSDformer显著优于现有方法，表明引入多尺度信息和建模多尺度模式可以大幅提高DTM方法生成时间序列的质量。</li>
<li>代码将在接受后发布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14202">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ca0365817c3e3e0fdce16f9e220dc4ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdadde5eb0fe1aa25ae97909b4d50547.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-561ab683a7531e420b58a1d7ff504915.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe97e7b764583c303af9e0ad39ee4e3f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70326c7591212f1a08bbbbe1e5097b67.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-05-22  ContextAgent Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c3d46f571e781e50ca048e545b9a06f2.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-22  Emerging Properties in Unified Multimodal Pretraining
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23539.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
