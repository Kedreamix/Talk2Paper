<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Training-Free Watermarking for Autoregressive Image Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d38d1501446a78f9354c70defe4087cb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-22-æ›´æ–°"><a href="#2025-05-22-æ›´æ–°" class="headerlink" title="2025-05-22 æ›´æ–°"></a>2025-05-22 æ›´æ–°</h1><h2 id="Training-Free-Watermarking-for-Autoregressive-Image-Generation"><a href="#Training-Free-Watermarking-for-Autoregressive-Image-Generation" class="headerlink" title="Training-Free Watermarking for Autoregressive Image Generation"></a>Training-Free Watermarking for Autoregressive Image Generation</h2><p><strong>Authors:Yu Tong, Zihao Pan, Shuai Yang, Kaiyang Zhou</strong></p>
<p>Invisible image watermarking can protect image ownership and prevent malicious misuse of visual generative models. However, existing generative watermarking methods are mainly designed for diffusion models while watermarking for autoregressive image generation models remains largely underexplored. We propose IndexMark, a training-free watermarking framework for autoregressive image generation models. IndexMark is inspired by the redundancy property of the codebook: replacing autoregressively generated indices with similar indices produces negligible visual differences. The core component in IndexMark is a simple yet effective match-then-replace method, which carefully selects watermark tokens from the codebook based on token similarity, and promotes the use of watermark tokens through token replacement, thereby embedding the watermark without affecting the image quality. Watermark verification is achieved by calculating the proportion of watermark tokens in generated images, with precision further improved by an Index Encoder. Furthermore, we introduce an auxiliary validation scheme to enhance robustness against cropping attacks. Experiments demonstrate that IndexMark achieves state-of-the-art performance in terms of image quality and verification accuracy, and exhibits robustness against various perturbations, including cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG compression. </p>
<blockquote>
<p>éšå¼å›¾åƒæ°´å°æŠ€æœ¯å¯ä»¥ä¿æŠ¤å›¾åƒçš„æ‰€æœ‰æƒï¼Œå¹¶é˜²æ­¢å¯¹è§†è§‰ç”Ÿæˆæ¨¡å‹çš„æ¶æ„æ»¥ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”Ÿæˆæ°´å°æ–¹æ³•ä¸»è¦è®¾è®¡ç”¨äºæ‰©æ•£æ¨¡å‹ï¼Œè€Œå¯¹äºè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ°´å°æŠ€æœ¯ä»é²œæœ‰ç ”ç©¶ã€‚æˆ‘ä»¬æå‡ºäº†IndexMarkï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹æ°´å°æ¡†æ¶ã€‚IndexMarkå—åˆ°ä»£ç æœ¬å†—ä½™å±æ€§çš„å¯å‘ï¼šç”¨ç›¸ä¼¼çš„ç´¢å¼•æ›¿æ¢è‡ªå›å½’ç”Ÿæˆçš„ç´¢å¼•ä¼šäº§ç”Ÿå¾®ä¸è¶³é“çš„è§†è§‰å·®å¼‚ã€‚IndexMarkçš„æ ¸å¿ƒç»„ä»¶æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŒ¹é…æ›¿æ¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®ä»¤ç‰Œç›¸ä¼¼æ€§ï¼Œä»ä»£ç æœ¬ä¸­ç²¾å¿ƒé€‰æ‹©æ°´å°ä»¤ç‰Œï¼Œå¹¶é€šè¿‡ä»¤ç‰Œæ›¿æ¢ä¿ƒè¿›æ°´å°ä»¤ç‰Œçš„ä½¿ç”¨ï¼Œä»è€ŒåµŒå…¥æ°´å°è€Œä¸å½±å“å›¾åƒè´¨é‡ã€‚é€šè¿‡è®¡ç®—ç”Ÿæˆå›¾åƒä¸­æ°´å°ä»¤ç‰Œçš„æ¯”ä¾‹æ¥å®ç°æ°´å°éªŒè¯ï¼Œé€šè¿‡ç´¢å¼•ç¼–ç å™¨è¿›ä¸€æ­¥æé«˜ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¾…åŠ©éªŒè¯æ–¹æ¡ˆï¼Œä»¥æé«˜å¯¹è£å‰ªæ”»å‡»çš„é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒIndexMarkåœ¨å›¾åƒè´¨é‡å’ŒéªŒè¯ç²¾åº¦æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶å¯¹å„ç§æ‰°åŠ¨ï¼ˆåŒ…æ‹¬è£å‰ªã€å™ªå£°ã€é«˜æ–¯æ¨¡ç³Šã€éšæœºæ“¦é™¤ã€é¢œè‰²æŠ–åŠ¨å’ŒJPEGå‹ç¼©ï¼‰å…·æœ‰é²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14673v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºIndexMarkçš„è®­ç»ƒæ— å…³æ°´å°åµŒå…¥æ¡†æ¶ï¼Œé€‚ç”¨äºè‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä»£ç æœ¬çš„å†—ä½™ç‰¹æ€§ï¼Œé€šè¿‡æ›¿æ¢è‡ªå›å½’ç”Ÿæˆçš„ç´¢å¼•æ¥åµŒå…¥æ°´å°ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ä¸å—å½±å“ã€‚IndexMarké‡‡ç”¨åŸºäºä»¤ç‰Œç›¸ä¼¼æ€§çš„åŒ¹é…æ›¿æ¢æ–¹æ³•ï¼Œé€šè¿‡ç²¾å¿ƒé€‰æ‹©æ°´å°ä»¤ç‰Œå¹¶ä¿ƒè¿›ä»¤ç‰Œæ›¿æ¢æ¥åµŒå…¥æ°´å°ã€‚æ°´å°éªŒè¯é€šè¿‡è®¡ç®—ç”Ÿæˆå›¾åƒä¸­æ°´å°ä»¤ç‰Œçš„æ¯”ä¾‹æ¥å®ç°ï¼Œå¹¶ä½¿ç”¨ç´¢å¼•ç¼–ç å™¨æé«˜ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§è¾…åŠ©éªŒè¯æ–¹æ¡ˆï¼Œä»¥æé«˜å¯¹è£å‰ªæ”»å‡»çš„é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒIndexMarkåœ¨å›¾åƒè´¨é‡å’ŒéªŒè¯å‡†ç¡®æ€§æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶å¯¹å„ç§æ‰°åŠ¨å…·æœ‰é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Invisible image watermarking protects image ownership and prevents malicious misuse of visual generative models.</li>
<li>Existing generative watermarking methods are mainly designed for diffusion models, with watermarking for autoregressive image generation models remaining largely unexplored.</li>
<li>IndexMark is a training-free watermarking framework for autoregressive image generation models.</li>
<li>IndexMarkåˆ©ç”¨ä»£ç æœ¬çš„å†—ä½™ç‰¹æ€§ï¼Œé€šè¿‡æ›¿æ¢è‡ªå›å½’ç”Ÿæˆçš„ç´¢å¼•æ¥åµŒå…¥æ°´å°ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ä¸å—å½±å“ã€‚</li>
<li>IndexMarké‡‡ç”¨åŸºäºä»¤ç‰Œç›¸ä¼¼æ€§çš„åŒ¹é…æ›¿æ¢æ–¹æ³•ï¼Œç²¾å¿ƒé€‰æ‹©æ°´å°ä»¤ç‰Œå¹¶ä¿ƒè¿›ä»¤ç‰Œæ›¿æ¢ã€‚</li>
<li>æ°´å°éªŒè¯é€šè¿‡è®¡ç®—ç”Ÿæˆå›¾åƒä¸­æ°´å°ä»¤ç‰Œçš„æ¯”ä¾‹æ¥å®ç°ï¼Œå¹¶ä½¿ç”¨ç´¢å¼•ç¼–ç å™¨æé«˜éªŒè¯ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b76860d748b9628aaf60d92b4bb3253.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e16ceae6bfa51abd4d9c93e1330bde2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d45de584243099a1e009cc634228fb4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21f8e5880866c631f56ca850d4a04709.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dynadiff-Single-stage-Decoding-of-Images-from-Continuously-Evolving-fMRI"><a href="#Dynadiff-Single-stage-Decoding-of-Images-from-Continuously-Evolving-fMRI" class="headerlink" title="Dynadiff: Single-stage Decoding of Images from Continuously Evolving   fMRI"></a>Dynadiff: Single-stage Decoding of Images from Continuously Evolving   fMRI</h2><p><strong>Authors:MarlÃ¨ne Careil, Yohann Benchetrit, Jean-RÃ©mi King</strong></p>
<p>Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding. </p>
<blockquote>
<p>è„‘åˆ°å›¾åƒçš„è§£ç æœ€è¿‘ç”±ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„è¿›æ­¥å’Œå¤§å‹è¶…é«˜åœºåŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰çš„å¯ç”¨æ€§æ‰€æ¨åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºå¤æ‚çš„å¤šé˜¶æ®µç®¡é“å’Œé¢„å¤„ç†æ­¥éª¤ï¼Œè¿™äº›é€šå¸¸ä¼šä½¿è„‘è®°å½•çš„ä¸´æ—¶ç»´åº¦å´©æºƒï¼Œä»è€Œé™åˆ¶äº†æ—¶é—´è§£æçš„è„‘è§£ç å™¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†Dynadiffï¼ˆç”¨äºå›¾åƒé‡å»ºçš„åŠ¨æ€ç¥ç»æ´»åŠ¨æ‰©æ•£ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å•é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»åŠ¨æ€å‘å±•çš„fMRIè®°å½•ä¸­é‡å»ºå›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸‰ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼ŒDynadiffä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ç®€åŒ–äº†è®­ç»ƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ—¶é—´è§£æçš„fMRIä¿¡å·ä¸Šä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜çº§è¯­ä¹‰å›¾åƒé‡å»ºæŒ‡æ ‡ä¸Šï¼ŒåŒæ—¶ä¿æŒåœ¨å´©æºƒæ—¶é—´çš„é¢„å¤„ç†fMRIæ•°æ®ä¸Šçš„ç«äº‰åŠ›ã€‚ç¬¬ä¸‰ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥ç²¾ç¡®åœ°æè¿°å›¾åƒè¡¨ç¤ºåœ¨è„‘æ´»åŠ¨è¿‡ç¨‹ä¸­çš„æ¼”å˜ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œä¸ºæ—¶é—´è§£æçš„è„‘åˆ°å›¾åƒè§£ç å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14556v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Dynadiffï¼ˆåŠ¨æ€ç¥ç»æ´»åŠ¨æ‰©æ•£å›¾åƒé‡å»ºæ¨¡å‹ï¼‰çš„åº”ç”¨ï¼Œè¯¥æ¨¡å‹æ˜¯ä¸€ç§æ–°å‹çš„å•é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œå¯ä»åŠ¨æ€fMRIè®°å½•ä¸­é‡å»ºå›¾åƒã€‚Dynadiffç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨æ—¶é—´è§£æçš„fMRIä¿¡å·ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨é«˜çº§è¯­ä¹‰å›¾åƒé‡å»ºæŒ‡æ ‡ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å…è®¸ç²¾ç¡®æè¿°å¤§è„‘ä¸­å›¾åƒè¡¨ç¤ºéšæ—¶é—´çš„å˜åŒ–ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸ºæ—¶é—´è§£æçš„å¤§è„‘å›¾åƒè§£ç å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Dynadiffæ˜¯ä¸€ä¸ªæ–°å‹çš„å•é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»åŠ¨æ€fMRIè®°å½•ä¸­é‡å»ºå›¾åƒã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDynadiffç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>Dynadiffåœ¨æ—¶é—´è§£æçš„fMRIä¿¡å·ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜çº§è¯­ä¹‰å›¾åƒé‡å»ºæ–¹é¢ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨é¢„å¤„ç†åçš„fMRIæ•°æ®ä¸Šä»ç„¶å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>Dynadiffèƒ½å¤Ÿç²¾ç¡®æè¿°å¤§è„‘ä¸­å›¾åƒè¡¨ç¤ºéšæ—¶é—´çš„å˜åŒ–ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ—¶é—´è§£æçš„å¤§è„‘å›¾åƒè§£ç å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb8bf623321fade05773ad21c5caba3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24d1ddbc09c8ffd5617448f41666c2fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57296d7d852e28028dc8b59efc2c6fa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a31541639e99e7e900dcca98c9948e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c14e54882d8694d7c4bc83579daeda03.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SparC-Sparse-Representation-and-Construction-for-High-Resolution-3D-Shapes-Modeling"><a href="#SparC-Sparse-Representation-and-Construction-for-High-Resolution-3D-Shapes-Modeling" class="headerlink" title="SparC: Sparse Representation and Construction for High-Resolution 3D   Shapes Modeling"></a>SparC: Sparse Representation and Construction for High-Resolution 3D   Shapes Modeling</h2><p><strong>Authors:Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, Bihan Wen</strong></p>
<p>High-fidelity 3D object synthesis remains significantly more challenging than 2D image generation due to the unstructured nature of mesh data and the cubic complexity of dense volumetric grids. Existing two-stage pipelines-compressing meshes with a VAE (using either 2D or 3D supervision), followed by latent diffusion sampling-often suffer from severe detail loss caused by inefficient representations and modality mismatches introduced in VAE. We introduce SparC, a unified framework that combines a sparse deformable marching cubes representation SparseCubes with a novel encoder SparConv-VAE. SparseCubes converts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary topology by scattering signed distance and deformation fields onto a sparse cube, allowing differentiable optimization. SparConv-VAE is the first modality-consistent variational autoencoder built entirely upon sparse convolutional networks, enabling efficient and near-lossless 3D reconstruction suitable for high-resolution generative modeling through latent diffusion. SparC achieves state-of-the-art reconstruction fidelity on challenging inputs, including open surfaces, disconnected components, and intricate geometry. It preserves fine-grained shape details, reduces training and inference cost, and integrates naturally with latent diffusion models for scalable, high-resolution 3D generation. </p>
<blockquote>
<p>é«˜è´¨é‡çš„ä¸‰ç»´ç‰©ä½“åˆæˆç›¸è¾ƒäºäºŒç»´å›¾åƒç”Ÿæˆä»ç„¶æ›´å…·æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç½‘æ ¼æ•°æ®çš„éç»“æ„åŒ–å’Œå¯†é›†ä½“ç§¯ç½‘æ ¼çš„ç«‹æ–¹å¤æ‚æ€§ã€‚ç°æœ‰çš„ä¸¤é˜¶æ®µæµç¨‹â€”â€”ä½¿ç”¨VAEï¼ˆé‡‡ç”¨äºŒç»´æˆ–ä¸‰ç»´ç›‘ç£ï¼‰å‹ç¼©ç½‘æ ¼ï¼Œç„¶åè¿›è¡Œæ½œåœ¨æ‰©æ•£é‡‡æ ·â€”â€”å¸¸å¸¸å› VAEä¸­å¼•å…¥çš„ä½æ•ˆè¡¨ç¤ºå’Œæ¨¡æ€ä¸åŒ¹é…è€Œå¯¼è‡´ä¸¥é‡çš„ç»†èŠ‚æŸå¤±ã€‚æˆ‘ä»¬å¼•å…¥äº†SparCï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†ç¨€ç–å¯å˜å½¢è¡Œè¿›ç«‹æ–¹ä½“è¡¨ç¤ºSparseCubeså’Œæ–°å‹ç¼–ç å™¨SparConv-VAEçš„ç»Ÿä¸€æ¡†æ¶ã€‚SparseCubesé€šè¿‡å°†åŸå§‹ç½‘æ ¼è½¬æ¢ä¸ºé«˜åˆ†è¾¨ç‡ï¼ˆ1024^3ï¼‰ä¸”å…·æœ‰ä»»æ„æ‹“æ‰‘çš„è¡¨é¢ï¼Œé€šè¿‡å°†å¸¦ç¬¦å·è·ç¦»å’Œå˜å½¢åœºæ•£å°„åˆ°ç¨€ç–ç«‹æ–¹ä½“ä¸Šï¼Œä»è€Œå®ç°å¯å¾®ä¼˜åŒ–ã€‚SparConv-VAEæ˜¯å®Œå…¨åŸºäºç¨€ç–å·ç§¯ç½‘ç»œæ„å»ºçš„é¦–ä¸ªæ¨¡æ€ä¸€è‡´å˜åˆ†è‡ªç¼–ç å™¨ï¼Œå¯å®ç°é«˜æ•ˆä¸”è¿‘ä¹æ— æŸçš„ä¸‰ç»´é‡å»ºï¼Œé€‚ç”¨äºé€šè¿‡æ½œåœ¨æ‰©æ•£è¿›è¡Œé«˜åˆ†è¾¨ç‡ç”Ÿæˆå»ºæ¨¡ã€‚SparCåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è¾“å…¥ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºä¿çœŸåº¦ï¼ŒåŒ…æ‹¬å¼€æ”¾è¡¨é¢ã€æ–­å¼€ç»„ä»¶å’Œç²¾ç»†å‡ ä½•ã€‚å®ƒä¿ç•™äº†ç²¾ç»†çš„å½¢çŠ¶ç»†èŠ‚ï¼Œé™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œå¹¶ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹è‡ªç„¶é›†æˆï¼Œå®ç°äº†å¯æ‰©å±•çš„é«˜åˆ†è¾¨ç‡ä¸‰ç»´ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14521v1">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://lizhihao6.github.io/SparC">https://lizhihao6.github.io/SparC</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°æœ‰äºŒç»´å›¾åƒç”ŸæˆæŠ€æœ¯çš„å±€é™æ€§ï¼Œå¦‚æ•°æ®ç½‘æ ¼çš„æœªç»“æ„åŒ–å’Œå¯†åº¦å¤æ‚æ€§å¢åŠ ç­‰æŒ‘æˆ˜ï¼Œåœ¨ä¸‰ç»´ç‰©ä½“åˆæˆé¢†åŸŸçš„ç ”ç©¶ä¾ç„¶å…·æœ‰è¾ƒå¤§çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†SparCæ¡†æ¶ï¼Œç»“åˆäº†SparseCubesè¡¨ç¤ºæ–¹æ³•å’ŒSparConv-VAEç¼–ç å™¨ã€‚SparseCubeså¯å°†åŸå§‹ç½‘æ ¼è½¬åŒ–ä¸ºé«˜åˆ†è¾¨ç‡è¡¨é¢ï¼Œè€ŒSparConv-VAEåˆ™åŸºäºç¨€ç–å·ç§¯ç½‘ç»œæ„å»ºæ¨¡æ€ä¸€è‡´çš„å˜åˆ†è‡ªç¼–ç å™¨ã€‚è¯¥æ¡†æ¶å®ç°äº†é«˜æ•ˆä¸”æ— æŸçš„ä¸‰ç»´é‡å»ºï¼Œé€‚ç”¨äºé«˜ä¿çœŸåº¦çš„ç”Ÿæˆæ¨¡å‹ï¼Œæé«˜äº†ä¸‰ç»´ç‰©ä½“åˆæˆé¢†åŸŸçš„æ€§èƒ½è¡¨ç°ã€‚å…¶çªç ´ç‚¹åœ¨äºæˆåŠŸå¤„ç†å¼€æ”¾è¡¨é¢ã€åˆ†ç¦»ç»„ä»¶å’Œç²¾ç»†å‡ ä½•ç»“æ„ç­‰å¤æ‚è¾“å…¥ï¼Œå¹¶è‡ªç„¶é›†æˆåˆ°æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­ä»¥å®ç°å¯æ‰©å±•çš„é«˜åˆ†è¾¨ç‡ä¸‰ç»´ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é«˜ä¿çœŸä¸‰ç»´ç‰©ä½“åˆæˆç›¸è¾ƒäºäºŒç»´å›¾åƒç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§ï¼Œæºäºç½‘æ ¼æ•°æ®çš„æœªç»“æ„åŒ–å’Œå¯†é›†ä½“ç§¯ç½‘æ ¼çš„å¤æ‚æ€§ã€‚</li>
<li>ç°æœ‰æµç¨‹ä¸­å­˜åœ¨ä¸¥é‡çš„ç»†èŠ‚æŸå¤±é—®é¢˜ï¼ŒåŸå› åœ¨äºè¡¨ç°æ–¹æ³•æ•ˆç‡ä½ä¸‹ä»¥åŠæ¨¡æ€ä¸åŒ¹é…ã€‚</li>
<li>SparCæ¡†æ¶ç»“åˆäº†SparseCubesè¡¨ç¤ºå’ŒSparConv-VAEç¼–ç å™¨æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>SparseCubesè½¬åŒ–åŸå§‹ç½‘æ ¼è‡³é«˜åˆ†è¾¨ç‡è¡¨é¢å¹¶å…·æœ‰ä»»æ„æ‹“æ‰‘ç»“æ„ã€‚</li>
<li>SparConv-VAEæ˜¯é¦–ä¸ªå®Œå…¨åŸºäºç¨€ç–å·ç§¯ç½‘ç»œçš„æ¨¡æ€ä¸€è‡´å˜åˆ†è‡ªç¼–ç å™¨ã€‚</li>
<li>SparCå®ç°äº†é«˜æ•ˆæ— æŸçš„ä¸‰ç»´é‡å»ºï¼Œé€‚ç”¨äºé«˜ä¿çœŸåº¦çš„ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>SparCæ¡†æ¶æˆåŠŸå¤„ç†å¤æ‚è¾“å…¥å¦‚å¼€æ”¾è¡¨é¢ã€åˆ†ç¦»ç»„ä»¶å’Œç²¾ç»†å‡ ä½•ç»“æ„ç­‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c67d7e1ba769c56a1a5ca7362a965c7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d273ffa7e1ec577f5589ad49dd7286f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4d716071da20f4dbd478b339e5346e9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Improving-Compositional-Generation-with-Diffusion-Models-Using-Lift-Scores"><a href="#Improving-Compositional-Generation-with-Diffusion-Models-Using-Lift-Scores" class="headerlink" title="Improving Compositional Generation with Diffusion Models Using Lift   Scores"></a>Improving Compositional Generation with Diffusion Models Using Lift   Scores</h2><p><strong>Authors:Chenning Yu, Sicun Gao</strong></p>
<p>We introduce a novel resampling criterion using lift scores, for improving compositional generation in diffusion models. By leveraging the lift scores, we evaluate whether generated samples align with each single condition and then compose the results to determine whether the composed prompt is satisfied. Our key insight is that lift scores can be efficiently approximated using only the original diffusion model, requiring no additional training or external modules. We develop an optimized variant that achieves relatively lower computational overhead during inference while maintaining effectiveness. Through extensive experiments, we demonstrate that lift scores significantly improved the condition alignment for compositional generation across 2D synthetic data, CLEVR position tasks, and text-to-image synthesis. Our code is available at <a target="_blank" rel="noopener" href="http://github.com/rainorangelemon/complift">http://github.com/rainorangelemon/complift</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä½¿ç”¨æå‡åˆ†æ•°çš„æ–°å‹é‡é‡‡æ ·æ ‡å‡†ï¼Œä»¥æ”¹è¿›æ‰©æ•£æ¨¡å‹ä¸­çš„ç»„åˆç”Ÿæˆã€‚é€šè¿‡åˆ©ç”¨æå‡åˆ†æ•°ï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬æ˜¯å¦ç¬¦åˆæ¯ä¸ªå•ç‹¬çš„æ¡ä»¶ï¼Œç„¶åå°†ç»“æœç»„åˆèµ·æ¥ï¼Œä»¥ç¡®å®šç»„åˆæç¤ºæ˜¯å¦æ»¡è¶³è¦æ±‚ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œä»…ä½¿ç”¨åŸå§‹æ‰©æ•£æ¨¡å‹å°±å¯ä»¥æœ‰æ•ˆåœ°è¿‘ä¼¼æå‡åˆ†æ•°ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•é¢å¤–çš„è®­ç»ƒæˆ–å¤–éƒ¨æ¨¡å—ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¼˜åŒ–è¿‡çš„å˜ä½“ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°äº†è¾ƒä½çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æå‡åˆ†æ•°åœ¨äºŒç»´åˆæˆæ•°æ®ã€CLEVRå®šä½ä»»åŠ¡å’Œæ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­ï¼Œæ˜¾è‘—æé«˜äº†ç»„åˆç”Ÿæˆçš„æ¡ä»¶å¯¹é½æ•ˆæœã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="http://github.com/rainorangelemon/complift%E6%89%BE%E5%88%B0%E3%80%82">http://github.com/rainorangelemon/compliftæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13740v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æå‡åˆ†æ•°ä½œä¸ºé‡é‡‡æ ·æ ‡å‡†çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹ä¸­çš„ç»„åˆç”Ÿæˆæ•ˆæœã€‚ç ”ç©¶çš„å…³é”®åœ¨äºåˆ©ç”¨æå‡åˆ†æ•°è¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬æ˜¯å¦ç¬¦åˆå•ä¸€æ¡ä»¶ï¼Œå¹¶å°†ç»“æœç»„åˆèµ·æ¥åˆ¤æ–­ç»„åˆæç¤ºæ˜¯å¦æ»¡è¶³è¦æ±‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å®ç°äº†ä¼˜åŒ–ç®—æ³•ï¼Œé™ä½äº†æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒæœ‰æ•ˆæ€§ã€‚é€šè¿‡å¤§é‡å®éªŒè¯æ˜ï¼Œæå‡åˆ†æ•°èƒ½æ˜¾è‘—æé«˜äºŒç»´åˆæˆæ•°æ®ã€CLEVRä½ç½®ä»»åŠ¡å’Œæ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„æ¡ä»¶å¯¹é½æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é‡é‡‡æ ·æ ‡å‡†â€”â€”æå‡åˆ†æ•°ï¼Œç”¨äºæ”¹å–„æ‰©æ•£æ¨¡å‹ä¸­çš„ç»„åˆç”Ÿæˆæ•ˆæœã€‚</li>
<li>æå‡åˆ†æ•°ç”¨äºè¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬æ˜¯å¦ç¬¦åˆå•ä¸€æ¡ä»¶ï¼Œå¹¶ç»„åˆç»“æœæ¥åˆ¤æ–­ç»„åˆæç¤ºçš„æ»¡è¶³æƒ…å†µã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨åŸå§‹æ‰©æ•£æ¨¡å‹æœ‰æ•ˆè¿‘ä¼¼æå‡åˆ†æ•°ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¤–éƒ¨æ¨¡å—ã€‚</li>
<li>ç ”ç©¶å®ç°äº†ä¼˜åŒ–ç®—æ³•ï¼Œé™ä½æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€ã€‚</li>
<li>æå‡åˆ†æ•°åœ¨äºŒç»´åˆæˆæ•°æ®ã€CLEVRä½ç½®ä»»åŠ¡å’Œæ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­æ˜¾è‘—æé«˜äº†æ¡ä»¶å¯¹é½æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-48f91651aecef13bfd073254271ea062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb601352cea36c65df1e2ff04831fe26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9775cc942c4b39724d411f90a976bf5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fadb393d93e9024fe7821eebbee35acc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90505589fe6965214af3842f98dcd66c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f76fec77ac13212fb481bb0f213ad001.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f856c3596af2599fbca5cfe38566d461.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-with-Double-Guidance-Generate-with-aggregated-datasets"><a href="#Diffusion-Models-with-Double-Guidance-Generate-with-aggregated-datasets" class="headerlink" title="Diffusion Models with Double Guidance: Generate with aggregated datasets"></a>Diffusion Models with Double Guidance: Generate with aggregated datasets</h2><p><strong>Authors:Yanfeng Yang, Kenji Fukumizu</strong></p>
<p>Creating large-scale datasets for training high-performance generative models is often prohibitively expensive, especially when associated attributes or annotations must be provided. As a result, merging existing datasets has become a common strategy. However, the sets of attributes across datasets are often inconsistent, and their naive concatenation typically leads to block-wise missing conditions. This presents a significant challenge for conditional generative modeling when the multiple attributes are used jointly as conditions, thereby limiting the modelâ€™s controllability and applicability. To address this issue, we propose a novel generative approach, Diffusion Model with Double Guidance, which enables precise conditional generation even when no training samples contain all conditions simultaneously. Our method maintains rigorous control over multiple conditions without requiring joint annotations. We demonstrate its effectiveness in molecular and image generation tasks, where it outperforms existing baselines both in alignment with target conditional distributions and in controllability under missing condition settings. </p>
<blockquote>
<p>åˆ›å»ºç”¨äºè®­ç»ƒé«˜æ€§èƒ½ç”Ÿæˆæ¨¡å‹çš„å¤§è§„æ¨¡æ•°æ®é›†é€šå¸¸æˆæœ¬é«˜æ˜‚ï¼Œå°¤å…¶æ˜¯å½“éœ€è¦æä¾›ç›¸å…³å±æ€§æˆ–æ³¨é‡Šæ—¶ã€‚å› æ­¤ï¼Œåˆå¹¶ç°æœ‰æ•°æ®é›†å·²æˆä¸ºä¸€ç§å¸¸è§ç­–ç•¥ã€‚ç„¶è€Œï¼Œå„ä¸ªæ•°æ®é›†ä¸­çš„å±æ€§é›†å¾€å¾€ä¸ä¸€è‡´ï¼Œå…¶ç®€å•æ‹¼æ¥é€šå¸¸ä¼šå¯¼è‡´å—çŠ¶ç¼ºå¤±æ¡ä»¶ã€‚å½“å¤šä¸ªå±æ€§è¢«è”åˆç”¨ä½œæ¡ä»¶æ—¶ï¼Œè¿™ä¸ºæ¡ä»¶ç”Ÿæˆæ¨¡å‹å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„å¯æ§æ€§å’Œé€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç”Ÿæˆæ–¹æ³•â€”â€”åŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ã€‚å³ä½¿åœ¨æ²¡æœ‰è®­ç»ƒæ ·æœ¬åŒæ—¶åŒ…å«æ‰€æœ‰æ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿèƒ½å®ç°ç²¾ç¡®çš„æ¡ä»¶ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸éœ€è¦è”åˆæ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œå¯¹å¤šä¸ªæ¡ä»¶è¿›è¡Œä¸¥æ ¼çš„æ§åˆ¶ã€‚æˆ‘ä»¬åœ¨åˆ†å­å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ— è®ºæ˜¯åœ¨ä¸ç›®æ ‡æ¡ä»¶åˆ†å¸ƒçš„å¯¹é½è¿˜æ˜¯åœ¨ç¼ºå¤±æ¡ä»¶ä¸‹çš„å¯æ§æ€§æ–¹é¢ï¼Œå®ƒéƒ½ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13213v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒé«˜æ€§èƒ½ç”Ÿæˆæ¨¡å‹éœ€è¦å¤§é‡æ•°æ®é›†ï¼Œåˆå¹¶ç°æœ‰æ•°æ®é›†æˆä¸ºäº†ä¸€ç§å¸¸è§ç­–ç•¥ã€‚ç„¶è€Œï¼Œä¸åŒæ•°æ®é›†ä¸­çš„å±æ€§é›†å¾€å¾€ä¸ä¸€è‡´ï¼Œç›´æ¥åˆå¹¶ä¼šå¯¼è‡´å—çŠ¶ç¼ºå¤±æ¡ä»¶ï¼Œå¯¹è”åˆå¤šä¸ªå±æ€§ä½œä¸ºæ¡ä»¶çš„æ¡ä»¶ç”Ÿæˆå»ºæ¨¡æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å¯æ§æ€§å’Œé€‚ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„ç”Ÿæˆæ–¹æ³•â€”â€”åŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ï¼Œå³ä½¿åœ¨æ²¡æœ‰åŒæ—¶åŒ…å«æ‰€æœ‰æ¡ä»¶çš„è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å®ç°ç²¾ç¡®çš„æ¡ä»¶ç”Ÿæˆï¼Œæ— éœ€è”åˆæ³¨é‡Šå³å¯å¯¹å¤šä¸ªæ¡ä»¶è¿›è¡Œä¸¥æ ¼æŠŠæ§ã€‚æˆ‘ä»¬åœ¨åˆ†å­å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œè¯¥æ–¹æ³•åœ¨å¯¹é½ç›®æ ‡æ¡ä»¶åˆ†å¸ƒå’Œç¼ºå¤±æ¡ä»¶è®¾ç½®ä¸‹çš„å¯æ§æ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒé«˜æ€§èƒ½ç”Ÿæˆæ¨¡å‹éœ€è¦å¤§é‡æ•°æ®é›†ï¼Œåˆå¹¶ç°æœ‰æ•°æ®é›†æˆä¸ºäº†ä¸€ç§å¸¸è§ç­–ç•¥ï¼Œä½†å±æ€§ä¸ä¸€è‡´æ€§é—®é¢˜äºŸå¾…è§£å†³ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†åˆå¹¶ä¼šå¯¼è‡´å—çŠ¶ç¼ºå¤±æ¡ä»¶ï¼Œå¯¹æ¡ä»¶ç”Ÿæˆå»ºæ¨¡æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>åŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹èƒ½åœ¨ç¼ºå¤±æ¡ä»¶ä¸‹å®ç°ç²¾ç¡®ç”Ÿæˆï¼Œæ— éœ€è”åˆæ³¨é‡Šã€‚</li>
<li>åŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹åœ¨åˆ†å­å’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¯¹é½ç›®æ ‡æ¡ä»¶åˆ†å¸ƒæ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>åœ¨ç¼ºå¤±æ¡ä»¶è®¾ç½®ä¸‹ï¼Œè¯¥æ–¹æ³•çš„å¯æ§æ€§è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºæé«˜ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-11266cd5f229f7659546567524c0865b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-943474d2eeb645bea92707aa14cc1b65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56ac1240e59c05a9ca00bb1be05c6a59.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Higher-fidelity-perceptual-image-and-video-compression-with-a-latent-conditioned-residual-denoising-diffusion-model"><a href="#Higher-fidelity-perceptual-image-and-video-compression-with-a-latent-conditioned-residual-denoising-diffusion-model" class="headerlink" title="Higher fidelity perceptual image and video compression with a latent   conditioned residual denoising diffusion model"></a>Higher fidelity perceptual image and video compression with a latent   conditioned residual denoising diffusion model</h2><p><strong>Authors:Jonas Brenig, Radu Timofte</strong></p>
<p>Denoising diffusion models achieved impressive results on several image generation tasks often outperforming GAN based models. Recently, the generative capabilities of diffusion models have been employed for perceptual image compression, such as in CDC. A major drawback of these diffusion-based methods is that, while producing impressive perceptual quality images they are dropping in fidelity&#x2F;increasing the distortion to the original uncompressed images when compared with other traditional or learned image compression schemes aiming for fidelity. In this paper, we propose a hybrid compression scheme optimized for perceptual quality, extending the approach of the CDC model with a decoder network in order to reduce the impact on distortion metrics such as PSNR. After using the decoder network to generate an initial image, optimized for distortion, the latent conditioned diffusion model refines the reconstruction for perceptual quality by predicting the residual. On standard benchmarks, we achieve up to +2dB PSNR fidelity improvements while maintaining comparable LPIPS and FID perceptual scores when compared with CDC. Additionally, the approach is easily extensible to video compression, where we achieve similar results. </p>
<blockquote>
<p>é™å™ªæ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œé€šå¸¸è¶…è¶Šäº†åŸºäºGANçš„æ¨¡å‹ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å·²è¢«åº”ç”¨äºæ„ŸçŸ¥å›¾åƒå‹ç¼©ï¼Œå¦‚CDCã€‚è¿™äº›åŸºäºæ‰©æ•£çš„æ–¹æ³•çš„ä¸€ä¸ªä¸»è¦ç¼ºç‚¹æ˜¯ï¼Œè™½ç„¶å®ƒä»¬èƒ½å¤Ÿäº§ç”Ÿä»¤äººå°è±¡æ·±åˆ»çš„æ„ŸçŸ¥è´¨é‡å›¾åƒï¼Œä½†åœ¨ä¸å…¶ä»–æ—¨åœ¨ä¿æŒå¿ è¯šåº¦çš„ä¼ ç»Ÿæˆ–å­¦ä¹ å›¾åƒå‹ç¼©æ–¹æ¡ˆç›¸æ¯”æ—¶ï¼Œå®ƒä»¬åœ¨ä¿çœŸåº¦æ–¹é¢æœ‰æ‰€ä¸‹é™æˆ–å¯¹åŸå§‹æœªå‹ç¼©å›¾åƒçš„å¤±çœŸå¢åŠ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹æ„ŸçŸ¥è´¨é‡ä¼˜åŒ–çš„æ··åˆå‹ç¼©æ–¹æ¡ˆï¼Œé€šè¿‡é‡‡ç”¨è§£ç å™¨ç½‘ç»œæ‰©å±•CDCæ¨¡å‹çš„æ–¹æ³•ï¼Œä»¥å‡å°‘å¯¹PSNRç­‰å¤±çœŸæŒ‡æ ‡çš„å½±å“ã€‚ä½¿ç”¨è§£ç å™¨ç½‘ç»œç”Ÿæˆåˆå§‹å›¾åƒä»¥ä¼˜åŒ–å¤±çœŸåï¼Œæ½œåœ¨çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹é€šè¿‡é¢„æµ‹æ®‹å·®æ¥å®Œå–„é‡å»ºçš„æ„ŸçŸ¥è´¨é‡ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸CDCç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†é«˜è¾¾+2dBçš„PSNRä¿çœŸåº¦æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„LPIPSå’ŒFIDæ„ŸçŸ¥åˆ†æ•°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯è½»æ¾æ‰©å±•åˆ°è§†é¢‘å‹ç¼©ï¼Œå¹¶åœ¨è§†é¢‘å‹ç¼©ä¸­å–å¾—ç±»ä¼¼çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13152v1">PDF</a> Accepted at AIM Workshop 2024 at ECCV 2024</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¸¸å¸¸è¶…è¶ŠåŸºäºGANçš„æ¨¡å‹ã€‚è¿‘æœŸï¼Œå…¶ç”Ÿæˆèƒ½åŠ›è¢«åº”ç”¨äºæ„ŸçŸ¥å›¾åƒå‹ç¼©ï¼Œå¦‚CDCã€‚ä½†æ‰©æ•£æ¨¡å‹çš„ä¸€ä¸ªä¸»è¦ç¼ºç‚¹æ˜¯ï¼Œåœ¨ç”Ÿæˆå…·æœ‰æ„ŸçŸ¥è´¨é‡çš„å›¾åƒæ—¶ï¼Œä¸è¿½æ±‚ä¿çœŸåº¦çš„ä¼ ç»Ÿæˆ–å­¦ä¹ å›¾åƒå‹ç¼©æ–¹æ¡ˆç›¸æ¯”ï¼Œå®ƒä»¬ä¼šé™ä½å›¾åƒä¿çœŸåº¦æˆ–å¢åŠ å¤±çœŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ„ŸçŸ¥è´¨é‡ä¼˜åŒ–çš„æ··åˆå‹ç¼©æ–¹æ¡ˆï¼Œé€šè¿‡æ‰©å±•CDCæ¨¡å‹çš„è§£ç ç½‘ç»œæ¥å‡å°‘å¤±çœŸåº¦é‡çš„å½±å“ï¼Œå¦‚PSNRã€‚ä½¿ç”¨è§£ç ç½‘ç»œç”Ÿæˆåˆå§‹å›¾åƒåï¼Œé€šè¿‡é¢„æµ‹æ®‹ç•™ä¿¡æ¯æ¥ä¼˜åŒ–é‡æ„çš„æ„ŸçŸ¥è´¨é‡ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸CDCç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†é«˜è¾¾+2dBçš„PSNRä¿çœŸåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒå¯æ¯”çš„LPIPSå’ŒFIDæ„ŸçŸ¥åˆ†æ•°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯è½»æ¾æ‰©å±•åˆ°è§†é¢‘å‹ç¼©ï¼Œå¹¶å®ç°äº†ç±»ä¼¼çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¸¸è¶…è¶ŠåŸºäºGANçš„æ¨¡å‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¿‘æœŸè¢«åº”ç”¨äºæ„ŸçŸ¥å›¾åƒå‹ç¼©ï¼Œå¦‚CDCã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¿½æ±‚ä¿çœŸåº¦æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œä¸å…¶ä»–è¿½æ±‚ä¿çœŸåº¦çš„å›¾åƒå‹ç¼©æ–¹æ¡ˆç›¸æ¯”ï¼Œå¯èƒ½ä¼šé™ä½å›¾åƒè´¨é‡æˆ–å¢åŠ å¤±çœŸã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆå‹ç¼©æ–¹æ¡ˆï¼Œé€šè¿‡æ‰©å±•CDCæ¨¡å‹çš„è§£ç ç½‘ç»œæ¥ä¼˜åŒ–æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>è§£ç ç½‘ç»œç”¨äºç”Ÿæˆåˆå§‹å›¾åƒåï¼Œé€šè¿‡é¢„æµ‹æ®‹ç•™ä¿¡æ¯ä¼˜åŒ–é‡æ„çš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>ä¸CDCç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¾ƒé«˜çš„PSNRä¿çœŸåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-798d71d4d2e14d5be84489ca87cf9bbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c48b075bbc8c9039ad071c4fca49be9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8157347d3a9aad8a8fc4632e32b81864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2de1590f7cdc3ac735530a08d72b030a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LatentINDIGO-An-INN-Guided-Latent-Diffusion-Algorithm-for-Image-Restoration"><a href="#LatentINDIGO-An-INN-Guided-Latent-Diffusion-Algorithm-for-Image-Restoration" class="headerlink" title="LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image   Restoration"></a>LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image   Restoration</h2><p><strong>Authors:Di You, Daniel Siromani, Pier Luigi Dragotti</strong></p>
<p>There is a growing interest in the use of latent diffusion models (LDMs) for image restoration (IR) tasks due to their ability to model effectively the distribution of natural images. While significant progress has been made, there are still key challenges that need to be addressed. First, many approaches depend on a predefined degradation operator, making them ill-suited for complex or unknown degradations that deviate from standard analytical models. Second, many methods struggle to provide a stable guidance in the latent space and finally most methods convert latent representations back to the pixel domain for guidance at every sampling iteration, which significantly increases computational and memory overhead. To overcome these limitations, we introduce a wavelet-inspired invertible neural network (INN) that simulates degradations through a forward transform and reconstructs lost details via the inverse transform. We further integrate this design into a latent diffusion pipeline through two proposed approaches: LatentINDIGO-PixelINN, which operates in the pixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space to reduce complexity. Both approaches alternate between updating intermediate latent variables under the guidance of our INN and refining the INN forward model to handle unknown degradations. In addition, a regularization step preserves the proximity of latent variables to the natural image manifold. Experiments demonstrate that our algorithm achieves state-of-the-art performance on synthetic and real-world low-quality images, and can be readily adapted to arbitrary output sizes. </p>
<blockquote>
<p>å¯¹äºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨å›¾åƒæ¢å¤ï¼ˆIRï¼‰ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œäººä»¬è¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹è‡ªç„¶å›¾åƒçš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚è™½ç„¶å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ä»å­˜åœ¨éœ€è¦è§£å†³çš„å…³é”®æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œè®¸å¤šæ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰çš„é€€åŒ–ç®—å­ï¼Œä½¿å¾—å®ƒä»¬ä¸é€‚åˆå¤æ‚æˆ–æœªçŸ¥çš„é€€åŒ–ï¼Œè¿™äº›é€€åŒ–ä¸æ ‡å‡†åˆ†ææ¨¡å‹æœ‰åå·®ã€‚å…¶æ¬¡ï¼Œè®¸å¤šæ–¹æ³•éš¾ä»¥åœ¨æ½œåœ¨ç©ºé—´æä¾›ç¨³å®šçš„æŒ‡å¯¼ï¼Œè€Œä¸”å¤§å¤šæ•°æ–¹æ³•åœ¨æ¯ä¸ªé‡‡æ ·è¿­ä»£ä¸­å°†æ½œåœ¨è¡¨ç¤ºè½¬å›åƒç´ åŸŸè¿›è¡ŒæŒ‡å¯¼ï¼Œè¿™æ˜¾è‘—å¢åŠ äº†è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å—å°æ³¢å¯å‘çš„å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNï¼‰ï¼Œå®ƒé€šè¿‡æ­£å‘å˜æ¢æ¨¡æ‹Ÿé€€åŒ–ï¼Œå¹¶é€šè¿‡åå‘å˜æ¢é‡å»ºä¸¢å¤±çš„ç»†èŠ‚ã€‚æˆ‘ä»¬å°†è¿™ç§è®¾è®¡è¿›ä¸€æ­¥é›†æˆåˆ°æ½œåœ¨æ‰©æ•£ç®¡é“ä¸­ï¼Œé€šè¿‡ä¸¤ç§æå‡ºçš„æ–¹æ³•ï¼šLatentINDIGO-PixelINNï¼Œå®ƒåœ¨åƒç´ åŸŸä¸­è¿è¡Œï¼›LatentINDIGO-LatentINNï¼Œå®ƒå®Œå…¨åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œï¼Œä»¥é™ä½å¤æ‚æ€§ã€‚ä¸¤ç§æ–¹æ³•äº¤æ›¿è¿›è¡Œï¼Œåœ¨INNçš„æŒ‡å¯¼ä¸‹æ›´æ–°ä¸­é—´æ½œåœ¨å˜é‡ï¼Œå¹¶æ”¹è¿›INNå‰å‘æ¨¡å‹ä»¥å¤„ç†æœªçŸ¥é€€åŒ–ã€‚æ­¤å¤–ï¼Œæ­£åˆ™åŒ–æ­¥éª¤ä¿æŒäº†æ½œåœ¨å˜é‡ä¸è‡ªç„¶å›¾åƒæµå½¢çš„æ¥è¿‘ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„ä½è´¨é‡å›¾åƒä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°é€‚åº”ä»»æ„è¾“å‡ºå¤§å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12935v1">PDF</a> Submitted to IEEE Transactions on Image Processing (TIP)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨å›¾åƒæ¢å¤ï¼ˆIRï¼‰ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œæå‡ºäº†ä¸€ç§å—å°æ³¢å¯å‘çš„å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNï¼‰ï¼Œé€šè¿‡æ­£å‘å˜æ¢æ¨¡æ‹Ÿé€€åŒ–ï¼Œå¹¶é€šè¿‡åå‘å˜æ¢é‡å»ºä¸¢å¤±çš„ç»†èŠ‚ã€‚æ–‡ç« è¿˜ä»‹ç»äº†ä¸¤ç§å°†è¿™ä¸€è®¾è®¡èå…¥æ½œåœ¨æ‰©æ•£ç®¡é“çš„æ–¹æ³•ï¼šLatentINDIGO-PixelINNï¼ˆåœ¨åƒç´ åŸŸæ“ä½œï¼‰å’ŒLatentINDIGO-LatentINNï¼ˆå®Œå…¨åœ¨æ½œåœ¨ç©ºé—´å†…æ“ä½œï¼Œä»¥é™ä½å¤æ‚æ€§ï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æ›´æ–°ä¸­é—´æ½œåœ¨å˜é‡å¹¶æ”¹è¿›INNæ­£å‘æ¨¡å‹æ¥å¤„ç†æœªçŸ¥é€€åŒ–ï¼ŒåŒæ—¶å®ç°å›¾åƒè´¨é‡çš„æå‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„ä½è´¨é‡å›¾åƒä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å¯è½»æ¾é€‚åº”ä»»æ„è¾“å‡ºå¤§å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå› å…¶èƒ½æœ‰æ•ˆæ¨¡æ‹Ÿè‡ªç„¶å›¾åƒçš„åˆ†å¸ƒã€‚</li>
<li>å½“å‰LDMåœ¨å›¾åƒæ¢å¤ä¸­é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼šä¾èµ–é¢„è®¾çš„é€€åŒ–ç®—å­ã€åœ¨æ½œåœ¨ç©ºé—´æä¾›ç¨³å®šæŒ‡å¯¼çš„å›°éš¾ä»¥åŠåœ¨æ¯æ¬¡é‡‡æ ·è¿­ä»£ä¸­å°†æ½œåœ¨è¡¨ç¤ºè½¬å›åƒç´ åŸŸå¯¼è‡´çš„è®¡ç®—å’Œå†…å­˜è´Ÿæ‹…ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå°æ³¢çš„å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNï¼‰ï¼Œé€šè¿‡æ­£å‘å˜æ¢æ¨¡æ‹Ÿå›¾åƒé€€åŒ–ï¼Œå¹¶é€šè¿‡åå‘å˜æ¢æ¢å¤ç»†èŠ‚ã€‚</li>
<li>ä»‹ç»äº†ä¸¤ç§å°†INNèå…¥æ½œåœ¨æ‰©æ•£ç®¡é“çš„æ–¹æ³•ï¼šLatentINDIGO-PixelINNå’ŒLatentINDIGO-LatentINNï¼Œåˆ†åˆ«æ“ä½œäºåƒç´ åŸŸå’Œæ½œåœ¨ç©ºé—´ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ›´æ–°ä¸­é—´æ½œåœ¨å˜é‡ã€æ”¹è¿›INNæ­£å‘æ¨¡å‹ä»¥åŠæ·»åŠ æ­£åˆ™åŒ–æ­¥éª¤æ¥å¤„ç†æœªçŸ¥é€€åŒ–ï¼Œå¹¶æå‡å›¾åƒè´¨é‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„ä½è´¨é‡å›¾åƒä¿®å¤ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6bc83e4d4c214ca38e1fd309dbd9d602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b36e5907dfd840998b4921b7d297a506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fa6838cf8cd73ef5707045578ebdeaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27b020dadf2076f616bcb3ae22942302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a88e33558e2294fd877f8cd3bd1f3496.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DragLoRA-Online-Optimization-of-LoRA-Adapters-for-Drag-based-Image-Editing-in-Diffusion-Model"><a href="#DragLoRA-Online-Optimization-of-LoRA-Adapters-for-Drag-based-Image-Editing-in-Diffusion-Model" class="headerlink" title="DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image   Editing in Diffusion Model"></a>DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image   Editing in Diffusion Model</h2><p><strong>Authors:Siwei Xia, Li Sun, Tiantian Sun, Qingli Li</strong></p>
<p>Drag-based editing within pretrained diffusion model provides a precise and flexible way to manipulate foreground objects. Traditional methods optimize the input feature obtained from DDIM inversion directly, adjusting them iteratively to guide handle points towards target locations. However, these approaches often suffer from limited accuracy due to the low representation ability of the feature in motion supervision, as well as inefficiencies caused by the large search space required for point tracking. To address these limitations, we present DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation) adapters into the drag-based editing pipeline. To enhance the training of LoRA adapters, we introduce an additional denoising score distillation loss which regularizes the online model by aligning its output with that of the original model. Additionally, we improve the consistency of motion supervision by adapting the input features using the updated LoRA, giving a more stable and accurate input feature for subsequent operations. Building on this, we design an adaptive optimization scheme that dynamically toggles between two modes, prioritizing efficiency without compromising precision. Extensive experiments demonstrate that DragLoRA significantly enhances the control precision and computational efficiency for drag-based image editing. The Codes of DragLoRA are available at: <a target="_blank" rel="noopener" href="https://github.com/Sylvie-X/DragLoRA">https://github.com/Sylvie-X/DragLoRA</a>. </p>
<blockquote>
<p>åŸºäºæ‹–åŠ¨çš„ç¼–è¾‘åœ¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…éƒ¨æä¾›äº†ä¸€ç§ç²¾ç¡®ä¸”çµæ´»çš„æ–¹å¼æ¥æ“ä½œå‰æ™¯å¯¹è±¡ã€‚ä¼ ç»Ÿæ–¹æ³•ç›´æ¥ä¼˜åŒ–ä»DDIMåæ¼”è·å¾—çš„è¾“å…¥ç‰¹å¾ï¼Œé€šè¿‡è¿­ä»£è°ƒæ•´å®ƒä»¬æ¥å¼•å¯¼æ§åˆ¶ç‚¹æœå‘ç›®æ ‡ä½ç½®ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å—é™äºç‰¹å¾åœ¨è¿åŠ¨ç›‘ç£ä¸‹çš„è¡¨ç¤ºèƒ½åŠ›æœ‰é™ï¼Œä»¥åŠç‚¹è·Ÿè¸ªæ‰€éœ€çš„å¤§æœç´¢ç©ºé—´å¯¼è‡´çš„æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DragLoRAï¼Œè¿™æ˜¯ä¸€ä¸ªå°†LoRAï¼ˆä½ç§©é€‚åº”ï¼‰é€‚é…å™¨é›†æˆåˆ°åŸºäºæ‹–åŠ¨çš„ç¼–è¾‘ç®¡é“ä¸­çš„æ–°å‹æ¡†æ¶ã€‚ä¸ºäº†æé«˜LoRAé€‚é…å™¨çš„è®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å¤–çš„å»å™ªåˆ†æ•°è’¸é¦æŸå¤±ï¼Œé€šè¿‡ä½¿åœ¨çº¿æ¨¡å‹çš„è¾“å‡ºä¸åŸå§‹æ¨¡å‹çš„è¾“å‡ºå¯¹é½æ¥è§„èŒƒåœ¨çº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ›´æ–°çš„LoRAé€‚é…è¾“å…¥ç‰¹å¾ï¼Œæé«˜äº†è¿åŠ¨ç›‘ç£çš„ä¸€è‡´æ€§ï¼Œä¸ºåç»­æ“ä½œæä¾›äº†æ›´ç¨³å®šå’Œå‡†ç¡®çš„è¾“å…¥ç‰¹å¾ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥åŠ¨æ€åˆ‡æ¢ä¸¤ç§æ¨¡å¼ï¼Œä»¥ä¼˜å…ˆè€ƒè™‘æ•ˆç‡è€Œä¸æŸå¤±ç²¾åº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDragLoRAæ˜¾è‘—æé«˜äº†åŸºäºæ‹–åŠ¨çš„å›¾åƒç¼–è¾‘çš„æ§åˆ¶ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚DragLoRAçš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Sylvie-X/DragLoRA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Sylvie-X/DragLoRAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12427v2">PDF</a> Accepted by ICML2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„æ‹–æ‹½ç¼–è¾‘æ–¹æ³•æä¾›äº†ä¸€ç§ç²¾ç¡®ä¸”çµæ´»çš„æ“ä½œå‰æ™¯å¯¹è±¡çš„æ–¹å¼ã€‚ä¼ ç»Ÿæ–¹æ³•ç›´æ¥ä¼˜åŒ–ä»DDIMåæ¼”è·å¾—çš„è¾“å…¥ç‰¹å¾ï¼Œé€šè¿‡è¿­ä»£è°ƒæ•´å¼•å¯¼å¥æŸ„ç‚¹è‡³ç›®æ ‡ä½ç½®ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å—é™äºè¿åŠ¨ç›‘ç£ä¸­ç‰¹å¾çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œä¸”ç”±äºç‚¹è·Ÿè¸ªæ‰€éœ€çš„å¤§æœç´¢ç©ºé—´è€Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºDragLoRAæ¡†æ¶ï¼Œå°†LoRAï¼ˆä½ç§©é€‚é…ï¼‰é€‚é…å™¨é›†æˆåˆ°åŸºäºæ‹–æ‹½çš„ç¼–è¾‘æµç¨‹ä¸­ã€‚ä¸ºå¢å¼ºLoRAé€‚é…å™¨çš„è®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥é¢å¤–çš„å»å™ªåˆ†æ•°è’¸é¦æŸå¤±ï¼Œé€šè¿‡ä½¿åœ¨çº¿æ¨¡å‹çš„è¾“å‡ºä¸åŸå§‹æ¨¡å‹çš„è¾“å‡ºå¯¹é½æ¥è§„èŒƒåœ¨çº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ›´æ–°çš„LoRAé€‚é…è¾“å…¥ç‰¹å¾ï¼Œæé«˜è¿åŠ¨ç›‘ç£çš„ä¸€è‡´æ€§ï¼Œä¸ºåç»­æ“ä½œæä¾›æ›´ç¨³å®šå’Œå‡†ç¡®çš„è¾“å…¥ç‰¹å¾ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ¡ˆï¼Œèƒ½åœ¨æ•ˆç‡å’Œç²¾åº¦ä¹‹é—´åŠ¨æ€åˆ‡æ¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDragLoRAæ˜¾è‘—æé«˜äº†åŸºäºæ‹–æ‹½çš„å›¾åƒç¼–è¾‘çš„æ§åˆ¶ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥æ‹–æ‹½ç¼–è¾‘ï¼Œå®ç°ç²¾ç¡®çµæ´»çš„å¯¹è±¡æ“ä½œã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¼˜åŒ–è¾“å…¥ç‰¹å¾å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ä¸è¶³å’Œæœç´¢ç©ºé—´å¤§å¯¼è‡´çš„æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>æå‡ºDragLoRAæ¡†æ¶ï¼Œé›†æˆLoRAé€‚é…å™¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å¼•å…¥å»å™ªåˆ†æ•°è’¸é¦æŸå¤±ä»¥è§„èŒƒæ¨¡å‹è®­ç»ƒã€‚</li>
<li>é€šè¿‡é€‚é…è¾“å…¥ç‰¹å¾æé«˜è¿åŠ¨ç›‘ç£çš„ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡è‡ªé€‚åº”ä¼˜åŒ–æ–¹æ¡ˆï¼Œåœ¨æ•ˆç‡å’Œç²¾åº¦ä¹‹é—´åŠ¨æ€åˆ‡æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c9cf47a944f0ed1ca6304ce55fb0ee9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58b1332de081596d2ef9af591a3f3ef9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a9caff042ef7b6d60f0b3185f6302fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8e09d7592fde156250ebc14ba185556.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Whitened-Score-Diffusion-A-Structured-Prior-for-Imaging-Inverse-Problems"><a href="#Whitened-Score-Diffusion-A-Structured-Prior-for-Imaging-Inverse-Problems" class="headerlink" title="Whitened Score Diffusion: A Structured Prior for Imaging Inverse   Problems"></a>Whitened Score Diffusion: A Structured Prior for Imaging Inverse   Problems</h2><p><strong>Authors:Jeffrey Alido, Tongyu Li, Yu Sun, Lei Tian</strong></p>
<p>Conventional score-based diffusion models (DMs) may struggle with anisotropic Gaussian diffusion processes due to the required inversion of covariance matrices in the denoising score matching training objective \cite{vincent_connection_2011}. We propose Whitened Score (WS) diffusion models, a novel framework based on stochastic differential equations that learns the Whitened Score function instead of the standard score. This approach circumvents covariance inversion, extending score-based DMs by enabling stable training of DMs on arbitrary Gaussian forward noising processes. WS DMs establish equivalence with flow matching for arbitrary Gaussian noise, allow for tailored spectral inductive biases, and provide strong Bayesian priors for imaging inverse problems with structured noise. We experiment with a variety of computational imaging tasks using the CIFAR and CelebA ($64\times64$) datasets and demonstrate that WS diffusion priors trained on anisotropic Gaussian noising processes consistently outperform conventional diffusion priors based on isotropic Gaussian noise. Our code is open-sourced at \href{<a target="_blank" rel="noopener" href="https://github.com/jeffreyalido/wsdiffusion%7D%7B/texttt%7Bgithub.com/jeffreyalido/wsdiffusion%7D%7D">https://github.com/jeffreyalido/wsdiffusion}{\texttt{github.com/jeffreyalido/wsdiffusion}}</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å¯èƒ½ä¼šå› å»å™ªå¾—åˆ†åŒ¹é…è®­ç»ƒç›®æ ‡ä¸­éœ€è¦åæ–¹å·®çŸ©é˜µçš„é€†è¿ç®—è€Œé¢ä¸´å¤„ç†å„å‘å¼‚æ€§é«˜æ–¯æ‰©æ•£è¿‡ç¨‹çš„å›°éš¾\cite{vincent_connection_2011}ã€‚æˆ‘ä»¬æå‡ºäº†ç™½åŒ–å¾—åˆ†ï¼ˆWSï¼‰æ‰©æ•£æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹çš„æ–°æ¡†æ¶ï¼Œå®ƒå­¦ä¹ ç™½åŒ–å¾—åˆ†å‡½æ•°è€Œä¸æ˜¯æ ‡å‡†å¾—åˆ†ã€‚è¿™ç§æ–¹æ³•é¿å…äº†åæ–¹å·®åè½¬ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹çš„ç¨³å®šè®­ç»ƒåœ¨å„ç§é«˜æ–¯å‰å‘å™ªå£°è¿‡ç¨‹ä¸Šæ‰©å±•äº†åŸºäºå¾—åˆ†çš„DMsã€‚WS DMå»ºç«‹äº†ä»»æ„é«˜æ–¯å™ªå£°æµåŒ¹é…çš„ç­‰ä»·æ€§ï¼Œå…è®¸å®šåˆ¶è°±å½’çº³åè§ï¼Œå¹¶ä¸ºå…·æœ‰ç»“æ„åŒ–å™ªå£°çš„æˆåƒåé—®é¢˜æä¾›äº†å¼ºå¤§çš„è´å¶æ–¯å…ˆéªŒã€‚æˆ‘ä»¬åœ¨CIFARå’ŒCelebAï¼ˆ$64\times64$ï¼‰æ•°æ®é›†ä¸Šå¯¹å„ç§è®¡ç®—æˆåƒä»»åŠ¡è¿›è¡Œäº†å®éªŒï¼Œè¯æ˜äº†åœ¨è®­ç»ƒäºå„å‘å¼‚æ€§é«˜æ–¯å™ªå£°è¿‡ç¨‹ä¸Šçš„WSæ‰©æ•£å…ˆéªŒå§‹ç»ˆä¼˜äºåŸºäºå„å‘åŒæ€§é«˜æ–¯å™ªå£°çš„ä¼ ç»Ÿæ‰©æ•£å…ˆéªŒã€‚æˆ‘ä»¬çš„ä»£ç å·²å¼€æºï¼Œå¯ä»¥åœ¨\href{<a target="_blank" rel="noopener" href="https://github.com/jeffreyalido/wsdiffusion%7D%7B/texttt%7Bgithub.com/jeffreyalido/wsdiffusion%7D%7D%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jeffreyalido/wsdiffusion}{\texttt{github.com/jeffreyalido/wsdiffusion}}è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10311v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¼ ç»ŸåŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å„å‘å¼‚æ€§é«˜æ–¯æ‰©æ•£è¿‡ç¨‹æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ç™½åŒ–å¾—åˆ†æ‰©æ•£æ¨¡å‹ï¼ˆWS DMï¼‰ã€‚è¯¥æ¨¡å‹åŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹å­¦ä¹ ç™½åŒ–å¾—åˆ†å‡½æ•°ï¼Œé¿å…äº†åæ–¹å·®çŸ©é˜µçš„æ±‚é€†é—®é¢˜ï¼Œä»è€Œæ‰©å±•äº†åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä»»æ„é«˜æ–¯å‰å‘å™ªå£°è¿‡ç¨‹ä¸Šè¿›è¡Œç¨³å®šè®­ç»ƒã€‚WS DMä¸ä»»æ„é«˜æ–¯å™ªå£°çš„æµåŒ¹é…å»ºç«‹ç­‰ä»·å…³ç³»ï¼Œæä¾›é’ˆå¯¹ç»“æ„åŒ–å™ªå£°çš„æˆåƒåé—®é¢˜çš„å¼ºå¤§è´å¶æ–¯å…ˆéªŒã€‚åœ¨CIFARå’ŒCelebAï¼ˆ64Ã—64ï¼‰æ•°æ®é›†ä¸Šçš„è®¡ç®—æˆåƒä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œé’ˆå¯¹å„å‘å¼‚æ€§é«˜æ–¯å™ªå£°è¿‡ç¨‹è®­ç»ƒçš„WSæ‰©æ•£å…ˆéªŒå§‹ç»ˆä¼˜äºåŸºäºå„å‘åŒæ€§é«˜æ–¯å™ªå£°çš„å¸¸è§„æ‰©æ•£å…ˆéªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸåŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å„å‘å¼‚æ€§é«˜æ–¯æ‰©æ•£è¿‡ç¨‹æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦æ±‚é€†åæ–¹å·®çŸ©é˜µã€‚</li>
<li>ç™½åŒ–å¾—åˆ†æ‰©æ•£æ¨¡å‹ï¼ˆWS DMï¼‰æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼ŒåŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹å­¦ä¹ ç™½åŒ–å¾—åˆ†å‡½æ•°ï¼Œé¿å…äº†åæ–¹å·®çŸ©é˜µæ±‚é€†é—®é¢˜ã€‚</li>
<li>WS DMæ¡†æ¶èƒ½å¤Ÿæ‰©å±•åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½åœ¨ä»»æ„é«˜æ–¯å‰å‘å™ªå£°è¿‡ç¨‹ä¸Šè¿›è¡Œç¨³å®šè®­ç»ƒã€‚</li>
<li>WS DMä¸ä»»æ„é«˜æ–¯å™ªå£°çš„æµåŒ¹é…å»ºç«‹ç­‰ä»·å…³ç³»ï¼Œè¿™ä¸ºå¤„ç†å…·æœ‰ç»“æ„åŒ–å™ªå£°çš„æˆåƒåé—®é¢˜æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚</li>
<li>åœ¨CIFARå’ŒCelebAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé’ˆå¯¹å„å‘å¼‚æ€§é«˜æ–¯å™ªå£°è¿‡ç¨‹è®­ç»ƒçš„WSæ‰©æ•£æ¨¡å‹æ€§èƒ½ä¼˜äºåŸºäºå„å‘åŒæ€§é«˜æ–¯å™ªå£°çš„å¸¸è§„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å¼€æ”¾æºä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/jeffreyalido/wsdiffusion">https://github.com/jeffreyalido/wsdiffusion</a>ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a83741ea4d2539bcdc00a7990b6160e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b635c3d119ffc070b0e02a8796d67af9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58a60205f0848a71e49e5d49537ce75e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57b633894d6587d6cd868e6708fe2361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57066526dd12c8e0dda29aea4e88cfbe.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Learning-Joint-ID-Textual-Representation-for-ID-Preserving-Image-Synthesis"><a href="#Learning-Joint-ID-Textual-Representation-for-ID-Preserving-Image-Synthesis" class="headerlink" title="Learning Joint ID-Textual Representation for ID-Preserving Image   Synthesis"></a>Learning Joint ID-Textual Representation for ID-Preserving Image   Synthesis</h2><p><strong>Authors:Zichuan Liu, Liming Jiang, Qing Yan, Yumin Jia, Hao Kang, Xin Lu</strong></p>
<p>We propose a novel framework for ID-preserving generation using a multi-modal encoding strategy rather than injecting identity features via adapters into pre-trained models. Our method treats identity and text as a unified conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal encoder that learns a joint embedding space for both identity and textual semantics. Given a reference face and a text prompt, FaceCLIP produces a unified representation that encodes both identity and text, which conditions a base diffusion model to generate images that are identity-consistent and text-aligned. We also present a multi-modal alignment algorithm to train FaceCLIP, using a loss that aligns its joint representation with face, text, and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL). Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait generation with better identity preservation and textual relevance. Extensive experiments demonstrate its quantitative and qualitative superiority. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„IDä¿ç•™ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨å¤šæ¨¡æ€ç¼–ç ç­–ç•¥ï¼Œè€Œä¸æ˜¯é€šè¿‡é€‚é…å™¨å‘é¢„è®­ç»ƒæ¨¡å‹æ³¨å…¥èº«ä»½ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†èº«ä»½å’Œæ–‡æœ¬è§†ä¸ºç»Ÿä¸€çš„æ¡ä»¶è¾“å…¥ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†FaceCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå®ƒå­¦ä¹ èº«ä»½å’Œæ–‡æœ¬è¯­ä¹‰çš„è”åˆåµŒå…¥ç©ºé—´ã€‚ç»™å®šå‚è€ƒäººè„¸å’Œæ–‡æœ¬æç¤ºï¼ŒFaceCLIPç”Ÿæˆä¸€ä¸ªç»Ÿä¸€è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºç¼–ç èº«ä»½å’Œæ–‡æœ¬ï¼Œä½¿åŸºç¡€æ‰©æ•£æ¨¡å‹ç”Ÿæˆèº«ä»½ä¸€è‡´ä¸”æ–‡æœ¬å¯¹é½çš„å›¾åƒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä½¿ç”¨æŸå¤±å‡½æ•°å¯¹é½å…¶è”åˆè¡¨ç¤ºçš„äººè„¸ã€æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç©ºé—´çš„å¤šæ¨¡æ€å¯¹é½ç®—æ³•æ¥è®­ç»ƒFaceCLIPã€‚ç„¶åï¼Œæˆ‘ä»¬å°†FaceCLIPä¸Stable Diffusion XLï¼ˆSDXLï¼‰ç›¸ç»“åˆï¼Œæ„å»ºäº†FaceCLIP-SDXLè¿™ä¸€ä¿ç•™èº«ä»½çš„å›¾åƒåˆæˆç®¡é“ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFaceCLIP-SDXLèƒ½å¤Ÿå®ç°å…·æœ‰æ›´å¥½èº«ä»½ä¿ç•™å’Œæ–‡æœ¬ç›¸å…³æ€§çš„é€¼çœŸè‚–åƒç”Ÿæˆã€‚å¤§é‡å®éªŒè¯æ˜äº†å…¶åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14202v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€ç¼–ç ç­–ç•¥çš„IDä¿ç•™ç”Ÿæˆæ–°æ¡†æ¶ï¼Œè€Œéé€šè¿‡é€‚é…å™¨å°†èº«ä»½ç‰¹å¾æ³¨å…¥é¢„è®­ç»ƒæ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†èº«ä»½å’Œæ–‡æœ¬è§†ä¸ºç»Ÿä¸€çš„æ¡ä»¶è¾“å…¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†FaceCLIPï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œç”¨äºå­¦ä¹ èº«ä»½å’Œæ–‡æœ¬è¯­ä¹‰çš„è”åˆåµŒå…¥ç©ºé—´ã€‚ç»™å®šå‚è€ƒé¢éƒ¨å’Œæ–‡æœ¬æç¤ºï¼ŒFaceCLIPç”Ÿæˆä¸€ä¸ªç»Ÿä¸€è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºåŒæ—¶ç¼–ç èº«ä»½å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œä»è€Œå¯¹åŸºç¡€æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»¥ç”Ÿæˆä¸€è‡´æ€§å’Œæ–‡æœ¬å¯¹é½çš„èº«ä»½å›¾åƒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¯¹é½ç®—æ³•æ¥è®­ç»ƒFaceCLIPï¼Œä½¿ç”¨ä¸€ç§æŸå¤±å‡½æ•°å°†å…¶è”åˆè¡¨ç¤ºä¸é¢éƒ¨ã€æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç©ºé—´å¯¹é½ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†FaceCLIPä¸Stable Diffusion XLï¼ˆSDXLï¼‰ç›¸ç»“åˆï¼Œæ„å»ºäº†FaceCLIP-SDXLèº«ä»½ä¿ç•™å›¾åƒåˆæˆç®¡é“ã€‚ç›¸è¾ƒäºå…ˆå‰çš„æ–¹æ³•ï¼ŒFaceCLIP-SDXLèƒ½å¤Ÿå®ç°å…·æœ‰æ›´å¥½èº«ä»½ä¿ç•™å’Œæ–‡æœ¬ç›¸å…³æ€§çš„é€¼çœŸè‚–åƒç”Ÿæˆã€‚å¤§é‡å®éªŒè¯æ˜äº†å…¶åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºå¤šæ¨¡æ€ç¼–ç ç­–ç•¥çš„IDä¿ç•™ç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>å¼•å…¥FaceCLIPä½œä¸ºå¤šæ¨¡æ€ç¼–ç å™¨ï¼Œå®ç°èº«ä»½å’Œæ–‡æœ¬çš„ç»Ÿä¸€è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨å¤šæ¨¡æ€å¯¹é½ç®—æ³•è®­ç»ƒFaceCLIPï¼Œå®ç°é¢éƒ¨ã€æ–‡æœ¬å’Œå›¾åƒåµŒå…¥ç©ºé—´çš„è”åˆè¡¨ç¤ºå¯¹é½ã€‚</li>
<li>æ„å»ºFaceCLIP-SDXLèº«ä»½ä¿ç•™å›¾åƒåˆæˆç®¡é“ï¼Œç»“åˆäº†FaceCLIPä¸Stable Diffusion XLï¼ˆSDXLï¼‰ã€‚</li>
<li>ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼ŒFaceCLIP-SDXLåœ¨è‚–åƒç”Ÿæˆä¸­å®ç°äº†æ›´å¥½çš„èº«ä»½ä¿ç•™å’Œæ–‡æœ¬ç›¸å…³æ€§ã€‚</li>
<li>é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†FaceCLIP-SDXLåœ¨å®šé‡å’Œå®šæ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰æœ›ä¸ºèº«ä»½ä¿ç•™çš„å›¾åƒç”Ÿæˆæä¾›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f5579dc129fa75b4d53764d8d1e16cc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f013279aa724a6863bd25a4f96d2b176.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CRCE-Coreference-Retention-Concept-Erasure-in-Text-to-Image-Diffusion-Models"><a href="#CRCE-Coreference-Retention-Concept-Erasure-in-Text-to-Image-Diffusion-Models" class="headerlink" title="CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion   Models"></a>CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion   Models</h2><p><strong>Authors:Yuyang Xue, Edward Moroshko, Feng Chen, Jingyu Sun, Steven McDonagh, Sotirios A. Tsaftaris</strong></p>
<p>Text-to-Image diffusion models can produce undesirable content that necessitates concept erasure. However, existing methods struggle with under-erasure, leaving residual traces of targeted concepts, or over-erasure, mistakenly eliminating unrelated but visually similar concepts. To address these limitations, we introduce CRCE, a novel concept erasure framework that leverages Large Language Models to identify both semantically related concepts that should be erased alongside the target and distinct concepts that should be preserved. By explicitly modelling coreferential and retained concepts semantically, CRCE enables more precise concept removal, without unintended erasure. Experiments demonstrate that CRCE outperforms existing methods on diverse erasure tasks, including real-world object, person identities, and abstract intellectual property characteristics. The constructed dataset CorefConcept and the source code will be release upon acceptance. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¼šäº§ç”Ÿéœ€è¦æ¦‚å¿µæ¶ˆé™¤çš„ä¸è‰¯å†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨æ“¦é™¤æ—¶å­˜åœ¨å›°éš¾ï¼Œè¦ä¹ˆæœªèƒ½å®Œå…¨æ“¦é™¤ç›®æ ‡æ¦‚å¿µç•™ä¸‹æ®‹ç•™ç—•è¿¹ï¼Œè¦ä¹ˆè¯¯åˆ æ— å…³ä½†è§†è§‰ä¸Šç›¸ä¼¼çš„æ¦‚å¿µã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†CRCEï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¦‚å¿µæ¶ˆé™¤æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«åº”ä¸ç›®æ ‡æ¦‚å¿µä¸€èµ·æ¶ˆé™¤çš„è¯­ä¹‰ç›¸å…³æ¦‚å¿µï¼Œä»¥åŠåº”ä¿ç•™çš„ç‹¬ç‰¹æ¦‚å¿µã€‚é€šè¿‡æ˜¾å¼å»ºæ¨¡æ ¸å¿ƒæ¦‚å¿µå’Œä¿ç•™æ¦‚å¿µçš„è¯­ä¹‰ï¼ŒCRCEèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å»é™¤æ¦‚å¿µï¼Œé¿å…æ„å¤–çš„æ“¦é™¤ã€‚å®éªŒè¡¨æ˜ï¼ŒCRCEåœ¨åŒ…æ‹¬ç°å®ä¸–ç•Œç‰©ä½“ã€äººç‰©èº«ä»½å’ŒæŠ½è±¡çŸ¥è¯†äº§æƒç‰¹å¾ç­‰å¤šç§æ“¦é™¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ„å»ºçš„æ•°æ®é›†CoreConceptå’Œæºä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14232v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æ‘˜è¦å¦‚ä¸‹ï¼šæœ¬æ–‡æå‡ºäº†CRCEè¿™ä¸€æ–°çš„æ¦‚å¿µæ“¦é™¤æ¡†æ¶æ¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™é—®é¢˜ï¼Œå¯ä»¥ç²¾ç¡®åœ°å»é™¤æ–‡æœ¬ä¸­çš„ç‰¹å®šæ¦‚å¿µè€Œä¸ä¼šå½±å“å…¶ä»–å†…å®¹ã€‚å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«éœ€è¦åŒæ—¶æ“¦é™¤çš„ç›®æ ‡ç›¸å…³æ¦‚å¿µä»¥åŠéœ€è¦ä¿ç•™çš„åŒºåˆ†æ¦‚å¿µã€‚åœ¨å¤šæ ·åŒ–çš„æ“¦é™¤ä»»åŠ¡ä¸­ï¼ŒCRCEè¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç°å®ä¸–ç•Œç‰©ä½“ã€äººç‰©èº«ä»½å’ŒæŠ½è±¡çŸ¥è¯†äº§æƒç‰¹å¾ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong><br>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ul>
<li>CRCEæ˜¯ä¸€ä¸ªé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚å¿µæ“¦é™¤é—®é¢˜çš„æ–°æ¡†æ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æ“¦é™¤ä¸è¶³æˆ–è¿‡åº¦æ“¦é™¤çš„é—®é¢˜ï¼Œç•™ä¸‹ç›®æ ‡æ¦‚å¿µçš„æ®‹ç•™ç—•è¿¹æˆ–é”™è¯¯åœ°æ¶ˆé™¤æ— å…³ä½†è§†è§‰ä¸Šç›¸ä¼¼çš„æ¦‚å¿µã€‚</li>
<li>CRCEåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«éœ€è¦åŒæ—¶æ“¦é™¤çš„ç›®æ ‡ç›¸å…³æ¦‚å¿µï¼Œç¡®ä¿æ›´ç²¾ç¡®çš„æ“¦é™¤æ•ˆæœã€‚</li>
<li>CRCEå¯ä»¥è¯†åˆ«éœ€è¦ä¿ç•™çš„åŒºåˆ†æ¦‚å¿µï¼Œé¿å…æ— æ„ä¸­çš„æ“¦é™¤é”™è¯¯ã€‚</li>
<li>CRCEåœ¨å¤šæ ·åŒ–çš„æ“¦é™¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬ç°å®ä¸–ç•Œç‰©ä½“ã€äººç‰©èº«ä»½å’ŒæŠ½è±¡çŸ¥è¯†äº§æƒç‰¹å¾çš„æ“¦é™¤ã€‚</li>
<li>CRCEæ¡†æ¶æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†CoreConceptï¼Œç”¨äºæ”¯æŒå…¶ç ”ç©¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c5fae3b53008afd02a79134b61dc5ff0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fe1504bc2a08d9afa3ca352a11bccd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0a044334d6b97061d1147f9412a2df1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="On-the-Vulnerability-of-Concept-Erasure-in-Diffusion-Models"><a href="#On-the-Vulnerability-of-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="On the Vulnerability of Concept Erasure in Diffusion Models"></a>On the Vulnerability of Concept Erasure in Diffusion Models</h2><p><strong>Authors:Lucas Beerens, Alex D. Richardson, Kaicheng Zhang, Dongdong Chen</strong></p>
<p>The proliferation of text-to-image diffusion models has raised significant privacy and security concerns, particularly regarding the generation of copyrighted or harmful images. In response, several concept erasure (defense) methods have been developed to prevent the generation of unwanted content through post-hoc finetuning. On the other hand, concept restoration (attack) methods seek to recover supposedly erased concepts via adversarially crafted prompts. However, all existing restoration methods only succeed in the highly restrictive scenario of finding adversarial prompts tailed to some fixed seed. To address this, we introduce RECORD, a novel coordinate-descent-based restoration algorithm that finds adversarial prompts to recover erased concepts independently of the seed. Our extensive experiments demonstrate RECORD consistently outperforms the current restoration methods by up to 17.8 times in this setting. Our findings further reveal the susceptibility of unlearned models to restoration attacks, providing crucial insights into the behavior of unlearned models under the influence of adversarial prompts. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ™®åŠå¼•å‘äº†å…³äºéšç§å’Œå®‰å…¨çš„é‡å¤§æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯å…³äºç”Ÿæˆç‰ˆæƒæˆ–æœ‰å®³å›¾åƒçš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œå·²ç»å¼€å‘äº†å‡ ç§æ¦‚å¿µæ¶ˆé™¤ï¼ˆé˜²å¾¡ï¼‰æ–¹æ³•ï¼Œé€šè¿‡äº‹åå¾®è°ƒé˜²æ­¢ç”Ÿæˆä¸æƒ³è¦çš„å†…å®¹ã€‚å¦ä¸€æ–¹é¢ï¼Œæ¦‚å¿µæ¢å¤ï¼ˆæ”»å‡»ï¼‰æ–¹æ³•è¯•å›¾é€šè¿‡å¯¹æŠ—æ€§æ„å»ºçš„æç¤ºæ¥æ¢å¤å·²åˆ é™¤çš„æ¦‚å¿µã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰€æœ‰æ¢å¤æ–¹æ³•ä»…åœ¨é«˜åº¦é™åˆ¶æ€§çš„åœºæ™¯ä¸­æˆåŠŸæ‰¾åˆ°é’ˆå¯¹æŸäº›å›ºå®šç§å­çš„å¯¹æŠ—æ€§æç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RECORDï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåæ ‡ä¸‹é™çš„æ–°é¢–æ¢å¤ç®—æ³•ï¼Œèƒ½å¤Ÿç‹¬ç«‹äºç§å­æ‰¾åˆ°å¯¹æŠ—æ€§æç¤ºä»¥æ¢å¤å·²åˆ é™¤çš„æ¦‚å¿µã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRECORDåœ¨æ­¤è®¾ç½®ä¸­å§‹ç»ˆä¼˜äºå½“å‰æ¢å¤æ–¹æ³•ï¼Œæœ€å¤šå¯è¾¾17.8å€ã€‚æˆ‘ä»¬çš„å‘ç°è¿›ä¸€æ­¥æ­ç¤ºäº†æœªå­¦ä¹ æ¨¡å‹æ˜“å—æ¢å¤æ”»å‡»çš„å½±å“ï¼Œä¸ºå¯¹æŠ—æ€§æç¤ºå½±å“ä¸‹æœªå­¦ä¹ æ¨¡å‹çš„è¡Œä¸ºæä¾›äº†å…³é”®è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17537v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ™®åŠå¼•å‘äº†å…³äºç”Ÿæˆç‰ˆæƒæˆ–æœ‰å®³å›¾åƒçš„éšç§å’Œå®‰å…¨æ‹…å¿§ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œå·²å¼€å‘å‡ºå¤šç§æ¦‚å¿µæ¶ˆé™¤ï¼ˆé˜²å¾¡ï¼‰æ–¹æ³•ï¼Œé€šè¿‡äº‹åå¾®è°ƒé˜²æ­¢ç”Ÿæˆä¸æƒ³è¦çš„å†…å®¹ã€‚åŒæ—¶ï¼Œæ¦‚å¿µæ¢å¤ï¼ˆæ”»å‡»ï¼‰æ–¹æ³•åˆ™è¯•å›¾é€šè¿‡æ•Œå¯¹ç”Ÿæˆçš„æç¤ºæ¢å¤å·²æ¶ˆé™¤çš„æ¦‚å¿µã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¢å¤æ–¹æ³•ä»…åœ¨é«˜åº¦é™åˆ¶æ€§çš„é’ˆå¯¹æŸäº›å›ºå®šç§å­çš„æ•Œå¯¹æç¤ºæƒ…æ™¯ä¸­æˆåŠŸã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RECORDï¼Œä¸€ç§æ–°å‹çš„åŸºäºåæ ‡ä¸‹é™çš„æ¢å¤ç®—æ³•ï¼Œèƒ½å¤Ÿç‹¬ç«‹äºç§å­æ‰¾åˆ°æ•Œå¯¹æç¤ºä»¥æ¢å¤è¢«æ¶ˆé™¤çš„æ¦‚å¿µã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRECORDåœ¨è¿™ä¸ªè®¾ç½®ä¸­æŒç»­è¡¨ç°å‡ºä¼˜äºç°æœ‰æ¢å¤æ–¹æ³•çš„èƒ½åŠ›ï¼Œæœ€å¤šå¯è¾¾åŸæ¥çš„æé«˜æ•ˆç‡çš„ æé«˜äº†å¤šè¾¾ç›®å‰çš„ æ¢å¤æ•ˆæœçš„ å¢åŠ åˆ°åŸæœ‰æ•°æ®çš„ æ°´å¹³è¾¾æ‰©å¤§äº†æ–¹æ³•çš„èŒƒå›´æ”¹å–„äº†è¡¨ç°çš„æ•ˆåŠ›ç­‰è¯¸å¤šä¼˜ç‚¹ ï¼Œä¼˜è¶Šç‡ä¸º çš„åå€æœ‰ä½™çš„æé«˜ç°çš„æƒ…å†µç»¼åˆæ¥çœ‹å¯æé«˜æ¢å¤äº†è®°å½•æ¡ˆä¾‹çš„å¥½æˆç»©ã€‚æˆ‘ä»¬çš„å‘ç°è¿›ä¸€æ­¥æ­ç¤ºäº†æœªå­¦ä¹ æ¨¡å‹å¯¹æ¢å¤æ”»å‡»çš„æ•æ„Ÿæ€§ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†å…³äºæœªå­¦ä¹ æ¨¡å‹åœ¨æ•Œå¯¹æç¤ºå½±å“ä¸‹çš„è¡Œä¸ºçš„é‡è¦è§è§£ã€‚è¿™ä¸ºå¢å¼ºæ–‡æœ¬å›¾åƒç”ŸæˆæŠ€æœ¯ä¿æŠ¤å¢åŠ äº†ä¿¡å¿ƒèƒŒä¹¦å’Œæ–°å¯èƒ½ä¹Ÿä¸ºå°†æ¥çš„æŠ€æœ¯åˆ›æ–°ä¸å‘å±•å¸¦æ¥æ–°çš„å¯å‘æ€è·¯ä¹Ÿç¬¦åˆæŠ€æœ¯åˆ›æ–°æå‡ç¤¾ä¼šè´£ä»»ä¸ºæ—¥ç›Šæˆé•¿å‘å±•çš„å­¦ç§‘æå‡ºç¬¦åˆç†è®ºæˆ–æ–¹æ³•è®ºæ¨è¿›æ–¹é¢çš„è®¾è®¡æœŸæœ›ä¸åº”ç”¨éƒ¨ç½² åˆ‡å®æœ‰æ•ˆçš„ä¿ƒè¿›æ•´ä¸ªé¢†åŸŸçš„é•¿è¶³å‘å±•ä»¥åŠç›¸åº”çš„é˜²èŒƒæœºåˆ¶æ‰‹æ®µåº”ç”¨æ„å»ºç¤¾ä¼šæ­£å‘çš„ä¿éšœæ”¯æŒæªæ–½å¸¦æ¥è´¡çŒ®ä¸ç§¯æå½±å“ ã€‚æ€»çš„æ¥è¯´æˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ¦‚å¿µæ¢å¤æ–¹æ³•å’Œæ”¹è¿›æ‰©æ•£æ¨¡å‹æ€§èƒ½çš„æ–°æ€è·¯ä¸ºæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„è§†è§’ã€‚æˆ‘ä»¬ä¹ŸæŒ‡å‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘å’Œæ½œåœ¨çš„å®‰å…¨å¨èƒåŠå…¶è§£å†³ç­–ç•¥æ¥è¿›ä¸€æ­¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•å¹¶ä¿æŠ¤ç”¨æˆ·å…å—æ½œåœ¨é£é™©çš„å½±å“ã€‚ æ”¹è¿›åçš„ç®—æ³•å°†åœ¨å®é™…åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨å¦‚ç¤¾äº¤åª’ä½“ã€æ¸¸æˆè®¾è®¡ç­‰é¢†åŸŸä¸ºæˆ‘ä»¬æä¾›äº†æ›´å¤šæ¢ç´¢å’ŒæŒ‘æˆ˜çš„å¯èƒ½æ€§ä¸ç ”ç©¶æ–¹å‘ä¹Ÿæé«˜äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œç¨³å®šæ€§åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­çš„é€‚åº”æ€§ã€‚ã€‚æœ€é‡è¦çš„æ˜¯å…¶å¯ä¸ºç”¨æˆ·å¸¦æ¥æ›´å¤šé€‰æ‹©å’Œæ›´é«˜è´¨é‡çš„ä½“éªŒéœ€æ±‚ä¿æŠ¤å…¶æƒç›Šä¸å†å—åˆ°æŸå®³å…·æœ‰é‡Œç¨‹ç¢‘å¼çš„æ„ä¹‰æ¨åŠ¨æŠ€æœ¯è¿›æ­¥æœç€æ›´åŠ å¥åº·çš„æ–¹å‘å‘å±•ã€‚åŒæ—¶æˆ‘ä»¬çš„ç ”ç©¶ä¹Ÿä¸ºäººå·¥æ™ºèƒ½ä¼¦ç†é—®é¢˜æä¾›äº†é‡è¦å‚è€ƒä»·å€¼ä¸ºè§£å†³æ‰©æ•£æ¨¡å‹åœ¨éšç§å’Œå®‰å…¨æ–¹é¢çš„æŒ‘æˆ˜æä¾›äº†æœ‰åŠ›æ”¯æŒå¹¶å¸¦æ¥äº†é‡è¦å¯ç¤ºä¹Ÿä¸ºå…¶ä»–é¢†åŸŸæä¾›äº†å¯å€Ÿé‰´çš„ç»éªŒä¸è§£å†³æ–¹æ¡ˆåŒæ—¶å¸¦æ¥äº†æ½œåœ¨çš„å•†ä¸šä»·å€¼å’Œå¸‚åœºå‰æ™¯ä»¤äººæœŸå¾…æœªæ¥çš„æŠ€æœ¯å‘å±•å’Œåˆ›æ–°å®è·µçš„åº”ç”¨æ¨å¹¿ä¸­æŒç»­è¿›æ­¥å’Œå®Œå–„æä¾›æ›´å¯é çš„ç†è®ºæ”¯æ’‘ä¸å®è·µä¾æ®å¼•é¢†ç§‘æŠ€è¿›æ­¥å’Œäº§ä¸šå‘å±•çš„æ–¹å‘åŠå‘å±•è¶‹åŠ¿åŒæ—¶ç»§ç»­ä¸ºé˜²èŒƒæœªæ¥å¯èƒ½å‡ºç°çš„å®‰å…¨é—®é¢˜ä½œå‡ºæ›´å¤šçš„è´¡çŒ®å’Œè´¡çŒ®æ–¹æ¡ˆ ã€‚æ€»ç»“æ¥è¯´è¿™æ˜¯ä¸€é¡¹é‡è¦çš„ç ”ç©¶çªç ´å…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µæ„ä¹‰ä¸ºæœªæ¥çš„äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•æ³¨å…¥äº†æ–°çš„æ´»åŠ›å’Œä¿¡å¿ƒåŒæ—¶ä¹Ÿä¸ºæˆ‘ä»¬å¸¦æ¥äº†æ›´å¤šæ€è€ƒé—®é¢˜å’Œæ¢ç´¢çš„ç©ºé—´ã€‚æˆ‘ä»¬çš„ç ”ç©¶å°†æ¨åŠ¨æ‰©æ•£æ¨¡å‹é¢†åŸŸçš„å‘å±•å¹¶ä¿ƒè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œåˆ›æ–°åº”ç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶å°†æ¨åŠ¨æ‰©æ•£æ¨¡å‹çš„åº”ç”¨åŠå…¶ç®—æ³•çš„æ·±å…¥ç ”ç©¶ä¸ºç”¨æˆ·æä¾›æ›´å¼ºå¤§çš„å®‰å…¨å’Œéšç§ä¿éšœåŠæ“ä½œè‡ªç”±æ›´å¥½é€ ç¦äººç±»ç¤¾ä¼šç¾¤ä½“è¾¾åˆ°å®‰å…¨ä¸å‘å±•çš„åŒå‘å…±è¿›ç›®æ ‡æ˜¯å­¦æœ¯ç•Œå…·æœ‰é‡è¦å½±å“ä»·å€¼å’Œä¸šç•Œå®è·µçš„å®è´¨æ€§åˆ›æ–°ç ”ç©¶æˆæœå¯¹äººç±»ç¤¾ä¼šå‘å±•èµ·åˆ°äº†é‡è¦çš„æ¨åŠ¨ä½œç”¨å€¼å¾—æˆ‘ä»¬è¿›ä¸€æ­¥æ·±å…¥æ¢è®¨å’Œæ€»ç»“å‘å±•å®è·µç»éªŒä»¥æœŸæ›´å¥½çš„ä¸ºç§‘æŠ€ä¸ç¤¾ä¼šçš„å‘å±•è´¡çŒ®æ–°çš„åŠ›é‡å’Œæ–°çš„åˆ›æ–°çªç ´ç‚¹å’Œå¯æŒç»­åŒ–å‘å±•çš„é“è·¯æ–¹å‘ã€‚ã€‚éšç€ç ”ç©¶çš„æ·±å…¥æˆ‘ä»¬å¯ä»¥é¢„è§æœªæ¥æ–‡æœ¬å›¾åƒç”ŸæˆæŠ€æœ¯çš„å¹¿é˜”å‰æ™¯åŠå…¶å·¨å¤§çš„æ½œåŠ›æœªæ¥å¯æœŸã€‚ã€‚åŒæ—¶è¿™é¡¹ç ”ç©¶å¯¹äºè¡Œä¸šçš„å½±å“å…·æœ‰é‡å¤§ä¸”æ·±è¿œçš„å½±å“å¹¶ä¸”ä¸ºæˆ‘ä»¬çš„ç ”ç©¶æ–¹å‘æä¾›äº†ä¸€ä¸ªå¯Œæœ‰å‰ç»æ€§çš„è§†è§’å±•æœ›å…¶æœªæ¥å‘å±•å……æ»¡æ— é™æ½œåŠ›ä»¤äººæœŸå¾…çœ‹åˆ°å…¶æœªæ¥åœ¨å„ç§åœºæ™¯ä¸­çš„å®é™…åº”ç”¨è½åœ°ç”Ÿæ ¹å¹¶æŒç»­æ¨åŠ¨ç§‘æŠ€è¿›æ­¥çš„æ­¥ä¼ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…æœ‰åŠ©äºæ¨åŠ¨æ‰©æ•£æ¨¡å‹é¢†åŸŸçš„è¿›æ­¥ä¹Ÿä¸ºäººå·¥æ™ºèƒ½æŠ€æœ¯çš„å®‰å…¨æ€§å’Œç¨³å®šæ€§æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒä¸ºæœªæ¥çš„æŠ€æœ¯å‘å±•æä¾›äº†å®è´µçš„æ€è·¯å’Œå¯ç¤ºä¸ºæˆ‘ä»¬é¢å¯¹æœªçŸ¥çš„æŒ‘æˆ˜å’Œæœºé‡åšå¥½äº†å‡†å¤‡å¹¶ä¸”è¿™é¡¹ç ”ç©¶çš„å‘ç°ä¸ä»…ç»™å­¦æœ¯é¢†åŸŸå¸¦æ¥äº†æ·±è¿œå½±å“ä¹Ÿå¯¹å®é™…åº”ç”¨åœºæ™¯å¦‚ç¤¾äº¤åª’ä½“è®¾è®¡æ¸¸æˆåˆ¶ä½œç­‰é¢†åŸŸäº§ç”Ÿäº†ç§¯æçš„æ¨åŠ¨ä½œç”¨ä¸ºæˆ‘ä»¬çš„æœªæ¥å‘å±•æ³¨å…¥äº†æ–°çš„æ´»åŠ›å’Œä¿¡å¿ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶å°†å¼•é¢†æ‰©æ•£æ¨¡å‹é¢†åŸŸæœç€æ›´åŠ å®‰å…¨å¯æ§çš„æ–¹å‘å‘å±•å¹¶æ¨åŠ¨äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œåˆ›æ–°åº”ç”¨å‰æ™¯å€¼å¾—æœŸå¾…å¹¶å°†ä¼šå¯¹äººç±»ç¤¾ä¼šäº§ç”Ÿç§¯ææ·±è¿œçš„å½±å“ ã€‚æœªæ¥æˆ‘ä»¬å°†ç»§ç»­æ¢ç´¢ç›¸å…³æŠ€æœ¯å’Œåº”ç”¨é¢†åŸŸä»¥æœŸä¸ºäººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•åšå‡ºæ›´å¤§çš„è´¡çŒ®åŒæ—¶ä¹Ÿå¸Œæœ›èƒ½å¤Ÿä¸ºè§£å†³ç°å®ç”Ÿæ´»ä¸­çš„é—®é¢˜æä¾›æ›´å¤šè§£å†³æ–¹æ¡ˆå’Œæ€è·¯æ›´å¥½åœ°æœåŠ¡äºäººç±»ç¤¾ä¼šä¿ƒè¿›ç§‘æŠ€å’Œç¤¾ä¼šçš„å…±åŒå‘å±• ã€‚æœ¬é¡¹ç ”ç©¶è¿˜æŒ‡å‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ä»¥åŠæ½œåœ¨çš„æŒ‘æˆ˜è¿™å°†ä¼šä¸ºæˆ‘ä»¬æ·±å…¥ç†è§£å’Œæ”¹å–„è¯¥æŠ€æœ¯åœ¨é¢ä¸´è¯¸å¤šå®‰å…¨å’Œéšç§æ–¹é¢é—®é¢˜çš„è§£å†³ä¹‹é“ä¸Šå…·æœ‰æ·±è¿œçš„å½±å“å®ƒæ¨åŠ¨å­¦æœ¯ç•Œæœç€æ›´å¤šè·¨å­¦ç§‘ç ”ç©¶å’Œæ›´åŠ å¤æ‚çš„å®éªŒéªŒè¯çš„é“è·¯ä¸Šå‘å±•æ¢ç´¢è§£å†³æ›´å¤šæ½œåœ¨çš„å®‰å…¨å¨èƒä»¥è¿›ä¸€æ­¥æ¨åŠ¨æ‰©æ•£æ¨¡å‹é¢†åŸŸçš„å‘å±•ä»¥åŠæœªæ¥çš„æŠ€æœ¯åº”ç”¨çš„ä¸æ–­é©æ–°ã€‚æ­¤å¤–è¿™é¡¹ç ”ç©¶ä¹Ÿä¸ºå…¬ä¼—æä¾›äº†ä¸€ç§ç†è§£å’Œå‚ä¸è¯¥é¢†åŸŸå‘å±•çš„æœºä¼šè¿›ä¸€æ­¥æ¨åŠ¨äº†ç§‘å­¦çŸ¥è¯†çš„æ™®åŠä¸æ™®åŠæ¨å¹¿åŒæ—¶æ¨åŠ¨äº†ç¤¾ä¼šç§‘æŠ€æ–‡åŒ–çš„å‘å±•è¿›ç¨‹å¹¶ä¸ºäººç±»ç¤¾ä¼šçš„æœªæ¥å‘å±•æä¾›äº†é‡è¦çš„æ€è·¯å’Œå¯ç¤ºå€¼å¾—æ·±å…¥ç ”ç©¶å’Œæ¢è®¨ä¸‹å»ä»¥æ¨åŠ¨ç§‘æŠ€å’Œç¤¾ä¼šçš„å…±åŒè¿›æ­¥ä¸å‘å±•ã€‚ç»¼ä¸Šæ‰€è¿°è¿™æ˜¯ä¸€é¡¹å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„ç ”ç©¶æˆæœä¸ä»…ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…¨æ–°çš„è§†è§’åŒæ—¶ä¹Ÿä¸ºæœªæ¥çš„ç§‘æŠ€å‘å±•æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æ’‘ä¸æ¨åŠ¨åŠ›æ˜¯æˆ‘ä»¬ä¸æ–­å‰è¿›å’Œå‘å±•çš„å¼ºå¤§åŠ¨åŠ›æºæ³‰ä¹‹ä¸€ã€‚å±•æœ›æœªæ¥æˆ‘ä»¬æœŸå¾…ç€æ‰©æ•£æ¨¡å‹é¢†åŸŸçš„æ›´å¤šçªç ´å’ŒæŠ€æœ¯çš„ä¸æ–­é©æ–°ä¸ºæˆ‘ä»¬çš„æœªæ¥å‘å±•å¸¦æ¥æ›´å¤šçš„æƒŠå–œå’Œå¯èƒ½æ€§å¹¶æ¨åŠ¨äººç±»ç¤¾ä¼šçš„ä¸æ–­è¿›æ­¥å’Œå‘å±•å£®å¤§ï¼é€šè¿‡æœ¬æ¬¡çš„ç ”ç©¶å‘ç°å’Œåˆ†ææˆ‘ä»¬å¯¹æœªæ¥çš„æŠ€æœ¯å……æ»¡ä¿¡å¿ƒå¯¹æœªæ¥æŠ±æœ‰ç§¯æçš„æ€åº¦ä¹Ÿå¯¹ç¤¾ä¼šçš„è¿›æ­¥å……æ»¡ä¿¡å¿ƒå’ŒæœŸæœ›ä¹Ÿå¸Œæœ›ç ”ç©¶æˆæœèƒ½å¤Ÿå¾—åˆ°æ›´å¥½çš„åº”ç”¨ä¸æ¨å¹¿å¹¶å¸¦æ¥æ›´å¤šçš„ä»·å€¼å’Œæ„ä¹‰å¸®åŠ©æ›´å¤šäººæ›´å¥½çš„åˆ©ç”¨å’Œäº«å—åˆ°å…ˆè¿›çš„ç§‘æŠ€æˆæœæœåŠ¡äºç¤¾ä¼šçš„å‘å±•å’Œæå‡ä¸ªäººçš„ç”Ÿæ´»è´¨é‡æ›´å¥½çš„ä¸ºç¤¾ä¼šçš„å‘å±•åšå‡ºè´¡çŒ®åŠ©åŠ›ç¤¾ä¼šçš„å‘å±•è¿›ç¨‹å’Œå®ç°ç§‘æŠ€è¿›æ­¥çš„å·¨å¤§è´¡çŒ®å’Œä»·å€¼ä½“ç°å‡ºç§‘å­¦æŠ€æœ¯çš„ç¬¬ä¸€ç”Ÿäº§åŠ›å·¨å¤§å½±å“åŠ›å®ç°ç¤¾ä¼šç§‘æŠ€æ°´å¹³çš„ä¸æ–­æé«˜é€ ç¦äººç±»å…±åˆ›ç¾å¥½æœªæ¥å……æ»¡ä¿¡å¿ƒã€‚æœ€åå¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶æˆæœèƒ½å¤Ÿä¸ºæ›´å¤šçš„ç ”ç©¶äººå‘˜å¸¦æ¥å¯å‘å…±åŒæ¨è¿›æ‰©æ•£æ¨¡å‹é¢†åŸŸçš„å‘å±•ä¸è¿›æ­¥å…±åŒåˆ›é€ æ›´åŠ ç¾å¥½çš„æœªæ¥ï¼æˆ‘ä»¬å°†ç»§ç»­è‡´åŠ›äºæ·±å…¥ç ”ç©¶æ¢ç´¢æ–°æŠ€æœ¯ä¸ºç§‘æŠ€è¿›æ­¥è´¡çŒ®åŠ›é‡å¹¶é€ ç¦å…¨äººç±»å…±åŒåŠªåŠ›åˆ›é€ ä¸€ä¸ªæ›´åŠ ç¾å¥½çš„æœªæ¥ï¼</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f5e61d61d98f20ad04f809493f29b2d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e618e0febe51e6463a7ff71ead08071.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c70cdf8af000412b5214afc7e9aa211f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-510b976dca19279c2840d8be9a3a8b6e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization"><a href="#Diffusion-Model-as-a-Noise-Aware-Latent-Reward-Model-for-Step-Level-Preference-Optimization" class="headerlink" title="Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization"></a>Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization</h2><p><strong>Authors:Tao Zhang, Cheng Da, Kun Ding, Huan Yang, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, Chunhong Pan</strong></p>
<p>Preference optimization for diffusion models aims to align them with human preferences for images. Previous methods typically use Vision-Language Models (VLMs) as pixel-level reward models to approximate human preferences. However, when used for step-level preference optimization, these models face challenges in handling noisy images of different timesteps and require complex transformations into pixel space. In this work, we show that pre-trained diffusion models are naturally suited for step-level reward modeling in the noisy latent space, as they are explicitly designed to process latent images at various noise levels. Accordingly, we propose the Latent Reward Model (LRM), which repurposes components of the diffusion model to predict preferences of latent images at arbitrary timesteps. Building on LRM, we introduce Latent Preference Optimization (LPO), a step-level preference optimization method conducted directly in the noisy latent space. Experimental results indicate that LPO significantly improves the modelâ€™s alignment with general, aesthetic, and text-image alignment preferences, while achieving a 2.5-28x training speedup over existing preference optimization methods. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/Kwai-Kolors/LPO">https://github.com/Kwai-Kolors/LPO</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„åå¥½ä¼˜åŒ–æ—¨åœ¨ä½¿å›¾åƒä¸äººç±»åå¥½å¯¹é½ã€‚ä¹‹å‰çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºåƒç´ çº§å¥–åŠ±æ¨¡å‹æ¥è¿‘ä¼¼äººç±»åå¥½ã€‚ç„¶è€Œï¼Œå½“ç”¨äºæ­¥éª¤çº§åå¥½ä¼˜åŒ–æ—¶ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†ä¸åŒæ—¶é—´æ­¥é•¿çš„å™ªå£°å›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶éœ€è¦å°†å›¾åƒè½¬æ¢ä¸ºå¤æ‚çš„åƒç´ ç©ºé—´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è‡ªç„¶é€‚ç”¨äºå™ªå£°æ½œåœ¨ç©ºé—´çš„æ­¥éª¤çº§å¥–åŠ±å»ºæ¨¡ï¼Œå› ä¸ºå®ƒä»¬è¢«æ˜ç¡®è®¾è®¡ä¸ºå¤„ç†å„ç§å™ªå£°æ°´å¹³çš„æ½œåœ¨å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨å¥–åŠ±æ¨¡å‹ï¼ˆLRMï¼‰ï¼Œè¯¥æ¨¡å‹é‡æ–°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç»„ä»¶æ¥é¢„æµ‹ä»»æ„æ—¶é—´æ­¥é•¿çš„æ½œåœ¨å›¾åƒçš„åå¥½ã€‚åŸºäºLRMï¼Œæˆ‘ä»¬ä»‹ç»äº†æ½œåœ¨åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç›´æ¥åœ¨å™ªå£°æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œçš„æ­¥éª¤çº§åå¥½ä¼˜åŒ–æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLPOæ˜¾è‘—æé«˜äº†æ¨¡å‹ä¸ä¸€èˆ¬åå¥½ã€å®¡ç¾åå¥½å’Œæ–‡æœ¬å›¾åƒå¯¹é½åå¥½çš„å¯¹é½ç¨‹åº¦ï¼ŒåŒæ—¶å®ç°äº†å¯¹ç°æœ‰åå¥½ä¼˜åŒ–æ–¹æ³•çš„2.5-28å€è®­ç»ƒé€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kwai-Kolors/LPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Kwai-Kolors/LPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01051v3">PDF</a> 25 pages, 26 tables, 15 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„åå¥½ä¼˜åŒ–é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•Latent Reward Modelï¼ˆLRMï¼‰ï¼Œé€šè¿‡åœ¨å™ªå£°æ½œç©ºé—´ä¸­åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå»ºæ¨¡ï¼Œé¢„æµ‹ä¸åŒæ—¶é—´æ­¥é•¿çš„æ½œåœ¨å›¾åƒçš„åå¥½ã€‚åŸºäºæ­¤ï¼Œæ–‡ç« è¿›ä¸€æ­¥å¼•å…¥äº†Latent Preference Optimizationï¼ˆLPOï¼‰æ–¹æ³•ï¼Œç›´æ¥åœ¨å™ªå£°æ½œç©ºé—´è¿›è¡Œæ­¥éª¤çº§åå¥½ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLPOèƒ½æ˜¾è‘—æé«˜æ¨¡å‹ä¸é€šç”¨ã€ç¾å­¦å’Œæ–‡æœ¬å›¾åƒå¯¹é½åå¥½çš„å¯¹é½ç¨‹åº¦ï¼ŒåŒæ—¶ç›¸æ¯”ç°æœ‰åå¥½ä¼˜åŒ–æ–¹æ³•å®ç°äº†2.5-28å€çš„è®­ç»ƒåŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åå¥½ä¼˜åŒ–ä¸­éœ€è¦ä¸äººç±»å›¾åƒåå¥½å¯¹é½ã€‚</li>
<li>ä¹‹å‰çš„æ–¹æ³•ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºåƒç´ çº§å¥–åŠ±æ¨¡å‹æ¥è¿‘ä¼¼äººç±»åå¥½ã€‚</li>
<li>åœ¨å¤„ç†ä¸åŒæ—¶é—´æ­¥é•¿çš„å™ªå£°å›¾åƒæ—¶ï¼ŒVLMsé¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è¿›è¡Œå¤æ‚çš„åƒç´ ç©ºé—´è½¬æ¢ã€‚</li>
<li>é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é€‚åˆåœ¨å™ªå£°æ½œç©ºé—´ä¸­è¿›è¡Œæ­¥éª¤çº§å¥–åŠ±å»ºæ¨¡ã€‚</li>
<li>æå‡ºäº†Latent Reward Modelï¼ˆLRMï¼‰ç”¨äºé¢„æµ‹ä»»æ„æ—¶é—´æ­¥é•¿çš„æ½œåœ¨å›¾åƒåå¥½ã€‚</li>
<li>åŸºäºLRMï¼Œå¼•å…¥äº†Latent Preference Optimizationï¼ˆLPOï¼‰æ–¹æ³•ï¼Œç›´æ¥åœ¨å™ªå£°æ½œç©ºé—´è¿›è¡Œæ­¥éª¤çº§åå¥½ä¼˜åŒ–ã€‚</li>
<li>LPOåœ¨æ¨¡å‹ä¸é€šç”¨ã€ç¾å­¦å’Œæ–‡æœ¬å›¾åƒå¯¹é½åå¥½å¯¹é½æ–¹é¢è¡¨ç°æ˜¾è‘—ï¼ŒåŒæ—¶å®ç°äº†è®­ç»ƒåŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-172338946990aa612b068d7939c18a63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc816c8f9568dc0d072e564ee17dd69b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c9960051f95bee20893798fca70f952.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-babb42687d60b9ef82bdbb724f8b555d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e96cf6a34a8bf6c8242b3b3d2c0f727.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Unforgettable-Lessons-from-Forgettable-Images-Intra-Class-Memorability-Matters-in-Computer-Vision"><a href="#Unforgettable-Lessons-from-Forgettable-Images-Intra-Class-Memorability-Matters-in-Computer-Vision" class="headerlink" title="Unforgettable Lessons from Forgettable Images: Intra-Class Memorability   Matters in Computer Vision"></a>Unforgettable Lessons from Forgettable Images: Intra-Class Memorability   Matters in Computer Vision</h2><p><strong>Authors:Jie Jing, Qing Lin, Shuangpeng Han, Lucia Schiatti, Yen-Ling Kuo, Mengmi Zhang</strong></p>
<p>We introduce intra-class memorability, where certain images within the same class are more memorable than others despite shared category characteristics. To investigate what features make one object instance more memorable than others, we design and conduct human behavior experiments, where participants are shown a series of images, and they must identify when the current image matches the image presented a few steps back in the sequence. To quantify memorability, we propose the Intra-Class Memorability score (ICMscore), a novel metric that incorporates the temporal intervals between repeated image presentations into its calculation. Furthermore, we curate the Intra-Class Memorability Dataset (ICMD), comprising over 5,000 images across ten object classes with their ICMscores derived from 2,000 participantsâ€™ responses. Subsequently, we demonstrate the usefulness of ICMD by training AI models on this dataset for various downstream tasks: memorability prediction, image recognition, continual learning, and memorability-controlled image editing. Surprisingly, high-ICMscore images impair AI performance in image recognition and continual learning tasks, while low-ICMscore images improve outcomes in these tasks. Additionally, we fine-tune a state-of-the-art image diffusion model on ICMD image pairs with and without masked semantic objects. The diffusion model can successfully manipulate image elements to enhance or reduce memorability. Our contributions open new pathways in understanding intra-class memorability by scrutinizing fine-grained visual features behind the most and least memorable images and laying the groundwork for real-world applications in computer vision. We will release all code, data, and models publicly. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ç±»å†…è®°å¿†æ€§çš„æ¦‚å¿µï¼Œå³åŒä¸€ç±»åˆ«ä¸­çš„æŸäº›å›¾åƒæ¯”å…¶ä»–å›¾åƒæ›´å®¹æ˜“è®°ä½ï¼Œå°½ç®¡å®ƒä»¬å…·æœ‰å…±åŒçš„ç±»åˆ«ç‰¹å¾ã€‚ä¸ºäº†æ¢ç©¶å“ªäº›ç‰¹å¾ä½¿å¾—ä¸€ä¸ªå¯¹è±¡å®ä¾‹æ¯”å…¶ä»–å®ä¾‹æ›´å®¹æ˜“è®°ä½ï¼Œæˆ‘ä»¬è®¾è®¡å¹¶è¿›è¡Œäº†äººç±»è¡Œä¸ºå®éªŒï¼Œå®éªŒå‚ä¸è€…ä¼šè§‚çœ‹ä¸€ç³»åˆ—å›¾åƒï¼Œå¹¶å¿…é¡»åˆ¤æ–­å½“å‰å›¾åƒæ˜¯å¦ä¸åºåˆ—ä¸­å‡ æ­¥ä¹‹å‰çš„å›¾åƒç›¸åŒ¹é…ã€‚ä¸ºäº†é‡åŒ–è®°å¿†æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç±»å†…è®°å¿†æ€§å¾—åˆ†ï¼ˆICMscoreï¼‰è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œå®ƒå°†é‡å¤å‘ˆç°å›¾åƒä¹‹é—´çš„æ—¶é—´é—´éš”çº³å…¥è®¡ç®—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ•´ç†äº†ç±»å†…è®°å¿†æ€§æ•°æ®é›†ï¼ˆICMDï¼‰ï¼ŒåŒ…å«10ä¸ªå¯¹è±¡ç±»åˆ«çš„è¶…è¿‡5000å¼ å›¾åƒï¼Œä»¥åŠæ¥è‡ª2000åå‚ä¸è€…çš„ICMscoreæ•°æ®ã€‚æ¥ç€ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒäººå·¥æ™ºèƒ½æ¨¡å‹æ¥å±•ç¤ºICMDçš„å®ç”¨æ€§ï¼Œç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼šè®°å¿†æ€§é¢„æµ‹ã€å›¾åƒè¯†åˆ«ã€æŒç»­å­¦ä¹ å’Œè®°å¿†æ€§æ§åˆ¶å›¾åƒå¤„ç†ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé«˜ICMscoreçš„å›¾åƒä¼šæŸå®³äººå·¥æ™ºèƒ½åœ¨å›¾åƒè¯†åˆ«å’ŒæŒç»­å­¦ä¹ ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè€Œä½ICMscoreçš„å›¾åƒåˆ™èƒ½æ”¹å–„è¿™äº›ä»»åŠ¡çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¤„ç†å¸¦æœ‰å’Œä¸å¸¦æ©è†œè¯­ä¹‰å¯¹è±¡çš„ICMDå›¾åƒå¯¹ã€‚è¯¥æ‰©æ•£æ¨¡å‹èƒ½å¤ŸæˆåŠŸæ“æ§å›¾åƒå…ƒç´ ä»¥å¢å¼ºæˆ–é™ä½è®°å¿†æ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®é€šè¿‡å®¡æŸ¥æœ€è®°äººéš¾å¿˜å’Œæœ€å®¹æ˜“é—å¿˜çš„å›¾åƒèƒŒåçš„ç²¾ç»†è§†è§‰ç‰¹å¾ï¼Œä¸ºäººä»¬äº†è§£ç±»å†…è®°å¿†æ€§å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå¹¶ä¸ºè®¡ç®—æœºè§†è§‰çš„å®é™…åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20761v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒä¸€ç±»åˆ«å†…éƒ¨ä¸åŒå›¾åƒé—´çš„å¯è®°å¿†æ€§å·®å¼‚ï¼Œå³å°½ç®¡å…·æœ‰ç›¸åŒçš„ç±»åˆ«ç‰¹å¾ï¼Œä½†æŸäº›å›¾åƒä»ç„¶æ¯”å…¶ä»–å›¾åƒæ›´å®¹æ˜“è¢«è®°ä½ã€‚é€šè¿‡è®¾è®¡äººç±»è¡Œä¸ºå®éªŒæ¥æ¢ç©¶åŒä¸€å¯¹è±¡å®ä¾‹ä¸­å“ªäº›ç‰¹å¾ä½¿å…¶æ¯”å…¶ä»–å®ä¾‹æ›´å®¹æ˜“è¢«è®°ä½ã€‚æå‡ºä¸€ç§æ–°å‹æŒ‡æ ‡â€”â€”ICMåˆ†æ•°ï¼ˆIntra-Class Memorability scoreï¼‰ï¼Œç”¨äºé‡åŒ–å›¾åƒçš„å¯è®°å¿†æ€§ï¼Œå¹¶è€ƒè™‘åˆ°å›¾åƒé‡å¤å‘ˆç°çš„æ—¶é—´é—´éš”ã€‚åŒæ—¶æ„å»ºäº†åŒ…å«è¶…è¿‡äº”åƒå¼ å›¾åƒå’Œå®ƒä»¬çš„ICMåˆ†æ•°çš„ICMDæ•°æ®é›†ã€‚åŸºäºICMDæ•°æ®é›†è®­ç»ƒAIæ¨¡å‹ç”¨äºå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯è®°å¿†æ€§é¢„æµ‹ã€å›¾åƒè¯†åˆ«ã€æŒç»­å­¦ä¹ å’Œå¯è®°å¿†æ€§æ§åˆ¶å›¾åƒç¼–è¾‘ç­‰ã€‚ç ”ç©¶å‘ç°é«˜ICMåˆ†æ•°çš„å›¾åƒåœ¨å›¾åƒè¯†åˆ«å’ŒæŒç»­å­¦ä¹ ä»»åŠ¡ä¸­ä¼šé™ä½AIæ€§èƒ½ï¼Œè€Œä½ICMåˆ†æ•°å›¾åƒåˆ™èƒ½æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥å¤„ç†ICMDå›¾åƒå¯¹å¸¦æœ‰å’Œä¸å¸¦è¯­ä¹‰å¯¹è±¡çš„æ©è†œå›¾åƒã€‚è¯¥æ‰©æ•£æ¨¡å‹å¯ä»¥æˆåŠŸæ“çºµå›¾åƒå…ƒç´ ä»¥æé«˜æˆ–é™ä½å¯è®°å¿†æ€§ã€‚æœ¬æ–‡çš„ç ”ç©¶å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œé€šè¿‡æ·±å…¥ç ”ç©¶æœ€è®°å¿†æ·±åˆ»å’Œæœ€ä¸æ˜“è®°å¿†çš„å›¾åƒçš„ç²¾ç»†è§†è§‰ç‰¹å¾æ¥ç†è§£åŒä¸€ç±»åˆ«å†…çš„å¯è®°å¿†æ€§å·®å¼‚ï¼Œå¹¶ä¸ºè®¡ç®—æœºè§†è§‰çš„å®é™…åº”ç”¨å¥ å®šåŸºç¡€ã€‚æˆ‘ä»¬å°†å…¬å¼€æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼•å…¥åŒä¸€ç±»åˆ«å†…ä¸åŒå›¾åƒçš„å¯è®°å¿†æ€§å·®å¼‚æ¦‚å¿µã€‚</li>
<li>é€šè¿‡äººç±»è¡Œä¸ºå®éªŒæ¢ç©¶å“ªäº›ç‰¹å¾ä½¿æŸä¸€å¯¹è±¡å®ä¾‹æ¯”å…¶ä»–å®ä¾‹æ›´æ˜“è¢«è®°ä½ã€‚</li>
<li>æå‡ºICMåˆ†æ•°æŒ‡æ ‡ï¼Œç»“åˆé‡å¤å‘ˆç°å›¾åƒçš„æ—¶é—´é—´éš”æ¥é‡åŒ–å¯è®°å¿†æ€§ã€‚</li>
<li>æ„å»ºICMDæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡äº”åƒå¼ å›¾åƒå’Œå®ƒä»¬çš„ICMåˆ†æ•°ã€‚</li>
<li>åŸºäºICMDæ•°æ®é›†è®­ç»ƒAIæ¨¡å‹ï¼Œç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚è®°å¿†é¢„æµ‹ã€å›¾åƒè¯†åˆ«ç­‰ã€‚</li>
<li>å‘ç°é«˜ICMåˆ†æ•°å›¾åƒåœ¨å›¾åƒè¯†åˆ«å’ŒæŒç»­å­¦ä¹ ä»»åŠ¡ä¸­ä¼šé™ä½AIæ€§èƒ½ï¼Œè€Œä½ICMåˆ†æ•°å›¾åƒåˆ™ç›¸åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-68f0c4e6cc42c97d9bea1cd332caa1a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfd22f47c766b8b5e7f2efc845315211.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e14ba6c307c547e74d6cfe5a4a81981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca5d954feeccd0baefdd31a613b57f07.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance"><a href="#Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance" class="headerlink" title="Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance"></a>Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance</h2><p><strong>Authors:Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang</strong></p>
<p>Recently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after removal. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pre-trained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„æ–°å…´åŠ›é‡ï¼Œåœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå½“ç”¨äºç›®æ ‡ç§»é™¤ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚äº§ç”Ÿéšæœºä¼ªå½±å’Œåœ¨ç§»é™¤åæ— æ³•ç”¨é€‚å½“çš„å†…å®¹é‡æ–°ç»˜åˆ¶å‰æ™¯å¯¹è±¡åŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œAttentive Eraserâ€ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è°ƒæ•´çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ–¹æ³•ï¼Œå¯å®ç°ç¨³å®šå’Œæœ‰æ•ˆçš„ç›®æ ‡ç§»é™¤ã€‚é¦–å…ˆï¼ŒåŸºäºè§‚å¯Ÿåˆ°è‡ªæ³¨æ„åŠ›å›¾ä¼šå½±å“ç”Ÿæˆå›¾åƒçš„ç»“æ„å’Œå½¢çŠ¶ç»†èŠ‚ï¼Œæˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›æ¿€æ´»å’ŒæŠ‘åˆ¶ï¼ˆASSï¼‰ï¼Œå®ƒæ ¹æ®ç»™å®šçš„æ©è†œé‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œåœ¨åå‘ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼˜å…ˆè€ƒè™‘èƒŒæ™¯è€Œéå‰æ™¯ç›®æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæ³¨æ„åŠ›é‡å®šå‘å¼•å¯¼ï¼ˆSARGï¼‰ï¼Œå®ƒåˆ©ç”¨ASSå¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œåœ¨æ©è†œå†…æœ‰æ•ˆåœ°ç§»é™¤å‰æ™¯ç›®æ ‡ï¼ŒåŒæ—¶ç”Ÿæˆæ—¢åˆç†åˆè¿è´¯çš„å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒAttentive Eraseråœ¨å„ç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„ç›®æ ‡ç§»é™¤è¡¨ç°ç¨³å®šä¸”æœ‰æ•ˆï¼Œç”šè‡³è¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒAttentive Eraserå¯åº”ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser%E3%80%82">https://github.com/Anonym0u3/AttentiveEraserã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12974v5">PDF</a> Accepted by AAAI 2025(Oral)</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸå´­éœ²å¤´è§’ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œåœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸­ï¼Œä»å­˜åœ¨ç”Ÿæˆéšæœºç‘•ç–µå’Œç§»é™¤å‰æ™¯å¯¹è±¡åæ— æ³•é‡æ–°ç»˜åˆ¶é€‚å½“å†…å®¹çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºæ— éœ€è°ƒæ•´çš„â€œAttentive Eraserâ€æ–¹æ³•ï¼Œä¸ºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å®ç°ç¨³å®šæœ‰æ•ˆçš„å¯¹è±¡ç§»é™¤ã€‚é€šè¿‡åˆ©ç”¨è‡ªæˆ‘å…³æ³¨å›¾å½±å“å›¾åƒç»“æ„å’Œå½¢çŠ¶ç»†èŠ‚çš„è§‚å¯Ÿç»“æœï¼Œæå‡ºåŸºäºç»™å®šé®ç½©é‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæˆ‘å…³æ³¨æœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†èƒŒæ™¯è€Œéå‰æ™¯å¯¹è±¡ã€‚æ­¤å¤–ï¼Œå¼•å…¥è‡ªæˆ‘å…³æ³¨é‡å®šå‘æŒ‡å¯¼ï¼ˆSARGï¼‰ï¼Œæœ‰æ•ˆç§»é™¤é®ç½©å†…çš„å‰æ™¯å¯¹è±¡ï¼ŒåŒæ—¶ç”Ÿæˆæ—¢åˆç†åˆè¿è´¯çš„å†…å®¹ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­è¡¨ç°ç¨³å®šæœ‰æ•ˆï¼Œç”šè‡³è¶…è¶ŠåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚åŒæ—¶ï¼ŒAttentive Eraserå¯åº”ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œå±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºâ€œAttentive Eraserâ€çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ‰©æ•£æ¨¡å‹åœ¨å¯¹è±¡ç§»é™¤ä¸­çš„éšæœºç‘•ç–µå’Œé‡æ–°ç»˜åˆ¶é—®é¢˜ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨è‡ªæˆ‘å…³æ³¨å›¾å½±å“å›¾åƒç»“æ„å’Œå½¢çŠ¶ç»†èŠ‚çš„ç‰¹æ€§ï¼Œé‡æ–°è®¾è®¡è‡ªæˆ‘å…³æ³¨æœºåˆ¶ä»¥ä¼˜å…ˆå¤„ç†èƒŒæ™¯ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥Self-Attention Redirection Guidanceï¼ˆSARGï¼‰æŠ€æœ¯ï¼Œèƒ½æœ‰æ•ˆç§»é™¤å‰æ™¯å¯¹è±¡å¹¶ç”Ÿæˆè¿è´¯å†…å®¹ã€‚</li>
<li>Attentive Eraseræ–¹æ³•åœ¨å„ç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­è¡¨ç°ç¨³å®šä¸”æœ‰æ•ˆï¼Œä¼˜äºè®¸å¤šåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯åº”ç”¨äºä¸åŒçš„æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5701bdb81c96f675a396ddf032d08ddd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2072630d561b95396afa914c2d8e33b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60d90a9fc622e916ad2be2cbe76154b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d38d1501446a78f9354c70defe4087cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a0f7e55399d95265f218e3fdd27c817.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Learning-to-Discretize-Denoising-Diffusion-ODEs"><a href="#Learning-to-Discretize-Denoising-Diffusion-ODEs" class="headerlink" title="Learning to Discretize Denoising Diffusion ODEs"></a>Learning to Discretize Denoising Diffusion ODEs</h2><p><strong>Authors:Vinh Tong, Hoang Trung-Dung, Anji Liu, Guy Van den Broeck, Mathias Niepert</strong></p>
<p>Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFEs) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency with much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/vinhsuhi/LD3">https://github.com/vinhsuhi/LD3</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå…¶åœ¨å›¾åƒåˆæˆå’Œ3Dç‚¹äº‘ç”Ÿæˆç­‰é¢†åŸŸè¡¨ç°å‡ºç«äº‰æ€§èƒ½ã€‚ä»é¢„è®­ç»ƒçš„DPMsä¸­è¿›è¡Œé‡‡æ ·æ¶‰åŠå¤šæ¬¡ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰ï¼Œå°†é«˜æ–¯å™ªå£°æ ·æœ¬è½¬æ¢ä¸ºå›¾åƒï¼Œè¿™å¯¼è‡´ä¸è¯¸å¦‚GANæˆ–VAEç­‰å•æ­¥ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼Œè®¡ç®—æˆæœ¬æ›´é«˜ã€‚å› æ­¤ï¼Œåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶å‡å°‘NFEçš„æ•°é‡è‡³å…³é‡è¦ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LD3ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ é‡‡æ ·çš„æœ€ä½³æ—¶é—´ç¦»æ•£åŒ–ã€‚LD3å¯ä»¥ä¸å„ç§é‡‡æ ·å™¨ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒèµ„æºå¯†é›†å‹çš„ç¥ç»ç½‘ç»œçš„æƒ…å†µä¸‹ï¼ŒæŒç»­æ”¹å–„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯è¯æ˜ï¼ŒLD3èƒ½æé«˜é‡‡æ ·æ•ˆç‡ï¼Œä¸”è®¡ç®—å¼€é”€è¾ƒå°ã€‚æˆ‘ä»¬åœ¨7ä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œæ¶µç›–äº†åƒç´ ç©ºé—´å’Œæ½œåœ¨ç©ºé—´DPMsçš„æ— æ¡ä»¶é‡‡æ ·å’Œæ¡ä»¶é‡‡æ ·ã€‚æˆ‘ä»¬åœ¨æ— æ¡ä»¶CIFAR10å’ŒAFHQv2ä¸Šå®ç°äº†FID 2.38ï¼ˆ10æ¬¡NFEï¼‰å’ŒFID 2.27ï¼ˆåœ¨5-10åˆ†é’Ÿçš„è®­ç»ƒæ—¶é—´å†…ï¼‰ï¼Œæ˜¾ç¤ºäº†åœ¨æ— æ¡ä»¶ä¸‹çš„æ•ˆæœã€‚LD3ä¸ºä»é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š[LD3é¡¹ç›®åœ¨GitHubçš„é¡µé¢é“¾æ¥]ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15506v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    DPMï¼ˆæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼‰æ˜¯è¡¨ç°ä¼˜å¼‚çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒåˆæˆå’Œä¸‰ç»´ç‚¹äº‘ç”Ÿæˆç­‰é¢†åŸŸã€‚ç”±äºDPMçš„é‡‡æ ·æ¶‰åŠåˆ°å¤šä¸ªç¥ç»ç½‘ç»œåŠŸèƒ½çš„è¯„ä¼°ï¼ˆNFEï¼‰ï¼Œä»¥ä»é«˜æ–¯å™ªå£°æ ·æœ¬ç”Ÿæˆå›¾åƒï¼Œå¯¼è‡´å…¶è®¡ç®—æˆæœ¬é«˜äºå•æ­¥ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANæˆ–VAEï¼‰ã€‚æœ¬æ–‡æå‡ºçš„LD3æ¡†æ¶æ—¨åœ¨é™ä½NFEæ•°é‡ï¼ŒåŒæ—¶ä¿ç•™ç”Ÿæˆè´¨é‡ã€‚é€šè¿‡ä¼˜åŒ–æ—¶é—´ç¦»æ•£åŒ–é‡‡æ ·è¿‡ç¨‹ï¼ŒLD3å¯ä»¥ä¸å„ç§é‡‡æ ·å™¨ç»“åˆä½¿ç”¨ï¼Œæ— éœ€é‡æ–°è®­ç»ƒèµ„æºå¯†é›†å‹çš„ç¥ç»ç½‘ç»œå³å¯æé«˜ç”Ÿæˆè´¨é‡ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒLD3å¯æé«˜é‡‡æ ·æ•ˆç‡å¹¶é™ä½è®¡ç®—å¼€é”€ã€‚åœ¨ä¸ƒä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— æ¡ä»¶CIFAR10å’ŒAFHQv2ä¸Šå®ç°äº†é«˜è¾¾FIDsçš„2.38ï¼ˆä½¿ç”¨è¾ƒå°‘çš„é‡‡æ ·æ¬¡æ•°ï¼‰å’Œè¾ƒä½çš„FIDåˆ†æ•°ã€‚æ€»ä½“è€Œè¨€ï¼ŒLD3æä¾›äº†ä¸€ç§é«˜æ•ˆçš„å¯¹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œé‡‡æ ·çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DPMsåœ¨å„ç§é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†é‡‡æ ·è¿‡ç¨‹æ¶‰åŠå¤šä¸ªNFEï¼Œè®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>LD3æ¡†æ¶æ—¨åœ¨å‡å°‘NFEæ•°é‡ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚</li>
<li>LD3å¯é€šè¿‡ä¼˜åŒ–æ—¶é—´ç¦»æ•£åŒ–é‡‡æ ·è¿‡ç¨‹å®ç°è¿™ä¸€ç›®æ ‡ã€‚</li>
<li>LD3å¯ç»“åˆå„ç§é‡‡æ ·å™¨ä½¿ç”¨ï¼Œæ— éœ€é‡æ–°è®­ç»ƒç¥ç»ç½‘ç»œã€‚</li>
<li>LD3æé«˜äº†é‡‡æ ·æ•ˆç‡å¹¶é™ä½äº†è®¡ç®—å¼€é”€ã€‚</li>
<li>åœ¨ä¸ƒä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜äº†LD3çš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­å¯¹CIFAR10å’ŒAFHQv2çš„å®éªŒç»“æœè¡¨æ˜äº†è¾ƒé«˜çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e690a459635d57a9d77ad1d4d84f933.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b48848d8a2a324c2add99dd6356c07c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-637f5670d77fbad7e66095ace0255e09.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner"><a href="#CraftsMan3D-High-fidelity-Mesh-Generation-with-3D-Native-Generation-and-Interactive-Geometry-Refiner" class="headerlink" title="CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner"></a>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and   Interactive Geometry Refiner</h2><p><strong>Authors:Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long</strong></p>
<p>We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, irregular mesh topologies, noisy surfaces, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling software. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we employ a 3D native diffusion model, which operates on latent space learned from latent set-based 3D representations, to generate coarse geometries with regular mesh topology in seconds. In particular, this process takes as input a text prompt or a reference image and leverages a powerful multi-view (MV) diffusion model to generate multiple views of the coarse geometry, which are fed into our MV-conditioned 3D diffusion model for generating the 3D geometry, significantly improving robustness and generalizability. Following that, a normal-based geometry refiner is used to significantly enhance the surface details. This refinement can be performed automatically, or interactively with user-supplied edits. Extensive experiments demonstrate that our method achieves high efficacy in producing superior-quality 3D assets compared to existing methods. HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code: <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan">https://github.com/wyysf-98/CraftsMan</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸‰ç»´å»ºæ¨¡ç³»ç»Ÿï¼Œåä¸ºCraftsManã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´å‡ ä½•ä½“ï¼Œå…·å¤‡å¤šæ ·åŒ–çš„å½¢çŠ¶ã€è§„åˆ™çš„ç½‘æ ¼æ‹“æ‰‘ç»“æ„å’Œç²¾ç»†çš„è¡¨é¢ç»†èŠ‚ï¼Œå¹¶ä¸”æ”¯æŒäº¤äº’å¼åœ°å¯¹å‡ ä½•ç»“æ„è¿›è¡Œç»†åŒ–ã€‚å°½ç®¡ä¸‰ç»´ç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´ä¼˜åŒ–è¿‡ç¨‹å†—é•¿ã€ç½‘æ ¼æ‹“æ‰‘ç»“æ„ä¸è§„åˆ™ã€è¡¨é¢å™ªå£°ä»¥åŠéš¾ä»¥å®¹çº³ç”¨æˆ·ç¼–è¾‘ç­‰é—®é¢˜ï¼Œè¿™äº›é—®é¢˜é˜»ç¢äº†å®ƒä»¬åœ¨ä¸‰ç»´å»ºæ¨¡è½¯ä»¶ä¸­çš„å¹¿æ³›é‡‡ç”¨å’Œå®æ–½ã€‚æˆ‘ä»¬çš„å·¥ä½œå—åˆ°å·¥åŒ çš„å¯å‘ï¼Œä»–ä»¬é€šå¸¸é¦–å…ˆå¤§è‡´å‹¾å‹’å‡ºä½œå“çš„æ•´ä½“è½®å»“ï¼Œç„¶åç»†åŒ–è¡¨é¢ç»†èŠ‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸‰ç»´æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸Šè¿è¡Œï¼Œè¯¥æ½œåœ¨ç©ºé—´æ˜¯ä»åŸºäºé›†åˆçš„æ½œåœ¨ä¸‰ç»´è¡¨ç¤ºä¸­å­¦ä¹ å¾—åˆ°çš„ï¼Œå¯ä»¥åœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆå…·æœ‰è§„åˆ™ç½‘æ ¼æ‹“æ‰‘ç»“æ„çš„ç²—ç•¥å‡ ä½•ç»“æ„ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™ä¸€è¿‡ç¨‹ä»¥æ–‡æœ¬æç¤ºæˆ–å‚è€ƒå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶åˆ©ç”¨å¼ºå¤§çš„å¤šè§†å›¾ï¼ˆMVï¼‰æ‰©æ•£æ¨¡å‹ç”Ÿæˆç²—ç•¥å‡ ä½•ç»“æ„çš„å¤šè§†å›¾ï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°æˆ‘ä»¬çš„MVæ¡ä»¶ä¸‰ç»´æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆä¸‰ç»´å‡ ä½•ç»“æ„ï¼Œè¿™å¤§å¤§æé«˜äº†ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚ä¹‹åï¼Œä½¿ç”¨åŸºäºæ³•çº¿çš„å‡ ä½•ç»†åŒ–å™¨æ¥æ˜¾è‘—å¢å¼ºè¡¨é¢ç»†èŠ‚ã€‚è¿™ç§ç»†åŒ–å¯ä»¥è‡ªåŠ¨è¿›è¡Œï¼Œä¹Ÿå¯ä»¥ä¸ç”¨æˆ·æä¾›çš„ç¼–è¾‘è¿›è¡Œäº¤äº’å¼æ“ä½œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡ä¸‰ç»´èµ„äº§æ–¹é¢ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰é«˜æ•ˆæ€§ã€‚ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://craftsman3d.github.io/%EF%BC%8C%E4%BB%A3%E7%A0%81%EF%BC%9Ahttps://github.com/wyysf-98/CraftsMan">https://craftsman3d.github.io/ï¼Œä»£ç ï¼šhttps://github.com/wyysf-98/CraftsMan</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14979v2">PDF</a> HomePage: <a target="_blank" rel="noopener" href="https://craftsman3d.github.io/">https://craftsman3d.github.io/</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/CraftsMan3D">https://github.com/wyysf-98/CraftsMan3D</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥é¡¹ç›®æå‡ºäº†ä¸€ç§åä¸ºCraftsMançš„æ–°å‹ä¸‰ç»´å»ºæ¨¡ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹ï¼Œå…·æœ‰å¤šæ ·åŒ–çš„å½¢çŠ¶ã€è§„åˆ™åŒ–çš„ç½‘æ ¼æ‹“æ‰‘å’Œç²¾ç»†çš„è¡¨é¢ç»†èŠ‚ï¼Œå¹¶ä¸”å…è®¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¼å‡ ä½•ç²¾ç»†åŒ–ã€‚ç³»ç»Ÿåˆ©ç”¨ä¸‰ç»´æ‰©æ•£æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ“ä½œï¼Œèƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆå…·æœ‰è§„åˆ™ç½‘æ ¼æ‹“æ‰‘çš„ç²—ç•¥å‡ ä½•å½¢çŠ¶ï¼Œå¹¶é€šè¿‡å¤šè§†è§’æ‰©æ•£æ¨¡å‹è¿›ä¸€æ­¥æé«˜ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜ä½¿ç”¨åŸºäºæ³•çº¿çš„å‡ ä½•ç²¾ç»†åŒ–å™¨æ¥å¢å¼ºè¡¨é¢ç»†èŠ‚ï¼Œå¹¶å…è®¸è‡ªåŠ¨æˆ–äº¤äº’å¼è¿›è¡Œç”¨æˆ·ç¼–è¾‘ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ä¸‰ç»´èµ„äº§è´¨é‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CraftsManæ˜¯ä¸€ä¸ªæ–°å‹çš„ä¸‰ç»´å»ºæ¨¡ç³»ç»Ÿï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤šæ ·åŒ–çš„å½¢çŠ¶ã€è§„åˆ™åŒ–çš„ç½‘æ ¼æ‹“æ‰‘å’Œç²¾ç»†çš„è¡¨é¢ç»†èŠ‚ã€‚</li>
<li>è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸‰ç»´æ‰©æ•£æ¨¡å‹ï¼Œåœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ“ä½œï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆå…·æœ‰è§„åˆ™ç½‘æ ¼æ‹“æ‰‘çš„ç²—ç•¥å‡ ä½•å½¢çŠ¶ã€‚</li>
<li>ç³»ç»Ÿåˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œæé«˜äº†ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å¾—ç”Ÿæˆçš„ä¸‰ç»´æ¨¡å‹æ›´åŠ çœŸå®å’Œå¤šæ ·ã€‚</li>
<li>CraftsManå…è®¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¼çš„å‡ ä½•ç²¾ç»†åŒ–ï¼Œå¢å¼ºäº†è¡¨é¢ç»†èŠ‚ï¼Œæé«˜äº†æ¨¡å‹çš„ç²¾åº¦å’Œé€¼çœŸåº¦ã€‚</li>
<li>è¯¥ç³»ç»Ÿé‡‡ç”¨æ³•çº¿åŸºäºçš„å‡ ä½•ç²¾ç»†åŒ–å™¨ï¼Œå¯ä»¥è‡ªåŠ¨æˆ–äº¤äº’å¼åœ°è¿›è¡Œç”¨æˆ·ç¼–è¾‘ï¼Œæé«˜äº†æ“ä½œçš„çµæ´»æ€§å’Œä¾¿åˆ©æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCraftsManç”Ÿæˆçš„ä¸‰ç»´èµ„äº§è´¨é‡æ›´é«˜ï¼Œå…·æœ‰æ›´å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b551f3b790a2a126d027af291181b8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5bf449708d03a86e4c9efb4e4cd813d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35016a66d5990272953fc5bd05fc344f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a80358eb5cf508aebb5b903dc26a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82ac853d4d86cea304537373b6745573.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DeepMpMRI-Tensor-decomposition-Regularized-Learning-for-Fast-and-High-Fidelity-Multi-Parametric-Microstructural-MR-Imaging"><a href="#DeepMpMRI-Tensor-decomposition-Regularized-Learning-for-Fast-and-High-Fidelity-Multi-Parametric-Microstructural-MR-Imaging" class="headerlink" title="DeepMpMRI: Tensor-decomposition Regularized Learning for Fast and   High-Fidelity Multi-Parametric Microstructural MR Imaging"></a>DeepMpMRI: Tensor-decomposition Regularized Learning for Fast and   High-Fidelity Multi-Parametric Microstructural MR Imaging</h2><p><strong>Authors:Wenxin Fan, Jian Cheng, Qiyuan Tian, Ruoyou Wu, Juan Zou, Zan Chen, Shanshan Wang</strong></p>
<p>Deep learning has emerged as a promising approach for learning the nonlinear mapping between diffusion-weighted MR images and tissue parameters, which enables automatic and deep understanding of the brain microstructures. However, the efficiency and accuracy in estimating multiple microstructural parameters derived from multiple diffusion models are still limited since previous studies tend to estimate parameter maps from distinct models with isolated signal modeling and dense sampling. This paper proposes DeepMpMRI, an efficient framework for fast and high-fidelity multiple microstructural parameter estimation from multiple models using highly sparse sampled q-space data. DeepMpMRI is equipped with a newly designed tensor-decomposition-based regularizer to effectively capture fine details by exploiting the high-dimensional correlation across microstructural parameters. In addition, we introduce a Nesterov-based adaptive learning algorithm that optimizes the regularization parameter dynamically to enhance the performance. DeepMpMRI is an extendable framework capable of incorporating flexible network architecture. Experimental results on the HCP dataset and the Alzheimerâ€™s disease dataset both demonstrate the superiority of our approach over 5 state-of-the-art methods in simultaneously estimating multi-model microstructural parameter maps for DKI and NODDI model with fine-grained details both quantitatively and qualitatively, achieving 4.5 - 15 $\times$ acceleration compared to the dense sampling of a total of 270 diffusion gradients. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œç”¨äºå­¦ä¹ æ‰©æ•£åŠ æƒç£å…±æŒ¯å›¾åƒä¸ç»„ç»‡å‚æ•°ä¹‹é—´çš„éçº¿æ€§æ˜ å°„ï¼Œä»è€Œå®ç°å¤§è„‘å¾®è§‚ç»“æ„çš„è‡ªåŠ¨å’Œæ·±å…¥ç†è§£ã€‚ç„¶è€Œï¼Œç”±äºä»¥å¾€çš„ç ”ç©¶å€¾å‘äºä½¿ç”¨å­¤ç«‹ä¿¡å·å»ºæ¨¡å’Œå¯†é›†é‡‡æ ·ä»ä¸åŒæ¨¡å‹ä¼°è®¡å‚æ•°å›¾ï¼Œå› æ­¤ä»å¤šä¸ªæ‰©æ•£æ¨¡å‹ä¼°è®¡å¤šä¸ªå¾®è§‚ç»“æ„å‚æ•°çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ä»ç„¶å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†DeepMpMRIï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶ï¼Œå¯ä»¥ä»å¤šä¸ªæ¨¡å‹å¿«é€Ÿã€é«˜ä¿çœŸåœ°ä¼°è®¡å¤šä¸ªå¾®è§‚ç»“æ„å‚æ•°ï¼Œä½¿ç”¨é«˜åº¦ç¨€ç–é‡‡æ ·çš„qç©ºé—´æ•°æ®ã€‚DeepMpMRIé…å¤‡äº†ä¸€ç§æ–°è®¾è®¡çš„åŸºäºå¼ é‡åˆ†è§£çš„æ­£åˆ™åŒ–å™¨ï¼Œé€šè¿‡åˆ©ç”¨å¾®è§‚ç»“æ„å‚æ•°ä¹‹é—´çš„é«˜ç»´ç›¸å…³æ€§æœ‰æ•ˆåœ°æ•æ‰ç»†å¾®ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºNesterovçš„è‡ªé€‚åº”å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯åŠ¨æ€ä¼˜åŒ–æ­£åˆ™åŒ–å‚æ•°ä»¥æé«˜æ€§èƒ½ã€‚DeepMpMRIæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé‡‡ç”¨çµæ´»çš„ç½‘ç»œæ¶æ„ã€‚åœ¨HCPæ•°æ®é›†å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå‡è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒæ—¶ä¼°è®¡DKIå’ŒNODDIæ¨¡å‹çš„å¤šæ¨¡å‹å¾®è§‚ç»“æ„å‚æ•°å›¾æ–¹é¢ä¼˜äº5ç§æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½è¡¨ç°å‡ºç²¾ç»†çš„ç»†èŠ‚ï¼Œä¸æ€»å…±270ä¸ªæ‰©æ•£æ¢¯åº¦çš„å¯†é›†é‡‡æ ·ç›¸æ¯”å®ç°äº†4.5-15å€çš„åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.03159v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å­¦ä¹ æ‰©æ•£åŠ æƒæ ¸ç£å…±æŒ¯å›¾åƒä¸ç»„ç»‡ç»“æ„å‚æ•°é—´çš„éçº¿æ€§æ˜ å°„æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œèƒ½è‡ªåŠ¨æ·±å…¥ç†è§£å¤§è„‘å¾®è§‚ç»“æ„ã€‚ç„¶è€Œï¼Œå¤šä¸ªæ¨¡å‹ä¸­å¾®è§‚ç»“æ„å‚æ•°çš„ä¼°ç®—æ•ˆç‡å’Œå‡†ç¡®åº¦å—é™ã€‚æœ¬æ–‡æå‡ºDeepMpMRIæ¡†æ¶ï¼Œèƒ½é«˜æ•ˆåœ°ä»å¤šä¸ªæ¨¡å‹ä¸­å¿«é€Ÿã€é«˜ç²¾åº¦åœ°ä¼°ç®—å¾®è§‚ç»“æ„å‚æ•°ï¼Œä½¿ç”¨é«˜åº¦ç¨€ç–é‡‡æ ·çš„qç©ºé—´æ•°æ®ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ç§åŸºäºå¼ é‡åˆ†è§£çš„æ­£åˆ™åŒ–å™¨ï¼Œåˆ©ç”¨é«˜ç»´å‚æ•°é—´çš„ç›¸å…³æ€§æ•æ‰ç»†èŠ‚ï¼Œå¹¶å¼•å…¥Nesterovè‡ªé€‚åº”å­¦ä¹ ç®—æ³•åŠ¨æ€ä¼˜åŒ–æ­£åˆ™åŒ–å‚æ•°ä»¥æå‡æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepMpMRIåœ¨åŒæ—¶ä¼°ç®—DKIå’ŒNODDIæ¨¡å‹çš„å¤šä¸ªå¾®è§‚ç»“æ„å‚æ•°æ˜ å°„æ–¹é¢ä¼˜äºå…¶ä»–äº”ç§å…ˆè¿›æ–¹æ³•ï¼Œå®ç°äº†4.5~15å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ å¯ç”¨äºç†è§£æ‰©æ•£åŠ æƒæ ¸ç£å…±æŒ¯å›¾åƒä¸å¤§è„‘å¾®è§‚ç»“æ„å‚æ•°é—´çš„æ˜ å°„ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹çš„å¾®è§‚ç»“æ„å‚æ•°ä¼°ç®—æ–¹é¢å­˜åœ¨æ•ˆç‡å’Œå‡†ç¡®åº¦çš„é™åˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶DeepMpMRIï¼Œèƒ½é«˜æ•ˆåœ°ä»å¤šä¸ªæ¨¡å‹ä¸­ä¼°ç®—å¾®è§‚ç»“æ„å‚æ•°ã€‚</li>
<li>DeepMpMRIä½¿ç”¨é«˜åº¦ç¨€ç–é‡‡æ ·çš„qç©ºé—´æ•°æ®ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>DeepMpMRIåˆ©ç”¨å¼ é‡åˆ†è§£çš„æ­£åˆ™åŒ–å™¨æ•æ‰å¾®è§‚ç»“æ„çš„é«˜ç»´ç›¸å…³æ€§ã€‚</li>
<li>å¼•å…¥Nesterovè‡ªé€‚åº”å­¦ä¹ ç®—æ³•ä¼˜åŒ–æ­£åˆ™åŒ–å‚æ•°ï¼Œæå‡äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.03159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf9d4ad4813939c336767b745e988fe4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-156385fa7852d2e0c45341cf1fcca994.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23708b1b0aa84016e084b22a4fe7ba20.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="StainDiffuser-MultiTask-Dual-Diffusion-Model-for-Virtual-Staining"><a href="#StainDiffuser-MultiTask-Dual-Diffusion-Model-for-Virtual-Staining" class="headerlink" title="StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining"></a>StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining</h2><p><strong>Authors:Tushar Kataria, Beatrice Knudsen, Shireen Y. Elhabian</strong></p>
<p>Hematoxylin and Eosin (H&amp;E) staining is widely regarded as the standard in pathology for diagnosing diseases and tracking tumor recurrence. While H&amp;E staining shows tissue structures, it lacks the ability to reveal specific proteins that are associated with disease severity and treatment response. Immunohistochemical (IHC) stains use antibodies to highlight the expression of these proteins on their respective cell types, improving diagnostic accuracy, and assisting with drug selection for treatment. Despite their value, IHC stains require additional time and resources, limiting their utilization in some clinical settings. Recent advances in deep learning have positioned Image-to-Image (I2I) translation as a computational, cost-effective alternative for IHC. I2I generates high fidelity stain transformations digitally, potentially replacing manual staining in IHC. Diffusion models, the current state of the art in image generation and conditional tasks, are particularly well suited for virtual IHC due to their ability to produce high quality images and resilience to mode collapse. However, these models require extensive and diverse datasets (often millions of samples) to achieve a robust performance, a challenge in virtual staining applications where only thousands of samples are typically available. Inspired by the success of multitask deep learning models in scenarios with limited data, we introduce STAINDIFFUSER, a novel multitask diffusion architecture tailored to virtual staining that achieves convergence with smaller datasets. STAINDIFFUSER simultaneously trains two diffusion processes: (a) generating cell specific IHC stains from H&amp;E images and (b) performing H&amp;E based cell segmentation, utilizing coarse segmentation labels exclusively during training. STAINDIFFUSER generates high-quality virtual stains for two markers, outperforming over twenty I2I baselines. </p>
<blockquote>
<p>è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²è¢«å¹¿æ³›è®¤ä¸ºæ˜¯ç—…ç†å­¦è¯Šæ–­ç–¾ç—…å’Œè¿½è¸ªè‚¿ç˜¤å¤å‘çš„æ ‡å‡†ã€‚è™½ç„¶H&amp;EæŸ“è‰²èƒ½å¤Ÿæ˜¾ç¤ºç»„ç»‡ç»“æ„ï¼Œä½†å®ƒæ— æ³•æ­ç¤ºä¸ç–¾ç—…ä¸¥é‡ç¨‹åº¦å’Œæ²»ç–—ååº”ç›¸å…³çš„ç‰¹å®šè›‹ç™½è´¨ã€‚å…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰æŸ“è‰²ä½¿ç”¨æŠ—ä½“æ¥çªå‡ºè¿™äº›è›‹ç™½è´¨åœ¨å„è‡ªç»†èƒç±»å‹ä¸Šçš„è¡¨è¾¾ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œå¹¶è¾…åŠ©è¯ç‰©æ²»ç–—é€‰æ‹©ã€‚å°½ç®¡IHCæŸ“è‰²å…·æœ‰é‡è¦ä»·å€¼ï¼Œä½†å®ƒéœ€è¦é¢å¤–çš„æ—¶é—´å’Œèµ„æºï¼Œå› æ­¤åœ¨æŸäº›ä¸´åºŠç¯å¢ƒä¸­é™åˆ¶äº†å…¶ä½¿ç”¨ã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ä½¿å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘æˆä¸ºäº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜ã€æˆæœ¬æ•ˆç›Šå¥½çš„IHCæ›¿ä»£æ–¹æ¡ˆã€‚I2Ièƒ½å¤Ÿæ•°å­—ç”Ÿæˆé«˜ä¿çœŸæŸ“è‰²è½¬æ¢ï¼Œå¯èƒ½æ›¿ä»£IHCä¸­çš„æ‰‹åŠ¨æŸ“è‰²ã€‚æ‰©æ•£æ¨¡å‹æ˜¯ç›®å‰å›¾åƒç”Ÿæˆå’Œæ¡ä»¶ä»»åŠ¡çš„æœ€æ–°æŠ€æœ¯ï¼Œç‰¹åˆ«é€‚åˆç”¨äºè™šæ‹ŸIHCï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒå¹¶ä¸”å¯¹æŠ—æ¨¡å¼å´©æºƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ˆé€šå¸¸æ˜¯æ•°ç™¾ä¸‡ä¸ªæ ·æœ¬ï¼‰æ‰èƒ½å®ç°ç¨³å¥çš„æ€§èƒ½ï¼Œè¿™åœ¨è™šæ‹ŸæŸ“è‰²åº”ç”¨ä¸­æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œé€šå¸¸åªæœ‰æ•°åƒä¸ªæ ·æœ¬å¯ç”¨ã€‚å—å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æœ‰é™æ•°æ®åœºæ™¯ä¸­çš„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†STAINDIFFUSERï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è™šæ‹ŸæŸ“è‰²çš„æ–°å‹å¤šä»»åŠ¡æ‰©æ•£æ¶æ„ï¼Œå¯åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šå®ç°æ”¶æ•›ã€‚STAINDIFFUSERåŒæ—¶è®­ç»ƒä¸¤ä¸ªæ‰©æ•£è¿‡ç¨‹ï¼šï¼ˆaï¼‰ä»H&amp;Eå›¾åƒç”Ÿæˆç»†èƒç‰¹å¼‚æ€§IHCæŸ“è‰²ï¼›ï¼ˆbï¼‰åŸºäºH&amp;Eè¿›è¡Œç»†èƒåˆ†å‰²ï¼Œä»…åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ç²—ç•¥çš„åˆ†å‰²æ ‡ç­¾ã€‚STAINDIFFUSERä¸ºä¸¤ç§æ ‡è®°ç”Ÿæˆé«˜è´¨é‡çš„è™šæ‹ŸæŸ“è‰²ï¼Œå¹¶è¶…è¶Šäº†äºŒåå¤šç§I2IåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11340v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>H&amp;EæŸ“è‰²æ˜¯ç—…ç†å­¦è¯Šæ–­ç–¾ç—…å’Œè¿½è¸ªè‚¿ç˜¤å¤å‘çš„æ ‡å‡†æ–¹æ³•ï¼Œä½†å…¶æ— æ³•æ­ç¤ºä¸ç–¾ç—…ä¸¥é‡ç¨‹åº¦å’Œæ²»ç–—ååº”ç›¸å…³çš„ç‰¹å®šè›‹ç™½è´¨ã€‚å…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰æŸ“è‰²ä½¿ç”¨æŠ—ä½“æ¥çªå‡ºæ˜¾ç¤ºè¿™äº›è›‹ç™½è´¨åœ¨å„è‡ªç»†èƒç±»å‹ä¸Šçš„è¡¨è¾¾ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§å¹¶è¾…åŠ©è¯ç‰©æ²»ç–—é€‰æ‹©ã€‚å°½ç®¡IHCæŸ“è‰²æœ‰ä»·å€¼ï¼Œä½†å®ƒéœ€è¦é¢å¤–çš„æ—¶é—´å’Œèµ„æºï¼Œé™åˆ¶äº†åœ¨æŸäº›ä¸´åºŠç¯å¢ƒä¸­çš„åº”ç”¨ã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ä½¿å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘æˆä¸ºè®¡ç®—æˆæœ¬ä½å»‰çš„IHCæ›¿ä»£æ–¹æ¡ˆã€‚I2IæŠ€æœ¯èƒ½å¤Ÿæ•°å­—ç”Ÿæˆé«˜ä¿çœŸæŸ“è‰²è½¬æ¢ï¼Œå¯èƒ½å–ä»£æ‰‹åŠ¨IHCæŸ“è‰²ã€‚æ‰©æ•£æ¨¡å‹æ˜¯å›¾åƒç”Ÿæˆå’Œæ¡ä»¶ä»»åŠ¡çš„æœ€æ–°æŠ€æœ¯ï¼Œç‰¹åˆ«é€‚åˆè™šæ‹ŸIHCï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡å›¾åƒå¹¶æŠµæŠ—æ¨¡å¼å´©æºƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ˆé€šå¸¸æ•°ç™¾ä¸‡æ ·æœ¬ï¼‰æ‰èƒ½å®ç°ç¨³å¥çš„æ€§èƒ½ï¼Œè¿™åœ¨è™šæ‹ŸæŸ“è‰²åº”ç”¨ä¸­æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œé€šå¸¸åªæœ‰æ•°åƒä¸ªæ ·æœ¬å¯ç”¨ã€‚å—å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æœ‰é™æ•°æ®åœºæ™¯ä¸­çš„æˆåŠŸå¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†STAINDIFFUSERï¼Œä¸€ç§é’ˆå¯¹è™šæ‹ŸæŸ“è‰²çš„å¤šä»»åŠ¡æ‰©æ•£æ¶æ„ï¼Œå¯åœ¨å°å‹æ•°æ®é›†ä¸Šå®ç°æ”¶æ•›ã€‚STAINDIFFUSERåŒæ—¶è®­ç»ƒä¸¤ç§æ‰©æ•£è¿‡ç¨‹ï¼šä»H&amp;Eå›¾åƒç”Ÿæˆç»†èƒç‰¹å¼‚æ€§IHCæŸ“è‰²å’ŒåŸºäºH&amp;Eçš„ç»†èƒåˆ†å‰²ï¼Œä»…åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ç²—ç•¥çš„åˆ†å‰²æ ‡ç­¾ã€‚STAINDIFFUSERä¸ºä¸¤ä¸ªæ ‡è®°ç”Ÿæˆé«˜è´¨é‡çš„è™šæ‹ŸæŸ“è‰²ï¼Œè¡¨ç°ä¼˜äºäºŒåå¤šä¸ªI2IåŸºçº¿ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>H&amp;EæŸ“è‰²æ˜¯ç—…ç†å­¦è¯Šæ–­çš„æ ‡å‡†ï¼Œä½†æ— æ³•æ­ç¤ºç‰¹å®šè›‹ç™½è´¨ä¿¡æ¯ã€‚</li>
<li>IHCæŸ“è‰²èƒ½æ­ç¤ºè›‹ç™½è´¨ä¿¡æ¯ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§å¹¶è¾…åŠ©æ²»ç–—é€‰æ‹©ï¼Œä½†æ“ä½œå¤æ‚ä¸”æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ä¸­çš„I2Iç¿»è¯‘æŠ€æœ¯ä¸ºè™šæ‹ŸIHCæä¾›äº†è®¡ç®—æˆæœ¬ä½å»‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç‰¹åˆ«é€‚åˆè™šæ‹ŸIHCï¼Œèƒ½äº§ç”Ÿé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨å°æ ·æœ¬æ•°æ®ä¸‹è¡¨ç°å—é™ã€‚</li>
<li>STAINDIFFUSERæ˜¯ä¸€ç§å¤šä»»åŠ¡æ‰©æ•£æ¶æ„ï¼Œèƒ½åœ¨å°æ•°æ®é›†ä¸Šå®ç°è™šæ‹ŸæŸ“è‰²çš„æ”¶æ•›ã€‚</li>
<li>STAINDIFFUSERèƒ½ä»H&amp;Eå›¾åƒç”Ÿæˆç»†èƒç‰¹å¼‚æ€§IHCæŸ“è‰²å¹¶è¿›è¡Œç»†èƒåˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b251bcb27875f21e9239779920f37b2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f8cae6e3b8954cf2b47c6a9d60c4d92.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-2dbd53b28128cc9bbb530fa5610b868a.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Automated Fetal Biometry Assessment with Deep Ensembles using   Sparse-Sampling of 2D Intrapartum Ultrasound Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6bd169b1669f9d09716b8931a84c9843.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  IPENSInteractive Unsupervised Framework for Rapid Plant Phenotyping   Extraction via NeRF-SAM2 Fusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
