<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  ContextAgent Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-70326c7591212f1a08bbbbe1e5097b67.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-22-æ›´æ–°"><a href="#2025-05-22-æ›´æ–°" class="headerlink" title="2025-05-22 æ›´æ–°"></a>2025-05-22 æ›´æ–°</h1><h2 id="ContextAgent-Context-Aware-Proactive-LLM-Agents-with-Open-World-Sensory-Perceptions"><a href="#ContextAgent-Context-Aware-Proactive-LLM-Agents-with-Open-World-Sensory-Perceptions" class="headerlink" title="ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions"></a>ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions</h2><p><strong>Authors:Bufang Yang, Lilin Xu, Liekang Zeng, Kaiwei Liu, Siyang Jiang, Wenrui Lu, Hongkai Chen, Xiaofan Jiang, Guoliang Xing, Zhenyu Yan</strong></p>
<p>Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts to enhance the proactive capabilities of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and the persona contexts from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†æ™ºèƒ½ä»£ç†ä»è¢«åŠ¨å“åº”å‘ä¸»åŠ¨æ”¯æŒçš„è½¬å˜ã€‚å°½ç®¡å‰æ™¯å¹¿é˜”ï¼Œä½†ç°æœ‰çš„ä¸»åŠ¨ä»£ç†è¦ä¹ˆä»…ä¾èµ–äºå°é—­ç¯å¢ƒï¼ˆå¦‚æ¡Œé¢UIï¼‰çš„è§‚å¯Ÿç»“æœè¿›è¡Œç›´æ¥LLMæ¨ç†ï¼Œè¦ä¹ˆé‡‡ç”¨åŸºäºè§„åˆ™çš„ä¸»åŠ¨é€šçŸ¥ï¼Œå¯¼è‡´å¯¹ç”¨æˆ·æ„å›¾çš„ç†è§£ä¸å¤Ÿå‡†ç¡®ï¼Œä¸»åŠ¨æœåŠ¡çš„åŠŸèƒ½æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ContextAgentï¼Œè¿™æ˜¯ä¸€æ¬¾é¦–æ¬¾ç»“åˆå¹¿æ³›æ„Ÿå®˜ä¸Šä¸‹æ–‡ä»¥å¢å¼ºLLMä»£ç†ä¸»åŠ¨èƒ½åŠ›çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸»åŠ¨ä»£ç†ã€‚ContextAgenté¦–å…ˆä»å¯ç©¿æˆ´è®¾å¤‡çš„å¤§é‡æ„Ÿå®˜æ„ŸçŸ¥ï¼ˆå¦‚è§†é¢‘å’ŒéŸ³é¢‘ï¼‰ä¸­æå–å¤šç»´ä¸Šä¸‹æ–‡ï¼Œä»¥äº†è§£ç”¨æˆ·æ„å›¾ã€‚ç„¶åï¼ŒContextAgentåˆ©ç”¨æ„Ÿå®˜ä¸Šä¸‹æ–‡å’Œå†å²æ•°æ®ä¸­çš„äººæ ¼ä¸Šä¸‹æ–‡æ¥é¢„æµ‹æ˜¯å¦éœ€è¦ä¸»åŠ¨æœåŠ¡ã€‚å½“éœ€è¦ä¸»åŠ¨ååŠ©æ—¶ï¼ŒContextAgentä¼šè¿›ä¸€æ­¥è‡ªåŠ¨è°ƒç”¨å¿…è¦çš„å·¥å…·æ¥ååŠ©ç”¨æˆ·ã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ContextAgentBenchï¼Œè¿™æ˜¯è¯„ä¼°ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸»åŠ¨LLMä»£ç†çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–9ä¸ªæ—¥å¸¸åœºæ™¯å’Œ20ä¸ªå·¥å…·çš„1000ä¸ªæ ·æœ¬ã€‚åœ¨ContextAgentBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒContextAgentçš„ä¸»åŠ¨é¢„æµ‹å’Œå·¥å…·è°ƒç”¨å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†8.5%å’Œ6.0%ï¼Œè¶…è¿‡äº†åŸºçº¿æ°´å¹³ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½æ¿€å‘æ›´å…ˆè¿›ã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„ä¸»åŠ¨äººå·¥æ™ºèƒ½åŠ©ç†çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14668v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€æ–°è¿›å±•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨åŠ¨äº†æ™ºèƒ½ä»£ç†ä»è¢«åŠ¨å“åº”å‘ä¸»åŠ¨æ”¯æŒçš„è½¬å˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸»åŠ¨ä»£ç†è¦ä¹ˆä»…ä¾èµ–äºå°é—­ç¯å¢ƒçš„è§‚å¯Ÿè¿›è¡Œç›´æ¥LLMæ¨ç†ï¼Œè¦ä¹ˆé‡‡ç”¨åŸºäºè§„åˆ™çš„ä¸»åŠ¨é€šçŸ¥ï¼Œå¯¼è‡´å¯¹ç”¨æˆ·æ„å›¾ç†è§£ä¸è¶³å’Œä¸»åŠ¨æœåŠ¡åŠŸèƒ½çš„å±€é™æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†é¦–ä¸ªèå…¥å¹¿æ³›ç¯å¢ƒæ„ŸçŸ¥çš„ContextAgentï¼Œå®ƒé€šè¿‡ç©¿æˆ´è®¾å¤‡ä¸Šçš„å¤§è§„æ¨¡æ„Ÿå®˜æ„ŸçŸ¥æå–å¤šç»´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢å¼ºLLMä»£ç†çš„ä¸»åŠ¨èƒ½åŠ›ã€‚ContextAgentåˆ©ç”¨æ„Ÿå®˜ä¸Šä¸‹æ–‡å’Œå†å²æ•°æ®ä¸­çš„ä¸ªäººä¸Šä¸‹æ–‡æ¥é¢„æµ‹ä¸»åŠ¨æœåŠ¡çš„å¿…è¦æ€§ã€‚å½“éœ€è¦ä¸»åŠ¨ååŠ©æ—¶ï¼ŒContextAgentä¼šè¿›ä¸€æ­¥è‡ªåŠ¨è°ƒç”¨å¿…è¦çš„å·¥å…·ä»¥ååŠ©ç”¨æˆ·ã€‚ä¸ºè¯„ä¼°æ­¤æ–°ä»»åŠ¡ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ContextAgentBenchï¼Œé¦–ä¸ªè¯„ä¼°æ„è¯†ç¯å¢ƒçš„ä¸»åŠ¨LLMä»£ç†çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä¸€åƒä¸ªæ ·æœ¬ã€ä¹ä¸ªæ—¥å¸¸åœºæ™¯å’ŒäºŒåä¸ªå·¥å…·ã€‚å®éªŒè¡¨æ˜ï¼ŒContextAgentåœ¨ä¸»åŠ¨é¢„æµ‹å’Œå·¥å…·è°ƒç”¨æ–¹é¢çš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†é«˜è¾¾8.5%å’Œ6.0%ã€‚æˆ‘ä»¬å¸Œæœ›è¯¥ç ”ç©¶èƒ½æ¿€å‘æ›´å…ˆè¿›ã€ä»¥äººä¸ºä¸­å¿ƒçš„ä¸»åŠ¨äººå·¥æ™ºèƒ½åŠ©ç†çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†æ™ºèƒ½ä»£ç†ä»è¢«åŠ¨å“åº”å‘ä¸»åŠ¨æ”¯æŒçš„æ¼”å˜ã€‚</li>
<li>ç°æœ‰ä¸»åŠ¨ä»£ç†å­˜åœ¨ç”¨æˆ·æ„å›¾ç†è§£ä¸è¶³å’Œä¸»åŠ¨æœåŠ¡åŠŸèƒ½å±€é™çš„é—®é¢˜ã€‚</li>
<li>ContextAgentæ˜¯é¦–ä¸ªèå…¥å¹¿æ³›ç¯å¢ƒæ„ŸçŸ¥çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸»åŠ¨ä»£ç†ï¼Œé€šè¿‡ç©¿æˆ´è®¾å¤‡ä¸Šçš„å¤§è§„æ¨¡æ„Ÿå®˜æ„ŸçŸ¥å¢å¼ºLLMä»£ç†çš„ä¸»åŠ¨èƒ½åŠ›ã€‚</li>
<li>ContextAgentåˆ©ç”¨æ„Ÿå®˜ä¸Šä¸‹æ–‡å’Œä¸ªäººå†å²ä¸Šä¸‹æ–‡æ¥é¢„æµ‹ä¸»åŠ¨æœåŠ¡çš„å¿…è¦æ€§ã€‚</li>
<li>ContextAgentèƒ½è‡ªåŠ¨è°ƒç”¨å¿…è¦çš„å·¥å…·ä»¥ååŠ©ç”¨æˆ·ã€‚</li>
<li>ContextAgentBenchæ˜¯é¦–ä¸ªè¯„ä¼°æ„è¯†ç¯å¢ƒçš„ä¸»åŠ¨LLMä»£ç†çš„åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9040c7152d5f02942263da1df2ca8223.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9087e17f390599c407981d7df1c30801.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e75153888530eb9a15a427723c9b60c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bef43eb44c3dc35014aa294443414bc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Agent-Context-Protocols-Enhance-Collective-Inference"><a href="#Agent-Context-Protocols-Enhance-Collective-Inference" class="headerlink" title="Agent Context Protocols Enhance Collective Inference"></a>Agent Context Protocols Enhance Collective Inference</h2><p><strong>Authors:Devansh Bhardwaj, Arjun Beniwal, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik R. Narasimhan, Ameet Deshpande, Vishvak Murahari</strong></p>
<p>AI agents have become increasingly adept at complex tasks such as coding, reasoning, and multimodal understanding. However, building generalist systems requires moving beyond individual agents to collective inference â€“ a paradigm where multi-agent systems with diverse, task-specialized agents complement one another through structured communication and collaboration. Today, coordination is usually handled with imprecise, ad-hoc natural language, which limits complex interaction and hinders interoperability with domain-specific agents. We introduce Agent context protocols (ACPs): a domain- and agent-agnostic family of structured protocols for agent-agent communication, coordination, and error handling. ACPs combine (i) persistent execution blueprints â€“ explicit dependency graphs that store intermediate agent outputs â€“ with (ii) standardized message schemas, enabling robust and fault-tolerant multi-agent collective inference. ACP-powered generalist systems reach state-of-the-art performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance and best-in-class multimodal technical reports, outperforming commercial AI systems in human evaluation. ACPs are highly modular and extensible, allowing practitioners to build top-tier generalist agents quickly. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä»£ç†åœ¨ç¼–ç¨‹ã€æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£ç­‰å¤æ‚ä»»åŠ¡ä¸Šè¶Šæ¥è¶Šå¨´ç†Ÿã€‚ç„¶è€Œï¼Œæ„å»ºå…¨èƒ½ç³»ç»Ÿéœ€è¦åœ¨å•ä¸ªä»£ç†ä¹‹å¤–è¿›è¡Œé›†ä½“æ¨ç†â€”â€”ä¸€ç§æ¨¡å¼ï¼Œå…¶ä¸­å…·æœ‰å¤šæ ·åŒ–ã€ä»»åŠ¡ä¸“ä¸šåŒ–çš„å¤šä»£ç†ç³»ç»Ÿé€šè¿‡ç»“æ„åŒ–é€šä¿¡å’Œåä½œç›¸äº’è¡¥å……ã€‚ç›®å‰ï¼Œåè°ƒé€šå¸¸é€šè¿‡ä¸ç²¾ç¡®ã€ä¸´æ—¶çš„è‡ªç„¶è¯­è¨€æ¥å¤„ç†ï¼Œè¿™é™åˆ¶äº†å¤æ‚çš„äº¤äº’å’Œä¸ç‰¹å®šé¢†åŸŸçš„ä»£ç†çš„äº’æ“ä½œæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä»£ç†ä¸Šä¸‹æ–‡åè®®ï¼ˆACPï¼‰ï¼šä¸€ä¸ªé¢å‘ä»£ç†é€šä¿¡ã€åè°ƒå’Œé”™è¯¯å¤„ç†çš„è·¨é¢†åŸŸå’Œè·¨ä»£ç†çš„ç»“æ„åŒ–åè®®æ—ã€‚ACPç»“åˆäº†ï¼ˆiï¼‰æŒä¹…æ‰§è¡Œè“å›¾â€”â€”å­˜å‚¨ä¸­é—´ä»£ç†è¾“å‡ºçš„æ˜¾å¼ä¾èµ–å›¾â€”â€”ä»¥åŠï¼ˆiiï¼‰æ ‡å‡†åŒ–æ¶ˆæ¯æ¨¡å¼ï¼Œå®ç°äº†ç¨³å¥å’Œå®¹é”™çš„å¤šä»£ç†é›†ä½“æ¨ç†ã€‚ACPèµ‹èƒ½çš„å…¨èƒ½ç³»ç»Ÿè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ï¼šåœ¨é•¿æ—¶é—´è§†é‡ç½‘ç»œè¾…åŠ©æ–¹é¢çš„AccuracyBenchæµ‹è¯•è¾¾åˆ°äº†28.3%çš„å‡†ç¡®ç‡ï¼Œåœ¨å¤šæ¨¡æ€æŠ€æœ¯æŠ¥å‘Šæ–¹é¢è¾¾åˆ°äº†æœ€ä½³æ°´å¹³ï¼Œåœ¨äººç±»è¯„ä¼°ä¸­è¶…è¶Šäº†å•†ä¸šAIç³»ç»Ÿã€‚ACPé«˜åº¦æ¨¡å—åŒ–ä¸”å¯æ‰©å±•ï¼Œä½¿ä»ä¸šè€…èƒ½å¤Ÿè¿…é€Ÿæ„å»ºé¡¶çº§å…¨èƒ½ä»£ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14569v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AIä»£ç†åœ¨ç¼–ç¨‹ã€æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£ç­‰å¤æ‚ä»»åŠ¡æ–¹é¢è¶Šæ¥è¶Šç†Ÿç»ƒã€‚ç„¶è€Œï¼Œæ„å»ºå…¨èƒ½ç³»ç»Ÿéœ€è¦è¶…è¶Šå•ä¸ªä»£ç†ï¼Œå®ç°å¤šä»£ç†ç³»ç»Ÿçš„é›†ä½“æ¨ç†ã€‚å½“å‰ï¼Œåè°ƒé€šå¸¸é€šè¿‡ä¸ç²¾ç¡®ã€ä¸´æ—¶çš„è‡ªç„¶è¯­è¨€è¿›è¡Œï¼Œè¿™é™åˆ¶äº†å¤æ‚äº¤äº’å’Œä¸ç‰¹å®šé¢†åŸŸä»£ç†çš„äº’æ“ä½œæ€§ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥ä»£ç†ä¸Šä¸‹æ–‡åè®®ï¼ˆACPï¼‰ï¼šä¸€ç§ç”¨äºä»£ç†é—´é€šä¿¡ã€åè°ƒå’Œé”™è¯¯å¤„ç†çš„è·¨é¢†åŸŸå’Œä»£ç†çš„åè®®å®¶æ—ã€‚ACPç»“åˆæŒä¹…æ‰§è¡Œè“å›¾ï¼ˆå­˜å‚¨ä¸­é—´ä»£ç†è¾“å‡ºçš„æ˜¾å¼ä¾èµ–å›¾ï¼‰å’Œæ ‡å‡†åŒ–æ¶ˆæ¯æ¨¡å¼ï¼Œå®ç°ç¨³å¥å’Œå®¹é”™çš„å¤šä»£ç†é›†ä½“æ¨ç†ã€‚ACPé©±åŠ¨çš„å…¨èƒ½ç³»ç»Ÿè¾¾åˆ°åŠ©ç†é•¿å‘¨æœŸç½‘ç»œè¾…åŠ©çš„å…ˆè¿›æ€§èƒ½å’Œæœ€ä½³çš„å¤šæ¨¡æ€æŠ€æœ¯æŠ¥å‘Šæ€§èƒ½ï¼Œåœ¨äººç±»è¯„ä¼°ä¸­ä¼˜äºå•†ä¸šAIç³»ç»Ÿã€‚ACPé«˜åº¦æ¨¡å—åŒ–ä¸”å¯æ‰©å±•æ€§å¼ºï¼Œä½¿ä»ä¸šè€…å¯ä»¥å¿«é€Ÿæ„å»ºé¡¶å°–çš„å…¨èƒ½ä»£ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ä¸æ–­æå‡ï¼Œä½†æ„å»ºå…¨èƒ½ç³»ç»Ÿéœ€è¦å¤šä»£ç†é›†ä½“æ¨ç†ã€‚</li>
<li>å½“å‰åè°ƒä¸»è¦é€šè¿‡ä¸ç²¾ç¡®çš„è‡ªç„¶è¯­è¨€è¿›è¡Œï¼Œé™åˆ¶äº†å¤æ‚äº¤äº’å’Œäº’æ“ä½œæ€§ã€‚</li>
<li>å¼•å…¥ACPï¼ˆä»£ç†ä¸Šä¸‹æ–‡åè®®ï¼‰ï¼Œç”¨äºä»£ç†é—´é€šä¿¡ã€åè°ƒå’Œé”™è¯¯å¤„ç†ã€‚</li>
<li>ACPç»“åˆæŒä¹…æ‰§è¡Œè“å›¾å’Œæ ‡å‡†åŒ–æ¶ˆæ¯æ¨¡å¼ï¼Œå®ç°å¤šä»£ç†é›†ä½“æ¨ç†çš„ç¨³å¥æ€§å’Œå®¹é”™æ€§ã€‚</li>
<li>ACPé©±åŠ¨çš„ç³»ç»Ÿåœ¨åŠ©ç†é•¿å‘¨æœŸç½‘ç»œè¾…åŠ©å’Œå¤šæ¨¡æ€æŠ€æœ¯æŠ¥å‘Šæ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>ACPå…·æœ‰å‡ºè‰²çš„æ€§èƒ½å’Œé«˜åº¦æ¨¡å—åŒ–ã€å¯æ‰©å±•æ€§å¼ºçš„ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b92db92dc5dd31cb0cf1e27397271fb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2b8f968a94b0fa877d8818d8149bf20.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57c9cceebe9bfcf4db39b66b1da06bda.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-agent-Reinforcement-Learning-vs-Fixed-Time-Control-for-Traffic-Signal-Optimization-A-Simulation-Study"><a href="#Multi-agent-Reinforcement-Learning-vs-Fixed-Time-Control-for-Traffic-Signal-Optimization-A-Simulation-Study" class="headerlink" title="Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic   Signal Optimization: A Simulation Study"></a>Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic   Signal Optimization: A Simulation Study</h2><p><strong>Authors:Saahil Mahato</strong></p>
<p>Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges. </p>
<blockquote>
<p>åŸå¸‚äº¤é€šæ‹¥å µï¼Œç‰¹åˆ«æ˜¯åœ¨äº¤å‰å£ï¼Œå¯¹æ—…è¡Œæ—¶é—´ã€ç‡ƒæ–™æ¶ˆè€—å’Œæ’æ”¾ç‰©äº§ç”Ÿé‡å¤§å½±å“ã€‚ä¼ ç»Ÿçš„å›ºå®šæ—¶é—´ä¿¡å·æ§åˆ¶ç³»ç»Ÿé€šå¸¸ç¼ºä¹é€‚åº”åŠ¨æ€äº¤é€šæ¨¡å¼è¿›è¡Œæœ‰æ•ˆç®¡ç†çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ä¼˜åŒ–äº¤é€šä¿¡å·åè°ƒå¤šä¸ªäº¤å‰å£äº¤é€šæµé‡çš„åº”ç”¨ã€‚åˆ©ç”¨Pygameï¼Œå¼€å‘äº†ä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡å‹ï¼Œæ¨¡æ‹Ÿç›¸äº’è¿æ¥äº¤å‰å£çš„ç½‘ç»œï¼Œéšæœºç”Ÿæˆè½¦è¾†æµé‡ä»¥åæ˜ å®é™…äº¤é€šæµé‡çš„å˜åŒ–ã€‚å®æ–½äº†ä¸€ä¸ªåˆ†æ•£çš„MARLæ§åˆ¶å™¨ï¼Œå…¶ä¸­æ¯ä¸ªäº¤é€šä¿¡å·ç¯ä½œä¸ºä¸€ä¸ªè‡ªä¸»çš„æ™ºèƒ½ä½“è¿›è¡Œå·¥ä½œï¼ŒåŸºäºæœ¬åœ°è§‚æµ‹å’Œé‚»è¿‘æ™ºèƒ½ä½“çš„ä¿¡æ¯åšå‡ºå†³ç­–ã€‚æ€§èƒ½ä»¥å›ºå®šçš„æ—¶é—´æ§åˆ¶å™¨ä¸ºåŸºå‡†ï¼Œä½¿ç”¨å¹³å‡è½¦è¾†ç­‰å¾…æ—¶é—´å’Œæ•´ä½“ååç‡ç­‰æ ‡å‡†è¿›è¡Œè¯„ä¼°ã€‚MARLæ–¹æ³•æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„ç»Ÿè®¡æ”¹è¿›ï¼ŒåŒ…æ‹¬å¹³å‡ç­‰å¾…æ—¶é—´å‡å°‘å’Œååé‡æé«˜ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒåŸºäºMARLçš„åŠ¨æ€æ§åˆ¶ç­–ç•¥åœ¨æé«˜åŸå¸‚äº¤é€šç®¡ç†æ•ˆç‡æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚å»ºè®®è¿›ä¸€æ­¥ç ”ç©¶è§£å†³å¯æ‰©å±•æ€§å’Œç°å®ä¸–ç•Œå®æ–½æŒ‘æˆ˜çš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14544v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸå¸‚äº¤é€šæ‹¥å µï¼Œç‰¹åˆ«æ˜¯äº¤å‰å£å¤„çš„æ‹¥å µï¼Œä¸¥é‡å½±å“è¡Œè½¦æ—¶é—´ã€ç‡ƒæ–™æ¶ˆè€—å’Œæ’æ”¾ã€‚ä¼ ç»Ÿå›ºå®šæ—¶é—´ä¿¡å·æ§åˆ¶ç³»ç»Ÿç¼ºä¹æœ‰æ•ˆç®¡ç†åŠ¨æ€äº¤é€šæ¨¡å¼çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ä¼˜åŒ–äº¤é€šä¿¡å·åè°ƒçš„åº”ç”¨ã€‚åˆ©ç”¨Pygameæ¨¡æ‹Ÿè½¯ä»¶ï¼Œå»ºç«‹ä¸€ä¸ªäº’è”äº¤å‰å£ç½‘ç»œæ¨¡å‹ï¼Œéšæœºç”Ÿæˆè½¦è¾†æµé‡ä»¥åæ˜ å®é™…äº¤é€šæµé‡çš„å˜åŒ–ã€‚å®æ–½äº†ä¸€ç§åˆ†æ•£å¼MARLæ§åˆ¶å™¨ï¼Œå…¶ä¸­æ¯ä¸ªäº¤é€šä¿¡å·ç¯ä½œä¸ºä¸€ä¸ªè‡ªä¸»çš„æ™ºèƒ½ä½“ï¼ŒåŸºäºæœ¬åœ°è§‚æµ‹å’Œé‚»è¿‘æ™ºèƒ½ä½“çš„ä¿¡æ¯è¿›è¡Œå†³ç­–ã€‚é€šè¿‡å¹³å‡è½¦è¾†ç­‰å¾…æ—¶é—´å’Œæ€»ä½“é€šè¡Œèƒ½åŠ›ç­‰æŒ‡æ ‡ï¼Œå¯¹å›ºå®šæ—¶é—´æ§åˆ¶å™¨çš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒMARLæ–¹æ³•æ˜¾è‘—æé«˜äº†æ•ˆç‡ï¼ŒåŒ…æ‹¬å‡å°‘äº†å¹³å‡ç­‰å¾…æ—¶é—´å’Œæé«˜äº†é€šè¡Œèƒ½åŠ›ã€‚è¿™è¡¨æ˜åŸºäºMARLçš„åŠ¨æ€æ§åˆ¶ç­–ç•¥åœ¨æé«˜åŸå¸‚äº¤é€šç®¡ç†æ•ˆç‡æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚æœªæ¥éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶æ¥è§£å†³å¯æ‰©å±•æ€§å’Œå®é™…å®æ–½ä¸­çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚äº¤é€šæ‹¥å µå¯¹è¡Œè½¦æ—¶é—´ã€ç‡ƒæ–™æ¶ˆè€—å’Œæ’æ”¾æœ‰ä¸¥é‡å½±å“ã€‚</li>
<li>ä¼ ç»Ÿå›ºå®šæ—¶é—´ä¿¡å·æ§åˆ¶ç³»ç»Ÿæ— æ³•æœ‰æ•ˆç®¡ç†åŠ¨æ€äº¤é€šæ¨¡å¼ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰å¯ç”¨äºä¼˜åŒ–æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„äº¤é€šä¿¡å·åè°ƒã€‚</li>
<li>åˆ©ç”¨Pygameæ¨¡æ‹Ÿè½¯ä»¶å»ºç«‹äº’è”äº¤å‰å£ç½‘ç»œæ¨¡å‹ã€‚</li>
<li>åˆ†æ•£å¼MARLæ§åˆ¶å™¨ä¸­ï¼Œæ¯ä¸ªäº¤é€šä¿¡å·ç¯ä½œä¸ºä¸€ä¸ªè‡ªä¸»æ™ºèƒ½ä½“è¿›è¡Œå†³ç­–ã€‚</li>
<li>ä¸å›ºå®šæ—¶é—´æ§åˆ¶å™¨ç›¸æ¯”ï¼ŒMARLæ–¹æ³•æé«˜äº†äº¤é€šæ•ˆç‡ï¼Œå‡å°‘äº†å¹³å‡ç­‰å¾…æ—¶é—´å’Œæé«˜äº†é€šè¡Œèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-90cb69746058feade702533889a95740.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-801e5a3de98947f2241b7f2f5fb23273.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a80cf7894f937458e40cbcacf08e9f57.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Visual-Agentic-Reinforcement-Fine-Tuning"><a href="#Visual-Agentic-Reinforcement-Fine-Tuning" class="headerlink" title="Visual Agentic Reinforcement Fine-Tuning"></a>Visual Agentic Reinforcement Fine-Tuning</h2><p><strong>Authors:Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang</strong></p>
<p>A key trend in Large Reasoning Models (e.g., OpenAIâ€™s o3) is the native agentic ability to use external tools such as web browsers for searching and writing&#x2F;executing code for image manipulation to think with images. In the open-source research community, while significant progress has been made in language-only agentic abilities such as function calling and tool integration, the development of multi-modal agentic capabilities that involve truly thinking with images, and their corresponding benchmarks, are still less explored. This work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning (Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the ability to browse websites for real-time information updates and write code to manipulate and analyze input images through cropping, rotation, and other image processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT) with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMsâ€™ agentic search and coding abilities. Our experimental results demonstrate that Visual-ARFT outperforms its baseline by +18.6% F1 &#x2F; +13.0% EM on MAT-Coding and +10.3% F1 &#x2F; +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT also achieves +29.3 F1% &#x2F; +25.9% EM gains on existing multi-hop QA benchmarks such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities. Our findings suggest that Visual-ARFT offers a promising path toward building robust and generalizable multimodal agents. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚OpenAIçš„o3ï¼‰çš„ä¸€ä¸ªå…³é”®è¶‹åŠ¿æ˜¯å…·å¤‡ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æµè§ˆå™¨è¿›è¡Œæœç´¢å’Œç¼–å†™&#x2F;æ‰§è¡Œå›¾åƒæ“ä½œä»£ç ï¼‰è¿›è¡Œå›¾åƒæ€è€ƒçš„æœ¬å¾èƒ½åŠ›ã€‚åœ¨å¼€æºç ”ç©¶ç¤¾åŒºä¸­ï¼Œè™½ç„¶åªæœ‰è¯­è¨€åŠŸèƒ½çš„æœ¬å¾èƒ½åŠ›ï¼ˆå¦‚å‡½æ•°è°ƒç”¨å’Œå·¥å…·é›†æˆï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†çœŸæ­£æ¶‰åŠå›¾åƒæ€è€ƒçš„å¤šæ¨¡æ€æœ¬å¾èƒ½åŠ›åŠå…¶ç›¸åº”åŸºå‡†æµ‹è¯•çš„å¼€å‘ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†è§†è§‰æœ¬å¾å¼ºåŒ–å¾®è°ƒï¼ˆVisual-ARFTï¼‰åœ¨èµ‹äºˆå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çµæ´»å’Œè‡ªé€‚åº”æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡Visual-ARFTï¼Œå¼€æºLVLMsè·å¾—äº†æµè§ˆç½‘ç«™ä»¥è·å–å®æ—¶ä¿¡æ¯æ›´æ–°ä»¥åŠé€šè¿‡è£å‰ªã€æ—‹è½¬å’Œå…¶ä»–å›¾åƒå¤„ç†æŠ€æœ¯ç¼–å†™ä»£ç æ¥æ“ä½œå’Œåˆ†æè¾“å…¥å›¾åƒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æœ¬å¾å·¥å…·å°ï¼ˆMATï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸¤ç§è®¾ç½®ï¼ˆMAT-Searchå’ŒMAT-Codingï¼‰ï¼Œç”¨äºè¯„ä¼°LVLMsçš„æœ¬å¾æœç´¢å’Œç¼–ç èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨MAT-Codingä¸Šï¼ŒVisual-ARFTçš„F1å¾—åˆ†æ¯”åŸºçº¿é«˜å‡º+18.6%ï¼ŒEMå¾—åˆ†é«˜å‡º+13.0%ï¼Œåœ¨MAT-Searchä¸Šï¼ŒF1å¾—åˆ†é«˜å‡º+10.3%ï¼ŒEMå¾—åˆ†é«˜å‡º+8.7%ï¼Œè¶…è¶Šäº†GPT-4oã€‚æ­¤å¤–ï¼ŒVisual-ARFTåœ¨ç°æœ‰çš„å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚2Wikiå’ŒHotpotQAï¼‰ä¸Šçš„F1å¾—åˆ†æé«˜äº†+29.3%ï¼ŒEMå¾—åˆ†æé«˜äº†+25.9%ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒVisual-ARFTä¸ºæ„å»ºç¨³å¥ä¸”å¯æ³›åŒ–çš„å¤šæ¨¡æ€ä»£ç†æä¾›äº†æœ‰å‰é€”çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14246v1">PDF</a> project url:   <a target="_blank" rel="noopener" href="https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT">https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚OpenAIçš„o3ï¼‰çš„å…³é”®è¶‹åŠ¿æ˜¯å…·å¤‡ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œå¦‚é€šè¿‡ç½‘é¡µæœç´¢å’Œç¼–å†™&#x2F;æ‰§è¡Œå›¾åƒæ“ä½œä»£ç æ¥è¿›è¡Œå›¾åƒæ€è€ƒã€‚æœ¬ç ”ç©¶å¼ºè°ƒè§†è§‰ä»£ç†å¼ºåŒ–å¾®è°ƒï¼ˆVisual-ARFTï¼‰åœ¨èµ‹äºˆå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çµæ´»å’Œé€‚åº”æ€§æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚Visual-ARFTä½¿å¼€æºLVLMsèƒ½å¤Ÿæµè§ˆç½‘ç«™è¿›è¡Œå®æ—¶ä¿¡æ¯æ›´æ–°ï¼Œå¹¶é€šè¿‡ç¼–å†™ä»£ç å¯¹è¾“å…¥å›¾åƒè¿›è¡Œè£å‰ªã€æ—‹è½¬å’Œå…¶ä»–å›¾åƒå¤„ç†æŠ€æœ¯æ¥åˆ†æå’Œæ“ä½œå›¾åƒã€‚æœ¬ç ”ç©¶è¿˜æå‡ºäº†å¤šæ¨¡æ€ä»£ç†å·¥å…·å°ï¼ˆMATï¼‰ï¼Œå…¶ä¸­åŒ…æ‹¬MAT-Searchå’ŒMAT-Codingä¸¤ç§è®¾ç½®ï¼Œä»¥è¯„ä¼°LVLMsçš„ä»£ç†æœç´¢å’Œç¼–ç èƒ½åŠ›ã€‚å®éªŒç»“æœè¯å®Visual-ARFTåœ¨MAT-Codingä¸Šçš„F1å¾—åˆ†é«˜å‡º+18.6%ï¼ŒEMé«˜å‡º+13.0%ï¼Œåœ¨MAT-Searchä¸Šçš„F1å¾—åˆ†é«˜å‡º+10.3%ï¼ŒEMé«˜å‡º+8.7%ï¼Œè¶…è¶Šäº†GPT-4oã€‚æ­¤å¤–ï¼Œåœ¨ç°æœ‰çš„å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚2Wikiå’ŒHotpotQAï¼‰ä¸Šï¼ŒVisual-ARFTå®ç°äº†+29.3 F1%å’Œ+25.9% EMçš„å¢ç›Šï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™è¡¨æ˜Visual-ARFTæ˜¯æœç€æ„å»ºç¨³å¥ä¸”å¯æ³›åŒ–çš„å¤šæ¨¡æ€ä»£ç†çš„æœ‰å‰é€”çš„è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹å…·å¤‡ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œå¦‚ç½‘é¡µæœç´¢å’Œå›¾åƒæ“ä½œä»£ç ç¼–å†™ï¼Œä»¥å®ç°å›¾åƒæ€è€ƒåŠŸèƒ½ã€‚</li>
<li>Visual-ARFTæŠ€æœ¯èƒ½èµ‹äºˆå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çµæ´»å’Œé€‚åº”æ€§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Visual-ARFTä½¿å¼€æºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæµè§ˆç½‘ç«™å¹¶ç¼–å†™ä»£ç è¿›è¡Œå›¾åƒå¤„ç†ã€‚</li>
<li>å¤šæ¨¡æ€ä»£ç†å·¥å…·å°ï¼ˆMATï¼‰åŒ…æ‹¬MAT-Searchå’ŒMAT-Codingè®¾ç½®ï¼Œç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æœç´¢å’Œç¼–ç èƒ½åŠ›ã€‚</li>
<li>Visual-ARFTåœ¨MAT-Codingå’ŒMAT-Searchä¸Šçš„è¡¨ç°è¶…è¶Šäº†GPT-4oï¼Œå®ç°äº†æ˜¾è‘—çš„F1å¾—åˆ†å’ŒEMæé«˜ã€‚</li>
<li>åœ¨å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVisual-ARFTæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5960f0fa323f748ae3d38fbc3e6333e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad922ee12f095d0ecdc591ea807b180d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b72e8b645bb25a1d48577dc5b7ccb798.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ffa3290db967c391562092127bafdc4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MM-Agent-LLM-as-Agents-for-Real-world-Mathematical-Modeling-Problem"><a href="#MM-Agent-LLM-as-Agents-for-Real-world-Mathematical-Modeling-Problem" class="headerlink" title="MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem"></a>MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem</h2><p><strong>Authors:Fan Liu, Zherui Yang, Cancheng Liu, Tianrui Song, Xiaofeng Gao, Hao Liu</strong></p>
<p>Mathematical modeling is a cornerstone of scientific discovery and engineering practice, enabling the translation of real-world problems into formal systems across domains such as physics, biology, and economics. Unlike mathematical reasoning, which assumes a predefined formulation, modeling requires open-ended problem analysis, abstraction, and principled formalization. While Large Language Models (LLMs) have shown strong reasoning capabilities, they fall short in rigorous model construction, limiting their utility in real-world problem-solving. To this end, we formalize the task of LLM-powered real-world mathematical modeling, where agents must analyze problems, construct domain-appropriate formulations, and generate complete end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111 problems from the Mathematical Contest in Modeling (MCM&#x2F;ICM), spanning the years 2000 to 2025 and across ten diverse domains such as physics, biology, and economics. To tackle this task, we propose MM-Agent, an expert-inspired framework that decomposes mathematical modeling into four stages: open-ended problem analysis, structured model formulation, computational problem solving, and report generation. Experiments on MM-Bench show that MM-Agent significantly outperforms baseline agents, achieving an 11.88% improvement over human expert solutions while requiring only 15 minutes and $0.88 per task using GPT-4o. Furthermore, under official MCM&#x2F;ICM protocols, MM-Agent assisted two undergraduate teams in winning the Finalist Award (\textbf{top 2.0% among 27,456 teams}) in MCM&#x2F;ICM 2025, demonstrating its practical effectiveness as a modeling copilot. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/usail-hkust/LLM-MM-Agent">https://github.com/usail-hkust/LLM-MM-Agent</a> </p>
<blockquote>
<p>æ•°å­¦å»ºæ¨¡æ˜¯ç§‘å­¦å‘ç°å’Œå·¥ç¨‹å®è·µçš„é‡è¦åŸºçŸ³ï¼Œå®ƒèƒ½å¤Ÿå°†ç°å®ä¸–ç•Œçš„é—®é¢˜è½¬åŒ–ä¸ºç‰©ç†ã€ç”Ÿç‰©ã€ç»æµç­‰é¢†åŸŸä¸­çš„å½¢å¼ç³»ç»Ÿã€‚ä¸åŒäºå‡è®¾é¢„å…ˆå®šä¹‰çš„æ•°å­¦æ¨ç†ï¼Œå»ºæ¨¡éœ€è¦è¿›è¡Œå¼€æ”¾å¼çš„é—®é¢˜åˆ†æã€æŠ½è±¡å’ŒåŸåˆ™æ€§å½¢å¼åŒ–ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨ä¸¥æ ¼çš„æ¨¡å‹æ„å»ºæ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶åœ¨è§£å†³å®é™…é—®é¢˜ä¸­çš„å®ç”¨æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ­£å¼æå‡ºäº†åŸºäºLLMçš„ç°å®ä¸–ç•Œæ•°å­¦å»ºæ¨¡ä»»åŠ¡ï¼Œå…¶ä¸­ä»£ç†éœ€è¦åˆ†æé—®é¡Œã€æ„å»ºåˆé€‚çš„é¢†åŸŸå…¬å¼ï¼Œå¹¶ç”Ÿæˆå®Œæ•´çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å¼•å…¥äº†MM-Benchï¼Œå®ƒæ˜¯MCM&#x2F;ICMæ•°å­¦å»ºæ¨¡ç«èµ›ä¸­ç²¾å¿ƒæŒ‘é€‰çš„111ä¸ªé—®é¢˜çš„åŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›–ä»2000å¹´åˆ°2025å¹´çš„åä¸ªä¸åŒé¢†åŸŸï¼Œå¦‚ç‰©ç†ã€ç”Ÿç‰©å’Œç»æµã€‚ä¸ºäº†è§£å†³è¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†MM-Agentè¿™ä¸€ä¸“å®¶å¯å‘å¼çš„æ¡†æ¶ï¼Œå®ƒå°†æ•°å­¦å»ºæ¨¡åˆ†è§£ä¸ºå››ä¸ªé˜¶æ®µï¼šå¼€æ”¾å¼é—®é¢˜åˆ†æã€ç»“æ„åŒ–æ¨¡å‹æ„å»ºã€è®¡ç®—é—®é¢˜æ±‚è§£å’ŒæŠ¥å‘Šç”Ÿæˆã€‚åœ¨MM-Benchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMM-Agentæ˜¾è‘—ä¼˜äºåŸºçº¿ä»£ç†ï¼Œä¸äººç±»ä¸“å®¶è§£å†³æ–¹æ¡ˆç›¸æ¯”å®ç°äº†11.88%çš„æå‡ï¼Œå¹¶ä¸”ä½¿ç”¨GPT-4oæ—¶åªéœ€èŠ±è´¹æ¯ä»»åŠ¡ä»…éœ€15åˆ†é’Ÿå’Œ0.88ç¾å…ƒçš„æˆæœ¬ã€‚æ­¤å¤–ï¼Œæ ¹æ®MCM&#x2F;ICMçš„å®˜æ–¹åè®®ï¼ŒMM-Agentè¾…åŠ©ä¸¤ä¸ªæœ¬ç§‘ç”Ÿå›¢é˜Ÿèµ¢å¾—äº†MCM&#x2F;ICM 2025çš„å…¥å›´å¥–ï¼ˆåœ¨27,456æ”¯å›¢é˜Ÿä¸­è„±é¢–è€Œå‡ºè¿›å…¥å‰2%ï¼‰ï¼Œè¯æ˜äº†å…¶ä½œä¸ºå»ºæ¨¡å‰¯é©¾é©¶çš„å®é™…æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/usail-hkust/LLM-MM-Agent%E4%BE%9B%E5%85%AC%E4%BC%97%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/usail-hkust/LLM-MM-Agentä¾›å…¬ä¼—ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14148v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ•°å­¦æ¨¡æ‹Ÿæ˜¯ç§‘å­¦å‘ç°å’Œå·¥ç¨‹å®è·µçš„é‡è¦åŸºçŸ³ï¼Œèƒ½å¤Ÿå°†ç°å®ä¸–ç•Œçš„é—®é¢˜è½¬åŒ–ä¸ºæ­£å¼ç³»ç»Ÿã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶å…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ„å»ºä¸¥è°¨æ¨¡å‹æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨LLMè¿›è¡Œå®é™…æ•°å­¦å»ºæ¨¡çš„ä»»åŠ¡ï¼Œå¹¶å¼•å…¥MM-BenchåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»æ•°å­¦å»ºæ¨¡ç«èµ›ä¸­ç²¾é€‰çš„111ä¸ªé—®é¢˜ã€‚åŒæ—¶æå‡ºMM-Agentæ¡†æ¶ï¼Œé€šè¿‡å››é˜¶æ®µå®Œæˆæ•°å­¦å»ºæ¨¡ã€‚å®éªŒè¡¨æ˜ï¼ŒMM-Agentåœ¨MM-Benchä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºäººç±»ä¸“å®¶è§£å†³æ–¹æ¡ˆæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¹¶åœ¨MCM&#x2F;ICMç«èµ›ä¸­ååŠ©å›¢é˜Ÿè·å¾—å†³èµ›å¥–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­¦å»ºæ¨¡æ˜¯è¿æ¥ç°å®ä¸–ç•Œä¸ç§‘å­¦ç†è®ºçš„é‡è¦æ¡¥æ¢ï¼Œç”¨äºè§£å†³å„é¢†åŸŸï¼ˆå¦‚ç‰©ç†ã€ç”Ÿç‰©ã€ç»æµç­‰ï¼‰çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ•°å­¦å»ºæ¨¡çš„ä¸¥è°¨æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>MM-BenchåŸºå‡†æµ‹è¯•åŒ…å«å¹¿æ³›çš„æ•°å­¦å»ºæ¨¡é—®é¢˜ï¼Œä¸ºè¯„ä¼°LLMåœ¨æ­¤é¢†åŸŸçš„æ€§èƒ½æä¾›äº†æ ‡å‡†ã€‚</li>
<li>MM-Agentæ¡†æ¶é€šè¿‡å››é˜¶æ®µï¼ˆé—®é¢˜åˆ†æã€æ¨¡å‹æ„å»ºã€é—®é¢˜è§£å†³ã€æŠ¥å‘Šç”Ÿæˆï¼‰å®Œæˆæ•°å­¦å»ºæ¨¡ä»»åŠ¡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºMM-Agentåœ¨MM-Benchä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿ï¼Œä¸äººç±»ä¸“å®¶è§£å†³æ–¹æ¡ˆç›¸æ¯”æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>MM-Agentåœ¨MCM&#x2F;ICMç«èµ›ä¸­å±•ç°äº†å…¶å®ç”¨æ€§ï¼ŒååŠ©å›¢é˜Ÿè·å¾—é¡¶çº§å¥–é¡¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0f00e454ef1b1f247edad6853be8ba1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24dcef22fff4cfc46a648bfdec35c585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9023837b54b72eaf28e20321ed51853e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e37d4ae8f2388c01144fb5c8c746a3d8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BAR-A-Backward-Reasoning-based-Agent-for-Complex-Minecraft-Tasks"><a href="#BAR-A-Backward-Reasoning-based-Agent-for-Complex-Minecraft-Tasks" class="headerlink" title="BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks"></a>BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks</h2><p><strong>Authors:Weihong Du, Wenrui Liao, Binyu Yan, Hongru Liang, Anthony G. Cohn, Wenqiang Lei</strong></p>
<p>Large language model (LLM) based agents have shown great potential in following human instructions and automatically completing various tasks. To complete a task, the agent needs to decompose it into easily executed steps by planning. Existing studies mainly conduct the planning by inferring what steps should be executed next starting from the agentâ€™s initial state. However, this forward reasoning paradigm doesnâ€™t work well for complex tasks. We propose to study this issue in Minecraft, a virtual environment that simulates complex tasks based on real-world scenarios. We believe that the failure of forward reasoning is caused by the big perception gap between the agentâ€™s initial state and task goal. To this end, we leverage backward reasoning and make the planning starting from the terminal state, which can directly achieve the task goal in one step. Specifically, we design a BAckward Reasoning based agent (BAR). It is equipped with a recursive goal decomposition module, a state consistency maintaining module and a stage memory module to make robust, consistent, and efficient planning starting from the terminal state. Experimental results demonstrate the superiority of BAR over existing methods and the effectiveness of proposed modules. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººåœ¨éµå¾ªäººç±»æŒ‡ä»¤å’Œè‡ªåŠ¨å®Œæˆå„ç§ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ä¸ºäº†å®Œæˆä»»åŠ¡ï¼Œä»£ç†äººéœ€è¦é€šè¿‡è§„åˆ’å°†ä»»åŠ¡åˆ†è§£ä¸ºå¯è½»æ¾æ‰§è¡Œçš„æ­¥éª¤ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡ä»ä»£ç†äººçš„åˆå§‹çŠ¶æ€æ¨æ–­åº”è¯¥æ‰§è¡Œå“ªäº›ä¸‹ä¸€æ­¥æ­¥éª¤æ¥è¿›è¡Œè§„åˆ’ã€‚ç„¶è€Œï¼Œè¿™ç§æ­£å‘æ¨ç†æ¨¡å¼å¯¹äºå¤æ‚ä»»åŠ¡å¹¶ä¸å¥æ•ˆã€‚æˆ‘ä»¬å»ºè®®åœ¨Minecraftè¿™ä¸ªæ¨¡æ‹ŸåŸºäºç°å®ä¸–ç•Œåœºæ™¯çš„å¤æ‚ä»»åŠ¡çš„è™šæ‹Ÿç¯å¢ƒä¸­ç ”ç©¶è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬è®¤ä¸ºæ­£å‘æ¨ç†çš„å¤±è´¥æ˜¯ç”±äºä»£ç†åˆå§‹çŠ¶æ€ä¸ä»»åŠ¡ç›®æ ‡ä¹‹é—´å­˜åœ¨çš„å·¨å¤§æ„ŸçŸ¥å·®è·æ‰€å¯¼è‡´çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨é€†å‘æ¨ç†ï¼Œä»ç»ˆç«¯çŠ¶æ€å¼€å§‹è¿›è¡Œè§„åˆ’ï¼Œè¿™æ ·å¯ä»¥ä¸€æ­¥åˆ°ä½å®ç°ä»»åŠ¡ç›®æ ‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºé€†å‘æ¨ç†çš„ä»£ç†ï¼ˆBARï¼‰ã€‚å®ƒé…å¤‡äº†ä¸€ä¸ªé€’å½’ç›®æ ‡åˆ†è§£æ¨¡å—ã€ä¸€ä¸ªçŠ¶æ€ä¸€è‡´æ€§ç»´æŠ¤æ¨¡å—å’Œä¸€ä¸ªé˜¶æ®µè®°å¿†æ¨¡å—ï¼Œä»ç»ˆç«¯çŠ¶æ€å¼€å§‹è¿›è¡Œç¨³å¥ã€ä¸€è‡´å’Œé«˜æ•ˆçš„è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBARä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ‰€æå‡ºæ¨¡å—çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14079v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨éµå¾ªäººç±»æŒ‡ä»¤å’Œè‡ªåŠ¨å®Œæˆä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ä¸ºå®Œæˆä»»åŠ¡ï¼Œä»£ç†éœ€è¦é€šè¿‡è§„åˆ’å°†ä»»åŠ¡åˆ†è§£ä¸ºå¯è½»æ¾æ‰§è¡Œæ­¥éª¤ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡ä»ä»£ç†çš„åˆå§‹çŠ¶æ€æ¨æ–­åº”æ‰§è¡Œå“ªäº›æ­¥éª¤æ¥è¿›è¡Œè§„åˆ’ã€‚ç„¶è€Œï¼Œè¿™ç§æ­£å‘æ¨ç†æ¨¡å¼å¯¹äºå¤æ‚ä»»åŠ¡å¹¶ä¸å¥æ•ˆã€‚æœ¬ç ”ç©¶åœ¨æ¨¡æ‹Ÿç°å®åœºæ™¯å¤æ‚ä»»åŠ¡çš„è™šæ‹Ÿç¯å¢ƒMinecraftä¸­æ¢è®¨è¿™ä¸€é—®é¢˜ã€‚ç ”ç©¶è®¤ä¸ºæ­£å‘æ¨ç†å¤±è´¥çš„åŸå› æ˜¯ä»£ç†åˆå§‹çŠ¶æ€ä¸ä»»åŠ¡ç›®æ ‡ä¹‹é—´å­˜åœ¨è¾ƒå¤§çš„æ„ŸçŸ¥å·®è·ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶é‡‡ç”¨é€†å‘æ¨ç†å¹¶ä»ç»ˆç«¯çŠ¶æ€å¼€å§‹è§„åˆ’ï¼Œæ—¨åœ¨ä¸€æ­¥ç›´æ¥å®ç°ä»»åŠ¡ç›®æ ‡ã€‚å…·ä½“æ¥è¯´ï¼Œç ”ç©¶è®¾è®¡ä¸€ä¸ªåŸºäºé€†å‘æ¨ç†çš„ä»£ç†ï¼ˆBARï¼‰ï¼Œé…å¤‡é€’å½’ç›®æ ‡åˆ†è§£æ¨¡å—ã€çŠ¶æ€ä¸€è‡´æ€§ç»´æŠ¤æ¨¡å—å’Œé˜¶æ®µè®°å¿†æ¨¡å—ï¼Œä»¥ä»ç»ˆç«¯çŠ¶æ€å¼€å§‹è¿›è¡Œç¨³å¥ã€ä¸€è‡´å’Œé«˜æ•ˆçš„è§„åˆ’ã€‚å®éªŒç»“æœè¯æ˜äº†BARç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ä»¥åŠæ‰€æå‡ºæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨éµå¾ªæŒ‡ä»¤å’Œå®Œæˆä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡æ­£å‘æ¨ç†è¿›è¡Œä»»åŠ¡è§„åˆ’ï¼Œä½†å¯¹äºå¤æ‚ä»»åŠ¡æ•ˆæœæœ‰é™ã€‚</li>
<li>ç ”ç©¶åœ¨Minecraftè™šæ‹Ÿç¯å¢ƒä¸­æ¢è®¨å¤æ‚ä»»åŠ¡å®Œæˆé—®é¢˜ã€‚</li>
<li>æ„ŸçŸ¥å·®è·æ˜¯æ­£å‘æ¨ç†å¤±è´¥çš„ä¸»è¦åŸå› ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨é€†å‘æ¨ç†æ–¹æ³•å¹¶ä»ä»»åŠ¡ç»ˆç«¯çŠ¶æ€å¼€å§‹è§„åˆ’ã€‚</li>
<li>BARä»£ç†è®¾è®¡åŒ…æ‹¬é€’å½’ç›®æ ‡åˆ†è§£ã€çŠ¶æ€ä¸€è‡´æ€§ç»´æŠ¤å’Œé˜¶æ®µè®°å¿†æ¨¡å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4557e6efd9d15b5d006d57fe7311eeb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2db149907cbd7cfa466bda594d700f99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c50e2f7529c6260e7dc57e372a234f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2dbf145856ed98fa666ac5672fee2eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f80fc9a9c49e6541f77b250fe4034291.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LLM-DSE-Searching-Accelerator-Parameters-with-LLM-Agents"><a href="#LLM-DSE-Searching-Accelerator-Parameters-with-LLM-Agents" class="headerlink" title="LLM-DSE: Searching Accelerator Parameters with LLM Agents"></a>LLM-DSE: Searching Accelerator Parameters with LLM Agents</h2><p><strong>Authors:Hanyu Wang, Xinrui Wu, Zijian Ding, Su Zheng, Chengyue Wang, Tony Nowatzki, Yizhou Sun, Jason Cong</strong></p>
<p>Even though high-level synthesis (HLS) tools mitigate the challenges of programming domain-specific accelerators (DSAs) by raising the abstraction level, optimizing hardware directive parameters remains a significant hurdle. Existing heuristic and learning-based methods struggle with adaptability and sample efficiency. We present LLM-DSE, a multi-agent framework designed specifically for optimizing HLS directives. Combining LLM with design space exploration (DSE), our explorer coordinates four agents: Router, Specialists, Arbitrator, and Critic. These multi-agent components interact with various tools to accelerate the optimization process. LLM-DSE leverages essential domain knowledge to identify efficient parameter combinations while maintaining adaptability through verbal learning from online interactions. Evaluations on the HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\times$ performance gains over state-of-the-art methods, uncovering novel designs while reducing runtime. Ablation studies validate the effectiveness and necessity of the proposed agent interactions. Our code is open-sourced here: <a target="_blank" rel="noopener" href="https://github.com/Nozidoali/LLM-DSE">https://github.com/Nozidoali/LLM-DSE</a>. </p>
<blockquote>
<p>å°½ç®¡é«˜å±‚æ¬¡ç»¼åˆï¼ˆHLSï¼‰å·¥å…·é€šè¿‡æé«˜æŠ½è±¡å±‚æ¬¡å‡è½»äº†ç¼–ç¨‹ç‰¹å®šåŸŸåŠ é€Ÿå™¨ï¼ˆDSAï¼‰çš„æŒ‘æˆ˜ï¼Œä½†ä¼˜åŒ–ç¡¬ä»¶æŒ‡ä»¤å‚æ•°ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§éšœç¢ã€‚ç°æœ‰çš„å¯å‘å¼æ–¹æ³•å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨é€‚åº”æ€§å’Œæ ·æœ¬æ•ˆç‡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†LLM-DSEï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºä¼˜åŒ–HLSæŒ‡ä»¤è®¾è®¡çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚å°†LLMä¸è®¾è®¡ç©ºé—´æ¢ç´¢ï¼ˆDSEï¼‰ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ¢ç´¢è€…åè°ƒäº†å››ä¸ªæ™ºèƒ½ä½“ï¼šè·¯ç”±å™¨ã€ä¸“å®¶ã€ä»²è£è€…å’Œè¯„è®ºå®¶ã€‚è¿™äº›å¤šæ™ºèƒ½ä½“ç»„ä»¶ä¸å„ç§å·¥å…·è¿›è¡Œäº¤äº’ï¼Œä»¥åŠ é€Ÿä¼˜åŒ–è¿‡ç¨‹ã€‚LLM-DSEåˆ©ç”¨é‡è¦çš„é¢†åŸŸçŸ¥è¯†æ¥è¯†åˆ«æœ‰æ•ˆçš„å‚æ•°ç»„åˆï¼ŒåŒæ—¶é€šè¿‡åœ¨çº¿äº¤äº’çš„å£å¤´å­¦ä¹ æ¥ä¿æŒé€‚åº”æ€§ã€‚åœ¨HLSynæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLLM-DSEç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å®ç°äº†é«˜è¾¾2.55å€çš„æ€§èƒ½æå‡ï¼Œèƒ½å¤Ÿå‘ç°æ–°é¢–çš„è®¾è®¡å¹¶å‡å°‘è¿è¡Œæ—¶é—´ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†æ‰€æå‡ºæ™ºèƒ½ä½“äº¤äº’çš„æœ‰æ•ˆæ€§å’Œå¿…è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨æ­¤å¼€æºï¼š<a target="_blank" rel="noopener" href="https://github.com/Nozidoali/LLM-DSE%E3%80%82">https://github.com/Nozidoali/LLM-DSEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12188v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é«˜å±¤æ¬¡ç¶œåˆï¼ˆHLSï¼‰å·¥å…·é›–å¯æå‡æŠ½è±¡å±¤ç´šä»¥æ¸›è¼•ç·¨å¯«åŸŸç‰¹å®šåŠ é€Ÿå™¨ï¼ˆDSAï¼‰çš„æŒ‘æˆ°ï¼Œä½†å„ªåŒ–ç¡¬ä»¶æŒ‡ä»¤åƒæ•¸ä»æ˜¯é‡å¤§éšœç¤™ã€‚ç¾æœ‰çš„å•Ÿç™¼å¼å’ŒåŸºæ–¼å­¸ç¿’çš„æ–¹æ³•åœ¨é©æ‡‰æ€§å’Œæ¨£æœ¬æ•ˆç‡æ–¹é¢å­˜åœ¨å›°é›£ã€‚æœ¬ç ”ç©¶æå‡ºLLM-DSEï¼Œä¸€å€‹å°ˆç‚ºå„ªåŒ–HLSæŒ‡ä»¤è€Œè¨­è¨ˆçš„å¤šä»£ç†æ¡†æ¶ã€‚çµåˆLLMèˆ‡è¨­è¨ˆç©ºé–“æ¢ç´¢ï¼ˆDSEï¼‰ï¼Œæœ¬æ¡†æ¶å”èª¿å››å€‹ä»£ç†ï¼šRouterã€Specialistsã€Arbitratorå’ŒCriticã€‚é€™äº›å¤šä»£ç†çµ„ä»¶èˆ‡å„ç¨®å·¥å…·äº’å‹•ä»¥åŠ é€Ÿå„ªåŒ–éç¨‹ã€‚LLM-DSEåˆ©ç”¨é—œéµåŸŸçŸ¥è­˜ä¾†è­˜åˆ¥æœ‰æ•ˆçš„åƒæ•¸çµ„åˆï¼ŒåŒæ™‚é€éåœ¨ç·šäº’å‹•é€²è¡Œè¨€è©å­¸ç¿’ä»¥ç¶­æŒé©æ‡‰æ€§ã€‚åœ¨HLSynæ•¸æ“šé›†ä¸Šçš„è©•ä¼°é¡¯ç¤ºï¼ŒLLM-DSEè¼ƒå…ˆé€²æ–¹æ³•å¯¦ç¾äº†å¯¦è³ªçš„2.55å€æ€§èƒ½æå‡ï¼Œèƒ½å¤ ç™¼ç¾æ–°è¨­è¨ˆä¸¦æ¸›å°‘é‹è¡Œæ™‚é–“ã€‚ç›¸é—œç ”ç©¶çµæœå¯åœ¨æ­¤é–‹æºä»£ç¢¼ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Nozidoali/LLM-DSE%E3%80%82">https://github.com/Nozidoali/LLM-DSEã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜å±¤æ¬¡ç¶œåˆï¼ˆHLSï¼‰å·¥å…·é›–èƒ½æ¸›è¼•ç·¨å¯«åŸŸç‰¹å®šåŠ é€Ÿå™¨ï¼ˆDSAï¼‰çš„æŒ‘æˆ°ï¼Œä½†å„ªåŒ–ç¡¬ä»¶æŒ‡ä»¤åƒæ•¸ä»æ˜¯é‡è¦å•é¡Œã€‚</li>
<li>ç›®å‰å•Ÿç™¼å¼å’ŒåŸºæ–¼å­¸ç¿’çš„æ–¹æ³•åœ¨é©æ‡‰æ€§å’Œæ¨£æœ¬æ•ˆç‡ä¸Šå­˜å›°é›£ã€‚</li>
<li>LLM-DSEæ˜¯ä¸€å€‹å¤šä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨å„ªåŒ–HLSæŒ‡ä»¤åƒæ•¸ï¼ŒåŒ…æ‹¬Routerã€Specialistsã€Arbitratorå’ŒCriticç­‰ä»£ç†çµ„ä»¶ã€‚</li>
<li>LLM-DSEåˆ©ç”¨è¨­è¨ˆç©ºé–“æ¢ç´¢ï¼ˆDSEï¼‰å’ŒåŸŸçŸ¥è­˜ä¾†è­˜åˆ¥æœ‰æ•ˆçš„åƒæ•¸çµ„åˆã€‚</li>
<li>LLM-DSEé€šéåœ¨ç·šäº’å‹•é€²è¡Œè¨€è©å­¸ç¿’ä»¥ç¶­æŒé©æ‡‰æ€§ã€‚</li>
<li>åœ¨HLSynæ•¸æ“šé›†ä¸Šçš„è©•ä¼°é¡¯ç¤ºï¼ŒLLM-DSEè¼ƒå…ˆé€²æ–¹æ³•æœ‰é¡¯è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c9b2186b39031733a485e6bad5aaf44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3689cf8c3cf0bae6233e9f799e38a14c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ce11dd72c0ecf360f6c9393ed6bd135.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a96bac3b455466f903679f9e4ec680f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Talk-to-Your-Slides-Language-Driven-Agents-for-Efficient-Slide-Editing"><a href="#Talk-to-Your-Slides-Language-Driven-Agents-for-Efficient-Slide-Editing" class="headerlink" title="Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing"></a>Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing</h2><p><strong>Authors:Kyudan Jung, Hojun Cho, Jooyeol Yun, Soyoung Yang, Jaehyeok Jang, Jagul Choo</strong></p>
<p>Editing presentation slides remains one of the most common and time-consuming tasks faced by millions of users daily, despite significant advances in automated slide generation. Existing approaches have successfully demonstrated slide editing via graphic user interface (GUI)-based agents, offering intuitive visual control. However, such methods often suffer from high computational cost and latency. In this paper, we propose Talk-to-Your-Slides, an LLM-powered agent designed to edit slides %in active PowerPoint sessions by leveraging structured information about slide objects rather than relying on image modality. The key insight of our work is designing the editing process with distinct high-level and low-level layers to facilitate interaction between user commands and slide objects. By providing direct access to application objects rather than screen pixels, our system enables 34.02% faster processing, 34.76% better instruction fidelity, and 87.42% cheaper operation than baselines. To evaluate slide editing capabilities, we introduce TSBench, a human-annotated dataset comprising 379 diverse editing instructions paired with corresponding slide variations in four categories. Our code, benchmark and demos are available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4C">https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4C</a>. </p>
<blockquote>
<p>å°½ç®¡è‡ªåŠ¨å¹»ç¯ç‰‡ç”ŸæˆæŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç¼–è¾‘å¹»ç¯ç‰‡ä»ç„¶æ˜¯æ¯å¤©æ•°ç™¾ä¸‡ç”¨æˆ·é¢ä¸´çš„æœ€å¸¸è§ä¸”æœ€è€—æ—¶çš„ä»»åŠ¡ä¹‹ä¸€ã€‚ç°æœ‰æ–¹æ³•å·²ç»æˆåŠŸé€šè¿‡åŸºäºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„ä»£ç†å®ç°äº†å¹»ç¯ç‰‡ç¼–è¾‘ï¼Œæä¾›äº†ç›´è§‚çš„è§†è§‰æ§åˆ¶ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿé—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Talk-to-Your-Slidesï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ä»£ç†ï¼Œæ—¨åœ¨åˆ©ç”¨å¹»ç¯ç‰‡å¯¹è±¡çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œåœ¨æ´»è·ƒçš„PowerPointä¼šè¯ä¸­ç¼–è¾‘å¹»ç¯ç‰‡ï¼Œè€Œä¸æ˜¯ä¾èµ–å›¾åƒæ¨¡å¼ã€‚æˆ‘ä»¬å·¥ä½œçš„å…³é”®è§è§£æ˜¯è®¾è®¡å…·æœ‰ä¸åŒé«˜çº§å’Œä½çº§å±‚çš„ç¼–è¾‘è¿‡ç¨‹ï¼Œä»¥ä¿ƒè¿›ç”¨æˆ·å‘½ä»¤å’Œå¹»ç¯ç‰‡å¯¹è±¡ä¹‹é—´çš„äº¤äº’ã€‚é€šè¿‡æä¾›å¯¹åº”ç”¨ç¨‹åºå¯¹è±¡çš„ç›´æ¥è®¿é—®ï¼Œè€Œä¸æ˜¯å±å¹•åƒç´ ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ¯”åŸºçº¿åŠ å¿«äº†34.02%çš„å¤„ç†é€Ÿåº¦ï¼Œæé«˜äº†34.76%çš„æŒ‡ä»¤ä¿çœŸåº¦ï¼Œå¹¶é™ä½äº†87.42%çš„æ“ä½œæˆæœ¬ã€‚ä¸ºäº†è¯„ä¼°å¹»ç¯ç‰‡ç¼–è¾‘èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†TSBenchï¼Œè¿™æ˜¯ä¸€ä¸ªäººç±»æ³¨é‡Šçš„æ•°æ®é›†ï¼ŒåŒ…å«379æ¡å¤šæ ·çš„ç¼–è¾‘æŒ‡ä»¤ä¸å››ä¸ªç±»åˆ«ä¸­ç›¸åº”çš„å¹»ç¯ç‰‡å˜ä½“é…å¯¹ã€‚æˆ‘ä»¬çš„ä»£ç ã€åŸºå‡†æµ‹è¯•å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4C%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4Cä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11604v2">PDF</a> 20 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œTalk-to-Your-Slidesâ€çš„LLMé©±åŠ¨ä»£ç†ï¼Œç”¨äºåœ¨æ´»è·ƒçš„PowerPointä¼šè¯ä¸­ç¼–è¾‘å¹»ç¯ç‰‡ã€‚å®ƒé€šè¿‡åˆ©ç”¨å…³äºå¹»ç¯ç‰‡å¯¹è±¡çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ä¾èµ–å›¾åƒæ¨¡å¼ï¼Œè®¾è®¡ç‹¬ç‰¹çš„ç¼–è¾‘è¿‡ç¨‹åŒ…å«é«˜çº§å’Œä½çº§å±‚æ¬¡ä»¥ä¿ƒæˆç”¨æˆ·å‘½ä»¤å’Œå¹»ç¯ç‰‡å¯¹è±¡ä¹‹é—´çš„äº¤äº’ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå®ƒæä¾›å¯¹åº”ç”¨ç¨‹åºå¯¹è±¡çš„ç›´æ¥è®¿é—®ï¼Œä½¿å¤„ç†é€Ÿåº¦æå‡34.02%ï¼ŒæŒ‡ä»¤ä¿çœŸåº¦æé«˜34.76%ï¼Œæ“ä½œæˆæœ¬é™ä½87.42%ã€‚ä¸ºè¯„ä¼°å¹»ç¯ç‰‡ç¼–è¾‘èƒ½åŠ›ï¼Œå¼•å…¥TSBenchæ•°æ®é›†ï¼ŒåŒ…å«äººç±»æ³¨é‡Šçš„379æ¡ä¸åŒç¼–è¾‘æŒ‡ä»¤åŠå…¶å¯¹åº”çš„å¹»ç¯ç‰‡å˜åŒ–ç±»åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>â€œTalk-to-Your-Slidesâ€æ˜¯ä¸€ä¸ªLLMé©±åŠ¨çš„ä»£ç†ï¼Œæ—¨åœ¨ç®€åŒ–å¹»ç¯ç‰‡ç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>è¯¥ç³»ç»Ÿåˆ©ç”¨ç»“æ„åŒ–ä¿¡æ¯å…³äºå¹»ç¯ç‰‡å¯¹è±¡è€Œéå›¾åƒæ¨¡å¼è¿›è¡Œè®¾è®¡ã€‚</li>
<li>ç¼–è¾‘è¿‡ç¨‹åˆ†ä¸ºé«˜çº§å’Œä½çº§å±‚æ¬¡ï¼Œä¿ƒè¿›ç”¨æˆ·å‘½ä»¤å’Œå¹»ç¯ç‰‡å¯¹è±¡çš„äº¤äº’ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿæä¾›äº†å¯¹åº”ç”¨ç¨‹åºå¯¹è±¡çš„ç›´æ¥è®¿é—®ï¼Œæé«˜äº†å¤„ç†é€Ÿåº¦ã€æŒ‡ä»¤ä¿çœŸåº¦å’Œæ“ä½œæ•ˆç‡ã€‚</li>
<li>ä¸ºè¯„ä¼°å…¶æ€§èƒ½ï¼Œå¼•å…¥äº†ä¸€ä¸ªåä¸ºTSBenchçš„äººç±»æ³¨é‡Šæ•°æ®é›†ã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…å«å¤šç§ç¼–è¾‘æŒ‡ä»¤åŠå…¶å¯¹åº”çš„å¹»ç¯ç‰‡å˜åŒ–ç±»åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1130ba611c83eccb580bf7e378775666.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0967c83eb9f1f1459f6d04bf3839ec3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a63b9f1de0de36791b13498a40d55385.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8ac10ff04e1e48693b673a575be340e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2508b0e4fca1f35a45ed8fee811e6ff8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d889979738b094133ac240d75f0a4394.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0d1542d046b361270a721ae990a1b87.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Adaptive-Thinking-via-Mode-Policy-Optimization-for-Social-Language-Agents"><a href="#Adaptive-Thinking-via-Mode-Policy-Optimization-for-Social-Language-Agents" class="headerlink" title="Adaptive Thinking via Mode Policy Optimization for Social Language   Agents"></a>Adaptive Thinking via Mode Policy Optimization for Social Language   Agents</h2><p><strong>Authors:Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao</strong></p>
<p>Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current studies. Existing methods either lack this kind of reasoning capability or enforce Long Chain-of-Thought reasoning uniformly across all scenarios, resulting in excessive token usage and inflexible social simulation. To address this, we propose an $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) framework in this paper, aiming to improve the adaptive thinking ability of language agents in dynamic social interactions. To this end, we first identify hierarchical thinking modes ranging from intuitive response to deep deliberation based on the cognitive control theory. We then develop the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm to optimize the context-aware mode switching and reasoning. Our framework advances existing research in three key aspects: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence benchmarks verify that AML achieves 15.6% higher task performance than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter reasoning chains, demonstrating the advantage of adaptive thinking mode selection and optimization mechanism in AMPO over GRPOâ€™s fixed-depth solution. </p>
<blockquote>
<p>æœ‰æ•ˆçš„ç¤¾ä¼šæ™ºèƒ½æ¨¡æ‹Ÿéœ€è¦è¯­è¨€ä»£ç†åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œè€Œå½“å‰çš„ç ”ç©¶ä¸­æ™®éç¼ºä¹è¿™ç§èƒ½åŠ›ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆä¸å…·å¤‡è¿™ç§æ¨ç†èƒ½åŠ›ï¼Œè¦ä¹ˆå¼ºåˆ¶æ‰€æœ‰åœºæ™¯éƒ½è¿›è¡Œé•¿é“¾æ€ç»´æ¨ç†ï¼Œå¯¼è‡´ä»¤ç‰Œä½¿ç”¨è¿‡å¤šå’Œç¤¾ä¼šæ¨¡æ‹Ÿä¸çµæ´»ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”æ¨¡å¼å­¦ä¹ ï¼ˆAMLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è¯­è¨€ä»£ç†åœ¨åŠ¨æ€ç¤¾ä¼šäº¤äº’ä¸­çš„è‡ªé€‚åº”æ€ç»´èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆä»è®¤çŸ¥æ§åˆ¶ç†è®ºä¸­è¯†åˆ«å‡ºä»ç›´è§‰ååº”åˆ°æ·±æ€ç†Ÿè™‘çš„åˆ†å±‚æ€ç»´æ¨¡å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†è‡ªé€‚åº”æ¨¡å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆAMPOï¼‰ç®—æ³•ï¼Œä»¥ä¼˜åŒ–ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¨¡å¼åˆ‡æ¢å’Œæ¨ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸‰ä¸ªæ–¹é¢æ¨åŠ¨äº†ç°æœ‰ç ”ç©¶ï¼šï¼ˆ1ï¼‰å¤šç²’åº¦æ€ç»´æ¨¡å¼è®¾è®¡ï¼›ï¼ˆ2ï¼‰ç¤¾ä¼šäº¤äº’ä¸­çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å¼åˆ‡æ¢ï¼›ï¼ˆ3ï¼‰é€šè¿‡æ·±åº¦è‡ªé€‚åº”å¤„ç†çš„ä»¤ç‰Œé«˜æ•ˆæ¨ç†ã€‚åœ¨ç¤¾ä¼šæ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAMLçš„ä»»åŠ¡æ€§èƒ½æ¯”GPT-4oé«˜å‡º15.6%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„AMPOåœ¨ä»»åŠ¡æ€§èƒ½ä¸Šä¼˜äºGRPOï¼Œæ¨ç†é“¾ç¼©çŸ­äº†32.8%ï¼Œè¿™è¯æ˜äº†AMPOä¸­è‡ªé€‚åº”æ€ç»´æ¨¡å¼é€‰æ‹©å’Œä¼˜åŒ–æœºåˆ¶çš„ä¼˜åŠ¿ï¼Œè¶…è¿‡äº†GRPOçš„å›ºå®šæ·±åº¦è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02156v3">PDF</a> Work in Progress. The code and data are available, see   <a target="_blank" rel="noopener" href="https://github.com/MozerWang/AMPO">https://github.com/MozerWang/AMPO</a></p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ ç®—æ³•AMLæ¡†æ¶æå‡è¯­è¨€æ™ºèƒ½ä½“çš„é€‚åº”æ€§æ€è€ƒèƒ½åŠ›ï¼Œé’ˆå¯¹åŠ¨æ€ç¤¾äº¤äº’åŠ¨ä¸­çš„å¤šå±‚æ¬¡æ€è€ƒæ¨¡å¼è¿›è¡Œä¼˜åŒ–ï¼Œå®ç°äº†æ¨¡å¼è‡ªé€‚åº”è½¬æ¢å’Œæ·±åº¦è‡ªé€‚åº”å¤„ç†ï¼Œæé«˜ä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AMLæ¡†æ¶å¼•å…¥é€‚åº”æ€§æ€è€ƒèƒ½åŠ›ï¼Œé€‚åº”åŠ¨æ€ç¤¾äº¤äº’åŠ¨ä¸­çš„ä¸åŒæ€è€ƒæ¨¡å¼ã€‚</li>
<li>åŸºäºè®¤çŸ¥æ§åˆ¶ç†è®ºï¼Œè¯†åˆ«äº†ä»ç›´è§‰ååº”åˆ°æ·±åº¦æ€è€ƒçš„å¤šå±‚æ¬¡æ€è€ƒæ¨¡å¼ã€‚</li>
<li>å¼€å‘AMPOç®—æ³•å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¨¡å¼åˆ‡æ¢å’Œæ¨ç†ä¼˜åŒ–ã€‚</li>
<li>åœ¨ç¤¾ä¼šæ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAMLçš„ä»»åŠ¡æ€§èƒ½æ¯”GPT-4oé«˜å‡º15.6%ã€‚</li>
<li>AMPOç›¸è¾ƒäºGRPOè¡¨ç°å‡ºä¼˜åŠ¿ï¼Œæ¨ç†é“¾ç¼©çŸ­32.8%ï¼Œæ¨¡å¼é€‰æ‹©å’Œä¼˜åŒ–æœºåˆ¶æ›´çµæ´»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e86874dc898a8e04b5b90d6877bb0bcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9c8cbfe3869be0676bb52518ea93fcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68cd0b83747fde13e9f213e0b578d697.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80ac63fcbd0cbd9531469c1e05cd6e0e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Iterative-Tool-Usage-Exploration-for-Multimodal-Agents-via-Step-wise-Preference-Tuning"><a href="#Iterative-Tool-Usage-Exploration-for-Multimodal-Agents-via-Step-wise-Preference-Tuning" class="headerlink" title="Iterative Tool Usage Exploration for Multimodal Agents via Step-wise   Preference Tuning"></a>Iterative Tool Usage Exploration for Multimodal Agents via Step-wise   Preference Tuning</h2><p><strong>Authors:Pengxiang Li, Zhi Gao, Bofei Zhang, Yapeng Mi, Xiaojian Ma, Chenrui Shi, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li</strong></p>
<p>Multimodal agents, which integrate a controller e.g., a vision language model) with external tools, have demonstrated remarkable capabilities in tackling complex multimodal tasks. Existing approaches for training these agents, both supervised fine-tuning and reinforcement learning, depend on extensive human-annotated task-answer pairs and tool trajectories. However, for complex multimodal tasks, such annotations are prohibitively expensive or impractical to obtain. In this paper, we propose an iterative tool usage exploration method for multimodal agents without any pre-collected data, namely SPORT, via step-wise preference optimization to refine the trajectories of tool usage. Our method enables multimodal agents to autonomously discover effective tool usage strategies through self-exploration and optimization, eliminating the bottleneck of human annotation. SPORT has four iterative components: task synthesis, step sampling, step verification, and preference tuning. We first synthesize multimodal tasks using language models. Then, we introduce a novel trajectory exploration scheme, where step sampling and step verification are executed alternately to solve synthesized tasks. In step sampling, the agent tries different tools and obtains corresponding results. In step verification, we employ a verifier to provide AI feedback to construct step-wise preference data. The data is subsequently used to update the controller for tool usage through preference tuning, producing a SPORT agent. By interacting with real environments, the SPORT agent gradually evolves into a more refined and capable system. Evaluation in the GTA and GAIA benchmarks shows that the SPORT agent achieves 6.41% and 3.64% improvements, underscoring the generalization and effectiveness introduced by our method. The project page is <a target="_blank" rel="noopener" href="https://sport-agents.github.io/">https://SPORT-Agents.github.io</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼ˆå¦‚é›†æˆäº†æ§åˆ¶å™¨å¦‚è§†è§‰è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼‰ä¸å¤–éƒ¨å·¥å…·çš„ç»“åˆï¼Œåœ¨å¤„ç†å¤æ‚çš„è·¨æ¨¡æ€ä»»åŠ¡æ—¶è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç°æœ‰çš„è®­ç»ƒè¿™äº›æ™ºèƒ½ä½“çš„æ–¹æ³•ï¼Œæ— è®ºæ˜¯ç›‘ç£å¾®è°ƒè¿˜æ˜¯å¼ºåŒ–å­¦ä¹ ï¼Œéƒ½ä¾èµ–äºå¤§é‡çš„äººåŠ›æ ‡æ³¨çš„ä»»åŠ¡-ç­”æ¡ˆå¯¹å’Œå·¥å…·è½¨è¿¹ã€‚ç„¶è€Œï¼Œå¯¹äºå¤æ‚çš„è·¨æ¨¡æ€ä»»åŠ¡æ¥è¯´ï¼Œè¿™æ ·çš„æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥è·å–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€é¢„å…ˆæ”¶é›†æ•°æ®çš„è¿­ä»£å·¥å…·ä½¿ç”¨æ¢ç´¢æ–¹æ³•ï¼Œä¸“ä¸ºå¤šæ¨¡æ€æ™ºèƒ½ä½“è®¾è®¡ï¼Œåä¸ºSPORTï¼Œé€šè¿‡é€æ­¥åå¥½ä¼˜åŒ–æ¥å®Œå–„å·¥å…·ä½¿ç”¨è½¨è¿¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿å¤šæ¨¡æ€æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘æ¢ç´¢å’Œä¼˜åŒ–è‡ªä¸»å‘ç°æœ‰æ•ˆçš„å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œä»è€Œæ¶ˆé™¤äº†äººå·¥æ ‡æ³¨çš„ç“¶é¢ˆã€‚SPORTåŒ…å«å››ä¸ªè¿­ä»£ç»„ä»¶ï¼šä»»åŠ¡åˆæˆã€æ­¥éª¤é‡‡æ ·ã€æ­¥éª¤éªŒè¯å’Œåå¥½è°ƒæ•´ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨è¯­è¨€æ¨¡å‹åˆæˆè·¨æ¨¡æ€ä»»åŠ¡ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å·¥å…·è½¨è¿¹æ¢ç´¢æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆäº¤æ›¿æ‰§è¡Œæ­¥éª¤é‡‡æ ·å’Œæ­¥éª¤éªŒè¯æ¥è§£å†³åˆæˆä»»åŠ¡ã€‚åœ¨æ­¥éª¤é‡‡æ ·ä¸­ï¼Œæ™ºèƒ½ä½“ä¼šå°è¯•ä¸åŒçš„å·¥å…·å¹¶è·å–ç›¸åº”çš„ç»“æœã€‚åœ¨æ­¥éª¤éªŒè¯ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨éªŒè¯å™¨ä¸ºAIæä¾›åé¦ˆæ¥æ„å»ºæ­¥éª¤åå¥½æ•°æ®ã€‚éšåï¼Œè¿™äº›æ•°æ®è¢«ç”¨æ¥é€šè¿‡åå¥½è°ƒæ•´æ›´æ–°æ§åˆ¶å™¨å¯¹å·¥å…·çš„ä½¿ç”¨ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªSPORTæ™ºèƒ½ä½“ã€‚é€šè¿‡ä¸çœŸå®ç¯å¢ƒçš„äº¤äº’ï¼ŒSPORTæ™ºèƒ½ä½“ä¼šé€æ¸è¿›åŒ–æˆä¸€ä¸ªæ›´åŠ ç²¾ç»†å’Œå¼ºå¤§çš„ç³»ç»Ÿã€‚åœ¨GTAå’ŒGAIAåŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒSPORTæ™ºèƒ½ä½“çš„è¡¨ç°æé«˜äº†6.41%å’Œ3.64%ï¼Œçªæ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•å¸¦æ¥çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢ä¸ºï¼š<a href="https://link.zhæ€ä¹ˆæ‰¾è®ºæ–‡å¹¶ç¿»è¯‘æˆä¸­æ–‡%E9%A1%B5%E7%9A%84%E9%93%BE%E6%8E%A5%E4%B8%BA:%20https://SPORT-Agents.github.io.%EF%BC%8C%Eã€‚ï¼ˆæ³¨ï¼‰](https://link">é“¾æ¥åœ°å€https://SPORT-Agents.github.ioâ€‹â€‹ã€‚ï¼ˆæ³¨ï¼šè¿™é‡Œæ— æ³•è¿›è¡Œç›´æ¥çš„ç½‘å€è·³è½¬ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21561v3">PDF</a> 24 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ— éœ€é¢„å…ˆæ”¶é›†æ•°æ®çš„è¿­ä»£å·¥å…·ä½¿ç”¨æ¢ç´¢æ–¹æ³•ï¼Œåä¸ºSPORTï¼Œç”¨äºè®­ç»ƒå¤šæ¨¡æ€æ™ºèƒ½ä½“ã€‚è¯¥æ–¹æ³•é€šè¿‡é€æ­¥åå¥½ä¼˜åŒ–ï¼Œæ— éœ€ä¾èµ–å¤§é‡äººå·¥æ ‡æ³¨çš„ä»»åŠ¡ç­”æ¡ˆå¯¹å’Œå·¥å…·è½¨è¿¹ï¼Œå°±èƒ½è®©å¤šæ¨¡æ€æ™ºèƒ½ä½“è‡ªä¸»å‘ç°æœ‰æ•ˆçš„å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚SPORTåŒ…æ‹¬ä»»åŠ¡åˆæˆã€æ­¥éª¤é‡‡æ ·ã€æ­¥éª¤éªŒè¯å’Œåå¥½è°ƒæ•´å››ä¸ªè¿­ä»£æ­¥éª¤ã€‚è¯¥æ–¹æ³•è§£å†³äº†å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚æˆ–ä¸å®é™…çš„é—®é¢˜ï¼Œä½¿æ™ºèƒ½ä½“åœ¨ä¸çœŸå®ç¯å¢ƒäº¤äº’ä¸­é€æ¸è¿›åŒ–ä¸ºæ›´ç²¾ç»†ã€æ›´å¼ºå¤§çš„ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨åº”å¯¹å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†éœ€è¦å¤§é‡çš„ä»»åŠ¡ç­”æ¡ˆå¯¹å’Œå·¥å…·è½¨è¿¹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºäººå·¥æ ‡æ³¨ï¼Œæˆæœ¬é«˜æ˜‚ä¸”ä¸åˆ‡å®é™…ã€‚</li>
<li>SPORTæ–¹æ³•é€šè¿‡è¿­ä»£å·¥å…·ä½¿ç”¨æ¢ç´¢ï¼Œæ— éœ€é¢„å…ˆæ”¶é›†æ•°æ®ï¼Œè®©å¤šæ¨¡æ€æ™ºèƒ½ä½“è‡ªä¸»å‘ç°æœ‰æ•ˆå·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚</li>
<li>SPORTåŒ…æ‹¬ä»»åŠ¡åˆæˆã€æ­¥éª¤é‡‡æ ·ã€æ­¥éª¤éªŒè¯å’Œåå¥½è°ƒæ•´å››ä¸ªæ­¥éª¤ã€‚</li>
<li>æ­¥éª¤é‡‡æ ·ä¸­æ™ºèƒ½ä½“å°è¯•ä¸åŒå·¥å…·å¹¶è·å–ç»“æœï¼Œæ­¥éª¤éªŒè¯åˆ™é€šè¿‡éªŒè¯å™¨æä¾›åé¦ˆæ¥æ„å»ºé€æ­¥åå¥½æ•°æ®ã€‚</li>
<li>åå¥½æ•°æ®ç”¨äºé€šè¿‡åå¥½è°ƒæ•´æ›´æ–°å·¥å…·ä½¿ç”¨æ§åˆ¶å™¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSPORTæ–¹æ³•æé«˜äº†å¤šæ¨¡æ€æ™ºèƒ½ä½“çš„æ³›åŒ–å’Œæ•ˆèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fca6fc4cab725d348f3b9e6180fbd7ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-939011b4fad6ec57cfcb7af4829dfa84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9efe7127d184df12ab924217c13636c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abf02315ebcb3956ae0027847916847e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MathAgent-Leveraging-a-Mixture-of-Math-Agent-Framework-for-Real-World-Multimodal-Mathematical-Error-Detection"><a href="#MathAgent-Leveraging-a-Mixture-of-Math-Agent-Framework-for-Real-World-Multimodal-Mathematical-Error-Detection" class="headerlink" title="MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World   Multimodal Mathematical Error Detection"></a>MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World   Multimodal Mathematical Error Detection</h2><p><strong>Authors:Yibo Yan, Shen Wang, Jiahao Huo, Philip S. Yu, Xuming Hu, Qingsong Wen</strong></p>
<p>Mathematical error detection in educational settings presents a significant challenge for Multimodal Large Language Models (MLLMs), requiring a sophisticated understanding of both visual and textual mathematical content along with complex reasoning capabilities. Though effective in mathematical problem-solving, MLLMs often struggle with the nuanced task of identifying and categorizing student errors in multimodal mathematical contexts. Therefore, we introduce MathAgent, a novel Mixture-of-Math-Agent framework designed specifically to address these challenges. Our approach decomposes error detection into three phases, each handled by a specialized agent: an image-text consistency validator, a visual semantic interpreter, and an integrative error analyzer. This architecture enables more accurate processing of mathematical content by explicitly modeling relationships between multimodal problems and student solution steps. We evaluate MathAgent on real-world educational data, demonstrating approximately 5% higher accuracy in error step identification and 3% improvement in error categorization compared to baseline models. Besides, MathAgent has been successfully deployed in an educational platform that has served over one million K-12 students, achieving nearly 90% student satisfaction while generating significant cost savings by reducing manual error detection. </p>
<blockquote>
<p>åœ¨æ•™è‚²ç¯å¢ƒä¸­ï¼Œå¯¹æ•°å­¦é”™è¯¯çš„æ£€æµ‹å¯¹äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™éœ€è¦å®ƒä»¬å¯¹è§†è§‰å’Œæ–‡æœ¬æ•°å­¦å†…å®¹æœ‰æ·±å…¥çš„ç†è§£ï¼Œä»¥åŠå¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶MLLMsåœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†åœ¨å¤šæ¨¡æ€æ•°å­¦ç¯å¢ƒä¸­è¯†åˆ«å’Œåˆ†ç±»å­¦ç”Ÿé”™è¯¯è¿™ä¸€å¾®å¦™ä»»åŠ¡ä¸Šå¸¸å¸¸é‡åˆ°å›°éš¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MathAgentï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºåº”å¯¹è¿™äº›æŒ‘æˆ˜çš„æ–°å‹MathAgentæ··åˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†é”™è¯¯æ£€æµ‹åˆ†è§£ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½ç”±ä¸€ä¸ªä¸“ä¸šä»£ç†å¤„ç†ï¼šå›¾åƒæ–‡æœ¬ä¸€è‡´æ€§éªŒè¯å™¨ã€è§†è§‰è¯­ä¹‰è§£é‡Šå™¨å’Œç»¼åˆé”™è¯¯åˆ†æå™¨ã€‚è¯¥æ¶æ„é€šè¿‡æ˜¾å¼å»ºæ¨¡å¤šæ¨¡æ€é—®é¢˜å’Œå­¦ç”Ÿè§£é¢˜æ­¥éª¤ä¹‹é—´çš„å…³ç³»ï¼Œå®ç°äº†å¯¹æ•°å­¦å†…å®¹çš„æ›´ç²¾ç¡®å¤„ç†ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„æ•™è‚²æ•°æ®ä¸Šè¯„ä¼°äº†MathAgentï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œå…¶åœ¨é”™è¯¯æ­¥éª¤è¯†åˆ«æ–¹é¢æœ‰å¤§çº¦5%çš„å‡†ç¡®ç‡æé«˜ï¼Œåœ¨é”™è¯¯åˆ†ç±»æ–¹é¢æœ‰3%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒMathAgentå·²æˆåŠŸéƒ¨ç½²åœ¨æ•™è‚²å¹³å°ä¸Šï¼ŒæœåŠ¡äºè¶…è¿‡ä¸€ç™¾ä¸‡çš„ä¸­å°å­¦å­¦ç”Ÿï¼Œå®ç°äº†è¿‘90%çš„å­¦ç”Ÿæ»¡æ„åº¦ï¼ŒåŒæ—¶é€šè¿‡å‡å°‘æ‰‹åŠ¨é”™è¯¯æ£€æµ‹äº§ç”Ÿäº†æ˜¾è‘—çš„æˆæœ¬èŠ‚çº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18132v2">PDF</a> Accepted by The 63rd Annual Meeting of the Association for   Computational Linguistics (ACL Industry 2025, Oral Presentation)</p>
<p><strong>Summary</strong><br>æ•°å­¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²ç¯å¢ƒä¸­è¿›è¡Œé”™è¯¯æ£€æµ‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MathAgentæ··åˆæ¨¡å‹æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µå¤„ç†å­¦ç”Ÿæ•°å­¦é¢˜çš„é”™è¯¯ï¼šå›¾æ–‡ä¸€è‡´æ€§éªŒè¯ã€è§†è§‰è¯­ä¹‰è§£é‡Šå’Œç»¼åˆè¯¯å·®åˆ†æã€‚è¯¥æ¶æ„æé«˜äº†å¯¹æ•°å­¦å†…å®¹çš„å¤„ç†å‡†ç¡®æ€§ï¼Œé€šè¿‡æ˜ç¡®å»ºæ¨¡å¤šæ¨¡æ€é—®é¢˜å’Œå­¦ç”Ÿçš„è§£é¢˜æ­¥éª¤ä¹‹é—´çš„å…³ç³»ã€‚åœ¨çœŸå®æ•™è‚²æ•°æ®ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMathAgentåœ¨é”™è¯¯æ­¥éª¤è¯†åˆ«å’Œé”™è¯¯åˆ†ç±»æ–¹é¢ç›¸æ¯”åŸºå‡†æ¨¡å‹æœ‰æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼ŒMathAgentå·²æˆåŠŸéƒ¨ç½²åœ¨æ•™è‚²å¹³å°ä¸Šï¼Œä¸ºK-12å­¦ç”Ÿæä¾›æœåŠ¡ï¼Œè·å¾—è¿‘90%çš„å­¦ç”Ÿæ»¡æ„åº¦ï¼Œå¹¶é€šè¿‡å‡å°‘æ‰‹åŠ¨é”™è¯¯æ£€æµ‹äº§ç”Ÿæ˜¾è‘—çš„æˆæœ¬æ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´æ•™è‚²ç¯å¢ƒä¸­é”™è¯¯æ£€æµ‹çš„éš¾é¢˜ã€‚</li>
<li>å¼•å…¥MathAgentæ··åˆæ¨¡å‹æ¡†æ¶æ¥è§£å†³æ­¤æŒ‘æˆ˜ã€‚</li>
<li>MathAgentåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µå¤„ç†å­¦ç”Ÿæ•°å­¦é¢˜çš„é”™è¯¯ï¼šå›¾æ–‡ä¸€è‡´æ€§éªŒè¯ã€è§†è§‰è¯­ä¹‰è§£é‡Šå’Œç»¼åˆè¯¯å·®åˆ†æã€‚</li>
<li>MathAgentæ¶æ„æé«˜äº†å¯¹æ•°å­¦å†…å®¹çš„å¤„ç†å‡†ç¡®æ€§ã€‚</li>
<li>MathAgentåœ¨çœŸå®æ•™è‚²æ•°æ®ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºå…¶æ˜¾è‘—æé«˜äº†é”™è¯¯æ£€æµ‹å’Œåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
<li>MathAgentå·²æˆåŠŸéƒ¨ç½²åœ¨æ•™è‚²å¹³å°ä¸Šï¼Œä¸ºK-12å­¦ç”Ÿæä¾›æœåŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6efcdf8f08f796154931946673848c65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d844a2bda09f518adffbf39a8810c8b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bea12b3e401531c1f8e17ecb2b382f29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19ea4a5b6793c35d0c0249a0185db3af.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="R2-KG-General-Purpose-Dual-Agent-Framework-for-Reliable-Reasoning-on-Knowledge-Graphs"><a href="#R2-KG-General-Purpose-Dual-Agent-Framework-for-Reliable-Reasoning-on-Knowledge-Graphs" class="headerlink" title="R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on   Knowledge Graphs"></a>R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on   Knowledge Graphs</h2><p><strong>Authors:Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi</strong></p>
<p>Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks still suffer two practical drawbacks: they must be re-tuned whenever the KG or reasoning task changes, and they depend on a single, high-capacity LLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across five diverse benchmarks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability with reduced inference cost but increased abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning, reducing reliance on high-capacity LLMs while ensuring trustworthy inference. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ekrxjwh2009/R2-KG/">https://github.com/ekrxjwh2009/R2-KG/</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ï¼Œä»¥æé«˜æ¨ç†èƒ½åŠ›ï¼Œå¯åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜æ¨ç†å‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡è½»è™šæ„ç°è±¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ¡†æ¶ä»å­˜åœ¨ä¸¤ä¸ªå®é™…ç¼ºé™·ï¼šæ¯å½“çŸ¥è¯†å›¾è°±æˆ–æ¨ç†ä»»åŠ¡å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå®ƒä»¬éƒ½å¿…é¡»é‡æ–°è°ƒæ•´ï¼Œå¹¶ä¸”å®ƒä»¬ä¾èµ–äºä¸€ä¸ªå¯é ï¼ˆå³å¯ä¿¡ï¼‰æ¨ç†çš„é«˜å®¹é‡LLMã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†R2-KGï¼Œè¿™æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„åŒä»£ç†æ¡†æ¶ï¼Œå®ƒå°†æ¨ç†åˆ†ä¸ºä¸¤ä¸ªè§’è‰²ï¼šä¸€ä¸ªæ“ä½œå‘˜ï¼ˆä½å®¹é‡LLMï¼‰ï¼Œè´Ÿè´£æ”¶é›†è¯æ®ï¼Œä¸€ä¸ªä¸»ç®¡ï¼ˆé«˜å®¹é‡LLMï¼‰ï¼Œè´Ÿè´£åšå‡ºæœ€ç»ˆåˆ¤æ–­ã€‚è¿™ç§è®¾è®¡åœ¨LLMæ¨ç†æ–¹é¢æ˜¯æˆæœ¬æ•ˆç›Šé«˜çš„ï¼ŒåŒæ—¶ä»èƒ½ä¿æŒå¼ºå¤§çš„æ¨ç†å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒR2-KGé‡‡ç”¨äº†ä¸€ç§æ‹’ç»æœºåˆ¶ï¼Œä»…åœ¨ä»çŸ¥è¯†å›¾è°±æ”¶é›†åˆ°è¶³å¤Ÿè¯æ®æ—¶æ‰ç”Ÿæˆç­”æ¡ˆï¼Œè¿™æ˜¾è‘—æé«˜äº†å¯é æ€§ã€‚åœ¨äº”ä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒR2-KGåœ¨å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢å§‹ç»ˆä¼˜äºåŸºå‡†æµ‹è¯•ï¼Œæ— è®ºä½¿ç”¨çš„æ“ä½œå‘˜LLMçš„å†…åœ¨èƒ½åŠ›å¦‚ä½•ã€‚è¿›ä¸€æ­¥çš„å®éªŒè¡¨æ˜ï¼Œé…å¤‡ä¸¥æ ¼è‡ªæˆ‘ä¸€è‡´æ€§ç­–ç•¥çš„å•ä¸€ä»£ç†ç‰ˆæœ¬çš„R2-KGï¼Œåœ¨é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†é«˜äºåŸºå‡†çš„å¯é æ€§ï¼Œä½†åœ¨å¤æ‚çŸ¥è¯†å›¾è°±ä¸­çš„æ‹’ç»ç‡æœ‰æ‰€å¢åŠ ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒR2-KGæ˜¯ä¸€ä¸ªçµæ´»ä¸”ç»æµé«˜æ•ˆçš„åŸºäºçŸ¥è¯†å›¾è°±çš„æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œé™ä½äº†å¯¹é«˜å®¹é‡LLMçš„ä¾èµ–ï¼ŒåŒæ—¶ç¡®ä¿å¯ä¿¡æ¨ç†ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ekrxjwh2009/R2-KG/">https://github.com/ekrxjwh2009/R2-KG/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12767v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„ç»“åˆæé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å®è·µåº”ç”¨ä¸­ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†R2-KGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŒä»£ç†è®¾è®¡ï¼Œå°†æ¨ç†åˆ†ä¸ºè¯æ®æ”¶é›†ä¸æœ€ç»ˆåˆ¤æ–­ä¸¤ä¸ªç¯èŠ‚ï¼Œç”±ä½å®¹é‡LLMæ‹…ä»»æ“ä½œè€…è§’è‰²ï¼Œé«˜å®¹é‡LLMæ‹…ä»»ç›‘ç£è€…è§’è‰²ã€‚æ­¤è®¾è®¡åœ¨é™ä½æˆæœ¬çš„åŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ¨ç†å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡æ‹’ç»æœºåˆ¶æé«˜äº†å¯é æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒR2-KGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ— è®ºä½¿ç”¨çš„LLMèƒ½åŠ›å¦‚ä½•ï¼Œéƒ½èƒ½æé«˜å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼Œå•ä»£ç†ç‰ˆæœ¬çš„R2-KGé‡‡ç”¨ä¸¥æ ¼çš„è‡ªæˆ‘ä¸€è‡´æ€§ç­–ç•¥ï¼Œåœ¨é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶æé«˜äº†å¯é æ€§ï¼Œä½†åœ¨å¤æ‚çŸ¥è¯†å›¾è°±ä¸­çš„æ‹’ç»ç‡æœ‰æ‰€ä¸Šå‡ã€‚æœ¬ç ”ç©¶ç¡®ç«‹äº†R2-KGåœ¨çŸ¥è¯†å›¾è°±æ¨ç†ä¸­çš„çµæ´»æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsä¸KGsç»“åˆå¢å¼ºäº†æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨å®ç”¨æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>R2-KGæ¡†æ¶é‡‡ç”¨åŒä»£ç†è®¾è®¡ï¼Œåˆ†ç¦»æ¨ç†è§’è‰²ä¸ºæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>R2-KGé€šè¿‡å¼•å…¥æ‹’ç»æœºåˆ¶æ˜¾è‘—æé«˜å¯é æ€§ã€‚</li>
<li>R2-KGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œé€‚åº”ä¸åŒèƒ½åŠ›çš„LLMsã€‚</li>
<li>å•ä»£ç†ç‰ˆæœ¬çš„R2-KGåœ¨å¤æ‚çŸ¥è¯†å›¾è°±ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„æ‹’ç»ç‡ã€‚</li>
<li>R2-KGæ¡†æ¶é™ä½äº†å¯¹é«˜å®¹é‡LLMsçš„ä¾èµ–ï¼Œå…·æœ‰æˆæœ¬æ•ˆç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a0e402645a97b1c9474744e7582729e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70326c7591212f1a08bbbbe1e5097b67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6af0546ff798a481cdca4430c8dcd5df.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Uncovering-Untapped-Potential-in-Sample-Efficient-World-Model-Agents"><a href="#Uncovering-Untapped-Potential-in-Sample-Efficient-World-Model-Agents" class="headerlink" title="Uncovering Untapped Potential in Sample-Efficient World Model Agents"></a>Uncovering Untapped Potential in Sample-Efficient World Model Agents</h2><p><strong>Authors:Lior Cohen, Kaixin Wang, Bingyi Kang, Uri Gadot, Shie Mannor</strong></p>
<p>World model (WM) agents enable sample-efficient reinforcement learning by learning policies entirely from simulated experience. However, existing token-based world models (TBWMs) are limited to visual inputs and discrete actions, restricting their adoption and applicability. Moreover, although both intrinsic motivation and prioritized WM replay have shown promise in improving WM performance and generalization, they remain underexplored in this setting, particularly in combination. We introduce Simulus, a highly modular TBWM agent that integrates (1) a modular multi-modality tokenization framework, (2) intrinsic motivation, (3) prioritized WM replay, and (4) regression-as-classification for reward and return prediction. Simulus achieves state-of-the-art sample efficiency for planning-free WMs across three diverse benchmarks. Ablation studies reveal the individual contribution of each component while highlighting their synergy. Our code and model weights are publicly available at <a target="_blank" rel="noopener" href="https://github.com/leor-c/Simulus">https://github.com/leor-c/Simulus</a>. </p>
<blockquote>
<p>ä¸–ç•Œæ¨¡å‹ï¼ˆWMï¼‰ä»£ç†é€šè¿‡å®Œå…¨ä»æ¨¡æ‹Ÿç»éªŒä¸­å­¦ä¹ ç­–ç•¥ï¼Œå®ç°äº†é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ ‡è®°çš„ä¸–ç•Œæ¨¡å‹ï¼ˆTBWMï¼‰ä»…é™äºè§†è§‰è¾“å…¥å’Œç¦»æ•£åŠ¨ä½œï¼Œé™åˆ¶äº†å…¶é‡‡ç”¨å’Œé€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œå°½ç®¡å†…åœ¨åŠ¨åŠ›æœºåˆ¶å’Œä¼˜å…ˆçš„WMå›æ”¾æŠ€æœ¯åœ¨æé«˜WMæ€§èƒ½å’Œæ³›åŒ–æ–¹é¢éƒ½æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨è¿™ä¸€ç‚¹ä¸Šä»è¢«ä½ä¼°äº†ï¼Œå°¤å…¶æ˜¯å®ƒä»¬çš„ç»“åˆä½¿ç”¨ã€‚æˆ‘ä»¬å¼•å…¥äº†Simulusï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜åº¦æ¨¡å—åŒ–çš„TBWMä»£ç†ï¼Œå®ƒé›†æˆäº†ï¼ˆ1ï¼‰æ¨¡å—åŒ–å¤šæ¨¡å¼æ ‡è®°æ¡†æ¶ï¼Œï¼ˆ2ï¼‰å†…åœ¨åŠ¨åŠ›æœºåˆ¶ï¼Œï¼ˆ3ï¼‰ä¼˜å…ˆWMå›æ”¾æŠ€æœ¯ï¼Œä»¥åŠï¼ˆ4ï¼‰å›å½’åˆ†ç±»ç”¨äºå¥–åŠ±å’Œå›æŠ¥é¢„æµ‹ã€‚Simulusåœ¨ä¸‰ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ ·æœ¬æ•ˆç‡çš„éè§„åˆ’WMsã€‚æ¶ˆé™¤ç ”ç©¶æ­ç¤ºäº†æ¯ä¸ªç»„ä»¶çš„å•ç‹¬è´¡çŒ®ï¼ŒåŒæ—¶å¼ºè°ƒäº†å®ƒä»¬çš„ååŒä½œç”¨ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/leor-c/Simulus%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/leor-c/Simuluså…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11537v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸–ç•Œæ¨¡å‹ï¼ˆWMï¼‰ä»£ç†é€šè¿‡å®Œå…¨ä»æ¨¡æ‹Ÿç»éªŒä¸­å­¦ä¹ ç­–ç•¥ï¼Œå®ç°äº†å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬é«˜æ•ˆæ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºä»¤ç‰Œçš„çš„ä¸–ç•Œæ¨¡å‹ï¼ˆTBWMsï¼‰ä»…é™äºè§†è§‰è¾“å…¥å’Œç¦»æ•£åŠ¨ä½œï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚Simulusæ˜¯ä¸€ä¸ªé«˜åº¦æ¨¡å—åŒ–çš„TBWMä»£ç†ï¼Œå®ƒé›†æˆäº†å¤šæ¨¡æ€ä»¤ç‰ŒåŒ–æ¡†æ¶ã€å†…åœ¨åŠ¨æœºã€ä¼˜å…ˆWMå›æ”¾å’Œå›å½’åˆ†ç±»ç”¨äºå¥–åŠ±å’Œå›æŠ¥é¢„æµ‹ã€‚Simulusåœ¨ä¸‰ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¸–ç•Œæ¨¡å‹å‰æ‰€æœªæœ‰çš„æ ·æœ¬æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸–ç•Œæ¨¡å‹ä»£ç†é€šè¿‡æ¨¡æ‹Ÿç»éªŒå­¦ä¹ ç­–ç•¥ï¼Œå®ç°å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬é«˜æ•ˆæ€§ã€‚</li>
<li>ç°æœ‰çš„åŸºäºä»¤ç‰Œçš„çš„ä¸–ç•Œæ¨¡å‹ï¼ˆTBWMsï¼‰å­˜åœ¨å±€é™æ€§ï¼Œä»…é™äºè§†è§‰è¾“å…¥å’Œç¦»æ•£åŠ¨ä½œã€‚</li>
<li>Simulusæ˜¯ä¸€ä¸ªé«˜åº¦æ¨¡å—åŒ–çš„TBWMä»£ç†ï¼Œé›†æˆäº†å¤šæ¨¡æ€ä»¤ç‰ŒåŒ–æ¡†æ¶ã€‚</li>
<li>Simuluså¼•å…¥äº†å†…åœ¨åŠ¨æœºå’Œä¼˜å…ˆWMå›æ”¾ï¼Œä»¥æé«˜WMæ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Simulusé‡‡ç”¨å›å½’åˆ†ç±»é¢„æµ‹å¥–åŠ±å’Œå›æŠ¥ã€‚</li>
<li>Simulusåœ¨ä¸‰ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸–ç•Œæ¨¡å‹æ ·æœ¬æ•ˆç‡çš„æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-936958a5dc6805bb82e3dd26aaffc0e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57b1e378102d0b4c1ebfe69a5b61dd25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7482f708f643fdc4184a95214ea4b5c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-901e9083fd61d4aa26d27fbd2e8497fe.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7964b2debc9adf20fd660a685fa7ceda.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  PRL Prompts from Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e0ce1a4a79fec8d00af82d3b31392152.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  UniGen Enhanced Training & Test-Time Strategies for Unified Multimodal   Understanding and Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
