<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Emerging Properties in Unified Multimodal Pretraining">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c3d46f571e781e50ca048e545b9a06f2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-22-æ›´æ–°"><a href="#2025-05-22-æ›´æ–°" class="headerlink" title="2025-05-22 æ›´æ–°"></a>2025-05-22 æ›´æ–°</h1><h2 id="Emerging-Properties-in-Unified-Multimodal-Pretraining"><a href="#Emerging-Properties-in-Unified-Multimodal-Pretraining" class="headerlink" title="Emerging Properties in Unified Multimodal Pretraining"></a>Emerging Properties in Unified Multimodal Pretraining</h2><p><strong>Authors:Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, Haoqi Fan</strong></p>
<p>Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at <a target="_blank" rel="noopener" href="https://bagel-ai.org/">https://bagel-ai.org/</a> </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ–¹é¢ï¼Œæœ€å‰æ²¿çš„ä¸“æœ‰ç³»ç»Ÿå·²ç»å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†BAGELï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„åŸºç¡€æ¨¡å‹ï¼Œå®ƒåŸç”Ÿæ”¯æŒå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚BAGELæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ã€ä»…è§£ç çš„æ¨¡å‹ï¼Œå®ƒåœ¨ä»å¤§è§„æ¨¡äº¤é”™æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç½‘ç»œæ•°æ®ä¸­ç²¾é€‰å‡ºçš„ä¸‡äº¿çº§æ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚å½“ä½¿ç”¨å¦‚æ­¤å¤šæ ·åŒ–çš„å¤šæ¨¡æ€äº¤é”™æ•°æ®è¿›è¡Œæ‰©å±•æ—¶ï¼ŒBAGELåœ¨å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†æ–¹é¢å±•ç°å‡ºæ–°å…´çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œå®ƒåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£æ–¹é¢å¤§å¤§ä¼˜äºå¼€æºç»Ÿä¸€æ¨¡å‹ï¼ŒåŒæ—¶å±•ç°å‡ºå…ˆè¿›çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œå¦‚è‡ªç”±å½¢å¼çš„å›¾åƒæ“ä½œã€æœªæ¥å¸§é¢„æµ‹ã€3Dæ“ä½œå’Œå…¨çƒå¯¼èˆªç­‰ã€‚æˆ‘ä»¬å¸Œæœ›ä¸ºè¿›ä¸€æ­¥çš„è·¨æ¨¡æ€ç ”ç©¶æä¾›æœºä¼šï¼Œå› æ­¤åˆ†äº«å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚ã€æ•°æ®åˆ›å»ºåè®®ä»¥åŠå‘ç¤¾åŒºå‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ£€æŸ¥ç‚¹ã€‚é¡¹ç›®é¡µé¢ä½äºï¼š[<a target="_blank" rel="noopener" href="https://bagel-ai.org/]">https://bagel-ai.org/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14683v1">PDF</a> 37 pages, 17 figures</p>
<p><strong>Summary</strong></p>
<p>BAGELæ˜¯ä¸€ä¸ªå¼€æºçš„åŸºç¡€æ¨¡å‹ï¼Œæ”¯æŒå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚è¯¥æ¨¡å‹åœ¨å¤§é‡å¤§è§„æ¨¡äº¤æ›¿æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç½‘é¡µæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºå¼€æºç»Ÿä¸€æ¨¡å‹ï¼Œå…·å¤‡è‡ªç”±å½¢å¼çš„å›¾åƒæ“ä½œã€æœªæ¥å¸§é¢„æµ‹ã€3Dæ“ä½œå’Œå¯¼èˆªç­‰é«˜çº§åŠŸèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BAGELæ˜¯ä¸€ä¸ªæ”¯æŒå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„å¤šåŠŸèƒ½å¼€æºåŸºç¡€æ¨¡å‹ã€‚</li>
<li>BAGELé€šè¿‡é¢„è®­ç»ƒåœ¨å¤§é‡å¤§è§„æ¨¡äº¤æ›¿æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œç½‘é¡µæ•°æ®ä¸Šå®ç°å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>BAGELå±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å¼€æºç»Ÿä¸€æ¨¡å‹ï¼Œåœ¨å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£æ ‡å‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>BAGELå…·å¤‡é«˜çº§å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œå¦‚è‡ªç”±å½¢å¼çš„å›¾åƒæ“ä½œã€æœªæ¥å¸§é¢„æµ‹å’Œ3Dæ“ä½œç­‰ã€‚</li>
<li>è¯¥æ¨¡å‹å…è®¸ç”¨æˆ·è¿›è¡Œæ•°æ®æ“æ§ã€å¯¼èˆªç­‰å¤šç§é«˜çº§åº”ç”¨æ“ä½œã€‚</li>
<li>ä¸ºæ¨åŠ¨å¤šæ¨¡æ€ç ”ç©¶çš„å‘å±•ï¼Œåˆ†äº«äº†å…³é”®å‘ç°ã€é¢„è®­ç»ƒç»†èŠ‚ã€æ•°æ®åˆ›å»ºåè®®ä»¥åŠä»£ç å’Œæ£€æŸ¥ç‚¹ã€‚</li>
<li>è¯¥é¡¹ç›®çš„ç½‘é¡µåœ°å€ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://bagel-ai.org/]">https://bagel-ai.org/]</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f45e7b2213eed9005807364423f3f74f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b67e37213c629c063e2c311045b36be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d04fcd0f8eefd46e9f30cc612d6dc0e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c69d6208ba8e6a38a20a4027fb8e40c6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Two-Experts-Are-All-You-Need-for-Steering-Thinking-Reinforcing-Cognitive-Effort-in-MoE-Reasoning-Models-Without-Additional-Training"><a href="#Two-Experts-Are-All-You-Need-for-Steering-Thinking-Reinforcing-Cognitive-Effort-in-MoE-Reasoning-Models-Without-Additional-Training" class="headerlink" title="Two Experts Are All You Need for Steering Thinking: Reinforcing   Cognitive Effort in MoE Reasoning Models Without Additional Training"></a>Two Experts Are All You Need for Steering Thinking: Reinforcing   Cognitive Effort in MoE Reasoning Models Without Additional Training</h2><p><strong>Authors:Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, Dong Yu</strong></p>
<p>Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed â€˜â€™cognitive expertsâ€™â€™ that orchestrate meta-level reasoning operations characterized by tokens like â€˜â€™<think>â€˜â€™. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the modelâ€™s general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ä¸­çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„é€šè¿‡é€‰æ‹©æ€§æ¿€æ´»ä¸“å®¶ä»¥ä¿ƒè¿›ç»“æ„åŒ–è®¤çŸ¥è¿‡ç¨‹ï¼Œå·²ç»å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ¨ç†æ¨¡å‹å¾€å¾€å­˜åœ¨è®¤çŸ¥æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¦‚è¿‡åº¦æ€è€ƒå’Œæ€è€ƒä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¨ç†æ—¶é—´å¼•å¯¼æ–¹æ³•ï¼Œç§°ä¸ºå¼ºåŒ–è®¤çŸ¥ä¸“å®¶ï¼ˆRICEï¼‰ï¼Œæ—¨åœ¨æé«˜æ¨ç†æ€§èƒ½ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–å¤æ‚çš„å¯å‘å¼æ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨æ ‡å‡†åŒ–ç‚¹äº’ä¿¡æ¯ï¼ˆnPMIï¼‰ç³»ç»Ÿåœ°è¯†åˆ«ä¸“é—¨ä»äº‹ç‰¹å®šä»»åŠ¡çš„ä¸“å®¶ï¼Œç§°ä¸ºâ€œè®¤çŸ¥ä¸“å®¶â€ï¼Œä»–ä»¬è´Ÿè´£åè°ƒä»¥ä»¤ç‰Œå¦‚â€œ&lt;æ€è€ƒ&gt;â€ä¸ºç‰¹å¾çš„å…ƒçº§æ¨ç†æ“ä½œã€‚åœ¨é¢†å…ˆçš„åŸºäºMoEçš„LRMsï¼ˆDeepSeek-R1å’ŒQwen3-235Bï¼‰ä¸Šè¿›è¡Œä¸¥æ ¼çš„å®šé‡å’Œç§‘å­¦æ¨ç†åŸºå‡†çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œåœ¨æ¨ç†å‡†ç¡®æ€§ã€è®¤çŸ¥æ•ˆç‡å’Œè·¨åŸŸæ³›åŒ–æ–¹é¢éƒ½æœ‰æ˜¾è‘—ä¸”ä¸€è‡´çš„æ”¹è¿›ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è½»é‡çº§æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¼˜äºæµè¡Œçš„æ¨ç†å¼•å¯¼æŠ€æœ¯ï¼Œå¦‚æç¤ºè®¾è®¡å’Œè§£ç çº¦æŸï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„é€šç”¨æŒ‡ä»¤éµå¾ªæŠ€èƒ½ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å¼ºåŒ–è®¤çŸ¥ä¸“å®¶ä½œä¸ºä¸€ä¸ªæœ‰å‰é€”ã€å®ç”¨å’Œå¯è§£é‡Šçš„æ–¹å‘ï¼Œå¯ä»¥åœ¨é«˜çº§æ¨ç†æ¨¡å‹ä¸­æé«˜è®¤çŸ¥æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14681v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†Mixture-of-Expertsï¼ˆMoEï¼‰æ¶æ„çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡é€‰æ‹©æ€§æ¿€æ´»ä¸“å®¶æ¥å®ç°ç»“æ„åŒ–è®¤çŸ¥è¿‡ç¨‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰æ¨ç†æ¨¡å‹çš„è®¤çŸ¥æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œæ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨ç†æ—¶é—´æ§åˆ¶æ–¹æ³•â€”â€”å¼ºåŒ–è®¤çŸ¥ä¸“å®¶ï¼ˆRICEï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ ‡å‡†åŒ–ç‚¹äº’ä¿¡æ¯ï¼ˆnPMIï¼‰ç³»ç»Ÿè¯†åˆ«ä¸“é—¨åŒ–çš„ä¸“å®¶ï¼Œç§°ä¸ºâ€œè®¤çŸ¥ä¸“å®¶â€ï¼Œè´Ÿè´£å…ƒçº§åˆ«æ¨ç†æ“ä½œï¼Œå¦‚ç”±â€œ<think>â€ç­‰æ ‡è®°çš„è¯è¿›è¡Œåè°ƒã€‚åœ¨é¢†å…ˆçš„MoEåŸºLRMsï¼ˆDeepSeek-R1å’ŒQwen3-235Bï¼‰ä¸Šè¿›è¡Œå®è¯è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºåœ¨å®šé‡æ¨ç†å’Œç§‘å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒRICEåœ¨æ¨ç†å‡†ç¡®æ€§ã€è®¤çŸ¥æ•ˆç‡å’Œè·¨åŸŸæ³›åŒ–æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¸”ä¸€è‡´çš„æå‡ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºå…¶ä»–å¸¸è§çš„æ¨ç†æ§åˆ¶æŠ€å·§ï¼Œå¦‚æç¤ºè®¾è®¡å’Œè§£ç çº¦æŸç­‰ï¼ŒRICEæ›´è½»ä¾¿ä¸”èƒ½ä¿ç•™æ¨¡å‹çš„é€šç”¨æŒ‡ä»¤è·ŸéšæŠ€èƒ½ã€‚è¿™ä¸ºå¢å¼ºé«˜çº§æ¨ç†æ¨¡å‹çš„è®¤çŸ¥æ•ˆç‡æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯ã€å®ç”¨å’Œå¯è§£é‡Šçš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoEæ¶æ„çš„LRMsé€šè¿‡æ¿€æ´»ç‰¹å®šä¸“å®¶å®ç°ç»“æ„åŒ–è®¤çŸ¥è¿‡ç¨‹ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨ç†æ¨¡å‹å­˜åœ¨è®¤çŸ¥æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¦‚è¿‡åº¦æ€è€ƒå’Œæ€è€ƒä¸è¶³ã€‚</li>
<li>RICEæ–¹æ³•é€šè¿‡è¯†åˆ«å¹¶å¼ºåŒ–â€œè®¤çŸ¥ä¸“å®¶â€æ¥æå‡æ¨ç†æ€§èƒ½ï¼Œè¿™äº›ä¸“å®¶è´Ÿè´£å…ƒçº§åˆ«æ¨ç†æ“ä½œã€‚</li>
<li>åˆ©ç”¨æ ‡å‡†åŒ–ç‚¹äº’ä¿¡æ¯ï¼ˆnPMIï¼‰ç³»ç»Ÿè¯†åˆ«â€œè®¤çŸ¥ä¸“å®¶â€ã€‚</li>
<li>RICEåœ¨å¤šä¸ªé¢†å…ˆMoEåŸºLRMsä¸Šçš„å®è¯è¯„ä¼°ä¸­ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†å‡†ç¡®æ€§ã€è®¤çŸ¥æ•ˆç‡å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>RICEç›¸è¾ƒäºå…¶ä»–æ¨ç†æ§åˆ¶æŠ€å·§æ›´è½»ä¾¿ï¼Œä¸”èƒ½ä¿ç•™æ¨¡å‹çš„é€šç”¨æŒ‡ä»¤è·ŸéšæŠ€èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf102d2bf8eb152a0e7292bdaa1fdc28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12babb34fff7637e48c62eda098b815e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3d46f571e781e50ca048e545b9a06f2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Visionary-R1-Mitigating-Shortcuts-in-Visual-Reasoning-with-Reinforcement-Learning"><a href="#Visionary-R1-Mitigating-Shortcuts-in-Visual-Reasoning-with-Reinforcement-Learning" class="headerlink" title="Visionary-R1: Mitigating Shortcuts in Visual Reasoning with   Reinforcement Learning"></a>Visionary-R1: Mitigating Shortcuts in Visual Reasoning with   Reinforcement Learning</h2><p><strong>Authors:Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, Kaiyang Zhou</strong></p>
<p>Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM â€“ by prompting the model to produce a reasoning chain before providing an answer â€“ can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks. </p>
<blockquote>
<p>å­¦ä¹ å’ŒæŒæ¡é€šç”¨æ¨ç†èƒ½åŠ›é•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€ä¸ªéš¾é¢˜ã€‚æœ€è¿‘ï¼Œå…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶ï¼Œå¦‚DeepSeek-R1ï¼Œè¡¨æ˜å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚GRPOï¼‰å¯ä»¥ä½¿é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹é€šè¿‡ç®€å•çš„é—®é¢˜ç­”æ¡ˆå¯¹æ¥å‘å±•æ¨ç†èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œè§†è§‰é—®ç­”å¯¹æ¥å¯¹å›¾åƒæ•°æ®è¿›è¡Œæ¨ç†ï¼Œè€Œæ— éœ€ä»»ä½•æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…ä»…å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹â€”â€”é€šè¿‡æç¤ºæ¨¡å‹åœ¨ç»™å‡ºç­”æ¡ˆä¹‹å‰äº§ç”Ÿæ¨ç†é“¾â€”â€”å¯èƒ½å¯¼è‡´æ¨¡å‹ä»ç®€å•é—®é¢˜ä¸­äº§ç”Ÿæ·å¾„ï¼Œä»è€Œé™ä½å…¶åœ¨æœªè§æ•°æ®åˆ†å¸ƒä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºé¿å…æ·å¾„å­¦ä¹ çš„å…³é”®åœ¨äºé¼“åŠ±æ¨¡å‹åœ¨æ¨ç†ä¹‹å‰è§£é‡Šå›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹éµå¾ªæ ‡é¢˜-æ¨ç†-ç­”æ¡ˆçš„è¾“å‡ºæ ¼å¼ï¼šé¦–å…ˆä¸ºå›¾åƒç”Ÿæˆè¯¦ç»†çš„æ ‡é¢˜ï¼Œç„¶åæ„å»ºå¹¿æ³›çš„æ¨ç†é“¾ã€‚åœ¨ä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¸”æ— éœ€æ€ç»´é“¾çš„è§†è§‰é—®ç­”å¯¹ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åä¸ºVisionary-R1ï¼Œåœ¨å¤šè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¦‚GPT-4oã€Claude3.5-Sonnetå’ŒGemini-1.5-Proç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14677v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œè§†è§‰é—®ç­”å¯¹è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå›¾åƒæ¨ç†çš„ç ”ç©¶ã€‚å®éªŒå‘ç°ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹åœ¨å›ç­”é—®é¢˜å‰äº§ç”Ÿæ¨ç†é“¾ï¼Œä¼šå¯¼è‡´æ¨¡å‹ä»ç®€å•é—®é¢˜ä¸­ä¹ å¾—æ·å¾„ï¼Œé™ä½å…¶åœ¨æœªè§æ•°æ®åˆ†å¸ƒä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œé¼“åŠ±æ¨¡å‹åœ¨æ¨ç†å‰å…ˆå¯¹å›¾åƒè¿›è¡Œè§£è¯»ã€‚è®­ç»ƒæ¨¡å‹éµå¾ªâ€œæè¿°-æ¨ç†-å›ç­”â€çš„è¾“å‡ºæ ¼å¼ï¼Œå…ˆä¸ºå›¾åƒç”Ÿæˆè¯¦ç»†æè¿°ï¼Œå†æ„å»ºæ¨ç†é“¾ã€‚ä½¿ç”¨ä»…åŒ…å«å¼ºåŒ–å­¦ä¹ å’Œæ— æ€ç»´é“¾ç›‘ç£çš„273Kè§†è§‰é—®ç­”å¯¹è¿›è¡Œè®­ç»ƒï¼Œæ‰€æå‡ºæ¨¡å‹Visionary-R1åœ¨å¤šè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜äºå…¶ä»–å¼ºå¤§è·¨æ¨¡æ€æ¨¡å‹ï¼ˆå¦‚GPT-4oã€Claude 3.5-Sonnetå’ŒGemini-1.5-Proï¼‰çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æŠ€æœ¯å¦‚GRPOèƒ½ä½¿é¢„è®­ç»ƒçš„LLMså…·å¤‡æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å¯¼æ¨¡å‹åœ¨å›ç­”é—®é¢˜å‰äº§ç”Ÿæ¨ç†é“¾ä¼šå¯¼è‡´æ¨¡å‹ä»ç®€å•é—®é¢˜ä¸­ä¹ å¾—æ·å¾„ï¼Œå½±å“å…¶åœ¨æœªè§æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ºç¼“è§£æ¨¡å‹æ·å¾„å­¦ä¹ é—®é¢˜ï¼Œéœ€è¦é¼“åŠ±æ¨¡å‹åœ¨æ¨ç†å‰å…ˆè§£è¯»å›¾åƒã€‚</li>
<li>è®­ç»ƒæ¨¡å‹éµå¾ªâ€œæè¿°-æ¨ç†-å›ç­”â€æ ¼å¼ï¼Œä»¥æé«˜å…¶åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œæ— æ€ç»´é“¾ç›‘ç£çš„å¤§é‡è§†è§‰é—®ç­”å¯¹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>Visionary-R1æ¨¡å‹åœ¨å¤šè§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-eedecaec5c153adc57b67a41d8d0a693.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ab580e632ed95049130be3c79d8fb5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36d55882085b3f5ebef82cd73865590a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc8674003374b06961547ba85760386b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Reward-Reasoning-Model"><a href="#Reward-Reasoning-Model" class="headerlink" title="Reward Reasoning Model"></a>Reward Reasoning Model</h2><p><strong>Authors:Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, Furu Wei</strong></p>
<p>Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at <a target="_blank" rel="noopener" href="https://huggingface.co/Reward-Reasoning">https://huggingface.co/Reward-Reasoning</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹åœ¨å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹äº§ç”Ÿç¬¦åˆäººç±»æœŸæœ›çš„è¾“å‡ºæ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºæ¥æå‡å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¥–åŠ±æ¨ç†æ¨¡å‹ï¼ˆRRMsï¼‰ï¼Œå®ƒä¸“é—¨è®¾è®¡ç”¨äºåœ¨ç”Ÿæˆæœ€ç»ˆå¥–åŠ±ä¹‹å‰æ‰§è¡Œæ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡é“¾å¼æ€ç»´æ¨ç†ï¼ŒRRMsèƒ½å¤Ÿåœ¨å¤æ‚æŸ¥è¯¢ä¸­åˆ©ç”¨é¢å¤–çš„æµ‹è¯•æ—¶é—´è®¡ç®—èµ„æºï¼Œåœ¨é€‚å½“çš„æ—¶å€™è·å¾—åˆé€‚çš„å¥–åŠ±ã€‚ä¸ºäº†å¼€å‘RRMsï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŸ¹è‚²è‡ªæˆ‘è¿›åŒ–çš„å¥–åŠ±æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€å°†æ˜ç¡®çš„æ¨ç†è½¨è¿¹ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRRMsåœ¨ä¸åŒé¢†åŸŸçš„å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†RRMsèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºæ¥è¿›ä¸€æ­¥æé«˜å¥–åŠ±çš„å‡†ç¡®æ€§ã€‚é¢„è®­ç»ƒçš„å¥–åŠ±æ¨ç†æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/Reward-Reasoning%E6%89%BE%E5%88%B0%E3%80%82">https://huggingface.co/Reward-Reasoningæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14674v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¥–åŠ±æ¨¡å‹å¯¹äºå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹äº§ç”Ÿç¬¦åˆäººç±»æœŸæœ›çš„è¾“å‡ºè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºæ¥æå‡å¥–åŠ±æ¨¡å‹æ€§èƒ½ä»æ˜¯æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†å¥–åŠ±æ¨ç†æ¨¡å‹ï¼ˆRRMsï¼‰ï¼Œä¸“é—¨è®¾è®¡ç”¨äºåœ¨ç”Ÿæˆæœ€ç»ˆå¥–åŠ±ä¹‹å‰è¿›è¡Œæ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹ã€‚å¯¹äºå¤æ‚æŸ¥è¯¢ï¼ŒRRMsåˆ©ç”¨é¢å¤–çš„æµ‹è¯•æ—¶é—´è®¡ç®—æ¥å¾—å‡ºé€‚å½“çš„å¥–åŠ±ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ¥åŸ¹å…»RRMsçš„è‡ªæˆ‘è¿›åŒ–å¥–åŠ±æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€æ˜ç¡®çš„æ¨ç†è½¨è¿¹ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRRMsåœ¨è·¨é¢†åŸŸçš„å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†RRMsèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ©ç”¨æµ‹è¯•æ—¶é—´è®¡ç®—æ¥è¿›ä¸€æ­¥æé«˜å¥–åŠ±å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¥–åŠ±æ¨¡å‹å¯¹å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹äº§å‡ºç¬¦åˆäººç±»æœŸæœ›çš„è¾“å‡ºè‡³å…³é‡è¦ã€‚</li>
<li>æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºæœ‰æ•ˆåˆ©ç”¨æ˜¯æå‡å¥–åŠ±æ¨¡å‹æ€§èƒ½çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>å¼•å…¥å¥–åŠ±æ¨ç†æ¨¡å‹ï¼ˆRRMsï¼‰ï¼Œä¸“é—¨è¿›è¡Œæ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹å†ç”Ÿæˆæœ€ç»ˆå¥–åŠ±ã€‚</li>
<li>RRMsåˆ©ç”¨é¢å¤–çš„æµ‹è¯•æ—¶é—´è®¡ç®—æ¥å¤„ç†å¤æ‚æŸ¥è¯¢ï¼Œå¾—å‡ºé€‚å½“å¥–åŠ±ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶åŸ¹å…»RRMsçš„è‡ªæˆ‘è¿›åŒ–å¥–åŠ±æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RRMsåœ¨è·¨é¢†åŸŸå¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-87e008355e834da2c01193f0474a59a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-488e48f0aafd6da22e75849470b37bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0334a638c74cecc248e99ebb6fe445e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="General-Reasoner-Advancing-LLM-Reasoning-Across-All-Domains"><a href="#General-Reasoner-Advancing-LLM-Reasoning-Across-All-Domains" class="headerlink" title="General-Reasoner: Advancing LLM Reasoning Across All Domains"></a>General-Reasoner: Advancing LLM Reasoning Across All Domains</h2><p><strong>Authors:Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen</strong></p>
<p>Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the â€œZeroâ€ reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æœ€è¿‘åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ½œåŠ›ã€‚ç‰¹åˆ«æ˜¯Deepseek-R1-Zeroå¼•å…¥çš„â€œZeroâ€å¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿå®ç°åŸºç¡€LLMçš„ç›´æ¥RLè®­ç»ƒï¼Œæ— éœ€ä¾èµ–ä¸­é—´ç›‘ç£å¾®è°ƒé˜¶æ®µã€‚å°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼Œä½†ç›®å‰LLMæ¨ç†çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ•°æ®ä¸°å¯Œä¸”ç­”æ¡ˆéªŒè¯å®¹æ˜“ã€‚è¿™é™åˆ¶äº†æ­¤ç±»æ¨¡å‹åœ¨æ›´å¹¿åŸŸå†…çš„é€‚ç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œé—®é¢˜å¾€å¾€å…·æœ‰å¤šæ ·çš„ç­”æ¡ˆè¡¨ç¤ºå½¢å¼ï¼Œä¸”æ•°æ®æ›´åŠ ç¨€ç¼ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†General-Reasonerï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨æé«˜LLMåœ¨å¤šæ ·åŒ–é¢†åŸŸæ¨ç†èƒ½åŠ›çš„æ–°å‹è®­ç»ƒèŒƒå¼ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é€šè¿‡ç½‘é¡µçˆ¬è™«æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é—®é¢˜æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¯éªŒè¯çš„ç­”æ¡ˆï¼Œæ¶µç›–å¹¿æ³›çš„å­¦ç§‘ï¼›ä»¥åŠï¼ˆ2ï¼‰å¼€å‘äº†ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œå®ƒç”¨åŸºäºæ€ç»´é“¾å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„èƒ½åŠ›å–ä»£äº†ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„éªŒè¯ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€ç³»åˆ—æ¨¡å‹ï¼Œå¹¶åœ¨æ¶µç›–ç‰©ç†ã€åŒ–å­¦ã€é‡‘èã€ç”µå­ç­‰å¹¿æ³›é¢†åŸŸçš„12ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆå¦‚MMLU-Proã€GPQAã€SuperGPQAã€TheoremQAã€BBEHå’ŒMATH AMCï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒGeneral-Reasoneråœ¨ä¿æŒæ•°å­¦æ¨ç†ä»»åŠ¡é«˜æ•ˆæ€§çš„åŒæ—¶ï¼Œå®ç°äº†ç¨³å¥ä¸”å¯æ³›åŒ–çš„æ¨ç†æ€§èƒ½ï¼Œå¹¶ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14652v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚Deepseek-R1-Zeroæå‡ºçš„â€œé›¶å¼ºåŒ–å­¦ä¹ â€å¯ç›´æ¥è®­ç»ƒåŸºç¡€è¯­è¨€æ¨¡å‹ï¼Œæ— éœ€ä¸­é—´ç›‘ç£å¾®è°ƒé˜¶æ®µã€‚å½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œä¸»è¦ç”±äºæ•°æ®ä¸°å¯Œå’Œç­”æ¡ˆéªŒè¯çš„ç®€ä¾¿æ€§ã€‚è¿™é™åˆ¶äº†æ¨¡å‹åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„é€‚ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºGeneral-Reasonerï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹è·¨ä¸åŒé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡é—®é¢˜æ•°æ®é›†å’ŒåŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ã€‚åœ¨ç‰©ç†ã€åŒ–å­¦ã€é‡‘èã€ç”µå­ç­‰é¢†åŸŸçš„å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°è¡¨æ˜ï¼ŒGeneral-Reasoneråœ¨ä¿æŒæ•°å­¦æ¨ç†ä»»åŠ¡é«˜æ•ˆæ€§çš„åŒæ—¶ï¼Œå±•ç°å‡ºç¨³å¥çš„è·¨é¢†åŸŸæ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Deepseek-R1-Zeroæå‡ºäº†â€œé›¶å¼ºåŒ–å­¦ä¹ â€æ–¹æ³•ï¼Œå¯ç›´æ¥è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚</li>
<li>å½“å‰LLMæ¨ç†ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œå­˜åœ¨é¢†åŸŸé€‚ç”¨æ€§çš„é™åˆ¶ã€‚</li>
<li>General-Reasoneræ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>General-Reasoneræ„å»ºäº†å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é—®é¢˜æ•°æ®é›†ã€‚</li>
<li>General-Reasoneré‡‡ç”¨åŸºäºç”Ÿæˆæ¨¡å‹çš„ç­”æ¡ˆéªŒè¯å™¨ï¼Œå…·å¤‡é“¾å¼æ€ç»´å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13f1a79a6b6dcc23b3f9a825e6f8478e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21cc0a7179f7bac17567dd372e69232a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa32ca03b448f35ebee2bd358bab9b61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49858842a88a5b99574dd1b67a920b32.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Think-Only-When-You-Need-with-Large-Hybrid-Reasoning-Models"><a href="#Think-Only-When-You-Need-with-Large-Hybrid-Reasoning-Models" class="headerlink" title="Think Only When You Need with Large Hybrid-Reasoning Models"></a>Think Only When You Need with Large Hybrid-Reasoning Models</h2><p><strong>Authors:Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei</strong></p>
<p>Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the modelâ€™s capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡èå…¥æœ€ç»ˆçš„å›ç­”ä¹‹å‰çš„æ‰©å±•æ€è€ƒè¿‡ç¨‹ï¼Œå±•ç°å‡ºç›¸è¾ƒäºä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®è´¨æå‡çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‡åº¦å†—é•¿çš„æ€è€ƒè¿‡ç¨‹åœ¨ç¬¦å·æ¶ˆè€—å’Œå»¶è¿Ÿæ–¹é¢å¼•å…¥äº†ç›¸å½“å¤§çš„å¼€é”€ï¼Œè¿™å¯¹äºç®€å•çš„æŸ¥è¯¢æ¥è¯´å°¤ä¸ºä¸å¿…è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å†³å®šæ˜¯å¦è¿›è¡Œæ€è€ƒçš„æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆæ˜¯ä½œä¸ºå†·å¯åŠ¨çš„æ··åˆå¾®è°ƒï¼ˆHFTï¼‰ï¼Œæ¥ç€æ˜¯é€šè¿‡æå‡ºçš„æ··åˆç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆHGPOï¼‰è¿›è¡Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä»¥éšå¼åœ°å­¦ä¹ é€‰æ‹©é€‚å½“çš„æ€è€ƒæ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåä¸ºæ··åˆå‡†ç¡®ç‡çš„æŒ‡æ ‡æ¥å®šé‡è¯„ä¼°æ¨¡å‹çš„æ··åˆæ€è€ƒèƒ½åŠ›ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLHRMså¯ä»¥è‡ªé€‚åº”åœ°å¯¹ä¸åŒéš¾åº¦å’Œç±»å‹çš„æŸ¥è¯¢è¿›è¡Œæ··åˆæ€è€ƒã€‚å®ƒåœ¨æ¨ç†å’Œä¸€èˆ¬èƒ½åŠ›æ–¹é¢ä¼˜äºç°æœ‰çš„LRMså’ŒLLMsï¼ŒåŒæ—¶å¤§å¤§æé«˜äº†æ•ˆç‡ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸»å¼ é‡æ–°è€ƒè™‘æ‰©å±•æ€è€ƒè¿‡ç¨‹çš„é€‚å½“ä½¿ç”¨ï¼Œå¹¶ä¸ºæ„å»ºæ··åˆæ€è€ƒç³»ç»Ÿæä¾›äº†ä¸€ä¸ªåšå®çš„èµ·ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14631v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸçš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡èå…¥æ‰©å±•çš„æ€è€ƒè¿‡ç¨‹ï¼Œå±•ç°å‡ºç›¸è¾ƒäºä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ˜¾è‘—æ”¹å–„çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‡åº¦å†—é•¿çš„æ€è€ƒå¸¦æ¥äº†å¤§é‡çš„ç¬¦å·æ¶ˆè€—å’Œå»¶è¿Ÿï¼Œè¿™å¯¹äºç®€å•çš„æŸ¥è¯¢æ¥è¯´å°¤ä¸ºä¸å¿…è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†å¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œæ€è€ƒçš„é¦–åˆ›æ¨¡å‹ã€‚é€šè¿‡æ··åˆå¾®è°ƒï¼ˆHFTï¼‰çš„å†·å¯åŠ¨å’Œæå‡ºçš„æ··åˆç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆHGPOï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬å®ç°äº†ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿéšå¼åœ°å­¦ä¹ é€‰æ‹©é€‚å½“çš„æ€è€ƒæ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ··åˆç²¾åº¦æŒ‡æ ‡æ¥å®šé‡è¯„ä¼°æ¨¡å‹çš„æ··åˆæ€è€ƒèƒ½åŠ›ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒLHRMså¯ä»¥åœ¨ä¸åŒç±»å‹çš„æŸ¥è¯¢ä¸Šè‡ªé€‚åº”åœ°è¿›è¡Œæ··åˆæ€è€ƒï¼Œåœ¨æ¨ç†å’Œç»¼åˆèƒ½åŠ›æ–¹é¢ä¼˜äºç°æœ‰çš„LRMså’ŒLLMsï¼ŒåŒæ—¶å¤§å¤§æé«˜äº†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰èå…¥æ‰©å±•æ€è€ƒè¿‡ç¨‹ä»¥æå‡æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿‡åº¦å†—é•¿çš„æ€è€ƒå¸¦æ¥ç¬¦å·æ¶ˆè€—å’Œå»¶è¿Ÿé—®é¢˜ï¼Œå°¤å…¶å¯¹äºç®€å•æŸ¥è¯¢ä¸å¿…è¦ã€‚</li>
<li>æå‡ºå¤§å‹æ··åˆæ¨ç†æ¨¡å‹ï¼ˆLHRMsï¼‰ï¼Œèƒ½æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è‡ªé€‚åº”æ€è€ƒã€‚</li>
<li>é€šè¿‡æ··åˆå¾®è°ƒï¼ˆHFTï¼‰çš„å†·å¯åŠ¨å’Œæ··åˆç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆHGPOï¼‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ å®ç°è‡ªé€‚åº”æ€è€ƒã€‚</li>
<li>å¼•å…¥æ··åˆç²¾åº¦æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ··åˆæ€è€ƒèƒ½åŠ›ã€‚</li>
<li>LHRMsåœ¨æ¨ç†å’Œç»¼åˆèƒ½åŠ›æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒæ—¶æé«˜æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a91a379cc090d05caebc5c82afc503e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-862a957a47f2ee33a00508036807f555.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-174a003a6a0d4479a481f386b88b6024.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Debating-for-Better-Reasoning-An-Unsupervised-Multimodal-Approach"><a href="#Debating-for-Better-Reasoning-An-Unsupervised-Multimodal-Approach" class="headerlink" title="Debating for Better Reasoning: An Unsupervised Multimodal Approach"></a>Debating for Better Reasoning: An Unsupervised Multimodal Approach</h2><p><strong>Authors:Ashutosh Adhikari, Mirella Lapata</strong></p>
<p>As Large Language Models (LLMs) gain expertise across diverse domains and modalities, scalable oversight becomes increasingly challenging, particularly when their capabilities may surpass human evaluators. Debate has emerged as a promising mechanism for enabling such oversight. In this work, we extend the debate paradigm to a multimodal setting, exploring its potential for weaker models to supervise and enhance the performance of stronger models. We focus on visual question answering (VQA), where two â€œsightedâ€ expert vision-language models debate an answer, while a â€œblindâ€ (text-only) judge adjudicates based solely on the quality of the arguments. In our framework, the experts defend only answers aligned with their beliefs, thereby obviating the need for explicit role-playing and concentrating the debate on instances of expert disagreement. Experiments on several multimodal tasks demonstrate that the debate framework consistently outperforms individual expert models. Moreover, judgments from weaker LLMs can help instill reasoning capabilities in vision-language models through finetuning. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸åŒé¢†åŸŸå’Œæ¨¡æ€ä¸­ç§¯ç´¯ä¸“ä¸šçŸ¥è¯†ï¼Œå¯æ‰©å±•çš„ç›‘ç£å˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å½“å®ƒä»¬çš„èƒ½åŠ›å¯èƒ½è¶…è¶Šäººç±»è¯„ä¼°è€…æ—¶ã€‚è¾©è®ºä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æœºåˆ¶ï¼Œå¯ä»¥å®ç°è¿™ç§ç›‘ç£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¾©è®ºèŒƒå¼æ‰©å±•åˆ°å¤šæ¨¡æ€ç¯å¢ƒï¼Œæ¢ç´¢å…¶æ½œåœ¨èƒ½åŠ›ï¼Œä½¿è¾ƒå¼±æ¨¡å‹èƒ½å¤Ÿç›‘ç£å¹¶å¢å¼ºè¾ƒå¼ºæ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬ä¸“æ³¨äºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼Œå…¶ä¸­ä¸¤ä¸ªâ€œæœ‰è§è¯†â€çš„ä¸“å®¶è§†è§‰è¯­è¨€æ¨¡å‹å¯¹ç­”æ¡ˆè¿›è¡Œè¾©è®ºï¼Œè€Œä¸€ä¸ªâ€œç›²ç›®â€ï¼ˆä»…æ–‡æœ¬ï¼‰çš„æ³•å®˜ä»…æ ¹æ®è®ºè¯çš„è´¨é‡åšå‡ºè£å†³ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œä¸“å®¶åªæå«ä¸å…¶ä¿¡å¿µç›¸ç¬¦çš„ç­”æ¡ˆï¼Œä»è€Œä¸éœ€è¦æ˜ç¡®çš„è§’è‰²æ‰®æ¼”ï¼Œå¹¶å°†è¾©è®ºé›†ä¸­åœ¨ä¸“å®¶åˆ†æ­§çš„å®ä¾‹ä¸Šã€‚åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¾©è®ºæ¡†æ¶å§‹ç»ˆä¼˜äºå•ä¸ªä¸“å®¶æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¾ƒå¼±çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­å¯ä»¥é€šè¿‡å¾®è°ƒå¸®åŠ©è§†è§‰è¯­è¨€æ¨¡å‹åŸ¹å…»æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14627v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šé¢†åŸŸå’Œå¤šæ¨¡æ€ä¸Šå±•ç°å“è¶Šèƒ½åŠ›ï¼Œç»™ç›‘ç£å¸¦æ¥æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å…¶èƒ½åŠ›å¯èƒ½è¶…è¶Šäººç±»è¯„ä¼°è€…ã€‚è¾©è®ºä½œä¸ºä¸€ç§ç›‘ç£æœºåˆ¶å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚æœ¬ç ”ç©¶å°†è¾©è®ºèŒƒå¼æ‰©å±•åˆ°å¤šæ¨¡æ€ç¯å¢ƒï¼Œæ¢ç´¢å¼±æ¨¡å‹ç›‘ç£å¼ºæ¨¡å‹æ€§èƒ½æå‡çš„å¯èƒ½æ€§ã€‚åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­ï¼Œä¸¤ä¸ªâ€œæœ‰è§†è§‰â€çš„ä¸“å®¶è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç­”æ¡ˆè¾©è®ºï¼Œä¸€ä¸ªâ€œç›²â€ï¼ˆä»…æ–‡æœ¬ï¼‰çš„è£åˆ¤æ ¹æ®è®ºè¯è´¨é‡è¿›è¡Œè£å†³ã€‚ä¸“å®¶çš„è¾©æŠ¤ä»…é’ˆå¯¹ä¸å…¶ä¿¡ä»°ç›¸ç¬¦çš„ç­”æ¡ˆï¼Œä»è€Œæ— éœ€æ˜¾å¼è§’è‰²æ‰®æ¼”ï¼Œé›†ä¸­è¾©è®ºåœ¨ä¸“å®¶åˆ†æ­§çš„å®ä¾‹ä¸Šã€‚åœ¨å¤šä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¾©è®ºæ¡†æ¶çš„è¡¨ç°æŒç»­ä¼˜äºå•ä¸ªä¸“å®¶æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ¥è‡ªè¾ƒå¼±çš„LLMçš„åˆ¤æ–­åŠ›å¯ä»¥é€šè¿‡å¾®è°ƒèµ‹äºˆè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šé¢†åŸŸå’Œå¤šæ¨¡æ€ä¸Šçš„èƒ½åŠ›å¸¦æ¥ç›‘ç£æŒ‘æˆ˜ã€‚</li>
<li>è¾©è®ºä½œä¸ºä¸€ç§ç›‘ç£æœºåˆ¶åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­ï¼Œä¸¤ä¸ªä¸“å®¶è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç­”æ¡ˆè¾©è®ºï¼Œå¼ºè°ƒå¯¹ç­”æ¡ˆçš„ä¸åŒçœ‹æ³•å’Œäº‰è®ºç‚¹ã€‚</li>
<li>ä¸€ä¸ªâ€œç›²â€è£åˆ¤æ ¹æ®è®ºè¯è´¨é‡è¿›è¡Œè£å†³ï¼Œç¡®ä¿è¾©è®ºçš„å…¬æ­£æ€§ã€‚</li>
<li>è¾©è®ºæ¡†æ¶çš„è¡¨ç°ä¼˜äºå•ä¸ªä¸“å®¶æ¨¡å‹ï¼Œå±•ç¤ºé›†ä½“æ™ºæ…§çš„ä¼˜åŠ¿ã€‚</li>
<li>è¾ƒå¼±çš„LLMçš„åˆ¤æ–­åŠ›å¯ä»¥é€šè¿‡è¾©è®ºèµ‹äºˆè§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚è¿™ç§æœºåˆ¶å¯¹äºæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå‡†ç¡®æ€§å…·æœ‰ç§¯ææ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6e15477a9d03323e28996b6760b0b59e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8124af251bffbf6d24ad78029cf71c59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-514e24f1b2e1cb22dfd8624dea2e206c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41838e885eecf591d9a4bf7f8727c02f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning"><a href="#TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning" class="headerlink" title="TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning"></a>TinyV: Reducing False Negatives in Verification Improves RL for LLM   Reasoning</h2><p><strong>Authors:Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran</strong></p>
<p>Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RLâ€™s success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problemâ€“false negativesâ€“where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV">https://github.com/uw-nsl/TinyV</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºé€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç­–ç•¥æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼ŒRLçš„æˆåŠŸä¾èµ–äºå¥–åŠ±çš„å¯é æ€§ï¼Œè¿™äº›å¥–åŠ±ç”±éªŒè¯å™¨æä¾›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºå¹¶åˆ†æäº†æ™®éå­˜åœ¨çš„é—®é¢˜â€”â€”å‡é˜´æ€§ï¼ˆFalse Negativeï¼‰â€”â€”å…¶ä¸­éªŒè¯å™¨é”™è¯¯åœ°æ‹’ç»äº†æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚æˆ‘ä»¬å¯¹Big-Math-RL-Verifiedæ•°æ®é›†çš„æ·±å…¥ç ”ç©¶æ˜¾ç¤ºï¼Œè¶…è¿‡3.8%çš„æ¨¡å‹ç”Ÿæˆå“åº”å­˜åœ¨å‡é˜´æ€§é—®é¢˜ï¼Œå³éªŒè¯å™¨æœªèƒ½è¯†åˆ«æ­£ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬å®è¯å’Œç†è®ºåœ°è¯æ˜ï¼Œè¿™äº›å‡é˜´æ€§ä¼šä¸¥é‡æŸå®³RLè®­ç»ƒï¼Œå› ä¸ºå®ƒä»¬å‰¥å¤ºäº†æ¨¡å‹çš„ä¿¡æ¯åŒ–æ¢¯åº¦ä¿¡å·å¹¶å‡ç¼“æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TinyVï¼Œä¸€ä¸ªåŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨ï¼Œå®ƒå¢å¼ºäº†ç°æœ‰çš„åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œèƒ½å¤ŸåŠ¨æ€è¯†åˆ«æ½œåœ¨çš„å‡é˜´æ€§å¹¶æ¢å¤æœ‰æ•ˆå“åº”ï¼Œä»¥äº§ç”Ÿæ›´å‡†ç¡®çš„å¥–åŠ±ä¼°è®¡ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼Œé›†æˆTinyVå°†é€šè¿‡ç‡æé«˜äº†é«˜è¾¾10%ï¼Œç›¸å¯¹äºåŸºçº¿åŠ é€Ÿäº†æ”¶æ•›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†è§£å†³éªŒè¯å™¨å‡é˜´æ€§çš„å…³é”®é‡è¦æ€§ï¼Œå¹¶ä¸ºæ”¹å–„åŸºäºRLçš„LLMå¾®è°ƒæä¾›äº†ä¸€ç§å®ç”¨æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/uw-nsl/TinyV%E3%80%82">https://github.com/uw-nsl/TinyVã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14625v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸»è¦é—®é¢˜æ˜¯éªŒè¯å™¨äº§ç”Ÿçš„å‡é˜´æ€§ç»“æœï¼Œå³éªŒè¯å™¨é”™è¯¯åœ°æ‹’ç»æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚ç ”ç©¶å‘ç°ï¼ŒBig-Math-RL-Verifiedæ•°æ®é›†ä¸­è¶…è¿‡38%çš„æ¨¡å‹ç”Ÿæˆå“åº”å—åˆ°å‡é˜´æ€§çš„å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨TinyVï¼Œå®ƒèƒ½å¤ŸåŠ¨æ€è¯†åˆ«æ½œåœ¨çš„å‡é˜´æ€§ï¼Œæ¢å¤æœ‰æ•ˆçš„å“åº”ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®çš„å¥–åŠ±ä¼°è®¡ã€‚é›†æˆTinyVåï¼Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„é€šè¿‡ç‡æé«˜äº†10%ï¼Œå¹¶ä¸”ç›¸å¯¹äºåŸºçº¿åŠ é€Ÿäº†æ”¶æ•›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºå¢å¼ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éªŒè¯å™¨äº§ç”Ÿçš„å‡é˜´æ€§é—®é¢˜æ˜¯RLé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>å‡é˜´æ€§ç»“æœä¼šä¸¥é‡å½±å“RLè®­ç»ƒï¼Œå‰¥å¤ºæ¨¡å‹çš„æ¢¯åº¦ä¿¡å·å¹¶å‡æ…¢æ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>Big-Math-RL-Verifiedæ•°æ®é›†ç ”ç©¶æ˜¾ç¤ºï¼Œè¶…è¿‡38%çš„æ¨¡å‹ç”Ÿæˆå“åº”å—åˆ°å‡é˜´æ€§çš„å½±å“ã€‚</li>
<li>TinyVæ˜¯ä¸€ç§åŸºäºLLMçš„è½»é‡çº§éªŒè¯å™¨ï¼Œæ—¨åœ¨è§£å†³å‡é˜´æ€§é—®é¢˜ã€‚</li>
<li>é›†æˆTinyVåï¼Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„é€šè¿‡ç‡æé«˜10%ï¼Œå¹¶ä¸”åŠ é€Ÿæ”¶æ•›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca76bd581877e2fa80edb606fc61e233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45315ad641603139bd5ff244f95ac9c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a08c30a5d241b44320c26435bfbb837.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b98fa5864234d9a3f6c406da5a995e97.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Linear-Control-of-Test-Awareness-Reveals-Differential-Compliance-in-Reasoning-Models"><a href="#Linear-Control-of-Test-Awareness-Reveals-Differential-Compliance-in-Reasoning-Models" class="headerlink" title="Linear Control of Test Awareness Reveals Differential Compliance in   Reasoning Models"></a>Linear Control of Test Awareness Reveals Differential Compliance in   Reasoning Models</h2><p><strong>Authors:Sahar Abdelnabi, Ahmed Salem</strong></p>
<p>Reasoning-focused large language models (LLMs) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such â€œtest awarenessâ€ impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning LLMs across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation. </p>
<blockquote>
<p>ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹åˆ°æ­£åœ¨è¿›è¡Œè¯„ä¼°æ—¶ï¼Œæœ‰æ—¶ä¼šæ”¹å˜å…¶è¡Œä¸ºï¼Œè¿™ç§å½±å“ç±»ä¼¼äºéœç´¢æ©ç°è±¡ï¼Œå¯èƒ½å¯¼è‡´å®ƒä»¬ä¼˜åŒ–é€šè¿‡æµ‹è¯•çš„æ€§èƒ½ï¼Œæˆ–è€…åœ¨ç°å®ä¸–ç•Œåæœä¼¼ä¹ä¸å­˜åœ¨çš„æƒ…å†µä¸‹æ›´å®¹æ˜“æ¥å—æœ‰å®³çš„æç¤ºã€‚æˆ‘ä»¬é¦–æ¬¡å¯¹â€œæµ‹è¯•æ„è¯†â€å¦‚ä½•å½±å“æ¨¡å‹è¡Œä¸ºè¿›è¡Œäº†å®šé‡ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯å…¶å®‰å…¨å¯¹é½æƒ…å†µã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç™½ç›’æ¢æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿï¼ˆiï¼‰çº¿æ€§è¯†åˆ«ä¸æ„è¯†ç›¸å…³çš„æ¿€æ´»ï¼Œï¼ˆiiï¼‰åœ¨ç›‘è§†ä¸‹æ¸¸æ€§èƒ½çš„åŒæ—¶ï¼Œä½¿æ¨¡å‹é è¿‘æˆ–è¿œç¦»æµ‹è¯•æ„è¯†ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•åº”ç”¨äºä¸åŒçš„æœ€æ–°å¼€æºæ¨ç†LLMï¼Œæ¶µç›–ç°å®å’Œå‡è®¾ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæµ‹è¯•æ„è¯†å¯¹å®‰å…¨å¯¹é½æœ‰æ˜¾è‘—å½±å“ï¼Œå¹¶ä¸”å¯¹ä¸åŒæ¨¡å‹çš„å½±å“ä¸åŒã€‚é€šè¿‡å¯¹æ­¤æ½œåœ¨æ•ˆåº”è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨å¢åŠ æˆ‘ä»¬å¯¹å¦‚ä½•è¿›è¡Œå®‰å…¨è¯„ä¼°çš„ä¿¡ä»»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14617v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹åˆ°è¯„ä¼°æ—¶ä¼šæ”¹å˜è¡Œä¸ºï¼Œç±»ä¼¼éœæ¡‘ç°è±¡ï¼Œä¼˜åŒ–æµ‹è¯•é€šè¿‡æ€§èƒ½æˆ–æ›´æ˜“äºéµå®ˆæœ‰å®³æç¤ºï¼Œç¼ºå°‘å®é™…åæœæ„ŸçŸ¥ã€‚ä¸€é¡¹ç ”ç©¶å®šé‡æ¢è®¨äº†æµ‹è¯•æ„è¯†å¦‚ä½•å½±å“æ¨¡å‹è¡Œä¸ºçš„å®‰å…¨æ€§ã€‚å¼•å…¥ç™½ç›’æ¢æµ‹æ¡†æ¶ï¼Œè¯†åˆ«æµ‹è¯•æ„è¯†ç›¸å…³æ¿€æ´»å¹¶æ§åˆ¶æ¨¡å‹æµ‹è¯•æ„è¯†ï¼ŒåŒæ—¶ç›‘æµ‹ä¸‹æ¸¸æ€§èƒ½ã€‚åº”ç”¨äºä¸åŒå…ˆè¿›æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“æœæ˜¾ç¤ºæµ‹è¯•æ„è¯†å¯¹å®‰å…¨å¯¹é½æœ‰é‡å¤§å½±å“ï¼Œä¸”ä¸åŒæ¨¡å‹é—´å­˜åœ¨å·®å¼‚ã€‚æ­¤ç ”ç©¶æ—¨åœ¨å¢åŠ å¯¹å®‰å…¨è¯„ä¼°çš„ä¿¡ä»»åº¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹è¯„ä¼°æ—¶ä¼šæ”¹å˜è¡Œä¸ºã€‚</li>
<li>æµ‹è¯•æ„è¯†ä¸éœæ¡‘ç°è±¡ç±»ä¼¼ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹ä¼˜åŒ–æµ‹è¯•é€šè¿‡æ€§èƒ½æˆ–æ›´æ˜“äºéµå®ˆæœ‰å®³æç¤ºã€‚</li>
<li>ç¼ºä¹å®é™…åæœæ„ŸçŸ¥ä¼šåŠ å‰§è¿™ä¸€å½±å“ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç™½ç›’æ¢æµ‹æ¡†æ¶å®šé‡ç ”ç©¶æµ‹è¯•æ„è¯†å¯¹æ¨¡å‹è¡Œä¸ºå®‰å…¨æ€§çš„å½±å“ã€‚</li>
<li>æµ‹è¯•æ„è¯†åœ¨ä¸åŒå…ˆè¿›æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å½±å“å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>æä¾›å¯¹æµ‹è¯•æ„è¯†çš„ç²¾ç»†æ§åˆ¶æœ‰åŠ©äºå¢åŠ å¯¹å®‰å…¨è¯„ä¼°çš„ä¿¡ä»»åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8f6a0bf3937da1484ccd19b4b2819a2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa3e0e96d0c1d8ec60c5e3c686bcbf9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872d9b59887a977155475c5fcd8d4259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-105f5bbf548ea39480233b3273ad0524.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Context-Reasoner-Incentivizing-Reasoning-Capability-for-Contextualized-Privacy-and-Safety-Compliance-via-Reinforcement-Learning"><a href="#Context-Reasoner-Incentivizing-Reasoning-Capability-for-Contextualized-Privacy-and-Safety-Compliance-via-Reinforcement-Learning" class="headerlink" title="Context Reasoner: Incentivizing Reasoning Capability for Contextualized   Privacy and Safety Compliance via Reinforcement Learning"></a>Context Reasoner: Incentivizing Reasoning Capability for Contextualized   Privacy and Safety Compliance via Reinforcement Learning</h2><p><strong>Authors:Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song</strong></p>
<p>While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +17.64% accuracy improvement in safety&#x2F;privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥äº†é‡å¤§çš„å®‰å…¨å’Œéšç§é£é™©ã€‚å½“å‰çš„ç¼“è§£ç­–ç•¥å¾€å¾€æ— æ³•ä¿ç•™å±é™©åœºæ™¯ä¸­çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œåè€Œè¿‡åˆ†ä¾èµ–äºæ•æ„Ÿæ¨¡å¼åŒ¹é…æ¥ä¿æŠ¤LLMï¼Œè¿™é™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¿½ç•¥äº†æ—¢å®šçš„å®‰å…¨å’Œéšç§æ ‡å‡†ï¼Œå¯¼è‡´æ³•å¾‹åˆè§„çš„ç³»ç»Ÿæ€§é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å°†å®‰å…¨å’Œéšç§é—®é¢˜åˆ¶å®šä¸ºç¬¦åˆæƒ…å¢ƒå®Œæ•´æ€§çš„åˆè§„æ€§é—®é¢˜ã€‚éµå¾ªæƒ…å¢ƒå®Œæ•´æ€§ï¼ˆCIï¼‰ç†è®ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä¸‰ä¸ªå…³é”®çš„ç›‘ç®¡æ ‡å‡†ä¿æŒä¸€è‡´ï¼šGDPRã€æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆå’ŒHIPAAã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥æ¿€åŠ±ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜éµå®ˆå®‰å…¨å’Œéšç§è§„èŒƒçš„èƒ½åŠ›ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜äº†æ³•å¾‹åˆè§„æ€§ï¼ˆåœ¨å®‰å…¨&#x2F;éšç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†+17.64%çš„å‡†ç¡®ç‡æå‡ï¼‰ï¼Œè€Œä¸”è¿˜è¿›ä¸€æ­¥æé«˜äº†é€šç”¨æ¨ç†èƒ½åŠ›ã€‚å¯¹äºåœ¨å¤šç§ä¸»é¢˜ä¸Šæ˜¾è‘—ä¼˜äºå…¶åŸºç¡€æ¨¡å‹çš„å¼ºå¤§æ¨ç†æ¨¡å‹OpenThinker-7Bï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å…¶åœ¨MMLUå’ŒLegalBenchåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ï¼Œåˆ†åˆ«æé«˜äº†+2.05%å’Œ+8.98%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14585v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®‰å…¨ä¸éšç§æ–¹é¢å­˜åœ¨æ˜¾è‘—é£é™©ï¼Œç°æœ‰ç¼“è§£ç­–ç•¥å¾€å¾€æ— æ³•å…¼é¡¾è¯­å¢ƒæ¨ç†èƒ½åŠ›ï¼Œå¹¶è¿‡åˆ†ä¾èµ–æ•æ„Ÿæ¨¡å¼åŒ¹é…æ¥ä¿æŠ¤LLMsï¼Œè¿™é™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æ ¹æ®è¯­å¢ƒå®Œæ•´æ€§ï¼ˆCIï¼‰ç†è®ºï¼Œå°†å®‰å…¨ä¸éšç§é—®é¢˜è½¬åŒ–ä¸ºè¯­å¢ƒåŒ–åˆè§„é—®é¢˜ï¼Œå¹¶ä¸ä¸‰å¤§å…³é”®ç›‘ç®¡æ ‡å‡†ï¼ˆGDPRã€æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆå’ŒHIPAAï¼‰å¯¹é½ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼Œåœ¨æå‡è¯­å¢ƒæ¨ç†èƒ½åŠ›çš„åŒæ—¶å¢å¼ºå¯¹å®‰å…¨ä¸éšç§è§„èŒƒçš„éµå®ˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜æ³•å¾‹åˆè§„æ€§ï¼Œè¿˜è¿›ä¸€æ­¥æå‡äº†é€šç”¨æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨å®‰å…¨ä¸éšç§é£é™©ã€‚</li>
<li>ç°æœ‰ç¼“è§£ç­–ç•¥éš¾ä»¥å…¼é¡¾è¯­å¢ƒæ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¾èµ–æ•æ„Ÿæ¨¡å¼åŒ¹é…ï¼Œé™åˆ¶äº†åº”ç”¨èŒƒå›´ã€‚</li>
<li>å¼•å…¥è¯­å¢ƒå®Œæ•´æ€§ï¼ˆCIï¼‰ç†è®ºæ¥è§£å†³å®‰å…¨ä¸éšç§é—®é¢˜ï¼Œå°†å…¶è½¬åŒ–ä¸ºè¯­å¢ƒåŒ–åˆè§„é—®é¢˜ã€‚</li>
<li>ä¸ä¸‰å¤§å…³é”®ç›‘ç®¡æ ‡å‡†ï¼ˆGDPRã€æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆå’ŒHIPAAï¼‰å¯¹é½ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æå‡è¯­å¢ƒæ¨ç†èƒ½åŠ›å’Œå¯¹å®‰å…¨ä¸éšç§è§„èŒƒçš„éµå®ˆã€‚</li>
<li>æ–¹æ³•æ˜¾è‘—æé«˜æ³•å¾‹åˆè§„æ€§ï¼Œåœ¨å®‰å…¨å’Œéšç§åŸºå‡†æµ‹è¯•ä¸Šå®ç°+17.64%çš„å‡†ç¡®åº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4db7d96cf7362e201aadebc4ea69aaf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-353b8163a4c131bd156ad9a92bc0f459.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cc3442a46b8dfafb225bfa8424ac7fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-335803ebc1268674293e56a126aa7c7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0c72ab2a84733a72d5d2ce64924715e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea5998feb82fe7d8cc676123848dc041.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="R2MED-A-Benchmark-for-Reasoning-Driven-Medical-Retrieval"><a href="#R2MED-A-Benchmark-for-Reasoning-Driven-Medical-Retrieval" class="headerlink" title="R2MED: A Benchmark for Reasoning-Driven Medical Retrieval"></a>R2MED: A Benchmark for Reasoning-Driven Medical Retrieval</h2><p><strong>Authors:Lei Li, Xiao Zhou, Zheng Liu</strong></p>
<p>Current medical retrieval benchmarks primarily emphasize lexical or shallow semantic similarity, overlooking the reasoning-intensive demands that are central to clinical decision-making. In practice, physicians often retrieve authoritative medical evidence to support diagnostic hypotheses. Such evidence typically aligns with an inferred diagnosis rather than the surface form of a patientâ€™s symptoms, leading to low lexical or semantic overlap between queries and relevant documents. To address this gap, we introduce R2MED, the first benchmark explicitly designed for reasoning-driven medical retrieval. It comprises 876 queries spanning three tasks: Q&amp;A reference retrieval, clinical evidence retrieval, and clinical case retrieval. These tasks are drawn from five representative medical scenarios and twelve body systems, capturing the complexity and diversity of real-world medical information needs. We evaluate 15 widely-used retrieval systems on R2MED and find that even the best model achieves only 31.4 nDCG@10, demonstrating the benchmarkâ€™s difficulty. Classical re-ranking and generation-augmented retrieval methods offer only modest improvements. Although large reasoning models improve performance via intermediate inference generation, the best results still peak at 41.4 nDCG@10. These findings underscore a substantial gap between current retrieval techniques and the reasoning demands of real clinical tasks. We release R2MED as a challenging benchmark to foster the development of next-generation medical retrieval systems with enhanced reasoning capabilities. Data and code are available at <a target="_blank" rel="noopener" href="https://github.com/R2MED/R2MED">https://github.com/R2MED/R2MED</a> </p>
<blockquote>
<p>å½“å‰åŒ»å­¦æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸»è¦å¼ºè°ƒè¯æ±‡æˆ–æµ…å±‚è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå¿½è§†äº†ä¸´åºŠå†³ç­–æ ¸å¿ƒä¸­çš„æ¨ç†éœ€æ±‚ã€‚åœ¨å®è·µä¸­ï¼ŒåŒ»ç”Ÿé€šå¸¸ä¼šæ£€ç´¢æƒå¨åŒ»å­¦è¯æ®æ¥æ”¯æŒè¯Šæ–­å‡è®¾ã€‚è¿™æ ·çš„è¯æ®é€šå¸¸ä¸æ¨æ–­å‡ºçš„è¯Šæ–­ç»“æœç›¸ç¬¦ï¼Œè€Œéæ‚£è€…ç—‡çŠ¶çš„è¡¨å±‚å½¢å¼ï¼Œè¿™å¯¼è‡´æŸ¥è¯¢ä¸ç›¸å…³æ–‡æ¡£ä¹‹é—´è¯æ±‡æˆ–è¯­ä¹‰ä¸Šçš„é‡å è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†R2MEDï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºæ¨ç†é©±åŠ¨åŒ»å­¦æ£€ç´¢è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«876ä¸ªæŸ¥è¯¢ï¼Œæ¶µç›–ä¸‰ä¸ªä»»åŠ¡ï¼šé—®ç­”å‚è€ƒæ£€ç´¢ã€ä¸´åºŠè¯æ®æ£€ç´¢å’Œä¸´åºŠæ¡ˆä¾‹æ£€ç´¢ã€‚è¿™äº›ä»»åŠ¡æ¥è‡ªäº”ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„åŒ»å­¦åœºæ™¯å’ŒåäºŒä¸ªèº«ä½“ç³»ç»Ÿï¼Œæ•æ‰äº†ç°å®ä¸–ç•ŒåŒ»å­¦ä¿¡æ¯éœ€æ±‚çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬åœ¨R2MEDä¸Šè¯„ä¼°äº†15ç§å¹¿æ³›ä½¿ç”¨çš„æ£€ç´¢ç³»ç»Ÿï¼Œå‘ç°å³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹ä¹Ÿä»…å®ç°äº†31.4çš„nDCG@10å¾—åˆ†ï¼Œè¿™æ˜¾ç¤ºäº†åŸºå‡†æµ‹è¯•çš„å›°éš¾ç¨‹åº¦ã€‚ä¼ ç»Ÿçš„é‡æ–°æ’åºå’Œç”Ÿæˆå¢å¼ºæ£€ç´¢æ–¹æ³•åªå¸¦æ¥äº†é€‚åº¦çš„æ”¹è¿›ã€‚å°½ç®¡å¤§å‹æ¨ç†æ¨¡å‹é€šè¿‡ä¸­é—´æ¨ç†ç”Ÿæˆæé«˜äº†æ€§èƒ½ï¼Œä½†æœ€ä½³ç»“æœä»ç„¶åªè¾¾åˆ°41.4çš„nDCG@10ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å½“å‰æ£€ç´¢æŠ€æœ¯ä¸å®é™…ä¸´åºŠä»»åŠ¡æ¨ç†éœ€æ±‚ä¹‹é—´çš„å·¨å¤§å·®è·ã€‚æˆ‘ä»¬å‘å¸ƒR2MEDä½œä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥æ¨åŠ¨å…·å¤‡å¢å¼ºæ¨ç†èƒ½åŠ›çš„æ–°ä¸€ä»£åŒ»å­¦æ£€ç´¢ç³»ç»Ÿçš„å‘å±•ã€‚æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/R2MED/R2MED">https://github.com/R2MED/R2MED</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14558v1">PDF</a> 38 pages, 16 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>å½“å‰åŒ»å­¦æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºè¯æ±‡æˆ–æµ…å±‚è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå¿½è§†äº†ä¸´åºŠå†³ç­–ä¸­çš„æ¨ç†éœ€æ±‚ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬æ¨å‡ºR2MEDåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºæ¨ç†é©±åŠ¨å‹åŒ»å­¦æ£€ç´¢ã€‚R2MEDåŒ…æ‹¬876ä¸ªæŸ¥è¯¢ï¼Œæ¶µç›–é—®ç­”å‚è€ƒæ£€ç´¢ã€ä¸´åºŠè¯æ®æ£€ç´¢å’Œä¸´åºŠæ¡ˆä¾‹æ£€ç´¢ä¸‰é¡¹ä»»åŠ¡ï¼Œåæ˜ çœŸå®ä¸–ç•Œçš„åŒ»å­¦ä¿¡æ¯éœ€æ±‚å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚å¯¹ç°æœ‰å¹¿æ³›ä½¿ç”¨çš„15ç§æ£€ç´¢ç³»ç»Ÿåœ¨R2MEDä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€ä¼˜ç§€çš„æ¨¡å‹ä¹Ÿä»…è¾¾åˆ°31.4çš„nDCG@10å¾—åˆ†ï¼Œæ˜¾ç¤ºè¯¥åŸºå‡†æµ‹è¯•çš„å›°éš¾æ€§ã€‚å°½ç®¡ç»å…¸çš„é‡æ’åºå’Œç”Ÿæˆå¢å¼ºæ£€ç´¢æ–¹æ³•ç•¥æœ‰æ”¹è¿›ï¼Œä½†æœ€ä½³ç»“æœä»ä»…é™äº41.4çš„nDCG@10ã€‚è¿™è¡¨æ˜å½“å‰æ£€ç´¢æŠ€æœ¯ä¸ä¸´åºŠä»»åŠ¡ä¸­çš„æ¨ç†éœ€æ±‚ä¹‹é—´å­˜åœ¨å·¨å¤§å·®è·ã€‚æˆ‘ä»¬å‘å¸ƒR2MEDä½œä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥ä¿ƒè¿›å…·æœ‰å¢å¼ºæ¨ç†èƒ½åŠ›çš„æ–°ä¸€ä»£åŒ»å­¦æ£€ç´¢ç³»ç»Ÿçš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰åŒ»å­¦æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºè¯æ±‡æˆ–æµ…å±‚è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥äº†ä¸´åºŠå†³ç­–ä¸­çš„æ¨ç†éœ€æ±‚ã€‚</li>
<li>R2MEDåŸºå‡†æµ‹è¯•æ˜¯ä¸“é—¨ä¸ºæ¨ç†é©±åŠ¨å‹åŒ»å­¦æ£€ç´¢è®¾è®¡çš„ï¼Œæ¶µç›–å¤šç§ä»»åŠ¡ï¼Œåæ˜ çœŸå®åŒ»å­¦ä¿¡æ¯éœ€æ±‚çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>å¯¹å¹¿æ³›ä½¿ç”¨çš„æ£€ç´¢ç³»ç»Ÿåœ¨R2MEDä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰æŠ€æœ¯é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œæœ€ä½³æ¨¡å‹è¡¨ç°ä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚</li>
<li>ç»å…¸çš„é‡æ’åºå’Œç”Ÿæˆå¢å¼ºæ£€ç´¢æ–¹æ³•ä»…å¸¦æ¥æœ‰é™æ”¹è¿›ã€‚</li>
<li>å¤§å‹æ¨ç†æ¨¡å‹é€šè¿‡ä¸­é—´æ¨ç†ç”Ÿæˆèƒ½æé«˜æ€§èƒ½ï¼Œä½†ä»æœªè¾¾åˆ°ç†æƒ³æ°´å¹³ã€‚</li>
<li>å½“å‰æ£€ç´¢æŠ€æœ¯ä¸ä¸´åºŠä»»åŠ¡ä¸­çš„æ¨ç†éœ€æ±‚ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ac5fb59c7ee4d5657edf565355d53af0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-362f8475c670891497991400920ecf61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2793d379c288014a97f48863bee5d7d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56795194fed7482e0be2723967e574f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80e223a19dd72852460e03344046b7f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Not-All-Correct-Answers-Are-Equal-Why-Your-Distillation-Source-Matters"><a href="#Not-All-Correct-Answers-Are-Equal-Why-Your-Distillation-Source-Matters" class="headerlink" title="Not All Correct Answers Are Equal: Why Your Distillation Source Matters"></a>Not All Correct Answers Are Equal: Why Your Distillation Source Matters</h2><p><strong>Authors:Xiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, Xiangang Li</strong></p>
<p>Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The AM-based model consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging Face\footnote{Datasets are available on Hugging Face: \href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled%7D%7BAM-Thinking-v1-Distilled%7D">https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled}</a>, \href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled%7D%7BAM-Qwen3-Distilled%7D.%7D">https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}</a>. </p>
<blockquote>
<p>è’¸é¦æ³•å·²æˆä¸ºæé«˜å¼€æºè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ä¸€ç§å®ç”¨æœ‰æ•ˆçš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹æ¨ç†æ•°æ®è’¸é¦è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œé€šè¿‡æ”¶é›†æ¥è‡ªAM-Thinking-v1ã€Qwen3-235B-A22Bå’ŒDeepSeek-R1ä¸‰ä¸ªå…ˆè¿›æ•™å¸ˆæ¨¡å‹çš„éªŒè¯è¾“å‡ºï¼Œåœ¨åŒ…å«189ä¸‡ä¸ªæŸ¥è¯¢çš„å…±äº«è¯­æ–™åº“ä¸Šè¿›è¡Œå¯¹æ¯”åˆ†æã€‚æˆ‘ä»¬æ„å»ºäº†ä¸‰ä¸ªå¹¶è¡Œæ•°æ®é›†å¹¶å¯¹å…¶åˆ†å¸ƒè¿›è¡Œäº†åˆ†æï¼Œå‘ç°ç”±AM-Thinking-v1è’¸é¦çš„æ•°æ®å…·æœ‰æ›´å¤§çš„ä»¤ç‰Œé•¿åº¦å¤šæ ·æ€§å’Œæ›´ä½çš„å›°æƒ‘åº¦ã€‚åœ¨AIME2024ã€AIME2025ã€MATH500å’ŒLiveCodeBenchç­‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼Œå¯¹å„ä¸ªæ•°æ®é›†è®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚åŸºäºAMçš„æ¨¡å‹è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼ˆä¾‹å¦‚ï¼Œåœ¨AIME2024ä¸Šå¾—åˆ†ä¸º84.3ï¼Œåœ¨AIME2025ä¸Šå¾—åˆ†ä¸º72.2ï¼Œåœ¨MATH500ä¸Šå¾—åˆ†ä¸º98.4ï¼Œåœ¨LiveCodeBenchä¸Šå¾—åˆ†ä¸º65.9ï¼‰ï¼Œå¹¶å±•ç°å‡ºè‡ªé€‚åº”è¾“å‡ºè¡Œä¸ºâ€”â€”ä¸ºæ›´å›°éš¾çš„ä»»åŠ¡ç”Ÿæˆæ›´é•¿çš„å“åº”ï¼Œä¸ºæ›´ç®€å•çš„ä»»åŠ¡ç”Ÿæˆæ›´çŸ­çš„å“åº”ã€‚è¿™äº›å‘ç°çªæ˜¾äº†é«˜è´¨é‡éªŒè¯æ¨ç†è½¨è¿¹çš„ä»·å€¼ã€‚æˆ‘ä»¬å‘å¸ƒäº†ç”±AM-Thinking-v1å’ŒQwen3-235B-A22Bè’¸é¦çš„æ•°æ®é›†ï¼Œä»¥æ”¯æŒæœªæ¥å¯¹å¼€æ”¾å’Œé«˜æ€§èƒ½æ¨ç†å¯¼å‘çš„è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ã€‚æ•°æ®é›†å·²åœ¨Hugging Faceä¸Šå…¬å¼€æä¾›ï¼ˆæ•°æ®é›†å¯åœ¨Hugging Faceä¸Šæ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled%EF%BC%89">https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilledï¼‰</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14464v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†è’¸é¦æ³•åœ¨æé«˜å¼€æºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢çš„å®é™…åº”ç”¨å’Œæ•ˆæœã€‚é€šè¿‡å¯¹ä¸‰ç§é¡¶çº§æ•™å¸ˆæ¨¡å‹ï¼ˆAM-Thinking-v1ã€Qwen3-235B-A22Bå’ŒDeepSeek-R1ï¼‰è¿›è¡Œå¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œæ„å»ºäº†ä¸‰ç»„å¹¶è¡Œæ•°æ®é›†ï¼Œå¹¶åˆ†æäº†å…¶åˆ†å¸ƒç‰¹ç‚¹ã€‚å‘ç°AM-Thinking-v1è’¸é¦æ•°æ®å…·æœ‰æ›´å¤§çš„ä»¤ç‰Œé•¿åº¦å¤šæ ·æ€§å’Œæ›´ä½çš„å›°æƒ‘åº¦ã€‚å­¦ç”Ÿæ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾ç¤ºï¼ŒåŸºäºAMçš„æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºé€‚åº”æ€§è¾“å‡ºè¡Œä¸ºï¼Œé’ˆå¯¹éš¾åº¦æ›´é«˜çš„ä»»åŠ¡äº§ç”Ÿæ›´é•¿çš„å“åº”ï¼Œé’ˆå¯¹ç®€å•çš„ä»»åŠ¡åˆ™äº§ç”Ÿè¾ƒçŸ­çš„å“åº”ã€‚æœ€åï¼Œå…¬å¼€å‘å¸ƒäº†ä¸¤ä¸ªè’¸é¦æ•°æ®é›†ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è’¸é¦æ³•æ˜¯ä¸€ç§æé«˜å¼€æºè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å®ç”¨æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å¯¹ä¸‰ç§é¡¶çº§æ•™å¸ˆæ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œæ„å»ºå¹¶è¡Œæ•°æ®é›†ã€‚</li>
<li>AM-Thinking-v1è’¸é¦æ•°æ®å…·æœ‰æ›´é«˜çš„ä»¤ç‰Œé•¿åº¦å¤šæ ·æ€§å’Œæ›´ä½çš„å›°æƒ‘åº¦ã€‚</li>
<li>åŸºäºAMçš„æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>AMæ¨¡å‹èƒ½é€‚åº”æ€§è¾“å‡ºä¸åŒé•¿åº¦çš„å“åº”ï¼Œé€‚åº”ä»»åŠ¡éš¾åº¦ã€‚</li>
<li>å…¬å¼€å‘å¸ƒäº†ä¸¤ä¸ªè’¸é¦æ•°æ®é›†ï¼Œæ”¯æŒæœªæ¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f38dbc2147651acc66156e5ff3f6232c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c61c772c5fd765f8c1f3570e98718fc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f80a7a6f2e7304db5df3d120e383e90d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de299db46c9745849befdbf8729a5fda.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VisualQuality-R1-Reasoning-Induced-Image-Quality-Assessment-via-Reinforcement-Learning-to-Rank"><a href="#VisualQuality-R1-Reasoning-Induced-Image-Quality-Assessment-via-Reinforcement-Learning-to-Rank" class="headerlink" title="VisualQuality-R1: Reasoning-Induced Image Quality Assessment via   Reinforcement Learning to Rank"></a>VisualQuality-R1: Reasoning-Induced Image Quality Assessment via   Reinforcement Learning to Rank</h2><p><strong>Authors:Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, Kede Ma</strong></p>
<p>DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computational modeling has not been thoroughly explored in the context of image quality assessment (IQA), a task critically dependent on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are then used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation. </p>
<blockquote>
<p>DeepSeek-R1é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨æ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œåœ¨å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰çš„æƒ…å¢ƒä¸­ï¼Œæ¨ç†è¯±å¯¼è®¡ç®—å»ºæ¨¡çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†çš„æ¢ç´¢ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¥é‡ä¾èµ–äºè§†è§‰æ¨ç†çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VisualQuality-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±æ¨ç†è¯±å¯¼çš„æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆNR-IQAï¼‰æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ’åå¯¹å…¶è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ç§é€‚åº”è§†è§‰è´¨é‡å†…åœ¨ç›¸å¯¹æ€§çš„å­¦ä¹ ç®—æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºä¸€å¯¹å›¾åƒï¼Œæˆ‘ä»¬é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ä¸ºæ¯å¼ å›¾åƒç”Ÿæˆå¤šä¸ªè´¨é‡åˆ†æ•°ã€‚ç„¶åï¼Œè¿™äº›ä¼°è®¡è¢«ç”¨æ¥åœ¨Thurstoneæ¨¡å‹ä¸‹è®¡ç®—ä¸€å¼ å›¾åƒæ¯”å¦ä¸€å¼ å›¾åƒè´¨é‡æ›´é«˜çš„æ¯”è¾ƒæ¦‚ç‡ã€‚æ¯ä¸ªè´¨é‡ä¼°è®¡çš„å¥–åŠ±æ˜¯ä½¿ç”¨è¿ç»­çš„ä¿çœŸåº¦åº¦é‡æ¥å®šä¹‰çš„ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ç¦»æ•£åŒ–çš„äºŒè¿›åˆ¶æ ‡ç­¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„VisualQuality-R1æŒç»­ä¼˜äºåŸºäºåˆ¤åˆ«æ·±åº¦å­¦ä¹ çš„NR-IQAæ¨¡å‹ä»¥åŠæœ€æ–°çš„æ¨ç†è¯±å¯¼è´¨é‡å›å½’æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒVisualQuality-R1èƒ½å¤Ÿç”Ÿæˆä¸Šä¸‹æ–‡ä¸°å¯Œã€ä¸äººç±»å¯¹é½çš„è´¨é‡æè¿°ï¼Œå¹¶æ”¯æŒå¤šæ•°æ®é›†è®­ç»ƒï¼Œæ— éœ€æ„ŸçŸ¥å°ºåº¦è°ƒæ•´ã€‚è¿™äº›ç‰¹ç‚¹ä½¿å¾—VisualQuality-R1å°¤å…¶é€‚åˆåœ¨è¶…åˆ†è¾¨ç‡å’Œå›¾åƒç”Ÿæˆç­‰å¹¿æ³›çš„å›¾åƒå¤„ç†ä»»åŠ¡ä¸­å¯é åœ°è¡¡é‡è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14460v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æ–‡æœ¬ä¸­ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ŒDeepSeek-R1å±•ç°äº†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­æ¿€åŠ±æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—æˆæ•ˆã€‚ä½†åŸºäºè§†è§‰æ¨ç†çš„å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ–¹é¢çš„æ½œåŠ›å°šæœªè¢«å……åˆ†å‘æ˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†VisualQuality-R1è¿™ä¸€æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆNR-IQAï¼‰æ¨¡å‹ï¼Œå¹¶ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ’åè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æŒç»­ä¼˜äºåŸºäºæ·±åº¦å­¦ä¹ çš„åˆ¤åˆ«å¼NR-IQAæ¨¡å‹ä»¥åŠä¸€ç§æœ€æ–°çš„æ¨ç†æ„Ÿåº”è´¨é‡å›å½’æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒVisualQuality-R1å¯ç”Ÿæˆä¸°å¯Œçš„ä¸Šä¸‹æ–‡è´¨é‡æè¿°ï¼Œå¹¶æ”¯æŒè·¨æ•°æ®é›†è®­ç»ƒè€Œæ— éœ€æ„ŸçŸ¥å°ºåº¦è°ƒæ•´å¯¹é½ï¼Œå°¤å…¶é€‚ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡å’Œå›¾åƒç”Ÿæˆç­‰ä»»åŠ¡çš„è´¨é‡å¯é è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1åœ¨æ¿€åŠ±å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æˆæ•ˆã€‚</li>
<li>VisualQuality-R1æ˜¯ä¸€ä¸ªåŸºäºè§†è§‰æ¨ç†çš„æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°æ¨¡å‹ã€‚</li>
<li>VisualQuality-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ’åè®­ç»ƒï¼Œå¹¶åŸºäºThurstoneæ¨¡å‹ç”Ÿæˆç›¸å¯¹è´¨é‡æ¦‚ç‡ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨è¿ç»­å¿ å®åº¦åº¦é‡ä½œä¸ºå¥–åŠ±å‡½æ•°ï¼Œè€Œéç¦»æ•£äºŒå…ƒæ ‡ç­¾ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºVisualQuality-R1æ€§èƒ½ä¼˜äºå…¶ä»–æ·±åº¦å­¦ä¹ å’Œæ¨ç†æ„Ÿåº”è´¨é‡å›å½’æ¨¡å‹ã€‚</li>
<li>VisualQuality-R1èƒ½ç”Ÿæˆä¸°å¯Œçš„ä¸Šä¸‹æ–‡è´¨é‡æè¿°ï¼Œæ”¯æŒå¤šæ•°æ®é›†è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4122138101e3a00c96d76299d068f032.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45a96f3ce9d0967b3b0a8c52538378bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7effbfc9d874683db7e860b54f7ac129.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Rank-K-Test-Time-Reasoning-for-Listwise-Reranking"><a href="#Rank-K-Test-Time-Reasoning-for-Listwise-Reranking" class="headerlink" title="Rank-K: Test-Time Reasoning for Listwise Reranking"></a>Rank-K: Test-Time Reasoning for Listwise Reranking</h2><p><strong>Authors:Eugene Yang, Andrew Yates, Kathryn Ricci, Orion Weller, Vivek Chari, Benjamin Van Durme, Dawn Lawrie</strong></p>
<p>Retrieve-and-rerank is a popular retrieval pipeline because of its ability to make slow but effective rerankers efficient enough at query time by reducing the number of comparisons. Recent works in neural rerankers take advantage of large language models for their capability in reasoning between queries and passages and have achieved state-of-the-art retrieval effectiveness. However, such rerankers are resource-intensive, even after heavy optimization. In this work, we introduce Rank-K, a listwise passage reranking model that leverages the reasoning capability of the reasoning language model at query time that provides test time scalability to serve hard queries. We show that Rank-K improves retrieval effectiveness by 23% over the RankZephyr, the state-of-the-art listwise reranker, when reranking a BM25 initial ranked list and 19% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is inherently a multilingual model, we found that it ranks passages based on queries in different languages as effectively as it does in monolingual retrieval. </p>
<blockquote>
<p>æ£€ç´¢å’Œé‡æ–°æ’åºï¼ˆRetrieve-and-rerankï¼‰æ˜¯ä¸€ç§æµè¡Œçš„æ£€ç´¢æµç¨‹ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å‡å°‘æ¯”è¾ƒæ¬¡æ•°ï¼Œä½¿ç¼“æ…¢ä½†æœ‰æ•ˆçš„é‡æ–°æ’åºå™¨åœ¨æŸ¥è¯¢æ—¶é—´å˜å¾—é«˜æ•ˆã€‚è¿‘æœŸç¥ç»é‡æ–°æ’åºå™¨çš„å·¥ä½œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒæŸ¥è¯¢å’Œæ®µè½ä¹‹é—´çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ£€ç´¢æ•ˆæœã€‚ç„¶è€Œï¼Œå³ä½¿ç»è¿‡é‡åº¦ä¼˜åŒ–ï¼Œè¿™ç§é‡æ–°æ’åºå™¨çš„èµ„æºæ¶ˆè€—ä»ç„¶å¾ˆå¤§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Rank-Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæŸ¥è¯¢æ—¶é—´çš„æ¨ç†è¯­è¨€æ¨¡å‹çš„æ®µè½é‡æ–°æ’åºæ¨¡å‹ï¼Œå®ƒä¸ºç¡¬æŸ¥è¯¢æä¾›äº†æµ‹è¯•æ—¶çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†Rank-Kåœ¨BM25åˆå§‹æ’ååˆ—è¡¨ä¸Šæ¯”æœ€æ–°åˆ—è¡¨çº§é‡æ–°æ’åºå™¨RankZephyré«˜å‡º23%çš„æ£€ç´¢æ•ˆæœï¼Œä»¥åŠåœ¨SPLADE-v3çš„å¼ºå¤§æ£€ç´¢ç»“æœä¸Šé«˜å‡º19%ã€‚ç”±äºRank-Kæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°å®ƒæ ¹æ®ä¸åŒè¯­è¨€çš„æŸ¥è¯¢è¿›è¡Œæ®µè½æ’åï¼Œå°±åƒå®ƒåœ¨å•è¯­æ£€ç´¢ä¸­ä¸€æ ·æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14432v1">PDF</a> 15 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºRetrieve-and-rerankç®¡é“çš„ä¼˜åŠ¿ï¼Œé€šè¿‡å‡å°‘æ¯”è¾ƒæ¬¡æ•°æé«˜äº†æ…¢é€Ÿä½†æœ‰æ•ˆçš„rerankersåœ¨æŸ¥è¯¢æ—¶çš„æ•ˆç‡ã€‚æœ€æ–°ç¥ç»rerankersåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå®ç°äº†å…ˆè¿›çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶éœ€è¦å¤§é‡èµ„æºï¼Œå°½ç®¡ç»è¿‡ä¼˜åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Rank-Kæ¨¡å‹ï¼Œä¸€ç§åŸºäºæ¨ç†è¯­è¨€æ¨¡å‹çš„listwiseæ®µè½é‡æ’æ¨¡å‹ï¼Œæä¾›äº†æŸ¥è¯¢æ—¶é—´çš„å¯æ‰©å±•æ€§ä»¥åº”å¯¹éš¾ä»¥å¤„ç†çš„æŸ¥è¯¢ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºæœ€æ–°æœ€å…ˆè¿›çš„listwiseé‡æ’å™¨RankZephyrï¼ŒRank-Kåœ¨BM25åˆå§‹æ’ååˆ—è¡¨é‡æ’æ—¶æé«˜äº†æ£€ç´¢æœ‰æ•ˆæ€§ï¼Œæå‡äº†23%ï¼Œåœ¨SPLADE-v3çš„å¼ºå¤§æ£€ç´¢ç»“æœé‡æ’æ—¶æé«˜äº†19%ã€‚ç”±äºRank-Kæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°å®ƒåœ¨å¤„ç†ä¸åŒè¯­è¨€çš„æŸ¥è¯¢æ—¶åŒæ ·æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rank-Kæ¨¡å‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¿›è¡Œlistwiseæ®µè½é‡æ’ã€‚</li>
<li>Rank-Kèƒ½å¤Ÿåœ¨å‡å°‘èµ„æºæ¶ˆè€—çš„åŒæ—¶æé«˜æ£€ç´¢æ•ˆç‡ã€‚</li>
<li>ä¸å…ˆè¿›çš„listwiseé‡æ’å™¨RankZephyrç›¸æ¯”ï¼ŒRank-Kæé«˜äº†æ£€ç´¢æœ‰æ•ˆæ€§ã€‚</li>
<li>Rank-Kæ¨¡å‹åœ¨å¤„ç†å¤šè¯­è¨€æŸ¥è¯¢æ—¶åŒæ ·æœ‰æ•ˆã€‚</li>
<li>Rank-Ké€šè¿‡ä¼˜åŒ–æé«˜æ£€ç´¢æ•ˆç‡çš„åŒæ—¶ï¼Œä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>Rank-Kæ¨¡å‹é€‚ç”¨äºå¤„ç†å¤æ‚çš„æŸ¥è¯¢éœ€æ±‚ï¼Œæä¾›äº†æŸ¥è¯¢æ—¶é—´çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14432">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2cabd34384fb4917644c98abcb0a4982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b18ebc77d2be439b9bcf0a38ef1a264a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baa519e7f1b10238ffadeef02c44f85f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b4d35513b74ab1adbb4726b454e8e4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f451c9cfaf568b3d073582ebccc9ae.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SCOPE-Compress-Mathematical-Reasoning-Steps-for-Efficient-Automated-Process-Annotation"><a href="#SCOPE-Compress-Mathematical-Reasoning-Steps-for-Efficient-Automated-Process-Annotation" class="headerlink" title="SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated   Process Annotation"></a>SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated   Process Annotation</h2><p><strong>Authors:Huimin Xu, Xin Mao, Feng-Lin Li, Xiaobao Wu, Wang Chen, Wei Zhang, Anh Tuan Luu</strong></p>
<p>Process Reward Models (PRMs) have demonstrated promising results in mathematical reasoning, but existing process annotation approaches, whether through human annotations or Monte Carlo simulations, remain computationally expensive. In this paper, we introduce Step COmpression for Process Estimation (SCOPE), a novel compression-based approach that significantly reduces annotation costs. We first translate natural language reasoning steps into code and normalize them through Abstract Syntax Tree, then merge equivalent steps to construct a prefix tree. Unlike simulation-based methods that waste numerous samples on estimation, SCOPE leverages a compression-based prefix tree where each root-to-leaf path serves as a training sample, reducing the complexity from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K samples with only 5% of the computational resources required by previous methods. Empirical results demonstrate that PRMs trained on our dataset consistently outperform existing automated annotation approaches on both Best-of-N strategy and ProcessBench. </p>
<blockquote>
<p>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢å±•ç°å‡ºäº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†æ˜¯ç°æœ‰è¿‡ç¨‹æ ‡æ³¨æ–¹æ³•ï¼Œæ— è®ºæ˜¯é€šè¿‡äººå·¥æ ‡æ³¨è¿˜æ˜¯è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼Œåœ¨è®¡ç®—ä¸Šä»ç„¶æˆæœ¬é«˜æ˜‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå‹ç¼©çš„è¿‡ç¨‹ä¼°è®¡ï¼ˆSCOPEï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å‹ç¼©æ–¹æ³•ï¼Œå¯ä»¥æ˜¾è‘—é™ä½æ ‡æ³¨æˆæœ¬ã€‚æˆ‘ä»¬é¦–å…ˆæŠŠè‡ªç„¶è¯­è¨€æ¨ç†æ­¥éª¤ç¿»è¯‘æˆä»£ç ï¼Œå¹¶é€šè¿‡æŠ½è±¡è¯­æ³•æ ‘è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œç„¶ååˆå¹¶ç­‰æ•ˆæ­¥éª¤æ¥æ„å»ºå‰ç¼€æ ‘ã€‚ä¸åŒäºåŸºäºæ¨¡æ‹Ÿçš„æ–¹æ³•åœ¨ä¼°è®¡ä¸Šæµªè´¹å¤§é‡æ ·æœ¬ï¼ŒSCOPEåˆ©ç”¨åŸºäºå‹ç¼©çš„å‰ç¼€æ ‘ï¼Œå…¶ä¸­æ¯ä¸ªä»æ ¹åˆ°å¶çš„è·¯å¾„éƒ½å¯ä½œä¸ºè®­ç»ƒæ ·æœ¬ï¼Œå°†å¤æ‚åº¦ä»$O(NMK)$é™ä½åˆ°$O(N)$ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«19ä¸‡6åƒä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»…ä½¿ç”¨äº†ä¹‹å‰æ–¹æ³•æ‰€éœ€çš„5%çš„è®¡ç®—èµ„æºã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬æ•°æ®é›†ä¸Šè®­ç»ƒçš„PRMåœ¨æœ€ä½³Nç­–ç•¥å’ŒProcessBenchä¸Šå§‹ç»ˆä¼˜äºç°æœ‰è‡ªåŠ¨åŒ–æ ‡æ³¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14419v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç°æœ‰çš„æµç¨‹æ ‡æ³¨æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡å¼•å…¥åŸºäºå‹ç¼©çš„æµç¨‹ä¼°ç®—æ–¹æ³•ï¼ˆSCOPEï¼‰ï¼Œé¦–å…ˆé€šè¿‡æŠ½è±¡è¯­æ³•æ ‘å°†è‡ªç„¶è¯­è¨€æ¨ç†æ­¥éª¤è½¬åŒ–ä¸ºä»£ç å¹¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œç„¶ååˆå¹¶ç­‰ä»·æ­¥éª¤æ„å»ºå‰ç¼€æ ‘ã€‚ä¸åŸºäºæ¨¡æ‹Ÿçš„æ–¹æ³•ç›¸æ¯”ï¼ŒSCOPEåˆ©ç”¨å‹ç¼©å¼å‰ç¼€æ ‘ï¼Œæ¯æ¡ä»æ ¹åˆ°å¶çš„è·¯å¾„å‡ä½œä¸ºè®­ç»ƒæ ·æœ¬ï¼Œå°†å¤æ‚åº¦ä»O(NMK)é™è‡³O(N)ã€‚æˆ‘ä»¬æ„å»ºçš„å¤§å‹æ•°æ®é›†åŒ…å«19.6ä¸‡æ ·æœ¬ï¼Œä»…éœ€ä¹‹å‰æ–¹æ³•çš„5%è®¡ç®—èµ„æºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æœ€ä½³Nç­–ç•¥å’ŒProcessBenchä¸Šï¼ŒåŸºäºæˆ‘ä»¬æ•°æ®é›†çš„PRMè¡¨ç°å‡ä¼˜äºç°æœ‰è‡ªåŠ¨æ ‡æ³¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SCOPEæ˜¯ä¸€ç§æ–°å‹çš„åŸºäºå‹ç¼©çš„æµç¨‹ä¼°ç®—æ–¹æ³•ï¼Œç”¨äºé™ä½æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>SCOPEé€šè¿‡å°†è‡ªç„¶è¯­è¨€æ¨ç†æ­¥éª¤è½¬åŒ–ä¸ºä»£ç å¹¶å½’ä¸€åŒ–ï¼Œè¿›è€Œæ„å»ºå‰ç¼€æ ‘ï¼Œå®ç°äº†é«˜æ•ˆçš„æ ·æœ¬ç”Ÿæˆã€‚</li>
<li>SCOPEæ–¹æ³•å°†å¤æ‚åº¦ä»O(NMK)é™ä½åˆ°O(N)ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>æ„å»ºçš„å¤§å‹æ•°æ®é›†åŒ…å«19.6ä¸‡æ ·æœ¬ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—èµ„æºéœ€æ±‚ã€‚</li>
<li>PRMåœ¨æœ€ä½³Nç­–ç•¥å’ŒProcessBenchä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰è‡ªåŠ¨æ ‡æ³¨æ–¹æ³•ã€‚</li>
<li>SCOPEæ–¹æ³•ä¸ºPRMçš„ç ”ç©¶æä¾›äº†æ–°æ€è·¯ï¼Œæœ‰åŠ©äºæ¨åŠ¨è‡ªç„¶è¯­è¨€æ¨ç†é¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cfa1e2ec745f202ff86e12957b6ddc88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbaa0479f41bc9d1f7d147e7fb9d0314.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-056439767db780390899e48ebcb1cded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb7806ea210aa59bd8aa1f1b4a6e0df4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a03ff80069033a491dc811060c61314.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Unearthing-Gems-from-Stones-Policy-Optimization-with-Negative-Sample-Augmentation-for-LLM-Reasoning"><a href="#Unearthing-Gems-from-Stones-Policy-Optimization-with-Negative-Sample-Augmentation-for-LLM-Reasoning" class="headerlink" title="Unearthing Gems from Stones: Policy Optimization with Negative Sample   Augmentation for LLM Reasoning"></a>Unearthing Gems from Stones: Policy Optimization with Negative Sample   Augmentation for LLM Reasoning</h2><p><strong>Authors:Zhaohui Yang, Shilei Jiang, Chen Hu, Linjing Li, Shihong Deng, Daxin Jiang</strong></p>
<p>Recent advances in reasoning language models have witnessed a paradigm shift from short to long CoT pattern. Given the substantial computational cost of rollouts in long CoT models, maximizing the utility of fixed training datasets becomes crucial. Our analysis reveals that negative responses contain valuable components such as self-reflection and error-correction steps, yet primary existing methods either completely discard negative samples (RFT) or apply equal penalization across all tokens (RL), failing to leverage these potential learning signals. In light of this, we propose Behavior Constrained Policy Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline RL framework that encompasses three stages: 1) sample segmentation, 2) consensus-based step correctness assessment combining LLM and PRM judgers, and 3) policy optimization with NSA designed to effectively mine positive steps within negative samples. Experimental results show that BCPG-NSA outperforms baselines on several challenging math&#x2F;coding reasoning benchmarks using the same training dataset, achieving improved sample efficiency and demonstrating robustness and scalability when extended to multiple iterations. </p>
<blockquote>
<p>è¿‘æœŸæ¨ç†è¯­è¨€æ¨¡å‹çš„è¿›æ­¥è§è¯äº†ä¸€ä¸ªä»çŸ­CoTæ¨¡å¼åˆ°é•¿CoTæ¨¡å¼çš„è½¬å˜ã€‚è€ƒè™‘åˆ°é•¿CoTæ¨¡å‹ä¸­rolloutsçš„å·¨å¤§è®¡ç®—æˆæœ¬ï¼Œæœ€å¤§é™åº¦åœ°æé«˜å›ºå®šè®­ç»ƒæ•°æ®é›†çš„ä½¿ç”¨ä»·å€¼å˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè´Ÿé¢åé¦ˆåŒ…å«æœ‰ä»·å€¼çš„æˆåˆ†ï¼Œå¦‚è‡ªæˆ‘åæ€å’Œçº é”™æ­¥éª¤ï¼Œä½†ç°æœ‰çš„ä¸»è¦æ–¹æ³•è¦ä¹ˆå®Œå…¨ä¸¢å¼ƒè´Ÿé¢æ ·æœ¬ï¼ˆRFTï¼‰ï¼Œè¦ä¹ˆå¯¹æ‰€æœ‰æ ‡è®°åº”ç”¨å‡ç­‰çš„æƒ©ç½šï¼ˆRLï¼‰ï¼Œæ— æ³•åˆ©ç”¨è¿™äº›æ½œåœ¨çš„å­¦ä¹ ä¿¡å·ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè´Ÿé¢æ ·æœ¬å¢å¼ºçš„è¡Œä¸ºçº¦æŸç­–ç•¥æ¢¯åº¦ï¼ˆBCPG-NSAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾ç»†çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼š1ï¼‰æ ·æœ¬åˆ†å‰²ï¼Œ2ï¼‰åŸºäºå…±è¯†çš„æ­¥éª¤æ­£ç¡®æ€§è¯„ä¼°ï¼Œç»“åˆLLMå’ŒPRMåˆ¤æ–­å™¨ï¼Œä»¥åŠ3ï¼‰é’ˆå¯¹åœ¨è´Ÿé¢æ ·æœ¬ä¸­æœ‰æ•ˆæŒ–æ˜æ­£é¢æ­¥éª¤çš„NSAç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦&#x2F;ç¼–ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒBCPG-NSAä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®é›†è¶…è¶Šäº†åŸºçº¿ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶åœ¨æ‰©å±•åˆ°å¤šæ¬¡è¿­ä»£æ—¶æ˜¾ç¤ºå‡ºç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14403v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ¨ç†è¯­è¨€æ¨¡å‹çš„è¿›å±•ç»å†äº†ä»çŸ­CoTæ¨¡å¼åˆ°é•¿CoTæ¨¡å¼çš„è½¬å˜ã€‚è€ƒè™‘åˆ°é•¿CoTæ¨¡å‹ä¸­æ»šåŠ¨è®¡ç®—çš„å·¨å¤§æˆæœ¬ï¼Œæœ€å¤§åŒ–å›ºå®šè®­ç»ƒæ•°æ®é›†çš„ä½¿ç”¨å˜å¾—è‡³å…³é‡è¦ã€‚åˆ†æè¡¨æ˜ï¼Œå¦å®šå›ç­”åŒ…å«æœ‰ä»·å€¼çš„æˆåˆ†ï¼Œå¦‚è‡ªæˆ‘åæ€å’Œçº é”™æ­¥éª¤ã€‚ç°æœ‰çš„ä¸»è¦æ–¹æ³•è¦ä¹ˆå®Œå…¨æ”¾å¼ƒè´Ÿé¢æ ·æœ¬ï¼ˆRFTï¼‰ï¼Œè¦ä¹ˆå¯¹æ‰€æœ‰ä»¤ç‰Œåº”ç”¨å¹³ç­‰çš„æƒ©ç½šï¼ˆRLï¼‰ï¼Œæœªèƒ½åˆ©ç”¨è¿™äº›æ½œåœ¨çš„å­¦ä¹ ä¿¡å·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è¡Œä¸ºçº¦æŸç­–ç•¥æ¢¯åº¦ä¸è´Ÿæ ·æœ¬æ‰©å……ï¼ˆBCPG-NSAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç²¾ç»†çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼š1ï¼‰æ ·æœ¬åˆ†å‰²ï¼›2ï¼‰ç»“åˆLLMå’ŒPRMåˆ¤æ–­å™¨è¿›è¡ŒåŸºäºå…±è¯†çš„æ­¥éª¤æ­£ç¡®æ€§è¯„ä¼°ï¼›3ï¼‰é’ˆå¯¹è´Ÿæ ·æœ¬ä¸­çš„ç§¯ææ­¥éª¤è¿›è¡Œè®¾è®¡çš„NSAç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBCPG-NSAåœ¨å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦&#x2F;ç¼–ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºçº¿ï¼Œä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ•°æ®é›†ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œå¹¶åœ¨æ‰©å±•åˆ°å¤šæ¬¡è¿­ä»£æ—¶è¡¨ç°å‡ºç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨ç†è¯­è¨€æ¨¡å‹ç»å†ä»çŸ­CoTåˆ°é•¿CoTçš„è½¬å˜ã€‚</li>
<li>é•¿CoTæ¨¡å‹é¢ä¸´å·¨å¤§çš„è®¡ç®—æˆæœ¬ï¼Œå› æ­¤æœ‰æ•ˆåˆ©ç”¨å›ºå®šè®­ç»ƒæ•°æ®é›†å˜å¾—é‡è¦ã€‚</li>
<li>å¦å®šå›ç­”åŒ…å«æœ‰ä»·å€¼çš„æˆåˆ†å¦‚è‡ªæˆ‘åæ€å’Œçº é”™æ­¥éª¤ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è´Ÿé¢æ ·æœ¬ä¸­çš„å­¦ä¹ ä¿¡å·ã€‚</li>
<li>BCPG-NSAæ˜¯ä¸€ä¸ªç²¾ç»†çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æ ·æœ¬åˆ†å‰²ã€åŸºäºå…±è¯†çš„æ­¥éª¤æ­£ç¡®æ€§è¯„ä¼°å’Œç­–ç•¥ä¼˜åŒ–ä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>BCPG-NSAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0b426c5d6404524fdb29bd621bbf25d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bc8c96345252d37df23c56b68216476.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e36e4193a9b05b78b3ce044ff15b035.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77977560a6e83d1fdd44887775fcae97.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Log-Augmented-Generation-Scaling-Test-Time-Reasoning-with-Reusable-Computation"><a href="#Log-Augmented-Generation-Scaling-Test-Time-Reasoning-with-Reusable-Computation" class="headerlink" title="Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable   Computation"></a>Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable   Computation</h2><p><strong>Authors:Peter Baile Chen, Yi Zhang, Dan Roth, Samuel Madden, Jacob Andreas, Michael Cafarella</strong></p>
<p>While humans naturally learn and adapt from past experiences, large language models (LLMs) and their agentic counterparts struggle to retain reasoning from previous tasks and apply them in future contexts. To address this limitation, we propose a novel framework, log-augmented generation (LAG) that directly reuses prior computation and reasoning from past logs at test time to enhance modelâ€™s ability to learn from previous tasks and perform better on new, unseen challenges, all while keeping the system efficient and scalable. Specifically, our system represents task logs using key-value (KV) caches, encoding the full reasoning context of prior tasks while storing KV caches for only a selected subset of tokens. When a new task arises, LAG retrieves the KV values from relevant logs to augment generation. Our approach differs from reflection-based memory mechanisms by directly reusing prior reasoning and computations without requiring additional steps for knowledge extraction or distillation. Our method also goes beyond existing KV caching techniques, which primarily target efficiency gains rather than improving accuracy. Experiments on knowledge- and reasoning-intensive datasets demonstrate that our method significantly outperforms standard agentic systems that do not utilize logs, as well as existing solutions based on reflection and KV cache techniques. </p>
<blockquote>
<p>äººç±»åœ¨è‡ªç„¶å­¦ä¹ è¿‡ç¨‹ä¸­èƒ½ä»è¿‡å»çš„ç»éªŒä¸­å¸å–æ•™è®­å¹¶é€‚åº”æ–°ç¯å¢ƒï¼Œç„¶è€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠå…¶ä»£ç†å¯¹åº”ç‰©åœ¨ä¿ç•™è¿‡å»ä»»åŠ¡çš„æ¨ç†èƒ½åŠ›å¹¶å°†å…¶åº”ç”¨äºæœªæ¥æƒ…å¢ƒæ–¹é¢å´é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶â€”â€”æ—¥å¿—å¢å¼ºç”Ÿæˆï¼ˆLAGï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨æµ‹è¯•æ—¶ç›´æ¥é‡ç”¨è¿‡å»çš„è®¡ç®—å’Œæ¨ç†æ—¥å¿—ï¼Œä»¥å¢å¼ºæ¨¡å‹ä»è¿‡å»ä»»åŠ¡ä¸­å­¦ä¹ å¹¶åœ¨æ–°å‡ºç°çš„ã€æœªè§è¿‡çš„æŒ‘æˆ˜ä¸­è¡¨ç°æ›´ä½³çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒç³»ç»Ÿçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿä½¿ç”¨é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜æ¥è¡¨ç¤ºä»»åŠ¡æ—¥å¿—ï¼Œç¼–ç å…ˆå‰ä»»åŠ¡çš„å…¨æ¨ç†ä¸Šä¸‹æ–‡ï¼Œä»…å¯¹é€‰å®šçš„ä¸€éƒ¨åˆ†æ ‡è®°å­˜å‚¨KVç¼“å­˜ã€‚å½“å‡ºç°æ–°ä»»åŠ¡æ—¶ï¼ŒLAGä¼šæ£€ç´¢ç›¸å…³æ—¥å¿—ä¸­çš„KVå€¼ä»¥è¾…åŠ©ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸åŸºäºåæ€çš„è®°å¿†æœºåˆ¶ä¸åŒï¼Œèƒ½ç›´æ¥é‡ç”¨å…ˆå‰çš„æ¨ç†å’Œè®¡ç®—ï¼Œæ— éœ€é¢å¤–çš„çŸ¥è¯†æå–æˆ–è’¸é¦æ­¥éª¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿè¶…è¶Šäº†ç°æœ‰çš„KVç¼“å­˜æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯ä¸»è¦å…³æ³¨æ•ˆç‡æå‡è€Œéæé«˜å‡†ç¡®æ€§ã€‚åœ¨çŸ¥è¯†å’Œæ¨ç†å¯†é›†å‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†ä¸ä½¿ç”¨æ—¥å¿—çš„ä»£ç†ç³»ç»Ÿä»¥åŠåŸºäºåæ€å’ŒKVç¼“å­˜æŠ€æœ¯çš„ç°æœ‰è§£å†³æ–¹æ¡ˆä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14398v1">PDF</a> Data and code are available at <a target="_blank" rel="noopener" href="https://peterbaile.github.io/lag/">https://peterbaile.github.io/lag/</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« è®¨è®ºäº†äººç±»ä»è¿‡å»çš„ç»éªŒä¸­å­¦ä¹ é€‚åº”çš„èƒ½åŠ›ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹åŠå…¶æ™ºèƒ½å¯¹åº”å®ä½“åˆ™éš¾ä»¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™ï¼Œæå‡ºäº†åä¸ºLAGçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æµ‹è¯•é˜¶æ®µç›´æ¥åˆ©ç”¨è¿‡å»çš„æ—¥å¿—ä¸­çš„è®¡ç®—å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥æé«˜æ¨¡å‹ä»è¿‡å»ä»»åŠ¡ä¸­å­¦ä¹ å¹¶åœ¨æ–°æŒ‘æˆ˜ä¸­è¡¨ç°çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒç³»ç»Ÿçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨é”®å€¼ç¼“å­˜æ¥ä»£è¡¨ä»»åŠ¡æ—¥å¿—ï¼Œå­˜å‚¨å…ˆå‰ä»»åŠ¡çš„å®Œæ•´æ¨ç†ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®å€¼ç¼“å­˜ä»…ç”¨äºä¸€ä¸ªå­é›†çš„æ ‡è®°ã€‚å½“å‡ºç°æ–°ä»»åŠ¡æ—¶ï¼ŒLAGä¼šä»ç›¸å…³æ—¥å¿—ä¸­æ£€ç´¢é”®å€¼ä»¥å¢å¼ºç”Ÿæˆèƒ½åŠ›ã€‚ä¸å…¶ä»–åŸºäºåå°„çš„è®°å¿†æœºåˆ¶ä¸åŒï¼ŒLAGç›´æ¥é‡ç”¨å…ˆå‰çš„æ¨ç†å’Œè®¡ç®—ï¼Œæ— éœ€é¢å¤–çš„çŸ¥è¯†æå–æˆ–è’¸é¦æ­¥éª¤ã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–ä¸»è¦å…³æ³¨æ•ˆç‡æå‡çš„KVç¼“å­˜æŠ€æœ¯ç›¸æ¯”ï¼ŒLAGä¾§é‡äºæé«˜å‡†ç¡®æ€§ã€‚åœ¨çŸ¥è¯†å’Œæ¨ç†å¯†é›†å‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¸ä½¿ç”¨æ—¥å¿—çš„æ ‡å‡†æ™ºèƒ½ç³»ç»Ÿå’Œç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åŠå…¶æ™ºèƒ½å¯¹åº”å®ä½“åœ¨ä¿ç•™è¿‡å»ä»»åŠ¡çš„æ¨ç†å¹¶å°†å…¶åº”ç”¨äºæœªæ¥ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>LAGæ¡†æ¶æ—¨åœ¨é€šè¿‡ç›´æ¥åˆ©ç”¨è¿‡å»çš„æ—¥å¿—ä¸­çš„è®¡ç®—å’Œæ¨ç†èƒ½åŠ›æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>LAGä½¿ç”¨é”®å€¼ç¼“å­˜æ¥å­˜å‚¨å’Œè¡¨ç¤ºä»»åŠ¡æ—¥å¿—ä¸­çš„å…³é”®ä¿¡æ¯ã€‚</li>
<li>LAGèƒ½å¤Ÿä»ç›¸å…³æ—¥å¿—ä¸­æ£€ç´¢é”®å€¼ä»¥å¢å¼ºæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>LAGä¸å…¶ä»–åŸºäºåå°„çš„è®°å¿†æœºåˆ¶ä¸åŒï¼Œå®ƒç›´æ¥é‡ç”¨å…ˆå‰çš„æ¨ç†å’Œè®¡ç®—ã€‚</li>
<li>LAGä¾§é‡äºæé«˜å‡†ç¡®æ€§ï¼Œä¸å…¶ä»–ä¸»è¦å…³æ³¨æ•ˆç‡æå‡çš„KVç¼“å­˜æŠ€æœ¯å½¢æˆå¯¹æ¯”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14398">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4528639247d183187eff0d95a6b397ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e09fd4fda1180b642dddbd788c462290.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcd5dcad10c6d4842481eb04ae449ce1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8aa18d5fb89c5602a50ac4b2650d873d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DeepEyes-Incentivizing-â€œThinking-with-Imagesâ€-via-Reinforcement-Learning"><a href="#DeepEyes-Incentivizing-â€œThinking-with-Imagesâ€-via-Reinforcement-Learning" class="headerlink" title="DeepEyes: Incentivizing â€œThinking with Imagesâ€ via Reinforcement   Learning"></a>DeepEyes: Incentivizing â€œThinking with Imagesâ€ via Reinforcement   Learning</h2><p><strong>Authors:Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, Xing Yu</strong></p>
<p>Large Vision-Language Models (VLMs) have shown strong capabilities in multimodal understanding and reasoning, yet they are primarily constrained by text-based reasoning processes. However, achieving seamless integration of visual and textual reasoning which mirrors human cognitive processes remains a significant challenge. In particular, effectively incorporating advanced visual input processing into reasoning mechanisms is still an open question. Thus, in this paper, we explore the interleaved multimodal reasoning paradigm and introduce DeepEyes, a model with â€œthinking with imagesâ€ capabilities incentivized through end-to-end reinforcement learning without the need for cold-start SFT. Notably, this ability emerges natively within the model itself, leveraging its inherent grounding ability as a tool instead of depending on separate specialized models. Specifically, we propose a tool-use-oriented data selection mechanism and a reward strategy to encourage successful tool-assisted reasoning trajectories. DeepEyes achieves significant performance gains on fine-grained perception and reasoning benchmarks and also demonstrates improvement in grounding, hallucination, and mathematical reasoning tasks. Interestingly, we observe the distinct evolution of tool-calling behavior from initial exploration to efficient and accurate exploitation, and diverse thinking patterns that closely mirror human visual reasoning processes. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Visual-Agent/DeepEyes">https://github.com/Visual-Agent/DeepEyes</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦å—åˆ°åŸºäºæ–‡æœ¬çš„æ¨ç†è¿‡ç¨‹çš„é™åˆ¶ã€‚ç„¶è€Œï¼Œå®ç°æ— ç¼çš„è§†è§‰å’Œæ–‡æœ¬æ¨ç†é›†æˆï¼Œä»¥åæ˜ äººç±»çš„è®¤çŸ¥è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†å…ˆè¿›çš„è§†è§‰è¾“å…¥å¤„ç†èå…¥æ¨ç†æœºåˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†äº¤ç»‡çš„å¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œå¹¶å¼•å…¥äº†DeepEyesæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰â€œä»¥å›¾åƒæ€è€ƒâ€çš„èƒ½åŠ›ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¿€åŠ±ï¼Œæ— éœ€å†·å¯åŠ¨SFTã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§èƒ½åŠ›æ˜¯åœ¨æ¨¡å‹æœ¬èº«å†…éƒ¨è‡ªç„¶äº§ç”Ÿçš„ï¼Œå®ƒåˆ©ç”¨æ¨¡å‹æœ¬èº«çš„å†…åœ¨å®šä½èƒ½åŠ›ä½œä¸ºå·¥å…·ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå•ç‹¬çš„ä¸“ç”¨æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥å·¥å…·ä½¿ç”¨ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©æœºåˆ¶å’Œå¥–åŠ±ç­–ç•¥ï¼Œä»¥é¼“åŠ±æˆåŠŸçš„å·¥å…·è¾…åŠ©æ¨ç†è½¨è¿¹ã€‚DeepEyesåœ¨ç²¾ç»†æ„ŸçŸ¥å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨å®šä½ã€å¹»è§‰å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å±•ç¤ºäº†æ”¹è¿›ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä»åˆæ­¥æ¢ç´¢åˆ°æœ‰æ•ˆå’Œç²¾ç¡®åˆ©ç”¨çš„å·¥å…·è°ƒç”¨è¡Œä¸ºçš„ç‹¬ç‰¹æ¼”å˜ï¼Œä»¥åŠç´§å¯†æ¨¡ä»¿äººç±»è§†è§‰æ¨ç†è¿‡ç¨‹çš„å¤šæ ·åŒ–æ€ç»´æ¨¡å¼ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Visual-Agent/DeepEyes">https://github.com/Visual-Agent/DeepEyes</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14362v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†äº¤ç»‡å¼å¤šæ¨¡æ€æ¨ç†èŒƒå¼ï¼Œå¹¶å¼•å…¥äº†DeepEyesæ¨¡å‹ã€‚è¯¥æ¨¡å‹å…·æœ‰â€œä»¥å›¾åƒæ€è€ƒâ€çš„èƒ½åŠ›ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ å®ç°ï¼Œæ— éœ€å†·å¯åŠ¨SFTã€‚DeepEyesåˆ©ç”¨å†…åœ¨çš„åœ°æ ‡èƒ½åŠ›ä½œä¸ºå·¥å…·ï¼Œä¸ä¾èµ–å•ç‹¬çš„ä¸“é—¨æ¨¡å‹ï¼Œå®ç°äº†å¯¹ç²¾ç»†æ„ŸçŸ¥å’Œæ¨ç†åŸºå‡†æµ‹è¯•çš„æ€§èƒ½æå‡ï¼Œå¹¶æ”¹å–„äº†åœ°æ ‡ã€å¹»è§‰å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ã€‚è§‚å¯Ÿåˆ°å·¥å…·è°ƒç”¨è¡Œä¸ºçš„ç‹¬ç‰¹æ¼”å˜ï¼Œä»åˆæ­¥æ¢ç´¢åˆ°é«˜æ•ˆå‡†ç¡®çš„åˆ©ç”¨ï¼Œä»¥åŠç´§å¯†æ¨¡ä»¿äººç±»è§†è§‰æ¨ç†è¿‡ç¨‹çš„å¤šæ ·åŒ–æ€ç»´æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Vision-Language Models (VLMs) å±•ç°å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†ä¸»è¦å—é™äºæ–‡æœ¬åŸºç¡€çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å®ç°è§†è§‰å’Œæ–‡æœ¬æ¨ç†æ— ç¼é›†æˆä»¥æ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>DeepEyesæ¨¡å‹å…·æœ‰â€œä»¥å›¾åƒæ€è€ƒâ€çš„èƒ½åŠ›ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ å®ç°ï¼Œæ— éœ€å†·å¯åŠ¨SFTã€‚</li>
<li>DeepEyesåˆ©ç”¨å†…åœ¨çš„åœ°æ ‡èƒ½åŠ›ï¼Œä¸ä¾èµ–å•ç‹¬çš„ä¸“é—¨æ¨¡å‹ã€‚</li>
<li>DeepEyesåœ¨ç²¾ç»†æ„ŸçŸ¥å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ€§èƒ½æå‡ï¼Œå¹¶æ”¹å–„äº†åœ°æ ‡ã€å¹»è§‰å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å·¥å…·è°ƒç”¨è¡Œä¸ºçš„ç‹¬ç‰¹æ¼”å˜ä»åˆæ­¥æ¢ç´¢åˆ°é«˜æ•ˆå‡†ç¡®çš„åˆ©ç”¨è¢«è§‚å¯Ÿåˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14362">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd1c9fcf42c71e9174f62ec8079a0d3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aac079d0f9d824c68c6fedbaf38ddd83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e87123b3f03c221c4c40d061ceb633.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8533be0af0ea2c3392bd7d441d82224.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-MIND-for-Reasoning-Meta-learning-for-In-context-Deduction"><a href="#A-MIND-for-Reasoning-Meta-learning-for-In-context-Deduction" class="headerlink" title="A MIND for Reasoning: Meta-learning for In-context Deduction"></a>A MIND for Reasoning: Meta-learning for In-context Deduction</h2><p><strong>Authors:Leonardo Bertolazzi, Manuel Vargas GuzmÃ¡n, Raffaella Bernardi, Maciej Malicki, Jakub Szymanik</strong></p>
<p>Large language models (LLMs) are increasingly evaluated on formal tasks, where strong reasoning abilities define the state of the art. However, their ability to generalize to out-of-distribution problems remains limited. In this paper, we investigate how LLMs can achieve a systematic understanding of deductive rules. Our focus is on the task of identifying the appropriate subset of premises within a knowledge base needed to derive a given hypothesis. To tackle this challenge, we propose Meta-learning for In-context Deduction (MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND is to enable models to generalize more effectively to unseen knowledge bases and to systematically apply inference rules. Our results show that MIND significantly improves generalization in small LMs ranging from 1.5B to 7B parameters. The benefits are especially pronounced in smaller models and low-data settings. Remarkably, small models fine-tuned with MIND outperform state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­£å¼ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¶Šæ¥è¶Šå¤šï¼Œå…¶ä¸­å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å®šä¹‰äº†æœ€æ–°æŠ€æœ¯çŠ¶æ€ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„é—®é¢˜çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMå¦‚ä½•å®ç°å¯¹æ¼”ç»è§„åˆ™çš„ç³»ç»Ÿæ€§ç†è§£ã€‚æˆ‘ä»¬çš„é‡ç‚¹æ˜¯ç¡®å®šåœ¨çŸ¥è¯†åº“ä¸­ç”¨äºæ¨å¯¼ç»™å®šå‡è®¾çš„é€‚å½“å‰æå­é›†çš„ä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºä¸Šä¸‹æ–‡å†…æ¼”ç»çš„å…ƒå­¦ä¹ ï¼ˆMINDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å°æ ·æœ¬å…ƒå­¦ä¹ å¾®è°ƒæ–¹æ³•ã€‚MINDçš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„çŸ¥è¯†åº“ï¼Œå¹¶ç³»ç»Ÿåœ°åº”ç”¨æ¨ç†è§„åˆ™ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMINDåœ¨å‚æ•°èŒƒå›´ä»1.5Båˆ°7Bçš„å°å‹LMä¸­æ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è¾ƒå°çš„æ¨¡å‹å’Œä½æ•°æ®è®¾ç½®ä¸‹ï¼Œè¿™äº›å¥½å¤„å°¤ä¸ºçªå‡ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨MINDè¿›è¡Œå¾®è°ƒçš„å°å‹æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oå’Œo3-miniï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­£å¼ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ—¥ç›Šå¢åŠ ï¼Œå…¶æ¨ç†èƒ½åŠ›æˆä¸ºè¡¡é‡æŠ€æœ¯å…ˆè¿›æ€§çš„å…³é”®æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹ç¦»åˆ†å¸ƒé—®é¢˜çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨LLMå¦‚ä½•ç³»ç»Ÿç†è§£æ¼”ç»è§„åˆ™ã€‚ç ”ç©¶ç„¦ç‚¹åœ¨äºè¯†åˆ«çŸ¥è¯†åº“ä¸­ç”¨äºæ¨å¯¼ç»™å®šå‡è®¾æ‰€éœ€çš„å‰ææ¡ä»¶çš„å­é›†ä»»åŠ¡ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºMeta-learning for In-context Deductionï¼ˆMINDï¼‰çš„æ–°å‹å…ƒå­¦ä¹ å¾®è°ƒæ–¹æ³•ã€‚MINDçš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„çŸ¥è¯†åº“å¹¶ç³»ç»Ÿåœ°åº”ç”¨æ¨ç†è§„åˆ™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMINDå¯¹å°è§„æ¨¡è‡³ä¸­ç­‰è§„æ¨¡LLMï¼ˆå‚æ•°èŒƒå›´ä»1.5Båˆ°7Bï¼‰çš„æ³›åŒ–èƒ½åŠ›æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå°¤å…¶æ˜¯åœ¨å°å‹æ¨¡å‹å’Œä½æ•°æ®è®¾ç½®ä¸‹å°¤ä¸ºæ˜¾è‘—ã€‚é€šè¿‡é‡‡ç”¨MINDè°ƒæ•™çš„å°å‹æ¨¡å‹è¡¨ç°å‡ºè¶…ä¹é¢„æœŸçš„æ€§èƒ½ï¼Œåœ¨è¿™é¡¹ä»»åŠ¡ä¸Šè¶…è¶Šäº†å¦‚GPT-4oå’Œo3-miniç­‰å…ˆè¿›LLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ­£å¼ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é¢å¯¹ç¦»åˆ†å¸ƒé—®é¢˜æ—¶çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>ç ”ç©¶å…³æ³¨LLMså¦‚ä½•ç³»ç»Ÿç†è§£æ¼”ç»è§„åˆ™ï¼Œç‰¹åˆ«æ˜¯è¯†åˆ«çŸ¥è¯†åº“ä¸­æ¨å¯¼å‡è®¾æ‰€éœ€çš„å‰æå­é›†çš„ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å…ƒå­¦ä¹ å¾®è°ƒæ–¹æ³•â€”â€”Meta-learning for In-context Deductionï¼ˆMINDï¼‰ã€‚</li>
<li>MINDæœ‰åŠ©äºæ¨¡å‹æ›´æœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„çŸ¥è¯†åº“ï¼Œå¹¶ç³»ç»Ÿåœ°åº”ç”¨æ¨ç†è§„åˆ™ã€‚</li>
<li>MINDå¯¹å°è§„æ¨¡è‡³ä¸­ç­‰è§„æ¨¡çš„LLMï¼ˆå‚æ•°èŒƒå›´ä»1.5Båˆ°7Bï¼‰çš„æ³›åŒ–èƒ½åŠ›æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>åœ¨ç‰¹å®šçš„ä»»åŠ¡ä¸Šï¼Œé‡‡ç”¨MINDè°ƒæ•™çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å…ˆè¿›LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c43c4f4fe9cb56474d4ec8609340548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c67fb438256b67575dd4c8aaea80bc85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0a0858d42483e3a8bbf1da6f9964a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36886669c30451fd961ec7fb1de712d4.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MultiTab-A-Comprehensive-Benchmark-Suite-for-Multi-Dimensional-Evaluation-in-Tabular-Domains"><a href="#MultiTab-A-Comprehensive-Benchmark-Suite-for-Multi-Dimensional-Evaluation-in-Tabular-Domains" class="headerlink" title="MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional   Evaluation in Tabular Domains"></a>MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional   Evaluation in Tabular Domains</h2><p><strong>Authors:Kyungeun Lee, Moonjung Eo, Hye-Seung Cho, Dongmin Kim, Ye Seul Sim, Seoyoon Kim, Min-Kook Suh, Woohyung Lim</strong></p>
<p>Despite the widespread use of tabular data in real-world applications, most benchmarks rely on average-case metrics, which fail to reveal how model behavior varies across diverse data regimes. To address this, we propose MultiTab, a benchmark suite and evaluation framework for multi-dimensional, data-aware analysis of tabular learning algorithms. Rather than comparing models only in aggregate, MultiTab categorizes 196 publicly available datasets along key data characteristics, including sample size, label imbalance, and feature interaction, and evaluates 13 representative models spanning a range of inductive biases. Our analysis shows that model performance is highly sensitive to such regimes: for example, models using sample-level similarity excel on datasets with large sample sizes or high inter-feature correlation, while models encoding inter-feature dependencies perform best with weakly correlated features. These findings reveal that inductive biases do not always behave as intended, and that regime-aware evaluation is essential for understanding and improving model behavior. MultiTab enables more principled model design and offers practical guidance for selecting models tailored to specific data characteristics. All datasets, code, and optimization logs are publicly available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LGAI-DILab/Multitab">https://huggingface.co/datasets/LGAI-DILab/Multitab</a>. </p>
<blockquote>
<p>å°½ç®¡è¡¨æ ¼æ•°æ®åœ¨ç°å®ä¸–ç•Œçš„å¹¿æ³›åº”ç”¨ï¼Œä½†å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä»ç„¶ä¾èµ–äºå¹³å‡æƒ…å†µä¸‹çš„æŒ‡æ ‡ï¼Œè¿™æ— æ³•æ­ç¤ºæ¨¡å‹è¡Œä¸ºåœ¨ä¸åŒæ•°æ®ç¯å¢ƒä¸‹çš„å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MultiTabï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•å¥—ä»¶å’Œè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå¯¹è¡¨æ ¼å­¦ä¹ ç®—æ³•è¿›è¡Œå¤šç»´åº¦çš„æ•°æ®æ„ŸçŸ¥åˆ†æã€‚MultiTabä¸ä»…ä»…æ˜¯åœ¨æ€»ä½“ä¸Šå¯¹æ¯”æ¨¡å‹ï¼Œè€Œæ˜¯å°†196ä¸ªå…¬å¼€æ•°æ®é›†æŒ‰ç…§å…³é”®æ•°æ®ç‰¹æ€§è¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬æ ·æœ¬å¤§å°ã€æ ‡ç­¾ä¸å¹³è¡¡å’Œç‰¹å¾äº¤äº’ç­‰ï¼Œå¹¶å¯¹13ç§å…·æœ‰ä»£è¡¨æ€§çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹æ¶µç›–äº†ä¸€ç³»åˆ—çš„å½’çº³åç½®ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹æ€§èƒ½å¯¹è¿™äº›ç¯å¢ƒéå¸¸æ•æ„Ÿï¼šä¾‹å¦‚ï¼Œä½¿ç”¨æ ·æœ¬çº§åˆ«ç›¸ä¼¼æ€§çš„æ¨¡å‹åœ¨æ ·æœ¬é‡å¤§æˆ–ç‰¹å¾é—´é«˜ç›¸å…³çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€Œç¼–ç ç‰¹å¾é—´ä¾èµ–å…³ç³»çš„æ¨¡å‹åœ¨å¼±ç›¸å…³ç‰¹å¾ä¸Šè¡¨ç°æœ€ä½³ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½’çº³åç½®å¹¶ä¸æ€»æ˜¯æŒ‰é¢„æœŸè¡¨ç°ï¼Œè€Œäº†è§£ç‰¹å®šæ•°æ®ç‰¹æ€§çš„è¯„ä¼°å¯¹äºç†è§£å’Œæ”¹è¿›æ¨¡å‹è¡Œä¸ºè‡³å…³é‡è¦ã€‚MultiTabä¸ºå®ç°æ›´æœ‰åŸåˆ™çš„æ¨¡å‹è®¾è®¡æä¾›äº†å¯èƒ½ï¼Œå¹¶ä¸ºé’ˆå¯¹ç‰¹å®šæ•°æ®ç‰¹æ€§é€‰æ‹©æ¨¡å‹æä¾›äº†å®é™…æŒ‡å¯¼ã€‚æ‰€æœ‰æ•°æ®é›†ã€ä»£ç å’Œä¼˜åŒ–æ—¥å¿—å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LGAI-DILab/Multitab%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/datasets/LGAI-DILab/Multitabå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14312v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºMultiTabï¼Œä¸€ä¸ªç”¨äºå¤šç»´æ•°æ®æ„ŸçŸ¥çš„è¡¨æ ¼å­¦ä¹ ç®—æ³•è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸ä»…å…³æ³¨å¹³å‡æƒ…å†µä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼Œæ›´æ³¨é‡æ¨¡å‹åœ¨ä¸åŒæ•°æ®ç‰¹æ€§ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚é€šè¿‡åˆ†ç±»196ä¸ªå…¬å¼€æ•°æ®é›†å’Œè¯„ä¼°13ç§ä»£è¡¨æ€§æ¨¡å‹ï¼Œç ”ç©¶å‘ç°æ¨¡å‹æ€§èƒ½å¯¹æ•°æ®ç‰¹æ€§é«˜åº¦æ•æ„Ÿã€‚MultiTabçš„æ¨å‡ºï¼Œæœ‰åŠ©äºæ›´ç³»ç»Ÿåœ°è®¾è®¡æ¨¡å‹ï¼Œå¹¶ä¸ºé’ˆå¯¹ç‰¹å®šæ•°æ®ç‰¹æ€§çš„æ¨¡å‹é€‰æ‹©æä¾›å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å¹³å‡æƒ…å†µä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼Œå¿½ç•¥äº†æ¨¡å‹åœ¨ä¸åŒæ•°æ®ç‰¹æ€§ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚</li>
<li>MultiTabæ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¿›è¡Œå¤šç»´æ•°æ®æ„ŸçŸ¥çš„è¡¨æ ¼å­¦ä¹ ç®—æ³•åˆ†æã€‚</li>
<li>è¯¥æ¡†æ¶åˆ†ç±»äº†196ä¸ªå…¬å¼€æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†13ç§ä»£è¡¨æ€§æ¨¡å‹ï¼Œæ¶µç›–å¤šç§å½’çº³åç½®ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½å¯¹æ•°æ®ç‰¹æ€§é«˜åº¦æ•æ„Ÿï¼Œä¾‹å¦‚æ ·æœ¬å¤§å°ã€æ ‡ç­¾ä¸å¹³è¡¡å’Œç‰¹å¾äº¤äº’ç­‰ã€‚</li>
<li>æŸäº›æ¨¡å‹åœ¨ç‰¹å®šæ•°æ®ç‰¹æ€§ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¾‹å¦‚æ ·æœ¬çº§åˆ«ç›¸ä¼¼æ€§åœ¨å¤§æ ·æœ¬æˆ–é«˜ç‰¹å¾ç›¸å…³æ€§æ•°æ®é›†ä¸Šè¡¨ç°è¾ƒå¥½ã€‚</li>
<li>å½’çº³åç½®å¹¶ä¸æ€»æ˜¯æŒ‰é¢„æœŸå·¥ä½œï¼Œéœ€è¦å¯¹ç‰¹å®šæ•°æ®ç‰¹æ€§çš„æ¨¡å‹è¡Œä¸ºè¿›è¡Œæ•æ„Ÿè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8439d773b7f0328a4491a33e7807838b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-602fb4d5c4642d999dbba490227cc997.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30d1001b184b05a05d4edb6db0be81d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-389f9bb7fc0f0b17e19a1ea0f7e1ea81.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e0ce1a4a79fec8d00af82d3b31392152.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  UniGen Enhanced Training & Test-Time Strategies for Unified Multimodal   Understanding and Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-21/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2de1590f7cdc3ac735530a08d72b030a.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-21  Higher fidelity perceptual image and video compression with a latent   conditioned residual denoising diffusion model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
