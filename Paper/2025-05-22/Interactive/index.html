<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive 方向最新论文已更新，请持续关注 Update in 2025-05-22  Beyond Words Multimodal LLM Knows When to Speak">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-cbeb15bbf97efd2c8e4239904defc5ce.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    49 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-22-更新"><a href="#2025-05-22-更新" class="headerlink" title="2025-05-22 更新"></a>2025-05-22 更新</h1><h2 id="Beyond-Words-Multimodal-LLM-Knows-When-to-Speak"><a href="#Beyond-Words-Multimodal-LLM-Knows-When-to-Speak" class="headerlink" title="Beyond Words: Multimodal LLM Knows When to Speak"></a>Beyond Words: Multimodal LLM Knows When to Speak</h2><p><strong>Authors:Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin</strong></p>
<p>While large language model (LLM)-based chatbots have demonstrated strong capabilities in generating coherent and contextually relevant responses, they often struggle with understanding when to speak, particularly in delivering brief, timely reactions during ongoing conversations. This limitation arises largely from their reliance on text input, lacking the rich contextual cues in real-world human dialogue. In this work, we focus on real-time prediction of response types, with an emphasis on short, reactive utterances that depend on subtle, multimodal signals across vision, audio, and text. To support this, we introduce a new multimodal dataset constructed from real-world conversational videos, containing temporally aligned visual, auditory, and textual streams. This dataset enables fine-grained modeling of response timing in dyadic interactions. Building on this dataset, we propose MM-When2Speak, a multimodal LLM-based model that adaptively integrates visual, auditory, and textual context to predict when a response should occur, and what type of response is appropriate. Experiments show that MM-When2Speak significantly outperforms state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x improvement in response timing accuracy over leading commercial LLMs. These results underscore the importance of multimodal inputs for producing timely, natural, and engaging conversational AI. </p>
<blockquote>
<p>基于大型语言模型（LLM）的聊天机器人已展现出生成连贯且上下文相关的回复的强大能力，但它们往往难以理解和把握何时应该发言，特别是在持续对话中提供简短、及时的反应。这一局限性主要源于它们对文本输入的依赖，缺乏现实世界中人类对话的丰富上下文线索。在这项工作中，我们专注于实时预测回复类型，重点是在视觉、音频和文本等细微多模式信号的基础上进行的简短、反应性的表述。为此，我们引入了一个新的多模式数据集，该数据集来自现实世界的对话视频，包含时间对齐的视觉、听觉和文本流。该数据集能够对二元互动中的回复时间进行精细建模。基于该数据集，我们提出了MM-When2Speak，这是一个基于多模式LLM的模型，可自适应地整合视觉、听觉和文本上下文，以预测何时应发出响应以及何种类型的响应是恰当的。实验表明，MM-When2Speak显著优于最新的单模式以及LLM基线，在响应时间准确性方面，相较于领先的商业LLM，其性能提高了高达4倍。这些结果突显了多模式输入对于产生及时、自然、引人入胜的会话AI的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14654v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/lzk901372/MM-When2Speak">https://github.com/lzk901372/MM-When2Speak</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLM）在聊天机器人应用中的响应时机问题。尽管LLM能生成连贯且语境相关的回应，但它们往往难以判断何时发言，特别是在进行中的对话中提供简短、及时的反应。为解决这一问题，本文专注于实时响应类型的预测，特别是依赖于视觉、音频和文本等多模式信号的简短、反应性话语。为此，引入了一个新构建的多模式数据集，包含对齐的视听文本流，支持精细粒度的双人互动响应时间建模。在此基础上，提出了MM-When2Speak模型，该模型自适应地整合视觉、听觉和文本上下文，预测何时应该发生反应以及何种类型的反应是恰当的。实验表明，MM-When2Speak在响应时间准确性方面显著优于单模态和LLM基线，达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based chatbots虽然能生成连贯的回应，但在判断何时发言方面存在困难。</li>
<li>此困难主要源于LLM依赖文本输入，缺乏真实世界对话中的丰富上下文线索。</li>
<li>引入了一个新构建的多模式数据集，包含对齐的视听文本流，以支持精细粒度的双人互动响应时间建模。</li>
<li>MM-When2Speak模型被提出，自适应地整合视觉、听觉和文本上下文以预测最佳回应时机和类型。</li>
<li>MM-When2Speak模型在响应时间准确性方面显著优于单模态和LLM基线。</li>
<li>实验结果显示MM-When2Speak相较于领先的商业LLMs达到高达4倍的响应时间准确性提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ab68199847e3a180f9e54e416f7bf209.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-866487d01c92c1bc84e158fc4af5eb50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f4adf1ab29cd85682b40f587d05f615.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f1bb3793f8ffdc8f3ef1a0e79b7fdcd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Listen-Analyze-and-Adapt-to-Learn-New-Attacks-An-Exemplar-Free-Class-Incremental-Learning-Method-for-Audio-Deepfake-Source-Tracing"><a href="#Listen-Analyze-and-Adapt-to-Learn-New-Attacks-An-Exemplar-Free-Class-Incremental-Learning-Method-for-Audio-Deepfake-Source-Tracing" class="headerlink" title="Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class   Incremental Learning Method for Audio Deepfake Source Tracing"></a>Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class   Incremental Learning Method for Audio Deepfake Source Tracing</h2><p><strong>Authors:Yang Xiao, Rohan Kumar Das</strong></p>
<p>As deepfake speech becomes common and hard to detect, it is vital to trace its source. Recent work on audio deepfake source tracing (ST) aims to find the origins of synthetic or manipulated speech. However, ST models must adapt to learn new deepfake attacks while retaining knowledge of the previous ones. A major challenge is catastrophic forgetting, where models lose the ability to recognize previously learned attacks. Some continual learning methods help with deepfake detection, but multi-class tasks such as ST introduce additional challenges as the number of classes grows. To address this, we propose an analytic class incremental learning method called AnaST. When new attacks appear, the feature extractor remains fixed, and the classifier is updated with a closed-form analytical solution in one epoch. This approach ensures data privacy, optimizes memory usage, and is suitable for online training. The experiments carried out in this work show that our method outperforms the baselines. </p>
<blockquote>
<p>随着深度伪造语音的普及和难以检测，追溯其来源变得至关重要。关于音频深度伪造源追溯（ST）的最新工作旨在寻找合成或操纵语音的来源。然而，ST模型必须适应学习新的深度伪造攻击，同时保留对之前攻击的知识。一个主要的挑战是灾难性遗忘，即模型失去识别先前学习攻击的能力。一些持续学习方法有助于深度伪造检测，但ST等多类任务随着类别数量的增长引入了额外的挑战。为解决这一问题，我们提出了一种分析类增量学习方法，名为AnaST。当出现新攻击时，特征提取器保持不变，分类器在一个周期内使用封闭式解析解进行更新。这种方法确保了数据隐私，优化了内存使用，并适用于在线培训。本工作进行的实验表明，我们的方法优于基线方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14601v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>随着深度伪造语音的普及和难以检测，追溯其来源变得至关重要。近期音频深度伪造源头追踪技术旨在寻找合成或操纵语音的来源。然而，ST模型必须在适应学习新的深度伪造攻击的同时保留对旧攻击的知识。一个主要挑战是灾难性遗忘，模型会失去识别旧攻击的能力。一些持续学习方法有助于深度伪造检测，但随着类的数量增长，如ST等多类任务引入了额外的挑战。为解决此问题，我们提出了一种解析类增量学习方法——AnaST。当出现新攻击时，特征提取器保持不变，分类器通过一个封闭形式的解析解在一个周期内进行更新。此方法保证了数据隐私，优化了内存使用，适用于在线训练。实验表明，我们的方法优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度伪造语音越来越普遍且难以检测，追溯其源头变得重要。</li>
<li>音频深度伪造源头追踪技术旨在寻找合成或操纵语音的来源。</li>
<li>ST模型面临在适应新深度伪造攻击时保留旧知识的问题。</li>
<li>灾难性遗忘是模型面临的一个主要挑战，会导致模型失去识别旧攻击的能力。</li>
<li>现有持续学习方法有助于深度伪造检测，但多类任务如ST存在挑战。</li>
<li>提出的AnaST方法通过固定特征提取器和快速更新分类器来解决这些问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14601">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b991830326371f8058bb5ac5c67e4492.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1c4c3cff46f414d3855a99871894d71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d25f5086ded17b0a03c78cb2367e3ef.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ReactDiff-Latent-Diffusion-for-Facial-Reaction-Generation"><a href="#ReactDiff-Latent-Diffusion-for-Facial-Reaction-Generation" class="headerlink" title="ReactDiff: Latent Diffusion for Facial Reaction Generation"></a>ReactDiff: Latent Diffusion for Facial Reaction Generation</h2><p><strong>Authors:Jiaming Li, Sheng Wang, Xin Wang, Yitao Zhu, Honglin Xiong, Zixu Zhuang, Qian Wang</strong></p>
<p>Given the audio-visual clip of the speaker, facial reaction generation aims to predict the listener’s facial reactions. The challenge lies in capturing the relevance between video and audio while balancing appropriateness, realism, and diversity. While prior works have mostly focused on uni-modal inputs or simplified reaction mappings, recent approaches such as PerFRDiff have explored multi-modal inputs and the one-to-many nature of appropriate reaction mappings. In this work, we propose the Facial Reaction Diffusion (ReactDiff) framework that uniquely integrates a Multi-Modality Transformer with conditional diffusion in the latent space for enhanced reaction generation. Unlike existing methods, ReactDiff leverages intra- and inter-class attention for fine-grained multi-modal interaction, while the latent diffusion process between the encoder and decoder enables diverse yet contextually appropriate outputs. Experimental results demonstrate that ReactDiff significantly outperforms existing approaches, achieving a facial reaction correlation of 0.26 and diversity score of 0.094 while maintaining competitive realism. The code is open-sourced at \href{<a target="_blank" rel="noopener" href="https://github.com/Hunan-Tiger/ReactDiff%7D%7Bgithub%7D">https://github.com/Hunan-Tiger/ReactDiff}{github}</a>. </p>
<blockquote>
<p>根据说话者的视听剪辑，面部表情生成旨在预测听众的面部表情。挑战在于在平衡适当性、现实性和多样性的同时，捕捉视频和音频之间的相关性。虽然早期的工作主要集中在单模态输入或简化的反应映射上，但最近的方法，如PerFRDiff，已经探索了多模态输入和适当的反应映射的一对多性质。在这项工作中，我们提出了面部反应扩散（ReactDiff）框架，该框架独特地结合了多模态变压器和有条件的潜在空间扩散，以增强反应生成。与现有方法不同，ReactDiff利用类内和类间注意力进行精细的多模态交互，而编码器和解码器之间的潜在扩散过程则使输出多样化，但上下文恰当。实验结果表明，ReactDiff显著优于现有方法，实现了面部反应相关性为0.26，多样性得分为0.094，同时保持竞争力水平的现实性。代码已开源在<a target="_blank" rel="noopener" href="https://github.com/Hunan-Tiger/ReactDiff">https://github.com/Hunan-Tiger/ReactDiff</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14151v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于音频视觉剪辑的预测听众面部反应的生成是一个挑战，需要捕捉视频和音频之间的相关性，同时平衡适宜性、现实性和多样性。当前方法大多聚焦于单一模态输入或简化的反应映射，而近期如PerFRDiff等方法开始探索多模态输入和适当的反应映射的一对多特性。本研究提出了面部反应扩散（ReactDiff）框架，该框架通过条件扩散潜在空间独特地整合多模态转换器，增强了反应生成能力。不同于现有方法，ReactDiff利用类内和类间注意力进行精细的多模态交互，而编码器和解码器之间的潜在扩散过程则产生了多样且语境恰当的输出。实验结果证明，ReactDiff显著优于现有方法，实现了面部反应相关性0.26和多样性得分0.094，同时保持竞争力水平。代码已开源于GitHub。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该文本研究的是基于音频视觉剪辑预测听众的面部反应生成问题。</li>
<li>面临的主要挑战在于捕捉视频和音频的相关性，同时平衡适宜性、现实性和多样性。</li>
<li>当前的方法大多只关注单一模态的输入或简化的反应映射。</li>
<li>近期的方法如PerFRDiff开始探索多模态输入和一对多的反应映射特性。</li>
<li>本研究提出了面部反应扩散（ReactDiff）框架，通过条件扩散潜在空间整合多模态转换器来增强反应生成能力。</li>
<li>ReactDiff利用类内和类间注意力进行精细的多模态交互。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14151">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bdcf6d98f390c166eb5dc306c5e6402b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66226dfa9b5f0bd90671c9a2969504b8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Active-Spin-State-Derived-Descriptor-for-Hydrogen-Evolution-Reaction-Catalysis"><a href="#Active-Spin-State-Derived-Descriptor-for-Hydrogen-Evolution-Reaction-Catalysis" class="headerlink" title="Active-Spin-State-Derived Descriptor for Hydrogen Evolution Reaction   Catalysis"></a>Active-Spin-State-Derived Descriptor for Hydrogen Evolution Reaction   Catalysis</h2><p><strong>Authors:Yu Tan, Lei Li, Zi-Xuan Yang, Tao Huang, Qiao-Ling Wang, Tao Zhang, Jing-Chun Luo, Gui-Fang Huang, Wangyu Hu, Wei-Qing Huang</strong></p>
<p>Spin states are pivotal in modulating the electrocatalytic activity of transition-metal (TM)-based compounds, yet quantitatively evaluating the activity-spin state correlation remains a formidable challenge. Here, we propose an ‘activity index n’ as a descriptor, to assess the activity of the spin states for the hydrogen evolution reaction (HER). n descriptor integrates three key electronic parameters: the proportion (P), broadening range (R) and center cc of active spin state, which collectively account for the electronic structure modulation induced by both the intrinsic active site and its local coordination environment. Using 1T-phase ZrSe2-anchored TM atoms (TM&#x3D;Sc to Ni) as prototypes, we reveal that the correlation between Gibbs free energy and the n value follows a linear relation, namely, the vGH reduces as the n decreases. Notably, ZrSe2-Mn exhibits the optimal n value (-0.56), corresponding the best HER activity with a vGH of 0.04 eV closer to the thermoneutral ideal value (0 eV) than even Pt (vGH &#x3D; -0.09 eV). This relationship suggests that n is the effective descriptor of active spin state for HER of TM-based catalysts. Our study brings fundamental insights into the HER activity-spin state correlation, offering new strategies for HER catalyst design. </p>
<blockquote>
<p>自旋态在调节过渡金属（TM）基化合物的电催化活性中起着关键作用，然而，定量评估活性与自旋态之间的关联仍然是一项艰巨的挑战。在这里，我们提出一个“活性指数n”作为描述符，来评估自旋态对析氢反应（HER）的活性。n描述符集成了三个关键的电子参数：自旋态的比例（P）、扩展范围（R）和中心cc，它们共同反映了由固有活性位点及其局部配位环境引起的电子结构调制。以1T相ZrSe2锚定的TM原子（TM&#x3D;Sc至Ni）为原型，我们发现吉布斯自由能与n值之间的相关性遵循线性关系，即随着n值的减小，vGH降低。值得注意的是，ZrSe2-Mn表现出最佳的n值（-0.56），对应的HER活性最佳，其vGH为0.04 eV，比铂（vGH&#x3D;-0.09 eV）更接近热中性理想值（0 eV）。这种关系表明n是TM基催化剂HER活性的有效自旋态描述符。我们的研究深入探讨了HER活性与自旋态之间的关联，为HER催化剂的设计提供了新的策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13786v1">PDF</a> 17 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出使用“活性指数n”作为描述符，评估过渡金属（TM）基化合物中自旋态对析氢反应（HER）的活性影响。该n描述符集成了三个关键电子参数，即活性自旋态的比例（P）、范围（R）和中心（cc），这些参数共同反映了由内在活性位点和其局部配位环境引起的电子结构变化。研究以1T相ZrSe2锚定的TM原子（TM为Sc至Ni）为原型，揭示了Gibbs自由能与n值之间的线性关系，即随着n值的减小，vGH降低。其中，ZrSe2-Mn具有最佳的n值（-0.56），对应的HER活性最好，vGH接近理想热力学值（接近零），甚至比铂更为优越。这表明n是TM基催化剂用于HER的自旋状态活性的有效描述符。本文对于HER活性与自旋状态关系的研究，为催化剂设计提供了新的策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出使用“活性指数n”作为描述符评估过渡金属基化合物中自旋态对HER的活性影响。</li>
<li>n描述符集成了比例（P）、范围（R）和中心（cc）三个关键电子参数。</li>
<li>研究表明，活性自旋态的n值与Gibbs自由能之间存在线性关系。</li>
<li>ZrSe2-Mn具有最佳的n值，表现出出色的HER活性，其vGH接近理想热力学值。</li>
<li>n值可以作为评估过渡金属基催化剂在HER中活性自旋状态的有效描述符。</li>
<li>研究结果揭示了HER活性与自旋状态之间的关联，为催化剂设计提供了新的视角和策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13786">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-814872eb272025944cce7c17a0c64287.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Magnetic-field-enhanced-oxygen-reduction-reaction-for-electrochemical-hydrogen-peroxide-production-with-different-cerium-oxide-nanostructures"><a href="#Magnetic-field-enhanced-oxygen-reduction-reaction-for-electrochemical-hydrogen-peroxide-production-with-different-cerium-oxide-nanostructures" class="headerlink" title="Magnetic field-enhanced oxygen reduction reaction for electrochemical   hydrogen peroxide production with different cerium oxide nanostructures"></a>Magnetic field-enhanced oxygen reduction reaction for electrochemical   hydrogen peroxide production with different cerium oxide nanostructures</h2><p><strong>Authors:Caio Machado Fernandes, Aila O. Santos, Vanessa S. Antonin, Joao Paulo C. Moura, Aline B. Trench, Odivaldo C. Alves, Yutao Xing, Julio Cesar M. Silva, Mauro C. Santos</strong></p>
<p>We investigated cerium oxide nanoparticles of various morphologies (nanosheets, nanocubes, and nanoparticles) supported on carbon Vulcan XC-72 for the two-electron oxygen reduction reaction (ORR). It was used a continuous magnetic field (2000 Oe) for the first time in the literature. The best results were for 5% (w&#x2F;w) CeO2 for all three different morphologies, more than doubling the ring current, enhancing the hydrogen peroxide selectivity from 51% (Vulcan XC-72) to 84-89%, and modifying the onset potential to lesser negative values. The presence of the magnetic field led to even higher ring currents with 5% (w&#x2F;w) CeO$_2$, H$_2$O$_2$ selectivity from 54% (Vulcan XC-72) to 88-96% and changing even more the onset potential. Those results were correlated with the Zeeman effect, the Lorentz force, generating magnetohydrodynamic effects, the Kelvin force, and the formation of Bound Magnetic Polarons. This pioneering research introduces an innovative approach, highlighting the potential of an external continuous magnetic field. </p>
<blockquote>
<p>我们研究了不同形态（纳米片、纳米立方体和纳米颗粒）的氧化铈纳米粒子在碳质载体（Vulcan XC-72）上对两电子氧还原反应（ORR）的应用。在文献中首次使用了连续磁场（2000 Oe）。对于所有三种不同形态的氧化铈，负载量为5%（w&#x2F;w）时效果最佳，环电流增加了一倍以上，过氧化氢选择性从51%（Vulcan XC-72）提高到84-89%，起始电位改为更低的负值。磁场的存在使得负载量为5%（w&#x2F;w）的CeO₂的环电流更高，过氧化氢选择性从54%（Vulcan XC-72）提升到88-96%，起始电位发生了更多变化。这些结果与塞曼效应、洛伦兹力产生磁流体动力学效应、开尔文力和束缚磁极化子的形成有关。这项开创性的研究引入了一种创新方法，突出了外部连续磁场的应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13665v1">PDF</a> </p>
<p><strong>Summary</strong><br>     研究了不同形态（纳米片、纳米立方体和纳米颗粒）的氧化铈纳米粒子在碳质材料Vulcan XC-72上的二电子氧还原反应（ORR）。首次使用连续磁场（2000 Oe），最佳结果来自所有三种形态的5%（w&#x2F;w）CeO₂，其环电流加倍，过氧化氢选择性从Vulcan XC-72的51%提高到84%~89%，且启动电位降低到较小的负值。连续磁场导致具有更大电流性能的结果提升到了较高的范围，过氧化氢选择性从Vulcan XC-72的54%~88%~96%，启动电位也有较大变化。这些结果与塞曼效应等密切相关。此研究首次使用连续磁场表明未来应用的潜力巨大。 </p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13665">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e54f040b824b6878fe9470f2dd77b528.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploring-Emotional-Synchrony-in-Dyadic-Interactions-The-Role-of-Speech-Conditions-in-Facial-and-Vocal-Affective-Alignment"><a href="#Exploring-Emotional-Synchrony-in-Dyadic-Interactions-The-Role-of-Speech-Conditions-in-Facial-and-Vocal-Affective-Alignment" class="headerlink" title="Exploring Emotional Synchrony in Dyadic Interactions: The Role of Speech   Conditions in Facial and Vocal Affective Alignment"></a>Exploring Emotional Synchrony in Dyadic Interactions: The Role of Speech   Conditions in Facial and Vocal Affective Alignment</h2><p><strong>Authors:Von Ralph Dane Marquez Herbuela, Yukie Nagai</strong></p>
<p>Understanding how humans express and synchronize emotions across multiple communication channels particularly facial expressions and speech has significant implications for emotion recognition systems and human computer interaction. Motivated by the notion that non-overlapping speech promotes clearer emotional coordination, while overlapping speech disrupts synchrony, this study examines how these conversational dynamics shape the spatial and temporal alignment of arousal and valence across facial and vocal modalities. Using dyadic interactions from the IEMOCAP dataset, we extracted continuous emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech audio). Segments were categorized based on speech overlap, and emotional alignment was assessed using Pearson correlation, lag adjusted analysis, and Dynamic Time Warping (DTW). Across analyses, non overlapping speech was associated with more stable and predictable emotional synchrony than overlapping speech. While zero-lag correlations were low and not statistically different, non overlapping speech showed reduced variability, especially for arousal. Lag adjusted correlations and best-lag distributions revealed clearer, more consistent temporal alignment in these segments. In contrast, overlapping speech exhibited higher variability and flatter lag profiles, though DTW indicated unexpectedly tighter alignment suggesting distinct coordination strategies. Notably, directionality patterns showed that facial expressions more often preceded speech during turn-taking, while speech led during simultaneous vocalizations. These findings underscore the importance of conversational structure in regulating emotional communication and provide new insight into the spatial and temporal dynamics of multimodal affective alignment in real world interaction. </p>
<blockquote>
<p>了解人类如何通过多个沟通渠道，特别是面部表情和言语来表达和同步情绪，对于情绪识别系统和人机交互具有重要影响。受非重叠语能促进更清晰的情感协调，而重叠语会破坏同步性的观点启发，本研究探讨了这些对话动态如何影响面部和声音模态的兴奋和效价的时空对齐。我们使用了IEMOCAP数据集中的二元互动，通过EmoNet（面部视频）和基于Wav2Vec2的模型（语音音频）提取了连续的情绪估计值。根据语音重叠情况对片段进行分类，并使用Pearson相关性、滞后调整分析和动态时间弯曲（DTW）来评估情感对齐情况。综合分析表明，在非重叠语音中，情感同步更加稳定和可预测，而重叠语音则表现出较高的可变性和较平坦的滞后分布。尽管DTW意外地显示出更紧密的对齐，表明存在不同的协调策略，但总体而言，非重叠语音与更清晰的情感同步有关。值得注意的是，方向性模式显示，在轮流发言时，面部表情通常先于言语，而在同时发声时，则是言语领先。这些发现强调了对话结构在调节情感沟通中的重要性，并为现实互动中多模式情感对齐的时空动态提供了新的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13455v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了人类在多通道沟通中如何表达和同步情感，特别是面部表情和言语。研究发现非重叠的言语能够促进更清晰的情感协调，而重叠的言语则会破坏同步性。研究使用IEMOCAP数据集进行交互分析，并基于面部视频和语音音频提取连续情感估计。分析显示，非重叠的言语与更稳定和可预测的情感同步相关，而重叠的言语则表现出更高的可变性和更平坦的滞后分布。此外，还发现面部表情在转向时更常先于言语，而同时发声时则相反。这些发现强调了对话结构在情感沟通中的重要性，并为现实互动中的多模态情感对齐的时空动态提供了新的见解。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是七个关于该文本的关键见解：</p>
<ol>
<li>人类在多通道沟通中如何表达和同步情感对于情感识别系统和人机交互具有重要意义。</li>
<li>非重叠的言语有助于更清晰和同步的情感协调，相较之下，重叠的言语则会破坏情感的同步性。</li>
<li>使用IEMOCAP数据集进行交互分析，发现非重叠言语与更稳定和可预测的情感同步相关。</li>
<li>使用多种模型对连续情感进行估计，如基于面部视频的EmoNet和基于语音音频的Wav2Vec模型进行情绪判断分析。</li>
<li>通过多种方法评估情绪对齐，包括Pearson相关性、滞后调整分析和动态时间弯曲（DTW）。</li>
<li>分析显示非重叠言语条件下情感对齐的时空动态更加稳定和一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13455">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-71c4351922abfdf58fdeeef5697f161f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e7bcee293262680c3989cd14efa2f22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd6442628e634b201a98c9f17518545b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79159f9c5b15d1993045934b9b1aacf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5819d31621a2d6404925fe2d18637574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c45c3025fa58d49710d9e515e13323ff.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rethinking-Stateful-Tool-Use-in-Multi-Turn-Dialogues-Benchmarks-and-Challenges"><a href="#Rethinking-Stateful-Tool-Use-in-Multi-Turn-Dialogues-Benchmarks-and-Challenges" class="headerlink" title="Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and   Challenges"></a>Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and   Challenges</h2><p><strong>Authors:Hongru Wang, Wenyu Huang, Yufei Wang, Yuanhao Xi, Jianqiao Lu, Huan Zhang, Nan Hu, Zeming Liu, Jeff Z. Pan, Kam-Fai Wong</strong></p>
<p>Existing benchmarks that assess Language Models (LMs) as Language Agents (LAs) for tool use primarily focus on stateless, single-turn interactions or partial evaluations, such as tool selection in a single turn, overlooking the inherent stateful nature of interactions in multi-turn applications. To fulfill this gap, we propose \texttt{DialogTool}, a multi-turn dialogue dataset with stateful tool interactions considering the whole life cycle of tool use, across six key tasks in three stages: 1) \textit{tool creation}; 2) \textit{tool utilization}: tool awareness, tool selection, tool execution; and 3) \textit{role-consistent response}: response generation and role play. Furthermore, we build \texttt{VirtualMobile} – an embodied virtual mobile evaluation environment to simulate API calls and assess the robustness of the created APIs\footnote{We will use tools and APIs alternatively, there are no significant differences between them in this paper.}. Taking advantage of these artifacts, we conduct comprehensive evaluation on 13 distinct open- and closed-source LLMs and provide detailed analysis at each stage, revealing that the existing state-of-the-art LLMs still cannot perform well to use tools over long horizons. </p>
<blockquote>
<p>现有的主要评估语言模型（LMs）作为工具使用语言代理（LAs）的基准测试主要侧重于无状态的单轮交互或部分评估，如单轮中的工具选择，忽略了多轮应用中交互的固有有状态性质。为了填补这一空白，我们提出了<code>DialogTool&#39;，这是一个多轮对话数据集，它考虑了工具使用的整个生命周期中的有状态工具交互，涵盖三个阶段中的六个关键任务：1）工具创建；2）工具利用：工具意识、工具选择、工具执行；以及3）角色一致响应：生成响应和角色扮演。此外，我们建立了</code>VirtualMobile’–一个模拟API调用的嵌入式虚拟移动评估环境，以评估创建的API的稳健性。利用这些工具，我们对13种不同的开源和闭源的大型语言模型进行了全面评估，并在每个阶段进行了详细分析，揭示现有的最先进的大型语言模型仍然不能在长期内很好地使用工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13328v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文针对现有评估语言模型作为工具使用代理的基准测试进行了补充。现有基准测试主要关注无状态、单回合的互动或部分评估，如单一回合中的工具选择，忽略了多回合应用中交互的固有状态性。为此，文章提出了<code>DialogTool</code>多回合对话数据集，涵盖工具使用的全生命周期，包括六个关键任务三个阶段：工具创建、工具利用（包括工具意识、工具选择、工具执行）和角色一致响应。此外，文章还构建了<code>VirtualMobile</code>虚拟移动评估环境，以模拟API调用并评估创建的API的稳健性。通过对13种不同开源和闭源LLMs的评估，文章揭示了现有先进技术水平的LLMs在长期使用工具方面的表现仍不理想。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有语言模型评估基准存在缺陷，主要关注无状态、单回合的互动或部分评估，忽略了多回合交互中工具使用的状态性。</li>
<li>提出了<code>DialogTool</code>数据集，涵盖多回合对话中的工具使用全生命周期，包括工具创建、利用和角色一致响应。</li>
<li>构建了<code>VirtualMobile</code>虚拟移动评估环境，用于模拟API调用并评估API稳健性。</li>
<li>文章涵盖了13种不同的开源和闭源LLMs的评估。</li>
<li>评估结果揭示了现有LLMs在使用工具方面的长期表现不理想。</li>
<li>文章强调了现有语言模型在模拟真实工具使用场景中的不足，需要更全面的评估方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13328">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-729ff367b623c3142262e2b19e7bc262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da4943d9e99e4565f609b25265dd85d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d6e22d212db8dcabd04807129301a82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9938d9b16113084942e93dcb0cebc79c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HyperDet-Source-Detection-in-Hypergraphs-via-Interactive-Relationship-Construction-and-Feature-rich-Attention-Fusion"><a href="#HyperDet-Source-Detection-in-Hypergraphs-via-Interactive-Relationship-Construction-and-Feature-rich-Attention-Fusion" class="headerlink" title="HyperDet: Source Detection in Hypergraphs via Interactive Relationship   Construction and Feature-rich Attention Fusion"></a>HyperDet: Source Detection in Hypergraphs via Interactive Relationship   Construction and Feature-rich Attention Fusion</h2><p><strong>Authors:Le Cheng, Peican Zhu, Yangming Guo, Keke Tang, Chao Gao, Zhen Wang</strong></p>
<p>Hypergraphs offer superior modeling capabilities for social networks, particularly in capturing group phenomena that extend beyond pairwise interactions in rumor propagation. Existing approaches in rumor source detection predominantly focus on dyadic interactions, which inadequately address the complexity of more intricate relational structures. In this study, we present a novel approach for Source Detection in Hypergraphs (HyperDet) via Interactive Relationship Construction and Feature-rich Attention Fusion. Specifically, our methodology employs an Interactive Relationship Construction module to accurately model both the static topology and dynamic interactions among users, followed by the Feature-rich Attention Fusion module, which autonomously learns node features and discriminates between nodes using a self-attention mechanism, thereby effectively learning node representations under the framework of accurately modeled higher-order relationships. Extensive experimental validation confirms the efficacy of our HyperDet approach, showcasing its superiority relative to current state-of-the-art methods. </p>
<blockquote>
<p>超图为社会网络提供了优越的建模能力，特别是在捕捉群体现象方面，这些现象超越了谣言传播中的两两互动。现有的谣言溯源方法主要集中在二元交互上，这并不能充分应对更复杂关系结构的复杂性。本研究提出了一种基于超图的源检测新方法（HyperDet），通过交互关系构建和丰富的特征融合来实现。具体来说，我们的方法采用了一个交互关系构建模块，以准确地对用户之间的静态拓扑和动态交互进行建模，然后是一个丰富的特征融合模块，该模块可以自主地学习节点特征，并使用自注意力机制对节点进行区分，从而在准确建模的高阶关系框架下有效地学习节点表示。大量的实验验证证实了我们HyperDet方法的有效性，证明了其相对于当前最先进的方法的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12894v1">PDF</a> Accepted by IJCAI25</p>
<p><strong>Summary</strong></p>
<p>超图为社会网络提供了卓越的建模能力，尤其在捕捉超越二元交互的群体现象中的谣言传播。现有的谣言源检测主要关注二元交互，未能解决更复杂的关系结构的问题。本研究提出了一种基于超图的源检测新方法（HyperDet），通过交互式关系构建和特征丰富的注意力融合来实现。具体来说，该方法通过交互式关系构建模块准确建模用户间的静态拓扑和动态交互，再通过特征丰富的注意力融合模块自主学习节点特征，并使用自注意力机制对节点进行区分，从而有效地学习在准确建模的高阶关系下的节点表示。实验验证表明，HyperDet方法的有效性优于当前先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超图为社会网络提供卓越的建模能力，尤其是针对超越二元交互的群体现象，如谣言传播。</li>
<li>现有谣言源检测主要关注二元交互，忽视复杂关系结构，导致检测效果不佳。</li>
<li>本研究提出了一种新的基于超图的源检测方法（HyperDet）。</li>
<li>HyperDet通过交互式关系构建模块准确建模用户间的静态拓扑和动态交互。</li>
<li>特征丰富的注意力融合模块使HyperDet能够自主学习节点特征，并通过自注意力机制区分节点。</li>
<li>HyperDet在准确建模的高阶关系下学习节点表示，实验验证其有效性优于当前先进方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12894">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d92a3818fe4e71ee889bb19b61833f0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86b8e19fc2eaa7841f6171ac0e0ff50e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c89c07535149d03cc59371d4d874eaba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe589f0a5a46c8ea28e70eea71c9136b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Efficient-Implementation-of-Gaussian-Process-Regression-Accelerated-Saddle-Point-Searches-with-Application-to-Molecular-Reactions"><a href="#Efficient-Implementation-of-Gaussian-Process-Regression-Accelerated-Saddle-Point-Searches-with-Application-to-Molecular-Reactions" class="headerlink" title="Efficient Implementation of Gaussian Process Regression Accelerated   Saddle Point Searches with Application to Molecular Reactions"></a>Efficient Implementation of Gaussian Process Regression Accelerated   Saddle Point Searches with Application to Molecular Reactions</h2><p><strong>Authors:Rohit Goswami, Maxim Masterov, Satish Kamath, Alejandro Peña-Torres, Hannes Jónsson</strong></p>
<p>The task of locating first order saddle points on high-dimensional surfaces describing the variation of energy as a function of atomic coordinates is an essential step for identifying the mechanism and estimating the rate of thermally activated events within the harmonic approximation of transition state theory. When combined directly with electronic structure calculations, the number of energy and atomic force evaluations needed for convergence is a primary issue. Here, we describe an efficient implementation of Gaussian process regression (GPR) acceleration of the minimum mode following method where a dimer is used to estimate the lowest eigenmode of the Hessian. A surrogate energy surface is constructed and updated after each electronic structure calculation. The method is applied to a test set of 500 molecular reactions previously generated by Hermez and coworkers [J. Chem. Theory Comput. 18, 6974 (2022)]. An order of magnitude reduction in the number of electronic structure calculations needed to reach the saddle point configurations is obtained by using the GPR compared to the dimer method. Despite the wide range in stiffness of the molecular degrees of freedom, the calculations are carried out using Cartesian coordinates and are found to require similar number of electronic structure calculations as an elaborate internal coordinate method implemented in the Sella software package. The present implementation of the GPR surrogate model in C++ is efficient enough for the wall time of the saddle point searches to be reduced in 3 out of 4 cases even though the calculations are carried out at a low Hartree-Fock level. </p>
<blockquote>
<p>确定描述能量随原子坐标变化的高维表面上一阶鞍点位置的任务是识别机制和估算过渡态理论谐波近似中热激活事件速率的必要步骤。当与电子结构计算直接结合时，达到收敛所需的能量和原子力评估次数是主要问题。在这里，我们描述了高斯过程回归（GPR）加速最小模式跟随方法的有效实现，其中二聚体用于估计Hessian的最低本征模式。在每次电子结构计算后，构建并更新替代能量表面。该方法应用于Hermez及其同事之前生成的500个分子反应的测试集（J. Chem. Theory Comput. 18, 6974 (2022)）。与使用二聚体方法相比，通过GPR达到鞍点配置所需的电子结构计算次数减少了数量级。尽管分子自由度刚度范围广泛，但计算采用笛卡尔坐标，并且发现所需的电子结构计算次数与在Sella软件包中实现的高级内部坐标方法相当。本实施中的GPR替代模型C++程序足够高效，在四种情况下减少三种情况的鞍点搜索壁时间，尽管这些计算是在低Hartree-Fock水平下进行的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12519v1">PDF</a> 13 pages, 4 figures</p>
<p><strong>Summary</strong><br>     本文描述了一种利用高斯过程回归（GPR）加速寻找高维表面上的一阶鞍点的方法。该方法结合电子结构计算，使用二聚体来估计海森矩阵的最低本征模式，构建并更新代理能量表面。应用于Hermez和同事生成的500个分子反应测试集，与二聚体方法相比，使用GPR后电子结构计算的数量减少了一个数量级。即使分子自由度刚度范围广泛，使用笛卡尔坐标的计算也与Sella软件包中实现的复杂内部坐标方法所需电子结构计算数量相似。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章介绍了一种高效的高斯过程回归（GPR）方法，用于加速寻找描述能量随原子坐标变化的高维表面上的第一阶鞍点。</li>
<li>该方法结合了电子结构计算，重点解决了能量和原子力评估的收敛性问题。</li>
<li>通过使用二聚体来估计海森矩阵的最低本征模式，构建了代理能量表面，并每次电子结构计算后都会更新该表面。</li>
<li>在Hermez和同事生成的分子反应测试集上应用此方法，与二聚体方法相比，GPR的使用使得电子结构计算的数量减少了一个数量级。</li>
<li>该方法既适用于笛卡尔坐标，也适用于复杂的内部坐标方法，且所需电子结构计算的数量相似。</li>
<li>文章指出，即使在低Hartree-Fock水平下进行计算，GPRS的当前实现也足够高效，可在3出4的情况下减少鞍点搜索的墙壁时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12519">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a9cd50483f465c3525c8d5f86bfb4a8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02d339c7a8993358340c680cd5e488f2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Generation-of-Drug-Induced-Cardiac-Reactions-towards-Virtual-Clinical-Trials"><a href="#Generation-of-Drug-Induced-Cardiac-Reactions-towards-Virtual-Clinical-Trials" class="headerlink" title="Generation of Drug-Induced Cardiac Reactions towards Virtual Clinical   Trials"></a>Generation of Drug-Induced Cardiac Reactions towards Virtual Clinical   Trials</h2><p><strong>Authors:Qian Shao, Bang Du, Zepeng Li, Qiyuan Chen, Hongxia Xu, Jimeng Sun, Jian Wu, Jintai Chen</strong></p>
<p>Clinical trials remain critical in cardiac drug development but face high failure rates due to efficacy limitations and safety risks, incurring substantial costs. In-silico trial methodologies, particularly generative models simulating drug-induced electrocardiogram (ECG) alterations, offer a potential solution to mitigate these challenges. While existing models show progress in ECG synthesis, their constrained fidelity and inability to characterize individual-specific pharmacological response patterns fundamentally limit clinical translatability. To address these issues, we propose a novel Drug-Aware Diffusion Model (DADM). Specifically, we construct a set of ordinary differential equations to provide external physical knowledge (EPK) of the realistic ECG morphology. The EPK is used to adaptively constrain the morphology of the generated ECGs through a dynamic cross-attention (DCA) mechanism. Furthermore, we propose an extension of ControlNet to incorporate demographic and drug data, simulating individual drug reactions. Compared to the other eight state-of-the-art (SOTA) ECG generative models: 1) Quantitative and expert evaluation demonstrate that DADM generates ECGs with superior fidelity; 2) Comparative results on two real-world databases covering 8 types of drug regimens verify that DADM can more accurately simulate drug-induced changes in ECGs, improving the accuracy by at least 5.79% and recall by 8%. In addition, the ECGs generated by DADM can also enhance model performance in downstream drug-effect classification tasks. </p>
<blockquote>
<p>临床试验在心脏药物开发中仍然至关重要，但由于疗效局限和安全风险，面临较高的失败率，产生了巨大的成本。计算机模拟试验方法，特别是模拟药物引起的心电图（ECG）变化的生成模型，为解决这些挑战提供了潜在的解决方案。尽管现有模型在心电图合成方面取得了一定的进展，但它们有限的保真度和无法表征个体特定药理反应模式的能力从根本上限制了其在临床上的可翻译性。为了解决这些问题，我们提出了一种新型的药品感知扩散模型（DADM）。具体来说，我们构建了一组常微分方程，以提供关于现实心电图形态的外部物理知识（EPK）。EPK通过动态交叉注意（DCA）机制自适应地约束生成的心电图的形态。此外，我们对ControlNet进行了扩展，以纳入人口统计学和药物数据，模拟个体药物反应。与其他八种先进的心电图生成模型相比：1）定量和专家评估表明，DADM生成的心电图具有更高的保真度；2）在两个涵盖8种药物类型真实世界数据库的比较结果表明，DADM更能准确地模拟药物引起的心电图变化，至少提高准确性5.79%，召回率提高8%。此外，由DADM生成的心电图还可以提高下游药物效应分类任务的模型性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07297v2">PDF</a> Under review</p>
<p><strong>Summary</strong><br>     临床试验在心脏药物开发中至关重要，但存在高失败率，成本高昂，安全风险及药效局限问题。为解决此问题，该研究提出了利用硅仿真模拟方法尝试药物对心电图影响的模型预测来优化这一过程。现有的模型虽然能合成心电图，但在真实性和个体差异表征上存在局限。本研究提出一种新型药物感知扩散模型（DADM），通过构建普通微分方程提供真实心电图形态的外部物理知识（EPK），并利用动态交叉注意力机制自适应约束生成心电图的形态。此外，该研究还扩展了ControlNet模型以纳入人口统计学数据和药物数据模拟个体药物反应。相较于其他八种顶尖心电图生成模型，本研究不仅在定量评估和专家评估中展现出更高的保真度，还能更准确地模拟药物引起的心电图变化，准确率至少提高5.79%，召回率提高8%，并能在下游药物效应分类任务中提升模型性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>临床试验在心脏药物开发中面临高失败率、成本高昂等问题。</li>
<li>硅仿真模拟方法是一种潜在解决方案，旨在优化药物开发过程中的挑战。</li>
<li>现有心电图合成模型存在真实性和个体差异表征的局限。</li>
<li>新型药物感知扩散模型（DADM）通过结合外部物理知识（EPK）和动态交叉注意力机制提高心电图生成的保真度。</li>
<li>DADM模型相较于其他顶尖模型在定量评估和专家评估中表现更优秀。</li>
<li>DADM能更准确地模拟药物引起的心电图变化，提高预测准确率至少5.79%，召回率提高8%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07297">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d6dc06153ce3d10926cd9dfe3dd4f28e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-509e557ce802cd40efc51fd38273b042.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05190536d6e452035d34c8b1cae59892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da1d1a0ae95c3ef6510d727ca1b9361c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7750e7810f4f222f8d1f864e831ed9ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbeb15bbf97efd2c8e4239904defc5ce.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Advancing-Multi-Party-Dialogue-Framework-with-Speaker-ware-Contrastive-Learning"><a href="#Advancing-Multi-Party-Dialogue-Framework-with-Speaker-ware-Contrastive-Learning" class="headerlink" title="Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive   Learning"></a>Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive   Learning</h2><p><strong>Authors:Zhongtian Hu, Qi He, Ronghan Li, Meng Zhao, Lifang Wang</strong></p>
<p>Multi-party dialogues, common in collaborative scenarios like brainstorming sessions and negotiations, pose significant challenges due to their complexity and diverse speaker roles. Current methods often use graph neural networks to model dialogue context, capturing structural dynamics but heavily relying on annotated graph structures and overlooking individual speaking styles. To address these challenges, we propose CMR, a Contrastive learning-based Multi-party dialogue Response generation framework. CMR employs a two-stage self-supervised contrastive learning framework. First, it captures global differences in speaking styles across individuals. Then, it focuses on intra-conversation comparisons to identify thematic transitions and contextually relevant facts. To the best of our knowledge, this is the first approach that applies contrastive learning in multi-party dialogue generation. Experimental results demonstrate that CMR not only significantly outperforms state-of-the-art models, but also generalizes well to large pre-trained language models, effectively enhancing their capability in handling multi-party conversations. </p>
<blockquote>
<p>在头脑风暴和谈判等协作场景中常见的多方对话，由于其复杂性和多样的说话者角色，构成了重大挑战。当前的方法通常使用图神经网络来建模对话上下文，捕捉结构动态，但过度依赖注释图结构，忽视了个人说话风格。为了解决这些挑战，我们提出了基于对比学习的多方对话响应生成框架CMR。CMR采用两阶段自监督对比学习框架。首先，它捕捉不同个体之间说话风格的全球差异。然后，它专注于会话内部的比较，以识别主题转换和上下文相关事实。据我们所知，这是首次将对比学习应用于多方对话生成的方法。实验结果表明，CMR不仅显著优于现有最先进的模型，而且在大型预训练语言模型中也能很好地通用化，有效地增强了它们处理多方对话的能力通辽的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11292v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于对比学习的多轮对话响应生成框架CMR用于处理多方对话中的复杂性和多样化的说话角色挑战。该方法通过两个阶段进行自我监督对比学习，先捕捉个体间全局说话风格差异，再关注对话内部比较以识别主题转换和上下文相关事实。实验结果表明，该方法不仅显著优于现有模型，而且能够很好地应用于大型预训练语言模型，有效提高它们处理多方对话的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多方对话在协作场景中如头脑风暴和谈判中普遍存在，但由于其复杂性和多样化的说话角色而具有挑战。</li>
<li>当前方法主要使用图神经网络来建模对话上下文，但过于依赖注释图结构，忽略了个人说话风格。</li>
<li>CMR框架采用基于对比学习的方法，旨在解决这些问题。</li>
<li>CMR通过两个阶段进行自我监督对比学习：首先捕捉个体间的全局说话风格差异，然后关注对话内的主题转换和上下文相关事实。</li>
<li>CMR是首个将对比学习应用于多轮对话生成的方法。</li>
<li>实验结果表明，CMR显著优于现有模型，并能很好地应用于大型预训练语言模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11292">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c2d3f6de4804259770c316cacf10688f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-929d3c3c2b10a47db76088b32e5284c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60a08f3cce8e8d555d7cf3cc1d829b12.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1fcc85ed6ef90271d86f81902a621a1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Stability-in-Reaction-Network-Models-via-an-Extension-of-the-Next-Generation-Matrix-Method"><a href="#Stability-in-Reaction-Network-Models-via-an-Extension-of-the-Next-Generation-Matrix-Method" class="headerlink" title="Stability in Reaction Network Models via an Extension of the Next   Generation Matrix Method"></a>Stability in Reaction Network Models via an Extension of the Next   Generation Matrix Method</h2><p><strong>Authors:Florin Avram, Rim Adenane, Andrei D. Halanay, Matthew D. Johnson</strong></p>
<p>In this essay, we investigate some relations between Chemical Reaction Networks (CRN) and Mathematical Epidemiology (ME) and report on several pleasant surprises which we had simply by putting these two topics together.   Firstly, we propose a definition of ME models as a subset of CRN models.   Secondly, we review a fundamental stability result for boundary points, known in ME as the NGM method since it replaces the investigation of the Jacobian by that of a matrix whose origins lie in probability (the theory of branching processes). This important result seems to be little known outside of ME; even in ME, it has not been made clear before that the method gets sometimes the right answer, even though the conditions of the NGM theorem are not all satisfied. Thus, beyond the theorem, there is a heuristic approach, the validity conditions for which are not sufficiently understood.   Thirdly, we show that some simple CRN models with absolute concentration robustness (ACR), are close qualitatively to simple ME models, in the sense that they have an unique disease free equilibrium, and a unique interior fixed point, and the latter enters the positive domain and becomes stable precisely when $R_0:&#x3D;s_{dfe} \mathcal{R}&#x3D;\frac{s_{dfe}}{s_e}&gt;1.$   (where $s$ denotes the “ACR species”). Thus, for these “ME type models”, a “relay phenomena” takes place: precisely when the DFE loses stability, a new fixed point enters the domain, and takes over.   Last but not least, we offer in the associated GitHub repository <a target="_blank" rel="noopener" href="https://github.com/adhalanay/epidemiology_crns">https://github.com/adhalanay/epidemiology_crns</a> a Mathematica package, Epid-CRN, which is addressed to researchers of both disciplines, and provide illustrative notebooks, which in particular solve a few minor open ME and CRN problems. Our package may also be used to study easy cases of analogue continuous time Markov chain (CTMC) ME and CRN models. </p>
<blockquote>
<p>在这篇论文中，我们探讨了化学反应网络（CRN）和数学流行病学（ME）之间的关系，并报告了将这两个主题结合起来所得到的几个令人愉快的惊喜。首先，我们提出将ME模型定义为CRN模型的一个子集。其次，我们回顾了边界点的基本稳定性结果，在ME中称为NGM方法，因为它用源于概率的矩阵（分支过程理论）来代替雅可比矩阵的研究。这个重要的结果似乎在数学流行病学领域之外鲜为人知；即使在ME领域，之前也没有明确过该方法有时即使不满足NGM定理的条件也能得出正确的答案。因此，除了定理之外，还存在一种启发式方法，但其有效条件尚未得到充分理解。第三，我们展示了一些具有绝对浓度稳健性（ACR）的简单CRN模型在定性上与简单的ME模型紧密相关，它们具有一个独特的无疾病平衡点和唯一的内部固定点，当$R_0:&#x3D;s_{dfe} \mathcal{R}&#x3D;\frac{s_{dfe}}{s_e}&gt;1$时，后者进入正域并变得稳定。（其中s表示“ACR物种”）。因此，对于这些“ME类型模型”，会发生一种“接力现象”：当无疾病平衡点失去稳定性时，一个新的固定点进入域并接管。最后但并非最不重要的是，我们在相关的GitHub仓库<a target="_blank" rel="noopener" href="https://github.com/adhalanay/epidemiology_crns%E4%B8%AD%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%80%E4%B8%AA%E5%90%8D%E4%B8%BAEpid-CRN%E7%9A%84Mathematica%E8%BD%AF%E4%BB%B6%E5%8C%85%EF%BC%8C%E8%AF%A5%E8%BD%AF%E4%BB%B6%E5%8C%85%E9%9D%A2%E5%90%91%E4%B8%A4%E4%B8%AA%E5%AD%A6%E7%A7%91%E7%9A%84%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%EF%BC%8C%E5%B9%B6%E6%8F%90%E4%BE%9B%E8%AF%B4%E6%98%8E%E6%80%A7%E7%AC%94%E8%AE%B0%E6%9C%AC%EF%BC%8C%E7%89%B9%E5%88%AB%E8%A7%A3%E5%86%B3%E4%BA%86%E4%B8%80%E4%BA%9B%E8%BD%BB%E5%BE%AE%E7%9A%84ME%E5%92%8CCRN%E9%97%AE%E9%A2%98%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%8C%85%E4%B9%9F%E5%8F%AF%E7%94%A8%E4%BA%8E%E7%A0%94%E7%A9%B6%E7%B1%BB%E4%BC%BC%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE%EF%BC%88CTMC%EF%BC%89%E7%9A%84ME%E5%92%8CCRN%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%AE%80%E5%8D%95%E6%83%85%E5%86%B5%E3%80%82">https://github.com/adhalanay/epidemiology_crns中提供了一个名为Epid-CRN的Mathematica软件包，该软件包面向两个学科的研究人员，并提供说明性笔记本，特别解决了一些轻微的ME和CRN问题。我们的软件包也可用于研究类似连续时间马尔可夫链（CTMC）的ME和CRN模型的简单情况。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11867v2">PDF</a> Major review. Title was changed</p>
<p><strong>Summary</strong></p>
<p>本文探讨了化学反应网络（CRN）与数学流行病学（ME）之间的关系，并报告了将这两个主题结合起来所得到的几个意外收获。文章定义了ME模型作为CRN模型的一个子集，介绍了ME中的NGM方法的稳定性结果，该方法通过概率理论中的分支过程理论替代了雅可比矩阵的研究。此外，文章展示了具有绝对浓度稳健性的简单CRN模型与简单ME模型之间的定性相似性，并提出了一种“接力现象”。最后，作者提供了一个针对两个学科的研究人员的Mathematica软件包Epid-CRN，并提供了一些解决问题的示例笔记本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章探讨了化学反应网络（CRN）与数学流行病学（ME）之间的关联，将两者结合起来得到了意外收获。</li>
<li>文章定义了ME模型是CRN模型的一个子集。</li>
<li>介绍了ME中的NGM方法的稳定性结果，该方法通过概率理论中的分支过程理论进行研究，但该方法的有效性条件尚不够充分理解。</li>
<li>简单CRN模型与简单ME模型之间存在定性相似性，表现在它们具有独特的无疾病平衡点和内部固定点等方面。</li>
<li>当DFE失去稳定性时，会出现一种“接力现象”，即新的固定点进入领域并接管。</li>
<li>文章提供了一个Mathematica软件包Epid-CRN，供两个学科的研究人员使用，其中包括解决一些开放问题的示例笔记本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11867">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49b3ea30b8f4569d4bd62f13983105c4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d889979738b094133ac240d75f0a4394.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-05-22  ClapFM-EVC High-Fidelity and Flexible Emotional Voice Conversion with   Dual Control from Natural Language and Speech
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bf76b9501af682faaab986e5985edde6.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-05-22  FMSD-TTS Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ü-Tsang, Amdo and Kham Speech Dataset Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23523.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
