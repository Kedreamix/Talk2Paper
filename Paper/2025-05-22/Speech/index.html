<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Vox-Profile A Speech Foundation Model Benchmark for Characterizing   Diverse Speaker and Speech Traits">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b4c0329fc995be23ad6682967b879b05.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    60 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-22-æ›´æ–°"><a href="#2025-05-22-æ›´æ–°" class="headerlink" title="2025-05-22 æ›´æ–°"></a>2025-05-22 æ›´æ–°</h1><h2 id="Vox-Profile-A-Speech-Foundation-Model-Benchmark-for-Characterizing-Diverse-Speaker-and-Speech-Traits"><a href="#Vox-Profile-A-Speech-Foundation-Model-Benchmark-for-Characterizing-Diverse-Speaker-and-Speech-Traits" class="headerlink" title="Vox-Profile: A Speech Foundation Model Benchmark for Characterizing   Diverse Speaker and Speech Traits"></a>Vox-Profile: A Speech Foundation Model Benchmark for Characterizing   Diverse Speaker and Speech Traits</h2><p><strong>Authors:Tiantian Feng, Jihwan Lee, Anfeng Xu, Yoonjeong Lee, Thanathai Lertpetchpun, Xuan Shi, Helin Wang, Thomas Thebaud, Laureano Moro-Velazquez, Dani Byrd, Najim Dehak, Shrikanth Narayanan</strong></p>
<p>We introduce Vox-Profile, a comprehensive benchmark to characterize rich speaker and speech traits using speech foundation models. Unlike existing works that focus on a single dimension of speaker traits, Vox-Profile provides holistic and multi-dimensional profiles that reflect both static speaker traits (e.g., age, sex, accent) and dynamic speech properties (e.g., emotion, speech flow). This benchmark is grounded in speech science and linguistics, developed with domain experts to accurately index speaker and speech characteristics. We report benchmark experiments using over 15 publicly available speech datasets and several widely used speech foundation models that target various static and dynamic speaker and speech properties. In addition to benchmark experiments, we showcase several downstream applications supported by Vox-Profile. First, we show that Vox-Profile can augment existing speech recognition datasets to analyze ASR performance variability. Vox-Profile is also used as a tool to evaluate the performance of speech generation systems. Finally, we assess the quality of our automated profiles through comparison with human evaluation and show convergent validity. Vox-Profile is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/vox-profile-release">https://github.com/tiantiaf0627/vox-profile-release</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Vox-Profileï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹æ¥è¡¨å¾ä¸°å¯Œçš„è¯´è¯äººå’Œè¯­éŸ³ç‰¹å¾ã€‚ä¸ç°æœ‰ä¸“æ³¨äºå•ä¸€ç»´åº¦è¯´è¯äººç‰¹å¾çš„å·¥ä½œä¸åŒï¼ŒVox-Profileæä¾›äº†å…¨é¢ä¸”å¤šç»´åº¦çš„ç‰¹å¾æè¿°ï¼Œè¿™äº›æè¿°æ—¢åæ˜ äº†é™æ€çš„è¯´è¯äººç‰¹å¾ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€å£éŸ³ï¼‰ï¼Œä¹Ÿåæ˜ äº†åŠ¨æ€çš„è¯­éŸ³å±æ€§ï¼ˆå¦‚æƒ…æ„Ÿã€è¯­é€Ÿï¼‰ã€‚è¯¥åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨è¯­éŸ³ç§‘å­¦å’Œè¯­è¨€å­¦çš„åŸºç¡€ä¸Šï¼Œä¸é¢†åŸŸä¸“å®¶åˆä½œå¼€å‘ï¼Œä»¥å‡†ç¡®ç´¢å¼•è¯´è¯äººå’Œè¯­éŸ³ç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨è¶…è¿‡15ä¸ªå…¬å¼€å¯ç”¨çš„è¯­éŸ³æ•°æ®é›†å’Œå‡ ä¸ªé’ˆå¯¹å„ç§é™æ€å’ŒåŠ¨æ€è¯´è¯äººå’Œè¯­éŸ³å±æ€§çš„å¹¿æ³›ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•å®éªŒã€‚é™¤äº†åŸºå‡†æµ‹è¯•å®éªŒå¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†Vox-Profileæ”¯æŒçš„å‡ ä¸ªä¸‹æ¸¸åº”ç”¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜Vox-Profileå¯ä»¥æ‰©å……ç°æœ‰çš„è¯­éŸ³è¯†åˆ«æ•°æ®é›†ï¼Œåˆ†æASRæ€§èƒ½å˜åŒ–ã€‚Vox-Profileè¿˜è¢«ç”¨ä½œè¯„ä¼°è¯­éŸ³ç”Ÿæˆç³»ç»Ÿæ€§èƒ½çš„å·¥å…·ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä¸äººç±»è¯„ä¼°çš„æ¯”è¾ƒæ¥è¯„ä¼°æˆ‘ä»¬çš„è‡ªåŠ¨åˆ†æç»“æœçš„å‡†ç¡®æ€§ï¼Œå¹¶å±•ç¤ºå…¶æ”¶æ•›æœ‰æ•ˆæ€§ã€‚Vox-Profileå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/vox-profile-release%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/tiantiaf0627/vox-profile-releaseè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Vox-Profileæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºåˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹æ¥åˆ»ç”»ä¸°å¯Œçš„è¯´è¯äººå’Œè¯­éŸ³ç‰¹å¾ã€‚å®ƒä¸åŒäºç°æœ‰ä¸“æ³¨äºå•ä¸€ç»´åº¦çš„è¯´è¯äººç‰¹å¾çš„å·¥ä½œï¼Œè€Œæ˜¯æä¾›åæ˜ é™æ€è¯´è¯äººç‰¹å¾ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€å£éŸ³ï¼‰å’ŒåŠ¨æ€è¯­éŸ³å±æ€§ï¼ˆå¦‚æƒ…æ„Ÿã€è¯­é€Ÿï¼‰çš„å…¨æ–¹ä½å¤šç»´åº¦çš„åˆ†æã€‚è¯¥åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨è¯­éŸ³ç§‘å­¦å’Œè¯­è¨€å­¦çš„åŸºç¡€ä¸Šï¼Œä¸é¢†åŸŸä¸“å®¶åˆä½œå¼€å‘ï¼Œä»¥å‡†ç¡®ç´¢å¼•è¯´è¯äººå’Œè¯­éŸ³ç‰¹å¾ã€‚ä½¿ç”¨è¶…è¿‡15ä¸ªå…¬å¼€å¯ç”¨çš„è¯­éŸ³æ•°æ®é›†å’Œå‡ ä¸ªé’ˆå¯¹å„ç§é™æ€å’ŒåŠ¨æ€è¯´è¯äººå’Œè¯­éŸ³å±æ€§çš„å¹¿æ³›ä½¿ç”¨çš„è¯­éŸ³åŸºç¡€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•å®éªŒã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†Vox-Profileæ”¯æŒçš„å‡ ä¸ªä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚å¢å¼ºç°æœ‰è¯­éŸ³è¯†åˆ«æ•°æ®é›†çš„åˆ†æèƒ½åŠ›ã€è¯„ä¼°è¯­éŸ³ç”Ÿæˆç³»ç»Ÿçš„æ€§èƒ½ä»¥åŠé€šè¿‡ä¸äººç±»è¯„ä¼°æ¯”è¾ƒæ¥è¯„ä¼°è‡ªåŠ¨åŒ–åˆ†æçš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vox-Profileæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨åˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹è¡¨å¾ä¸°å¯Œçš„è¯´è¯äººå’Œè¯­éŸ³ç‰¹å¾ã€‚</li>
<li>å®ƒæä¾›é™æ€ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€å£éŸ³ï¼‰å’ŒåŠ¨æ€ï¼ˆå¦‚æƒ…æ„Ÿã€è¯­é€Ÿï¼‰çš„å¤šç»´åº¦åˆ†æã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨è¯­éŸ³ç§‘å­¦å’Œè¯­è¨€å­¦çš„åŸºç¡€ä¸Šï¼Œä¸é¢†åŸŸä¸“å®¶åˆä½œå¼€å‘ã€‚</li>
<li>ä½¿ç”¨å¤šä¸ªå…¬å¼€æ•°æ®é›†å’Œå¹¿æ³›ä½¿ç”¨çš„è¯­éŸ³åŸºç¡€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•å®éªŒã€‚</li>
<li>Vox-Profileå¯å¢å¼ºç°æœ‰è¯­éŸ³è¯†åˆ«æ•°æ®é›†çš„åˆ†æèƒ½åŠ›ã€‚</li>
<li>å®ƒè¢«ç”¨æ¥è¯„ä¼°è¯­éŸ³ç”Ÿæˆç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2e2a7cee673a7a35c0a60264676616ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82663d97944f427c4590154fa6fecbe8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2429b688a934304001849483e6be02d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63f712dbd88f774113478ffb5ab9b2ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c2610087abd4e92fb942f1ba2c4eae1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dual-Precision-Quantization-for-Efficient-and-Accurate-Deep-Neural-Networks-Inference"><a href="#Dual-Precision-Quantization-for-Efficient-and-Accurate-Deep-Neural-Networks-Inference" class="headerlink" title="Dual Precision Quantization for Efficient and Accurate Deep Neural   Networks Inference"></a>Dual Precision Quantization for Efficient and Accurate Deep Neural   Networks Inference</h2><p><strong>Authors:Tomer Gafni, Asaf Karnieli, Yair Hanani</strong></p>
<p>Deep neural networks have achieved state-of-the-art results in a wide range of applications, from natural language processing and computer vision to speech recognition. However, as tasks become increasingly complex, model sizes continue to grow, posing challenges in latency and memory efficiency. To meet these constraints, post-training quantization has emerged as a promising solution. In this paper, we propose a novel hardware-efficient quantization and inference scheme that exploits hardware advantages with minimal accuracy degradation. Specifically, we introduce a W4A8 scheme, where weights are quantized and stored using 4-bit integer precision, and inference computations are performed using 8-bit floating-point arithmetic, demonstrating significant speedups and improved memory utilization compared to 16-bit operations, applicable on various modern accelerators. To mitigate accuracy loss, we develop a novel quantization algorithm, dubbed Dual Precision Quantization (DPQ), that leverages the unique structure of our scheme without introducing additional inference overhead. Experimental results demonstrate improved performance (i.e., increased throughput) while maintaining tolerable accuracy degradation relative to the full-precision model. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨ä¼—å¤šåº”ç”¨ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œè¯­éŸ³è¯†åˆ«ã€‚ç„¶è€Œï¼Œéšç€ä»»åŠ¡çš„å¤æ‚æ€§ä¸æ–­å¢åŠ ï¼Œæ¨¡å‹è§„æ¨¡æŒç»­æ‰©å¤§ï¼Œè¿™å¯¹å»¶è¿Ÿå’Œå†…å­˜æ•ˆç‡æå‡ºäº†æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›çº¦æŸï¼Œè®­ç»ƒåé‡åŒ–ä½œä¸ºä¸€ç§å…·æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¡¬ä»¶é«˜æ•ˆé‡åŒ–å’Œæ¨ç†æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨ç¡¬ä»¶ä¼˜åŠ¿ï¼Œä»¥æœ€å°çš„ç²¾åº¦æŸå¤±ä¸ºç›®æ ‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†W4A8æ–¹æ¡ˆï¼Œå…¶ä¸­æƒé‡ä½¿ç”¨4ä½æ•´æ•°ç²¾åº¦è¿›è¡Œé‡åŒ–å’Œå­˜å‚¨ï¼Œæ¨ç†è®¡ç®—ä½¿ç”¨8ä½æµ®ç‚¹ç®—æœ¯æ‰§è¡Œï¼Œä¸16ä½æ“ä½œç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆåœ¨å¤šç§ç°ä»£åŠ é€Ÿå™¨ä¸Šå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œå†…å­˜åˆ©ç”¨ç‡æ”¹å–„ã€‚ä¸ºäº†ç¼“è§£ç²¾åº¦æŸå¤±ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹é‡åŒ–ç®—æ³•ï¼Œç§°ä¸ºåŒç²¾åº¦é‡åŒ–ï¼ˆDPQï¼‰ï¼Œè¯¥ç®—æ³•åˆ©ç”¨äº†æˆ‘ä»¬æ–¹æ¡ˆçš„ç‹¬ç‰¹ç»“æ„ï¼Œè€Œæ²¡æœ‰å¼•å…¥é¢å¤–çš„æ¨ç†å¼€é”€ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨æé«˜æ€§èƒ½ï¼ˆå³æé«˜ååé‡ï¼‰çš„åŒæ—¶ï¼Œç›¸å¯¹äºå…¨ç²¾åº¦æ¨¡å‹ï¼Œç²¾åº¦æŸå¤±çš„å®¹å¿åº¦å¾—åˆ°äº†ä¿æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14638v1">PDF</a> Accepted at eLVM Workshop, CVPR, 2025</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ç¡¬ä»¶é«˜æ•ˆé‡åŒ–åŠæ¨ç†æ–¹æ¡ˆï¼Œåˆ©ç”¨ç¡¬ä»¶ä¼˜åŠ¿å®ç°æœ€ä½ç²¾åº¦æŸå¤±ã€‚é€šè¿‡W4A8æ–¹æ¡ˆï¼Œæƒé‡ä»¥4ä½æ•´æ•°ç²¾åº¦é‡åŒ–å­˜å‚¨ï¼Œæ¨ç†è®¡ç®—åˆ™é‡‡ç”¨8ä½æµ®ç‚¹è¿ç®—ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„16ä½æ“ä½œï¼Œæ˜¾è‘—æå‡äº†è¿ç®—é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨ç‡ã€‚åŒæ—¶ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ç§æ–°å‹é‡åŒ–ç®—æ³•â€”â€”åŒç²¾åº¦é‡åŒ–ï¼ˆDPQï¼‰ï¼Œèƒ½åœ¨ä¸å¢åŠ æ¨ç†å¼€é”€çš„å‰æä¸‹ï¼Œå……åˆ†åˆ©ç”¨è¯¥æ–¹æ¡ˆçš„ç‹¬ç‰¹ç»“æ„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æé«˜æ€§èƒ½ï¼ˆå³æå‡ååé‡ï¼‰çš„åŒæ—¶ï¼Œç›¸è¾ƒäºå…¨ç²¾åº¦æ¨¡å‹ï¼Œç²¾åº¦æŸå¤±åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å¤šä¸ªåº”ç”¨é¢†åŸŸä¸­å®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä½†éšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼Œæ¨¡å‹è§„æ¨¡çš„å¢é•¿å¸¦æ¥äº†å»¶è¿Ÿå’Œå†…å­˜æ•ˆç‡çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œè®­ç»ƒåé‡åŒ–ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆå‡ºç°ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¡¬ä»¶é«˜æ•ˆé‡åŒ–åŠæ¨ç†æ–¹æ¡ˆï¼Œåˆ©ç”¨ç¡¬ä»¶ä¼˜åŠ¿å®ç°æœ€ä½ç²¾åº¦æŸå¤±ã€‚</li>
<li>å¼•å…¥W4A8æ–¹æ¡ˆï¼Œæƒé‡ä»¥4ä½æ•´æ•°ç²¾åº¦é‡åŒ–å­˜å‚¨ï¼Œæ¨ç†è®¡ç®—é‡‡ç”¨8ä½æµ®ç‚¹è¿ç®—ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹é‡åŒ–ç®—æ³•â€”â€”åŒç²¾åº¦é‡åŒ–ï¼ˆDPQï¼‰ï¼Œå……åˆ†åˆ©ç”¨è¯¥æ–¹æ¡ˆçš„ç‹¬ç‰¹ç»“æ„ä¸”æ— éœ€å¢åŠ é¢å¤–çš„æ¨ç†å¼€é”€ã€‚</li>
<li>è¯¥æ–¹æ¡ˆåœ¨å¤šç§ç°ä»£åŠ é€Ÿå™¨ä¸Šå¯å®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œå†…å­˜åˆ©ç”¨æ”¹å–„ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºåœ¨æé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œç›¸è¾ƒäºå…¨ç²¾åº¦æ¨¡å‹ï¼Œç²¾åº¦æŸå¤±è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aab430fdfadc42dce291c0ffb044d8bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d95ab34e842ab3a269e4803c76de555.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f511b32c421c45448d03c4c5748b7b2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach"><a href="#Mitigating-Subgroup-Disparities-in-Multi-Label-Speech-Emotion-Recognition-A-Pseudo-Labeling-and-Unsupervised-Learning-Approach" class="headerlink" title="Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach"></a>Mitigating Subgroup Disparities in Multi-Label Speech Emotion   Recognition: A Pseudo-Labeling and Unsupervised Learning Approach</h2><p><strong>Authors:Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee</strong></p>
<p>While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 26% improvement in fairness metrics with a drop of less than 4% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential in scenarios where explicit demographic information is unavailable. </p>
<blockquote>
<p>è™½ç„¶å­ç¾¤ä½“å·®å¼‚å’Œæ€§èƒ½åå·®åœ¨è®¡ç®—ç ”ç©¶ä¸­å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶ï¼Œä½†åœ¨åˆ†ç±»è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„å…¬å¹³æ€§ä»ç„¶è¢«å¿½è§†ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜ç¡®çš„äººå£ç»Ÿè®¡æ ‡ç­¾ï¼Œä½†ç”±äºéšç§æ‹…å¿§ï¼Œè¿™äº›æ ‡ç­¾å¾ˆéš¾è·å¾—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéšå¼äººå£ç»Ÿè®¡æ¨æ–­ï¼ˆIDIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼ªæ ‡ç­¾å’Œé€šè¿‡k-meansèšç±»è¿›è¡Œçš„æ— ç›‘ç£å­¦ä¹ æ¥å‡è½»SERä¸­çš„åè§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¼ªæ ‡ç­¾IDIå‡å°‘äº†å­ç¾¤ä½“å·®å¼‚ï¼Œå…¬å¹³åº¦æŒ‡æ ‡æé«˜äº†33%ä»¥ä¸Šï¼Œè€ŒSERå‡†ç¡®ç‡ä¸‹é™äº†ä¸åˆ°3%ã€‚æ­¤å¤–ï¼Œæ— ç›‘ç£çš„IDIåœ¨å…¬å¹³åº¦æŒ‡æ ‡ä¸Šæé«˜äº†è¶…è¿‡2.6%ï¼ŒåŒæ—¶SERæ€§èƒ½ä¸‹é™äº†ä¸åˆ°4%ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæ— ç›‘ç£çš„IDIå§‹ç»ˆåœ¨ç§æ—å’Œå¹´é¾„å·®å¼‚æ–¹é¢æœ‰æ‰€ç¼“è§£ï¼Œè¿™æ˜¾ç¤ºäº†å…¶åœ¨æ— æ³•ä½¿ç”¨æ˜ç¡®çš„äººå£ç»Ÿè®¡ä¿¡æ¯çš„æƒ…å†µä¸‹å…·æœ‰æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14449v1">PDF</a> Accepted by InterSpeech 2025. 7 pages including 2 pages of appendix</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶å…³æ³¨è®¡ç®—ç ”ç©¶ä¸­æ—¥ç›Šçªå‡ºçš„å­ç¾¤ä½“å·®å¼‚å’Œæ€§èƒ½åè§é—®é¢˜ï¼Œæ¢è®¨äº†åˆ†ç±»è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„å…¬å¹³æ€§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–éš¾ä»¥è·å¾—çš„æ˜ç¡®äººå£ç»Ÿè®¡æ ‡ç­¾çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§éšæ€§äººå£ç»Ÿè®¡æ¨æ–­ï¼ˆIDIï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼ªæ ‡ç­¾å’Œkå‡å€¼èšç±»çš„æ— ç›‘ç£å­¦ä¹ æ¥å‡è½»SERä¸­çš„åè§ã€‚å®éªŒè¡¨æ˜ï¼Œä¼ªæ ‡ç­¾IDIå‡å°‘äº†å­ç¾¤ä½“å·®å¼‚ï¼Œå…¬å¹³åº¦æŒ‡æ ‡æé«˜äº†33%ä»¥ä¸Šï¼Œè€ŒSERå‡†ç¡®ç‡ä»…ä¸‹é™ä¸åˆ°3%ã€‚æ­¤å¤–ï¼Œæ— ç›‘ç£çš„IDIåœ¨å…¬å¹³åº¦æŒ‡æ ‡ä¸Šæé«˜äº†è¶…è¿‡26%ï¼ŒSERæ€§èƒ½ä¸‹é™ä¸åˆ°4%ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæ— ç›‘ç£çš„IDIæŒç»­ç¼“è§£äº†ç§æ—å’Œå¹´é¾„å·®å¼‚ï¼Œè¡¨æ˜åœ¨ç¼ºä¹æ˜ç¡®äººå£ç»Ÿè®¡ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå…¶å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„å…¬å¹³æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å­ç¾¤ä½“å·®å¼‚å’Œæ€§èƒ½åè§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–éš¾ä»¥è·å–çš„äººå£ç»Ÿè®¡æ ‡ç­¾ï¼Œè¯¥ç ”ç©¶å¼•å…¥éšæ€§äººå£ç»Ÿè®¡æ¨æ–­ï¼ˆIDIï¼‰æ¨¡å—æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ä¼ªæ ‡ç­¾IDIæ–¹æ³•èƒ½å¤Ÿå‡å°‘å­ç¾¤ä½“å·®å¼‚ï¼Œå…¬å¹³åº¦æŒ‡æ ‡æé«˜æ˜¾è‘—ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„SERå‡†ç¡®ç‡ã€‚</li>
<li>æ— ç›‘ç£çš„IDIæ–¹æ³•åœ¨å…¬å¹³åº¦æŒ‡æ ‡ä¸Šä¹Ÿæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œä¸”åœ¨ç¼ºä¹æ˜ç¡®äººå£ç»Ÿè®¡ä¿¡æ¯çš„æƒ…å†µä¸‹è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸¤ç§IDIæ–¹æ³•éƒ½æœ‰æ•ˆç¼“è§£äº†ç§æ—å’Œå¹´é¾„å·®å¼‚ã€‚</li>
<li>è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥IDIæ¨¡å—ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³SERä¸­åè§é—®é¢˜çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0197e65214e54023c8ae9c2386b732b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48805447a56da299c38d5aeb921aecd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1446e47e2efdff41f6751858208bc760.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d6de9cd829211815b2e3bfe41807c93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7faca6b6383e210de16348d3a3c75829.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aa9a4d98f38fb6ecbd409bf9c96945f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Single-Channel-Target-Speech-Extraction-Utilizing-Distance-and-Room-Clues"><a href="#Single-Channel-Target-Speech-Extraction-Utilizing-Distance-and-Room-Clues" class="headerlink" title="Single-Channel Target Speech Extraction Utilizing Distance and Room   Clues"></a>Single-Channel Target Speech Extraction Utilizing Distance and Room   Clues</h2><p><strong>Authors:Runwu Shi, Zirui Lin, Benjamin Yen, Jiang Wang, Ragib Amin Nihal, Kazuhiro Nakadai</strong></p>
<p>This paper aims to achieve single-channel target speech extraction (TSE) in enclosures utilizing distance clues and room information. Recent works have verified the feasibility of distance clues for the TSE task, which can imply the sound sourceâ€™s direct-to-reverberation ratio (DRR) and thus can be utilized for speech separation and TSE systems. However, such distance clue is significantly influenced by the roomâ€™s acoustic characteristics, such as dimension and reverberation time, making it challenging for TSE systems that rely solely on distance clues to generalize across a variety of different rooms. To solve this, we suggest providing room environmental information (room dimensions and reverberation time) for distance-based TSE for better generalization capabilities. Especially, we propose a distance and environment-based TSE model in the time-frequency (TF) domain with learnable distance and room embedding. Results on both simulated and real collected datasets demonstrate its feasibility. Demonstration materials are available at <a target="_blank" rel="noopener" href="https://runwushi.github.io/distance-room-demo-page/">https://runwushi.github.io/distance-room-demo-page/</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨åˆ©ç”¨è·ç¦»çº¿ç´¢å’Œæˆ¿é—´ä¿¡æ¯å®ç°å°é—­ç¯å¢ƒä¸‹çš„å•é€šé“ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰ã€‚è¿‘æœŸçš„ç ”ç©¶å·²ç»éªŒè¯äº†è·ç¦»çº¿ç´¢å¯¹äºTSEä»»åŠ¡çš„å¯è¡Œæ€§ï¼Œè·ç¦»çº¿ç´¢å¯ä»¥æš—ç¤ºå£°æºçš„ç›´è¾¾å£°ä¸æ··å“å£°æ¯”ï¼ˆDRRï¼‰ï¼Œå› æ­¤å¯ä»¥ç”¨äºè¯­éŸ³åˆ†ç¦»å’ŒTSEç³»ç»Ÿã€‚ç„¶è€Œï¼Œè¿™æ ·çš„è·ç¦»çº¿ç´¢ä¼šå—åˆ°æˆ¿é—´å£°å­¦ç‰¹æ€§çš„å¾ˆå¤§å½±å“ï¼Œå¦‚æˆ¿é—´å°ºå¯¸å’Œæ··å“æ—¶é—´ï¼Œå¯¹äºä»…ä¾èµ–è·ç¦»çº¿ç´¢çš„TSEç³»ç»Ÿæ¥è¯´ï¼Œåœ¨ä¸åŒçš„æˆ¿é—´ä¹‹é—´è¿›è¡Œæ¨å¹¿å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®åœ¨åŸºäºè·ç¦»çš„TSEä¸­æä¾›æˆ¿é—´ç¯å¢ƒä¿¡æ¯ï¼ˆæˆ¿é—´å°ºå¯¸å’Œæ··å“æ—¶é—´ï¼‰ï¼Œä»¥æé«˜å…¶æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ—¶é—´å’Œé¢‘ç‡ï¼ˆTFï¼‰åŸŸçš„è·ç¦»å’Œç¯å¢ƒTSEæ¨¡å‹ï¼Œå…·æœ‰å¯å­¦ä¹ çš„è·ç¦»å’Œæˆ¿é—´åµŒå…¥ã€‚åœ¨æ¨¡æ‹Ÿå’Œå®é™…æ”¶é›†çš„æ•°æ®é›†ä¸Šçš„ç»“æœè¯æ˜äº†å…¶å¯è¡Œæ€§ã€‚æ¼”ç¤ºææ–™å¯åœ¨<a target="_blank" rel="noopener" href="https://runwushi.github.io/distance-room-demo-page/%E6%89%BE%E5%88%B0%E3%80%82">https://runwushi.github.io/distance-room-demo-page/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14433v1">PDF</a> 5 pages, 3 figures, accepted by Eusipco 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ—¨åœ¨åˆ©ç”¨è·ç¦»çº¿ç´¢å’Œæˆ¿é—´ä¿¡æ¯å®ç°å°é—­ç¯å¢ƒä¸‹çš„å•é€šé“ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰ã€‚è™½ç„¶è·ç¦»çº¿ç´¢å¯ä»¥æš—ç¤ºå£°æºçš„ç›´è¾¾å£°ä¸æ··å“å£°æ¯”ï¼ˆDRRï¼‰ï¼Œå¹¶ç”¨äºè¯­éŸ³åˆ†ç¦»å’ŒTSEç³»ç»Ÿï¼Œä½†å…¶å—æˆ¿é—´å£°å­¦ç‰¹æ€§ï¼ˆå¦‚å°ºå¯¸å’Œæ··å“æ—¶é—´ï¼‰çš„å½±å“å¾ˆå¤§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä¸ºåŸºäºè·ç¦»çš„TSEæä¾›æˆ¿é—´ç¯å¢ƒä¿¡æ¯ï¼ˆæˆ¿é—´å°ºå¯¸å’Œæ··å“æ—¶é—´ï¼‰ï¼Œä»¥æé«˜å…¶è·¨ä¸åŒæˆ¿é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªåŸºäºæ—¶é—´å’Œé¢‘ç‡åŸŸçš„è·ç¦»å’Œç¯å¢ƒTSEæ¨¡å‹ï¼Œå…·æœ‰å¯å­¦ä¹ çš„è·ç¦»å’Œæˆ¿é—´åµŒå…¥ã€‚åœ¨æ¨¡æ‹Ÿå’Œå®é™…æ”¶é›†çš„æ•°æ®é›†ä¸Šçš„ç»“æœè¯æ˜äº†å…¶å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡è‡´åŠ›äºåœ¨å°é—­ç¯å¢ƒä¸­å®ç°å•é€šé“ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰ã€‚</li>
<li>è·ç¦»çº¿ç´¢å¯¹äºTSEä»»åŠ¡å…·æœ‰å¯è¡Œæ€§ï¼Œèƒ½å¤Ÿæš—ç¤ºå£°æºçš„ç›´è¾¾å£°ä¸æ··å“å£°æ¯”ï¼ˆDRRï¼‰ã€‚</li>
<li>è·ç¦»çº¿ç´¢å—åˆ°æˆ¿é—´å£°å­¦ç‰¹æ€§çš„å½±å“ï¼Œå¦‚æˆ¿é—´å°ºå¯¸å’Œæ··å“æ—¶é—´ã€‚</li>
<li>å•çº¯ä¾èµ–è·ç¦»çº¿ç´¢çš„TSEç³»ç»Ÿåœ¨é¢å¯¹ä¸åŒæˆ¿é—´æ—¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>ä¸ºæé«˜TSEç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ï¼Œå»ºè®®æä¾›æˆ¿é—´ç¯å¢ƒä¿¡æ¯ï¼ˆå¦‚æˆ¿é—´å°ºå¯¸å’Œæ··å“æ—¶é—´ï¼‰ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ—¶é—´å’Œé¢‘ç‡åŸŸçš„è·ç¦»å’Œç¯å¢ƒTSEæ¨¡å‹ï¼Œå…·æœ‰å¯å­¦ä¹ çš„è·ç¦»å’Œæˆ¿é—´åµŒå…¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14433">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7fb088194adb9dbc22e3c39e9464056b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-378326b1fb33ebbd88f489701a7f437e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7c8d6ef8c832a11aca97a43d0c3fc76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd00758e6b02cb7e16725ab536c97aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a72881274a9ecda17ccb785e772d385.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62428d40bac7ccf649d448251d232d7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dceab43d149003ca2fe3c8c4c89833be.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HausaNLP-Current-Status-Challenges-and-Future-Directions-for-Hausa-Natural-Language-Processing"><a href="#HausaNLP-Current-Status-Challenges-and-Future-Directions-for-Hausa-Natural-Language-Processing" class="headerlink" title="HausaNLP: Current Status, Challenges and Future Directions for Hausa   Natural Language Processing"></a>HausaNLP: Current Status, Challenges and Future Directions for Hausa   Natural Language Processing</h2><p><strong>Authors:Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Kenneth Church, Vukosi Marivate</strong></p>
<p>Hausa Natural Language Processing (NLP) has gained increasing attention in recent years, yet remains understudied as a low-resource language despite having over 120 million first-language (L1) and 80 million second-language (L2) speakers worldwide. While significant advances have been made in high-resource languages, Hausa NLP faces persistent challenges, including limited open-source datasets and inadequate model representation. This paper presents an overview of the current state of Hausa NLP, systematically examining existing resources, research contributions, and gaps across fundamental NLP tasks: text classification, machine translation, named entity recognition, speech recognition, and question answering. We introduce HausaNLP (<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org/">https://catalog.hausanlp.org</a>), a curated catalog that aggregates datasets, tools, and research works to enhance accessibility and drive further development. Furthermore, we discuss challenges in integrating Hausa into large language models (LLMs), addressing issues of suboptimal tokenization and dialectal variation. Finally, we propose strategic research directions emphasizing dataset expansion, improved language modeling approaches, and strengthened community collaboration to advance Hausa NLP. Our work provides both a foundation for accelerating Hausa NLP progress and valuable insights for broader multilingual NLP research. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè±ªè¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå°½ç®¡å…¨çƒæœ‰è¶…è¿‡1.2äº¿çš„ç¬¬ä¸€è¯­è¨€ï¼ˆL1ï¼‰å’Œ8åƒä¸‡çš„ç¬¬äºŒè¯­è¨€ï¼ˆL2ï¼‰ä½¿ç”¨è€…ï¼Œä½†å®ƒä»ç„¶æ˜¯ä¸€ä¸ªèµ„æºåŒ®ä¹çš„è¯­è¨€é¢†åŸŸï¼Œç›¸å…³ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚å°½ç®¡åœ¨é«˜èµ„æºè¯­è¨€æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è±ªè¨NLPä»ç„¶é¢ä¸´æŒç»­æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ‰é™çš„å¼€æºæ•°æ®é›†å’Œä¸è¶³çš„æ¨¡å‹è¡¨ç¤ºã€‚æœ¬æ–‡æ¦‚è¿°äº†è±ªè¨NLPçš„å½“å‰çŠ¶æ€ï¼Œç³»ç»Ÿåœ°æ£€æŸ¥äº†ç°æœ‰èµ„æºã€ç ”ç©¶è´¡çŒ®ä»¥åŠåŸºç¡€NLPä»»åŠ¡ä¸­çš„ç©ºç™½é¢†åŸŸï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œé—®ç­”ã€‚æˆ‘ä»¬ä»‹ç»äº†è±ªè¨NLPç›®å½•ï¼ˆ<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org),è¯¥ç›®å½•æ±‡é›†äº†æ•°æ®é›†ã€å·¥å…·å’Œç ”ç©¶æˆæœ,ä»¥æé«˜å¯è®¿é—®æ€§å¹¶æ¨åŠ¨è¿›ä¸€æ­¥å‘å±•.æ­¤å¤–,æˆ‘ä»¬è®¨è®ºäº†å°†è±ªè¨è¯­é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹(llm)ä¸­çš„æŒ‘æˆ˜,è§£å†³æ¬¡ä¼˜åˆ†è¯å’Œæ–¹è¨€å˜åŒ–çš„é—®é¢˜.æœ€å,æˆ‘ä»¬æå‡ºäº†æˆ˜ç•¥ç ”ç©¶æ–¹å‘,å¼ºè°ƒæ‰©å¤§æ•°æ®é›†ã€æ”¹è¿›è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåˆä½œ,ä»¥æ¨åŠ¨è±ªè¨nlpçš„å‘å±•.æˆ‘ä»¬çš„å·¥ä½œä¸ºåŠ é€Ÿè±ªè¨nlpçš„è¿›æ­¥æä¾›äº†åŸºç¡€,å¹¶ä¸ºæ›´å¹¿æ³›çš„å¤šè¯­è¨€nlpç ”ç©¶æä¾›äº†å®è´µçš„è§è§£./">https://catalog.hausanlp.orgï¼‰ï¼Œè¯¥ç›®å½•æ±‡é›†äº†æ•°æ®é›†ã€å·¥å…·å’Œç ”ç©¶æˆæœï¼Œä»¥æé«˜å¯è®¿é—®æ€§å¹¶æ¨åŠ¨è¿›ä¸€æ­¥å‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¨è®ºäº†å°†è±ªè¨è¯­é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œè§£å†³æ¬¡ä¼˜åˆ†è¯å’Œæ–¹è¨€å˜åŒ–çš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†æˆ˜ç•¥ç ”ç©¶æ–¹å‘ï¼Œå¼ºè°ƒæ‰©å¤§æ•°æ®é›†ã€æ”¹è¿›è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåˆä½œï¼Œä»¥æ¨åŠ¨è±ªè¨NLPçš„å‘å±•ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºåŠ é€Ÿè±ªè¨NLPçš„è¿›æ­¥æä¾›äº†åŸºç¡€ï¼Œå¹¶ä¸ºæ›´å¹¿æ³›çš„å¤šè¯­è¨€NLPç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14311v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Hausaè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è™½ç„¶æ‹¥æœ‰è¶…è¿‡ä¸€äº¿ä½¿ç”¨è€…ï¼Œä½†ä½œä¸ºä½èµ„æºè¯­è¨€ä»å—åˆ°å¿½è§†ã€‚å°½ç®¡åœ¨é«˜èµ„æºè¯­è¨€æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†Hausa NLPä»é¢ä¸´å¼€æ”¾æ•°æ®é›†æœ‰é™å’Œæ¨¡å‹è¡¨ç¤ºä¸è¶³ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¦‚è¿°äº†Hausa NLPçš„ç°çŠ¶ï¼Œç³»ç»Ÿæ£€æŸ¥åŸºæœ¬NLPä»»åŠ¡çš„ç°æœ‰èµ„æºã€ç ”ç©¶è´¡çŒ®å’Œå·®è·ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œé—®é¢˜å›ç­”ç­‰ã€‚æ­¤å¤–ï¼Œä»‹ç»äº†HausaNLPï¼ˆ<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org),ä¸€ä¸ªæ±‡é›†æ•°æ®é›†ã€å·¥å…·å’Œç ”ç©¶æˆæœçš„ç›®å½•,ä»¥æé«˜å¯ç”¨æ€§å’Œæ¨åŠ¨è¿›ä¸€æ­¥å‘å±•.æ–‡ç« è¿˜è®¨è®ºäº†å°†hausaèå…¥å¤§å‹è¯­è¨€æ¨¡å‹(llm)çš„æŒ‘æˆ˜,å¹¶å¼ºè°ƒæ•°æ®é›†æ‰©å±•ã€æ”¹è¿›çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåä½œçš„æˆ˜ç•¥ç ”ç©¶æ–¹å‘.æœ¬æ–‡æ—¢ä¸ºåŠ é€Ÿhausa/">https://catalog.hausanlp.orgï¼‰ï¼Œä¸€ä¸ªæ±‡é›†æ•°æ®é›†ã€å·¥å…·å’Œç ”ç©¶æˆæœçš„ç›®å½•ï¼Œä»¥æé«˜å¯ç”¨æ€§å’Œæ¨åŠ¨è¿›ä¸€æ­¥å‘å±•ã€‚æ–‡ç« è¿˜è®¨è®ºäº†å°†Hausaèå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒæ•°æ®é›†æ‰©å±•ã€æ”¹è¿›çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåä½œçš„æˆ˜ç•¥ç ”ç©¶æ–¹å‘ã€‚æœ¬æ–‡æ—¢ä¸ºåŠ é€ŸHausa</a> NLPçš„å‘å±•æä¾›äº†åŸºç¡€ï¼Œä¹Ÿä¸ºæ›´å¹¿æ³›çš„å¤šå…ƒè¯­è¨€NLPç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Hausa NLPè™½å—åˆ°é‡è§†ï¼Œä½†ä½œä¸ºä½èµ„æºè¯­è¨€ä»å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹å¼€æ”¾æ•°æ®é›†å’Œæ¨¡å‹è¡¨ç¤ºæ˜¯Hausa NLPé¢ä¸´çš„ä¸»è¦é—®é¢˜ã€‚</li>
<li>å½“å‰ç³»ç»Ÿç ”ç©¶äº†Hausa NLPçš„åŸºæœ¬ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ç­‰ã€‚</li>
<li>ä»‹ç»äº†HausaNLPç›®å½•ï¼Œæ—¨åœ¨æé«˜èµ„æºçš„å¯ç”¨æ€§å’Œæ¨åŠ¨è¿›ä¸€æ­¥å‘å±•ã€‚</li>
<li>Hausaèå…¥å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜åŒ…æ‹¬æ¬¡ä¼˜çš„åˆ†è¯å’Œæ–¹è¨€å·®å¼‚ã€‚</li>
<li>éœ€è¦æ‰©å±•æ•°æ®é›†ã€æ”¹è¿›è¯­è¨€å»ºæ¨¡æ–¹æ³•å¹¶åŠ å¼ºç¤¾åŒºåä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-da921480504d8eb6d896c03ce15d6040.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42759b51c5122b9d93aca1f303c8c6fd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-Hate-Speech-Detection-via-Cross-Lingual-Nearest-Neighbor-Retrieval-with-Limited-Labeled-Data"><a href="#Data-Efficient-Hate-Speech-Detection-via-Cross-Lingual-Nearest-Neighbor-Retrieval-with-Limited-Labeled-Data" class="headerlink" title="Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor   Retrieval with Limited Labeled Data"></a>Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor   Retrieval with Limited Labeled Data</h2><p><strong>Authors:Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser</strong></p>
<p>Considering the importance of detecting hateful language, labeled hate speech data is expensive and time-consuming to collect, particularly for low-resource languages. Prior work has demonstrated the effectiveness of cross-lingual transfer learning and data augmentation in improving performance on tasks with limited labeled data. To develop an efficient and scalable cross-lingual transfer learning approach, we leverage nearest-neighbor retrieval to augment minimal labeled data in the target language, thereby enhancing detection performance. Specifically, we assume access to a small set of labeled training instances in the target language and use these to retrieve the most relevant labeled examples from a large multilingual hate speech detection pool. We evaluate our approach on eight languages and demonstrate that it consistently outperforms models trained solely on the target language data. Furthermore, in most cases, our method surpasses the current state-of-the-art. Notably, our approach is highly data-efficient, retrieving as small as 200 instances in some cases while maintaining superior performance. Moreover, it is scalable, as the retrieval pool can be easily expanded, and the method can be readily adapted to new languages and tasks. We also apply maximum marginal relevance to mitigate redundancy and filter out highly similar retrieved instances, resulting in improvements in some languages. </p>
<blockquote>
<p>è€ƒè™‘åˆ°æ£€æµ‹ä»‡æ¨æ€§è¨€è®ºçš„é‡è¦æ€§ï¼Œæ ‡æ³¨ä»‡æ¨è¨€è®ºçš„æ•°æ®æ”¶é›†æ—¢æ˜‚è´µåˆè€—æ—¶ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä½èµ„æºè¯­è¨€ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»è¯æ˜äº†è·¨è¯­è¨€è¿ç§»å­¦ä¹ å’Œæ•°æ®å¢å¼ºåœ¨æ”¹å–„æœ‰é™æ ‡è®°æ•°æ®ä»»åŠ¡æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†å¼€å‘ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„è·¨è¯­è¨€è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€è¿‘é‚»æ£€ç´¢æ¥å¢å¼ºç›®æ ‡è¯­è¨€ä¸­çš„å°‘é‡æ ‡è®°æ•°æ®ï¼Œä»è€Œæé«˜æ£€æµ‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‡è®¾ç›®æ ‡è¯­è¨€ä¸­æœ‰å°‘é‡æ ‡è®°è®­ç»ƒå®ä¾‹å¯ä¾›è®¿é—®ï¼Œå¹¶åˆ©ç”¨è¿™äº›å®ä¾‹ä»å¤§å‹å¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹æ± ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ ‡è®°ç¤ºä¾‹ã€‚æˆ‘ä»¬åœ¨å…«ç§è¯­è¨€ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜å…¶æ€§èƒ½å§‹ç»ˆä¼˜äºä»…ä½¿ç”¨ç›®æ ‡è¯­è¨€æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¿‡äº†å½“å‰çš„æœ€ä½³æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éå¸¸æ³¨é‡æ•°æ®æ•ˆç‡ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹åªéœ€æ£€ç´¢200ä¸ªå®ä¾‹å³å¯ä¿æŒå“è¶Šæ€§èƒ½ã€‚è€Œä¸”ï¼Œå®ƒæ˜¯å¯æ‰©å±•çš„ï¼Œå› ä¸ºæ£€ç´¢æ± å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•ï¼Œå¹¶ä¸”è¯¥æ–¹æ³•å¯ä»¥è½»æ¾åœ°é€‚åº”æ–°è¯­è¨€å’Œä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜åº”ç”¨æœ€å¤§è¾¹ç¼˜ç›¸å…³æ€§æ¥ç¼“è§£å†—ä½™å¹¶è¿‡æ»¤æ‰é«˜åº¦ç›¸ä¼¼çš„æ£€ç´¢å®ä¾‹ï¼Œä»è€Œåœ¨æŸäº›è¯­è¨€ä¸­å–å¾—äº†æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14272v1">PDF</a> </p>
<p><strong>Summary</strong><br>é«˜æ•ˆç‡ã€å¯æ‹“å±•çš„è·¨è¯­è¨€è¿ç§»å­¦ä¹ æ–¹æ³•ç”¨äºå¢å¼ºç›®æ ‡è¯­è¨€çš„å°‘é‡æ ‡ç­¾æ•°æ®ï¼Œæé«˜ä»‡æ¨è¨€è®ºæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚é€šè¿‡æœ€è¿‘é‚»æ£€ç´¢ä»å¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹æ± ä¸­è·å–æœ€ç›¸å…³çš„å®ä¾‹ã€‚åœ¨å¤šè¯­è¨€æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šå•ä¸€è®­ç»ƒæ¨¡å‹åŠå½“å‰æœ€ä¼˜æ–¹æ³•çš„æ€§èƒ½ã€‚æ­¤æ–¹æ³•å…·å¤‡é«˜æ•ˆæ€§ã€å¯æ‹“å±•æ€§ï¼Œå¯é€‚åº”æ–°è¯­è¨€å’Œä»»åŠ¡ã€‚ä½¿ç”¨æœ€å¤§è¾¹ç¼˜ç›¸å…³æ€§å‡å°‘å†—ä½™å’Œè¿‡æ»¤é«˜åº¦ç›¸ä¼¼çš„æ£€ç´¢å®ä¾‹ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨è¯­è¨€è¿ç§»å­¦ä¹ æ–¹æ³•ç”¨äºå¢å¼ºç›®æ ‡è¯­è¨€çš„å°‘é‡æ ‡ç­¾æ•°æ®ï¼Œæé«˜ä»‡æ¨è¨€è®ºæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨æœ€è¿‘é‚»æ£€ç´¢ä»å¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹æ± ä¸­è·å–ç›¸å…³å®ä¾‹ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šè¯­è¨€æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¶Šå•ä¸€è®­ç»ƒæ¨¡å‹åŠå½“å‰æœ€ä¼˜æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•å…·å¤‡é«˜æ•ˆæ€§å’Œå¯æ‹“å±•æ€§ï¼Œå¯é€‚åº”æ–°è¯­è¨€å’Œä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨æœ€å¤§è¾¹ç¼˜ç›¸å…³æ€§å‡å°‘å†—ä½™å®ä¾‹ï¼Œæé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åªéœ€å°‘é‡ç›®æ ‡è¯­è¨€æ•°æ®å³å¯è¿›è¡Œé«˜æ•ˆçš„ä»‡æ¨è¨€è®ºæ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de254ad9901f133112329e35c1c065aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f467bca691d37698d9f05f6135e64e4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-331cbf8c3b53eb673a660dd4621f4ead.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement"><a href="#SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement" class="headerlink" title="SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement"></a>SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement</h2><p><strong>Authors:Kuan-Yu Chen, Jeng-Lin Li, Jian-Jiun Ding</strong></p>
<p>With the fast development of zero-shot text-to-speech technologies, it is possible to generate high-quality speech signals that are indistinguishable from the real ones. Speech editing, including speech insertion and replacement, appeals to researchers due to its potential applications. However, existing studies only considered clean speech scenarios. In real-world applications, the existence of environmental noise could significantly degrade the quality of the generation. In this study, we propose a noise-resilient speech editing framework, SeamlessEdit, for noisy speech editing. SeamlessEdit adopts a frequency-band-aware noise suppression module and an in-content refinement strategy. It can well address the scenario where the frequency bands of voice and background noise are not separated. The proposed SeamlessEdit framework outperforms state-of-the-art approaches in multiple quantitative and qualitative evaluations. </p>
<blockquote>
<p>éšç€é›¶èµ·æ­¥æ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆé«˜è´¨é‡ã€ä¸çœŸå®è¯­éŸ³éš¾ä»¥åŒºåˆ†çš„è¯­éŸ³ä¿¡å·æˆä¸ºå¯èƒ½ã€‚è¯­éŸ³ç¼–è¾‘ï¼ŒåŒ…æ‹¬è¯­éŸ³æ’å…¥å’Œæ›¿æ¢ï¼Œå› å…¶æ½œåœ¨çš„åº”ç”¨ä»·å€¼è€Œå¸å¼•ç ”ç©¶è€…å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä»…æ¶‰åŠå¹²å‡€è¯­éŸ³åœºæ™¯ã€‚åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­ï¼Œç¯å¢ƒå™ªå£°çš„å­˜åœ¨å¯èƒ½ä¼šæ˜¾è‘—é™ä½ç”Ÿæˆè¯­éŸ³çš„è´¨é‡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºå™ªå£°è¯­éŸ³ç¼–è¾‘çš„å™ªå£°é²æ£’æ€§è¯­éŸ³ç¼–è¾‘æ¡†æ¶ï¼Œæ— ç¼ç¼–è¾‘ï¼ˆSeamlessEditï¼‰ã€‚SeamlessEdité‡‡ç”¨é¢‘å¸¦æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶æ¨¡å—å’ŒåŸºäºå†…å®¹çš„ç»†åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°è§£å†³è¯­éŸ³å’ŒèƒŒæ™¯å™ªå£°é¢‘å¸¦æœªåˆ†ç¦»çš„åœºæ™¯ã€‚æ‰€æå‡ºçš„æ— ç¼ç¼–è¾‘æ¡†æ¶åœ¨å¤šä¸ªå®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14066v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>éšç€é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆé«˜è´¨é‡ã€ä¸ç°å®æ— æ³•åŒºåˆ†çš„è¯­éŸ³ä¿¡å·æˆä¸ºå¯èƒ½ã€‚å°½ç®¡è¯­éŸ³ç¼–è¾‘ï¼ŒåŒ…æ‹¬è¯­éŸ³æ’å…¥å’Œæ›¿æ¢ï¼Œåœ¨æ½œåœ¨åº”ç”¨æ–¹é¢é¢‡å…·å¸å¼•åŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ¸…æ´è¯­éŸ³åœºæ™¯ä¸Šã€‚å®é™…åº”ç”¨ä¸­ï¼Œç¯å¢ƒå™ªå£°çš„å­˜åœ¨ä¼šä¸¥é‡å½±å“ç”Ÿæˆè¯­éŸ³çš„è´¨é‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºå™ªå£°è¯­éŸ³ç¼–è¾‘çš„ç¨³å¥æ¡†æ¶SeamlessEditï¼Œå®ƒé‡‡ç”¨é¢‘å¸¦æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶æ¨¡å—å’Œå†…å®¹å†…ä¼˜åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†è¯­éŸ³å’ŒèƒŒæ™¯å™ªå£°é¢‘å¸¦æœªåˆ†ç¦»çš„åœºæ™¯ã€‚SeamlessEditæ¡†æ¶åœ¨å¤šé¡¹å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­è¡¨ç°å‡ºè¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œå¯ç”Ÿæˆé«˜è´¨é‡ä¸ç°å®æ— æ³•åŒºåˆ†çš„è¯­éŸ³ä¿¡å·ã€‚</li>
<li>è¯­éŸ³ç¼–è¾‘å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ¸…æ´è¯­éŸ³åœºæ™¯ã€‚</li>
<li>å®é™…åº”ç”¨ä¸­ï¼Œç¯å¢ƒå™ªå£°ä¼šä¸¥é‡å½±å“ç”Ÿæˆè¯­éŸ³çš„è´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç”¨äºå™ªå£°è¯­éŸ³ç¼–è¾‘çš„ç¨³å¥æ¡†æ¶SeamlessEditã€‚</li>
<li>SeamlessEdité‡‡ç”¨é¢‘å¸¦æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶æ¨¡å—ã€‚</li>
<li>SeamlessEdité‡‡ç”¨å†…å®¹å†…ä¼˜åŒ–ç­–ç•¥ï¼Œèƒ½å¤„ç†è¯­éŸ³å’ŒèƒŒæ™¯å™ªå£°é¢‘å¸¦æœªåˆ†ç¦»çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3331afb116eb5a4dd722f832aec65be7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-352190ad478243fe4b96bd0542df1f43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3b0f11694b33d58d87e7efc60fe78c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-476197b8220b75e12102d575aa4a5566.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Multimodal-Information-Based-Speech-Processing-MISP-2025-Challenge-Audio-Visual-Diarization-and-Recognition"><a href="#The-Multimodal-Information-Based-Speech-Processing-MISP-2025-Challenge-Audio-Visual-Diarization-and-Recognition" class="headerlink" title="The Multimodal Information Based Speech Processing (MISP) 2025   Challenge: Audio-Visual Diarization and Recognition"></a>The Multimodal Information Based Speech Processing (MISP) 2025   Challenge: Audio-Visual Diarization and Recognition</h2><p><strong>Authors:Ming Gao, Shilong Wu, Hang Chen, Jun Du, Chin-Hui Lee, Shinji Watanabe, Jingdong Chen, Siniscalchi Sabato Marco, Odette Scharenborg</strong></p>
<p>Meetings are a valuable yet challenging scenario for speech applications due to complex acoustic conditions. This paper summarizes the outcomes of the MISP 2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal, multi-device meeting transcription by incorporating video modality alongside audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR). We present the challengeâ€™s objectives, tasks, dataset, baseline systems, and solutions proposed by participants. The best-performing systems achieved significant improvements over the baseline: the top AVSD model achieved a Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the best AVDR system achieved a concatenated minimum-permutation Character Error Rate (cpCER) of 11.56%, improving by 72.49%. </p>
<blockquote>
<p>ä¼šè®®æ˜¯è¯­éŸ³åº”ç”¨çš„æœ‰ä»·å€¼ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼ŒåŸå› åœ¨äºå…¶å¤æ‚çš„å£°å­¦æ¡ä»¶ã€‚æœ¬æ–‡æ€»ç»“äº†MISP 2025æŒ‘æˆ˜çš„ç»“æœï¼Œè¯¥æŒ‘æˆ˜ç”±Interspeech 2025ä¸»åŠï¼Œèšç„¦äºå¤šæ¨¡æ€ã€å¤šè®¾å¤‡ä¼šè®®è½¬å½•ï¼Œé™¤äº†éŸ³é¢‘å¤–è¿˜çº³å…¥äº†è§†é¢‘æ¨¡æ€ã€‚ä»»åŠ¡åŒ…æ‹¬éŸ³é¢‘-è§†è§‰è¯´è¯äººè¯†åˆ«ï¼ˆAVSDï¼‰ã€éŸ³é¢‘-è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’ŒéŸ³é¢‘-è§†è§‰è¯´è¯äººè¯†åˆ«å’Œè¯­éŸ³è¯†åˆ«ï¼ˆAVDRï¼‰ã€‚æˆ‘ä»¬ä»‹ç»äº†æŒ‘æˆ˜çš„ç›®æ ‡ã€ä»»åŠ¡ã€æ•°æ®é›†ã€åŸºçº¿ç³»ç»Ÿå’Œå‚ä¸è€…æå‡ºçš„è§£å†³æ–¹æ¡ˆã€‚è¡¨ç°æœ€ä½³çš„ç³»ç»Ÿåœ¨åŸºçº¿çš„åŸºç¡€ä¸Šå–å¾—äº†é‡å¤§æ”¹è¿›ï¼šæœ€ä½³AVSDæ¨¡å‹çš„èšç±»é”™è¯¯ç‡ï¼ˆDERï¼‰è¾¾åˆ°äº†8.09%ï¼Œæé«˜äº†7.43%ï¼›æœ€ä½³AVSRç³»ç»Ÿçš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰è¾¾åˆ°äº†9.48%ï¼Œæé«˜äº†10.62%ï¼›æœ€ä½³AVDRç³»ç»Ÿçš„ç»„åˆæœ€å°æ’åˆ—å­—ç¬¦é”™è¯¯ç‡ï¼ˆcpCERï¼‰è¾¾åˆ°äº†11.56%ï¼Œæé«˜äº†72.49%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13971v1">PDF</a> Accepted by Interspeech 2025. Camera-ready version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ€»ç»“äº†MISP 2025æŒ‘æˆ˜çš„æˆæœï¼Œè¯¥æŒ‘æˆ˜å…³æ³¨å¤šæ¨¡æ€ã€å¤šè®¾å¤‡çš„ä¼šè®®è½¬å½•ï¼Œç»“åˆäº†è§†é¢‘æ¨¡æ€å’ŒéŸ³é¢‘ã€‚åŒ…æ‹¬éŸ³è§†é¢‘æ¼”è®²è€…è¯†åˆ«ã€éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«å’ŒéŸ³è§†é¢‘è¯†åˆ«å’Œå½•éŸ³ä»»åŠ¡ã€‚æå‡ºæŒ‘æˆ˜çš„ç›®æ ‡ã€ä»»åŠ¡ã€æ•°æ®é›†ã€åŸºå‡†ç³»ç»Ÿå’Œå‚ä¸è€…è§£å†³æ–¹æ¡ˆã€‚æœ€ä½³ç³»ç»Ÿç›¸æ¯”åŸºçº¿æœ‰æ˜¾è‘—æ”¹å–„ï¼šæœ€ä½³éŸ³è§†é¢‘æ¼”è®²è€…è¯†åˆ«æ¨¡å‹çš„å¯¹æ•°ä¼¼ç„¶è¯¯å·®é™ä½è‡³8.09%ï¼Œæœ€ä½³éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å­—ç¬¦é”™è¯¯ç‡é™è‡³9.48%ï¼Œæœ€ä½³éŸ³è§†é¢‘è¯†åˆ«å’Œå½•éŸ³ç³»ç»Ÿçš„å­—ç¬¦é”™è¯¯ç‡ä¹Ÿæœ‰æ‰€é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MISP 2025æŒ‘æˆ˜å…³æ³¨å¤šæ¨¡æ€ã€å¤šè®¾å¤‡çš„ä¼šè®®è½¬å½•ï¼Œç»“åˆäº†è§†é¢‘æ¨¡æ€å’ŒéŸ³é¢‘ã€‚</li>
<li>åŒ…æ‹¬éŸ³è§†é¢‘æ¼”è®²è€…è¯†åˆ«ï¼ˆAVSDï¼‰ã€éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’ŒéŸ³è§†é¢‘è¯†åˆ«å’Œå½•éŸ³ï¼ˆAVDRï¼‰ä¸‰å¤§ä»»åŠ¡ã€‚</li>
<li>æŒ‘æˆ˜æä¾›äº†æ•°æ®é›†ä¾›å‚ä¸è€…ä½¿ç”¨ã€‚</li>
<li>æœ€ä½³ç³»ç»Ÿåœ¨ä¸‰å¤§ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹å–„æˆæœã€‚</li>
<li>æœ€ä½³AVSDæ¨¡å‹çš„Diarization Error Rateï¼ˆDERï¼‰é™ä½è‡³8.09%ã€‚</li>
<li>æœ€ä½³AVSRç³»ç»Ÿçš„Character Error Rateï¼ˆCERï¼‰é™ä½è‡³9.48%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c96c17a9352ebe86db8823498332376e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17607389671e8b796c4b75f1b7ea4ae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e4ec8a20dd9c07acc03934a81d075dc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BiCrossMamba-ST-Speech-Deepfake-Detection-with-Bidirectional-Mamba-Spectro-Temporal-Cross-Attention"><a href="#BiCrossMamba-ST-Speech-Deepfake-Detection-with-Bidirectional-Mamba-Spectro-Temporal-Cross-Attention" class="headerlink" title="BiCrossMamba-ST: Speech Deepfake Detection with Bidirectional Mamba   Spectro-Temporal Cross-Attention"></a>BiCrossMamba-ST: Speech Deepfake Detection with Bidirectional Mamba   Spectro-Temporal Cross-Attention</h2><p><strong>Authors:Yassine El Kheir, Tim Polzehl, Sebastian MÃ¶ller</strong></p>
<p>We propose BiCrossMamba-ST, a robust framework for speech deepfake detection that leverages a dual-branch spectro-temporal architecture powered by bidirectional Mamba blocks and mutual cross-attention. By processing spectral sub-bands and temporal intervals separately and then integrating their representations, BiCrossMamba-ST effectively captures the subtle cues of synthetic speech. In addition, our proposed framework leverages a convolution-based 2D attention map to focus on specific spectro-temporal regions, enabling robust deepfake detection. Operating directly on raw features, BiCrossMamba-ST achieves significant performance improvements, a 67.74% and 26.3% relative gain over state-of-the-art AASIST on ASVSpoof LA21 and ASVSpoof DF21 benchmarks, respectively, and a 6.80% improvement over RawBMamba on ASVSpoof DF21. Code and models will be made publicly available. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†BiCrossMamba-STè¿™ä¸€ç¨³å¥çš„è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨åŒåˆ†æ”¯æ—¶ç©ºæ¶æ„ï¼Œå€ŸåŠ©åŒå‘Mambaå—å’Œäº¤å‰äº’æ³¨æ„åŠ›æœºåˆ¶ã€‚é€šè¿‡å¯¹å…‰è°±å­å¸¦å’Œæ—¶é—´æ®µè¿›è¡Œåˆ†åˆ«å¤„ç†ï¼Œç„¶åæ•´åˆå®ƒä»¬çš„è¡¨ç¤ºï¼ŒBiCrossMamba-STå¯ä»¥æœ‰æ•ˆåœ°æ•æ‰åˆæˆè¯­éŸ³çš„ç»†å¾®çº¿ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„æ¡†æ¶åˆ©ç”¨åŸºäºå·ç§¯çš„äºŒç»´æ³¨æ„åŠ›å›¾æ¥å…³æ³¨ç‰¹å®šçš„æ—¶ç©ºåŒºåŸŸï¼Œä»è€Œå®ç°ç¨³å¥çš„æ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚ç›´æ¥åœ¨åŸå§‹ç‰¹å¾ä¸Šæ“ä½œï¼ŒBiCrossMamba-STåœ¨ASVSpoof LA21å’ŒASVSpoof DF21åŸºå‡†æµ‹è¯•ä¸Šç›¸å¯¹äºæœ€æ–°æŠ€æœ¯AASISTå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç›¸å¯¹å¢ç›Šåˆ†åˆ«ä¸º67.74%å’Œ26.3%ï¼Œå¹¶ä¸”åœ¨ASVSpoof DF21ä¸Šçš„åŸå§‹BMambaæ¨¡å‹ä¹Ÿæœ‰6.80%çš„æå‡ã€‚ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13930v1">PDF</a> Accepted Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†BiCrossMamba-STè¿™ä¸€ç¨³å¥çš„è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶ã€‚å®ƒé‡‡ç”¨åŒåˆ†æ”¯å…‰è°±æ—¶é—´æ¶æ„ï¼Œç»“åˆåŒå‘Mambaå—å’Œäº¤å‰äº’æ³¨æ„åŠ›æœºåˆ¶ã€‚é€šè¿‡åˆ†åˆ«å¤„ç†é¢‘è°±å­å¸¦å’Œæ—¶é—´é—´éš”å¹¶æ•´åˆå…¶è¡¨ç¤ºï¼Œæœ‰æ•ˆæ•æ‰åˆæˆè¯­éŸ³çš„ç»†å¾®çº¿ç´¢ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºäºå·ç§¯çš„2Dæ³¨æ„åŠ›å›¾ï¼Œå…³æ³¨ç‰¹å®šçš„å…‰è°±æ—¶é—´åŒºåŸŸï¼Œä»è€Œå®ç°ç¨³å¥çš„æ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚ç›´æ¥åœ¨åŸå§‹ç‰¹å¾ä¸Šæ“ä½œï¼ŒBiCrossMamba-STåœ¨ASVSpoof LA21å’ŒASVSpoof DF21åŸºå‡†æµ‹è¯•ä¸Šè¾ƒå…ˆè¿›æ–¹æ³•AASISTæœ‰67.74%å’Œ26.3%çš„ç›¸å¯¹å¢ç›Šï¼Œå¹¶åœ¨ASVSpoof DF21ä¸Šè¾ƒRawBMambaæœ‰6.80%çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BiCrossMamba-STæ˜¯ä¸€ä¸ªç”¨äºè¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„ç¨³å¥æ¡†æ¶ã€‚</li>
<li>é‡‡ç”¨åŒåˆ†æ”¯å…‰è°±æ—¶é—´æ¶æ„ï¼Œç»“åˆåŒå‘Mambaå—å’Œäº¤å‰äº’æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>é€šè¿‡å¤„ç†é¢‘è°±å­å¸¦å’Œæ—¶é—´é—´éš”ï¼Œæœ‰æ•ˆæ•æ‰åˆæˆè¯­éŸ³çš„ç»†å¾®ç‰¹å¾ã€‚</li>
<li>åˆ©ç”¨åŸºäºå·ç§¯çš„2Dæ³¨æ„åŠ›å›¾ï¼Œå…³æ³¨ç‰¹å®šå…‰è°±æ—¶é—´åŒºåŸŸã€‚</li>
<li>åœ¨ASVSpoof LA21å’ŒASVSpoof DF21åŸºå‡†æµ‹è¯•ä¸Šè¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æ€§èƒ½æå‡ã€‚</li>
<li>BiCrossMamba-STç›´æ¥åœ¨åŸå§‹ç‰¹å¾ä¸Šæ“ä½œï¼Œå®ç°æ›´é«˜æ•ˆçš„æ£€æµ‹ã€‚</li>
<li>å…¬å¼€æä¾›ä»£ç å’Œæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09a1819cf25998ca67c1d0ced320efe0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0022318a43410aa00ee25472ec95102f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3219262b4dba92192b1bdbe98e12d88f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-810cf3db6c2b0080c50b0b467fbb4fdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a474780414614812109d2a8132920976.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="U-SAM-An-audio-language-Model-for-Unified-Speech-Audio-and-Music-Understanding"><a href="#U-SAM-An-audio-language-Model-for-Unified-Speech-Audio-and-Music-Understanding" class="headerlink" title="U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding"></a>U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding</h2><p><strong>Authors:Ziqian Wang, Xianjun Xia, Xinfa Zhu, Lei Xie</strong></p>
<p>The text generation paradigm for audio tasks has opened new possibilities for unified audio understanding. However, existing models face significant challenges in achieving a comprehensive understanding across diverse audio types, such as speech, general audio events, and music. Furthermore, their exclusive reliance on cross-entropy loss for alignment often falls short, as it treats all tokens equally and fails to account for redundant audio features, leading to weaker cross-modal alignment. To deal with the above challenges, this paper introduces U-SAM, an advanced audio language model that integrates specialized encoders for speech, audio, and music with a pre-trained large language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for task-aware feature fusion, dynamically routing and integrating the domain-specific encoder outputs. Additionally, U-SAM incorporates a Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant audio features under language supervision and rectifies their semantic and spectral representations to enhance cross-modal alignment. Extensive experiments demonstrate that U-SAM consistently outperforms both specialized models and existing audio language models across multiple benchmarks. Moreover, it exhibits emergent capabilities on unseen tasks, showcasing its generalization potential. Code is available (<a target="_blank" rel="noopener" href="https://github.com/Honee-W/U-SAM/">https://github.com/Honee-W/U-SAM/</a>). </p>
<blockquote>
<p>éŸ³é¢‘ä»»åŠ¡çš„æ–‡æœ¬ç”ŸæˆèŒƒå¼ä¸ºç»Ÿä¸€éŸ³é¢‘ç†è§£æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å®ç°ä¸åŒç±»å‹éŸ³é¢‘çš„å…¨é¢ç†è§£æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¦‚è¯­éŸ³ã€é€šç”¨éŸ³é¢‘äº‹ä»¶å’ŒéŸ³ä¹ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¯¹äº¤å‰ç†µæŸå¤±çš„è¿‡åº¦ä¾èµ–å¾€å¾€éš¾ä»¥å®ç°è‰¯å¥½çš„å¯¹é½ï¼Œå› ä¸ºäº¤å‰ç†µæŸå¤±å¹³ç­‰å¯¹å¾…æ‰€æœ‰æ ‡è®°ï¼Œæ— æ³•å¤„ç†å†—ä½™çš„éŸ³é¢‘ç‰¹å¾ï¼Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½è¾ƒå¼±ã€‚ä¸ºäº†åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†U-SAMï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå®ƒç»“åˆäº†é’ˆå¯¹è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹çš„ä¸“ç”¨ç¼–ç å™¨ä»¥åŠé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚U-SAMé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ•å½±ä»ªè¿›è¡Œä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾èåˆï¼ŒåŠ¨æ€è·¯ç”±å’Œé›†æˆç‰¹å®šé¢†åŸŸçš„ç¼–ç å™¨è¾“å‡ºã€‚æ­¤å¤–ï¼ŒU-SAMè¿˜é‡‡ç”¨äº†è¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è¯­è¨€å­¦ç›‘ç£ä¸‹æ˜ç¡®è¯†åˆ«å†—ä½™éŸ³é¢‘ç‰¹å¾ï¼Œå¹¶çº æ­£å…¶è¯­ä¹‰å’Œå…‰è°±è¡¨ç¤ºï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒU-SAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºä¸“ä¸šæ¨¡å‹å’Œç°æœ‰éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚è€Œä¸”ï¼Œå®ƒåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šå±•ç°å‡ºæ½œåœ¨èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶æ³›åŒ–æ½œåŠ›ã€‚ä»£ç å¯ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Honee-W/U-SAM/%EF%BC%89%E3%80%82">https://github.com/Honee-W/U-SAM/ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13880v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹éŸ³é¢‘ä»»åŠ¡çš„æ–°å‹æ–‡æœ¬ç”ŸæˆèŒƒå¼ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨ç»Ÿä¸€éŸ³é¢‘ç†è§£æ–¹é¢çš„æ–°å¯èƒ½æ€§ã€‚ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šæ ·åŒ–éŸ³é¢‘ç±»å‹æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚è¯­éŸ³ã€é€šç”¨éŸ³é¢‘äº‹ä»¶å’ŒéŸ³ä¹ç­‰ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥U-SAMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é›†æˆäº†é’ˆå¯¹è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹çš„ä¸“ç”¨ç¼–ç å™¨ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚U-SAMä½¿ç”¨æ··åˆä¸“å®¶æŠ•å½±æŠ€æœ¯å®ç°ä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾èåˆï¼ŒåŒæ—¶åŠ¨æ€è·¯ç”±å’Œé›†æˆç‰¹å®šé¢†åŸŸç¼–ç å™¨çš„è¾“å‡ºã€‚æ­¤å¤–ï¼ŒU-SAMè¿˜åŒ…å«è¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è¯­è¨€å­¦ç›‘ç£ä¸‹æ˜ç¡®è¯†åˆ«å†—ä½™éŸ³é¢‘ç‰¹å¾ï¼Œå¹¶çº æ­£å…¶è¯­ä¹‰å’Œå…‰è°±è¡¨ç¤ºï¼Œä»¥æé«˜è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒU-SAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¸“ä¸šæ¨¡å‹å’Œç°æœ‰éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æœªè§ä»»åŠ¡ä¸Šå±•ç°å‡ºæ½œåŠ›ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘ä»»åŠ¡çš„æ–‡æœ¬ç”ŸæˆèŒƒå¼å¸¦æ¥äº†æ–°çš„ç»Ÿä¸€éŸ³é¢‘ç†è§£çš„å¯èƒ½æ€§ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šæ ·åŒ–éŸ³é¢‘ç±»å‹æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>U-SAMæ¨¡å‹é›†æˆäº†ä¸“ç”¨ç¼–ç å™¨ä»¥å¤„ç†è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ç­‰ä¸åŒç±»å‹çš„æ•°æ®ã€‚</li>
<li>U-SAMé‡‡ç”¨æ··åˆä¸“å®¶æŠ•å½±æŠ€æœ¯å®ç°ä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾èåˆã€‚</li>
<li>U-SAMåŒ…å«è¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—ï¼Œä»¥æé«˜è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚</li>
<li>U-SAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ½œåœ¨ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-670c93c1cf97e3fa04cb9884162cf34f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b367bb8b913388b6cde7df16fdc82eee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-145be9705a0d75da8c744da23ca6faa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-668509d0ee1f5b8c7d8b6a820b7d8662.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Semantic-Information-based-Hierarchical-Speech-Enhancement-Method-Using-Factorized-Codec-and-Diffusion-Model"><a href="#A-Semantic-Information-based-Hierarchical-Speech-Enhancement-Method-Using-Factorized-Codec-and-Diffusion-Model" class="headerlink" title="A Semantic Information-based Hierarchical Speech Enhancement Method   Using Factorized Codec and Diffusion Model"></a>A Semantic Information-based Hierarchical Speech Enhancement Method   Using Factorized Codec and Diffusion Model</h2><p><strong>Authors:Yang Xiang, Canan Huang, Desheng Hu, Jingguang Tian, Xinhui Hu, Chao Zhang</strong></p>
<p>Most current speech enhancement (SE) methods recover clean speech from noisy inputs by directly estimating time-frequency masks or spectrums. However, these approaches often neglect the distinct attributes, such as semantic content and acoustic details, inherent in speech signals, which can hinder performance in downstream tasks. Moreover, their effectiveness tends to degrade in complex acoustic environments. To overcome these challenges, we propose a novel, semantic information-based, step-by-step factorized SE method using factorized codec and diffusion model. Unlike traditional SE methods, our hierarchical modeling of semantic and acoustic attributes enables more robust clean speech recovery, particularly in challenging acoustic scenarios. Moreover, this method offers further advantages for downstream TTS tasks. Experimental results demonstrate that our algorithm not only outperforms SOTA baselines in terms of speech quality but also enhances TTS performance in noisy environments. </p>
<blockquote>
<p>å½“å‰å¤§å¤šæ•°è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•é€šè¿‡ç›´æ¥ä¼°è®¡æ—¶é—´é¢‘ç‡æ©è†œæˆ–é¢‘è°±ä»å™ªå£°è¾“å…¥ä¸­æ¢å¤æ¸…æ´è¯­éŸ³ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½è§†äº†è¯­éŸ³ä¿¡å·ä¸­å›ºæœ‰çš„ä¸åŒå±æ€§ï¼Œå¦‚è¯­ä¹‰å†…å®¹å’Œå£°éŸ³ç»†èŠ‚ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨å¤æ‚çš„å£°å­¦ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å¾€å¾€ä¼šé™ä½ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºè¯­ä¹‰ä¿¡æ¯çš„é€æ­¥åˆ†è§£çš„SEæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨åˆ†è§£ç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„SEæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å¯¹è¯­ä¹‰å’Œå£°éŸ³å±æ€§çš„åˆ†å±‚å»ºæ¨¡ï¼Œèƒ½å¤Ÿå®ç°æ›´ç¨³å¥çš„æ¸…æ´è¯­éŸ³æ¢å¤ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œæ­¤æ–¹æ³•å¯¹äºä¸‹æ¸¸çš„TTSä»»åŠ¡è¿˜å…·æœ‰è¿›ä¸€æ­¥çš„ä¼˜ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•ä¸ä»…åœ¨è¯­éŸ³è´¨é‡æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯æ°´å¹³çš„åŸºå‡†æµ‹è¯•ï¼Œè€Œä¸”åœ¨æœ‰å™ªéŸ³çš„ç¯å¢ƒä¸‹æé«˜äº†TTSçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13843v1">PDF</a> Accepted by interspeech 2025</p>
<p><strong>Summary</strong>ï¼š<br>å½“å‰å¤§å¤šæ•°è¯­éŸ³å¢å¼ºæ–¹æ³•é€šè¿‡ç›´æ¥ä¼°è®¡æ—¶é¢‘æ©æ¨¡æˆ–é¢‘è°±æ¥ä»å™ªå£°è¾“å…¥ä¸­æ¢å¤æ¸…æ´è¯­éŸ³ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¿½ç•¥äº†è¯­éŸ³ä¿¡å·ä¸­çš„ä¸åŒå±æ€§ï¼Œå¦‚è¯­ä¹‰å†…å®¹å’Œå£°éŸ³ç»†èŠ‚ï¼Œè¿™å¯èƒ½ä¼šå½±å“ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰ä¿¡æ¯çš„æ–°é¢–ã€é€æ­¥åˆ†è§£çš„è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œä½¿ç”¨åˆ†è§£ç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„è¯­éŸ³å¢å¼ºæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å¯¹è¯­ä¹‰å’Œå£°éŸ³å±æ€§çš„åˆ†å±‚å»ºæ¨¡èƒ½å¤Ÿå®ç°æ›´ç¨³å¥çš„æ¸…æ´è¯­éŸ³æ¢å¤ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„å£°å­¦åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œæ­¤æ–¹æ³•å¯¹ä¸‹æ¸¸æ–‡æœ¬è½¬è¯­éŸ³ä»»åŠ¡å…·æœ‰ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•ä¸ä»…åœ¨è¯­éŸ³è´¨é‡æ–¹é¢ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œè¿˜èƒ½æé«˜å™ªå£°ç¯å¢ƒä¸‹çš„æ–‡æœ¬è½¬è¯­éŸ³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å½“å‰è¯­éŸ³å¢å¼ºæ–¹æ³•ä¸»è¦é€šè¿‡ä¼°è®¡æ—¶é¢‘æ©æ¨¡æˆ–é¢‘è°±æ¥æ¢å¤æ¸…æ´è¯­éŸ³ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¿½ç•¥äº†è¯­éŸ³ä¿¡å·çš„è¯­ä¹‰å†…å®¹å’Œå£°éŸ³ç»†èŠ‚ç­‰ç‹¬ç‰¹å±æ€§ã€‚</li>
<li>æ‰€ææ–¹æ³•é‡‡ç”¨åŸºäºè¯­ä¹‰ä¿¡æ¯çš„é€æ­¥åˆ†è§£è¯­éŸ³å¢å¼ºç­–ç•¥ï¼Œç»“åˆåˆ†è§£ç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åˆ†å±‚å»ºæ¨¡è¯­ä¹‰å’Œå£°éŸ³å±æ€§ä½¿è¯¥æ–¹æ³•åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸‹å®ç°æ›´ç¨³å¥çš„è¯­éŸ³æ¢å¤ã€‚</li>
<li>æ‰€ææ–¹æ³•ä¸ä»…æé«˜äº†è¯­éŸ³è´¨é‡ï¼Œè¿˜æå‡äº†å™ªå£°ç¯å¢ƒä¸‹çš„æ–‡æœ¬è½¬è¯­éŸ³ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ¢å¤æ¸…æ´è¯­éŸ³æ–¹é¢ä¼˜äºç°æœ‰æœ€æ–°åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49a63609cdc23e745f58767ab60aad88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a5ebd56600882d57e45be7fd643b077.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-453df19eca6bbc0e7e95cdadcca49a68.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising"><a href="#Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising" class="headerlink" title="Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising"></a>Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising</h2><p><strong>Authors:Ye-Xin Lu, Hui-Peng Du, Fei Liu, Yang Ai, Zhen-Hua Ling</strong></p>
<p>Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•å€¾å‘äºä¿ç•™éŸ³é¢‘æç¤ºçš„å£°å­¦ç¯å¢ƒï¼Œä½†å½“éŸ³é¢‘æç¤ºåŒ…å«å™ªå£°æ—¶ï¼Œä¼šå¯¼è‡´åˆæˆè¯­éŸ³è´¨é‡ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºç¥ç»ç½‘ç»œç¼–è§£ç å™¨çš„è¯­éŸ³å»å™ªå™¨ï¼Œå¹¶å°†å…¶ä¸å…ˆè¿›çš„LLM-based TTSæ¨¡å‹LauraTTSç›¸ç»“åˆï¼Œå®ç°äº†å™ªå£°é²æ£’çš„é›¶æ ·æœ¬TTSã€‚æ‰€æå‡ºçš„ç¼–è§£ç å™¨å»å™ªå™¨ç”±éŸ³é¢‘ç¼–è§£ç å™¨ã€ä»¤ç‰Œå»å™ªå™¨å’ŒåµŒå…¥ç²¾ç‚¼å™¨ç»„æˆã€‚ä»¤ç‰Œå»å™ªå™¨ä»å™ªå£°ä»¤ç‰Œé¢„æµ‹å‰ä¸¤ä¸ªç»„çš„å¹²å‡€å£°å­¦ä»¤ç‰Œï¼Œè¿™å¯ä»¥ä½œä¸ºLauraTTSåˆæˆé«˜è´¨é‡ä¸ªæ€§åŒ–è¯­éŸ³çš„å£°å­¦æç¤ºï¼Œæˆ–é€šè¿‡åµŒå…¥ç²¾ç‚¼å™¨å’Œç¼–è§£ç å™¨è§£ç å™¨è½¬æ¢ä¸ºå¹²å‡€çš„è¯­éŸ³æ³¢å½¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ç¼–è§£ç å™¨å»å™ªå™¨ä¼˜äºæœ€æ–°çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•ï¼Œå¹¶ä¸”æ‰€æå‡ºçš„å™ªå£°é²æ£’çš„LauraTTSè¶…è¿‡äº†ä½¿ç”¨é™„åŠ SEæ¨¡å‹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13830v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œç¼–è§£ç å™¨çš„è¯­éŸ³å»å™ªå™¨ï¼Œå¹¶å°†å…¶ä¸å…ˆè¿›çš„LLMåŸºTTSæ¨¡å‹LauraTTSç›¸ç»“åˆï¼Œå®ç°äº†å™ªå£°é²æ£’çš„é›¶æ ·æœ¬TTSã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé€šè¿‡å¯¹å™ªå£°éŸ³é¢‘è¿›è¡Œå»å™ªå¤„ç†ï¼Œæé«˜åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåŸºTTSæ–¹æ³•åœ¨æ–‡æœ¬è½¬è¯­éŸ³è¿‡ç¨‹ä¸­å­˜åœ¨å™ªå£°é—®é¢˜ï¼Œå¯¼è‡´åˆæˆè¯­éŸ³è´¨é‡ä¸‹é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºç¥ç»ç½‘ç»œç¼–è§£ç å™¨çš„è¯­éŸ³å»å™ªå™¨ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç¼–è§£ç å™¨ã€ä»¤ç‰Œå»å™ªå™¨å’ŒåµŒå…¥ç²¾ç‚¼å™¨ã€‚</li>
<li>ä»¤ç‰Œå»å™ªå™¨èƒ½å¤Ÿä»å«å™ªå£°çš„éŸ³é¢‘ä¸­é¢„æµ‹å‡ºå‰ä¸¤ç»„å¹²å‡€çš„å£°å­¦ä»¤ç‰Œï¼Œä¸ºLauraTTSåˆæˆé«˜è´¨é‡ä¸ªæ€§åŒ–è¯­éŸ³æä¾›å£°å­¦æç¤ºã€‚</li>
<li>åµŒå…¥ç²¾ç‚¼å™¨å’Œç¼–è§£ç å™¨è§£ç å™¨å¯ä»¥å°†å£°å­¦ä»¤ç‰Œè½¬æ¢ä¸ºå¹²å‡€çš„è¯­éŸ³æ³¢å½¢ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç¼–è§£ç å™¨å»å™ªå™¨åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ç»“åˆå™ªå£°é²æ£’çš„LauraTTSæ¨¡å‹ï¼Œæ— éœ€é¢å¤–çš„è¯­éŸ³å¢å¼ºæ¨¡å‹å³å¯å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-996abe2385b849300f64b29747a9b154.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba58973ea83532ce32150a761118e124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-582a9fe21eabd309d2c2f7fd1e78776e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02c233800f084b2074fac3697b931790.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Counterspeech-the-ultimate-shield-Multi-Conditioned-Counterspeech-Generation-through-Attributed-Prefix-Learning"><a href="#Counterspeech-the-ultimate-shield-Multi-Conditioned-Counterspeech-Generation-through-Attributed-Prefix-Learning" class="headerlink" title="Counterspeech the ultimate shield! Multi-Conditioned Counterspeech   Generation through Attributed Prefix Learning"></a>Counterspeech the ultimate shield! Multi-Conditioned Counterspeech   Generation through Attributed Prefix Learning</h2><p><strong>Authors:Aswini Kumar Padhi, Anil Bandhakavi, Tanmoy Chakraborty</strong></p>
<p>Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems. </p>
<blockquote>
<p>åè¯­å·²è¢«è¯æ˜æ˜¯æ‰“å‡»ç½‘ç»œä»‡æ¨è¨€è®ºçš„æœ‰åŠ›å·¥å…·ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä»…é’ˆå¯¹ç‰¹å®šæ„å›¾ï¼ˆå•ä¸€å±æ€§ï¼‰ç”Ÿæˆåè¯­ã€‚ç„¶è€Œï¼Œè€ƒè™‘å¤šä¸ªå±æ€§åŒæ—¶çš„æ•´ä½“æ–¹æ³•å¯ä»¥äº§ç”Ÿæ›´ç²¾ç»†å’Œæœ‰æ•ˆçš„å“åº”ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†HiPPrOï¼Œå³å¸¦æœ‰åå¥½ä¼˜åŒ–çš„åˆ†å±‚å‰ç¼€å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å±æ€§ç‰¹å®šå‰ç¼€åµŒå…¥ç©ºé—´åœ¨åè¯­ç”Ÿæˆè¿‡ç¨‹ä¸­çš„åˆ†å±‚ä¼˜åŒ–æ•ˆæœã€‚ä¹‹åï¼Œæˆ‘ä»¬ç»“åˆäº†å‚è€ƒå’Œæ— å¥–åŠ±åå¥½ä¼˜åŒ–ï¼Œä»¥ç”Ÿæˆæ›´æœ‰å»ºè®¾æ€§çš„åè¯­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ‰©å±•äº†IntentCONANv2ï¼Œé€šè¿‡äº”ä½æ³¨é‡Šè€…å¯¹13973ä¸ªåè¯­å®ä¾‹è¿›è¡Œæƒ…ç»ªæ ‡ç­¾æ³¨é‡Šã€‚HiPPrOåˆ©ç”¨åˆ†å±‚å‰ç¼€ä¼˜åŒ–æœ‰æ•ˆåœ°æ•´åˆäº†è¿™äº›åŒé‡å±æ€§ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œä¸å‡ ç§åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒHiPPrOåœ¨æ„å›¾ä¸€è‡´æ€§æ–¹é¢å®ç°äº†çº¦38%çš„æ”¹è¿›ï¼Œåœ¨Rouge-1ã€Rouge-2å’ŒRouge-Læ–¹é¢åˆ†åˆ«å®ç°äº†çº¦3%ã€2%ã€3%çš„æ”¹è¿›ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œçªå‡ºäº†ç”Ÿæˆåè¯­çš„ç›¸å…³æ€§å’Œé€‚å½“æ€§çš„æé«˜ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¤šå±æ€§æ¡ä»¶åœ¨æå‡åè¯­ç”Ÿæˆç³»ç»Ÿæ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11958v2">PDF</a> Accepted in ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹æŠ—ç½‘ç»œä»‡æ¨è¨€è®ºçš„å¼ºæœ‰åŠ›å·¥å…·â€”â€”åè¯æœ¯ï¼ˆcounterspeechï¼‰ã€‚ä»¥å¾€ç ”ç©¶å¤šä¾§é‡äºç”Ÿæˆç‰¹å®šæ„å›¾çš„åè¯æœ¯ï¼Œè€Œå…¨é¢è€ƒè™‘å¤šä¸ªå±æ€§çš„æ–¹æ³•èƒ½ç”Ÿæˆæ›´ç²¾ç»†å’Œæœ‰æ•ˆçš„å›åº”ã€‚æœ¬æ–‡æå‡ºHiPPrOæ¡†æ¶ï¼Œåˆ©ç”¨å±æ€§ç‰¹å®šå‰ç¼€åµŒå…¥ç©ºé—´çš„å±‚æ¬¡ä¼˜åŒ–ç”Ÿæˆåè¯æœ¯ï¼Œå¹¶é€šè¿‡å‚è€ƒå’Œæ— å¥–åŠ±åå¥½ä¼˜åŒ–ç”Ÿæˆæ›´å…·å»ºè®¾æ€§çš„å†…å®¹ã€‚æ­¤å¤–ï¼Œæ‰©å±•äº†IntentCONANv2æ•°æ®é›†ï¼Œæ ‡æ³¨äº†å¸¦æœ‰æƒ…æ„Ÿæ ‡ç­¾çš„åè¯æœ¯å®ä¾‹ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºHiPPrOåœ¨æ„å›¾ç¬¦åˆåº¦ä¸Šæé«˜äº†çº¦38%ï¼Œåœ¨Rouge-1ã€Rouge-2å’ŒRouge-Lä¸Šåˆ†åˆ«æé«˜äº†çº¦3%ã€2%ã€3%ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†å…¶ä¼˜è¶Šæ€§ï¼Œå¼ºè°ƒç”Ÿæˆçš„åè¯æœ¯ç›¸å…³æ€§å’Œé€‚å½“æ€§å¾—åˆ°æé«˜ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†å¤šå±æ€§æ¡ä»¶åœ¨æå‡åè¯æœ¯ç”Ÿæˆç³»ç»Ÿæ•ˆèƒ½æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åè¯æœ¯æ˜¯æ‰“å‡»ç½‘ç»œä»‡æ¨è¨€è®ºçš„æœ‰æ•ˆå·¥å…·ã€‚</li>
<li>ä»¥å¾€ç ”ç©¶å¤šå…³æ³¨ç‰¹å®šæ„å›¾çš„åè¯æœ¯ç”Ÿæˆï¼Œä½†å…¨é¢è€ƒè™‘å¤šä¸ªå±æ€§å¯ç”Ÿæˆæ›´ç²¾ç»†å’Œæœ‰æ•ˆçš„å›åº”ã€‚</li>
<li>HiPPrOæ¡†æ¶åˆ©ç”¨å±æ€§ç‰¹å®šå‰ç¼€åµŒå…¥ç©ºé—´çš„å±‚æ¬¡ä¼˜åŒ–ç”Ÿæˆåè¯æœ¯ã€‚</li>
<li>é‡‡ç”¨äº†å‚è€ƒå’Œæ— å¥–åŠ±åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œæå‡åè¯æœ¯çš„å»ºè®¾æ€§ã€‚</li>
<li>æ‰©å±•äº†IntentCONANv2æ•°æ®é›†ï¼ŒåŒ…å«æƒ…æ„Ÿæ ‡ç­¾çš„åè¯æœ¯å®ä¾‹ã€‚</li>
<li>HiPPrOåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨æ„å›¾ç¬¦åˆåº¦å’Œå†…å®¹è¯„ä¼°ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f017f88457e0dc1802e06feab55d026.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea30737895843e827f990b402e939a0b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1620bb008ba79d3faab82f5af4606c01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3d78f214ce8f0f97b0e2c4ddd7e29e8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="xLSTM-SENet-xLSTM-for-Single-Channel-Speech-Enhancement"><a href="#xLSTM-SENet-xLSTM-for-Single-Channel-Speech-Enhancement" class="headerlink" title="xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement"></a>xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement</h2><p><strong>Authors:Nikolai Lund KÃ¼hne, Jan Ã˜stergaard, Jesper Jensen, Zheng-Hua Tan</strong></p>
<p>While attention-based architectures, such as Conformers, excel in speech enhancement, they face challenges such as scalability with respect to input sequence length. In contrast, the recently proposed Extended Long Short-Term Memory (xLSTM) architecture offers linear scalability. However, xLSTM-based models remain unexplored for speech enhancement. This paper introduces xLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. A comparative analysis reveals that xLSTM-and notably, even LSTM-can match or outperform state-of-the-art Mamba- and Conformer-based systems across various model sizes in speech enhancement on the VoiceBank+Demand dataset. Through ablation studies, we identify key architectural design choices such as exponential gating and bidirectionality contributing to its effectiveness. Our best xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- and Conformer-based systems of similar complexity on the Voicebank+DEMAND dataset. </p>
<blockquote>
<p>åŸºäºæ³¨æ„åŠ›çš„æ¶æ„ï¼Œå¦‚Conformersï¼Œåœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é¢ä¸´ç€è¾“å…¥åºåˆ—é•¿åº¦æ–¹é¢çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘æå‡ºçš„æ‰©å±•é•¿çŸ­æœŸè®°å¿†ï¼ˆxLSTMï¼‰æ¶æ„æä¾›äº†çº¿æ€§å¯æ‰©å±•æ€§ã€‚ç„¶è€Œï¼ŒåŸºäºxLSTMçš„æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢å°šæœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†xLSTM-SENetï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºxLSTMçš„å•é€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿã€‚å¯¹æ¯”åˆ†æè¡¨æ˜ï¼ŒxLSTMï¼ˆå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”šè‡³æ˜¯LSTMï¼‰åœ¨VoiceBank+Demandæ•°æ®é›†ä¸Šçš„è¯­éŸ³å¢å¼ºæ–¹é¢ï¼Œå¯ä»¥åŒ¹é…æˆ–ä¼˜äºæœ€æ–°çš„Mambaå’ŒConformerç³»ç»Ÿï¼Œå¹¶ä¸”é€‚ç”¨äºå„ç§æ¨¡å‹å¤§å°ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬ç¡®å®šäº†å…³é”®çš„è®¾è®¡é€‰æ‹©ï¼Œå¦‚æŒ‡æ•°é—¨æ§å’ŒåŒå‘æ€§ï¼Œè¿™äº›é€‰æ‹©å¯¹æ¨¡å‹çš„æœ‰æ•ˆæ€§æœ‰æ‰€è´¡çŒ®ã€‚æˆ‘ä»¬æœ€å¥½çš„åŸºäºxLSTMçš„æ¨¡å‹xLSTM-SENet2åœ¨Voicebank+DEMANDæ•°æ®é›†ä¸Šä¼˜äºå…·æœ‰ç›¸ä¼¼å¤æ‚åº¦çš„æœ€æ–°Mambaå’ŒConformerç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06146v2">PDF</a> Accepted at INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºxLSTMçš„è¯­éŸ³å¢å¼ºç³»ç»ŸxLSTM-SENetã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶æ¶æ„å¦‚Conformersï¼ŒxLSTMæ¶æ„å…·æœ‰çº¿æ€§å¯æ‰©å±•æ€§ä¼˜åŠ¿ï¼Œåœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æœ¬æ–‡é€šè¿‡å¯¹æ¯”åˆ†æå‘ç°ï¼ŒxLSTMï¼ˆå°¤å…¶æ˜¯LSTMï¼‰åœ¨VoiceBank+Demandæ•°æ®é›†ä¸Šçš„è¯­éŸ³å¢å¼ºæ•ˆæœå¯ä¸å½“å‰å…ˆè¿›çš„Mambaå’ŒConformerç³»ç»Ÿç›¸åª²ç¾æˆ–æ›´ä¼˜ç§€ã€‚é€šè¿‡æ¶ˆå»ç ”ç©¶ï¼Œç¡®å®šäº†å…³é”®æ¶æ„è®¾è®¡é€‰æ‹©å¦‚æŒ‡æ•°é—¨æ§å’ŒåŒå‘æ€§å¯¹å…¶æ€§èƒ½çš„å½±å“ã€‚å…¶ä¸­ï¼Œæœ€ä½³æ¨¡å‹xLSTM-SENet2åœ¨ç›¸ä¼¼å¤æ‚åº¦çš„æ¡ä»¶ä¸‹ï¼Œæ€§èƒ½ä¼˜äºå½“å‰å…ˆè¿›çš„Mambaå’ŒConformerç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>xLSTMæ¶æ„å…·æœ‰çº¿æ€§å¯æ‰©å±•æ€§ä¼˜åŠ¿ï¼Œé€‚ç”¨äºè¯­éŸ³å¢å¼ºä»»åŠ¡ã€‚</li>
<li>å¼•å…¥çš„xLSTM-SENetç³»ç»Ÿæ˜¯é¦–ä¸ªåŸºäºxLSTMçš„å•é€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿã€‚</li>
<li>xLSTMï¼ˆåŒ…æ‹¬LSTMï¼‰åœ¨VoiceBank+Demandæ•°æ®é›†ä¸Šçš„è¯­éŸ³å¢å¼ºæ•ˆæœä¸å½“å‰å…ˆè¿›æŠ€æœ¯ç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
<li>å¯¹æ¯”åˆ†ææ˜¾ç¤ºï¼ŒxLSTM-basedæ¨¡å‹åœ¨å„ç±»æ¨¡å‹å¤§å°ä¸Šå‡æœ‰è‰¯å¥½è¡¨ç°ã€‚</li>
<li>æ¶ˆå»ç ”ç©¶ç¡®å®šäº†æŒ‡æ•°é—¨æ§å’ŒåŒå‘æ€§ç­‰å…³é”®æ¶æ„è®¾è®¡å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“ã€‚</li>
<li>æœ€ä½³æ¨¡å‹xLSTM-SENet2åœ¨ç›¸ä¼¼å¤æ‚åº¦æ¡ä»¶ä¸‹ï¼Œæ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b90f9a10bfd1556c71fbc9bf5808479d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c45a589f7d466f9922910d606df4aa40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a88a3e80606698ac0b8ff2496eb49a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4484ca84a9449f52c1d7749dfc758e38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3c54a0a033a02f4541b766571ccad41.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TF-Mamba-A-Time-Frequency-Network-for-Sound-Source-Localization"><a href="#TF-Mamba-A-Time-Frequency-Network-for-Sound-Source-Localization" class="headerlink" title="TF-Mamba: A Time-Frequency Network for Sound Source Localization"></a>TF-Mamba: A Time-Frequency Network for Sound Source Localization</h2><p><strong>Authors:Yang Xiao, Rohan Kumar Das</strong></p>
<p>Sound source localization (SSL) determines the position of sound sources using multi-channel audio data. It is commonly used to improve speech enhancement and separation. Extracting spatial features is crucial for SSL, especially in challenging acoustic environments. Recently, a novel structure referred to as Mamba demonstrated notable performance across various sequence-based modalities. This study introduces the Mamba for SSL tasks. We consider the Mamba-based model to analyze spatial features from speech signals by fusing both time and frequency features, and we develop an SSL system called TF-Mamba. This system integrates time and frequency fusion, with Bidirectional Mamba managing both time-wise and frequency-wise processing. We conduct the experiments on the simulated and real datasets. Experiments show that TF-Mamba significantly outperforms other advanced methods. The code will be publicly released in due course. </p>
<blockquote>
<p>å£°æºå®šä½ï¼ˆSSLï¼‰ä½¿ç”¨å¤šé€šé“éŸ³é¢‘æ•°æ®æ¥ç¡®å®šå£°æºçš„ä½ç½®ã€‚å®ƒé€šå¸¸ç”¨äºæ”¹è¿›è¯­éŸ³å¢å¼ºå’Œåˆ†ç¦»ã€‚æå–ç©ºé—´ç‰¹å¾å¯¹äºSSLè‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­æ›´æ˜¯å¦‚æ­¤ã€‚æœ€è¿‘ï¼Œä¸€ç§æ–°å‹ç»“æ„Mambaåœ¨å„ç§åŸºäºåºåˆ—çš„æ¨¡æ€ä¸Šè¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶å°†Mambaå¼•å…¥SSLä»»åŠ¡ã€‚æˆ‘ä»¬è€ƒè™‘åŸºäºMambaçš„æ¨¡å‹ï¼Œé€šè¿‡èåˆæ—¶é—´å’Œé¢‘ç‡ç‰¹å¾æ¥åˆ†æè¯­éŸ³ä¿¡å·ä¸­çš„ç©ºé—´ç‰¹å¾ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåä¸ºTF-Mambaçš„SSLç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç»“åˆäº†æ—¶é—´å’Œé¢‘ç‡èåˆï¼ŒåŒå‘Mambaè´Ÿè´£ç®¡ç†æ—¶é—´å’Œé¢‘ç‡æ–¹é¢çš„å¤„ç†ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å®éªŒè¡¨æ˜ï¼ŒTF-Mambaæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚ä»£ç å°†åœ¨é€‚å½“çš„æ—¶å€™å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05034v2">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å£°éŸ³æºå®šä½ï¼ˆSSLï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨å¤šé€šé“éŸ³é¢‘æ•°æ®ç¡®å®šå£°éŸ³æºçš„ä½ç½®ï¼Œç”¨äºæé«˜è¯­éŸ³å¢å¼ºå’Œåˆ†ç¦»ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†åä¸ºMambaçš„æ–°å‹ç»“æ„åœ¨SSLä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡èåˆæ—¶é—´å’Œé¢‘ç‡ç‰¹å¾åˆ†æè¯­éŸ³ä¿¡å·çš„ç©ºé—´ç‰¹å¾ã€‚æ–‡ç« å¼€å‘äº†ä¸€ä¸ªåä¸ºTF-Mambaçš„SSLç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†æ—¶é—´å’Œé¢‘ç‡èåˆï¼Œä½¿ç”¨åŒå‘Mambaç®¡ç†æ—¶é—´å’Œé¢‘ç‡å¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒTF-Mambaæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å£°éŸ³æºå®šä½ï¼ˆSSLï¼‰æŠ€æœ¯åˆ©ç”¨å¤šé€šé“éŸ³é¢‘æ•°æ®ç¡®å®šå£°éŸ³æºä½ç½®ï¼Œå¸¸ç”¨äºè¯­éŸ³å¢å¼ºå’Œåˆ†ç¦»ã€‚</li>
<li>Mambaæ˜¯ä¸€ç§æ–°å‹ç»“æ„ï¼Œåœ¨SSLä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ã€‚</li>
<li>TF-Mambaç³»ç»Ÿç»“åˆäº†æ—¶é—´å’Œé¢‘ç‡èåˆï¼Œä½¿ç”¨åŒå‘Mambaç®¡ç†æ—¶é—´å’Œé¢‘ç‡å¤„ç†ã€‚</li>
<li>TF-Mambaç³»ç»Ÿåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>æ–‡ä¸­å¼ºè°ƒäº†æå–ç©ºé—´ç‰¹å¾åœ¨SSLä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­ã€‚</li>
<li>Mambaç»“æ„åœ¨åˆ†æå’Œå¤„ç†è¯­éŸ³ä¿¡å·çš„ç©ºé—´ç‰¹å¾æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b29d5a75d91b1b5718045571bbbfb85d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ac5a366bed88d37d01dc721cbddd737.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf301ec740f905a7be3f816d2426f511.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="aTENNuate-Optimized-Real-time-Speech-Enhancement-with-Deep-SSMs-on-Raw-Audio"><a href="#aTENNuate-Optimized-Real-time-Speech-Enhancement-with-Deep-SSMs-on-Raw-Audio" class="headerlink" title="aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw   Audio"></a>aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw   Audio</h2><p><strong>Authors:Yan Ru Pei, Ritik Shrivastava, FNU Sidharth</strong></p>
<p>We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The networkâ€™s performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†aTENNuateï¼Œè¿™æ˜¯ä¸€ä¸ªé…ç½®ç®€å•çš„æ·±åº¦çŠ¶æ€ç©ºé—´è‡ªç¼–ç å™¨ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼å®ç°é«˜æ•ˆçš„åœ¨çº¿åŸå§‹è¯­éŸ³å¢å¼ºã€‚è¯¥ç½‘ç»œçš„ä¸»è¦è¯„ä¼°æŒ‡æ ‡æ˜¯åŸå§‹è¯­éŸ³å»å™ªï¼Œæ­¤å¤–è¿˜å¯¹å…¶ä»–ä»»åŠ¡å¦‚è¶…åˆ†è¾¨ç‡å’Œå»é‡åŒ–è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨VoiceBank + DEMANDå’ŒMicrosoft DNS1åˆæˆæµ‹è¯•é›†ä¸Šå¯¹aTENNuateè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚è¯¥ç½‘ç»œåœ¨PESQå¾—åˆ†ã€å‚æ•°è®¡æ•°ã€MACså’Œå»¶è¿Ÿæ–¹é¢ä¼˜äºä¹‹å‰çš„å®æ—¶å»å™ªæ¨¡å‹ã€‚å³ä½¿ä½œä¸ºåŸå§‹æ³¢å½¢å¤„ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹æ¸…æ´ä¿¡å·çš„ä¿çœŸåº¦ä¹Ÿå¾ˆé«˜ï¼Œå‡ ä¹å¬ä¸åˆ°ä»»ä½•æ‚éŸ³ã€‚æ­¤å¤–ï¼Œå³ä½¿å˜ˆæ‚çš„è¾“å…¥å‹ç¼©åˆ°4000Hzå’Œ4ä½ï¼Œè¯¥æ¨¡å‹ä»ç„¶è¡¨ç°è‰¯å¥½ï¼Œè¿™è¡¨æ˜å…¶åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„é€šç”¨è¯­éŸ³å¢å¼ºèƒ½åŠ›ã€‚å¯ä»¥é€šè¿‡pip install attenuateæ¥å°è¯•ä½¿ç”¨å®ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03377v4">PDF</a> 7 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>aTENNuateæ˜¯ä¸€æ¬¾é’ˆå¯¹åœ¨çº¿åŸå§‹è¯­éŸ³å¢å¼ºè®¾è®¡çš„ç®€å•æ·±åº¦çŠ¶æ€ç©ºé—´è‡ªç¼–ç å™¨ã€‚å®ƒé‡‡ç”¨ç«¯åˆ°ç«¯çš„æ–¹å¼ï¼Œä¸»è¦ç”¨äºåŸå§‹è¯­éŸ³é™å™ªï¼Œå¹¶é¢å¤–ç”¨äºè¶…åˆ†è¾¨ç‡å’Œå»é‡åŒ–ä»»åŠ¡ã€‚åœ¨VoiceBank + DEMANDå’ŒMicrosoft DNS1åˆæˆæµ‹è¯•é›†ä¸Šè¿›è¡Œçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè¯¥ç½‘ç»œåœ¨PESQåˆ†æ•°ã€å‚æ•°è®¡æ•°ã€MACså’Œå»¶è¿Ÿæ–¹é¢ä¼˜äºä¹‹å‰çš„å®æ—¶é™å™ªæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå³ä½¿ä½œä¸ºåŸå§‹æ³¢å½¢å¤„ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹æ¸…æ´ä¿¡å·çš„ä¿çœŸåº¦ä¹Ÿå¾ˆé«˜ï¼Œå‡ ä¹å¬ä¸åˆ°å¤±çœŸã€‚å³ä½¿åœ¨å°†å˜ˆæ‚çš„è¾“å…¥å‹ç¼©åˆ°4000Hzå’Œ4ä½çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨ä½èµ„æºç¯å¢ƒä¸­ä»å…·æœ‰å‡ºè‰²çš„è¯­éŸ³å¢å¼ºèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>aTENNuateæ˜¯ä¸€ä¸ªç”¨äºåœ¨çº¿åŸå§‹è¯­éŸ³å¢å¼ºçš„æ·±åº¦çŠ¶æ€ç©ºé—´è‡ªç¼–ç å™¨ã€‚</li>
<li>å®ƒä¸»è¦ç”¨äºåŸå§‹è¯­éŸ³é™å™ªï¼Œå¹¶åœ¨VoiceBank + DEMANDå’ŒMicrosoft DNS1æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>aTENNuateåœ¨PESQåˆ†æ•°ã€å‚æ•°è®¡æ•°ã€MACså’Œå»¶è¿Ÿæ–¹é¢ä¼˜äºå…¶ä»–å®æ—¶é™å™ªæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å¯¹æ¸…æ´è¯­éŸ³ä¿¡å·çš„ä¿çœŸåº¦é«˜ï¼Œäº§ç”Ÿçš„éŸ³é¢‘å‡ ä¹æ— å¤±çœŸã€‚</li>
<li>aTENNuateåœ¨ä½èµ„æºç¯å¢ƒä¸­ï¼ˆå¦‚ä½¿ç”¨ä½é¢‘ç‡å’Œä½æ¯”ç‰¹ç‡çš„è¾“å…¥ï¼‰ä»å…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>å®‰è£…aTENNuateåªéœ€é€šè¿‡pip install attenuateå³å¯å®Œæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aaeb5099105a84413d2b9a5017e81763.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5aad36f0e6572fb0bcde56a1d065d356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f2249dfe499c62b19b4125ab81512a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcd52e7d75301aa8d35e9ec58051e63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4c0329fc995be23ad6682967b879b05.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="USEF-TSE-Universal-Speaker-Embedding-Free-Target-Speaker-Extraction"><a href="#USEF-TSE-Universal-Speaker-Embedding-Free-Target-Speaker-Extraction" class="headerlink" title="USEF-TSE: Universal Speaker Embedding Free Target Speaker Extraction"></a>USEF-TSE: Universal Speaker Embedding Free Target Speaker Extraction</h2><p><strong>Authors:Bang Zeng, Ming Li</strong></p>
<p>Target speaker extraction aims to separate the voice of a specific speaker from mixed speech. Traditionally, this process has relied on extracting a speaker embedding from a reference speech, in which a speaker recognition model is required. However, identifying an appropriate speaker recognition model can be challenging, and using the target speaker embedding as reference information may not be optimal for target speaker extraction tasks. This paper introduces a Universal Speaker Embedding-Free Target Speaker Extraction (USEF-TSE) framework that operates without relying on speaker embeddings. USEF-TSE utilizes a multi-head cross-attention mechanism as a frame-level target speaker feature extractor. This innovative approach allows mainstream speaker extraction solutions to bypass the dependency on speaker recognition models and better leverage the information available in the enrollment speech, including speaker characteristics and contextual details. Additionally, USEF-TSE can seamlessly integrate with other time-domain or time-frequency domain speech separation models to achieve effective speaker extraction. Experimental results show that our proposed method achieves state-of-the-art (SOTA) performance in terms of Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) on the WSJ0-2mix, WHAM!, and WHAMR! datasets, which are standard benchmarks for monaural anechoic, noisy and noisy-reverberant two-speaker speech separation and speaker extraction. The results on the LibriMix and the blind test set of the ICASSP 2023 DNS Challenge demonstrate that the model performs well on more diverse and out-of-domain data. For access to the source code, please visit: <a target="_blank" rel="noopener" href="https://github.com/ZBang/USEF-TSE">https://github.com/ZBang/USEF-TSE</a>. </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººæå–æ—¨åœ¨ä»æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»å‡ºç‰¹å®šè¯´è¯äººçš„å£°éŸ³ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹ä¾èµ–äºä»å‚è€ƒè¯­éŸ³ä¸­æå–è¯´è¯äººåµŒå…¥ï¼Œè¿™éœ€è¦è¯´è¯äººè¯†åˆ«æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¯†åˆ«åˆé€‚çš„è¯´è¯äººè¯†åˆ«æ¨¡å‹å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”ä½¿ç”¨ç›®æ ‡è¯´è¯äººåµŒå…¥ä½œä¸ºå‚è€ƒä¿¡æ¯å¯èƒ½ä¸æ˜¯é’ˆå¯¹ç›®æ ‡è¯´è¯äººæå–ä»»åŠ¡çš„æœ€ä½³é€‰æ‹©ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— é€šç”¨è¯´è¯äººåµŒå…¥ç›®æ ‡è¯´è¯äººæå–ï¼ˆUSEF-TSEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— éœ€ä¾èµ–è¯´è¯äººåµŒå…¥å³å¯è¿è¡Œã€‚USEF-TSEåˆ©ç”¨å¤šå¤´äº¤å‰æ³¨æ„æœºåˆ¶ä½œä¸ºå¸§çº§ç›®æ ‡è¯´è¯äººç‰¹å¾æå–å™¨ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•å…è®¸ä¸»æµè¯´è¯äººæå–è§£å†³æ–¹æ¡ˆç»•è¿‡å¯¹è¯´è¯äººè¯†åˆ«æ¨¡å‹çš„ä¾èµ–ï¼Œå¹¶æ›´å¥½åœ°åˆ©ç”¨æ³¨å†Œè¯­éŸ³ä¸­å¯ç”¨çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯´è¯äººç‰¹å¾å’Œä¸Šä¸‹æ–‡ç»†èŠ‚ã€‚æ­¤å¤–ï¼ŒUSEF-TSEå¯ä»¥æ— ç¼é›†æˆåˆ°å…¶ä»–æ—¶åŸŸæˆ–æ—¶é¢‘åŸŸè¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œä»¥å®ç°æœ‰æ•ˆçš„è¯´è¯äººæå–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨WSJ0-2mixã€WHAMï¼å’ŒWHAMRï¼æ•°æ®é›†ä¸Šçš„å°ºåº¦ä¸å˜ä¿¡å·å¤±çœŸæ¯”ï¼ˆSI-SDRï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¿™äº›æ•°æ®é›†æ˜¯å•å£°é“æ— å›å£°ã€å˜ˆæ‚å’Œå˜ˆæ‚æ··å“ä¸¤è¯´è¯äººè¯­éŸ³åˆ†ç¦»å’Œè¯´è¯äººæå–çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ã€‚åœ¨LibriMixå’ŒICASSP 2023 DNSæŒ‘æˆ˜çš„ç›²æµ‹è¯•é›†ä¸Šçš„ç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ›´å¤šæ ·åŒ–å’Œé¢†åŸŸå¤–çš„æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚å¦‚éœ€è®¿é—®æºä»£ç ï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/ZBang/USEF-TSE%E3%80%82">https://github.com/ZBang/USEF-TSEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02615v3">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing (TASLP)</p>
<p><strong>æ‘˜è¦</strong><br>ç›®æ ‡è¯´è¯äººæå–æ—¨åœ¨ä»æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»ç‰¹å®šè¯´è¯äººçš„å£°éŸ³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— é€šç”¨è¯´è¯äººåµŒå…¥ç›®æ ‡è¯´è¯äººæå–ï¼ˆUSEF-TSEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— éœ€ä¾èµ–è¯´è¯äººåµŒå…¥å³å¯è¿è¡Œã€‚USEF-TSEåˆ©ç”¨å¤šå¤´äº¤å‰æ³¨æ„æœºåˆ¶ä½œä¸ºå¸§çº§ç›®æ ‡è¯´è¯äººç‰¹å¾æå–å™¨ï¼Œä½¿ä¸»æµè¯´è¯äººæå–è§£å†³æ–¹æ¡ˆèƒ½å¤Ÿç»•è¿‡å¯¹è¯´è¯äººè¯†åˆ«æ¨¡å‹çš„ä¾èµ–ï¼Œæ›´å¥½åœ°åˆ©ç”¨æŠ¥åè¯­éŸ³ä¸­çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯´è¯äººç‰¹å¾å’Œä¸Šä¸‹æ–‡ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨WSJ0-2mixã€WHAM!å’ŒWHAMR!æ•°æ®é›†ä¸Šçš„å°ºåº¦ä¸å˜ä¿¡å·å¤±çœŸæ¯”ï¼ˆSI-SDRï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚è¯¥æ¨¡å‹åœ¨æ›´å¤šæ ·åŒ–å’ŒéåŸŸæ•°æ®çš„LibriMixå’ŒICASSP 2023 DNS Challengeçš„ç›²æµ‹è¯•é›†ä¸Šè¡¨ç°è‰¯å¥½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç›®æ ‡è¯´è¯äººæå–æ—¨åœ¨ä»æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»ç‰¹å®šè¯´è¯äººçš„å£°éŸ³ã€‚</li>
<li>ä¼ ç»Ÿçš„è¯´è¯äººæå–æ–¹æ³•ä¾èµ–äºä»å‚è€ƒè¯­éŸ³ä¸­æå–è¯´è¯äººåµŒå…¥ï¼Œè¿™éœ€è¦ä½¿ç”¨è¯´è¯äººè¯†åˆ«æ¨¡å‹ï¼Œä½†è¯†åˆ«åˆé€‚çš„æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>USEF-TSEæ¡†æ¶æ— éœ€ä¾èµ–é€šç”¨è¯´è¯äººåµŒå…¥è¿›è¡Œç›®æ ‡è¯´è¯äººæå–ã€‚</li>
<li>USEF-TSEä½¿ç”¨å¤šå¤´äº¤å‰æ³¨æ„æœºåˆ¶ä½œä¸ºå¸§çº§ç›®æ ‡è¯´è¯äººç‰¹å¾æå–å™¨ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æŠ¥åè¯­éŸ³ä¸­çš„ä¿¡æ¯ã€‚</li>
<li>USEF-TSEå¯ä»¥ä¸å…¶ä»–æ—¶åŸŸæˆ–æ—¶é¢‘åŸŸè¯­éŸ³åˆ†ç¦»æ¨¡å‹æ— ç¼é›†æˆï¼Œä»¥å®ç°æœ‰æ•ˆçš„è¯´è¯äººæå–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUSEF-TSEåœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34a23db5bbe1ab9ab7a80d621f4e62c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1fb77093b0c33ce9b82323f50a59e76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bc5942f67cba55e25a622e43f5c80cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de6eddbed0e27b0dea332db968d17308.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47cf946a696cdd0fe49676bccf321c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd2c409b622a88073bd1eb90b41e47c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a263f60f0937bcab0fdd9413a099b08.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-55f3832f5ad23d02c6e631db6fc7c428.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Provably Near-Optimal Federated Ensemble Distillation with Negligible   Overhead
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-63007b140ebcb19ff596b0e50b37e300.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  View-Invariant Pixelwise Anomaly Detection in Multi-object Scenes with   Adaptive View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23523.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
