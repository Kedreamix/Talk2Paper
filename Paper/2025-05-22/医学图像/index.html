<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Automated Fetal Biometry Assessment with Deep Ensembles using   Sparse-Sampling of 2D Intrapartum Ultrasound Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-2dbd53b28128cc9bbb530fa5610b868a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-22-æ›´æ–°"><a href="#2025-05-22-æ›´æ–°" class="headerlink" title="2025-05-22 æ›´æ–°"></a>2025-05-22 æ›´æ–°</h1><h2 id="Automated-Fetal-Biometry-Assessment-with-Deep-Ensembles-using-Sparse-Sampling-of-2D-Intrapartum-Ultrasound-Images"><a href="#Automated-Fetal-Biometry-Assessment-with-Deep-Ensembles-using-Sparse-Sampling-of-2D-Intrapartum-Ultrasound-Images" class="headerlink" title="Automated Fetal Biometry Assessment with Deep Ensembles using   Sparse-Sampling of 2D Intrapartum Ultrasound Images"></a>Automated Fetal Biometry Assessment with Deep Ensembles using   Sparse-Sampling of 2D Intrapartum Ultrasound Images</h2><p><strong>Authors:Jayroop Ramesh, Valentin Bacher, Mark C. Eid, Hoda Kalabizadeh, Christian Rupprecht, Ana IL Namburete, Pak-Hei Yeung, Madeleine K. Wyburd, Nicola K. Dinsdale</strong></p>
<p>The International Society of Ultrasound advocates Intrapartum Ultrasound (US) Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression through changes in fetal head position. Two reliable ultrasound-derived parameters that are used to predict outcomes of instrumental vaginal delivery are the angle of progression (AoP) and head-symphysis distance (HSD). In this work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we propose an automated fetal biometry measurement pipeline to reduce intra- and inter-observer variability and improve measurement reliability. Our pipeline consists of three key tasks: (i) classification of standard planes (SP) from US videos, (ii) segmentation of fetal head and pubic symphysis from the detected SPs, and (iii) computation of the AoP and HSD from the segmented regions. We perform sparse sampling to mitigate class imbalances and reduce spurious correlations in task (i), and utilize ensemble-based deep learning methods for task (i) and (ii) to enhance generalizability under different US acquisition settings. Finally, to promote robustness in task iii) with respect to the structural fidelity of measurements, we retain the largest connected components and apply ellipse fitting to the segmentations. Our solution achieved ACC: 0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71, $\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of 4 patients and 224 US frames. The results from the proposed automated pipeline can improve the understanding of labour arrest causes and guide the development of clinical risk stratification tools for efficient and effective prenatal care. </p>
<blockquote>
<p>å›½é™…è¶…å£°å­¦ä¼šå€¡å¯¼åœ¨äº§ç§‘å’Œå¦‡ç§‘é¢†åŸŸä½¿ç”¨äº§æ—¶è¶…å£°ï¼ˆUSï¼‰æˆåƒæŠ€æœ¯ï¼Œé€šè¿‡ç›‘æµ‹èƒå„¿å¤´éƒ¨ä½ç½®çš„å˜åŒ–æ¥ç›‘æ§äº§ç¨‹è¿›å±•ã€‚é¢„æµ‹å™¨æ¢°åˆ†å¨©ç»“æœçš„ä¸¤ä¸ªå¯é çš„è¶…å£°å‚æ•°æ˜¯è¿›å±•è§’åº¦ï¼ˆAoPï¼‰å’Œå¤´éª¨ç›†å…¥å£è·ç¦»ï¼ˆHSDï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä½œä¸ºäº§æ—¶è¶…å£°æŒ‘æˆ˜èµ›ï¼ˆIUGCï¼‰2024çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–èƒå„¿ç”Ÿç‰©æµ‹é‡ç®¡é“ï¼Œæ—¨åœ¨å‡å°‘è§‚å¯Ÿè€…å†…éƒ¨å’Œè§‚å¯Ÿè€…ä¹‹é—´çš„å˜å¼‚æ€§ï¼Œæé«˜æµ‹é‡å¯é æ€§ã€‚æˆ‘ä»¬çš„ç®¡é“åŒ…å«ä¸‰ä¸ªå…³é”®ä»»åŠ¡ï¼šï¼ˆiï¼‰ä»è¶…å£°è§†é¢‘ä¸­åˆ†ç±»æ ‡å‡†å¹³é¢ï¼ˆSPï¼‰ï¼Œï¼ˆiiï¼‰ä»æ£€æµ‹åˆ°çš„SPä¸­åˆ†å‰²èƒå„¿å¤´éƒ¨å’Œè€»éª¨å¼“ï¼Œä»¥åŠï¼ˆiiiï¼‰ä»åˆ†å‰²åŒºåŸŸè®¡ç®—AoPå’ŒHSDã€‚æˆ‘ä»¬é€šè¿‡ç¨€ç–é‡‡æ ·ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¹¶å‡å°‘ä»»åŠ¡ï¼ˆiï¼‰ä¸­çš„è™šå‡ç›¸å…³æ€§ï¼Œå¹¶åŸºäºç»„åˆæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åº”ç”¨äºä»»åŠ¡ï¼ˆiï¼‰å’Œï¼ˆiiï¼‰ï¼Œä»¥å¢å¼ºåœ¨ä¸åŒè¶…å£°é‡‡é›†ç¯å¢ƒä¸‹çš„é€šç”¨æ€§ã€‚æœ€åï¼Œä¸ºäº†ä¿ƒè¿›ä»»åŠ¡iiiçš„ç»“æ„æµ‹é‡å¿ å®æ€§ï¼Œæˆ‘ä»¬ä¿ç•™æœ€å¤§çš„è¿é€šç»„ä»¶ï¼Œå¹¶å¯¹åˆ†å‰²åº”ç”¨æ¤­åœ†æ‹Ÿåˆã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåœ¨æœªç»è®­ç»ƒçš„4ä½æ‚£è€…å’Œ224ä¸ªè¶…å£°å¸§çš„æµ‹è¯•é›†ä¸Šå–å¾—äº†ä»¥ä¸‹æˆæœï¼šACCï¼š0.9452ï¼ŒF1ï¼š0.9225ï¼ŒAUCï¼š0.983ï¼ŒMCCï¼š0.8361ï¼ŒDSCï¼š0.918ï¼ŒHDï¼š19.73ï¼ŒASDï¼š5.71ï¼ŒÎ”AoPï¼š8.9 å’ŒÎ”HSDï¼š14.35ã€‚æ‰€æå‡ºçš„è‡ªåŠ¨åŒ–ç®¡é“çš„ç»“æœå¯ä»¥æé«˜å¯¹äº§ç¨‹åœæ»åŸå› çš„ç†è§£ï¼Œå¹¶æœ‰åŠ©äºå¼€å‘ç”¨äºé«˜æ•ˆæœ‰æ•ˆäº§å‰æŠ¤ç†çš„ä¸´åºŠé£é™©åˆ†å±‚å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14572v1">PDF</a> Top 5 in MICCAI IUGC 2024: Intrapartum Ultrasound Grand Challenge &amp;   Runners up in Classification!</p>
<p><strong>Summary</strong><br>     è¶…å£°æˆåƒå­¦ä¼šæå€¡åœ¨å¦‡äº§ç§‘é¢†åŸŸä½¿ç”¨äº§æ—¶è¶…å£°æˆåƒæŠ€æœ¯æ¥ç›‘æµ‹äº§ç¨‹è¿›å±•ã€‚è¯¥ç ”ç©¶æå‡ºä¸€ä¸ªè‡ªåŠ¨åŒ–èƒå„¿ç”Ÿç‰©æµ‹é‡ç®¡é“ï¼Œç”¨äºå‡å°‘è§‚å¯Ÿè€…å†…éƒ¨å’Œå¤–éƒ¨çš„å˜å¼‚æ€§ï¼Œæé«˜æµ‹é‡å¯é æ€§ã€‚è¯¥ç®¡é“åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ä»»åŠ¡ï¼šä»è¶…å£°è§†é¢‘ä¸­åˆ†ç±»æ ‡å‡†å¹³é¢ã€ä»æ£€æµ‹åˆ°çš„æ ‡å‡†å¹³é¢ä¸­åˆ†å‰²èƒå„¿å¤´éƒ¨å’Œè€»éª¨è”åˆï¼Œä»¥åŠè®¡ç®—è¿›å±•è§’å’Œå¤´éª¨è·ç¦»ã€‚è¯¥ç ”ç©¶é‡‡ç”¨ç¨€ç–é‡‡æ ·æŠ€æœ¯ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶åˆ©ç”¨åŸºäºé›†æˆæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æé«˜ä»»åŠ¡çš„ä¸€èˆ¬æ€§ã€‚æœ€ç»ˆï¼Œè¯¥è‡ªåŠ¨åŒ–ç®¡é“æé«˜äº†å¯¹äº§ç¨‹åœæ»åŸå› çš„ç†è§£ï¼Œæœ‰åŠ©äºå¼€å‘ä¸´åºŠé£é™©åˆ†å±‚å·¥å…·ï¼Œä¸ºé«˜æ•ˆæœ‰æ•ˆçš„äº§å‰æŠ¤ç†æä¾›æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº§æ—¶è¶…å£°æˆåƒæŠ€æœ¯è¢«ç”¨äºç›‘æµ‹äº§ç¨‹è¿›å±•ã€‚</li>
<li>ä¸€ä¸ªè‡ªåŠ¨åŒ–èƒå„¿ç”Ÿç‰©æµ‹é‡ç®¡é“è¢«æå‡ºï¼Œä»¥å‡å°‘è§‚å¯Ÿè€…å˜å¼‚å¹¶æé«˜æµ‹é‡å¯é æ€§ã€‚</li>
<li>è¯¥ç®¡é“åŒ…å«ä¸‰ä¸ªå…³é”®ä»»åŠ¡ï¼šä»è¶…å£°è§†é¢‘ä¸­åˆ†ç±»æ ‡å‡†å¹³é¢ã€åˆ†å‰²èƒå„¿å¤´éƒ¨å’Œè€»éª¨è”åˆï¼Œä»¥åŠè®¡ç®—è¿›å±•è§’å’Œå¤´éª¨è·ç¦»ã€‚</li>
<li>ç¨€ç–é‡‡æ ·æŠ€æœ¯ç”¨äºç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>é›†æˆæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ç”¨äºæé«˜ä»»åŠ¡çš„ä¸€èˆ¬æ€§ã€‚</li>
<li>è‡ªåŠ¨åŒ–ç®¡é“æœ‰åŠ©äºç†è§£äº§ç¨‹åœæ»çš„åŸå› ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-118562a71d01b35b36d73a41525bc479.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5095dddda87bfe8d884de2409f6bdd06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9015d2c43f4706a8ef87590e5d14c737.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f148fdbd75cc0a54de86590e4b6b1b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-626cdac82d44c118b8270c9d340ccc65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9aaaf330f229a52a506aeefc478e2411.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dynadiff-Single-stage-Decoding-of-Images-from-Continuously-Evolving-fMRI"><a href="#Dynadiff-Single-stage-Decoding-of-Images-from-Continuously-Evolving-fMRI" class="headerlink" title="Dynadiff: Single-stage Decoding of Images from Continuously Evolving   fMRI"></a>Dynadiff: Single-stage Decoding of Images from Continuously Evolving   fMRI</h2><p><strong>Authors:MarlÃ¨ne Careil, Yohann Benchetrit, Jean-RÃ©mi King</strong></p>
<p>Brain-to-image decoding has been recently propelled by the progress in generative AI models and the availability of large ultra-high field functional Magnetic Resonance Imaging (fMRI). However, current approaches depend on complicated multi-stage pipelines and preprocessing steps that typically collapse the temporal dimension of brain recordings, thereby limiting time-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural Activity Diffusion for Image Reconstruction), a new single-stage diffusion model designed for reconstructing images from dynamically evolving fMRI recordings. Our approach offers three main contributions. First, Dynadiff simplifies training as compared to existing approaches. Second, our model outperforms state-of-the-art models on time-resolved fMRI signals, especially on high-level semantic image reconstruction metrics, while remaining competitive on preprocessed fMRI data that collapse time. Third, this approach allows a precise characterization of the evolution of image representations in brain activity. Overall, this work lays the foundation for time-resolved brain-to-image decoding. </p>
<blockquote>
<p>è„‘åˆ°å›¾åƒçš„è§£ç æœ€è¿‘å¾—åˆ°äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„è¿›æ­¥å’Œå¤§å‹è¶…é«˜åœºåŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰çš„å¯ç”¨æ€§æ¨åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºå¤æ‚çš„å¤šé˜¶æ®µç®¡é“å’Œé¢„å¤„ç†æ­¥éª¤ï¼Œè¿™äº›æ­¥éª¤é€šå¸¸ä¼šå‹ç¼©è„‘è®°å½•çš„æš‚æ—¶ç»´åº¦ï¼Œä»è€Œé™åˆ¶äº†æ—¶é—´è§£æçš„è„‘è§£ç å™¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼•å…¥äº†Dynadiffï¼ˆåŠ¨æ€ç¥ç»æ´»åŠ¨æ‰©æ•£å›¾åƒé‡å»ºï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å•é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨ä»åŠ¨æ€å‘å±•çš„fMRIè®°å½•ä¸­é‡å»ºå›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸‰ä¸ªä¸»è¦çš„è´¡çŒ®ã€‚é¦–å…ˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDynadiffç®€åŒ–äº†è®­ç»ƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è§£ææ€§fMRIä¿¡å·ä¸Šä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜çº§è¯­ä¹‰å›¾åƒé‡å»ºæŒ‡æ ‡ä¸Šè¡¨ç°çªå‡ºï¼ŒåŒæ—¶åœ¨å‹ç¼©æ—¶é—´çš„é¢„å¤„ç†fMRIæ•°æ®ä¸Šä¿æŒç«äº‰åŠ›ã€‚ç¬¬ä¸‰ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥ç²¾ç¡®æè¿°å›¾åƒè¡¨ç¤ºåœ¨è„‘æ´»åŠ¨ä¸­çš„æ¼”å˜ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸ºæ—¶é—´è§£æçš„è„‘åˆ°å›¾åƒè§£ç å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14556v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è„‘å›¾åƒè§£ç æŠ€æœ¯å› ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„è¿›æ­¥å’Œè¶…é«˜åœºåŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰çš„æ™®åŠè€Œå¾—åˆ°æ¨åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ä¾èµ–äºå¤æ‚çš„å¤šé˜¶æ®µç®¡é“å’Œé¢„å¤„ç†æ­¥éª¤ï¼Œé€šå¸¸ä¼šå¿½ç•¥è„‘è®°å½•çš„æ—¶é—´ç»´åº¦ï¼Œä»è€Œé™åˆ¶äº†æ—¶é—´è§£æçš„å¤§è„‘è§£ç å™¨ã€‚æœ¬æ–‡ä»‹ç»äº†Dynadiffï¼ˆç”¨äºå›¾åƒé‡å»ºçš„åŠ¨æ€ç¥ç»æ´»åŠ¨æ‰©æ•£ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å•é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œå¯ä»åŠ¨æ€fMRIè®°å½•ä¸­é‡å»ºå›¾åƒã€‚Dynadiffæä¾›äº†ä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼šç®€åŒ–è®­ç»ƒã€åœ¨æ—¶è§£æçš„fMRIä¿¡å·ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€å‡†ç¡®åˆ»ç”»å¤§è„‘æ´»åŠ¨ä¸­å›¾åƒè¡¨ç¤ºçš„å˜åŒ–ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œä¸ºæ—¶é—´è§£æçš„å¤§è„‘å›¾åƒè§£ç å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è„‘å›¾åƒè§£ç æŠ€æœ¯å—ç›Šäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„è¿›æ­¥å’Œè¶…é«˜åœºåŠŸèƒ½ç£å…±æŒ¯æˆåƒçš„æ™®åŠã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨å¤æ‚çš„å¤šé˜¶æ®µç®¡é“å’Œé¢„å¤„ç†æ­¥éª¤ï¼Œå¯¼è‡´å¿½ç•¥è„‘è®°å½•çš„æ—¶é—´ç»´åº¦ã€‚</li>
<li>Dynadiffæ˜¯ä¸€ç§æ–°çš„å•é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œå¯ä»åŠ¨æ€fMRIè®°å½•ä¸­é‡å»ºå›¾åƒã€‚</li>
<li>Dynadiffç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>Dynadiffåœ¨æ—¶è§£æçš„fMRIä¿¡å·ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜çº§è¯­ä¹‰å›¾åƒé‡å»ºæŒ‡æ ‡ä¸Šã€‚</li>
<li>Dynadiffåœ¨é¢„å¤„ç†å¿½ç•¥æ—¶é—´ç»´åº¦çš„fMRIæ•°æ®ä¸Šä»å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb8bf623321fade05773ad21c5caba3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24d1ddbc09c8ffd5617448f41666c2fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57296d7d852e28028dc8b59efc2c6fa0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a31541639e99e7e900dcca98c9948e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c14e54882d8694d7c4bc83579daeda03.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RADAR-Enhancing-Radiology-Report-Generation-with-Supplementary-Knowledge-Injection"><a href="#RADAR-Enhancing-Radiology-Report-Generation-with-Supplementary-Knowledge-Injection" class="headerlink" title="RADAR: Enhancing Radiology Report Generation with Supplementary   Knowledge Injection"></a>RADAR: Enhancing Radiology Report Generation with Supplementary   Knowledge Injection</h2><p><strong>Authors:Wenjun Hou, Yi Cheng, Kaishuai Xu, Heng Li, Yan Hu, Wenjie Li, Jiang Liu</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in various domains, including radiology report generation. Previous approaches have attempted to utilize multimodal LLMs for this task, enhancing their performance through the integration of domain-specific knowledge retrieval. However, these approaches often overlook the knowledge already embedded within the LLMs, leading to redundant information integration and inefficient utilization of learned representations. To address this limitation, we propose RADAR, a framework for enhancing radiology report generation with supplementary knowledge injection. RADAR improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. Specifically, it first extracts the modelâ€™s acquired knowledge that aligns with expert image-based classification outputs. It then retrieves relevant supplementary knowledge to further enrich this information. Finally, by aggregating both sources, RADAR generates more accurate and informative radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU X-ray demonstrate that our model outperforms state-of-the-art LLMs in both language quality and clinical accuracy </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢ï¼Œè¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ä»¥å‰çš„æ–¹æ³•è¯•å›¾ä½¿ç”¨å¤šæ¨¡å¼LLMsæ¥å®Œæˆæ­¤ä»»åŠ¡ï¼Œé€šè¿‡æ•´åˆç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ£€ç´¢æ¥å¢å¼ºå®ƒä»¬çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½è§†äº†LLMsä¸­å·²ç»åµŒå…¥çš„çŸ¥è¯†ï¼Œå¯¼è‡´å†—ä½™çš„ä¿¡æ¯é›†æˆå’Œæ‰€å­¦è¡¨ç¤ºçš„åˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RADARï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è¡¥å……çŸ¥è¯†æ³¨å…¥å¢å¼ºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„æ¡†æ¶ã€‚RADARé€šè¿‡ç³»ç»Ÿåœ°åˆ©ç”¨LLMçš„å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æ£€ç´¢ä¿¡æ¯æ¥æ”¹å–„æŠ¥å‘Šç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé¦–å…ˆæå–ä¸ä¸“å®¶å›¾åƒåˆ†ç±»è¾“å‡ºç›¸ç¬¦çš„æ¨¡å‹è·å–çš„çŸ¥è¯†ã€‚ç„¶åï¼Œå®ƒæ£€ç´¢ç›¸å…³çš„è¡¥å……çŸ¥è¯†æ¥è¿›ä¸€æ­¥ä¸°å¯Œè¿™äº›ä¿¡æ¯ã€‚æœ€åï¼Œé€šè¿‡èšåˆè¿™ä¸¤ä¸ªæ¥æºï¼ŒRADARç”Ÿæˆæ›´å‡†ç¡®å’Œä¿¡æ¯ä¸°å¯Œçš„æ”¾å°„å­¦æŠ¥å‘Šã€‚åœ¨MIMIC-CXRã€CheXpert-Pluså’ŒIU X-rayä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯­è¨€è´¨é‡å’Œä¸´åºŠå‡†ç¡®æ€§æ–¹é¢éƒ½ä¼˜äºæœ€æ–°çš„LLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14318v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ…æ‹¬æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆåœ¨å†…çš„å„é¢†åŸŸå±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚å°½ç®¡å…ˆå‰çš„æ–¹æ³•å°è¯•ä½¿ç”¨å¤šæ¨¡æ€LLMså¹¶æ•´åˆé¢†åŸŸç‰¹å®šçŸ¥è¯†æ£€ç´¢æ¥æå‡æ€§èƒ½ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†LLMså†…éƒ¨å·²åµŒå…¥çš„çŸ¥è¯†ï¼Œå¯¼è‡´ä¿¡æ¯æ•´åˆå†—ä½™å’Œè¡¨å¾åˆ©ç”¨ä½æ•ˆã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºRADARæ¡†æ¶ï¼Œé€šè¿‡æ³¨å…¥è¡¥å……çŸ¥è¯†æå‡æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚RADARé€šè¿‡ç³»ç»Ÿåˆ©ç”¨LLMçš„å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æ£€ç´¢ä¿¡æ¯æ¥æ”¹å–„æŠ¥å‘Šç”Ÿæˆã€‚å®ƒé¦–å…ˆæå–ä¸ä¸“å®¶å›¾åƒåˆ†ç±»è¾“å‡ºå¯¹é½çš„æ¨¡å‹ä¹ å¾—çŸ¥è¯†ï¼Œç„¶åæ£€ç´¢ç›¸å…³è¡¥å……çŸ¥è¯†è¿›ä¸€æ­¥ä¸°å¯Œè¿™äº›ä¿¡æ¯ã€‚æœ€åï¼Œé€šè¿‡èšåˆè¿™ä¸¤éƒ¨åˆ†çŸ¥è¯†ï¼ŒRADARç”Ÿæˆæ›´å‡†ç¡®å’Œæ›´å…·ä¿¡æ¯é‡çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚åœ¨MIMIC-CXRã€CheXpert-Pluså’ŒIU X-rayä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯­è¨€è´¨é‡å’Œä¸´åºŠå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºæœ€æ–°LLMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²åœ¨åŒ…æ‹¬æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆåœ¨å†…çš„å¤šä¸ªé¢†åŸŸå±•ç°å¼ºå¤§èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è™½å°è¯•æ•´åˆå¤šæ¨¡æ€LLMså’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†ä»¥æå‡æ€§èƒ½ï¼Œä½†å¿½è§†äº†LLMså†…éƒ¨çŸ¥è¯†ï¼Œå¯¼è‡´ä¿¡æ¯å†—ä½™å’Œæ•ˆç‡ä¸é«˜ã€‚</li>
<li>RADARæ¡†æ¶æ—¨åœ¨é€šè¿‡æ³¨å…¥è¡¥å……çŸ¥è¯†æå‡æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼Œç³»ç»Ÿåˆ©ç”¨LLMçš„å†…éƒ¨çŸ¥è¯†å’Œå¤–éƒ¨æ£€ç´¢ä¿¡æ¯ã€‚</li>
<li>RADARèƒ½æå–ä¸ä¸“å®¶å›¾åƒåˆ†ç±»è¾“å‡ºå¯¹é½çš„æ¨¡å‹çŸ¥è¯†ï¼Œå¹¶æ£€ç´¢ç›¸å…³è¡¥å……çŸ¥è¯†ä¸°å¯Œè¿™äº›ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ç»“åˆå†…å¤–éƒ¨çŸ¥è¯†ï¼ŒRADARèƒ½ç”Ÿæˆæ›´å‡†ç¡®å’Œæ›´å…·ä¿¡æ¯é‡çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRADARåœ¨è¯­è¨€å’Œä¸´åºŠå‡†ç¡®æ€§æ–¹é¢è¶…è¶Šç°æœ‰LLMsã€‚</li>
<li>RADARæ¡†æ¶å¯¹äºæé«˜æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-48481e4c3c66fbf36a6b8ae292a79b77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cf4a52ebd60501ee92d5754dea193b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fc9f582c8dde055385c7870a9fe726b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1de477f0d052ad3c1234d652ac8ff30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dbd53b28128cc9bbb530fa5610b868a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4bee5b6d4282aea27d511b9e0cd6f09.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-Concept-Driven-Logical-Rules-for-Interpretable-and-Generalizable-Medical-Image-Classification"><a href="#Learning-Concept-Driven-Logical-Rules-for-Interpretable-and-Generalizable-Medical-Image-Classification" class="headerlink" title="Learning Concept-Driven Logical Rules for Interpretable and   Generalizable Medical Image Classification"></a>Learning Concept-Driven Logical Rules for Interpretable and   Generalizable Medical Image Classification</h2><p><strong>Authors:Yibo Gao, Hangqi Zhou, Zheyao Gao, Bomin Wang, Shangqi Gao, Sihan Wang, Xiahai Zhuang</strong></p>
<p>The pursuit of decision safety in clinical applications highlights the potential of concept-based methods in medical imaging. While these models offer active interpretability, they often suffer from concept leakages, where unintended information within soft concept representations undermines both interpretability and generalizability. Moreover, most concept-based models focus solely on local explanations (instance-level), neglecting the global decision logic (dataset-level). To address these limitations, we propose Concept Rule Learner (CRL), a novel framework to learn Boolean logical rules from binarized visual concepts. CRL employs logical layers to capture concept correlations and extract clinically meaningful rules, thereby providing both local and global interpretability. Experiments on two medical image classification tasks show that CRL achieves competitive performance with existing methods while significantly improving generalizability to out-of-distribution data. The code of our work is available at <a target="_blank" rel="noopener" href="https://github.com/obiyoag/crl">https://github.com/obiyoag/crl</a>. </p>
<blockquote>
<p>åœ¨ä¸´åºŠåº”ç”¨ä¸­è¿½æ±‚å†³ç­–å®‰å…¨æ€§å‡¸æ˜¾äº†åŸºäºæ¦‚å¿µçš„æ–¹æ³•åœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›ã€‚è™½ç„¶è¿™äº›æ¨¡å‹æä¾›äº†ç§¯æçš„å¯è§£é‡Šæ€§ï¼Œä½†å®ƒä»¬å¸¸å¸¸å—åˆ°æ¦‚å¿µæ³„æ¼çš„å½±å“ï¼Œå³è½¯æ¦‚å¿µè¡¨ç¤ºä¸­çš„æ„å¤–ä¿¡æ¯ç ´åäº†å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°åŸºäºæ¦‚å¿µæ¨¡å‹åªå…³æ³¨å±€éƒ¨è§£é‡Šï¼ˆå®ä¾‹çº§ï¼‰ï¼Œå¿½ç•¥äº†å…¨å±€å†³ç­–é€»è¾‘ï¼ˆæ•°æ®é›†çº§ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ¦‚å¿µè§„åˆ™å­¦ä¹ è€…ï¼ˆCRLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä»äºŒå…ƒåŒ–è§†è§‰æ¦‚å¿µä¸­å­¦ä¹ å¸ƒå°”é€»è¾‘è§„åˆ™çš„æ–°æ¡†æ¶ã€‚CRLé€šè¿‡é€»è¾‘å±‚æ•æ‰æ¦‚å¿µç›¸å…³æ€§å¹¶æå–å…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§„åˆ™ï¼Œä»è€Œæä¾›å±€éƒ¨å’Œå…¨å±€çš„å¯è§£é‡Šæ€§ã€‚åœ¨ä¸¤ä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCRLåœ¨ç°æœ‰æ–¹æ³•ä¸­è¡¨ç°è‰¯å¥½ï¼Œåœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šæ˜¾è‘—æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/obiyoag/crl%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/obiyoag/crlè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14049v1">PDF</a> early accepted by MICCAI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦æˆåƒä¸­å†³ç­–å®‰å…¨æ€§çš„è¿½æ±‚å‡¸æ˜¾äº†æ¦‚å¿µæ–¹æ³•å­¦çš„æ½œåŠ›ã€‚ä½†æ¦‚å¿µæ¨¡å‹å­˜åœ¨æ¦‚å¿µæ³„éœ²é—®é¢˜ï¼Œä¸”å¤šå…³æ³¨å±€éƒ¨è§£é‡Šï¼ˆå®ä¾‹çº§ï¼‰ï¼Œå¿½ç•¥å…¨å±€å†³ç­–é€»è¾‘ï¼ˆæ•°æ®é›†çº§ï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºConcept Rule Learnerï¼ˆCRLï¼‰æ¡†æ¶ï¼Œèƒ½ä»äºŒå…ƒåŒ–è§†è§‰æ¦‚å¿µä¸­å­¦ä¹ å¸ƒå°”é€»è¾‘è§„åˆ™ã€‚CRLåˆ©ç”¨é€»è¾‘å±‚æ•æ‰æ¦‚å¿µç›¸å…³æ€§ï¼Œæå–ä¸´åºŠæ„ä¹‰è§„åˆ™ï¼Œå®ç°å±€éƒ¨å’Œå…¨å±€åŒé‡è§£é‡Šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCRLåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶èƒ½æ˜¾è‘—æé«˜å¯¹åˆ†å¸ƒå¤–æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µæ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†éœ€è§£å†³æ¦‚å¿µæ³„éœ²é—®é¢˜ã€‚</li>
<li>å¤§å¤šæ•°æ¦‚å¿µæ¨¡å‹å…³æ³¨å±€éƒ¨è§£é‡Šï¼Œå¿½ç•¥å…¨å±€å†³ç­–é€»è¾‘ã€‚</li>
<li>æå‡ºConcept Rule Learnerï¼ˆCRLï¼‰æ¡†æ¶ï¼Œèƒ½ä»äºŒå…ƒåŒ–è§†è§‰æ¦‚å¿µä¸­å­¦ä¹ å¸ƒå°”é€»è¾‘è§„åˆ™ã€‚</li>
<li>CRLé€šè¿‡é€»è¾‘å±‚æ•æ‰æ¦‚å¿µç›¸å…³æ€§ï¼Œæä¾›å±€éƒ¨å’Œå…¨å±€åŒé‡è§£é‡Šæ€§ã€‚</li>
<li>CRLåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ç«äº‰åŠ›ã€‚</li>
<li>CRLèƒ½æé«˜å¯¹åˆ†å¸ƒå¤–æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d63dcf17afebf26a423bf22fa39581b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85e4c32bbaa4bbee2ba32572d115979.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f9cf77c9fa095ad1faaf5bf6f5d253c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Quasar-identifications-from-the-slitless-spectra-a-test-from-3D-HST"><a href="#Quasar-identifications-from-the-slitless-spectra-a-test-from-3D-HST" class="headerlink" title="Quasar identifications from the slitless spectra: a test from 3D-HST"></a>Quasar identifications from the slitless spectra: a test from 3D-HST</h2><p><strong>Authors:Yuxuan Pang, Xue-Bing Wu, Yuming Fu, Rui Zhu, Tao Ji, Qinchun Ma, Xiaotong Feng</strong></p>
<p>Slitless spectroscopy is a traditional method for selecting quasars. In this paper, we develop a procedure for selecting quasars (QSOs) using the 3D-HST G141 slitless spectra. We initially identify over 6,000 sources with emission lines broader than those typically found in emission line galaxies (ELGs) by analyzing the 1D spectra. These &#96;&#96;broadâ€™â€™ emission lines may originate from actual QSO broad lines ($\rm FWHM\geq1200~\rm km&#x2F;s$) or the convolved narrow lines ($\rm FWHM &#x3D; 200\sim 300\rm km&#x2F;s$) in ELGs with effective radii $\geq$0.3â€ (2.5Kpc at z&#x3D;1). We then propose a criterion based on the reliability of the QSO component in the forward modeling results. Using the known QSOs, ELGs, and simulation spectra, we find that our criterion successfully selects about 90% of known QSOs with H$\alpha$ or H$\beta$ line detection and point-like structures, with an ELG contamination rate of about 5%. We apply this method to emission line sources without significant contamination and select 19 QSO candidates at redshift $z&#x3D;0.12-1.56$. 12 of these candidates have Chandra X-ray detections. This sample covers a broader range of the rest-frame UV colors and has redder optical slopes compared to the SDSS QSOs, yet it is more likely to be composed of normal QSOs rather than little red dots. Through spectral analysis, the estimated black hole masses of the sample are $10^{6.9}-10^{8.3} M_{\odot}$. Our new candidates improve the completeness of the QSO sample at $z&#x3D;0.8-1.6$ in the 3D-HST field. The proposed method will also be helpful for QSO selections via slitless spectroscopy in Euclid and the Chinese Space Station Telescope. </p>
<blockquote>
<p>æ— ç¼éš™å…‰è°±æ³•æ˜¯é€‰æ‹©ç±»æ˜Ÿä½“çš„ä¼ ç»Ÿæ–¹æ³•ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ©ç”¨3D-HST G141æ— ç¼éš™å…‰è°±é€‰æ‹©ç±»æ˜Ÿä½“ï¼ˆQSOï¼‰çš„ç¨‹åºã€‚æˆ‘ä»¬æœ€åˆé€šè¿‡åˆ†æä¸€ç»´å…‰è°±ï¼Œè¯†åˆ«äº†6000å¤šä¸ªå…‰æºï¼Œè¿™äº›å…‰æºçš„å‘å°„çº¿æ¯”é€šå¸¸å‡ºç°åœ¨å‘å°„çº¿æ˜Ÿç³»ï¼ˆELGsï¼‰ä¸­çš„å‘å°„çº¿æ›´å®½ã€‚è¿™äº›â€œå®½â€å‘å°„çº¿å¯èƒ½æ¥æºäºå®é™…çš„QSOå®½çº¿ï¼ˆFWHMâ‰¥1200km&#x2F;sï¼‰æˆ–æ˜¯é€šè¿‡æœ‰æ•ˆåŠå¾„â‰¥0.3â€ï¼ˆåœ¨z&#x3D;1æ—¶ä¸º2.5Kpcï¼‰çš„ELGså·ç§¯åçš„çª„çº¿ï¼ˆFWHM&#x3D;200~300km&#x2F;sï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ­£å‘å»ºæ¨¡ç»“æœä¸­ç±»æ˜Ÿä½“æˆåˆ†å¯é æ€§çš„æ ‡å‡†ã€‚åˆ©ç”¨å·²çŸ¥çš„ç±»æ˜Ÿä½“ã€å‘å°„çº¿æ˜Ÿç³»å’Œæ¨¡æ‹Ÿå…‰è°±ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ ‡å‡†æˆåŠŸé€‰æ‹©äº†çº¦90%å…·æœ‰HÎ±æˆ–HÎ²çº¿æ£€æµ‹ç‚¹å’Œç‚¹çŠ¶ç»“æ„çš„å·²çŸ¥ç±»æ˜Ÿä½“ï¼Œä¸”å‘å°„çº¿æ˜Ÿç³»çš„æ±¡æŸ“ç‡çº¦ä¸º5%ã€‚æˆ‘ä»¬å°†è¿™ç§æ–¹æ³•åº”ç”¨äºæ— æ˜æ˜¾æ±¡æŸ“çš„å‘å°„çº¿å…‰æºï¼Œé€‰æ‹©äº†19ä¸ªçº¢ç§»èŒƒå›´ä¸ºz&#x3D;0.12-1.56çš„ç±»æ˜Ÿä½“å€™é€‰è€…ã€‚å…¶ä¸­12ä¸ªå€™é€‰è€…è¢«Chandra Xå°„çº¿æ¢æµ‹åˆ°ã€‚è¿™ä¸ªæ ·æœ¬æ¶µç›–äº†æ›´å¹¿æ³›çš„é™æ­¢å¸§ç´«å¤–çº¿é¢œè‰²ï¼Œå¹¶ä¸”å…·æœ‰æ¯”SDSSç±»æ˜Ÿä½“æ›´çº¢çš„å…‰å­¦æ–œç‡ï¼Œä½†å®ƒæ›´å¯èƒ½ç”±æ­£å¸¸çš„ç±»æ˜Ÿä½“ç»„æˆï¼Œè€Œä¸æ˜¯å°çº¢ç‚¹ã€‚é€šè¿‡å…‰è°±åˆ†æï¼Œæ ·æœ¬ä¼°è®¡çš„é»‘æ´è´¨é‡ä¸º$10^{6.9}-10^{8.3} M_{\odot}$ã€‚æˆ‘ä»¬çš„æ–°å€™é€‰è€…æé«˜äº†åœ¨çº¢ç§»z&#x3D;0.8-1.6æ—¶åœ¨3D-HSTåœºçš„ç±»æ˜Ÿä½“æ ·æœ¬çš„å®Œå¤‡æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¹Ÿå°†æœ‰åŠ©äºé€šè¿‡Euclidå’Œä¸­å›½ç©ºé—´ç«™æœ›è¿œé•œçš„æ— ç¼éš™å…‰è°±æ³•é€‰æ‹©ç±»æ˜Ÿä½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14025v1">PDF</a> 22 pages, 15 figures, accepted by MNRAS</p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨æ— ç¼éš™å…‰è°±æ³•é€‰æ‹©ç±»æ˜Ÿä½“ï¼ˆQSOsï¼‰çš„æ–°æ–¹æ³•ï¼ŒåŸºäº3D-HST G141æ— ç¼éš™å…‰è°±è¿›è¡Œåˆ†æã€‚åˆæ­¥ç­›é€‰å‡º6,000å¤šä¸ªæºï¼Œå…·æœ‰æ¯”å‘å°„çº¿æ˜Ÿç³»ï¼ˆELGsï¼‰æ›´å®½çš„å‘å°„çº¿ã€‚æå‡ºåŸºäºæ­£å‘å»ºæ¨¡ç»“æœä¸­ç±»æ˜Ÿä½“æˆåˆ†å¯é æ€§çš„æ ‡å‡†ï¼ŒæˆåŠŸé€‰å‡ºçº¦90%å·²çŸ¥ç±»æ˜Ÿä½“ï¼Œæ±¡æŸ“ç‡çº¦ä¸º5%ã€‚åº”ç”¨æ­¤æ–¹æ³•ç­›é€‰å‡º19ä¸ªçº¢ç§»z&#x3D;0.12-1.56çš„ç±»æ˜Ÿä½“å€™é€‰è€…ï¼Œå…¶ä¸­12ä¸ªæœ‰Chandra Xå°„çº¿æ£€æµ‹ã€‚æ­¤æ–¹æ³•æé«˜äº†åœ¨3D-HSTå­—æ®µä¸Šz&#x3D;0.8-1.6çš„ç±»æ˜Ÿä½“æ ·æœ¬çš„å®Œæ•´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨3D-HST G141æ— ç¼éš™å…‰è°±è¿›è¡Œç±»æ˜Ÿä½“ï¼ˆQSOsï¼‰é€‰æ‹©ã€‚</li>
<li>é€šè¿‡åˆ†æ1Då…‰è°±ï¼Œåˆæ­¥ç­›é€‰å‡ºå…·æœ‰æ¯”å‘å°„çº¿æ˜Ÿç³»ï¼ˆELGsï¼‰æ›´å®½å‘å°„çº¿çš„â€œå®½â€æºã€‚</li>
<li>æå‡ºåŸºäºæ­£å‘å»ºæ¨¡ç»“æœä¸­ç±»æ˜Ÿä½“æˆåˆ†å¯é æ€§çš„ç­›é€‰æ ‡å‡†ã€‚</li>
<li>æˆåŠŸé€‰å‡ºçº¦90%å·²çŸ¥ç±»æ˜Ÿä½“ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„ELGæ±¡æŸ“ç‡ï¼ˆçº¦5%ï¼‰ã€‚</li>
<li>ç­›é€‰å‡º19ä¸ªçº¢ç§»èŒƒå›´åœ¨z&#x3D;0.12-1.56çš„ç±»æ˜Ÿä½“å€™é€‰è€…ï¼Œå…¶ä¸­éƒ¨åˆ†æœ‰Chandra Xå°„çº¿æ£€æµ‹ã€‚</li>
<li>æ­¤ç±»æ˜Ÿä½“æ ·æœ¬å…·æœ‰ä¸åŒçš„é™æ­¢å¸§UVé¢œè‰²å’Œå…‰å­¦æ–œç‡ï¼Œä½†æ›´å¯èƒ½ç”±æ­£å¸¸ç±»æ˜Ÿä½“ç»„æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14025">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd91f1deb73e08fd2c1ab42c09a60c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e21bcd54cda4b243aeaee14bdd003acc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b6a1b6808bbae55bbdfd69af3f42b71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffdb5c8188cd87394e568d23fc010e6a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Paradigm-Shift-in-Infrastructure-Inspection-Technology-Leveraging-High-performance-Imaging-and-Advanced-AI-Analytics-to-Inspect-Road-Infrastructure"><a href="#Paradigm-Shift-in-Infrastructure-Inspection-Technology-Leveraging-High-performance-Imaging-and-Advanced-AI-Analytics-to-Inspect-Road-Infrastructure" class="headerlink" title="Paradigm Shift in Infrastructure Inspection Technology: Leveraging   High-performance Imaging and Advanced AI Analytics to Inspect Road   Infrastructure"></a>Paradigm Shift in Infrastructure Inspection Technology: Leveraging   High-performance Imaging and Advanced AI Analytics to Inspect Road   Infrastructure</h2><p><strong>Authors:Du Wu, Enzhi Zhang, Isaac Lyngaas, Xiao Wang, Amir Ziabari, Tao Luo, Peng Chen, Kento Sato, Fumiyoshi Shoji, Takaki Hatsui, Kentaro Uesugi, Akira Seo, Yasuhito Sakai, Toshio Endo, Tetsuya Ishikawa, Satoshi Matsuoka, Mohamed Wahib</strong></p>
<p>Effective road infrastructure management is crucial for modern society. Traditional manual inspection techniques remain constrained by cost, efficiency, and scalability, while camera and laser imaging methods fail to capture subsurface defects critical for long-term structural integrity. This paper introduces ROVAI, an end-to-end framework that integrates high-resolution X-ray computed tomography imaging and advanced AI-driven analytics, aiming to transform road infrastructure inspection technologies. By leveraging the computational power of world-leading supercomputers, Fugaku and Frontier, and SoTA synchrotron facility (Spring-8), ROVAI enables scalable and high-throughput processing of massive 3D tomographic datasets. Our approach overcomes key challenges, such as the high memory requirements of vision models, the lack of labeled training data, and storage I&#x2F;O bottlenecks. This seamless integration of imaging and AI analytics facilitates automated defect detection, material composition analysis, and lifespan prediction. Experimental results demonstrate the effectiveness of ROVAI in real-world scenarios, setting a new standard for intelligent, data-driven infrastructure management. </p>
<blockquote>
<p>åœ¨ç°ä»£ç¤¾ä¼šï¼Œæœ‰æ•ˆçš„é“è·¯åŸºç¡€è®¾æ–½ç®¡ç†è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„æ‰‹åŠ¨æ£€æŸ¥æŠ€æœ¯ä»å—åˆ°æˆæœ¬ã€æ•ˆç‡å’Œå¯æ‰©å±•æ€§çš„é™åˆ¶ï¼Œè€Œæ‘„åƒå’Œæ¿€å…‰æˆåƒæ–¹æ³•æ— æ³•æ•æ‰åˆ°å¯¹é•¿æœŸç»“æ„å®Œæ•´æ€§è‡³å…³é‡è¦çš„åœ°ä¸‹ç¼ºé™·ã€‚æœ¬æ–‡ä»‹ç»äº†ROVAIï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé›†æˆäº†é«˜åˆ†è¾¨ç‡Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«ææˆåƒå’Œå…ˆè¿›çš„AIé©±åŠ¨åˆ†æï¼Œæ—¨åœ¨æ”¹å˜é“è·¯åŸºç¡€è®¾æ–½æ£€æŸ¥æŠ€æœ¯ã€‚é€šè¿‡åˆ©ç”¨ä¸–ç•Œé¢†å…ˆçš„è¶…çº§è®¡ç®—æœºFugakuå’ŒFrontierä»¥åŠSoTAåŒæ­¥åŠ é€Ÿå™¨è®¾æ–½ï¼ˆSpring-8ï¼‰çš„è®¡ç®—èƒ½åŠ›ï¼ŒROVAIèƒ½å¤Ÿå®ç°å¤§è§„æ¨¡3Dæ–­å±‚æ•°æ®é›†çš„å¯æ‰©å±•æ€§å’Œé«˜ååé‡å¤„ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…‹æœäº†å…³é”®æŒ‘æˆ˜ï¼Œå¦‚è§†è§‰æ¨¡å‹çš„é«˜å†…å­˜è¦æ±‚ã€ç¼ºä¹æ ‡è®°çš„è®­ç»ƒæ•°æ®å’Œå­˜å‚¨I&#x2F;Oç“¶é¢ˆã€‚æˆåƒå’ŒAIåˆ†æçš„æ— ç¼é›†æˆä¿ƒè¿›äº†è‡ªåŠ¨åŒ–ç¼ºé™·æ£€æµ‹ã€ææ–™ç»„æˆåˆ†æå’Œå¯¿å‘½é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROVAIåœ¨çœŸå®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæ™ºèƒ½ã€æ•°æ®é©±åŠ¨çš„è®¾æ–½ç®¡ç†æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13955v1">PDF</a> Submitting this work to be considered for the Gordon Bell Award in   SC25</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ROVAIè¿™ä¸€ç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé›†æˆäº†é«˜åˆ†è¾¨ç‡çš„Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«ææˆåƒå’Œå…ˆè¿›çš„AIé©±åŠ¨åˆ†ææŠ€æœ¯ï¼Œæ—¨åœ¨æ”¹å˜ä¼ ç»Ÿçš„é“è·¯åŸºç¡€è®¾æ–½æ£€æµ‹æ–¹å¼ã€‚åˆ©ç”¨è¶…çº§è®¡ç®—æœºFugakuå’ŒFrontierçš„è®¡ç®—èƒ½åŠ›ï¼Œä»¥åŠSoTAåŒæ­¥åŠ é€Ÿå™¨è®¾æ–½ï¼ˆSpring-8ï¼‰ï¼Œå®ç°äº†å¤§è§„æ¨¡çš„ä¸‰ç»´æ–­å±‚æ•°æ®é›†çš„é«˜é€šé‡å¤„ç†ã€‚æ­¤å¤–ï¼Œè¿˜å…‹æœäº†æ¨¡å‹é«˜å†…å­˜è¦æ±‚ç­‰éš¾é¢˜ã€‚æ•´åˆæˆåƒä¸äººå·¥æ™ºèƒ½åˆ†æèƒ½å¤Ÿè‡ªåŠ¨åŒ–è¿›è¡Œç¼ºé™·æ£€æµ‹ã€ææ–™æˆåˆ†åˆ†æå’Œå¯¿å‘½é¢„æµ‹ç­‰ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒROVAIåœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæœå“è¶Šï¼Œä¸ºæ™ºèƒ½åŒ–æ•°æ®é©±åŠ¨çš„åŸºç¡€è®¾æ–½ç®¡ç†è®¾ç«‹äº†æ–°æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ROVAIæ¡†æ¶ç»“åˆäº†é«˜åˆ†è¾¨ç‡Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«ææˆåƒå’ŒAIé©±åŠ¨åˆ†ææŠ€æœ¯ï¼Œç”¨äºé“è·¯åŸºç¡€è®¾æ–½æ£€æµ‹ã€‚</li>
<li>åˆ©ç”¨è¶…çº§è®¡ç®—æœºFugakuå’ŒFrontierçš„é«˜æ€§èƒ½è®¡ç®—èƒ½åŠ›å¤„ç†å¤§è§„æ¨¡ä¸‰ç»´æ–­å±‚æ•°æ®é›†ã€‚</li>
<li>ROVAIå…‹æœäº†æ¨¡å‹é«˜å†…å­˜è¦æ±‚ç­‰éš¾é¢˜ã€‚</li>
<li>é€šè¿‡æ•´åˆæˆåƒä¸äººå·¥æ™ºèƒ½åˆ†æï¼Œå®ç°è‡ªåŠ¨åŒ–ç¼ºé™·æ£€æµ‹ã€ææ–™æˆåˆ†åˆ†æå’Œå¯¿å‘½é¢„æµ‹ç­‰åŠŸèƒ½ã€‚</li>
<li>ROVAIæ¡†æ¶åœ¨çœŸå®åœºæ™¯ä¸­çš„å®éªŒæ•ˆæœæ˜¾è‘—ï¼Œå‡†ç¡®åº¦é«˜ã€‚</li>
<li>ROVAIæ¡†æ¶çš„åº”ç”¨æœ‰åŠ©äºæå‡é“è·¯åŸºç¡€è®¾æ–½ç®¡ç†çš„æ™ºèƒ½åŒ–æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8fdee2b6e7a015421e4ae32f28d9885c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b609594f7121f0a9faee5e39aeb6fd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04bd6b8678158d8b47df116f0af4acf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aedbb1ebe7c4bdf42df8ea9cf98c96fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5776d1b4c7400afe35999ee2000f532c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dc7be5d882274e42389dbbecdb43226.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Blind-Restoration-of-High-Resolution-Ultrasound-Video"><a href="#Blind-Restoration-of-High-Resolution-Ultrasound-Video" class="headerlink" title="Blind Restoration of High-Resolution Ultrasound Video"></a>Blind Restoration of High-Resolution Ultrasound Video</h2><p><strong>Authors:Chu Chen, Kangning Cui, Pasquale Cascarano, Wei Tang, Elena Loli Piccolomini, Raymond H. Chan</strong></p>
<p>Ultrasound imaging is widely applied in clinical practice, yet ultrasound videos often suffer from low signal-to-noise ratios (SNR) and limited resolutions, posing challenges for diagnosis and analysis. Variations in equipment and acquisition settings can further exacerbate differences in data distribution and noise levels, reducing the generalizability of pre-trained models. This work presents a self-supervised ultrasound video super-resolution algorithm called Deep Ultrasound Prior (DUP). DUP employs a video-adaptive optimization process of a neural network that enhances the resolution of given ultrasound videos without requiring paired training data while simultaneously removing noise. Quantitative and visual evaluations demonstrate that DUP outperforms existing super-resolution algorithms, leading to substantial improvements for downstream applications. </p>
<blockquote>
<p>è¶…å£°æˆåƒåœ¨ä¸´åºŠå®è·µä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†è¶…å£°è§†é¢‘å¸¸å¸¸å­˜åœ¨ä¿¡å·å™ªå£°æ¯”ï¼ˆSNRï¼‰ä½å’Œåˆ†è¾¨ç‡æœ‰é™çš„é—®é¢˜ï¼Œç»™è¯Šæ–­å’Œæ²»ç–—å¸¦æ¥æŒ‘æˆ˜ã€‚è®¾å¤‡å’Œé‡‡é›†è®¾ç½®çš„å·®å¼‚å¯èƒ½ä¼šè¿›ä¸€æ­¥åŠ å‰§æ•°æ®åˆ†å¸ƒå’Œå™ªå£°æ°´å¹³çš„å·®å¼‚ï¼Œé™ä½é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨æ€§ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§åä¸ºDeep Ultrasound Priorï¼ˆDUPï¼‰çš„è‡ªç›‘ç£è¶…å£°è§†é¢‘è¶…åˆ†è¾¨ç‡ç®—æ³•ã€‚DUPé‡‡ç”¨ç¥ç»ç½‘ç»œçš„è§†é¢‘è‡ªé€‚åº”ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦é…å¯¹è®­ç»ƒæ•°æ®çš„åŒæ—¶ï¼Œæé«˜ç»™å®šè¶…å£°è§†é¢‘çš„åˆ†è¾¨ç‡å¹¶å»é™¤å™ªå£°ã€‚å®šé‡å’Œè§†è§‰è¯„ä¼°è¡¨æ˜ï¼ŒDUPä¼˜äºç°æœ‰çš„è¶…åˆ†è¾¨ç‡ç®—æ³•ï¼Œä¸ºä¸‹æ¸¸åº”ç”¨å¸¦æ¥äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13915v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºDeep Ultrasound Priorï¼ˆDUPï¼‰çš„è‡ªç›‘ç£è¶…å£°è§†é¢‘è¶…åˆ†è¾¨ç‡ç®—æ³•ã€‚è¯¥ç®—æ³•é‡‡ç”¨ç¥ç»ç½‘ç»œè§†é¢‘è‡ªé€‚åº”ä¼˜åŒ–è¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€é…å¯¹è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹æé«˜è¶…å£°è§†é¢‘çš„åˆ†è¾¨ç‡å¹¶å»é™¤å™ªå£°ã€‚å®šé‡å’Œè§†è§‰è¯„ä¼°è¡¨æ˜ï¼ŒDUPåœ¨è¶…åˆ†è¾¨ç‡ç®—æ³•æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºä¸‹æ¸¸åº”ç”¨å¸¦æ¥äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°æˆåƒåœ¨ä¸´åºŠå®è·µä¸­åº”ç”¨å¹¿æ³›ï¼Œä½†å­˜åœ¨ä¿¡å·å™ªå£°æ¯”ï¼ˆSNRï¼‰ä½å’Œåˆ†è¾¨ç‡æœ‰é™çš„é—®é¢˜ï¼Œå¯¹è¯Šæ–­å’Œæ²»ç–—å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>è®¾å¤‡åŠé‡‡é›†è®¾ç½®çš„å·®å¼‚ä¼šè¿›ä¸€æ­¥åŠ å‰§æ•°æ®åˆ†å¸ƒå’Œå™ªå£°æ°´å¹³çš„ä¸åŒï¼Œé™ä½é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨æ€§ã€‚</li>
<li>Deep Ultrasound Priorï¼ˆDUPï¼‰æ˜¯ä¸€ç§è‡ªç›‘ç£è¶…å£°è§†é¢‘è¶…åˆ†è¾¨ç‡ç®—æ³•ï¼Œå¯æé«˜è¶…å£°è§†é¢‘çš„åˆ†è¾¨ç‡å¹¶æ¶ˆé™¤å™ªå£°ã€‚</li>
<li>DUPé‡‡ç”¨è§†é¢‘è‡ªé€‚åº”ä¼˜åŒ–è¿‡ç¨‹çš„ç¥ç»ç½‘ç»œï¼Œæ— éœ€é…å¯¹è®­ç»ƒæ•°æ®ã€‚</li>
<li>ä¸ç°æœ‰è¶…åˆ†è¾¨ç‡ç®—æ³•ç›¸æ¯”ï¼ŒDUPè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>DUPèƒ½æ˜¾è‘—æ”¹å–„è¶…å£°è§†é¢‘çš„åˆ†è¾¨ç‡å’Œå™ªå£°æ°´å¹³ï¼Œæœ‰åŠ©äºæé«˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œåˆ†æçš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-464ef2c4f17051fe710132e2d4e978f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b755fa8bffe22082ab37ae91f595154.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b72e8ff143f28eaa3be41e8f4c7a907a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="XDementNET-An-Explainable-Attention-Based-Deep-Convolutional-Network-to-Detect-Alzheimer-Progression-from-MRI-data"><a href="#XDementNET-An-Explainable-Attention-Based-Deep-Convolutional-Network-to-Detect-Alzheimer-Progression-from-MRI-data" class="headerlink" title="XDementNET: An Explainable Attention Based Deep Convolutional Network to   Detect Alzheimer Progression from MRI data"></a>XDementNET: An Explainable Attention Based Deep Convolutional Network to   Detect Alzheimer Progression from MRI data</h2><p><strong>Authors:Soyabul Islam Lincoln, Mirza Mohd Shahriar Maswood</strong></p>
<p>A common neurodegenerative disease, Alzheimerâ€™s disease requires a precise diagnosis and efficient treatment, particularly in light of escalating healthcare expenses and the expanding use of artificial intelligence in medical diagnostics. Many recent studies shows that the combination of brain Magnetic Resonance Imaging (MRI) and deep neural networks have achieved promising results for diagnosing AD. Using deep convolutional neural networks, this paper introduces a novel deep learning architecture that incorporates multiresidual blocks, specialized spatial attention blocks, grouped query attention, and multi-head attention. The study assessed the modelâ€™s performance on four publicly accessible datasets and concentrated on identifying binary and multiclass issues across various categories. This paper also takes into account of the explainability of ADâ€™s progression and compared with state-of-the-art methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster Score-CAM, and XGRADCAM. Our methodology consistently outperforms current approaches, achieving 99.66% accuracy in 4-class classification, 99.63% in 3-class classification, and 100% in binary classification using Kaggle datasets. For Open Access Series of Imaging Studies (OASIS) datasets the accuracies are 99.92%, 99.90%, and 99.95% respectively. The Alzheimerâ€™s Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in three planes (axial, sagittal, and coronal) and a combination of all planes. The study achieved accuracies of 99.08% for axis, 99.85% for sagittal, 99.5% for coronal, and 99.17% for all axis, and 97.79% and 8.60% respectively for ADNI-2. The networkâ€™s ability to retrieve important information from MRI images is demonstrated by its excellent accuracy in categorizing AD stages. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…æ˜¯ä¸€ç§å¸¸è§çš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œéœ€è¦ç²¾ç¡®è¯Šæ–­å’Œæ²»ç–—ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ä¿å¥è´¹ç”¨ä¸æ–­å¢åŠ å’Œäººå·¥æ™ºèƒ½åœ¨åŒ»å­¦è¯Šæ–­ä¸­å¹¿æ³›åº”ç”¨çš„æƒ…å†µä¸‹ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç»“åˆè„‘éƒ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’Œæ·±åº¦ç¥ç»ç½‘ç»œå¯¹ADçš„è¯Šæ–­å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œé‡‡ç”¨å¤šæ®‹å·®å—ã€ä¸“ç”¨ç©ºé—´æ³¨æ„åŠ›å—ã€åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›ã€‚è¯¥ç ”ç©¶åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¸“æ³¨äºè¯†åˆ«ä¸åŒç±»åˆ«çš„äºŒè¿›åˆ¶å’Œå¤šç±»åˆ«é—®é¢˜ã€‚æœ¬æ–‡è¿˜è€ƒè™‘äº†ADè¿›å±•çš„è§£é‡Šæ€§ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå³æ¢¯åº¦ç±»æ¿€æ´»æ˜ å°„ï¼ˆGradCAMï¼‰ã€åˆ†æ•°-CAMã€æ›´å¿«çš„åˆ†æ•°-CAMå’ŒXGRADCAMã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨Kaggleæ•°æ®é›†ä¸Šå®ç°äº†å››åˆ†ç±»99.66%ã€ä¸‰åˆ†ç±»99.63%ã€äºŒåˆ†ç±»100%çš„å‡†ç¡®ç‡ã€‚å¯¹äºå¼€æ”¾è®¿é—®æˆåƒç ”ç©¶ç³»åˆ—ï¼ˆOASISï¼‰æ•°æ®é›†ï¼Œå‡†ç¡®ç‡åˆ†åˆ«ä¸º99.92%ã€99.90%ã€å’Œ99.95%ã€‚åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…ç¥ç»å½±åƒå­¦å€¡è®®-1ï¼ˆADNI-1ï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„ä¸‰ä¸ªå¹³é¢ï¼ˆè½´å‘ã€çŸ¢çŠ¶å’Œå† çŠ¶ï¼‰ä»¥åŠæ‰€æœ‰å¹³é¢çš„ç»„åˆå®éªŒè¡¨æ˜ï¼Œå‡†ç¡®ç‡ä¸ºè½´å‘99.08%ã€çŸ¢çŠ¶é¢99.85%ã€å† çŠ¶é¢99.5%ã€æ‰€æœ‰è½´å¹³é¢ç»„åˆä¸ºç»¼åˆçš„å‡†ç¡®ç‡99.17%ï¼Œå¯¹ADNI-2å‡†ç¡®ç‡åˆ†åˆ«å®ç°äº†åˆ°è¾¾87%ï¼Œè¯æ˜äº†ç½‘ç»œä»MRIå›¾åƒä¸­æå–é‡è¦ä¿¡æ¯çš„èƒ½åŠ›åŠå…¶åœ¨åˆ†ç±»ADé˜¶æ®µæ–¹é¢çš„å‡ºè‰²å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13906v1">PDF</a> 20 pages, 12 figures,</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ç»“åˆè„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å’Œæ·±åº¦ç¥ç»ç½‘ç»œï¼Œæå‡ºä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç”¨äºè¯Šæ–­é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ã€‚è¯¥æ¶æ„é‡‡ç”¨å¤šæ®‹å·®å—ã€ä¸“ä¸šç©ºé—´æ³¨æ„åŠ›å—ã€åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›ç­‰æŠ€æœ¯ï¼Œåœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶å…³æ³¨ä¸åŒç±»åˆ«çš„äºŒå…ƒå’Œå¤šç±»é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜è€ƒè™‘äº†ADè¿›å±•çš„è§£é‡Šæ€§ï¼Œå¹¶ä¸Gradient Class Activation Mapping (GradCAM)ã€Score-CAMã€Faster Score-CAMå’ŒXGRADCAMç­‰æœ€æ–°æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æ–°å‹æ¶æ„åœ¨åˆ†ç±»ADé˜¶æ®µæ—¶è¡¨ç°å‡ºå“è¶Šå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ç»“åˆè„‘MRIå’Œæ·±åº¦ç¥ç»ç½‘ç»œè¯Šæ–­ADï¼Œæå‡ºæ–°å‹æ·±åº¦å­¦ä¹ æ¶æ„ã€‚</li>
<li>æ–°å‹æ¶æ„é‡‡ç”¨å¤šæ®‹å·®å—ã€ä¸“ä¸šç©ºé—´æ³¨æ„åŠ›å—ç­‰æŠ€æœ¯ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜ç§€ï¼ŒåŒ…æ‹¬äºŒå…ƒå’Œå¤šç±»åˆ†ç±»é—®é¢˜ã€‚</li>
<li>ç ”ç©¶è€ƒè™‘äº†ADè¿›å±•çš„è§£é‡Šæ€§ï¼Œå¹¶ä¸å¤šç§æœ€æ–°æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>æ–°å‹æ¶æ„åœ¨åˆ†ç±»ADé˜¶æ®µæ—¶è¡¨ç°å‡ºå“è¶Šå‡†ç¡®æ€§ï¼Œè¾¾åˆ°é«˜å‡†ç¡®ç‡ã€‚</li>
<li>ä½¿ç”¨Kaggleæ•°æ®é›†ï¼Œ4ç±»åˆ†ç±»å‡†ç¡®ç‡ä¸º99.66%ï¼Œ3ç±»åˆ†ç±»å‡†ç¡®ç‡ä¸º99.63%ï¼ŒäºŒå…ƒåˆ†ç±»å‡†ç¡®ç‡ä¸º100%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1db867881150e7ea247853e5a7c2ad61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df70bdd7935ef3983eed403f3b0d9864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a628b222d34e40c18ecae28071c3ac7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38320387e849e7356da1f35cbceb4f8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2d66d2a7a39740bdc1329d9c7f9e33.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Orbital-Phase-resolved-Analysis-of-X-ray-and-Gamma-ray-Observations-of-the-High-Mass-Gamma-ray-Binary-4FGL-J1405-1-6119"><a href="#Orbital-Phase-resolved-Analysis-of-X-ray-and-Gamma-ray-Observations-of-the-High-Mass-Gamma-ray-Binary-4FGL-J1405-1-6119" class="headerlink" title="Orbital Phase-resolved Analysis of X-ray and Gamma-ray Observations of   the High-Mass Gamma-ray Binary 4FGL J1405.1-6119"></a>Orbital Phase-resolved Analysis of X-ray and Gamma-ray Observations of   the High-Mass Gamma-ray Binary 4FGL J1405.1-6119</h2><p><strong>Authors:Alexander Lange, Robin H. D. Corbet, Joel B. Coley, Guillaume Dubus, Jeremy Hare, Nazma Islam, Jonathan Barnes</strong></p>
<p>We present the results of multi-wavelength observations of the High-Mass Gamma-Ray Binary 4FGL J1405.1-6119. A pair of joint XMM-Newton and NuSTAR observations taken in 2019 (sampling the gamma-ray maximum and X-ray maximum) characterize the emission of soft and hard X-rays. We find variability of the hydrogen column density along our line of sight, $N_{\rm H}$, and photon index, $\Gamma$, and find no evidence of pulsations in X-rays. We also refine a new best-fit orbital period to $P&#x3D;13.7157\pm0.0014$ days, the first orbital phase-resolved analysis based on nearly 16 years of Fermiâ€“LAT observations of 4FGL J1405.1-6119 and the evolution of the spectral shape as a function of orbital phase. Finally, the X-ray and $\gamma$-ray spectra for the phases sampled in the new X-ray observations can be interpreted in the framework of the intrabinary shock model, previously applied to High-Mass Gamma-Ray binaries such as LS 5039. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹é«˜è´¨é‡ä¼½é©¬å°„çº¿åŒæ˜Ÿ4FGL J1405.1-6119è¿›è¡Œäº†å¤šæ³¢é•¿è§‚æµ‹çš„ç»“æœå±•ç¤ºã€‚é€šè¿‡2019å¹´è”åˆXMM-ç‰›é¡¿å’ŒNuSTARçš„è§‚æµ‹ï¼ˆé‡‡æ ·ä¼½é©¬å°„çº¿æœ€å¤§å€¼å’ŒXå°„çº¿æœ€å¤§å€¼ï¼‰ï¼Œæˆ‘ä»¬æè¿°äº†è½¯Xå°„çº¿å’Œç¡¬Xå°„çº¿çš„å‘å°„ç‰¹å¾ã€‚æˆ‘ä»¬å‘ç°è§†çº¿æ–¹å‘çš„æ°¢æŸ±å¯†åº¦$N_{\rm H}$å’Œå…‰å­æŒ‡æ•°$\Gamma$å­˜åœ¨å˜åŒ–ï¼Œä¸”åœ¨Xå°„çº¿ä¸Šæœªå‘ç°è„‰åŠ¨è¯æ®ã€‚æˆ‘ä»¬è¿˜å¯¹æœ€ä½³æ‹Ÿåˆè½¨é“å‘¨æœŸè¿›è¡Œäº†æ›´æ–°ï¼Œä¸º$P&#x3D;13.7157\pm0.0014$å¤©ï¼Œè¿™æ˜¯åŸºäºè¿‘16å¹´å¯¹4FGL J1405.1-6119çš„è´¹ç±³-LATè§‚æµ‹æ•°æ®è¿›è¡Œçš„é¦–æ¬¡è½¨é“ç›¸ä½è§£æåˆ†æï¼Œä»¥åŠè°±å½¢éšè½¨é“ç›¸ä½çš„æ¼”åŒ–ã€‚æœ€åï¼Œæ–°Xå°„çº¿è§‚æµ‹ä¸­é‡‡æ ·é˜¶æ®µçš„Xå°„çº¿å’Œä¼½é©¬å°„çº¿å…‰è°±å¯ä»¥åœ¨åŒæ˜Ÿå†…å†²å‡»æ¨¡å‹çš„æ¡†æ¶å†…è¿›è¡Œè§£é‡Šï¼Œè¯¥æ¨¡å‹ä¹‹å‰å·²åº”ç”¨äºé«˜è´¨é‡ä¼½é©¬å°„çº¿åŒæ˜Ÿï¼Œå¦‚LS 5039ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13716v1">PDF</a> Accepted for publication in the Astrophysical Journal</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŠ¥å‘Šäº†é«˜è´¨èƒ½Î³å°„çº¿åŒæ˜Ÿ4FGL J1405.1-6119çš„å¤šæ³¢é•¿è§‚æµ‹ç»“æœã€‚é€šè¿‡è”åˆXMM-Newtonå’ŒNuSTARåœ¨2019å¹´çš„è§‚æµ‹ï¼Œå‘ç°å…¶è½¯ã€ç¡¬Xå°„çº¿çš„å‘å°„ç‰¹å¾ï¼Œå¹¶è§‚å¯Ÿåˆ°è§†çº¿ä¸Šæ°¢æŸ±å¯†åº¦ï¼ˆ$N_{\rm H}$ï¼‰å’Œå…‰å­æŒ‡æ•°ï¼ˆ$\Gamma$ï¼‰çš„å˜åŒ–ã€‚æ²¡æœ‰å‘ç°Xå°„çº¿è„‰å†²ï¼Œå¹¶ç²¾ç»†æµ‹é‡äº†æœ€ä½³æ‹Ÿåˆè½¨é“å‘¨æœŸä¸º13.7157Â±0.0014å¤©ã€‚æ­¤å¤–ï¼ŒåŸºäºè¿‘16å¹´çš„Fermi-LATè§‚æµ‹æ•°æ®ï¼Œé¦–æ¬¡è¿›è¡Œäº†è½¨é“ç›¸ä½è§£æåˆ†æï¼Œå¹¶ç ”ç©¶äº†å…‰è°±å½¢çŠ¶éšè½¨é“ç›¸ä½çš„å˜åŒ–ã€‚æœ€åï¼Œæ–°è§‚æµ‹é˜¶æ®µçš„Xå°„çº¿å’ŒÎ³å°„çº¿å…‰è°±å¯åœ¨äºŒä½“å†²å‡»æ¨¡å‹çš„æ¡†æ¶å†…è¿›è¡Œè§£é‡Šï¼Œè¯¥æ¨¡å‹å·²åº”ç”¨äºå…¶ä»–é«˜è´¨èƒ½Î³å°„çº¿åŒæ˜Ÿå¦‚LS 5039ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ¥å‘Šäº†é«˜è´¨èƒ½Î³å°„çº¿åŒæ˜Ÿ4FGL J1405.1-6119çš„XMM-Newtonå’ŒNuSTARè”åˆè§‚æµ‹ç»“æœã€‚</li>
<li>è§‚å¯Ÿåˆ°è½¯ã€ç¡¬Xå°„çº¿çš„å‘å°„ç‰¹å¾ã€‚</li>
<li>è§†çº¿ä¸Šæ°¢æŸ±å¯†åº¦ï¼ˆ$N_{\rm H}$ï¼‰å’Œå…‰å­æŒ‡æ•°ï¼ˆ$\Gamma$ï¼‰å­˜åœ¨å˜åŒ–ã€‚</li>
<li>æ²¡æœ‰å‘ç°Xå°„çº¿è„‰å†²ã€‚</li>
<li>è½¨é“å‘¨æœŸè¢«ç²¾ç»†æµ‹é‡ä¸º13.7157Â±0.0014å¤©ã€‚</li>
<li>åŸºäºè¿‘16å¹´çš„Fermi-LATè§‚æµ‹æ•°æ®è¿›è¡Œäº†è½¨é“ç›¸ä½è§£æåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2de74c7a340cbebbcaff82a9e00fc30b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a15991c531c6c9906c0505bfb812e38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca6965aecd69706aa19f3930232de8dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f7b9e95e7b1be8ed421e58aab3e5b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0071e43bd619fc853bf3fe020007fa72.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Origin-of-the-X-ray-emission-in-blazars-through-multiwavelength-polarization"><a href="#Origin-of-the-X-ray-emission-in-blazars-through-multiwavelength-polarization" class="headerlink" title="Origin of the X-ray emission in blazars through multiwavelength   polarization"></a>Origin of the X-ray emission in blazars through multiwavelength   polarization</h2><p><strong>Authors:Ioannis Liodakis, Haocheng Zhang, Stella Boula, Riccardo Middei, Jorge Otero-Santos, Dmitry Blinov, IvÃ¡n Agudo, Markus BÃ¶ttcher, Chien-Ting Chen, Steven R. Ehlert, Svetlana G. Jorstad, Philip Kaaret, Henric Krawczynski, Abel L. Peirson, Roger W. Romani, Fabrizio Tavecchio, Martin C. Weisskopf, Pouya M. Kouch, Elina Lindfors, Kari Nilsson, Callum McCall, Helen E. Jermak, Iain A. Steele, Ioannis Myserlis, Mark Gurwell, Garrett K. Keating, Ramprasad Rao, Sincheol Kang, Sang-Sung Lee, Sanghyun Kim, Whee Yeon Cheong, Hyeon-Woo Jeong, Emmanouil Angelakis, Alexander Kraus, Francisco JosÃ© Aceituno, Giacomo Bonnoli, VÃ­ctor Casanova, Juan Escudero, Beatriz AgÃ­s-GonzÃ¡lez, Daniel Morcuende, Alfredo Sota, Rumen Bachev, Tatiana S. Grishina, Evgenia N. Kopatskaya, Elena G. Larionova, Daria A. Morozova, Sergey S. Savchenko, Ekaterina V. Shishkina, Ivan S. Troitskiy, Yulia V. Troitskaya, Andrey A. Vasilyev</strong></p>
<p>The origin of the high-energy emission in astrophysical jets from black holes is a highly debated issue. This is particularly true for jets from supermassive black holes that are among the most powerful particle accelerators in the Universe. So far, the addition of new observations and new messengers have only managed to create more questions than answers. However, the newly available X-ray polarization observations promise to finally distinguish between emission models. We use extensive multiwavelength and polarization campaigns as well as state-of-the-art polarized spectral energy distribution models to attack this problem by focusing on two X-ray polarization observations of blazar BL Lacertae in flaring and quiescent $\gamma$-ray states. We find that regardless of the jet composition and underlying emission model, inverse-Compton scattering from relativistic electrons dominates at X-ray energies. </p>
<blockquote>
<p>å¤©æ–‡ç‰©ç†å­¦ä¸­é»‘æ´å–·å°„çš„é«˜èƒ½è¾å°„æ¥æºæ˜¯ä¸€ä¸ªå¤‡å—äº‰è®®çš„è¯é¢˜ã€‚ç‰¹åˆ«æ˜¯æ¥è‡ªè¶…çº§å¤§è´¨é‡é»‘æ´çš„å–·å°„æµï¼Œå®ƒä»¬æ˜¯å®‡å®™ä¸­æœ€ä¸ºå¼ºå¤§çš„ç²’å­åŠ é€Ÿå™¨ä¹‹ä¸€ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œæ–°çš„è§‚æµ‹å’Œä¿¡ä½¿çš„å¢åŠ åªæ˜¯äº§ç”Ÿäº†æ›´å¤šçš„é—®é¢˜è€Œéç­”æ¡ˆã€‚ç„¶è€Œï¼Œæ–°å¯ç”¨çš„Xå°„çº¿åæŒ¯è§‚æµ‹æœ‰æœ›æœ€ç»ˆåŒºåˆ†ä¸åŒçš„å‘å°„æ¨¡å‹ã€‚æˆ‘ä»¬é‡‡ç”¨äº†å¹¿æ³›çš„å¤šæ³¢é•¿å’ŒåæŒ¯è§‚æµ‹æ´»åŠ¨ä»¥åŠæœ€å…ˆè¿›çš„åæŒ¯è°±èƒ½é‡åˆ†å¸ƒæ¨¡å‹ï¼Œé€šè¿‡ä¸“æ³¨äºå¤„äºè€€å‘å’Œé™æ­¢ä¼½é©¬å°„çº¿çš„BL Lacertaeçš„ä¸¤ä¸ªXå°„çº¿åæŒ¯è§‚æµ‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ— è®ºå°„æµæˆåˆ†å’Œåº•å±‚å‘å°„æ¨¡å‹å¦‚ä½•ï¼Œç›¸å¯¹è®ºæ€§ç”µå­çš„é€†åº·æ™®é¡¿æ•£å°„åœ¨Xå°„çº¿èƒ½é‡ä¸­å ä¸»å¯¼åœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13603v1">PDF</a> 10 pages, 15 figures, 4 Tables, accepted for publication in A&amp;A</p>
<p><strong>Summary</strong><br>     æ¢è®¨é»‘æ´å¼•èµ·çš„å¤©ä½“ç‰©ç†å­¦å–·å°„æµä¸­é«˜èƒ½é‡å‘å°„çš„èµ·æºæ˜¯ä¸€ä¸ªå¤‡å—äº‰è®®çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ¥è‡ªè¶…å¤§è´¨é‡é»‘æ´çš„å–·å°„æµï¼Œå®ƒä»¬è¢«è®¤ä¸ºæ˜¯å®‡å®™ä¸­æœ€å¼ºå¤§çš„ç²’å­åŠ é€Ÿå™¨ã€‚æ–°è§‚æµ‹æ•°æ®å’Œæ–°æŠ€æœ¯å¸¦æ¥æ–°çš„å¯å‘åŒæ—¶ä¹Ÿäº§ç”Ÿäº†æ›´å¤šçš„é—®é¢˜ã€‚æœ€æ–°çš„Xå°„çº¿åæŒ¯è§‚æµ‹å¯èƒ½ä¸ºåŒºåˆ†å‘å°„æ¨¡å‹æä¾›ä¾æ®ã€‚é€šè¿‡å…¨æ–¹ä½çš„å¤šæ³¢é•¿å’ŒåæŒ¯è§‚æµ‹ä»¥åŠæœ€å…ˆè¿›çš„åæŒ¯è°±èƒ½é‡åˆ†å¸ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å¯¹è€€æ–‘çŠ¶æ€å’Œé«˜é™æ­¢Î³å°„çº¿çŠ¶æ€ä¸‹çš„BLLacertaeçš„Xå°„çº¿åæŒ¯è§‚æµ‹è¿›è¡Œç ”ç©¶ï¼Œå‘ç°æ— è®ºå–·å°„æµç»„æˆå’Œæ½œåœ¨å‘å°„æ¨¡å‹å¦‚ä½•ï¼Œç›¸å¯¹è®ºæ€§ç”µå­çš„é€†åº·æ™®é¡¿æ•£å°„åœ¨Xå°„çº¿èƒ½é‡æ®µå ä¸»å¯¼åœ°ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜èƒ½å¤©ä½“ç‰©ç†å­¦å–·å°„æµä¸­é»‘æ´å¼•å‘çš„é«˜èƒ½é‡å‘å°„èµ·æºæ˜¯ä¸€ä¸ªæœ‰äº‰è®®çš„é—®é¢˜ã€‚</li>
<li>è¶…å¤§è´¨é‡é»‘æ´å–·å°„æµæ˜¯æœ€å¼ºå¤§çš„ç²’å­åŠ é€Ÿå™¨ä¹‹ä¸€ã€‚</li>
<li>æ–°è§‚æµ‹æ•°æ®å’Œæ–°æŠ€æœ¯å¸¦æ¥æ›´å¤šé—®é¢˜ï¼ŒXå°„çº¿åæŒ¯è§‚æµ‹å¯èƒ½ä¸ºåŒºåˆ†å‘å°„æ¨¡å‹æä¾›ä¾æ®ã€‚</li>
<li>é€šè¿‡å…¨æ–¹ä½çš„å¤šæ³¢é•¿å’ŒåæŒ¯è§‚æµ‹ä»¥åŠå…ˆè¿›çš„æ¨¡å‹è¿›è¡Œç ”ç©¶ã€‚</li>
<li>æ— è®ºå–·å°„æµç»„æˆå’Œæ½œåœ¨å‘å°„æ¨¡å‹å¦‚ä½•ï¼ŒXå°„çº¿èƒ½é‡æ®µçš„é€†åº·æ™®é¡¿æ•£å°„å ä¸»å¯¼åœ°ä½ã€‚</li>
<li>BLLacertaeçš„Xå°„çº¿åæŒ¯è§‚æµ‹æ˜¯ç ”ç©¶æ­¤é—®é¢˜çš„å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9cbe47b433782f3927ac3a04c575e925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-369a5bc8bf1430a69e71d0f8ef1fcb06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f37a5965bd627abb2da02216a86b0e17.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VTBench-Evaluating-Visual-Tokenizers-for-Autoregressive-Image-Generation"><a href="#VTBench-Evaluating-Visual-Tokenizers-for-Autoregressive-Image-Generation" class="headerlink" title="VTBench: Evaluating Visual Tokenizers for Autoregressive Image   Generation"></a>VTBench: Evaluating Visual Tokenizers for Autoregressive Image   Generation</h2><p><strong>Authors:Huawei Lin, Tong Geng, Zhaozhuo Xu, Weijie Zhao</strong></p>
<p>Autoregressive (AR) models have recently shown strong performance in image generation, where a critical component is the visual tokenizer (VT) that maps continuous pixel inputs to discrete token sequences. The quality of the VT largely defines the upper bound of AR model performance. However, current discrete VTs fall significantly behind continuous variational autoencoders (VAEs), leading to degraded image reconstructions and poor preservation of details and text. Existing benchmarks focus on end-to-end generation quality, without isolating VT performance. To address this gap, we introduce VTBench, a comprehensive benchmark that systematically evaluates VTs across three core tasks: Image Reconstruction, Detail Preservation, and Text Preservation, and covers a diverse range of evaluation scenarios. We systematically assess state-of-the-art VTs using a set of metrics to evaluate the quality of reconstructed images. Our findings reveal that continuous VAEs produce superior visual representations compared to discrete VTs, particularly in retaining spatial structure and semantic detail. In contrast, the degraded representations produced by discrete VTs often lead to distorted reconstructions, loss of fine-grained textures, and failures in preserving text and object integrity. Furthermore, we conduct experiments on GPT-4o image generation and discuss its potential AR nature, offering new insights into the role of visual tokenization. We release our benchmark and codebase publicly to support further research and call on the community to develop strong, general-purpose open-source VTs. </p>
<blockquote>
<p>è¿‘æœŸè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå…¶ä¸­å…³é”®ç»„ä»¶æ˜¯è§†è§‰ä»¤ç‰ŒåŒ–å™¨ï¼ˆVTï¼‰ï¼Œå®ƒå°†è¿ç»­çš„åƒç´ è¾“å…¥æ˜ å°„åˆ°ç¦»æ•£çš„ä»¤ç‰Œåºåˆ—ã€‚VTçš„è´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå†³å®šäº†ARæ¨¡å‹æ€§èƒ½çš„ä¸Šé™ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç¦»æ•£VTæ˜æ˜¾è½åäºè¿ç»­å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ï¼Œå¯¼è‡´å›¾åƒé‡å»ºè´¨é‡ä¸‹é™ï¼Œç»†èŠ‚å’Œæ–‡æœ¬ä¿ç•™ä¸è¶³ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç«¯åˆ°ç«¯çš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶æ²¡æœ‰å­¤ç«‹åœ°è¯„ä¼°VTçš„æ€§èƒ½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†VTBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†VTåœ¨ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ä¸­çš„è¡¨ç°ï¼šå›¾åƒé‡å»ºã€ç»†èŠ‚ä¿ç•™å’Œæ–‡æœ¬ä¿ç•™ï¼Œæ¶µç›–äº†å„ç§è¯„ä¼°åœºæ™¯ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç»„åº¦é‡æŒ‡æ ‡ç³»ç»Ÿåœ°è¯„ä¼°äº†æœ€å…ˆè¿›çš„VTåœ¨é‡å»ºå›¾åƒè´¨é‡æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä¸ç¦»æ•£VTç›¸æ¯”ï¼Œè¿ç»­VAEsäº§ç”Ÿäº†æ›´ä¼˜è¶Šçš„è§†è§‰è¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿ç•™ç©ºé—´ç»“æ„å’Œè¯­ä¹‰ç»†èŠ‚æ–¹é¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¦»æ•£VTäº§ç”Ÿçš„é€€åŒ–è¡¨ç¤ºç»å¸¸å¯¼è‡´é‡å»ºå¤±çœŸã€ä¸¢å¤±ç»†å¾®çº¹ç†ä»¥åŠåœ¨ä¿ç•™æ–‡æœ¬å’Œå¯¹è±¡å®Œæ•´æ€§æ–¹é¢çš„å¤±è´¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹GPT-4oçš„å›¾åƒç”Ÿæˆè¿›è¡Œäº†å®éªŒï¼Œå¹¶å¯¹å…¶æ½œåœ¨çš„ARç‰¹æ€§è¿›è¡Œäº†è®¨è®ºï¼Œä¸ºè§†è§‰ä»¤ç‰ŒåŒ–ä½œç”¨æä¾›äº†æ–°çš„è§è§£ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œä»£ç åº“ï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œå¹¶å‘¼åç¤¾åŒºå¼€å‘å¼ºå¤§ã€é€šç”¨çš„å¼€æºVTã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13439v1">PDF</a> 24 pages, 13 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€è¿‘è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­çš„è¡¨ç°ï¼Œå¯è§†åŒ–ä»¤ç‰Œæ˜ å°„åœ¨ç”Ÿæˆè¿ç»­åƒç´ è¾“å…¥ç¦»æ•£åºåˆ—æ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ã€‚å°½ç®¡VTæŠ€æœ¯è¿›å±•è¿…é€Ÿï¼Œä½†ä¸è¿ç»­å˜åˆ†è‡ªç¼–ç å™¨ç›¸æ¯”ä»æœ‰æ‰€ä¸è¶³ï¼Œå¯¼è‡´å›¾åƒé‡å»ºè´¨é‡ä¸‹é™ï¼Œç»†èŠ‚å’Œæ–‡æœ¬ä¿ç•™ä¸è¶³ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç«¯åˆ°ç«¯çš„ç”Ÿæˆè´¨é‡ï¼Œå¿½ç•¥äº†VTæ€§èƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨é¢è¯„ä¼°VTçš„ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ç³»ç»Ÿåœ°è¯„ä¼°äº†æœ€æ–°çš„VTæ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒé‡å»ºã€ç»†èŠ‚ä¿ç•™å’Œæ–‡æœ¬ä¿ç•™ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿ç»­å˜åˆ†è‡ªç¼–ç å™¨åœ¨ç”Ÿæˆè§†è§‰è¡¨ç¤ºæ–¹é¢ä¼˜äºç¦»æ•£VTï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿ç•™ç©ºé—´ç»“æ„å’Œè¯­ä¹‰ç»†èŠ‚æ–¹é¢ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†GPT-4oå›¾åƒç”Ÿæˆçš„æ½œåœ¨è‡ªå›å½’æ€§è´¨ã€‚å‘¼åå¼€å‘å¼ºå¤§ã€é€šç”¨çš„å¼€æºVTä»¥æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå…¶ä¸­å¯è§†åŒ–ä»¤ç‰Œæ˜ å°„æ˜¯å…³é”®ç»„ä»¶ã€‚</li>
<li>ç°æœ‰ç¦»æ•£å¯è§†åŒ–ä»¤ç‰Œæ˜ å°„æŠ€æœ¯ç›¸è¾ƒäºè¿ç»­å˜åˆ†è‡ªç¼–ç å™¨å­˜åœ¨æ€§èƒ½å·®è·ï¼Œå¯¼è‡´å›¾åƒé‡å»ºè´¨é‡ä¸‹é™å’Œç»†èŠ‚ä¸¢å¤±ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•VTBenchï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°å¯è§†åŒ–ä»¤ç‰Œæ˜ å°„çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒé‡å»ºã€ç»†èŠ‚ä¿ç•™å’Œæ–‡æœ¬ä¿ç•™ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶å‘ç°è¿ç»­å˜åˆ†è‡ªç¼–ç å™¨åœ¨ç”Ÿæˆè§†è§‰è¡¨ç¤ºæ–¹é¢ä¼˜äºç¦»æ•£å¯è§†åŒ–ä»¤ç‰Œæ˜ å°„æŠ€æœ¯ã€‚</li>
<li>GPT-4oå›¾åƒç”Ÿæˆå…·æœ‰æ½œåœ¨çš„è‡ªå›å½’æ€§è´¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-64b22884301f6b2d5c1be29cafe6e584.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36caa036030417284840dd3028f3141e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43d9e2295238468fece03b14d9f07119.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bba5abc7f47c4cf614b9567efddf186.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e6cb872ca54b80df70f69105d4f6f3d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields-in-Efficient-CNNs-for-Fair-Medical-Image-Classification"><a href="#Expert-Like-Reparameterization-of-Heterogeneous-Pyramid-Receptive-Fields-in-Efficient-CNNs-for-Fair-Medical-Image-Classification" class="headerlink" title="Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields   in Efficient CNNs for Fair Medical Image Classification"></a>Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields   in Efficient CNNs for Fair Medical Image Classification</h2><p><strong>Authors:Xiao Wu, Xiaoqing Zhang, Zunjie Xiao, Lingxi Hu, Risa Higashita, Jiang Liu</strong></p>
<p>Efficient convolutional neural network (CNN) architecture designs have attracted growing research interests. However, they usually apply single receptive field (RF), small asymmetric RFs, or pyramid RFs to learn different feature representations, still encountering two significant challenges in medical image classification tasks: 1) They have limitations in capturing diverse lesion characteristics efficiently, e.g., tiny, coordination, small and salient, which have unique roles on results, especially imbalanced medical image classification. 2) The predictions generated by those CNNs are often unfair&#x2F;biased, bringing a high risk by employing them to real-world medical diagnosis conditions. To tackle these issues, we develop a new concept, Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields (ERoHPRF), to simultaneously boost medical image classification performance and fairness. This concept aims to mimic the multi-expert consultation mode by applying the well-designed heterogeneous pyramid RF bags to capture different lesion characteristics effectively via convolution operations with multiple heterogeneous kernel sizes. Additionally, ERoHPRF introduces an expert-like structural reparameterization technique to merge its parameters with the two-stage strategy, ensuring competitive computation cost and inference speed through comparisons to a single RF. To manifest the effectiveness and generalization ability of ERoHPRF, we incorporate it into mainstream efficient CNN architectures. The extensive experiments show that our method maintains a better trade-off than state-of-the-art methods in terms of medical image classification, fairness, and computation overhead. The codes of this paper will be released soon. </p>
<blockquote>
<p>é«˜æ•ˆçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„è®¾è®¡å·²å¼•èµ·è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…´è¶£ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸åº”ç”¨å•ä¸ªæ„Ÿå—é‡ï¼ˆRFï¼‰ã€å°å‹ä¸å¯¹ç§°RFsæˆ–é‡‘å­—å¡”RFæ¥å­¦ä¹ ä¸åŒçš„ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ä»é¢ä¸´ä¸¤ä¸ªé‡å¤§æŒ‘æˆ˜ï¼š1ï¼‰å®ƒä»¬åœ¨æœ‰æ•ˆæ•è·å„ç§ç—…å˜ç‰¹å¾æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚å¾®å°ã€åè°ƒã€å°è€Œæ˜¾è‘—çš„ç‰¹å¾åœ¨ç»“æœä¸­å…·æœ‰ç‹¬ç‰¹ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸å¹³è¡¡åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ã€‚2ï¼‰è¿™äº›CNNç”Ÿæˆçš„é¢„æµ‹é€šå¸¸æ˜¯ä¸å…¬å¹³&#x2F;æœ‰åè§çš„ï¼Œå°†å®ƒä»¬åº”ç”¨äºç°å®ä¸–ç•Œçš„åŒ»å­¦è¯Šæ–­æ¡ä»¶ä¼šå¸¦æ¥é«˜é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¦‚å¿µâ€”â€”å¼‚è´¨é‡‘å­—å¡”æ„Ÿå—é‡çš„ä¸“å®¶å‹å†å‚æ•°åŒ–ï¼ˆERoHPRFï¼‰ï¼Œä»¥åŒæ—¶æé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„æ€§èƒ½å’Œå…¬å¹³æ€§ã€‚è¯¥æ¦‚å¿µæ—¨åœ¨é€šè¿‡åº”ç”¨è®¾è®¡ç²¾è‰¯çš„å¼‚è´¨é‡‘å­—å¡”RFåŒ…æ¥æ¨¡ä»¿å¤šä¸“å®¶å’¨è¯¢æ¨¡å¼ï¼Œé€šè¿‡å…·æœ‰å¤šç§å¼‚è´¨å†…æ ¸å¤§å°çš„å·ç§¯æ“ä½œæœ‰æ•ˆæ•è·ä¸åŒçš„ç—…å˜ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒERoHPRFå¼•å…¥äº†ä¸€ç§ç±»ä¼¼äºä¸“å®¶çš„ç»“æ„åŒ–å†å‚æ•°åŒ–æŠ€æœ¯ï¼Œå°†å…¶å‚æ•°ä¸ä¸¤é˜¶æ®µç­–ç•¥ç›¸ç»“åˆï¼Œé€šè¿‡ä¸å•ä¸ªRFçš„æ¯”è¾ƒï¼Œç¡®ä¿å…·æœ‰ç«äº‰åŠ›çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†é€Ÿåº¦ã€‚ä¸ºäº†è¯æ˜ERoHPRFçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œæˆ‘ä»¬å°†å…¶çº³å…¥ä¸»æµçš„é«˜æ•ˆCNNæ¶æ„ä¸­ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ã€å…¬å¹³æ€§å’Œè®¡ç®—å¼€é”€æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å¹³è¡¡ã€‚æœ¬æ–‡çš„ä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13039v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>é«˜æ•ˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„è®¾è®¡æ—¥ç›Šå—åˆ°ç ”ç©¶å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯æ•æ‰å¤šæ ·ç—…ç¶ç‰¹å¾æ•ˆç‡æœ‰é™ï¼›äºŒæ˜¯ç”Ÿæˆçš„é¢„æµ‹å¸¸æœ‰åè§ã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¦‚å¿µâ€”â€”å¼‚è´¨é‡‘å­—å¡”æ„Ÿå—é‡çš„ä¸“å®¶å‹å†å‚æ•°åŒ–ï¼ˆERoHPRFï¼‰ï¼Œæ—¨åœ¨åŒæ—¶æé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„æ€§èƒ½å’Œå…¬å¹³æ€§ã€‚å®ƒæ—¨åœ¨é€šè¿‡åº”ç”¨ç²¾å¿ƒè®¾è®¡äº†çš„å¼‚è´¨é‡‘å­—å¡”æ„Ÿå—é‡è¢‹æ¥æ¨¡ä»¿å¤šä¸“å®¶å’¨è¯¢æ¨¡å¼ï¼Œå¹¶é€šè¿‡å¤šç§å¼‚è´¨å†…æ ¸å¤§å°çš„å·ç§¯æ“ä½œæœ‰æ•ˆæ•æ‰ä¸åŒçš„ç—…ç¶ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒERoHPRFè¿˜å¼•å…¥äº†ä¸€ç§ä¸“å®¶å‹ç»“æ„å†å‚æ•°åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ä¸¤é˜¶æ®µç­–ç•¥å°†å…¶å‚æ•°è¿›è¡Œåˆå¹¶ï¼Œä»¥ç¡®ä¿ä¸å•ä¸€æ„Ÿå—é‡ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†é€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ã€å…¬å¹³æ€§å’Œè®¡ç®—å¼€é”€æ–¹é¢è¾¾åˆ°äº†æ›´å¥½çš„æƒè¡¡ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>é«˜æ•ˆCNNæ¶æ„åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•æ‰å¤šæ ·ç—…ç¶ç‰¹å¾å’Œé¢„æµ‹å…¬å¹³æ€§ã€‚</li>
<li>æå‡ºERoHPRFæ¦‚å¿µï¼Œé€šè¿‡å¼‚è´¨é‡‘å­—å¡”æ„Ÿå—é‡è¢‹æœ‰æ•ˆæ•æ‰ä¸åŒç—…ç¶ç‰¹å¾ã€‚</li>
<li>ERoHPRFæ¨¡ä»¿å¤šä¸“å®¶å’¨è¯¢æ¨¡å¼ï¼Œæé«˜åŒ»å­¦å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚</li>
<li>å¼•å…¥ä¸“å®¶å‹ç»“æ„å†å‚æ•°åŒ–æŠ€æœ¯ï¼Œç¡®ä¿è®¡ç®—æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ã€å…¬å¹³æ€§å’Œè®¡ç®—å¼€é”€æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ä»£ç å°†å¾ˆå¿«å‘å¸ƒï¼Œä»¥ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-417d9c85da084d92b3ae699c39b02181.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-050625163f30ba004a21b3c8a68a2e3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-840309ba0e804a41d50dff1e47f4e18b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ec14e463498af0e84daf7dad9db6114.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0483d663d8cf68febe9217337691637.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mutual-Evidential-Deep-Learning-for-Medical-Image-Segmentation"><a href="#Mutual-Evidential-Deep-Learning-for-Medical-Image-Segmentation" class="headerlink" title="Mutual Evidential Deep Learning for Medical Image Segmentation"></a>Mutual Evidential Deep Learning for Medical Image Segmentation</h2><p><strong>Authors:Yuanpeng He, Yali Bi, Lijian Li, Chi-Man Pun, Wenpin Jiao, Zhi Jin</strong></p>
<p>Existing semi-supervised medical segmentation co-learning frameworks have realized that model performance can be diminished by the biases in model recognition caused by low-quality pseudo-labels. Due to the averaging nature of their pseudo-label integration strategy, they fail to explore the reliability of pseudo-labels from different sources. In this paper, we propose a mutual evidential deep learning (MEDL) framework that offers a potentially viable solution for pseudo-label generation in semi-supervised learning from two perspectives. First, we introduce networks with different architectures to generate complementary evidence for unlabeled samples and adopt an improved class-aware evidential fusion to guide the confident synthesis of evidential predictions sourced from diverse architectural networks. Second, utilizing the uncertainty in the fused evidence, we design an asymptotic Fisher information-based evidential learning strategy. This strategy enables the model to initially focus on unlabeled samples with more reliable pseudo-labels, gradually shifting attention to samples with lower-quality pseudo-labels while avoiding over-penalization of mislabeled classes in high data uncertainty samples. Additionally, for labeled data, we continue to adopt an uncertainty-driven asymptotic learning strategy, gradually guiding the model to focus on challenging voxels. Extensive experiments on five mainstream datasets have demonstrated that MEDL achieves state-of-the-art performance. </p>
<blockquote>
<p>ç°æœ‰çš„åŠç›‘ç£åŒ»å­¦åˆ†å‰²ååŒå­¦ä¹ æ¡†æ¶å·²ç»æ„è¯†åˆ°ï¼Œæ¨¡å‹è¯†åˆ«ä¸­çš„åè§ä¼šå¯¼è‡´ä¼ªæ ‡ç­¾è´¨é‡ä¸‹é™ï¼Œè¿›è€Œå½±å“æ¨¡å‹æ€§èƒ½ã€‚ç”±äºä»–ä»¬çš„ä¼ªæ ‡ç­¾é›†æˆç­–ç•¥çš„å¹³å‡æ€§è´¨ï¼Œä»–ä»¬æ— æ³•æ¢ç´¢ä»ä¸åŒæ¥æºè·å¾—çš„ä¼ªæ ‡ç­¾çš„å¯é æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§äº’è¯æ·±åº¦å­¦ä¹ ï¼ˆMEDLï¼‰æ¡†æ¶ï¼Œä»ä¸¤ä¸ªè§’åº¦ä¸ºåŠç›‘ç£å­¦ä¹ ä¸­çš„ä¼ªæ ‡ç­¾ç”Ÿæˆæä¾›äº†æ½œåœ¨å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥å…·æœ‰ä¸åŒæ¶æ„çš„ç½‘ç»œï¼Œä¸ºæœªæ ‡è®°æ ·æœ¬ç”Ÿæˆäº’è¡¥è¯æ®ï¼Œå¹¶é‡‡ç”¨æ”¹è¿›çš„åˆ†ç±»æ„ŸçŸ¥è¯æ®èåˆæ¥æŒ‡å¯¼æ¥è‡ªä¸åŒæ¶æ„ç½‘ç»œçš„è¯æ®é¢„æµ‹çš„è‡ªä¿¡åˆæˆã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨èåˆè¯æ®ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ¸è¿‘è´¹èˆå°”ä¿¡æ¯çš„è¯æ®å­¦ä¹ ç­–ç•¥ã€‚è¯¥ç­–ç•¥ä½¿æ¨¡å‹æœ€åˆå…³æ³¨å…·æœ‰æ›´å¯é ä¼ªæ ‡ç­¾çš„æœªæ ‡è®°æ ·æœ¬ï¼Œé€æ¸å°†æ³¨æ„åŠ›è½¬ç§»åˆ°å…·æœ‰è¾ƒä½è´¨é‡ä¼ªæ ‡ç­¾çš„æ ·æœ¬ä¸Šï¼ŒåŒæ—¶é¿å…åœ¨é«˜æ•°æ®ä¸ç¡®å®šæ€§æ ·æœ¬ä¸­è¿‡åº¦æƒ©ç½šè¯¯æ ‡è®°çš„ç±»åˆ«ã€‚æ­¤å¤–ï¼Œå¯¹äºæ ‡è®°æ•°æ®ï¼Œæˆ‘ä»¬ç»§ç»­é‡‡ç”¨åŸºäºä¸ç¡®å®šæ€§çš„æ¸è¿‘å­¦ä¹ ç­–ç•¥ï¼Œé€æ¸å¼•å¯¼æ¨¡å‹å…³æ³¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä½“ç´ ã€‚åœ¨äº”ä¸ªä¸»æµæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMEDLè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12418v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ååŒå­¦ä¹ æ¡†æ¶é¢ä¸´å› ä¼ªæ ‡ç­¾è´¨é‡å¯¼è‡´çš„æ¨¡å‹è¯†åˆ«åå·®é—®é¢˜ï¼Œå½±å“äº†æ¨¡å‹æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºä¸€ç§äº’è¯æ·±åº¦å­¦ä¹ ï¼ˆMEDLï¼‰æ¡†æ¶ï¼Œä»ä¸¤ä¸ªè§’åº¦ä¸ºè§£å†³ä¼ªæ ‡ç­¾ç”Ÿæˆé—®é¢˜æä¾›æ½œåœ¨å¯è¡Œæ–¹æ¡ˆã€‚é¦–å…ˆï¼Œå¼•å…¥ä¸åŒæ¶æ„çš„ç½‘ç»œä¸ºæœªæ ‡è®°æ ·æœ¬ç”Ÿæˆäº’è¡¥è¯æ®ï¼Œå¹¶é‡‡ç”¨æ”¹è¿›çš„åˆ†ç±»æ„ŸçŸ¥è¯æ®èåˆæŒ‡å¯¼æ¥è‡ªä¸åŒæ¶æ„ç½‘ç»œçš„è¯æ®é¢„æµ‹çš„åˆæˆã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨èåˆè¯æ®çš„ä¸ç¡®å®šæ€§ï¼Œè®¾è®¡åŸºäºæ¸è¿‘Fisherä¿¡æ¯çš„è¯æ®å­¦ä¹ ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å…³æ³¨ä¼ªæ ‡ç­¾æ›´å¯é çš„æœªæ ‡è®°æ ·æœ¬ï¼Œé€æ¸è½¬å‘ä¼ªæ ‡ç­¾è´¨é‡è¾ƒä½çš„æ ·æœ¬ï¼ŒåŒæ—¶é¿å…é«˜æ•°æ®ä¸ç¡®å®šæ€§æ ·æœ¬ä¸­è¯¯æ ‡è®°ç±»çš„è¿‡åº¦æƒ©ç½šã€‚å¯¹äº”ä¸ªä¸»æµæ•°æ®é›†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMEDLå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ååŒå­¦ä¹ æ¡†æ¶å—ä¼ªæ ‡ç­¾è´¨é‡å½±å“ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>ç°æœ‰æ¡†æ¶å› å¹³å‡ä¼ªæ ‡ç­¾æ•´åˆç­–ç•¥ï¼Œæ— æ³•æ¢ç´¢ä¸åŒæ¥æºä¼ªæ ‡ç­¾çš„å¯é æ€§ã€‚</li>
<li>MEDLæ¡†æ¶é€šè¿‡å¼•å…¥ä¸åŒæ¶æ„çš„ç½‘ç»œç”Ÿæˆäº’è¡¥è¯æ®ï¼Œæé«˜ä¼ªæ ‡ç­¾çš„è´¨é‡ã€‚</li>
<li>MEDLé‡‡ç”¨æ”¹è¿›çš„è¯æ®èåˆæ–¹æ³•å’Œæ¸è¿‘Fisherä¿¡æ¯è¯æ®å­¦ä¹ ç­–ç•¥ï¼Œå…³æ³¨æ›´å¯é çš„æœªæ ‡è®°æ ·æœ¬ã€‚</li>
<li>MEDLæ¡†æ¶èƒ½é€æ¸è½¬å‘ä¼ªæ ‡ç­¾è´¨é‡è¾ƒä½çš„æ ·æœ¬ï¼ŒåŒæ—¶é¿å…è¯¯æ ‡è®°ç±»çš„è¿‡åº¦æƒ©ç½šã€‚</li>
<li>MEDLåœ¨äº”ä¸ªä¸»æµæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f1c5b166f3bd457a6dfba9ed57f505b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-149364b749b9ca11a10a167570c6c2cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b23554d1956f7fb59687423014e352e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13a104fa7efb2c612bf496d55409ba18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2855ae4789dfe0b79018936b1adbb8e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e58e470a6309bf79df7aab828215ef5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ae7a2af58d4ef003e4e9ff0f55ae34b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e936f4fb2d6aff31c002a383dc196fae.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CTLformer-A-Hybrid-Denoising-Model-Combining-Convolutional-Layers-and-Self-Attention-for-Enhanced-CT-Image-Reconstruction"><a href="#CTLformer-A-Hybrid-Denoising-Model-Combining-Convolutional-Layers-and-Self-Attention-for-Enhanced-CT-Image-Reconstruction" class="headerlink" title="CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and   Self-Attention for Enhanced CT Image Reconstruction"></a>CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and   Self-Attention for Enhanced CT Image Reconstruction</h2><p><strong>Authors:Zhiting Zheng, Shuqi Wu, Wen Ding</strong></p>
<p>Low-dose CT (LDCT) images are often accompanied by significant noise, which negatively impacts image quality and subsequent diagnostic accuracy. To address the challenges of multi-scale feature fusion and diverse noise distribution patterns in LDCT denoising, this paper introduces an innovative model, CTLformer, which combines convolutional structures with transformer architecture. Two key innovations are proposed: a multi-scale attention mechanism and a dynamic attention control mechanism. The multi-scale attention mechanism, implemented through the Token2Token mechanism and self-attention interaction modules, effectively captures both fine details and global structures at different scales, enhancing relevant features and suppressing noise. The dynamic attention control mechanism adapts the attention distribution based on the noise characteristics of the input image, focusing on high-noise regions while preserving details in low-noise areas, thereby enhancing robustness and improving denoising performance. Furthermore, CTLformer integrates convolutional layers for efficient feature extraction and uses overlapping inference to mitigate boundary artifacts, further strengthening its denoising capability. Experimental results on the 2016 National Institutes of Health AAPM Mayo Clinic LDCT Challenge dataset demonstrate that CTLformer significantly outperforms existing methods in both denoising performance and model efficiency, greatly improving the quality of LDCT images. The proposed CTLformer not only provides an efficient solution for LDCT denoising but also shows broad potential in medical image analysis, especially for clinical applications dealing with complex noise patterns. </p>
<blockquote>
<p>ä½å‰‚é‡CTï¼ˆLDCTï¼‰å›¾åƒå¾€å¾€ä¼´éšç€æ˜¾è‘—çš„å™ªå£°ï¼Œè¿™ä¼šå¯¹å›¾åƒè´¨é‡å’Œéšåçš„è¯Šæ–­å‡†ç¡®æ€§äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºäº†è§£å†³LDCTå»å™ªä¸­çš„å¤šå°ºåº¦ç‰¹å¾èåˆå’Œå¤šæ ·å™ªå£°åˆ†å¸ƒæ¨¡å¼çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ›æ–°æ¨¡å‹CTLformerï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å·ç§¯ç»“æ„å’Œè½¬æ¢å™¨æ¶æ„ã€‚æå‡ºäº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šå¤šå°ºåº¦æ³¨æ„æœºåˆ¶å’ŒåŠ¨æ€æ³¨æ„æ§åˆ¶æœºåˆ¶ã€‚å¤šå°ºåº¦æ³¨æ„æœºåˆ¶é€šè¿‡Token2Tokenæœºåˆ¶å’Œè‡ªæ³¨æ„äº¤äº’æ¨¡å—å®ç°ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰ä¸åŒå°ºåº¦ä¸‹çš„ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€ç»“æ„ï¼Œå¢å¼ºç›¸å…³ç‰¹å¾å¹¶æŠ‘åˆ¶å™ªå£°ã€‚åŠ¨æ€æ³¨æ„æ§åˆ¶æœºåˆ¶æ ¹æ®è¾“å…¥å›¾åƒçš„å™ªå£°ç‰¹æ€§è°ƒæ•´æ³¨æ„åˆ†å¸ƒï¼Œå…³æ³¨é«˜å™ªå£°åŒºåŸŸçš„åŒæ—¶ä¿æŒä½å™ªå£°åŒºåŸŸçš„ç»†èŠ‚ï¼Œä»è€Œæé«˜ç¨³å¥æ€§ï¼Œæ”¹å–„å»å™ªæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCTLformeré›†æˆäº†å·ç§¯å±‚è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–ï¼Œå¹¶ä½¿ç”¨é‡å æ¨ç†æ¥ç¼“è§£è¾¹ç•Œä¼ªå½±ï¼Œè¿›ä¸€æ­¥åŠ å¼ºå…¶å»å™ªèƒ½åŠ›ã€‚åœ¨2016å¹´å›½ç«‹å«ç”Ÿç ”ç©¶é™¢AAPMæ¢…å¥¥è¯Šæ‰€LDCTæŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCTLformeråœ¨é™å™ªæ€§èƒ½å’Œæ¨¡å‹æ•ˆç‡æ–¹é¢éƒ½å¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå¤§åœ°æé«˜äº†LDCTå›¾åƒçš„è´¨é‡ã€‚æ‰€æå‡ºçš„CTLformerä¸ä»…ä¸ºLDCTå»å™ªæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œè¿˜æ˜¾ç¤ºå‡ºåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¹¿é˜”æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å™ªå£°æ¨¡å¼çš„ä¸´åºŠåº”ç”¨æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12203v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ä½å‰‚é‡CTï¼ˆLDCTï¼‰å›¾åƒå»å™ªçš„æ–°æ¨¡å‹CTLformerã€‚è¯¥æ¨¡å‹ç»“åˆäº†å·ç§¯ç»“æ„å’Œå˜å‹å™¨æ¶æ„ï¼Œé€šè¿‡å¤šå°ºåº¦æ³¨æ„æœºåˆ¶å’ŒåŠ¨æ€æ³¨æ„æ§åˆ¶æœºåˆ¶ï¼Œæœ‰æ•ˆæ•æ‰ä¸åŒå°ºåº¦çš„ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€ç»“æ„ï¼Œå¹¶æ ¹æ®è¾“å…¥å›¾åƒçš„å™ªå£°ç‰¹æ€§è‡ªé€‚åº”è°ƒæ•´æ³¨æ„åŠ›åˆ†å¸ƒï¼Œä»è€Œæé«˜ç¨³å¥æ€§å’Œå»å™ªæ€§èƒ½ã€‚åœ¨NIH AAPM Mayo Clinic LDCT Challengeæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCTLformeråœ¨é™å™ªæ€§èƒ½å’Œæ¨¡å‹æ•ˆç‡æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæå¤§æé«˜äº†LDCTå›¾åƒçš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDCTå›¾åƒå¸¸ä¼´éšæ˜¾è‘—å™ªå£°ï¼Œå½±å“å›¾åƒè´¨é‡å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚<br>2.CTLformeræ¨¡å‹ç»“åˆå·ç§¯ç»“æ„å’Œå˜å‹å™¨æ¶æ„ï¼Œæ—¨åœ¨è§£å†³LDCTå›¾åƒå»å™ªä¸­çš„å¤šå°ºåº¦ç‰¹å¾èåˆå’Œå™ªå£°åˆ†å¸ƒå¤šæ ·æ€§é—®é¢˜ã€‚</li>
<li>å¤šå°ºåº¦æ³¨æ„æœºåˆ¶é€šè¿‡Token2Tokenæœºåˆ¶å’Œè‡ªæ³¨æ„äº¤äº’æ¨¡å—ï¼Œæœ‰æ•ˆæ•æ‰ä¸åŒå°ºåº¦çš„ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€ç»“æ„ã€‚</li>
<li>åŠ¨æ€æ³¨æ„æ§åˆ¶æœºåˆ¶æ ¹æ®è¾“å…¥å›¾åƒçš„å™ªå£°ç‰¹æ€§è‡ªé€‚åº”è°ƒæ•´æ³¨æ„åŠ›åˆ†å¸ƒï¼Œå…¼é¡¾é«˜å™ªå£°åŒºåŸŸçš„å…³æ³¨ä¸ä½å™ªå£°åŒºåŸŸçš„ç»†èŠ‚ä¿ç•™ã€‚</li>
<li>CTLformeré€šè¿‡å·ç§¯å±‚è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–ï¼Œå¹¶ä½¿ç”¨é‡å æ¨ç†æ¥å‡è½»è¾¹ç•Œä¼ªå½±ï¼Œè¿›ä¸€æ­¥å¢å¼ºå»å™ªèƒ½åŠ›ã€‚</li>
<li>åœ¨NIH AAPM Mayo Clinic LDCT Challengeæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCTLformeråœ¨é™å™ªæ€§èƒ½å’Œæ¨¡å‹æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-83b71d6be8f68ee48fc06885918d83e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-399a412a3a7b68db5dfacb7cea03a6e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cbe4f5ba3767723fe37afd636a23215.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-434da360a217c4ec101e9943e64bdb31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8de597ab141d846406cacec0027d31d2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="BenSParX-A-Robust-Explainable-Machine-Learning-Framework-for-Parkinsonâ€™s-Disease-Detection-from-Bengali-Conversational-Speech"><a href="#BenSParX-A-Robust-Explainable-Machine-Learning-Framework-for-Parkinsonâ€™s-Disease-Detection-from-Bengali-Conversational-Speech" class="headerlink" title="BenSParX: A Robust Explainable Machine Learning Framework for   Parkinsonâ€™s Disease Detection from Bengali Conversational Speech"></a>BenSParX: A Robust Explainable Machine Learning Framework for   Parkinsonâ€™s Disease Detection from Bengali Conversational Speech</h2><p><strong>Authors:Riad Hossain, Muhammad Ashad Kabir, Arat Ibne Golam Mowla, Animesh Chandra Roy, Ranjit Kumar Ghosh</strong></p>
<p>Parkinsonâ€™s disease (PD) poses a growing global health challenge, with Bangladesh experiencing a notable rise in PD-related mortality. Early detection of PD remains particularly challenging in resource-constrained settings, where voice-based analysis has emerged as a promising non-invasive and cost-effective alternative. However, existing studies predominantly focus on English or other major languages; notably, no voice dataset for PD exists for Bengali - posing a significant barrier to culturally inclusive and accessible healthcare solutions. Moreover, most prior studies employed only a narrow set of acoustic features, with limited or no hyperparameter tuning and feature selection strategies, and little attention to model explainability. This restricts the development of a robust and generalizable machine learning model. To address this gap, we present BenSparX, the first Bengali conversational speech dataset for PD detection, along with a robust and explainable machine learning framework tailored for early diagnosis. The proposed framework incorporates diverse acoustic feature categories, systematic feature selection methods, and state-of-the-art machine learning algorithms with extensive hyperparameter optimization. Furthermore, to enhance interpretability and trust in model predictions, the framework incorporates SHAP (SHapley Additive exPlanations) analysis to quantify the contribution of individual acoustic features toward PD detection. Our framework achieves state-of-the-art performance, yielding an accuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further externally validated our approach by applying the framework to existing PD datasets in other languages, where it consistently outperforms state-of-the-art approaches. To facilitate further research and reproducibility, the dataset has been made publicly available at <a target="_blank" rel="noopener" href="https://github.com/Riad071/BenSParX">https://github.com/Riad071/BenSParX</a>. </p>
<blockquote>
<p>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰å·²æˆä¸ºå…¨çƒæ—¥ç›Šä¸¥å³»çš„å¥åº·æŒ‘æˆ˜ï¼Œå­ŸåŠ æ‹‰å›½çš„PDç›¸å…³æ­»äº¡ç‡ä¹Ÿæ˜¾è‘—ä¸Šå‡ã€‚åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ï¼Œæ—©æœŸPDæ£€æµ‹ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚åŸºäºå£°éŸ³çš„åˆ†æä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„éä¾µå…¥æ€§å’Œä½æˆæœ¬æ›¿ä»£æ–¹æ³•åº”è¿è€Œç”Ÿã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æˆ–å…¶ä»–ä¸»è¦è¯­è¨€ä¸Šï¼›å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°šæ— é’ˆå¯¹PDçš„å­ŸåŠ æ‹‰è¯­è¯­éŸ³æ•°æ®é›†ï¼Œè¿™å¯¹åŒ…å«æ–‡åŒ–ç‰¹å¾çš„å…¨é¢åŒ»ç–—è§£å†³æ–¹æ¡ˆæ„æˆäº†é‡å¤§éšœç¢ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æ—©æœŸç ”ç©¶ä»…ä½¿ç”¨äº†æœ‰é™çš„å£°å­¦ç‰¹å¾ï¼Œå¹¶ä¸”å¾ˆå°‘æœ‰æˆ–æ²¡æœ‰è¶…å‚æ•°è°ƒæ•´å’Œç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œå¯¹æ¨¡å‹è§£é‡Šæ€§çš„å…³æ³¨ä¹Ÿè¾ƒå°‘ã€‚è¿™é™åˆ¶äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BenSparXï¼Œè¿™æ˜¯é’ˆå¯¹PDæ£€æµ‹çš„é¦–ä¸ªå­ŸåŠ æ‹‰è¯­å¯¹è¯è¯­éŸ³æ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªç”¨äºæ—©æœŸè¯Šæ–­çš„ç¨³å¥ä¸”å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚æ‰€æå‡ºçš„æ¡†æ¶ç»“åˆäº†å¤šç§å£°å­¦ç‰¹å¾ç±»åˆ«ã€ç³»ç»Ÿç‰¹å¾é€‰æ‹©æ–¹æ³•ä»¥åŠå…·æœ‰å¹¿æ³›è¶…å‚æ•°ä¼˜åŒ–çš„æœ€æ–°æœºå™¨å­¦ä¹ ç®—æ³•ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜å¯¹æ¨¡å‹é¢„æµ‹çš„è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†SHAPï¼ˆSHapley Additive exPlanationsï¼‰åˆ†æï¼Œä»¥é‡åŒ–å•ä¸ªå£°å­¦ç‰¹å¾å¯¹PDæ£€æµ‹çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°æ°´å¹³ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†95.77%ï¼ŒF1åˆ†æ•°ä¸º95.57%ï¼ŒAUC-ROCä¸º0.982ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å°†è¯¥æ¡†æ¶åº”ç”¨äºå…¶ä»–è¯­è¨€çš„ç°æœ‰PDæ•°æ®é›†æ¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®ƒåœ¨å„ç§æƒ…å†µä¸‹å‡è¡¨ç°å‡ºè¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•çš„è¡¨ç°ã€‚ä¸ºäº†ä¾¿äºè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¯é‡å¤æ€§ï¼Œæ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Riad071/BenSParX%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Riad071/BenSParXä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12192v1">PDF</a> 46 pages, 16 figures</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†å­ŸåŠ æ‹‰å›½å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ£€æµ‹çš„æ–°ç ”ç©¶ã€‚é’ˆå¯¹èµ„æºå—é™ç¯å¢ƒä¸­æ—©æœŸPDæ£€æµ‹çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†BenSparXâ€”â€”é¦–ä¸ªç”¨äºPDæ£€æµ‹çš„å­ŸåŠ æ‹‰è¯­å¯¹è¯è¯­éŸ³æ•°æ®é›†å’Œä¸€ä¸ªç¨³å¥ã€å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆå¤šç§å£°å­¦ç‰¹å¾ã€ç³»ç»Ÿç‰¹å¾é€‰æ‹©æ–¹æ³•å’Œä¼˜åŒ–è¶…å‚æ•°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¹¶é‡‡ç”¨SHAPåˆ†ææé«˜æ¨¡å‹é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚ç ”ç©¶å–å¾—å“è¶Šæ€§èƒ½ï¼Œå‡†ç¡®ç‡è¾¾95.77%ï¼ŒF1åˆ†æ•°ä¸º95.57%ï¼ŒAUC-ROCä¸º0.982ï¼Œå¹¶åœ¨å…¶ä»–è¯­è¨€çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æ•°æ®é›†å·²å…¬å¼€åˆ†äº«ï¼Œä»¥æ¨åŠ¨è¿›ä¸€æ­¥ç ”ç©¶å’Œå¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰åœ¨å­ŸåŠ æ‹‰å›½å‘ˆç°ä¸Šå‡è¶‹åŠ¿ï¼Œæ—©æœŸæ£€æµ‹é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚</li>
<li>è¯­éŸ³åˆ†æä¸ºPDçš„æ—©æœŸéä¾µå…¥æ€§å’Œæˆæœ¬æ•ˆç›Šé«˜çš„æ£€æµ‹æä¾›äº†å¸Œæœ›ã€‚</li>
<li>ç›®å‰ç¼ºä¹é’ˆå¯¹å­ŸåŠ æ‹‰è¯­çš„PDè¯­éŸ³æ•°æ®é›†ï¼Œé˜»ç¢äº†æ–‡åŒ–åŒ…å®¹å’Œå¯è®¿é—®çš„åŒ»ç–—å«ç”Ÿè§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚</li>
<li>BenSparXæ˜¯é¦–ä¸ªä¸ºPDæ£€æµ‹è®¾è®¡çš„å­ŸåŠ æ‹‰è¯­å¯¹è¯è¯­éŸ³æ•°æ®é›†ã€‚</li>
<li>æå‡ºçš„æœºå™¨å­¦ä¹ æ¡†æ¶ç»“åˆå¤šç§å£°å­¦ç‰¹å¾ã€ç³»ç»Ÿç‰¹å¾é€‰æ‹©æ–¹æ³•å’Œå…ˆè¿›çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚</li>
<li>é‡‡ç”¨SHAPåˆ†ææé«˜æ¨¡å‹é¢„æµ‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶å–å¾—å“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å…¶ä»–è¯­è¨€çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ•°æ®é›†å·²å…¬å¼€åˆ†äº«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7c4fecdec4e47f61080415d247c35b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a2f03924730ccdb52845b31abcb4d2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Multimodal-Cancer-Survival-Analysis-via-Hypergraph-Learning-with-Cross-Modality-Rebalance"><a href="#Multimodal-Cancer-Survival-Analysis-via-Hypergraph-Learning-with-Cross-Modality-Rebalance" class="headerlink" title="Multimodal Cancer Survival Analysis via Hypergraph Learning with   Cross-Modality Rebalance"></a>Multimodal Cancer Survival Analysis via Hypergraph Learning with   Cross-Modality Rebalance</h2><p><strong>Authors:Mingcheng Qu, Guang Yang, Donglin Di, Tonghua Su, Yue Gao, Yang Song, Lei Fan</strong></p>
<p>Multimodal pathology-genomic analysis has become increasingly prominent in cancer survival prediction. However, existing studies mainly utilize multi-instance learning to aggregate patch-level features, neglecting the information loss of contextual and hierarchical details within pathology images. Furthermore, the disparity in data granularity and dimensionality between pathology and genomics leads to a significant modality imbalance. The high spatial resolution inherent in pathology data renders it a dominant role while overshadowing genomics in multimodal integration. In this paper, we propose a multimodal survival prediction framework that incorporates hypergraph learning to effectively capture both contextual and hierarchical details from pathology images. Moreover, it employs a modality rebalance mechanism and an interactive alignment fusion strategy to dynamically reweight the contributions of the two modalities, thereby mitigating the pathology-genomics imbalance. Quantitative and qualitative experiments are conducted on five TCGA datasets, demonstrating that our model outperforms advanced methods by over 3.4% in C-Index performance. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç—…ç†åŸºå› ç»„å­¦åˆ†æåœ¨ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ä¸­è¶Šæ¥è¶Šçªå‡ºã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦åˆ©ç”¨å¤šå®ä¾‹å­¦ä¹ æ¥èšåˆè¡¥ä¸çº§åˆ«çš„ç‰¹å¾ï¼Œå¿½è§†äº†ç—…ç†å›¾åƒä¸­ä¸Šä¸‹æ–‡å’Œå±‚æ¬¡ç»†èŠ‚çš„ä¿¡æ¯ä¸¢å¤±ã€‚æ­¤å¤–ï¼Œç—…ç†å’ŒåŸºå› ç»„ä¹‹é—´æ•°æ®ç²’åº¦å’Œç»´åº¦çš„å·®å¼‚å¯¼è‡´äº†æ˜æ˜¾çš„æ¨¡æ€ä¸å¹³è¡¡ã€‚ç—…ç†æ•°æ®çš„é«˜ç©ºé—´åˆ†è¾¨ç‡ä½¿å…¶åœ¨å¤šæ¨¡æ€èåˆä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œæ©ç›–äº†åŸºå› ç»„å­¦çš„ä½œç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€ç”Ÿå­˜é¢„æµ‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†è¶…å›¾å­¦ä¹ ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•è·ç—…ç†å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡å’Œå±‚æ¬¡ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨æ¨¡æ€å†å¹³è¡¡æœºåˆ¶å’Œäº¤äº’å¯¹é½èåˆç­–ç•¥ï¼ŒåŠ¨æ€åœ°é‡æ–°è®¡ç®—ä¸¤ç§æ¨¡æ€çš„è´¡çŒ®ï¼Œä»è€Œç¼“è§£ç—…ç†åŸºå› ç»„å­¦çš„ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨äº”ä¸ªTCGAæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹çš„C-Indexæ€§èƒ½æ¯”å…ˆè¿›çš„æ–¹æ³•é«˜å‡ºè¶…è¿‡3.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11997v2">PDF</a> accepted by IJCAI2025 Code: <a target="_blank" rel="noopener" href="https://github.com/MCPathology/MRePath">https://github.com/MCPathology/MRePath</a></p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå¤šæ¨¡æ€åˆ†æåœ¨ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ä¸­è¶Šæ¥è¶Šé‡è¦ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡å¤šå®ä¾‹å­¦ä¹ æ¥èšåˆè¡¥ä¸çº§åˆ«çš„ç‰¹å¾ï¼Œä½†å¿½ç•¥äº†ç—…ç†å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡å’Œå±‚æ¬¡ç»†èŠ‚çš„ä¸¢å¤±ã€‚åŒæ—¶ï¼Œç—…ç†å’ŒåŸºå› ç»„æ•°æ®åœ¨ç²’åº¦å’Œç»´åº¦ä¸Šçš„å·®å¼‚å¯¼è‡´æ¨¡æ€ä¸å¹³è¡¡ã€‚æœ¬æ–‡æå‡ºä¸€ç§å¤šæ¨¡æ€ç”Ÿå­˜é¢„æµ‹æ¡†æ¶ï¼Œé‡‡ç”¨è¶…å›¾å­¦ä¹ æœ‰æ•ˆæ•æ‰ç—…ç†å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡å’Œå±‚æ¬¡ç»†èŠ‚ï¼Œå¹¶å¼•å…¥æ¨¡æ€å¹³è¡¡æœºåˆ¶å’Œäº¤äº’å¯¹é½èåˆç­–ç•¥ï¼ŒåŠ¨æ€è°ƒæ•´ä¸¤ç§æ¨¡æ€çš„è´¡çŒ®ï¼Œç¼“è§£ç—…ç†-åŸºå› ç»„ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨äº”ä¸ªTCGAæ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨C-Indexæ€§èƒ½ä¸Šæ¯”å…ˆè¿›æ–¹æ³•é«˜å‡º3.4%ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ç—…ç†åŸºå› ç»„åˆ†æåœ¨ç™Œç—‡ç”Ÿå­˜é¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰ç ”ç©¶é€šè¿‡å¤šå®ä¾‹å­¦ä¹ èšåˆè¡¥ä¸çº§åˆ«ç‰¹å¾ï¼Œä½†å¿½ç•¥äº†ä¸Šä¸‹æ–‡å’Œå±‚æ¬¡ç»†èŠ‚çš„ä¸¢å¤±ã€‚</li>
<li>ç—…ç†å’ŒåŸºå› ç»„æ•°æ®åœ¨ç²’åº¦å’Œç»´åº¦ä¸Šçš„å·®å¼‚å¯¼è‡´æ¨¡æ€ä¸å¹³è¡¡ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶é‡‡ç”¨è¶…å›¾å­¦ä¹ æ•æ‰ç—…ç†å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡å’Œå±‚æ¬¡ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥æ¨¡æ€å¹³è¡¡æœºåˆ¶æ¥ç¼“è§£ç—…ç†-åŸºå› ç»„ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>å®šé‡å’Œå®šæ€§å®éªŒåœ¨äº”ä¸ªTCGAæ•°æ®é›†ä¸ŠéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5e326c4accb53966f7eea6f11e3e3cd2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-272ee61de76c00a74cf06935848812d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aca270db3cfc52ef073d0a94081a18b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f1afa56eb67e8246d104abdc929c3cb.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Online-Iterative-Self-Alignment-for-Radiology-Report-Generation"><a href="#Online-Iterative-Self-Alignment-for-Radiology-Report-Generation" class="headerlink" title="Online Iterative Self-Alignment for Radiology Report Generation"></a>Online Iterative Self-Alignment for Radiology Report Generation</h2><p><strong>Authors:Ting Xiao, Lei Shi, Yang Zhang, HaoFeng Yang, Zhe Wang, Chenjia Bai</strong></p>
<p>Radiology Report Generation (RRG) is an important research topic for relieving radiologistâ€™ heavy workload. Existing RRG models mainly rely on supervised fine-tuning (SFT) based on different model architectures using data pairs of radiological images and corresponding radiologist-annotated reports. Recent research has shifted focus to post-training improvements, aligning RRG model outputs with human preferences using reinforcement learning (RL). However, the limited data coverage of high-quality annotated data poses risks of overfitting and generalization. This paper proposes a novel Online Iterative Self-Alignment (OISA) method for RRG that consists of four stages: self-generation of diverse data, self-evaluation for multi-objective preference data,self-alignment for multi-objective optimization and self-iteration for further improvement. Our approach allows for generating varied reports tailored to specific clinical objectives, enhancing the overall performance of the RRG model iteratively. Unlike existing methods, our frame-work significantly increases data quality and optimizes performance through iterative multi-objective optimization. Experimental results demonstrate that our method surpasses previous approaches, achieving state-of-the-art performance across multiple evaluation metrics. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒæŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ˜¯ç¼“è§£æ”¾å°„ç§‘åŒ»ç”Ÿç¹é‡å·¥ä½œé‡çš„ä¸€é¡¹é‡è¦ç ”ç©¶è¯¾é¢˜ã€‚ç°æœ‰çš„RRGæ¨¡å‹ä¸»è¦ä¾èµ–äºåŸºäºä¸åŒæ¨¡å‹æ¶æ„çš„ç²¾ç»†è°ƒæ•´ï¼ˆSFTï¼‰ï¼Œä½¿ç”¨æ”¾å°„å›¾åƒå’Œç›¸åº”çš„æ”¾å°„ç§‘åŒ»ç”Ÿæ³¨é‡ŠæŠ¥å‘Šçš„æ•°æ®å¯¹è¿›è¡Œè®­ç»ƒã€‚æœ€è¿‘çš„ç ”ç©¶é‡ç‚¹è½¬å‘äº†å¯¹è®­ç»ƒåçš„æ”¹è¿›ï¼Œé€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½¿RRGæ¨¡å‹çš„è¾“å‡ºä¸äººç±»åå¥½å¯¹é½ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„æœ‰é™è¦†ç›–èŒƒå›´å¸¦æ¥äº†è¿‡æ‹Ÿåˆå’Œæ³›åŒ–çš„é£é™©ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åœ¨çº¿è¿­ä»£è‡ªå¯¹é½ï¼ˆOISAï¼‰æ–¹æ³•ç”¨äºåŒ»å­¦å½±åƒæŠ¥å‘Šç”Ÿæˆï¼ŒåŒ…æ‹¬å››ä¸ªé˜¶æ®µï¼šå¤šæ ·åŒ–æ•°æ®çš„è‡ªæˆ‘ç”Ÿæˆï¼Œå¤šç›®æ ‡åå¥½æ•°æ®çš„è‡ªæˆ‘è¯„ä»·ï¼Œå¤šç›®æ ‡ä¼˜åŒ–çš„è‡ªæˆ‘å¯¹é½ä»¥åŠè¿›ä¸€æ­¥çš„è‡ªæˆ‘è¿­ä»£æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ ¹æ®ç‰¹å®šçš„ä¸´åºŠç›®æ ‡ç”Ÿæˆå¤šç§æŠ¥å‘Šï¼Œé€šè¿‡è¿­ä»£æ–¹å¼æé«˜åŒ»å­¦å½±åƒæŠ¥å‘Šç”Ÿæˆæ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚ä¸åŒäºç°æœ‰çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡è¿­ä»£å¤šç›®æ ‡ä¼˜åŒ–æ˜¾è‘—æé«˜äº†æ•°æ®è´¨é‡å¹¶ä¼˜åŒ–äº†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè¯„ä»·æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11983v2">PDF</a> Accepted by ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åœ¨çº¿è¿­ä»£è‡ªå¯¹é½ï¼ˆOISAï¼‰æ–¹æ³•ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰ï¼ŒåŒ…æ‹¬å››ä¸ªé˜¶æ®µï¼šå¤šæ ·æ•°æ®çš„è‡ªæˆ‘ç”Ÿæˆã€å¤šç›®æ ‡åå¥½æ•°æ®çš„è‡ªæˆ‘è¯„ä»·ã€å¤šç›®æ ‡ä¼˜åŒ–çš„è‡ªæˆ‘å¯¹é½ä»¥åŠè¿›ä¸€æ­¥çš„è‡ªæˆ‘è¿­ä»£æ”¹è¿›ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆç¬¦åˆç‰¹å®šä¸´åºŠç›®æ ‡çš„å¤šæ ·åŒ–æŠ¥å‘Šï¼Œé€šè¿‡è¿­ä»£å¤šç›®æ ‡ä¼˜åŒ–æé«˜RRGæ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ï¼Œåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RRGæ˜¯ç¼“è§£æ”¾å°„ç§‘åŒ»ç”Ÿç¹é‡å·¥ä½œé‡çš„é‡è¦ç ”ç©¶è¯é¢˜ã€‚</li>
<li>ç°æœ‰RRGæ¨¡å‹ä¸»è¦ä¾èµ–äºä½¿ç”¨å›¾åƒå’Œç›¸åº”æ”¾å°„ç§‘åŒ»ç”Ÿæ³¨é‡ŠæŠ¥å‘Šçš„æ•°æ®å¯¹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶é›†ä¸­åœ¨è®­ç»ƒåçš„æ”¹è¿›ä¸Šï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½¿RRGæ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>é«˜è´¨é‡æ³¨é‡Šæ•°æ®çš„æœ‰é™è¦†ç›–å­˜åœ¨è¿‡æ‹Ÿåˆå’Œæ³›åŒ–é£é™©ã€‚</li>
<li>æå‡ºçš„OISAæ–¹æ³•åŒ…æ‹¬å¤šæ ·æ•°æ®è‡ªæˆ‘ç”Ÿæˆã€å¤šç›®æ ‡åå¥½è‡ªæˆ‘è¯„ä»·ã€å¤šç›®æ ‡ä¼˜åŒ–è‡ªæˆ‘å¯¹é½å’Œè¿›ä¸€æ­¥è‡ªæˆ‘è¿­ä»£æ”¹è¿›å››ä¸ªé˜¶æ®µã€‚</li>
<li>OISAæ–¹æ³•å…è®¸ç”Ÿæˆç¬¦åˆç‰¹å®šä¸´åºŠç›®æ ‡çš„å¤šæ ·åŒ–æŠ¥å‘Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd74402581fe6fd3c7beb943610e64b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eca1516754e5e3be8c69103b10781ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8f2bde3471c5877621a196f1a54ec69.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AoP-SAM-Automation-of-Prompts-for-Efficient-Segmentation"><a href="#AoP-SAM-Automation-of-Prompts-for-Efficient-Segmentation" class="headerlink" title="AoP-SAM: Automation of Prompts for Efficient Segmentation"></a>AoP-SAM: Automation of Prompts for Efficient Segmentation</h2><p><strong>Authors:Yi Chen, Mu-Young Son, Chuanbo Hua, Joo-Young Kim</strong></p>
<p>The Segment Anything Model (SAM) is a powerful foundation model for image segmentation, showing robust zero-shot generalization through prompt engineering. However, relying on manual prompts is impractical for real-world applications, particularly in scenarios where rapid prompt provision and resource efficiency are crucial. In this paper, we propose the Automation of Prompts for SAM (AoP-SAM), a novel approach that learns to generate essential prompts in optimal locations automatically. AoP-SAM enhances SAMâ€™s efficiency and usability by eliminating manual input, making it better suited for real-world tasks. Our approach employs a lightweight yet efficient Prompt Predictor model that detects key entities across images and identifies the optimal regions for placing prompt candidates. This method leverages SAMâ€™s image embeddings, preserving its zero-shot generalization capabilities without requiring fine-tuning. Additionally, we introduce a test-time instance-level Adaptive Sampling and Filtering mechanism that generates prompts in a coarse-to-fine manner. This notably enhances both prompt and mask generation efficiency by reducing computational overhead and minimizing redundant mask refinements. Evaluations of three datasets demonstrate that AoP-SAM substantially improves both prompt generation efficiency and mask generation accuracy, making SAM more effective for automated segmentation tasks. </p>
<blockquote>
<p>Segment Anything Modelï¼ˆSAMï¼‰æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹å±•ç¤ºç¨³å¥çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¾èµ–æ‰‹åŠ¨æç¤ºå¯¹äºå®é™…åº”ç”¨æ¥è¯´å¹¶ä¸å®ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿæä¾›æç¤ºå’Œèµ„æºæ•ˆç‡è‡³å…³é‡è¦çš„åœºæ™¯ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SAMçš„è‡ªåŠ¨æç¤ºï¼ˆAoP-SAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå¯ä»¥è‡ªåŠ¨å­¦ä¹ åœ¨æœ€ä½³ä½ç½®ç”Ÿæˆå…³é”®æç¤ºã€‚AoP-SAMé€šè¿‡æ¶ˆé™¤æ‰‹åŠ¨è¾“å…¥ï¼Œæé«˜äº†SAMçš„æ•ˆç‡å’Œå¯ç”¨æ€§ï¼Œä½¿å…¶æ›´é€‚åˆå®é™…ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è½»ä¾¿é«˜æ•ˆçš„æç¤ºé¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ£€æµ‹å›¾åƒä¸­çš„å…³é”®å®ä½“å¹¶è¯†åˆ«æ”¾ç½®æç¤ºå€™é€‰äººçš„æœ€ä½³åŒºåŸŸã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨SAMçš„å›¾åƒåµŒå…¥ï¼Œåœ¨ä¸éœ€è¦è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ä¿ç•™å…¶é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æµ‹è¯•æ—¶å®ä¾‹çº§è‡ªé€‚åº”é‡‡æ ·å’Œè¿‡æ»¤æœºåˆ¶ï¼Œä»¥ç²—ç»†ç»“åˆçš„æ–¹å¼ç”Ÿæˆæç¤ºã€‚è¿™æ˜¾è‘—æé«˜äº†æç¤ºå’Œè’™ç‰ˆç”Ÿæˆæ•ˆç‡ï¼Œå‡å°‘äº†è®¡ç®—å¼€é”€å’Œå†—ä½™è’™ç‰ˆä¿®æ­£ã€‚å¯¹ä¸‰ä¸ªæ•°æ®é›†çš„è¯„ä»·è¡¨æ˜ï¼ŒAoP-SAMå¤§å¤§æé«˜äº†æç¤ºç”Ÿæˆæ•ˆç‡å’Œè’™ç‰ˆç”Ÿæˆå‡†ç¡®æ€§ï¼Œä½¿SAMåœ¨è‡ªåŠ¨åŒ–åˆ†å‰²ä»»åŠ¡ä¸­æ›´åŠ æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11980v1">PDF</a> Accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹é€šè¿‡æç¤ºå·¥ç¨‹æŠ€æœ¯å®ç°äº†å¼ºå¤§çš„å›¾åƒåˆ†å‰²åŠŸèƒ½ï¼Œä½†æ‰‹åŠ¨æç¤ºåœ¨å®é™…åº”ç”¨ä¸­ä¸å®ç”¨ã€‚æœ¬æ–‡æå‡ºè‡ªåŠ¨åŒ–æç¤ºæ–¹æ³•AoP-SAMï¼Œé€šè¿‡è‡ªåŠ¨åœ¨æœ€ä½³ä½ç½®ç”Ÿæˆå…³é”®æç¤ºæ¥å¢å¼ºSAMçš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è½»é‡çº§ä¸”é«˜æ•ˆçš„æç¤ºé¢„æµ‹å™¨æ¨¡å‹ï¼Œåœ¨å›¾åƒä¸­æ£€æµ‹å…³é”®å®ä½“å¹¶ç¡®å®šæœ€ä½³æç¤ºåŒºåŸŸã€‚åˆ©ç”¨SAMçš„å›¾åƒåµŒå…¥ï¼Œæ— éœ€å¾®è°ƒå³å¯ä¿ç•™å…¶é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¼•å…¥æµ‹è¯•æ—¶å®ä¾‹çº§è‡ªé€‚åº”é‡‡æ ·å’Œè¿‡æ»¤æœºåˆ¶ï¼Œä»¥ç²—åˆ°ç»†çš„æ–¹å¼ç”Ÿæˆæç¤ºï¼Œæé«˜æç¤ºå’Œæ©è†œç”Ÿæˆæ•ˆç‡ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒAoP-SAMæ˜¾è‘—æé«˜äº†æç¤ºç”Ÿæˆæ•ˆç‡å’Œæ©è†œç”Ÿæˆå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMæ¨¡å‹æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹æŠ€æœ¯å®ç°ã€‚</li>
<li>æ‰‹åŠ¨æç¤ºåœ¨å®é™…åº”ç”¨ä¸­ä¸å®ç”¨ï¼Œéœ€è¦è‡ªåŠ¨åŒ–æç¤ºæ–¹æ³•ã€‚</li>
<li>AoP-SAMæ–¹æ³•è‡ªåŠ¨åœ¨æœ€ä½³ä½ç½®ç”Ÿæˆå…³é”®æç¤ºï¼Œæé«˜SAMçš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚</li>
<li>æç¤ºé¢„æµ‹å™¨æ¨¡å‹ç”¨äºæ£€æµ‹å›¾åƒä¸­çš„å…³é”®å®ä½“å¹¶ç¡®å®šæœ€ä½³æç¤ºåŒºåŸŸã€‚</li>
<li>åˆ©ç”¨SAMçš„å›¾åƒåµŒå…¥ï¼Œæ— éœ€å¾®è°ƒå³å¯ä¿ç•™å…¶é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥æµ‹è¯•æ—¶å®ä¾‹çº§è‡ªé€‚åº”é‡‡æ ·å’Œè¿‡æ»¤æœºåˆ¶ï¼Œæé«˜æç¤ºå’Œæ©è†œç”Ÿæˆçš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d05f5cadc8fe82b546012055d1fd5fa3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-983cf48ce7fc04962927facd22b98aa4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ba30cf265ccd75b4f778fbfe4a1037f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c27e599fd139bc2affbc003f97fbfde4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8d390c7432c36b287614b038206bfd0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DC-Seg-Disentangled-Contrastive-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities"><a href="#DC-Seg-Disentangled-Contrastive-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities" class="headerlink" title="DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation   with Missing Modalities"></a>DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation   with Missing Modalities</h2><p><strong>Authors:Haitao Li, Ziyu Li, Yiheng Mao, Zhengyao Ding, Zhengxing Huang</strong></p>
<p>Accurate segmentation of brain images typically requires the integration of complementary information from multiple image modalities. However, clinical data for all modalities may not be available for every patient, creating a significant challenge. To address this, previous studies encode multiple modalities into a shared latent space. While somewhat effective, it remains suboptimal, as each modality contains distinct and valuable information. In this study, we propose DC-Seg (Disentangled Contrastive Learning for Segmentation), a new method that explicitly disentangles images into modality-invariant anatomical representation and modality-specific representation, by using anatomical contrastive learning and modality contrastive learning respectively. This solution improves the separation of anatomical and modality-specific features by considering the modality gaps, leading to more robust representations. Furthermore, we introduce a segmentation-based regularizer that enhances the modelâ€™s robustness to missing modalities. Extensive experiments on the BraTS 2020 and a private white matter hyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperforms state-of-the-art methods in handling incomplete multimodal brain tumor segmentation tasks with varying missing modalities, while also demonstrate strong generalizability in WMH segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/CuCl-2/DC-Seg">https://github.com/CuCl-2/DC-Seg</a>. </p>
<blockquote>
<p>å¯¹è„‘å›¾åƒçš„ç²¾ç¡®åˆ†å‰²é€šå¸¸éœ€è¦æ•´åˆæ¥è‡ªå¤šç§å›¾åƒæ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå¹¶éæ¯ä½æ‚£è€…çš„æ‰€æœ‰æ¨¡æ€çš„ä¸´åºŠæ•°æ®éƒ½æ˜¯å¯ç”¨çš„ï¼Œè¿™å°±å½¢æˆäº†ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä¹‹å‰çš„ç ”ç©¶å°†å¤šç§æ¨¡æ€ç¼–ç åˆ°å…±äº«æ½œåœ¨ç©ºé—´ä¸­ã€‚è™½ç„¶è¿™ç§åšæ³•åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒä»ç„¶ä¸å¤Ÿç†æƒ³ï¼Œå› ä¸ºæ¯ä¸ªæ¨¡æ€éƒ½åŒ…å«ç‹¬ç‰¹ä¸”æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DC-Segï¼ˆç”¨äºåˆ†å‰²çš„è§£çº ç¼ å¯¹æ¯”å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ†åˆ«ä½¿ç”¨è§£å‰–å¯¹æ¯”å­¦ä¹ å’Œæ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼Œæ˜¾å¼åœ°å°†å›¾åƒè§£çº ç¼ æˆæ¨¡æ€ä¸å˜çš„è§£å‰–è¡¨ç¤ºå’Œæ¨¡æ€ç‰¹å®šçš„è¡¨ç¤ºã€‚æ­¤è§£å†³æ–¹æ¡ˆé€šè¿‡è€ƒè™‘æ¨¡æ€é—´éš™ï¼Œæé«˜äº†è§£å‰–å’Œæ¨¡æ€ç‰¹å®šç‰¹å¾çš„åˆ†ç¦»ï¼Œä»è€Œå¾—åˆ°æ›´ç¨³å¥çš„è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºåˆ†å‰²çš„æ­£åˆ™åŒ–å™¨ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ç¼ºå¤±æ¨¡æ€çš„é²æ£’æ€§ã€‚åœ¨BraTS 2020å’Œç§äººç™½è´¨é«˜ä¿¡å·ï¼ˆWMHï¼‰åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDC-Segåœ¨å¤„ç†å…·æœ‰ä¸åŒç¼ºå¤±æ¨¡æ€çš„ä¸å®Œæ•´å¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶åœ¨WMHåˆ†å‰²ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CuCl-2/DC-Seg%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CuCl-2/DC-Segæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11921v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•DC-Segï¼Œè¯¥æ–¹æ³•é€šè¿‡è§£è€¦å¯¹æ¯”å­¦ä¹ æ˜¾å¼åœ°å°†å›¾åƒåˆ†è§£ä¸ºæ¨¡æ€ä¸å˜çš„ç»“æ„è¡¨ç¤ºå’Œæ¨¡æ€ç‰¹å®šçš„è¡¨ç¤ºï¼Œä»è€Œæ”¹è¿›äº†ç»“æ„ç‰¹å¾å’Œæ¨¡æ€ç‰¹å¾çš„åˆ†ç¦»ã€‚DC-Segåœ¨å¤„ç†ä¸åŒç¼ºå¤±æ¨¡æ€çš„ä¸å®Œæ•´å¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨WMHåˆ†å‰²ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DC-Segè§£å†³äº†å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¯èƒ½å­˜åœ¨çš„æ¨¡æ€æ•°æ®ä¸å®Œæ•´çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡è§£è€¦å¯¹æ¯”å­¦ä¹ ï¼ŒDC-Segå°†å›¾åƒåˆ†è§£ä¸ºæ¨¡æ€ä¸å˜çš„ç»“æ„è¡¨ç¤ºå’Œæ¨¡æ€ç‰¹å®šçš„è¡¨ç¤ºï¼Œä»¥æ”¹è¿›ç‰¹å¾åˆ†ç¦»ã€‚</li>
<li>DC-Segè€ƒè™‘äº†æ¨¡æ€å·®å¼‚ï¼Œå¢å¼ºäº†ç»“æ„å’Œæ¨¡æ€ç‰¹å¾çš„é²æ£’æ€§è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åˆ†å‰²æ­£åˆ™åŒ–å™¨ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ç¼ºå¤±æ¨¡æ€çš„é²æ£’æ€§ã€‚</li>
<li>åœ¨BraTS 2020æ•°æ®é›†å’Œç§æœ‰WMHåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDC-Segåœ¨å¤„ç†ä¸å®Œæ•´å¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡æ—¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>DC-Segåœ¨WMHåˆ†å‰²ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11921">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f785bdd688632a6cf275eee9a58f213.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8aa3c044634d4c71c24020aa158d44cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f966cd08ec99b09a6ef121e244276fed.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Inter-Domain-Gap-through-Low-Level-Features-for-Cross-Modal-Medical-Image-Segmentation"><a href="#Bridging-the-Inter-Domain-Gap-through-Low-Level-Features-for-Cross-Modal-Medical-Image-Segmentation" class="headerlink" title="Bridging the Inter-Domain Gap through Low-Level Features for Cross-Modal   Medical Image Segmentation"></a>Bridging the Inter-Domain Gap through Low-Level Features for Cross-Modal   Medical Image Segmentation</h2><p><strong>Authors:Pengfei Lyu, Pak-Hei Yeung, Xiaosheng Yu, Jing Xia, Jianning Chi, Chengdong Wu, Jagath C. Rajapakse</strong></p>
<p>This paper addresses the task of cross-modal medical image segmentation by exploring unsupervised domain adaptation (UDA) approaches. We propose a model-agnostic UDA framework, LowBridge, which builds on a simple observation that cross-modal images share some similar low-level features (e.g., edges) as they are depicting the same structures. Specifically, we first train a generative model to recover the source images from their edge features, followed by training a segmentation model on the generated source images, separately. At test time, edge features from the target images are input to the pretrained generative model to generate source-style target domain images, which are then segmented using the pretrained segmentation network. Despite its simplicity, extensive experiments on various publicly available datasets demonstrate that \proposed achieves state-of-the-art performance, outperforming eleven existing UDA approaches under different settings. Notably, further ablation studies show that \proposed is agnostic to different types of generative and segmentation models, suggesting its potential to be seamlessly plugged with the most advanced models to achieve even more outstanding results in the future. The code is available at <a target="_blank" rel="noopener" href="https://github.com/JoshuaLPF/LowBridge">https://github.com/JoshuaLPF/LowBridge</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†é€šè¿‡æ¢ç´¢æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ–¹æ³•è¿›è¡Œè·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¨¡å‹æ— å…³çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ¡†æ¶LowBridgeï¼Œè¯¥æ¡†æ¶åŸºäºä¸€ç§ç®€å•è§‚å¯Ÿï¼Œå³è·¨æ¨¡æ€å›¾åƒç”±äºæè¿°ç›¸åŒç»“æ„è€Œå…±äº«ä¸€äº›ç›¸ä¼¼çš„ä½çº§ç‰¹å¾ï¼ˆä¾‹å¦‚è¾¹ç¼˜ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œä»è¾¹ç¼˜ç‰¹å¾æ¢å¤æºå›¾åƒï¼Œç„¶ååˆ†åˆ«åœ¨ç”Ÿæˆçš„æºå›¾åƒä¸Šè®­ç»ƒä¸€ä¸ªåˆ†å‰²æ¨¡å‹ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œå°†ç›®æ ‡å›¾åƒçš„è¾¹ç¼˜ç‰¹å¾è¾“å…¥åˆ°é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆæºé£æ ¼çš„ç›®æ ‡åŸŸå›¾åƒï¼Œç„¶åä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†å‰²ç½‘ç»œå¯¹å…¶è¿›è¡Œåˆ†å‰²ã€‚å°½ç®¡å…¶ç®€å•æ€§ï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä¸åŒè®¾ç½®ä¸‹è¶…è¶Šäº†ç°æœ‰çš„åä¸€ç§UDAæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿›ä¸€æ­¥çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯¹ä¸åŒç±»å‹çš„ç”Ÿæˆå’Œåˆ†å‰²æ¨¡å‹å…·æœ‰é€šç”¨æ€§ï¼Œè¿™è¡¨æ˜å®ƒæœ‰å¯èƒ½ä¸æœ€å…ˆè¿›çš„æ¨¡å‹æ— ç¼é›†æˆï¼Œåœ¨æœªæ¥å®ç°æ›´å‡ºè‰²çš„ç»“æœã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/JoshuaLPF/LowBridge%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/JoshuaLPF/LowBridgeæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11909v1">PDF</a> 11 pages, 2 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒè·¨æ¨¡æ€åˆ†å‰²ä»»åŠ¡é€šè¿‡æ¢ç´¢æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ–¹æ³•è¿›è¡Œç ”ç©¶ã€‚æå‡ºäº†ä¸€ç§æ¨¡å‹é€šç”¨çš„UDAæ¡†æ¶LowBridgeï¼Œå®ƒåŸºäºè·¨æ¨¡æ€å›¾åƒå…±äº«ç›¸åŒç»“æ„çš„ä¸€äº›ç›¸ä¼¼ä½çº§åˆ«ç‰¹å¾ï¼ˆå¦‚è¾¹ç¼˜ï¼‰çš„ç®€å•è§‚å¯Ÿã€‚é€šè¿‡è®­ç»ƒç”Ÿæˆæ¨¡å‹æ¥ä»è¾¹ç¼˜ç‰¹å¾æ¢å¤æºå›¾åƒï¼Œå¹¶å•ç‹¬è®­ç»ƒåˆ†å‰²æ¨¡å‹ã€‚æµ‹è¯•æ—¶ï¼Œå°†ç›®æ ‡å›¾åƒçš„è¾¹ç¼˜ç‰¹å¾è¾“å…¥é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œç”Ÿæˆæºé£æ ¼çš„ç›®æ ‡åŸŸå›¾åƒï¼Œç„¶åä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†å‰²ç½‘ç»œè¿›è¡Œåˆ†å‰²ã€‚å°½ç®¡å…¶ç®€å•æ€§ï¼Œåœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ä¸åŒè®¾ç½®ä¸‹ä¼˜äºåä¸€ç§ç°æœ‰çš„UDAæ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JoshuaLPF/LowBridge">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ¨¡å‹é€šç”¨çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æ¡†æ¶LowBridgeï¼Œç”¨äºè§£å†³åŒ»å­¦å›¾åƒè·¨æ¨¡æ€åˆ†å‰²é—®é¢˜ã€‚</li>
<li>LowBridgeæ¡†æ¶åŸºäºè·¨æ¨¡æ€å›¾åƒå…±äº«ä½çº§åˆ«ç‰¹å¾çš„è§‚å¯Ÿï¼Œå¦‚è¾¹ç¼˜ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬è®­ç»ƒç”Ÿæˆæ¨¡å‹ä»¥ä»è¾¹ç¼˜ç‰¹å¾æ¢å¤æºå›¾åƒï¼Œå¹¶å•ç‹¬è®­ç»ƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>åœ¨æµ‹è¯•é˜¶æ®µï¼Œä½¿ç”¨ç›®æ ‡å›¾åƒçš„è¾¹ç¼˜ç‰¹å¾ç”Ÿæˆæºé£æ ¼çš„ç›®æ ‡åŸŸå›¾åƒï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†å‰²ç½‘ç»œè¿›è¡Œåˆ†å‰²ã€‚</li>
<li>è®ºæ–‡åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¡¨æ˜è¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ä¸å¤šç§ç°æœ‰çš„UDAæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒè®¾ç½®ä¸‹å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6697be9897c618250b0a774808e33c9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5803371c74a53bc1adce7f217f6e23e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5c801ad32c6c597b18f64a5393fcd6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9d34feb89099ec83161a6417e9cd84.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-bf76b9501af682faaab986e5985edde6.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  FMSD-TTS Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d38d1501446a78f9354c70defe4087cb.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Training-Free Watermarking for Autoregressive Image Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
