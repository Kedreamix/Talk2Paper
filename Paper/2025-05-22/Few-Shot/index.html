<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  PRL Prompts from Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7964b2debc9adf20fd660a685fa7ceda.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-22-æ›´æ–°"><a href="#2025-05-22-æ›´æ–°" class="headerlink" title="2025-05-22 æ›´æ–°"></a>2025-05-22 æ›´æ–°</h1><h2 id="PRL-Prompts-from-Reinforcement-Learning"><a href="#PRL-Prompts-from-Reinforcement-Learning" class="headerlink" title="PRL: Prompts from Reinforcement Learning"></a>PRL: Prompts from Reinforcement Learning</h2><p><strong>Authors:PaweÅ‚ Batorski, Adrian Kosmala, Paul Swoboda</strong></p>
<p>Effective prompt engineering remains a central challenge in fully harnessing the capabilities of LLMs. While well-designed prompts can dramatically enhance performance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on subtle semantic cues, ones that may elude human perception but are crucial for guiding LLM behavior. In this paper, we introduce PRL (Prompts from Reinforcement Learning), a novel RL-based approach for automatic prompt generation. Unlike previous methods, PRL can produce novel few-shot examples that were not seen during training. Our approach achieves state-of-the-art performance across a range of benchmarks, including text classification, simplification, and summarization. On the classification task, it surpasses prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it improves the average ROUGE scores on the summarization task by 4.32 over APE and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over APE and by 6.01 over EvoPrompt. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Batorskq/prl">https://github.com/Batorskq/prl</a> . </p>
<blockquote>
<p>æœ‰æ•ˆæç¤ºå·¥ç¨‹ä»ç„¶æ˜¯å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è™½ç„¶ç²¾å¿ƒè®¾è®¡å¥½çš„æç¤ºå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä½†åˆ¶ä½œå®ƒä»¬é€šå¸¸éœ€è¦ä¸“å®¶çš„ç›´è§‰å’Œå¯¹ä»»åŠ¡çš„å¾®å¦™ç†è§£ã€‚æ­¤å¤–ï¼Œæœ€æœ‰å½±å“åŠ›çš„æç¤ºé€šå¸¸ä¾èµ–äºå¾®å¦™çš„è¯­ä¹‰çº¿ç´¢ï¼Œè¿™äº›çº¿ç´¢å¯èƒ½ä¼šé€ƒé¿äººç±»çš„æ„ŸçŸ¥ï¼Œä½†å¯¹äºæŒ‡å¯¼LLMè¡Œä¸ºè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PRLï¼ˆåŸºäºå¼ºåŒ–å­¦ä¹ çš„æç¤ºï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè‡ªåŠ¨æç¤ºç”Ÿæˆçš„æ–°å‹åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ã€‚ä¸åŒäºä»¥å‰çš„æ–¹æ³•ï¼ŒPRLå¯ä»¥ç”Ÿæˆåœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„å…¨æ–°å°‘æ ·æœ¬ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€ç®€åŒ–å’Œæ‘˜è¦ã€‚åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œå®ƒæ¯”APEé«˜å‡º2.58%ï¼Œæ¯”EvoPrompté«˜å‡º1.00%ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ‘˜è¦ä»»åŠ¡çš„å¹³å‡ROUGEå¾—åˆ†ä¸Šæ¯”APEé«˜å‡º4.32%ï¼Œæ¯”EvoPrompté«˜å‡º2.12ï¼›åœ¨ç®€åŒ–ä»»åŠ¡çš„SARIå¾—åˆ†ä¸Šæ¯”APEé«˜å‡º6.93%ï¼Œæ¯”EvoPrompté«˜å‡º6.01ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Batorskq/prl%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Batorskq/prlæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14412v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªåŠ¨æç¤ºç”Ÿæˆæ–¹æ³•PRLï¼ˆPrompts from Reinforcement Learningï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿæ–°é¢–çš„åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§è¿‡çš„å°‘æ•°ä¾‹å­æç¤ºã€‚åœ¨æ–‡æœ¬åˆ†ç±»ã€ç®€åŒ–å’Œæ€»ç»“ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPRLå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥æ–¹æ³•è¶…è¶Šäº†å…ˆå‰çš„è‡ªåŠ¨æç¤ºç”Ÿæˆæ–¹æ³•ï¼Œå…·æœ‰æå¤§çš„å®ç”¨ä»·å€¼ã€‚æœ‰å…³ä»£ç å¯ä»¥åœ¨æŒ‡å®šçš„GitHubåœ°å€ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PRLæ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨æç¤ºç”Ÿæˆæ–¹æ³•ï¼Œå¯ä»¥ç”Ÿæˆæ–°é¢–çš„å°‘æ•°ä¾‹å­æç¤ºã€‚</li>
<li>PRLåœ¨æ–‡æœ¬åˆ†ç±»ã€ç®€åŒ–å’Œæ€»ç»“ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>PRLè¶…è¶Šäº†å…ˆå‰çš„è‡ªåŠ¨æç¤ºç”Ÿæˆæ–¹æ³•ï¼Œå±•ç°å‡ºæ›´å¥½çš„å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
<li>PRLçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå°¤å…¶åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œç›¸æ¯”APEå’ŒEvoPromptåˆ†åˆ«æœ‰æ›´é«˜çš„å‡†ç¡®æ€§æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14412">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-044889130030332563dff3aa0e66a4cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a19bc824cb1c07b8c78df300a6a8d05d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0de9e71bb1a30459171b3b63eb5e0b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3008c0890fecc712f0c17bb5f469debc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation"><a href="#FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation" class="headerlink" title="FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation"></a>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation</h2><p><strong>Authors:Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi</strong></p>
<p>Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-&quot;U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality. </p>
<blockquote>
<p>è—è¯­æ˜¯ä¸€ç§èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œå…¶ä¸‰å¤§æ–¹è¨€åŒºâ€”â€”ä¹Œé½ã€å®‰å¤šå’Œåº·åŒºçš„å¹³è¡Œè¯­éŸ³è¯­æ–™åº“æå…¶æœ‰é™ï¼Œé™åˆ¶äº†è¯­éŸ³å»ºæ¨¡çš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FMSD-TTSï¼Œè¿™æ˜¯ä¸€ä¸ªå°‘æ ·æœ¬ã€å¤šå‘è¨€äººã€å¤šæ–¹è¨€çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»æœ‰é™çš„å‚è€ƒéŸ³é¢‘å’Œæ˜ç¡®çš„æ–¹è¨€æ ‡ç­¾ä¸­åˆæˆå¹³è¡Œæ–¹è¨€è¯­éŸ³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ–°é¢–çš„å‘å£°äºº-æ–¹è¨€èåˆæ¨¡å—å’Œæ–¹è¨€ä¸“ä¸šåŒ–åŠ¨æ€è·¯ç”±ç½‘ç»œï¼ˆDSDR-Netï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰ä¸åŒæ–¹è¨€ä¹‹é—´çš„ç»†å¾®å£°å­¦å’Œè¯­è¨€å˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™å‘å£°äººçš„èº«ä»½ã€‚å¤§é‡çš„å®¢è§‚å’Œä¸»è§‚è¯„ä¼°è¡¨æ˜ï¼ŒFMSD-TTSåœ¨æ–¹è¨€è¡¨ç°åŠ›å’Œå‘å£°äººç›¸ä¼¼æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†çº¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­éŸ³åˆ°è¯­éŸ³æ–¹è¨€è½¬æ¢ä»»åŠ¡éªŒè¯äº†åˆæˆè¯­éŸ³çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹è—è¯­å¤šæ–¹è¨€è¯­éŸ³åˆæˆçš„æ–°å‹å°‘æ ·æœ¬TTSç³»ç»Ÿï¼Œï¼ˆ2ï¼‰å…¬å¼€å‘å¸ƒç”±FMSD-TTSç”Ÿæˆçš„å¤§è§„æ¨¡åˆæˆè—è¯­è¯­éŸ³è¯­æ–™åº“ï¼Œï¼ˆ3ï¼‰å¼€æ”¾æºä»£ç è¯„ä¼°å·¥å…·åŒ…ï¼Œç”¨äºæ ‡å‡†åŒ–è¯„ä¼°å‘å£°äººç›¸ä¼¼æ€§ã€æ–¹è¨€ä¸€è‡´æ€§å’ŒéŸ³é¢‘è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14351v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹è—è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€ï¼Œå¦‚ä½•é€šè¿‡ä½¿ç”¨å°‘é‡æ•°æ®è§£å†³å…¶åœ¨è¯­éŸ³å»ºæ¨¡æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ä¸ªé¢å‘å°‘æ•°æ¼”è®²è€…ã€å¤šæ–¹è¨€çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¡†æ¶FMSD-TTSï¼Œé€šè¿‡æœ‰é™å‚è€ƒéŸ³é¢‘å’Œæ˜ç¡®çš„æ–¹è¨€æ ‡ç­¾åˆæˆå¹¶è¡Œæ–¹è¨€è¯­éŸ³ã€‚è¯¥æ¡†æ¶å…·æœ‰æ–°é¢–çš„æ¼”è®²è€…-æ–¹è¨€èåˆæ¨¡å—å’Œæ–¹è¨€ç‰¹å®šåŠ¨æ€è·¯ç”±ç½‘ç»œï¼ˆDSDR-Netï¼‰ï¼Œå¯æ•æ‰æ–¹è¨€é—´çš„ç»†å¾®å£°å­¦å·®å¼‚å’Œè¯­è¨€å˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™æ¼”è®²è€…èº«ä»½ã€‚è¯„ä¼°å’Œå®éªŒè¯æ˜ï¼ŒFMSD-TTSåœ¨æ–¹è¨€è¡¨è¾¾åŠ›å’Œæ¼”è®²è€…ç›¸ä¼¼æ€§æ–¹é¢æ˜æ˜¾ä¼˜äºåŸºçº¿ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œé€šè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­éŸ³åˆ°è¯­éŸ³æ–¹è¨€è½¬æ¢ä»»åŠ¡éªŒè¯äº†åˆæˆè¯­éŸ³çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è—è¯­æ˜¯ä½èµ„æºè¯­è¨€ï¼Œç¼ºä¹å¹³è¡Œè¯­æ–™åº“ï¼Œé™åˆ¶å…¶è¯­éŸ³å»ºæ¨¡å‘å±•ã€‚</li>
<li>æå‡ºFMSD-TTSæ¡†æ¶ï¼Œå®ç°åˆ©ç”¨æœ‰é™æ•°æ®åˆæˆæ–¹è¨€è¯­éŸ³ã€‚</li>
<li>FMSD-TTSå…·å¤‡å¤šæ–¹è¨€å’Œæ¼”è®²è€…èƒ½åŠ›ï¼Œå¹¶é€šè¿‡èåˆæ¨¡å—å’ŒDSDR-NetæŠ€æœ¯æ•æ‰æ–¹è¨€å·®å¼‚å’Œè¯­è¨€å˜åŒ–ã€‚</li>
<li>FMSD-TTSæ˜¾è‘—æé«˜äº†æ–¹è¨€è¡¨è¾¾åŠ›å’Œæ¼”è®²è€…ç›¸ä¼¼æ€§æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡è¯­éŸ³åˆ°è¯­éŸ³æ–¹è¨€è½¬æ¢ä»»åŠ¡éªŒè¯äº†åˆæˆè¯­éŸ³çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚</li>
<li>å…¬å¼€å‘å¸ƒç”±FMSD-TTSç”Ÿæˆçš„å¤§è§„æ¨¡è—è¯­åˆæˆè¯­éŸ³è¯­æ–™åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3a712b79d7ffc4c1dca0f96958d2ba5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1785cc70a11836b2715822d27c917e0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a811d1d51e8b11dcb30516dfa2c64ac7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a42a77d797ad5aa3c3834fc192c0333d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a0dee36095191e6784bfb9b357a3309.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-MIND-for-Reasoning-Meta-learning-for-In-context-Deduction"><a href="#A-MIND-for-Reasoning-Meta-learning-for-In-context-Deduction" class="headerlink" title="A MIND for Reasoning: Meta-learning for In-context Deduction"></a>A MIND for Reasoning: Meta-learning for In-context Deduction</h2><p><strong>Authors:Leonardo Bertolazzi, Manuel Vargas GuzmÃ¡n, Raffaella Bernardi, Maciej Malicki, Jakub Szymanik</strong></p>
<p>Large language models (LLMs) are increasingly evaluated on formal tasks, where strong reasoning abilities define the state of the art. However, their ability to generalize to out-of-distribution problems remains limited. In this paper, we investigate how LLMs can achieve a systematic understanding of deductive rules. Our focus is on the task of identifying the appropriate subset of premises within a knowledge base needed to derive a given hypothesis. To tackle this challenge, we propose Meta-learning for In-context Deduction (MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND is to enable models to generalize more effectively to unseen knowledge bases and to systematically apply inference rules. Our results show that MIND significantly improves generalization in small LMs ranging from 1.5B to 7B parameters. The benefits are especially pronounced in smaller models and low-data settings. Remarkably, small models fine-tuned with MIND outperform state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­£å¼ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¶Šæ¥è¶Šå¤šï¼Œå…¶ä¸­å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å®šä¹‰äº†å‰æ²¿æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹äºéåˆ†å¸ƒé—®é¢˜çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMå¦‚ä½•å®ç°å¯¹æ¼”ç»è§„åˆ™çš„ç³»ç»Ÿç†è§£ã€‚æˆ‘ä»¬çš„é‡ç‚¹æ˜¯è§£å†³åœ¨çŸ¥è¯†åº“ä¸­è¯†åˆ«ç”¨äºæ¨å¯¼ç»™å®šå‡è®¾æ‰€éœ€å‰æçš„é€‚å½“å­é›†çš„ä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºä¸Šä¸‹æ–‡å†…æ¼”ç»çš„å…ƒå­¦ä¹ ï¼ˆMINDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å°æ ·æœ¬å…ƒå­¦ä¹ å¾®è°ƒæ–¹æ³•ã€‚MINDçš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„çŸ¥è¯†åº“ï¼Œå¹¶ç³»ç»Ÿåœ°åº”ç”¨æ¨ç†è§„åˆ™ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMINDæ˜¾è‘—æé«˜äº†ä»1.5Båˆ°7Bå‚æ•°èŒƒå›´å†…çš„å°å‹LMçš„æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨å°å‹æ¨¡å‹å’Œä½æ•°æ®è®¾ç½®ä¸‹ï¼Œå…¶ä¼˜åŠ¿å°¤ä¸ºçªå‡ºã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨MINDå¾®è°ƒçš„å°å‹æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPT-4oå’Œo3-miniã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­£å¼ä»»åŠ¡ä¸Šçš„è¡¨ç°æ—¥ç›Šå—åˆ°é‡è§†ï¼Œå…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›æˆä¸ºè¡¡é‡æŠ€æœ¯å‘å±•æ°´å¹³çš„å…³é”®æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹äºè·¨åˆ†å¸ƒé—®é¢˜çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶å¦‚ä½•è®©å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿç†è§£æ¼”ç»è§„åˆ™ã€‚æˆ‘ä»¬èšç„¦äºè¯†åˆ«çŸ¥è¯†åº“ä¸­ç”¨äºæ¨å¯¼ç»™å®šå‡è®¾æ‰€éœ€çš„å‰ææ¡ä»¶çš„ä»»åŠ¡ã€‚ä¸ºè§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸º MINDï¼ˆç”¨äºä¸Šä¸‹æ–‡å†…æ¼”ç»çš„å…ƒå­¦ä¹ ï¼‰çš„æ–°å‹å…ƒå­¦ä¹ å¾®è°ƒæ–¹æ³•ã€‚MIND çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹æ›´æœ‰æ•ˆåœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„çŸ¥è¯†åº“ï¼Œå¹¶ç³»ç»Ÿåœ°åº”ç”¨æ¨ç†è§„åˆ™ã€‚ç»“æœæ˜¾ç¤ºï¼ŒMIND åœ¨å°å‹è¯­è¨€æ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯å‚æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œé‡‡ç”¨ MIND ç²¾ç»†è®­ç»ƒçš„å°å‹æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPT-4oå’Œo3-miniã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­£å¼ä»»åŠ¡ä¸Šçš„æ¨ç†èƒ½åŠ›æˆä¸ºè¡¡é‡æŠ€æœ¯å‘å±•çš„å…³é”®æŒ‡æ ‡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¯¹äºè·¨åˆ†å¸ƒé—®é¢˜ä»ç„¶æœ‰é™ã€‚</li>
<li>æœ¬æ–‡èšç„¦äºå¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ¼”ç»è§„åˆ™çš„ç³»ç»Ÿç†è§£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å…ƒå­¦ä¹ å¾®è°ƒæ–¹æ³•MINDï¼Œç”¨äºè§£å†³è¯†åˆ«çŸ¥è¯†åº“ä¸­æ¨å¯¼ç»™å®šå‡è®¾æ‰€éœ€å‰ææ¡ä»¶çš„ä»»åŠ¡ã€‚</li>
<li>MINDæé«˜äº†å°å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚</li>
<li>é‡‡ç”¨MINDè®­ç»ƒçš„å°å‹æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c43c4f4fe9cb56474d4ec8609340548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c67fb438256b67575dd4c8aaea80bc85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0a0858d42483e3a8bbf1da6f9964a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36886669c30451fd961ec7fb1de712d4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Decoupling-Classifier-for-Boosting-Few-shot-Object-Detection-and-Instance-Segmentation"><a href="#Decoupling-Classifier-for-Boosting-Few-shot-Object-Detection-and-Instance-Segmentation" class="headerlink" title="Decoupling Classifier for Boosting Few-shot Object Detection and   Instance Segmentation"></a>Decoupling Classifier for Boosting Few-shot Object Detection and   Instance Segmentation</h2><p><strong>Authors:Bin-Bin Gao, Xiaochen Chen, Zhongyi Huang, Congchong Nie, Jun Liu, Jinxiang Lai, Guannan Jiang, Xi Wang, Chengjie Wang</strong></p>
<p>This paper focus on few-shot object detection<del>(FSOD) and instance segmentation</del>(FSIS), which requires a model to quickly adapt to novel classes with a few labeled instances. The existing methods severely suffer from bias classification because of the missing label issue which naturally exists in an instance-level few-shot scenario and is first formally proposed by us. Our analysis suggests that the standard classification head of most FSOD or FSIS models needs to be decoupled to mitigate the bias classification. Therefore, we propose an embarrassingly simple but effective method that decouples the standard classifier into two heads. Then, these two individual heads are capable of independently addressing clear positive samples and noisy negative samples which are caused by the missing label. In this way, the model can effectively learn novel classes while mitigating the effects of noisy negative samples. Without bells and whistles, our model without any additional computation cost and parameters consistently outperforms its baseline and state-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for FSOD and FSIS tasks. The Code is available at <a target="_blank" rel="noopener" href="https://csgaobb.github.io/Projects/DCFS">https://csgaobb.github.io/Projects/DCFS</a>. </p>
<blockquote>
<p>æœ¬æ–‡å…³æ³¨å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰å’Œå®ä¾‹åˆ†å‰²ï¼ˆFSISï¼‰ï¼Œè¿™éœ€è¦æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿåœ°é€‚åº”å…·æœ‰å°‘é‡æ ‡è®°å®ä¾‹çš„æ–°ç±»åˆ«ã€‚ç”±äºå®ä¾‹çº§å°‘æ ·æœ¬åœºæ™¯ä¸­è‡ªç„¶å­˜åœ¨çš„ç¼ºå¤±æ ‡ç­¾é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é­å—ä¸¥é‡çš„åç½®åˆ†ç±»ã€‚æˆ‘ä»¬é¦–æ¬¡æ­£å¼æå‡ºè¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¤§å¤šæ•°FSODæˆ–FSISæ¨¡å‹çš„æ ‡å‡†åˆ†ç±»å¤´éœ€è¦è§£è€¦ï¼Œä»¥å‡è½»åç½®åˆ†ç±»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†æ ‡å‡†åˆ†ç±»å™¨è§£è€¦ä¸ºä¸¤ä¸ªå¤´ã€‚ç„¶åï¼Œè¿™ä¸¤ä¸ªç‹¬ç«‹çš„å¤´èƒ½å¤Ÿç‹¬ç«‹åœ°å¤„ç†ç”±ç¼ºå¤±æ ‡ç­¾å¼•èµ·çš„æ¸…æ™°æ­£æ ·æœ¬å’Œå˜ˆæ‚è´Ÿæ ·æœ¬ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶å‡è½»å˜ˆæ‚è´Ÿæ ·æœ¬çš„å½±å“ã€‚æ²¡æœ‰é¢å¤–çš„è®¡ç®—å’Œå‚æ•°æˆæœ¬ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨FSODå’ŒFSISä»»åŠ¡çš„PASCAL VOCå’ŒMS-COCOåŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºå…¶åŸºçº¿å’Œå…¶ä»–æœ€æ–°æŠ€æœ¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://csgaobb.github.io/Projects/DCFS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://csgaobb.github.io/Projects/DCFSä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14239v1">PDF</a> Accepted by NeurIPS 2022</p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡å…³æ³¨å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰å’Œå®ä¾‹åˆ†å‰²ï¼ˆFSISï¼‰ï¼Œé’ˆå¯¹æ¨¡å‹åœ¨æ–°ç±»åˆ«ä¸­å¿«é€Ÿé€‚åº”çš„é—®é¢˜æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰æ–¹æ³•å—åˆ°ç¼ºå¤±æ ‡ç­¾å¯¼è‡´çš„åè§åˆ†ç±»çš„å›°æ‰°ï¼Œè®ºæ–‡é¦–æ¬¡æ­£å¼æå‡ºè¿™ä¸€é—®é¢˜å¹¶åˆ†æäº†æ ‡å‡†çš„åˆ†ç±»å¤´éœ€è¦è¿›è¡Œè§£è€¦ä»¥å‡å°‘åè§åˆ†ç±»ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥è§£è€¦æ ‡å‡†åˆ†ç±»å™¨ä¸ºä¸¤ä¸ªå¤´ï¼Œèƒ½å¤Ÿç‹¬ç«‹å¤„ç†æ¸…æ™°çš„é˜³æ€§æ ·æœ¬å’Œç¼ºå¤±æ ‡ç­¾å¯¼è‡´çš„å™ªå£°é˜´æ€§æ ·æœ¬ã€‚æ­¤æ–¹æ³•ä¸ä»…æœ‰æ•ˆå­¦ä¹ äº†æ–°ç±»åˆ«ï¼Œè¿˜å‡å°‘äº†å™ªå£°é˜´æ€§æ ·æœ¬çš„å½±å“ï¼Œåœ¨PASCAL VOCå’ŒMS-COCOçš„FSODå’ŒFSISä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­å¤§å¹…åº¦ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸”æ— é¢å¤–çš„è®¡ç®—æˆæœ¬å’Œå‚æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡èšç„¦äºå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²é¢†åŸŸçš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹åœ¨æ–°ç±»åˆ«ä¸­å¿«é€Ÿé€‚åº”çš„é—®é¢˜æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—åˆ°ç¼ºå¤±æ ‡ç­¾å¯¼è‡´çš„åè§åˆ†ç±»å›°æ‰°ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„è§£è€¦æ ‡å‡†åˆ†ç±»å™¨çš„æ–¹æ³•ï¼Œå°†å…¶åˆ†ä¸ºä¸¤ä¸ªå¤´æ¥å¤„ç†ä¸åŒçš„æ ·æœ¬ç±»å‹ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå‡å°‘å™ªå£°é˜´æ€§æ ·æœ¬çš„å½±å“ï¼Œæé«˜æ¨¡å‹å­¦ä¹ æ•ˆæœã€‚</li>
<li>åœ¨PASCAL VOCå’ŒMS-COCOçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-10d06bb2d86f8956c1a940a7a8592f88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be77d9da841f59a66d2a4308f1f4c44c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b4b0ef9e21ba757a0fce8ea667aa79.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Mechanistic-Fine-tuning-for-In-context-Learning"><a href="#Mechanistic-Fine-tuning-for-In-context-Learning" class="headerlink" title="Mechanistic Fine-tuning for In-context Learning"></a>Mechanistic Fine-tuning for In-context Learning</h2><p><strong>Authors:Hakaze Cho, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue</strong></p>
<p>In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åˆ©ç”¨ç»“æ„åŒ–æ¼”ç¤ºæŸ¥è¯¢è¾“å…¥ï¼Œä»¥åœ¨è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ä¸Šå®ç°å°‘é‡å­¦ä¹ ï¼Œè¿™äº›è¯­è¨€æ¨¡å‹æœ€åˆå¹¶æœªåœ¨ICLé£æ ¼çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚ä¸ºäº†å¼¥è¡¥ICLå’Œé¢„è®­ç»ƒä¹‹é—´çš„å·®è·ï¼Œä¸€äº›æ–¹æ³•é€šè¿‡ç«¯åˆ°ç«¯çš„èŒƒå¼å¯¹LMsè¿›è¡Œå¾®è°ƒä»¥é€‚åº”å¤§è§„æ¨¡ICLé£æ ¼çš„æ•°æ®é›†ï¼Œè¿™äº§ç”Ÿäº†å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†é™ä½è¿™ç§æˆæœ¬ï¼Œæœ¬æ–‡æå‡ºäº†æ³¨æ„åŠ›è¡Œä¸ºå¾®è°ƒï¼ˆABFTï¼‰ï¼Œå®ƒåˆ©ç”¨å¯¹ICLå†…åœ¨æœºåˆ¶çš„å‰æœŸå‘ç°ï¼Œåœ¨æ³¨æ„åŠ›åˆ†æ•°ä¸Šæ„å»ºè®­ç»ƒç›®æ ‡ï¼Œè€Œä¸æ˜¯æœ€ç»ˆçš„è¾“å‡ºã€‚è¿™è¿«ä½¿æ³¨æ„åŠ›åˆ†æ•°å…³æ³¨ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„æ­£ç¡®æ ‡ç­¾ä»¤ç‰Œï¼Œå¹¶å‡è½»äº†å¯¹é”™è¯¯æ ‡ç­¾ä»¤ç‰Œçš„æ³¨æ„åŠ›åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨9ä¸ªç°ä»£LMså’Œ8ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯å®ï¼ŒABFTåœ¨æ€§èƒ½ã€ç¨³å¥æ€§ã€å…¬æ­£æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ•°æ®æˆæœ¬åªæœ‰çº¦0.01%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åç»­åˆ†æå‘ç°ï¼Œç«¯åˆ°ç«¯çš„è®­ç»ƒç›®æ ‡åŒ…å«ABFTç›®æ ‡ï¼Œè¿™è¡¨æ˜ICLé£æ ¼æ•°æ®å¯¹å½’çº³å¤´çš„å‡ºç°å­˜åœ¨éšæ€§åè§ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†æ§åˆ¶LMå†…ç‰¹å®šæ¨¡å—åºåˆ—çš„å¯èƒ½æ€§ï¼Œä»¥æé«˜å…¶è¡Œä¸ºè¡¨ç°ï¼Œä¸ºæœªæ¥çš„æœºæ¢°è§£é‡Šæ€§åº”ç”¨å¼€è¾Ÿäº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14233v1">PDF</a> 28 pages, 31 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ¼”ç¤ºæŸ¥è¯¢è¾“å…¥å®ç°å°‘æ•°æ ·æœ¬å­¦ä¹ çš„è¯­å¢ƒå­¦ä¹ ï¼ˆICLï¼‰æ–¹æ³•ï¼Œé€šè¿‡è°ƒæ•´è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰æ¥å¼¥è¡¥å…¶ä¸é¢„è®­ç»ƒä¹‹é—´çš„å·®è·ã€‚ä¸ºé™ä½æˆæœ¬ï¼Œæœ¬æ–‡æå‡ºåˆ©ç”¨è¯­å¢ƒå­¦ä¹ çš„å†…åœ¨æœºåˆ¶ï¼Œé€šè¿‡å…³æ³¨æ³¨æ„åŠ›åˆ†æ•°æ„å»ºè®­ç»ƒç›®æ ‡ï¼Œè¿«ä½¿æ³¨æ„åŠ›åˆ†æ•°èšç„¦äºæ­£ç¡®æ ‡ç­¾è¯ï¼ŒåŒæ—¶å‰Šå¼±é”™è¯¯æ ‡ç­¾è¯çš„æ³¨æ„åŠ›åˆ†æ•°çš„æ–¹æ³•â€”â€”æ³¨æ„åŠ›è¡Œä¸ºå¾®è°ƒï¼ˆABFTï¼‰ã€‚å®éªŒè¯æ˜ï¼ŒABFTåœ¨æ€§èƒ½ã€ç¨³å¥æ€§ã€å…¬æ­£æ€§å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ•°æ®æˆæœ¬ä»…çº¦ä¸ºä¼ ç»Ÿæ–¹æ³•çš„åƒåˆ†ä¹‹ä¸€ã€‚åŒæ—¶åˆ†ææ˜¾ç¤ºï¼Œç«¯åˆ°ç«¯çš„è®­ç»ƒç›®æ ‡åŒ…å«äº†ABFTç›®æ ‡ï¼Œè¡¨æ˜è¯­å¢ƒå­¦ä¹ æ•°æ®çš„éšæ€§åå‘å½’çº³å¤´çš„å‡ºç°ã€‚æœ¬æ–‡å±•ç¤ºäº†æ§åˆ¶è¯­è¨€æ¨¡å‹å†…éƒ¨ç‰¹å®šæ¨¡å—åºåˆ—ä»¥æ”¹å–„å…¶è¡Œä¸ºçš„å¯èƒ½æ€§ï¼Œä¸ºæœªæ¥çš„æœºæ¢°è§£é‡Šæ€§åº”ç”¨æ‰“å¼€äº†å¤§é—¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICLåˆ©ç”¨ç»“æ„åŒ–çš„æ¼”ç¤ºæŸ¥è¯¢è¾“å…¥å®ç°å°‘æ•°æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>ABFTæ–¹æ³•é€šè¿‡å…³æ³¨æ³¨æ„åŠ›åˆ†æ•°æ¥æ„å»ºè®­ç»ƒç›®æ ‡ï¼Œå¼ºåŒ–æ¨¡å‹å¯¹æ­£ç¡®æ ‡ç­¾çš„å…³æ³¨å¹¶å‰Šå¼±é”™è¯¯æ ‡ç­¾çš„å½±å“ã€‚</li>
<li>ABFTåœ¨æ€§èƒ½ã€ç¨³å¥æ€§ã€å…¬æ­£æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>ABFTæ•°æ®æˆæœ¬æä½ï¼Œçº¦ä¸ºä¼ ç»Ÿæ–¹æ³•çš„åƒåˆ†ä¹‹ä¸€ã€‚</li>
<li>ç«¯åˆ°ç«¯çš„è®­ç»ƒç›®æ ‡åŒ…å«ABFTç›®æ ‡ï¼Œæ˜¾ç¤ºå‡ºè¯­å¢ƒå­¦ä¹ æ•°æ®çš„éšæ€§åå‘ã€‚</li>
<li>æœ¬æ–‡æ­ç¤ºäº†æ§åˆ¶è¯­è¨€æ¨¡å‹å†…éƒ¨ç‰¹å®šæ¨¡å—åºåˆ—çš„å¯èƒ½æ€§ï¼Œä»¥æé«˜å…¶è¡Œä¸ºè¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dd0729679a1f6063c63564f15f0b9109.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6393779448043989c3454adc3393bd47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9eea7a49e56f072f3639588ae05415a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8dc36bbb22846b92c4e52b45edab364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a693ed1f9c24c0cb9c3e9eee75be3c3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unlocking-the-Power-of-SAM-2-for-Few-Shot-Segmentation"><a href="#Unlocking-the-Power-of-SAM-2-for-Few-Shot-Segmentation" class="headerlink" title="Unlocking the Power of SAM 2 for Few-Shot Segmentation"></a>Unlocking the Power of SAM 2 for Few-Shot Segmentation</h2><p><strong>Authors:Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao</strong></p>
<p>Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few classes to segment arbitrary classes, but at the risk of overfitting. To address this, some methods use the well-learned knowledge of foundation models (e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM by supporting video segmentation, whose class-agnostic matching ability is useful to FSS. A simple idea is to encode support foreground (FG) features as memory, with which query FG features are matched and fused. Unfortunately, the FG objects in different frames of SAM 2â€™s video data are always the same identity, while those in FSS are different identities, i.e., the matching step is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo query memory, matching with query features in a compatible way. However, the memories can never be as accurate as the real ones, i.e., they are likely to contain incomplete query FG, and some unexpected query background (BG) features, leading to wrong segmentation. Hence, we further design Iterative Memory Refinement to fuse more query FG features into the memory, and devise a Support-Calibrated Memory Attention to suppress the unexpected query BG features in memory. Extensive experiments have been conducted on PASCAL-5$^i$ and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot mIoU can be 4.2% better than the best baseline. </p>
<blockquote>
<p>å°‘æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰æ—¨åœ¨å­¦ä¹ å¯¹å°‘æ•°ç±»åˆ«çš„ç±»æ— å…³åˆ†å‰²ï¼Œä»¥å®ç°å¯¹ä»»æ„ç±»åˆ«çš„åˆ†å‰²ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä¸€äº›æ–¹æ³•ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„å·²å­¦çŸ¥è¯†ï¼ˆä¾‹å¦‚SAMï¼‰æ¥ç®€åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚æœ€è¿‘ï¼ŒSAM 2é€šè¿‡æ”¯æŒè§†é¢‘åˆ†å‰²æ‰©å±•äº†SAMçš„åº”ç”¨ï¼Œå…¶ç±»æ— å…³åŒ¹é…èƒ½åŠ›å¯¹FSSå¾ˆæœ‰ç”¨ã€‚ä¸€ä¸ªç®€å•çš„æƒ³æ³•æ˜¯å°†æ”¯æŒå‰æ™¯ï¼ˆFGï¼‰ç‰¹å¾ç¼–ç ä¸ºå†…å­˜ï¼Œé€šè¿‡å…¶ä¸æŸ¥è¯¢å‰æ™¯ç‰¹å¾è¿›è¡ŒåŒ¹é…å’Œèåˆã€‚ç„¶è€Œï¼Œä¸å¹¸çš„æ˜¯ï¼ŒSAM 2çš„è§†é¢‘æ•°æ®ä¸åŒå¸§ä¸­çš„å‰æ™¯å¯¹è±¡å§‹ç»ˆæ˜¯åŒä¸€èº«ä»½ï¼Œè€ŒFSSä¸­çš„åˆ™æ˜¯ä¸åŒèº«ä»½ï¼Œå³åŒ¹é…æ­¥éª¤æ˜¯ä¸å…¼å®¹çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¼ªæç¤ºç”Ÿæˆå™¨æ¥ç¼–ç ä¼ªæŸ¥è¯¢å†…å­˜ï¼Œä»¥ä¸æŸ¥è¯¢ç‰¹å¾è¿›è¡Œå…¼å®¹åŒ¹é…ã€‚ç„¶è€Œï¼Œè¿™äº›è®°å¿†æ— è®ºå¦‚ä½•éƒ½æ— æ³•è¾¾åˆ°çœŸå®è®°å¿†çš„æ°´å¹³ï¼Œå³å®ƒä»¬å¯èƒ½åŒ…å«ä¸å®Œæ•´æŸ¥è¯¢å‰æ™¯å’Œä¸€äº›æ„å¤–çš„æŸ¥è¯¢èƒŒæ™¯ï¼ˆBGï¼‰ç‰¹å¾ï¼Œä»è€Œå¯¼è‡´é”™è¯¯çš„åˆ†å‰²ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†è¿­ä»£å†…å­˜ä¼˜åŒ–ï¼Œå°†æ›´å¤šæŸ¥è¯¢å‰æ™¯ç‰¹å¾èåˆåˆ°å†…å­˜ä¸­ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ”¯æŒæ ¡å‡†çš„å†…å­˜æ³¨æ„åŠ›æ¥æŠ‘åˆ¶å†…å­˜ä¸­æ„å¤–çš„æŸ¥è¯¢èƒŒæ™¯ç‰¹å¾ã€‚åœ¨PASCAL-5$^i$å’ŒCOCO-20$^i$ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡æ•ˆæœï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨1-shot mIoUä¸Šçš„è¡¨ç°æ¯”æœ€ä½³åŸºçº¿æé«˜äº†4.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14100v1">PDF</a> This paper is accepted by ICMLâ€™25</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Few-Shot Segmentationï¼ˆFSSï¼‰çš„ç›®æ ‡æ˜¯åˆ©ç”¨æœ‰é™çš„ç±»åˆ«æ•°æ®è¿›è¡Œç±»æ— å…³çš„åˆ†å‰²å­¦ä¹ ã€‚ä¸ºäº†è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä¸€äº›æ–¹æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ç®€åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚æœ€è¿‘ï¼ŒSAM 2æ‰©å±•äº†SAMï¼Œæ”¯æŒè§†é¢‘åˆ†å‰²ï¼Œå…¶ç±»æ— å…³çš„åŒ¹é…èƒ½åŠ›å¯¹FSSå¾ˆæœ‰ç”¨ã€‚æ–‡æœ¬ä¸­æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå³ç¼–ç æ”¯æŒå‰æ™¯ç‰¹å¾ä½œä¸ºå†…å­˜ï¼Œä¸æŸ¥è¯¢å‰æ™¯ç‰¹å¾è¿›è¡ŒåŒ¹é…å’Œèåˆã€‚ç„¶è€Œï¼ŒSAM 2çš„è§†é¢‘æ•°æ®ä¸­çš„å‰æ™¯å¯¹è±¡èº«ä»½å§‹ç»ˆç›¸åŒï¼Œè€ŒFSSä¸­çš„åˆ™ä¸åŒï¼Œå¯¼è‡´åŒ¹é…æ­¥éª¤ä¸å…¼å®¹ã€‚å› æ­¤ï¼Œè®¾è®¡äº†ä¼ªæç¤ºç”Ÿæˆå™¨æ¥ç¼–ç ä¼ªæŸ¥è¯¢å†…å­˜ï¼Œä»¥å…¼å®¹æ–¹å¼åŒ¹é…æŸ¥è¯¢ç‰¹å¾ã€‚ä½†è®°å¿†æ°¸è¿œæ— æ³•åƒçœŸå®è®°å¿†é‚£æ ·å‡†ç¡®ï¼Œå¯èƒ½åŒ…å«ä¸å®Œæ•´æˆ–æ„å¤–çš„æŸ¥è¯¢å‰æ™¯å’ŒèƒŒæ™¯ç‰¹å¾ï¼Œå¯¼è‡´é”™è¯¯åˆ†å‰²ã€‚å› æ­¤ï¼Œè¿›ä¸€æ­¥è®¾è®¡äº†è¿­ä»£å†…å­˜ç»†åŒ–ï¼Œå°†æ›´å¤šæŸ¥è¯¢å‰æ™¯ç‰¹å¾èåˆåˆ°å†…å­˜ä¸­ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ”¯æŒæ ¡å‡†çš„å†…å­˜æ³¨æ„åŠ›æ¥æŠ‘åˆ¶å†…å­˜ä¸­çš„æ„å¤–æŸ¥è¯¢èƒŒæ™¯ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Segmentation (FSS) æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„ç±»åˆ«æ•°æ®è¿›è¡Œç±»æ— å…³çš„åˆ†å‰²å­¦ä¹ ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚</li>
<li>SAM 2æ‰©å±•äº†SAMæ–¹æ³•ä»¥æ”¯æŒè§†é¢‘åˆ†å‰²ï¼Œåˆ©ç”¨ç±»æ— å…³çš„åŒ¹é…èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œé€šè¿‡ç¼–ç æ”¯æŒå‰æ™¯ç‰¹å¾ä½œä¸ºå†…å­˜è¿›è¡ŒåŒ¹é…å’Œèåˆã€‚</li>
<li>SAM 2ä¸FSSåœ¨å‰æ™¯å¯¹è±¡èº«ä»½ä¸Šå­˜åœ¨ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œè®¾è®¡äº†ä¼ªæç¤ºç”Ÿæˆå™¨å’Œè¿­ä»£å†…å­˜ç»†åŒ–æ–¹æ³•ã€‚</li>
<li>ä¼ªæç¤ºç”Ÿæˆå™¨é€šè¿‡ç¼–ç ä¼ªæŸ¥è¯¢å†…å­˜è¿›è¡ŒåŒ¹é…ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯äº†è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚1-shot mIoUæ€§èƒ½å¯æé«˜4.2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-896d4d256e255a0fa37d84b40ebad68d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55b5349ebeac0a93d3e773c4a1498977.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20cc4d224ada50805ed8aec4e78bd194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad2cd2d49b136b45c8885f9fb3e6224f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7676cfabb099634e3a7f26118c0fec09.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation"><a href="#CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation" class="headerlink" title="CLEVER: A Curated Benchmark for Formally Verified Code Generation"></a>CLEVER: A Curated Benchmark for Formally Verified Code Generation</h2><p><strong>Authors:Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzche, Greg Durrett, Yisong Yue, Swarat Chaudhuri</strong></p>
<p>We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Leanâ€™s type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever">https://github.com/trishullab/clever</a>) as well as HuggingFace(<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/amitayusht/clever">https://huggingface.co/datasets/amitayusht/clever</a>). All our evaluation code is also available online(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever-prover">https://github.com/trishullab/clever-prover</a>). </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†${\rm C{\small LEVER}}$ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„ã€ç»è¿‡ç­›é€‰çš„åŒ…å«161ä¸ªé—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºç«¯åˆ°ç«¯çš„Leanä»£ç ç”ŸæˆéªŒè¯ã€‚æ¯ä¸ªé—®é¢˜ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šï¼ˆ1ï¼‰ç”Ÿæˆä¸ä¿ç•™çš„çœŸå®è§„æ ¼ç›¸åŒ¹é…çš„è§„æ ¼çš„ä»»åŠ¡ï¼›ï¼ˆ2ï¼‰ç”Ÿæˆèƒ½å¤Ÿè¯æ˜æ»¡è¶³æ­¤è§„æ ¼è¦æ±‚çš„Leanå®ç°çš„ä»»åŠ¡ã€‚ä¸åŒäºä»¥å‰çš„åŸºå‡†æµ‹è¯•ï¼Œ${\rm C{\small LEVER}}$é¿å…äº†æµ‹è¯•ç”¨ä¾‹çš„ç›‘ç£ã€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ³¨é‡Šä»¥åŠæ³„éœ²å®ç°é€»è¾‘æˆ–å…è®¸æ— æ•ˆè§£å†³æ–¹æ¡ˆçš„è§„æ ¼ã€‚æ‰€æœ‰è¾“å‡ºéƒ½åˆ©ç”¨Leançš„ç±»å‹æ£€æŸ¥å™¨è¿›è¡Œäº‹åéªŒè¯ï¼Œä»¥ç¡®ä¿æœºå™¨å¯æ£€æŸ¥çš„æ­£ç¡®æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨${\rm C{\small LEVER}}$æ¥è¯„ä¼°åŸºäºæœ€æ–°è¯­è¨€æ¨¡å‹çš„å‡ ç§å°‘æ ·æœ¬å’Œæ™ºèƒ½æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•éƒ½å¾ˆéš¾å®ç°å®Œå…¨éªŒè¯ï¼Œä»è€Œä½¿å…¶æˆä¸ºç¨‹åºåˆæˆå’Œå½¢å¼æ¨ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„å‰æ²¿åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¯ä»¥åœ¨GitHubï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever%EF%BC%89%E4%BB%A5%E5%8F%8AHuggingFace%EF%BC%88https://huggingface.co/datasets/amitayusht/clever%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%84%E4%BC%B0%E4%BB%A3%E7%A0%81%E4%B9%9F%E5%9C%A8%E7%BA%BF%E5%8F%AF%E7%94%A8%EF%BC%88https://github.com/trishullab/clever-prover%EF%BC%89%E3%80%82">https://github.com/trishullab/cleverï¼‰ä»¥åŠHuggingFaceï¼ˆhttps://huggingface.co/datasets/amitayusht/cleverï¼‰ä¸Šæ‰¾åˆ°ã€‚æˆ‘ä»¬çš„è¯„ä¼°ä»£ç ä¹Ÿåœ¨çº¿å¯ç”¨ï¼ˆhttps://github.com/trishullab/clever-proverï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13938v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¸­å›½ç ”ç©¶å›¢é˜Ÿæ¨å‡ºçš„é«˜è´¨é‡åŸºå‡†æµ‹è¯•é›†CLEVERä¸­ï¼Œå…±æœ‰åŒ…å«ç«¯åˆ°ç«¯éªŒè¯çš„ä»£ç ç”Ÿæˆä»»åŠ¡å…±è®¡æœ‰æ¶µç›–çš„161ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚æ­¤åŸºå‡†æµ‹è¯•é›†å¼ºè°ƒé¿å…æµ‹è¯•æ¡ˆä¾‹ç›‘ç£ã€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ³¨é‡Šä»¥åŠæ³„éœ²å®ç°é€»è¾‘æˆ–å…è®¸ç©ºæ´è§£å†³æ–¹æ¡ˆçš„é—®é¢˜ã€‚æ‰€æœ‰è¾“å‡ºå‡é€šè¿‡Leançš„ç±»å‹æ£€æŸ¥å™¨è¿›è¡Œäº‹åéªŒè¯ï¼Œä»¥ç¡®ä¿æœºå™¨æ£€æŸ¥æ­£ç¡®æ€§ã€‚é’ˆå¯¹åŸºäºå½“å‰æŠ€æœ¯æ°´å¹³çš„è¯­è¨€æ¨¡å‹çš„å°‘é‡ç ”ç©¶å’Œäººå·¥æ™ºèƒ½çš„æ–¹æ³•è¿›è¡Œè¯„ä»·ã€‚è¿™ä¸ªæŒ‘æˆ˜æå…·æ ‡æ†æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šæœ‰è®¸å¤šéªŒè¯äº†äººç±»ç¼ºä¹ä¸€ç§å¯ç”¨çš„ç¼–ç ç¯å¢ƒä¾›äº¤æµçš„æŠ€æœ¯è¯„ä»·å¹³å°çš„ä¿¡æ¯ç³»ç»Ÿå·²è¯ç”Ÿï¼Œä½†åœ¨ä¸šç•Œè®¸å¤šå·²å…¬å¼€çš„å®è·µå®ä¾‹é¢å‰æˆ‘ä»¬é¢ä¸´æå¤§æŒ‘æˆ˜ï¼Œè¿™é¡¹æˆæœèƒ½å¤Ÿåœ¨GitHubã€HuggingFaceç­‰å¹³å°ä¸Šæ‰¾åˆ°ã€‚åŒæ—¶ï¼Œæ‰€æœ‰çš„è¯„ä¼°ä»£ç ä¹Ÿå·²åœ¨çº¿å‘å¸ƒã€‚è¿™ä¸ºäººå·¥æ™ºèƒ½åœ¨ç¨‹åºåˆæˆå’Œå½¢å¼æ¨ç†é¢†åŸŸçš„å‘å±•æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>${\rm C{\small LEVER}}$ æ˜¯ä¸€ä¸ªç”¨äºç«¯åˆ°ç«¯éªŒè¯çš„ä»£ç ç”Ÿæˆçš„é«˜è´¨é‡çš„åŸºå‡†æµ‹è¯•é›†ã€‚å…·æœ‰ç¡®ä¿æœºå™¨æ£€æŸ¥æ­£ç¡®æ€§çš„ç‰¹æ€§ã€‚å®ƒåŒ…å«æ¶µç›–çš„æ¶µç›–å¤šä¸ªé¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>${\rm C{\small LEVER}}$å¼ºè°ƒçœŸå®æ€§å’Œéš¾åº¦çº§åˆ«é«˜çš„é—®é¢˜è®¾ç½®ï¼Œæ—¨åœ¨é¿å…ç®€å•ä»»åŠ¡å’Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¾èµ–æ€§é—®é¢˜è¿›è¡Œç›‘ç£å­¦ä¹ çš„æ–¹å¼è®¾è®¡ä»»åŠ¡å†…å®¹çš„é—®é¢˜ç±»å‹é—®é¢˜è¦æ±‚ä¸åŒæŠ€æœ¯ç‰¹å¾çš„è§£é¢˜æ–¹æ³•æ•°é‡ä¹‹é—´æœ‰è‰¯å¥½çš„å¹³è¡¡ç­‰é—®é¢˜ä»¥è€ƒå¯Ÿé€šç”¨æ€§èƒ½ç®—æ³•æ–¹é¢ï¼šè§„é¿æ¼æ´çš„é—®é¢˜ï¼›è¿›è¡Œè¾“å…¥æ—¶å­¦ä¹ æ³¨é‡Šçš„ç¨‹åºè¯æ˜æŠ€æœ¯ä»¥æ”¹è¿›è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºï¼›è§£å†³å¤æ‚æ€§å’Œéšæœºæ€§é—®é¢˜ï¼›ç¡®ä¿ç®—æ³•è´¨é‡æ°´å¹³åœ¨é€šè¿‡ç¼–ç¨‹å®è·µå®ä¾‹æ–¹é¢èƒ½å¤Ÿæ¯”è‚©äººå·¥æ™ºèƒ½æ°´å¹³çš„è¶…è¶Šæ ‡å‡†çš„æ–¹æ³•ä¸­ååˆ—å‰èŒ…ç­‰éš¾ç‚¹ï¼Œå±•ç°äº†å®ƒçš„ç‹¬ç‰¹æ€§ã€‚è¿™æ˜¯ä¸­å›½ç ”ç©¶å›¢é˜Ÿçš„åˆ›æ–°æ€§æˆæœã€‚éšç€ç§‘æŠ€çš„ä¸æ–­å‘å±•ï¼Œäººä»¬å¯¹äºç¼–ç¨‹è¯­è¨€å’Œè®¡ç®—æœºçš„ç†è§£ä¸æ–­åŠ æ·±ï¼Œå…¶å‰æ™¯å°†ä¼šè¶Šæ¥è¶Šå¹¿é˜”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-175e9300620f6ffeeb08c92d2d5a6af7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf4bb3beeb9a906f276484cb351bd82c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-059b0011d1e2a8174241dab92289ec5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e1a8461994df8449d3c7925819fe90e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92dbad38362c5c981f79f4e240389835.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Physics-Driven-Local-Whole-Elastic-Deformation-Modeling-for-Point-Cloud-Representation-Learning"><a href="#Physics-Driven-Local-Whole-Elastic-Deformation-Modeling-for-Point-Cloud-Representation-Learning" class="headerlink" title="Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud   Representation Learning"></a>Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud   Representation Learning</h2><p><strong>Authors:Zhongyu Chen, Rong Zhao, Xie Han, Xindong Guo, Song Wang, Zherui Qiao</strong></p>
<p>Existing point cloud representation learning tend to learning the geometric distribution of objects through data-driven approaches, emphasizing structural features while overlooking the relationship between the local information and the whole structure. Local features reflect the fine-grained variations of an object, while the whole structure is determined by the interaction and combination of these local features, collectively defining the objectâ€™s shape. In real-world, objects undergo elastic deformation under external forces, and this deformation gradually affects the whole structure through the propagation of forces from local regions, thereby altering the objectâ€™s geometric properties. Inspired by this, we propose a physics-driven self-supervised learning method for point cloud representation, which captures the relationship between parts and the whole by constructing a local-whole force propagation mechanism. Specifically, we employ a dual-task encoder-decoder framework, integrating the geometric modeling capability of implicit fields with physics-driven elastic deformation. The encoder extracts features from the point cloud and its tetrahedral mesh representation, capturing both geometric and physical properties. These features are then fed into two decoders: one learns the whole geometric shape of the point cloud through an implicit field, while the other predicts local deformations using two specifically designed physics information loss functions, modeling the deformation relationship between local and whole shapes. Experimental results show that our method outperforms existing approaches in object classification, few-shot learning, and segmentation, demonstrating its effectiveness. </p>
<blockquote>
<p>ç°æœ‰çš„ç‚¹äº‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•å¾€å¾€é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹æ³•å­¦ä¹ å¯¹è±¡çš„å‡ ä½•åˆ†å¸ƒï¼Œå¼ºè°ƒç»“æ„ç‰¹å¾ï¼Œä½†å¿½è§†äº†å±€éƒ¨ä¿¡æ¯ä¸æ•´ä½“ç»“æ„ä¹‹é—´çš„å…³ç³»ã€‚å±€éƒ¨ç‰¹å¾åæ˜ äº†å¯¹è±¡çš„ç»†å¾®å˜åŒ–ï¼Œè€Œæ•´ä½“ç»“æ„åˆ™ç”±è¿™äº›å±€éƒ¨ç‰¹å¾çš„ç›¸äº’ä½œç”¨å’Œç»„åˆå†³å®šï¼Œå…±åŒå®šä¹‰äº†å¯¹è±¡çš„å½¢çŠ¶ã€‚åœ¨ç°å®ä¸­ï¼Œç‰©ä½“åœ¨å¤–éƒ¨åŠ›çš„ä½œç”¨ä¸‹ä¼šå‘ç”Ÿå¼¹æ€§å˜å½¢ï¼Œè¿™ç§å˜å½¢é€šè¿‡ä»å±€éƒ¨åŒºåŸŸä¼ æ’­çš„åŠ›é‡é€æ¸å½±å“æ•´ä½“ç»“æ„ï¼Œä»è€Œæ”¹å˜ç‰©ä½“çš„å‡ ä½•å±æ€§ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç‚¹äº‘è¡¨ç¤ºçš„ç‰©ç†é©±åŠ¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å»ºç«‹å±€éƒ¨-æ•´ä½“åŠ›ä¼ æ’­æœºåˆ¶æ¥æ•æ‰éƒ¨åˆ†ä¸æ•´ä½“ä¹‹é—´çš„å…³ç³»ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é‡‡ç”¨åŒä»»åŠ¡ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œå°†éšåœºçš„å‡ ä½•å»ºæ¨¡èƒ½åŠ›ä¸ç‰©ç†é©±åŠ¨çš„å¼¹æ€§å˜å½¢ç›¸ç»“åˆã€‚ç¼–ç å™¨ä»ç‚¹äº‘åŠå…¶å››é¢ä½“ç½‘æ ¼è¡¨ç¤ºä¸­æå–ç‰¹å¾ï¼Œæ•æ‰å‡ ä½•å’Œç‰©ç†å±æ€§ã€‚è¿™äº›ç‰¹å¾ç„¶åè¾“å…¥åˆ°ä¸¤ä¸ªè§£ç å™¨ä¸­ï¼šä¸€ä¸ªé€šè¿‡éšåœºå­¦ä¹ ç‚¹äº‘çš„æ•´ä½“å‡ ä½•å½¢çŠ¶ï¼Œå¦ä¸€ä¸ªä½¿ç”¨ä¸¤ä¸ªä¸“é—¨è®¾è®¡çš„ç‰©ç†ä¿¡æ¯æŸå¤±å‡½æ•°æ¥é¢„æµ‹å±€éƒ¨å˜å½¢ï¼Œå»ºæ¨¡å±€éƒ¨å’Œæ•´ä½“å½¢çŠ¶ä¹‹é—´çš„å˜å½¢å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç›®æ ‡åˆ†ç±»ã€å°æ ·æœ¬å­¦ä¹ å’Œåˆ†å‰²æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13812v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç‰©ç†é©±åŠ¨çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºç‚¹äº‘è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºå±€éƒ¨ä¸æ•´ä½“çš„åŠ›ä¼ æ’­æœºåˆ¶ï¼Œæ•æ‰éƒ¨åˆ†ä¸æ•´ä½“ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¼ºè°ƒåœ¨å¤–éƒ¨åŠ›ä½œç”¨ä¸‹ç‰©ä½“çš„å¼¹æ€§å˜å½¢å¦‚ä½•å½±å“æ•´ä½“ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‰©ä½“åˆ†ç±»ã€å°‘æ ·æœ¬å­¦ä¹ å’Œåˆ†å‰²æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰ç‚¹äº‘è¡¨ç¤ºå­¦ä¹ æ–¹æ³•å¤šé€šè¿‡æ•°æ®é©±åŠ¨æ–¹æ³•å­¦ä¹ ç‰©ä½“çš„å‡ ä½•åˆ†å¸ƒï¼Œä½†å¿½ç•¥äº†å±€éƒ¨ä¿¡æ¯ä¸æ•´ä½“ç»“æ„ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>å±€éƒ¨ç‰¹å¾åæ˜ ç‰©ä½“çš„ç»†å¾®å˜åŒ–ï¼Œè€Œæ•´ä½“ç»“æ„ç”±è¿™äº›å±€éƒ¨ç‰¹å¾çš„ç›¸äº’ä½œç”¨å’Œç»„åˆå†³å®šï¼Œå…±åŒå®šä¹‰ç‰©ä½“çš„å½¢çŠ¶ã€‚</li>
<li>ç‰©ä½“åœ¨å¤–éƒ¨åŠ›ä½œç”¨ä¸‹ä¼šå‘ç”Ÿå¼¹æ€§å˜å½¢ï¼Œè¿™ç§å˜å½¢é€šè¿‡ä»å±€éƒ¨åŒºåŸŸä¼ æ’­çš„åŠ›é€æ¸å½±å“æ•´ä½“ç»“æ„ï¼Œæ”¹å˜ç‰©ä½“çš„å‡ ä½•ç‰¹æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç‰©ç†é©±åŠ¨çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå±€éƒ¨ä¸æ•´ä½“çš„åŠ›ä¼ æ’­æœºåˆ¶ï¼Œæ•æ‰ç‚¹äº‘è¡¨ç¤ºä¸­çš„éƒ¨åˆ†ä¸æ•´ä½“å…³ç³»ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ç‚¹äº‘å’Œå…¶å››é¢ä½“ç½‘æ ¼è¡¨ç¤ºçš„å‡ ä½•å»ºæ¨¡èƒ½åŠ›ä¸ç‰©ç†é©±åŠ¨çš„å¼¹æ€§å˜å½¢ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‰©ä½“åˆ†ç±»ã€å°‘æ ·æœ¬å­¦ä¹ å’Œåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨åŒä»»åŠ¡ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œå…¶ä¸­ç¼–ç å™¨æå–ç‚¹äº‘åŠå…¶å››é¢ä½“ç½‘æ ¼è¡¨ç¤ºçš„ç‰¹å¾ï¼Œè§£ç å™¨åˆ™ç”¨äºå­¦ä¹ æ•´ä¸ªå‡ ä½•å½¢çŠ¶å¹¶é¢„æµ‹å±€éƒ¨å˜å½¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7964b2debc9adf20fd660a685fa7ceda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a28aa57c3402d56725cf9dc572a65d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b28b03e73de9436ed2a21996a1a40eb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc02514e3adb621bb9d774889cfa3043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bafb08a84402263e93dd939fc410eae.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MAFA-A-multi-agent-framework-for-annotation"><a href="#MAFA-A-multi-agent-framework-for-annotation" class="headerlink" title="MAFA: A multi-agent framework for annotation"></a>MAFA: A multi-agent framework for annotation</h2><p><strong>Authors:Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem</strong></p>
<p>Modern applications require accurate and efficient retrieval of information in response to user queries. Mapping user utterances to the most relevant Frequently Asked Questions (FAQs) is a crucial component of these systems. Traditional approaches often rely on a single model or technique, which may not capture the nuances of diverse user inquiries. In this paper, we introduce a multi-agent framework for FAQ annotation that combines multiple specialized agents with different approaches and a judge agent that reranks candidates to produce optimal results. Our agents utilize a structured reasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides them through systematic reasoning steps using targeted, task-specific JSON queries. Our framework features a specialized few-shot example strategy, where each agent receives different few-shots, enhancing ensemble diversity and coverage of the query space. We evaluate our framework on a real-world banking dataset as well as public benchmark datasets (LCQMC and FiQA), demonstrating significant improvements over single-agent approaches across multiple metrics, including a 14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12% improvement in Mean Reciprocal Rank on our dataset, and similar gains on public benchmarks when compared with traditional single agent annotation techniques. Our framework is particularly effective at handling ambiguous queries, making it well-suited for deployment in production applications while showing strong generalization capabilities across different domains and languages. </p>
<blockquote>
<p>ç°ä»£åº”ç”¨ç¨‹åºéœ€è¦å‡†ç¡®é«˜æ•ˆåœ°æ£€ç´¢ä¿¡æ¯ä»¥å“åº”ç”¨æˆ·æŸ¥è¯¢ã€‚å°†ç”¨æˆ·çš„è¯è¯­æ˜ å°„åˆ°æœ€ç›¸å…³çš„å¸¸è§é—®é¢˜è§£ç­”ï¼ˆFAQsï¼‰æ˜¯è¿™äº›ç³»ç»Ÿçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸ªæ¨¡å‹æˆ–æŠ€æœ¯ï¼Œè¿™å¯èƒ½æ— æ³•æ•æ‰åˆ°å„ç§ç”¨æˆ·æŸ¥è¯¢çš„ç»†å¾®å·®åˆ«ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç»“åˆå¤šä¸ªä¸“ä¸šä»£ç†å’Œæ³•å®˜ä»£ç†è¿›è¡Œé‡æ–°æ’åçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºFAQæ³¨é‡Šï¼Œä»¥äº§ç”Ÿæœ€ä½³ç»“æœã€‚æˆ‘ä»¬çš„ä»£ç†é‡‡ç”¨å—å…³æ³¨æ¨ç†æŸ¥è¯¢ï¼ˆARQï¼‰å¯å‘çš„ç»“æ„åŒ–æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨æœ‰é’ˆå¯¹æ€§çš„ä»»åŠ¡ç‰¹å®šJSONæŸ¥è¯¢æ¥æŒ‡å¯¼ä»–ä»¬è¿›è¡Œç³»ç»ŸåŒ–çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨ä¸“é—¨çš„å°‘é‡ç¤ºä¾‹ç­–ç•¥ï¼Œæ¯ä¸ªä»£ç†æ¥æ”¶ä¸åŒçš„å°‘é‡ç¤ºä¾‹ï¼Œå¢å¼ºäº†ç»„åˆå¤šæ ·æ€§å’ŒæŸ¥è¯¢ç©ºé—´çš„è¦†ç›–ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„é“¶è¡Œæ•°æ®é›†ä»¥åŠå…¬å…±åŸºå‡†æ•°æ®é›†ï¼ˆLCQMCå’ŒFiQAï¼‰ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œä¸å•æ™ºèƒ½ä½“æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬ç¬¬ä¸€åå‡†ç¡®ç‡æé«˜14%ï¼Œå‰äº”åå‡†ç¡®ç‡æé«˜18%ï¼Œä»¥åŠåœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šçš„å¹³å‡å€’æ•°æ’åæé«˜12%ï¼Œä¸ä¼ ç»Ÿå•æ™ºèƒ½ä½“æ³¨é‡ŠæŠ€æœ¯ç›¸æ¯”ï¼Œåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿæœ‰ç±»ä¼¼çš„æ”¶ç›Šã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢æ–¹é¢ç‰¹åˆ«æœ‰æ•ˆï¼Œå› æ­¤éå¸¸é€‚åˆåœ¨ç”Ÿäº§åº”ç”¨ç¨‹åºä¸­éƒ¨ç½²ï¼Œå¹¶åœ¨ä¸åŒé¢†åŸŸå’Œè¯­è¨€æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13668v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ç”¨äºFAQæ ‡æ³¨ï¼Œç»“åˆäº†å¤šä¸ªé‡‡ç”¨ä¸åŒæ–¹æ³•çš„æ™ºèƒ½ä½“å’Œä¸€ä¸ªæ³•å®˜æ™ºèƒ½ä½“è¿›è¡Œå€™é€‰é‡æ–°æ’åºä»¥äº§ç”Ÿæœ€ä½³ç»“æœã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„JSONæŸ¥è¯¢æŒ‡å¯¼æ™ºèƒ½ä½“è¿›è¡Œç³»ç»Ÿæ€§æ¨ç†æ­¥éª¤ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·æœ‰ä¸“é—¨ç”¨äºå¿«é€Ÿç¤ºä¾‹çš„ç­–ç•¥ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“æ¥æ”¶ä¸åŒçš„å¿«é€Ÿç¤ºä¾‹ï¼Œå¢å¼ºäº†ç»„åˆå¤šæ ·æ€§å’ŒæŸ¥è¯¢ç©ºé—´è¦†ç›–ç‡ã€‚è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨çœŸå®é“¶è¡Œæ•°æ®é›†å’Œå…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå•æ™ºèƒ½ä½“æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¯¥æ¡†æ¶ç‰¹åˆ«æ“…é•¿å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢ï¼Œé€‚åˆåœ¨ç”Ÿäº§åº”ç”¨ä¸­éƒ¨ç½²ï¼Œå¹¶åœ¨ä¸åŒé¢†åŸŸå’Œè¯­è¨€ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ç”¨äºFAQæ ‡æ³¨ï¼Œæ—¨åœ¨æ”¹è¿›ä¼ ç»Ÿçš„å•ä¸€æ¨¡å‹æˆ–æŠ€æœ¯æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“æ¡†æ¶ç»“åˆäº†å¤šç§ä¸åŒçš„æ–¹æ³•å’ŒæŠ€æœ¯ï¼Œå¹¶åŒ…æ‹¬ä¸€ä¸ªæ³•å®˜æ™ºèƒ½ä½“è¿›è¡Œå€™é€‰é‡æ–°æ’åºã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ç»“æ„åŒ–æ¨ç†æ–¹æ³•ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šçš„JSONæŸ¥è¯¢æŒ‡å¯¼æ™ºèƒ½ä½“çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æ¡†æ¶å…·æœ‰ä¸“é—¨ç”¨äºå¿«é€Ÿç¤ºä¾‹çš„ç­–ç•¥ï¼Œå¢å¼ºäº†æ™ºèƒ½ä½“çš„ç»„åˆå¤šæ ·æ€§å’ŒæŸ¥è¯¢ç©ºé—´è¦†ç›–ç‡ã€‚</li>
<li>åœ¨çœŸå®é“¶è¡Œæ•°æ®é›†å’Œå…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¼˜äºå•æ™ºèƒ½ä½“æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥æ¡†æ¶ç‰¹åˆ«æ“…é•¿å¤„ç†æ¨¡ç³ŠæŸ¥è¯¢ï¼Œé€‚åº”ç”Ÿäº§ç¯å¢ƒä¸­çš„å¤æ‚éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1afc039d07417c6177bfe826a6a736cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a23d978281ffa9d1cabf222b20dad01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-293988a3782880b15ccbc18e13fc0357.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Spectral-Spatial-Self-Supervised-Learning-for-Few-Shot-Hyperspectral-Image-Classification"><a href="#Spectral-Spatial-Self-Supervised-Learning-for-Few-Shot-Hyperspectral-Image-Classification" class="headerlink" title="Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral   Image Classification"></a>Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral   Image Classification</h2><p><strong>Authors:Wenchen Chen, Yanmei Zhang, Zhongwei Xiao, Jianping Chu, Xingbo Wang</strong></p>
<p>Few-shot classification of hyperspectral images (HSI) faces the challenge of scarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning (FSL) offer promising avenues to address this issue. However, existing methods often struggle to adapt to the spatial geometric diversity of HSIs and lack sufficient spectral prior knowledge. To tackle these challenges, we propose a method, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification (S4L-FSC), aimed at improving the performance of few-shot HSI classification. Specifically, we first leverage heterogeneous datasets to pretrain a spatial feature extractor using a designed Rotation-Mirror Self-Supervised Learning (RM-SSL) method, combined with FSL. This approach enables the model to learn the spatial geometric diversity of HSIs using rotation and mirroring labels as supervisory signals, while acquiring transferable spatial meta-knowledge through few-shot learning. Subsequently, homogeneous datasets are utilized to pretrain a spectral feature extractor via a combination of FSL and Masked Reconstruction Self-Supervised Learning (MR-SSL). The model learns to reconstruct original spectral information from randomly masked spectral vectors, inferring spectral dependencies. In parallel, FSL guides the model to extract pixel-level discriminative features, thereby embedding rich spectral priors into the model. This spectral-spatial pretraining method, along with the integration of knowledge from heterogeneous and homogeneous sources, significantly enhances model performance. Extensive experiments on four HSI datasets demonstrate the effectiveness and superiority of the proposed S4L-FSC approach for few-shot HSI classification. </p>
<blockquote>
<p>é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰çš„å°‘é‡æ ·æœ¬åˆ†ç±»é¢ä¸´ç€æ ‡æ³¨æ ·æœ¬ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å’Œå°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥é€‚åº”HSIçš„ç©ºé—´å‡ ä½•å¤šæ ·æ€§ï¼Œä¸”ç¼ºä¹è¶³å¤Ÿçš„å…‰è°±å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œåä¸ºâ€œç”¨äºé«˜å…‰è°±å›¾åƒå°‘é‡æ ·æœ¬åˆ†ç±»çš„è°±ç©ºé—´è‡ªç›‘ç£å­¦ä¹ ï¼ˆS4L-FSCï¼‰â€ï¼Œæ—¨åœ¨æé«˜é«˜å…‰è°±å›¾åƒå°‘é‡æ ·æœ¬åˆ†ç±»çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨å¼‚è´¨æ•°æ®é›†é¢„è®­ç»ƒä¸€ä¸ªç©ºé—´ç‰¹å¾æå–å™¨ï¼Œé‡‡ç”¨è®¾è®¡çš„æ—‹è½¬é•œåƒè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆRM-SSLï¼‰ä¸å°‘é‡å­¦ä¹ ç›¸ç»“åˆã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨æ—‹è½¬å’Œé•œåƒæ ‡ç­¾ä½œä¸ºç›‘ç£ä¿¡å·æ¥å­¦ä¹ HSIçš„ç©ºé—´å‡ ä½•å¤šæ ·æ€§ï¼ŒåŒæ—¶é€šè¿‡å°‘é‡å­¦ä¹ è·å¾—å¯è¿ç§»çš„ç©ºé—´å…ƒçŸ¥è¯†ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨åŒè´¨æ•°æ®é›†é¢„è®­ç»ƒä¸€ä¸ªå…‰è°±ç‰¹å¾æå–å™¨ï¼Œç»“åˆå°‘é‡å­¦ä¹ å’Œæ©ç é‡å»ºè‡ªç›‘ç£å­¦ä¹ ï¼ˆMR-SSLï¼‰ã€‚æ¨¡å‹å­¦ä¼šä»éšæœºæ©ç çš„å…‰è°±å‘é‡ä¸­é‡å»ºåŸå§‹å…‰è°±ä¿¡æ¯ï¼Œæ¨æ–­å…‰è°±ä¾èµ–æ€§ã€‚åŒæ—¶ï¼Œå°‘é‡å­¦ä¹ æŒ‡å¯¼æ¨¡å‹æå–åƒç´ çº§åˆ¤åˆ«ç‰¹å¾ï¼Œä»è€Œå°†ä¸°å¯Œçš„å…‰è°±å…ˆéªŒåµŒå…¥æ¨¡å‹ä¸­ã€‚è¿™ç§è°±ç©ºé—´é¢„è®­ç»ƒæ–¹æ³•ï¼Œä»¥åŠæ¥è‡ªå¼‚è´¨å’ŒåŒè´¨æºçš„çŸ¥è¯†çš„æ•´åˆï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨å››ä¸ªHSIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æ‰€æå‡ºçš„S4L-FSCæ–¹æ³•åœ¨å°‘é‡é«˜å…‰è°±å›¾åƒåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12482v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/Wenchen-Chen/S4L-FSC">https://github.com/Wenchen-Chen/S4L-FSC</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰çš„å°‘é‡æ ·æœ¬åˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡ç»“åˆè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å’Œå°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ¥è§£å†³ç©ºé—´å‡ ä½•å¤šæ ·æ€§å’Œå…‰è°±å…ˆéªŒçŸ¥è¯†çš„ä¸è¶³é—®é¢˜ã€‚æ–‡ä¸­ä»‹ç»äº†ä¸€ç§åä¸ºSpectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classificationï¼ˆS4L-FSCï¼‰çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç©ºé—´ç‰¹å¾å’Œå…‰è°±ç‰¹å¾çš„æå–ï¼Œä½¿ç”¨RM-SSLæ–¹æ³•å’Œæ©è”½é‡å»ºè‡ªç›‘ç£å­¦ä¹ æ¥è®­ç»ƒæ¨¡å‹ã€‚ç»è¿‡å¤§é‡å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªHSIæ•°æ®é›†ä¸Šçš„å°‘é‡æ ·æœ¬åˆ†ç±»è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢ä¸´é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰å°‘é‡æ ·æœ¬åˆ†ç±»çš„æŒ‘æˆ˜ã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å’Œå°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ˜¯è§£å†³è¯¥é—®é¢˜æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚</li>
<li>SSLé€šè¿‡è®¾è®¡Rotation-Mirroræ–¹æ³•è¿›è¡Œç©ºé—´ç‰¹å¾æå–ã€‚å€ŸåŠ©å°‘é‡å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç©ºé—´å‡ ä½•å¤šæ ·æ€§å¹¶ä½¿ç”¨æ—‹è½¬å’Œé•œåƒæ ‡ç­¾ä½œä¸ºç›‘ç£ä¿¡å·ã€‚</li>
<li>ä½¿ç”¨åŒè´¨æ•°æ®é›†è¿›è¡Œå…‰è°±ç‰¹å¾æå–ï¼Œé€šè¿‡æ©è”½é‡å»ºè‡ªç›‘ç£å­¦ä¹ å’Œå°‘é‡å­¦ä¹ ç›¸ç»“åˆçš„æ–¹å¼è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œæ¨æ–­å…‰è°±ä¾èµ–å…³ç³»å¹¶æå–åƒç´ çº§åˆ«çš„åˆ¤åˆ«ç‰¹å¾ã€‚</li>
<li>ç»“åˆå¼‚è´¨å’ŒåŒè´¨æ¥æºçš„çŸ¥è¯†ï¼Œé€šè¿‡è°±ç©ºé—´é¢„è®­ç»ƒæ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å››ä¸ªHSIæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b45d0f7be98461bb8e998f0f17592fdc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ee7c2987ba34ce7008baa3016f5b58d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40f9faf91573c222123c7276c67314cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a14ab6e63668a27e64b7ae982767cef1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a635e0b2674fa5f87e1332a56bf7fe7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HyPerAlign-Interpretable-Personalized-LLM-Alignment-via-Hypothesis-Generation"><a href="#HyPerAlign-Interpretable-Personalized-LLM-Alignment-via-Hypothesis-Generation" class="headerlink" title="HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis   Generation"></a>HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis   Generation</h2><p><strong>Authors:Cristina Garbacea, Chenhao Tan</strong></p>
<p>Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the &#96;&#96;average-userâ€™â€™ preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users. We aim to generate customized responses tailored to specific individuals instead of generic outputs that emulate the collective voices of diverse populations. We propose HyPerAlign, an interpretable and sample-efficient hypothesis-driven personalization approach for LLM models. Given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality, and writing style, then prompt LLM models with these hypotheses and user-specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, namely authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks). Results demonstrate the superiority of hypothesis-driven LLM personalization compared to preference-based fine-tuning methods. For authorship attribution, HyPerAlign generations have consistently high win-rates (commonly $&gt; 90%$) against state-of-the-art preference fine-tuning approaches across diverse user profiles and LLM models. For deliberative alignment, the helpfulness of LLM models is improved by up to $70%$ on average. Overall, HyPerAlign represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users. </p>
<blockquote>
<p>å¯¹é½ç®—æ³•å¹¿æ³›åº”ç”¨äºåŸºäºåå¥½æ³¨é‡Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ç”¨æˆ·çš„å¯¹é½ã€‚é€šå¸¸è¿™äº›ï¼ˆç»å¸¸ä¸åŒçš„ï¼‰åå¥½ä¼šåœ¨ä¸åŒçš„ç”¨æˆ·ç¾¤ä½“ä¸­è¿›è¡Œæ±‡æ€»ï¼Œä»è€Œå¾—åˆ°ä¸â€œå¹³å‡ç”¨æˆ·â€åå¥½å¯¹é½çš„å¾®è°ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œå½“å‰æ¨¡å‹æ˜¯åœ¨ç‰¹å®šç”¨æˆ·å’Œæƒ…å¢ƒä¸‹ä½¿ç”¨çš„ï¼Œè¿™å¼ºè°ƒäº†å¯¹ç”¨æˆ·ä¾èµ–çš„åå¥½æ§åˆ¶çš„å¿…è¦æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†å°†LLMè¾“å‡ºä¸ªæ€§åŒ–åˆ°å…¶ç”¨æˆ·çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç”Ÿæˆé’ˆå¯¹ç‰¹å®šä¸ªäººçš„å®šåˆ¶å“åº”ï¼Œè€Œä¸æ˜¯æ¨¡æ‹Ÿä¸åŒäººç¾¤é›†ä½“å£°éŸ³çš„é€šç”¨è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†HyPerAlignï¼Œè¿™æ˜¯ä¸€ç§å¯è§£é‡Šä¸”æ ·æœ¬æ•ˆç‡é«˜çš„å‡è®¾é©±åŠ¨å¼LLMä¸ªæ€§åŒ–æ–¹æ³•ã€‚ç»™å®šç‰¹å®šç”¨æˆ·ç¼–å†™çš„å°‘æ•°æ ·æœ¬ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹å…¶æ²Ÿé€šç­–ç•¥ã€ä¸ªæ€§å’Œå†™ä½œé£æ ¼è¿›è¡Œå‡è®¾æ¨æ–­ï¼Œç„¶åæç¤ºLLMæ¨¡å‹ä½¿ç”¨è¿™äº›å‡è®¾å’Œç”¨æˆ·ç‰¹å®šå±æ€§æ¥ç”Ÿæˆå®šåˆ¶è¾“å‡ºã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„ä¸ªæ€§åŒ–ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå³ä½œè€…å½’å±å’Œå®¡æ…å¯¹é½ï¼Œå¹¶ä½¿ç”¨æ¥è‡ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ï¼ˆæ–°é—»æ–‡ç« ã€åšå®¢æ–‡ç« ã€ç”µå­é‚®ä»¶ã€è¶Šç‹±åŸºå‡†æµ‹è¯•ï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œå‡è®¾é©±åŠ¨çš„LLMä¸ªæ€§åŒ–æ–¹æ³•ä¼˜äºåŸºäºåå¥½çš„å¾®è°ƒæ–¹æ³•ã€‚åœ¨ä½œè€…å½’å±æ–¹é¢ï¼ŒHyPerAlignç”Ÿæˆçš„å“åº”ä¸æœ€æ–°åå¥½å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰å§‹ç»ˆè¾ƒé«˜çš„èƒœç‡ï¼ˆé€šå¸¸è¶…è¿‡90ï¼…ï¼‰ï¼Œæ¶µç›–å„ç§ç”¨æˆ·é…ç½®æ–‡ä»¶å’ŒLLMæ¨¡å‹ã€‚åœ¨å®¡æ…å¯¹é½æ–¹é¢ï¼ŒLLMæ¨¡å‹çš„æœ‰ç”¨æ€§å¹³å‡æé«˜äº†é«˜è¾¾70ï¼…ã€‚æ€»ä½“è€Œè¨€ï¼ŒHyPerAlignä»£è¡¨äº†å¯è§£é‡Šä¸”æ ·æœ¬æ•ˆç‡é«˜çš„ç­–ç•¥ï¼Œç”¨äºå°†LLMæ¨¡å‹ä¸ªæ€§åŒ–åˆ°å•ä¸ªç”¨æˆ·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00038v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHyPerAlignçš„æ–¹æ³•ï¼Œç”¨äºä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºä»¥é€‚åº”ç‰¹å®šç”¨æˆ·ã€‚é€šè¿‡å°‘é‡ç”¨æˆ·å†™ä½œçš„æ ·æœ¬ï¼Œæ¨æ–­ç”¨æˆ·çš„æ²Ÿé€šç­–ç•¥ã€ä¸ªæ€§å’Œå†™ä½œé£æ ¼ï¼Œç„¶åç»“åˆè¿™äº›å‡è®¾å’Œç”¨æˆ·ç‰¹å®šå±æ€§ï¼Œç”Ÿæˆå®šåˆ¶åŒ–çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä½œè€…å½’å±å’Œå®¡æ…å¯¹é½ç­‰ä»»åŠ¡ä¸Šï¼ŒHyPerAlignçš„è¡¨ç°ä¼˜äºåŸºäºåå¥½å¾®è°ƒçš„æ–¹æ³•ã€‚æ€»ä½“è€Œè¨€ï¼ŒHyPerAlignæ˜¯ä¸€ç§å¯è§£é‡Šæ€§å¼ºã€æ ·æœ¬æ•ˆç‡é«˜çš„ä¸ªæ€§åŒ–LLMæ¨¡å‹ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HyPerAlignæ˜¯ä¸€ç§ç”¨äºä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆé’ˆå¯¹ç‰¹å®šç”¨æˆ·çš„å®šåˆ¶å“åº”ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å°‘é‡ç”¨æˆ·å†™ä½œæ ·æœ¬æ¨æ–­ç”¨æˆ·çš„æ²Ÿé€šç­–ç•¥ã€ä¸ªæ€§å’Œå†™ä½œé£æ ¼ã€‚</li>
<li>HyPerAlignç»“åˆè¿™äº›å‡è®¾å’Œç”¨æˆ·ç‰¹å®šå±æ€§ï¼Œç”Ÿæˆé€‚åº”ç‰¹å®šç”¨æˆ·çš„è¾“å‡ºã€‚</li>
<li>åœ¨ä½œè€…å½’å±ä»»åŠ¡ä¸Šï¼ŒHyPerAlignçš„ç”Ÿæˆç»“æœèƒœè¿‡ç°æœ‰åå¥½å¾®è°ƒæ–¹æ³•ï¼Œèƒœç‡é«˜ä¸”ç¨³å®šã€‚</li>
<li>åœ¨å®¡æ…å¯¹é½ä»»åŠ¡ä¸Šï¼ŒLLMæ¨¡å‹çš„å®ç”¨æ€§å¹³å‡æé«˜äº†70%ã€‚</li>
<li>HyPerAlignå…·æœ‰å¯è§£é‡Šæ€§å’Œæ ·æœ¬é«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2cb7e5750ac8d27335ceaf8b8bb36679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24535d2d312f7d503107b8a678d84def.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-972b08e611345537f74e86faf9181e59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72796667f586a441c2fc20ba92815597.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LogicQA-Logical-Anomaly-Detection-with-Vision-Language-Model-Generated-Questions"><a href="#LogicQA-Logical-Anomaly-Detection-with-Vision-Language-Model-Generated-Questions" class="headerlink" title="LogicQA: Logical Anomaly Detection with Vision Language Model Generated   Questions"></a>LogicQA: Logical Anomaly Detection with Vision Language Model Generated   Questions</h2><p><strong>Authors:Yejin Kwon, Daeun Moon, Youngje Oh, Hyunsoo Yoon</strong></p>
<p>Anomaly Detection (AD) focuses on detecting samples that differ from the standard pattern, making it a vital tool in process control. Logical anomalies may appear visually normal yet violate predefined constraints on object presence, arrangement, or quantity, depending on reasoning and explainability. We introduce LogicQA, a framework that enhances AD by providing industrial operators with explanations for logical anomalies. LogicQA compiles automatically generated questions into a checklist and collects responses to identify violations of logical constraints. LogicQA is training-free, annotation-free, and operates in a few-shot setting. We achieve state-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO AD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the explanations of anomalies. Also, our approach has shown outstanding performance on semiconductor SEM corporate data, further validating its effectiveness in industrial applications. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä¸»è¦ä¸“æ³¨äºæ£€æµ‹ä¸æ ‡å‡†æ¨¡å¼ä¸åŒçš„æ ·æœ¬ï¼Œä½¿å…¶æˆä¸ºè¿‡ç¨‹æ§åˆ¶ä¸­çš„å…³é”®å·¥å…·ã€‚é€»è¾‘å¼‚å¸¸åœ¨è§†è§‰ä¸Šå¯èƒ½çœ‹ä¼¼æ­£å¸¸ï¼Œä½†ä¼šè¿åå¯¹è±¡å­˜åœ¨ã€æ’åˆ—æˆ–æ•°é‡ç­‰æ–¹é¢çš„é¢„å®šä¹‰çº¦æŸï¼Œè¿™å–å†³äºæ¨ç†å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†LogicQAæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸ºå·¥ä¸šæ“ä½œå‘˜æä¾›é€»è¾‘å¼‚å¸¸çš„è§£é‡Šæ¥å¢å¼ºADçš„åŠŸèƒ½ã€‚LogicQAå°†è‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜ç¼–è¯‘æˆæ¸…å•ï¼Œå¹¶æ”¶é›†å“åº”æ¥è¯†åˆ«é€»è¾‘çº¦æŸçš„è¿åæƒ…å†µã€‚LogicQAæ— éœ€è®­ç»ƒå’Œæ ‡æ³¨ï¼Œä¸”åœ¨å°‘é‡æ ·æœ¬æ•°æ®ä¸‹å³å¯è¿è¡Œã€‚æˆ‘ä»¬åœ¨å…¬å…±åŸºå‡†æµ‹è¯•MVTec LOCO ADä¸Šå®ç°äº†æœ€æ–°çš„é€»è¾‘ADæ€§èƒ½ï¼Œå…·æœ‰87.6ï¼…çš„AUROCå’Œ87.0ï¼…çš„F1æœ€å¤§å€¼ï¼Œå¹¶æä¾›äº†å¼‚å¸¸æƒ…å†µçš„è§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŠå¯¼ä½“çš„SEMä¼ä¸šæ•°æ®ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20252v2">PDF</a> Accepted Industry Track at ACL 2025</p>
<p><strong>Summary</strong><br>åŸºäºé€»è¾‘å¼‚å¸¸æ£€æµ‹çš„æ¡†æ¶LogicQAï¼Œæ—¨åœ¨æé«˜å·¥ä¸šè¿è¥è€…å¯¹é€»è¾‘å¼‚å¸¸çš„è¯†åˆ«èƒ½åŠ›ã€‚å®ƒé€šè¿‡è‡ªåŠ¨ç”Ÿæˆé—®é¢˜æ¸…å•å¹¶æ”¶é›†å›åº”ï¼Œä»¥è¯†åˆ«é€»è¾‘çº¦æŸçš„è¿è§„æƒ…å†µã€‚è¯¥æ¡†æ¶æ— éœ€è®­ç»ƒå’Œæ ‡æ³¨ï¼Œåœ¨å°‘é‡æ ·æœ¬ä¸‹è¿è¡Œï¼Œå¹¶åœ¨MVTec LOCO ADå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å…ˆè¿›çš„é€»è¾‘å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼ŒåŒ…æ‹¬é«˜æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰å’Œæœ€å¤§F1åˆ†æ•°ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨åŠå¯¼ä½“SEMä¼ä¸šæ•°æ®ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LogicQAæ˜¯ä¸€ä¸ªç”¨äºå¢å¼ºå¼‚å¸¸æ£€æµ‹çš„æ¡†æ¶ï¼Œå¯è§£é‡Šé€»è¾‘å¼‚å¸¸ã€‚</li>
<li>LogicQAé€šè¿‡è‡ªåŠ¨ç”Ÿæˆé—®é¢˜æ¸…å•æ¥è¯†åˆ«é€»è¾‘çº¦æŸè¿è§„æƒ…å†µã€‚</li>
<li>è¯¥æ¡†æ¶æ— éœ€è®­ç»ƒå’Œæ ‡æ³¨ï¼Œé€‚ç”¨äºå°‘é‡æ ·æœ¬ã€‚</li>
<li>LogicQAåœ¨MVTec LOCO ADå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>LogicQAå®ç°äº†é«˜AUROCå’ŒF1-maxåˆ†æ•°ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨åŠå¯¼ä½“SEMä¼ä¸šæ•°æ®ä¸Šçš„è¡¨ç°è¯æ˜äº†å…¶åœ¨å·¥ä¸šåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa13c28bfe8b85794334765e811a4ccb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a84f1eadb30a499a5e68d4dae8a043b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b35b3e797d30a70be5b06e152c338dcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9069c06875e78034da4f7c6c37b67de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ddcb731fe555b7ecf8c0e12a1accf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abeaab0cecbcc26b32aeea9ae6b8e577.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Generalized-Few-shot-3D-Point-Cloud-Segmentation-with-Vision-Language-Model"><a href="#Generalized-Few-shot-3D-Point-Cloud-Segmentation-with-Vision-Language-Model" class="headerlink" title="Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language   Model"></a>Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language   Model</h2><p><strong>Authors:Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Junlin Han, Ender Konukoglu, Serge Belongie</strong></p>
<p>Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world novel classes, contain rich but noisy novel class knowledge. In this work, we introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design a novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide a solid foundation for advancing GFS-PCS in the real world. The code is at <a target="_blank" rel="noopener" href="https://github.com/ZhaochongAn/GFS-VL">https://github.com/ZhaochongAn/GFS-VL</a> </p>
<blockquote>
<p>å¹¿ä¹‰å°‘æ ·æœ¬3Dç‚¹äº‘åˆ†å‰²ï¼ˆGFS-PCSï¼‰èƒ½å¤Ÿåœ¨ä¿ç•™åŸºç¡€ç±»åˆ«åˆ†å‰²çš„åŒæ—¶ï¼Œé€‚åº”æ–°çš„ç±»åˆ«å¹¶å¤„ç†å°‘é‡çš„æ ·æœ¬æ”¯æŒã€‚ç°æœ‰çš„GFS-PCSæ–¹æ³•é€šè¿‡æ”¯æŒç‰¹å¾æˆ–æŸ¥è¯¢ç‰¹å¾ä¸åŸå‹è¿›è¡Œäº¤äº’ï¼Œä½†ä»å—é™äºå°‘æ ·æœ¬çš„ç¨€ç–çŸ¥è¯†ã€‚åŒæ—¶ï¼Œç”¨äºå¼€æ”¾ä¸–ç•Œæ–°ç±»åˆ«çš„é€šç”¨3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆ3D VLMsï¼‰åŒ…å«ä¸°å¯Œä½†å˜ˆæ‚çš„æ–°ç±»åˆ«çŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªGFS-PCSæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒç»“åˆäº†æ¥è‡ª3D VLMsçš„å¯†é›†ä½†å˜ˆæ‚çš„ä¼ªæ ‡ç­¾å’Œç²¾ç¡®ä½†ç¨€ç–çš„å°‘é‡æ ·æœ¬ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‘æŒ¥ä¸¤è€…çš„ä¼˜åŠ¿ï¼Œç§°ä¸ºGFS-VLã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸå‹å¼•å¯¼ä¼ªæ ‡ç­¾é€‰æ‹©æ–¹æ³•ï¼Œç”¨äºè¿‡æ»¤ä½è´¨é‡åŒºåŸŸï¼Œéšåé‡‡ç”¨è‡ªé€‚åº”å¡«å……ç­–ç•¥ï¼Œç»“åˆä¼ªæ ‡ç­¾ä¸Šä¸‹æ–‡å’Œå°‘é‡æ ·æœ¬çŸ¥è¯†ï¼Œå¯¹è¿‡æ»¤åçš„æœªæ ‡è®°åŒºåŸŸè¿›è¡Œè‡ªé€‚åº”æ ‡è®°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹åŸºç¡€æ··åˆç­–ç•¥ï¼Œå°†å°‘é‡æ ·æœ¬åµŒå…¥è®­ç»ƒåœºæ™¯ï¼Œä¿ç•™é‡è¦ä¸Šä¸‹æ–‡ï¼Œä»¥æ”¹è¿›æ–°ç±»åˆ«çš„å­¦ä¹ ã€‚è€Œä¸”ï¼Œè€ƒè™‘åˆ°å½“å‰GFS-PCSåŸºå‡†æµ‹è¯•å¤šæ ·æ€§çš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå…·æœ‰å¤šç§æ–°ç±»åˆ«çš„æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ï¼Œä»¥è¿›è¡Œå…¨é¢çš„ä¸€èˆ¬åŒ–è¯„ä¼°ã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ä¸ºæ¨è¿›GFS-PCSåœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/ZhaochongAn/GFS-VL">https://github.com/ZhaochongAn/GFS-VL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16282v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆäº†3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆ3D VLMsï¼‰å’Œå¹¿ä¹‰å°‘æ ·æœ¬3Dç‚¹äº‘åˆ†å‰²ï¼ˆGFS-PCSï¼‰æ¡†æ¶çš„æ–¹æ³•ï¼Œç§°ä¸ºGFS-VLã€‚è¯¥æ–¹æ³•åˆ©ç”¨3D VLMsæä¾›çš„ä¸°å¯Œä½†å¸¦æœ‰å™ªå£°çš„ä¼ªæ ‡ç­¾ä¸å°‘æ ·æœ¬æ•°æ®ç›¸ç»“åˆï¼Œé€šè¿‡åŸå‹å¼•å¯¼ä¼ªæ ‡ç­¾é€‰æ‹©å’Œè‡ªé€‚åº”å¡«å……ç­–ç•¥ï¼Œæœ€å¤§é™åº¦åœ°å‘æŒ¥ä¸¤è€…çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹åŸºç¡€æ··åˆç­–ç•¥ï¼Œå°†å°‘é‡æ ·æœ¬åµŒå…¥è®­ç»ƒåœºæ™¯ä¸­ï¼Œä¿ç•™é‡è¦ä¸Šä¸‹æ–‡ï¼Œä»¥æ”¹è¿›æ–°å‹ç±»çš„å­¦ä¹ ã€‚æ–‡ç« è¿˜ä»‹ç»äº†ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GFS-VLç»“åˆ3D VLMså’ŒGFS-PCSæ¡†æ¶ï¼Œåˆ©ç”¨ä¼ªæ ‡ç­¾å’Œå°‘æ ·æœ¬æ•°æ®è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>æå‡ºåŸå‹å¼•å¯¼ä¼ªæ ‡ç­¾é€‰æ‹©ç­–ç•¥ï¼Œè¿‡æ»¤ä½è´¨é‡åŒºåŸŸã€‚</li>
<li>é‡‡ç”¨è‡ªé€‚åº”å¡«å……ç­–ç•¥ï¼Œç»“åˆä¼ªæ ‡ç­¾ä¸Šä¸‹æ–‡å’Œå°‘é‡æ ·æœ¬è¿›è¡Œæ ‡æ³¨ã€‚</li>
<li>è®¾è®¡æ–°å‹åŸºç¡€æ··åˆç­–ç•¥ï¼Œå°†å°‘é‡æ ·æœ¬åµŒå…¥è®­ç»ƒåœºæ™¯ï¼Œä¿ç•™é‡è¦ä¸Šä¸‹æ–‡ã€‚</li>
<li>å¼•å…¥ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d40761f96ec5cd845fb0566592fc0f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c1249e2e6bb7508adca5e077f48f3d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0605d2dc9fc4a10ad1374de047600aa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb68326f2c93c82c51a6dd449804308.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Conjuring-Positive-Pairs-for-Efficient-Unification-of-Representation-Learning-and-Image-Synthesis"><a href="#Conjuring-Positive-Pairs-for-Efficient-Unification-of-Representation-Learning-and-Image-Synthesis" class="headerlink" title="Conjuring Positive Pairs for Efficient Unification of Representation   Learning and Image Synthesis"></a>Conjuring Positive Pairs for Efficient Unification of Representation   Learning and Image Synthesis</h2><p><strong>Authors:Imanol G. Estepa, JesÃºs M. RodrÃ­guez-de-Vera, Ignacio SarasÃºa, Bhalaji Nagarajan, Petia Radeva</strong></p>
<p>While representation learning and generative modeling seek to understand visual data, unifying both domains remains unexplored. Recent Unified Self-Supervised Learning (SSL) methods have started to bridge the gap between both paradigms. However, they rely solely on semantic token reconstruction, which requires an external tokenizer during training â€“ introducing a significant overhead. In this work, we introduce Sorcen, a novel unified SSL framework, incorporating a synergic Contrastive-Reconstruction objective. Our Contrastive objective, â€œEcho Contrastâ€, leverages the generative capabilities of Sorcen, eliminating the need for additional image crops or augmentations during training. Sorcen â€œgeneratesâ€ an echo sample in the semantic token space, forming the contrastive positive pair. Sorcen operates exclusively on precomputed tokens, eliminating the need for an online token transformation during training, thereby significantly reducing computational overhead. Extensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the previous Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear probing, unconditional image generation, few-shot learning, and transfer learning, respectively, while being 60.8% more efficient. Additionally, Sorcen surpasses previous single-crop MIM SoTA in linear probing and achieves SoTA performance in unconditional image generation, highlighting significant improvements and breakthroughs in Unified SSL models. </p>
<blockquote>
<p>è¡¨ç¤ºå­¦ä¹ å’Œç”Ÿæˆå»ºæ¨¡éƒ½åœ¨åŠªåŠ›ç†è§£è§†è§‰æ•°æ®ï¼Œä½†ç»Ÿä¸€è¿™ä¸¤ä¸ªé¢†åŸŸä»ç„¶æœªè¢«æ¢ç´¢ã€‚æœ€è¿‘çš„ç»Ÿä¸€è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆSSLï¼‰å·²ç»å¼€å§‹å¼¥åˆä¸¤ç§èŒƒå¼ä¹‹é—´çš„é¸¿æ²Ÿã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»…ä¾èµ–äºè¯­ä¹‰ä»¤ç‰Œé‡å»ºï¼Œè¿™éœ€è¦è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¤–éƒ¨ä»¤ç‰Œå™¨â€”â€”å¸¦æ¥äº†ç›¸å½“å¤§çš„å¼€é”€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Sorcenï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç»Ÿä¸€SSLæ¡†æ¶ï¼Œå®ƒç»“åˆäº†ååŒå¯¹æ¯”é‡å»ºç›®æ ‡ã€‚æˆ‘ä»¬çš„å¯¹æ¯”ç›®æ ‡â€œå›å£°å¯¹æ¯”â€åˆ©ç”¨äº†Sorcençš„ç”Ÿæˆèƒ½åŠ›ï¼Œæ¶ˆé™¤äº†è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é¢å¤–å›¾åƒè£å‰ªæˆ–å¢å¼ºçš„éœ€æ±‚ã€‚Sorcenåœ¨è¯­ä¹‰ä»¤ç‰Œç©ºé—´ä¸­â€œç”Ÿæˆâ€ä¸€ä¸ªå›å£°æ ·æœ¬ï¼Œå½¢æˆå¯¹æ¯”æ­£å¯¹ã€‚Sorcenåªåœ¨é¢„è®¡ç®—ä»¤ç‰Œä¸Šè¿è¡Œï¼Œæ¶ˆé™¤äº†è®­ç»ƒè¿‡ç¨‹ä¸­åœ¨çº¿ä»¤ç‰Œè½¬æ¢çš„éœ€è¦ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚åœ¨ImageNet-1kä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨çº¿æ€§æ¢æµ‹ã€æ— æ¡ä»¶å›¾åƒç”Ÿæˆã€å°æ ·æœ¬å­¦ä¹ å’Œè¿ç§»å­¦ä¹ æ–¹é¢ï¼ŒSorcenåˆ†åˆ«è¶…è¿‡äº†ä¹‹å‰çš„ç»Ÿä¸€SSLæ°´å¹³0.4%ã€FIDé™ä½1.48ã€ä¸‹é™1.76%ã€ä¸‹é™ç‡ä¸ºè‡³æœ€ä½è‡³æœ€å¤šç™¾åˆ†ä¹‹åäº”ï¼Œè€Œæ•ˆç‡æé«˜è‡³æœ€å¤šç™¾åˆ†ä¹‹å…­åä»¥ä¸Šå…«æå‡æ•ˆç‡æ—¶åˆ™æ˜¯å¤§å¹…æå‡ã€‚æ­¤å¤–ï¼Œåœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ–¹é¢ï¼ŒSorcenè¶…è¶Šäº†ä¹‹å‰çš„å•è£å‰ªMIMæ°´å¹³æœ€é«˜è¡¨ç°ï¼Œå¹¶åœ¨ç»Ÿä¸€SSLæ¨¡å‹ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›å’Œçªç ´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15060v3">PDF</a> The source code is available in <a target="_blank" rel="noopener" href="https://github.com/ImaGonEs/Sorcen">https://github.com/ImaGonEs/Sorcen</a></p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„ç»Ÿä¸€è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶Sorcenï¼Œå®ƒç»“åˆäº†å¯¹æ¯”å’Œé‡å»ºç›®æ ‡ï¼Œæ— éœ€é¢å¤–çš„å›¾åƒè£å‰ªæˆ–å¢å¼ºå³å¯è¿›è¡Œè®­ç»ƒã€‚Sorcenåœ¨é¢„è®¡ç®—ä»¤ç‰Œä¸Šè¿è¡Œï¼Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚åœ¨ImageNet-1kä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSorcenåœ¨ç»Ÿä¸€SSLé¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šä¹Ÿæœ‰æ‰€çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†æ–°å‹ç»Ÿä¸€è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶Sorcenã€‚</li>
<li>Sorcenç»“åˆäº†å¯¹æ¯”å’Œé‡å»ºç›®æ ‡ï¼Œæé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ•ˆæœã€‚</li>
<li>Sorcenåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›ç”Ÿæˆå¯¹æ¯”æ­£æ ·æœ¬å¯¹ï¼Œæ— éœ€é¢å¤–çš„å›¾åƒè£å‰ªæˆ–å¢å¼ºã€‚</li>
<li>Sorcenåœ¨é¢„è®¡ç®—ä»¤ç‰Œä¸Šè¿è¡Œï¼Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚</li>
<li>åœ¨ImageNet-1kä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒSorcenåœ¨ç»Ÿä¸€SSLé¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>Sorcenåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬çº¿æ€§æ¢æµ‹ã€æ— æ¡ä»¶å›¾åƒç”Ÿæˆã€å°‘æ ·æœ¬å­¦ä¹ å’Œè¿ç§»å­¦ä¹ ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-51c375b6d1aa15a2bc3a86d290d7019c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c0439c7a81af2941727063a1f161805.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e789df577b419d5a45db630edef0eaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a0eeba2eca04982030c2592cc91ec63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ef503478314bf60bdd111b2bf54492e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="STD-PLM-Understanding-Both-Spatial-and-Temporal-Properties-of-Spatial-Temporal-Data-with-PLM"><a href="#STD-PLM-Understanding-Both-Spatial-and-Temporal-Properties-of-Spatial-Temporal-Data-with-PLM" class="headerlink" title="STD-PLM: Understanding Both Spatial and Temporal Properties of   Spatial-Temporal Data with PLM"></a>STD-PLM: Understanding Both Spatial and Temporal Properties of   Spatial-Temporal Data with PLM</h2><p><strong>Authors:YiHeng Huang, Xiaowei Mao, Shengnan Guo, Yubin Chen, Junfeng Shen, Tiankuo Li, Youfang Lin, Huaiyu Wan</strong></p>
<p>Spatial-temporal forecasting and imputation are important for real-world intelligent systems. Most existing methods are tailored for individual forecasting or imputation tasks but are not designed for both. Additionally, they are less effective for zero-shot and few-shot learning. While pre-trained language model (PLM) have exhibited strong pattern recognition and reasoning abilities across various tasks, including few-shot and zero-shot learning, their applications in spatial-temporal data understanding has been constrained by insufficient modeling of complex correlations such as the temporal correlations, spatial connectivity, non-pairwise and high-order spatial-temporal correlations within data. In this paper, we propose STD-PLM for understanding both spatial and temporal properties of \underline{S}patial-\underline{T}emporal \underline{D}ata with \underline{PLM}, which is capable of implementing both spatial-temporal forecasting and imputation tasks. STD-PLM understands spatial-temporal correlations via explicitly designed spatial and temporal tokenizers. Topology-aware node embeddings are designed for PLM to comprehend and exploit the topology structure of data in inductive manner. Furthermore, to mitigate the efficiency issues introduced by the PLM, we design a sandglass attention module (SGA) combined with a specific constrained loss function, which significantly improves the modelâ€™s efficiency while ensuring performance. Extensive experiments demonstrate that STD-PLM exhibits competitive performance and generalization capabilities across the forecasting and imputation tasks on various datasets. Moreover, STD-PLM achieves promising results on both few-shot and zero-shot tasks. The code is made available at \href{<a target="_blank" rel="noopener" href="https://github.com/Hyheng/STD-PLM%7D%7Bhttps://github.com/Hyheng/STD-PLM%7D">https://github.com/Hyheng/STD-PLM}{https://github.com/Hyheng/STD-PLM}</a> </p>
<blockquote>
<p>ç©ºé—´æ—¶é—´é¢„æµ‹å’Œè¡¥å…¨å¯¹äºç°å®ä¸–ç•Œçš„æ™ºèƒ½ç³»ç»Ÿéå¸¸é‡è¦ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½æ˜¯é’ˆå¯¹ä¸ªåˆ«é¢„æµ‹æˆ–è¡¥å…¨ä»»åŠ¡å®šåˆ¶çš„ï¼Œå¹¶ä¸é€‚ç”¨äºä¸¤è€…åŒæ—¶è¿›è¡Œã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹é¢çš„æ•ˆæœè¾ƒå·®ã€‚è™½ç„¶é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ¨¡å¼è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ ï¼Œä½†å…¶åœ¨ç©ºé—´æ—¶é—´æ•°æ®ç†è§£æ–¹é¢çš„åº”ç”¨å—åˆ°äº†å»ºæ¨¡å¤æ‚å…³ç³»ä¸è¶³çš„åˆ¶çº¦ï¼Œå¦‚æ—¶é—´ç›¸å…³æ€§ã€ç©ºé—´è¿é€šæ€§ã€æ•°æ®å†…éƒ¨éé…å¯¹å’Œé«˜é˜¶æ—¶ç©ºå…³ç³»ç­‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºPLMçš„STD-PLMæ¨¡å‹ï¼Œç”¨äºç†è§£æ—¶ç©ºæ•°æ®çš„ç©ºé—´å’Œæ—¶é—´å±æ€§ï¼Œå¹¶å…·å¤‡è¿›è¡Œæ—¶ç©ºé¢„æµ‹å’Œè¡¥å…¨ä»»åŠ¡çš„èƒ½åŠ›ã€‚STD-PLMé€šè¿‡æ˜ç¡®è®¾è®¡çš„ç©ºé—´å’Œæ—¶é—´æ ‡è®°å™¨ç†è§£æ—¶ç©ºç›¸å…³æ€§ã€‚é’ˆå¯¹PLMè®¾è®¡äº†æ‹“æ‰‘æ„ŸçŸ¥èŠ‚ç‚¹åµŒå…¥ï¼Œä»¥å½’çº³æ–¹å¼ç†è§£å’Œåˆ©ç”¨æ•°æ®çš„æ‹“æ‰‘ç»“æ„ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³ç”±PLMå¼•å…¥çš„æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ²™æ¼æ³¨æ„æ¨¡å—ï¼ˆSGAï¼‰ç»“åˆç‰¹å®šçš„çº¦æŸæŸå¤±å‡½æ•°ï¼Œè¿™ä¸ä»…æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ï¼Œè€Œä¸”ä¿è¯äº†æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSTD-PLMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„é¢„æµ‹å’Œè¡¥å…¨ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒSTD-PLMåœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Hyheng/STD-PLM">https://github.com/Hyheng/STD-PLM</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.09096v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ç©ºé—´å’Œæ—¶é—´æ•°æ®çš„ç†è§£æ–¹æ³•STD-PLMã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ‰§è¡Œç©ºé—´æ—¶é—´é¢„æµ‹å’Œæ’å€¼ä»»åŠ¡ï¼Œé€šè¿‡æ˜ç¡®è®¾è®¡çš„ç©ºé—´å’Œæ—¶é—´æ ‡è®°å™¨ç†è§£æ—¶ç©ºç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†æ‹“æ‰‘æ„ŸçŸ¥èŠ‚ç‚¹åµŒå…¥å’Œæ²™æ¼æ³¨æ„åŠ›æ¨¡å—ï¼ˆSGAï¼‰ï¼Œä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒSTD-PLMåœ¨é¢„æµ‹å’Œæ’å€¼ä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>STD-PLMç»“åˆäº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ç©ºé—´æ—¶é—´é¢„æµ‹å’Œæ’å€¼ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡æ˜ç¡®è®¾è®¡çš„ç©ºé—´å’Œæ—¶é—´æ ‡è®°å™¨ï¼ŒSTD-PLMèƒ½å¤Ÿç†è§£æ—¶ç©ºæ•°æ®ä¸­çš„å¤æ‚ç›¸å…³æ€§ã€‚</li>
<li>æ‹“æ‰‘æ„ŸçŸ¥èŠ‚ç‚¹åµŒå…¥çš„è®¾è®¡ä½¿å¾—PLMèƒ½å¤Ÿæ„ŸçŸ¥å¹¶åˆ©ç”¨æ•°æ®çš„æ‹“æ‰‘ç»“æ„ã€‚</li>
<li>æ²™æ¼æ³¨æ„åŠ›æ¨¡å—ï¼ˆSGAï¼‰ä¸ç‰¹å®šçº¦æŸæŸå¤±å‡½æ•°çš„è®¾è®¡æé«˜äº†æ¨¡å‹çš„æ•ˆç‡å¹¶ä¿è¯æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSTD-PLMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„é¢„æµ‹å’Œæ’å€¼ä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>STD-PLMåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.09096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30ff77f93744d40af631fd36a9a0db82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-565204ee75c438dfec1acb3af0807f85.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Iterative-Deployment-Exposure-for-Unsupervised-Out-of-Distribution-Detection"><a href="#Iterative-Deployment-Exposure-for-Unsupervised-Out-of-Distribution-Detection" class="headerlink" title="Iterative Deployment Exposure for Unsupervised Out-of-Distribution   Detection"></a>Iterative Deployment Exposure for Unsupervised Out-of-Distribution   Detection</h2><p><strong>Authors:Lars Doorenbos, Raphael Sznitman, Pablo MÃ¡rquez-Neila</strong></p>
<p>Deep learning models are vulnerable to performance degradation when encountering out-of-distribution (OOD) images, potentially leading to misdiagnoses and compromised patient care. These shortcomings have led to great interest in the field of OOD detection. Existing unsupervised OOD (U-OOD) detection methods typically assume that OOD samples originate from an unconcentrated distribution complementary to the training distribution, neglecting the reality that deployed models passively accumulate task-specific OOD samples over time. To better reflect this real-world scenario, we introduce Iterative Deployment Exposure (IDE), a novel and more realistic setting for U-OOD detection. We propose CSO, a method for IDE that starts from a U-OOD detector that is agnostic to the OOD distribution and slowly refines it during deployment using observed unlabeled data. CSO uses a new U-OOD scoring function that combines the Mahalanobis distance with a nearest-neighbor approach, along with a novel confidence-scaled few-shot OOD detector to effectively learn from limited OOD examples. We validate our approach on a dedicated benchmark, showing that our method greatly improves upon strong baselines on three medical imaging modalities. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é‡åˆ°ç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰å›¾åƒæ—¶å®¹æ˜“å‡ºç°æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯¯è¯Šå’Œæ‚£è€…æŠ¤ç†å—åˆ°å½±å“ã€‚è¿™äº›ç¼ºé™·å¼•å‘äº†äººä»¬å¯¹OODæ£€æµ‹é¢†åŸŸçš„æå¤§å…´è¶£ã€‚ç°æœ‰çš„æ— ç›‘ç£OODï¼ˆU-OODï¼‰æ£€æµ‹æ–¹æ³•é€šå¸¸å‡è®¾OODæ ·æœ¬æ¥è‡ªä¸è®­ç»ƒåˆ†å¸ƒäº’è¡¥çš„éé›†ä¸­åˆ†å¸ƒï¼Œå¿½è§†äº†è¿™æ ·ä¸€ä¸ªç°å®æƒ…å†µï¼šéƒ¨ç½²çš„æ¨¡å‹éšç€æ—¶é—´çš„æ¨ç§»ä¼šè¢«åŠ¨åœ°ç´¯ç§¯ç‰¹å®šä»»åŠ¡çš„OODæ ·æœ¬ã€‚ä¸ºäº†æ›´å¥½åœ°åæ˜ è¿™ç§çœŸå®åœºæ™¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¿­ä»£éƒ¨ç½²æš´éœ²ï¼ˆIDEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºU-OODæ£€æµ‹çš„æ–°å‹ä¸”æ›´ç°å®çš„è®¾ç½®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§IDEæ–¹æ³•CSOï¼Œè¯¥æ–¹æ³•ä»å¯¹OODåˆ†å¸ƒä¸äº†è§£çš„U-OODæ£€æµ‹å™¨å¼€å§‹ï¼Œåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ä½¿ç”¨è§‚å¯Ÿåˆ°çš„æ— æ ‡ç­¾æ•°æ®ç¼“æ…¢åœ°å¯¹å…¶è¿›è¡Œä¼˜åŒ–ã€‚CSOä½¿ç”¨äº†ä¸€ç§æ–°çš„U-OODè¯„åˆ†å‡½æ•°ï¼Œç»“åˆäº†é©¬æ°è·ç¦»å’Œæœ€è¿‘é‚»æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨äº†ä¸€ç§æ–°å‹çš„ä¿¡å¿ƒè§„æ¨¡æœ‰é™æ ·æœ¬OODæ£€æµ‹å™¨ï¼Œä»¥æœ‰æ•ˆåœ°ä»æœ‰é™çš„OODæ ·æœ¬ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨ä¸“ç”¨åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç§åŒ»å­¦å½±åƒæ¨¡æ€ä¸Šå¤§å¤§æ”¹è¿›äº†å¼ºåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02327v2">PDF</a> Accepted at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é¢ä¸´éåˆ†å¸ƒå›¾åƒæ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯¯è¯Šå’Œæ‚£è€…æŠ¤ç†å—åˆ°æŸå®³ã€‚ç°æœ‰æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆU-OODï¼‰æ–¹æ³•å‡è®¾å¼‚å¸¸æ ·æœ¬æ¥æºäºéé›†ä¸­åˆ†å¸ƒï¼Œå¿½ç•¥ç°å®åœºæ™¯æ¨¡å‹éƒ¨ç½²ä¸­ä»»åŠ¡ç‰¹å®šçš„å¼‚å¸¸æ ·æœ¬é€æ¸ç§¯ç´¯çš„é—®é¢˜ã€‚ä¸ºåæ˜ çœŸå®æƒ…å†µï¼Œæå‡ºäº†è¿­ä»£éƒ¨ç½²æš´éœ²ï¼ˆIDEï¼‰è¿™ä¸€æ–°å‹çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹åœºæ™¯è®¾ç½®ã€‚é’ˆå¯¹IDEåœºæ™¯ï¼Œæå‡ºäº†ä¸€ç§æ–¹æ³•CSOï¼Œè¯¥æ–¹æ³•ä»å¯¹å¼‚å¸¸åˆ†å¸ƒæ— æ„ŸçŸ¥çš„U-OODæ£€æµ‹å™¨å‡ºå‘ï¼Œåˆ©ç”¨è§‚å¯Ÿåˆ°çš„æœªæ ‡è®°æ•°æ®åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­é€æ­¥ä¼˜åŒ–ã€‚CSOé‡‡ç”¨æ–°çš„U-OODè¯„åˆ†å‡½æ•°ï¼Œç»“åˆé©¬æ°è·ç¦»å’Œæœ€è¿‘é‚»æ–¹æ³•ï¼Œä»¥åŠæ–°çš„ä¿¡å¿ƒç¼©æ”¾å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹å™¨ï¼Œä»æœ‰é™çš„å¼‚å¸¸æ ·æœ¬ä¸­å­¦ä¹ ã€‚åœ¨ä¸“ç”¨åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œåœ¨ä¸‰ç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šå¤§å¤§ä¼˜äºå¼ºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é¢ä¸´éåˆ†å¸ƒå›¾åƒæ—¶å¯èƒ½æ€§èƒ½ä¸‹é™ï¼Œå¯¼è‡´è¯¯è¯Šå’Œæ‚£è€…æŠ¤ç†é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•å¿½ç•¥äº†ç°å®åœºæ™¯ä¸­æ¨¡å‹éƒ¨ç½²ä¸­ä»»åŠ¡ç‰¹å®šå¼‚å¸¸æ ·æœ¬é€æ¸ç§¯ç´¯çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†è¿­ä»£éƒ¨ç½²æš´éœ²ï¼ˆIDEï¼‰è¿™ä¸€æ–°å‹çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹åœºæ™¯è®¾ç½®ï¼Œä»¥æ›´å¥½åœ°åæ˜ çœŸå®æƒ…å†µã€‚</li>
<li>é’ˆå¯¹IDEåœºæ™¯ï¼Œæå‡ºäº†CSOæ–¹æ³•ï¼Œä»å¯¹å¼‚å¸¸åˆ†å¸ƒæ— æ„ŸçŸ¥çš„U-OODæ£€æµ‹å™¨å‡ºå‘ï¼Œé€æ­¥ä¼˜åŒ–ã€‚</li>
<li>CSOé‡‡ç”¨ç»“åˆé©¬æ°è·ç¦»å’Œæœ€è¿‘é‚»æ–¹æ³•çš„æ–°çš„U-OODè¯„åˆ†å‡½æ•°ã€‚</li>
<li>CSOå¼•å…¥ä¿¡å¿ƒç¼©æ”¾å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹å™¨ï¼Œå¯ä»æœ‰é™çš„å¼‚å¸¸æ ·æœ¬ä¸­å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-993c56f8262600b24f4ab926c4d5cdba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccfe5169e9fcb1c000353768095b19f9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4f8cae6e3b8954cf2b47c6a9d60c4d92.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Replace in Translation Boost Concept Alignment in Counterfactual   Text-to-Image
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70326c7591212f1a08bbbbe1e5097b67.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  ContextAgent Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
