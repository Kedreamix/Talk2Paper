<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-22  PRL Prompts from Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7964b2debc9adf20fd660a685fa7ceda.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    65 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-22-更新"><a href="#2025-05-22-更新" class="headerlink" title="2025-05-22 更新"></a>2025-05-22 更新</h1><h2 id="PRL-Prompts-from-Reinforcement-Learning"><a href="#PRL-Prompts-from-Reinforcement-Learning" class="headerlink" title="PRL: Prompts from Reinforcement Learning"></a>PRL: Prompts from Reinforcement Learning</h2><p><strong>Authors:Paweł Batorski, Adrian Kosmala, Paul Swoboda</strong></p>
<p>Effective prompt engineering remains a central challenge in fully harnessing the capabilities of LLMs. While well-designed prompts can dramatically enhance performance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on subtle semantic cues, ones that may elude human perception but are crucial for guiding LLM behavior. In this paper, we introduce PRL (Prompts from Reinforcement Learning), a novel RL-based approach for automatic prompt generation. Unlike previous methods, PRL can produce novel few-shot examples that were not seen during training. Our approach achieves state-of-the-art performance across a range of benchmarks, including text classification, simplification, and summarization. On the classification task, it surpasses prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it improves the average ROUGE scores on the summarization task by 4.32 over APE and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over APE and by 6.01 over EvoPrompt. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Batorskq/prl">https://github.com/Batorskq/prl</a> . </p>
<blockquote>
<p>有效提示工程仍然是充分利用大型语言模型（LLM）能力的核心挑战。虽然精心设计好的提示可以显著提高性能，但制作它们通常需要专家的直觉和对任务的微妙理解。此外，最有影响力的提示通常依赖于微妙的语义线索，这些线索可能会逃避人类的感知，但对于指导LLM行为至关重要。在本文中，我们介绍了PRL（基于强化学习的提示），这是一种用于自动提示生成的新型基于强化学习的方法。不同于以前的方法，PRL可以生成在训练期间未见过的全新少样本示例。我们的方法在各种基准测试中实现了最先进的性能，包括文本分类、简化和摘要。在分类任务上，它比APE高出2.58%，比EvoPrompt高出1.00%。此外，它在摘要任务的平均ROUGE得分上比APE高出4.32%，比EvoPrompt高出2.12；在简化任务的SARI得分上比APE高出6.93%，比EvoPrompt高出6.01。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Batorskq/prl%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Batorskq/prl找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14412v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于强化学习（RL）的自动提示生成方法PRL（Prompts from Reinforcement Learning）。该方法能够产生新颖的在训练过程中未见过的少数例子提示。在文本分类、简化和总结等多个基准测试中，PRL取得了最先进的性能表现。该方法超越了先前的自动提示生成方法，具有极大的实用价值。有关代码可以在指定的GitHub地址中找到。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PRL是一种基于强化学习的自动提示生成方法，可以生成新颖的少数例子提示。</li>
<li>PRL在文本分类、简化和总结等多个基准测试中取得了最先进的性能表现。</li>
<li>PRL超越了先前的自动提示生成方法，展现出更好的实际应用价值。</li>
<li>PRL的性能提升显著，尤其在分类任务上，相比APE和EvoPrompt分别有更高的准确性提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14412">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-044889130030332563dff3aa0e66a4cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a19bc824cb1c07b8c78df300a6a8d05d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0de9e71bb1a30459171b3b63eb5e0b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3008c0890fecc712f0c17bb5f469debc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation"><a href="#FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation" class="headerlink" title="FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ü-Tsang, Amdo and Kham Speech Dataset Generation"></a>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ü-Tsang, Amdo and Kham Speech Dataset Generation</h2><p><strong>Authors:Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi</strong></p>
<p>Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-&quot;U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality. </p>
<blockquote>
<p>藏语是一种资源匮乏的语言，其三大方言区——乌齐、安多和康区的平行语音语料库极其有限，限制了语音建模的进展。为了解决这一问题，我们提出了FMSD-TTS，这是一个少样本、多发言人、多方言的文本到语音框架，它可以从有限的参考音频和明确的方言标签中合成平行方言语音。我们的方法具有新颖的发声人-方言融合模块和方言专业化动态路由网络（DSDR-Net），能够捕捉不同方言之间的细微声学和语言变化，同时保留发声人的身份。大量的客观和主观评估表明，FMSD-TTS在方言表现力和发声人相似性方面显著优于基准线。我们进一步通过具有挑战性的语音到语音方言转换任务验证了合成语音的质量和实用性。我们的贡献包括：（1）针对藏语多方言语音合成的新型少样本TTS系统，（2）公开发布由FMSD-TTS生成的大规模合成藏语语音语料库，（3）开放源代码评估工具包，用于标准化评估发声人相似性、方言一致性和音频质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14351v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了针对藏语这一低资源语言，如何通过使用少量数据解决其在语音建模方面的挑战。为此，提出了一个面向少数演讲者、多方言的文本到语音框架FMSD-TTS，通过有限参考音频和明确的方言标签合成并行方言语音。该框架具有新颖的演讲者-方言融合模块和方言特定动态路由网络（DSDR-Net），可捕捉方言间的细微声学差异和语言变化，同时保留演讲者身份。评估和实验证明，FMSD-TTS在方言表达力和演讲者相似性方面明显优于基线系统。此外，通过具有挑战性的语音到语音方言转换任务验证了合成语音的质量和实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>藏语是低资源语言，缺乏平行语料库，限制其语音建模发展。</li>
<li>提出FMSD-TTS框架，实现利用有限数据合成方言语音。</li>
<li>FMSD-TTS具备多方言和演讲者能力，并通过融合模块和DSDR-Net技术捕捉方言差异和语言变化。</li>
<li>FMSD-TTS显著提高了方言表达力和演讲者相似性方面的性能。</li>
<li>通过语音到语音方言转换任务验证了合成语音的质量和实用性。</li>
<li>公开发布由FMSD-TTS生成的大规模藏语合成语音语料库。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3a712b79d7ffc4c1dca0f96958d2ba5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1785cc70a11836b2715822d27c917e0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a811d1d51e8b11dcb30516dfa2c64ac7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a42a77d797ad5aa3c3834fc192c0333d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a0dee36095191e6784bfb9b357a3309.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-MIND-for-Reasoning-Meta-learning-for-In-context-Deduction"><a href="#A-MIND-for-Reasoning-Meta-learning-for-In-context-Deduction" class="headerlink" title="A MIND for Reasoning: Meta-learning for In-context Deduction"></a>A MIND for Reasoning: Meta-learning for In-context Deduction</h2><p><strong>Authors:Leonardo Bertolazzi, Manuel Vargas Guzmán, Raffaella Bernardi, Maciej Malicki, Jakub Szymanik</strong></p>
<p>Large language models (LLMs) are increasingly evaluated on formal tasks, where strong reasoning abilities define the state of the art. However, their ability to generalize to out-of-distribution problems remains limited. In this paper, we investigate how LLMs can achieve a systematic understanding of deductive rules. Our focus is on the task of identifying the appropriate subset of premises within a knowledge base needed to derive a given hypothesis. To tackle this challenge, we propose Meta-learning for In-context Deduction (MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND is to enable models to generalize more effectively to unseen knowledge bases and to systematically apply inference rules. Our results show that MIND significantly improves generalization in small LMs ranging from 1.5B to 7B parameters. The benefits are especially pronounced in smaller models and low-data settings. Remarkably, small models fine-tuned with MIND outperform state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task. </p>
<blockquote>
<p>大型语言模型（LLM）在正式任务上的评估越来越多，其中强大的推理能力定义了前沿技术。然而，它们对于非分布问题的泛化能力仍然有限。在本文中，我们研究了LLM如何实现对演绎规则的系统理解。我们的重点是解决在知识库中识别用于推导给定假设所需前提的适当子集的任务。为了应对这一挑战，我们提出了用于上下文内演绎的元学习（MIND），这是一种新型的小样本元学习微调方法。MIND的目标是使模型能够更有效地泛化到未见过的知识库，并系统地应用推理规则。我们的结果表明，MIND显著提高了从1.5B到7B参数范围内的小型LM的泛化能力。特别是在小型模型和低数据设置下，其优势尤为突出。值得注意的是，使用MIND微调的小型模型在此任务上的表现优于最新的大型语言模型，如GPT-4o和o3-mini。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在正式任务上的表现日益受到重视，其强大的推理能力成为衡量技术发展水平的关键指标。然而，它们对于跨分布问题的泛化能力仍然有限。本文旨在探究如何让大型语言模型系统理解演绎规则。我们聚焦于识别知识库中用于推导给定假设所需的前提条件的任务。为解决这一难题，我们提出了名为 MIND（用于上下文内演绎的元学习）的新型元学习微调方法。MIND 的目标是让模型更有效地泛化到未见过的知识库，并系统地应用推理规则。结果显示，MIND 在小型语言模型上的泛化能力显著提升，尤其是参数较少的情况下。值得一提的是，采用 MIND 精细训练的小型模型在此任务上的表现优于现有的大型语言模型，如GPT-4o和o3-mini。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在正式任务上的推理能力成为衡量技术发展的关键指标。</li>
<li>大型语言模型的泛化能力对于跨分布问题仍然有限。</li>
<li>本文聚焦于大型语言模型对演绎规则的系统理解。</li>
<li>提出了一种新型的元学习微调方法MIND，用于解决识别知识库中推导给定假设所需前提条件的任务。</li>
<li>MIND提高了小型语言模型的泛化能力，特别是在参数较少的情况下。</li>
<li>采用MIND训练的小型模型在特定任务上的表现优于现有大型语言模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14313">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9c43c4f4fe9cb56474d4ec8609340548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c67fb438256b67575dd4c8aaea80bc85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0a0858d42483e3a8bbf1da6f9964a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36886669c30451fd961ec7fb1de712d4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Decoupling-Classifier-for-Boosting-Few-shot-Object-Detection-and-Instance-Segmentation"><a href="#Decoupling-Classifier-for-Boosting-Few-shot-Object-Detection-and-Instance-Segmentation" class="headerlink" title="Decoupling Classifier for Boosting Few-shot Object Detection and   Instance Segmentation"></a>Decoupling Classifier for Boosting Few-shot Object Detection and   Instance Segmentation</h2><p><strong>Authors:Bin-Bin Gao, Xiaochen Chen, Zhongyi Huang, Congchong Nie, Jun Liu, Jinxiang Lai, Guannan Jiang, Xi Wang, Chengjie Wang</strong></p>
<p>This paper focus on few-shot object detection<del>(FSOD) and instance segmentation</del>(FSIS), which requires a model to quickly adapt to novel classes with a few labeled instances. The existing methods severely suffer from bias classification because of the missing label issue which naturally exists in an instance-level few-shot scenario and is first formally proposed by us. Our analysis suggests that the standard classification head of most FSOD or FSIS models needs to be decoupled to mitigate the bias classification. Therefore, we propose an embarrassingly simple but effective method that decouples the standard classifier into two heads. Then, these two individual heads are capable of independently addressing clear positive samples and noisy negative samples which are caused by the missing label. In this way, the model can effectively learn novel classes while mitigating the effects of noisy negative samples. Without bells and whistles, our model without any additional computation cost and parameters consistently outperforms its baseline and state-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for FSOD and FSIS tasks. The Code is available at <a target="_blank" rel="noopener" href="https://csgaobb.github.io/Projects/DCFS">https://csgaobb.github.io/Projects/DCFS</a>. </p>
<blockquote>
<p>本文关注少样本目标检测（FSOD）和实例分割（FSIS），这需要模型能够快速地适应具有少量标记实例的新类别。由于实例级少样本场景中自然存在的缺失标签问题，现有方法遭受严重的偏置分类。我们首次正式提出这一问题。我们的分析表明，大多数FSOD或FSIS模型的标准分类头需要解耦，以减轻偏置分类。因此，我们提出了一种简单而有效的方法，将标准分类器解耦为两个头。然后，这两个独立的头能够独立地处理由缺失标签引起的清晰正样本和嘈杂负样本。通过这种方式，模型可以有效地学习新类别，同时减轻嘈杂负样本的影响。没有额外的计算和参数成本，我们的模型在FSOD和FSIS任务的PASCAL VOC和MS-COCO基准测试上始终优于其基线和其他最新技术。代码可在<a target="_blank" rel="noopener" href="https://csgaobb.github.io/Projects/DCFS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://csgaobb.github.io/Projects/DCFS上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14239v1">PDF</a> Accepted by NeurIPS 2022</p>
<p><strong>Summary</strong><br>该论文关注少样本目标检测（FSOD）和实例分割（FSIS），针对模型在新类别中快速适应的问题提出了解决方案。现有方法受到缺失标签导致的偏见分类的困扰，论文首次正式提出这一问题并分析了标准的分类头需要进行解耦以减少偏见分类。论文提出了一个简单而有效的方法来解耦标准分类器为两个头，能够独立处理清晰的阳性样本和缺失标签导致的噪声阴性样本。此方法不仅有效学习了新类别，还减少了噪声阴性样本的影响，在PASCAL VOC和MS-COCO的FSOD和FSIS任务基准测试中大幅度优于其他方法，且无额外的计算成本和参数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文聚焦于少样本目标检测和实例分割领域的问题。</li>
<li>模型在新类别中快速适应的问题是一大挑战。</li>
<li>现有方法受到缺失标签导致的偏见分类困扰。</li>
<li>论文提出了一个简单有效的解耦标准分类器的方法，将其分为两个头来处理不同的样本类型。</li>
<li>该方法能够减少噪声阴性样本的影响，提高模型学习效果。</li>
<li>在PASCAL VOC和MS-COCO的基准测试中，该方法显著优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14239">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-10d06bb2d86f8956c1a940a7a8592f88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be77d9da841f59a66d2a4308f1f4c44c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b4b0ef9e21ba757a0fce8ea667aa79.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Mechanistic-Fine-tuning-for-In-context-Learning"><a href="#Mechanistic-Fine-tuning-for-In-context-Learning" class="headerlink" title="Mechanistic Fine-tuning for In-context Learning"></a>Mechanistic Fine-tuning for In-context Learning</h2><p><strong>Authors:Hakaze Cho, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue</strong></p>
<p>In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability. </p>
<blockquote>
<p>上下文学习（ICL）利用结构化演示查询输入，以在语言模型（LMs）上实现少量学习，这些语言模型最初并未在ICL风格的数据上进行预训练。为了弥补ICL和预训练之间的差距，一些方法通过端到端的范式对LMs进行微调以适应大规模ICL风格的数据集，这产生了巨大的计算成本。为了降低这种成本，本文提出了注意力行为微调（ABFT），它利用对ICL内在机制的前期发现，在注意力分数上构建训练目标，而不是最终的输出。这迫使注意力分数关注上下文中出现的正确标签令牌，并减轻了对错误标签令牌的注意力分数。我们在9个现代LMs和8个数据集上的实验证实，ABFT在性能、稳健性、公正性和效率方面优于其他方法，与以前的方法相比，数据成本只有约0.01%。此外，我们的后续分析发现，端到端的训练目标包含ABFT目标，这表明ICL风格数据对归纳头的出现存在隐性偏见。我们的工作展示了控制LM内特定模块序列的可能性，以提高其行为表现，为未来的机械解释性应用开辟了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14233v1">PDF</a> 28 pages, 31 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>基于演示查询输入实现少数样本学习的语境学习（ICL）方法，通过调整语言模型（LM）来弥补其与预训练之间的差距。为降低成本，本文提出利用语境学习的内在机制，通过关注注意力分数构建训练目标，迫使注意力分数聚焦于正确标签词，同时削弱错误标签词的注意力分数的方法——注意力行为微调（ABFT）。实验证明，ABFT在性能、稳健性、公正性和效率方面均优于传统方法，数据成本仅约为传统方法的千分之一。同时分析显示，端到端的训练目标包含了ABFT目标，表明语境学习数据的隐性偏向归纳头的出现。本文展示了控制语言模型内部特定模块序列以改善其行为的可能性，为未来的机械解释性应用打开了大门。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICL利用结构化的演示查询输入实现少数样本学习。</li>
<li>ABFT方法通过关注注意力分数来构建训练目标，强化模型对正确标签的关注并削弱错误标签的影响。</li>
<li>ABFT在性能、稳健性、公正性和效率方面优于传统方法。</li>
<li>ABFT数据成本极低，约为传统方法的千分之一。</li>
<li>端到端的训练目标包含ABFT目标，显示出语境学习数据的隐性偏向。</li>
<li>本文揭示了控制语言模型内部特定模块序列的可能性，以提高其行为表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14233">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-dd0729679a1f6063c63564f15f0b9109.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6393779448043989c3454adc3393bd47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9eea7a49e56f072f3639588ae05415a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8dc36bbb22846b92c4e52b45edab364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a693ed1f9c24c0cb9c3e9eee75be3c3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unlocking-the-Power-of-SAM-2-for-Few-Shot-Segmentation"><a href="#Unlocking-the-Power-of-SAM-2-for-Few-Shot-Segmentation" class="headerlink" title="Unlocking the Power of SAM 2 for Few-Shot Segmentation"></a>Unlocking the Power of SAM 2 for Few-Shot Segmentation</h2><p><strong>Authors:Qianxiong Xu, Lanyun Zhu, Xuanyi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao</strong></p>
<p>Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few classes to segment arbitrary classes, but at the risk of overfitting. To address this, some methods use the well-learned knowledge of foundation models (e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM by supporting video segmentation, whose class-agnostic matching ability is useful to FSS. A simple idea is to encode support foreground (FG) features as memory, with which query FG features are matched and fused. Unfortunately, the FG objects in different frames of SAM 2’s video data are always the same identity, while those in FSS are different identities, i.e., the matching step is incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo query memory, matching with query features in a compatible way. However, the memories can never be as accurate as the real ones, i.e., they are likely to contain incomplete query FG, and some unexpected query background (BG) features, leading to wrong segmentation. Hence, we further design Iterative Memory Refinement to fuse more query FG features into the memory, and devise a Support-Calibrated Memory Attention to suppress the unexpected query BG features in memory. Extensive experiments have been conducted on PASCAL-5$^i$ and COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot mIoU can be 4.2% better than the best baseline. </p>
<blockquote>
<p>少样本分割（FSS）旨在学习对少数类别的类无关分割，以实现对任意类别的分割，但存在过拟合的风险。为了解决这一问题，一些方法使用基础模型的已学知识（例如SAM）来简化学习过程。最近，SAM 2通过支持视频分割扩展了SAM的应用，其类无关匹配能力对FSS很有用。一个简单的想法是将支持前景（FG）特征编码为内存，通过其与查询前景特征进行匹配和融合。然而，不幸的是，SAM 2的视频数据不同帧中的前景对象始终是同一身份，而FSS中的则是不同身份，即匹配步骤是不兼容的。因此，我们设计了伪提示生成器来编码伪查询内存，以与查询特征进行兼容匹配。然而，这些记忆无论如何都无法达到真实记忆的水平，即它们可能包含不完整查询前景和一些意外的查询背景（BG）特征，从而导致错误的分割。因此，我们进一步设计了迭代内存优化，将更多查询前景特征融合到内存中，并设计了一个支持校准的内存注意力来抑制内存中意外的查询背景特征。在PASCAL-5$^i$和COCO-20$^i$上进行了大量实验，验证了我们的设计效果，例如，我们的方法在1-shot mIoU上的表现比最佳基线提高了4.2%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14100v1">PDF</a> This paper is accepted by ICML’25</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了Few-Shot Segmentation（FSS）的目标是利用有限的类别数据进行类无关的分割学习。为了解决过拟合问题，一些方法利用基础模型的先验知识简化学习过程。最近，SAM 2扩展了SAM，支持视频分割，其类无关的匹配能力对FSS很有用。文本中提出了一种简单的方法，即编码支持前景特征作为内存，与查询前景特征进行匹配和融合。然而，SAM 2的视频数据中的前景对象身份始终相同，而FSS中的则不同，导致匹配步骤不兼容。因此，设计了伪提示生成器来编码伪查询内存，以兼容方式匹配查询特征。但记忆永远无法像真实记忆那样准确，可能包含不完整或意外的查询前景和背景特征，导致错误分割。因此，进一步设计了迭代内存细化，将更多查询前景特征融合到内存中，并设计了一种支持校准的内存注意力来抑制内存中的意外查询背景特征。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Segmentation (FSS) 旨在利用有限的类别数据进行类无关的分割学习，但存在过拟合的风险。</li>
<li>SAM 2扩展了SAM方法以支持视频分割，利用类无关的匹配能力。</li>
<li>提出了一种简单的方法，通过编码支持前景特征作为内存进行匹配和融合。</li>
<li>SAM 2与FSS在前景对象身份上存在不匹配问题。</li>
<li>为了解决此问题，设计了伪提示生成器和迭代内存细化方法。</li>
<li>伪提示生成器通过编码伪查询内存进行匹配。</li>
<li>通过广泛实验验证了设计的有效性，例如1-shot mIoU性能可提高4.2%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14100">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-896d4d256e255a0fa37d84b40ebad68d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55b5349ebeac0a93d3e773c4a1498977.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20cc4d224ada50805ed8aec4e78bd194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad2cd2d49b136b45c8885f9fb3e6224f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7676cfabb099634e3a7f26118c0fec09.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation"><a href="#CLEVER-A-Curated-Benchmark-for-Formally-Verified-Code-Generation" class="headerlink" title="CLEVER: A Curated Benchmark for Formally Verified Code Generation"></a>CLEVER: A Curated Benchmark for Formally Verified Code Generation</h2><p><strong>Authors:Amitayush Thakur, Jasper Lee, George Tsoukalas, Meghana Sistla, Matthew Zhao, Stefan Zetzche, Greg Durrett, Yisong Yue, Swarat Chaudhuri</strong></p>
<p>We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Lean’s type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever">https://github.com/trishullab/clever</a>) as well as HuggingFace(<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/amitayusht/clever">https://huggingface.co/datasets/amitayusht/clever</a>). All our evaluation code is also available online(<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever-prover">https://github.com/trishullab/clever-prover</a>). </p>
<blockquote>
<p>我们介绍了${\rm C{\small LEVER}}$，这是一个高质量的、经过筛选的包含161个问题的基准测试，用于端到端的Lean代码生成验证。每个问题由两部分组成：（1）生成与保留的真实规格相匹配的规格的任务；（2）生成能够证明满足此规格要求的Lean实现的任务。不同于以前的基准测试，${\rm C{\small LEVER}}$避免了测试用例的监督、大型语言模型生成的注释以及泄露实现逻辑或允许无效解决方案的规格。所有输出都利用Lean的类型检查器进行事后验证，以确保机器可检查的正确性。我们使用${\rm C{\small LEVER}}$来评估基于最新语言模型的几种少样本和智能方法。这些方法都很难实现完全验证，从而使其成为程序合成和形式推理具有挑战性的前沿基准测试。我们的基准测试可以在GitHub（<a target="_blank" rel="noopener" href="https://github.com/trishullab/clever%EF%BC%89%E4%BB%A5%E5%8F%8AHuggingFace%EF%BC%88https://huggingface.co/datasets/amitayusht/clever%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E8%AF%84%E4%BC%B0%E4%BB%A3%E7%A0%81%E4%B9%9F%E5%9C%A8%E7%BA%BF%E5%8F%AF%E7%94%A8%EF%BC%88https://github.com/trishullab/clever-prover%EF%BC%89%E3%80%82">https://github.com/trishullab/clever）以及HuggingFace（https://huggingface.co/datasets/amitayusht/clever）上找到。我们的评估代码也在线可用（https://github.com/trishullab/clever-prover）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13938v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在中国研究团队推出的高质量基准测试集CLEVER中，共有包含端到端验证的代码生成任务共计有涵盖的161个挑战性问题。此基准测试集强调避免测试案例监督、大型语言模型生成的注释以及泄露实现逻辑或允许空洞解决方案的问题。所有输出均通过Lean的类型检查器进行事后验证，以确保机器检查正确性。针对基于当前技术水平的语言模型的少量研究和人工智能的方法进行评价。这个挑战极具标杆性。在此基础上有许多验证了人类缺乏一种可用的编码环境供交流的技术评价平台的信息系统已诞生，但在业界许多已公开的实践实例面前我们面临极大挑战，这项成果能够在GitHub、HuggingFace等平台上找到。同时，所有的评估代码也已在线发布。这为人工智能在程序合成和形式推理领域的发展提供了强有力的支持。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>${\rm C{\small LEVER}}$ 是一个用于端到端验证的代码生成的高质量的基准测试集。具有确保机器检查正确性的特性。它包含涵盖的涵盖多个领域的挑战性问题，旨在评估语言模型的能力。</li>
<li>${\rm C{\small LEVER}}$强调真实性和难度级别高的问题设置，旨在避免简单任务和对大型语言模型的依赖性问题进行监督学习的方式设计任务内容的问题类型问题要求不同技术特征的解题方法数量之间有良好的平衡等问题以考察通用性能算法方面：规避漏洞的问题；进行输入时学习注释的程序证明技术以改进语言模型生成的输出；解决复杂性和随机性问题；确保算法质量水平在通过编程实践实例方面能够比肩人工智能水平的超越标准的方法中名列前茅等难点，展现了它的独特性。这是中国研究团队的创新性成果。随着科技的不断发展，人们对于编程语言和计算机的理解不断加深，其前景将会越来越广阔。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-175e9300620f6ffeeb08c92d2d5a6af7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf4bb3beeb9a906f276484cb351bd82c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-059b0011d1e2a8174241dab92289ec5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e1a8461994df8449d3c7925819fe90e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92dbad38362c5c981f79f4e240389835.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Physics-Driven-Local-Whole-Elastic-Deformation-Modeling-for-Point-Cloud-Representation-Learning"><a href="#Physics-Driven-Local-Whole-Elastic-Deformation-Modeling-for-Point-Cloud-Representation-Learning" class="headerlink" title="Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud   Representation Learning"></a>Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud   Representation Learning</h2><p><strong>Authors:Zhongyu Chen, Rong Zhao, Xie Han, Xindong Guo, Song Wang, Zherui Qiao</strong></p>
<p>Existing point cloud representation learning tend to learning the geometric distribution of objects through data-driven approaches, emphasizing structural features while overlooking the relationship between the local information and the whole structure. Local features reflect the fine-grained variations of an object, while the whole structure is determined by the interaction and combination of these local features, collectively defining the object’s shape. In real-world, objects undergo elastic deformation under external forces, and this deformation gradually affects the whole structure through the propagation of forces from local regions, thereby altering the object’s geometric properties. Inspired by this, we propose a physics-driven self-supervised learning method for point cloud representation, which captures the relationship between parts and the whole by constructing a local-whole force propagation mechanism. Specifically, we employ a dual-task encoder-decoder framework, integrating the geometric modeling capability of implicit fields with physics-driven elastic deformation. The encoder extracts features from the point cloud and its tetrahedral mesh representation, capturing both geometric and physical properties. These features are then fed into two decoders: one learns the whole geometric shape of the point cloud through an implicit field, while the other predicts local deformations using two specifically designed physics information loss functions, modeling the deformation relationship between local and whole shapes. Experimental results show that our method outperforms existing approaches in object classification, few-shot learning, and segmentation, demonstrating its effectiveness. </p>
<blockquote>
<p>现有的点云表示学习方法往往通过数据驱动的方法学习对象的几何分布，强调结构特征，但忽视了局部信息与整体结构之间的关系。局部特征反映了对象的细微变化，而整体结构则由这些局部特征的相互作用和组合决定，共同定义了对象的形状。在现实中，物体在外部力的作用下会发生弹性变形，这种变形通过从局部区域传播的力量逐渐影响整体结构，从而改变物体的几何属性。受此启发，我们提出了一种用于点云表示的物理驱动自监督学习方法，通过建立局部-整体力传播机制来捕捉部分与整体之间的关系。具体而言，我们采用双任务编码器-解码器框架，将隐场的几何建模能力与物理驱动的弹性变形相结合。编码器从点云及其四面体网格表示中提取特征，捕捉几何和物理属性。这些特征然后输入到两个解码器中：一个通过隐场学习点云的整体几何形状，另一个使用两个专门设计的物理信息损失函数来预测局部变形，建模局部和整体形状之间的变形关系。实验结果表明，我们的方法在目标分类、小样本学习和分割方面优于现有方法，证明了其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13812v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个基于物理驱动的自我监督学习方法，用于点云表示。该方法通过构建局部与整体的力传播机制，捕捉部分与整体之间的关系，并强调在外部力作用下物体的弹性变形如何影响整体结构。实验结果表明，该方法在物体分类、少样本学习和分割方面优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有点云表示学习方法多通过数据驱动方法学习物体的几何分布，但忽略了局部信息与整体结构之间的关系。</li>
<li>局部特征反映物体的细微变化，而整体结构由这些局部特征的相互作用和组合决定，共同定义物体的形状。</li>
<li>物体在外部力作用下会发生弹性变形，这种变形通过从局部区域传播的力逐渐影响整体结构，改变物体的几何特性。</li>
<li>本文提出了一个基于物理驱动的自我监督学习方法，通过构建局部与整体的力传播机制，捕捉点云表示中的部分与整体关系。</li>
<li>该方法结合了点云和其四面体网格表示的几何建模能力与物理驱动的弹性变形。</li>
<li>实验结果表明，该方法在物体分类、少样本学习和分割任务上表现出优异性能。</li>
<li>该方法采用双任务编码器-解码器框架，其中编码器提取点云及其四面体网格表示的特征，解码器则用于学习整个几何形状并预测局部变形。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13812">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7964b2debc9adf20fd660a685fa7ceda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a28aa57c3402d56725cf9dc572a65d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b28b03e73de9436ed2a21996a1a40eb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc02514e3adb621bb9d774889cfa3043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bafb08a84402263e93dd939fc410eae.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MAFA-A-multi-agent-framework-for-annotation"><a href="#MAFA-A-multi-agent-framework-for-annotation" class="headerlink" title="MAFA: A multi-agent framework for annotation"></a>MAFA: A multi-agent framework for annotation</h2><p><strong>Authors:Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem</strong></p>
<p>Modern applications require accurate and efficient retrieval of information in response to user queries. Mapping user utterances to the most relevant Frequently Asked Questions (FAQs) is a crucial component of these systems. Traditional approaches often rely on a single model or technique, which may not capture the nuances of diverse user inquiries. In this paper, we introduce a multi-agent framework for FAQ annotation that combines multiple specialized agents with different approaches and a judge agent that reranks candidates to produce optimal results. Our agents utilize a structured reasoning approach inspired by Attentive Reasoning Queries (ARQs), which guides them through systematic reasoning steps using targeted, task-specific JSON queries. Our framework features a specialized few-shot example strategy, where each agent receives different few-shots, enhancing ensemble diversity and coverage of the query space. We evaluate our framework on a real-world banking dataset as well as public benchmark datasets (LCQMC and FiQA), demonstrating significant improvements over single-agent approaches across multiple metrics, including a 14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12% improvement in Mean Reciprocal Rank on our dataset, and similar gains on public benchmarks when compared with traditional single agent annotation techniques. Our framework is particularly effective at handling ambiguous queries, making it well-suited for deployment in production applications while showing strong generalization capabilities across different domains and languages. </p>
<blockquote>
<p>现代应用程序需要准确高效地检索信息以响应用户查询。将用户的话语映射到最相关的常见问题解答（FAQs）是这些系统的关键组成部分。传统方法通常依赖于单个模型或技术，这可能无法捕捉到各种用户查询的细微差别。在本文中，我们介绍了一种结合多个专业代理和法官代理进行重新排名的多智能体框架，用于FAQ注释，以产生最佳结果。我们的代理采用受关注推理查询（ARQ）启发的结构化推理方法，通过使用有针对性的任务特定JSON查询来指导他们进行系统化的推理步骤。我们的框架采用专门的少量示例策略，每个代理接收不同的少量示例，增强了组合多样性和查询空间的覆盖。我们在现实世界中的银行数据集以及公共基准数据集（LCQMC和FiQA）上评估了我们的框架，与单智能体方法相比，在多个指标上取得了显著改进，包括第一名准确率提高14%，前五名准确率提高18%，以及在我们的数据集上的平均倒数排名提高12%，与传统单智能体注释技术相比，在公共基准测试上也有类似的收益。我们的框架在处理模糊查询方面特别有效，因此非常适合在生产应用程序中部署，并在不同领域和语言方面表现出强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13668v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种多智能体框架用于FAQ标注，结合了多个采用不同方法的智能体和一个法官智能体进行候选重新排序以产生最佳结果。该框架利用结构化推理方法，通过任务特定的JSON查询指导智能体进行系统性推理步骤。此外，该框架具有专门用于快速示例的策略，每个智能体接收不同的快速示例，增强了组合多样性和查询空间覆盖率。评估表明，该框架在真实银行数据集和公共基准数据集上的表现优于单智能体方法，并在多个指标上取得了显著改进。该框架特别擅长处理模糊查询，适合在生产应用中部署，并在不同领域和语言中显示出强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文提出了一种多智能体框架用于FAQ标注，旨在改进传统的单一模型或技术方法的不足。</li>
<li>多智能体框架结合了多种不同的方法和技术，并包括一个法官智能体进行候选重新排序。</li>
<li>该框架采用结构化推理方法，通过任务特定的JSON查询指导智能体的推理过程。</li>
<li>框架具有专门用于快速示例的策略，增强了智能体的组合多样性和查询空间覆盖率。</li>
<li>在真实银行数据集和公共基准数据集上的评估表明，该框架优于单智能体方法，并在多个指标上取得了显著改进。</li>
<li>该框架特别擅长处理模糊查询，适应生产环境中的复杂需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13668">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1afc039d07417c6177bfe826a6a736cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a23d978281ffa9d1cabf222b20dad01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-293988a3782880b15ccbc18e13fc0357.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Spectral-Spatial-Self-Supervised-Learning-for-Few-Shot-Hyperspectral-Image-Classification"><a href="#Spectral-Spatial-Self-Supervised-Learning-for-Few-Shot-Hyperspectral-Image-Classification" class="headerlink" title="Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral   Image Classification"></a>Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral   Image Classification</h2><p><strong>Authors:Wenchen Chen, Yanmei Zhang, Zhongwei Xiao, Jianping Chu, Xingbo Wang</strong></p>
<p>Few-shot classification of hyperspectral images (HSI) faces the challenge of scarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning (FSL) offer promising avenues to address this issue. However, existing methods often struggle to adapt to the spatial geometric diversity of HSIs and lack sufficient spectral prior knowledge. To tackle these challenges, we propose a method, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification (S4L-FSC), aimed at improving the performance of few-shot HSI classification. Specifically, we first leverage heterogeneous datasets to pretrain a spatial feature extractor using a designed Rotation-Mirror Self-Supervised Learning (RM-SSL) method, combined with FSL. This approach enables the model to learn the spatial geometric diversity of HSIs using rotation and mirroring labels as supervisory signals, while acquiring transferable spatial meta-knowledge through few-shot learning. Subsequently, homogeneous datasets are utilized to pretrain a spectral feature extractor via a combination of FSL and Masked Reconstruction Self-Supervised Learning (MR-SSL). The model learns to reconstruct original spectral information from randomly masked spectral vectors, inferring spectral dependencies. In parallel, FSL guides the model to extract pixel-level discriminative features, thereby embedding rich spectral priors into the model. This spectral-spatial pretraining method, along with the integration of knowledge from heterogeneous and homogeneous sources, significantly enhances model performance. Extensive experiments on four HSI datasets demonstrate the effectiveness and superiority of the proposed S4L-FSC approach for few-shot HSI classification. </p>
<blockquote>
<p>高光谱图像（HSI）的少量样本分类面临着标注样本稀缺的挑战。自监督学习（SSL）和少量学习（FSL）为解决这一问题提供了有前景的途径。然而，现有方法往往难以适应HSI的空间几何多样性，且缺乏足够的光谱先验知识。为了应对这些挑战，我们提出了一种方法，名为“用于高光谱图像少量样本分类的谱空间自监督学习（S4L-FSC）”，旨在提高高光谱图像少量样本分类的性能。具体来说，我们首先利用异质数据集预训练一个空间特征提取器，采用设计的旋转镜像自监督学习方法（RM-SSL）与少量学习相结合。这种方法使模型能够利用旋转和镜像标签作为监督信号来学习HSI的空间几何多样性，同时通过少量学习获得可迁移的空间元知识。然后，我们利用同质数据集预训练一个光谱特征提取器，结合少量学习和掩码重建自监督学习（MR-SSL）。模型学会从随机掩码的光谱向量中重建原始光谱信息，推断光谱依赖性。同时，少量学习指导模型提取像素级判别特征，从而将丰富的光谱先验嵌入模型中。这种谱空间预训练方法，以及来自异质和同质源的知识的整合，显著提高了模型的性能。在四个HSI数据集上的广泛实验证明了所提出的S4L-FSC方法在少量高光谱图像分类中的有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12482v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/Wenchen-Chen/S4L-FSC">https://github.com/Wenchen-Chen/S4L-FSC</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了针对高光谱图像（HSI）的少量样本分类问题，通过结合自监督学习（SSL）和少量学习（FSL）来解决空间几何多样性和光谱先验知识的不足问题。文中介绍了一种名为Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification（S4L-FSC）的方法，它通过预训练模型进行空间特征和光谱特征的提取，使用RM-SSL方法和掩蔽重建自监督学习来训练模型。经过大量实验验证，该方法在四个HSI数据集上的少量样本分类表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>面临高光谱图像（HSI）少量样本分类的挑战。</li>
<li>自监督学习（SSL）和少量学习（FSL）是解决该问题有前景的方法。</li>
<li>SSL通过设计Rotation-Mirror方法进行空间特征提取。借助少量学习，模型能够学习空间几何多样性并使用旋转和镜像标签作为监督信号。</li>
<li>使用同质数据集进行光谱特征提取，通过掩蔽重建自监督学习和少量学习相结合的方式训练模型，从而推断光谱依赖关系并提取像素级别的判别特征。</li>
<li>结合异质和同质来源的知识，通过谱空间预训练方法显著提高了模型性能。</li>
<li>在四个HSI数据集上的实验验证了该方法的优越性和有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b45d0f7be98461bb8e998f0f17592fdc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ee7c2987ba34ce7008baa3016f5b58d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40f9faf91573c222123c7276c67314cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a14ab6e63668a27e64b7ae982767cef1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a635e0b2674fa5f87e1332a56bf7fe7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HyPerAlign-Interpretable-Personalized-LLM-Alignment-via-Hypothesis-Generation"><a href="#HyPerAlign-Interpretable-Personalized-LLM-Alignment-via-Hypothesis-Generation" class="headerlink" title="HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis   Generation"></a>HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis   Generation</h2><p><strong>Authors:Cristina Garbacea, Chenhao Tan</strong></p>
<p>Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the &#96;&#96;average-user’’ preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users. We aim to generate customized responses tailored to specific individuals instead of generic outputs that emulate the collective voices of diverse populations. We propose HyPerAlign, an interpretable and sample-efficient hypothesis-driven personalization approach for LLM models. Given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality, and writing style, then prompt LLM models with these hypotheses and user-specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, namely authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks). Results demonstrate the superiority of hypothesis-driven LLM personalization compared to preference-based fine-tuning methods. For authorship attribution, HyPerAlign generations have consistently high win-rates (commonly $&gt; 90%$) against state-of-the-art preference fine-tuning approaches across diverse user profiles and LLM models. For deliberative alignment, the helpfulness of LLM models is improved by up to $70%$ on average. Overall, HyPerAlign represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users. </p>
<blockquote>
<p>对齐算法广泛应用于基于偏好注释的大型语言模型（LLM）与人类用户的对齐。通常这些（经常不同的）偏好会在不同的用户群体中进行汇总，从而得到与“平均用户”偏好对齐的微调模型。然而，当前模型是在特定用户和情境下使用的，这强调了对用户依赖的偏好控制的必要性。在这项工作中，我们解决了将LLM输出个性化到其用户的问题。我们的目标是生成针对特定个人的定制响应，而不是模拟不同人群集体声音的通用输出。我们提出了HyPerAlign，这是一种可解释且样本效率高的假设驱动式LLM个性化方法。给定特定用户编写的少数样本，我们首先对其沟通策略、个性和写作风格进行假设推断，然后提示LLM模型使用这些假设和用户特定属性来生成定制输出。我们在两个不同的个性化任务上进行了实验，即作者归属和审慎对齐，并使用来自不同领域的数据集（新闻文章、博客文章、电子邮件、越狱基准测试）。结果表明，假设驱动的LLM个性化方法优于基于偏好的微调方法。在作者归属方面，HyPerAlign生成的响应与最新偏好微调方法相比，具有始终较高的胜率（通常超过90％），涵盖各种用户配置文件和LLM模型。在审慎对齐方面，LLM模型的有用性平均提高了高达70％。总体而言，HyPerAlign代表了可解释且样本效率高的策略，用于将LLM模型个性化到单个用户。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00038v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为HyPerAlign的方法，用于个性化大型语言模型（LLM）的输出以适应特定用户。通过少量用户写作的样本，推断用户的沟通策略、个性和写作风格，然后结合这些假设和用户特定属性，生成定制化的输出。实验结果表明，在作者归属和审慎对齐等任务上，HyPerAlign的表现优于基于偏好微调的方法。总体而言，HyPerAlign是一种可解释性强、样本效率高的个性化LLM模型策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HyPerAlign是一种用于个性化大型语言模型（LLM）的方法，旨在生成针对特定用户的定制响应。</li>
<li>该方法通过少量用户写作样本推断用户的沟通策略、个性和写作风格。</li>
<li>HyPerAlign结合这些假设和用户特定属性，生成适应特定用户的输出。</li>
<li>在作者归属任务上，HyPerAlign的生成结果胜过现有偏好微调方法，胜率高且稳定。</li>
<li>在审慎对齐任务上，LLM模型的实用性平均提高了70%。</li>
<li>HyPerAlign具有可解释性和样本高效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00038">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2cb7e5750ac8d27335ceaf8b8bb36679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24535d2d312f7d503107b8a678d84def.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-972b08e611345537f74e86faf9181e59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72796667f586a441c2fc20ba92815597.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LogicQA-Logical-Anomaly-Detection-with-Vision-Language-Model-Generated-Questions"><a href="#LogicQA-Logical-Anomaly-Detection-with-Vision-Language-Model-Generated-Questions" class="headerlink" title="LogicQA: Logical Anomaly Detection with Vision Language Model Generated   Questions"></a>LogicQA: Logical Anomaly Detection with Vision Language Model Generated   Questions</h2><p><strong>Authors:Yejin Kwon, Daeun Moon, Youngje Oh, Hyunsoo Yoon</strong></p>
<p>Anomaly Detection (AD) focuses on detecting samples that differ from the standard pattern, making it a vital tool in process control. Logical anomalies may appear visually normal yet violate predefined constraints on object presence, arrangement, or quantity, depending on reasoning and explainability. We introduce LogicQA, a framework that enhances AD by providing industrial operators with explanations for logical anomalies. LogicQA compiles automatically generated questions into a checklist and collects responses to identify violations of logical constraints. LogicQA is training-free, annotation-free, and operates in a few-shot setting. We achieve state-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO AD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the explanations of anomalies. Also, our approach has shown outstanding performance on semiconductor SEM corporate data, further validating its effectiveness in industrial applications. </p>
<blockquote>
<p>异常检测（AD）主要专注于检测与标准模式不同的样本，使其成为过程控制中的关键工具。逻辑异常在视觉上可能看似正常，但会违反对象存在、排列或数量等方面的预定义约束，这取决于推理和可解释性。我们引入了LogicQA框架，它通过为工业操作员提供逻辑异常的解释来增强AD的功能。LogicQA将自动生成的问题编译成清单，并收集响应来识别逻辑约束的违反情况。LogicQA无需训练和标注，且在少量样本数据下即可运行。我们在公共基准测试MVTec LOCO AD上实现了最新的逻辑AD性能，具有87.6％的AUROC和87.0％的F1最大值，并提供了异常情况的解释。此外，我们的方法在半导体的SEM企业数据上表现出卓越的性能，进一步验证了其在工业应用中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20252v2">PDF</a> Accepted Industry Track at ACL 2025</p>
<p><strong>Summary</strong><br>基于逻辑异常检测的框架LogicQA，旨在提高工业运营者对逻辑异常的识别能力。它通过自动生成问题清单并收集回应，以识别逻辑约束的违规情况。该框架无需训练和标注，在少量样本下运行，并在MVTec LOCO AD公共基准测试上实现了先进的逻辑异常检测性能，包括高曲线下面积（AUROC）和最大F1分数。此外，该框架在半导体SEM企业数据上表现出卓越性能，证明了其在工业应用中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LogicQA是一个用于增强异常检测的框架，可解释逻辑异常。</li>
<li>LogicQA通过自动生成问题清单来识别逻辑约束违规情况。</li>
<li>该框架无需训练和标注，适用于少量样本。</li>
<li>LogicQA在MVTec LOCO AD公共基准测试上表现出卓越性能。</li>
<li>LogicQA实现了高AUROC和F1-max分数。</li>
<li>该框架在半导体SEM企业数据上的表现证明了其在工业应用中的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20252">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fa13c28bfe8b85794334765e811a4ccb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a84f1eadb30a499a5e68d4dae8a043b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b35b3e797d30a70be5b06e152c338dcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9069c06875e78034da4f7c6c37b67de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ddcb731fe555b7ecf8c0e12a1accf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abeaab0cecbcc26b32aeea9ae6b8e577.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Generalized-Few-shot-3D-Point-Cloud-Segmentation-with-Vision-Language-Model"><a href="#Generalized-Few-shot-3D-Point-Cloud-Segmentation-with-Vision-Language-Model" class="headerlink" title="Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language   Model"></a>Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language   Model</h2><p><strong>Authors:Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Junlin Han, Ender Konukoglu, Serge Belongie</strong></p>
<p>Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world novel classes, contain rich but noisy novel class knowledge. In this work, we introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design a novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide a solid foundation for advancing GFS-PCS in the real world. The code is at <a target="_blank" rel="noopener" href="https://github.com/ZhaochongAn/GFS-VL">https://github.com/ZhaochongAn/GFS-VL</a> </p>
<blockquote>
<p>广义少样本3D点云分割（GFS-PCS）能够在保留基础类别分割的同时，适应新的类别并处理少量的样本支持。现有的GFS-PCS方法通过支持特征或查询特征与原型进行交互，但仍受限于少样本的稀疏知识。同时，用于开放世界新类别的通用3D视觉语言模型（3D VLMs）包含丰富但嘈杂的新类别知识。在这项工作中，我们引入了一个GFS-PCS框架，该框架协同结合了来自3D VLMs的密集但嘈杂的伪标签和精确但稀疏的少量样本，以最大限度地发挥两者的优势，称为GFS-VL。具体来说，我们提出了一种原型引导伪标签选择方法，用于过滤低质量区域，随后采用自适应填充策略，结合伪标签上下文和少量样本知识，对过滤后的未标记区域进行自适应标记。此外，我们设计了一种新型基础混合策略，将少量样本嵌入训练场景，保留重要上下文，以改进新类别的学习。而且，考虑到当前GFS-PCS基准测试多样性的局限性，我们引入了两个具有多种新类别的挑战性基准测试，以进行全面的一般化评估。实验验证了我们的框架在不同模型和数据集上的有效性。我们的方法和基准测试为推进GFS-PCS在现实世界的应用提供了坚实的基础。代码地址为：<a target="_blank" rel="noopener" href="https://github.com/ZhaochongAn/GFS-VL">https://github.com/ZhaochongAn/GFS-VL</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16282v2">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种结合了3D视觉语言模型（3D VLMs）和广义少样本3D点云分割（GFS-PCS）框架的方法，称为GFS-VL。该方法利用3D VLMs提供的丰富但带有噪声的伪标签与少样本数据相结合，通过原型引导伪标签选择和自适应填充策略，最大限度地发挥两者的优势。此外，还引入了一种新型基础混合策略，将少量样本嵌入训练场景中，保留重要上下文，以改进新型类的学习。文章还介绍了两个具有挑战性的基准测试，以全面评估模型的泛化能力。实验验证了该方法在不同模型和数据集上的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GFS-VL结合3D VLMs和GFS-PCS框架，利用伪标签和少样本数据进行学习。</li>
<li>提出原型引导伪标签选择策略，过滤低质量区域。</li>
<li>采用自适应填充策略，结合伪标签上下文和少量样本进行标注。</li>
<li>设计新型基础混合策略，将少量样本嵌入训练场景，保留重要上下文。</li>
<li>引入两个具有挑战性的基准测试，用于评估模型的泛化能力。</li>
<li>实验验证了该方法在不同模型和数据集上的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d40761f96ec5cd845fb0566592fc0f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c1249e2e6bb7508adca5e077f48f3d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0605d2dc9fc4a10ad1374de047600aa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb68326f2c93c82c51a6dd449804308.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Conjuring-Positive-Pairs-for-Efficient-Unification-of-Representation-Learning-and-Image-Synthesis"><a href="#Conjuring-Positive-Pairs-for-Efficient-Unification-of-Representation-Learning-and-Image-Synthesis" class="headerlink" title="Conjuring Positive Pairs for Efficient Unification of Representation   Learning and Image Synthesis"></a>Conjuring Positive Pairs for Efficient Unification of Representation   Learning and Image Synthesis</h2><p><strong>Authors:Imanol G. Estepa, Jesús M. Rodríguez-de-Vera, Ignacio Sarasúa, Bhalaji Nagarajan, Petia Radeva</strong></p>
<p>While representation learning and generative modeling seek to understand visual data, unifying both domains remains unexplored. Recent Unified Self-Supervised Learning (SSL) methods have started to bridge the gap between both paradigms. However, they rely solely on semantic token reconstruction, which requires an external tokenizer during training – introducing a significant overhead. In this work, we introduce Sorcen, a novel unified SSL framework, incorporating a synergic Contrastive-Reconstruction objective. Our Contrastive objective, “Echo Contrast”, leverages the generative capabilities of Sorcen, eliminating the need for additional image crops or augmentations during training. Sorcen “generates” an echo sample in the semantic token space, forming the contrastive positive pair. Sorcen operates exclusively on precomputed tokens, eliminating the need for an online token transformation during training, thereby significantly reducing computational overhead. Extensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the previous Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear probing, unconditional image generation, few-shot learning, and transfer learning, respectively, while being 60.8% more efficient. Additionally, Sorcen surpasses previous single-crop MIM SoTA in linear probing and achieves SoTA performance in unconditional image generation, highlighting significant improvements and breakthroughs in Unified SSL models. </p>
<blockquote>
<p>表示学习和生成建模都在努力理解视觉数据，但统一这两个领域仍然未被探索。最近的统一自监督学习方法（SSL）已经开始弥合两种范式之间的鸿沟。然而，它们仅依赖于语义令牌重建，这需要训练过程中的外部令牌器——带来了相当大的开销。在这项工作中，我们介绍了Sorcen，这是一个新的统一SSL框架，它结合了协同对比重建目标。我们的对比目标“回声对比”利用了Sorcen的生成能力，消除了训练过程中对额外图像裁剪或增强的需求。Sorcen在语义令牌空间中“生成”一个回声样本，形成对比正对。Sorcen只在预计算令牌上运行，消除了训练过程中在线令牌转换的需要，从而显著降低了计算开销。在ImageNet-1k上的大量实验表明，在线性探测、无条件图像生成、小样本学习和迁移学习方面，Sorcen分别超过了之前的统一SSL水平0.4%、FID降低1.48、下降1.76%、下降率为至最低至最多百分之十五，而效率提高至最多百分之六十以上八提升效率时则是大幅提升。此外，在无条件图像生成方面，Sorcen超越了之前的单裁剪MIM水平最高表现，并在统一SSL模型中取得了显著的改进和突破。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15060v3">PDF</a> The source code is available in <a target="_blank" rel="noopener" href="https://github.com/ImaGonEs/Sorcen">https://github.com/ImaGonEs/Sorcen</a></p>
<p><strong>Summary</strong><br>     本研究提出了一种新型的统一自监督学习（SSL）框架Sorcen，它结合了对比和重建目标，无需额外的图像裁剪或增强即可进行训练。Sorcen在预计算令牌上运行，降低了计算开销。在ImageNet-1k上的实验表明，Sorcen在统一SSL领域取得了最先进的性能，同时在效率和效果上也有所突破。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了新型统一自监督学习（SSL）框架Sorcen。</li>
<li>Sorcen结合了对比和重建目标，提高了模型的效率和效果。</li>
<li>Sorcen利用生成能力生成对比正样本对，无需额外的图像裁剪或增强。</li>
<li>Sorcen在预计算令牌上运行，降低了计算开销。</li>
<li>在ImageNet-1k上的实验显示，Sorcen在统一SSL领域实现了最先进的性能。</li>
<li>Sorcen在多种任务上表现出卓越性能，包括线性探测、无条件图像生成、少样本学习和迁移学习等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15060">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-51c375b6d1aa15a2bc3a86d290d7019c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c0439c7a81af2941727063a1f161805.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e789df577b419d5a45db630edef0eaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a0eeba2eca04982030c2592cc91ec63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ef503478314bf60bdd111b2bf54492e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="STD-PLM-Understanding-Both-Spatial-and-Temporal-Properties-of-Spatial-Temporal-Data-with-PLM"><a href="#STD-PLM-Understanding-Both-Spatial-and-Temporal-Properties-of-Spatial-Temporal-Data-with-PLM" class="headerlink" title="STD-PLM: Understanding Both Spatial and Temporal Properties of   Spatial-Temporal Data with PLM"></a>STD-PLM: Understanding Both Spatial and Temporal Properties of   Spatial-Temporal Data with PLM</h2><p><strong>Authors:YiHeng Huang, Xiaowei Mao, Shengnan Guo, Yubin Chen, Junfeng Shen, Tiankuo Li, Youfang Lin, Huaiyu Wan</strong></p>
<p>Spatial-temporal forecasting and imputation are important for real-world intelligent systems. Most existing methods are tailored for individual forecasting or imputation tasks but are not designed for both. Additionally, they are less effective for zero-shot and few-shot learning. While pre-trained language model (PLM) have exhibited strong pattern recognition and reasoning abilities across various tasks, including few-shot and zero-shot learning, their applications in spatial-temporal data understanding has been constrained by insufficient modeling of complex correlations such as the temporal correlations, spatial connectivity, non-pairwise and high-order spatial-temporal correlations within data. In this paper, we propose STD-PLM for understanding both spatial and temporal properties of \underline{S}patial-\underline{T}emporal \underline{D}ata with \underline{PLM}, which is capable of implementing both spatial-temporal forecasting and imputation tasks. STD-PLM understands spatial-temporal correlations via explicitly designed spatial and temporal tokenizers. Topology-aware node embeddings are designed for PLM to comprehend and exploit the topology structure of data in inductive manner. Furthermore, to mitigate the efficiency issues introduced by the PLM, we design a sandglass attention module (SGA) combined with a specific constrained loss function, which significantly improves the model’s efficiency while ensuring performance. Extensive experiments demonstrate that STD-PLM exhibits competitive performance and generalization capabilities across the forecasting and imputation tasks on various datasets. Moreover, STD-PLM achieves promising results on both few-shot and zero-shot tasks. The code is made available at \href{<a target="_blank" rel="noopener" href="https://github.com/Hyheng/STD-PLM%7D%7Bhttps://github.com/Hyheng/STD-PLM%7D">https://github.com/Hyheng/STD-PLM}{https://github.com/Hyheng/STD-PLM}</a> </p>
<blockquote>
<p>空间时间预测和补全对于现实世界的智能系统非常重要。现有的大多数方法都是针对个别预测或补全任务定制的，并不适用于两者同时进行。此外，它们在零样本和少样本学习方面的效果较差。虽然预训练语言模型（PLM）在各种任务中表现出了强大的模式识别和推理能力，包括少样本和零样本学习，但其在空间时间数据理解方面的应用受到了建模复杂关系不足的制约，如时间相关性、空间连通性、数据内部非配对和高阶时空关系等。在本文中，我们提出了基于PLM的STD-PLM模型，用于理解时空数据的空间和时间属性，并具备进行时空预测和补全任务的能力。STD-PLM通过明确设计的空间和时间标记器理解时空相关性。针对PLM设计了拓扑感知节点嵌入，以归纳方式理解和利用数据的拓扑结构。此外，为了解决由PLM引入的效率问题，我们设计了一种沙漏注意模块（SGA）结合特定的约束损失函数，这不仅显著提高了模型的效率，而且保证了性能。大量实验表明，STD-PLM在多个数据集上的预测和补全任务上表现出具有竞争力的性能和泛化能力。此外，STD-PLM在少样本和零样本任务上也取得了令人鼓舞的结果。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/Hyheng/STD-PLM">https://github.com/Hyheng/STD-PLM</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.09096v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于预训练语言模型（PLM），本文提出了一种针对空间和时间数据的理解方法STD-PLM。该方法能够执行空间时间预测和插值任务，通过明确设计的空间和时间标记器理解时空相关性。此外，还设计了拓扑感知节点嵌入和沙漏注意力模块（SGA），以提高模型的效率和性能。在多个数据集上进行的实验表明，STD-PLM在预测和插值任务上具有竞争力，并在零样本和少样本任务上取得了有前景的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>STD-PLM结合了预训练语言模型（PLM），能够同时处理空间时间预测和插值任务。</li>
<li>通过明确设计的空间和时间标记器，STD-PLM能够理解时空数据中的复杂相关性。</li>
<li>拓扑感知节点嵌入的设计使得PLM能够感知并利用数据的拓扑结构。</li>
<li>沙漏注意力模块（SGA）与特定约束损失函数的设计提高了模型的效率并保证性能。</li>
<li>实验结果表明，STD-PLM在多个数据集上的预测和插值任务上具有竞争力。</li>
<li>STD-PLM在零样本和少样本任务上取得了有前景的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.09096">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-30ff77f93744d40af631fd36a9a0db82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-565204ee75c438dfec1acb3af0807f85.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Iterative-Deployment-Exposure-for-Unsupervised-Out-of-Distribution-Detection"><a href="#Iterative-Deployment-Exposure-for-Unsupervised-Out-of-Distribution-Detection" class="headerlink" title="Iterative Deployment Exposure for Unsupervised Out-of-Distribution   Detection"></a>Iterative Deployment Exposure for Unsupervised Out-of-Distribution   Detection</h2><p><strong>Authors:Lars Doorenbos, Raphael Sznitman, Pablo Márquez-Neila</strong></p>
<p>Deep learning models are vulnerable to performance degradation when encountering out-of-distribution (OOD) images, potentially leading to misdiagnoses and compromised patient care. These shortcomings have led to great interest in the field of OOD detection. Existing unsupervised OOD (U-OOD) detection methods typically assume that OOD samples originate from an unconcentrated distribution complementary to the training distribution, neglecting the reality that deployed models passively accumulate task-specific OOD samples over time. To better reflect this real-world scenario, we introduce Iterative Deployment Exposure (IDE), a novel and more realistic setting for U-OOD detection. We propose CSO, a method for IDE that starts from a U-OOD detector that is agnostic to the OOD distribution and slowly refines it during deployment using observed unlabeled data. CSO uses a new U-OOD scoring function that combines the Mahalanobis distance with a nearest-neighbor approach, along with a novel confidence-scaled few-shot OOD detector to effectively learn from limited OOD examples. We validate our approach on a dedicated benchmark, showing that our method greatly improves upon strong baselines on three medical imaging modalities. </p>
<blockquote>
<p>深度学习模型在遇到离群分布（OOD）图像时容易出现性能下降的问题，这可能导致误诊和患者护理受到影响。这些缺陷引发了人们对OOD检测领域的极大兴趣。现有的无监督OOD（U-OOD）检测方法通常假设OOD样本来自与训练分布互补的非集中分布，忽视了这样一个现实情况：部署的模型随着时间的推移会被动地累积特定任务的OOD样本。为了更好地反映这种真实场景，我们引入了迭代部署暴露（IDE），这是一种用于U-OOD检测的新型且更现实的设置。我们提出了一种IDE方法CSO，该方法从对OOD分布不了解的U-OOD检测器开始，在部署过程中使用观察到的无标签数据缓慢地对其进行优化。CSO使用了一种新的U-OOD评分函数，结合了马氏距离和最近邻方法，并使用了一种新型的信心规模有限样本OOD检测器，以有效地从有限的OOD样本中学习。我们在专用基准测试上验证了我们的方法，结果显示我们的方法在三种医学影像模态上大大改进了强基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02327v2">PDF</a> Accepted at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了深度学习模型在面临非分布图像时性能下降的问题，这可能导致误诊和患者护理受到损害。现有无监督异常检测（U-OOD）方法假设异常样本来源于非集中分布，忽略现实场景模型部署中任务特定的异常样本逐渐积累的问题。为反映真实情况，提出了迭代部署暴露（IDE）这一新型的无监督异常检测场景设置。针对IDE场景，提出了一种方法CSO，该方法从对异常分布无感知的U-OOD检测器出发，利用观察到的未标记数据在部署过程中逐步优化。CSO采用新的U-OOD评分函数，结合马氏距离和最近邻方法，以及新的信心缩放少样本异常检测器，从有限的异常样本中学习。在专用基准测试上验证了该方法，在三种医学成像模态上大大优于强基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习模型在面临非分布图像时可能性能下降，导致误诊和患者护理问题。</li>
<li>现有无监督异常检测方法忽略了现实场景中模型部署中任务特定异常样本逐渐积累的问题。</li>
<li>提出了迭代部署暴露（IDE）这一新型的无监督异常检测场景设置，以更好地反映真实情况。</li>
<li>针对IDE场景，提出了CSO方法，从对异常分布无感知的U-OOD检测器出发，逐步优化。</li>
<li>CSO采用结合马氏距离和最近邻方法的新的U-OOD评分函数。</li>
<li>CSO引入信心缩放少样本异常检测器，可从有限的异常样本中学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02327">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-993c56f8262600b24f4ab926c4d5cdba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccfe5169e9fcb1c000353768095b19f9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4f8cae6e3b8954cf2b47c6a9d60c4d92.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-22  Replace in Translation Boost Concept Alignment in Counterfactual   Text-to-Image
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70326c7591212f1a08bbbbe1e5097b67.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-05-22  ContextAgent Context-Aware Proactive LLM Agents with Open-World Sensory   Perceptions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
