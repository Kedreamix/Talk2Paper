<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  FMSD-TTS Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-bf76b9501af682faaab986e5985edde6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    46 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-22-æ›´æ–°"><a href="#2025-05-22-æ›´æ–°" class="headerlink" title="2025-05-22 æ›´æ–°"></a>2025-05-22 æ›´æ–°</h1><h2 id="FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation"><a href="#FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation" class="headerlink" title="FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation"></a>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation</h2><p><strong>Authors:Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi</strong></p>
<p>Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-&quot;U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality. </p>
<blockquote>
<p>è—è¯­æ˜¯ä¸€ç§èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œå…¶ä¸‰å¤§æ–¹è¨€åŒºâ€”â€”ä¹Œé½ã€å®‰å¤šå’Œåº·åŒºçš„å¹³è¡Œè¯­éŸ³è¯­æ–™åº“æå…¶æœ‰é™ï¼Œé™åˆ¶äº†è¯­éŸ³å»ºæ¨¡çš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FMSD-TTSï¼Œè¿™æ˜¯ä¸€ä¸ªå°æ ·æœ¬ã€å¤šå‘è¨€äººã€å¤šæ–¹è¨€çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»æœ‰é™çš„å‚è€ƒéŸ³é¢‘å’Œæ˜ç¡®çš„æ–¹è¨€æ ‡ç­¾ä¸­åˆæˆå¹³è¡Œçš„æ–¹è¨€è¯­éŸ³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ–°é¢–çš„å‘å£°äºº-æ–¹è¨€èåˆæ¨¡å—å’Œæ–¹è¨€ä¸“ç”¨åŠ¨æ€è·¯ç”±ç½‘ç»œï¼ˆDSDR-Netï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰æ–¹è¨€é—´çš„ç²¾ç»†å£°å­¦å’Œè¯­è¨€å˜å¼‚ï¼ŒåŒæ—¶ä¿ç•™å‘å£°äººçš„èº«ä»½ã€‚å¤§é‡çš„å®¢è§‚å’Œä¸»è§‚è¯„ä¼°è¡¨æ˜ï¼ŒFMSD-TTSåœ¨æ–¹è¨€è¡¨è¾¾åŠ›å’Œå‘å£°äººç›¸ä¼¼æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿ç³»ç»Ÿã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­éŸ³åˆ°è¯­éŸ³çš„æ–¹è¨€è½¬æ¢ä»»åŠ¡æ¥éªŒè¯åˆæˆè¯­éŸ³çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹è—è¯­å¤šæ–¹è¨€è¯­éŸ³åˆæˆçš„å°‘æ ·æœ¬TTSç³»ç»Ÿï¼Œï¼ˆ2ï¼‰å…¬å¼€å‘å¸ƒç”±FMSD-TTSç”Ÿæˆçš„å¤§è§„æ¨¡åˆæˆè—è¯­è¯­éŸ³è¯­æ–™åº“ï¼Œï¼ˆ3ï¼‰å¼€æ”¾æºä»£ç è¯„ä¼°å·¥å…·åŒ…ï¼Œç”¨äºæ ‡å‡†åŒ–è¯„ä¼°å‘å£°äººç›¸ä¼¼æ€§ã€æ–¹è¨€ä¸€è‡´æ€§å’ŒéŸ³é¢‘è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14351v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è—è¯­è¿™ä¸€èµ„æºåŒ®ä¹å‹è¯­è¨€çš„TTSç³»ç»Ÿã€‚ä¸ºåº”å¯¹è—è¯­ä¸‰ç§ä¸»è¦æ–¹è¨€ä¹‹é—´å¹³è¡Œè¯­æ–™ç¨€ç¼ºçš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†FMSD-TTSç³»ç»Ÿï¼Œè¯¥æ¡†æ¶èƒ½ä»æœ‰é™çš„å‚è€ƒéŸ³é¢‘å’Œæ˜ç¡®çš„æ–¹è¨€æ ‡ç­¾ä¸­åˆæˆå¹³è¡Œæ–¹è¨€è¯­éŸ³ã€‚å®ƒé‡‡ç”¨äº†å¤šè¯­ç§è¯­éŸ³èåˆæ¨¡å—åŠæ–¹è¨€ä¸“ä¸šåŠ¨æ€è·¯ç”±ç½‘ç»œï¼Œå¯ä»¥ç²¾ç»†æ•æ‰æ–¹è¨€é—´çš„å£°å­¦å·®å¼‚ï¼ŒåŒæ—¶åœ¨å‘éŸ³ä¸Šä¿ç•™ä¸ªä½“å·®å¼‚ã€‚FMSD-TTSåœ¨å„ç§è¯„ä»·å’Œå®éªŒä¸­çš„è¡¨ç°å‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜é¦–æ¬¡å…¬å¼€äº†å¤§è§„æ¨¡è—è¯­åˆæˆè¯­éŸ³åº“å’Œå¼€æºè¯„ä¼°å·¥å…·åŒ…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FMSD-TTSæ˜¯ä¸ºè—è¯­è¿™ä¸€èµ„æºåŒ®ä¹å‹è¯­è¨€è®¾è®¡çš„TTSç³»ç»Ÿã€‚</li>
<li>ç³»ç»Ÿèƒ½æœ‰æ•ˆåˆæˆè—è¯­çš„ä¸‰å¤§æ–¹è¨€ï¼ˆä¹Œå…¹ã€å®‰å¤šå’Œåº·åŒºæ–¹è¨€ï¼‰ã€‚</li>
<li>FMSD-TTSé€šè¿‡å¼•å…¥å¤šè¯­ç§è¯­éŸ³èåˆæ¨¡å—å’Œæ–¹è¨€ä¸“ä¸šåŠ¨æ€è·¯ç”±ç½‘ç»œï¼Œèƒ½å¤Ÿæ•æ‰æ–¹è¨€é—´çš„ç»†å¾®å·®å¼‚å¹¶ä¿ç•™è¯´è¯äººçš„èº«ä»½ç‰¹å¾ã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä»·ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–¹è¨€è¡¨è¾¾åŠ›å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢ã€‚</li>
<li>FMSD-TTSæˆåŠŸåœ°å®Œæˆäº†æŒ‘æˆ˜æ€§æ–¹è¨€è¯­éŸ³è½¬æ¢ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶è€…é¦–æ¬¡å…¬å¼€äº†ç”±FMSD-TTSç”Ÿæˆçš„å¤§è§„æ¨¡è—è¯­åˆæˆè¯­éŸ³åº“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a712b79d7ffc4c1dca0f96958d2ba5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1785cc70a11836b2715822d27c917e0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a811d1d51e8b11dcb30516dfa2c64ac7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a42a77d797ad5aa3c3834fc192c0333d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a0dee36095191e6784bfb9b357a3309.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AudioJailbreak-Jailbreak-Attacks-against-End-to-End-Large-Audio-Language-Models"><a href="#AudioJailbreak-Jailbreak-Attacks-against-End-to-End-Large-Audio-Language-Models" class="headerlink" title="AudioJailbreak: Jailbreak Attacks against End-to-End Large   Audio-Language Models"></a>AudioJailbreak: Jailbreak Attacks against End-to-End Large   Audio-Language Models</h2><p><strong>Authors:Guangke Chen, Fu Song, Zhe Zhao, Xiaojun Jia, Yang Liu, Yanchen Qiao, Weizhe Zhang</strong></p>
<p>Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website <a target="_blank" rel="noopener" href="https://audiojailbreak.github.io/AudioJailbreak">https://audiojailbreak.github.io/AudioJailbreak</a>. </p>
<blockquote>
<p>è¿‘æœŸå¯¹å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¶Šç‹±æ”»å‡»ï¼ˆJailbreakæ”»å‡»ï¼‰è¿›è¡Œäº†ç ”ç©¶ï¼Œä½†è¿™äº›æ”»å‡»çš„æ•ˆèƒ½ã€é€‚ç”¨æ€§å’Œå®ç”¨æ€§å‡ä¸å¤Ÿç†æƒ³ï¼Œç‰¹åˆ«æ˜¯å‡è®¾æ”»å‡»è€…å¯ä»¥å……åˆ†æ“æ§ç”¨æˆ·æç¤ºçš„æƒ…å†µä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜å…ˆè¿›çš„æ–‡æœ¬è¶Šç‹±æ”»å‡»æ— æ³•è½»æ˜“é€šè¿‡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯åº”ç”¨äºç«¯åˆ°ç«¯çš„LALMæ¨¡å‹ã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†AudioJailbreakè¿™ä¸€æ–°å‹éŸ³é¢‘è¶Šç‹±æ”»å‡»æ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š</p>
</blockquote>
<ol>
<li>å¼‚æ­¥æ€§ï¼šè¶Šç‹±éŸ³é¢‘æ— éœ€é€šè¿‡åˆ¶ä½œåç¼€è¶Šç‹±éŸ³é¢‘ä¸ç”¨æˆ·æç¤ºåœ¨æ—¶é—´è½´ä¸Šè¿›è¡Œå¯¹é½ï¼›</li>
<li>é€šç”¨æ€§ï¼šé€šè¿‡èå…¥å¤šä¸ªæç¤ºæ¥ç”Ÿæˆæ‰°åŠ¨ï¼Œå•ä¸ªè¶Šç‹±æ‰°åŠ¨å¯¹ä¸åŒçš„æç¤ºéƒ½æœ‰æ•ˆï¼›</li>
<li>éšè”½æ€§ï¼šé€šè¿‡æå‡ºå„ç§æ„å›¾éšè—ç­–ç•¥ï¼Œè¶Šç‹±éŸ³é¢‘çš„æ¶æ„æ„å›¾ä¸ä¼šä½¿å—å®³è€…æé«˜è­¦æƒ•ï¼›</li>
</ol>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14103v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç ”ç©¶äº†å¯¹å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¶Šç‹±æ”»å‡»ï¼Œä½†ç°æœ‰æ”»å‡»åœ¨è½¬æ¢æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„ç«¯åˆ°ç«¯LALMæ¨¡å¼ä¸‹çš„æ•ˆæœã€é€‚ç”¨æ€§å’Œå®ç”¨æ€§æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†AudioJailbreakæ–°å‹éŸ³é¢‘è¶Šç‹±æ”»å‡»ï¼Œå…·æœ‰å¼‚æ­¥æ€§ã€æ™®éæ€§ã€éšè”½æ€§å’ŒæŠ—å¹²æ‰°æ€§ç­‰ç‰¹ç‚¹ï¼Œé€‚ç”¨äºæ— æ³•å®Œå…¨æ“æ§ç”¨æˆ·æç¤ºçš„å¯¹æ‰‹ï¼Œæ”»å‡»åœºæ™¯æ›´å¹¿æ³›ã€‚å®éªŒè¯æ˜AudioJailbreakå¯¹ç›®å‰å¤§å¤šæ•°LALMæ¨¡å‹é«˜åº¦æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ–‡æœ¬è¶Šç‹±æ”»å‡»éš¾ä»¥ç›´æ¥åº”ç”¨äºç«¯åˆ°ç«¯çš„LALMæ¨¡å‹ã€‚</li>
<li>AudioJailbreakæ˜¯ä¸€ç§æ–°å‹éŸ³é¢‘è¶Šç‹±æ”»å‡»ï¼Œå…·å¤‡å¼‚æ­¥æ€§ï¼Œä¸éœ€è¦ä¸ç”¨æˆ·æç¤ºåœ¨æ—¶é—´è½´ä¸Šå¯¹é½ã€‚</li>
<li>AudioJailbreakå…·æœ‰æ™®éæ€§ï¼Œå•ä¸ªè¶Šç‹±æ‰°åŠ¨å¯é€‚ç”¨äºä¸åŒçš„æç¤ºã€‚</li>
<li>AudioJailbreakå…·æœ‰éšè”½æ€§ï¼Œæ¶æ„æ„å›¾ä¸ä¼šè¢«å—å®³è€…å¯Ÿè§‰ã€‚</li>
<li>AudioJailbreakå…·å¤‡æŠ—å¹²æ‰°æ€§ï¼Œåœ¨æ’­æ”¾è¿‡ç¨‹ä¸­èƒ½æœ‰æ•ˆæŠµå¾¡ç¯å¢ƒå™ªå£°ã€‚</li>
<li>AudioJailbreaké€‚ç”¨äºæ— æ³•å®Œå…¨æ“æ§ç”¨æˆ·æç¤ºçš„å¯¹æ‰‹ï¼Œæ”»å‡»åœºæ™¯æ›´å¹¿æ³›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14103">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-10c0994278ffcd0bb6d912472d0e4f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88755946603f8cbe258b2fe3cb0bb019.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe8d54d42c0ffd4affae82ca1a116e94.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement"><a href="#SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement" class="headerlink" title="SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement"></a>SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement</h2><p><strong>Authors:Kuan-Yu Chen, Jeng-Lin Li, Jian-Jiun Ding</strong></p>
<p>With the fast development of zero-shot text-to-speech technologies, it is possible to generate high-quality speech signals that are indistinguishable from the real ones. Speech editing, including speech insertion and replacement, appeals to researchers due to its potential applications. However, existing studies only considered clean speech scenarios. In real-world applications, the existence of environmental noise could significantly degrade the quality of the generation. In this study, we propose a noise-resilient speech editing framework, SeamlessEdit, for noisy speech editing. SeamlessEdit adopts a frequency-band-aware noise suppression module and an in-content refinement strategy. It can well address the scenario where the frequency bands of voice and background noise are not separated. The proposed SeamlessEdit framework outperforms state-of-the-art approaches in multiple quantitative and qualitative evaluations. </p>
<blockquote>
<p>éšç€é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆé«˜è´¨é‡ã€ä¸ç°å®æ— æ³•åŒºåˆ†çš„è¯­éŸ³ä¿¡å·æˆä¸ºå¯èƒ½ã€‚è¯­éŸ³ç¼–è¾‘ï¼ŒåŒ…æ‹¬è¯­éŸ³æ’å…¥å’Œæ›¿æ¢ï¼Œå› å…¶æ½œåœ¨çš„åº”ç”¨ä»·å€¼è€Œå¸å¼•ç ”ç©¶è€…å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä»…æ¶‰åŠæ¸…æ´è¯­éŸ³åœºæ™¯ã€‚åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­ï¼Œç¯å¢ƒå™ªå£°çš„å­˜åœ¨å¯èƒ½ä¼šæ˜¾è‘—é™ä½ç”Ÿæˆè¯­éŸ³çš„è´¨é‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é€‚ç”¨äºå™ªå£°è¯­éŸ³ç¼–è¾‘çš„é²æ£’æ€§è¯­éŸ³ç¼–è¾‘æ¡†æ¶SeamlessEditã€‚SeamlessEdité‡‡ç”¨é¢‘å¸¦æ„ŸçŸ¥å™ªå£°æŠ‘åˆ¶æ¨¡å—å’ŒåŸºäºå†…å®¹çš„ä¼˜åŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°è§£å†³è¯­éŸ³å’ŒèƒŒæ™¯å™ªå£°é¢‘å¸¦æœªåˆ†ç¦»çš„åœºæ™¯ã€‚æ‰€æå‡ºçš„SeamlessEditæ¡†æ¶åœ¨å¤šä¸ªå®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14066v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºå™ªå£°è¯­éŸ³ç¼–è¾‘çš„SeamlessEditæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰å™ªå£°æŠ‘åˆ¶å’Œå†…å®¹ç»†åŒ–ç­–ç•¥ï¼Œé€‚ç”¨äºé¢‘ç‡å¸¦æœªåˆ†ç¦»çš„å™ªå£°è¯­éŸ³ç¼–è¾‘åœºæ™¯ï¼Œå¹¶åœ¨å¤šé¡¹å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨å™ªå£°ç¯å¢ƒä¸‹çš„è¯­éŸ³ç¼–è¾‘ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç°å®ä¸–ç•Œä¸­é¢‘ç‡å¸¦æœªåˆ†ç¦»çš„å™ªå£°è¯­éŸ³åœºæ™¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºSeamlessEditçš„å™ªå£°é²æ£’æ€§è¯­éŸ³ç¼–è¾‘æ¡†æ¶ã€‚</li>
<li>SeamlessEditæ¡†æ¶åŒ…å«é¢‘ç‡å¸¦æ„ŸçŸ¥çš„å™ªå£°æŠ‘åˆ¶æ¨¡å—å’ŒåŸºäºå†…å®¹çš„ç»†åŒ–ç­–ç•¥ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šé¡¹å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
<li>è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶ä¸­åªè€ƒè™‘å¹²å‡€è¯­éŸ³åœºæ™¯çš„å±€é™æ€§ã€‚</li>
<li>SeamlessEditå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦é«˜è´¨é‡è¯­éŸ³ç”Ÿæˆçš„é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3331afb116eb5a4dd722f832aec65be7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-352190ad478243fe4b96bd0542df1f43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3b0f11694b33d58d87e7efc60fe78c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-476197b8220b75e12102d575aa4a5566.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Semantic-Information-based-Hierarchical-Speech-Enhancement-Method-Using-Factorized-Codec-and-Diffusion-Model"><a href="#A-Semantic-Information-based-Hierarchical-Speech-Enhancement-Method-Using-Factorized-Codec-and-Diffusion-Model" class="headerlink" title="A Semantic Information-based Hierarchical Speech Enhancement Method   Using Factorized Codec and Diffusion Model"></a>A Semantic Information-based Hierarchical Speech Enhancement Method   Using Factorized Codec and Diffusion Model</h2><p><strong>Authors:Yang Xiang, Canan Huang, Desheng Hu, Jingguang Tian, Xinhui Hu, Chao Zhang</strong></p>
<p>Most current speech enhancement (SE) methods recover clean speech from noisy inputs by directly estimating time-frequency masks or spectrums. However, these approaches often neglect the distinct attributes, such as semantic content and acoustic details, inherent in speech signals, which can hinder performance in downstream tasks. Moreover, their effectiveness tends to degrade in complex acoustic environments. To overcome these challenges, we propose a novel, semantic information-based, step-by-step factorized SE method using factorized codec and diffusion model. Unlike traditional SE methods, our hierarchical modeling of semantic and acoustic attributes enables more robust clean speech recovery, particularly in challenging acoustic scenarios. Moreover, this method offers further advantages for downstream TTS tasks. Experimental results demonstrate that our algorithm not only outperforms SOTA baselines in terms of speech quality but also enhances TTS performance in noisy environments. </p>
<blockquote>
<p>å½“å‰å¤§å¤šæ•°è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•é€šè¿‡ç›´æ¥ä¼°è®¡æ—¶é—´é¢‘ç‡æ©è†œæˆ–é¢‘è°±ä»å™ªå£°è¾“å…¥ä¸­æ¢å¤æ¸…æ´è¯­éŸ³ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½è§†äº†è¯­éŸ³ä¿¡å·æ‰€å›ºæœ‰çš„ä¸åŒå±æ€§ï¼Œå¦‚è¯­ä¹‰å†…å®¹å’Œå£°å­¦ç»†èŠ‚ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨å¤æ‚çš„å£°å­¦ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å¾€å¾€ä¼šé™ä½ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰ä¿¡æ¯ã€ä½¿ç”¨åˆ†è§£ç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹çš„åˆ†æ­¥åˆ†è§£SEæ–°æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„SEæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å¯¹è¯­ä¹‰å’Œå£°éŸ³å±æ€§çš„åˆ†å±‚å»ºæ¨¡ï¼Œèƒ½å¤Ÿå®ç°æ›´ç¨³å¥çš„æ¸…æ´è¯­éŸ³æ¢å¤ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œæ­¤æ–¹æ³•å¯¹äºä¸‹æ¸¸çš„TTSä»»åŠ¡è¿˜å…·æœ‰è¿›ä¸€æ­¥çš„ä¼˜ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•ä¸ä»…åœ¨è¯­éŸ³è´¨é‡æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯æ°´å¹³çš„åŸºå‡†æµ‹è¯•ï¼Œè€Œä¸”è¿˜æé«˜äº†åœ¨å™ªå£°ç¯å¢ƒä¸­TTSçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13843v1">PDF</a> Accepted by interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰ä¿¡æ¯çš„æ–°å‹åˆ†æ­¥åˆ†è§£è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œä½¿ç”¨åˆ†è§£ç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿè¯­éŸ³å¢å¼ºæ–¹æ³•åœ¨å¤„ç†å¤æ‚å£°å­¦ç¯å¢ƒä¸‹çš„å±€é™æ€§ï¼Œå®ƒé€šè¿‡å±‚æ¬¡åŒ–å»ºæ¨¡è¯­ä¹‰å’Œå£°éŸ³å±æ€§æ¥å®ç°æ›´ç¨³å¥çš„å¹²å‡€è¯­éŸ³æ¢å¤ï¼Œå¹¶æœ‰åŠ©äºæé«˜ä¸‹æ¸¸æ–‡æœ¬è½¬è¯­éŸ³ä»»åŠ¡çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•ä¸ä»…åœ¨è¯­éŸ³è´¨é‡ä¸Šä¼˜äºç°æœ‰æœ€ä½³åŸºçº¿ï¼Œè€Œä¸”åœ¨å™ªå£°ç¯å¢ƒä¸­æé«˜äº†æ–‡æœ¬è½¬è¯­éŸ³çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è¯­éŸ³å¢å¼ºæ–¹æ³•ä¸»è¦é€šè¿‡ç›´æ¥ä¼°è®¡æ—¶é—´-é¢‘ç‡æ©è†œæˆ–é¢‘è°±æ¥æ¢å¤å¹²å‡€è¯­éŸ³ï¼Œä½†å¿½ç•¥äº†è¯­éŸ³ä¿¡å·ä¸­çš„è¯­ä¹‰å†…å®¹å’Œå£°éŸ³ç»†èŠ‚ã€‚</li>
<li>æå‡ºçš„åŸºäºè¯­ä¹‰ä¿¡æ¯çš„æ–°å‹åˆ†æ­¥åˆ†è§£è¯­éŸ³å¢å¼ºæ–¹æ³•ä½¿ç”¨åˆ†è§£ç¼–ç å™¨å’Œæ‰©æ•£æ¨¡å‹ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤æ‚å£°å­¦ç¯å¢ƒä¸‹çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å±‚æ¬¡åŒ–å»ºæ¨¡è¯­ä¹‰å’Œå£°éŸ³å±æ€§ï¼Œå®ç°äº†æ›´ç¨³å¥çš„å¹²å‡€è¯­éŸ³æ¢å¤ã€‚</li>
<li>æ–¹æ³•åœ¨å™ªå£°ç¯å¢ƒä¸‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸ä»…æé«˜äº†è¯­éŸ³è´¨é‡ï¼Œè¿˜æœ‰åŠ©äºæé«˜ä¸‹æ¸¸æ–‡æœ¬è½¬è¯­éŸ³ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨è¯­éŸ³è´¨é‡ä¸Šä¼˜äºç°æœ‰æœ€ä½³åŸºçº¿ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºè§£å†³å¤æ‚å£°å­¦ç¯å¢ƒä¸­çš„è¯­éŸ³å¢å¼ºé—®é¢˜å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49a63609cdc23e745f58767ab60aad88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a5ebd56600882d57e45be7fd643b077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-453df19eca6bbc0e7e95cdadcca49a68.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising"><a href="#Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising" class="headerlink" title="Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising"></a>Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising</h2><p><strong>Authors:Ye-Xin Lu, Hui-Peng Du, Fei Liu, Yang Ai, Zhen-Hua Ling</strong></p>
<p>Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•å€¾å‘äºä¿ç•™éŸ³é¢‘æç¤ºçš„å£°å­¦ç¯å¢ƒï¼Œè¿™å¯¼è‡´å½“éŸ³é¢‘æç¤ºåŒ…å«å™ªå£°æ—¶ï¼Œåˆæˆè¯­éŸ³çš„è´¨é‡ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºç¥ç»ç¼–ç å™¨çš„è¯­éŸ³å»å™ªå™¨ï¼Œå¹¶å°†å…¶ä¸å…ˆè¿›çš„LLM-based TTSæ¨¡å‹LauraTTSç›¸ç»“åˆï¼Œå®ç°äº†å™ªå£°é²æ£’çš„é›¶æ ·æœ¬TTSã€‚æ‰€æå‡ºçš„ç¼–ç å»å™ªå™¨ç”±éŸ³é¢‘ç¼–ç å™¨ã€ä»¤ç‰Œå»å™ªå™¨å’ŒåµŒå…¥ç²¾ç‚¼å™¨ç»„æˆã€‚ä»¤ç‰Œå»å™ªå™¨ä»å™ªå£°ä»¤ç‰Œé¢„æµ‹å‰ä¸¤ä¸ªç»„çš„å¹²å‡€å£°å­¦ä»¤ç‰Œï¼Œè¿™å¯ä»¥ä½œä¸ºLauraTTSåˆæˆé«˜è´¨é‡ä¸ªæ€§åŒ–è¯­éŸ³çš„å£°å­¦æç¤ºï¼Œæˆ–è€…é€šè¿‡åµŒå…¥ç²¾ç‚¼å™¨å’Œç¼–ç å™¨è§£ç å™¨è½¬æ¢ä¸ºå¹²å‡€çš„è¯­éŸ³æ³¢å½¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ç¼–ç å»å™ªå™¨ä¼˜äºæœ€æ–°çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•ï¼Œå¹¶ä¸”æ‰€æå‡ºçš„å™ªå£°é²æ£’çš„LauraTTSè¶…è¿‡äº†ä½¿ç”¨é™„åŠ SEæ¨¡å‹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13830v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•ä¼šä¿ç•™éŸ³é¢‘æç¤ºçš„å£°å­¦ç¯å¢ƒï¼Œå½“éŸ³é¢‘æç¤ºåŒ…å«å™ªå£°æ—¶ï¼Œä¼šå¯¼è‡´åˆæˆè¯­éŸ³è´¨é‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºç¥ç»ç½‘ç»œç¼–è§£ç å™¨çš„è¯­éŸ³å»å™ªå™¨ï¼Œå¹¶å°†å…¶ä¸å…ˆè¿›çš„LLM-based TTSæ¨¡å‹LauraTTSç›¸ç»“åˆï¼Œå®ç°äº†å™ªå£°é²æ£’çš„é›¶æ ·æœ¬TTSã€‚ç¼–è§£ç å™¨å»å™ªå™¨ç”±éŸ³é¢‘ç¼–è§£ç å™¨ã€ä»¤ç‰Œå»å™ªå™¨å’ŒåµŒå…¥ç²¾ç‚¼å™¨ç»„æˆã€‚ä»¤ç‰Œå»å™ªå™¨ä»å™ªå£°ä»¤ç‰Œé¢„æµ‹å‰ä¸¤ç»„å¹²å‡€çš„å£°å­¦ä»¤ç‰Œï¼Œå¯ä½œä¸ºLauraTTSåˆæˆé«˜è´¨é‡ä¸ªæ€§åŒ–è¯­éŸ³çš„å£°å­¦æç¤ºï¼Œæˆ–é€šè¿‡åµŒå…¥ç²¾ç‚¼å™¨å’Œç¼–è§£ç å™¨è§£ç å™¨è½¬æ¢ä¸ºå¹²å‡€çš„è¯­éŸ³æ³¢å½¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„ç¼–è§£ç å™¨å»å™ªå™¨ä¼˜äºå…ˆè¿›çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•ï¼Œæ‰€æå‡ºçš„å™ªå£°é²æ£’LauraTTSè¶…è¶Šäº†ä½¿ç”¨é™„åŠ SEæ¨¡å‹çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åº”ç”¨ä¸­é¢ä¸´å™ªå£°é—®é¢˜ï¼Œå™ªå£°ä¼šå½±å“åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œç¼–è§£ç å™¨çš„è¯­éŸ³å»å™ªå™¨ï¼Œä»¥å¤„ç†éŸ³é¢‘ä¸­çš„å™ªå£°ã€‚</li>
<li>å™ªå£°å»é™¤è¿‡ç¨‹åŒ…æ‹¬é¢„æµ‹å¹²å‡€çš„å£°å­¦ä»¤ç‰Œå’Œç²¾ç‚¼åµŒå…¥ã€‚</li>
<li>è¯¥å»å™ªå™¨ä¸LauraTTSæ¨¡å‹ç»“åˆï¼Œå®ç°äº†å™ªå£°é²æ£’çš„é›¶æ ·æœ¬TTSã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œæå‡ºçš„ç¼–è§£ç å™¨å»å™ªå™¨åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•ã€‚</li>
<li>å™ªå£°é²æ£’çš„LauraTTSæ¨¡å‹åœ¨åˆæˆé«˜è´¨é‡è¯­éŸ³æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œç›¸æ¯”ä½¿ç”¨é™„åŠ SEæ¨¡å‹çš„æ–¹æ³•æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ”¹è¿›TTSæŠ€æœ¯åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-996abe2385b849300f64b29747a9b154.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba58973ea83532ce32150a761118e124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-582a9fe21eabd309d2c2f7fd1e78776e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02c233800f084b2074fac3697b931790.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A3-an-Analytical-Low-Rank-Approximation-Framework-for-Attention"><a href="#A3-an-Analytical-Low-Rank-Approximation-Framework-for-Attention" class="headerlink" title="A3 : an Analytical Low-Rank Approximation Framework for Attention"></a>A3 : an Analytical Low-Rank Approximation Framework for Attention</h2><p><strong>Authors:Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao</strong></p>
<p>Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the componentâ€™s functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTAâ€™s 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†å…¶åºå¤§çš„å‚æ•°æ•°é‡å¯¼è‡´éƒ¨ç½²æˆæœ¬é«˜æ˜‚ã€‚ä½ç§©è¿‘ä¼¼æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„å‹ç¼©è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰å®ƒä»¬ä¸“æ³¨äºæœ€å°åŒ–å•ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºè¯¯å·®ï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°Transformerçš„æ¶æ„ç‰¹æ€§ï¼›ï¼ˆ2ï¼‰å®ƒä»¬å°†å¤§å‹æƒé‡çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªå°çš„ä½ç§©çŸ©é˜µã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ¯”å…¶ä»–å‹ç¼©æŠ€æœ¯ï¼ˆå¦‚å‰ªæå’Œé‡åŒ–ï¼‰é€Šè‰²ï¼Œå¹¶ä¸”å¼•å…¥äº†è¿è¡Œæ—¶å¼€é”€ï¼Œä¾‹å¦‚ä¸ºåˆ†è§£çš„å°çŸ©é˜µé¢å¤–å¯åŠ¨GEMMå†…æ ¸ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†$\tt A^\tt 3$ï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒåçš„ä½ç§©è¿‘ä¼¼æ¡†æ¶ã€‚$\tt A^\tt 3$å°†Transformerå±‚åˆ†ä¸ºä¸‰ä¸ªåŠŸèƒ½ç»„ä»¶ï¼Œå³$\tt QK$ã€$\tt OV$å’Œ$\tt MLP$ã€‚å¯¹äºæ¯ä¸ªç»„ä»¶ï¼Œ$\tt A^\tt 3$æä¾›äº†ä¸€ç§åˆ†æè§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨å‡å°æ¯ä¸ªç»„ä»¶å†…çš„éšè—ç»´åº¦å¤§å°çš„åŒæ—¶ï¼Œæœ€å°åŒ–ç»„ä»¶çš„åŠŸèƒ½æŸå¤±ï¼ˆå³æ³¨æ„åˆ†æ•°ã€æ³¨æ„è¾“å‡ºå’ŒMLPè¾“å‡ºçš„è¯¯å·®ï¼‰ã€‚è¿™ç§æ–¹æ³•ç›´æ¥å‡å°äº†æ¨¡å‹å¤§å°ã€KVç¼“å­˜å¤§å°å¹¶å‡å°‘äº†æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ŒåŒæ—¶æ²¡æœ‰å¼•å…¥ä»»ä½•è¿è¡Œæ—¶å¼€é”€ã€‚æ­¤å¤–ï¼Œå®ƒä»å•ä¸€çš„çº¿æ€§å±‚æŸå¤±ä¼˜åŒ–é—®é¢˜å‡ºå‘ï¼Œä¸ºä¼˜åŒ–é—®é¢˜æä¾›äº†æ–°çš„å™äº‹æ–¹å‘ï¼Œä»¥æ”¹è¿›ç«¯åˆ°ç«¯çš„æ€§èƒ½ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†$\tt A^\tt 3$åœ¨ä¿æŒä¼˜äºç°æœ‰æŠ€æœ¯çš„åŒæ—¶ï¼Œä¹Ÿå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨ç›¸åŒçš„è®¡ç®—å’Œå†…å­˜å‡å°‘é¢„ç®—ä¸‹ï¼Œæˆ‘ä»¬çš„ä½ç§©ä¼°è®¡LLaMA 3.1-70Båœ¨WikiText-2ä¸Šå–å¾—äº†å›°æƒ‘åº¦4.69çš„æˆç»©ï¼Œä¼˜äºä»¥å‰çš„æœ€ä¼˜æˆç»©7.87è¾¾3.18ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†$\tt A^\tt 3$çš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬KVç¼“å­˜å‹ç¼©ã€é‡åŒ–å’Œæ··åˆæ’ååˆ†é…ä»¥å¢å¼ºæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12942v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶åºå¤§çš„å‚æ•°æ•°é‡å¯¼è‡´éƒ¨ç½²æˆæœ¬é«˜æ˜‚ã€‚ä½ç§©è¿‘ä¼¼æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„å‹ç¼©è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æœ€å°åŒ–å•ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºè¯¯å·®ï¼Œä¸è€ƒè™‘Transformerçš„æ¶æ„ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å°†å¤§æƒé‡çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªå°ä½ç§©çŸ©é˜µï¼Œå¯¼è‡´ä¸å…¶ä»–å‹ç¼©æŠ€æœ¯ç›¸æ¯”æ•ˆæœæœ‰é™ï¼Œå¹¶å¼•å…¥äº†è¿è¡Œæ—¶å¼€é”€ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†$\tt A^\tt 3$ï¼Œä¸€ä¸ªé’ˆå¯¹Transformerå±‚çš„åè®­ç»ƒä½ç§©è¿‘ä¼¼æ¡†æ¶ã€‚$\tt A^\tt 3$å°†Transformerå±‚åˆ†ä¸º$\tt QK$ã€$\tt OV$å’Œ$\tt MLP$ä¸‰ä¸ªåŠŸèƒ½ç»„ä»¶ï¼Œå¹¶ä¸ºæ¯ä¸ªç»„ä»¶æä¾›åˆ†æè§£å†³æ–¹æ¡ˆï¼Œä»¥å‡å°‘éšè—ç»´åº¦å¤§å°å¹¶æœ€å°åŒ–ç»„ä»¶çš„åŠŸèƒ½æŸå¤±ã€‚è¿™ç§æ–¹æ³•ç›´æ¥å‡å°äº†æ¨¡å‹å¤§å°ã€KVç¼“å­˜å¤§å°å’Œæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼Œä¸”æ²¡æœ‰å¼•å…¥ä»»ä½•è¿è¡Œæ—¶å¼€é”€ã€‚æ­¤å¤–ï¼Œå®ƒæ”¹å˜äº†ä¼˜åŒ–é—®é¢˜çš„å™äº‹æ–¹å‘ï¼Œä»å•ä¸€çº¿æ€§å±‚æŸå¤±ä¼˜åŒ–è½¬å‘æ”¹è¿›ç«¯åˆ°ç«¯æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„è®¡ç®—å’Œå†…å­˜å‡å°‘é¢„ç®—ä¸‹ï¼Œæˆ‘ä»¬çš„ä½ç§©è¿‘ä¼¼LLaMA 3.1-70Båœ¨WikiText-2ä¸Šå®ç°äº†4.69çš„å›°æƒ‘åº¦ï¼Œä¼˜äºä¹‹å‰çš„æœ€ä¼˜æ°´å¹³7.87ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†$\tt A^\tt 3$çš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬KVç¼“å­˜å‹ç¼©ã€é‡åŒ–å’Œæ··åˆç§©åˆ†é…ä»¥å¢å¼ºæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œä½†éƒ¨ç½²æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ä½ç§©è¿‘ä¼¼æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰ä½ç§©è¿‘ä¼¼æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šå…³æ³¨å•ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºè¯¯å·®å’Œåˆ†è§£æˆå°ä½ç§©çŸ©é˜µå¯¼è‡´çš„è¿è¡Œæ—¶å¼€é”€ã€‚</li>
<li>$\tt A^\tt 3$æ¡†æ¶è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œé€šè¿‡è€ƒè™‘Transformerçš„æ¶æ„ç‰¹æ€§æ¥å‡å°‘éšè—ç»´åº¦å¹¶æœ€å°åŒ–åŠŸèƒ½æŸå¤±ã€‚</li>
<li>$\tt A^\tt 3$ç›´æ¥å‡å°äº†æ¨¡å‹å¤§å°ã€KVç¼“å­˜å¤§å°å’Œæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼Œä¸”æ²¡æœ‰å¢åŠ è¿è¡Œæ—¶å¼€é”€ã€‚</li>
<li>$\tt A^\tt 3$æ”¹å˜äº†ä¼˜åŒ–é—®é¢˜çš„æ–¹å‘ï¼Œä»å•ä¸€çº¿æ€§å±‚æŸå¤±ä¼˜åŒ–è½¬å‘æ”¹è¿›ç«¯åˆ°ç«¯æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd46b514cce404e31b1a39da66e3770e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f63b2e5df75e2ab6f6ffb71c4f651a40.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OZSpeech-One-step-Zero-shot-Speech-Synthesis-with-Learned-Prior-Conditioned-Flow-Matching"><a href="#OZSpeech-One-step-Zero-shot-Speech-Synthesis-with-Learned-Prior-Conditioned-Flow-Matching" class="headerlink" title="OZSpeech: One-step Zero-shot Speech Synthesis with   Learned-Prior-Conditioned Flow Matching"></a>OZSpeech: One-step Zero-shot Speech Synthesis with   Learned-Prior-Conditioned Flow Matching</h2><p><strong>Authors:Hieu-Nghia Huynh-Nguyen, Ngoc Son Nguyen, Huynh Nguyen Dang, Thieu Vo, Truong-Son Hy, Van Nguyen</strong></p>
<p>Text-to-speech (TTS) systems have seen significant advancements in recent years, driven by improvements in deep learning and neural network architectures. Viewing the output speech as a data distribution, previous approaches often employ traditional speech representations, such as waveforms or spectrograms, within the Flow Matching framework. However, these methods have limitations, including overlooking various speech attributes and incurring high computational costs due to additional constraints introduced during training. To address these challenges, we introduce OZSpeech, the first TTS method to explore optimal transport conditional flow matching with one-step sampling and a learned prior as the condition, effectively disregarding preceding states and reducing the number of sampling steps. Our approach operates on disentangled, factorized components of speech in token format, enabling accurate modeling of each speech attribute, which enhances the TTS systemâ€™s ability to precisely clone the prompt speech. Experimental results show that our method achieves promising performance over existing methods in content accuracy, naturalness, prosody generation, and speaker style preservation. Audio samples are available at our demo page <a target="_blank" rel="noopener" href="https://ozspeech.github.io/OZSpeech_Web/">https://ozspeech.github.io/OZSpeech_Web/</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿåœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™ä¸€è¿›å±•æ˜¯ç”±æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œæ¶æ„çš„æ”¹è¿›æ‰€é©±åŠ¨çš„ã€‚å°†è¾“å‡ºè¯­éŸ³è§†ä¸ºæ•°æ®åˆ†å¸ƒï¼Œä¹‹å‰çš„æ–¹æ³•ç»å¸¸åœ¨æµåŒ¹é…æ¡†æ¶ä¸­ä½¿ç”¨ä¼ ç»Ÿçš„è¯­éŸ³è¡¨ç¤ºæ–¹æ³•ï¼Œä¾‹å¦‚æ³¢å½¢æˆ–é¢‘è°±å›¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å…·æœ‰å±€é™æ€§ï¼ŒåŒ…æ‹¬å¿½ç•¥äº†å„ç§è¯­éŸ³å±æ€§ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥çš„é¢å¤–çº¦æŸå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OZSpeechï¼Œè¿™æ˜¯ä¸€ç§TTSæ–¹æ³•ï¼Œé¦–æ¬¡å°è¯•åœ¨ä¸€æ­¥é‡‡æ ·è¿‡ç¨‹ä¸­é‡‡ç”¨æœ€ä¼˜ä¼ è¾“æ¡ä»¶æµåŒ¹é…ä»¥åŠé€šè¿‡å­¦ä¹ è·å¾—çš„å…ˆéªŒæ¡ä»¶ï¼Œæœ‰æ•ˆåœ°å¿½ç•¥äº†å…ˆå‰çš„çŠ¶æ€å¹¶å‡å°‘äº†é‡‡æ ·æ­¥éª¤çš„æ•°é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥ä»¤ç‰Œæ ¼å¼è¿è¡Œè¯­éŸ³çš„è§£è€¦å’Œåˆ†è§£æˆåˆ†ï¼Œèƒ½å¤Ÿå¯¹æ¯ä¸ªè¯­éŸ³å±æ€§è¿›è¡Œç²¾ç¡®å»ºæ¨¡ï¼Œè¿™æé«˜äº†TTSç³»ç»Ÿç²¾ç¡®å…‹éš†æç¤ºè¯­éŸ³çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å†…å®¹å‡†ç¡®æ€§ã€è‡ªç„¶åº¦ã€è¯­è°ƒç”Ÿæˆå’Œè¯´è¯äººé£æ ¼ä¿æŒæ–¹é¢å‡å–å¾—äº†ä»¤äººé¼“èˆçš„æ€§èƒ½è¡¨ç°ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨æˆ‘ä»¬çš„æ¼”ç¤ºé¡µé¢<a target="_blank" rel="noopener" href="https://ozspeech.github.io/OZSpeech_Web/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://ozspeech.github.io/OZSpeech_Web/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TTSç³»ç»Ÿè¿‘å¹´å› æ·±åº¦å­¦ä¹ ä¸ç¥ç»ç½‘ç»œæ¶æ„çš„è¿›å±•è€Œæœ‰æ‰€çªç ´ã€‚ä¼ ç»Ÿæ–¹æ³•å¸¸é‡‡ç”¨æ³¢å½¢æˆ–é¢‘è°±å›¾ç­‰è¯­éŸ³è¡¨ç°å½¢å¼ï¼Œåœ¨Flow Matchingæ¡†æ¶ä¸‹å­˜åœ¨å¿½ç•¥ä¸åŒè¯­éŸ³å±æ€§å’Œè®­ç»ƒè¿‡ç¨‹ä¸­é¢å¤–çº¦æŸå¯¼è‡´çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤ï¼Œæå‡ºOZSpeechæ–¹æ³•ï¼Œæ¢ç´¢ä¼˜åŒ–ä¼ è¾“æ¡ä»¶æµåŒ¹é…ï¼Œé‡‡ç”¨ä¸€æ­¥é‡‡æ ·å’Œæ¡ä»¶å­¦ä¹ å…ˆéªŒï¼Œæ‘’å¼ƒå…ˆå‰çŠ¶æ€ï¼Œå‡å°‘é‡‡æ ·æ­¥éª¤ã€‚è¯¥æ–¹æ³•åœ¨è¯­éŸ³æ ‡è®°æ ¼å¼ä¸Šæ“ä½œè§£è€¦ã€åˆ†è§£çš„ç»„ä»¶ï¼Œèƒ½ç²¾å‡†å»ºæ¨¡å„è¯­éŸ³å±æ€§ï¼Œæå‡TTSç³»ç»Ÿå¯¹æç¤ºè¯­éŸ³çš„ç²¾ç¡®å…‹éš†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å†…å®¹å‡†ç¡®æ€§ã€è‡ªç„¶åº¦ã€è¯­è°ƒç”Ÿæˆå’Œè¯´è¯äººé£æ ¼ä¿æŒç­‰æ–¹é¢è¾ƒç°æœ‰æ–¹æ³•è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSç³»ç»Ÿè¿‘å¹´æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä¸»è¦å¾—ç›Šäºæ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„å‘å±•ã€‚</li>
<li>ä¼ ç»ŸTTSæ–¹æ³•é‡‡ç”¨æ³¢å½¢æˆ–é¢‘è°±å›¾ç­‰è¯­éŸ³è¡¨ç°å½¢å¼ï¼Œåœ¨Flow Matchingæ¡†æ¶ä¸‹é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>OZSpeechæ˜¯é¦–ä¸ªæ¢ç´¢ä¼˜åŒ–ä¼ è¾“æ¡ä»¶æµåŒ¹é…çš„TTSæ–¹æ³•ã€‚</li>
<li>OZSpeeché‡‡ç”¨ä¸€æ­¥é‡‡æ ·å’Œæ¡ä»¶å­¦ä¹ å…ˆéªŒï¼Œç®€åŒ–è¿‡ç¨‹å¹¶æå‡æ€§èƒ½ã€‚</li>
<li>OZSpeechæ–¹æ³•åœ¨è¯­éŸ³æ ‡è®°æ ¼å¼ä¸Šæ“ä½œè§£è€¦çš„ç»„ä»¶ï¼Œèƒ½ç²¾å‡†å»ºæ¨¡å„è¯­éŸ³å±æ€§ã€‚</li>
<li>OZSpeechåœ¨å†…å®¹å‡†ç¡®æ€§ã€è‡ªç„¶åº¦ã€è¯­è°ƒç”Ÿæˆå’Œè¯´è¯äººé£æ ¼ä¿æŒç­‰æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c243f932226d92c40468490fa20dd6de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7b9151fe44927a89689f6c4b1c4048a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ea6aa40f2e6467556f52b55cf84de6c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RoVo-Robust-Voice-Protection-Against-Unauthorized-Speech-Synthesis-with-Embedding-Level-Perturbations"><a href="#RoVo-Robust-Voice-Protection-Against-Unauthorized-Speech-Synthesis-with-Embedding-Level-Perturbations" class="headerlink" title="RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with   Embedding-Level Perturbations"></a>RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with   Embedding-Level Perturbations</h2><p><strong>Authors:Seungmin Kim, Sohee Park, Donghyun Kim, Jisu Lee, Daeseon Choi</strong></p>
<p>With the advancement of AI-based speech synthesis technologies such as Deep Voice, there is an increasing risk of voice spoofing attacks, including voice phishing and fake news, through unauthorized use of othersâ€™ voices. Existing defenses that inject adversarial perturbations directly into audio signals have limited effectiveness, as these perturbations can easily be neutralized by speech enhancement methods. To overcome this limitation, we propose RoVo (Robust Voice), a novel proactive defense technique that injects adversarial perturbations into high-dimensional embedding vectors of audio signals, reconstructing them into protected speech. This approach effectively defends against speech synthesis attacks and also provides strong resistance to speech enhancement models, which represent a secondary attack threat.   In extensive experiments, RoVo increased the Defense Success Rate (DSR) by over 70% compared to unprotected speech, across four state-of-the-art speech synthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial speaker-verification API, effectively neutralizing speech synthesis attack. Moreover, RoVoâ€™s perturbations remained robust even under strong speech enhancement conditions, outperforming traditional methods. A user study confirmed that RoVo preserves both naturalness and usability of protected speech, highlighting its effectiveness in complex and evolving threat scenarios. </p>
<blockquote>
<p>éšç€åŸºäºæ·±åº¦å£°éŸ³ç­‰äººå·¥æ™ºèƒ½çš„è¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›æ­¥ï¼Œé€šè¿‡éæ³•ä½¿ç”¨ä»–äººå£°éŸ³è¿›è¡Œè¯­éŸ³æ¬ºéª—æ”»å‡»ï¼ˆåŒ…æ‹¬è¯­éŸ³é’“é±¼å’Œå‡æ–°é—»ï¼‰çš„é£é™©æ—¥ç›Šå¢åŠ ã€‚ç°æœ‰é€šè¿‡åœ¨éŸ³é¢‘ä¿¡å·ä¸­ç›´æ¥æ³¨å…¥å¯¹æŠ—æ€§æ‰°åŠ¨æ¥è¿›è¡Œé˜²å¾¡çš„æ–¹æ³•æ•ˆæœæœ‰é™ï¼Œå› ä¸ºè¿™äº›æ‰°åŠ¨å¾ˆå®¹æ˜“è¢«è¯­éŸ³å¢å¼ºæ–¹æ³•æ‰€ä¸­å’Œã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RoVoï¼ˆRobust Voiceï¼‰è¿™ä¸€æ–°å‹ä¸»åŠ¨é˜²å¾¡æŠ€æœ¯ï¼Œå®ƒå°†å¯¹æŠ—æ€§æ‰°åŠ¨æ³¨å…¥éŸ³é¢‘ä¿¡å·çš„é«˜ç»´åµŒå…¥å‘é‡ä¸­ï¼Œç„¶åé‡æ„ä¸ºå—ä¿æŠ¤çš„è¯­éŸ³ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆé˜²å¾¡è¯­éŸ³åˆæˆæ”»å‡»ï¼Œå¹¶å¯¹ä»£è¡¨æ¬¡è¦æ”»å‡»å¨èƒçš„è¯­éŸ³å¢å¼ºæ¨¡å‹æä¾›å¼ºå¤§çš„æŠµæŠ—åŠ›ã€‚åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œä¸æœªå—ä¿æŠ¤çš„è¯­éŸ³ç›¸æ¯”ï¼ŒRoVoå°†é˜²å¾¡æˆåŠŸç‡ï¼ˆDSRï¼‰æé«˜äº†70%ä»¥ä¸Šï¼Œæ¶µç›–äº†å››ç§æœ€å…ˆè¿›çš„è¯­éŸ³åˆæˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒRoVoåœ¨å•†ä¸šè¯­éŸ³éªŒè¯APIä¸Šå®ç°äº†99.5%çš„DSRï¼Œæœ‰æ•ˆä¸­å’Œäº†è¯­éŸ³åˆæˆæ”»å‡»ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨å¼ºçƒˆçš„è¯­éŸ³å¢å¼ºæ¡ä»¶ä¸‹ï¼ŒRoVoçš„æ‰°åŠ¨ä¾ç„¶ç¨³å¥ï¼Œä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚ç”¨æˆ·ç ”ç©¶è¯å®ï¼ŒRoVoä¿ç•™äº†å—ä¿æŠ¤è¯­éŸ³çš„è‡ªç„¶æ€§å’Œå¯ç”¨æ€§ï¼Œçªæ˜¾å…¶åœ¨å¤æ‚å’Œä¸æ–­å‘å±•çš„å¨èƒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12686v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRoVoï¼ˆRobust Voiceï¼‰çš„æ–°å‹ä¸»åŠ¨é˜²å¾¡æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡å‘éŸ³é¢‘ä¿¡å·çš„é«˜ç»´åµŒå…¥å‘é‡æ³¨å…¥å¯¹æŠ—æ€§æ‰°åŠ¨æ¥æŠµæŠ—è¯­éŸ³æ¬ºéª—æ”»å‡»ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„ç›´æ¥å¯¹éŸ³é¢‘ä¿¡å·æ³¨å…¥æ‰°åŠ¨çš„æ–¹æ³•ï¼ŒRoVoèƒ½å¤Ÿæœ‰æ•ˆå¯¹æŠ—è¯­éŸ³å¢å¼ºæ–¹æ³•çš„ç ´è§£ï¼Œå¹¶æä¾›å¼ºå¤§çš„é˜²æŠ¤æ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼ŒRoVoç›¸è¾ƒäºæœªä¿æŠ¤çš„è¯­éŸ³ï¼Œåœ¨å››ç§å…ˆè¿›çš„è¯­éŸ³åˆæˆæ¨¡å‹ä¸Šçš„é˜²å¾¡æˆåŠŸç‡ï¼ˆDSRï¼‰æå‡äº†70%ä»¥ä¸Šã€‚ç‰¹åˆ«æ˜¯åœ¨å•†ä¸šè¯­éŸ³è¯†åˆ«APIä¸Šï¼ŒRoVoçš„DSRè¾¾åˆ°äº†99.5%ï¼Œæœ‰æ•ˆä¸­æ–­äº†è¯­éŸ³åˆæˆæ”»å‡»ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨å¼ºçƒˆçš„è¯­éŸ³å¢å¼ºæ¡ä»¶ä¸‹ï¼ŒRoVoçš„æ‰°åŠ¨ä¾ç„¶ç¨³å¥ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚ç”¨æˆ·ç ”ç©¶è¯å®ï¼ŒRoVoèƒ½å¤Ÿä¿æŒä¿æŠ¤è¯­éŸ³çš„è‡ªç„¶æ€§å’Œå¯ç”¨æ€§ï¼Œåœ¨å¤æ‚çš„ä¸æ–­å˜åŒ–çš„å¨èƒåœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RoVoæ˜¯ä¸€ç§æ–°å‹çš„ä¸»åŠ¨é˜²å¾¡æŠ€æœ¯ï¼Œé€šè¿‡å‘éŸ³é¢‘ä¿¡å·çš„é«˜ç»´åµŒå…¥å‘é‡æ³¨å…¥å¯¹æŠ—æ€§æ‰°åŠ¨æ¥é˜²å¾¡è¯­éŸ³æ¬ºéª—æ”»å‡»ã€‚</li>
<li>RoVoèƒ½æœ‰æ•ˆå¯¹æŠ—è¯­éŸ³å¢å¼ºæ–¹æ³•çš„ç ´è§£ï¼Œæä¾›äº†å¼ºå¤§çš„é˜²æŠ¤æ•ˆæœã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒRoVoåœ¨å¤šç§å…ˆè¿›çš„è¯­éŸ³åˆæˆæ¨¡å‹ä¸Šçš„é˜²å¾¡æˆåŠŸç‡ï¼ˆDSRï¼‰æå‡äº†70%ä»¥ä¸Šã€‚</li>
<li>RoVoåœ¨å•†ä¸šè¯­éŸ³è¯†åˆ«APIä¸Šçš„DSRè¾¾åˆ°äº†99.5%ï¼Œèƒ½æœ‰æ•ˆä¸­æ–­è¯­éŸ³åˆæˆæ”»å‡»ã€‚</li>
<li>RoVoçš„æ‰°åŠ¨åœ¨å¼ºçƒˆçš„è¯­éŸ³å¢å¼ºæ¡ä»¶ä¸‹ä¾ç„¶ç¨³å¥ï¼Œæ€§èƒ½è¶…è¶Šä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶è¯å®ï¼ŒRoVoèƒ½å¤Ÿä¿æŒä¿æŠ¤è¯­éŸ³çš„è‡ªç„¶æ€§å’Œå¯ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fa34982b0b1a7c43396f3023c6c2763f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-319386b67e3804d8bf39be30815b9ee8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3920983b2afe1e96e515cd26c6a28565.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab7d27b07152d54b88a6eb423ba039d0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Chain-Talker-Chain-Understanding-and-Rendering-for-Empathetic-Conversational-Speech-Synthesis"><a href="#Chain-Talker-Chain-Understanding-and-Rendering-for-Empathetic-Conversational-Speech-Synthesis" class="headerlink" title="Chain-Talker: Chain Understanding and Rendering for Empathetic   Conversational Speech Synthesis"></a>Chain-Talker: Chain Understanding and Rendering for Empathetic   Conversational Speech Synthesis</h2><p><strong>Authors:Yifan Hu, Rui Liu, Yi Ren, Xiang Yin, Haizhou Li</strong></p>
<p>Conversational Speech Synthesis (CSS) aims to align synthesized speech with the emotional and stylistic context of user-agent interactions to achieve empathy. Current generative CSS models face interpretability limitations due to insufficient emotional perception and redundant discrete speech coding. To address the above issues, we present Chain-Talker, a three-stage framework mimicking human cognition: Emotion Understanding derives context-aware emotion descriptors from dialogue history; Semantic Understanding generates compact semantic codes via serialized prediction; and Empathetic Rendering synthesizes expressive speech by integrating both components. To support emotion modeling, we develop CSS-EmCap, an LLM-driven automated pipeline for generating precise conversational speech emotion captions. Experiments on three benchmark datasets demonstrate that Chain-Talker produces more expressive and empathetic speech than existing methods, with CSS-EmCap contributing to reliable emotion modeling. The code and demos are available at: <a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/Chain-Talker">https://github.com/AI-S2-Lab/Chain-Talker</a>. </p>
<blockquote>
<p>å¯¹è¯å¼è¯­éŸ³åˆæˆï¼ˆCSSï¼‰æ—¨åœ¨å°†åˆæˆè¯­éŸ³ä¸ç”¨æˆ·ä»£ç†äº¤äº’çš„æƒ…æ„Ÿå’Œé£æ ¼è¯­å¢ƒç›¸ç»“åˆï¼Œä»¥å®ç°å…±æƒ…ã€‚å½“å‰çš„ç”Ÿæˆå¼CSSæ¨¡å‹é¢ä¸´å¯è§£é‡Šæ€§æœ‰é™çš„éš¾é¢˜ï¼ŒåŸå› åœ¨äºæƒ…æ„Ÿæ„ŸçŸ¥ä¸è¶³å’Œå†—ä½™çš„ç¦»æ•£è¯­éŸ³ç¼–ç ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Chain-Talkerï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡ä»¿äººç±»è®¤çŸ¥çš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼šæƒ…æ„Ÿç†è§£ä»å¯¹è¯å†å²ä¸­æ¨å¯¼å‡ºä¸Šä¸‹æ–‡ç›¸å…³çš„æƒ…æ„Ÿæè¿°ç¬¦ï¼›è¯­ä¹‰ç†è§£é€šè¿‡åºåˆ—åŒ–é¢„æµ‹ç”Ÿæˆç´§å‡‘çš„è¯­ä¹‰ä»£ç ï¼›å…±æƒ…æ¸²æŸ“é€šè¿‡æ•´åˆè¿™ä¸¤ä¸ªç»„ä»¶åˆæˆè¡¨è¾¾æ€§è¯­éŸ³ã€‚ä¸ºäº†æ”¯æŒæƒ…æ„Ÿå»ºæ¨¡ï¼Œæˆ‘ä»¬å¼€å‘äº†CSS-EmCapï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºç”Ÿæˆç²¾ç¡®çš„å¯¹è¯è¯­éŸ³æƒ…æ„Ÿå­—å¹•ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒChain-Talkerç›¸æ¯”ç°æœ‰æ–¹æ³•äº§ç”Ÿäº†æ›´å…·è¡¨ç°åŠ›å’Œå…±æƒ…æ•ˆæœçš„è¯­éŸ³ï¼ŒCSS-EmCapå¯¹å¯é çš„æƒ…æ„Ÿå»ºæ¨¡åšå‡ºäº†è´¡çŒ®ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/Chain-Talker%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/Chain-Talkeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12597v1">PDF</a> 16 pages, 5 figures, 5 tables. Accepted by ACL 2025 (Findings)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Conversational Speech Synthesisï¼ˆCSSï¼‰çš„ç›®æ ‡æ˜¯é€šè¿‡åˆæˆä¸ç”¨æˆ·ä»£ç†äº’åŠ¨æ—¶çš„æƒ…æ„Ÿå’Œé£æ ¼ä¸€è‡´çš„è¯­éŸ³æ¥å®ç°å…±æƒ…ã€‚é’ˆå¯¹å½“å‰CSSæ¨¡å‹åœ¨æƒ…æ„Ÿæ„ŸçŸ¥å’Œå†—ä½™ç¦»æ•£è¯­éŸ³ç¼–ç æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†Chain-Talkeræ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¨¡ä»¿äººç±»è®¤çŸ¥çš„ä¸‰ä¸ªæ­¥éª¤ï¼šæƒ…æ„Ÿç†è§£ä»å¯¹è¯å†å²ä¸­å¾—å‡ºæƒ…å¢ƒæ„ŸçŸ¥çš„æƒ…æ„Ÿæè¿°ç¬¦ï¼›è¯­ä¹‰ç†è§£é€šè¿‡åºåˆ—åŒ–é¢„æµ‹ç”Ÿæˆç´§å‡‘çš„è¯­ä¹‰ä»£ç ï¼›å…±æƒ…æ¸²æŸ“é€šè¿‡ç»“åˆè¿™ä¸¤ä¸ªç»„ä»¶åˆæˆå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚ä¸ºæ”¯æŒæƒ…æ„Ÿå»ºæ¨¡ï¼Œå¼€å‘äº†CSS-EmCapï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–ç®¡é“ï¼Œç”¨äºç”Ÿæˆç²¾ç¡®çš„å¯¹è¯è¯­æ–™æƒ…æ„Ÿå­—å¹•ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒChain-Talkerç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œèƒ½ç”Ÿæˆæ›´å¯Œæœ‰è¡¨ç°åŠ›å’Œå…±æƒ…çš„è¯­éŸ³ï¼ŒCSS-EmCapå¯¹å¯é çš„æƒ…æ„Ÿå»ºæ¨¡èµ·åˆ°äº†è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Conversational Speech Synthesis (CSS)æ—¨åœ¨åˆæˆä¸ç”¨æˆ·ä»£ç†äº’åŠ¨æ—¶æƒ…æ„Ÿå’Œé£æ ¼ä¸€è‡´çš„è¯­éŸ³ï¼Œä»¥å®ç°å…±æƒ…ã€‚</li>
<li>å½“å‰CSSæ¨¡å‹é¢ä¸´æƒ…æ„Ÿæ„ŸçŸ¥ä¸è¶³å’Œå†—ä½™ç¦»æ•£è¯­éŸ³ç¼–ç çš„å±€é™æ€§ã€‚</li>
<li>Chain-Talkeræ¡†æ¶æ¨¡ä»¿äººç±»è®¤çŸ¥çš„ä¸‰ä¸ªæ­¥éª¤ï¼šæƒ…æ„Ÿç†è§£ã€è¯­ä¹‰ç†è§£å’Œå…±æƒ…æ¸²æŸ“ã€‚</li>
<li>æƒ…æ„Ÿç†è§£é€šè¿‡å¯¹è¯å†å²å¾—å‡ºæƒ…å¢ƒæ„ŸçŸ¥çš„æƒ…æ„Ÿæè¿°ç¬¦ã€‚</li>
<li>è¯­ä¹‰ç†è§£ç”Ÿæˆç´§å‡‘çš„è¯­ä¹‰ä»£ç ï¼Œé€šè¿‡åºåˆ—åŒ–é¢„æµ‹å®ç°ã€‚</li>
<li>Empathetic Renderingç»“åˆäº†æƒ…æ„Ÿç†è§£å’Œè¯­ä¹‰ç†è§£ï¼Œåˆæˆå¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-da9f623349aa038129f1df66714e9b2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f6cdd6135c0673428456dceaddb61d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf76b9501af682faaab986e5985edde6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Shallow-Flow-Matching-for-Coarse-to-Fine-Text-to-Speech-Synthesis"><a href="#Shallow-Flow-Matching-for-Coarse-to-Fine-Text-to-Speech-Synthesis" class="headerlink" title="Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis"></a>Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis</h2><p><strong>Authors:Dong Yang, Yiyi Cai, Yuki Saito, Lixu Wang, Hiroshi Saruwatari</strong></p>
<p>We propose a shallow flow matching (SFM) mechanism to enhance flow matching (FM)-based text-to-speech (TTS) models within a coarse-to-fine generation paradigm. SFM constructs intermediate states along the FM paths using coarse output representations. During training, we introduce an orthogonal projection method to adaptively determine the temporal position of these states, and apply a principled construction strategy based on a single-segment piecewise flow. The SFM inference starts from the intermediate state rather than pure noise and focuses computation on the latter stages of the FM paths. We integrate SFM into multiple TTS models with a lightweight SFM head. Experiments show that SFM consistently improves the naturalness of synthesized speech in both objective and subjective evaluations, while significantly reducing inference when using adaptive-step ODE solvers. Demo and codes are available at <a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/">https://ydqmkkx.github.io/SFMDemo/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æµ…æµåŒ¹é…ï¼ˆSFMï¼‰æœºåˆ¶ï¼Œä»¥åœ¨ç²—åˆ°ç»†ç”ŸæˆèŒƒå¼å†…å¢å¼ºåŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚SFMåˆ©ç”¨ç²—è¾“å‡ºè¡¨ç¤ºæ„å»ºFMè·¯å¾„ä¸­çš„ä¸­é—´çŠ¶æ€ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ­£äº¤æŠ•å½±æ–¹æ³•æ¥è‡ªé€‚åº”åœ°ç¡®å®šè¿™äº›çŠ¶æ€çš„æ—¶é—´ä½ç½®ï¼Œå¹¶åŸºäºå•æ®µåˆ†æ®µæµåº”ç”¨äº†ä¸€ç§æœ‰åŸåˆ™çš„æ„å»ºç­–ç•¥ã€‚SFMæ¨ç†ä»ä¸­é—´çŠ¶æ€å¼€å§‹ï¼Œè€Œä¸æ˜¯ä»çº¯å™ªå£°å¼€å§‹ï¼Œå¹¶å°†è®¡ç®—é‡ç‚¹æ”¾åœ¨FMè·¯å¾„çš„åæœŸé˜¶æ®µã€‚æˆ‘ä»¬å°†SFMé›†æˆåˆ°å¤šä¸ªTTSæ¨¡å‹ä¸­ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§çš„SFMå¤´ã€‚å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å®¢è§‚è¿˜æ˜¯ä¸»è§‚è¯„ä¼°ä¸­ï¼ŒSFMéƒ½èƒ½æŒç»­æé«˜åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ï¼ŒåŒæ—¶åœ¨é‡‡ç”¨è‡ªé€‚åº”æ­¥é•¿ODEæ±‚è§£å™¨æ—¶æ˜¾è‘—é™ä½æ¨ç†æ—¶é—´ã€‚æ¼”ç¤ºå’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/%E8%AE%BF%E9%97%AE%E3%80%82">https://ydqmkkx.github.io/SFMDemo/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12226v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æµ…æµåŒ¹é…ï¼ˆSFMï¼‰æœºåˆ¶ï¼Œç”¨äºåœ¨ç²—åˆ°ç»†ç”ŸæˆèŒƒå¼ä¸­å¢å¼ºåŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚SFMé€šè¿‡åœ¨FMè·¯å¾„ä¸Šæ„å»ºä¸­é—´çŠ¶æ€ï¼Œåˆ©ç”¨ç²—è¾“å‡ºè¡¨ç¤ºã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¼•å…¥æ­£äº¤æŠ•å½±æ–¹æ³•è‡ªé€‚åº”ç¡®å®šè¿™äº›çŠ¶æ€çš„æ—¶é—´ä½ç½®ï¼Œå¹¶åŸºäºå•æ®µåˆ†æ®µæµåº”ç”¨æœ‰åŸåˆ™çš„æ„å»ºç­–ç•¥ã€‚SFMæ¨ç†ä»ä¸­é—´çŠ¶æ€å¼€å§‹ï¼Œè€Œä¸æ˜¯ä»çº¯å™ªå£°å¼€å§‹ï¼Œå¹¶å°†è®¡ç®—é‡ç‚¹æ”¾åœ¨FMè·¯å¾„çš„åæœŸé˜¶æ®µã€‚æˆ‘ä»¬å°†SFMé›†æˆåˆ°å¤šä¸ªTTSæ¨¡å‹ä¸­ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§SFMå¤´ã€‚å®éªŒè¡¨æ˜ï¼ŒSFMåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­å‡æé«˜äº†åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ï¼ŒåŒæ—¶åœ¨è‡ªé€‚åº”æ­¥é•¿ODEæ±‚è§£å™¨ä½¿ç”¨æ—¶æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚ç›¸å…³æ¼”ç¤ºå’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/%E6%89%BE%E5%88%B0%E3%80%82">https://ydqmkkx.github.io/SFMDemo/æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æµ…æµåŒ¹é…ï¼ˆSFMï¼‰æœºåˆ¶æ¥å¢å¼ºåŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SFMæœºåˆ¶é€šè¿‡æ„å»ºFMè·¯å¾„ä¸Šçš„ä¸­é—´çŠ¶æ€å¹¶åˆ©ç”¨ç²—è¾“å‡ºè¡¨ç¤ºæ¥å®ç°ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨æ­£äº¤æŠ•å½±æ–¹æ³•è‡ªé€‚åº”ç¡®å®šä¸­é—´çŠ¶æ€çš„æ—¶é—´ä½ç½®ã€‚</li>
<li>å¼•å…¥äº†åŸºäºå•æ®µåˆ†æ®µæµçš„æ„å»ºç­–ç•¥ã€‚</li>
<li>SFMæ¨ç†ä»ä¸­é—´çŠ¶æ€å¼€å§‹ï¼Œå…³æ³¨FMè·¯å¾„çš„åæœŸè®¡ç®—ã€‚</li>
<li>SFMè¢«é›†æˆåˆ°å¤šä¸ªTTSæ¨¡å‹ä¸­ï¼Œå¹¶é€šè¿‡è½»é‡çº§SFMå¤´å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-05b333ababfe421ec07abae91b89b520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93ccf19c0b69d0575db4e284364761f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5e6db09a073f6c66aee05d9747bc3f7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Rethinking-Optimal-Verification-Granularity-for-Compute-Efficient-Test-Time-Scaling"><a href="#Rethinking-Optimal-Verification-Granularity-for-Compute-Efficient-Test-Time-Scaling" class="headerlink" title="Rethinking Optimal Verification Granularity for Compute-Efficient   Test-Time Scaling"></a>Rethinking Optimal Verification Granularity for Compute-Efficient   Test-Time Scaling</h2><p><strong>Authors:Hao Mark Chen, Guanxi Lu, Yasuyuki Okoshi, Zhiwen Mo, Masato Motomura, Hongxiang Fan</strong></p>
<p>Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1% over Beam Search and 3.6% over Best-of-N, while reducing FLOPs by over 52%. We will open-source the code to support future research. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰å·²è¯æ˜å¯ä»¥æœ‰æ•ˆæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚éªŒè¯åœ¨TTSä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œç”±äºéªŒè¯çš„è´¨é‡å’Œè®¡ç®—æˆæœ¬ï¼Œå®ƒåŒæ—¶å½±å“ï¼ˆ1ï¼‰æ¨ç†æ€§èƒ½å’Œï¼ˆ2ï¼‰è®¡ç®—æ•ˆç‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æŒ‘æˆ˜äº†éªŒè¯çš„ä¼ ç»ŸèŒƒå¼ï¼Œå¹¶é¦–æ¬¡å°è¯•ç³»ç»Ÿåœ°ç ”ç©¶éªŒè¯ç²’åº¦çš„å½±å“ï¼Œå³éªŒè¯å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¢«è°ƒç”¨çš„é¢‘ç‡ï¼Œè€Œä¸ä»…ä»…æ˜¯éªŒè¯æœ€ç»ˆè¾“å‡ºæˆ–å•ä¸ªç”Ÿæˆæ­¥éª¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯å˜ç²’åº¦æœç´¢ï¼ˆVG-Searchï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¯è°ƒç²’åº¦å‚æ•°gæ¨å¹¿æŸæœç´¢å’ŒNé€‰æœ€ä½³é‡‡æ ·çš„ç»Ÿä¸€ç®—æ³•ã€‚ä½¿ç”¨VG-Searchåœ¨å¤šç§è®¡ç®—é¢„ç®—ã€ç”Ÿæˆå™¨éªŒè¯å™¨é…ç½®å’Œä»»åŠ¡å±æ€§ä¸‹è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒåŠ¨æ€é€‰æ‹©gå¯ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œç¼©æ”¾æ€§èƒ½ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”VG-Searchç­–ç•¥ï¼Œä¸æŸæœç´¢ç›¸æ¯”ï¼Œå®ç°äº†é«˜è¾¾3.1%çš„å‡†ç¡®ç‡æå‡ï¼Œä¸Né€‰æœ€ä½³ç­–ç•¥ç›¸æ¯”æé«˜äº†3.6%ï¼ŒåŒæ—¶å‡å°‘äº†è¶…è¿‡52%çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€‚æˆ‘ä»¬å°†å…¬å¼€æºä»£ç ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11730v1">PDF</a> Preprint. Under review</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æè¿°äº†æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚éªŒè¯åœ¨TTSä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œå½±å“æ¨ç†æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚æœ¬ç ”ç©¶æŒ‘æˆ˜äº†éªŒè¯çš„ä¼ ç»Ÿæ¨¡å¼ï¼Œé¦–æ¬¡å°è¯•ç³»ç»Ÿåœ°ç ”ç©¶éªŒè¯ç²’åº¦çš„å½±å“ï¼Œå³éªŒè¯å™¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¢«è°ƒç”¨çš„é¢‘ç‡ï¼Œè€Œä¸ä»…ä»…æ˜¯éªŒè¯æœ€ç»ˆè¾“å‡ºæˆ–å•ä¸ªç”Ÿæˆæ­¥éª¤ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯å˜ç²’åº¦æœç´¢ï¼ˆVG-Searchï¼‰ç®—æ³•ï¼Œé€šè¿‡å¯è°ƒçš„ç²’åº¦å‚æ•°gæ¥æ¦‚æ‹¬beamæœç´¢å’ŒBest-of-Né‡‡æ ·ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°åŠ¨æ€é€‰æ‹©gå¯ä»¥æé«˜è®¡ç®—æ•ˆç‡å’Œç¼©æ”¾æ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”VG-Searchç­–ç•¥ï¼Œä¸Beam Searchç›¸æ¯”æé«˜äº†é«˜è¾¾3.1%çš„å‡†ç¡®ç‡ï¼Œä¸Best-of-Nç›¸æ¯”æé«˜äº†é«˜è¾¾3.6%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶é™ä½äº†è¶…è¿‡52%çš„FLOPsã€‚æˆ‘ä»¬å°†å¼€æºä»£ç ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰æŠ€æœ¯å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éªŒè¯åœ¨TTSä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼ŒåŒæ—¶å½±å“æ¨ç†æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>æœ¬ç ”ç©¶æŒ‘æˆ˜äº†éªŒè¯çš„ä¼ ç»Ÿæ¨¡å¼ï¼Œå¹¶ç³»ç»Ÿåœ°ç ”ç©¶äº†éªŒè¯ç²’åº¦å¯¹TTSçš„å½±å“ã€‚</li>
<li>å¼•å…¥äº†å¯å˜ç²’åº¦æœç´¢ï¼ˆVG-Searchï¼‰ç®—æ³•ï¼Œé€šè¿‡è°ƒèŠ‚ç²’åº¦å‚æ•°gæ¥æé«˜è®¡ç®—æ•ˆç‡å’Œæ¨ç†æ€§èƒ½ã€‚</li>
<li>åŠ¨æ€é€‰æ‹©gå¯ä»¥è¿›ä¸€æ­¥æé«˜è®¡ç®—æ•ˆç‡å’Œç¼©æ”¾æ€§èƒ½ã€‚</li>
<li>è‡ªé€‚åº”VG-Searchç­–ç•¥åœ¨å‡†ç¡®ç‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4f101bf2775b97a597e50486b127906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8cd0bb8c67055f2a9a9b831099a461f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-332a1cbf8a71aa0e5e28b983c9e17111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b78848698f88512ea5afc13d1da6d49a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="F5-TTS-A-Fairytaler-that-Fakes-Fluent-and-Faithful-Speech-with-Flow-Matching"><a href="#F5-TTS-A-Fairytaler-that-Fakes-Fluent-and-Faithful-Speech-with-Flow-Matching" class="headerlink" title="F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow   Matching"></a>F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow   Matching</h2><p><strong>Authors:Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen</strong></p>
<p>This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our modelâ€™s performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our F5-TTS exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. We have released all codes and checkpoints to promote community development, at <a target="_blank" rel="noopener" href="https://swivid.github.io/F5-TTS/">https://SWivid.github.io/F5-TTS/</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†F5-TTSï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨éè‡ªå›å½’çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿï¼ŒåŸºäºå¸¦æœ‰æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰çš„æµç¨‹åŒ¹é…ã€‚è¯¥ç³»ç»Ÿæ— éœ€å¤æ‚çš„è®¾è®¡ï¼Œå¦‚æŒç»­æ—¶é—´æ¨¡å‹ã€æ–‡æœ¬ç¼–ç å™¨å’ŒéŸ³ç´ å¯¹é½ã€‚æ–‡æœ¬è¾“å…¥åªéœ€ç”¨å¡«å……æ ‡è®°å¡«å……è‡³ä¸è¾“å…¥è¯­éŸ³ç›¸åŒçš„é•¿åº¦ï¼Œç„¶åå¯¹è¯­éŸ³ç”Ÿæˆè¿›è¡Œå»å™ªå¤„ç†ï¼Œè¿™ç§æ–¹å¼çš„å¯è¡Œæ€§æœ€åˆç”±E2 TTSè¯æ˜ã€‚ç„¶è€Œï¼ŒE2 TTSçš„åŸå§‹è®¾è®¡ç”±äºå…¶æ”¶æ•›é€Ÿåº¦æ…¢å’Œç¨³å¥æ€§ä½ï¼Œéš¾ä»¥è·Ÿéšåº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ConvNeXtå¯¹è¾“å…¥è¿›è¡Œå»ºæ¨¡ï¼Œä»¥ä¼˜åŒ–æ–‡æœ¬è¡¨ç¤ºï¼Œä½¿å…¶æ˜“äºä¸è¯­éŸ³å¯¹é½ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é—´çš„æ‘‡æ‘†é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è¿™ç§ç”¨äºæµç¨‹æ­¥éª¤çš„é‡‡æ ·ç­–ç•¥å¯ä»¥è½»æ¾åœ°åº”ç”¨äºç°æœ‰çš„åŸºäºæµç¨‹åŒ¹é…æ¨¡å‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬çš„è®¾è®¡å…è®¸æ›´å¿«çš„è®­ç»ƒï¼Œå®ç°æ¨ç†å®æ—¶è½¬å½•å› å­ï¼ˆRTFï¼‰ä¸º0.15ï¼Œä¸ç°æœ‰çš„åŸºäºæ‰©æ•£çš„TTSæ¨¡å‹ç›¸æ¯”ï¼Œè¿™ä¸€æ”¹è¿›éå¸¸æ˜¾è‘—ã€‚æˆ‘ä»¬çš„F5-TTSåœ¨å…¬å…±çš„10ä¸‡å°æ—¶å¤šè¯­ç§æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¡¨ç°å‡ºé«˜åº¦è‡ªç„¶å’Œè¡¨è¾¾èƒ½åŠ›çš„é›¶æ ·æœ¬èƒ½åŠ›ã€æ— ç¼çš„ä»£ç åˆ‡æ¢èƒ½åŠ›å’Œé€Ÿåº¦æ§åˆ¶æ•ˆç‡ã€‚ä¸ºäº†ä¿ƒè¿›ç¤¾åŒºå‘å±•ï¼Œæˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://swivid.github.io/F5-TTS/%E5%8F%96%E5%B9%BF%E5%AE%9E%E7%94%A8%E4%BA%86%E5%B9%B3%E9%A3%9F%E6%AF%AD%E4%BF%AE">https://SWivid.github.io/F5-TTS/å‘å¸ƒäº†æ‰€æœ‰ä»£ç å’Œæ£€æŸ¥ç‚¹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06885v3">PDF</a> 17 pages, 9 tables, 3 figures</p>
<p><strong>Summary</strong><br>åŸºäºæ‰©æ•£å˜æ¢ï¼ˆDiTï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»ŸF5-TTSè¢«å¼•å…¥ã€‚å®ƒé€šè¿‡æµåŒ¹é…å®ç°éè‡ªå›å½’æ€§ï¼Œæ— éœ€å¤æ‚çš„ç»„ä»¶è®¾è®¡å¦‚æŒç»­æ—¶é—´æ¨¡å‹ã€æ–‡æœ¬ç¼–ç å™¨å’ŒéŸ³ç´ å¯¹é½ã€‚é€šè¿‡å¯¹æ–‡æœ¬è¾“å…¥å¡«å……ç©ºç™½æ ‡è®°ï¼Œå¯¹è¯­éŸ³è¿›è¡Œé™å™ªç”Ÿæˆã€‚ä¸ºæ”¹è¿›åŸå§‹E2 TTSè®¾è®¡å¸¦æ¥çš„ç¼“æ…¢æ”¶æ•›å’Œä½é²æ£’æ€§é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ConvNeXtè¿›è¡Œè¾“å…¥å»ºæ¨¡å¹¶æ”¹è¿›äº†æ–‡æœ¬è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSway Samplingçš„ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½å’Œæ•ˆç‡ã€‚æ­¤é‡‡æ ·ç­–ç•¥å¯è½»æ¾åº”ç”¨äºç°æœ‰çš„æµåŒ¹é…æ¨¡å‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬çš„è®¾è®¡åŠ é€Ÿäº†è®­ç»ƒè¿‡ç¨‹ï¼Œå®ç°äº†0.15çš„æ¨ç†RTFï¼Œç›¸è¾ƒäºç°æœ‰çš„æ‰©æ•£å¼TTSæ¨¡å‹æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚F5-TTSåœ¨å…¬å…±çš„10ä¸‡å°æ—¶å¤šè¯­ç§æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå±•ç°å‡ºè‡ªç„¶ã€è¡¨è¾¾åŠ›å¼ºã€æ— ç¼åˆ‡æ¢ä»£ç èƒ½åŠ›å’Œé€Ÿåº¦æ§åˆ¶æ•ˆç‡ç­‰ç‰¹ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>F5-TTSæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£å˜æ¢çš„éè‡ªå›å½’æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡ç®€å•çš„å¡«å……ç©ºç™½æ ‡è®°å’Œé™å™ªå®ç°è¯­éŸ³ç”Ÿæˆï¼Œæ— éœ€å¤æ‚çš„ç»„ä»¶è®¾è®¡ã€‚</li>
<li>å¼•å…¥ConvNeXtæ”¹è¿›æ–‡æœ¬è¡¨ç¤ºå’Œå¯¹é½é—®é¢˜ã€‚</li>
<li>æå‡ºSway Samplingç­–ç•¥ï¼Œæé«˜æ¨¡å‹æ€§èƒ½å’Œæ•ˆç‡ï¼Œå¯è½»æ¾åº”ç”¨äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>å®ç°äº†å¿«é€Ÿçš„è®­ç»ƒè¿‡ç¨‹å’Œé«˜æ•ˆçš„æ¨ç†RTFã€‚</li>
<li>F5-TTSåœ¨å¤šè¯­ç§æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå±•ç°å‡ºè‡ªç„¶ã€è¡¨è¾¾åŠ›å¼ºã€æ— ç¼åˆ‡æ¢ä»£ç èƒ½åŠ›å’Œé€Ÿåº¦æ§åˆ¶æ•ˆç‡ç­‰ç‰¹ç‚¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-871a524a04b54257cec43839b351bc99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67dcb157c753501c66d6578d02ab5c0d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cbeb15bbf97efd2c8e4239904defc5ce.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Beyond Words Multimodal LLM Knows When to Speak
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-2dbd53b28128cc9bbb530fa5610b868a.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-22  Automated Fetal Biometry Assessment with Deep Ensembles using   Sparse-Sampling of 2D Intrapartum Ultrasound Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
