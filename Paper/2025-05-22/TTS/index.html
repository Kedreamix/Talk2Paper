<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-05-22  FMSD-TTS Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ü-Tsang, Amdo and Kham Speech Dataset Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-bf76b9501af682faaab986e5985edde6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    46 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-22-更新"><a href="#2025-05-22-更新" class="headerlink" title="2025-05-22 更新"></a>2025-05-22 更新</h1><h2 id="FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation"><a href="#FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation" class="headerlink" title="FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ü-Tsang, Amdo and Kham Speech Dataset Generation"></a>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ü-Tsang, Amdo and Kham Speech Dataset Generation</h2><p><strong>Authors:Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi</strong></p>
<p>Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-&quot;U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality. </p>
<blockquote>
<p>藏语是一种资源匮乏的语言，其三大方言区——乌齐、安多和康区的平行语音语料库极其有限，限制了语音建模的进展。为了解决这一问题，我们提出了FMSD-TTS，这是一个小样本、多发言人、多方言的文本到语音框架，它可以从有限的参考音频和明确的方言标签中合成平行的方言语音。我们的方法具有新颖的发声人-方言融合模块和方言专用动态路由网络（DSDR-Net），能够捕捉方言间的精细声学和语言变异，同时保留发声人的身份。大量的客观和主观评估表明，FMSD-TTS在方言表达力和发声人相似性方面显著优于基线系统。我们进一步通过具有挑战性的语音到语音的方言转换任务来验证合成语音的质量和实用性。我们的贡献包括：（1）针对藏语多方言语音合成的少样本TTS系统，（2）公开发布由FMSD-TTS生成的大规模合成藏语语音语料库，（3）开放源代码评估工具包，用于标准化评估发声人相似性、方言一致性和音频质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14351v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对藏语这一资源匮乏型语言的TTS系统。为应对藏语三种主要方言之间平行语料稀缺的问题，研究者提出了FMSD-TTS系统，该框架能从有限的参考音频和明确的方言标签中合成平行方言语音。它采用了多语种语音融合模块及方言专业动态路由网络，可以精细捕捉方言间的声学差异，同时在发音上保留个体差异。FMSD-TTS在各种评价和实验中的表现均优于基准模型。此外，研究者还首次公开了大规模藏语合成语音库和开源评估工具包。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FMSD-TTS是为藏语这一资源匮乏型语言设计的TTS系统。</li>
<li>系统能有效合成藏语的三大方言（乌兹、安多和康区方言）。</li>
<li>FMSD-TTS通过引入多语种语音融合模块和方言专业动态路由网络，能够捕捉方言间的细微差异并保留说话人的身份特征。</li>
<li>该系统在客观和主观评价中均表现出优异的性能，特别是在方言表达力和说话人相似性方面。</li>
<li>FMSD-TTS成功地完成了挑战性方言语音转换任务。</li>
<li>研究者首次公开了由FMSD-TTS生成的大规模藏语合成语音库。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3a712b79d7ffc4c1dca0f96958d2ba5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1785cc70a11836b2715822d27c917e0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a811d1d51e8b11dcb30516dfa2c64ac7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a42a77d797ad5aa3c3834fc192c0333d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a0dee36095191e6784bfb9b357a3309.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AudioJailbreak-Jailbreak-Attacks-against-End-to-End-Large-Audio-Language-Models"><a href="#AudioJailbreak-Jailbreak-Attacks-against-End-to-End-Large-Audio-Language-Models" class="headerlink" title="AudioJailbreak: Jailbreak Attacks against End-to-End Large   Audio-Language Models"></a>AudioJailbreak: Jailbreak Attacks against End-to-End Large   Audio-Language Models</h2><p><strong>Authors:Guangke Chen, Fu Song, Zhe Zhao, Xiaojun Jia, Yang Liu, Yanchen Qiao, Weizhe Zhang</strong></p>
<p>Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website <a target="_blank" rel="noopener" href="https://audiojailbreak.github.io/AudioJailbreak">https://audiojailbreak.github.io/AudioJailbreak</a>. </p>
<blockquote>
<p>近期对大型音频语言模型（LALM）的越狱攻击（Jailbreak攻击）进行了研究，但这些攻击的效能、适用性和实用性均不够理想，特别是假设攻击者可以充分操控用户提示的情况下。在这项工作中，我们首先进行了大量实验，证明先进的文本越狱攻击无法轻易通过文本到语音（TTS）技术应用于端到端的LALM模型。随后，我们提出了AudioJailbreak这一新型音频越狱攻击方法，具有以下特点：</p>
</blockquote>
<ol>
<li>异步性：越狱音频无需通过制作后缀越狱音频与用户提示在时间轴上进行对齐；</li>
<li>通用性：通过融入多个提示来生成扰动，单个越狱扰动对不同的提示都有效；</li>
<li>隐蔽性：通过提出各种意图隐藏策略，越狱音频的恶意意图不会使受害者提高警惕；</li>
</ol>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14103v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文研究了对大型音频语言模型（LALM）的越狱攻击，但现有攻击在转换文本到语音（TTS）技术的端到端LALM模式下的效果、适用性和实用性有限。为此，提出了AudioJailbreak新型音频越狱攻击，具有异步性、普遍性、隐蔽性和抗干扰性等特点，适用于无法完全操控用户提示的对手，攻击场景更广泛。实验证明AudioJailbreak对目前大多数LALM模型高度有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有文本越狱攻击难以直接应用于端到端的LALM模型。</li>
<li>AudioJailbreak是一种新型音频越狱攻击，具备异步性，不需要与用户提示在时间轴上对齐。</li>
<li>AudioJailbreak具有普遍性，单个越狱扰动可适用于不同的提示。</li>
<li>AudioJailbreak具有隐蔽性，恶意意图不会被受害者察觉。</li>
<li>AudioJailbreak具备抗干扰性，在播放过程中能有效抵御环境噪声。</li>
<li>AudioJailbreak适用于无法完全操控用户提示的对手，攻击场景更广泛。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14103">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-10c0994278ffcd0bb6d912472d0e4f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88755946603f8cbe258b2fe3cb0bb019.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fe8d54d42c0ffd4affae82ca1a116e94.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement"><a href="#SeamlessEdit-Background-Noise-Aware-Zero-Shot-Speech-Editing-with-in-Context-Enhancement" class="headerlink" title="SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement"></a>SeamlessEdit: Background Noise Aware Zero-Shot Speech Editing with   in-Context Enhancement</h2><p><strong>Authors:Kuan-Yu Chen, Jeng-Lin Li, Jian-Jiun Ding</strong></p>
<p>With the fast development of zero-shot text-to-speech technologies, it is possible to generate high-quality speech signals that are indistinguishable from the real ones. Speech editing, including speech insertion and replacement, appeals to researchers due to its potential applications. However, existing studies only considered clean speech scenarios. In real-world applications, the existence of environmental noise could significantly degrade the quality of the generation. In this study, we propose a noise-resilient speech editing framework, SeamlessEdit, for noisy speech editing. SeamlessEdit adopts a frequency-band-aware noise suppression module and an in-content refinement strategy. It can well address the scenario where the frequency bands of voice and background noise are not separated. The proposed SeamlessEdit framework outperforms state-of-the-art approaches in multiple quantitative and qualitative evaluations. </p>
<blockquote>
<p>随着零样本文本到语音技术的快速发展，生成高质量、与现实无法区分的语音信号成为可能。语音编辑，包括语音插入和替换，因其潜在的应用价值而吸引研究者关注。然而，现有研究仅涉及清洁语音场景。在真实世界应用中，环境噪声的存在可能会显著降低生成语音的质量。本研究提出了一种适用于噪声语音编辑的鲁棒性语音编辑框架SeamlessEdit。SeamlessEdit采用频带感知噪声抑制模块和基于内容的优化策略，能够很好地解决语音和背景噪声频带未分离的场景。所提出的SeamlessEdit框架在多个定量和定性评估中均优于最新技术方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14066v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong><br>     该研究提出了一种用于噪声语音编辑的SeamlessEdit框架，该框架具有噪声抑制和内容细化策略，适用于频率带未分离的噪声语音编辑场景，并在多项定量和定性评估中表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究关注噪声环境下的语音编辑，特别是针对现实世界中频率带未分离的噪声语音场景。</li>
<li>提出了一种名为SeamlessEdit的噪声鲁棒性语音编辑框架。</li>
<li>SeamlessEdit框架包含频率带感知的噪声抑制模块和基于内容的细化策略。</li>
<li>该框架在多项定量和定性评估中表现优于现有先进技术。</li>
<li>该研究旨在解决现有研究中只考虑干净语音场景的局限性。</li>
<li>SeamlessEdit具有广泛的应用潜力，尤其在需要高质量语音生成的领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14066">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3331afb116eb5a4dd722f832aec65be7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-352190ad478243fe4b96bd0542df1f43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3b0f11694b33d58d87e7efc60fe78c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-476197b8220b75e12102d575aa4a5566.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Semantic-Information-based-Hierarchical-Speech-Enhancement-Method-Using-Factorized-Codec-and-Diffusion-Model"><a href="#A-Semantic-Information-based-Hierarchical-Speech-Enhancement-Method-Using-Factorized-Codec-and-Diffusion-Model" class="headerlink" title="A Semantic Information-based Hierarchical Speech Enhancement Method   Using Factorized Codec and Diffusion Model"></a>A Semantic Information-based Hierarchical Speech Enhancement Method   Using Factorized Codec and Diffusion Model</h2><p><strong>Authors:Yang Xiang, Canan Huang, Desheng Hu, Jingguang Tian, Xinhui Hu, Chao Zhang</strong></p>
<p>Most current speech enhancement (SE) methods recover clean speech from noisy inputs by directly estimating time-frequency masks or spectrums. However, these approaches often neglect the distinct attributes, such as semantic content and acoustic details, inherent in speech signals, which can hinder performance in downstream tasks. Moreover, their effectiveness tends to degrade in complex acoustic environments. To overcome these challenges, we propose a novel, semantic information-based, step-by-step factorized SE method using factorized codec and diffusion model. Unlike traditional SE methods, our hierarchical modeling of semantic and acoustic attributes enables more robust clean speech recovery, particularly in challenging acoustic scenarios. Moreover, this method offers further advantages for downstream TTS tasks. Experimental results demonstrate that our algorithm not only outperforms SOTA baselines in terms of speech quality but also enhances TTS performance in noisy environments. </p>
<blockquote>
<p>当前大多数语音增强（SE）方法通过直接估计时间频率掩膜或频谱从噪声输入中恢复清洁语音。然而，这些方法往往忽视了语音信号所固有的不同属性，如语义内容和声学细节，这可能会阻碍下游任务的性能。此外，它们在复杂的声学环境中的有效性往往会降低。为了克服这些挑战，我们提出了一种基于语义信息、使用分解编码器和扩散模型的分步分解SE新方法。与传统的SE方法不同，我们对语义和声音属性的分层建模，能够实现更稳健的清洁语音恢复，特别是在具有挑战性的声学场景中。此外，此方法对于下游的TTS任务还具有进一步的优点。实验结果表明，我们的算法不仅在语音质量方面优于最新技术水平的基准测试，而且还提高了在噪声环境中TTS的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13843v1">PDF</a> Accepted by interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于语义信息的新型分步分解语音增强方法，使用分解编码器和扩散模型。该方法克服了传统语音增强方法在处理复杂声学环境下的局限性，它通过层次化建模语义和声音属性来实现更稳健的干净语音恢复，并有助于提高下游文本转语音任务的性能。实验结果表明，该算法不仅在语音质量上优于现有最佳基线，而且在噪声环境中提高了文本转语音的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前语音增强方法主要通过直接估计时间-频率掩膜或频谱来恢复干净语音，但忽略了语音信号中的语义内容和声音细节。</li>
<li>提出的基于语义信息的新型分步分解语音增强方法使用分解编码器和扩散模型，克服了传统方法在复杂声学环境下的性能下降问题。</li>
<li>该方法通过层次化建模语义和声音属性，实现了更稳健的干净语音恢复。</li>
<li>方法在噪声环境下表现出优异的性能，不仅提高了语音质量，还有助于提高下游文本转语音任务的性能。</li>
<li>实验结果表明，该算法在语音质量上优于现有最佳基线。</li>
<li>该方法对于解决复杂声学环境中的语音增强问题具有潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13843">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-49a63609cdc23e745f58767ab60aad88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a5ebd56600882d57e45be7fd643b077.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-453df19eca6bbc0e7e95cdadcca49a68.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising"><a href="#Improving-Noise-Robustness-of-LLM-based-Zero-shot-TTS-via-Discrete-Acoustic-Token-Denoising" class="headerlink" title="Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising"></a>Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete   Acoustic Token Denoising</h2><p><strong>Authors:Ye-Xin Lu, Hui-Peng Du, Fei Liu, Yang Ai, Zhen-Hua Ling</strong></p>
<p>Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models. </p>
<blockquote>
<p>基于大型语言模型（LLM）的零样本文本到语音（TTS）方法倾向于保留音频提示的声学环境，这导致当音频提示包含噪声时，合成语音的质量下降。在本文中，我们提出了一种新颖的基于神经编码器的语音去噪器，并将其与先进的LLM-based TTS模型LauraTTS相结合，实现了噪声鲁棒的零样本TTS。所提出的编码去噪器由音频编码器、令牌去噪器和嵌入精炼器组成。令牌去噪器从噪声令牌预测前两个组的干净声学令牌，这可以作为LauraTTS合成高质量个性化语音的声学提示，或者通过嵌入精炼器和编码器解码器转换为干净的语音波形。实验结果表明，我们提出的编码去噪器优于最新的语音增强（SE）方法，并且所提出的噪声鲁棒的LauraTTS超过了使用附加SE模型的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13830v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的零样本文本到语音（TTS）方法会保留音频提示的声学环境，当音频提示包含噪声时，会导致合成语音质量下降。本文提出了一种新颖的基于神经网络编解码器的语音去噪器，并将其与先进的LLM-based TTS模型LauraTTS相结合，实现了噪声鲁棒的零样本TTS。编解码器去噪器由音频编解码器、令牌去噪器和嵌入精炼器组成。令牌去噪器从噪声令牌预测前两组干净的声学令牌，可作为LauraTTS合成高质量个性化语音的声学提示，或通过嵌入精炼器和编解码器解码器转换为干净的语音波形。实验结果表明，本文提出的编解码器去噪器优于先进的语音增强（SE）方法，所提出的噪声鲁棒LauraTTS超越了使用附加SE模型的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在零样本文本到语音（TTS）应用中面临噪声问题，噪声会影响合成语音的质量。</li>
<li>提出了一种基于神经网络编解码器的语音去噪器，以处理音频中的噪声。</li>
<li>噪声去除过程包括预测干净的声学令牌和精炼嵌入。</li>
<li>该去噪器与LauraTTS模型结合，实现了噪声鲁棒的零样本TTS。</li>
<li>实验结果显示，提出的编解码器去噪器在性能上超越了现有的语音增强（SE）方法。</li>
<li>噪声鲁棒的LauraTTS模型在合成高质量语音方面表现出优势，相比使用附加SE模型的方法有更好的性能。</li>
<li>该研究为改进TTS技术在噪声环境下的性能提供了有效方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13830">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-996abe2385b849300f64b29747a9b154.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba58973ea83532ce32150a761118e124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-582a9fe21eabd309d2c2f7fd1e78776e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02c233800f084b2074fac3697b931790.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A3-an-Analytical-Low-Rank-Approximation-Framework-for-Attention"><a href="#A3-an-Analytical-Low-Rank-Approximation-Framework-for-Attention" class="headerlink" title="A3 : an Analytical Low-Rank Approximation Framework for Attention"></a>A3 : an Analytical Low-Rank Approximation Framework for Attention</h2><p><strong>Authors:Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao</strong></p>
<p>Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component’s functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA’s 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance. </p>
<blockquote>
<p>大型语言模型展现了出色的性能，但其庞大的参数数量导致部署成本高昂。低秩近似提供了一种有前景的压缩解决方案，但现有方法存在两个主要局限性：（1）它们专注于最小化单个线性层的输出误差，而没有考虑到Transformer的架构特性；（2）它们将大型权重矩阵分解为两个小的低秩矩阵。因此，这些方法通常比其他压缩技术（如剪枝和量化）逊色，并且引入了运行时开销，例如为分解的小矩阵额外启动GEMM内核。为了解决这些局限性，我们提出了$\tt A^\tt 3$，这是一种训练后的低秩近似框架。$\tt A^\tt 3$将Transformer层分为三个功能组件，即$\tt QK$、$\tt OV$和$\tt MLP$。对于每个组件，$\tt A^\tt 3$提供了一种分析解决方案，该方案在减小每个组件内的隐藏维度大小的同时，最小化组件的功能损失（即注意分数、注意输出和MLP输出的误差）。这种方法直接减小了模型大小、KV缓存大小并减少了浮点运算次数，同时没有引入任何运行时开销。此外，它从单一的线性层损失优化问题出发，为优化问题提供了新的叙事方向，以改进端到端的性能。通过广泛的实验，我们证明了$\tt A^\tt 3$在保持优于现有技术的同时，也取得了出色的性能。例如，在相同的计算和内存减少预算下，我们的低秩估计LLaMA 3.1-70B在WikiText-2上取得了困惑度4.69的成绩，优于以前的最优成绩7.87达3.18。我们还证明了$\tt A^\tt 3$的通用性，包括KV缓存压缩、量化和混合排名分配以增强性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12942v1">PDF</a> </p>
<p><strong>摘要</strong><br>大型语言模型表现出卓越的性能，但其庞大的参数数量导致部署成本高昂。低秩近似提供了一种有前景的压缩解决方案，但现有方法主要关注最小化单个线性层的输出误差，不考虑Transformer的架构特性。此外，它们将大权重矩阵分解为两个小低秩矩阵，导致与其他压缩技术相比效果有限，并引入了运行时开销。为解决这些问题，我们提出了$\tt A^\tt 3$，一个针对Transformer层的后训练低秩近似框架。$\tt A^\tt 3$将Transformer层分为$\tt QK$、$\tt OV$和$\tt MLP$三个功能组件，并为每个组件提供分析解决方案，以减少隐藏维度大小并最小化组件的功能损失。这种方法直接减小了模型大小、KV缓存大小和浮点运算次数，且没有引入任何运行时开销。此外，它改变了优化问题的叙事方向，从单一线性层损失优化转向改进端到端性能。实验表明，在相同的计算和内存减少预算下，我们的低秩近似LLaMA 3.1-70B在WikiText-2上实现了4.69的困惑度，优于之前的最优水平7.87。我们还展示了$\tt A^\tt 3$的通用性，包括KV缓存压缩、量化和混合秩分配以增强性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型具有卓越性能，但部署成本高昂。</li>
<li>低秩近似是一种有效的模型压缩解决方案。</li>
<li>现有低秩近似方法存在两个主要局限性：关注单个线性层的输出误差和分解成小低秩矩阵导致的运行时开销。</li>
<li>$\tt A^\tt 3$框架解决了这些问题，通过考虑Transformer的架构特性来减少隐藏维度并最小化功能损失。</li>
<li>$\tt A^\tt 3$直接减小了模型大小、KV缓存大小和浮点运算次数，且没有增加运行时开销。</li>
<li>$\tt A^\tt 3$改变了优化问题的方向，从单一线性层损失优化转向改进端到端性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12942">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd46b514cce404e31b1a39da66e3770e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f63b2e5df75e2ab6f6ffb71c4f651a40.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OZSpeech-One-step-Zero-shot-Speech-Synthesis-with-Learned-Prior-Conditioned-Flow-Matching"><a href="#OZSpeech-One-step-Zero-shot-Speech-Synthesis-with-Learned-Prior-Conditioned-Flow-Matching" class="headerlink" title="OZSpeech: One-step Zero-shot Speech Synthesis with   Learned-Prior-Conditioned Flow Matching"></a>OZSpeech: One-step Zero-shot Speech Synthesis with   Learned-Prior-Conditioned Flow Matching</h2><p><strong>Authors:Hieu-Nghia Huynh-Nguyen, Ngoc Son Nguyen, Huynh Nguyen Dang, Thieu Vo, Truong-Son Hy, Van Nguyen</strong></p>
<p>Text-to-speech (TTS) systems have seen significant advancements in recent years, driven by improvements in deep learning and neural network architectures. Viewing the output speech as a data distribution, previous approaches often employ traditional speech representations, such as waveforms or spectrograms, within the Flow Matching framework. However, these methods have limitations, including overlooking various speech attributes and incurring high computational costs due to additional constraints introduced during training. To address these challenges, we introduce OZSpeech, the first TTS method to explore optimal transport conditional flow matching with one-step sampling and a learned prior as the condition, effectively disregarding preceding states and reducing the number of sampling steps. Our approach operates on disentangled, factorized components of speech in token format, enabling accurate modeling of each speech attribute, which enhances the TTS system’s ability to precisely clone the prompt speech. Experimental results show that our method achieves promising performance over existing methods in content accuracy, naturalness, prosody generation, and speaker style preservation. Audio samples are available at our demo page <a target="_blank" rel="noopener" href="https://ozspeech.github.io/OZSpeech_Web/">https://ozspeech.github.io/OZSpeech_Web/</a>. </p>
<blockquote>
<p>文本转语音（TTS）系统在近年来取得了显著进展，这一进展是由深度学习和神经网络架构的改进所驱动的。将输出语音视为数据分布，之前的方法经常在流匹配框架中使用传统的语音表示方法，例如波形或频谱图。然而，这些方法具有局限性，包括忽略了各种语音属性以及在训练过程中引入的额外约束导致计算成本高昂。为了解决这些挑战，我们引入了OZSpeech，这是一种TTS方法，首次尝试在一步采样过程中采用最优传输条件流匹配以及通过学习获得的先验条件，有效地忽略了先前的状态并减少了采样步骤的数量。我们的方法以令牌格式运行语音的解耦和分解成分，能够对每个语音属性进行精确建模，这提高了TTS系统精确克隆提示语音的能力。实验结果表明，我们的方法在内容准确性、自然度、语调生成和说话人风格保持方面均取得了令人鼓舞的性能表现。音频样本可在我们的演示页面<a target="_blank" rel="noopener" href="https://ozspeech.github.io/OZSpeech_Web/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://ozspeech.github.io/OZSpeech_Web/上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TTS系统近年因深度学习与神经网络架构的进展而有所突破。传统方法常采用波形或频谱图等语音表现形式，在Flow Matching框架下存在忽略不同语音属性和训练过程中额外约束导致的高计算成本问题。为解决此，提出OZSpeech方法，探索优化传输条件流匹配，采用一步采样和条件学习先验，摒弃先前状态，减少采样步骤。该方法在语音标记格式上操作解耦、分解的组件，能精准建模各语音属性，提升TTS系统对提示语音的精确克隆能力。实验结果显示，该方法在内容准确性、自然度、语调生成和说话人风格保持等方面较现有方法表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS系统近年有显著进步，主要得益于深度学习和神经网络的发展。</li>
<li>传统TTS方法采用波形或频谱图等语音表现形式，在Flow Matching框架下面临挑战。</li>
<li>OZSpeech是首个探索优化传输条件流匹配的TTS方法。</li>
<li>OZSpeech采用一步采样和条件学习先验，简化过程并提升性能。</li>
<li>OZSpeech方法在语音标记格式上操作解耦的组件，能精准建模各语音属性。</li>
<li>OZSpeech在内容准确性、自然度、语调生成和说话人风格保持等方面表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c243f932226d92c40468490fa20dd6de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7b9151fe44927a89689f6c4b1c4048a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ea6aa40f2e6467556f52b55cf84de6c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RoVo-Robust-Voice-Protection-Against-Unauthorized-Speech-Synthesis-with-Embedding-Level-Perturbations"><a href="#RoVo-Robust-Voice-Protection-Against-Unauthorized-Speech-Synthesis-with-Embedding-Level-Perturbations" class="headerlink" title="RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with   Embedding-Level Perturbations"></a>RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with   Embedding-Level Perturbations</h2><p><strong>Authors:Seungmin Kim, Sohee Park, Donghyun Kim, Jisu Lee, Daeseon Choi</strong></p>
<p>With the advancement of AI-based speech synthesis technologies such as Deep Voice, there is an increasing risk of voice spoofing attacks, including voice phishing and fake news, through unauthorized use of others’ voices. Existing defenses that inject adversarial perturbations directly into audio signals have limited effectiveness, as these perturbations can easily be neutralized by speech enhancement methods. To overcome this limitation, we propose RoVo (Robust Voice), a novel proactive defense technique that injects adversarial perturbations into high-dimensional embedding vectors of audio signals, reconstructing them into protected speech. This approach effectively defends against speech synthesis attacks and also provides strong resistance to speech enhancement models, which represent a secondary attack threat.   In extensive experiments, RoVo increased the Defense Success Rate (DSR) by over 70% compared to unprotected speech, across four state-of-the-art speech synthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial speaker-verification API, effectively neutralizing speech synthesis attack. Moreover, RoVo’s perturbations remained robust even under strong speech enhancement conditions, outperforming traditional methods. A user study confirmed that RoVo preserves both naturalness and usability of protected speech, highlighting its effectiveness in complex and evolving threat scenarios. </p>
<blockquote>
<p>随着基于深度声音等人工智能的语音合成技术的进步，通过非法使用他人声音进行语音欺骗攻击（包括语音钓鱼和假新闻）的风险日益增加。现有通过在音频信号中直接注入对抗性扰动来进行防御的方法效果有限，因为这些扰动很容易被语音增强方法所中和。为了克服这一局限性，我们提出了RoVo（Robust Voice）这一新型主动防御技术，它将对抗性扰动注入音频信号的高维嵌入向量中，然后重构为受保护的语音。该方法有效防御语音合成攻击，并对代表次要攻击威胁的语音增强模型提供强大的抵抗力。在广泛的实验中，与未受保护的语音相比，RoVo将防御成功率（DSR）提高了70%以上，涵盖了四种最先进的语音合成模型。具体来说，RoVo在商业语音验证API上实现了99.5%的DSR，有效中和了语音合成攻击。此外，即使在强烈的语音增强条件下，RoVo的扰动依然稳健，优于传统方法。用户研究证实，RoVo保留了受保护语音的自然性和可用性，突显其在复杂和不断发展的威胁场景中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12686v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为RoVo（Robust Voice）的新型主动防御技术，该技术通过向音频信号的高维嵌入向量注入对抗性扰动来抵抗语音欺骗攻击。相较于传统的直接对音频信号注入扰动的方法，RoVo能够有效对抗语音增强方法的破解，并提供强大的防护效果。实验表明，RoVo相较于未保护的语音，在四种先进的语音合成模型上的防御成功率（DSR）提升了70%以上。特别是在商业语音识别API上，RoVo的DSR达到了99.5%，有效中断了语音合成攻击。此外，即使在强烈的语音增强条件下，RoVo的扰动依然稳健，超越了传统方法。用户研究证实，RoVo能够保持保护语音的自然性和可用性，在复杂的不断变化的威胁场景中表现出强大的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RoVo是一种新型的主动防御技术，通过向音频信号的高维嵌入向量注入对抗性扰动来防御语音欺骗攻击。</li>
<li>RoVo能有效对抗语音增强方法的破解，提供了强大的防护效果。</li>
<li>实验显示，RoVo在多种先进的语音合成模型上的防御成功率（DSR）提升了70%以上。</li>
<li>RoVo在商业语音识别API上的DSR达到了99.5%，能有效中断语音合成攻击。</li>
<li>RoVo的扰动在强烈的语音增强条件下依然稳健，性能超越传统方法。</li>
<li>用户研究证实，RoVo能够保持保护语音的自然性和可用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12686">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fa34982b0b1a7c43396f3023c6c2763f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-319386b67e3804d8bf39be30815b9ee8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3920983b2afe1e96e515cd26c6a28565.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab7d27b07152d54b88a6eb423ba039d0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Chain-Talker-Chain-Understanding-and-Rendering-for-Empathetic-Conversational-Speech-Synthesis"><a href="#Chain-Talker-Chain-Understanding-and-Rendering-for-Empathetic-Conversational-Speech-Synthesis" class="headerlink" title="Chain-Talker: Chain Understanding and Rendering for Empathetic   Conversational Speech Synthesis"></a>Chain-Talker: Chain Understanding and Rendering for Empathetic   Conversational Speech Synthesis</h2><p><strong>Authors:Yifan Hu, Rui Liu, Yi Ren, Xiang Yin, Haizhou Li</strong></p>
<p>Conversational Speech Synthesis (CSS) aims to align synthesized speech with the emotional and stylistic context of user-agent interactions to achieve empathy. Current generative CSS models face interpretability limitations due to insufficient emotional perception and redundant discrete speech coding. To address the above issues, we present Chain-Talker, a three-stage framework mimicking human cognition: Emotion Understanding derives context-aware emotion descriptors from dialogue history; Semantic Understanding generates compact semantic codes via serialized prediction; and Empathetic Rendering synthesizes expressive speech by integrating both components. To support emotion modeling, we develop CSS-EmCap, an LLM-driven automated pipeline for generating precise conversational speech emotion captions. Experiments on three benchmark datasets demonstrate that Chain-Talker produces more expressive and empathetic speech than existing methods, with CSS-EmCap contributing to reliable emotion modeling. The code and demos are available at: <a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/Chain-Talker">https://github.com/AI-S2-Lab/Chain-Talker</a>. </p>
<blockquote>
<p>对话式语音合成（CSS）旨在将合成语音与用户代理交互的情感和风格语境相结合，以实现共情。当前的生成式CSS模型面临可解释性有限的难题，原因在于情感感知不足和冗余的离散语音编码。为了解决上述问题，我们推出了Chain-Talker，这是一个模仿人类认知的三阶段框架：情感理解从对话历史中推导出上下文相关的情感描述符；语义理解通过序列化预测生成紧凑的语义代码；共情渲染通过整合这两个组件合成表达性语音。为了支持情感建模，我们开发了CSS-EmCap，这是一个基于大型语言模型的自动化管道，用于生成精确的对话语音情感字幕。在三个基准数据集上的实验表明，Chain-Talker相比现有方法产生了更具表现力和共情效果的语音，CSS-EmCap对可靠的情感建模做出了贡献。代码和演示可在<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/Chain-Talker%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AI-S2-Lab/Chain-Talker找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12597v1">PDF</a> 16 pages, 5 figures, 5 tables. Accepted by ACL 2025 (Findings)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Conversational Speech Synthesis（CSS）的目标是通过合成与用户代理互动时的情感和风格一致的语音来实现共情。针对当前CSS模型在情感感知和冗余离散语音编码方面的不足，提出了Chain-Talker框架，该框架模仿人类认知的三个步骤：情感理解从对话历史中得出情境感知的情感描述符；语义理解通过序列化预测生成紧凑的语义代码；共情渲染通过结合这两个组件合成富有表现力的语音。为支持情感建模，开发了CSS-EmCap，这是一个基于大型语言模型的自动化管道，用于生成精确的对话语料情感字幕。在三个基准数据集上的实验表明，Chain-Talker相较于现有方法，能生成更富有表现力和共情的语音，CSS-EmCap对可靠的情感建模起到了贡献。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Conversational Speech Synthesis (CSS)旨在合成与用户代理互动时情感和风格一致的语音，以实现共情。</li>
<li>当前CSS模型面临情感感知不足和冗余离散语音编码的局限性。</li>
<li>Chain-Talker框架模仿人类认知的三个步骤：情感理解、语义理解和共情渲染。</li>
<li>情感理解通过对话历史得出情境感知的情感描述符。</li>
<li>语义理解生成紧凑的语义代码，通过序列化预测实现。</li>
<li>Empathetic Rendering结合了情感理解和语义理解，合成富有表现力的语音。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-da9f623349aa038129f1df66714e9b2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f6cdd6135c0673428456dceaddb61d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf76b9501af682faaab986e5985edde6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Shallow-Flow-Matching-for-Coarse-to-Fine-Text-to-Speech-Synthesis"><a href="#Shallow-Flow-Matching-for-Coarse-to-Fine-Text-to-Speech-Synthesis" class="headerlink" title="Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis"></a>Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis</h2><p><strong>Authors:Dong Yang, Yiyi Cai, Yuki Saito, Lixu Wang, Hiroshi Saruwatari</strong></p>
<p>We propose a shallow flow matching (SFM) mechanism to enhance flow matching (FM)-based text-to-speech (TTS) models within a coarse-to-fine generation paradigm. SFM constructs intermediate states along the FM paths using coarse output representations. During training, we introduce an orthogonal projection method to adaptively determine the temporal position of these states, and apply a principled construction strategy based on a single-segment piecewise flow. The SFM inference starts from the intermediate state rather than pure noise and focuses computation on the latter stages of the FM paths. We integrate SFM into multiple TTS models with a lightweight SFM head. Experiments show that SFM consistently improves the naturalness of synthesized speech in both objective and subjective evaluations, while significantly reducing inference when using adaptive-step ODE solvers. Demo and codes are available at <a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/">https://ydqmkkx.github.io/SFMDemo/</a>. </p>
<blockquote>
<p>我们提出了一种浅流匹配（SFM）机制，以在粗到细生成范式内增强基于流匹配（FM）的文本到语音（TTS）模型。SFM利用粗输出表示构建FM路径中的中间状态。在训练过程中，我们引入了一种正交投影方法来自适应地确定这些状态的时间位置，并基于单段分段流应用了一种有原则的构建策略。SFM推理从中间状态开始，而不是从纯噪声开始，并将计算重点放在FM路径的后期阶段。我们将SFM集成到多个TTS模型中，并使用轻量级的SFM头。实验表明，无论是在客观还是主观评估中，SFM都能持续提高合成语音的自然度，同时在采用自适应步长ODE求解器时显著降低推理时间。演示和代码可通过<a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/%E8%AE%BF%E9%97%AE%E3%80%82">https://ydqmkkx.github.io/SFMDemo/访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12226v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种浅流匹配（SFM）机制，用于在粗到细生成范式中增强基于流匹配（FM）的文本到语音（TTS）模型。SFM通过在FM路径上构建中间状态，利用粗输出表示。在训练过程中，引入正交投影方法自适应确定这些状态的时间位置，并基于单段分段流应用有原则的构建策略。SFM推理从中间状态开始，而不是从纯噪声开始，并将计算重点放在FM路径的后期阶段。我们将SFM集成到多个TTS模型中，并使用轻量级SFM头。实验表明，SFM在客观和主观评估中均提高了合成语音的自然度，同时在自适应步长ODE求解器使用时显著减少了推理时间。相关演示和代码可在<a target="_blank" rel="noopener" href="https://ydqmkkx.github.io/SFMDemo/%E6%89%BE%E5%88%B0%E3%80%82">https://ydqmkkx.github.io/SFMDemo/找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了浅流匹配（SFM）机制来增强基于流匹配（FM）的文本到语音（TTS）模型性能。</li>
<li>SFM机制通过构建FM路径上的中间状态并利用粗输出表示来实现。</li>
<li>训练过程中，采用正交投影方法自适应确定中间状态的时间位置。</li>
<li>引入了基于单段分段流的构建策略。</li>
<li>SFM推理从中间状态开始，关注FM路径的后期计算。</li>
<li>SFM被集成到多个TTS模型中，并通过轻量级SFM头实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12226">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-05b333ababfe421ec07abae91b89b520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93ccf19c0b69d0575db4e284364761f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5e6db09a073f6c66aee05d9747bc3f7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Rethinking-Optimal-Verification-Granularity-for-Compute-Efficient-Test-Time-Scaling"><a href="#Rethinking-Optimal-Verification-Granularity-for-Compute-Efficient-Test-Time-Scaling" class="headerlink" title="Rethinking Optimal Verification Granularity for Compute-Efficient   Test-Time Scaling"></a>Rethinking Optimal Verification Granularity for Compute-Efficient   Test-Time Scaling</h2><p><strong>Authors:Hao Mark Chen, Guanxi Lu, Yasuyuki Okoshi, Zhiwen Mo, Masato Motomura, Hongxiang Fan</strong></p>
<p>Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1% over Beam Search and 3.6% over Best-of-N, while reducing FLOPs by over 52%. We will open-source the code to support future research. </p>
<blockquote>
<p>测试时缩放（TTS）已证明可以有效提高大型语言模型（LLM）的推理能力。验证在TTS中扮演着关键角色，由于验证的质量和计算成本，它同时影响（1）推理性能和（2）计算效率。在这项工作中，我们挑战了验证的传统范式，并首次尝试系统地研究验证粒度的影响，即验证器在生成过程中被调用的频率，而不仅仅是验证最终输出或单个生成步骤。为此，我们引入了可变粒度搜索（VG-Search），这是一种通过可调粒度参数g推广束搜索和N选最佳采样的统一算法。使用VG-Search在多种计算预算、生成器验证器配置和任务属性下进行的广泛实验表明，动态选择g可以提高计算效率和缩放性能。基于这些发现，我们提出了自适应VG-Search策略，与束搜索相比，实现了高达3.1%的准确率提升，与N选最佳策略相比提高了3.6%，同时减少了超过52%的浮点运算次数。我们将公开源代码以支持未来的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11730v1">PDF</a> Preprint. Under review</p>
<p><strong>Summary</strong><br>文本描述了测试时间缩放（TTS）在提高大型语言模型（LLM）的推理能力方面的有效性。验证在TTS中扮演关键角色，影响推理性能和计算效率。本研究挑战了验证的传统模式，首次尝试系统地研究验证粒度的影响，即验证器在生成过程中被调用的频率，而不仅仅是验证最终输出或单个生成步骤。为此，我们引入了可变粒度搜索（VG-Search）算法，通过可调的粒度参数g来概括beam搜索和Best-of-N采样。通过广泛的实验，我们发现动态选择g可以提高计算效率和缩放性能。在此基础上，我们提出了自适应VG-Search策略，与Beam Search相比提高了高达3.1%的准确率，与Best-of-N相比提高了高达3.6%的准确率，同时降低了超过52%的FLOPs。我们将开源代码以支持未来的研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时间缩放（TTS）技术增强了大型语言模型（LLM）的推理能力。</li>
<li>验证在TTS中扮演关键角色，同时影响推理性能和计算效率。</li>
<li>本研究挑战了验证的传统模式，并系统地研究了验证粒度对TTS的影响。</li>
<li>引入了可变粒度搜索（VG-Search）算法，通过调节粒度参数g来提高计算效率和推理性能。</li>
<li>动态选择g可以进一步提高计算效率和缩放性能。</li>
<li>自适应VG-Search策略在准确率上取得了显著的提升，同时降低了计算成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d4f101bf2775b97a597e50486b127906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8cd0bb8c67055f2a9a9b831099a461f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-332a1cbf8a71aa0e5e28b983c9e17111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b78848698f88512ea5afc13d1da6d49a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="F5-TTS-A-Fairytaler-that-Fakes-Fluent-and-Faithful-Speech-with-Flow-Matching"><a href="#F5-TTS-A-Fairytaler-that-Fakes-Fluent-and-Faithful-Speech-with-Flow-Matching" class="headerlink" title="F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow   Matching"></a>F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow   Matching</h2><p><strong>Authors:Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, Xie Chen</strong></p>
<p>This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model’s performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our F5-TTS exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. We have released all codes and checkpoints to promote community development, at <a target="_blank" rel="noopener" href="https://swivid.github.io/F5-TTS/">https://SWivid.github.io/F5-TTS/</a>. </p>
<blockquote>
<p>本文介绍了F5-TTS，这是一个完全非自回归的文本到语音系统，基于带有扩散转换器（DiT）的流程匹配。该系统无需复杂的设计，如持续时间模型、文本编码器和音素对齐。文本输入只需用填充标记填充至与输入语音相同的长度，然后对语音生成进行去噪处理，这种方式的可行性最初由E2 TTS证明。然而，E2 TTS的原始设计由于其收敛速度慢和稳健性低，难以跟随应用。为了解决这些问题，我们首先使用ConvNeXt对输入进行建模，以优化文本表示，使其易于与语音对齐。我们还提出了一种推理时间的摇摆采样策略，该策略显著提高了模型的性能和效率。这种用于流程步骤的采样策略可以轻松地应用于现有的基于流程匹配模型，无需重新训练。我们的设计允许更快的训练，实现推理实时转录因子（RTF）为0.15，与现有的基于扩散的TTS模型相比，这一改进非常显著。我们的F5-TTS在公共的10万小时多语种数据集上进行训练，表现出高度自然和表达能力的零样本能力、无缝的代码切换能力和速度控制效率。为了促进社区发展，我们已在<a target="_blank" rel="noopener" href="https://swivid.github.io/F5-TTS/%E5%8F%96%E5%B9%BF%E5%AE%9E%E7%94%A8%E4%BA%86%E5%B9%B3%E9%A3%9F%E6%AF%AD%E4%BF%AE">https://SWivid.github.io/F5-TTS/发布了所有代码和检查点。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06885v3">PDF</a> 17 pages, 9 tables, 3 figures</p>
<p><strong>Summary</strong><br>基于扩散变换（DiT）的文本转语音系统F5-TTS被引入。它通过流匹配实现非自回归性，无需复杂的组件设计如持续时间模型、文本编码器和音素对齐。通过对文本输入填充空白标记，对语音进行降噪生成。为改进原始E2 TTS设计带来的缓慢收敛和低鲁棒性问题，我们引入了ConvNeXt进行输入建模并改进了文本表示。此外，我们提出了一种名为Sway Sampling的策略，显著提高了模型性能和效率。此采样策略可轻松应用于现有的流匹配模型，无需重新训练。我们的设计加速了训练过程，实现了0.15的推理RTF，相较于现有的扩散式TTS模型有了显著改进。F5-TTS在公共的10万小时多语种数据集上训练，展现出自然、表达力强、无缝切换代码能力和速度控制效率等特点。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>F5-TTS是一个基于扩散变换的非自回归文本转语音系统。</li>
<li>通过简单的填充空白标记和降噪实现语音生成，无需复杂的组件设计。</li>
<li>引入ConvNeXt改进文本表示和对齐问题。</li>
<li>提出Sway Sampling策略，提高模型性能和效率，可轻松应用于现有模型。</li>
<li>实现了快速的训练过程和高效的推理RTF。</li>
<li>F5-TTS在多语种数据集上训练，展现出自然、表达力强、无缝切换代码能力和速度控制效率等特点。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-871a524a04b54257cec43839b351bc99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67dcb157c753501c66d6578d02ab5c0d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-22/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cbeb15bbf97efd2c8e4239904defc5ce.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-05-22  Beyond Words Multimodal LLM Knows When to Speak
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-2dbd53b28128cc9bbb530fa5610b868a.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-22  Automated Fetal Biometry Assessment with Deep Ensembles using   Sparse-Sampling of 2D Intrapartum Ultrasound Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
